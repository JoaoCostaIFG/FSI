{
  "responseHeader": {
    "status": 0,
    "QTime": 0
  },
  "response": {
    "numFound": 16079,
    "start": 0,
    "numFoundExact": true,
    "docs": [
      {
        "story_id": [19706396],
        "story_author": ["beefhash"],
        "story_descendants": [10],
        "story_score": [125],
        "story_time": ["2019-04-20T13:22:38Z"],
        "story_title": "Fh: File history with ed, diff, awk, sed, and sh",
        "search": [
          "Fh: File history with ed, diff, awk, sed, and sh",
          "https://github.com/xorhash/fh",
          "fh records changes to a file on a per-file basis, similar to RCS and SCCS. It is, however, considerably more primitive. Design goals: no support for multi-user environments (no locking, etc.) implemented in shell script must use ed(1) should work or easily be made to work on 7th Edition UNIX I've taken care not to use any shell scripting constructions that didn't exist in the Bourne shell, but I may have missed things; however, the shebang needs to be removed on 7th Edition UNIX. fh uses a chain of ed(1) scripts to construct any version of a file. It even allows recording of commit messages. Why? I saw the following passage in diff(1) of 7th Edition UNIX: The -e option produces a script of a, c and d commands for the editor ed, which will recreate file2 from file1. The -f option produces a similar script, not useful with ed, in the opposite order. In connection with -e, the following shell program may help maintain multiple versions of a file. Only an ancestral file ($1) and a chain of version-to-version ed scripts ($2,$3,...) made by diff need be on hand. A `latest version' appears on the standard output. (shift; cat $*; echo 1,$p) ed - $1 After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious. License ISC, see LICENSE. Installation and usage Copy the fl, fr and fu files to a directory in $PATH; make sure they are marked as executable. Copy the man pages fl.1, fr.1, fu.1 and fh.5 to a directory in $MANPATH. For usage, see the supplied man pages. man.md is available on the web. For 7th Edition UNIX, the man pages written in mdoc macrosneed to be converted to old man macros first: mkdir man mandoc -Tman fl.1 > man/fl.1 mandoc -Tman fr.1 > man/fr.1 mandoc -Tman fu.1 > man/fu.1 mandoc -Tman fh.5 > man/fh.5 ",
          "Wait, what?<p><i>\"After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious.\"</i><p>Ah, ok.<p>Also, if you want to experiment with diffs and recreating versions:<p>Patch: <a href=\"https://en.m.wikipedia.org/wiki/Patch_(Unix)\" rel=\"nofollow\">https://en.m.wikipedia.org/wiki/Patch_(Unix)</a><p>Xdelta:  <a href=\"https://github.com/jmacd/xdelta\" rel=\"nofollow\">https://github.com/jmacd/xdelta</a>",
          "It's not a bad exercise.  In a Unix class I took, we had to implement an RDBMS using only the traditional Unix software tools.  A little later, for work, I had to learn to write highly portable and fairly secure shell scripts.<p>A few reasons to do exercises like this:<p>* To understand the Unix software tools and Bourne/etc. shell scripting models.  There are good lessons, and also examples of what not to do (or the tradeoffs), which I don't think you'll find anywhere else.  (For example, the simple Unix streams model is very powerful, and also often used in very kludgey ways in practice, and the Bourne evaluation model is probably much worse and dangerous than a beginner might think.)<p>* Knowing how to do things with just shell scripts is great for some kinds of bootstrapping, commands in build tools, or other situations in which you can only assume a shell interpreter and possibly certain command-line tools.<p>* Handy for configuring your interactive shell to do what you want.<p>* It's one way to familiarize with parts of a Unix-ish system that you might not normally, especially if you're spending all your time learning your way around huge stacks of tools that obscure the lower-level mechanics.<p>* You'll appreciate Perl and other languages much more, in some ways."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/xorhash/fh",
        "url_text": "fh records changes to a file on a per-file basis, similar to RCS and SCCS. It is, however, considerably more primitive. Design goals: no support for multi-user environments (no locking, etc.) implemented in shell script must use ed(1) should work or easily be made to work on 7th Edition UNIX I've taken care not to use any shell scripting constructions that didn't exist in the Bourne shell, but I may have missed things; however, the shebang needs to be removed on 7th Edition UNIX. fh uses a chain of ed(1) scripts to construct any version of a file. It even allows recording of commit messages. Why? I saw the following passage in diff(1) of 7th Edition UNIX: The -e option produces a script of a, c and d commands for the editor ed, which will recreate file2 from file1. The -f option produces a similar script, not useful with ed, in the opposite order. In connection with -e, the following shell program may help maintain multiple versions of a file. Only an ancestral file ($1) and a chain of version-to-version ed scripts ($2,$3,...) made by diff need be on hand. A `latest version' appears on the standard output. (shift; cat $*; echo 1,$p) ed - $1 After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious. License ISC, see LICENSE. Installation and usage Copy the fl, fr and fu files to a directory in $PATH; make sure they are marked as executable. Copy the man pages fl.1, fr.1, fu.1 and fh.5 to a directory in $MANPATH. For usage, see the supplied man pages. man.md is available on the web. For 7th Edition UNIX, the man pages written in mdoc macrosneed to be converted to old man macros first: mkdir man mandoc -Tman fl.1 > man/fl.1 mandoc -Tman fr.1 > man/fr.1 mandoc -Tman fu.1 > man/fu.1 mandoc -Tman fh.5 > man/fh.5 ",
        "comments.comment_id": [19706631, 19706816],
        "comments.comment_author": ["tyingq", "neilv"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-04-20T14:22:11Z",
          "2019-04-20T14:54:29Z"
        ],
        "comments.comment_text": [
          "Wait, what?<p><i>\"After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious.\"</i><p>Ah, ok.<p>Also, if you want to experiment with diffs and recreating versions:<p>Patch: <a href=\"https://en.m.wikipedia.org/wiki/Patch_(Unix)\" rel=\"nofollow\">https://en.m.wikipedia.org/wiki/Patch_(Unix)</a><p>Xdelta:  <a href=\"https://github.com/jmacd/xdelta\" rel=\"nofollow\">https://github.com/jmacd/xdelta</a>",
          "It's not a bad exercise.  In a Unix class I took, we had to implement an RDBMS using only the traditional Unix software tools.  A little later, for work, I had to learn to write highly portable and fairly secure shell scripts.<p>A few reasons to do exercises like this:<p>* To understand the Unix software tools and Bourne/etc. shell scripting models.  There are good lessons, and also examples of what not to do (or the tradeoffs), which I don't think you'll find anywhere else.  (For example, the simple Unix streams model is very powerful, and also often used in very kludgey ways in practice, and the Bourne evaluation model is probably much worse and dangerous than a beginner might think.)<p>* Knowing how to do things with just shell scripts is great for some kinds of bootstrapping, commands in build tools, or other situations in which you can only assume a shell interpreter and possibly certain command-line tools.<p>* Handy for configuring your interactive shell to do what you want.<p>* It's one way to familiarize with parts of a Unix-ish system that you might not normally, especially if you're spending all your time learning your way around huge stacks of tools that obscure the lower-level mechanics.<p>* You'll appreciate Perl and other languages much more, in some ways."
        ],
        "id": "c6e31655-ccb7-4cba-a30a-ac4e1fea6799",
        "_version_": 1718527400651784192
      },
      {
        "story_id": [20745393],
        "story_author": ["ingve"],
        "story_descendants": [355],
        "story_score": [357],
        "story_time": ["2019-08-20T10:39:18Z"],
        "story_title": "Sunsetting Mercurial Support in Bitbucket",
        "search": [
          "Sunsetting Mercurial Support in Bitbucket",
          "https://bitbucket.org/blog/sunsetting-mercurial-support-in-bitbucket",
          "[Update Aug 26, 2020] All hg repos have now been disabled and cannot be accessed.[Update July 1, 2020] Today, mercurial repositories, snippets, and wikis will turn to read-only mode. After July 8th, 2020 they will no longer be accessible. The version control software market has evolved a lot since Bitbucket began in 2008. When we launched, centralized version control was the norm and we only supported Mercurial repos. But Git adoption has grown over the years to become the default system, helping teams of all sizes work faster as they become more distributed.As we surpass 10 million registered users on the platform, we're at a point in our growth where we are conducting a deeper evaluation of the market and how we can best support our users going forward. After much consideration, we've decided to remove Mercurial support from Bitbucket Cloud and its API. Mercurial features and repositories will be officially deprecated on July 1, 2020.Read on to learn more about this decision, the important timelines, and get migration resources and support. The timeline and how this may affect your team Here are the key dates as we sunset Mercurial functionality: February 1, 2020: users will no longer be able to create new Mercurial repositories [Extended] July 1, 2020: users will not be able to use Mercurial features. All hg repos, wikis, and snippets will be in read-only mode. Heres why were focusing on Git This wasnt an easy decision, and Mercurial will always have a special place in Bitbuckets history. DevOps adoption has skyrocketed over the last decade and our customers are adopting this new way of working at an exponential rate. In this time, Bitbucket has steadily grown from being just a version control management tool to being a place to manage the entire software development lifecycle. And there's always more work to be done. This year we will concentrate on building deeper integrations to enhance automation and collaboration. Our improvements will make it even easier and safer to plan, code, test, and deploy all from within Bitbucket. Building quality features requires intense focus, and supporting two version control systems means splitting focus doubling shipping time and technical overhead. With Git being the more popularly used tool, Mercurial runs the risk of overlooked issues as we scale. According to a Stack Overflow Developer Survey, almost 90% of developers use Git, while Mercurial is the least popular version control system with only about 3% developer adoption. In fact, Mercurial usage on Bitbucket is steadily declining, and the percentage of new Bitbucket users choosing Mercurial has fallen to less than 1%. This deprecation will enable us to focus on building the best possible experience for our users. How to migrate and export We recommend that teams migrate their existing Mercurial repos to Git. There are various Git conversion tools in the market, including hg-fast-export and hg-git mercurial plugin. We are happy to support your migration, and you can find a discussion about available options in our dedicated Community thread. If you prefer to continue using the Mercurial system, there are a number of free and paid Mercurial hosting services. We realize that there is no one-size-fits-all solution. That's why we've created the following resources to best equip you with the knowledge and tools for a seamless transition: A Community thread to discuss conversion tools, migration, tips, and offer troubleshooting help A Git tutorial that covers anywhere from the basics of creating pull requests to rebasing and Git hooks We want to thank all the loyal users who have grown with us over the years. We look forward to this new focus on our roadmap and to introducing exciting new features. ",
          "It's funny to see how the whole world concentrates on this Git thing, while there is a treasure trove called Mercurial.<p>Mercurial was made for humans. It is seriously convenient and productive. Something I cannot say about Git, which more reminds me of an adhoc job.<p>I use both Git and Mercurial on daily basis. But my preference goes to Mercurial: it is just more sane in a big way. It is clearly a piece of art and love.",
          "It's very sad to see bitbucket dropping mercurial support. Now only Facebook and volunteers are keeping mercurial alive. \nSometimes technically better architecture and user interface lose to a non user friendly hard solutions due to inertia of mass adoption.<p>So a lesson in Software development is similar to betamax and VHS, so marketing is still a winner over technically superior architecture and ease of use. GitHub successfully marketed git, so git and GitHub are synonymous for most developers. Now majority of open source projects are reliant on a single proprietary solution Github by Microsoft, for managing code and project. Can understand the difficulty of bitbucket, when Python language itself moved out of mercurial due to the same inertia.<p>Hopefully gitlab can come out with mercurial support to migrate projects using it from bitbucket.<p>For people who believe in self hosted solution can install Kallithea (<a href=\"https://kallithea-scm.org\" rel=\"nofollow\">https://kallithea-scm.org</a>) or Rhodecode open source edition. Kallithea is used by Unity engine to manage their source code internally with mercurial."
        ],
        "story_type": ["Normal"],
        "url": "https://bitbucket.org/blog/sunsetting-mercurial-support-in-bitbucket",
        "url_text": "[Update Aug 26, 2020] All hg repos have now been disabled and cannot be accessed.[Update July 1, 2020] Today, mercurial repositories, snippets, and wikis will turn to read-only mode. After July 8th, 2020 they will no longer be accessible. The version control software market has evolved a lot since Bitbucket began in 2008. When we launched, centralized version control was the norm and we only supported Mercurial repos. But Git adoption has grown over the years to become the default system, helping teams of all sizes work faster as they become more distributed.As we surpass 10 million registered users on the platform, we're at a point in our growth where we are conducting a deeper evaluation of the market and how we can best support our users going forward. After much consideration, we've decided to remove Mercurial support from Bitbucket Cloud and its API. Mercurial features and repositories will be officially deprecated on July 1, 2020.Read on to learn more about this decision, the important timelines, and get migration resources and support. The timeline and how this may affect your team Here are the key dates as we sunset Mercurial functionality: February 1, 2020: users will no longer be able to create new Mercurial repositories [Extended] July 1, 2020: users will not be able to use Mercurial features. All hg repos, wikis, and snippets will be in read-only mode. Heres why were focusing on Git This wasnt an easy decision, and Mercurial will always have a special place in Bitbuckets history. DevOps adoption has skyrocketed over the last decade and our customers are adopting this new way of working at an exponential rate. In this time, Bitbucket has steadily grown from being just a version control management tool to being a place to manage the entire software development lifecycle. And there's always more work to be done. This year we will concentrate on building deeper integrations to enhance automation and collaboration. Our improvements will make it even easier and safer to plan, code, test, and deploy all from within Bitbucket. Building quality features requires intense focus, and supporting two version control systems means splitting focus doubling shipping time and technical overhead. With Git being the more popularly used tool, Mercurial runs the risk of overlooked issues as we scale. According to a Stack Overflow Developer Survey, almost 90% of developers use Git, while Mercurial is the least popular version control system with only about 3% developer adoption. In fact, Mercurial usage on Bitbucket is steadily declining, and the percentage of new Bitbucket users choosing Mercurial has fallen to less than 1%. This deprecation will enable us to focus on building the best possible experience for our users. How to migrate and export We recommend that teams migrate their existing Mercurial repos to Git. There are various Git conversion tools in the market, including hg-fast-export and hg-git mercurial plugin. We are happy to support your migration, and you can find a discussion about available options in our dedicated Community thread. If you prefer to continue using the Mercurial system, there are a number of free and paid Mercurial hosting services. We realize that there is no one-size-fits-all solution. That's why we've created the following resources to best equip you with the knowledge and tools for a seamless transition: A Community thread to discuss conversion tools, migration, tips, and offer troubleshooting help A Git tutorial that covers anywhere from the basics of creating pull requests to rebasing and Git hooks We want to thank all the loyal users who have grown with us over the years. We look forward to this new focus on our roadmap and to introducing exciting new features. ",
        "comments.comment_id": [20745989, 20746077],
        "comments.comment_author": ["garganzol", "dragonsh"],
        "comments.comment_descendants": [4, 19],
        "comments.comment_time": [
          "2019-08-20T12:13:11Z",
          "2019-08-20T12:22:53Z"
        ],
        "comments.comment_text": [
          "It's funny to see how the whole world concentrates on this Git thing, while there is a treasure trove called Mercurial.<p>Mercurial was made for humans. It is seriously convenient and productive. Something I cannot say about Git, which more reminds me of an adhoc job.<p>I use both Git and Mercurial on daily basis. But my preference goes to Mercurial: it is just more sane in a big way. It is clearly a piece of art and love.",
          "It's very sad to see bitbucket dropping mercurial support. Now only Facebook and volunteers are keeping mercurial alive. \nSometimes technically better architecture and user interface lose to a non user friendly hard solutions due to inertia of mass adoption.<p>So a lesson in Software development is similar to betamax and VHS, so marketing is still a winner over technically superior architecture and ease of use. GitHub successfully marketed git, so git and GitHub are synonymous for most developers. Now majority of open source projects are reliant on a single proprietary solution Github by Microsoft, for managing code and project. Can understand the difficulty of bitbucket, when Python language itself moved out of mercurial due to the same inertia.<p>Hopefully gitlab can come out with mercurial support to migrate projects using it from bitbucket.<p>For people who believe in self hosted solution can install Kallithea (<a href=\"https://kallithea-scm.org\" rel=\"nofollow\">https://kallithea-scm.org</a>) or Rhodecode open source edition. Kallithea is used by Unity engine to manage their source code internally with mercurial."
        ],
        "id": "3ead87e1-a79d-4f46-9dd5-9db60461d19a",
        "_version_": 1718527420733063168
      },
      {
        "story_id": [21071181],
        "story_author": ["JA7Cal"],
        "story_descendants": [15],
        "story_score": [43],
        "story_time": ["2019-09-25T14:36:38Z"],
        "story_title": "Automated Reports with Jupyter Notebooks (Using Jupytext and Papermill)",
        "search": [
          "Automated Reports with Jupyter Notebooks (Using Jupytext and Papermill)",
          "https://medium.com/capital-fund-management/automated-reports-with-jupyter-notebooks-using-jupytext-and-papermill-619e60c37330",
          "Jupyter notebooks are one of the best available tools for running code interactively and writing a narrative with data and plots. What is less known is that they can be conveniently versioned and run automatically.Do you have a Jupyter notebook with plots and figures that you regularly run manually? Wouldnt it be nice to use the same notebook and instead have an automated reporting system, launched from a script? What if this script could even pass some parameters to the notebook it runs?This post explains in a few steps how this can be done concretely, including within a production environment.Example notebookWe will show you how to version control, automatically run and publish a notebook that depends on a parameter. As an example, we will use a notebook that describes the world population and the gross domestic product for a given year. It is simple to use: just change the year variable in the first cell, re-run, and you get the plots for the chosen year. But this requires a manual intervention. It would be much more convenient if the update could be automated and produce a report for each possible value of the year parameter (more generally, a notebook can update its results based not only on some user-provided parameters, but also through a connection to a database, etc.).Version controlIn a professional environment, notebooks are designed by, say, a data scientist, but the task of running them in production may be handled by a different team. So in general people have to share notebooks. This is best done through a version control system.Jupyter notebooks are famous for the difficulty of their version control. Lets consider our notebook above, with a file size of 3 MB, much of it being contributed by the embedded Plotly library. The notebook is less than 80 KB if we remove the output of the second code cell. And as small as 1.75 KB when all outputs are removed. This shows how much of its contents is unrelated to pure code! If we dont pay attention, code changes in the notebook will be lost in an ocean of binary contents.To get meaningful diffs, we use Jupytext (disclaimer: Im the author of Jupytext). Jupytext can be installed with pip or conda. Once the notebook server is restarted, a Jupytext menu appears in Jupyter:We click on Pair Notebook with Markdown, save the notebook and we obtain two representations of the notebook: world_fact.ipynb (with both input and output cells) and world_fact.md (with only the input cells).Jupytexts representation of notebooks as Markdown files is compatible with all major Markdown editors and viewers, including GitHub and VS Code. The Markdown version is for example rendered by GitHub as:As you can see, the Markdown file does not include any output. Indeed, we dont want it at this stage since we only need to share the notebook code. The Markdown file also has a very clear diff history, which makes versioning notebooks simple.The world_facts.md file is automatically updated by Jupyter when you save the notebook. And the other way round also works! If you modify world_facts.md with either a text editor, or by pulling the latest contributions from the version control system, then the changes appear in Jupyter when you refresh the notebook in the browser.In our version control system, we only need to track the Markdown file (and we even explicitly ignore all .ipynb files). Obviously, the team that will execute the notebook needs to regenerate the world_fact.ipynb document. For this they use Jupytext in the command line:$ jupytext world_facts.md --to ipynb[jupytext] Reading world_facts.md[jupytext] Writing world_facts.ipynbWe are now properly versioning the notebook. The diff history is much clearer. See for instance how the addition of the gross domestic products to our report looks like:Jupyter notebooks as Scripts?As an alternative to the Markdown representation, we could have paired the notebook to a world_facts.py script using Jupytext. You should give it a try if your notebook contains more code than text. That's often a first good step towards a complete and efficient refactoring of long notebooks: once the notebook is represented as a script, you can extract any complex code and move it to a (unit-tested) library using the refactoring tools in your IDE.JupyterLab, JupyterHub, Binder, Nteract, Colab & Cloud notebooks?Do you use JupyterLab and not Jupyter Notebook? No worries: the method above also applies in this case. You will just have to use the Jupytext extension for JupyterLab instead of the Jupytext menu. And in case you were wondering, Jupytext also work in JupyterHub and Binder.If you use other notebook editors like Nteract desktop, CoCalc, Google Colab, or another cloud notebook editor, you may not be able to use Jupytext as a plugin in the editor. In this case you can simply use Jupytext in the command line. Close your notebook and inject the pairing information into world_facts.ipynb with$ jupytext --set-formats ipynb,md world_facts.ipynband then keep the two representations synchronised with$ jupytext --sync world_facts.ipynbNotebook parametersPapermill is the reference library for executing notebooks with parameters.Papermill needs to know which cell contains the notebook parameters. This is simply done by adding a parameter tag in that cell with the cell toolbar in Jupyter Notebook:In JupyterLab you can use the celltags extension.And if you prefer you can also directly edit world_facts.md and add the tag there:```python tags=[\"parameters\"]year = 2000```Automated executionWe now have all the information required to execute the notebook on a production server.Production environmentIn order to execute the notebook, we need to know in which environment it should run. As we are working with a Python notebook in this example, we list its dependencies in a requirements.txt file, as is standard for Python projects.For simplicity, we also include the notebook tools in the same environment, i.e. add jupytext and papermill to the same requirements.txt file. Strictly speaking, these tools could be installed and executed in another Python environment.The corresponding Python environment is created with either$ conda create -n run_notebook --file requirements.txt -yor$ pip install -r requirements.txt(if in a virtual environment).Please note that the requirements.txt file is just one way of specifying an execution environment. The Reproducible Execution Environment Specification by the Binder team is one of the most complete references on the subject.Continuous IntegrationIt is a good practice to test each new contribution to either the notebook or its requirements. For this you can use for example Travis CI, a continuous integration solution. You will need only these two commands:pip install -r requirements.txt to install the dependenciesjupytext world_facts.md --set-kernel - --execute to test the execution of the notebook in the current Python environment.You can find a concrete example in our .travis.yml file.We are already executing the notebook automatically, arent we? Travis will tell us if a regression is introduced in the project What progress! But were not 100% done yet, as we promised to execute the notebook with parameters.Using the right kernelJupyter notebooks are associated with a kernel (i.e. a pointer to a local Python environment), but that kernel might not be available on your production machine. In this case, we simply update the notebook kernel so as to point to the environment that we have just created:$ jupytext world_facts.ipynb --set-kernel -Note that the minus sign in --set-kernel - above represents the current Python environment. In our example this yields:[jupytext] Reading world_facts.ipynb[jupytext] Updating notebook metadata with '{\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}}' [jupytext] Writing world_facts.ipynb (destination file replaced)In case you want to use another kernel just pass the kernel name to the --set-kernel option (you can get the list of all available kernels with jupyter kernelspec list and/or declare a new kernel with python -m ipykernel install --name kernel_name --user).Executing the notebook with parametersWe are now ready to use Papermill for executing the notebook.$ papermill world_facts.ipynb world_facts_2017.ipynb -p year 2017Input Notebook: world_facts.ipynb Output Notebook: world_facts_2017.ipynb 100%|| 8/8 [00:04<00:00, 1.41it/s]Were done! The notebook has been executed and the file world_facts_2017.ipynb contains the outputs.Publishing the NotebookIts time to deliver the notebook that was just executed. Maybe you want it in your mailbox? Or maybe you prefer to get a URL where you can see the result? We cover a few ways of doing that.GitHub can display Jupyter notebooks. This is a convenient solution, as you can easily choose who can access repositories. This works well as long as you dont include any interactive JavaScript plots or widgets in the notebook (the JavaScript parts are ignored by GitHub). In the case of our notebook, the interactive plots do not appear on GitHub, so we need another approach.Another option is to use the Jupyter Notebook Viewer. The nbviewer service can render any notebook which is publicly available on GitHub. Our notebook is thus rendered correctly there. If your notebook is not public, you can choose to install nbviewer locally.Alternatively, you can convert the executed notebook to HTML, and publish it on GitHub pages, or on your own HTML server, or send it over email. Converting the notebook to HTML is easily done with$ jupyter nbconvert world_facts_2017.ipynb --to html[NbConvertApp] Converting notebook world_facts_2017.ipynb to html [NbConvertApp] Writing 3361863 bytes to world_facts_2017.htmlThe resulting HTML file includes the code cells as below:But maybe you dont want to see the input cells in the HTML? You just need to add --no-input:$ jupyter nbconvert --to html --no-input world_facts_2017.ipynb --output world_facts_2017_report.htmlAnd youll get a cleaner report:Sending the standalone HTML file as an attachment in an email is an easy exercise. Embedding the report in the body of the email is also possible (but interactive plots wont work).Finally, if you are looking for a polished report and have some knowledge of LaTeX, you can give the PDF export option of Jupyters nbconvert command a try.Using pipesAn alternative to using named files would be to use pipes. jupytext, nbconvert and papermill all support them. A one-liner substitute for the previous commands is:$ cat world_facts.md \\ | jupytext --from md --to ipynb --set-kernel - \\ | papermill -p year 2017 \\ | jupyter nbconvert --stdin --output world_facts_2017_report.htmlConclusionYou should now be able to set up a full pipeline for generating reports in production, based on Jupyter notebooks. We have seen how to:version control a notebook with Jupytextshare a notebook and its dependencies between various userstest a notebook with continuous integrationexecute a notebook with parameters using Papermilland finally, how to publish the notebook (on GitHub or nbviewer), or render it as a static HTML page.The technology used in this example is fully based on the Jupyter Project, which is the de facto standard for Data Science. The tools used here are all open source and work well with any continuous integration framework.You have everything you need to schedule and deliver fine-tuned, code-free reports!EpilogueThe tools used here are written in Python. But they are language agnostic. Thanks to the Jupyter framework, they actually apply to any of the 40+ programming language for which a Jupyter kernel exists.Now, imagine that you have authored a document containing a few Bash command lines, just like this blog post. Install Jupytext and the bash kernel, and the blog post becomes this interactive Jupyter notebook!Going further, shouldnt we make sure that every instruction in our post actually works? We do that via our continuous integration spoiler alert: thats as simple as jupytext --execute README.md!AcknowledgmentsMarc would like to thank Eric Lebigot and Florent Zara for their contributions to this article, and to CFM for supporting this work through their Open-Source Program.About the authorThis article was written by Marc Wouts. Marc joined the research team of CFM in 2012 and has worked on a range of research projects, from optimal trading to portfolio construction.Marc has always been interested in finding efficient workflows for doing collaborative research involving data and code. In 2015 he authored an internal tool for publishing Jupyter and R Markdown notebooks on Atlassians Confluence wiki, providing a first solution for collaborating on notebooks. In 2018, he authored Jupytext, an open-source program that facilitates the version control of Jupyter notebooks. Marc is also interested in data visualisation, and coordinates a working group on this subject at CFM.Marc obtained a PhD in Probability Theory from the Paris Diderot University in 2007.DisclaimerAll views included in this document constitute judgments of its author(s) and do not necessarily reflect the views of Capital Fund Management or any of its affiliates. The information provided in this document is general information only, does not constitute investment or other advice, and is subject to change without notice. ",
          "Awesome article - I'm wondering, for \"Publishing the Notebook\" part of the workflow, have you ever seen Kyso (<a href=\"https://kyso.io\" rel=\"nofollow\">https://kyso.io</a>) - disclaimer, I'm a founder. We started Kyso to make it easier to communicate insights gained from analysis to non-technical people by converting data science tools (e.g. Jupyter Notebooks) into conversational tools in the form of blog posts. You can make public posts or have an internal \"data blog\" for your team, where you push your work to Github and it is reflected on Kyso. Would love to hear your thoughts on how it could fit into existing workflows.",
          "I don't think Jupyter notebooks should be used for automated jobs. They're great for exploratory stuff but once things are getting fleshed out and cleaned up, one should move to proper python files that can be unit tested and versioned without having to go to crazy lengths..."
        ],
        "story_type": ["Normal"],
        "url": "https://medium.com/capital-fund-management/automated-reports-with-jupyter-notebooks-using-jupytext-and-papermill-619e60c37330",
        "comments.comment_id": [21071306, 21072228],
        "comments.comment_author": ["KyleOS", "brummm"],
        "comments.comment_descendants": [0, 1],
        "comments.comment_time": [
          "2019-09-25T14:49:46Z",
          "2019-09-25T16:21:21Z"
        ],
        "comments.comment_text": [
          "Awesome article - I'm wondering, for \"Publishing the Notebook\" part of the workflow, have you ever seen Kyso (<a href=\"https://kyso.io\" rel=\"nofollow\">https://kyso.io</a>) - disclaimer, I'm a founder. We started Kyso to make it easier to communicate insights gained from analysis to non-technical people by converting data science tools (e.g. Jupyter Notebooks) into conversational tools in the form of blog posts. You can make public posts or have an internal \"data blog\" for your team, where you push your work to Github and it is reflected on Kyso. Would love to hear your thoughts on how it could fit into existing workflows.",
          "I don't think Jupyter notebooks should be used for automated jobs. They're great for exploratory stuff but once things are getting fleshed out and cleaned up, one should move to proper python files that can be unit tested and versioned without having to go to crazy lengths..."
        ],
        "id": "8208729c-9f7b-4230-9054-99c21d4b4c62",
        "url_text": "Jupyter notebooks are one of the best available tools for running code interactively and writing a narrative with data and plots. What is less known is that they can be conveniently versioned and run automatically.Do you have a Jupyter notebook with plots and figures that you regularly run manually? Wouldnt it be nice to use the same notebook and instead have an automated reporting system, launched from a script? What if this script could even pass some parameters to the notebook it runs?This post explains in a few steps how this can be done concretely, including within a production environment.Example notebookWe will show you how to version control, automatically run and publish a notebook that depends on a parameter. As an example, we will use a notebook that describes the world population and the gross domestic product for a given year. It is simple to use: just change the year variable in the first cell, re-run, and you get the plots for the chosen year. But this requires a manual intervention. It would be much more convenient if the update could be automated and produce a report for each possible value of the year parameter (more generally, a notebook can update its results based not only on some user-provided parameters, but also through a connection to a database, etc.).Version controlIn a professional environment, notebooks are designed by, say, a data scientist, but the task of running them in production may be handled by a different team. So in general people have to share notebooks. This is best done through a version control system.Jupyter notebooks are famous for the difficulty of their version control. Lets consider our notebook above, with a file size of 3 MB, much of it being contributed by the embedded Plotly library. The notebook is less than 80 KB if we remove the output of the second code cell. And as small as 1.75 KB when all outputs are removed. This shows how much of its contents is unrelated to pure code! If we dont pay attention, code changes in the notebook will be lost in an ocean of binary contents.To get meaningful diffs, we use Jupytext (disclaimer: Im the author of Jupytext). Jupytext can be installed with pip or conda. Once the notebook server is restarted, a Jupytext menu appears in Jupyter:We click on Pair Notebook with Markdown, save the notebook and we obtain two representations of the notebook: world_fact.ipynb (with both input and output cells) and world_fact.md (with only the input cells).Jupytexts representation of notebooks as Markdown files is compatible with all major Markdown editors and viewers, including GitHub and VS Code. The Markdown version is for example rendered by GitHub as:As you can see, the Markdown file does not include any output. Indeed, we dont want it at this stage since we only need to share the notebook code. The Markdown file also has a very clear diff history, which makes versioning notebooks simple.The world_facts.md file is automatically updated by Jupyter when you save the notebook. And the other way round also works! If you modify world_facts.md with either a text editor, or by pulling the latest contributions from the version control system, then the changes appear in Jupyter when you refresh the notebook in the browser.In our version control system, we only need to track the Markdown file (and we even explicitly ignore all .ipynb files). Obviously, the team that will execute the notebook needs to regenerate the world_fact.ipynb document. For this they use Jupytext in the command line:$ jupytext world_facts.md --to ipynb[jupytext] Reading world_facts.md[jupytext] Writing world_facts.ipynbWe are now properly versioning the notebook. The diff history is much clearer. See for instance how the addition of the gross domestic products to our report looks like:Jupyter notebooks as Scripts?As an alternative to the Markdown representation, we could have paired the notebook to a world_facts.py script using Jupytext. You should give it a try if your notebook contains more code than text. That's often a first good step towards a complete and efficient refactoring of long notebooks: once the notebook is represented as a script, you can extract any complex code and move it to a (unit-tested) library using the refactoring tools in your IDE.JupyterLab, JupyterHub, Binder, Nteract, Colab & Cloud notebooks?Do you use JupyterLab and not Jupyter Notebook? No worries: the method above also applies in this case. You will just have to use the Jupytext extension for JupyterLab instead of the Jupytext menu. And in case you were wondering, Jupytext also work in JupyterHub and Binder.If you use other notebook editors like Nteract desktop, CoCalc, Google Colab, or another cloud notebook editor, you may not be able to use Jupytext as a plugin in the editor. In this case you can simply use Jupytext in the command line. Close your notebook and inject the pairing information into world_facts.ipynb with$ jupytext --set-formats ipynb,md world_facts.ipynband then keep the two representations synchronised with$ jupytext --sync world_facts.ipynbNotebook parametersPapermill is the reference library for executing notebooks with parameters.Papermill needs to know which cell contains the notebook parameters. This is simply done by adding a parameter tag in that cell with the cell toolbar in Jupyter Notebook:In JupyterLab you can use the celltags extension.And if you prefer you can also directly edit world_facts.md and add the tag there:```python tags=[\"parameters\"]year = 2000```Automated executionWe now have all the information required to execute the notebook on a production server.Production environmentIn order to execute the notebook, we need to know in which environment it should run. As we are working with a Python notebook in this example, we list its dependencies in a requirements.txt file, as is standard for Python projects.For simplicity, we also include the notebook tools in the same environment, i.e. add jupytext and papermill to the same requirements.txt file. Strictly speaking, these tools could be installed and executed in another Python environment.The corresponding Python environment is created with either$ conda create -n run_notebook --file requirements.txt -yor$ pip install -r requirements.txt(if in a virtual environment).Please note that the requirements.txt file is just one way of specifying an execution environment. The Reproducible Execution Environment Specification by the Binder team is one of the most complete references on the subject.Continuous IntegrationIt is a good practice to test each new contribution to either the notebook or its requirements. For this you can use for example Travis CI, a continuous integration solution. You will need only these two commands:pip install -r requirements.txt to install the dependenciesjupytext world_facts.md --set-kernel - --execute to test the execution of the notebook in the current Python environment.You can find a concrete example in our .travis.yml file.We are already executing the notebook automatically, arent we? Travis will tell us if a regression is introduced in the project What progress! But were not 100% done yet, as we promised to execute the notebook with parameters.Using the right kernelJupyter notebooks are associated with a kernel (i.e. a pointer to a local Python environment), but that kernel might not be available on your production machine. In this case, we simply update the notebook kernel so as to point to the environment that we have just created:$ jupytext world_facts.ipynb --set-kernel -Note that the minus sign in --set-kernel - above represents the current Python environment. In our example this yields:[jupytext] Reading world_facts.ipynb[jupytext] Updating notebook metadata with '{\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}}' [jupytext] Writing world_facts.ipynb (destination file replaced)In case you want to use another kernel just pass the kernel name to the --set-kernel option (you can get the list of all available kernels with jupyter kernelspec list and/or declare a new kernel with python -m ipykernel install --name kernel_name --user).Executing the notebook with parametersWe are now ready to use Papermill for executing the notebook.$ papermill world_facts.ipynb world_facts_2017.ipynb -p year 2017Input Notebook: world_facts.ipynb Output Notebook: world_facts_2017.ipynb 100%|| 8/8 [00:04<00:00, 1.41it/s]Were done! The notebook has been executed and the file world_facts_2017.ipynb contains the outputs.Publishing the NotebookIts time to deliver the notebook that was just executed. Maybe you want it in your mailbox? Or maybe you prefer to get a URL where you can see the result? We cover a few ways of doing that.GitHub can display Jupyter notebooks. This is a convenient solution, as you can easily choose who can access repositories. This works well as long as you dont include any interactive JavaScript plots or widgets in the notebook (the JavaScript parts are ignored by GitHub). In the case of our notebook, the interactive plots do not appear on GitHub, so we need another approach.Another option is to use the Jupyter Notebook Viewer. The nbviewer service can render any notebook which is publicly available on GitHub. Our notebook is thus rendered correctly there. If your notebook is not public, you can choose to install nbviewer locally.Alternatively, you can convert the executed notebook to HTML, and publish it on GitHub pages, or on your own HTML server, or send it over email. Converting the notebook to HTML is easily done with$ jupyter nbconvert world_facts_2017.ipynb --to html[NbConvertApp] Converting notebook world_facts_2017.ipynb to html [NbConvertApp] Writing 3361863 bytes to world_facts_2017.htmlThe resulting HTML file includes the code cells as below:But maybe you dont want to see the input cells in the HTML? You just need to add --no-input:$ jupyter nbconvert --to html --no-input world_facts_2017.ipynb --output world_facts_2017_report.htmlAnd youll get a cleaner report:Sending the standalone HTML file as an attachment in an email is an easy exercise. Embedding the report in the body of the email is also possible (but interactive plots wont work).Finally, if you are looking for a polished report and have some knowledge of LaTeX, you can give the PDF export option of Jupyters nbconvert command a try.Using pipesAn alternative to using named files would be to use pipes. jupytext, nbconvert and papermill all support them. A one-liner substitute for the previous commands is:$ cat world_facts.md \\ | jupytext --from md --to ipynb --set-kernel - \\ | papermill -p year 2017 \\ | jupyter nbconvert --stdin --output world_facts_2017_report.htmlConclusionYou should now be able to set up a full pipeline for generating reports in production, based on Jupyter notebooks. We have seen how to:version control a notebook with Jupytextshare a notebook and its dependencies between various userstest a notebook with continuous integrationexecute a notebook with parameters using Papermilland finally, how to publish the notebook (on GitHub or nbviewer), or render it as a static HTML page.The technology used in this example is fully based on the Jupyter Project, which is the de facto standard for Data Science. The tools used here are all open source and work well with any continuous integration framework.You have everything you need to schedule and deliver fine-tuned, code-free reports!EpilogueThe tools used here are written in Python. But they are language agnostic. Thanks to the Jupyter framework, they actually apply to any of the 40+ programming language for which a Jupyter kernel exists.Now, imagine that you have authored a document containing a few Bash command lines, just like this blog post. Install Jupytext and the bash kernel, and the blog post becomes this interactive Jupyter notebook!Going further, shouldnt we make sure that every instruction in our post actually works? We do that via our continuous integration spoiler alert: thats as simple as jupytext --execute README.md!AcknowledgmentsMarc would like to thank Eric Lebigot and Florent Zara for their contributions to this article, and to CFM for supporting this work through their Open-Source Program.About the authorThis article was written by Marc Wouts. Marc joined the research team of CFM in 2012 and has worked on a range of research projects, from optimal trading to portfolio construction.Marc has always been interested in finding efficient workflows for doing collaborative research involving data and code. In 2015 he authored an internal tool for publishing Jupyter and R Markdown notebooks on Atlassians Confluence wiki, providing a first solution for collaborating on notebooks. In 2018, he authored Jupytext, an open-source program that facilitates the version control of Jupyter notebooks. Marc is also interested in data visualisation, and coordinates a working group on this subject at CFM.Marc obtained a PhD in Probability Theory from the Paris Diderot University in 2007.DisclaimerAll views included in this document constitute judgments of its author(s) and do not necessarily reflect the views of Capital Fund Management or any of its affiliates. The information provided in this document is general information only, does not constitute investment or other advice, and is subject to change without notice. ",
        "_version_": 1718527428058415104
      },
      {
        "story_id": [19669153],
        "story_author": ["themlaiguy"],
        "story_descendants": [5],
        "story_score": [7],
        "story_time": ["2019-04-15T21:37:08Z"],
        "story_title": "Why is version control in Jupyter notebooks so hard?",
        "search": [
          "Why is version control in Jupyter notebooks so hard?",
          "Are there any tools that help with version control on notebooks?",
          "I've been clearing my output using nbconvert before putting the notebook into version control. I have a precommit hook and a check in CI. This works for my use case but I can understand needing to preserve output.<p>jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace my_notebook_name.ipynb",
          "You bet. I built ReviewNB[1] specifically for Jupyter Notebook code reviews.<p>There's also,<p>- nbstripout[2] for stripping outputs automatically before every commit<p>- nbdime[3] for diff'ing notebooks locally<p>- jupytext[4] for converting notebooks to markdown and vice-a-versa<p>[1] <a href=\"https://www.reviewnb.com/\" rel=\"nofollow\">https://www.reviewnb.com/</a><p>[2] <a href=\"https://github.com/kynan/nbstripout\" rel=\"nofollow\">https://github.com/kynan/nbstripout</a><p>[3] <a href=\"https://github.com/jupyter/nbdime\" rel=\"nofollow\">https://github.com/jupyter/nbdime</a><p>[4] <a href=\"https://github.com/mwouts/jupytext\" rel=\"nofollow\">https://github.com/mwouts/jupytext</a>"
        ],
        "story_text": "Are there any tools that help with version control on notebooks?",
        "story_type": ["AskHN"],
        "comments.comment_id": [19670482, 19674241],
        "comments.comment_author": ["snilzzor", "amirathi"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-04-16T01:47:18Z",
          "2019-04-16T15:04:38Z"
        ],
        "comments.comment_text": [
          "I've been clearing my output using nbconvert before putting the notebook into version control. I have a precommit hook and a check in CI. This works for my use case but I can understand needing to preserve output.<p>jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace my_notebook_name.ipynb",
          "You bet. I built ReviewNB[1] specifically for Jupyter Notebook code reviews.<p>There's also,<p>- nbstripout[2] for stripping outputs automatically before every commit<p>- nbdime[3] for diff'ing notebooks locally<p>- jupytext[4] for converting notebooks to markdown and vice-a-versa<p>[1] <a href=\"https://www.reviewnb.com/\" rel=\"nofollow\">https://www.reviewnb.com/</a><p>[2] <a href=\"https://github.com/kynan/nbstripout\" rel=\"nofollow\">https://github.com/kynan/nbstripout</a><p>[3] <a href=\"https://github.com/jupyter/nbdime\" rel=\"nofollow\">https://github.com/jupyter/nbdime</a><p>[4] <a href=\"https://github.com/mwouts/jupytext\" rel=\"nofollow\">https://github.com/mwouts/jupytext</a>"
        ],
        "id": "5a0080db-2e34-40ec-b716-3c8edf40990c",
        "_version_": 1718527399615791104
      },
      {
        "story_id": [20646674],
        "story_author": ["tosh"],
        "story_descendants": [68],
        "story_score": [107],
        "story_time": ["2019-08-08T17:40:55Z"],
        "story_title": "The market figured out Gitlabs secret",
        "search": [
          "The market figured out Gitlabs secret",
          "https://about.gitlab.com/2019/08/08/built-in-ci-cd-version-control-secret/",
          "Theres a movement in the DevOps industry and the world right now: to do more in a simple way that inspires us to innovate. GitLab started this trend in the DevOps space by simplifying the delivery of code by combining GitLab CI and GitLab version control. We didn't originally buy into the idea that this was the right way to do things, but it became our secret capability that weve doubled down on. Lets combine applications The story starts with Kamil Trzciski, now a distinguished engineer at GitLab. Soon after Kamil came to work for GitLab full time, he began talking with me and my co-founder, Dmitriy Zaporozhets, suggesting that we bring our two projects together GitLab Version Control and GitLab CI, making it into one application. Dmitriy didnt think it was a good idea. GitLab version control and CI were already perfectly integrated with single sign-on and APIs that fit like a glove. He thought that combining them would make GitLab a monolith of an application, that it would be disastrous for our code quality, and an unfortunate user experience. After time though, Dmitriy started to think it was the right idea as it would deliver a seamless experience for developers to deliver code quickly. After Dmitriy was convinced, they came to me. I also didnt think it was a good idea. At the time I believed we needed to have tools that are composable and that could integrate with other tools, in line with the Unix philosophy. Kamil convinced me to think about the efficiencies of having a single application. Well, if you dont believe that its better for a user, at least believe its more efficient for us, because we only have to release one application instead of two. Efficiency is in our values. - Kamil Trzcinski, distinguished engineer at GitLab Realizing the future of DevOps is a single application That made sense to me and I no longer stood in their way. The two projects merged and the results were beyond my expectations. The efficiencies that were so appealing to us, also made it appealing to our customers. We realized we stumbled on a big secret because nobody believed that the two combined together would be a better way of continuously delivering code to market. We doubled down on this philosophy and we started doing continuous delivery. From that day on, I saw the value of having a single application. For example, a new feature we are implementing is auto-remediation. When a vulnerability comes out, say a heart bleed, GitLab will automatically detect where in your codebase that vulnerability exists, update the dependency, and deliver it to your production environment. This level of automation would be hard to implement without being in a single application. By combining the projects we unified teams helping them realize the original intent of DevOps and that is magical to see. The market validates our secret And while we bet on this philosophy the industry is now seeing it as well. In September of 2015 we combined GitLab CI and GitLab version control to create a single application. By March of 2017, Bitbucket also realized the advantages of this architecture and released Pipelines as a built-in part of Bitbucket. In 2018, GitHub announced Actions with CI-like functionality built into a single application offering. In the last six months, JFrog acquired Shippable and Idera acquired Travis CI, showing a consolidation of the DevOps market and a focus on CI. The market is validating what we continually hear from our users and customers: that a simple, single DevOps application meets their needs better. We hope you will continue to join us in our effort to bring teams together to innovate. Everyone can contribute here at GitLab and as always, we value your feedback, thoughts, and contributions. Want to hear me talk through the origin story? Listen to the Software Engineering Daily podcast where I talk about combining GitLab CI and GitLab Version Control. The industry has caught onto @GitLabs secret. Learn more about why GitLab combined GitLab CI and GitLab version control Sid Sijbrandij Click to tweet Sign up for GitLabs twice-monthly newsletter ",
          "Every time github releases a feature this is the response. These posts are so pathetic. Im surprised that they continue to play this angle. Its a very polarizing way to address the community that will definitely continue to stir up us vs them mentality between GitHub and Gitlab users.",
          "For all the bashing GitLab gets, personally, I want GitLab to survive and keep competing at some level with GitHub.<p>Should GitHub dominate the market and gobble up competition, we all know how it goes for its parent company."
        ],
        "story_type": ["Normal"],
        "url": "https://about.gitlab.com/2019/08/08/built-in-ci-cd-version-control-secret/",
        "comments.comment_id": [20647790, 20648739],
        "comments.comment_author": ["whalesalad", "asadkn"],
        "comments.comment_descendants": [5, 1],
        "comments.comment_time": [
          "2019-08-08T19:32:16Z",
          "2019-08-08T21:04:04Z"
        ],
        "comments.comment_text": [
          "Every time github releases a feature this is the response. These posts are so pathetic. Im surprised that they continue to play this angle. Its a very polarizing way to address the community that will definitely continue to stir up us vs them mentality between GitHub and Gitlab users.",
          "For all the bashing GitLab gets, personally, I want GitLab to survive and keep competing at some level with GitHub.<p>Should GitHub dominate the market and gobble up competition, we all know how it goes for its parent company."
        ],
        "id": "6d40bf87-dfb7-48c1-917e-83af14eaca10",
        "url_text": "Theres a movement in the DevOps industry and the world right now: to do more in a simple way that inspires us to innovate. GitLab started this trend in the DevOps space by simplifying the delivery of code by combining GitLab CI and GitLab version control. We didn't originally buy into the idea that this was the right way to do things, but it became our secret capability that weve doubled down on. Lets combine applications The story starts with Kamil Trzciski, now a distinguished engineer at GitLab. Soon after Kamil came to work for GitLab full time, he began talking with me and my co-founder, Dmitriy Zaporozhets, suggesting that we bring our two projects together GitLab Version Control and GitLab CI, making it into one application. Dmitriy didnt think it was a good idea. GitLab version control and CI were already perfectly integrated with single sign-on and APIs that fit like a glove. He thought that combining them would make GitLab a monolith of an application, that it would be disastrous for our code quality, and an unfortunate user experience. After time though, Dmitriy started to think it was the right idea as it would deliver a seamless experience for developers to deliver code quickly. After Dmitriy was convinced, they came to me. I also didnt think it was a good idea. At the time I believed we needed to have tools that are composable and that could integrate with other tools, in line with the Unix philosophy. Kamil convinced me to think about the efficiencies of having a single application. Well, if you dont believe that its better for a user, at least believe its more efficient for us, because we only have to release one application instead of two. Efficiency is in our values. - Kamil Trzcinski, distinguished engineer at GitLab Realizing the future of DevOps is a single application That made sense to me and I no longer stood in their way. The two projects merged and the results were beyond my expectations. The efficiencies that were so appealing to us, also made it appealing to our customers. We realized we stumbled on a big secret because nobody believed that the two combined together would be a better way of continuously delivering code to market. We doubled down on this philosophy and we started doing continuous delivery. From that day on, I saw the value of having a single application. For example, a new feature we are implementing is auto-remediation. When a vulnerability comes out, say a heart bleed, GitLab will automatically detect where in your codebase that vulnerability exists, update the dependency, and deliver it to your production environment. This level of automation would be hard to implement without being in a single application. By combining the projects we unified teams helping them realize the original intent of DevOps and that is magical to see. The market validates our secret And while we bet on this philosophy the industry is now seeing it as well. In September of 2015 we combined GitLab CI and GitLab version control to create a single application. By March of 2017, Bitbucket also realized the advantages of this architecture and released Pipelines as a built-in part of Bitbucket. In 2018, GitHub announced Actions with CI-like functionality built into a single application offering. In the last six months, JFrog acquired Shippable and Idera acquired Travis CI, showing a consolidation of the DevOps market and a focus on CI. The market is validating what we continually hear from our users and customers: that a simple, single DevOps application meets their needs better. We hope you will continue to join us in our effort to bring teams together to innovate. Everyone can contribute here at GitLab and as always, we value your feedback, thoughts, and contributions. Want to hear me talk through the origin story? Listen to the Software Engineering Daily podcast where I talk about combining GitLab CI and GitLab Version Control. The industry has caught onto @GitLabs secret. Learn more about why GitLab combined GitLab CI and GitLab version control Sid Sijbrandij Click to tweet Sign up for GitLabs twice-monthly newsletter ",
        "_version_": 1718527418757545985
      },
      {
        "story_id": [19738327],
        "story_author": ["ariehkovler"],
        "story_descendants": [88],
        "story_score": [75],
        "story_time": ["2019-04-24T13:55:08Z"],
        "story_title": "We need a new generation of source control",
        "search": [
          "We need a new generation of source control",
          "https://www.rookout.com/cant-git-no-satisfaction-why-we-need-a-new-gen-source-control/",
          "By: | January 5, 2019Liran is the Co-Founder and CTO of Rookout. Hes an Observability and Instrumentation expert with a deep understanding of Java, Python, Node, and C++. Liran has broad experience in cybersecurity and compliance from his past roles. When not coding, you can find Liran hosting his podcast, speaking at conferences, writing about his tech adventures, and trying out the local cuisine when traveling.Remember the good old days of enterprise software? When everything had to be installed on-premises? To install an application, youd have to set up a big, vertically scalable server. You would then have to execute a single process written in C/C++, Java or .NET. Well, as you know, those days are long gone.Everything has changed with the transition to the cloud and SaaS. Today, instead of comprising a single vertically scalable process, most applications comprise multiple horizontally scalable processes. This model was first pioneered by Googles borg and by Netflix on EC2. Nowadays, though, you no longer have to be a large enterprise to access microservice infrastructures. Kubernetes and serverless have made microservices viable and accessible to even small startups and lone coders.Lets Git down to businessSo where does Git fit into the picture? Git is an excellent match for single-process applications, but it starts to fail when it comes to multi-process applications. This is precisely what gave birth to the endless mono-repo vs. multi-repo flame-wars.Each side of this debate classifies the other as zealous extremists (as only developers can!), but both of them miss the crux of the matter: Git and its accompanying ecosystem are not yet fit for the task of developing modern cloud-native applications.Shots fired: multi-repos suckBefore we dive in, lets answer this: whats great about Git? Its the almighty atomic commit, the groundbreaking (at the time) branching capabilities, and the ever-useful blame. Well, these beloved features all but disappear in a multi-repo setup. Working in multiple repositories comes with significant drawbacks, which is why its not at all surprising that some of the biggest names in the tech world, including Google and Facebook, have gone down the mono-repo path at a huge investment of time and resources.Dependency management in a multi-repo setup is a nightmare. Instead of having everything in a single repository, you end up with repositories pointing to each other using two git features (git submodules and git subtree) and language-specific dependency management such as npm or Maven. The very existence of the many different methods to manage multi-repos is in itself proof that none of these tools are enough on their own. Gits source-of-truth is no longer a single folder on your computer but a mishmash of source providers and various artifactories.In developers everyday work, repository separation becomes an artificial barrier that impacts technological decisions. This creates a Conways Law effect, making early design decisions about component boundaries very hard to change. It also makes large scale refactorings a much trickier business.However, the biggest failure of the multi-repo is cultural. Instead of having all your source code readily available to all developers, they have to jump hurdles to figure out which repo they need and then clone it. These seemingly-small obstacles often become high fences: developers stop reading and updating code in components and repositories that arent directly in their responsibility.With all these engineering, operations and cultural barriers, why doesnt everyone go the mono-repo route?Take no prisoners: mono-repos suck tooOnce youve packed everything into a single repository, figuring out the connections within the repository becomes a challenge. For humans, that can chip away at the original architecture, breaking away useful abstractions and jumbling everything together.For machines, this lack of separation within the repo is even worse. When you push a code change to a repo, automated processes kick in. CI systems build and test the code, and then CD systems deploy it. Sometimes its to a test or staging environment, and sometimes directly to production.There are certain components you will need to build and deploy hundreds of times a day. At the same time, there are other more delicate and mission-critical components. These require human supervision and extra precaution. The problem with mono-repository is that it mixes all of these components into one. More surprising is the fact that todays vast Git CI ecosystem, with its impressive offerings in both the hosted and the SaaS space, doesnt even try to tackle the issue. In fact, not only will Git CI tools rebuild and redeploy your entire repo, they are often built explicitly for multi-repo projects.Another issue is large repository sizes. Git doesnt handle large repos gracefully. You can easily end up with repo sizes that dont fit in your hard-drive, or clone time that ends up in the hours. For big projects, this requires careful management and pruning of commit history. It is also essential to avoid committing dependencies, auto-generated files and other large files which may be necessary for specific scenarios.Is there still hope for multi-repos?There are new tools that seek to bring some of the benefits of mono-repos to multi-repos. These tools try to set up a configuration that would unite multiple repos under a single umbrella/abstraction layer, thus making managing multiple-repositories easier for example, TwoSigmas Git-meta, mateodelnortes meta, gitslave ,and a bunch of others.These tools bring back a bit of sanity into the complexities of managing multi-repos, reducing some of the toil and error-prone manual operations. But none of them truly give back the control and power of a single Git repo.You cant have your cake and Git it tooThe downsides of multi-repos are real. You cant deny the value of a (truly) single source of truth, (truly) atomic commits, and a (truly) single place to develop and collaborate. On the other hand, none of the downsides of mono-repos are inherent. All of them are related to the current implementation of the Git source control tool itself and its accompanying eco-system, especially CI/CD tools.Its time for a new generation of source control that wasnt purely designed for open-source projects, C and the Linux kernel. A source control designed for delivering modern applications in a polyglot cloud-native world. One that embraces code dependencies and helps the engineering team define and manage them, rather than scaring them away. A source control that treats CI, CD, and releases as first-class citizens, rather than relying on the very useful add-ons provided by GitHub and its community. ",
          "Are we mistaking a dependency control problem as a revision control problem?<p>In a previous life, before microservices, CI/CD etc. existed, we did just fine with 20-30 CVS repositories, each representing a separate component (a running process) in a very large distributed system.<p>The only difference was that we did not have to marshal a large number of 3rd party dependencies that were constantly undergoing version changes. We basically relied on C++, the standard template library and a tightly version controlled set of internal libraries with a single stable version shared across the entire org. The whole system would have been between 750,000 - 1,000,000 lines of code (libraries included).<p>I'm not saying that that's the right approach. But it's mind boggling for me that we can't solve this problem easily anymore.",
          "The source control system is not the piece of the equation that matters to most people.  The build system is the important part.  That's what prevents you from rebuilding the repository when you only change one Kubernetes config file, or what causes 100 docker images to be built because you changed a file in libc.<p>I think the tooling around this is fairly limited right now.  I feel that most people are hoping docker caches stuff intelligently, which it doesn't.  People should probably be using Bazel, but language support is hit-or-miss and it's very complicated.  (It's aggravated by the fact that every language now considers itself responsible for building its own code.  go \"just works\", which is great, but it's hard to translate that local caching to something that can be spread among multiple build workers.  Bazel attempts to make all that work, but it basically has to start from scratch, which is unfortunate.  It also means that you can't just start using some crazy new language unless you want to now support it in the build system.  We all hate Makefiles, but the whole \"foo.c becomes foo.o\" model was much more straightforward than what languages do today.)"
        ],
        "story_type": ["Normal"],
        "url": "https://www.rookout.com/cant-git-no-satisfaction-why-we-need-a-new-gen-source-control/",
        "comments.comment_id": [19739585, 19739587],
        "comments.comment_author": ["hliyan", "jrockway"],
        "comments.comment_descendants": [5, 0],
        "comments.comment_time": [
          "2019-04-24T15:48:25Z",
          "2019-04-24T15:48:36Z"
        ],
        "comments.comment_text": [
          "Are we mistaking a dependency control problem as a revision control problem?<p>In a previous life, before microservices, CI/CD etc. existed, we did just fine with 20-30 CVS repositories, each representing a separate component (a running process) in a very large distributed system.<p>The only difference was that we did not have to marshal a large number of 3rd party dependencies that were constantly undergoing version changes. We basically relied on C++, the standard template library and a tightly version controlled set of internal libraries with a single stable version shared across the entire org. The whole system would have been between 750,000 - 1,000,000 lines of code (libraries included).<p>I'm not saying that that's the right approach. But it's mind boggling for me that we can't solve this problem easily anymore.",
          "The source control system is not the piece of the equation that matters to most people.  The build system is the important part.  That's what prevents you from rebuilding the repository when you only change one Kubernetes config file, or what causes 100 docker images to be built because you changed a file in libc.<p>I think the tooling around this is fairly limited right now.  I feel that most people are hoping docker caches stuff intelligently, which it doesn't.  People should probably be using Bazel, but language support is hit-or-miss and it's very complicated.  (It's aggravated by the fact that every language now considers itself responsible for building its own code.  go \"just works\", which is great, but it's hard to translate that local caching to something that can be spread among multiple build workers.  Bazel attempts to make all that work, but it basically has to start from scratch, which is unfortunate.  It also means that you can't just start using some crazy new language unless you want to now support it in the build system.  We all hate Makefiles, but the whole \"foo.c becomes foo.o\" model was much more straightforward than what languages do today.)"
        ],
        "id": "14363763-823a-4285-b58a-8230ae289191",
        "url_text": "By: | January 5, 2019Liran is the Co-Founder and CTO of Rookout. Hes an Observability and Instrumentation expert with a deep understanding of Java, Python, Node, and C++. Liran has broad experience in cybersecurity and compliance from his past roles. When not coding, you can find Liran hosting his podcast, speaking at conferences, writing about his tech adventures, and trying out the local cuisine when traveling.Remember the good old days of enterprise software? When everything had to be installed on-premises? To install an application, youd have to set up a big, vertically scalable server. You would then have to execute a single process written in C/C++, Java or .NET. Well, as you know, those days are long gone.Everything has changed with the transition to the cloud and SaaS. Today, instead of comprising a single vertically scalable process, most applications comprise multiple horizontally scalable processes. This model was first pioneered by Googles borg and by Netflix on EC2. Nowadays, though, you no longer have to be a large enterprise to access microservice infrastructures. Kubernetes and serverless have made microservices viable and accessible to even small startups and lone coders.Lets Git down to businessSo where does Git fit into the picture? Git is an excellent match for single-process applications, but it starts to fail when it comes to multi-process applications. This is precisely what gave birth to the endless mono-repo vs. multi-repo flame-wars.Each side of this debate classifies the other as zealous extremists (as only developers can!), but both of them miss the crux of the matter: Git and its accompanying ecosystem are not yet fit for the task of developing modern cloud-native applications.Shots fired: multi-repos suckBefore we dive in, lets answer this: whats great about Git? Its the almighty atomic commit, the groundbreaking (at the time) branching capabilities, and the ever-useful blame. Well, these beloved features all but disappear in a multi-repo setup. Working in multiple repositories comes with significant drawbacks, which is why its not at all surprising that some of the biggest names in the tech world, including Google and Facebook, have gone down the mono-repo path at a huge investment of time and resources.Dependency management in a multi-repo setup is a nightmare. Instead of having everything in a single repository, you end up with repositories pointing to each other using two git features (git submodules and git subtree) and language-specific dependency management such as npm or Maven. The very existence of the many different methods to manage multi-repos is in itself proof that none of these tools are enough on their own. Gits source-of-truth is no longer a single folder on your computer but a mishmash of source providers and various artifactories.In developers everyday work, repository separation becomes an artificial barrier that impacts technological decisions. This creates a Conways Law effect, making early design decisions about component boundaries very hard to change. It also makes large scale refactorings a much trickier business.However, the biggest failure of the multi-repo is cultural. Instead of having all your source code readily available to all developers, they have to jump hurdles to figure out which repo they need and then clone it. These seemingly-small obstacles often become high fences: developers stop reading and updating code in components and repositories that arent directly in their responsibility.With all these engineering, operations and cultural barriers, why doesnt everyone go the mono-repo route?Take no prisoners: mono-repos suck tooOnce youve packed everything into a single repository, figuring out the connections within the repository becomes a challenge. For humans, that can chip away at the original architecture, breaking away useful abstractions and jumbling everything together.For machines, this lack of separation within the repo is even worse. When you push a code change to a repo, automated processes kick in. CI systems build and test the code, and then CD systems deploy it. Sometimes its to a test or staging environment, and sometimes directly to production.There are certain components you will need to build and deploy hundreds of times a day. At the same time, there are other more delicate and mission-critical components. These require human supervision and extra precaution. The problem with mono-repository is that it mixes all of these components into one. More surprising is the fact that todays vast Git CI ecosystem, with its impressive offerings in both the hosted and the SaaS space, doesnt even try to tackle the issue. In fact, not only will Git CI tools rebuild and redeploy your entire repo, they are often built explicitly for multi-repo projects.Another issue is large repository sizes. Git doesnt handle large repos gracefully. You can easily end up with repo sizes that dont fit in your hard-drive, or clone time that ends up in the hours. For big projects, this requires careful management and pruning of commit history. It is also essential to avoid committing dependencies, auto-generated files and other large files which may be necessary for specific scenarios.Is there still hope for multi-repos?There are new tools that seek to bring some of the benefits of mono-repos to multi-repos. These tools try to set up a configuration that would unite multiple repos under a single umbrella/abstraction layer, thus making managing multiple-repositories easier for example, TwoSigmas Git-meta, mateodelnortes meta, gitslave ,and a bunch of others.These tools bring back a bit of sanity into the complexities of managing multi-repos, reducing some of the toil and error-prone manual operations. But none of them truly give back the control and power of a single Git repo.You cant have your cake and Git it tooThe downsides of multi-repos are real. You cant deny the value of a (truly) single source of truth, (truly) atomic commits, and a (truly) single place to develop and collaborate. On the other hand, none of the downsides of mono-repos are inherent. All of them are related to the current implementation of the Git source control tool itself and its accompanying eco-system, especially CI/CD tools.Its time for a new generation of source control that wasnt purely designed for open-source projects, C and the Linux kernel. A source control designed for delivering modern applications in a polyglot cloud-native world. One that embraces code dependencies and helps the engineering team define and manage them, rather than scaring them away. A source control that treats CI, CD, and releases as first-class citizens, rather than relying on the very useful add-ons provided by GitHub and its community. ",
        "_version_": 1718527401276735488
      },
      {
        "story_id": [21836168],
        "story_author": ["bbrennan"],
        "story_descendants": [2],
        "story_score": [10],
        "story_time": ["2019-12-19T16:13:05Z"],
        "story_title": "Kubernetes Security Tools Abound",
        "search": [
          "Kubernetes Security Tools Abound",
          "https://searchitoperations.techtarget.com/news/252475750/New-Kubernetes-security-tools-abound-as-container-deployments-grow",
          "As Kubernetes use expands in production, enterprises have a number of IT security tools to choose from for centralized, policy-based control over container clusters. Emerging Kubernetes security tools focus security operations at higher layers of the IT stack, as the container orchestration platform grows into a production staple for mainstream enterprises. Approaches to Kubernetes security vary among these tools -- one newcomer, Octarine, piggybacks on the Envoy proxy and Istio service mesh to monitor container infrastructure for threats and enforce security policy, while an existing container security startup, NeuVector, plugs policy-as-code tools into the CI/CD pipeline. Another tool from managed Kubernetes player Fairwinds combines security and IT performance and reliability monitoring with a multi-cluster dashboard view. What they all have in common is their emergence as enterprises look for centralized points of Kubernetes security control over an entire environment, in addition to tools that operate at the individual container or application workload level. \"Looking at the container workload is great, but if you don't have a sense of everything that's running within your Kubernetes ecosystem and how it's communicating, it's very easy for rogue deployments to slip in when you have hundreds of namespaces and thousands of pods,\" said Trevor Bossert, manager of DevOps at Primer AI, a data analytics firm in San Francisco which began using Octarine's Kubernetes security software six months ago. \"This will let you know when [cluster configurations] are violating policies, like if they're public by default when they're not supposed to be.\" Octarine rides along in service mesh sidecar Octarine, based in Sunnyvale, Calif., came out of stealth last month with a service mesh-based approach to Kubernetes security. The company claims that installing its software on the Envoy service mesh proxy gives users a clearer picture of the Kubernetes orchestration layer than tools that monitor the infrastructure from privileged containers on hosts. Placing security monitoring and enforcement inside Envoy lets Octarine see whether container workloads are exposed to the internet, how secrets are exposed to container workloads and monitor east-west traffic more effectively, according to Octarine CTO and co-founder Haim Helman. Envoy is often associated with the Istio service mesh, which has its own security features, but Octarine doesn't replace those features, which include the enforcement of role-based access control and mutual TLS encryption. Instead, Octarine collects security telemetry and identifies anomalies and threats with its Octarine Runtime module, and manages Kubernetes security policy-as-code with a tool it calls Guardrails. It can feed security monitoring information into Istio's control plane if a user already has it, or run its own service mesh control plane if the user doesn't have Istio in place. There are other ways to create and enforce Kubernetes policy-as-code, among them the open source Open Policy Agent (OPA) that rose in popularity among large organizations in 2019, but midsize companies with smaller teams may find Octarine's policy-as-code features easier to use. Octarine's service-mesh-based Kubernetes security tool centralizes policy management at the network level \"Not having to craft all policies from scratch, being able to [let] Octarine observe the traffic and providing the best policy, is less time-consuming and involves less duplication of work, especially for a smaller team like ours,\" said Primer AI's Bossert. Running Octarine on Envoy offloads some of the resource requirements from the container host, and managing mTLS encryption and policy-as-code together through Istio is also convenient, he said. Larger organizations such as the U.S. Air Force will also keep an eye on Octarine as it matures, as OPA has been unwieldy to use so far, but would most like to use a Kubernetes policy as code tool that isn't tied to a particular service mesh. \"You can end up with massive lock-in if you abstract teams from the infrastructure, but then couple [security policy] tightly with a mesh again,\" said Nicolas Chaillan, chief software officer for the military branch, which has deployed Istio in production but plans to evaluate other service meshes, including Linkerd. NeuVector loops in CRDs for Kubernetes security NeuVector released a Kubernetes security policy-as-code tool that moved it up the stack last month, which deploys Kubernetes Custom Resource Definitions (CRDs) that are version-controlled and tested within a CI/CD pipeline instead of a service mesh. The company, which began as a container runtime scanning tool, also added network-based data loss prevention (DLP) features and multi-cluster management in version 3.0 in March. A lot of tools cover just one aspect of security management, and just figuring out how all the pieces fit together is a hassle. In about three to five years, I think we'll see consolidation in the market and more complete [products]. Sean McCormickVice president of engineering, Element Analytics Like Octarine, NeuVector can observe normal container behavior on a Kubernetes cluster network and define appropriate application behavior instead of requiring that users create policy from scratch. But for users interested in OPA, NeuVector's tool can import OPA-based policy-as-code data into CRDs as well. \"With an engineering team of 20 people it's hard to pull in new things like service mesh,\" said NeuVector user Sean McCormick, vice president of engineering at Element Analytics, an industrial data analytics firm in San Francisco. \"Being able to export security rules is also nice, so you don't have to spend a week learning rules in a new place.\" McCormick also plans to evaluate NeuVector's DLP features, and would like to see the vendor expand further to offer a web application firewall and application code security analysis. \"There are way too many security tools,\" he said. \"A lot of tools cover just one aspect of security management, and just figuring out how all the pieces fit together is a hassle. In about three to five years, I think we'll see consolidation in the market and more complete [products].\" NeuVector's CRD workflow for Kubernetes policy-as-code. Fairwinds tackles Kubernetes security fundamentals Another container management vendor that looks to expand its influence in the Kubernetes security realm is Fairwinds, a managed Kubernetes service provider in Boston. Fairwinds, formerly ReactiveOps, originally specialized in fully managed Kubernetes clusters, but launched Kubernetes management tools customers can use on their own beginning with the Polaris Kubernetes distro and Goldilocks resource request optimization tool in July. Last month, it added Fairwinds Insights, which displays Kubernetes security monitoring data alongside performance and reliability feedback. Fairwinds Insights also presents ranked remediation recommendations that include YAML code users can copy and paste to shore up vulnerabilities. The tool will also pull in and orchestrate third-party Kubernetes security utilities such as Aqua's kube-hunter. Fairwinds Insights is not as in-depth a tool as OPA or full-blown policy-as-code, but it could help smaller shops move from Kubernetes clusters fully managed by the vendor to self-managed environments, while maintaining security best practices. Fairwinds Insights prioritizes Kubernetes security and reliability action items and includes remediation recommendations for users. For companies such as Philadelphia-based Sidecar, a marketing and advertising software firm, Fairwinds Insights will cover the most crucial Kubernetes security management requirements at a cluster-wide level while the IT team hones its container management skills. \"A tool at the network infrastructure level gets past the most immediate security concerns, such as locking down public access to clusters and configuring AWS load-balancers,\" said Dominic O'Kane, manager of cloud engineering at Sidecar, which also uses Fairwinds' managed services. \"Then we can take on more fine-grained tools that look at individual applications and containers.\" Dig Deeper on Managing Virtual Containers CNCF policy-as-code project bridges Kubernetes security gaps By: BethPariseau Sysdig deal reflects infrastructure-as-code security buzz By: BethPariseau Kubernetes security automation saves SecOps sanity By: BethPariseau What is container management and why is it important? By: EmilyMell ",
          "As a (perhaps overly cynical) outside observer it feels that \"kubernetes X abound\" for all X.  There's just such a complex ecosystem of tooling evolving here.",
          "I find it interesting that companies can convince auditors that security sidecars that add auth and encryption actually meet compliance requirements... it's a nice architecture but I'd argue it renders the environment non-compliant."
        ],
        "story_type": ["Normal"],
        "url": "https://searchitoperations.techtarget.com/news/252475750/New-Kubernetes-security-tools-abound-as-container-deployments-grow",
        "comments.comment_id": [21836565, 21836803],
        "comments.comment_author": ["seanhunter", "kerng"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-12-19T16:49:03Z",
          "2019-12-19T17:09:49Z"
        ],
        "comments.comment_text": [
          "As a (perhaps overly cynical) outside observer it feels that \"kubernetes X abound\" for all X.  There's just such a complex ecosystem of tooling evolving here.",
          "I find it interesting that companies can convince auditors that security sidecars that add auth and encryption actually meet compliance requirements... it's a nice architecture but I'd argue it renders the environment non-compliant."
        ],
        "id": "18f5b57c-9025-4ed3-a9f7-b1696c7e9da5",
        "url_text": "As Kubernetes use expands in production, enterprises have a number of IT security tools to choose from for centralized, policy-based control over container clusters. Emerging Kubernetes security tools focus security operations at higher layers of the IT stack, as the container orchestration platform grows into a production staple for mainstream enterprises. Approaches to Kubernetes security vary among these tools -- one newcomer, Octarine, piggybacks on the Envoy proxy and Istio service mesh to monitor container infrastructure for threats and enforce security policy, while an existing container security startup, NeuVector, plugs policy-as-code tools into the CI/CD pipeline. Another tool from managed Kubernetes player Fairwinds combines security and IT performance and reliability monitoring with a multi-cluster dashboard view. What they all have in common is their emergence as enterprises look for centralized points of Kubernetes security control over an entire environment, in addition to tools that operate at the individual container or application workload level. \"Looking at the container workload is great, but if you don't have a sense of everything that's running within your Kubernetes ecosystem and how it's communicating, it's very easy for rogue deployments to slip in when you have hundreds of namespaces and thousands of pods,\" said Trevor Bossert, manager of DevOps at Primer AI, a data analytics firm in San Francisco which began using Octarine's Kubernetes security software six months ago. \"This will let you know when [cluster configurations] are violating policies, like if they're public by default when they're not supposed to be.\" Octarine rides along in service mesh sidecar Octarine, based in Sunnyvale, Calif., came out of stealth last month with a service mesh-based approach to Kubernetes security. The company claims that installing its software on the Envoy service mesh proxy gives users a clearer picture of the Kubernetes orchestration layer than tools that monitor the infrastructure from privileged containers on hosts. Placing security monitoring and enforcement inside Envoy lets Octarine see whether container workloads are exposed to the internet, how secrets are exposed to container workloads and monitor east-west traffic more effectively, according to Octarine CTO and co-founder Haim Helman. Envoy is often associated with the Istio service mesh, which has its own security features, but Octarine doesn't replace those features, which include the enforcement of role-based access control and mutual TLS encryption. Instead, Octarine collects security telemetry and identifies anomalies and threats with its Octarine Runtime module, and manages Kubernetes security policy-as-code with a tool it calls Guardrails. It can feed security monitoring information into Istio's control plane if a user already has it, or run its own service mesh control plane if the user doesn't have Istio in place. There are other ways to create and enforce Kubernetes policy-as-code, among them the open source Open Policy Agent (OPA) that rose in popularity among large organizations in 2019, but midsize companies with smaller teams may find Octarine's policy-as-code features easier to use. Octarine's service-mesh-based Kubernetes security tool centralizes policy management at the network level \"Not having to craft all policies from scratch, being able to [let] Octarine observe the traffic and providing the best policy, is less time-consuming and involves less duplication of work, especially for a smaller team like ours,\" said Primer AI's Bossert. Running Octarine on Envoy offloads some of the resource requirements from the container host, and managing mTLS encryption and policy-as-code together through Istio is also convenient, he said. Larger organizations such as the U.S. Air Force will also keep an eye on Octarine as it matures, as OPA has been unwieldy to use so far, but would most like to use a Kubernetes policy as code tool that isn't tied to a particular service mesh. \"You can end up with massive lock-in if you abstract teams from the infrastructure, but then couple [security policy] tightly with a mesh again,\" said Nicolas Chaillan, chief software officer for the military branch, which has deployed Istio in production but plans to evaluate other service meshes, including Linkerd. NeuVector loops in CRDs for Kubernetes security NeuVector released a Kubernetes security policy-as-code tool that moved it up the stack last month, which deploys Kubernetes Custom Resource Definitions (CRDs) that are version-controlled and tested within a CI/CD pipeline instead of a service mesh. The company, which began as a container runtime scanning tool, also added network-based data loss prevention (DLP) features and multi-cluster management in version 3.0 in March. A lot of tools cover just one aspect of security management, and just figuring out how all the pieces fit together is a hassle. In about three to five years, I think we'll see consolidation in the market and more complete [products]. Sean McCormickVice president of engineering, Element Analytics Like Octarine, NeuVector can observe normal container behavior on a Kubernetes cluster network and define appropriate application behavior instead of requiring that users create policy from scratch. But for users interested in OPA, NeuVector's tool can import OPA-based policy-as-code data into CRDs as well. \"With an engineering team of 20 people it's hard to pull in new things like service mesh,\" said NeuVector user Sean McCormick, vice president of engineering at Element Analytics, an industrial data analytics firm in San Francisco. \"Being able to export security rules is also nice, so you don't have to spend a week learning rules in a new place.\" McCormick also plans to evaluate NeuVector's DLP features, and would like to see the vendor expand further to offer a web application firewall and application code security analysis. \"There are way too many security tools,\" he said. \"A lot of tools cover just one aspect of security management, and just figuring out how all the pieces fit together is a hassle. In about three to five years, I think we'll see consolidation in the market and more complete [products].\" NeuVector's CRD workflow for Kubernetes policy-as-code. Fairwinds tackles Kubernetes security fundamentals Another container management vendor that looks to expand its influence in the Kubernetes security realm is Fairwinds, a managed Kubernetes service provider in Boston. Fairwinds, formerly ReactiveOps, originally specialized in fully managed Kubernetes clusters, but launched Kubernetes management tools customers can use on their own beginning with the Polaris Kubernetes distro and Goldilocks resource request optimization tool in July. Last month, it added Fairwinds Insights, which displays Kubernetes security monitoring data alongside performance and reliability feedback. Fairwinds Insights also presents ranked remediation recommendations that include YAML code users can copy and paste to shore up vulnerabilities. The tool will also pull in and orchestrate third-party Kubernetes security utilities such as Aqua's kube-hunter. Fairwinds Insights is not as in-depth a tool as OPA or full-blown policy-as-code, but it could help smaller shops move from Kubernetes clusters fully managed by the vendor to self-managed environments, while maintaining security best practices. Fairwinds Insights prioritizes Kubernetes security and reliability action items and includes remediation recommendations for users. For companies such as Philadelphia-based Sidecar, a marketing and advertising software firm, Fairwinds Insights will cover the most crucial Kubernetes security management requirements at a cluster-wide level while the IT team hones its container management skills. \"A tool at the network infrastructure level gets past the most immediate security concerns, such as locking down public access to clusters and configuring AWS load-balancers,\" said Dominic O'Kane, manager of cloud engineering at Sidecar, which also uses Fairwinds' managed services. \"Then we can take on more fine-grained tools that look at individual applications and containers.\" Dig Deeper on Managing Virtual Containers CNCF policy-as-code project bridges Kubernetes security gaps By: BethPariseau Sysdig deal reflects infrastructure-as-code security buzz By: BethPariseau Kubernetes security automation saves SecOps sanity By: BethPariseau What is container management and why is it important? By: EmilyMell ",
        "_version_": 1718527443244941312
      },
      {
        "story_id": [19078281],
        "story_author": ["anishathalye"],
        "story_descendants": [79],
        "story_score": [1028],
        "story_time": ["2019-02-04T17:21:56Z"],
        "story_title": "MIT Hacker Tools: a lecture series on programmer tools",
        "search": [
          "MIT Hacker Tools: a lecture series on programmer tools",
          "https://hacker-tools.github.io/",
          "This class has moved to https://missing.csail.mit.edu/. You should go there to see the newest version of the material. This site is being left up for archival purposes. You can see all the lectures from the IAP 2019 course here. ",
          "Hi all! We (@anishathalye, @jjgo, and @jonhoo) have long felt that while university CS classes are great at teaching specific topics, they often leave it to students to figure out a lot of the common knowledge about how to actually use your computer. And in particular, how to use it efficiently.<p>Theres just no class in the undergrad curriculum that teaches you how to become familiar with the system youre working with! Students are expected to know about, or figure out, the shell, editors, remote access and file management, version control, debugging and profiling utilities, and all sorts of other useful tools on their own. Often times, they wont even know that many of these tools exist, and instead do things in roundabout ways or simply be left frustrated about their development environment.<p>To help mitigate this, we decided to run this short lecture series at MIT during the January Independent Activities Period that we called Hacker Tools (in reference to hacker culture, not hacking computers). Our hope was that through this class, and the resulting lecture materials and videos, we might be able to bootstrap students knowledge about the tools that are available to them, which they can then put to use throughout their time at university, and beyond.<p>Weve shared both the lecture notes and the recordings of the lectures in the hopes that people outside of MIT may also find these resources useful in making better use of their tools. If that turns out to be true, were also thinking of re-doing the videos in screen-cast style with live chat and a proper microphone when we get the time. If that sounds interesting to you, and if you have ideas about other things youd like to see us cover, please leave a comment below; wed love to hear from you!<p>Were sure there are also plenty of cool tools that we didnt get to cover in this series that you all know and love. Please share them below along with a short description so we can all learn something new!<p>Anish, Jose, and Jon",
          "The equivalent UCLA course is CS35L: Software Construction Laboratory (<a href=\"https://web.cs.ucla.edu/classes/winter19/cs35L/\" rel=\"nofollow\">https://web.cs.ucla.edu/classes/winter19/cs35L/</a>). It's taught by Paul Eggert (big open source/coreutils/emacs contributor + author of diff/sort)."
        ],
        "story_type": ["Normal"],
        "url": "https://hacker-tools.github.io/",
        "url_text": "This class has moved to https://missing.csail.mit.edu/. You should go there to see the newest version of the material. This site is being left up for archival purposes. You can see all the lectures from the IAP 2019 course here. ",
        "comments.comment_id": [19078338, 19080440],
        "comments.comment_author": ["Jonhoo", "bhchiang"],
        "comments.comment_descendants": [6, 2],
        "comments.comment_time": [
          "2019-02-04T17:25:35Z",
          "2019-02-04T20:44:16Z"
        ],
        "comments.comment_text": [
          "Hi all! We (@anishathalye, @jjgo, and @jonhoo) have long felt that while university CS classes are great at teaching specific topics, they often leave it to students to figure out a lot of the common knowledge about how to actually use your computer. And in particular, how to use it efficiently.<p>Theres just no class in the undergrad curriculum that teaches you how to become familiar with the system youre working with! Students are expected to know about, or figure out, the shell, editors, remote access and file management, version control, debugging and profiling utilities, and all sorts of other useful tools on their own. Often times, they wont even know that many of these tools exist, and instead do things in roundabout ways or simply be left frustrated about their development environment.<p>To help mitigate this, we decided to run this short lecture series at MIT during the January Independent Activities Period that we called Hacker Tools (in reference to hacker culture, not hacking computers). Our hope was that through this class, and the resulting lecture materials and videos, we might be able to bootstrap students knowledge about the tools that are available to them, which they can then put to use throughout their time at university, and beyond.<p>Weve shared both the lecture notes and the recordings of the lectures in the hopes that people outside of MIT may also find these resources useful in making better use of their tools. If that turns out to be true, were also thinking of re-doing the videos in screen-cast style with live chat and a proper microphone when we get the time. If that sounds interesting to you, and if you have ideas about other things youd like to see us cover, please leave a comment below; wed love to hear from you!<p>Were sure there are also plenty of cool tools that we didnt get to cover in this series that you all know and love. Please share them below along with a short description so we can all learn something new!<p>Anish, Jose, and Jon",
          "The equivalent UCLA course is CS35L: Software Construction Laboratory (<a href=\"https://web.cs.ucla.edu/classes/winter19/cs35L/\" rel=\"nofollow\">https://web.cs.ucla.edu/classes/winter19/cs35L/</a>). It's taught by Paul Eggert (big open source/coreutils/emacs contributor + author of diff/sort)."
        ],
        "id": "1efcaa82-32a2-49a6-8818-eb1c85262b7e",
        "_version_": 1718527384888541185
      },
      {
        "story_id": [19185570],
        "story_author": ["flagada"],
        "story_descendants": [10],
        "story_score": [53],
        "story_time": ["2019-02-17T17:26:52Z"],
        "story_title": "Arm Helium: New vector extension for the M-Profile Architecture",
        "search": [
          "Arm Helium: New vector extension for the M-Profile Architecture",
          "https://community.arm.com/processors/b/blog/posts/arm-helium-the-new-vector-extension-for-arm-m-profile-architecture",
          "Arm has announced the latest version of the Armv8-M architecture, known as Armv8.1-M, including the new M-Profile Vector Extension (MVE). The vector extensionbrings up to15times performance uplift to machine learning (ML) functions, and up to5times uplift to signal processing functionscompared to existing Armv8-M implementations. It may be viewed as the Armv8-M architectures version of the Advanced SIMD Extension (Neon) in the A-Profile. Arm Helium technologyis the M-Profile Vector Extension (MVE)for the Arm Cortex-M processor series. Armv8.1-M architecture new features A new vector instruction set extension (MVE) Additional instruction set enhancements for loops and branches (Low Overhead Branch Extension) Instructions providing half precision floating-point support Instruction improving state management of the Floating Point Unit (FPU) Enhancements to debug including: Performance Monitoring Unit (PMU) Unprivileged Debug Extension Debug support for MVE Reliability, Availability and Serviceability (RAS) extension Start early software development Arm tools aredeveloped along with the architecture. They are now ready for lead partners to start developing software and migrating libraries and other code to Helium,to enable performance increases for DSP and machine learning applications. Tools with support include: Fast Models for software execution and optimization on a virtual platform Arm Development Studio for comprehensive software development and debugging on Windows or Linux for any Arm-based projects Arm Compiler 6 for maximizing code performance Keil MDK for software development and debugging in Windows for Cortex-M and microcontrollers The Armv8.1-M simple programmers model, combined with familiar Arm tools, is a key advantage of Helium. Using a single toolchain for control and data processing, leads to lower development costs and less code maintenance. Virtual platform with Fast Models Arm Fast Modelsprovide fast, flexible programmer's view models of Arm architecture and IP, enabling software development of drivers, firmware, operating systems, and applications prior to silicon availability. Fast Models allow full control over the simulation, including profiling, debug and trace. There is a Fast Model available for lead partners, which can be used for early software development. It is based on the MPS2 Fixed Virtual Platform (FVP). The Armv8-M architecture envelope model (AEM) has been extended via the plugin interface to support Helium. This provides a suitable platform to get started writing and debugging software. Code, build and debug with Development Studio Development Studiofeaturing Keil MDK(Vision) has added Helium support for software compilation (Arm Compiler 6) and debugging. This includes disassembly and updated register views for new registers in Armv8.1-M.The toolsuite is also available for lead partners today. Performance enhancements to Cortex-M Helium, the M-Profile Vector Extension included in Armv8.1-M, brings significant enhancements to the Cortex-M processor range and will enable the use of a single CPU for both control and data processing code. The performance enhancements enable applications, such as machine learning and DSP. Arm tools have been developed in parallel with the architecture and are available now for lead partners to start developing software on both Windows and Linux. The Helium support in Arm Compiler 6, combined with leading performance and code density, make it a great choice to get a jumpstart on migrating software to Helium. Arm Fast Models combined with Arm debuggers make it possible to run code and see the Architecture Reference Manual in action. Further reading The press announcementgives a high-level overview of Armv8.1-M and Helium, plusdetails on the performanceenhancements. The 'Making Helium' blogoffers insight intothe creation of Arm's MVE. For full details on the architecture see the Architecture Reference Manual for Armv8.1-M. The Introduction to Armv8.1-M architecture white paper showcases the technical highlights of the new features and is available to download below. Download Armv8.1-M Architecture White Paper November 3, 2021 November 3, 2021 October 29, 2021 ",
          "The blog post <a href=\"https://community.arm.com/arm-research/b/articles/posts/making-helium-why-not-just-add-neon\" rel=\"nofollow\">https://community.arm.com/arm-research/b/articles/posts/maki...</a> has more useful content.",
          "Meantime, in the royalty-free side, RISCV's work on V extension continues: <a href=\"https://www.embecosm.com/2018/09/09/supporting-the-risc-v-vector-extension-in-gcc-and-llvm/\" rel=\"nofollow\">https://www.embecosm.com/2018/09/09/supporting-the-risc-v-ve...</a>"
        ],
        "story_type": ["Normal"],
        "url": "https://community.arm.com/processors/b/blog/posts/arm-helium-the-new-vector-extension-for-arm-m-profile-architecture",
        "comments.comment_id": [19186393, 19187144],
        "comments.comment_author": ["jedharris", "snvzz"],
        "comments.comment_descendants": [1, 2],
        "comments.comment_time": [
          "2019-02-17T19:44:13Z",
          "2019-02-17T21:52:28Z"
        ],
        "comments.comment_text": [
          "The blog post <a href=\"https://community.arm.com/arm-research/b/articles/posts/making-helium-why-not-just-add-neon\" rel=\"nofollow\">https://community.arm.com/arm-research/b/articles/posts/maki...</a> has more useful content.",
          "Meantime, in the royalty-free side, RISCV's work on V extension continues: <a href=\"https://www.embecosm.com/2018/09/09/supporting-the-risc-v-vector-extension-in-gcc-and-llvm/\" rel=\"nofollow\">https://www.embecosm.com/2018/09/09/supporting-the-risc-v-ve...</a>"
        ],
        "id": "bd29fc75-7d05-4bb3-a009-4d0c55aab1c1",
        "url_text": "Arm has announced the latest version of the Armv8-M architecture, known as Armv8.1-M, including the new M-Profile Vector Extension (MVE). The vector extensionbrings up to15times performance uplift to machine learning (ML) functions, and up to5times uplift to signal processing functionscompared to existing Armv8-M implementations. It may be viewed as the Armv8-M architectures version of the Advanced SIMD Extension (Neon) in the A-Profile. Arm Helium technologyis the M-Profile Vector Extension (MVE)for the Arm Cortex-M processor series. Armv8.1-M architecture new features A new vector instruction set extension (MVE) Additional instruction set enhancements for loops and branches (Low Overhead Branch Extension) Instructions providing half precision floating-point support Instruction improving state management of the Floating Point Unit (FPU) Enhancements to debug including: Performance Monitoring Unit (PMU) Unprivileged Debug Extension Debug support for MVE Reliability, Availability and Serviceability (RAS) extension Start early software development Arm tools aredeveloped along with the architecture. They are now ready for lead partners to start developing software and migrating libraries and other code to Helium,to enable performance increases for DSP and machine learning applications. Tools with support include: Fast Models for software execution and optimization on a virtual platform Arm Development Studio for comprehensive software development and debugging on Windows or Linux for any Arm-based projects Arm Compiler 6 for maximizing code performance Keil MDK for software development and debugging in Windows for Cortex-M and microcontrollers The Armv8.1-M simple programmers model, combined with familiar Arm tools, is a key advantage of Helium. Using a single toolchain for control and data processing, leads to lower development costs and less code maintenance. Virtual platform with Fast Models Arm Fast Modelsprovide fast, flexible programmer's view models of Arm architecture and IP, enabling software development of drivers, firmware, operating systems, and applications prior to silicon availability. Fast Models allow full control over the simulation, including profiling, debug and trace. There is a Fast Model available for lead partners, which can be used for early software development. It is based on the MPS2 Fixed Virtual Platform (FVP). The Armv8-M architecture envelope model (AEM) has been extended via the plugin interface to support Helium. This provides a suitable platform to get started writing and debugging software. Code, build and debug with Development Studio Development Studiofeaturing Keil MDK(Vision) has added Helium support for software compilation (Arm Compiler 6) and debugging. This includes disassembly and updated register views for new registers in Armv8.1-M.The toolsuite is also available for lead partners today. Performance enhancements to Cortex-M Helium, the M-Profile Vector Extension included in Armv8.1-M, brings significant enhancements to the Cortex-M processor range and will enable the use of a single CPU for both control and data processing code. The performance enhancements enable applications, such as machine learning and DSP. Arm tools have been developed in parallel with the architecture and are available now for lead partners to start developing software on both Windows and Linux. The Helium support in Arm Compiler 6, combined with leading performance and code density, make it a great choice to get a jumpstart on migrating software to Helium. Arm Fast Models combined with Arm debuggers make it possible to run code and see the Architecture Reference Manual in action. Further reading The press announcementgives a high-level overview of Armv8.1-M and Helium, plusdetails on the performanceenhancements. The 'Making Helium' blogoffers insight intothe creation of Arm's MVE. For full details on the architecture see the Architecture Reference Manual for Armv8.1-M. The Introduction to Armv8.1-M architecture white paper showcases the technical highlights of the new features and is available to download below. Download Armv8.1-M Architecture White Paper November 3, 2021 November 3, 2021 October 29, 2021 ",
        "_version_": 1718527386530611201
      },
      {
        "story_id": [21534619],
        "story_author": ["tombrm"],
        "story_descendants": [3],
        "story_score": [10],
        "story_time": ["2019-11-14T11:39:21Z"],
        "story_title": "Show HN: Respresso  localization and design asset optimizer for iOS and Android",
        "search": [
          "Show HN: Respresso  localization and design asset optimizer for iOS and Android",
          "https://respresso.io/",
          "Save hours with efficient collaboration Manage your resources in multiplatform environments. Fonts Easy integration of custom fonts Localizations Modify localization texts or add a new language to your project, without developers Images Change or resize an image anytime and keep in sync on all platforms App icons No more generating thousands of icon sizes, just use one SVG for all platforms. Colors No more incorrect guideline colors. Your designers will have the ability to set the perfect colors to be used on all platforms Raw Easy access to all your common config files(JSON, XML, YAML) Customize Respresso Extend Respressos functionality or connect your work tools like Slack, Teams, Jenkins etc. Version control All resources are under version control. You can easily lock your assets version and reuse it later. Be agile and spare development time Collaborate on assets with your team members or customer from anywhere in a transparent way to boost your productivity. It automatically transforms and delivers to your project without assistance. Your assets will be ready for use almost immediately. It takes care of your digital assets (images, texts, colors, fonts, etc.) across multiple platforms and projects. Help your team focus during the development stage. Developers code, designers deliver graphics, the marketing team, translators manage your localization and this is just the beginning of stress-free development. Start using in 3 simple steps Easy as pie 1-minute setup Upload your origin resources Sync converted resources across multiple platforms Respresso is simple but powerful Respresso can easily be integrated into your build process and also works well with your Continuous Integration tools. Respresso manages your resources Convert your resources automatically to platform-specific formats such as VectorDrawable for Android, PDF for iOS etc. Synchronize with your project in build-time regardless of which platform you use What else is it good for? Versioned resources and repeatable build support for CI & CD A better team experience via team and project creation. It provides you with a way to track every change in each project and send feedback. For example you will be notified when someone changes the key of a localization you use Well-separated roles spell correction and translation without developer intervention fix any graphics files and change icons without coding skills rebrand the application by changing colors and app icons Enforce naming conventions to enhance code quality Try our image converter Simply upload your chosen SVG file and download the generated resources as VectorDrawable for Android and PDF for iOS. Get access for Free Once you create an account you can try Respresso for free, for iOS, Android and Web frontend projects, too. Do you have any question? ",
          "Respresso helps mobile developers by automatically optimizing their design assets for iOS and Android and comes with a live localization feature.<p>It's still in beta and all feedback is highly appreciated.",
          "What is the pricing?"
        ],
        "story_type": ["ShowHN"],
        "url": "https://respresso.io/",
        "url_text": "Save hours with efficient collaboration Manage your resources in multiplatform environments. Fonts Easy integration of custom fonts Localizations Modify localization texts or add a new language to your project, without developers Images Change or resize an image anytime and keep in sync on all platforms App icons No more generating thousands of icon sizes, just use one SVG for all platforms. Colors No more incorrect guideline colors. Your designers will have the ability to set the perfect colors to be used on all platforms Raw Easy access to all your common config files(JSON, XML, YAML) Customize Respresso Extend Respressos functionality or connect your work tools like Slack, Teams, Jenkins etc. Version control All resources are under version control. You can easily lock your assets version and reuse it later. Be agile and spare development time Collaborate on assets with your team members or customer from anywhere in a transparent way to boost your productivity. It automatically transforms and delivers to your project without assistance. Your assets will be ready for use almost immediately. It takes care of your digital assets (images, texts, colors, fonts, etc.) across multiple platforms and projects. Help your team focus during the development stage. Developers code, designers deliver graphics, the marketing team, translators manage your localization and this is just the beginning of stress-free development. Start using in 3 simple steps Easy as pie 1-minute setup Upload your origin resources Sync converted resources across multiple platforms Respresso is simple but powerful Respresso can easily be integrated into your build process and also works well with your Continuous Integration tools. Respresso manages your resources Convert your resources automatically to platform-specific formats such as VectorDrawable for Android, PDF for iOS etc. Synchronize with your project in build-time regardless of which platform you use What else is it good for? Versioned resources and repeatable build support for CI & CD A better team experience via team and project creation. It provides you with a way to track every change in each project and send feedback. For example you will be notified when someone changes the key of a localization you use Well-separated roles spell correction and translation without developer intervention fix any graphics files and change icons without coding skills rebrand the application by changing colors and app icons Enforce naming conventions to enhance code quality Try our image converter Simply upload your chosen SVG file and download the generated resources as VectorDrawable for Android and PDF for iOS. Get access for Free Once you create an account you can try Respresso for free, for iOS, Android and Web frontend projects, too. Do you have any question? ",
        "comments.comment_id": [21534748, 21554836],
        "comments.comment_author": ["tombrm", "deca6cda37d0"],
        "comments.comment_descendants": [0, 1],
        "comments.comment_time": [
          "2019-11-14T12:06:56Z",
          "2019-11-16T20:42:53Z"
        ],
        "comments.comment_text": [
          "Respresso helps mobile developers by automatically optimizing their design assets for iOS and Android and comes with a live localization feature.<p>It's still in beta and all feedback is highly appreciated.",
          "What is the pricing?"
        ],
        "id": "73b42f55-0894-48dd-8666-76bb234c411d",
        "_version_": 1718527435659542528
      }
    ]
  }
}
