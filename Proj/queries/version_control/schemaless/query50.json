{
  "responseHeader": {
    "status": 0,
    "QTime": 0
  },
  "response": {
    "numFound": 16079,
    "start": 0,
    "numFoundExact": true,
    "docs": [
      {
        "story_id": [19706396],
        "story_author": ["beefhash"],
        "story_descendants": [10],
        "story_score": [125],
        "story_time": ["2019-04-20T13:22:38Z"],
        "story_title": "Fh: File history with ed, diff, awk, sed, and sh",
        "search": [
          "Fh: File history with ed, diff, awk, sed, and sh",
          "https://github.com/xorhash/fh",
          "fh records changes to a file on a per-file basis, similar to RCS and SCCS. It is, however, considerably more primitive. Design goals: no support for multi-user environments (no locking, etc.) implemented in shell script must use ed(1) should work or easily be made to work on 7th Edition UNIX I've taken care not to use any shell scripting constructions that didn't exist in the Bourne shell, but I may have missed things; however, the shebang needs to be removed on 7th Edition UNIX. fh uses a chain of ed(1) scripts to construct any version of a file. It even allows recording of commit messages. Why? I saw the following passage in diff(1) of 7th Edition UNIX: The -e option produces a script of a, c and d commands for the editor ed, which will recreate file2 from file1. The -f option produces a similar script, not useful with ed, in the opposite order. In connection with -e, the following shell program may help maintain multiple versions of a file. Only an ancestral file ($1) and a chain of version-to-version ed scripts ($2,$3,...) made by diff need be on hand. A `latest version' appears on the standard output. (shift; cat $*; echo 1,$p) ed - $1 After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious. License ISC, see LICENSE. Installation and usage Copy the fl, fr and fu files to a directory in $PATH; make sure they are marked as executable. Copy the man pages fl.1, fr.1, fu.1 and fh.5 to a directory in $MANPATH. For usage, see the supplied man pages. man.md is available on the web. For 7th Edition UNIX, the man pages written in mdoc macrosneed to be converted to old man macros first: mkdir man mandoc -Tman fl.1 > man/fl.1 mandoc -Tman fr.1 > man/fr.1 mandoc -Tman fu.1 > man/fu.1 mandoc -Tman fh.5 > man/fh.5 ",
          "Wait, what?<p><i>\"After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious.\"</i><p>Ah, ok.<p>Also, if you want to experiment with diffs and recreating versions:<p>Patch: <a href=\"https://en.m.wikipedia.org/wiki/Patch_(Unix)\" rel=\"nofollow\">https://en.m.wikipedia.org/wiki/Patch_(Unix)</a><p>Xdelta:  <a href=\"https://github.com/jmacd/xdelta\" rel=\"nofollow\">https://github.com/jmacd/xdelta</a>",
          "It's not a bad exercise.  In a Unix class I took, we had to implement an RDBMS using only the traditional Unix software tools.  A little later, for work, I had to learn to write highly portable and fairly secure shell scripts.<p>A few reasons to do exercises like this:<p>* To understand the Unix software tools and Bourne/etc. shell scripting models.  There are good lessons, and also examples of what not to do (or the tradeoffs), which I don't think you'll find anywhere else.  (For example, the simple Unix streams model is very powerful, and also often used in very kludgey ways in practice, and the Bourne evaluation model is probably much worse and dangerous than a beginner might think.)<p>* Knowing how to do things with just shell scripts is great for some kinds of bootstrapping, commands in build tools, or other situations in which you can only assume a shell interpreter and possibly certain command-line tools.<p>* Handy for configuring your interactive shell to do what you want.<p>* It's one way to familiarize with parts of a Unix-ish system that you might not normally, especially if you're spending all your time learning your way around huge stacks of tools that obscure the lower-level mechanics.<p>* You'll appreciate Perl and other languages much more, in some ways."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/xorhash/fh",
        "url_text": "fh records changes to a file on a per-file basis, similar to RCS and SCCS. It is, however, considerably more primitive. Design goals: no support for multi-user environments (no locking, etc.) implemented in shell script must use ed(1) should work or easily be made to work on 7th Edition UNIX I've taken care not to use any shell scripting constructions that didn't exist in the Bourne shell, but I may have missed things; however, the shebang needs to be removed on 7th Edition UNIX. fh uses a chain of ed(1) scripts to construct any version of a file. It even allows recording of commit messages. Why? I saw the following passage in diff(1) of 7th Edition UNIX: The -e option produces a script of a, c and d commands for the editor ed, which will recreate file2 from file1. The -f option produces a similar script, not useful with ed, in the opposite order. In connection with -e, the following shell program may help maintain multiple versions of a file. Only an ancestral file ($1) and a chain of version-to-version ed scripts ($2,$3,...) made by diff need be on hand. A `latest version' appears on the standard output. (shift; cat $*; echo 1,$p) ed - $1 After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious. License ISC, see LICENSE. Installation and usage Copy the fl, fr and fu files to a directory in $PATH; make sure they are marked as executable. Copy the man pages fl.1, fr.1, fu.1 and fh.5 to a directory in $MANPATH. For usage, see the supplied man pages. man.md is available on the web. For 7th Edition UNIX, the man pages written in mdoc macrosneed to be converted to old man macros first: mkdir man mandoc -Tman fl.1 > man/fl.1 mandoc -Tman fr.1 > man/fr.1 mandoc -Tman fu.1 > man/fu.1 mandoc -Tman fh.5 > man/fh.5 ",
        "comments.comment_id": [19706631, 19706816],
        "comments.comment_author": ["tyingq", "neilv"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-04-20T14:22:11Z",
          "2019-04-20T14:54:29Z"
        ],
        "comments.comment_text": [
          "Wait, what?<p><i>\"After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious.\"</i><p>Ah, ok.<p>Also, if you want to experiment with diffs and recreating versions:<p>Patch: <a href=\"https://en.m.wikipedia.org/wiki/Patch_(Unix)\" rel=\"nofollow\">https://en.m.wikipedia.org/wiki/Patch_(Unix)</a><p>Xdelta:  <a href=\"https://github.com/jmacd/xdelta\" rel=\"nofollow\">https://github.com/jmacd/xdelta</a>",
          "It's not a bad exercise.  In a Unix class I took, we had to implement an RDBMS using only the traditional Unix software tools.  A little later, for work, I had to learn to write highly portable and fairly secure shell scripts.<p>A few reasons to do exercises like this:<p>* To understand the Unix software tools and Bourne/etc. shell scripting models.  There are good lessons, and also examples of what not to do (or the tradeoffs), which I don't think you'll find anywhere else.  (For example, the simple Unix streams model is very powerful, and also often used in very kludgey ways in practice, and the Bourne evaluation model is probably much worse and dangerous than a beginner might think.)<p>* Knowing how to do things with just shell scripts is great for some kinds of bootstrapping, commands in build tools, or other situations in which you can only assume a shell interpreter and possibly certain command-line tools.<p>* Handy for configuring your interactive shell to do what you want.<p>* It's one way to familiarize with parts of a Unix-ish system that you might not normally, especially if you're spending all your time learning your way around huge stacks of tools that obscure the lower-level mechanics.<p>* You'll appreciate Perl and other languages much more, in some ways."
        ],
        "id": "c6e31655-ccb7-4cba-a30a-ac4e1fea6799",
        "_version_": 1718527400651784192
      },
      {
        "story_id": [20745393],
        "story_author": ["ingve"],
        "story_descendants": [355],
        "story_score": [357],
        "story_time": ["2019-08-20T10:39:18Z"],
        "story_title": "Sunsetting Mercurial Support in Bitbucket",
        "search": [
          "Sunsetting Mercurial Support in Bitbucket",
          "https://bitbucket.org/blog/sunsetting-mercurial-support-in-bitbucket",
          "[Update Aug 26, 2020] All hg repos have now been disabled and cannot be accessed.[Update July 1, 2020] Today, mercurial repositories, snippets, and wikis will turn to read-only mode. After July 8th, 2020 they will no longer be accessible. The version control software market has evolved a lot since Bitbucket began in 2008. When we launched, centralized version control was the norm and we only supported Mercurial repos. But Git adoption has grown over the years to become the default system, helping teams of all sizes work faster as they become more distributed.As we surpass 10 million registered users on the platform, we're at a point in our growth where we are conducting a deeper evaluation of the market and how we can best support our users going forward. After much consideration, we've decided to remove Mercurial support from Bitbucket Cloud and its API. Mercurial features and repositories will be officially deprecated on July 1, 2020.Read on to learn more about this decision, the important timelines, and get migration resources and support. The timeline and how this may affect your team Here are the key dates as we sunset Mercurial functionality: February 1, 2020: users will no longer be able to create new Mercurial repositories [Extended] July 1, 2020: users will not be able to use Mercurial features. All hg repos, wikis, and snippets will be in read-only mode. Heres why were focusing on Git This wasnt an easy decision, and Mercurial will always have a special place in Bitbuckets history. DevOps adoption has skyrocketed over the last decade and our customers are adopting this new way of working at an exponential rate. In this time, Bitbucket has steadily grown from being just a version control management tool to being a place to manage the entire software development lifecycle. And there's always more work to be done. This year we will concentrate on building deeper integrations to enhance automation and collaboration. Our improvements will make it even easier and safer to plan, code, test, and deploy all from within Bitbucket. Building quality features requires intense focus, and supporting two version control systems means splitting focus doubling shipping time and technical overhead. With Git being the more popularly used tool, Mercurial runs the risk of overlooked issues as we scale. According to a Stack Overflow Developer Survey, almost 90% of developers use Git, while Mercurial is the least popular version control system with only about 3% developer adoption. In fact, Mercurial usage on Bitbucket is steadily declining, and the percentage of new Bitbucket users choosing Mercurial has fallen to less than 1%. This deprecation will enable us to focus on building the best possible experience for our users. How to migrate and export We recommend that teams migrate their existing Mercurial repos to Git. There are various Git conversion tools in the market, including hg-fast-export and hg-git mercurial plugin. We are happy to support your migration, and you can find a discussion about available options in our dedicated Community thread. If you prefer to continue using the Mercurial system, there are a number of free and paid Mercurial hosting services. We realize that there is no one-size-fits-all solution. That's why we've created the following resources to best equip you with the knowledge and tools for a seamless transition: A Community thread to discuss conversion tools, migration, tips, and offer troubleshooting help A Git tutorial that covers anywhere from the basics of creating pull requests to rebasing and Git hooks We want to thank all the loyal users who have grown with us over the years. We look forward to this new focus on our roadmap and to introducing exciting new features. ",
          "It's funny to see how the whole world concentrates on this Git thing, while there is a treasure trove called Mercurial.<p>Mercurial was made for humans. It is seriously convenient and productive. Something I cannot say about Git, which more reminds me of an adhoc job.<p>I use both Git and Mercurial on daily basis. But my preference goes to Mercurial: it is just more sane in a big way. It is clearly a piece of art and love.",
          "It's very sad to see bitbucket dropping mercurial support. Now only Facebook and volunteers are keeping mercurial alive. \nSometimes technically better architecture and user interface lose to a non user friendly hard solutions due to inertia of mass adoption.<p>So a lesson in Software development is similar to betamax and VHS, so marketing is still a winner over technically superior architecture and ease of use. GitHub successfully marketed git, so git and GitHub are synonymous for most developers. Now majority of open source projects are reliant on a single proprietary solution Github by Microsoft, for managing code and project. Can understand the difficulty of bitbucket, when Python language itself moved out of mercurial due to the same inertia.<p>Hopefully gitlab can come out with mercurial support to migrate projects using it from bitbucket.<p>For people who believe in self hosted solution can install Kallithea (<a href=\"https://kallithea-scm.org\" rel=\"nofollow\">https://kallithea-scm.org</a>) or Rhodecode open source edition. Kallithea is used by Unity engine to manage their source code internally with mercurial."
        ],
        "story_type": ["Normal"],
        "url": "https://bitbucket.org/blog/sunsetting-mercurial-support-in-bitbucket",
        "url_text": "[Update Aug 26, 2020] All hg repos have now been disabled and cannot be accessed.[Update July 1, 2020] Today, mercurial repositories, snippets, and wikis will turn to read-only mode. After July 8th, 2020 they will no longer be accessible. The version control software market has evolved a lot since Bitbucket began in 2008. When we launched, centralized version control was the norm and we only supported Mercurial repos. But Git adoption has grown over the years to become the default system, helping teams of all sizes work faster as they become more distributed.As we surpass 10 million registered users on the platform, we're at a point in our growth where we are conducting a deeper evaluation of the market and how we can best support our users going forward. After much consideration, we've decided to remove Mercurial support from Bitbucket Cloud and its API. Mercurial features and repositories will be officially deprecated on July 1, 2020.Read on to learn more about this decision, the important timelines, and get migration resources and support. The timeline and how this may affect your team Here are the key dates as we sunset Mercurial functionality: February 1, 2020: users will no longer be able to create new Mercurial repositories [Extended] July 1, 2020: users will not be able to use Mercurial features. All hg repos, wikis, and snippets will be in read-only mode. Heres why were focusing on Git This wasnt an easy decision, and Mercurial will always have a special place in Bitbuckets history. DevOps adoption has skyrocketed over the last decade and our customers are adopting this new way of working at an exponential rate. In this time, Bitbucket has steadily grown from being just a version control management tool to being a place to manage the entire software development lifecycle. And there's always more work to be done. This year we will concentrate on building deeper integrations to enhance automation and collaboration. Our improvements will make it even easier and safer to plan, code, test, and deploy all from within Bitbucket. Building quality features requires intense focus, and supporting two version control systems means splitting focus doubling shipping time and technical overhead. With Git being the more popularly used tool, Mercurial runs the risk of overlooked issues as we scale. According to a Stack Overflow Developer Survey, almost 90% of developers use Git, while Mercurial is the least popular version control system with only about 3% developer adoption. In fact, Mercurial usage on Bitbucket is steadily declining, and the percentage of new Bitbucket users choosing Mercurial has fallen to less than 1%. This deprecation will enable us to focus on building the best possible experience for our users. How to migrate and export We recommend that teams migrate their existing Mercurial repos to Git. There are various Git conversion tools in the market, including hg-fast-export and hg-git mercurial plugin. We are happy to support your migration, and you can find a discussion about available options in our dedicated Community thread. If you prefer to continue using the Mercurial system, there are a number of free and paid Mercurial hosting services. We realize that there is no one-size-fits-all solution. That's why we've created the following resources to best equip you with the knowledge and tools for a seamless transition: A Community thread to discuss conversion tools, migration, tips, and offer troubleshooting help A Git tutorial that covers anywhere from the basics of creating pull requests to rebasing and Git hooks We want to thank all the loyal users who have grown with us over the years. We look forward to this new focus on our roadmap and to introducing exciting new features. ",
        "comments.comment_id": [20745989, 20746077],
        "comments.comment_author": ["garganzol", "dragonsh"],
        "comments.comment_descendants": [4, 19],
        "comments.comment_time": [
          "2019-08-20T12:13:11Z",
          "2019-08-20T12:22:53Z"
        ],
        "comments.comment_text": [
          "It's funny to see how the whole world concentrates on this Git thing, while there is a treasure trove called Mercurial.<p>Mercurial was made for humans. It is seriously convenient and productive. Something I cannot say about Git, which more reminds me of an adhoc job.<p>I use both Git and Mercurial on daily basis. But my preference goes to Mercurial: it is just more sane in a big way. It is clearly a piece of art and love.",
          "It's very sad to see bitbucket dropping mercurial support. Now only Facebook and volunteers are keeping mercurial alive. \nSometimes technically better architecture and user interface lose to a non user friendly hard solutions due to inertia of mass adoption.<p>So a lesson in Software development is similar to betamax and VHS, so marketing is still a winner over technically superior architecture and ease of use. GitHub successfully marketed git, so git and GitHub are synonymous for most developers. Now majority of open source projects are reliant on a single proprietary solution Github by Microsoft, for managing code and project. Can understand the difficulty of bitbucket, when Python language itself moved out of mercurial due to the same inertia.<p>Hopefully gitlab can come out with mercurial support to migrate projects using it from bitbucket.<p>For people who believe in self hosted solution can install Kallithea (<a href=\"https://kallithea-scm.org\" rel=\"nofollow\">https://kallithea-scm.org</a>) or Rhodecode open source edition. Kallithea is used by Unity engine to manage their source code internally with mercurial."
        ],
        "id": "3ead87e1-a79d-4f46-9dd5-9db60461d19a",
        "_version_": 1718527420733063168
      },
      {
        "story_id": [21071181],
        "story_author": ["JA7Cal"],
        "story_descendants": [15],
        "story_score": [43],
        "story_time": ["2019-09-25T14:36:38Z"],
        "story_title": "Automated Reports with Jupyter Notebooks (Using Jupytext and Papermill)",
        "search": [
          "Automated Reports with Jupyter Notebooks (Using Jupytext and Papermill)",
          "https://medium.com/capital-fund-management/automated-reports-with-jupyter-notebooks-using-jupytext-and-papermill-619e60c37330",
          "Jupyter notebooks are one of the best available tools for running code interactively and writing a narrative with data and plots. What is less known is that they can be conveniently versioned and run automatically.Do you have a Jupyter notebook with plots and figures that you regularly run manually? Wouldnt it be nice to use the same notebook and instead have an automated reporting system, launched from a script? What if this script could even pass some parameters to the notebook it runs?This post explains in a few steps how this can be done concretely, including within a production environment.Example notebookWe will show you how to version control, automatically run and publish a notebook that depends on a parameter. As an example, we will use a notebook that describes the world population and the gross domestic product for a given year. It is simple to use: just change the year variable in the first cell, re-run, and you get the plots for the chosen year. But this requires a manual intervention. It would be much more convenient if the update could be automated and produce a report for each possible value of the year parameter (more generally, a notebook can update its results based not only on some user-provided parameters, but also through a connection to a database, etc.).Version controlIn a professional environment, notebooks are designed by, say, a data scientist, but the task of running them in production may be handled by a different team. So in general people have to share notebooks. This is best done through a version control system.Jupyter notebooks are famous for the difficulty of their version control. Lets consider our notebook above, with a file size of 3 MB, much of it being contributed by the embedded Plotly library. The notebook is less than 80 KB if we remove the output of the second code cell. And as small as 1.75 KB when all outputs are removed. This shows how much of its contents is unrelated to pure code! If we dont pay attention, code changes in the notebook will be lost in an ocean of binary contents.To get meaningful diffs, we use Jupytext (disclaimer: Im the author of Jupytext). Jupytext can be installed with pip or conda. Once the notebook server is restarted, a Jupytext menu appears in Jupyter:We click on Pair Notebook with Markdown, save the notebook and we obtain two representations of the notebook: world_fact.ipynb (with both input and output cells) and world_fact.md (with only the input cells).Jupytexts representation of notebooks as Markdown files is compatible with all major Markdown editors and viewers, including GitHub and VS Code. The Markdown version is for example rendered by GitHub as:As you can see, the Markdown file does not include any output. Indeed, we dont want it at this stage since we only need to share the notebook code. The Markdown file also has a very clear diff history, which makes versioning notebooks simple.The world_facts.md file is automatically updated by Jupyter when you save the notebook. And the other way round also works! If you modify world_facts.md with either a text editor, or by pulling the latest contributions from the version control system, then the changes appear in Jupyter when you refresh the notebook in the browser.In our version control system, we only need to track the Markdown file (and we even explicitly ignore all .ipynb files). Obviously, the team that will execute the notebook needs to regenerate the world_fact.ipynb document. For this they use Jupytext in the command line:$ jupytext world_facts.md --to ipynb[jupytext] Reading world_facts.md[jupytext] Writing world_facts.ipynbWe are now properly versioning the notebook. The diff history is much clearer. See for instance how the addition of the gross domestic products to our report looks like:Jupyter notebooks as Scripts?As an alternative to the Markdown representation, we could have paired the notebook to a world_facts.py script using Jupytext. You should give it a try if your notebook contains more code than text. That's often a first good step towards a complete and efficient refactoring of long notebooks: once the notebook is represented as a script, you can extract any complex code and move it to a (unit-tested) library using the refactoring tools in your IDE.JupyterLab, JupyterHub, Binder, Nteract, Colab & Cloud notebooks?Do you use JupyterLab and not Jupyter Notebook? No worries: the method above also applies in this case. You will just have to use the Jupytext extension for JupyterLab instead of the Jupytext menu. And in case you were wondering, Jupytext also work in JupyterHub and Binder.If you use other notebook editors like Nteract desktop, CoCalc, Google Colab, or another cloud notebook editor, you may not be able to use Jupytext as a plugin in the editor. In this case you can simply use Jupytext in the command line. Close your notebook and inject the pairing information into world_facts.ipynb with$ jupytext --set-formats ipynb,md world_facts.ipynband then keep the two representations synchronised with$ jupytext --sync world_facts.ipynbNotebook parametersPapermill is the reference library for executing notebooks with parameters.Papermill needs to know which cell contains the notebook parameters. This is simply done by adding a parameter tag in that cell with the cell toolbar in Jupyter Notebook:In JupyterLab you can use the celltags extension.And if you prefer you can also directly edit world_facts.md and add the tag there:```python tags=[\"parameters\"]year = 2000```Automated executionWe now have all the information required to execute the notebook on a production server.Production environmentIn order to execute the notebook, we need to know in which environment it should run. As we are working with a Python notebook in this example, we list its dependencies in a requirements.txt file, as is standard for Python projects.For simplicity, we also include the notebook tools in the same environment, i.e. add jupytext and papermill to the same requirements.txt file. Strictly speaking, these tools could be installed and executed in another Python environment.The corresponding Python environment is created with either$ conda create -n run_notebook --file requirements.txt -yor$ pip install -r requirements.txt(if in a virtual environment).Please note that the requirements.txt file is just one way of specifying an execution environment. The Reproducible Execution Environment Specification by the Binder team is one of the most complete references on the subject.Continuous IntegrationIt is a good practice to test each new contribution to either the notebook or its requirements. For this you can use for example Travis CI, a continuous integration solution. You will need only these two commands:pip install -r requirements.txt to install the dependenciesjupytext world_facts.md --set-kernel - --execute to test the execution of the notebook in the current Python environment.You can find a concrete example in our .travis.yml file.We are already executing the notebook automatically, arent we? Travis will tell us if a regression is introduced in the project What progress! But were not 100% done yet, as we promised to execute the notebook with parameters.Using the right kernelJupyter notebooks are associated with a kernel (i.e. a pointer to a local Python environment), but that kernel might not be available on your production machine. In this case, we simply update the notebook kernel so as to point to the environment that we have just created:$ jupytext world_facts.ipynb --set-kernel -Note that the minus sign in --set-kernel - above represents the current Python environment. In our example this yields:[jupytext] Reading world_facts.ipynb[jupytext] Updating notebook metadata with '{\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}}' [jupytext] Writing world_facts.ipynb (destination file replaced)In case you want to use another kernel just pass the kernel name to the --set-kernel option (you can get the list of all available kernels with jupyter kernelspec list and/or declare a new kernel with python -m ipykernel install --name kernel_name --user).Executing the notebook with parametersWe are now ready to use Papermill for executing the notebook.$ papermill world_facts.ipynb world_facts_2017.ipynb -p year 2017Input Notebook: world_facts.ipynb Output Notebook: world_facts_2017.ipynb 100%|| 8/8 [00:04<00:00, 1.41it/s]Were done! The notebook has been executed and the file world_facts_2017.ipynb contains the outputs.Publishing the NotebookIts time to deliver the notebook that was just executed. Maybe you want it in your mailbox? Or maybe you prefer to get a URL where you can see the result? We cover a few ways of doing that.GitHub can display Jupyter notebooks. This is a convenient solution, as you can easily choose who can access repositories. This works well as long as you dont include any interactive JavaScript plots or widgets in the notebook (the JavaScript parts are ignored by GitHub). In the case of our notebook, the interactive plots do not appear on GitHub, so we need another approach.Another option is to use the Jupyter Notebook Viewer. The nbviewer service can render any notebook which is publicly available on GitHub. Our notebook is thus rendered correctly there. If your notebook is not public, you can choose to install nbviewer locally.Alternatively, you can convert the executed notebook to HTML, and publish it on GitHub pages, or on your own HTML server, or send it over email. Converting the notebook to HTML is easily done with$ jupyter nbconvert world_facts_2017.ipynb --to html[NbConvertApp] Converting notebook world_facts_2017.ipynb to html [NbConvertApp] Writing 3361863 bytes to world_facts_2017.htmlThe resulting HTML file includes the code cells as below:But maybe you dont want to see the input cells in the HTML? You just need to add --no-input:$ jupyter nbconvert --to html --no-input world_facts_2017.ipynb --output world_facts_2017_report.htmlAnd youll get a cleaner report:Sending the standalone HTML file as an attachment in an email is an easy exercise. Embedding the report in the body of the email is also possible (but interactive plots wont work).Finally, if you are looking for a polished report and have some knowledge of LaTeX, you can give the PDF export option of Jupyters nbconvert command a try.Using pipesAn alternative to using named files would be to use pipes. jupytext, nbconvert and papermill all support them. A one-liner substitute for the previous commands is:$ cat world_facts.md \\ | jupytext --from md --to ipynb --set-kernel - \\ | papermill -p year 2017 \\ | jupyter nbconvert --stdin --output world_facts_2017_report.htmlConclusionYou should now be able to set up a full pipeline for generating reports in production, based on Jupyter notebooks. We have seen how to:version control a notebook with Jupytextshare a notebook and its dependencies between various userstest a notebook with continuous integrationexecute a notebook with parameters using Papermilland finally, how to publish the notebook (on GitHub or nbviewer), or render it as a static HTML page.The technology used in this example is fully based on the Jupyter Project, which is the de facto standard for Data Science. The tools used here are all open source and work well with any continuous integration framework.You have everything you need to schedule and deliver fine-tuned, code-free reports!EpilogueThe tools used here are written in Python. But they are language agnostic. Thanks to the Jupyter framework, they actually apply to any of the 40+ programming language for which a Jupyter kernel exists.Now, imagine that you have authored a document containing a few Bash command lines, just like this blog post. Install Jupytext and the bash kernel, and the blog post becomes this interactive Jupyter notebook!Going further, shouldnt we make sure that every instruction in our post actually works? We do that via our continuous integration spoiler alert: thats as simple as jupytext --execute README.md!AcknowledgmentsMarc would like to thank Eric Lebigot and Florent Zara for their contributions to this article, and to CFM for supporting this work through their Open-Source Program.About the authorThis article was written by Marc Wouts. Marc joined the research team of CFM in 2012 and has worked on a range of research projects, from optimal trading to portfolio construction.Marc has always been interested in finding efficient workflows for doing collaborative research involving data and code. In 2015 he authored an internal tool for publishing Jupyter and R Markdown notebooks on Atlassians Confluence wiki, providing a first solution for collaborating on notebooks. In 2018, he authored Jupytext, an open-source program that facilitates the version control of Jupyter notebooks. Marc is also interested in data visualisation, and coordinates a working group on this subject at CFM.Marc obtained a PhD in Probability Theory from the Paris Diderot University in 2007.DisclaimerAll views included in this document constitute judgments of its author(s) and do not necessarily reflect the views of Capital Fund Management or any of its affiliates. The information provided in this document is general information only, does not constitute investment or other advice, and is subject to change without notice. ",
          "Awesome article - I'm wondering, for \"Publishing the Notebook\" part of the workflow, have you ever seen Kyso (<a href=\"https://kyso.io\" rel=\"nofollow\">https://kyso.io</a>) - disclaimer, I'm a founder. We started Kyso to make it easier to communicate insights gained from analysis to non-technical people by converting data science tools (e.g. Jupyter Notebooks) into conversational tools in the form of blog posts. You can make public posts or have an internal \"data blog\" for your team, where you push your work to Github and it is reflected on Kyso. Would love to hear your thoughts on how it could fit into existing workflows.",
          "I don't think Jupyter notebooks should be used for automated jobs. They're great for exploratory stuff but once things are getting fleshed out and cleaned up, one should move to proper python files that can be unit tested and versioned without having to go to crazy lengths..."
        ],
        "story_type": ["Normal"],
        "url": "https://medium.com/capital-fund-management/automated-reports-with-jupyter-notebooks-using-jupytext-and-papermill-619e60c37330",
        "comments.comment_id": [21071306, 21072228],
        "comments.comment_author": ["KyleOS", "brummm"],
        "comments.comment_descendants": [0, 1],
        "comments.comment_time": [
          "2019-09-25T14:49:46Z",
          "2019-09-25T16:21:21Z"
        ],
        "comments.comment_text": [
          "Awesome article - I'm wondering, for \"Publishing the Notebook\" part of the workflow, have you ever seen Kyso (<a href=\"https://kyso.io\" rel=\"nofollow\">https://kyso.io</a>) - disclaimer, I'm a founder. We started Kyso to make it easier to communicate insights gained from analysis to non-technical people by converting data science tools (e.g. Jupyter Notebooks) into conversational tools in the form of blog posts. You can make public posts or have an internal \"data blog\" for your team, where you push your work to Github and it is reflected on Kyso. Would love to hear your thoughts on how it could fit into existing workflows.",
          "I don't think Jupyter notebooks should be used for automated jobs. They're great for exploratory stuff but once things are getting fleshed out and cleaned up, one should move to proper python files that can be unit tested and versioned without having to go to crazy lengths..."
        ],
        "id": "8208729c-9f7b-4230-9054-99c21d4b4c62",
        "url_text": "Jupyter notebooks are one of the best available tools for running code interactively and writing a narrative with data and plots. What is less known is that they can be conveniently versioned and run automatically.Do you have a Jupyter notebook with plots and figures that you regularly run manually? Wouldnt it be nice to use the same notebook and instead have an automated reporting system, launched from a script? What if this script could even pass some parameters to the notebook it runs?This post explains in a few steps how this can be done concretely, including within a production environment.Example notebookWe will show you how to version control, automatically run and publish a notebook that depends on a parameter. As an example, we will use a notebook that describes the world population and the gross domestic product for a given year. It is simple to use: just change the year variable in the first cell, re-run, and you get the plots for the chosen year. But this requires a manual intervention. It would be much more convenient if the update could be automated and produce a report for each possible value of the year parameter (more generally, a notebook can update its results based not only on some user-provided parameters, but also through a connection to a database, etc.).Version controlIn a professional environment, notebooks are designed by, say, a data scientist, but the task of running them in production may be handled by a different team. So in general people have to share notebooks. This is best done through a version control system.Jupyter notebooks are famous for the difficulty of their version control. Lets consider our notebook above, with a file size of 3 MB, much of it being contributed by the embedded Plotly library. The notebook is less than 80 KB if we remove the output of the second code cell. And as small as 1.75 KB when all outputs are removed. This shows how much of its contents is unrelated to pure code! If we dont pay attention, code changes in the notebook will be lost in an ocean of binary contents.To get meaningful diffs, we use Jupytext (disclaimer: Im the author of Jupytext). Jupytext can be installed with pip or conda. Once the notebook server is restarted, a Jupytext menu appears in Jupyter:We click on Pair Notebook with Markdown, save the notebook and we obtain two representations of the notebook: world_fact.ipynb (with both input and output cells) and world_fact.md (with only the input cells).Jupytexts representation of notebooks as Markdown files is compatible with all major Markdown editors and viewers, including GitHub and VS Code. The Markdown version is for example rendered by GitHub as:As you can see, the Markdown file does not include any output. Indeed, we dont want it at this stage since we only need to share the notebook code. The Markdown file also has a very clear diff history, which makes versioning notebooks simple.The world_facts.md file is automatically updated by Jupyter when you save the notebook. And the other way round also works! If you modify world_facts.md with either a text editor, or by pulling the latest contributions from the version control system, then the changes appear in Jupyter when you refresh the notebook in the browser.In our version control system, we only need to track the Markdown file (and we even explicitly ignore all .ipynb files). Obviously, the team that will execute the notebook needs to regenerate the world_fact.ipynb document. For this they use Jupytext in the command line:$ jupytext world_facts.md --to ipynb[jupytext] Reading world_facts.md[jupytext] Writing world_facts.ipynbWe are now properly versioning the notebook. The diff history is much clearer. See for instance how the addition of the gross domestic products to our report looks like:Jupyter notebooks as Scripts?As an alternative to the Markdown representation, we could have paired the notebook to a world_facts.py script using Jupytext. You should give it a try if your notebook contains more code than text. That's often a first good step towards a complete and efficient refactoring of long notebooks: once the notebook is represented as a script, you can extract any complex code and move it to a (unit-tested) library using the refactoring tools in your IDE.JupyterLab, JupyterHub, Binder, Nteract, Colab & Cloud notebooks?Do you use JupyterLab and not Jupyter Notebook? No worries: the method above also applies in this case. You will just have to use the Jupytext extension for JupyterLab instead of the Jupytext menu. And in case you were wondering, Jupytext also work in JupyterHub and Binder.If you use other notebook editors like Nteract desktop, CoCalc, Google Colab, or another cloud notebook editor, you may not be able to use Jupytext as a plugin in the editor. In this case you can simply use Jupytext in the command line. Close your notebook and inject the pairing information into world_facts.ipynb with$ jupytext --set-formats ipynb,md world_facts.ipynband then keep the two representations synchronised with$ jupytext --sync world_facts.ipynbNotebook parametersPapermill is the reference library for executing notebooks with parameters.Papermill needs to know which cell contains the notebook parameters. This is simply done by adding a parameter tag in that cell with the cell toolbar in Jupyter Notebook:In JupyterLab you can use the celltags extension.And if you prefer you can also directly edit world_facts.md and add the tag there:```python tags=[\"parameters\"]year = 2000```Automated executionWe now have all the information required to execute the notebook on a production server.Production environmentIn order to execute the notebook, we need to know in which environment it should run. As we are working with a Python notebook in this example, we list its dependencies in a requirements.txt file, as is standard for Python projects.For simplicity, we also include the notebook tools in the same environment, i.e. add jupytext and papermill to the same requirements.txt file. Strictly speaking, these tools could be installed and executed in another Python environment.The corresponding Python environment is created with either$ conda create -n run_notebook --file requirements.txt -yor$ pip install -r requirements.txt(if in a virtual environment).Please note that the requirements.txt file is just one way of specifying an execution environment. The Reproducible Execution Environment Specification by the Binder team is one of the most complete references on the subject.Continuous IntegrationIt is a good practice to test each new contribution to either the notebook or its requirements. For this you can use for example Travis CI, a continuous integration solution. You will need only these two commands:pip install -r requirements.txt to install the dependenciesjupytext world_facts.md --set-kernel - --execute to test the execution of the notebook in the current Python environment.You can find a concrete example in our .travis.yml file.We are already executing the notebook automatically, arent we? Travis will tell us if a regression is introduced in the project What progress! But were not 100% done yet, as we promised to execute the notebook with parameters.Using the right kernelJupyter notebooks are associated with a kernel (i.e. a pointer to a local Python environment), but that kernel might not be available on your production machine. In this case, we simply update the notebook kernel so as to point to the environment that we have just created:$ jupytext world_facts.ipynb --set-kernel -Note that the minus sign in --set-kernel - above represents the current Python environment. In our example this yields:[jupytext] Reading world_facts.ipynb[jupytext] Updating notebook metadata with '{\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}}' [jupytext] Writing world_facts.ipynb (destination file replaced)In case you want to use another kernel just pass the kernel name to the --set-kernel option (you can get the list of all available kernels with jupyter kernelspec list and/or declare a new kernel with python -m ipykernel install --name kernel_name --user).Executing the notebook with parametersWe are now ready to use Papermill for executing the notebook.$ papermill world_facts.ipynb world_facts_2017.ipynb -p year 2017Input Notebook: world_facts.ipynb Output Notebook: world_facts_2017.ipynb 100%|| 8/8 [00:04<00:00, 1.41it/s]Were done! The notebook has been executed and the file world_facts_2017.ipynb contains the outputs.Publishing the NotebookIts time to deliver the notebook that was just executed. Maybe you want it in your mailbox? Or maybe you prefer to get a URL where you can see the result? We cover a few ways of doing that.GitHub can display Jupyter notebooks. This is a convenient solution, as you can easily choose who can access repositories. This works well as long as you dont include any interactive JavaScript plots or widgets in the notebook (the JavaScript parts are ignored by GitHub). In the case of our notebook, the interactive plots do not appear on GitHub, so we need another approach.Another option is to use the Jupyter Notebook Viewer. The nbviewer service can render any notebook which is publicly available on GitHub. Our notebook is thus rendered correctly there. If your notebook is not public, you can choose to install nbviewer locally.Alternatively, you can convert the executed notebook to HTML, and publish it on GitHub pages, or on your own HTML server, or send it over email. Converting the notebook to HTML is easily done with$ jupyter nbconvert world_facts_2017.ipynb --to html[NbConvertApp] Converting notebook world_facts_2017.ipynb to html [NbConvertApp] Writing 3361863 bytes to world_facts_2017.htmlThe resulting HTML file includes the code cells as below:But maybe you dont want to see the input cells in the HTML? You just need to add --no-input:$ jupyter nbconvert --to html --no-input world_facts_2017.ipynb --output world_facts_2017_report.htmlAnd youll get a cleaner report:Sending the standalone HTML file as an attachment in an email is an easy exercise. Embedding the report in the body of the email is also possible (but interactive plots wont work).Finally, if you are looking for a polished report and have some knowledge of LaTeX, you can give the PDF export option of Jupyters nbconvert command a try.Using pipesAn alternative to using named files would be to use pipes. jupytext, nbconvert and papermill all support them. A one-liner substitute for the previous commands is:$ cat world_facts.md \\ | jupytext --from md --to ipynb --set-kernel - \\ | papermill -p year 2017 \\ | jupyter nbconvert --stdin --output world_facts_2017_report.htmlConclusionYou should now be able to set up a full pipeline for generating reports in production, based on Jupyter notebooks. We have seen how to:version control a notebook with Jupytextshare a notebook and its dependencies between various userstest a notebook with continuous integrationexecute a notebook with parameters using Papermilland finally, how to publish the notebook (on GitHub or nbviewer), or render it as a static HTML page.The technology used in this example is fully based on the Jupyter Project, which is the de facto standard for Data Science. The tools used here are all open source and work well with any continuous integration framework.You have everything you need to schedule and deliver fine-tuned, code-free reports!EpilogueThe tools used here are written in Python. But they are language agnostic. Thanks to the Jupyter framework, they actually apply to any of the 40+ programming language for which a Jupyter kernel exists.Now, imagine that you have authored a document containing a few Bash command lines, just like this blog post. Install Jupytext and the bash kernel, and the blog post becomes this interactive Jupyter notebook!Going further, shouldnt we make sure that every instruction in our post actually works? We do that via our continuous integration spoiler alert: thats as simple as jupytext --execute README.md!AcknowledgmentsMarc would like to thank Eric Lebigot and Florent Zara for their contributions to this article, and to CFM for supporting this work through their Open-Source Program.About the authorThis article was written by Marc Wouts. Marc joined the research team of CFM in 2012 and has worked on a range of research projects, from optimal trading to portfolio construction.Marc has always been interested in finding efficient workflows for doing collaborative research involving data and code. In 2015 he authored an internal tool for publishing Jupyter and R Markdown notebooks on Atlassians Confluence wiki, providing a first solution for collaborating on notebooks. In 2018, he authored Jupytext, an open-source program that facilitates the version control of Jupyter notebooks. Marc is also interested in data visualisation, and coordinates a working group on this subject at CFM.Marc obtained a PhD in Probability Theory from the Paris Diderot University in 2007.DisclaimerAll views included in this document constitute judgments of its author(s) and do not necessarily reflect the views of Capital Fund Management or any of its affiliates. The information provided in this document is general information only, does not constitute investment or other advice, and is subject to change without notice. ",
        "_version_": 1718527428058415104
      },
      {
        "story_id": [19669153],
        "story_author": ["themlaiguy"],
        "story_descendants": [5],
        "story_score": [7],
        "story_time": ["2019-04-15T21:37:08Z"],
        "story_title": "Why is version control in Jupyter notebooks so hard?",
        "search": [
          "Why is version control in Jupyter notebooks so hard?",
          "Are there any tools that help with version control on notebooks?",
          "I've been clearing my output using nbconvert before putting the notebook into version control. I have a precommit hook and a check in CI. This works for my use case but I can understand needing to preserve output.<p>jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace my_notebook_name.ipynb",
          "You bet. I built ReviewNB[1] specifically for Jupyter Notebook code reviews.<p>There's also,<p>- nbstripout[2] for stripping outputs automatically before every commit<p>- nbdime[3] for diff'ing notebooks locally<p>- jupytext[4] for converting notebooks to markdown and vice-a-versa<p>[1] <a href=\"https://www.reviewnb.com/\" rel=\"nofollow\">https://www.reviewnb.com/</a><p>[2] <a href=\"https://github.com/kynan/nbstripout\" rel=\"nofollow\">https://github.com/kynan/nbstripout</a><p>[3] <a href=\"https://github.com/jupyter/nbdime\" rel=\"nofollow\">https://github.com/jupyter/nbdime</a><p>[4] <a href=\"https://github.com/mwouts/jupytext\" rel=\"nofollow\">https://github.com/mwouts/jupytext</a>"
        ],
        "story_text": "Are there any tools that help with version control on notebooks?",
        "story_type": ["AskHN"],
        "comments.comment_id": [19670482, 19674241],
        "comments.comment_author": ["snilzzor", "amirathi"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-04-16T01:47:18Z",
          "2019-04-16T15:04:38Z"
        ],
        "comments.comment_text": [
          "I've been clearing my output using nbconvert before putting the notebook into version control. I have a precommit hook and a check in CI. This works for my use case but I can understand needing to preserve output.<p>jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace my_notebook_name.ipynb",
          "You bet. I built ReviewNB[1] specifically for Jupyter Notebook code reviews.<p>There's also,<p>- nbstripout[2] for stripping outputs automatically before every commit<p>- nbdime[3] for diff'ing notebooks locally<p>- jupytext[4] for converting notebooks to markdown and vice-a-versa<p>[1] <a href=\"https://www.reviewnb.com/\" rel=\"nofollow\">https://www.reviewnb.com/</a><p>[2] <a href=\"https://github.com/kynan/nbstripout\" rel=\"nofollow\">https://github.com/kynan/nbstripout</a><p>[3] <a href=\"https://github.com/jupyter/nbdime\" rel=\"nofollow\">https://github.com/jupyter/nbdime</a><p>[4] <a href=\"https://github.com/mwouts/jupytext\" rel=\"nofollow\">https://github.com/mwouts/jupytext</a>"
        ],
        "id": "5a0080db-2e34-40ec-b716-3c8edf40990c",
        "_version_": 1718527399615791104
      },
      {
        "story_id": [20646674],
        "story_author": ["tosh"],
        "story_descendants": [68],
        "story_score": [107],
        "story_time": ["2019-08-08T17:40:55Z"],
        "story_title": "The market figured out Gitlab’s secret",
        "search": [
          "The market figured out Gitlab’s secret",
          "https://about.gitlab.com/2019/08/08/built-in-ci-cd-version-control-secret/",
          "Theres a movement in the DevOps industry and the world right now: to do more in a simple way that inspires us to innovate. GitLab started this trend in the DevOps space by simplifying the delivery of code by combining GitLab CI and GitLab version control. We didn't originally buy into the idea that this was the right way to do things, but it became our secret capability that weve doubled down on. Lets combine applications The story starts with Kamil Trzciski, now a distinguished engineer at GitLab. Soon after Kamil came to work for GitLab full time, he began talking with me and my co-founder, Dmitriy Zaporozhets, suggesting that we bring our two projects together GitLab Version Control and GitLab CI, making it into one application. Dmitriy didnt think it was a good idea. GitLab version control and CI were already perfectly integrated with single sign-on and APIs that fit like a glove. He thought that combining them would make GitLab a monolith of an application, that it would be disastrous for our code quality, and an unfortunate user experience. After time though, Dmitriy started to think it was the right idea as it would deliver a seamless experience for developers to deliver code quickly. After Dmitriy was convinced, they came to me. I also didnt think it was a good idea. At the time I believed we needed to have tools that are composable and that could integrate with other tools, in line with the Unix philosophy. Kamil convinced me to think about the efficiencies of having a single application. Well, if you dont believe that its better for a user, at least believe its more efficient for us, because we only have to release one application instead of two. Efficiency is in our values. - Kamil Trzcinski, distinguished engineer at GitLab Realizing the future of DevOps is a single application That made sense to me and I no longer stood in their way. The two projects merged and the results were beyond my expectations. The efficiencies that were so appealing to us, also made it appealing to our customers. We realized we stumbled on a big secret because nobody believed that the two combined together would be a better way of continuously delivering code to market. We doubled down on this philosophy and we started doing continuous delivery. From that day on, I saw the value of having a single application. For example, a new feature we are implementing is auto-remediation. When a vulnerability comes out, say a heart bleed, GitLab will automatically detect where in your codebase that vulnerability exists, update the dependency, and deliver it to your production environment. This level of automation would be hard to implement without being in a single application. By combining the projects we unified teams helping them realize the original intent of DevOps and that is magical to see. The market validates our secret And while we bet on this philosophy the industry is now seeing it as well. In September of 2015 we combined GitLab CI and GitLab version control to create a single application. By March of 2017, Bitbucket also realized the advantages of this architecture and released Pipelines as a built-in part of Bitbucket. In 2018, GitHub announced Actions with CI-like functionality built into a single application offering. In the last six months, JFrog acquired Shippable and Idera acquired Travis CI, showing a consolidation of the DevOps market and a focus on CI. The market is validating what we continually hear from our users and customers: that a simple, single DevOps application meets their needs better. We hope you will continue to join us in our effort to bring teams together to innovate. Everyone can contribute here at GitLab and as always, we value your feedback, thoughts, and contributions. Want to hear me talk through the origin story? Listen to the Software Engineering Daily podcast where I talk about combining GitLab CI and GitLab Version Control. The industry has caught onto @GitLabs secret. Learn more about why GitLab combined GitLab CI and GitLab version control Sid Sijbrandij Click to tweet Sign up for GitLabs twice-monthly newsletter ",
          "Every time github releases a feature this is the response. These posts are so pathetic. I’m surprised that they continue to play this angle. It’s a very polarizing way to address the community that will definitely continue to stir up us vs them mentality between GitHub and Gitlab users.",
          "For all the bashing GitLab gets, personally, I want GitLab to survive and keep competing at some level with GitHub.<p>Should GitHub dominate the market and gobble up competition, we all know how it goes for its parent company."
        ],
        "story_type": ["Normal"],
        "url": "https://about.gitlab.com/2019/08/08/built-in-ci-cd-version-control-secret/",
        "comments.comment_id": [20647790, 20648739],
        "comments.comment_author": ["whalesalad", "asadkn"],
        "comments.comment_descendants": [5, 1],
        "comments.comment_time": [
          "2019-08-08T19:32:16Z",
          "2019-08-08T21:04:04Z"
        ],
        "comments.comment_text": [
          "Every time github releases a feature this is the response. These posts are so pathetic. I’m surprised that they continue to play this angle. It’s a very polarizing way to address the community that will definitely continue to stir up us vs them mentality between GitHub and Gitlab users.",
          "For all the bashing GitLab gets, personally, I want GitLab to survive and keep competing at some level with GitHub.<p>Should GitHub dominate the market and gobble up competition, we all know how it goes for its parent company."
        ],
        "id": "6d40bf87-dfb7-48c1-917e-83af14eaca10",
        "url_text": "Theres a movement in the DevOps industry and the world right now: to do more in a simple way that inspires us to innovate. GitLab started this trend in the DevOps space by simplifying the delivery of code by combining GitLab CI and GitLab version control. We didn't originally buy into the idea that this was the right way to do things, but it became our secret capability that weve doubled down on. Lets combine applications The story starts with Kamil Trzciski, now a distinguished engineer at GitLab. Soon after Kamil came to work for GitLab full time, he began talking with me and my co-founder, Dmitriy Zaporozhets, suggesting that we bring our two projects together GitLab Version Control and GitLab CI, making it into one application. Dmitriy didnt think it was a good idea. GitLab version control and CI were already perfectly integrated with single sign-on and APIs that fit like a glove. He thought that combining them would make GitLab a monolith of an application, that it would be disastrous for our code quality, and an unfortunate user experience. After time though, Dmitriy started to think it was the right idea as it would deliver a seamless experience for developers to deliver code quickly. After Dmitriy was convinced, they came to me. I also didnt think it was a good idea. At the time I believed we needed to have tools that are composable and that could integrate with other tools, in line with the Unix philosophy. Kamil convinced me to think about the efficiencies of having a single application. Well, if you dont believe that its better for a user, at least believe its more efficient for us, because we only have to release one application instead of two. Efficiency is in our values. - Kamil Trzcinski, distinguished engineer at GitLab Realizing the future of DevOps is a single application That made sense to me and I no longer stood in their way. The two projects merged and the results were beyond my expectations. The efficiencies that were so appealing to us, also made it appealing to our customers. We realized we stumbled on a big secret because nobody believed that the two combined together would be a better way of continuously delivering code to market. We doubled down on this philosophy and we started doing continuous delivery. From that day on, I saw the value of having a single application. For example, a new feature we are implementing is auto-remediation. When a vulnerability comes out, say a heart bleed, GitLab will automatically detect where in your codebase that vulnerability exists, update the dependency, and deliver it to your production environment. This level of automation would be hard to implement without being in a single application. By combining the projects we unified teams helping them realize the original intent of DevOps and that is magical to see. The market validates our secret And while we bet on this philosophy the industry is now seeing it as well. In September of 2015 we combined GitLab CI and GitLab version control to create a single application. By March of 2017, Bitbucket also realized the advantages of this architecture and released Pipelines as a built-in part of Bitbucket. In 2018, GitHub announced Actions with CI-like functionality built into a single application offering. In the last six months, JFrog acquired Shippable and Idera acquired Travis CI, showing a consolidation of the DevOps market and a focus on CI. The market is validating what we continually hear from our users and customers: that a simple, single DevOps application meets their needs better. We hope you will continue to join us in our effort to bring teams together to innovate. Everyone can contribute here at GitLab and as always, we value your feedback, thoughts, and contributions. Want to hear me talk through the origin story? Listen to the Software Engineering Daily podcast where I talk about combining GitLab CI and GitLab Version Control. The industry has caught onto @GitLabs secret. Learn more about why GitLab combined GitLab CI and GitLab version control Sid Sijbrandij Click to tweet Sign up for GitLabs twice-monthly newsletter ",
        "_version_": 1718527418757545985
      },
      {
        "story_id": [19738327],
        "story_author": ["ariehkovler"],
        "story_descendants": [88],
        "story_score": [75],
        "story_time": ["2019-04-24T13:55:08Z"],
        "story_title": "We need a new generation of source control",
        "search": [
          "We need a new generation of source control",
          "https://www.rookout.com/cant-git-no-satisfaction-why-we-need-a-new-gen-source-control/",
          "By: | January 5, 2019Liran is the Co-Founder and CTO of Rookout. Hes an Observability and Instrumentation expert with a deep understanding of Java, Python, Node, and C++. Liran has broad experience in cybersecurity and compliance from his past roles. When not coding, you can find Liran hosting his podcast, speaking at conferences, writing about his tech adventures, and trying out the local cuisine when traveling.Remember the good old days of enterprise software? When everything had to be installed on-premises? To install an application, youd have to set up a big, vertically scalable server. You would then have to execute a single process written in C/C++, Java or .NET. Well, as you know, those days are long gone.Everything has changed with the transition to the cloud and SaaS. Today, instead of comprising a single vertically scalable process, most applications comprise multiple horizontally scalable processes. This model was first pioneered by Googles borg and by Netflix on EC2. Nowadays, though, you no longer have to be a large enterprise to access microservice infrastructures. Kubernetes and serverless have made microservices viable and accessible to even small startups and lone coders.Lets Git down to businessSo where does Git fit into the picture? Git is an excellent match for single-process applications, but it starts to fail when it comes to multi-process applications. This is precisely what gave birth to the endless mono-repo vs. multi-repo flame-wars.Each side of this debate classifies the other as zealous extremists (as only developers can!), but both of them miss the crux of the matter: Git and its accompanying ecosystem are not yet fit for the task of developing modern cloud-native applications.Shots fired: multi-repos suckBefore we dive in, lets answer this: whats great about Git? Its the almighty atomic commit, the groundbreaking (at the time) branching capabilities, and the ever-useful blame. Well, these beloved features all but disappear in a multi-repo setup. Working in multiple repositories comes with significant drawbacks, which is why its not at all surprising that some of the biggest names in the tech world, including Google and Facebook, have gone down the mono-repo path at a huge investment of time and resources.Dependency management in a multi-repo setup is a nightmare. Instead of having everything in a single repository, you end up with repositories pointing to each other using two git features (git submodules and git subtree) and language-specific dependency management such as npm or Maven. The very existence of the many different methods to manage multi-repos is in itself proof that none of these tools are enough on their own. Gits source-of-truth is no longer a single folder on your computer but a mishmash of source providers and various artifactories.In developers everyday work, repository separation becomes an artificial barrier that impacts technological decisions. This creates a Conways Law effect, making early design decisions about component boundaries very hard to change. It also makes large scale refactorings a much trickier business.However, the biggest failure of the multi-repo is cultural. Instead of having all your source code readily available to all developers, they have to jump hurdles to figure out which repo they need and then clone it. These seemingly-small obstacles often become high fences: developers stop reading and updating code in components and repositories that arent directly in their responsibility.With all these engineering, operations and cultural barriers, why doesnt everyone go the mono-repo route?Take no prisoners: mono-repos suck tooOnce youve packed everything into a single repository, figuring out the connections within the repository becomes a challenge. For humans, that can chip away at the original architecture, breaking away useful abstractions and jumbling everything together.For machines, this lack of separation within the repo is even worse. When you push a code change to a repo, automated processes kick in. CI systems build and test the code, and then CD systems deploy it. Sometimes its to a test or staging environment, and sometimes directly to production.There are certain components you will need to build and deploy hundreds of times a day. At the same time, there are other more delicate and mission-critical components. These require human supervision and extra precaution. The problem with mono-repository is that it mixes all of these components into one. More surprising is the fact that todays vast Git CI ecosystem, with its impressive offerings in both the hosted and the SaaS space, doesnt even try to tackle the issue. In fact, not only will Git CI tools rebuild and redeploy your entire repo, they are often built explicitly for multi-repo projects.Another issue is large repository sizes. Git doesnt handle large repos gracefully. You can easily end up with repo sizes that dont fit in your hard-drive, or clone time that ends up in the hours. For big projects, this requires careful management and pruning of commit history. It is also essential to avoid committing dependencies, auto-generated files and other large files which may be necessary for specific scenarios.Is there still hope for multi-repos?There are new tools that seek to bring some of the benefits of mono-repos to multi-repos. These tools try to set up a configuration that would unite multiple repos under a single umbrella/abstraction layer, thus making managing multiple-repositories easier for example, TwoSigmas Git-meta, mateodelnortes meta, gitslave ,and a bunch of others.These tools bring back a bit of sanity into the complexities of managing multi-repos, reducing some of the toil and error-prone manual operations. But none of them truly give back the control and power of a single Git repo.You cant have your cake and Git it tooThe downsides of multi-repos are real. You cant deny the value of a (truly) single source of truth, (truly) atomic commits, and a (truly) single place to develop and collaborate. On the other hand, none of the downsides of mono-repos are inherent. All of them are related to the current implementation of the Git source control tool itself and its accompanying eco-system, especially CI/CD tools.Its time for a new generation of source control that wasnt purely designed for open-source projects, C and the Linux kernel. A source control designed for delivering modern applications in a polyglot cloud-native world. One that embraces code dependencies and helps the engineering team define and manage them, rather than scaring them away. A source control that treats CI, CD, and releases as first-class citizens, rather than relying on the very useful add-ons provided by GitHub and its community. ",
          "Are we mistaking a dependency control problem as a revision control problem?<p>In a previous life, before microservices, CI/CD etc. existed, we did just fine with 20-30 CVS repositories, each representing a separate component (a running process) in a very large distributed system.<p>The only difference was that we did not have to marshal a large number of 3rd party dependencies that were constantly undergoing version changes. We basically relied on C++, the standard template library and a tightly version controlled set of internal libraries with a single stable version shared across the entire org. The whole system would have been between 750,000 - 1,000,000 lines of code (libraries included).<p>I'm not saying that that's the right approach. But it's mind boggling for me that we can't solve this problem easily anymore.",
          "The source control system is not the piece of the equation that matters to most people.  The build system is the important part.  That's what prevents you from rebuilding the repository when you only change one Kubernetes config file, or what causes 100 docker images to be built because you changed a file in libc.<p>I think the tooling around this is fairly limited right now.  I feel that most people are hoping docker caches stuff intelligently, which it doesn't.  People should probably be using Bazel, but language support is hit-or-miss and it's very complicated.  (It's aggravated by the fact that every language now considers itself responsible for building its own code.  go \"just works\", which is great, but it's hard to translate that local caching to something that can be spread among multiple build workers.  Bazel attempts to make all that work, but it basically has to start from scratch, which is unfortunate.  It also means that you can't just start using some crazy new language unless you want to now support it in the build system.  We all hate Makefiles, but the whole \"foo.c becomes foo.o\" model was much more straightforward than what languages do today.)"
        ],
        "story_type": ["Normal"],
        "url": "https://www.rookout.com/cant-git-no-satisfaction-why-we-need-a-new-gen-source-control/",
        "comments.comment_id": [19739585, 19739587],
        "comments.comment_author": ["hliyan", "jrockway"],
        "comments.comment_descendants": [5, 0],
        "comments.comment_time": [
          "2019-04-24T15:48:25Z",
          "2019-04-24T15:48:36Z"
        ],
        "comments.comment_text": [
          "Are we mistaking a dependency control problem as a revision control problem?<p>In a previous life, before microservices, CI/CD etc. existed, we did just fine with 20-30 CVS repositories, each representing a separate component (a running process) in a very large distributed system.<p>The only difference was that we did not have to marshal a large number of 3rd party dependencies that were constantly undergoing version changes. We basically relied on C++, the standard template library and a tightly version controlled set of internal libraries with a single stable version shared across the entire org. The whole system would have been between 750,000 - 1,000,000 lines of code (libraries included).<p>I'm not saying that that's the right approach. But it's mind boggling for me that we can't solve this problem easily anymore.",
          "The source control system is not the piece of the equation that matters to most people.  The build system is the important part.  That's what prevents you from rebuilding the repository when you only change one Kubernetes config file, or what causes 100 docker images to be built because you changed a file in libc.<p>I think the tooling around this is fairly limited right now.  I feel that most people are hoping docker caches stuff intelligently, which it doesn't.  People should probably be using Bazel, but language support is hit-or-miss and it's very complicated.  (It's aggravated by the fact that every language now considers itself responsible for building its own code.  go \"just works\", which is great, but it's hard to translate that local caching to something that can be spread among multiple build workers.  Bazel attempts to make all that work, but it basically has to start from scratch, which is unfortunate.  It also means that you can't just start using some crazy new language unless you want to now support it in the build system.  We all hate Makefiles, but the whole \"foo.c becomes foo.o\" model was much more straightforward than what languages do today.)"
        ],
        "id": "14363763-823a-4285-b58a-8230ae289191",
        "url_text": "By: | January 5, 2019Liran is the Co-Founder and CTO of Rookout. Hes an Observability and Instrumentation expert with a deep understanding of Java, Python, Node, and C++. Liran has broad experience in cybersecurity and compliance from his past roles. When not coding, you can find Liran hosting his podcast, speaking at conferences, writing about his tech adventures, and trying out the local cuisine when traveling.Remember the good old days of enterprise software? When everything had to be installed on-premises? To install an application, youd have to set up a big, vertically scalable server. You would then have to execute a single process written in C/C++, Java or .NET. Well, as you know, those days are long gone.Everything has changed with the transition to the cloud and SaaS. Today, instead of comprising a single vertically scalable process, most applications comprise multiple horizontally scalable processes. This model was first pioneered by Googles borg and by Netflix on EC2. Nowadays, though, you no longer have to be a large enterprise to access microservice infrastructures. Kubernetes and serverless have made microservices viable and accessible to even small startups and lone coders.Lets Git down to businessSo where does Git fit into the picture? Git is an excellent match for single-process applications, but it starts to fail when it comes to multi-process applications. This is precisely what gave birth to the endless mono-repo vs. multi-repo flame-wars.Each side of this debate classifies the other as zealous extremists (as only developers can!), but both of them miss the crux of the matter: Git and its accompanying ecosystem are not yet fit for the task of developing modern cloud-native applications.Shots fired: multi-repos suckBefore we dive in, lets answer this: whats great about Git? Its the almighty atomic commit, the groundbreaking (at the time) branching capabilities, and the ever-useful blame. Well, these beloved features all but disappear in a multi-repo setup. Working in multiple repositories comes with significant drawbacks, which is why its not at all surprising that some of the biggest names in the tech world, including Google and Facebook, have gone down the mono-repo path at a huge investment of time and resources.Dependency management in a multi-repo setup is a nightmare. Instead of having everything in a single repository, you end up with repositories pointing to each other using two git features (git submodules and git subtree) and language-specific dependency management such as npm or Maven. The very existence of the many different methods to manage multi-repos is in itself proof that none of these tools are enough on their own. Gits source-of-truth is no longer a single folder on your computer but a mishmash of source providers and various artifactories.In developers everyday work, repository separation becomes an artificial barrier that impacts technological decisions. This creates a Conways Law effect, making early design decisions about component boundaries very hard to change. It also makes large scale refactorings a much trickier business.However, the biggest failure of the multi-repo is cultural. Instead of having all your source code readily available to all developers, they have to jump hurdles to figure out which repo they need and then clone it. These seemingly-small obstacles often become high fences: developers stop reading and updating code in components and repositories that arent directly in their responsibility.With all these engineering, operations and cultural barriers, why doesnt everyone go the mono-repo route?Take no prisoners: mono-repos suck tooOnce youve packed everything into a single repository, figuring out the connections within the repository becomes a challenge. For humans, that can chip away at the original architecture, breaking away useful abstractions and jumbling everything together.For machines, this lack of separation within the repo is even worse. When you push a code change to a repo, automated processes kick in. CI systems build and test the code, and then CD systems deploy it. Sometimes its to a test or staging environment, and sometimes directly to production.There are certain components you will need to build and deploy hundreds of times a day. At the same time, there are other more delicate and mission-critical components. These require human supervision and extra precaution. The problem with mono-repository is that it mixes all of these components into one. More surprising is the fact that todays vast Git CI ecosystem, with its impressive offerings in both the hosted and the SaaS space, doesnt even try to tackle the issue. In fact, not only will Git CI tools rebuild and redeploy your entire repo, they are often built explicitly for multi-repo projects.Another issue is large repository sizes. Git doesnt handle large repos gracefully. You can easily end up with repo sizes that dont fit in your hard-drive, or clone time that ends up in the hours. For big projects, this requires careful management and pruning of commit history. It is also essential to avoid committing dependencies, auto-generated files and other large files which may be necessary for specific scenarios.Is there still hope for multi-repos?There are new tools that seek to bring some of the benefits of mono-repos to multi-repos. These tools try to set up a configuration that would unite multiple repos under a single umbrella/abstraction layer, thus making managing multiple-repositories easier for example, TwoSigmas Git-meta, mateodelnortes meta, gitslave ,and a bunch of others.These tools bring back a bit of sanity into the complexities of managing multi-repos, reducing some of the toil and error-prone manual operations. But none of them truly give back the control and power of a single Git repo.You cant have your cake and Git it tooThe downsides of multi-repos are real. You cant deny the value of a (truly) single source of truth, (truly) atomic commits, and a (truly) single place to develop and collaborate. On the other hand, none of the downsides of mono-repos are inherent. All of them are related to the current implementation of the Git source control tool itself and its accompanying eco-system, especially CI/CD tools.Its time for a new generation of source control that wasnt purely designed for open-source projects, C and the Linux kernel. A source control designed for delivering modern applications in a polyglot cloud-native world. One that embraces code dependencies and helps the engineering team define and manage them, rather than scaring them away. A source control that treats CI, CD, and releases as first-class citizens, rather than relying on the very useful add-ons provided by GitHub and its community. ",
        "_version_": 1718527401276735488
      },
      {
        "story_id": [21836168],
        "story_author": ["bbrennan"],
        "story_descendants": [2],
        "story_score": [10],
        "story_time": ["2019-12-19T16:13:05Z"],
        "story_title": "Kubernetes Security Tools Abound",
        "search": [
          "Kubernetes Security Tools Abound",
          "https://searchitoperations.techtarget.com/news/252475750/New-Kubernetes-security-tools-abound-as-container-deployments-grow",
          "As Kubernetes use expands in production, enterprises have a number of IT security tools to choose from for centralized, policy-based control over container clusters. Emerging Kubernetes security tools focus security operations at higher layers of the IT stack, as the container orchestration platform grows into a production staple for mainstream enterprises. Approaches to Kubernetes security vary among these tools -- one newcomer, Octarine, piggybacks on the Envoy proxy and Istio service mesh to monitor container infrastructure for threats and enforce security policy, while an existing container security startup, NeuVector, plugs policy-as-code tools into the CI/CD pipeline. Another tool from managed Kubernetes player Fairwinds combines security and IT performance and reliability monitoring with a multi-cluster dashboard view. What they all have in common is their emergence as enterprises look for centralized points of Kubernetes security control over an entire environment, in addition to tools that operate at the individual container or application workload level. \"Looking at the container workload is great, but if you don't have a sense of everything that's running within your Kubernetes ecosystem and how it's communicating, it's very easy for rogue deployments to slip in when you have hundreds of namespaces and thousands of pods,\" said Trevor Bossert, manager of DevOps at Primer AI, a data analytics firm in San Francisco which began using Octarine's Kubernetes security software six months ago. \"This will let you know when [cluster configurations] are violating policies, like if they're public by default when they're not supposed to be.\" Octarine rides along in service mesh sidecar Octarine, based in Sunnyvale, Calif., came out of stealth last month with a service mesh-based approach to Kubernetes security. The company claims that installing its software on the Envoy service mesh proxy gives users a clearer picture of the Kubernetes orchestration layer than tools that monitor the infrastructure from privileged containers on hosts. Placing security monitoring and enforcement inside Envoy lets Octarine see whether container workloads are exposed to the internet, how secrets are exposed to container workloads and monitor east-west traffic more effectively, according to Octarine CTO and co-founder Haim Helman. Envoy is often associated with the Istio service mesh, which has its own security features, but Octarine doesn't replace those features, which include the enforcement of role-based access control and mutual TLS encryption. Instead, Octarine collects security telemetry and identifies anomalies and threats with its Octarine Runtime module, and manages Kubernetes security policy-as-code with a tool it calls Guardrails. It can feed security monitoring information into Istio's control plane if a user already has it, or run its own service mesh control plane if the user doesn't have Istio in place. There are other ways to create and enforce Kubernetes policy-as-code, among them the open source Open Policy Agent (OPA) that rose in popularity among large organizations in 2019, but midsize companies with smaller teams may find Octarine's policy-as-code features easier to use. Octarine's service-mesh-based Kubernetes security tool centralizes policy management at the network level \"Not having to craft all policies from scratch, being able to [let] Octarine observe the traffic and providing the best policy, is less time-consuming and involves less duplication of work, especially for a smaller team like ours,\" said Primer AI's Bossert. Running Octarine on Envoy offloads some of the resource requirements from the container host, and managing mTLS encryption and policy-as-code together through Istio is also convenient, he said. Larger organizations such as the U.S. Air Force will also keep an eye on Octarine as it matures, as OPA has been unwieldy to use so far, but would most like to use a Kubernetes policy as code tool that isn't tied to a particular service mesh. \"You can end up with massive lock-in if you abstract teams from the infrastructure, but then couple [security policy] tightly with a mesh again,\" said Nicolas Chaillan, chief software officer for the military branch, which has deployed Istio in production but plans to evaluate other service meshes, including Linkerd. NeuVector loops in CRDs for Kubernetes security NeuVector released a Kubernetes security policy-as-code tool that moved it up the stack last month, which deploys Kubernetes Custom Resource Definitions (CRDs) that are version-controlled and tested within a CI/CD pipeline instead of a service mesh. The company, which began as a container runtime scanning tool, also added network-based data loss prevention (DLP) features and multi-cluster management in version 3.0 in March. A lot of tools cover just one aspect of security management, and just figuring out how all the pieces fit together is a hassle. In about three to five years, I think we'll see consolidation in the market and more complete [products]. Sean McCormickVice president of engineering, Element Analytics Like Octarine, NeuVector can observe normal container behavior on a Kubernetes cluster network and define appropriate application behavior instead of requiring that users create policy from scratch. But for users interested in OPA, NeuVector's tool can import OPA-based policy-as-code data into CRDs as well. \"With an engineering team of 20 people it's hard to pull in new things like service mesh,\" said NeuVector user Sean McCormick, vice president of engineering at Element Analytics, an industrial data analytics firm in San Francisco. \"Being able to export security rules is also nice, so you don't have to spend a week learning rules in a new place.\" McCormick also plans to evaluate NeuVector's DLP features, and would like to see the vendor expand further to offer a web application firewall and application code security analysis. \"There are way too many security tools,\" he said. \"A lot of tools cover just one aspect of security management, and just figuring out how all the pieces fit together is a hassle. In about three to five years, I think we'll see consolidation in the market and more complete [products].\" NeuVector's CRD workflow for Kubernetes policy-as-code. Fairwinds tackles Kubernetes security fundamentals Another container management vendor that looks to expand its influence in the Kubernetes security realm is Fairwinds, a managed Kubernetes service provider in Boston. Fairwinds, formerly ReactiveOps, originally specialized in fully managed Kubernetes clusters, but launched Kubernetes management tools customers can use on their own beginning with the Polaris Kubernetes distro and Goldilocks resource request optimization tool in July. Last month, it added Fairwinds Insights, which displays Kubernetes security monitoring data alongside performance and reliability feedback. Fairwinds Insights also presents ranked remediation recommendations that include YAML code users can copy and paste to shore up vulnerabilities. The tool will also pull in and orchestrate third-party Kubernetes security utilities such as Aqua's kube-hunter. Fairwinds Insights is not as in-depth a tool as OPA or full-blown policy-as-code, but it could help smaller shops move from Kubernetes clusters fully managed by the vendor to self-managed environments, while maintaining security best practices. Fairwinds Insights prioritizes Kubernetes security and reliability action items and includes remediation recommendations for users. For companies such as Philadelphia-based Sidecar, a marketing and advertising software firm, Fairwinds Insights will cover the most crucial Kubernetes security management requirements at a cluster-wide level while the IT team hones its container management skills. \"A tool at the network infrastructure level gets past the most immediate security concerns, such as locking down public access to clusters and configuring AWS load-balancers,\" said Dominic O'Kane, manager of cloud engineering at Sidecar, which also uses Fairwinds' managed services. \"Then we can take on more fine-grained tools that look at individual applications and containers.\" Dig Deeper on Managing Virtual Containers CNCF policy-as-code project bridges Kubernetes security gaps By: BethPariseau Sysdig deal reflects infrastructure-as-code security buzz By: BethPariseau Kubernetes security automation saves SecOps sanity By: BethPariseau What is container management and why is it important? By: EmilyMell ",
          "As a (perhaps overly cynical) outside observer it feels that \"kubernetes X abound\" for all X.  There's just such a complex ecosystem of tooling evolving here.",
          "I find it interesting that companies can convince auditors that security sidecars that add auth and encryption actually meet compliance requirements... it's a nice architecture but I'd argue it renders the environment non-compliant."
        ],
        "story_type": ["Normal"],
        "url": "https://searchitoperations.techtarget.com/news/252475750/New-Kubernetes-security-tools-abound-as-container-deployments-grow",
        "comments.comment_id": [21836565, 21836803],
        "comments.comment_author": ["seanhunter", "kerng"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-12-19T16:49:03Z",
          "2019-12-19T17:09:49Z"
        ],
        "comments.comment_text": [
          "As a (perhaps overly cynical) outside observer it feels that \"kubernetes X abound\" for all X.  There's just such a complex ecosystem of tooling evolving here.",
          "I find it interesting that companies can convince auditors that security sidecars that add auth and encryption actually meet compliance requirements... it's a nice architecture but I'd argue it renders the environment non-compliant."
        ],
        "id": "18f5b57c-9025-4ed3-a9f7-b1696c7e9da5",
        "url_text": "As Kubernetes use expands in production, enterprises have a number of IT security tools to choose from for centralized, policy-based control over container clusters. Emerging Kubernetes security tools focus security operations at higher layers of the IT stack, as the container orchestration platform grows into a production staple for mainstream enterprises. Approaches to Kubernetes security vary among these tools -- one newcomer, Octarine, piggybacks on the Envoy proxy and Istio service mesh to monitor container infrastructure for threats and enforce security policy, while an existing container security startup, NeuVector, plugs policy-as-code tools into the CI/CD pipeline. Another tool from managed Kubernetes player Fairwinds combines security and IT performance and reliability monitoring with a multi-cluster dashboard view. What they all have in common is their emergence as enterprises look for centralized points of Kubernetes security control over an entire environment, in addition to tools that operate at the individual container or application workload level. \"Looking at the container workload is great, but if you don't have a sense of everything that's running within your Kubernetes ecosystem and how it's communicating, it's very easy for rogue deployments to slip in when you have hundreds of namespaces and thousands of pods,\" said Trevor Bossert, manager of DevOps at Primer AI, a data analytics firm in San Francisco which began using Octarine's Kubernetes security software six months ago. \"This will let you know when [cluster configurations] are violating policies, like if they're public by default when they're not supposed to be.\" Octarine rides along in service mesh sidecar Octarine, based in Sunnyvale, Calif., came out of stealth last month with a service mesh-based approach to Kubernetes security. The company claims that installing its software on the Envoy service mesh proxy gives users a clearer picture of the Kubernetes orchestration layer than tools that monitor the infrastructure from privileged containers on hosts. Placing security monitoring and enforcement inside Envoy lets Octarine see whether container workloads are exposed to the internet, how secrets are exposed to container workloads and monitor east-west traffic more effectively, according to Octarine CTO and co-founder Haim Helman. Envoy is often associated with the Istio service mesh, which has its own security features, but Octarine doesn't replace those features, which include the enforcement of role-based access control and mutual TLS encryption. Instead, Octarine collects security telemetry and identifies anomalies and threats with its Octarine Runtime module, and manages Kubernetes security policy-as-code with a tool it calls Guardrails. It can feed security monitoring information into Istio's control plane if a user already has it, or run its own service mesh control plane if the user doesn't have Istio in place. There are other ways to create and enforce Kubernetes policy-as-code, among them the open source Open Policy Agent (OPA) that rose in popularity among large organizations in 2019, but midsize companies with smaller teams may find Octarine's policy-as-code features easier to use. Octarine's service-mesh-based Kubernetes security tool centralizes policy management at the network level \"Not having to craft all policies from scratch, being able to [let] Octarine observe the traffic and providing the best policy, is less time-consuming and involves less duplication of work, especially for a smaller team like ours,\" said Primer AI's Bossert. Running Octarine on Envoy offloads some of the resource requirements from the container host, and managing mTLS encryption and policy-as-code together through Istio is also convenient, he said. Larger organizations such as the U.S. Air Force will also keep an eye on Octarine as it matures, as OPA has been unwieldy to use so far, but would most like to use a Kubernetes policy as code tool that isn't tied to a particular service mesh. \"You can end up with massive lock-in if you abstract teams from the infrastructure, but then couple [security policy] tightly with a mesh again,\" said Nicolas Chaillan, chief software officer for the military branch, which has deployed Istio in production but plans to evaluate other service meshes, including Linkerd. NeuVector loops in CRDs for Kubernetes security NeuVector released a Kubernetes security policy-as-code tool that moved it up the stack last month, which deploys Kubernetes Custom Resource Definitions (CRDs) that are version-controlled and tested within a CI/CD pipeline instead of a service mesh. The company, which began as a container runtime scanning tool, also added network-based data loss prevention (DLP) features and multi-cluster management in version 3.0 in March. A lot of tools cover just one aspect of security management, and just figuring out how all the pieces fit together is a hassle. In about three to five years, I think we'll see consolidation in the market and more complete [products]. Sean McCormickVice president of engineering, Element Analytics Like Octarine, NeuVector can observe normal container behavior on a Kubernetes cluster network and define appropriate application behavior instead of requiring that users create policy from scratch. But for users interested in OPA, NeuVector's tool can import OPA-based policy-as-code data into CRDs as well. \"With an engineering team of 20 people it's hard to pull in new things like service mesh,\" said NeuVector user Sean McCormick, vice president of engineering at Element Analytics, an industrial data analytics firm in San Francisco. \"Being able to export security rules is also nice, so you don't have to spend a week learning rules in a new place.\" McCormick also plans to evaluate NeuVector's DLP features, and would like to see the vendor expand further to offer a web application firewall and application code security analysis. \"There are way too many security tools,\" he said. \"A lot of tools cover just one aspect of security management, and just figuring out how all the pieces fit together is a hassle. In about three to five years, I think we'll see consolidation in the market and more complete [products].\" NeuVector's CRD workflow for Kubernetes policy-as-code. Fairwinds tackles Kubernetes security fundamentals Another container management vendor that looks to expand its influence in the Kubernetes security realm is Fairwinds, a managed Kubernetes service provider in Boston. Fairwinds, formerly ReactiveOps, originally specialized in fully managed Kubernetes clusters, but launched Kubernetes management tools customers can use on their own beginning with the Polaris Kubernetes distro and Goldilocks resource request optimization tool in July. Last month, it added Fairwinds Insights, which displays Kubernetes security monitoring data alongside performance and reliability feedback. Fairwinds Insights also presents ranked remediation recommendations that include YAML code users can copy and paste to shore up vulnerabilities. The tool will also pull in and orchestrate third-party Kubernetes security utilities such as Aqua's kube-hunter. Fairwinds Insights is not as in-depth a tool as OPA or full-blown policy-as-code, but it could help smaller shops move from Kubernetes clusters fully managed by the vendor to self-managed environments, while maintaining security best practices. Fairwinds Insights prioritizes Kubernetes security and reliability action items and includes remediation recommendations for users. For companies such as Philadelphia-based Sidecar, a marketing and advertising software firm, Fairwinds Insights will cover the most crucial Kubernetes security management requirements at a cluster-wide level while the IT team hones its container management skills. \"A tool at the network infrastructure level gets past the most immediate security concerns, such as locking down public access to clusters and configuring AWS load-balancers,\" said Dominic O'Kane, manager of cloud engineering at Sidecar, which also uses Fairwinds' managed services. \"Then we can take on more fine-grained tools that look at individual applications and containers.\" Dig Deeper on Managing Virtual Containers CNCF policy-as-code project bridges Kubernetes security gaps By: BethPariseau Sysdig deal reflects infrastructure-as-code security buzz By: BethPariseau Kubernetes security automation saves SecOps sanity By: BethPariseau What is container management and why is it important? By: EmilyMell ",
        "_version_": 1718527443244941312
      },
      {
        "story_id": [19078281],
        "story_author": ["anishathalye"],
        "story_descendants": [79],
        "story_score": [1028],
        "story_time": ["2019-02-04T17:21:56Z"],
        "story_title": "MIT Hacker Tools: a lecture series on programmer tools",
        "search": [
          "MIT Hacker Tools: a lecture series on programmer tools",
          "https://hacker-tools.github.io/",
          "This class has moved to https://missing.csail.mit.edu/. You should go there to see the newest version of the material. This site is being left up for archival purposes. You can see all the lectures from the IAP 2019 course here. ",
          "Hi all! We (@anishathalye, @jjgo, and @jonhoo) have long felt that while university CS classes are great at teaching specific topics, they often leave it to students to figure out a lot of the common knowledge about how to actually use your computer. And in particular, how to use it efficiently.<p>There’s just no class in the undergrad curriculum that teaches you how to become familiar with the system you’re working with! Students are expected to know about, or figure out, the shell, editors, remote access and file management, version control, debugging and profiling utilities, and all sorts of other useful tools on their own. Often times, they won’t even know that many of these tools exist, and instead do things in roundabout ways or simply be left frustrated about their development environment.<p>To help mitigate this, we decided to run this short lecture series at MIT during the January Independent Activities Period that we called “Hacker Tools” (in reference to “hacker culture”, not hacking computers). Our hope was that through this class, and the resulting lecture materials and videos, we might be able to bootstrap students’ knowledge about the tools that are available to them, which they can then put to use throughout their time at university, and beyond.<p>We’ve shared both the lecture notes and the recordings of the lectures in the hopes that people outside of MIT may also find these resources useful in making better use of their tools. If that turns out to be true, we’re also thinking of re-doing the videos in screen-cast style with live chat and a proper microphone when we get the time. If that sounds interesting to you, and if you have ideas about other things you’d like to see us cover, please leave a comment below; we’d love to hear from you!<p>We’re sure there are also plenty of cool tools that we didn’t get to cover in this series that you all know and love. Please share them below along with a short description so we can all learn something new!<p>Anish, Jose, and Jon",
          "The equivalent UCLA course is CS35L: Software Construction Laboratory (<a href=\"https://web.cs.ucla.edu/classes/winter19/cs35L/\" rel=\"nofollow\">https://web.cs.ucla.edu/classes/winter19/cs35L/</a>). It's taught by Paul Eggert (big open source/coreutils/emacs contributor + author of diff/sort)."
        ],
        "story_type": ["Normal"],
        "url": "https://hacker-tools.github.io/",
        "url_text": "This class has moved to https://missing.csail.mit.edu/. You should go there to see the newest version of the material. This site is being left up for archival purposes. You can see all the lectures from the IAP 2019 course here. ",
        "comments.comment_id": [19078338, 19080440],
        "comments.comment_author": ["Jonhoo", "bhchiang"],
        "comments.comment_descendants": [6, 2],
        "comments.comment_time": [
          "2019-02-04T17:25:35Z",
          "2019-02-04T20:44:16Z"
        ],
        "comments.comment_text": [
          "Hi all! We (@anishathalye, @jjgo, and @jonhoo) have long felt that while university CS classes are great at teaching specific topics, they often leave it to students to figure out a lot of the common knowledge about how to actually use your computer. And in particular, how to use it efficiently.<p>There’s just no class in the undergrad curriculum that teaches you how to become familiar with the system you’re working with! Students are expected to know about, or figure out, the shell, editors, remote access and file management, version control, debugging and profiling utilities, and all sorts of other useful tools on their own. Often times, they won’t even know that many of these tools exist, and instead do things in roundabout ways or simply be left frustrated about their development environment.<p>To help mitigate this, we decided to run this short lecture series at MIT during the January Independent Activities Period that we called “Hacker Tools” (in reference to “hacker culture”, not hacking computers). Our hope was that through this class, and the resulting lecture materials and videos, we might be able to bootstrap students’ knowledge about the tools that are available to them, which they can then put to use throughout their time at university, and beyond.<p>We’ve shared both the lecture notes and the recordings of the lectures in the hopes that people outside of MIT may also find these resources useful in making better use of their tools. If that turns out to be true, we’re also thinking of re-doing the videos in screen-cast style with live chat and a proper microphone when we get the time. If that sounds interesting to you, and if you have ideas about other things you’d like to see us cover, please leave a comment below; we’d love to hear from you!<p>We’re sure there are also plenty of cool tools that we didn’t get to cover in this series that you all know and love. Please share them below along with a short description so we can all learn something new!<p>Anish, Jose, and Jon",
          "The equivalent UCLA course is CS35L: Software Construction Laboratory (<a href=\"https://web.cs.ucla.edu/classes/winter19/cs35L/\" rel=\"nofollow\">https://web.cs.ucla.edu/classes/winter19/cs35L/</a>). It's taught by Paul Eggert (big open source/coreutils/emacs contributor + author of diff/sort)."
        ],
        "id": "1efcaa82-32a2-49a6-8818-eb1c85262b7e",
        "_version_": 1718527384888541185
      },
      {
        "story_id": [19185570],
        "story_author": ["flagada"],
        "story_descendants": [10],
        "story_score": [53],
        "story_time": ["2019-02-17T17:26:52Z"],
        "story_title": "Arm Helium: New vector extension for the M-Profile Architecture",
        "search": [
          "Arm Helium: New vector extension for the M-Profile Architecture",
          "https://community.arm.com/processors/b/blog/posts/arm-helium-the-new-vector-extension-for-arm-m-profile-architecture",
          "Arm has announced the latest version of the Armv8-M architecture, known as Armv8.1-M, including the new M-Profile Vector Extension (MVE). The vector extensionbrings up to15times performance uplift to machine learning (ML) functions, and up to5times uplift to signal processing functionscompared to existing Armv8-M implementations. It may be viewed as the Armv8-M architectures version of the Advanced SIMD Extension (Neon) in the A-Profile. Arm Helium technologyis the M-Profile Vector Extension (MVE)for the Arm Cortex-M processor series. Armv8.1-M architecture new features A new vector instruction set extension (MVE) Additional instruction set enhancements for loops and branches (Low Overhead Branch Extension) Instructions providing half precision floating-point support Instruction improving state management of the Floating Point Unit (FPU) Enhancements to debug including: Performance Monitoring Unit (PMU) Unprivileged Debug Extension Debug support for MVE Reliability, Availability and Serviceability (RAS) extension Start early software development Arm tools aredeveloped along with the architecture. They are now ready for lead partners to start developing software and migrating libraries and other code to Helium,to enable performance increases for DSP and machine learning applications. Tools with support include: Fast Models for software execution and optimization on a virtual platform Arm Development Studio for comprehensive software development and debugging on Windows or Linux for any Arm-based projects Arm Compiler 6 for maximizing code performance Keil MDK for software development and debugging in Windows for Cortex-M and microcontrollers The Armv8.1-M simple programmers model, combined with familiar Arm tools, is a key advantage of Helium. Using a single toolchain for control and data processing, leads to lower development costs and less code maintenance. Virtual platform with Fast Models Arm Fast Modelsprovide fast, flexible programmer's view models of Arm architecture and IP, enabling software development of drivers, firmware, operating systems, and applications prior to silicon availability. Fast Models allow full control over the simulation, including profiling, debug and trace. There is a Fast Model available for lead partners, which can be used for early software development. It is based on the MPS2 Fixed Virtual Platform (FVP). The Armv8-M architecture envelope model (AEM) has been extended via the plugin interface to support Helium. This provides a suitable platform to get started writing and debugging software. Code, build and debug with Development Studio Development Studiofeaturing Keil MDK(Vision) has added Helium support for software compilation (Arm Compiler 6) and debugging. This includes disassembly and updated register views for new registers in Armv8.1-M.The toolsuite is also available for lead partners today. Performance enhancements to Cortex-M Helium, the M-Profile Vector Extension included in Armv8.1-M, brings significant enhancements to the Cortex-M processor range and will enable the use of a single CPU for both control and data processing code. The performance enhancements enable applications, such as machine learning and DSP. Arm tools have been developed in parallel with the architecture and are available now for lead partners to start developing software on both Windows and Linux. The Helium support in Arm Compiler 6, combined with leading performance and code density, make it a great choice to get a jumpstart on migrating software to Helium. Arm Fast Models combined with Arm debuggers make it possible to run code and see the Architecture Reference Manual in action. Further reading The press announcementgives a high-level overview of Armv8.1-M and Helium, plusdetails on the performanceenhancements. The 'Making Helium' blogoffers insight intothe creation of Arm's MVE. For full details on the architecture see the Architecture Reference Manual for Armv8.1-M. The Introduction to Armv8.1-M architecture white paper showcases the technical highlights of the new features and is available to download below. Download Armv8.1-M Architecture White Paper November 3, 2021 November 3, 2021 October 29, 2021 ",
          "The blog post <a href=\"https://community.arm.com/arm-research/b/articles/posts/making-helium-why-not-just-add-neon\" rel=\"nofollow\">https://community.arm.com/arm-research/b/articles/posts/maki...</a> has more useful content.",
          "Meantime, in the royalty-free side, RISCV's work on V extension continues: <a href=\"https://www.embecosm.com/2018/09/09/supporting-the-risc-v-vector-extension-in-gcc-and-llvm/\" rel=\"nofollow\">https://www.embecosm.com/2018/09/09/supporting-the-risc-v-ve...</a>"
        ],
        "story_type": ["Normal"],
        "url": "https://community.arm.com/processors/b/blog/posts/arm-helium-the-new-vector-extension-for-arm-m-profile-architecture",
        "comments.comment_id": [19186393, 19187144],
        "comments.comment_author": ["jedharris", "snvzz"],
        "comments.comment_descendants": [1, 2],
        "comments.comment_time": [
          "2019-02-17T19:44:13Z",
          "2019-02-17T21:52:28Z"
        ],
        "comments.comment_text": [
          "The blog post <a href=\"https://community.arm.com/arm-research/b/articles/posts/making-helium-why-not-just-add-neon\" rel=\"nofollow\">https://community.arm.com/arm-research/b/articles/posts/maki...</a> has more useful content.",
          "Meantime, in the royalty-free side, RISCV's work on V extension continues: <a href=\"https://www.embecosm.com/2018/09/09/supporting-the-risc-v-vector-extension-in-gcc-and-llvm/\" rel=\"nofollow\">https://www.embecosm.com/2018/09/09/supporting-the-risc-v-ve...</a>"
        ],
        "id": "bd29fc75-7d05-4bb3-a009-4d0c55aab1c1",
        "url_text": "Arm has announced the latest version of the Armv8-M architecture, known as Armv8.1-M, including the new M-Profile Vector Extension (MVE). The vector extensionbrings up to15times performance uplift to machine learning (ML) functions, and up to5times uplift to signal processing functionscompared to existing Armv8-M implementations. It may be viewed as the Armv8-M architectures version of the Advanced SIMD Extension (Neon) in the A-Profile. Arm Helium technologyis the M-Profile Vector Extension (MVE)for the Arm Cortex-M processor series. Armv8.1-M architecture new features A new vector instruction set extension (MVE) Additional instruction set enhancements for loops and branches (Low Overhead Branch Extension) Instructions providing half precision floating-point support Instruction improving state management of the Floating Point Unit (FPU) Enhancements to debug including: Performance Monitoring Unit (PMU) Unprivileged Debug Extension Debug support for MVE Reliability, Availability and Serviceability (RAS) extension Start early software development Arm tools aredeveloped along with the architecture. They are now ready for lead partners to start developing software and migrating libraries and other code to Helium,to enable performance increases for DSP and machine learning applications. Tools with support include: Fast Models for software execution and optimization on a virtual platform Arm Development Studio for comprehensive software development and debugging on Windows or Linux for any Arm-based projects Arm Compiler 6 for maximizing code performance Keil MDK for software development and debugging in Windows for Cortex-M and microcontrollers The Armv8.1-M simple programmers model, combined with familiar Arm tools, is a key advantage of Helium. Using a single toolchain for control and data processing, leads to lower development costs and less code maintenance. Virtual platform with Fast Models Arm Fast Modelsprovide fast, flexible programmer's view models of Arm architecture and IP, enabling software development of drivers, firmware, operating systems, and applications prior to silicon availability. Fast Models allow full control over the simulation, including profiling, debug and trace. There is a Fast Model available for lead partners, which can be used for early software development. It is based on the MPS2 Fixed Virtual Platform (FVP). The Armv8-M architecture envelope model (AEM) has been extended via the plugin interface to support Helium. This provides a suitable platform to get started writing and debugging software. Code, build and debug with Development Studio Development Studiofeaturing Keil MDK(Vision) has added Helium support for software compilation (Arm Compiler 6) and debugging. This includes disassembly and updated register views for new registers in Armv8.1-M.The toolsuite is also available for lead partners today. Performance enhancements to Cortex-M Helium, the M-Profile Vector Extension included in Armv8.1-M, brings significant enhancements to the Cortex-M processor range and will enable the use of a single CPU for both control and data processing code. The performance enhancements enable applications, such as machine learning and DSP. Arm tools have been developed in parallel with the architecture and are available now for lead partners to start developing software on both Windows and Linux. The Helium support in Arm Compiler 6, combined with leading performance and code density, make it a great choice to get a jumpstart on migrating software to Helium. Arm Fast Models combined with Arm debuggers make it possible to run code and see the Architecture Reference Manual in action. Further reading The press announcementgives a high-level overview of Armv8.1-M and Helium, plusdetails on the performanceenhancements. The 'Making Helium' blogoffers insight intothe creation of Arm's MVE. For full details on the architecture see the Architecture Reference Manual for Armv8.1-M. The Introduction to Armv8.1-M architecture white paper showcases the technical highlights of the new features and is available to download below. Download Armv8.1-M Architecture White Paper November 3, 2021 November 3, 2021 October 29, 2021 ",
        "_version_": 1718527386530611201
      },
      {
        "story_id": [21534619],
        "story_author": ["tombrm"],
        "story_descendants": [3],
        "story_score": [10],
        "story_time": ["2019-11-14T11:39:21Z"],
        "story_title": "Show HN: Respresso – localization and design asset optimizer for iOS and Android",
        "search": [
          "Show HN: Respresso – localization and design asset optimizer for iOS and Android",
          "https://respresso.io/",
          "Save hours with efficient collaboration Manage your resources in multiplatform environments. Fonts Easy integration of custom fonts Localizations Modify localization texts or add a new language to your project, without developers Images Change or resize an image anytime and keep in sync on all platforms App icons No more generating thousands of icon sizes, just use one SVG for all platforms. Colors No more incorrect guideline colors. Your designers will have the ability to set the perfect colors to be used on all platforms Raw Easy access to all your common config files(JSON, XML, YAML) Customize Respresso Extend Respressos functionality or connect your work tools like Slack, Teams, Jenkins etc. Version control All resources are under version control. You can easily lock your assets version and reuse it later. Be agile and spare development time Collaborate on assets with your team members or customer from anywhere in a transparent way to boost your productivity. It automatically transforms and delivers to your project without assistance. Your assets will be ready for use almost immediately. It takes care of your digital assets (images, texts, colors, fonts, etc.) across multiple platforms and projects. Help your team focus during the development stage. Developers code, designers deliver graphics, the marketing team, translators manage your localization and this is just the beginning of stress-free development. Start using in 3 simple steps Easy as pie 1-minute setup Upload your origin resources Sync converted resources across multiple platforms Respresso is simple but powerful Respresso can easily be integrated into your build process and also works well with your Continuous Integration tools. Respresso manages your resources Convert your resources automatically to platform-specific formats such as VectorDrawable for Android, PDF for iOS etc. Synchronize with your project in build-time regardless of which platform you use What else is it good for? Versioned resources and repeatable build support for CI & CD A better team experience via team and project creation. It provides you with a way to track every change in each project and send feedback. For example you will be notified when someone changes the key of a localization you use Well-separated roles spell correction and translation without developer intervention fix any graphics files and change icons without coding skills rebrand the application by changing colors and app icons Enforce naming conventions to enhance code quality Try our image converter Simply upload your chosen SVG file and download the generated resources as VectorDrawable for Android and PDF for iOS. Get access for Free Once you create an account you can try Respresso for free, for iOS, Android and Web frontend projects, too. Do you have any question? ",
          "Respresso helps mobile developers by automatically optimizing their design assets for iOS and Android and comes with a live localization feature.<p>It's still in beta and all feedback is highly appreciated.",
          "What is the pricing?"
        ],
        "story_type": ["ShowHN"],
        "url": "https://respresso.io/",
        "url_text": "Save hours with efficient collaboration Manage your resources in multiplatform environments. Fonts Easy integration of custom fonts Localizations Modify localization texts or add a new language to your project, without developers Images Change or resize an image anytime and keep in sync on all platforms App icons No more generating thousands of icon sizes, just use one SVG for all platforms. Colors No more incorrect guideline colors. Your designers will have the ability to set the perfect colors to be used on all platforms Raw Easy access to all your common config files(JSON, XML, YAML) Customize Respresso Extend Respressos functionality or connect your work tools like Slack, Teams, Jenkins etc. Version control All resources are under version control. You can easily lock your assets version and reuse it later. Be agile and spare development time Collaborate on assets with your team members or customer from anywhere in a transparent way to boost your productivity. It automatically transforms and delivers to your project without assistance. Your assets will be ready for use almost immediately. It takes care of your digital assets (images, texts, colors, fonts, etc.) across multiple platforms and projects. Help your team focus during the development stage. Developers code, designers deliver graphics, the marketing team, translators manage your localization and this is just the beginning of stress-free development. Start using in 3 simple steps Easy as pie 1-minute setup Upload your origin resources Sync converted resources across multiple platforms Respresso is simple but powerful Respresso can easily be integrated into your build process and also works well with your Continuous Integration tools. Respresso manages your resources Convert your resources automatically to platform-specific formats such as VectorDrawable for Android, PDF for iOS etc. Synchronize with your project in build-time regardless of which platform you use What else is it good for? Versioned resources and repeatable build support for CI & CD A better team experience via team and project creation. It provides you with a way to track every change in each project and send feedback. For example you will be notified when someone changes the key of a localization you use Well-separated roles spell correction and translation without developer intervention fix any graphics files and change icons without coding skills rebrand the application by changing colors and app icons Enforce naming conventions to enhance code quality Try our image converter Simply upload your chosen SVG file and download the generated resources as VectorDrawable for Android and PDF for iOS. Get access for Free Once you create an account you can try Respresso for free, for iOS, Android and Web frontend projects, too. Do you have any question? ",
        "comments.comment_id": [21534748, 21554836],
        "comments.comment_author": ["tombrm", "deca6cda37d0"],
        "comments.comment_descendants": [0, 1],
        "comments.comment_time": [
          "2019-11-14T12:06:56Z",
          "2019-11-16T20:42:53Z"
        ],
        "comments.comment_text": [
          "Respresso helps mobile developers by automatically optimizing their design assets for iOS and Android and comes with a live localization feature.<p>It's still in beta and all feedback is highly appreciated.",
          "What is the pricing?"
        ],
        "id": "73b42f55-0894-48dd-8666-76bb234c411d",
        "_version_": 1718527435659542528
      },
      {
        "story_id": [19128641],
        "story_author": ["slyrus"],
        "story_descendants": [21],
        "story_score": [77],
        "story_time": ["2019-02-10T17:08:57Z"],
        "story_title": "Ubuntu 18.04.2 LTS Released",
        "search": [
          "Ubuntu 18.04.2 LTS Released",
          "https://wiki.ubuntu.com/BionicBeaver/ReleaseNotes/ChangeSummary/18.04.2",
          "Contents Installation bug fixes Upgrade bug fixes Desktop fixes Server and Cloud related fixes Kernel and Hardware support updates Unsorted changesThis is a brief summary of bugs fixed between Ubuntu 18.04.1 and 18.04.2. This summary covers only changes to packages in main and restricted, which account for all packages in the officially-supported CD images; there are further changes to various packages in universe and multiverse. Some of these fixes were by Ubuntu developers directly, while others were by upstream developers and backported to Ubuntu. For full details, see the individual package changelogs. In addition to the bugs listed below, this update includes all security updates from the Ubuntu Security Notice list affecting Ubuntu 18.04 LTS that were released up to and including February 4, 2019. The last update included was USN-3871-3 (Linux kernel vulnerabilities). Installation bug fixes Updated CD images are provided with this release, including fixes for some installation bugs. (Many installation problems are hardware-specific; for those, see \"Hardware support bugs\" below.) console-setup 1762952 keyboard-configuration.config: While sourcing config files to re-seed debconf, load missing variables as empty values of same console-setup 1762952 keyboard-configuration.{config,templates}: There is no good default for layout toggling, stop pretending there is. Console users can set one with dpkg-reconfigure or editing /etc/defaults/keyboard livecd-rootfs 1302192 generate all tar files with --xattrs. livecd-rootfs 1585233 ubuntu-cpc: Reintroduce the -root.tar.xz artifact. console-setup 1788597 keyboard-configuration.config: Fix exit/return thinko livecd-rootfs 1783129 Disentangle enabling universe in the final image a little from having PREINSTALLED=true set and enable it for a live-server build. livecd-rootfs 1776891 Disable journald rate limiting in the live-server live session. initramfs-tools 1769682 scripts/functions: write netplan config files to /run/netplan for network devices configured with configure_networking. initramfs-tools 1769682 scripts/functions: add 'critical: true' parameter; requires netplan 0.36.2. initramfs-tools 1661629 Work out the kernel modules required to support ZFS filesystems and add them as necessary. ubiquity 1749289 Implement missing reboot and shutdown methods in debconf_ui ubiquity 1777900 Add systemd-resolved to oem-config.target's Wants livecd-rootfs 1792905 Ensure /lib/modules exists in root tarballs and sqashfs. initramfs-tools 1667512 b4804dd] Only sync the filesystem containing the initramfs initramfs-tools 1791959 debian/initramfs-tools.postinst: remove orphaned old-dkms initrd files in /boot. ubiquity 1789920 Mask ubiquity.service in a system installed with oem-config/enabled=true, as this prevents getty@tty1.service from running. livecd-rootfs 1805190 Include grub efi packages in manifests for uefi images. livecd-rootfs 1799773 Disable checksum generation. installation-guide 1730322 Describe HWE kernel, point release and url for supported arm64 platforms. installation-guide 1804306 Move supported arm64 server list to a wiki page to ease maintenance debian-installer 1807023 build/pkg-lists/base: add ca-certificates-udeb to enable HTTPS without d-i/allow_unauthenticated_ssl in stock initramfs image as in Debian. ubiquity 1768230 scripts/plugininstall.py: don't hard-code a resume partition in /etc/initramfs-tools/conf.d/resume at install time. In bionic and later, initramfs-tools will autodetect an appropriate resume partition at initramfs generation time, so ubiquity's resume setting is redundant and possibly wrong. debian-installer 1809021 Add HWE variants for all architectures partman-efi 1803031 check.d/efi: Make sure we block on a missing EFI partition no matter what architecture, not just for ia64. One could attempt to install on EFI x86 and will need an ESP to be able to install GRUB. ubiquity 1772374 scripts/plugininstall.py: Make sure efivars is bind-mounted when installing the bootloader. ubiquity 1803031 Automatic update of included source packages: partman-efi 71ubuntu2.2. Upgrade bug fixes These changes fix upgrade issues, smoothing the way for future upgrades to later releases of Ubuntu. ubuntu-release-upgrader 1783589 make sure that snapd is installed before trying to use it. ubuntu-release-upgrader 1783593 update the view with information regarding the progress of snaps being installed. ubuntu-release-upgrader 1783738 when checking for connectivity to the snap store use C.UTF-8 for the language so error message matching works. ubuntu-release-upgrader 1785096 Remove debs from apt's \"Dir::Cache::archives\" folder after the upgrade has completed successfully. ubuntu-release-upgrader 1766890 Upgrade libc6 before other packages to work around trigger issues ubuntu-release-upgrader 1787668 DistUpgradeQuirks.py: if ubuntu-desktop or snapd isn't in the package cache don't try and run the quirk for installing or upgrading snaps. gnome-initial-setup 1781417 Display the ubuntu welcome wizard in Unity, don't display the \"what's new\" page though since Unity didn't change ubuntu-release-upgrader 1787649 DistUpgradeController.py: If the attempt to only upgrade libc6 ends up creating an issue with apt's problem resolver try the full upgrade. This is what would have happened before the fix for bug 1766890. update-manager 1317164 Print transaction error and let the user try again applying updates update-manager 1791931 Don't ask backend to do package operations aready done. Aptdaemon cancels the transaction when asked to remove packages already removed which results the failure being shown to the user. This is unnecessary as update-manager can just filter the package operations to be done using a fresh cache and decrease the likelyhood of hitting a race condition where packages to be removed are already removed. ubuntu-release-upgrader 1797209 do-release-upgrade: do not run the release upgrade if either not all updates are installed or a reboot is required due to a libc6 upgrade. ubuntu-release-upgrader 1796940 debian/control: change ubuntu-release-upgrader-core to depend on ca-certificates unattended-upgrades 1789637 Unlock for dpkg operations with apt_pkg.pkgsystem_unlock_inner() when it is available. Also stop running when reacquiring the lock fails. Thanks to Julian Andres Klode for original partial patch unattended-upgrades 1781586 Skip rebuilding python-apt in upgrade autopkgtests. Python-apt has a new build dependency making the rebuilding as is failing and the reference handling issue is worked around in unattended-upgrades already. unattended-upgrades 1785093 Stop trying when no adjustment could be made and adjust package candidates only to lower versions unattended-upgrades 1785093 Skip already adjusted packages from being checked for readjusting. This makes it clearer that the recursion ends and can also be a bit quicker. update-manager 1072136 Keep or delete packages after looping over all of them. This prevents the resolver from changing the packages in the loop resulting in not keeping some phased packages back from being upgraded. update-manager 1795898 Stop lazy import of InstallBackends. Lazy imports made update-manager crash when an update-manager update changed the backend API and an updated incompatible backend was loaded to the not updated running update-manager process. update-manager 1790670 Cancel transaction on exit only when Cancel button is active. Also ignore exception when cancellation fails. ubuntu-release-upgrader 1799839 DistUpgrade/DistUpgradeController.py: check all the python symlinks and versions instead of the python one. Thanks to juliank for the assistance. ubuntu-release-upgrader 1799710 do-release-upgrade: Do not block release upgrades if the installable updates are ones which are not fully phased. update-manager 1787553 Add a reminder to enable Livepatch. ubuntu-release-upgrader 1797384 Set icon_name in the dialogs so that there will be an icon in gnome-shell. update-notifier 1800862 Check if a Livepatch patch has been applied during boot or before update-notifier has started. unattended-upgrades 1778219 Trigger unattended-upgrade-shutdown actions with PrepareForShutdown() Performing upgrades in service's ExecStop did not work when the upgrades involved restarting services because systemd blocked other stop/start actions making maintainer scripts time out and be killed leaving a broken system behind. Running unattended-upgrades.service before shutdown.target as a oneshot service made it run after unmounting filesystems and scheduling services properly on shutdown is a complex problem and adding more services to the mix make it even more fragile. The solution of monitoring PrepareForShutdown() signal from DBus allows Unattended Upgrade to run _before_ the jobs related to shutdown are queued thus package upgrades can safely restart services without risking causing deadlocks or breaking part of the shutdown actions. Also ask running unattended-upgrades to stop when shutdown starts even in InstallOnShutdown mode and refactor most of unattended-upgrade-shutdown to UnattendedUpgradesShutdown class. unattended-upgrades 1778219 Increase logind's InhibitDelayMaxSec to 30s. This allows more time for unattended-upgrades to shut down gracefully or even install a few packages in InstallOnShutdown mode, but is still a big step back from the 30 minutes allowed for InstallOnShutdown previously. Users enabling InstallOnShutdown node are advised to increase InhibitDelayMaxSec even further possibly to 30 minutes. unattended-upgrades 1803749 Stop using ActionGroups, they interfere with apt.Cache.clear() causing all autoremovable packages to be handled as newly autoremovable ones and be removed by default. Dropping ActionGroup usage does not slow down the most frequent case of not having anything to upgrade and when there are packages to upgrade the gain is small compared to the actual package installation. Also collect autoremovable packages before adjusting candidates because that also changed .is_auto_removable attribute of some of them. update-manager 1805118 Do not show the livepatch reminder if update-manager is running on a distribution without software-properties-gtk. update-manager 1787553 Add a reminder to enable Livepatch. software-properties 1805436 cloudarchive: Enable support for the Stein Ubuntu Cloud Archive on 18.04. shotwell 1802895 New bugfix update shotwell 1606491 Fix \"Out of memory\" issues when scrolling through large collections shotwell 1723181 direct: Fix crash when dismissing modifications libreoffice 1799230 New upstream release libreoffice-l10n 1799230 New upstream release unattended-upgrades 1806487 Start service after systemd-logind.service to be able to take inhibition lock unattended-upgrades 1806487 Handle gracefully when logind is down update-notifier 1809505 src/update-notifier.c: Don't use G_SPAWN_DO_NOT_REAP_CHILD in order to avoid zombie processes. update-manager 1798618 UpdateManager/Core/MetaRelease.py: set prompt in MetaReleaseCore so that do-release-upgrade can provide more informative error messages. update-manager 1795024 UpdateManager/Core/MetaRelease.py: set prompt in MetaReleaseCore so that do-release-upgrade can provide more informative error messages. ubuntu-release-upgrader 1795024 data/release-upgrades: Clarify documentation regarding the behavior for different Prompt settings. ubuntu-release-upgrader 1798618 do-release-upgrade: Utilize information regarding what Prompt is set to so that a more informative error message can be displayed. ubuntu-release-upgrader 1795024 do-release-upgrade: Utilize information regarding what Prompt is set to so that a more informative error message can be displayed. ubuntu-release-upgrader 1786484 DistUpgrade/DistUpgradeCache.py: When calculating free space needed for mount points don't use a negative number as the buffer. ubuntu-release-upgrader 1807043 DistUpgrade/DistUpgradeController.py: When rewriting sources.list entries check to see if the source provides packages for the release to which the upgrade is occurring. If it doesn't the entry is disabled thereby improving upgrades with PPAs. ubuntu-release-upgrader 1807032 do-release-upgrade: add a parameter to allow third party mirrors and repositories, additionally pass along the environmental variable RELEASE_UPGRADER_ALLOW_THIRD_PARTY via pkexec and sudo. ubuntu-release-upgrader 1771387 DistUpgrade/DistUpgradeCache.py: in the event there is a failure to calculate the upgrade provide information about the log files in /var/log/dist-upgrade. ubuntu-release-upgrader 1773637 DistUpgrade/xorg_fix_proprietary.py: modify how the system is checked to see if nvidia is being used, drop fglrx check since it has been deprecated. Desktop fixes These changes mainly affect desktop installations of Ubuntu, Kubuntu, Edubuntu and other Ubuntu-based systems. xorg 1768610 Rename nux config leftovers which might change the environment even when not running an unity session vte2.91 1780501 Revert the changes to revert-pcre2.patch in the previous SRU since they introduced API incompatibilies which aren't OK in an SRU. gnome-menus 1765799 Fix app menus not updating correctly after app install or removal. This patch was accidentally dropped in previous merge. gnome-shell-extension-ubuntu-dock 1712661 Fix crash connecting/disconnecting monitors gnome-shell-extension-ubuntu-dock 1784920 Fix showApps button label position gnome-control-center 1779051 Fix crash when settings changed after panel closed gnome-software 1778160 Fix crash when have plugs with multiple slots available gnome-software 1781996 don't segfault on null snap installation date gnome-shell 1718931 New upstream release gnome-shell 1782614 New upstream release gnome-shell 1784671 Don't handle key presses directly if there are modifiers mutter 1754949 Avoid crashing when warning about wrongly set crtc mode mutter 1767956 Don't crash if drmModeGetResources returns NULL mutter 1784398 Always update monitor in wayland, avoiding crash mutter 1772831 Don't return screen resolutions that can't be applied mutter 1422253 Don't crash if a modal dialog closes while being dragged mutter 1723615 Don't try to use an invalid monitor mode to figure out scaling mutter 1783311 No-change backport to bionic gnome-software 1775226 also disable offline updates in refresh plugin ubuntu-report 1786432 Include optional DCD OEM file ubuntu-report 1784383 Collect number of disks and their sizes apport 1778497 Add a remember option to whoopsie so that users can diminish crash interactions apport 1778694 Move apport autoreport service files to apport binary package. Having them in apport-noui was creating a bug where autoreport wasn't working on desktop. As we check in the launched script for whoopsie and autoreport file, we don't autoreport by default. apport-noui still touches the file to enable autoreport on install. apport 1778694 Start apport-autoreport after installing apport-noui which is part of improving apport's automatic crash reporing feature. software-properties 1770686 SoftwarePropertiesGtk.py: Hide livepatch widgets in flavors without an online account panel in gnome-control-center glib2.0 1789472 New upstream release ubuntu-settings 1782190 revert in communitheme session default font size to 11, as it creates rendering issues in GNOME Shell firefox 1791789 Fix : Mark distribution search engines as read-only, so that they are marked as hidden in rather than removed from the search engine cache when a user \"removes\" them (they can't actually be removed from disk). This stops them from reappearing on cache rebuilds firefox 1791789 Backport upstream change to the search service to not handle locale changes on shutdown. As well as resulting in en-US search engines being added to the search engine cache for all locales, it was resulting in a cache rebuild on every restart, making the above bug worse (fixes another part of ) firefox 1791789 Set \"spellchecker.dictionary_path\" by default to point to /usr/share/hunspell so that system dictionaries are loaded again, now that Firefox no longer loads them from its own install directory. Fixes another part of firefox 1791789 Final part of the fix for : Cleanup extra Amazon.com search engine in locales that have their own Amazon search engine fonts-liberation 1769654 Backport to bionic to fix skewed font metrics fonts-liberation2 1769654 Backport to bionic to fix skewed font metrics libreoffice 1785679 New upstream release libreoffice-l10n 1785679 New upstream release evolution-data-server 1784514 New upstream release apport 1791324 Handle old reports generated pre-apport with \"remember\" option. If the option isn't there, consider as false. nautilus 1782681 New upstream release: 3.26.4 nautilus 1765776 New upstream release: 3.26.4 nautilus 1767027 Follow nautilus settings to search only in current folder. This doesn't apply to gnome-shell search provider for indexed searches. nautilus 1764779 don't crash if selecting a volume that is not mounted nautilus 1713581 don't crash when try to select a file multiple times in a single run nvidia-graphics-drivers-340 1791542 Use diversions for \"libGLESv1_CM.so\" \"libGLESv1_CM.so.1\". nvidia-graphics-drivers-340 1761593 Install the modprobe file in /lib, and remove the blacklist file manually. gnome-mines 1772191 Backport fix from 3.30 for missing high resolution app icon gnome-software 1789336 Show verified developers gnome-software 1789338 Use wide scope searching gnome-software 1756379 Delay startup of service to allow the shell to load first ubuntu-themes 1773045 Remove nonexistent gnome-builder.css ubuntu-themes 1781736 Radiance: fix typo in assets link for focused buttons ubuntu-themes 1781736 Radiance: Use scaled image for buttons border ubuntu-themes 1782038 Ambiance, Radiance: don't use padding on window buttons for chromium ubuntu-themes 1758841 Ambiance: use default foreground color for toolbar menus ubuntu-themes 1743373 Ambiance, Radiance: show proper arrow in combobox ubuntu-themes 1785699 Ambiance, Radiance: use default disabled color for actions headerbar buttons ubuntu-themes 1761684 Ambiance, Radiance: fix list selected color for gnome-boxes ubuntu-themes 1795895 Ambiance, Radiance: properly theme disabled and hovered accelerators xorg-server 1789913 prime-sync-refactor.diff: Fix crash on modesetting+amdgpu hybrid. nautilus 1795028 Fix remote filesystem check on file during search nautilus 1798426 Refreshed to add memory leak and potential crash fixes gnome-settings-daemon 1797322 backport fix from upstream to resolve suspend/resume rfk issues xdg-utils 1743216 Use perl's decode() to ensure we don't pass invalid UTF-8 to D-Bus, as doing so triggers an assertion from libdbus which makes us crash. fonts-noto-color-emoji 1788256 New upstream release evince 1790609 New upstream release apport 1760220 apport/ui.py: when using ubuntu-bug properly handle executables which start with /snap/bin. apport 1780767 test/test_ui_gtk.py: Increase the timeout so that when the autopkgtest infrastructure is busy the tests should not fail. gnome-shell 1739931 use defined color for menu separators gnome-shell 1743058 set StEntry minimun height to work properly with Ubuntu font gnome-shell 1745888 Don't emit two click events on touch under X11 gnome-shell 1725312 Handle NULL scroll bars in st-scroll-view gnome-calculator 1790876 New upstream stable release mutter 1727356 Create back buffers in early intel GPU generations mutter 1730211 Fix typing capital letters when using OSD keyboard gnome-initial-setup 1789925 rework the patch from the previous upload to change the right reference this time gnome-initial-setup 1764723 Show an error message in case livepatch setup fails. This also will make sure that gnome-intial-setup does not quit before livepatch responds back. totem 1433984 backport fix for nautilus sometime crashing when closing the video properties dialog totem 1798399 backport fixes to make the gallery plugin work again desktop-file-utils 1768271 backport debian change to fix update-desktop-error with gnome-font-viewer 3.28 gnome-desktop3 1795668 Enable bubblewrap hardening for thumnbailers ghostscript 1806517 SECURITY REGRESSION: multiple regressions libgnomekbd 1721893 backport a patch available on bugzilla to fix a segfault in the keyboard layout preview code. The most common case seems to be g-c-c trying to preview a 'default' layout which was working before some other fixes landed, that fixes the regression. gnome-software 1552792 Build with PackageKit autoremove support gnome-software 1785240 Stop cancelling snapd authorization triggers error notification gnome-software 1754864 Pull related flatpak refs gnome-software 1798228 Fix snap search result ordering gnome-software 1719797 Stop reboot notification from timing out gnome-software 1798470 Support composite CAB files gnome-desktop3 1807127 Fix thumbnailer on 32-bit systems where /lib64 is not available. This fixes a regression introduced in the previous update. gedit 1800179 'document selector: make search caseless', it makes the filter in the open document dialog work in a more logical way gnome-shell 1765304 Cherry-pick upstream commit to prevent focus stealing on password fields in firefox when ibus is used libgweather 1803487 New upstream release: 3.28.2 nautilus 1804685 'preferences-window: Fix icon views captions order' software-properties 1807373 SoftwarePropertiesGtk.py: when checking a package's depends for DKMS also pass on an AttributeError gvfs 1803158 Use O_RDWR to fix fstat when writing gvfs 1798725 common: Prevent crashes on invalid autorun file gvfs 1630905 daemon: Prevent deadlock and invalid read when closing channels gvfs 1792878 workaround libsoup limitation to prevent dav lockups gvfs 1778322 smbbrowse: Force NT1 protocol version for workgroup support gvfs 1803190 smb: Add workaround to fix removal of non-empty dir gjs 1809181 New upstream release gjs 1803271 New upstream release gparted 1779292 d/patches/lvm2_prompt_patch.patch: Apply upstream patch to fix pvresize compatibility with LVM2 >= 2.02.171, which broke LVM PV resizing due to the addition of a confirmation dialogue for the resize. cups 1804576 fix-handling-of-MaxJobTime.patch: Fix handling of MaxJobTime 0 totem 1726688 Together, these patches fix the right-click .desktop actions gnome-shell-extension-ubuntu-dock 1743976 Avoid repainting an unchanging dock. gnome-shell-extension-ubuntu-dock 1769383 theming: Ensure _trackingWindows contains valid windows gnome-shell-extension-ubuntu-dock 1769383 extension: Ensure signal disconnection firefox 1808980 Build with --enable-rust-simd (except on i386 and armhf) libdrm 1798597 Backport to bionic for 18.04.2 HWE stack update. wayland 1798597 Backport for 18.04.2 HWE stack update. wayland 1781440 control: Bump Breaks/Replaces again to match reality. mesa 1798597 Backport for 18.04.2 HWE stack update. xf86-input-wacom-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xorg-server-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-input-libinput-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-amdgpu-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-ati-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-vesa-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-vmware-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-nouveau-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-fbdev-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-qxl-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-dummy-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xorg-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. cairo 1560286 \"Revert \"Correctly decode Adobe CMYK JPEGs in PDF export\" From further testing and investigation it appears that many PDF viewers already have a workaround to invert Adobe CMYK JPEGs, so our generated PDFs display incorrectly with those viewers due to double-inversion. xserver-xorg-video-intel-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. These changes mainly affect installations of Ubuntu on server systems and clouds. keepalived 1744062 d/p/fix-removing-left-over-addresses-if-keepalived-abort.patch: Cherry-picked from upstream to ensure left-over VIPs and eVIPs are properly removed on restart if keepalived terminates abonormally. This fix is from the upstream 1.4.0 release. cloud-init 1784685 cherry-pick 3cee0bf8: oracle: fix detect_openstack to report True on cloud-init 1777912 New upstream snapshot. cloud-init 1770712 debian/rules: update version.version_string to contain packaged version. cloud-init 1777912 New upstream release. aodh 1776375 Support same projects in different domain maas 1773201 Stable Release Update. New upstream release, MAAS 2.4.0 freeipmi 1784926 Cherry-pick patches from upstream that add support to ipmi-locate for parsing ACPI/SPMI tables out of sysfs: rax-nova-agent 1783614 New upstream version 2.1.15 libvirt 1788226 Fix an issue where guests with plenty of hostdevs attached where detected as not shut down due to the kernel needing more time to free up resources apache2 1750356 d/p/balance-member-long-hostname-part{1,2}.patch: Provide an RFC1035 compliant version of the hostname in the proxy_worker_shared structure. A hostname that is too long is no longer a fatal error. chrony 1787366 d/p/lp-1787366-fall-back-to-urandom.patch: avoid hangs when starting the service on newer kernels by falling back to urandom. avahi 1752411 debian/avahi-daemon-check-dns.sh: On some hardware, the 'host' command gets stuck and does not timeout as it should leaving this script and boot-up hanging indefinitely. Launch host with 'timeout' to kill it after 5 seconds in these cases as a workaround. unbound 1788622 d/p/lp-1788622-fix-systemd-reload.patch: Fix hang due to all worker threads stopping on reload maas 1788641 Stable Release Update. New upstream release, MAAS 2.4.2 postfix 1753470 debian/patches/fix-postconf-segfault.diff: Fix a postconf segfault when map file cannot be read. Thanks to Viktor Dukhovni <postfix- users@dukhovni.org>. qemu 1755912 d/p/lp-1755912-qxl-fix-local-renderer-crash.patch: Fix an issue triggered by migrations with UI frontends or frequent guest resolution changes qemu 1787408 d/p/ubuntu/target-ppc-extend-eieio-for-POWER9.patch: Backport to extend eieio for POWER9 emulation. rax-nova-agent 1788716 Use DefaultDependencies=no to prevent forming service ordering cycles openjdk-lts 1788250 debian/rules: by default leave atk disabled, move accessibility bridge to recommends. openjdk-lts 1788267 debian/rules: by default leave atk disabled, move accessibility bridge to recommends. openvpn 1787208 d/openvpn@.service: Add CAP_AUDIT_WRITE to avoid issues with callout scripts breaking due to sudo/pam being unable to audit the action. Fixed in upstream issue #918, suggested to Debian in #868806 libvirt 1788603 d/p/ubuntu-aa/lp-1788603-fix-ptrace-rules-with-kernel-4.18.patch: avoid issues with newer kernels >=4.18 libvirt 1789659 d/p/ubuntu/lp-1789659-don-t-check-for-parallel-iteration-in-hash.patch: remove broken and redundant check for parallel iteration in hash functions nova-lxd 1789427 New upstream stable point release for OpenStack Queens. nginx 1782226 Stable Release Update. Do not attempt to start nginx if other daemon is binding to port 80, to prevent install failure: open-vm-tools 1791220 d/p/ubuntu/lp-1791220-Disable-hgfsServer-not-VMware.patch: avoid crashing with segfaults when force starting the service in non VMWare environments. open-vm-tools 1790145 d/p/debian/scsi-udev-rule: fix applying of the scsi timeout horizon 1790189 d/theme/ubuntu/_styles.scss: Ensure btn-danger rules are preferred when used with a, a:link, a:visited and dropdown-menu.li. nova 1761140 d/control: Drop circular dependencies. nova-compute depends on nova-compute-* packages. nova-compute-* packages shouldn't depend on nova-compute. nova-compute-* should however depend on nova-common.. cloud-initramfs-tools 1792905 copymods: Take ownership of lib/modules squid3 1792728 d/usr.sbin.squid: Update apparmor profile to grant read access to squid binary webkit2gtk 1795901 Install missing headers lxcfs 1788232 New upstream bugfix release: lxc 1788457 New upstream bugfix release: snapd-glib 1785240 Support auth cancelled error snapd-glib 1789338 Support wide scope searches snapd-glib 1789336 Support publisher information snapd-glib 1789335 Support refresh information networkd-dispatcher 1797884 Allow overriding /usr/lib scripts in /etc/networkd-dispatcher. Replaces our patch to use /usr/lib/networkd-dispatcher with the solution contributed upstream that has a search path with both /usr/lib/ and /etc locations. qemu 1790901 Update pxe netboot images for KVM s390x to qemu 3.0 level The SLOF source pieces in src:qemu are only used for s390x netboot, which are independent ROMs (no linking). All other binaries out of this are part of src:slof and independent. cloud-init 1795953 New upstream release. horizon 1778771 d/p/add-enabled-check-in-backups-panel.patch: Cherry-picked from https://review.openstack.org/#/c/605994/ to ensure Volume Backups panel is disabled if enable_backup is False. lxd 1788280 New upstream bugfix release: lxd 1788314 Temporarily disable ZFS tests on s390x due to conflict between zfsutils-linux and s390-tools netplan.io 1770082 Fix typo breaking rename on 'netplan apply'. netplan.io 1793309 Backport netplan 0.40.1 to 18.04. netplan.io 1795343 Deal gracefully with empty files on 'netplan apply' netplan.io 1786726 Don't render ipv4 dns-search unless we have an ipv4 address. netplan.io 1736965 Set permissive umask on networkd .network, .link and .netdev files netplan.io 1768560 Set permissive umask on networkd .network, .link and .netdev files netplan.io 1747455 Fix support for link-scope routes. netplan.io 1756701 Spell Gratuitous ARP correctly and make it work. netplan.io 1783940 Many typo fixes for documentation. netplan.io 1771704 Allow link-local addresses to be configured. netplan.io 1736975 Forces bridges with no addresses to be brought online. netplan.io 1770082 Write udev .rules files to /run/udev/rules.d to enforce interface renaming. netplan.io 1768823 Don't traceback for 'netplan ip leases' when iface is not managed or doesn't DHCP netplan.io 1771440 Fix duplicate \"/\" path separator in error messages netplan.io 1768798 Fix incorrect terminal reset in 'netplan try' on Ctrl-C. netplan.io 1768783 Updated doc entries: mtu, fix fwmark->mark, cleanup optional. netplan.io 1770082 Generate udev rules files to rename devices Due to a systemd issue[1], using link files to rename interfaces doesn't work as expected. Link files will not rename an interface if it was already renamed, and interfaces are renamed in initrd, so set-name will often not work as expected when rebooting. However, rules files will cause a renaming, even if the interface has been renamed in initrd. amavisd-new 1792293 d/p/100_more_amavisd_helpers_fixes: Fix Debian/Ubuntu pathing in amavisd-release amavisd-new 1770532 d/p/105_amavisd_fix_originating_dkim_signing.patch: Fix DKIM signing in 2.11.0 nova 1795424 New stable point release for OpenStack Queens. heat 1795424 New stable point release for OpenStack Queens. keystone 1795424 New stable point release for OpenStack Queens. python-openstackclient 1800490 New stable point release for OpenStack Queens. open-vm-tools 1793219 d/p/lp-1793219-fix-stats-overflow.patch: fix potential overflow of 32 bit /proc values bind9 1769440 d/p/skip-rtld-deepbind-for-dyndb.diff: fix named-pkcs11 crashing on startup. Thanks to Petr Menk <pemensik@redhat.com> neutron 1795424 New stable point release for OpenStack Queens. neutron 1790598 d/p/metadata-use-requests-for-comms-with-nova-api.patch: Cherry-picked from https://review.openstack.org/#/c/599541/ to enable cert management where IP addresses are used in subject alternate names. openldap 1783183 d/apparmor-profile: update apparmor profile to allow reading of files needed when slapd is behaving as a kerberos/gssapi client and acquiring its own ticket. samba 1795772 d/p/fix-rmdir.patch: fix the patch to not apply with offset, which previously made it change the wrong, almost identical, function. samba 1795772 d/p/fix-rmdir.patch: Fix to make smbclient report directory-not-empty errors ca-certificates-java 1770553 Backport from Cosmic. ca-certificates-java 1771815 Merge from Debian unstable. Remaining changes: ca-certificates-java 1771363 debian/postinst.in: Detect PKCS12 cacert keystore generated by previous ca-certificates-java and convert them to JKS. ca-certificates-java 1769013 Merge from debian unstable. Remaining changes: ca-certificates-java 1739631 Merge from debian unstable. Remaining changes: apache2 1782806 d/debhelper/apache2-maintscript-helper: fix typo in apache2_switch_mpm()'s a2query call. openjdk-lts 1800792 debian/patches/sec-webrev-11.0.1-b21-S8211731.patch: apply missing patch from the security update. bubblewrap 1795668 Don't install setuid on Ubuntu & derivatives since Ubuntu's kernel enables unprivileged user namespaces ubuntu-keyring 1798073 Validate that all shipped fragments are signed. netplan.io 1802322 Fix idempotency in renaming: bond members should be exempt from rename, as they may all share a single MAC for the bond device. netplan.io 1770082 Fix typo breaking rename on 'netplan apply'. netplan.io 1793309 Backport netplan 0.40.1 to 18.04. netplan.io 1795343 Deal gracefully with empty files on 'netplan apply' netplan.io 1786726 Don't render ipv4 dns-search unless we have an ipv4 address. netplan.io 1736965 Set permissive umask on networkd .network, .link and .netdev files netplan.io 1768560 Set permissive umask on networkd .network, .link and .netdev files netplan.io 1747455 Fix support for link-scope routes. netplan.io 1756701 Spell Gratuitous ARP correctly and make it work. netplan.io 1783940 Many typo fixes for documentation. netplan.io 1771704 Allow link-local addresses to be configured. netplan.io 1736975 Forces bridges with no addresses to be brought online. netplan.io 1770082 Write udev .rules files to /run/udev/rules.d to enforce interface renaming. netplan.io 1768823 Don't traceback for 'netplan ip leases' when iface is not managed or doesn't DHCP netplan.io 1771440 Fix duplicate \"/\" path separator in error messages netplan.io 1768798 Fix incorrect terminal reset in 'netplan try' on Ctrl-C. netplan.io 1768783 Updated doc entries: mtu, fix fwmark->mark, cleanup optional. rax-nova-agent 1804445 New upstream version 2.1.18 rax-nova-agent 1788716 Use DefaultDependencies=no to prevent forming service ordering cycles libvirt 1801666 d/control: explicitly Build-dep on libwiretap-dev to fix FTBFS since libwireshark 2.6.x SRU upload exim4 1786508 d/p/eximstats_unitialized_value.patch: Fix uninitialized value error in eximstats. postfix 1791139 d/postfix-{cdb,ldap,lmdb,mysql,pcre,pgsql}.postinst, d/postfix.postinst: Handle empty alias_database field in main.cf lxc 1804755 New upstream bugfix release: lxcfs 1804753 New upstream bugfix release: barbican 1806043 New stable point release for OpenStack Queens. neutron-fwaas 1806043 New stable point release for OpenStack Queens. keystone 1806043 New stable point release for OpenStack Queens. nova 1744079 d/p/disk-size-live-migration-overcommit.patch: Cherry-picked from https://review.openstack.org/#/c/602478 to ensure proper disk calculation during live migration with over-commit. nova 1806043 New stable point release for OpenStack Queens. horizon 1802226 d/openstack-dashboard.postinst: Ensure that if memcached is installed it is restarted in post-install script after collecting/compressing static assets, enabling refresh of memcached static assets after upgrade. psmisc 1806060 fix killall option parsing walinuxagent 1799498 New upstream release. ceph 1796645 New upstream point release. cinder 1806043 New stable point release for OpenStack Queens. netplan.io 1811210 No change rebuild in -security pocket to fix dependency issue samba 1801227 d/p/auth-fail-eexist.diff: smbc_opendir should not return EEXIST with invalid login credentials. Thanks to David Mulder. network-manager-applet 1807460 'libnma: unescape certificate paths in URIs', that fixes selecting files with a space in their name openvswitch 1780752 New upstream point release: sssd 1807246 d/p/fix-id-out-of-range-lookup.patch: CACHE_REQ: Do not fail the domain locator plugin if ID outside the domain range is looked up. Thanks to Jakub Hrozek <jhrozek@redhat.com>. sssd 1793882 d/t/common-tests, d/t/control, d/t/ldap-user-group-krb5-auth, d/t/ldap-user-group-ldap-auth, d/t/login.exp, d/t/util: add DEP8 tests for kerberos and LDAP deja-dup 1804744 Fix bug preventing restore on a fresh install if you don't also set your current backup location to the old backup. snapd 1811233 New upstream release, snapd 1779403 New upstream release, snapd 1814355 disable systemd environment generator on bionic to fix ceph 1813582 d/p/lp1813582.patch: Cherry pick fix for crash in rados py binding under gnocchi (LP: #1813582). libmemcached 1573594 Fix missing null termination in PROTOCOL_BINARY_CMD_SASL_LIST_MECHS response handling (LP: #1573594) haproxy 1804069 d/p/stksess-align.patch: Make sure stksess is properly aligned. (LP: #1804069) landscape-client 1788219 debian/patches/nutanix-kvm.patch: Update vm_info.py to include Nutanix hypervisor. (LP: #1788219) landscape-client 1670291 debian/patches/release-upgrade-success.patch: Enable landscape-client to survive trusty upgrade. (LP: #1670291) landscape-client 1670291 debian/patches/post-upgrade-reboot.patch: Force reboot operation in case systemd fails. (LP: #1670291) landscape-client 1765518 debian/patches/unicode-tags-script.patch: Permit environments containing unicode chars for script execution. (LP: #1765518) landscape-client 1616116 debian/patches/1616116-resync-loop.patch: Clear hash id database on package resync. (LP: #1616116) Kernel and Hardware support updates Considerable work has been done in Ubuntu 18.04.2 on improving support for many specific items of hardware. intel-microcode 1778738 Default to early instead of auto, and install all of the microcode, not just the one matching the current CPU, if MODULES=most is set in the initramfs-tools config amd64-microcode 1778738 Default to 'early' instead of 'auto' in the initramfs-tools hook when building with MODULES=most lshw 1752523 d/patches/lshw-fix-unknown-version-issue.patch: Cherry pick from upstream. open-iscsi 1785108 d/net-interface-handler: Apply changes only for the iscsi-root grub2 1786491 Verify that the current and newer kernels are signed when grub is updated, to make sure people do not accidentally shutdown without a signed kernel. grub2-signed 1786491 Rebuild against grub2 2.02-2ubuntu8.3 and check kernel is signed on amd64 EFI before installing grub. linux-meta-gcp 1780923 linux-gcp: add a meta package for the extra modules linux-oem 1781895 Bluetooth: Redpine: Bionics: L2test transfer is failed to start in Ubuntu 18.04 linux-oem 1782070 Redpine] Upgrades to improve throughput and stability grub2-signed 1785859 Rebuild against grub2 2.02-2ubuntu8.4 grub2 1785859 debian/patches/ofnet-init-structs-in-bootpath-parser.patch: initialize structs in bootpath parser. Fixes netboot issues on ppc64el. gnu-efi 1790709 New upstream version 3.0.8. shim-signed 1790724 Backport shim-signed 1.37 to Ubuntu 18.04. shim-signed 1778848 debian/shim-signed.postinst: use --auto-nvram with grub-install in case we're installing on a NVRAM-unavailable platform. shim-signed 1778848 debian/control: bump the dependency for grub2-common to make sure grub-install supports --auto-nvram. shim-signed 1778848 debian/control: switch the grub-efi-amd64-bin dependency to grub-efi-amd64-signed. ipxe 1789319 Build ROMs for QEMU with CONFIG=qemu libglvnd 1782285 rules, libgles2: Add GLESv1 support. libglvnd 1780039 Always return an error from eglMakeCurrent if EGLDisplay is invalid. bolt 1786265 New upstream version bolt 1778020 should resolve issues with devices not being authorized on initial boot e.g in lightdm open-iscsi 1755858 make iscsid socket activated to only activate it as-needed ubuntu-drivers-common 1789201 Start before oem-config.service. open-iscsi 1791108 d/net-interface-handler: replace 'domainsearch' with the correct configuration option 'search' in net-interface-handler shim-signed 1792575 debian/control: add Breaks: grub-efi-amd64-signed (<< 1.93.7), as the new version of shim exercises a bug in relocation code for chainload that was fixed in that upload of grub, affecting Windows 7, Windows 10, and some netboot scenarios where chainloading is required. shim-signed 1790724 Backport shim-signed 1.37 to Ubuntu 18.04. grub2-signed 1792575 Rebuild against grub2 2.02-2ubuntu8.6 grub2-signed 788298 Rebuild against grub2 2.02-2ubuntu8.5 grub2 1792575 debian/patches/linuxefi_fix_relocate_coff.patch: fix typo in relocate_coff() causing issues with relocation of code in chainload. grub2 1792575 debian/patches/linuxefi_truncate_overlong_reloc_section.patch: The Windows 7 bootloader has inconsistent headers; truncate to the smaller, correct size to fix chainloading Windows 7. grub2 788298 debian/patches/grub-reboot-warn.patch: Warn when \"for the next boot only\" promise cannot be kept. util-linux 1783810 Use getrandom() with GRND_NONBLOCK to avoid hangs in early boot when e.g. the partition is resized. Cherry picked from upstream. skiboot 1785026 d/opal-prd.logrotate: fix ownership of /var/log/opal-prd.log. friendly-recovery 1766872 Cleanup lintian warnings. linux 1796542 Silent data corruption in Linux kernel 4.15 linux 1789746 getxattr: always handle namespaced attributes linux 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux 1791569 some nvidia p1000 graphic cards hang during the boot linux 1783746 ipmmu is always registered linux 1794889 Bionic update: upstream stable patchset 2018-09-27 linux-kvm 1796542 Silent data corruption in Linux kernel 4.15 linux-kvm 1793841 IP_SET modules not included in kernel build, prevents container functionality linux-kvm 1789746 getxattr: always handle namespaced attributes linux-kvm 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux-kvm 1791569 some nvidia p1000 graphic cards hang during the boot linux-kvm 1783746 ipmmu is always registered linux-kvm 1794889 Bionic update: upstream stable patchset 2018-09-27 gdm3 1780076 Add utils-add-new-gdm-disable-wayland-binary.patch, cherry-picked from cosmic. linux-aws 1796542 Silent data corruption in Linux kernel 4.15 linux-aws 1794175 kyber-iosched module missing from linux-modules package linux-aws 1789746 getxattr: always handle namespaced attributes linux-aws 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux-aws 1791569 some nvidia p1000 graphic cards hang during the boot linux-aws 1783746 ipmmu is always registered linux-aws 1794889 Bionic update: upstream stable patchset 2018-09-27 linux-gcp 1796542 Silent data corruption in Linux kernel 4.15 linux-gcp 1789746 getxattr: always handle namespaced attributes linux-gcp 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux-gcp 1791569 some nvidia p1000 graphic cards hang during the boot linux-gcp 1783746 ipmmu is always registered linux-gcp 1794889 Bionic update: upstream stable patchset 2018-09-27 linux-oem 1796542 Silent data corruption in Linux kernel 4.15 grub2 1785033 debian/patches/0001-i386-linux-Add-support-for-ext_lfb_base.patch: Add support for ext_lfb_base. grub2-signed 1785033 Rebuild against grub2 2.02-2ubuntu8.7 linux-azure 1798185 Enable CONFIG_INFINIBAND_USER_MAD linux-azure 1798185 Enable CONFIG_INFINIBAND_USER_MAD linux-azure 1796542 Silent data corruption in Linux kernel 4.15 linux-azure 1789746 getxattr: always handle namespaced attributes linux-azure 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux-azure 1791569 some nvidia p1000 graphic cards hang during the boot linux-azure 1783746 ipmmu is always registered linux-azure 1794889 Bionic update: upstream stable patchset 2018-09-27 s390-tools 1777600 zdev: Adjust zdev modprobe path to be compatible with split-usr systems. s390-tools 1794308 zdev: Trigger generic_ccw devices on any kernel module loads. secureboot-db 1776996 Backport secureboot-db from cosmic to apply the August 2016 dbx updates from Microsoft. mokutil 1797011 Backport mokutil 0.3.0+1538710437.fb6250f-0ubuntu2 to 18.04. parted 1798675 debian/patches/Read-NVMe-model-names-from-sysfs.patch: Expose NVMe model names when available instead of the generic \"NVMe Device\" string. kmod 1786574 Remove i2c_i801 from d/modprobe.d/blacklist.conf. ubuntu-drivers-common 1797147 Improve pid detection, and restore the default pci power control profile in performance mode. nvidia-prime 1797147 Give udev the time to add the drm device. Fixes a race condition that causes problems when using lightdm and sddm. shim-signed 1726803 Don't fail non-interactive upgrade of nvidia module and module removals linux-oem 1800770 Thunderbolt runtime D3 and PCIe D3 Cold support linux-azure 1722226 linux-azure: fix systemd ADT test failure grub2 1784363 debian/default/grub.md5sum: add entry for 2.02-2ubuntu8.7; to force an update of /etc/default/grub back to the correct timeout value of 0 if the file has otherwise not been edited by the user. grub2 1788727 debian/grub-check-signatures: Handle the case where we have unsigned vmlinuz and signed vmlinuz.efi.signed. grub2-signed 1784363 Rebuild against grub2 2.02-2ubuntu8.9 grub2-signed 1788727 Rebuild against grub2 2.02-2ubuntu8.9 bolt 1798014 New upstream version bolt 1800715 use -Dprivileged-group=sudo, the admin group is not 'wheel' for us linux-aws 1801305 Restore request-based mode to xen-blkfront for AWS kernels fwupd 1791999 1768627 1719797 Restrict libsmbios-dev to x86 architectures linux-gcp-edge 1796647 Shared folders cannot be mounted in ubuntu/cosmic64 due to missing vbox modules open-iscsi 1802354 d/iscsi-disk.rules, d/rules: Add a udev rule so that iscsid.service will be started when iscsi disks are attached. usbmuxd 1778767 backport some fixes for missing udev events on new kernel which were leading to the service not restarting after disconnecting and reconnecting a device. Thanks Leo Soares for suggesting a first version of the backport hwdata 1755490 Change PNP vendor name for GSM to LG Electronics open-iscsi 1807978 debian/iscsi-disk.rules: Fix bug with LVM on top of iscsi devices. open-iscsi 1806777 debian/extra/initramfs.local-top: handle iSCSI iBFT DHCP to correctly run ipconfig to gather all DHCP config info, including DNS search domain, which iBFT can't provide. lxd 1804876 New upstream bugfix release e2fsprogs 1807288 debian/patches/0001-libext2fs-fix-regression-so-we-are-correctly- transla.patch: cherry-pick upstream fix so we are correctly translating acls in mkfs.ext4. Closes initramfs-tools 1802591 scripts/functions: include a new option to skip enslaved network devices. Include the new variable NETWORK_SKIP_ENSLAVED. When set to a value different than \"0\", this variable will cause any enslaved network devices to be skipped from the list of netbootable devices. This variable can be set via the configuration files under /etc/initramfs-tools/ or via any configuration file under the initrd directory /conf/conf.d/ via a hook script. rdma-core 1794825 Drop to avoid issues with the sysV to systemd wrapper starting the service instead of the socket linux-oracle 1802591 Skip enslaved devices during boot linux-firmware 1808528 Update linux-firmware in bionic for 18.10 hwe kernel util-linux 1784347 debian/patches/support_alternate_partition_sep.patch: support alternate partition separators. Common cases of no separator, using \"p\" only, and \"-part\" (the only previously supported), are covered now, which should let fdisk show partitions in a way closer to reality on most systems. Patch from Kyle Mahlkuch, with changes by Karel Zak. irqbalance 1811655 Added aarch64-log-spam preventing syslog spam on small aarch64 systems which lack a PCI bus (e.g. rpi3b, rpi3b+). flash-kernel 1764491 Add Raspberry Pi 3 Model B+ to the db. flash-kernel 1811216 Modify the Pi 3 boot.scr addresses to fit a bigger kernel, prepare separate versions for armhf and arm64. linux-meta-hwe 1798352 linux-snapdragon: missing meta packages for this flavour livecd-rootfs 1805668 More changes for raspi3 build support: livecd-rootfs 1805668 Another batch of cherry-picks for raspi3 support u-boot 1805668 Backport to bionic. grub2 1789918 debian/grub-check-signatures: check kernel signatures against keys known in firmware, in case a kernel is signed but not using a key that will pass validation, such as when using kernels coming from a PPA. grub2 1812863 debian/patches/mkconfig_leave_breadcrumbs.patch: make sure grub-mkconfig leaves a trace of what files were sourced to help generate the config we're building. grub2 1800722 debian/patches/quick-boot-lvm.patch: If we don't have writable grubenv and we're on EFI, always show the menu. Closes linux 1813663 External monitors does not work anymore 4.15.0-44 thermald 1803360 Honor ACPI _CRT for processor thermal zone There are some new fanless platforms use DPTF's virtual sensor instead of INT340X devices. Because of that, the _PSV is no longer in use, at least not directly, hence its value may set higher then _CRT. To a fanless system that means no cooling device gets activated before _CRT, so the system will be considered overheated by Linux kernel, and gets shutdown by the kernel. Upstream fixes: kmod 1802689 Add i2c_i801 back to d/modprobe.d/blacklist.conf again due to regressions. e2fsprogs 1798562 d/patches/0001-resize2fs-update-checksums-in-the-extent-tree-s-relo.patch: do the checksum update later in extent tree relocated block to denote the inode number change, otherwise the checksum update might be done in the old copy of the block. linux-oem 1813663 External monitors does not work anymore 4.15.0-44 linux-oem 1811777 Fix non-working pinctrl-intel linux-oem 1811929 Fix not working Goodix touchpad linux-hwe 1814555 Ubuntu boot failure. 4.18.0-14 boot stalls. (does not boot) linux-hwe 1813873 Userspace break as a result of missing patch backport grub2 1814575 debian/grub-check-signatures: properly account for DB showing as empty on some broken firmwares: Guard against mokutil --export --db failing, and do a better job at finding the DER certs for conversion to PEM format. grub2 1401532 debian/patches/linuxefi_disable_sb_fallback.patch: Disallow unsigned kernels if UEFI Secure Boot is enabled. If UEFI Secure Boot is enabled and kernel signature verification fails, do not boot the kernel. Patch from Linn Crosetto. grub2 1814403 debian/patches/quick-boot-lvm.patch: checking the return value of 'lsefi' when the command doesn't exist does not do what's expected, so instead check the value of $grub_platform which is simpler anyway. grub2-signed 1401532 Rebuild against grub2 2.02-2ubuntu8.11. Unsorted changes python-wadllib 1729754 Fix MIME encoding of binary parts. guile-2.0 1780996 Convert triggers to noawait (Closes: #903915) dpdk 1784816 Make DPDK LTS release available in Bionic open-vm-tools 1784638 Merge with Upstream Tag stable-10.3.0 from https://github.com/vmware/open-vm-tools/releases/tag/stable-10.3.0 Remaining changes: gpgme1.0 1762384 Make debian/libgpgme-dev.links executable, it uses dh-exec packagekit 1722185 Fix debconf interaction base-files 1134036 Install locale-check command to /usr/bin and invoke it from /etc/profile.d/01-locale-fix.sh to ensure locale related environment variables are set to valid values. cryptsetup 1651818 Apply patch from Trent Nelson to fix cryptroot-unlock for busybox compatibility. clamav 1792051 debian/clamav-daemon.config.in: fix infinite loop during dpkg-reconfigure python3.6 1792143 SRU: Update Python 3.6 to the recent subminor release. python3.6 1768644 Don't inject dpkg's compiler specs into distutils. python-pylxd 1775238 d/control: Add python(3)-requests-toolbelt to python(3)-pylxd Depends and python3-requests-unixsocket to python3-pylxd Depends. Also set min version of toolbelt globally. appstream 1792537 cache-explicit-variants.patch: fix crash when upgrading from bionic to cosmic gcc-defaults 1769657 SRU: Bump GCC 8 based versions to 8.2.0. gcc-8-cross 1769657 SRU: Build using gcc 8.2.0-1ubuntu2 gcc-8 1769657 SRU: Update the package to the current version in cosmic. gcc-7 1769657 SRU: Update the package to the current version in cosmic. gcc-7 1721355 Update the gcc-foffload-default patch. cross-toolchain-base 1769657 SRU: gcc-7-cross 1769657 SRU: Build using 7.3.0-18ubuntu0.1. binutils 1769657 SRU: binutils 1763098 Fix PR gprof/23056, memory corruption in gprof. binutils 1763096 Fix PR binutils/23054, memory corruption in as. binutils 1763094 Fix PR ld/23055, memory corruption in ld. plymouth 1767918 debian/patches/git_ensure_tty_closed_0a662723.patch: ensure tty is closed on deactivate. apparmor 1788929 disallow writes to thumbnailer dir apparmor 1794848 disallow access to the dirs of private files apturl 1338482 really* work golang-1.10 1794395 Backport to 18.04. man-db 1785414 Backport seccomp sandbox improvements from 2.8.4: packagekit 1790613 Pass --no-restart-after-upgrade to dh_installsystemd to avoid PackageKit restarting while upgrading under PackageKit packagekit 1795614 debian/patches/frontend-locking.diff: Implement frontend locking in a simple way. Will need some more work to upstream, and possibly some error checking. packagekit 1790671 debian/patches/aptcc-Fix-invalid-version-dereference-in-AptInf-prov.patch, aptcc-removing-duplicate-delete-call.patch: Fix invalid dereference and delete wrong (duplicate) \"delete\" statement in providesCodec apt 1796808 Set DPKG_FRONTEND_LOCKED when running {pre,post}-invoke scripts. Some post-invoke scripts install packages, which fails because the environment variable is not set. This sets the variable for all three kinds of scripts {pre,post-}invoke and pre-install-pkgs, but we will only allow post-invoke at a later time. apt 1787120 Support records larger than 32kb in 'apt show' (Closes: #905527) apt 1781169 Add support for dpkg frontend lock (Closes: #869546) apt 1794957 http: Stop pipeline after close only if it was not filled before apt 1794053 pkgCacheFile: Only unlock in destructor if locked before python-apt 1795407 Frontend locking and related locking improvements dpkg 1796081 Apply patch from upstream to add frontend locking: distro-info-data 1800656 Add Ubuntu 19.04 Disco Dingo. valgrind 1781128 Apply post 3.13 PPC64 related patches. clamav 1783632 SECURITY REGRESSION: clamav-daemon fails to start due to options removed in new version and manually edited configuration file. packagekit 1552792 Correct autoremove behaviour to only autoremove packages that relate to the current transaction sosreport 1803735 d/p/dont-collect-some-tracing-instance-files.patch: sosreport 1775195 New 3.6 upstream release. major enhancements to core features and existing plugins: dh-golang 1794936 d/patches/0001-Fix-index-out-of-range-when-using-gccgo.-Closes-9072.patch: backport fix for building with gccgo from debian. python-wsme 1805125 d/control: Correct dependency on python3-netaddr for python3-wsme, avoiding needless install of Python 2. python-memcache 1802487 d/p/py2-perf.patch: Cherry pick fix to use native C implementation for pickle under Python 2, resolving issues with performance degradation. tmux 1766942 d/p/tmux-pane-border-status-leak.patch: Fixed memory leak in screen_redraw_make_pane_status (upstream fix). livecd-rootfs 1805497 Include snaps in image manifests debootstrap 1773496 For (Ubuntu) releases disco+ default to MERGED_USR=yes, -k extract option. tar 1809827 backport \"Fix the --add-file option.\" upstream commit, thanks Martin Vogt llvm-toolchain-7 1798597 Backport to bionic for 18.04.2 HWE stack update. systemd 1811471 d/p/resolve-enable-EDNS0-towards-the-127.0.0.53-stub-res.patch getaddrinfo() failures when fallback to dns tcp queries, so enable edns0 in resolv.conf systemd 1804487 d/p/resolved-Increase-size-of-TCP-stub-replies.patch dns failures with edns0 disabled and truncated response BionicBeaver/ReleaseNotes/ChangeSummary/18.04.2 (last edited 2019-02-14 15:43:41 by sil2100) ",
          "I for one would very much like to see this bug in 18.04 fixed as a priority:<p><a href=\"https://bugs.launchpad.net/ubuntu/+source/openjdk-lts/+bug/1796027\" rel=\"nofollow\">https://bugs.launchpad.net/ubuntu/+source/openjdk-lts/+bug/1...</a><p>The Java 11 package installs Java 10. This was supposed to be a short term hack (because Ubuntu's long term supported version 18.04 was being released shortly before Java's long term supported version 11) but it's been a good while now - six months or so.<p>The short term support version 18.10 of Ubuntu has Java 11 so it's very non-obvious what the blocker is.<p>To me this seems like a really poor choice. The result of installing the Java 11 package but getting Java 10 <i>clearly</i> fails the principle of least astonishment. If we can live without the fix a quarter of the way to the next LTS edition of Ubuntu then we could have lived with Java 10 as the preferred (and correctly named) package in the first place.<p>Meanwhile the bug asks us not to spam with requests for updates yet there's no suggestion of where we can go to gauge what the timescales we're up against are.<p>It definitely dents my confidence in Ubuntu as a well organised distribution.",
          "This is not an official announcement and it has not been officially declared released or ISOs updated.<p>The last official comment was it was delayed till Thursday, Feb. 14:<p><a href=\"https://lists.ubuntu.com/archives/ubuntu-release/2019-February/004694.html\" rel=\"nofollow\">https://lists.ubuntu.com/archives/ubuntu-release/2019-Februa...</a><p>Those preparing the release will update release notes and fixed bug lists (like this one) before the release."
        ],
        "story_type": ["Normal"],
        "url": "https://wiki.ubuntu.com/BionicBeaver/ReleaseNotes/ChangeSummary/18.04.2",
        "comments.comment_id": [19129170, 19131050],
        "comments.comment_author": ["dcminter", "powersj"],
        "comments.comment_descendants": [3, 1],
        "comments.comment_time": [
          "2019-02-10T18:28:31Z",
          "2019-02-11T00:09:51Z"
        ],
        "comments.comment_text": [
          "I for one would very much like to see this bug in 18.04 fixed as a priority:<p><a href=\"https://bugs.launchpad.net/ubuntu/+source/openjdk-lts/+bug/1796027\" rel=\"nofollow\">https://bugs.launchpad.net/ubuntu/+source/openjdk-lts/+bug/1...</a><p>The Java 11 package installs Java 10. This was supposed to be a short term hack (because Ubuntu's long term supported version 18.04 was being released shortly before Java's long term supported version 11) but it's been a good while now - six months or so.<p>The short term support version 18.10 of Ubuntu has Java 11 so it's very non-obvious what the blocker is.<p>To me this seems like a really poor choice. The result of installing the Java 11 package but getting Java 10 <i>clearly</i> fails the principle of least astonishment. If we can live without the fix a quarter of the way to the next LTS edition of Ubuntu then we could have lived with Java 10 as the preferred (and correctly named) package in the first place.<p>Meanwhile the bug asks us not to spam with requests for updates yet there's no suggestion of where we can go to gauge what the timescales we're up against are.<p>It definitely dents my confidence in Ubuntu as a well organised distribution.",
          "This is not an official announcement and it has not been officially declared released or ISOs updated.<p>The last official comment was it was delayed till Thursday, Feb. 14:<p><a href=\"https://lists.ubuntu.com/archives/ubuntu-release/2019-February/004694.html\" rel=\"nofollow\">https://lists.ubuntu.com/archives/ubuntu-release/2019-Februa...</a><p>Those preparing the release will update release notes and fixed bug lists (like this one) before the release."
        ],
        "id": "e8e15f1d-3249-4c76-9729-e15fb11c1d93",
        "url_text": "Contents Installation bug fixes Upgrade bug fixes Desktop fixes Server and Cloud related fixes Kernel and Hardware support updates Unsorted changesThis is a brief summary of bugs fixed between Ubuntu 18.04.1 and 18.04.2. This summary covers only changes to packages in main and restricted, which account for all packages in the officially-supported CD images; there are further changes to various packages in universe and multiverse. Some of these fixes were by Ubuntu developers directly, while others were by upstream developers and backported to Ubuntu. For full details, see the individual package changelogs. In addition to the bugs listed below, this update includes all security updates from the Ubuntu Security Notice list affecting Ubuntu 18.04 LTS that were released up to and including February 4, 2019. The last update included was USN-3871-3 (Linux kernel vulnerabilities). Installation bug fixes Updated CD images are provided with this release, including fixes for some installation bugs. (Many installation problems are hardware-specific; for those, see \"Hardware support bugs\" below.) console-setup 1762952 keyboard-configuration.config: While sourcing config files to re-seed debconf, load missing variables as empty values of same console-setup 1762952 keyboard-configuration.{config,templates}: There is no good default for layout toggling, stop pretending there is. Console users can set one with dpkg-reconfigure or editing /etc/defaults/keyboard livecd-rootfs 1302192 generate all tar files with --xattrs. livecd-rootfs 1585233 ubuntu-cpc: Reintroduce the -root.tar.xz artifact. console-setup 1788597 keyboard-configuration.config: Fix exit/return thinko livecd-rootfs 1783129 Disentangle enabling universe in the final image a little from having PREINSTALLED=true set and enable it for a live-server build. livecd-rootfs 1776891 Disable journald rate limiting in the live-server live session. initramfs-tools 1769682 scripts/functions: write netplan config files to /run/netplan for network devices configured with configure_networking. initramfs-tools 1769682 scripts/functions: add 'critical: true' parameter; requires netplan 0.36.2. initramfs-tools 1661629 Work out the kernel modules required to support ZFS filesystems and add them as necessary. ubiquity 1749289 Implement missing reboot and shutdown methods in debconf_ui ubiquity 1777900 Add systemd-resolved to oem-config.target's Wants livecd-rootfs 1792905 Ensure /lib/modules exists in root tarballs and sqashfs. initramfs-tools 1667512 b4804dd] Only sync the filesystem containing the initramfs initramfs-tools 1791959 debian/initramfs-tools.postinst: remove orphaned old-dkms initrd files in /boot. ubiquity 1789920 Mask ubiquity.service in a system installed with oem-config/enabled=true, as this prevents getty@tty1.service from running. livecd-rootfs 1805190 Include grub efi packages in manifests for uefi images. livecd-rootfs 1799773 Disable checksum generation. installation-guide 1730322 Describe HWE kernel, point release and url for supported arm64 platforms. installation-guide 1804306 Move supported arm64 server list to a wiki page to ease maintenance debian-installer 1807023 build/pkg-lists/base: add ca-certificates-udeb to enable HTTPS without d-i/allow_unauthenticated_ssl in stock initramfs image as in Debian. ubiquity 1768230 scripts/plugininstall.py: don't hard-code a resume partition in /etc/initramfs-tools/conf.d/resume at install time. In bionic and later, initramfs-tools will autodetect an appropriate resume partition at initramfs generation time, so ubiquity's resume setting is redundant and possibly wrong. debian-installer 1809021 Add HWE variants for all architectures partman-efi 1803031 check.d/efi: Make sure we block on a missing EFI partition no matter what architecture, not just for ia64. One could attempt to install on EFI x86 and will need an ESP to be able to install GRUB. ubiquity 1772374 scripts/plugininstall.py: Make sure efivars is bind-mounted when installing the bootloader. ubiquity 1803031 Automatic update of included source packages: partman-efi 71ubuntu2.2. Upgrade bug fixes These changes fix upgrade issues, smoothing the way for future upgrades to later releases of Ubuntu. ubuntu-release-upgrader 1783589 make sure that snapd is installed before trying to use it. ubuntu-release-upgrader 1783593 update the view with information regarding the progress of snaps being installed. ubuntu-release-upgrader 1783738 when checking for connectivity to the snap store use C.UTF-8 for the language so error message matching works. ubuntu-release-upgrader 1785096 Remove debs from apt's \"Dir::Cache::archives\" folder after the upgrade has completed successfully. ubuntu-release-upgrader 1766890 Upgrade libc6 before other packages to work around trigger issues ubuntu-release-upgrader 1787668 DistUpgradeQuirks.py: if ubuntu-desktop or snapd isn't in the package cache don't try and run the quirk for installing or upgrading snaps. gnome-initial-setup 1781417 Display the ubuntu welcome wizard in Unity, don't display the \"what's new\" page though since Unity didn't change ubuntu-release-upgrader 1787649 DistUpgradeController.py: If the attempt to only upgrade libc6 ends up creating an issue with apt's problem resolver try the full upgrade. This is what would have happened before the fix for bug 1766890. update-manager 1317164 Print transaction error and let the user try again applying updates update-manager 1791931 Don't ask backend to do package operations aready done. Aptdaemon cancels the transaction when asked to remove packages already removed which results the failure being shown to the user. This is unnecessary as update-manager can just filter the package operations to be done using a fresh cache and decrease the likelyhood of hitting a race condition where packages to be removed are already removed. ubuntu-release-upgrader 1797209 do-release-upgrade: do not run the release upgrade if either not all updates are installed or a reboot is required due to a libc6 upgrade. ubuntu-release-upgrader 1796940 debian/control: change ubuntu-release-upgrader-core to depend on ca-certificates unattended-upgrades 1789637 Unlock for dpkg operations with apt_pkg.pkgsystem_unlock_inner() when it is available. Also stop running when reacquiring the lock fails. Thanks to Julian Andres Klode for original partial patch unattended-upgrades 1781586 Skip rebuilding python-apt in upgrade autopkgtests. Python-apt has a new build dependency making the rebuilding as is failing and the reference handling issue is worked around in unattended-upgrades already. unattended-upgrades 1785093 Stop trying when no adjustment could be made and adjust package candidates only to lower versions unattended-upgrades 1785093 Skip already adjusted packages from being checked for readjusting. This makes it clearer that the recursion ends and can also be a bit quicker. update-manager 1072136 Keep or delete packages after looping over all of them. This prevents the resolver from changing the packages in the loop resulting in not keeping some phased packages back from being upgraded. update-manager 1795898 Stop lazy import of InstallBackends. Lazy imports made update-manager crash when an update-manager update changed the backend API and an updated incompatible backend was loaded to the not updated running update-manager process. update-manager 1790670 Cancel transaction on exit only when Cancel button is active. Also ignore exception when cancellation fails. ubuntu-release-upgrader 1799839 DistUpgrade/DistUpgradeController.py: check all the python symlinks and versions instead of the python one. Thanks to juliank for the assistance. ubuntu-release-upgrader 1799710 do-release-upgrade: Do not block release upgrades if the installable updates are ones which are not fully phased. update-manager 1787553 Add a reminder to enable Livepatch. ubuntu-release-upgrader 1797384 Set icon_name in the dialogs so that there will be an icon in gnome-shell. update-notifier 1800862 Check if a Livepatch patch has been applied during boot or before update-notifier has started. unattended-upgrades 1778219 Trigger unattended-upgrade-shutdown actions with PrepareForShutdown() Performing upgrades in service's ExecStop did not work when the upgrades involved restarting services because systemd blocked other stop/start actions making maintainer scripts time out and be killed leaving a broken system behind. Running unattended-upgrades.service before shutdown.target as a oneshot service made it run after unmounting filesystems and scheduling services properly on shutdown is a complex problem and adding more services to the mix make it even more fragile. The solution of monitoring PrepareForShutdown() signal from DBus allows Unattended Upgrade to run _before_ the jobs related to shutdown are queued thus package upgrades can safely restart services without risking causing deadlocks or breaking part of the shutdown actions. Also ask running unattended-upgrades to stop when shutdown starts even in InstallOnShutdown mode and refactor most of unattended-upgrade-shutdown to UnattendedUpgradesShutdown class. unattended-upgrades 1778219 Increase logind's InhibitDelayMaxSec to 30s. This allows more time for unattended-upgrades to shut down gracefully or even install a few packages in InstallOnShutdown mode, but is still a big step back from the 30 minutes allowed for InstallOnShutdown previously. Users enabling InstallOnShutdown node are advised to increase InhibitDelayMaxSec even further possibly to 30 minutes. unattended-upgrades 1803749 Stop using ActionGroups, they interfere with apt.Cache.clear() causing all autoremovable packages to be handled as newly autoremovable ones and be removed by default. Dropping ActionGroup usage does not slow down the most frequent case of not having anything to upgrade and when there are packages to upgrade the gain is small compared to the actual package installation. Also collect autoremovable packages before adjusting candidates because that also changed .is_auto_removable attribute of some of them. update-manager 1805118 Do not show the livepatch reminder if update-manager is running on a distribution without software-properties-gtk. update-manager 1787553 Add a reminder to enable Livepatch. software-properties 1805436 cloudarchive: Enable support for the Stein Ubuntu Cloud Archive on 18.04. shotwell 1802895 New bugfix update shotwell 1606491 Fix \"Out of memory\" issues when scrolling through large collections shotwell 1723181 direct: Fix crash when dismissing modifications libreoffice 1799230 New upstream release libreoffice-l10n 1799230 New upstream release unattended-upgrades 1806487 Start service after systemd-logind.service to be able to take inhibition lock unattended-upgrades 1806487 Handle gracefully when logind is down update-notifier 1809505 src/update-notifier.c: Don't use G_SPAWN_DO_NOT_REAP_CHILD in order to avoid zombie processes. update-manager 1798618 UpdateManager/Core/MetaRelease.py: set prompt in MetaReleaseCore so that do-release-upgrade can provide more informative error messages. update-manager 1795024 UpdateManager/Core/MetaRelease.py: set prompt in MetaReleaseCore so that do-release-upgrade can provide more informative error messages. ubuntu-release-upgrader 1795024 data/release-upgrades: Clarify documentation regarding the behavior for different Prompt settings. ubuntu-release-upgrader 1798618 do-release-upgrade: Utilize information regarding what Prompt is set to so that a more informative error message can be displayed. ubuntu-release-upgrader 1795024 do-release-upgrade: Utilize information regarding what Prompt is set to so that a more informative error message can be displayed. ubuntu-release-upgrader 1786484 DistUpgrade/DistUpgradeCache.py: When calculating free space needed for mount points don't use a negative number as the buffer. ubuntu-release-upgrader 1807043 DistUpgrade/DistUpgradeController.py: When rewriting sources.list entries check to see if the source provides packages for the release to which the upgrade is occurring. If it doesn't the entry is disabled thereby improving upgrades with PPAs. ubuntu-release-upgrader 1807032 do-release-upgrade: add a parameter to allow third party mirrors and repositories, additionally pass along the environmental variable RELEASE_UPGRADER_ALLOW_THIRD_PARTY via pkexec and sudo. ubuntu-release-upgrader 1771387 DistUpgrade/DistUpgradeCache.py: in the event there is a failure to calculate the upgrade provide information about the log files in /var/log/dist-upgrade. ubuntu-release-upgrader 1773637 DistUpgrade/xorg_fix_proprietary.py: modify how the system is checked to see if nvidia is being used, drop fglrx check since it has been deprecated. Desktop fixes These changes mainly affect desktop installations of Ubuntu, Kubuntu, Edubuntu and other Ubuntu-based systems. xorg 1768610 Rename nux config leftovers which might change the environment even when not running an unity session vte2.91 1780501 Revert the changes to revert-pcre2.patch in the previous SRU since they introduced API incompatibilies which aren't OK in an SRU. gnome-menus 1765799 Fix app menus not updating correctly after app install or removal. This patch was accidentally dropped in previous merge. gnome-shell-extension-ubuntu-dock 1712661 Fix crash connecting/disconnecting monitors gnome-shell-extension-ubuntu-dock 1784920 Fix showApps button label position gnome-control-center 1779051 Fix crash when settings changed after panel closed gnome-software 1778160 Fix crash when have plugs with multiple slots available gnome-software 1781996 don't segfault on null snap installation date gnome-shell 1718931 New upstream release gnome-shell 1782614 New upstream release gnome-shell 1784671 Don't handle key presses directly if there are modifiers mutter 1754949 Avoid crashing when warning about wrongly set crtc mode mutter 1767956 Don't crash if drmModeGetResources returns NULL mutter 1784398 Always update monitor in wayland, avoiding crash mutter 1772831 Don't return screen resolutions that can't be applied mutter 1422253 Don't crash if a modal dialog closes while being dragged mutter 1723615 Don't try to use an invalid monitor mode to figure out scaling mutter 1783311 No-change backport to bionic gnome-software 1775226 also disable offline updates in refresh plugin ubuntu-report 1786432 Include optional DCD OEM file ubuntu-report 1784383 Collect number of disks and their sizes apport 1778497 Add a remember option to whoopsie so that users can diminish crash interactions apport 1778694 Move apport autoreport service files to apport binary package. Having them in apport-noui was creating a bug where autoreport wasn't working on desktop. As we check in the launched script for whoopsie and autoreport file, we don't autoreport by default. apport-noui still touches the file to enable autoreport on install. apport 1778694 Start apport-autoreport after installing apport-noui which is part of improving apport's automatic crash reporing feature. software-properties 1770686 SoftwarePropertiesGtk.py: Hide livepatch widgets in flavors without an online account panel in gnome-control-center glib2.0 1789472 New upstream release ubuntu-settings 1782190 revert in communitheme session default font size to 11, as it creates rendering issues in GNOME Shell firefox 1791789 Fix : Mark distribution search engines as read-only, so that they are marked as hidden in rather than removed from the search engine cache when a user \"removes\" them (they can't actually be removed from disk). This stops them from reappearing on cache rebuilds firefox 1791789 Backport upstream change to the search service to not handle locale changes on shutdown. As well as resulting in en-US search engines being added to the search engine cache for all locales, it was resulting in a cache rebuild on every restart, making the above bug worse (fixes another part of ) firefox 1791789 Set \"spellchecker.dictionary_path\" by default to point to /usr/share/hunspell so that system dictionaries are loaded again, now that Firefox no longer loads them from its own install directory. Fixes another part of firefox 1791789 Final part of the fix for : Cleanup extra Amazon.com search engine in locales that have their own Amazon search engine fonts-liberation 1769654 Backport to bionic to fix skewed font metrics fonts-liberation2 1769654 Backport to bionic to fix skewed font metrics libreoffice 1785679 New upstream release libreoffice-l10n 1785679 New upstream release evolution-data-server 1784514 New upstream release apport 1791324 Handle old reports generated pre-apport with \"remember\" option. If the option isn't there, consider as false. nautilus 1782681 New upstream release: 3.26.4 nautilus 1765776 New upstream release: 3.26.4 nautilus 1767027 Follow nautilus settings to search only in current folder. This doesn't apply to gnome-shell search provider for indexed searches. nautilus 1764779 don't crash if selecting a volume that is not mounted nautilus 1713581 don't crash when try to select a file multiple times in a single run nvidia-graphics-drivers-340 1791542 Use diversions for \"libGLESv1_CM.so\" \"libGLESv1_CM.so.1\". nvidia-graphics-drivers-340 1761593 Install the modprobe file in /lib, and remove the blacklist file manually. gnome-mines 1772191 Backport fix from 3.30 for missing high resolution app icon gnome-software 1789336 Show verified developers gnome-software 1789338 Use wide scope searching gnome-software 1756379 Delay startup of service to allow the shell to load first ubuntu-themes 1773045 Remove nonexistent gnome-builder.css ubuntu-themes 1781736 Radiance: fix typo in assets link for focused buttons ubuntu-themes 1781736 Radiance: Use scaled image for buttons border ubuntu-themes 1782038 Ambiance, Radiance: don't use padding on window buttons for chromium ubuntu-themes 1758841 Ambiance: use default foreground color for toolbar menus ubuntu-themes 1743373 Ambiance, Radiance: show proper arrow in combobox ubuntu-themes 1785699 Ambiance, Radiance: use default disabled color for actions headerbar buttons ubuntu-themes 1761684 Ambiance, Radiance: fix list selected color for gnome-boxes ubuntu-themes 1795895 Ambiance, Radiance: properly theme disabled and hovered accelerators xorg-server 1789913 prime-sync-refactor.diff: Fix crash on modesetting+amdgpu hybrid. nautilus 1795028 Fix remote filesystem check on file during search nautilus 1798426 Refreshed to add memory leak and potential crash fixes gnome-settings-daemon 1797322 backport fix from upstream to resolve suspend/resume rfk issues xdg-utils 1743216 Use perl's decode() to ensure we don't pass invalid UTF-8 to D-Bus, as doing so triggers an assertion from libdbus which makes us crash. fonts-noto-color-emoji 1788256 New upstream release evince 1790609 New upstream release apport 1760220 apport/ui.py: when using ubuntu-bug properly handle executables which start with /snap/bin. apport 1780767 test/test_ui_gtk.py: Increase the timeout so that when the autopkgtest infrastructure is busy the tests should not fail. gnome-shell 1739931 use defined color for menu separators gnome-shell 1743058 set StEntry minimun height to work properly with Ubuntu font gnome-shell 1745888 Don't emit two click events on touch under X11 gnome-shell 1725312 Handle NULL scroll bars in st-scroll-view gnome-calculator 1790876 New upstream stable release mutter 1727356 Create back buffers in early intel GPU generations mutter 1730211 Fix typing capital letters when using OSD keyboard gnome-initial-setup 1789925 rework the patch from the previous upload to change the right reference this time gnome-initial-setup 1764723 Show an error message in case livepatch setup fails. This also will make sure that gnome-intial-setup does not quit before livepatch responds back. totem 1433984 backport fix for nautilus sometime crashing when closing the video properties dialog totem 1798399 backport fixes to make the gallery plugin work again desktop-file-utils 1768271 backport debian change to fix update-desktop-error with gnome-font-viewer 3.28 gnome-desktop3 1795668 Enable bubblewrap hardening for thumnbailers ghostscript 1806517 SECURITY REGRESSION: multiple regressions libgnomekbd 1721893 backport a patch available on bugzilla to fix a segfault in the keyboard layout preview code. The most common case seems to be g-c-c trying to preview a 'default' layout which was working before some other fixes landed, that fixes the regression. gnome-software 1552792 Build with PackageKit autoremove support gnome-software 1785240 Stop cancelling snapd authorization triggers error notification gnome-software 1754864 Pull related flatpak refs gnome-software 1798228 Fix snap search result ordering gnome-software 1719797 Stop reboot notification from timing out gnome-software 1798470 Support composite CAB files gnome-desktop3 1807127 Fix thumbnailer on 32-bit systems where /lib64 is not available. This fixes a regression introduced in the previous update. gedit 1800179 'document selector: make search caseless', it makes the filter in the open document dialog work in a more logical way gnome-shell 1765304 Cherry-pick upstream commit to prevent focus stealing on password fields in firefox when ibus is used libgweather 1803487 New upstream release: 3.28.2 nautilus 1804685 'preferences-window: Fix icon views captions order' software-properties 1807373 SoftwarePropertiesGtk.py: when checking a package's depends for DKMS also pass on an AttributeError gvfs 1803158 Use O_RDWR to fix fstat when writing gvfs 1798725 common: Prevent crashes on invalid autorun file gvfs 1630905 daemon: Prevent deadlock and invalid read when closing channels gvfs 1792878 workaround libsoup limitation to prevent dav lockups gvfs 1778322 smbbrowse: Force NT1 protocol version for workgroup support gvfs 1803190 smb: Add workaround to fix removal of non-empty dir gjs 1809181 New upstream release gjs 1803271 New upstream release gparted 1779292 d/patches/lvm2_prompt_patch.patch: Apply upstream patch to fix pvresize compatibility with LVM2 >= 2.02.171, which broke LVM PV resizing due to the addition of a confirmation dialogue for the resize. cups 1804576 fix-handling-of-MaxJobTime.patch: Fix handling of MaxJobTime 0 totem 1726688 Together, these patches fix the right-click .desktop actions gnome-shell-extension-ubuntu-dock 1743976 Avoid repainting an unchanging dock. gnome-shell-extension-ubuntu-dock 1769383 theming: Ensure _trackingWindows contains valid windows gnome-shell-extension-ubuntu-dock 1769383 extension: Ensure signal disconnection firefox 1808980 Build with --enable-rust-simd (except on i386 and armhf) libdrm 1798597 Backport to bionic for 18.04.2 HWE stack update. wayland 1798597 Backport for 18.04.2 HWE stack update. wayland 1781440 control: Bump Breaks/Replaces again to match reality. mesa 1798597 Backport for 18.04.2 HWE stack update. xf86-input-wacom-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xorg-server-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-input-libinput-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-amdgpu-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-ati-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-vesa-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-vmware-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-nouveau-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-fbdev-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-qxl-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-dummy-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xorg-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. cairo 1560286 \"Revert \"Correctly decode Adobe CMYK JPEGs in PDF export\" From further testing and investigation it appears that many PDF viewers already have a workaround to invert Adobe CMYK JPEGs, so our generated PDFs display incorrectly with those viewers due to double-inversion. xserver-xorg-video-intel-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. These changes mainly affect installations of Ubuntu on server systems and clouds. keepalived 1744062 d/p/fix-removing-left-over-addresses-if-keepalived-abort.patch: Cherry-picked from upstream to ensure left-over VIPs and eVIPs are properly removed on restart if keepalived terminates abonormally. This fix is from the upstream 1.4.0 release. cloud-init 1784685 cherry-pick 3cee0bf8: oracle: fix detect_openstack to report True on cloud-init 1777912 New upstream snapshot. cloud-init 1770712 debian/rules: update version.version_string to contain packaged version. cloud-init 1777912 New upstream release. aodh 1776375 Support same projects in different domain maas 1773201 Stable Release Update. New upstream release, MAAS 2.4.0 freeipmi 1784926 Cherry-pick patches from upstream that add support to ipmi-locate for parsing ACPI/SPMI tables out of sysfs: rax-nova-agent 1783614 New upstream version 2.1.15 libvirt 1788226 Fix an issue where guests with plenty of hostdevs attached where detected as not shut down due to the kernel needing more time to free up resources apache2 1750356 d/p/balance-member-long-hostname-part{1,2}.patch: Provide an RFC1035 compliant version of the hostname in the proxy_worker_shared structure. A hostname that is too long is no longer a fatal error. chrony 1787366 d/p/lp-1787366-fall-back-to-urandom.patch: avoid hangs when starting the service on newer kernels by falling back to urandom. avahi 1752411 debian/avahi-daemon-check-dns.sh: On some hardware, the 'host' command gets stuck and does not timeout as it should leaving this script and boot-up hanging indefinitely. Launch host with 'timeout' to kill it after 5 seconds in these cases as a workaround. unbound 1788622 d/p/lp-1788622-fix-systemd-reload.patch: Fix hang due to all worker threads stopping on reload maas 1788641 Stable Release Update. New upstream release, MAAS 2.4.2 postfix 1753470 debian/patches/fix-postconf-segfault.diff: Fix a postconf segfault when map file cannot be read. Thanks to Viktor Dukhovni <postfix- users@dukhovni.org>. qemu 1755912 d/p/lp-1755912-qxl-fix-local-renderer-crash.patch: Fix an issue triggered by migrations with UI frontends or frequent guest resolution changes qemu 1787408 d/p/ubuntu/target-ppc-extend-eieio-for-POWER9.patch: Backport to extend eieio for POWER9 emulation. rax-nova-agent 1788716 Use DefaultDependencies=no to prevent forming service ordering cycles openjdk-lts 1788250 debian/rules: by default leave atk disabled, move accessibility bridge to recommends. openjdk-lts 1788267 debian/rules: by default leave atk disabled, move accessibility bridge to recommends. openvpn 1787208 d/openvpn@.service: Add CAP_AUDIT_WRITE to avoid issues with callout scripts breaking due to sudo/pam being unable to audit the action. Fixed in upstream issue #918, suggested to Debian in #868806 libvirt 1788603 d/p/ubuntu-aa/lp-1788603-fix-ptrace-rules-with-kernel-4.18.patch: avoid issues with newer kernels >=4.18 libvirt 1789659 d/p/ubuntu/lp-1789659-don-t-check-for-parallel-iteration-in-hash.patch: remove broken and redundant check for parallel iteration in hash functions nova-lxd 1789427 New upstream stable point release for OpenStack Queens. nginx 1782226 Stable Release Update. Do not attempt to start nginx if other daemon is binding to port 80, to prevent install failure: open-vm-tools 1791220 d/p/ubuntu/lp-1791220-Disable-hgfsServer-not-VMware.patch: avoid crashing with segfaults when force starting the service in non VMWare environments. open-vm-tools 1790145 d/p/debian/scsi-udev-rule: fix applying of the scsi timeout horizon 1790189 d/theme/ubuntu/_styles.scss: Ensure btn-danger rules are preferred when used with a, a:link, a:visited and dropdown-menu.li. nova 1761140 d/control: Drop circular dependencies. nova-compute depends on nova-compute-* packages. nova-compute-* packages shouldn't depend on nova-compute. nova-compute-* should however depend on nova-common.. cloud-initramfs-tools 1792905 copymods: Take ownership of lib/modules squid3 1792728 d/usr.sbin.squid: Update apparmor profile to grant read access to squid binary webkit2gtk 1795901 Install missing headers lxcfs 1788232 New upstream bugfix release: lxc 1788457 New upstream bugfix release: snapd-glib 1785240 Support auth cancelled error snapd-glib 1789338 Support wide scope searches snapd-glib 1789336 Support publisher information snapd-glib 1789335 Support refresh information networkd-dispatcher 1797884 Allow overriding /usr/lib scripts in /etc/networkd-dispatcher. Replaces our patch to use /usr/lib/networkd-dispatcher with the solution contributed upstream that has a search path with both /usr/lib/ and /etc locations. qemu 1790901 Update pxe netboot images for KVM s390x to qemu 3.0 level The SLOF source pieces in src:qemu are only used for s390x netboot, which are independent ROMs (no linking). All other binaries out of this are part of src:slof and independent. cloud-init 1795953 New upstream release. horizon 1778771 d/p/add-enabled-check-in-backups-panel.patch: Cherry-picked from https://review.openstack.org/#/c/605994/ to ensure Volume Backups panel is disabled if enable_backup is False. lxd 1788280 New upstream bugfix release: lxd 1788314 Temporarily disable ZFS tests on s390x due to conflict between zfsutils-linux and s390-tools netplan.io 1770082 Fix typo breaking rename on 'netplan apply'. netplan.io 1793309 Backport netplan 0.40.1 to 18.04. netplan.io 1795343 Deal gracefully with empty files on 'netplan apply' netplan.io 1786726 Don't render ipv4 dns-search unless we have an ipv4 address. netplan.io 1736965 Set permissive umask on networkd .network, .link and .netdev files netplan.io 1768560 Set permissive umask on networkd .network, .link and .netdev files netplan.io 1747455 Fix support for link-scope routes. netplan.io 1756701 Spell Gratuitous ARP correctly and make it work. netplan.io 1783940 Many typo fixes for documentation. netplan.io 1771704 Allow link-local addresses to be configured. netplan.io 1736975 Forces bridges with no addresses to be brought online. netplan.io 1770082 Write udev .rules files to /run/udev/rules.d to enforce interface renaming. netplan.io 1768823 Don't traceback for 'netplan ip leases' when iface is not managed or doesn't DHCP netplan.io 1771440 Fix duplicate \"/\" path separator in error messages netplan.io 1768798 Fix incorrect terminal reset in 'netplan try' on Ctrl-C. netplan.io 1768783 Updated doc entries: mtu, fix fwmark->mark, cleanup optional. netplan.io 1770082 Generate udev rules files to rename devices Due to a systemd issue[1], using link files to rename interfaces doesn't work as expected. Link files will not rename an interface if it was already renamed, and interfaces are renamed in initrd, so set-name will often not work as expected when rebooting. However, rules files will cause a renaming, even if the interface has been renamed in initrd. amavisd-new 1792293 d/p/100_more_amavisd_helpers_fixes: Fix Debian/Ubuntu pathing in amavisd-release amavisd-new 1770532 d/p/105_amavisd_fix_originating_dkim_signing.patch: Fix DKIM signing in 2.11.0 nova 1795424 New stable point release for OpenStack Queens. heat 1795424 New stable point release for OpenStack Queens. keystone 1795424 New stable point release for OpenStack Queens. python-openstackclient 1800490 New stable point release for OpenStack Queens. open-vm-tools 1793219 d/p/lp-1793219-fix-stats-overflow.patch: fix potential overflow of 32 bit /proc values bind9 1769440 d/p/skip-rtld-deepbind-for-dyndb.diff: fix named-pkcs11 crashing on startup. Thanks to Petr Menk <pemensik@redhat.com> neutron 1795424 New stable point release for OpenStack Queens. neutron 1790598 d/p/metadata-use-requests-for-comms-with-nova-api.patch: Cherry-picked from https://review.openstack.org/#/c/599541/ to enable cert management where IP addresses are used in subject alternate names. openldap 1783183 d/apparmor-profile: update apparmor profile to allow reading of files needed when slapd is behaving as a kerberos/gssapi client and acquiring its own ticket. samba 1795772 d/p/fix-rmdir.patch: fix the patch to not apply with offset, which previously made it change the wrong, almost identical, function. samba 1795772 d/p/fix-rmdir.patch: Fix to make smbclient report directory-not-empty errors ca-certificates-java 1770553 Backport from Cosmic. ca-certificates-java 1771815 Merge from Debian unstable. Remaining changes: ca-certificates-java 1771363 debian/postinst.in: Detect PKCS12 cacert keystore generated by previous ca-certificates-java and convert them to JKS. ca-certificates-java 1769013 Merge from debian unstable. Remaining changes: ca-certificates-java 1739631 Merge from debian unstable. Remaining changes: apache2 1782806 d/debhelper/apache2-maintscript-helper: fix typo in apache2_switch_mpm()'s a2query call. openjdk-lts 1800792 debian/patches/sec-webrev-11.0.1-b21-S8211731.patch: apply missing patch from the security update. bubblewrap 1795668 Don't install setuid on Ubuntu & derivatives since Ubuntu's kernel enables unprivileged user namespaces ubuntu-keyring 1798073 Validate that all shipped fragments are signed. netplan.io 1802322 Fix idempotency in renaming: bond members should be exempt from rename, as they may all share a single MAC for the bond device. netplan.io 1770082 Fix typo breaking rename on 'netplan apply'. netplan.io 1793309 Backport netplan 0.40.1 to 18.04. netplan.io 1795343 Deal gracefully with empty files on 'netplan apply' netplan.io 1786726 Don't render ipv4 dns-search unless we have an ipv4 address. netplan.io 1736965 Set permissive umask on networkd .network, .link and .netdev files netplan.io 1768560 Set permissive umask on networkd .network, .link and .netdev files netplan.io 1747455 Fix support for link-scope routes. netplan.io 1756701 Spell Gratuitous ARP correctly and make it work. netplan.io 1783940 Many typo fixes for documentation. netplan.io 1771704 Allow link-local addresses to be configured. netplan.io 1736975 Forces bridges with no addresses to be brought online. netplan.io 1770082 Write udev .rules files to /run/udev/rules.d to enforce interface renaming. netplan.io 1768823 Don't traceback for 'netplan ip leases' when iface is not managed or doesn't DHCP netplan.io 1771440 Fix duplicate \"/\" path separator in error messages netplan.io 1768798 Fix incorrect terminal reset in 'netplan try' on Ctrl-C. netplan.io 1768783 Updated doc entries: mtu, fix fwmark->mark, cleanup optional. rax-nova-agent 1804445 New upstream version 2.1.18 rax-nova-agent 1788716 Use DefaultDependencies=no to prevent forming service ordering cycles libvirt 1801666 d/control: explicitly Build-dep on libwiretap-dev to fix FTBFS since libwireshark 2.6.x SRU upload exim4 1786508 d/p/eximstats_unitialized_value.patch: Fix uninitialized value error in eximstats. postfix 1791139 d/postfix-{cdb,ldap,lmdb,mysql,pcre,pgsql}.postinst, d/postfix.postinst: Handle empty alias_database field in main.cf lxc 1804755 New upstream bugfix release: lxcfs 1804753 New upstream bugfix release: barbican 1806043 New stable point release for OpenStack Queens. neutron-fwaas 1806043 New stable point release for OpenStack Queens. keystone 1806043 New stable point release for OpenStack Queens. nova 1744079 d/p/disk-size-live-migration-overcommit.patch: Cherry-picked from https://review.openstack.org/#/c/602478 to ensure proper disk calculation during live migration with over-commit. nova 1806043 New stable point release for OpenStack Queens. horizon 1802226 d/openstack-dashboard.postinst: Ensure that if memcached is installed it is restarted in post-install script after collecting/compressing static assets, enabling refresh of memcached static assets after upgrade. psmisc 1806060 fix killall option parsing walinuxagent 1799498 New upstream release. ceph 1796645 New upstream point release. cinder 1806043 New stable point release for OpenStack Queens. netplan.io 1811210 No change rebuild in -security pocket to fix dependency issue samba 1801227 d/p/auth-fail-eexist.diff: smbc_opendir should not return EEXIST with invalid login credentials. Thanks to David Mulder. network-manager-applet 1807460 'libnma: unescape certificate paths in URIs', that fixes selecting files with a space in their name openvswitch 1780752 New upstream point release: sssd 1807246 d/p/fix-id-out-of-range-lookup.patch: CACHE_REQ: Do not fail the domain locator plugin if ID outside the domain range is looked up. Thanks to Jakub Hrozek <jhrozek@redhat.com>. sssd 1793882 d/t/common-tests, d/t/control, d/t/ldap-user-group-krb5-auth, d/t/ldap-user-group-ldap-auth, d/t/login.exp, d/t/util: add DEP8 tests for kerberos and LDAP deja-dup 1804744 Fix bug preventing restore on a fresh install if you don't also set your current backup location to the old backup. snapd 1811233 New upstream release, snapd 1779403 New upstream release, snapd 1814355 disable systemd environment generator on bionic to fix ceph 1813582 d/p/lp1813582.patch: Cherry pick fix for crash in rados py binding under gnocchi (LP: #1813582). libmemcached 1573594 Fix missing null termination in PROTOCOL_BINARY_CMD_SASL_LIST_MECHS response handling (LP: #1573594) haproxy 1804069 d/p/stksess-align.patch: Make sure stksess is properly aligned. (LP: #1804069) landscape-client 1788219 debian/patches/nutanix-kvm.patch: Update vm_info.py to include Nutanix hypervisor. (LP: #1788219) landscape-client 1670291 debian/patches/release-upgrade-success.patch: Enable landscape-client to survive trusty upgrade. (LP: #1670291) landscape-client 1670291 debian/patches/post-upgrade-reboot.patch: Force reboot operation in case systemd fails. (LP: #1670291) landscape-client 1765518 debian/patches/unicode-tags-script.patch: Permit environments containing unicode chars for script execution. (LP: #1765518) landscape-client 1616116 debian/patches/1616116-resync-loop.patch: Clear hash id database on package resync. (LP: #1616116) Kernel and Hardware support updates Considerable work has been done in Ubuntu 18.04.2 on improving support for many specific items of hardware. intel-microcode 1778738 Default to early instead of auto, and install all of the microcode, not just the one matching the current CPU, if MODULES=most is set in the initramfs-tools config amd64-microcode 1778738 Default to 'early' instead of 'auto' in the initramfs-tools hook when building with MODULES=most lshw 1752523 d/patches/lshw-fix-unknown-version-issue.patch: Cherry pick from upstream. open-iscsi 1785108 d/net-interface-handler: Apply changes only for the iscsi-root grub2 1786491 Verify that the current and newer kernels are signed when grub is updated, to make sure people do not accidentally shutdown without a signed kernel. grub2-signed 1786491 Rebuild against grub2 2.02-2ubuntu8.3 and check kernel is signed on amd64 EFI before installing grub. linux-meta-gcp 1780923 linux-gcp: add a meta package for the extra modules linux-oem 1781895 Bluetooth: Redpine: Bionics: L2test transfer is failed to start in Ubuntu 18.04 linux-oem 1782070 Redpine] Upgrades to improve throughput and stability grub2-signed 1785859 Rebuild against grub2 2.02-2ubuntu8.4 grub2 1785859 debian/patches/ofnet-init-structs-in-bootpath-parser.patch: initialize structs in bootpath parser. Fixes netboot issues on ppc64el. gnu-efi 1790709 New upstream version 3.0.8. shim-signed 1790724 Backport shim-signed 1.37 to Ubuntu 18.04. shim-signed 1778848 debian/shim-signed.postinst: use --auto-nvram with grub-install in case we're installing on a NVRAM-unavailable platform. shim-signed 1778848 debian/control: bump the dependency for grub2-common to make sure grub-install supports --auto-nvram. shim-signed 1778848 debian/control: switch the grub-efi-amd64-bin dependency to grub-efi-amd64-signed. ipxe 1789319 Build ROMs for QEMU with CONFIG=qemu libglvnd 1782285 rules, libgles2: Add GLESv1 support. libglvnd 1780039 Always return an error from eglMakeCurrent if EGLDisplay is invalid. bolt 1786265 New upstream version bolt 1778020 should resolve issues with devices not being authorized on initial boot e.g in lightdm open-iscsi 1755858 make iscsid socket activated to only activate it as-needed ubuntu-drivers-common 1789201 Start before oem-config.service. open-iscsi 1791108 d/net-interface-handler: replace 'domainsearch' with the correct configuration option 'search' in net-interface-handler shim-signed 1792575 debian/control: add Breaks: grub-efi-amd64-signed (<< 1.93.7), as the new version of shim exercises a bug in relocation code for chainload that was fixed in that upload of grub, affecting Windows 7, Windows 10, and some netboot scenarios where chainloading is required. shim-signed 1790724 Backport shim-signed 1.37 to Ubuntu 18.04. grub2-signed 1792575 Rebuild against grub2 2.02-2ubuntu8.6 grub2-signed 788298 Rebuild against grub2 2.02-2ubuntu8.5 grub2 1792575 debian/patches/linuxefi_fix_relocate_coff.patch: fix typo in relocate_coff() causing issues with relocation of code in chainload. grub2 1792575 debian/patches/linuxefi_truncate_overlong_reloc_section.patch: The Windows 7 bootloader has inconsistent headers; truncate to the smaller, correct size to fix chainloading Windows 7. grub2 788298 debian/patches/grub-reboot-warn.patch: Warn when \"for the next boot only\" promise cannot be kept. util-linux 1783810 Use getrandom() with GRND_NONBLOCK to avoid hangs in early boot when e.g. the partition is resized. Cherry picked from upstream. skiboot 1785026 d/opal-prd.logrotate: fix ownership of /var/log/opal-prd.log. friendly-recovery 1766872 Cleanup lintian warnings. linux 1796542 Silent data corruption in Linux kernel 4.15 linux 1789746 getxattr: always handle namespaced attributes linux 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux 1791569 some nvidia p1000 graphic cards hang during the boot linux 1783746 ipmmu is always registered linux 1794889 Bionic update: upstream stable patchset 2018-09-27 linux-kvm 1796542 Silent data corruption in Linux kernel 4.15 linux-kvm 1793841 IP_SET modules not included in kernel build, prevents container functionality linux-kvm 1789746 getxattr: always handle namespaced attributes linux-kvm 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux-kvm 1791569 some nvidia p1000 graphic cards hang during the boot linux-kvm 1783746 ipmmu is always registered linux-kvm 1794889 Bionic update: upstream stable patchset 2018-09-27 gdm3 1780076 Add utils-add-new-gdm-disable-wayland-binary.patch, cherry-picked from cosmic. linux-aws 1796542 Silent data corruption in Linux kernel 4.15 linux-aws 1794175 kyber-iosched module missing from linux-modules package linux-aws 1789746 getxattr: always handle namespaced attributes linux-aws 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux-aws 1791569 some nvidia p1000 graphic cards hang during the boot linux-aws 1783746 ipmmu is always registered linux-aws 1794889 Bionic update: upstream stable patchset 2018-09-27 linux-gcp 1796542 Silent data corruption in Linux kernel 4.15 linux-gcp 1789746 getxattr: always handle namespaced attributes linux-gcp 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux-gcp 1791569 some nvidia p1000 graphic cards hang during the boot linux-gcp 1783746 ipmmu is always registered linux-gcp 1794889 Bionic update: upstream stable patchset 2018-09-27 linux-oem 1796542 Silent data corruption in Linux kernel 4.15 grub2 1785033 debian/patches/0001-i386-linux-Add-support-for-ext_lfb_base.patch: Add support for ext_lfb_base. grub2-signed 1785033 Rebuild against grub2 2.02-2ubuntu8.7 linux-azure 1798185 Enable CONFIG_INFINIBAND_USER_MAD linux-azure 1798185 Enable CONFIG_INFINIBAND_USER_MAD linux-azure 1796542 Silent data corruption in Linux kernel 4.15 linux-azure 1789746 getxattr: always handle namespaced attributes linux-azure 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux-azure 1791569 some nvidia p1000 graphic cards hang during the boot linux-azure 1783746 ipmmu is always registered linux-azure 1794889 Bionic update: upstream stable patchset 2018-09-27 s390-tools 1777600 zdev: Adjust zdev modprobe path to be compatible with split-usr systems. s390-tools 1794308 zdev: Trigger generic_ccw devices on any kernel module loads. secureboot-db 1776996 Backport secureboot-db from cosmic to apply the August 2016 dbx updates from Microsoft. mokutil 1797011 Backport mokutil 0.3.0+1538710437.fb6250f-0ubuntu2 to 18.04. parted 1798675 debian/patches/Read-NVMe-model-names-from-sysfs.patch: Expose NVMe model names when available instead of the generic \"NVMe Device\" string. kmod 1786574 Remove i2c_i801 from d/modprobe.d/blacklist.conf. ubuntu-drivers-common 1797147 Improve pid detection, and restore the default pci power control profile in performance mode. nvidia-prime 1797147 Give udev the time to add the drm device. Fixes a race condition that causes problems when using lightdm and sddm. shim-signed 1726803 Don't fail non-interactive upgrade of nvidia module and module removals linux-oem 1800770 Thunderbolt runtime D3 and PCIe D3 Cold support linux-azure 1722226 linux-azure: fix systemd ADT test failure grub2 1784363 debian/default/grub.md5sum: add entry for 2.02-2ubuntu8.7; to force an update of /etc/default/grub back to the correct timeout value of 0 if the file has otherwise not been edited by the user. grub2 1788727 debian/grub-check-signatures: Handle the case where we have unsigned vmlinuz and signed vmlinuz.efi.signed. grub2-signed 1784363 Rebuild against grub2 2.02-2ubuntu8.9 grub2-signed 1788727 Rebuild against grub2 2.02-2ubuntu8.9 bolt 1798014 New upstream version bolt 1800715 use -Dprivileged-group=sudo, the admin group is not 'wheel' for us linux-aws 1801305 Restore request-based mode to xen-blkfront for AWS kernels fwupd 1791999 1768627 1719797 Restrict libsmbios-dev to x86 architectures linux-gcp-edge 1796647 Shared folders cannot be mounted in ubuntu/cosmic64 due to missing vbox modules open-iscsi 1802354 d/iscsi-disk.rules, d/rules: Add a udev rule so that iscsid.service will be started when iscsi disks are attached. usbmuxd 1778767 backport some fixes for missing udev events on new kernel which were leading to the service not restarting after disconnecting and reconnecting a device. Thanks Leo Soares for suggesting a first version of the backport hwdata 1755490 Change PNP vendor name for GSM to LG Electronics open-iscsi 1807978 debian/iscsi-disk.rules: Fix bug with LVM on top of iscsi devices. open-iscsi 1806777 debian/extra/initramfs.local-top: handle iSCSI iBFT DHCP to correctly run ipconfig to gather all DHCP config info, including DNS search domain, which iBFT can't provide. lxd 1804876 New upstream bugfix release e2fsprogs 1807288 debian/patches/0001-libext2fs-fix-regression-so-we-are-correctly- transla.patch: cherry-pick upstream fix so we are correctly translating acls in mkfs.ext4. Closes initramfs-tools 1802591 scripts/functions: include a new option to skip enslaved network devices. Include the new variable NETWORK_SKIP_ENSLAVED. When set to a value different than \"0\", this variable will cause any enslaved network devices to be skipped from the list of netbootable devices. This variable can be set via the configuration files under /etc/initramfs-tools/ or via any configuration file under the initrd directory /conf/conf.d/ via a hook script. rdma-core 1794825 Drop to avoid issues with the sysV to systemd wrapper starting the service instead of the socket linux-oracle 1802591 Skip enslaved devices during boot linux-firmware 1808528 Update linux-firmware in bionic for 18.10 hwe kernel util-linux 1784347 debian/patches/support_alternate_partition_sep.patch: support alternate partition separators. Common cases of no separator, using \"p\" only, and \"-part\" (the only previously supported), are covered now, which should let fdisk show partitions in a way closer to reality on most systems. Patch from Kyle Mahlkuch, with changes by Karel Zak. irqbalance 1811655 Added aarch64-log-spam preventing syslog spam on small aarch64 systems which lack a PCI bus (e.g. rpi3b, rpi3b+). flash-kernel 1764491 Add Raspberry Pi 3 Model B+ to the db. flash-kernel 1811216 Modify the Pi 3 boot.scr addresses to fit a bigger kernel, prepare separate versions for armhf and arm64. linux-meta-hwe 1798352 linux-snapdragon: missing meta packages for this flavour livecd-rootfs 1805668 More changes for raspi3 build support: livecd-rootfs 1805668 Another batch of cherry-picks for raspi3 support u-boot 1805668 Backport to bionic. grub2 1789918 debian/grub-check-signatures: check kernel signatures against keys known in firmware, in case a kernel is signed but not using a key that will pass validation, such as when using kernels coming from a PPA. grub2 1812863 debian/patches/mkconfig_leave_breadcrumbs.patch: make sure grub-mkconfig leaves a trace of what files were sourced to help generate the config we're building. grub2 1800722 debian/patches/quick-boot-lvm.patch: If we don't have writable grubenv and we're on EFI, always show the menu. Closes linux 1813663 External monitors does not work anymore 4.15.0-44 thermald 1803360 Honor ACPI _CRT for processor thermal zone There are some new fanless platforms use DPTF's virtual sensor instead of INT340X devices. Because of that, the _PSV is no longer in use, at least not directly, hence its value may set higher then _CRT. To a fanless system that means no cooling device gets activated before _CRT, so the system will be considered overheated by Linux kernel, and gets shutdown by the kernel. Upstream fixes: kmod 1802689 Add i2c_i801 back to d/modprobe.d/blacklist.conf again due to regressions. e2fsprogs 1798562 d/patches/0001-resize2fs-update-checksums-in-the-extent-tree-s-relo.patch: do the checksum update later in extent tree relocated block to denote the inode number change, otherwise the checksum update might be done in the old copy of the block. linux-oem 1813663 External monitors does not work anymore 4.15.0-44 linux-oem 1811777 Fix non-working pinctrl-intel linux-oem 1811929 Fix not working Goodix touchpad linux-hwe 1814555 Ubuntu boot failure. 4.18.0-14 boot stalls. (does not boot) linux-hwe 1813873 Userspace break as a result of missing patch backport grub2 1814575 debian/grub-check-signatures: properly account for DB showing as empty on some broken firmwares: Guard against mokutil --export --db failing, and do a better job at finding the DER certs for conversion to PEM format. grub2 1401532 debian/patches/linuxefi_disable_sb_fallback.patch: Disallow unsigned kernels if UEFI Secure Boot is enabled. If UEFI Secure Boot is enabled and kernel signature verification fails, do not boot the kernel. Patch from Linn Crosetto. grub2 1814403 debian/patches/quick-boot-lvm.patch: checking the return value of 'lsefi' when the command doesn't exist does not do what's expected, so instead check the value of $grub_platform which is simpler anyway. grub2-signed 1401532 Rebuild against grub2 2.02-2ubuntu8.11. Unsorted changes python-wadllib 1729754 Fix MIME encoding of binary parts. guile-2.0 1780996 Convert triggers to noawait (Closes: #903915) dpdk 1784816 Make DPDK LTS release available in Bionic open-vm-tools 1784638 Merge with Upstream Tag stable-10.3.0 from https://github.com/vmware/open-vm-tools/releases/tag/stable-10.3.0 Remaining changes: gpgme1.0 1762384 Make debian/libgpgme-dev.links executable, it uses dh-exec packagekit 1722185 Fix debconf interaction base-files 1134036 Install locale-check command to /usr/bin and invoke it from /etc/profile.d/01-locale-fix.sh to ensure locale related environment variables are set to valid values. cryptsetup 1651818 Apply patch from Trent Nelson to fix cryptroot-unlock for busybox compatibility. clamav 1792051 debian/clamav-daemon.config.in: fix infinite loop during dpkg-reconfigure python3.6 1792143 SRU: Update Python 3.6 to the recent subminor release. python3.6 1768644 Don't inject dpkg's compiler specs into distutils. python-pylxd 1775238 d/control: Add python(3)-requests-toolbelt to python(3)-pylxd Depends and python3-requests-unixsocket to python3-pylxd Depends. Also set min version of toolbelt globally. appstream 1792537 cache-explicit-variants.patch: fix crash when upgrading from bionic to cosmic gcc-defaults 1769657 SRU: Bump GCC 8 based versions to 8.2.0. gcc-8-cross 1769657 SRU: Build using gcc 8.2.0-1ubuntu2 gcc-8 1769657 SRU: Update the package to the current version in cosmic. gcc-7 1769657 SRU: Update the package to the current version in cosmic. gcc-7 1721355 Update the gcc-foffload-default patch. cross-toolchain-base 1769657 SRU: gcc-7-cross 1769657 SRU: Build using 7.3.0-18ubuntu0.1. binutils 1769657 SRU: binutils 1763098 Fix PR gprof/23056, memory corruption in gprof. binutils 1763096 Fix PR binutils/23054, memory corruption in as. binutils 1763094 Fix PR ld/23055, memory corruption in ld. plymouth 1767918 debian/patches/git_ensure_tty_closed_0a662723.patch: ensure tty is closed on deactivate. apparmor 1788929 disallow writes to thumbnailer dir apparmor 1794848 disallow access to the dirs of private files apturl 1338482 really* work golang-1.10 1794395 Backport to 18.04. man-db 1785414 Backport seccomp sandbox improvements from 2.8.4: packagekit 1790613 Pass --no-restart-after-upgrade to dh_installsystemd to avoid PackageKit restarting while upgrading under PackageKit packagekit 1795614 debian/patches/frontend-locking.diff: Implement frontend locking in a simple way. Will need some more work to upstream, and possibly some error checking. packagekit 1790671 debian/patches/aptcc-Fix-invalid-version-dereference-in-AptInf-prov.patch, aptcc-removing-duplicate-delete-call.patch: Fix invalid dereference and delete wrong (duplicate) \"delete\" statement in providesCodec apt 1796808 Set DPKG_FRONTEND_LOCKED when running {pre,post}-invoke scripts. Some post-invoke scripts install packages, which fails because the environment variable is not set. This sets the variable for all three kinds of scripts {pre,post-}invoke and pre-install-pkgs, but we will only allow post-invoke at a later time. apt 1787120 Support records larger than 32kb in 'apt show' (Closes: #905527) apt 1781169 Add support for dpkg frontend lock (Closes: #869546) apt 1794957 http: Stop pipeline after close only if it was not filled before apt 1794053 pkgCacheFile: Only unlock in destructor if locked before python-apt 1795407 Frontend locking and related locking improvements dpkg 1796081 Apply patch from upstream to add frontend locking: distro-info-data 1800656 Add Ubuntu 19.04 Disco Dingo. valgrind 1781128 Apply post 3.13 PPC64 related patches. clamav 1783632 SECURITY REGRESSION: clamav-daemon fails to start due to options removed in new version and manually edited configuration file. packagekit 1552792 Correct autoremove behaviour to only autoremove packages that relate to the current transaction sosreport 1803735 d/p/dont-collect-some-tracing-instance-files.patch: sosreport 1775195 New 3.6 upstream release. major enhancements to core features and existing plugins: dh-golang 1794936 d/patches/0001-Fix-index-out-of-range-when-using-gccgo.-Closes-9072.patch: backport fix for building with gccgo from debian. python-wsme 1805125 d/control: Correct dependency on python3-netaddr for python3-wsme, avoiding needless install of Python 2. python-memcache 1802487 d/p/py2-perf.patch: Cherry pick fix to use native C implementation for pickle under Python 2, resolving issues with performance degradation. tmux 1766942 d/p/tmux-pane-border-status-leak.patch: Fixed memory leak in screen_redraw_make_pane_status (upstream fix). livecd-rootfs 1805497 Include snaps in image manifests debootstrap 1773496 For (Ubuntu) releases disco+ default to MERGED_USR=yes, -k extract option. tar 1809827 backport \"Fix the --add-file option.\" upstream commit, thanks Martin Vogt llvm-toolchain-7 1798597 Backport to bionic for 18.04.2 HWE stack update. systemd 1811471 d/p/resolve-enable-EDNS0-towards-the-127.0.0.53-stub-res.patch getaddrinfo() failures when fallback to dns tcp queries, so enable edns0 in resolv.conf systemd 1804487 d/p/resolved-Increase-size-of-TCP-stub-replies.patch dns failures with edns0 disabled and truncated response BionicBeaver/ReleaseNotes/ChangeSummary/18.04.2 (last edited 2019-02-14 15:43:41 by sil2100) ",
        "_version_": 1718527385643515904
      },
      {
        "story_id": [19470064],
        "story_author": ["anewhnaccount2"],
        "story_descendants": [42],
        "story_score": [204],
        "story_time": ["2019-03-23T11:52:03Z"],
        "story_title": "Qri: A global dataset version control system built on the distributed web",
        "search": [
          "Qri: A global dataset version control system built on the distributed web",
          "https://github.com/qri-io/qri",
          "Qri CLI a dataset version control system built on the distributed web Website | Packages | Contribute | Issues | Docs | Download Welcome Question Answer \"I want to learn about Qri\" Read the official documentation \"I want to download Qri\" Download Qri or brew install qri-io/qri/qri \"I have a question\" Create an issue and use the label 'question' \"I found a bug\" Create an issue and use the label 'bug' \"I want to help build the Qri backend\" Read the Contributing guides \"I want to build Qri from source\" Build Qri from source qri is a global dataset version control system built on the distributed web Breaking that down: global so that if anyone, anywhere has published work with the same or similar datasets, you can discover it. Specific to datasets because data deserves purpose-built tools version control to keep data in sync, attributing all changes to authors On the distributed web to make all of the data published on qri simultaneously available, letting peers work on data together. If youre unfamiliar with version control, particularly the distributed kind, well you're probably viewing this document on github which is a version control system intended for code. Its underlying technology git popularized some magic sauce that has inspired a generation of programmers and popularized concepts at the heart of the distributed web. Qri is applying that family of concepts to four common data problems: Discovery Can I find data Im looking for? Trust Can I trust what Ive found? Friction Can I make this work with my other stuff? Sync How do I handle changes in data? Because qri is global and content-addressed, adding data to qri also checks the entire network to see if someone has added it before. Since qri is focused solely on datasets, it can provide meaningful search results. Every change on qri is associated with a peer, creating an audit-able trail you can use to quickly see what has changed and who has changed it. All datasets on qri are automatically described at the time of ingest using a flexible schema that makes data naturally inter-operate. Qri comes with tools to turn all datasets on the network into a JSON API with a single command. Finally, all changes in qri are tracked & synced. Building From Source To build qri you'll need the go programming language on your machine. $ git clone https://github.com/qri-io/qri $ cd qri $ make install If this is your first time building, this command will have a lot of output. That's good! Its means it's working :) It'll take a minute or two to build. After this is done, there will be a new binary qri in your ~/go/bin directory if using go modules, and $GOPATH/bin directory otherwise. You should be able to run: and see help output. Building on Windows To start, make sure that you have enabled Developer Mode. A library that we depend on needs it enabled in order to properly handle symlinks. If not done, you'll likely get the error message \"A required privilege is not held by the client\". You should not need to Run As Administrator to build or run qri. We do not recommend using administrator to run qri. Shell For your shell, we recommend using msys2. Other shells, such as cmd, Powershell, or cygwin may also be usable, but msys2 makes it easy to install our required dependencies. IPFS also recommends msys2, and qri is built on top of IPFS. Dependencies Building depends upon having git and make installed. If using msys2, you can easily install these by using the package manager \"pacman\". In a shell, type: Assuming you've also installed go using the official Windows installer linked above, you will also need to add go to your PATH by modifying your environment variable. See the next section on \"Environment variables\" for more information. Due to how msys2 treats the PATH variable, you also need to add a new environment variable MSYS2_PATH_TYPE, with the value inherit, using the same procedure. Once these steps are complete, proceed to building. Building on Rasberry PI On a Raspberry PI, you'll need to increase your swap file size in order to build. Normal desktop and server linux OSes should be fine to proceed to building. One symptom of having not enough swap space is the go install command producing an error message ending with: To increase your swapfile size, first turn off the swapfile: sudo dphys-swapfile swapoff Then edit /etc/dphys-swapfile as root and set CONF_SWAPSIZE to 1024. Finally turn on the swapfile again: sudo dphys-swapfile swapon Otherwise linux machines with reduced memory will have other ways to increase their swap file sizes. Check documentation for your particular machine. Packages Qri is comprised of many specialized packages. Below you will find a summary of each package. Package Go Docs Go Report Card Description api user accessible layer, primarily made for communication with our frontend webapp cmd our command line interface config user configuration details, includes peer's profile lib takes arguments from the cmd and api layer and forms proper requests to call to the action layer p2p the peer to peer communication layer of qri repo the repository: saving, removing, and storing datasets, profiles, and the config dataset the blueprint for a dataset, the atoms that make up qri registry the blueprint for a registry: the service that allows profiles to be unique and datasets to be searchable starlib the starlark standard library available for qri transform scripts qfs \"qri file sytem\" is Qri's file system abstraction for getting & storing data from different sources ioes package to handle in, out, and error streams: gives us better control of where we send output and errors jsonschema used to describe the structure of a dataset, so we can validate datasets and determine dataset interop Outside Libraries The following packages are not under Qri, but are important dependencies, so we display their latest versions for convenience. Package Version ipfs This documentation has been adapted from the Cycle.js documentation. ",
          "Interesting project, particularly with the choice of IPFS and DCAT -- something I'll have to look into.  There have been other efforts to handle mostly file-based scientific data with versioning in both distributed (Dat <a href=\"https://blog.datproject.org/tag/science/\" rel=\"nofollow\">https://blog.datproject.org/tag/science/</a>) and centralized ways (DataHub <a href=\"https://datahub.csail.mit.edu/www/\" rel=\"nofollow\">https://datahub.csail.mit.edu/www/</a>).  Juan Benet visited our research center to give a talk about IPFS a few years ago. Really fantastic stuff.<p>I'm the creator of DVID (<a href=\"http://dvid.io\" rel=\"nofollow\">http://dvid.io</a>), which has an entirely different approach to how we might handle distributed versioning of scientific data primarily at a larger scale (100 GB to petabytes).  Like Qri and IPFS, DVID is written in Go.  Our research group works in Connectomics.  We start with massive 3D brain image volumes and apply automated and manual segmentation to mine the neurons and synapses of all that data.  There's also a lot of associated data to manage the production of connectomes.<p>One of our requirements, though, is having low-latency reads and writes to the data.  We decided to create a Science API that shields clients from how the data is actually represented, and for now, have used an ordered key-value stores for the backend.  Pluggable \"datatypes\" provide the Science API and also translate requests into the underlying key-value pairs, which are the units for versioning.  It's worked out pretty well for us and I'm now working on overhauling the store interface and improving the movement of versions between servers.  At our scale, it's useful to be able to mail a hard drive to a collaborator to establish the base DAG data and then let them eventually do a \"pull request\" for their relatively small modifications.<p>We've published some of our data online (<a href=\"http://emdata.janelia.org\" rel=\"nofollow\">http://emdata.janelia.org</a>) and visitors can actually browse through the 3d images using a Google-developed web app, Neuroglancer.  It's running on a relatively small VM so I imagine any significant HN traffic might crush it :/  We are still figuring out the best way to handle the public-facing side.<p>I think a lot of people are coming up with their own ideas about how to version scientific data, so maybe we should establish a meeting or workshop to discuss how some of these systems might interoperate?  The RDA (<a href=\"https://rd-alliance.org/\" rel=\"nofollow\">https://rd-alliance.org/</a>) \nhas been trying to establish working groups and standards, although they weren't really looking at distributed versioning a few years ago.  We need something like a Github for scientific data where papers can reference data at a particular commit and then offer improvements through pull requests.",
          "I really love the design and style qri! It is fun!<p>Can I ask why, for a git-style system, IPFS was chosen instead of GUN or SSB?<p>Certainly, images/files/etc. are better in IPFS than GUN or SSB.<p>But, you're gonna have a nightmare doing any git-style index/patch/object/etc. operations with it - both GUN & SSB's algorithms are meant to handle this type of stuff.<p>Did you guys do any analysis?"
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/qri-io/qri",
        "comments.comment_id": [19470305, 19472145],
        "comments.comment_author": ["DocSavage", "marknadal"],
        "comments.comment_descendants": [2, 1],
        "comments.comment_time": [
          "2019-03-23T13:11:31Z",
          "2019-03-23T20:17:31Z"
        ],
        "comments.comment_text": [
          "Interesting project, particularly with the choice of IPFS and DCAT -- something I'll have to look into.  There have been other efforts to handle mostly file-based scientific data with versioning in both distributed (Dat <a href=\"https://blog.datproject.org/tag/science/\" rel=\"nofollow\">https://blog.datproject.org/tag/science/</a>) and centralized ways (DataHub <a href=\"https://datahub.csail.mit.edu/www/\" rel=\"nofollow\">https://datahub.csail.mit.edu/www/</a>).  Juan Benet visited our research center to give a talk about IPFS a few years ago. Really fantastic stuff.<p>I'm the creator of DVID (<a href=\"http://dvid.io\" rel=\"nofollow\">http://dvid.io</a>), which has an entirely different approach to how we might handle distributed versioning of scientific data primarily at a larger scale (100 GB to petabytes).  Like Qri and IPFS, DVID is written in Go.  Our research group works in Connectomics.  We start with massive 3D brain image volumes and apply automated and manual segmentation to mine the neurons and synapses of all that data.  There's also a lot of associated data to manage the production of connectomes.<p>One of our requirements, though, is having low-latency reads and writes to the data.  We decided to create a Science API that shields clients from how the data is actually represented, and for now, have used an ordered key-value stores for the backend.  Pluggable \"datatypes\" provide the Science API and also translate requests into the underlying key-value pairs, which are the units for versioning.  It's worked out pretty well for us and I'm now working on overhauling the store interface and improving the movement of versions between servers.  At our scale, it's useful to be able to mail a hard drive to a collaborator to establish the base DAG data and then let them eventually do a \"pull request\" for their relatively small modifications.<p>We've published some of our data online (<a href=\"http://emdata.janelia.org\" rel=\"nofollow\">http://emdata.janelia.org</a>) and visitors can actually browse through the 3d images using a Google-developed web app, Neuroglancer.  It's running on a relatively small VM so I imagine any significant HN traffic might crush it :/  We are still figuring out the best way to handle the public-facing side.<p>I think a lot of people are coming up with their own ideas about how to version scientific data, so maybe we should establish a meeting or workshop to discuss how some of these systems might interoperate?  The RDA (<a href=\"https://rd-alliance.org/\" rel=\"nofollow\">https://rd-alliance.org/</a>) \nhas been trying to establish working groups and standards, although they weren't really looking at distributed versioning a few years ago.  We need something like a Github for scientific data where papers can reference data at a particular commit and then offer improvements through pull requests.",
          "I really love the design and style qri! It is fun!<p>Can I ask why, for a git-style system, IPFS was chosen instead of GUN or SSB?<p>Certainly, images/files/etc. are better in IPFS than GUN or SSB.<p>But, you're gonna have a nightmare doing any git-style index/patch/object/etc. operations with it - both GUN & SSB's algorithms are meant to handle this type of stuff.<p>Did you guys do any analysis?"
        ],
        "id": "75f37f91-7181-41b2-911b-11cd348477b1",
        "url_text": "Qri CLI a dataset version control system built on the distributed web Website | Packages | Contribute | Issues | Docs | Download Welcome Question Answer \"I want to learn about Qri\" Read the official documentation \"I want to download Qri\" Download Qri or brew install qri-io/qri/qri \"I have a question\" Create an issue and use the label 'question' \"I found a bug\" Create an issue and use the label 'bug' \"I want to help build the Qri backend\" Read the Contributing guides \"I want to build Qri from source\" Build Qri from source qri is a global dataset version control system built on the distributed web Breaking that down: global so that if anyone, anywhere has published work with the same or similar datasets, you can discover it. Specific to datasets because data deserves purpose-built tools version control to keep data in sync, attributing all changes to authors On the distributed web to make all of the data published on qri simultaneously available, letting peers work on data together. If youre unfamiliar with version control, particularly the distributed kind, well you're probably viewing this document on github which is a version control system intended for code. Its underlying technology git popularized some magic sauce that has inspired a generation of programmers and popularized concepts at the heart of the distributed web. Qri is applying that family of concepts to four common data problems: Discovery Can I find data Im looking for? Trust Can I trust what Ive found? Friction Can I make this work with my other stuff? Sync How do I handle changes in data? Because qri is global and content-addressed, adding data to qri also checks the entire network to see if someone has added it before. Since qri is focused solely on datasets, it can provide meaningful search results. Every change on qri is associated with a peer, creating an audit-able trail you can use to quickly see what has changed and who has changed it. All datasets on qri are automatically described at the time of ingest using a flexible schema that makes data naturally inter-operate. Qri comes with tools to turn all datasets on the network into a JSON API with a single command. Finally, all changes in qri are tracked & synced. Building From Source To build qri you'll need the go programming language on your machine. $ git clone https://github.com/qri-io/qri $ cd qri $ make install If this is your first time building, this command will have a lot of output. That's good! Its means it's working :) It'll take a minute or two to build. After this is done, there will be a new binary qri in your ~/go/bin directory if using go modules, and $GOPATH/bin directory otherwise. You should be able to run: and see help output. Building on Windows To start, make sure that you have enabled Developer Mode. A library that we depend on needs it enabled in order to properly handle symlinks. If not done, you'll likely get the error message \"A required privilege is not held by the client\". You should not need to Run As Administrator to build or run qri. We do not recommend using administrator to run qri. Shell For your shell, we recommend using msys2. Other shells, such as cmd, Powershell, or cygwin may also be usable, but msys2 makes it easy to install our required dependencies. IPFS also recommends msys2, and qri is built on top of IPFS. Dependencies Building depends upon having git and make installed. If using msys2, you can easily install these by using the package manager \"pacman\". In a shell, type: Assuming you've also installed go using the official Windows installer linked above, you will also need to add go to your PATH by modifying your environment variable. See the next section on \"Environment variables\" for more information. Due to how msys2 treats the PATH variable, you also need to add a new environment variable MSYS2_PATH_TYPE, with the value inherit, using the same procedure. Once these steps are complete, proceed to building. Building on Rasberry PI On a Raspberry PI, you'll need to increase your swap file size in order to build. Normal desktop and server linux OSes should be fine to proceed to building. One symptom of having not enough swap space is the go install command producing an error message ending with: To increase your swapfile size, first turn off the swapfile: sudo dphys-swapfile swapoff Then edit /etc/dphys-swapfile as root and set CONF_SWAPSIZE to 1024. Finally turn on the swapfile again: sudo dphys-swapfile swapon Otherwise linux machines with reduced memory will have other ways to increase their swap file sizes. Check documentation for your particular machine. Packages Qri is comprised of many specialized packages. Below you will find a summary of each package. Package Go Docs Go Report Card Description api user accessible layer, primarily made for communication with our frontend webapp cmd our command line interface config user configuration details, includes peer's profile lib takes arguments from the cmd and api layer and forms proper requests to call to the action layer p2p the peer to peer communication layer of qri repo the repository: saving, removing, and storing datasets, profiles, and the config dataset the blueprint for a dataset, the atoms that make up qri registry the blueprint for a registry: the service that allows profiles to be unique and datasets to be searchable starlib the starlark standard library available for qri transform scripts qfs \"qri file sytem\" is Qri's file system abstraction for getting & storing data from different sources ioes package to handle in, out, and error streams: gives us better control of where we send output and errors jsonschema used to describe the structure of a dataset, so we can validate datasets and determine dataset interop Outside Libraries The following packages are not under Qri, but are important dependencies, so we display their latest versions for convenience. Package Version ipfs This documentation has been adapted from the Cycle.js documentation. ",
        "_version_": 1718527395446652928
      },
      {
        "story_id": [19412088],
        "story_author": ["mykowebhn"],
        "story_descendants": [27],
        "story_score": [62],
        "story_time": ["2019-03-17T03:41:51Z"],
        "story_title": "Sviatoslav Richter: A Pianist Who Made the Earth Move (2015)",
        "search": [
          "Sviatoslav Richter: A Pianist Who Made the Earth Move (2015)",
          "https://www.npr.org/sections/deceptivecadence/2015/03/19/393778706/sviatoslav-richter-the-pianist-who-made-the-earth-move",
          "NPRs sites use cookies, similar tracking and storage technologies, and information about the device you use to access our sites (together, cookies) to enhance your viewing, listening and user experience, personalize content, personalize messages from NPRs sponsors, provide social media features, and analyze NPRs traffic. This information is shared with social media, sponsorship, analytics, and other vendors or service providers. See details. You may click on Your Choices below to learn about and use cookie management tools to limit use of cookies when you visit NPRs sites. You can adjust your cookie choices in those tools at any time. If you click Agree and Continue below, you acknowledge that your cookie choices in those tools will be respected and that you otherwise agree to the use of cookies on NPRs sites. ",
          "A colleague came to Richter after a concert and asked him: \"How could you play that passage, I practiced it for 100 times with no luck\". Richter: \"I practiced it 1000 times\".",
          "I discovered Richter when I listened to Mussorgsky's Pictures of an Exhibition on an old LP. But I really understood Richter's boldness and genius when a friend of mine lended me a CD with Schubert's last piano sonata (D960).<p>Just listen to the way the first bars, containing the unforgettable opening theme, are usually played by most of the best pianists:<p>- Brendel: <a href=\"https://m.youtube.com/watch?v=TKy0Lyl4g-s\" rel=\"nofollow\">https://m.youtube.com/watch?v=TKy0Lyl4g-s</a><p>- Uchida: <a href=\"https://m.youtube.com/watch?v=l7cc2FD06FM\" rel=\"nofollow\">https://m.youtube.com/watch?v=l7cc2FD06FM</a><p>- Kovacevich: <a href=\"https://m.youtube.com/watch?v=MAZ8PA5_gVA\" rel=\"nofollow\">https://m.youtube.com/watch?v=MAZ8PA5_gVA</a><p>And now listen to Richter's version:<p><a href=\"https://m.youtube.com/watch?v=ZbJtHzaFpBQ\" rel=\"nofollow\">https://m.youtube.com/watch?v=ZbJtHzaFpBQ</a><p>This very slow tempo is extremely difficult from the point of view of the pianist, because it requires great control of dynamics and precision. However, it allows the listener to understand all the intricacies of the harmony and of the texture. I do not usually listen Richter's version because it requires deep concentration, but I can say I never really understood this piece until I heard Richter.<p>Happy birthday, Maestro!"
        ],
        "story_type": ["Normal"],
        "url": "https://www.npr.org/sections/deceptivecadence/2015/03/19/393778706/sviatoslav-richter-the-pianist-who-made-the-earth-move",
        "url_text": "NPRs sites use cookies, similar tracking and storage technologies, and information about the device you use to access our sites (together, cookies) to enhance your viewing, listening and user experience, personalize content, personalize messages from NPRs sponsors, provide social media features, and analyze NPRs traffic. This information is shared with social media, sponsorship, analytics, and other vendors or service providers. See details. You may click on Your Choices below to learn about and use cookie management tools to limit use of cookies when you visit NPRs sites. You can adjust your cookie choices in those tools at any time. If you click Agree and Continue below, you acknowledge that your cookie choices in those tools will be respected and that you otherwise agree to the use of cookies on NPRs sites. ",
        "comments.comment_id": [19412712, 19413161],
        "comments.comment_author": ["holri", "ziotom78"],
        "comments.comment_descendants": [0, 4],
        "comments.comment_time": [
          "2019-03-17T07:22:47Z",
          "2019-03-17T09:48:43Z"
        ],
        "comments.comment_text": [
          "A colleague came to Richter after a concert and asked him: \"How could you play that passage, I practiced it for 100 times with no luck\". Richter: \"I practiced it 1000 times\".",
          "I discovered Richter when I listened to Mussorgsky's Pictures of an Exhibition on an old LP. But I really understood Richter's boldness and genius when a friend of mine lended me a CD with Schubert's last piano sonata (D960).<p>Just listen to the way the first bars, containing the unforgettable opening theme, are usually played by most of the best pianists:<p>- Brendel: <a href=\"https://m.youtube.com/watch?v=TKy0Lyl4g-s\" rel=\"nofollow\">https://m.youtube.com/watch?v=TKy0Lyl4g-s</a><p>- Uchida: <a href=\"https://m.youtube.com/watch?v=l7cc2FD06FM\" rel=\"nofollow\">https://m.youtube.com/watch?v=l7cc2FD06FM</a><p>- Kovacevich: <a href=\"https://m.youtube.com/watch?v=MAZ8PA5_gVA\" rel=\"nofollow\">https://m.youtube.com/watch?v=MAZ8PA5_gVA</a><p>And now listen to Richter's version:<p><a href=\"https://m.youtube.com/watch?v=ZbJtHzaFpBQ\" rel=\"nofollow\">https://m.youtube.com/watch?v=ZbJtHzaFpBQ</a><p>This very slow tempo is extremely difficult from the point of view of the pianist, because it requires great control of dynamics and precision. However, it allows the listener to understand all the intricacies of the harmony and of the texture. I do not usually listen Richter's version because it requires deep concentration, but I can say I never really understood this piece until I heard Richter.<p>Happy birthday, Maestro!"
        ],
        "id": "2d387251-d139-4004-ac53-8f0a047ab7dc",
        "_version_": 1718527394494545920
      },
      {
        "story_id": [19652376],
        "story_author": ["based2"],
        "story_descendants": [61],
        "story_score": [145],
        "story_time": ["2019-04-13T11:35:47Z"],
        "story_title": "Infrastructure as Code, Part One",
        "search": [
          "Infrastructure as Code, Part One",
          "https://crate.io/a/infrastructure-as-code-part-one/",
          "Let's say you've developed a new feature and you want to release it. You've avoided all the typical pitfalls when it comes to making a new release and you've done everything as you were meant to. It's not a Friday, it's not 5 pm, and so on. But your organization is still doing manual releases. So that means that a systems administrator is logging on to each one of your production machines and deploying the latest version of the code. At your organization, the rules demand that you submit a rollout request in advance. And in this instance, you are granted a rollout window for Tuesday afternoon. Your application changes have already been successfully deployed in the staging environment. The tests pass, and everything else has gone smoothly. You're feeling confident. Tuesday afternoon rolls around, and it's time to make the release to the production environment. The deployment is successful on the first machine. And the second machine. But wait. Something goes wrong on the third machine. It turns out that the third production server has a different set of application dependencies installed. The versions are incompatible. You start debugging, but there's a fixed time window for the deployment, and time is running out... Eventually, time runs out. You've missed the window. The entire release is rolled back. And now you have to request another rollout. But approval from stakeholders is slow, so the next opportunity is a week away. Damn. You spend the rest of the day investigating what went wrong. Eventually, you figure it out. Somebody logged on to the third machine last week and manually updated some of the software. These changes were never propagated to the other servers, or back to the staging environment, or dev environments. Does any of this feel familiar? If so, you're not alone. Fortunately, Infrastructure as Code (IaC) can help you mitigate all of the problems described above. In part one of this IaC miniseries, I will introduce you to the basic concepts and explain some of the benefits. An Introduction As the name suggests, Infrastructure as Code uses code to provision, configure, and manage infrastructure. Using the right set of tools, it is straightforward to create a description of the infrastructure on which your application should be deployed. This description includes the specification of virtual machines, storage, software stacks, network configurations, security features, user accounts, access control limits, and so on. This description is done using code, often using a declarative language. The language you use varies depending on the tools you use, from common scripting languages to Domain Specific Languages (DSL) provided by the tools. IaC has evolved alongside the emergence of Infrastructure as a Service (IaaS) and other cloud-based services. The programmability of IaaS and the declarative nature of IaC work very well together. Instead of setting up that cloud environment by hand each time, you can just automate it with IaC. But that doesn't mean that IaC limits you to IaaS, whether public, private, or hybrid. With a little extra work, you can use infrastructure configuration tools to manage a traditional collection of physical servers. Alternatives Before I continue, it would be remiss of me not to mention the other options. There are three main ones that I can think of: Manually setting up your infrastructure using the visual console provided by your cloud provider.For example, using the Azure Portal by hand to set up your Microsoft Azure products, one by one. Clicking around the interface, creating a new VM, choosing the operating system from a drop-down menu, launching it, monitoring the status, and so on. Using the CLI tool provided by a cloud provider. Instead of using a cloud provider, you're managing your own physical machines or virtual machines. And you have written your own collection of configuration tools, management tools, deployment scripts, and so on. If you're using a cloud provider, the console is a great way to learn the ropes. But this quickly grows tiresome and error-prone if you try to manage your whole setup like this. And in addition, there's usually no built-in change visibility. You have to remember what actions you took, and document them for the rest of the team. And if you're managing your own servers by hand, developing your own system administration scripts can work okay. But it's easy for such a system to become a bit of a non-standard hodgepodge. And, well, things can quickly get out of hand... The Landscape Okay, let's take a high-level look at the IaC landscape. There are basically two categories of tools: Orchestration tools are used to provision, organize, and manage infrastructure components. Examples include Terraform, AWS CloudFormation, Azure Resource Manager. Configuration management tools are used to install, update, and manage the software running on the infrastructure components. Examples include SaltStack, Puppet, Chef, and Ansible. Additionally, when it comes to IaC, there are two primary approaches to managing infrastructure: Some tools (e.g., SaltStack) treat infrastructure components as mutable objects. That is, every time you make a change to the configuration, the necessary set of changes are made to the already-running components. Other tools (e.g., Terraform) treat infrastructure components as immutable objects. That is, when you make a change, new components with the new configuration are created to replace the old components. What Are the Benefits? Developer Mindset Developers ideally concern themselves with creating stable and sustainable software. But when infrastructure is \"something those other people take care of,\" it's easy for developers not to consider how the software they are building is going to be run. When you manage the infrastructure using code and involve the application developers in that process, it can prompt a change in mindset. How is this application going to be deployed? How is it going to be maintained once it's running? How are upgrades done? Have you thought about all the ways it might fail on a production machine? What preventative measures can you take? All of this and more becomes a natural part of the application development process itself when it is integrated with IaC. Version Control Because you are defining your infrastructure with code, it should also be versioned in a repository like the rest of your code. All of the benefits that change control offers application development are made available for infrastructure management: A single source of truth. The code itself, and the way it is laid out is a communication tool and can be used to understand the way things are built. The history of changes is recorded and accessible so you can understand what has changed and why over time. Coding standards can be developed and maintained. Pull requests and code reviews increase knowledge sharing and improve the overall quality of the code. Experimental changes can be made on branches and tested in isolation. Making changes is less daunting because if something goes wrong, you can revert back to a previously known working version. And so on, and so on... Knowledge Sharing It's a widespread phenomenon that one or two people in an organization will accumulate critical information that only they seem to possess. [At one of my previous companies, we kept a list of such individuals called the \"red bus\" list, so named because of the risk that one of them might be hit by one. Ed.] Less dramatically, such people take holidays and sometimes get sick, like anyone. So you should be able to cope with them not being around. With your infrastructure documented using code, it is hopefully relatively straightforward to understand. And what's more, this is a type of living documentation that should always be up-to-date. Better Use of Time In general, humans are bad with tedious, repetitive tasks. We lose interest, and that's when we're most likely to make mistakes. Fortunately, this is what machines are good at. When you manage your infrastructure with code, you offload all of the tedious, repetitive work to computers. And as a result, you reduce the chance of inconsistencies, mistakes, incomplete work, and other forms of human error. This also allows your people to spend their time and energies on other more important, high-value tasks Continuous Integration Continuous Integration (CI) is the practice of team members merging their code changes into a mainline branch multiple times per day. The benefit is that you are continually validating your code against your test suites, and you can adapt to changes being made to the codebase in a manageable, incremental fashion, as and when it happens, and not all in one go at the end of your project (what has been termed \"integration hell\"). IaC improves CI because changes to the infrastructure can be tested like changes to the application code. Additionally, a temporary testing environment can be provisioned for every change to application code. Continuous Delivery Continuous Delivery (CD) is the process of making regular automated releases every time code is pushed to the mainline branch. CD and IaC are a match made in heaven. With IaC, you can set up a deployment pipeline that automates the process of moving different versions of your application from one environment to the next as your testing and release schedules dictate. But more on that topic in another post. :) Wrap Up Infrastructure as Code (IaC) bridges the gap between system administrators and developers, and in doing so it: Helps developers think about the entire lifecycle of the software they are developing. Brings version control, code reviews, knowledge sharing, and other benefits to systems administration. Enables you to radically transform your CI and CD pipeline. Reduces the need for repetitive and error-prone tasks, freeing people up to work on more exciting things. The end result should be a more consistent, reliable, and well-understood infrastructure that is easier to develop and easier to maintain. In part two of this miniseries, I dive into the details and show you some of the ways we're using Terraform at Crate.io for provisioning and managing infrastructure. ",
          "Nice piece. Looking forward to Part II.<p>What I am missing (often, in these type of articles as well as in actual production environments) is the fact that if you develop (infrastructure) code, you also need  to test your (infrastructure) code. Which means you need actual infrastructure to test on.<p>In my case, this means network equipment, storage equipment and actual physical servers.<p>If you're in a cloud, this means you need a seperate account on a seperate creditcard and start from there to build up the infra that Dev and Ops can start deploying their infra-as-code on.<p>And this test-infrastructure is <i>not</i> the test environments other teams run <i>their</i> tests on.<p>If that is not available, automating your infrastructure can be dangerous at best. Since you cannot properly test your code. And your code will rot.",
          "What I find unfortunate about infrastructure-as-code tooling is that a lot of the tooling isn't actually using code, but instead uses esoteric configuration languages. Indeed, the article refers to Terraform with its custom syntax.<p>Imho tools should use actual code (whether it's TypeScript or Kotlin or whatever) instead of reinventing constructs like loops and string interpolation.<p>Thankfully these tools are getting more popular, because frankly I can't stand configuring another Kubernetes or GCP resource using a huge block of copy/pasted YAML."
        ],
        "story_type": ["Normal"],
        "url": "https://crate.io/a/infrastructure-as-code-part-one/",
        "comments.comment_id": [19653312, 19653843],
        "comments.comment_author": ["mverwijs", "royjacobs"],
        "comments.comment_descendants": [3, 14],
        "comments.comment_time": [
          "2019-04-13T14:51:53Z",
          "2019-04-13T16:12:39Z"
        ],
        "comments.comment_text": [
          "Nice piece. Looking forward to Part II.<p>What I am missing (often, in these type of articles as well as in actual production environments) is the fact that if you develop (infrastructure) code, you also need  to test your (infrastructure) code. Which means you need actual infrastructure to test on.<p>In my case, this means network equipment, storage equipment and actual physical servers.<p>If you're in a cloud, this means you need a seperate account on a seperate creditcard and start from there to build up the infra that Dev and Ops can start deploying their infra-as-code on.<p>And this test-infrastructure is <i>not</i> the test environments other teams run <i>their</i> tests on.<p>If that is not available, automating your infrastructure can be dangerous at best. Since you cannot properly test your code. And your code will rot.",
          "What I find unfortunate about infrastructure-as-code tooling is that a lot of the tooling isn't actually using code, but instead uses esoteric configuration languages. Indeed, the article refers to Terraform with its custom syntax.<p>Imho tools should use actual code (whether it's TypeScript or Kotlin or whatever) instead of reinventing constructs like loops and string interpolation.<p>Thankfully these tools are getting more popular, because frankly I can't stand configuring another Kubernetes or GCP resource using a huge block of copy/pasted YAML."
        ],
        "id": "8101ddfc-0bc6-4eeb-b4f5-8a29a1746cb5",
        "url_text": "Let's say you've developed a new feature and you want to release it. You've avoided all the typical pitfalls when it comes to making a new release and you've done everything as you were meant to. It's not a Friday, it's not 5 pm, and so on. But your organization is still doing manual releases. So that means that a systems administrator is logging on to each one of your production machines and deploying the latest version of the code. At your organization, the rules demand that you submit a rollout request in advance. And in this instance, you are granted a rollout window for Tuesday afternoon. Your application changes have already been successfully deployed in the staging environment. The tests pass, and everything else has gone smoothly. You're feeling confident. Tuesday afternoon rolls around, and it's time to make the release to the production environment. The deployment is successful on the first machine. And the second machine. But wait. Something goes wrong on the third machine. It turns out that the third production server has a different set of application dependencies installed. The versions are incompatible. You start debugging, but there's a fixed time window for the deployment, and time is running out... Eventually, time runs out. You've missed the window. The entire release is rolled back. And now you have to request another rollout. But approval from stakeholders is slow, so the next opportunity is a week away. Damn. You spend the rest of the day investigating what went wrong. Eventually, you figure it out. Somebody logged on to the third machine last week and manually updated some of the software. These changes were never propagated to the other servers, or back to the staging environment, or dev environments. Does any of this feel familiar? If so, you're not alone. Fortunately, Infrastructure as Code (IaC) can help you mitigate all of the problems described above. In part one of this IaC miniseries, I will introduce you to the basic concepts and explain some of the benefits. An Introduction As the name suggests, Infrastructure as Code uses code to provision, configure, and manage infrastructure. Using the right set of tools, it is straightforward to create a description of the infrastructure on which your application should be deployed. This description includes the specification of virtual machines, storage, software stacks, network configurations, security features, user accounts, access control limits, and so on. This description is done using code, often using a declarative language. The language you use varies depending on the tools you use, from common scripting languages to Domain Specific Languages (DSL) provided by the tools. IaC has evolved alongside the emergence of Infrastructure as a Service (IaaS) and other cloud-based services. The programmability of IaaS and the declarative nature of IaC work very well together. Instead of setting up that cloud environment by hand each time, you can just automate it with IaC. But that doesn't mean that IaC limits you to IaaS, whether public, private, or hybrid. With a little extra work, you can use infrastructure configuration tools to manage a traditional collection of physical servers. Alternatives Before I continue, it would be remiss of me not to mention the other options. There are three main ones that I can think of: Manually setting up your infrastructure using the visual console provided by your cloud provider.For example, using the Azure Portal by hand to set up your Microsoft Azure products, one by one. Clicking around the interface, creating a new VM, choosing the operating system from a drop-down menu, launching it, monitoring the status, and so on. Using the CLI tool provided by a cloud provider. Instead of using a cloud provider, you're managing your own physical machines or virtual machines. And you have written your own collection of configuration tools, management tools, deployment scripts, and so on. If you're using a cloud provider, the console is a great way to learn the ropes. But this quickly grows tiresome and error-prone if you try to manage your whole setup like this. And in addition, there's usually no built-in change visibility. You have to remember what actions you took, and document them for the rest of the team. And if you're managing your own servers by hand, developing your own system administration scripts can work okay. But it's easy for such a system to become a bit of a non-standard hodgepodge. And, well, things can quickly get out of hand... The Landscape Okay, let's take a high-level look at the IaC landscape. There are basically two categories of tools: Orchestration tools are used to provision, organize, and manage infrastructure components. Examples include Terraform, AWS CloudFormation, Azure Resource Manager. Configuration management tools are used to install, update, and manage the software running on the infrastructure components. Examples include SaltStack, Puppet, Chef, and Ansible. Additionally, when it comes to IaC, there are two primary approaches to managing infrastructure: Some tools (e.g., SaltStack) treat infrastructure components as mutable objects. That is, every time you make a change to the configuration, the necessary set of changes are made to the already-running components. Other tools (e.g., Terraform) treat infrastructure components as immutable objects. That is, when you make a change, new components with the new configuration are created to replace the old components. What Are the Benefits? Developer Mindset Developers ideally concern themselves with creating stable and sustainable software. But when infrastructure is \"something those other people take care of,\" it's easy for developers not to consider how the software they are building is going to be run. When you manage the infrastructure using code and involve the application developers in that process, it can prompt a change in mindset. How is this application going to be deployed? How is it going to be maintained once it's running? How are upgrades done? Have you thought about all the ways it might fail on a production machine? What preventative measures can you take? All of this and more becomes a natural part of the application development process itself when it is integrated with IaC. Version Control Because you are defining your infrastructure with code, it should also be versioned in a repository like the rest of your code. All of the benefits that change control offers application development are made available for infrastructure management: A single source of truth. The code itself, and the way it is laid out is a communication tool and can be used to understand the way things are built. The history of changes is recorded and accessible so you can understand what has changed and why over time. Coding standards can be developed and maintained. Pull requests and code reviews increase knowledge sharing and improve the overall quality of the code. Experimental changes can be made on branches and tested in isolation. Making changes is less daunting because if something goes wrong, you can revert back to a previously known working version. And so on, and so on... Knowledge Sharing It's a widespread phenomenon that one or two people in an organization will accumulate critical information that only they seem to possess. [At one of my previous companies, we kept a list of such individuals called the \"red bus\" list, so named because of the risk that one of them might be hit by one. Ed.] Less dramatically, such people take holidays and sometimes get sick, like anyone. So you should be able to cope with them not being around. With your infrastructure documented using code, it is hopefully relatively straightforward to understand. And what's more, this is a type of living documentation that should always be up-to-date. Better Use of Time In general, humans are bad with tedious, repetitive tasks. We lose interest, and that's when we're most likely to make mistakes. Fortunately, this is what machines are good at. When you manage your infrastructure with code, you offload all of the tedious, repetitive work to computers. And as a result, you reduce the chance of inconsistencies, mistakes, incomplete work, and other forms of human error. This also allows your people to spend their time and energies on other more important, high-value tasks Continuous Integration Continuous Integration (CI) is the practice of team members merging their code changes into a mainline branch multiple times per day. The benefit is that you are continually validating your code against your test suites, and you can adapt to changes being made to the codebase in a manageable, incremental fashion, as and when it happens, and not all in one go at the end of your project (what has been termed \"integration hell\"). IaC improves CI because changes to the infrastructure can be tested like changes to the application code. Additionally, a temporary testing environment can be provisioned for every change to application code. Continuous Delivery Continuous Delivery (CD) is the process of making regular automated releases every time code is pushed to the mainline branch. CD and IaC are a match made in heaven. With IaC, you can set up a deployment pipeline that automates the process of moving different versions of your application from one environment to the next as your testing and release schedules dictate. But more on that topic in another post. :) Wrap Up Infrastructure as Code (IaC) bridges the gap between system administrators and developers, and in doing so it: Helps developers think about the entire lifecycle of the software they are developing. Brings version control, code reviews, knowledge sharing, and other benefits to systems administration. Enables you to radically transform your CI and CD pipeline. Reduces the need for repetitive and error-prone tasks, freeing people up to work on more exciting things. The end result should be a more consistent, reliable, and well-understood infrastructure that is easier to develop and easier to maintain. In part two of this miniseries, I dive into the details and show you some of the ways we're using Terraform at Crate.io for provisioning and managing infrastructure. ",
        "_version_": 1718527399165952000
      },
      {
        "story_id": [21158487],
        "story_author": ["danicgross"],
        "story_descendants": [65],
        "story_score": [470],
        "story_time": ["2019-10-04T15:15:12Z"],
        "story_title": "Streamlit: Turn a Python script into an interactive data analysis tool",
        "search": [
          "Streamlit: Turn a Python script into an interactive data analysis tool",
          "https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace",
          "Introducing Streamlit, an app framework built for ML engineersCoding a semantic search engine with real-time neural-net inference in 300 lines of Python.In my experience, every nontrivial machine learning project is eventually stitched together with bug-ridden and unmaintainable internal tools. These tools often a patchwork of Jupyter Notebooks and Flask apps are difficult to deploy, require reasoning about client-server architecture, and dont integrate well with machine learning constructs like Tensorflow GPU sessions.I saw this first at Carnegie Mellon, then at Berkeley, Google X, and finally while building autonomous robots at Zoox. These tools were often born as little Jupyter notebooks: the sensor calibration tool, the simulation comparison app, the LIDAR alignment app, the scenario replay tool, and so on.As a tool grew in importance, project managers stepped in. Processes sprouted. Requirements flowered. These solo projects gestated into scripts, and matured into gangly maintenance nightmares.The machine learning engineers ad-hoc app building flow.When a tool became crucial, we called in the tools team. They wrote fluent Vue and React. They blinged their laptops with stickers about declarative frameworks. They had a design process:The tools teams clean-slate app building flow.Which was awesome. But these tools all needed new features, like weekly. And the tools team was supporting ten other projects. They would say, well update your tool again in two months.So we were back to building our own tools, deploying Flask apps, writing HTML, CSS, and JavaScript, and trying to version control everything from notebooks to stylesheets. So my old Google X friend, Thiago Teixeira, and I began thinking about the following question: What if we could make building tools as easy as writing Python scripts?We wanted machine learning engineers to be able to create beautiful apps without needing a tools team. These internal tools should arise as a natural byproduct of the ML workflow. Writing such tools should feel like training a neural net or performing an ad-hoc analysis in Jupyter! At the same time, we wanted to preserve all of the flexibility of a powerful app framework. We wanted to create beautiful, performant tools that engineers could show off. Basically, we wanted this:The Streamlit app building flow.With an amazing beta community including engineers from Uber, Twitter, Stitch Fix, and Dropbox, we worked for a year to create Streamlit, a completely free and open source app framework for ML engineers. With each prototype, the core principles of Streamlit became simpler and purer. They are:#1: Embrace Python scripting. Streamlit apps are really just scripts that run from top to bottom. Theres no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. For example, this is how you write to the screen:import streamlit as stst.write('Hello, world!')Nice to meet you.#2: Treat widgets as variables. There are no callbacks in Streamlit! Every interaction simply reruns the script from top to bottom. This approach leads to really clean code:import streamlit as stx = st.slider('x')st.write(x, 'squared is', x * x)An interactive Streamlit app in three lines of code.#3: Reuse data and computation. What if you download lots of data or perform complex computation? The key is to safely reuse information across runs. Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default, data store that lets Streamlit apps safely and effortlessly reuse information. For example, this code downloads data only once from the Udacity Self-driving car project, yielding a simple, fast app:Using st.cache to persist data across Streamlit runs. To run this code, please follow these instructions.The output of running the st.cache example above.In short, Streamlit works like this:The entire script is run from scratch for each user interaction.Streamlit assigns each variable an up-to-date value given widget states.Caching allows Streamlit to skip redundant data fetches and computation.Or in pictures:User events trigger Streamlit to rerun the script from scratch. Only the cache persists across runs.If this sounds intriguing, you can try it right now! Just run:$ pip install --upgrade streamlit $ streamlit hello You can now view your Streamlit app in your browser. Local URL: http://localhost:8501 Network URL: http://10.0.1.29:8501This will automatically pop open a web browser pointing to your local Streamlit app. If not, just click the link.To see more examples like this fractal animation, run streamlit hello from the command line.Ok. Are you back from playing with fractals? Those can be mesmerizing.The simplicity of these ideas does not prevent you from creating incredibly rich and useful apps with Streamlit. During my time at Zoox and Google X, I watched as self-driving car projects ballooned into gigabytes of visual data, which needed to be searched and understood, including running models on images to compare performance. Every self-driving car project Ive seen eventually has had entire teams working on this tooling.Building such a tool in Streamlit is easy. This Streamlit demo lets you perform semantic search across the entire Udacity self-driving car photo dataset, visualize human-annotated ground truth labels, and run a complete neural net (YOLO) in real time from within the app [1].This 300-line Streamlit demo combines semantic visual search with interactive neural net inference.The whole app is a completely self-contained, 300-line Python script, most of which is machine learning code. In fact, there are only 23 Streamlit calls in the whole app. You can run it yourself right now!$ pip install --upgrade streamlit opencv-python$ streamlit runhttps://raw.githubusercontent.com/streamlit/demo-self-driving/master/app.pyAs we worked with machine learning teams on their own projects, we came to realize that these simple ideas yield a number of important benefits:Streamlit apps are pure Python files. So you can use your favorite editor and debugger with Streamlit.My favorite layout for writing Streamlit apps has VSCode on the left and Chrome on the right.Pure Python scripts work seamlessly with Git and other source control software, including commits, pull requests, issues, and comments. Because Streamlits underlying language is pure Python, you get all the benefits of these amazing collaboration tools for free .Because Streamlit apps are just Python scripts, you can easily version control them with Git.Streamlit provides an immediate-mode live coding environment. Just click Always rerun when Streamlit detects a source file change.Click Always rerun to enable live coding.Caching simplifies setting up computation pipelines. Amazingly, chaining cached functions automatically creates efficient computation pipelines! Consider this code adapted from our Udacity demo:A simple computation pipeline in Streamlit. To run this code, please follow these instructions.Basically, the pipeline is load_metadata create_summary. Every time the script is run Streamlit only recomputes whatever subset of the pipeline is required to get the right answer. Cool!To make apps performant, Streamlit only recomputes whatever is necessary to update the UI.Streamlit is built for GPUs. Streamlit allows direct access to machine-level primitives like TensorFlow and PyTorch and complements these libraries. For example in this demo, Streamlits cache stores the entire NVIDIA celebrity face GAN [2]. This approach enables nearly instantaneous inference as the user updates sliders.This Streamlit app demonstrates NVIDIA celebrity face GAN [2] model using Shaobo Guans TL-GAN [3].Streamlit is a free and open-source library rather than a proprietary web app. You can serve Streamlit apps on-prem without contacting us. You can even run Streamlit locally on a laptop without an Internet connection! Furthermore, existing projects can adopt Streamlit incrementally.Several ways incrementally adopt Streamlit. (Icons courtesy of fullvector / Freepik.)This just scratches the surface of what you can do with Streamlit. One of the most exciting aspects of Streamlit is how these primitives can be easily composed into complex apps that look like scripts. Theres a lot more we could say about how our architecture works and the features we have planned, but well save that for future posts.Block diagram of Streamlits components. More coming soon!Were excited to finally share Streamlit with the community today and see what you all build with it. We hope that youll find it easy and delightful to turn your Python scripts into beautiful ML apps.Thanks to Amanda Kelly, Thiago Teixeira, TC Ricks, Seth Weidman, Regan Carey, Beverly Treuille, Genevive Wachtell, and Barney Pell for their helpful input on this article.References:[1] J. Redmon and A. Farhadi, YOLOv3: An Incremental Improvement (2018), arXiv.[2] T. Karras, T. Aila, S. Laine, and J. Lehtinen, Progressive Growing of GANs for Improved Quality, Stability, and Variation (2018), ICLR.[3] S. Guan, Controlled image synthesis and editing using a novel TL-GAN model (2018), Insight Data Science Blog. ",
          "This looks really slick, can't wait to try it out!<p>If anyone is curious about other tools in the same space, our data scientists use Dash[1] and plotly to build interactive exploration and visualization apps. We set up a Git repo that deploys their apps internally with every merge to master, so they're actually building and updating tools that our operations, marketing, etc teams use every day.<p>[1] <a href=\"https://plot.ly/dash/\" rel=\"nofollow\">https://plot.ly/dash/</a>",
          "Interesting project, but why does an open source developer tool needs browser telemetry?<p>You should ask for telemetry permissions _before_ the process starts up (as you do for email address), and keep the default as \"No\", instead of start to send the data transparently unless non user friendly steps are taken by the user."
        ],
        "story_type": ["Normal"],
        "url": "https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace",
        "comments.comment_id": [21169495, 21170226],
        "comments.comment_author": ["_gwlb", "random42"],
        "comments.comment_descendants": [2, 0],
        "comments.comment_time": [
          "2019-10-06T02:43:38Z",
          "2019-10-06T06:33:03Z"
        ],
        "comments.comment_text": [
          "This looks really slick, can't wait to try it out!<p>If anyone is curious about other tools in the same space, our data scientists use Dash[1] and plotly to build interactive exploration and visualization apps. We set up a Git repo that deploys their apps internally with every merge to master, so they're actually building and updating tools that our operations, marketing, etc teams use every day.<p>[1] <a href=\"https://plot.ly/dash/\" rel=\"nofollow\">https://plot.ly/dash/</a>",
          "Interesting project, but why does an open source developer tool needs browser telemetry?<p>You should ask for telemetry permissions _before_ the process starts up (as you do for email address), and keep the default as \"No\", instead of start to send the data transparently unless non user friendly steps are taken by the user."
        ],
        "id": "2bf5e965-b868-4069-8a2c-20f57648f178",
        "url_text": "Introducing Streamlit, an app framework built for ML engineersCoding a semantic search engine with real-time neural-net inference in 300 lines of Python.In my experience, every nontrivial machine learning project is eventually stitched together with bug-ridden and unmaintainable internal tools. These tools often a patchwork of Jupyter Notebooks and Flask apps are difficult to deploy, require reasoning about client-server architecture, and dont integrate well with machine learning constructs like Tensorflow GPU sessions.I saw this first at Carnegie Mellon, then at Berkeley, Google X, and finally while building autonomous robots at Zoox. These tools were often born as little Jupyter notebooks: the sensor calibration tool, the simulation comparison app, the LIDAR alignment app, the scenario replay tool, and so on.As a tool grew in importance, project managers stepped in. Processes sprouted. Requirements flowered. These solo projects gestated into scripts, and matured into gangly maintenance nightmares.The machine learning engineers ad-hoc app building flow.When a tool became crucial, we called in the tools team. They wrote fluent Vue and React. They blinged their laptops with stickers about declarative frameworks. They had a design process:The tools teams clean-slate app building flow.Which was awesome. But these tools all needed new features, like weekly. And the tools team was supporting ten other projects. They would say, well update your tool again in two months.So we were back to building our own tools, deploying Flask apps, writing HTML, CSS, and JavaScript, and trying to version control everything from notebooks to stylesheets. So my old Google X friend, Thiago Teixeira, and I began thinking about the following question: What if we could make building tools as easy as writing Python scripts?We wanted machine learning engineers to be able to create beautiful apps without needing a tools team. These internal tools should arise as a natural byproduct of the ML workflow. Writing such tools should feel like training a neural net or performing an ad-hoc analysis in Jupyter! At the same time, we wanted to preserve all of the flexibility of a powerful app framework. We wanted to create beautiful, performant tools that engineers could show off. Basically, we wanted this:The Streamlit app building flow.With an amazing beta community including engineers from Uber, Twitter, Stitch Fix, and Dropbox, we worked for a year to create Streamlit, a completely free and open source app framework for ML engineers. With each prototype, the core principles of Streamlit became simpler and purer. They are:#1: Embrace Python scripting. Streamlit apps are really just scripts that run from top to bottom. Theres no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. For example, this is how you write to the screen:import streamlit as stst.write('Hello, world!')Nice to meet you.#2: Treat widgets as variables. There are no callbacks in Streamlit! Every interaction simply reruns the script from top to bottom. This approach leads to really clean code:import streamlit as stx = st.slider('x')st.write(x, 'squared is', x * x)An interactive Streamlit app in three lines of code.#3: Reuse data and computation. What if you download lots of data or perform complex computation? The key is to safely reuse information across runs. Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default, data store that lets Streamlit apps safely and effortlessly reuse information. For example, this code downloads data only once from the Udacity Self-driving car project, yielding a simple, fast app:Using st.cache to persist data across Streamlit runs. To run this code, please follow these instructions.The output of running the st.cache example above.In short, Streamlit works like this:The entire script is run from scratch for each user interaction.Streamlit assigns each variable an up-to-date value given widget states.Caching allows Streamlit to skip redundant data fetches and computation.Or in pictures:User events trigger Streamlit to rerun the script from scratch. Only the cache persists across runs.If this sounds intriguing, you can try it right now! Just run:$ pip install --upgrade streamlit $ streamlit hello You can now view your Streamlit app in your browser. Local URL: http://localhost:8501 Network URL: http://10.0.1.29:8501This will automatically pop open a web browser pointing to your local Streamlit app. If not, just click the link.To see more examples like this fractal animation, run streamlit hello from the command line.Ok. Are you back from playing with fractals? Those can be mesmerizing.The simplicity of these ideas does not prevent you from creating incredibly rich and useful apps with Streamlit. During my time at Zoox and Google X, I watched as self-driving car projects ballooned into gigabytes of visual data, which needed to be searched and understood, including running models on images to compare performance. Every self-driving car project Ive seen eventually has had entire teams working on this tooling.Building such a tool in Streamlit is easy. This Streamlit demo lets you perform semantic search across the entire Udacity self-driving car photo dataset, visualize human-annotated ground truth labels, and run a complete neural net (YOLO) in real time from within the app [1].This 300-line Streamlit demo combines semantic visual search with interactive neural net inference.The whole app is a completely self-contained, 300-line Python script, most of which is machine learning code. In fact, there are only 23 Streamlit calls in the whole app. You can run it yourself right now!$ pip install --upgrade streamlit opencv-python$ streamlit runhttps://raw.githubusercontent.com/streamlit/demo-self-driving/master/app.pyAs we worked with machine learning teams on their own projects, we came to realize that these simple ideas yield a number of important benefits:Streamlit apps are pure Python files. So you can use your favorite editor and debugger with Streamlit.My favorite layout for writing Streamlit apps has VSCode on the left and Chrome on the right.Pure Python scripts work seamlessly with Git and other source control software, including commits, pull requests, issues, and comments. Because Streamlits underlying language is pure Python, you get all the benefits of these amazing collaboration tools for free .Because Streamlit apps are just Python scripts, you can easily version control them with Git.Streamlit provides an immediate-mode live coding environment. Just click Always rerun when Streamlit detects a source file change.Click Always rerun to enable live coding.Caching simplifies setting up computation pipelines. Amazingly, chaining cached functions automatically creates efficient computation pipelines! Consider this code adapted from our Udacity demo:A simple computation pipeline in Streamlit. To run this code, please follow these instructions.Basically, the pipeline is load_metadata create_summary. Every time the script is run Streamlit only recomputes whatever subset of the pipeline is required to get the right answer. Cool!To make apps performant, Streamlit only recomputes whatever is necessary to update the UI.Streamlit is built for GPUs. Streamlit allows direct access to machine-level primitives like TensorFlow and PyTorch and complements these libraries. For example in this demo, Streamlits cache stores the entire NVIDIA celebrity face GAN [2]. This approach enables nearly instantaneous inference as the user updates sliders.This Streamlit app demonstrates NVIDIA celebrity face GAN [2] model using Shaobo Guans TL-GAN [3].Streamlit is a free and open-source library rather than a proprietary web app. You can serve Streamlit apps on-prem without contacting us. You can even run Streamlit locally on a laptop without an Internet connection! Furthermore, existing projects can adopt Streamlit incrementally.Several ways incrementally adopt Streamlit. (Icons courtesy of fullvector / Freepik.)This just scratches the surface of what you can do with Streamlit. One of the most exciting aspects of Streamlit is how these primitives can be easily composed into complex apps that look like scripts. Theres a lot more we could say about how our architecture works and the features we have planned, but well save that for future posts.Block diagram of Streamlits components. More coming soon!Were excited to finally share Streamlit with the community today and see what you all build with it. We hope that youll find it easy and delightful to turn your Python scripts into beautiful ML apps.Thanks to Amanda Kelly, Thiago Teixeira, TC Ricks, Seth Weidman, Regan Carey, Beverly Treuille, Genevive Wachtell, and Barney Pell for their helpful input on this article.References:[1] J. Redmon and A. Farhadi, YOLOv3: An Incremental Improvement (2018), arXiv.[2] T. Karras, T. Aila, S. Laine, and J. Lehtinen, Progressive Growing of GANs for Improved Quality, Stability, and Variation (2018), ICLR.[3] S. Guan, Controlled image synthesis and editing using a novel TL-GAN model (2018), Insight Data Science Blog. ",
        "_version_": 1718527429833654272
      },
      {
        "story_id": [19647692],
        "story_author": ["robto"],
        "story_descendants": [149],
        "story_score": [242],
        "story_time": ["2019-04-12T18:33:18Z"],
        "story_title": "IPFS Project Roadmap",
        "search": [
          "IPFS Project Roadmap",
          "https://github.com/ipfs/roadmap",
          "Table of Contents IPFS Mission Statement 2020 Priority Content Routing 2020 Working Groups 2020 Epics 2019 Priority Package Managers Future Goals Large Files Decentralized Web Encrypted Web Distributed Web Personal Web Sneaker Web Interplanetary Web - Mars 2024 Packet Switched Web Data Web Package Switched Web Self-Archiving Web Versioning Datasets Interplanetary DevOps The World's Knowledge becomes accessible through the DWeb WebOS IPFS Mission Statement The mission of IPFS is to create a resilient, upgradable, open network to preserve and grow humanitys knowledge. This looks different! Want to participate in helping define our \"Mission Statement 2.0\"? Add your thoughts here! 2020 Priority Scoping in to 2020 H1 Instead of a 2020 year-long plan, we decided to focus on a 2020 H1 plan (covering Q1 & Q2) so as to: Enable our team to truly focus on one thing, complete it, and then move on to other challenges instead of doing many things at once Better understand the components of each goal and plan our time accordingly to hit them by not trying to nail down plans too far into the future Be adaptable and prepared for surprises, re-prioritizations, or market shifts that require us to refocus energy or change our plan in the course of the year 2020 H1 Priority Selection Criteria Before selecting a 2020 H1 priority, we did an open call for Theme Proposals to surface areas the community felt were of high importance and urgency. We combined these great proposals with an analysis of the project, team, and ecosystem state - and the biggest risks to IPFS Project success. Out of that analysis, we identified there were two main aspects our 2020 H1 plan MUST address: Mitigate current IPFS pain points around network performance and end user experience that are hindering wider adoption and scale Increase velocity, alignment, and capacity for IPFS devs and contributors to ensure our time and efforts are highly leveraged (because if we can make fast, sustained, high-quality progress by leveling-up our focus and healthy habits, we can achieve our goals faster and ensure contributing to IPFS is fun and productive!) Content Routing Given the selection criteria, our main priority for the first half of 2020 - the next 6 months - is improving the performance and reliability of content routing in the IPFS network. 'Content routing' is the process of finding a node hosting the content you're looking for, such that you can fetch the desired data and quickly load your website/dapp/video/etc. As the IPFS network scaled this past year (over 30x!), it ran into new problems in our distributed routing algorithms - struggling to find content spread across many unreliable nodes. This was especially painful for IPNS, which relied on multiple of these slow/unreliable queries to find the latest version of a file. These performance gaps caused IPFS to lag and stall while searching for the needed content, hurting the end user experience and making IPFS feel broken. Searching the network to find desired content (aka, using IPFS as a decentralized CDN) is one of the most common actions for new IPFS users and is required by most ipfs-powered dapp use cases - therefore, it's the number 1 pain point we need to mitigate in order to unlock increased adoption and scalability of the network! We considered a number of other potential goals - especially all the great 2020 Theme Proposals - before selecting this priority. However, we decided it was more important to focus core working group dev time on the main blockers and pain points to enable the entire ecosystem to grow and succeed. Many of these proposals are actually very well suited for community ownership via DevGrants and collaborations - and some of them, like \"IPFS in Rust\" and \"Examples and Tutorials\", already have grants or bounties associated with them! 2020 Working Groups The IPFS project includes the collective work of serveral focused teams, called Working Groups (WGs). Each group defines its own roadmap with tasks and priorities derived from the main IPFS Project Priority. To better orient around our core focus for 2020 H1, we created a few new working groups (notably \"Content Routing\"), and spun others down (notably our \"Package Managers\" working group). For 2020 H1, we have 5 main working groups - with our \"Ecosystem\" working group divided into 3 sub-groups. Each WGs top-line focus: Content Routing: Ensure all IPFS users can find and access content they care about in a distributed network of nodes Testground: Provide robust feedback loops for content routing development, debugging, and benchmarking at scale Bifrost (IPFS Infra): Make sure our gateway and infra scale to support access to the IPFS network Ecosystem: Ensure community health and growth through collaborations, developer experience and platform availability Browsers / Connectivity: Maximize the availability and connectivity of IPFS on the web Collabs / Community: Support IPFS users and grow new opportunities through research, collaborations and community engagement Dev Ex: Support the IPFS technical community through documentation, contributor experience, API ergonomics and tooling Project: Support team functioning, prioritization, and day-to-day operations Looking for more specifics? Check out the docs on our team roles and structure! 2020 Epics We've expanded our 2020 Priority into a list of Epic Endeavours that give an overview of the primary targets IPFS has for 2020 H1. If you are pumped about these Epics and want to help, you can get involved! See the call to action (CTA) for each section below. 1. Build IPFS dev capacity and velocity In order to achieve our content routing goal for 2020 H1, we need to level up our own leverage, coordination, velocity, and planning as a project to ensure all contributors spend their time and energy effectively. This includes a few different dimensions: Integrate research via the ResNetLab into our design practice to ensure our work builds on the knowledge and experience of leading researchers in our fields Empower new contributors in the IPFS ecosystem through DevGrants and collaborations to upgrade and extend IPFS to solve new problems Invest in developer tooling, automation, and fast feedback loops to accelerate experimentation and iteration Upgrade project planning and management within and between working groups to ensure we define, estimate, track and unblock our work efficiently Focus our attention on fewer things to improve completion rate and reduce churn, saying \"not now\" or finding other champions for nice-to-have projects in order to allocate energy and attention to the most important work You can get involved with ResNetLab RFPs or by proposing/funding projects in the DevGrants repo! 2. Improve content routing performance such that 95th percentile content routing speed is <5s Improving content routing performance requires making improvements and bugfixes to the go-libp2p DHT at scale, and changing how we form, query, and resolve content in the IPFS network to be faster and more scalable. This involves a combination of research, design, implementation, and testing. Making changes to the configuration of the entire network is non-trivial - that's why we've been investing in the InterPlanetary Testground, a new set of tools for testing next generation P2P applications, to help us diagnose issues and evaluate improvements prior to rolling out upgrades to the entire public network. You can track the work in these milestones on ZenHub: Content Routing ZenHub Roadmap Testground ZenHub Roadmap If you want to help refine the detailed milestones, or take on some of the improvements required to hit this goal, see the Content Routing Work Plan to dive deeper! 3. Invest in IPFS community enablement and support Supporting community health and growth continues to be a core focus for IPFS as we scale to more users, applications, and use cases. Refining our adoption pathways, continuing to grow platform availability, and supporting our collaborators to bring IPFS to new users and use cases helps us maximize the impact and value we create in the world. Scale the number of users and applications supported by IPFS through talks, guides, and how-tos Refine our APIs to simplify end-user adoption and maximize ease of use Bring IPFS to browsers to maximize default availability and connectivity on the web Continue impoving our new IPFS Docs Site, to ensure developer & user questions are clearly answered and actionable Invest in explicit community stewardship responsibilities to ensure there are answers, tools, and fast feedback loops to support new IPFS users and contributors Great ways to start helping enable the IPFS community include: suggesting or building new tools to support IPFS users, reviewing open PRs, answering questions on http://discuss.ipfs.io and on our IRC channels on freenode/matrix, or writing your own how-tos and guides to use IPFS for your use case! 2019 Priority Our core goal for 2019 was to make large-scale improvements to the IPFS network around scalability, performance, and usability. By focusing on the Package Managers use case, we hoped to identify, prioritize, and demonstrably resolve performance/usability issues, while driving adoption via a common and compelling use case that all developers experience daily. We hoped this focus would help us hone IPFS to be production ready (in functionality and practice), help scale network usage to millions of nodes, and accelerate our project and community growth/velocity. Graded 2019 Epics The reference implementations of the IPFS Protocol (Go & JS) become Production Ready Support Software Package Managers in entering the Distributed Web Scale the IPFS Network The IPFS Community of Builders gets together for the 1st IPFS Conf IPFS testing, benchmarks, and performance optimizations Support the growing IPFS Community You can see the details in what work we took on in each milestone, and which we achieved in the archived 2019 IPFS Project Roadmap. Sorting Function D = Difficulty (or \"Delta\" or \"Distance\"), E = Ecosystem Growth, I = Importance To identify our top focus for 2019 and rank the future goals in our upgrade path, we used a sorting function to prioritize potential focus areas. Each goal was given a score from 1 (low) - 5 (high) on each axis. We sorted first in terms of low difficulty or \"delta\" (i.e. minimal additional requirements and fewer dependencies from the capabilities IPFS has now), then high ecosystem growth (growing our community and resources to help us gravity assist and accelerate our progress), and finally high importance (we want IPFS to have a strong, positive impact on the world). Future goals below are listed in priority order using this sorting function. Package Managers (D1 E5 I3) The most used code and binary Package Managers are powered by IPFS. Package Managers collect and curate sizable datasets. Top package managers distribute code libraries (eg npm, pypi, cargo, ...), binaries and program source code (eg apt, pacman, brew ...), full applications (app stores), datasets, and more. They are critical components of the programming and computing experience, and are the perfect use case for IPFS. Most package managers can benefit tremendously from the content-addressing, peer-to-peer, decentralized, and offline capabilities of IPFS. Existing Package Managers should switch over to using IPFS as a default, or at least an optional way of distributing their assets, and their own updates. New Package Managers should be built entirely on IPFS. --- Code libraries, programs, and datasets should become permanent, resilient, partition tolerant, and authenticated, and IPFS can get them there. Registries become curators and seeds, but do not have to bear the costs of the entire bandwidth. Registries could become properly decentralized. --- We have a number of challenges ahead to make this a reality, but we are already beyond half-way. We should be able to get top package managers to (a) use IPFS as an optional distribution mechanism, then (b) use that to test all kinds of edge cases in the wild and to drive performance improvements , then (c) get package managers to switch over the defaults. Future Goals Large Files (D1 E4 I3) By 2020, IPFS becomes the default way to distribute files or collections of files above 1GB HTTP is not good for distributing large files or large collections of small files. Anything above 1GB starts running into problems (resuming, duplication, centralized bandwidth limitations, etc). BitTorrent works well for single archives that won't change, or won't duplicate, but fails in a number of places. IPFS has solved most of the hard problems but hasn't yet made the experience so easy that people default to IPFS to distribute large files. IPFS should solve this problem so well, it should be so easy and delightful to use, and it should be so high performance that it becomes the default way to move anything above 1GB world-wide. This is a massive hole right now that IPFS is well-poised to fill -- we just need to solve some performance and usability problems. Decentralized Web (D2 E4 I3) IPFS supports decentralized web apps built on p2p connections with power and capabilities at the edges. In web 2.0, control of the web is centralized - its location-addressing model and client-server architecture encourage reliance and trust of centralized operators to host services, data, and intermediate connections. Walled gardens are common, and our data is locked into centralized systems that increase the risk of privacy breaches, state control, or that a single party can shut down valued services. The decentralized web is all about peer-to-peer connections and putting users in control of their tools and data. It does this by connecting users directly to each other and using verifiable tools like hash-links and encryption to ensure the power and control in the network is retained by the participants themselves. The decentralized web (as distinguished from Distributed Web) is NOT about partition tolerance, or making the web work equally well in local-area networks/mobile/offline - the focus here is on the control and ownership of services. IPFS has solved most of the hard underlying design problems for decentralized web, but hasn't yet made the experience easy enough for end-users to experience it in the applications, tools, and services they use. This requires tooling and solutions for developers to sustainably run their business without any centralized intermediary facilitating the network (though centralized providers may still exist to augment and improve the experience for services that already work decentralized by design). Designing Federation for interop with current systems is key for the Migration Path. Encrypted Web (D2 E3 I4) Apps and Data are fully end-to-end encrypted at rest. Users have reader, writer, and usage privacy. Apps and user data on IPFS are completely end-to-end encrypted, at rest, with only users having access. Users get reader and writer privacy by default. Any nodes providing services usually do so over encrypted data and never get access to the plain data. The apps themselves are distributed encrypted, decrypted and loaded in a safe sandbox in the users' control. Attackers (including ISPs) lose the ability to spy on users' data, and even which applications users are using. This works with all top use case apps -- email, chat, forums, collaboration tools, etc. Distributed Web (D2 E2 I4) Info and apps function equally well in local area networks and offline. The Web is a partitionable fabric, like the internet. The web and mobile -- the most important application platforms on the planet -- are capable of working entirely in sub-networks. The norm for networked apps is to use the available data and connections, to sync asynchronously, and to leverage local connectivity protocols. The main apps for every top use case work equally well in offline or local network settings. It means IPFS and apps on top work excellently on desktops, the browser, and mobile. Users can use webapps like email, chat, forums, social networks, collaboration tools, games, and so on without having to be connected to the global internet. Oh, and getting files from one computer to another right next to it finally becomes an easy thing to do (airdrop level of easy). Personal Web (D3 E4 I2) Personal Data and programs are under user control. The memex becomes reality. The web becomes a drastically more personal thing. Users' data and exploration is under the users' control -- similar to how a \"personal computer\" is under the user's control, and \"the cloud\" is not. Users decide which apps and other people get access to their data. Explorations can be recorded for the user in memex fashion. The user gets to keep copies of all the data they have observed through the web. A self-archiving personal record forms, which the user can always go back to, explore, and use -- whether or not those applications are still in development by their authors. Sneaker Web (D3 E2 I4) The web functions over disconnected sneaker networks, spreading information, app data, apps, and more. The web is capable of working fully distributed, and can even hop across disconnected components of the internet. Apps and their data can flow across high latency, intermittent, asynchronous links across them. People in disconnected networks get the same applications, the same quality of experience, and the same ability to distribute their contributions as anybody in the strongest connected component (\"the backbone of the internet\"). The Web is totally resistant to large scale partitions. Information can flow so easily across disconnected components that there is no use in trying to block or control information at the borders. Interplanetary Web - Mars 2024. (D3 E3 I4) Mars. Let's live the interplanetary dream!** SpaceX plans to land on mars in 2022, and send humans in 2024. By then, IPFS should be the default/best choice for SpaceX networking. The first humans on mars should use IPFS to run the top 10 networked apps. That means truly excellent and well-known IPFS apps addressing the top 10 networked use cases must exist. For that to happen, the entire system needs to be rock solid, audited, performant, powerful, easy-to-use, well known, and so on. It means IPFS must work on a range of platforms (desktop, servers, web, mobile), and to work with both special purpose local area networks, and across interplanetary distances. If we achieve this, while solving for general use and general users (not specifically for the Mars use case, then IPFS will be in tremendous standing. Packet Switched Web (D3 E2 I3) IPFS protocols use packet switching, and the network can relay all kinds of traffic easily, tolerating switch failures. The infrastructure protocols (libp2p, IPFS, etc.) and the end-user app protocols (the logic of the app) can work entirely over a packet switching layer. Protocols like BitSwap, DHT, PubSub become drastically higher performance, and unconstrained by packets sent before theirs. Web applications can form their own isolated virtual networks, allowing their users to distribute the packets. Users can form their own groups and their own virtual networks, allowing users to only operate in a subnet they trust, and ensure all of their traffic is moving between trusted switches. The big public network uses packet switching by default. Data Web (D4 E3 I3) Large Datasets are open, easy to access, easy to replicate, version controlled, secure, permanent. We constantly lose access to important information, either because it ceases to exist or simply due to virtual virtual barriers (i.e. censorship, lack of connectivity and so on). Information also often loses its way into the peers that most needed it and there aren't good ways to signal that some dataset wasn't contemplated, referenced. We want to improve this dramatically, making the data that is produced more easy to access through making it versionased, secure and easier to replicate and locate. Package Switched Web (D4 E2 I2) Data in the web can be moved around over a package switching network. Shipping TB or PB hard drives of data becomes normal. Beyond circuit switching and packet switching, the web works over package switching! It is possible to send apps, app assets, app user generated data, and so on through hard drives means. This means that the network stack and the IPLD graph sync layers are natively capable of using data in external, removable media. It is easy for a user Alice to save a lot of data to a removable drive, for Alice to mail the drive to another user Bob, and for Bob to plug in the drive to see his application finish loading what Alice wanted to show Bob. Instead of having to fumble with file exports, file systems, OS primitives, and worse -- IPFS, libp2p, and the apps just work -- there is a standard way to say \"i want this data in this drive\", and \"i want to use the data from this drive\". Once that happens, it can enable proper sneakernet web. Self-Archiving Web (D4 E4 I4) The Web becomes permanent, no more broken Links. Increase the lifespan of a Webpage from 6 months to (as good as a book). The Internet Archive(s, plural) Content Address their snapshots to maximize deduplications and hit factor. IPFS becomes the platform that enables the multiple Internet Archives to store, replicate and share responsibility over who possesses what. It becomes simple for any institution (from large organization to small local library) to become an Internet Archive node. Users can search through these Internet Archives nodes, fully compliant with data protection laws. Versioning Datasets (D4 E3 I3) IPFS becomes the default way to version datasets, and unlocks a dataset distribution and utility explosion similar to what VCS did for code. IPFS emerged from dataset versioning, package management, and distribution concerns. There are huge gaping holes in this space because large datasets are very unwieldy and defy most systems that make small files easy to version, package, and distribute. IPFS was designed with this kind of problem in mind and has the primitives in place to solve many of these problems. There are many things missing: (a) most importantly, a toolchain for version history management that works with these large graphs (most of what git does). (b) Better deduplication and representation techniques. (c) Virtual filesystem support -- to plug under existing architectures. (d) Ways to easily wrap existing data layouts (filestore) -- to plug on top existing architectures. (e) An unrelenting focus on extremely high performance. (f) primitives to retrieve and query relevant pieces of versioned datasets (IPLD Selectors and Queries). --- But luckily, all of these things can be added incrementally to enhance the tooling and win over more user segments. Interplanetary DevOps (D4 E2 I2) Versioning, packaging, distribution, and loading of Programs, Containers, OSes, VMs, defaults to IPFS. IPFS is great for versioning, deduping, packaging, distributing assets, through a variety of mediums. IPFS can revolutionize computing infrastructure systems. It has the potential to become the default way for datacenter and server infrastructure users to set up their infrastructure. This can happen at several different layers. (a) In the simplest sense, IPFS can help distribute programs to servers, by sitting within the OS, and plugging in as the downloading mechanism (replace wget, rsync, etc.). (b) IPFS can also distribute containers -- it can sit alongside docker, kubernetes, and similar systems to help version, dedup, and distribute containerized services. (c) IPFS can also distribute OSes themselves, by plugging in at the OS package manager level, and by distributing OS installation media. (d) IPFS can also version, dedup, and distribute VMs, first by sitting alongside host OSes and hypervisors moving around VM snapshots, and then by modeling VMs themselves on top of IPFS/IPLD. --- To get there, we will need to solve many of the same problems as package managers, and more. We will need the IPLD importers to model and version the media super-effectively. The World's Knowledge becomes accessible through the DWeb (D5 E2 I5) Humanity deserves equal access to the Knowledge. Platforms such as Wikipedia, Coursera, Edx, Khan Academy and others need to be available independently of Location and Connectivity. The content of this services need to exist everywhere. These replicas should be part of the whole world's dataset and not disjoint dataset. Anyone should be able to access these through the protocol, without having to deploy new services per area. WebOS (D5 E2 I3) The Web Platform and the OS'es merge. The rift between the web and the OS is finally healed. The OS and local programs and WebApps merge. They are not just indistinguishable, they are the same thing. \"Installing\" becomes pinning applications to the local computer. \"Saving\" things locally is also just pinning. The browser and the OS are no longer distinguishable. The entire OS data itself is modelled on top of IPLD, and the internal FileSystem is IPFS (or something on top, like unixfs). The OS and programs can manipulate IPLD data structures natively. The entire state of the OS itself can be represented and stored as IPLD. The OS can be frozen or snapshotted, and then resumed. A computer boots from the same OS hash, drastically reducing attack surface. ",
          "\"2019 Goal: The most used code and binary Package Managers are powered by IPFS.\"<p>That's kind of stupid-ambitious for 2019 when another 2019 goal is \"a production-ready implementation\" and IPFS has been around for 3 years already.<p>This isn't a roadmap, it's a wishlist. And I'm someone who wants to see IPFS succeed.",
          "This is not a roadmap, but rather a wishlist. There is a fundamental problem that IPFS needs to solve first. This problem is called an efficient WebRTC-based DHT. In order to change the web, IPFS needs to become usable in the browsers. Since the backbone of IPFS is DHT, there need to be an efficient UDP-based solution for \"DHT in the web\". Right now this isn't possible and reasons are not just technical, but political. The IPFS team would need to convince all major players that enabling this DHT scenario is a good idea."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/ipfs/roadmap",
        "comments.comment_id": [19649768, 19652003],
        "comments.comment_author": ["fwip", "pdxww"],
        "comments.comment_descendants": [5, 2],
        "comments.comment_time": [
          "2019-04-12T22:54:33Z",
          "2019-04-13T09:47:13Z"
        ],
        "comments.comment_text": [
          "\"2019 Goal: The most used code and binary Package Managers are powered by IPFS.\"<p>That's kind of stupid-ambitious for 2019 when another 2019 goal is \"a production-ready implementation\" and IPFS has been around for 3 years already.<p>This isn't a roadmap, it's a wishlist. And I'm someone who wants to see IPFS succeed.",
          "This is not a roadmap, but rather a wishlist. There is a fundamental problem that IPFS needs to solve first. This problem is called an efficient WebRTC-based DHT. In order to change the web, IPFS needs to become usable in the browsers. Since the backbone of IPFS is DHT, there need to be an efficient UDP-based solution for \"DHT in the web\". Right now this isn't possible and reasons are not just technical, but political. The IPFS team would need to convince all major players that enabling this DHT scenario is a good idea."
        ],
        "id": "a1a3d08c-e902-4c0a-9fd1-21a7d053257d",
        "url_text": "Table of Contents IPFS Mission Statement 2020 Priority Content Routing 2020 Working Groups 2020 Epics 2019 Priority Package Managers Future Goals Large Files Decentralized Web Encrypted Web Distributed Web Personal Web Sneaker Web Interplanetary Web - Mars 2024 Packet Switched Web Data Web Package Switched Web Self-Archiving Web Versioning Datasets Interplanetary DevOps The World's Knowledge becomes accessible through the DWeb WebOS IPFS Mission Statement The mission of IPFS is to create a resilient, upgradable, open network to preserve and grow humanitys knowledge. This looks different! Want to participate in helping define our \"Mission Statement 2.0\"? Add your thoughts here! 2020 Priority Scoping in to 2020 H1 Instead of a 2020 year-long plan, we decided to focus on a 2020 H1 plan (covering Q1 & Q2) so as to: Enable our team to truly focus on one thing, complete it, and then move on to other challenges instead of doing many things at once Better understand the components of each goal and plan our time accordingly to hit them by not trying to nail down plans too far into the future Be adaptable and prepared for surprises, re-prioritizations, or market shifts that require us to refocus energy or change our plan in the course of the year 2020 H1 Priority Selection Criteria Before selecting a 2020 H1 priority, we did an open call for Theme Proposals to surface areas the community felt were of high importance and urgency. We combined these great proposals with an analysis of the project, team, and ecosystem state - and the biggest risks to IPFS Project success. Out of that analysis, we identified there were two main aspects our 2020 H1 plan MUST address: Mitigate current IPFS pain points around network performance and end user experience that are hindering wider adoption and scale Increase velocity, alignment, and capacity for IPFS devs and contributors to ensure our time and efforts are highly leveraged (because if we can make fast, sustained, high-quality progress by leveling-up our focus and healthy habits, we can achieve our goals faster and ensure contributing to IPFS is fun and productive!) Content Routing Given the selection criteria, our main priority for the first half of 2020 - the next 6 months - is improving the performance and reliability of content routing in the IPFS network. 'Content routing' is the process of finding a node hosting the content you're looking for, such that you can fetch the desired data and quickly load your website/dapp/video/etc. As the IPFS network scaled this past year (over 30x!), it ran into new problems in our distributed routing algorithms - struggling to find content spread across many unreliable nodes. This was especially painful for IPNS, which relied on multiple of these slow/unreliable queries to find the latest version of a file. These performance gaps caused IPFS to lag and stall while searching for the needed content, hurting the end user experience and making IPFS feel broken. Searching the network to find desired content (aka, using IPFS as a decentralized CDN) is one of the most common actions for new IPFS users and is required by most ipfs-powered dapp use cases - therefore, it's the number 1 pain point we need to mitigate in order to unlock increased adoption and scalability of the network! We considered a number of other potential goals - especially all the great 2020 Theme Proposals - before selecting this priority. However, we decided it was more important to focus core working group dev time on the main blockers and pain points to enable the entire ecosystem to grow and succeed. Many of these proposals are actually very well suited for community ownership via DevGrants and collaborations - and some of them, like \"IPFS in Rust\" and \"Examples and Tutorials\", already have grants or bounties associated with them! 2020 Working Groups The IPFS project includes the collective work of serveral focused teams, called Working Groups (WGs). Each group defines its own roadmap with tasks and priorities derived from the main IPFS Project Priority. To better orient around our core focus for 2020 H1, we created a few new working groups (notably \"Content Routing\"), and spun others down (notably our \"Package Managers\" working group). For 2020 H1, we have 5 main working groups - with our \"Ecosystem\" working group divided into 3 sub-groups. Each WGs top-line focus: Content Routing: Ensure all IPFS users can find and access content they care about in a distributed network of nodes Testground: Provide robust feedback loops for content routing development, debugging, and benchmarking at scale Bifrost (IPFS Infra): Make sure our gateway and infra scale to support access to the IPFS network Ecosystem: Ensure community health and growth through collaborations, developer experience and platform availability Browsers / Connectivity: Maximize the availability and connectivity of IPFS on the web Collabs / Community: Support IPFS users and grow new opportunities through research, collaborations and community engagement Dev Ex: Support the IPFS technical community through documentation, contributor experience, API ergonomics and tooling Project: Support team functioning, prioritization, and day-to-day operations Looking for more specifics? Check out the docs on our team roles and structure! 2020 Epics We've expanded our 2020 Priority into a list of Epic Endeavours that give an overview of the primary targets IPFS has for 2020 H1. If you are pumped about these Epics and want to help, you can get involved! See the call to action (CTA) for each section below. 1. Build IPFS dev capacity and velocity In order to achieve our content routing goal for 2020 H1, we need to level up our own leverage, coordination, velocity, and planning as a project to ensure all contributors spend their time and energy effectively. This includes a few different dimensions: Integrate research via the ResNetLab into our design practice to ensure our work builds on the knowledge and experience of leading researchers in our fields Empower new contributors in the IPFS ecosystem through DevGrants and collaborations to upgrade and extend IPFS to solve new problems Invest in developer tooling, automation, and fast feedback loops to accelerate experimentation and iteration Upgrade project planning and management within and between working groups to ensure we define, estimate, track and unblock our work efficiently Focus our attention on fewer things to improve completion rate and reduce churn, saying \"not now\" or finding other champions for nice-to-have projects in order to allocate energy and attention to the most important work You can get involved with ResNetLab RFPs or by proposing/funding projects in the DevGrants repo! 2. Improve content routing performance such that 95th percentile content routing speed is <5s Improving content routing performance requires making improvements and bugfixes to the go-libp2p DHT at scale, and changing how we form, query, and resolve content in the IPFS network to be faster and more scalable. This involves a combination of research, design, implementation, and testing. Making changes to the configuration of the entire network is non-trivial - that's why we've been investing in the InterPlanetary Testground, a new set of tools for testing next generation P2P applications, to help us diagnose issues and evaluate improvements prior to rolling out upgrades to the entire public network. You can track the work in these milestones on ZenHub: Content Routing ZenHub Roadmap Testground ZenHub Roadmap If you want to help refine the detailed milestones, or take on some of the improvements required to hit this goal, see the Content Routing Work Plan to dive deeper! 3. Invest in IPFS community enablement and support Supporting community health and growth continues to be a core focus for IPFS as we scale to more users, applications, and use cases. Refining our adoption pathways, continuing to grow platform availability, and supporting our collaborators to bring IPFS to new users and use cases helps us maximize the impact and value we create in the world. Scale the number of users and applications supported by IPFS through talks, guides, and how-tos Refine our APIs to simplify end-user adoption and maximize ease of use Bring IPFS to browsers to maximize default availability and connectivity on the web Continue impoving our new IPFS Docs Site, to ensure developer & user questions are clearly answered and actionable Invest in explicit community stewardship responsibilities to ensure there are answers, tools, and fast feedback loops to support new IPFS users and contributors Great ways to start helping enable the IPFS community include: suggesting or building new tools to support IPFS users, reviewing open PRs, answering questions on http://discuss.ipfs.io and on our IRC channels on freenode/matrix, or writing your own how-tos and guides to use IPFS for your use case! 2019 Priority Our core goal for 2019 was to make large-scale improvements to the IPFS network around scalability, performance, and usability. By focusing on the Package Managers use case, we hoped to identify, prioritize, and demonstrably resolve performance/usability issues, while driving adoption via a common and compelling use case that all developers experience daily. We hoped this focus would help us hone IPFS to be production ready (in functionality and practice), help scale network usage to millions of nodes, and accelerate our project and community growth/velocity. Graded 2019 Epics The reference implementations of the IPFS Protocol (Go & JS) become Production Ready Support Software Package Managers in entering the Distributed Web Scale the IPFS Network The IPFS Community of Builders gets together for the 1st IPFS Conf IPFS testing, benchmarks, and performance optimizations Support the growing IPFS Community You can see the details in what work we took on in each milestone, and which we achieved in the archived 2019 IPFS Project Roadmap. Sorting Function D = Difficulty (or \"Delta\" or \"Distance\"), E = Ecosystem Growth, I = Importance To identify our top focus for 2019 and rank the future goals in our upgrade path, we used a sorting function to prioritize potential focus areas. Each goal was given a score from 1 (low) - 5 (high) on each axis. We sorted first in terms of low difficulty or \"delta\" (i.e. minimal additional requirements and fewer dependencies from the capabilities IPFS has now), then high ecosystem growth (growing our community and resources to help us gravity assist and accelerate our progress), and finally high importance (we want IPFS to have a strong, positive impact on the world). Future goals below are listed in priority order using this sorting function. Package Managers (D1 E5 I3) The most used code and binary Package Managers are powered by IPFS. Package Managers collect and curate sizable datasets. Top package managers distribute code libraries (eg npm, pypi, cargo, ...), binaries and program source code (eg apt, pacman, brew ...), full applications (app stores), datasets, and more. They are critical components of the programming and computing experience, and are the perfect use case for IPFS. Most package managers can benefit tremendously from the content-addressing, peer-to-peer, decentralized, and offline capabilities of IPFS. Existing Package Managers should switch over to using IPFS as a default, or at least an optional way of distributing their assets, and their own updates. New Package Managers should be built entirely on IPFS. --- Code libraries, programs, and datasets should become permanent, resilient, partition tolerant, and authenticated, and IPFS can get them there. Registries become curators and seeds, but do not have to bear the costs of the entire bandwidth. Registries could become properly decentralized. --- We have a number of challenges ahead to make this a reality, but we are already beyond half-way. We should be able to get top package managers to (a) use IPFS as an optional distribution mechanism, then (b) use that to test all kinds of edge cases in the wild and to drive performance improvements , then (c) get package managers to switch over the defaults. Future Goals Large Files (D1 E4 I3) By 2020, IPFS becomes the default way to distribute files or collections of files above 1GB HTTP is not good for distributing large files or large collections of small files. Anything above 1GB starts running into problems (resuming, duplication, centralized bandwidth limitations, etc). BitTorrent works well for single archives that won't change, or won't duplicate, but fails in a number of places. IPFS has solved most of the hard problems but hasn't yet made the experience so easy that people default to IPFS to distribute large files. IPFS should solve this problem so well, it should be so easy and delightful to use, and it should be so high performance that it becomes the default way to move anything above 1GB world-wide. This is a massive hole right now that IPFS is well-poised to fill -- we just need to solve some performance and usability problems. Decentralized Web (D2 E4 I3) IPFS supports decentralized web apps built on p2p connections with power and capabilities at the edges. In web 2.0, control of the web is centralized - its location-addressing model and client-server architecture encourage reliance and trust of centralized operators to host services, data, and intermediate connections. Walled gardens are common, and our data is locked into centralized systems that increase the risk of privacy breaches, state control, or that a single party can shut down valued services. The decentralized web is all about peer-to-peer connections and putting users in control of their tools and data. It does this by connecting users directly to each other and using verifiable tools like hash-links and encryption to ensure the power and control in the network is retained by the participants themselves. The decentralized web (as distinguished from Distributed Web) is NOT about partition tolerance, or making the web work equally well in local-area networks/mobile/offline - the focus here is on the control and ownership of services. IPFS has solved most of the hard underlying design problems for decentralized web, but hasn't yet made the experience easy enough for end-users to experience it in the applications, tools, and services they use. This requires tooling and solutions for developers to sustainably run their business without any centralized intermediary facilitating the network (though centralized providers may still exist to augment and improve the experience for services that already work decentralized by design). Designing Federation for interop with current systems is key for the Migration Path. Encrypted Web (D2 E3 I4) Apps and Data are fully end-to-end encrypted at rest. Users have reader, writer, and usage privacy. Apps and user data on IPFS are completely end-to-end encrypted, at rest, with only users having access. Users get reader and writer privacy by default. Any nodes providing services usually do so over encrypted data and never get access to the plain data. The apps themselves are distributed encrypted, decrypted and loaded in a safe sandbox in the users' control. Attackers (including ISPs) lose the ability to spy on users' data, and even which applications users are using. This works with all top use case apps -- email, chat, forums, collaboration tools, etc. Distributed Web (D2 E2 I4) Info and apps function equally well in local area networks and offline. The Web is a partitionable fabric, like the internet. The web and mobile -- the most important application platforms on the planet -- are capable of working entirely in sub-networks. The norm for networked apps is to use the available data and connections, to sync asynchronously, and to leverage local connectivity protocols. The main apps for every top use case work equally well in offline or local network settings. It means IPFS and apps on top work excellently on desktops, the browser, and mobile. Users can use webapps like email, chat, forums, social networks, collaboration tools, games, and so on without having to be connected to the global internet. Oh, and getting files from one computer to another right next to it finally becomes an easy thing to do (airdrop level of easy). Personal Web (D3 E4 I2) Personal Data and programs are under user control. The memex becomes reality. The web becomes a drastically more personal thing. Users' data and exploration is under the users' control -- similar to how a \"personal computer\" is under the user's control, and \"the cloud\" is not. Users decide which apps and other people get access to their data. Explorations can be recorded for the user in memex fashion. The user gets to keep copies of all the data they have observed through the web. A self-archiving personal record forms, which the user can always go back to, explore, and use -- whether or not those applications are still in development by their authors. Sneaker Web (D3 E2 I4) The web functions over disconnected sneaker networks, spreading information, app data, apps, and more. The web is capable of working fully distributed, and can even hop across disconnected components of the internet. Apps and their data can flow across high latency, intermittent, asynchronous links across them. People in disconnected networks get the same applications, the same quality of experience, and the same ability to distribute their contributions as anybody in the strongest connected component (\"the backbone of the internet\"). The Web is totally resistant to large scale partitions. Information can flow so easily across disconnected components that there is no use in trying to block or control information at the borders. Interplanetary Web - Mars 2024. (D3 E3 I4) Mars. Let's live the interplanetary dream!** SpaceX plans to land on mars in 2022, and send humans in 2024. By then, IPFS should be the default/best choice for SpaceX networking. The first humans on mars should use IPFS to run the top 10 networked apps. That means truly excellent and well-known IPFS apps addressing the top 10 networked use cases must exist. For that to happen, the entire system needs to be rock solid, audited, performant, powerful, easy-to-use, well known, and so on. It means IPFS must work on a range of platforms (desktop, servers, web, mobile), and to work with both special purpose local area networks, and across interplanetary distances. If we achieve this, while solving for general use and general users (not specifically for the Mars use case, then IPFS will be in tremendous standing. Packet Switched Web (D3 E2 I3) IPFS protocols use packet switching, and the network can relay all kinds of traffic easily, tolerating switch failures. The infrastructure protocols (libp2p, IPFS, etc.) and the end-user app protocols (the logic of the app) can work entirely over a packet switching layer. Protocols like BitSwap, DHT, PubSub become drastically higher performance, and unconstrained by packets sent before theirs. Web applications can form their own isolated virtual networks, allowing their users to distribute the packets. Users can form their own groups and their own virtual networks, allowing users to only operate in a subnet they trust, and ensure all of their traffic is moving between trusted switches. The big public network uses packet switching by default. Data Web (D4 E3 I3) Large Datasets are open, easy to access, easy to replicate, version controlled, secure, permanent. We constantly lose access to important information, either because it ceases to exist or simply due to virtual virtual barriers (i.e. censorship, lack of connectivity and so on). Information also often loses its way into the peers that most needed it and there aren't good ways to signal that some dataset wasn't contemplated, referenced. We want to improve this dramatically, making the data that is produced more easy to access through making it versionased, secure and easier to replicate and locate. Package Switched Web (D4 E2 I2) Data in the web can be moved around over a package switching network. Shipping TB or PB hard drives of data becomes normal. Beyond circuit switching and packet switching, the web works over package switching! It is possible to send apps, app assets, app user generated data, and so on through hard drives means. This means that the network stack and the IPLD graph sync layers are natively capable of using data in external, removable media. It is easy for a user Alice to save a lot of data to a removable drive, for Alice to mail the drive to another user Bob, and for Bob to plug in the drive to see his application finish loading what Alice wanted to show Bob. Instead of having to fumble with file exports, file systems, OS primitives, and worse -- IPFS, libp2p, and the apps just work -- there is a standard way to say \"i want this data in this drive\", and \"i want to use the data from this drive\". Once that happens, it can enable proper sneakernet web. Self-Archiving Web (D4 E4 I4) The Web becomes permanent, no more broken Links. Increase the lifespan of a Webpage from 6 months to (as good as a book). The Internet Archive(s, plural) Content Address their snapshots to maximize deduplications and hit factor. IPFS becomes the platform that enables the multiple Internet Archives to store, replicate and share responsibility over who possesses what. It becomes simple for any institution (from large organization to small local library) to become an Internet Archive node. Users can search through these Internet Archives nodes, fully compliant with data protection laws. Versioning Datasets (D4 E3 I3) IPFS becomes the default way to version datasets, and unlocks a dataset distribution and utility explosion similar to what VCS did for code. IPFS emerged from dataset versioning, package management, and distribution concerns. There are huge gaping holes in this space because large datasets are very unwieldy and defy most systems that make small files easy to version, package, and distribute. IPFS was designed with this kind of problem in mind and has the primitives in place to solve many of these problems. There are many things missing: (a) most importantly, a toolchain for version history management that works with these large graphs (most of what git does). (b) Better deduplication and representation techniques. (c) Virtual filesystem support -- to plug under existing architectures. (d) Ways to easily wrap existing data layouts (filestore) -- to plug on top existing architectures. (e) An unrelenting focus on extremely high performance. (f) primitives to retrieve and query relevant pieces of versioned datasets (IPLD Selectors and Queries). --- But luckily, all of these things can be added incrementally to enhance the tooling and win over more user segments. Interplanetary DevOps (D4 E2 I2) Versioning, packaging, distribution, and loading of Programs, Containers, OSes, VMs, defaults to IPFS. IPFS is great for versioning, deduping, packaging, distributing assets, through a variety of mediums. IPFS can revolutionize computing infrastructure systems. It has the potential to become the default way for datacenter and server infrastructure users to set up their infrastructure. This can happen at several different layers. (a) In the simplest sense, IPFS can help distribute programs to servers, by sitting within the OS, and plugging in as the downloading mechanism (replace wget, rsync, etc.). (b) IPFS can also distribute containers -- it can sit alongside docker, kubernetes, and similar systems to help version, dedup, and distribute containerized services. (c) IPFS can also distribute OSes themselves, by plugging in at the OS package manager level, and by distributing OS installation media. (d) IPFS can also version, dedup, and distribute VMs, first by sitting alongside host OSes and hypervisors moving around VM snapshots, and then by modeling VMs themselves on top of IPFS/IPLD. --- To get there, we will need to solve many of the same problems as package managers, and more. We will need the IPLD importers to model and version the media super-effectively. The World's Knowledge becomes accessible through the DWeb (D5 E2 I5) Humanity deserves equal access to the Knowledge. Platforms such as Wikipedia, Coursera, Edx, Khan Academy and others need to be available independently of Location and Connectivity. The content of this services need to exist everywhere. These replicas should be part of the whole world's dataset and not disjoint dataset. Anyone should be able to access these through the protocol, without having to deploy new services per area. WebOS (D5 E2 I3) The Web Platform and the OS'es merge. The rift between the web and the OS is finally healed. The OS and local programs and WebApps merge. They are not just indistinguishable, they are the same thing. \"Installing\" becomes pinning applications to the local computer. \"Saving\" things locally is also just pinning. The browser and the OS are no longer distinguishable. The entire OS data itself is modelled on top of IPLD, and the internal FileSystem is IPFS (or something on top, like unixfs). The OS and programs can manipulate IPLD data structures natively. The entire state of the OS itself can be represented and stored as IPLD. The OS can be frozen or snapshotted, and then resumed. A computer boots from the same OS hash, drastically reducing attack surface. ",
        "_version_": 1718527399094648832
      },
      {
        "story_id": [21055373],
        "story_author": ["gk1"],
        "story_descendants": [5],
        "story_score": [26],
        "story_time": ["2019-09-24T00:47:23Z"],
        "story_title": "Building a Modern CI/CD Pipeline in the Serverless Era with GitOps",
        "search": [
          "Building a Modern CI/CD Pipeline in the Serverless Era with GitOps",
          "https://aws.amazon.com/blogs/aws/building-a-modern-ci-cd-pipeline-in-the-serverless-era-with-gitops/",
          "Guest post by AWS Community Hero Shimon Tolts, CTO and co-founder at Datree.io. He specializes in developer tools and infrastructure, running a company that is 100% serverless. In recent years, there was a major transition in the way you build and ship software. This was mainly around microservices, splitting code into small components, using infrastructure as code, and using Git as the single source of truth that glues it all together. In this post, I discuss the transition and the different steps of modern software development to showcase the possible solutions for the serverless world. In addition, I list useful tools that were designed for this era. What is serverless? Before I dive into the wonderful world of serverless development and tooling, heres what I mean by serverless. The AWS website talks about four main benefits: No server management. Flexible scaling. Pay for value. Automated high availability. To me, serverless is any infrastructure that you dont have to manage and scale yourself. At my company Datree.io, we run 95% of our workload on AWS Fargate and 5% on AWS Lambda. We are a serverless company; we have zero Amazon EC2 instances in our AWS account. For more information, see the following: Datree.io case study Migrating to AWS ECS Fargate in production CON320: Operational Excellence w/ Containerized Workloads Using AWS Fargate (re:Invent 2018) What is GitOps? Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. According to Luis Faceira, a CI/CD consultant, GitOps is a way of working. You might look at it as an approach in which everything starts and ends with Git. Here are some key concepts: Git as the SINGLE source of truth of a system Git as the SINGLE place where we operate (create, change and destroy) ALL environments ALL changes are observable/verifiable. How you built software before the cloud Back in the waterfall pre-cloud era, you used to have separate teams for development, testing, security, operations, monitoring, and so on. Nowadays, in most organizations, there is a transition to full developer autonomy and developers owning the entire production path. The developer is the King or Queen :) Those teams (Ops/Security/IT/etc) used to be gatekeepers to validate and control every developer change. Now they have become more of a satellite unit that drives policy and sets best practices and standards. They are no longer the production bottleneck, so they provide organization-wide platforms and enablement solutions. Everything is codified With the transition into full developer ownership of the entire pipeline, developers automated everything. We have more code than ever, and processes that used to be manual are now described in code. This is a good transition, in my opinion. Here are some of the benefits: Automation: By storing all things as code, everything can be automated, reused, and re-created in moments. Immutable: If anything goes wrong, create it again from the stored configuration. Versioning: Changes can be applied and reverted, and are tracked to a single user who made the change. GitOps: Git has become the single source of truth The second major transition is that now everything is in one place! Git is the place where all of the code is stored and where all operations are initiated. Whether its testing, building, packaging, or releasing, nowadays everything is triggered through pull requests. This is amplified by the codification of everything. Useful tools in the serverless era There are many useful tools in the market, here is a list of ones that were designed for serverless. Code Always store your code in a source control system. In recent years, more and more functions are codified, such as, BI, ops, security, and AI. For new developers, it is not always obvious that they should use source control for some functionality. GitHub AWS CodeCommit GitLab BitBucket Build and test The most common mistake I see is manually configuring build jobs in the GUI. This might be good for a small POC but it is not scalable. You should have your job codified and inside your Git repository. Here are some tools to help with building and testing: AWS CodeBuild CodeFresh GitHub Actions Jenkins-x CircleCI TravisCI Security and governance When working in a serverless way, you end up having many Git repos. The number of code packages can be overwhelming. The demand for unified code standards remains as it was but now it is much harder to enforce it on top of your R&D org. Here are some tools that might help you with the challenge: Snyk Datree PureSec Aqua Protego Bundle and release Building a serverless application is connecting microservices into one unit. For example, you might be using Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. Instead of configuring each one separately, you should use a bundler to hold the configuration in one place. That allows for easy versioning and replication of the app for several environments. Here are a couple of bundlers: Serverless Framework AWS Serverless Application Model (AWS SAM) Package When working with many different serverless components, you should create small packages of tools to be able to import across different Lambda functions. You can use a language-specific store like npm or RubyGems, or use a more holistic solution. Here are several package artifact stores that allow hosting for multiple programming languages: GitHub Package Registry Jfrog Artifactory Sonatype Nexus Monitor This part is especially tricky when working with serverless applications, as everything is split into small pieces. Its important to use monitoring tools that support this mode of work. Here are some tools that can handle serverless: Rookout Amazon CloudWatch Epsagon Lumigo NewRelic DataDog Summary The serverless era brings many transitions along with it like a codification of the entire pipeline and Git being the single source of truth. This doesnt mean that the same problems that we use to have like security, logging and more disappeared, you should continue addressing them and leveraging tools that enable you to focus on your business. ",
          "Lol at the image where before microservices, it was a single monolithic application.<p>FaaS has its use cases but this “serverless for every solution” or 100% serverless marketing is annoying and NOT customer centric (Amazon said they were earths most customer centric company).",
          "This post was disappointing when it first ran: I was expecting some content after the basic intro but then it’s just a couple of saved Google searches with no discussion or analysis. It would have been a lot more interesting if they’d discussed anything about the trade offs of the different services or what they liked about a particular combination."
        ],
        "story_type": ["Normal"],
        "url": "https://aws.amazon.com/blogs/aws/building-a-modern-ci-cd-pipeline-in-the-serverless-era-with-gitops/",
        "comments.comment_id": [21057480, 21063347],
        "comments.comment_author": ["tracer4201", "acdha"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-09-24T07:36:59Z",
          "2019-09-24T18:46:43Z"
        ],
        "comments.comment_text": [
          "Lol at the image where before microservices, it was a single monolithic application.<p>FaaS has its use cases but this “serverless for every solution” or 100% serverless marketing is annoying and NOT customer centric (Amazon said they were earths most customer centric company).",
          "This post was disappointing when it first ran: I was expecting some content after the basic intro but then it’s just a couple of saved Google searches with no discussion or analysis. It would have been a lot more interesting if they’d discussed anything about the trade offs of the different services or what they liked about a particular combination."
        ],
        "id": "79f3cf80-1316-4640-b4d0-287c1cf27739",
        "url_text": "Guest post by AWS Community Hero Shimon Tolts, CTO and co-founder at Datree.io. He specializes in developer tools and infrastructure, running a company that is 100% serverless. In recent years, there was a major transition in the way you build and ship software. This was mainly around microservices, splitting code into small components, using infrastructure as code, and using Git as the single source of truth that glues it all together. In this post, I discuss the transition and the different steps of modern software development to showcase the possible solutions for the serverless world. In addition, I list useful tools that were designed for this era. What is serverless? Before I dive into the wonderful world of serverless development and tooling, heres what I mean by serverless. The AWS website talks about four main benefits: No server management. Flexible scaling. Pay for value. Automated high availability. To me, serverless is any infrastructure that you dont have to manage and scale yourself. At my company Datree.io, we run 95% of our workload on AWS Fargate and 5% on AWS Lambda. We are a serverless company; we have zero Amazon EC2 instances in our AWS account. For more information, see the following: Datree.io case study Migrating to AWS ECS Fargate in production CON320: Operational Excellence w/ Containerized Workloads Using AWS Fargate (re:Invent 2018) What is GitOps? Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. According to Luis Faceira, a CI/CD consultant, GitOps is a way of working. You might look at it as an approach in which everything starts and ends with Git. Here are some key concepts: Git as the SINGLE source of truth of a system Git as the SINGLE place where we operate (create, change and destroy) ALL environments ALL changes are observable/verifiable. How you built software before the cloud Back in the waterfall pre-cloud era, you used to have separate teams for development, testing, security, operations, monitoring, and so on. Nowadays, in most organizations, there is a transition to full developer autonomy and developers owning the entire production path. The developer is the King or Queen :) Those teams (Ops/Security/IT/etc) used to be gatekeepers to validate and control every developer change. Now they have become more of a satellite unit that drives policy and sets best practices and standards. They are no longer the production bottleneck, so they provide organization-wide platforms and enablement solutions. Everything is codified With the transition into full developer ownership of the entire pipeline, developers automated everything. We have more code than ever, and processes that used to be manual are now described in code. This is a good transition, in my opinion. Here are some of the benefits: Automation: By storing all things as code, everything can be automated, reused, and re-created in moments. Immutable: If anything goes wrong, create it again from the stored configuration. Versioning: Changes can be applied and reverted, and are tracked to a single user who made the change. GitOps: Git has become the single source of truth The second major transition is that now everything is in one place! Git is the place where all of the code is stored and where all operations are initiated. Whether its testing, building, packaging, or releasing, nowadays everything is triggered through pull requests. This is amplified by the codification of everything. Useful tools in the serverless era There are many useful tools in the market, here is a list of ones that were designed for serverless. Code Always store your code in a source control system. In recent years, more and more functions are codified, such as, BI, ops, security, and AI. For new developers, it is not always obvious that they should use source control for some functionality. GitHub AWS CodeCommit GitLab BitBucket Build and test The most common mistake I see is manually configuring build jobs in the GUI. This might be good for a small POC but it is not scalable. You should have your job codified and inside your Git repository. Here are some tools to help with building and testing: AWS CodeBuild CodeFresh GitHub Actions Jenkins-x CircleCI TravisCI Security and governance When working in a serverless way, you end up having many Git repos. The number of code packages can be overwhelming. The demand for unified code standards remains as it was but now it is much harder to enforce it on top of your R&D org. Here are some tools that might help you with the challenge: Snyk Datree PureSec Aqua Protego Bundle and release Building a serverless application is connecting microservices into one unit. For example, you might be using Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. Instead of configuring each one separately, you should use a bundler to hold the configuration in one place. That allows for easy versioning and replication of the app for several environments. Here are a couple of bundlers: Serverless Framework AWS Serverless Application Model (AWS SAM) Package When working with many different serverless components, you should create small packages of tools to be able to import across different Lambda functions. You can use a language-specific store like npm or RubyGems, or use a more holistic solution. Here are several package artifact stores that allow hosting for multiple programming languages: GitHub Package Registry Jfrog Artifactory Sonatype Nexus Monitor This part is especially tricky when working with serverless applications, as everything is split into small pieces. Its important to use monitoring tools that support this mode of work. Here are some tools that can handle serverless: Rookout Amazon CloudWatch Epsagon Lumigo NewRelic DataDog Summary The serverless era brings many transitions along with it like a codification of the entire pipeline and Git being the single source of truth. This doesnt mean that the same problems that we use to have like security, logging and more disappeared, you should continue addressing them and leveraging tools that enable you to focus on your business. ",
        "_version_": 1718527427826679811
      },
      {
        "story_id": [21196107],
        "story_author": ["joebaf"],
        "story_descendants": [145],
        "story_score": [194],
        "story_time": ["2019-10-08T19:25:11Z"],
        "story_title": "C++ Ecosystem: Compilers, IDEs, Tools, Testing",
        "search": [
          "C++ Ecosystem: Compilers, IDEs, Tools, Testing",
          "https://www.bfilipek.com/2019/10/cppecosystem.html",
          "Table of Contents Introduction Compilers GCC Microsoft Visual C++ Clang Intel C++ Compiler Build Tools & Package Managers Make Cmake Ninja Microsoft Build Engine (MSBuild) Conan, Vcpkg, Buckaroo Integrated Development Environments Sublime Text, Atom, And Visual Studio Code Vi/Vim & Emacs Clion Qt Creator C++Builder Visual Studio Xcode KDevelop Eclipse CDT IDE Cevelop Android Studio Oracle Studio Extra: Compiler Explorer & Online Tools Debugging & Testing GDB LLDB Debugging Tools For Windows Mozillas RR CATCH/CATCH2 BOOST.TEST GOOGLE TEST CUTE DocTest Mull Sanitizers Valgrind HeapTrack Dr. Memory Deleaker Summary & More Your Turn To write a professional C++ application, you not only need a basic text editor and a compiler. You require some more tooling. In this blog post, youll see a broad list of tools that make C++ programming possible: compilers, IDEs, debuggers and other. Last Update: 14th October 2019. Note: This is a blog post based on the White Paper created by Embarcadero, see the full paper here: C++ Ecosystem White Paper. Introduction The C++ computer programming language has become one of the most widely used modern programming languages. Software built with C++ is known for its performance and efficiency. C++ has been used to build numerous vastly popular core libraries, applications such as Microsoft Office, game engines such as Unreal, software tools like Adobe Photoshop, compilers like Clang, databases like MySQL, and even operating systems such as Windows across a wide variety of platforms as it continues to evolve and grow. Modern C++ is generally defined as C++ code that utilizes language features in C++11, C++14, and C++17 based compilers. These are language standards named after the year they were defined (2011, 2014 and 2017 respectively) and include a number of significant additions and enhancements to the original core language for powerful, highly performant, and bug-free code. Modern C++ has high-level features that support object-oriented programming, functional programming, generic programming, and low-level memory manipulation features. Big names in the computer industry such as Microsoft, Intel, the Free Software Foundation, and others have their modern C++ compilers. Companies such as Microsoft, The QT Company, JetBrains, and Embarcadero provide integrated development environments for writing code in modern C++. Popular libraries are available for C++ across a wide range of computer disciplines including Artificial Intelligence, Machine Learning, Robotics, Math, Scientific Computing, Audio Processing, and Image Processing. In this blog post, we are going to cover a number of these compilers, build tools, IDEs, libraries, frameworks, coding assistants, and much more that can support and enhance your development with modern C++. Lets get started! Compilers There are a number of popular compilers that support modern C++ including GCC/g++, MSVC (Microsoft Visual C++), Clang and Intel Compiler. Each compiler has varying support for each of the major operating systems with the open-source GCC/g++ originating in the late 1980s, Microsofts Visual C++ in the early 1990s, and Clang in the late 2000s. All four compilers support modern C++ up to at least C++17, but the source code licenses for each of them vary greatly. GCC GCC is a general-use compiler developed and maintained and regularly updated by the GCC Steering committee as part of the GNU Project. GCC describes a large growing family of compilers targeting many hardware platforms and several languages. While it mainly targets Unix-like platforms, Windows support is provided through the Cygwin or MinGW runtime libraries. GCC compiles modern C++ code up to C++17 with experimental support for some C++20 features. It also compiles with a variety of language extensions that build upon C++ standards. It is free and open-source (GPL3) with the GCC Runtime Library Exception. GCC has support from build tools such as CMake and Ninja and many IDEs such as CLion, Qt Creator, and Visual Studio Code. https://gcc.gnu.org/ https://gcc.gnu.org/projects/cxx-status.html Microsoft Visual C++ Microsoft Visual C++ (MSVC) is Microsofts compiler for their custom implementation of the C++ standard, known as Visual C++. It is regularly updated, and like GCC and Clang, supports modern C++ standards up to C++17 with experimental support for some C++20 features. MSVC is the primary method for building C++ applications in Microsofts own Visual Studio. It generally targets a number of architectures on Windows, Android, iOS, and Linux. Support for build tools and IDEs are limited but growing. CMake extensions are available in Visual Studio 2019. MSVC can be used with Visual Studio Code, with limited support from CLion and Qt Creator with additional extensions. MSVC is proprietary and available under a commercial license, but theres also a Community edition. https://en.wikipedia.org/wiki/Microsoft_Visual_C%2B%2B https://devblogs.microsoft.com/visualstudio/ https://visualstudio.microsoft.com/vs/community/ Clang Clang describes a large family of compilers for the C family of languages maintained and regularly developed as part of the LLVM project. Although it targets many popular architectures, it generally targets fewer platforms than GCC. The LLVM project defines Clang through key design principles - strict adherence to C++ standards (although support for GCC extensions is offered), modular design, and minimal modification to the source codes structure during compilation, to name a few. Like GCC, Clang compiles modern C++ code with support for the C++17 standard with experimental C++20 support. It is available under an open-source (Apache License Version 2.0) license. Clang also has widespread support from build tools such as CMake and Ninja and IDEs such as CLion, Qt Creator, Xcode, and others. https://clang.llvm.org/ https://clang.llvm.org/cxx_status.html Intel C++ Compiler Intel C++ Compiler can generate highly optimized code for various Intel CPUs (including Xeon, Core, and Atom processors). The compiler can seamlessly integrate with popular IDE like Visual Studio, GCC toolchain and others. It can leverage advanced instruction set (even AVX512) and generate parallel code (for example, thanks to OpenMP 5.0 support). Intel doesnt ship the compiler with the Standard Library implementation, so it uses the library you provide on your platform. The compiler is available as a part of Intel Parallel Studio XE or Intel System Studio. https://software.intel.com/en-us/c-compilers https://software.intel.com/en-us/articles/c17-features-supported-by-intel-c-compiler On top of compilers, you need an infrastructure that helps to build a whole application: build tools, pipelines and package managers. Make Make is a well-known build system widely used, especially in Unix and Unix-like operating systems. Make is typically used to build executable programs and libraries from source code. But the tool applies to any process that involves executing arbitrary commands to transform a source file to a target result. Make is not tight to any particular programming language. It automatically determines which source files has been changed and then performs the minimal build process to get the final output. It also helps with the installation of the results in the system https://www.gnu.org/software/make/ Cmake CMake is a cross-platform tool for managing your build process. Building, especially large apps and with dependent libraries, can be a very complex process, especially when you support multiple compilers; CMake abstracts this. You can define complex build processes in one common language and convert them to native build directives for any number of supported compilers, IDEs, and build tools, including Ninja (below.) There are versions of CMake available for Windows, macOS, and Linux. https://cmake.org/ Note: Heres a good answer about the differences between Make and Cmake: Difference between using Makefile and CMake to compile the code - Stack Overflow. Ninja The Ninja build system is used for the actual process of building apps and is similar to Make. It focuses on running as fast as possible by parallelizing builds. It is commonly used paired with CMake, which supports creating build files for the Ninja build system. The feature set of Ninja is intentionally kept minimal because the focus is on speed. https://ninja-build.org/ Microsoft Build Engine (MSBuild) MSBuild is a command-line based built platform available from Microsoft under an open-source (MIT) license. It can be used to automate the process of compiling and deploying projects. It is available standalone, packaged with Visual Studio, or from Github. The structure and function of MSBuild files is very similar to Make. MSBuild has an XML based file format and mainly has support for Windows but also macOS and Linux. IDEs such as CLion and C++Builder can integrate with MSBuild as well. https://docs.microsoft.com/en-us/visualstudio/msbuild/msbuild Conan, Vcpkg, Buckaroo Package managers such as Conan, vcpkg, Buckaroo and NIX have been gaining popularity in the C++ community. A package manager is a tool to install libraries or components. Conan is a decentralized open-source (MIT) package manager that supports multiple platforms and all build systems (such as CMake and MSBuild). Conan supports binaries with a goal of automating dependency management to save time in development and continuous integration. Microsofts vcpkg is open source under an MIT license and supports Windows, macOS, and Linux. Out of the box, it makes installed libraries available in Visual Studio, but it also supports CMake build recipes. It can build libs for every toolchain that can be fitted into CMake. Buckaroo is a lesser-known open-source package manager that can pull dependencies from GitHub, BitBucket, GitLab, and others. Buckaroo offers integrations for a number of IDEs including CLion, Visual Studio Code, XCode, and others. Here are the links for the mentioned package managers: https://conan.io/ https://github.com/microsoft/vcpkg https://buckaroo.pm/ Integrated Development Environments A host of editors and integrated development environments (IDEs) can be used for developing with modern C++. Text editors are typically lightweight, but are less featureful than a full IDE and so are used only for the process of writing code, not debugging or testing it. Full development requires other tools, and an IDE contains those and integrates into a cohesive, integrated development environment. Any number of text editors like Sublime Text, Atom, Visual Studio Code, vi/vim, and Emacs can be used for writing C++ code. However, some IDEs are specifically designed with modern C++ in mind like CLion, Qt Creator, and C++Builder, while IDEs like Xcode and Visual Studio also support other languages. You can also compare various IDE for C++ in this handy table on Wikipedia: Comparison of integrated development environments - C++ - Wikipedia Sublime Text, Atom, And Visual Studio Code The list below summarises a set of advanced source code editors that thanks to various plugins and extensions allow creating applications in almost all programming languages. Sublime Text is a commercial text editor with extended support for modern C++ available via plugins. Atom is an open-source (MIT license) text editor that supports modern C++ via packages with integrations available for debugging and compiling. Visual Studio Code is a popular open-source (MIT license) source-code editor from Microsoft. A wide variety of extensions are available that bring features such as debugging and code completion for modern C++ to Visual Studio Code. Sublime Text, Atom, and Visual Studio Code are all available for Windows, macOS, and Linux. Here are the links for the above tools: https://www.sublimetext.com/ https://atom.io/ https://code.visualstudio.com/ Vi/Vim & Emacs Vi/Vim and Emacs are free command-line based text editors that are mainly used on Linux but are also available for macOS and Windows. Modern C++ support can be added to Vi/Vim through the use of scripts while modern C++ support can be added to Emacs through the use of modules. https://www.vim.org/ https://www.gnu.org/software/emacs/ Clion CLion is a commercial IDE from JetBrains that supports modern C++. It can be used with build tools like CMake and Gradle, integrates with the GDB and LLDB debuggers, can be used with version control systems like Git, test libraries like Boost.Test, and various documentation tools. It has features such as code generation, refactoring, on the fly code analysis, symbol navigation, and more. https://www.jetbrains.com/clion/ Qt Creator Qt Creator is a (non)commercial IDE from The Qt Company which supports Windows, macOS, and Linux. Qt Creator has features such as a UI designer, syntax highlighting, auto-completion, and integration with a number of different modern C++ compilers like GCC and Clang. Qt Creator tightly integrates with the Qt library for rapidly building cross-platform applications. Additionally, it integrates with standard version control systems like Git, debuggers like GDB and LLDB, build systems like CMake, and can deploy cross-platform to iOS and Android devices. https://www.qt.io/ C++Builder C++Builder is a commercial IDE from Embarcadero Technologies which runs on Windows and supports modern C++. It features the award-winning Visual Component Library (VCL) for Windows development and FireMonkey (FMX) for cross-platform development for Windows, iOS and Android. The C++Builder compiler features an enhanced version of Clang, an integrated debugger, visual UI designer, database library, comprehensive RTL, and standard features like syntax highlighting, code completion, and refactoring. C++Builder has integrations for CMake, can be used with Ninja, and also MSBuild. https://www.embarcadero.com/products/cbuilder https://www.embarcadero.com/products/cbuilder/starter Visual Studio Visual C++ is a commercial Visual Studio IDE from Microsoft. Visual Studio integrates building, debugging, and testing within the IDE. It provides the Microsoft Foundation Class (MFC) library which gives access to the Win32 APIs. Visual Studio features a visual UI designer for certain platforms, comes with MSBuild, supports CMake, and provides standard features such as code completion, refactoring, and syntax highlighting. Additionally, Visual Studio supports a number of other programming languages, and the C++ side of it is focused on Windows, with other platforms slowly being added. https://visualstudio.microsoft.com/ Xcode Xcode is a multi-language IDE from Apple available only on macOS that supports modern C++. Xcode is proprietary but available for free from Apple. Xcode has an integrated debugger, supports version control systems like Git, features a Clang compiler, and utilizes libc++ as its standard library. It supports standard features such as syntax highlighting, code completion, and finally, Xcode supports external build systems like CMake and utilizes the LLDB debugger. https://developer.apple.com/xcode/ KDevelop KDevelop (its 0.1 version was released in 1998) is a cross-platform IDE for C, C++, Python, QML/JavaScript and PHP. This IDE is part of the KDE project, and is based on KDE Frameworks and Qt. The C/C++ backend uses Clang and LLVM. It has UI integration with several version control systems: Git, SVN, Bazaar and more, build process based on CMake, QMake or custom makefiles. Among many interesting features, its essential to mention advanced syntax colouring and Context-sensitive, semantic code completion. https://www.kdevelop.org/ https://www.kdevelop.org/features Eclipse CDT IDE The Eclipse C/C++ Development Toolkit (CDT) is a combination of the Eclipse IDE with a C++ toolchain (usually GNU - GCC). This IDE supports project creation and build management for various toolchains, like the standard make build. CDT IDE offers source navigation, various source knowledge tools, such as type hierarchy, call graph, include browser, macro definition browser, code editor with syntax highlighting, folding and hyperlink navigation, source code refactoring and code generation, visual debugging tools, including memory, registers, and disassembly viewers. https://www.eclipse.org/cdt/ Cevelop Cevelop is a powerful IDE based Eclipse CDT. Its main strength lies in the powerful refactoring and static analysis support for code modernization. In addition, it comes with unit testing and TDD support for the CUTE unit testing framework. Whats more, you can easily visualize your template instantiation/function overload resolution and optimize includes. https://www.cevelop.com/ Android Studio Android Studio is the official IDE for Googles Android operating system, built on JetBrains IntelliJ IDEA software and designed specifically for Android development. It is available for download on Windows, macOS and Linux based operating systems. It is a replacement for the Eclipse Android Development Tools (ADT) as the primary IDE for native Android application development. Android Studio focuses mainly on Kotlin but you can also write applications in C++. Oracle Studio Oracle Developer Studio is Oracle Corporations flagship software development product for the Solaris and Linux operating systems. It includes optimizing C, C++, and Fortran compilers, libraries, and performance analysis and debugging tools, for Solaris on SPARC and x86 platforms, and Linux on x86/x64 platforms, including multi-core systems. You can download Developer Studio at no charge but if you want the full support and patch updates, then you need a paid support contract. The C++ Compiler supports C++14. https://www.oracle.com/technetwork/server-storage/developerstudio/overview/index.html https://www.oracle.com/technetwork/server-storage/solarisstudio/features/compilers-2332272.html If you want to check some shorter code samples and you dont want to install the whole compiler/.IDE suite then we have lots of online tools that can make those tests super simple. Just open a web browser and put the code Compiler Explorer is a web-based tool that allows you to select from a wide variety of C++ compilers and different versions of the same compiler to test out your code. This allows developers to compare the generated code for specific C++ constructs among compilers, and test for correct behaviour. Clang, GCC, and MSVC are all there but also lesser-known compilers such as DJGPP, ELLCC, Intel C++, and others. https://godbolt.org/ Extra: Heres a list of handy online compilers that you can use: like Coliru, Wandbox, CppInsighs and more: https://arnemertz.github.io/online-compilers/ Debugging & Testing GDB GDB is a portable command-line based debugging platform that supports modern C++ and is available under an open-source license (GPL). A number of editors and IDEs like Visual Studio, Qt Creator, and CLion support integration with GDB. It can also be used to debug applications remotely where GDB is running on one device, and the application being debugged is running on another device. It supports a number of platforms including Windows, macOS, and Linux. https://www.gnu.org/software/gdb/ LLDB LLDB is an open-source debugging interface that supports modern C++ and integrates with the Clang compiler. It has a number of optional performance-enhancing features such as JIT but also supports debugging memory, multiple threads, and machine code analysis. It is built in C++. LLDB is the default debugger for Xcode and can be used with Visual Studio Code, CLion, and Qt Creator. It supports a number of platforms including Windows, macOS, and Linux. https://lldb.llvm.org/ Debugging Tools For Windows On Windows, you can use several debuggers, ranging from Visual Studio Debugger (integrated and one of the most user-friendly), WinDBG, CDB and several more. WinDbg is a multipurpose debugger for the Microsoft Windows Platform. It can be used to debug user-mode applications, device drivers, and the operating system itself in kernel mode. It has a graphical user interface (GUI) and is more powerful than Visual Studio Debugger. You can debug memory dumps obtained even from kernel drivers. One of the recent exciting features in Debugging on Windows is called Time Travel Debugging (Available in WinDBG Preview and also in Visual Studio Ultimate). It allows you to record the execution of the process and then replay the steps backwards or forwards. This flexibility enables us to easily tracks back the state that caused a bug. https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/ https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/time-travel-debugging-overview Mozillas RR RR is an advanced debugger that aims to replace GDB on Linux. It offers the full state recordings of the application so that you can replay the action backwards and forwards (similarly to Time Travel Debugging). The debugger is used to work with large applications like Chrome, OpenOffice or even Firefox code bases. https://rr-project.org/ CATCH/CATCH2 Catch2 is a cross-platform open-source (BSL-1.0) testing framework for modern C++. It is very lightweight because only a header file needs to be included. Unit tests can be tagged and run in groups. It supports both test-driven development and behaviour-driven development. Catch2 also easily integrates with CLion. https://github.com/catchorg/Catch2 BOOST.TEST Boost.Test is a feature-rich open-source (BSL-1.0) testing framework that utilizes modern C++ standards. It can be used to quickly detect errors, failures, and time outs through customizable logging and real-time monitoring. Tests can be grouped into suites, and the framework supports both small scale testing and large scale testing. https://github.com/boostorg/test GOOGLE TEST Google Test is Googles C++ testing and mocking framework, which is available under an open-source (BSD) license. Google test can be used on a broad range of platforms, including Linux, macOS, Windows, and others. It contains a unit testing framework, assertions, death tests, detects failures, handles parameterized tests, and creates XML test reports. https://github.com/google/googletest CUTE CUTE is a unit testing framework integrated into Cevelop, but it can also be used standalone. It spans C++ versions from c++98 to c++2a and is header-only. While not as popular as Google Test it is less macro-ridden and uses macros only, where no appropriate C++ feature is available. In addition, it features a mode that easily allows it to run on embedded platforms, by sidestepping some of the I/O formatting features. https://cute-test.com/ DocTest DocTest is a single-header unit testing framework. Available for C++11 up to C++20 and is easy to configure and works on probably all platforms. It offers regular TDD testing macros (also with subcases) as well as BDD-style test cases. http://bit.ly/doctest-docs https://github.com/onqtam/doctest Mull Mull is an LLVM-based tool for Mutation Testing with a strong focus on C and C++ languages. In general, it creates many variations of the input source code (using LLVM bytecode) and then checks it against the test cases. Thanks to this advanced testing technique, you can make your code more secure. https://github.com/mull-project/mull PDF: https://lowlevelbits.org/pdfs/Mull_Mutation_2018.pdf Sanitizers AddressSanitizer - https://clang.llvm.org/docs/AddressSanitizer.html (supported in Clang, GCC and XCode) UndefinedBehaviorSanitizer - https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html LeakSanitizer - https://clang.llvm.org/docs/LeakSanitizer.html Application Verifier for Windows - https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/application-verifier Sanitizers are relatively new tools that add extra instrumentation to your application (for example they replace new/malloc/delete calls) and can detect various runtime errors: leaks, use after delete, double free and many others. To improve your build pipeline, many guides advice to add sanitizers steps when doing tests. Most sanitizers come from the LLVM/Clang platform, but now they also work with GCC. Unfortunately not yet with Visual Studio (but you can try Application Verifier). Valgrind Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. When you run a program through Valgrind its run on a virtual machine that emulates your host environment. Having that abstraction the tools can leverage various information about the source code and its execution. http://valgrind.org/ http://valgrind.org/info/about.html http://valgrind.org/docs/manual/quick-start.html HeapTrack HeapTrack is a FOSS project and a heap memory profiler for Linux. It traces all memory allocations and annotates these events with stack traces. The tool has two forms the command line version that grabs the data, and then the UI part that you can use to read and analyze the results. This tool is comparable to Valgrinds massif; its easier to use and should be faster to load and analyze for large projects. https://github.com/KDE/heaptrack Dr. Memory Dr. Memory is an LGPL licenced tool that allows you to monitor and intensify memory -related errors for binaries on Windows, Linux, Mac, Android. Its based on the DynamoRIO dynamic instrumentation tool platform. With the tool, you can find errors like double frees, memory leaks, handle leaks (on Windows), GDI issues, access to uninitialized memory or even errors in multithreading memory scenarios. http://drmemory.org/ https://github.com/DynamoRIO/drmemory Deleaker The primary role of Deleaker is to find leaks in your native applications. It supports Visual Studio (since 2008 till the latest 2019 version), Delphi/C++ Builder, Qt Creator, CLion (soon!). Can be used as an extension in Visual Studio or as a standalone application. Deleaker tracks leaks in C/C++ applications (Native and CLR), plus .NET code. Memory (new/delete, malloc), GDI objects, User32 objects, Handles, File views, Fibres, Critical Sections, and even more. It gathers full call stack, ability to take snapshots, compare them, view source files related to allocation. https://www.deleaker.com/ https://www.deleaker.com/docs/deleaker/tutorial.html Summary & More I hope that with the above list, you get a useful overview of the tools that are essential for C++ development. If you want to read more about other ecosystem elements: libraries, frameworks, and other tools, then please see the full report from Embarcadero: C++ Ecosystem White Paper (Its a nice looking pdf, with more than 20 pages of content!) You might check this Resource for a super long list of tools, libs, frameworks that enhance C++ development: https://github.com/fffaraz/awesome-cpp Your Turn What are your favourite tools that you use when writing C++ apps? ",
          "CLion is an amazing tool -- I've purchased licenses for my personal self in the past, but my employer pays for it these days.<p>My problem is they've done a terrible job of making it scale up to large code bases. I work on the chromium tree -- CLion is completely useless on it.  I have a dual 24-core xeon with 128GB of RAM and SSD and I've given it a wackload of memory, and it becomes completely inoperable, freezing all over the place.<p>Awful because I have such muscle memory for the JetBrains tools, and such a fondness for them.<p>I've gone back to using Emacs, but now with Eclim. I just couldn't get into VSCode.",
          "I had to use Qt as the UI lib for a project, it made me discover that QtCreator was actually not a Qt-only tool but a very good lightweight and generic C++ IDE.<p>That's my choice now. I need something that can navigate easily in a code base, I don't really like learning all the oddities around emacs and vim (even though I am a bit competent at vim) and I don't see what is so bad in using a mouse.<p>At first I thought annoying to have to manually edit the .includes and .config to add the includes and the macro I needed in our complex, hard-to-parse CMake based project, but now I really enjoy the freedom it gives."
        ],
        "story_type": ["Normal"],
        "url": "https://www.bfilipek.com/2019/10/cppecosystem.html",
        "comments.comment_id": [21199733, 21201412],
        "comments.comment_author": ["cmrdporcupine", "Iv"],
        "comments.comment_descendants": [9, 0],
        "comments.comment_time": [
          "2019-10-09T03:27:23Z",
          "2019-10-09T09:10:25Z"
        ],
        "comments.comment_text": [
          "CLion is an amazing tool -- I've purchased licenses for my personal self in the past, but my employer pays for it these days.<p>My problem is they've done a terrible job of making it scale up to large code bases. I work on the chromium tree -- CLion is completely useless on it.  I have a dual 24-core xeon with 128GB of RAM and SSD and I've given it a wackload of memory, and it becomes completely inoperable, freezing all over the place.<p>Awful because I have such muscle memory for the JetBrains tools, and such a fondness for them.<p>I've gone back to using Emacs, but now with Eclim. I just couldn't get into VSCode.",
          "I had to use Qt as the UI lib for a project, it made me discover that QtCreator was actually not a Qt-only tool but a very good lightweight and generic C++ IDE.<p>That's my choice now. I need something that can navigate easily in a code base, I don't really like learning all the oddities around emacs and vim (even though I am a bit competent at vim) and I don't see what is so bad in using a mouse.<p>At first I thought annoying to have to manually edit the .includes and .config to add the includes and the macro I needed in our complex, hard-to-parse CMake based project, but now I really enjoy the freedom it gives."
        ],
        "id": "21cd7c54-9e56-44e3-a6c1-cef295105008",
        "url_text": "Table of Contents Introduction Compilers GCC Microsoft Visual C++ Clang Intel C++ Compiler Build Tools & Package Managers Make Cmake Ninja Microsoft Build Engine (MSBuild) Conan, Vcpkg, Buckaroo Integrated Development Environments Sublime Text, Atom, And Visual Studio Code Vi/Vim & Emacs Clion Qt Creator C++Builder Visual Studio Xcode KDevelop Eclipse CDT IDE Cevelop Android Studio Oracle Studio Extra: Compiler Explorer & Online Tools Debugging & Testing GDB LLDB Debugging Tools For Windows Mozillas RR CATCH/CATCH2 BOOST.TEST GOOGLE TEST CUTE DocTest Mull Sanitizers Valgrind HeapTrack Dr. Memory Deleaker Summary & More Your Turn To write a professional C++ application, you not only need a basic text editor and a compiler. You require some more tooling. In this blog post, youll see a broad list of tools that make C++ programming possible: compilers, IDEs, debuggers and other. Last Update: 14th October 2019. Note: This is a blog post based on the White Paper created by Embarcadero, see the full paper here: C++ Ecosystem White Paper. Introduction The C++ computer programming language has become one of the most widely used modern programming languages. Software built with C++ is known for its performance and efficiency. C++ has been used to build numerous vastly popular core libraries, applications such as Microsoft Office, game engines such as Unreal, software tools like Adobe Photoshop, compilers like Clang, databases like MySQL, and even operating systems such as Windows across a wide variety of platforms as it continues to evolve and grow. Modern C++ is generally defined as C++ code that utilizes language features in C++11, C++14, and C++17 based compilers. These are language standards named after the year they were defined (2011, 2014 and 2017 respectively) and include a number of significant additions and enhancements to the original core language for powerful, highly performant, and bug-free code. Modern C++ has high-level features that support object-oriented programming, functional programming, generic programming, and low-level memory manipulation features. Big names in the computer industry such as Microsoft, Intel, the Free Software Foundation, and others have their modern C++ compilers. Companies such as Microsoft, The QT Company, JetBrains, and Embarcadero provide integrated development environments for writing code in modern C++. Popular libraries are available for C++ across a wide range of computer disciplines including Artificial Intelligence, Machine Learning, Robotics, Math, Scientific Computing, Audio Processing, and Image Processing. In this blog post, we are going to cover a number of these compilers, build tools, IDEs, libraries, frameworks, coding assistants, and much more that can support and enhance your development with modern C++. Lets get started! Compilers There are a number of popular compilers that support modern C++ including GCC/g++, MSVC (Microsoft Visual C++), Clang and Intel Compiler. Each compiler has varying support for each of the major operating systems with the open-source GCC/g++ originating in the late 1980s, Microsofts Visual C++ in the early 1990s, and Clang in the late 2000s. All four compilers support modern C++ up to at least C++17, but the source code licenses for each of them vary greatly. GCC GCC is a general-use compiler developed and maintained and regularly updated by the GCC Steering committee as part of the GNU Project. GCC describes a large growing family of compilers targeting many hardware platforms and several languages. While it mainly targets Unix-like platforms, Windows support is provided through the Cygwin or MinGW runtime libraries. GCC compiles modern C++ code up to C++17 with experimental support for some C++20 features. It also compiles with a variety of language extensions that build upon C++ standards. It is free and open-source (GPL3) with the GCC Runtime Library Exception. GCC has support from build tools such as CMake and Ninja and many IDEs such as CLion, Qt Creator, and Visual Studio Code. https://gcc.gnu.org/ https://gcc.gnu.org/projects/cxx-status.html Microsoft Visual C++ Microsoft Visual C++ (MSVC) is Microsofts compiler for their custom implementation of the C++ standard, known as Visual C++. It is regularly updated, and like GCC and Clang, supports modern C++ standards up to C++17 with experimental support for some C++20 features. MSVC is the primary method for building C++ applications in Microsofts own Visual Studio. It generally targets a number of architectures on Windows, Android, iOS, and Linux. Support for build tools and IDEs are limited but growing. CMake extensions are available in Visual Studio 2019. MSVC can be used with Visual Studio Code, with limited support from CLion and Qt Creator with additional extensions. MSVC is proprietary and available under a commercial license, but theres also a Community edition. https://en.wikipedia.org/wiki/Microsoft_Visual_C%2B%2B https://devblogs.microsoft.com/visualstudio/ https://visualstudio.microsoft.com/vs/community/ Clang Clang describes a large family of compilers for the C family of languages maintained and regularly developed as part of the LLVM project. Although it targets many popular architectures, it generally targets fewer platforms than GCC. The LLVM project defines Clang through key design principles - strict adherence to C++ standards (although support for GCC extensions is offered), modular design, and minimal modification to the source codes structure during compilation, to name a few. Like GCC, Clang compiles modern C++ code with support for the C++17 standard with experimental C++20 support. It is available under an open-source (Apache License Version 2.0) license. Clang also has widespread support from build tools such as CMake and Ninja and IDEs such as CLion, Qt Creator, Xcode, and others. https://clang.llvm.org/ https://clang.llvm.org/cxx_status.html Intel C++ Compiler Intel C++ Compiler can generate highly optimized code for various Intel CPUs (including Xeon, Core, and Atom processors). The compiler can seamlessly integrate with popular IDE like Visual Studio, GCC toolchain and others. It can leverage advanced instruction set (even AVX512) and generate parallel code (for example, thanks to OpenMP 5.0 support). Intel doesnt ship the compiler with the Standard Library implementation, so it uses the library you provide on your platform. The compiler is available as a part of Intel Parallel Studio XE or Intel System Studio. https://software.intel.com/en-us/c-compilers https://software.intel.com/en-us/articles/c17-features-supported-by-intel-c-compiler On top of compilers, you need an infrastructure that helps to build a whole application: build tools, pipelines and package managers. Make Make is a well-known build system widely used, especially in Unix and Unix-like operating systems. Make is typically used to build executable programs and libraries from source code. But the tool applies to any process that involves executing arbitrary commands to transform a source file to a target result. Make is not tight to any particular programming language. It automatically determines which source files has been changed and then performs the minimal build process to get the final output. It also helps with the installation of the results in the system https://www.gnu.org/software/make/ Cmake CMake is a cross-platform tool for managing your build process. Building, especially large apps and with dependent libraries, can be a very complex process, especially when you support multiple compilers; CMake abstracts this. You can define complex build processes in one common language and convert them to native build directives for any number of supported compilers, IDEs, and build tools, including Ninja (below.) There are versions of CMake available for Windows, macOS, and Linux. https://cmake.org/ Note: Heres a good answer about the differences between Make and Cmake: Difference between using Makefile and CMake to compile the code - Stack Overflow. Ninja The Ninja build system is used for the actual process of building apps and is similar to Make. It focuses on running as fast as possible by parallelizing builds. It is commonly used paired with CMake, which supports creating build files for the Ninja build system. The feature set of Ninja is intentionally kept minimal because the focus is on speed. https://ninja-build.org/ Microsoft Build Engine (MSBuild) MSBuild is a command-line based built platform available from Microsoft under an open-source (MIT) license. It can be used to automate the process of compiling and deploying projects. It is available standalone, packaged with Visual Studio, or from Github. The structure and function of MSBuild files is very similar to Make. MSBuild has an XML based file format and mainly has support for Windows but also macOS and Linux. IDEs such as CLion and C++Builder can integrate with MSBuild as well. https://docs.microsoft.com/en-us/visualstudio/msbuild/msbuild Conan, Vcpkg, Buckaroo Package managers such as Conan, vcpkg, Buckaroo and NIX have been gaining popularity in the C++ community. A package manager is a tool to install libraries or components. Conan is a decentralized open-source (MIT) package manager that supports multiple platforms and all build systems (such as CMake and MSBuild). Conan supports binaries with a goal of automating dependency management to save time in development and continuous integration. Microsofts vcpkg is open source under an MIT license and supports Windows, macOS, and Linux. Out of the box, it makes installed libraries available in Visual Studio, but it also supports CMake build recipes. It can build libs for every toolchain that can be fitted into CMake. Buckaroo is a lesser-known open-source package manager that can pull dependencies from GitHub, BitBucket, GitLab, and others. Buckaroo offers integrations for a number of IDEs including CLion, Visual Studio Code, XCode, and others. Here are the links for the mentioned package managers: https://conan.io/ https://github.com/microsoft/vcpkg https://buckaroo.pm/ Integrated Development Environments A host of editors and integrated development environments (IDEs) can be used for developing with modern C++. Text editors are typically lightweight, but are less featureful than a full IDE and so are used only for the process of writing code, not debugging or testing it. Full development requires other tools, and an IDE contains those and integrates into a cohesive, integrated development environment. Any number of text editors like Sublime Text, Atom, Visual Studio Code, vi/vim, and Emacs can be used for writing C++ code. However, some IDEs are specifically designed with modern C++ in mind like CLion, Qt Creator, and C++Builder, while IDEs like Xcode and Visual Studio also support other languages. You can also compare various IDE for C++ in this handy table on Wikipedia: Comparison of integrated development environments - C++ - Wikipedia Sublime Text, Atom, And Visual Studio Code The list below summarises a set of advanced source code editors that thanks to various plugins and extensions allow creating applications in almost all programming languages. Sublime Text is a commercial text editor with extended support for modern C++ available via plugins. Atom is an open-source (MIT license) text editor that supports modern C++ via packages with integrations available for debugging and compiling. Visual Studio Code is a popular open-source (MIT license) source-code editor from Microsoft. A wide variety of extensions are available that bring features such as debugging and code completion for modern C++ to Visual Studio Code. Sublime Text, Atom, and Visual Studio Code are all available for Windows, macOS, and Linux. Here are the links for the above tools: https://www.sublimetext.com/ https://atom.io/ https://code.visualstudio.com/ Vi/Vim & Emacs Vi/Vim and Emacs are free command-line based text editors that are mainly used on Linux but are also available for macOS and Windows. Modern C++ support can be added to Vi/Vim through the use of scripts while modern C++ support can be added to Emacs through the use of modules. https://www.vim.org/ https://www.gnu.org/software/emacs/ Clion CLion is a commercial IDE from JetBrains that supports modern C++. It can be used with build tools like CMake and Gradle, integrates with the GDB and LLDB debuggers, can be used with version control systems like Git, test libraries like Boost.Test, and various documentation tools. It has features such as code generation, refactoring, on the fly code analysis, symbol navigation, and more. https://www.jetbrains.com/clion/ Qt Creator Qt Creator is a (non)commercial IDE from The Qt Company which supports Windows, macOS, and Linux. Qt Creator has features such as a UI designer, syntax highlighting, auto-completion, and integration with a number of different modern C++ compilers like GCC and Clang. Qt Creator tightly integrates with the Qt library for rapidly building cross-platform applications. Additionally, it integrates with standard version control systems like Git, debuggers like GDB and LLDB, build systems like CMake, and can deploy cross-platform to iOS and Android devices. https://www.qt.io/ C++Builder C++Builder is a commercial IDE from Embarcadero Technologies which runs on Windows and supports modern C++. It features the award-winning Visual Component Library (VCL) for Windows development and FireMonkey (FMX) for cross-platform development for Windows, iOS and Android. The C++Builder compiler features an enhanced version of Clang, an integrated debugger, visual UI designer, database library, comprehensive RTL, and standard features like syntax highlighting, code completion, and refactoring. C++Builder has integrations for CMake, can be used with Ninja, and also MSBuild. https://www.embarcadero.com/products/cbuilder https://www.embarcadero.com/products/cbuilder/starter Visual Studio Visual C++ is a commercial Visual Studio IDE from Microsoft. Visual Studio integrates building, debugging, and testing within the IDE. It provides the Microsoft Foundation Class (MFC) library which gives access to the Win32 APIs. Visual Studio features a visual UI designer for certain platforms, comes with MSBuild, supports CMake, and provides standard features such as code completion, refactoring, and syntax highlighting. Additionally, Visual Studio supports a number of other programming languages, and the C++ side of it is focused on Windows, with other platforms slowly being added. https://visualstudio.microsoft.com/ Xcode Xcode is a multi-language IDE from Apple available only on macOS that supports modern C++. Xcode is proprietary but available for free from Apple. Xcode has an integrated debugger, supports version control systems like Git, features a Clang compiler, and utilizes libc++ as its standard library. It supports standard features such as syntax highlighting, code completion, and finally, Xcode supports external build systems like CMake and utilizes the LLDB debugger. https://developer.apple.com/xcode/ KDevelop KDevelop (its 0.1 version was released in 1998) is a cross-platform IDE for C, C++, Python, QML/JavaScript and PHP. This IDE is part of the KDE project, and is based on KDE Frameworks and Qt. The C/C++ backend uses Clang and LLVM. It has UI integration with several version control systems: Git, SVN, Bazaar and more, build process based on CMake, QMake or custom makefiles. Among many interesting features, its essential to mention advanced syntax colouring and Context-sensitive, semantic code completion. https://www.kdevelop.org/ https://www.kdevelop.org/features Eclipse CDT IDE The Eclipse C/C++ Development Toolkit (CDT) is a combination of the Eclipse IDE with a C++ toolchain (usually GNU - GCC). This IDE supports project creation and build management for various toolchains, like the standard make build. CDT IDE offers source navigation, various source knowledge tools, such as type hierarchy, call graph, include browser, macro definition browser, code editor with syntax highlighting, folding and hyperlink navigation, source code refactoring and code generation, visual debugging tools, including memory, registers, and disassembly viewers. https://www.eclipse.org/cdt/ Cevelop Cevelop is a powerful IDE based Eclipse CDT. Its main strength lies in the powerful refactoring and static analysis support for code modernization. In addition, it comes with unit testing and TDD support for the CUTE unit testing framework. Whats more, you can easily visualize your template instantiation/function overload resolution and optimize includes. https://www.cevelop.com/ Android Studio Android Studio is the official IDE for Googles Android operating system, built on JetBrains IntelliJ IDEA software and designed specifically for Android development. It is available for download on Windows, macOS and Linux based operating systems. It is a replacement for the Eclipse Android Development Tools (ADT) as the primary IDE for native Android application development. Android Studio focuses mainly on Kotlin but you can also write applications in C++. Oracle Studio Oracle Developer Studio is Oracle Corporations flagship software development product for the Solaris and Linux operating systems. It includes optimizing C, C++, and Fortran compilers, libraries, and performance analysis and debugging tools, for Solaris on SPARC and x86 platforms, and Linux on x86/x64 platforms, including multi-core systems. You can download Developer Studio at no charge but if you want the full support and patch updates, then you need a paid support contract. The C++ Compiler supports C++14. https://www.oracle.com/technetwork/server-storage/developerstudio/overview/index.html https://www.oracle.com/technetwork/server-storage/solarisstudio/features/compilers-2332272.html If you want to check some shorter code samples and you dont want to install the whole compiler/.IDE suite then we have lots of online tools that can make those tests super simple. Just open a web browser and put the code Compiler Explorer is a web-based tool that allows you to select from a wide variety of C++ compilers and different versions of the same compiler to test out your code. This allows developers to compare the generated code for specific C++ constructs among compilers, and test for correct behaviour. Clang, GCC, and MSVC are all there but also lesser-known compilers such as DJGPP, ELLCC, Intel C++, and others. https://godbolt.org/ Extra: Heres a list of handy online compilers that you can use: like Coliru, Wandbox, CppInsighs and more: https://arnemertz.github.io/online-compilers/ Debugging & Testing GDB GDB is a portable command-line based debugging platform that supports modern C++ and is available under an open-source license (GPL). A number of editors and IDEs like Visual Studio, Qt Creator, and CLion support integration with GDB. It can also be used to debug applications remotely where GDB is running on one device, and the application being debugged is running on another device. It supports a number of platforms including Windows, macOS, and Linux. https://www.gnu.org/software/gdb/ LLDB LLDB is an open-source debugging interface that supports modern C++ and integrates with the Clang compiler. It has a number of optional performance-enhancing features such as JIT but also supports debugging memory, multiple threads, and machine code analysis. It is built in C++. LLDB is the default debugger for Xcode and can be used with Visual Studio Code, CLion, and Qt Creator. It supports a number of platforms including Windows, macOS, and Linux. https://lldb.llvm.org/ Debugging Tools For Windows On Windows, you can use several debuggers, ranging from Visual Studio Debugger (integrated and one of the most user-friendly), WinDBG, CDB and several more. WinDbg is a multipurpose debugger for the Microsoft Windows Platform. It can be used to debug user-mode applications, device drivers, and the operating system itself in kernel mode. It has a graphical user interface (GUI) and is more powerful than Visual Studio Debugger. You can debug memory dumps obtained even from kernel drivers. One of the recent exciting features in Debugging on Windows is called Time Travel Debugging (Available in WinDBG Preview and also in Visual Studio Ultimate). It allows you to record the execution of the process and then replay the steps backwards or forwards. This flexibility enables us to easily tracks back the state that caused a bug. https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/ https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/time-travel-debugging-overview Mozillas RR RR is an advanced debugger that aims to replace GDB on Linux. It offers the full state recordings of the application so that you can replay the action backwards and forwards (similarly to Time Travel Debugging). The debugger is used to work with large applications like Chrome, OpenOffice or even Firefox code bases. https://rr-project.org/ CATCH/CATCH2 Catch2 is a cross-platform open-source (BSL-1.0) testing framework for modern C++. It is very lightweight because only a header file needs to be included. Unit tests can be tagged and run in groups. It supports both test-driven development and behaviour-driven development. Catch2 also easily integrates with CLion. https://github.com/catchorg/Catch2 BOOST.TEST Boost.Test is a feature-rich open-source (BSL-1.0) testing framework that utilizes modern C++ standards. It can be used to quickly detect errors, failures, and time outs through customizable logging and real-time monitoring. Tests can be grouped into suites, and the framework supports both small scale testing and large scale testing. https://github.com/boostorg/test GOOGLE TEST Google Test is Googles C++ testing and mocking framework, which is available under an open-source (BSD) license. Google test can be used on a broad range of platforms, including Linux, macOS, Windows, and others. It contains a unit testing framework, assertions, death tests, detects failures, handles parameterized tests, and creates XML test reports. https://github.com/google/googletest CUTE CUTE is a unit testing framework integrated into Cevelop, but it can also be used standalone. It spans C++ versions from c++98 to c++2a and is header-only. While not as popular as Google Test it is less macro-ridden and uses macros only, where no appropriate C++ feature is available. In addition, it features a mode that easily allows it to run on embedded platforms, by sidestepping some of the I/O formatting features. https://cute-test.com/ DocTest DocTest is a single-header unit testing framework. Available for C++11 up to C++20 and is easy to configure and works on probably all platforms. It offers regular TDD testing macros (also with subcases) as well as BDD-style test cases. http://bit.ly/doctest-docs https://github.com/onqtam/doctest Mull Mull is an LLVM-based tool for Mutation Testing with a strong focus on C and C++ languages. In general, it creates many variations of the input source code (using LLVM bytecode) and then checks it against the test cases. Thanks to this advanced testing technique, you can make your code more secure. https://github.com/mull-project/mull PDF: https://lowlevelbits.org/pdfs/Mull_Mutation_2018.pdf Sanitizers AddressSanitizer - https://clang.llvm.org/docs/AddressSanitizer.html (supported in Clang, GCC and XCode) UndefinedBehaviorSanitizer - https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html LeakSanitizer - https://clang.llvm.org/docs/LeakSanitizer.html Application Verifier for Windows - https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/application-verifier Sanitizers are relatively new tools that add extra instrumentation to your application (for example they replace new/malloc/delete calls) and can detect various runtime errors: leaks, use after delete, double free and many others. To improve your build pipeline, many guides advice to add sanitizers steps when doing tests. Most sanitizers come from the LLVM/Clang platform, but now they also work with GCC. Unfortunately not yet with Visual Studio (but you can try Application Verifier). Valgrind Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. When you run a program through Valgrind its run on a virtual machine that emulates your host environment. Having that abstraction the tools can leverage various information about the source code and its execution. http://valgrind.org/ http://valgrind.org/info/about.html http://valgrind.org/docs/manual/quick-start.html HeapTrack HeapTrack is a FOSS project and a heap memory profiler for Linux. It traces all memory allocations and annotates these events with stack traces. The tool has two forms the command line version that grabs the data, and then the UI part that you can use to read and analyze the results. This tool is comparable to Valgrinds massif; its easier to use and should be faster to load and analyze for large projects. https://github.com/KDE/heaptrack Dr. Memory Dr. Memory is an LGPL licenced tool that allows you to monitor and intensify memory -related errors for binaries on Windows, Linux, Mac, Android. Its based on the DynamoRIO dynamic instrumentation tool platform. With the tool, you can find errors like double frees, memory leaks, handle leaks (on Windows), GDI issues, access to uninitialized memory or even errors in multithreading memory scenarios. http://drmemory.org/ https://github.com/DynamoRIO/drmemory Deleaker The primary role of Deleaker is to find leaks in your native applications. It supports Visual Studio (since 2008 till the latest 2019 version), Delphi/C++ Builder, Qt Creator, CLion (soon!). Can be used as an extension in Visual Studio or as a standalone application. Deleaker tracks leaks in C/C++ applications (Native and CLR), plus .NET code. Memory (new/delete, malloc), GDI objects, User32 objects, Handles, File views, Fibres, Critical Sections, and even more. It gathers full call stack, ability to take snapshots, compare them, view source files related to allocation. https://www.deleaker.com/ https://www.deleaker.com/docs/deleaker/tutorial.html Summary & More I hope that with the above list, you get a useful overview of the tools that are essential for C++ development. If you want to read more about other ecosystem elements: libraries, frameworks, and other tools, then please see the full report from Embarcadero: C++ Ecosystem White Paper (Its a nice looking pdf, with more than 20 pages of content!) You might check this Resource for a super long list of tools, libs, frameworks that enhance C++ development: https://github.com/fffaraz/awesome-cpp Your Turn What are your favourite tools that you use when writing C++ apps? ",
        "_version_": 1718527430422953984
      },
      {
        "story_id": [21511411],
        "story_author": ["whoisnnamdi"],
        "story_descendants": [70],
        "story_score": [293],
        "story_time": ["2019-11-12T04:01:34Z"],
        "story_title": "Building a Developer Tools Business",
        "search": [
          "Building a Developer Tools Business",
          "https://manifold.co/blog/founders-guide-developer-tools-sell-to-engineers",
          "Proudly powered by LiteSpeed Web ServerPlease be advised that LiteSpeed Technologies Inc. is not a web hosting company and, as such, has no control over content found on this site. ",
          "I would also recommend dev tools founders (really any founder) read GitLab’s company handbook at <a href=\"https://about.gitlab.com/handbook/\" rel=\"nofollow\">https://about.gitlab.com/handbook/</a>, especially the sales and marketing sections. It’s awesome how public they are about how they operate. You’ll learn a lot.<p>We (Sourcegraph) have started our own company handbook (<a href=\"https://about.sourcegraph.com/handbook\" rel=\"nofollow\">https://about.sourcegraph.com/handbook</a>), inspired by GitLab. So far it has helped new teammates onboard more quickly and (along with other efforts to diligently document internal practices) helped us work efficiently on a growing team spanning many timezones.",
          "I find that the key thing to overcome when selling to developers is the extreme reluctance to spend money buying something that they can build themselves.  You see,<p>1.) Developers love to build things.<p>2.) Developers hate spending money.<p>3.) Developers undervalue their time.<p>If your product looks like it would have been fun to build, you'll lose the entire \"insufficiently supervised developer\" demographic.  Those guys will happily spend tens of thousands of dollars of billable hours implementing an in-house version of your thing to avoid the possibility of outgrowing your Free Tier.<p>I've seen this play out with S3stat customers (which costs $10/month, or three minutes twenty seconds of fully loaded engineer cost), where somebody will spend a week building an in-house version of the service and standing up a server to run it.  Nicely done.  You'll break even on your investment in 21 years.<p>I've had moderate success with my latest API product pointing to a \"Boss Page\" that outlines things like build-vs-buy costs, and why you really would be better off paying us for this thing rather than dedicating an in-house guy to building and maintaining it.<p>It's a tough one."
        ],
        "story_type": ["Normal"],
        "url": "https://manifold.co/blog/founders-guide-developer-tools-sell-to-engineers",
        "url_text": "Proudly powered by LiteSpeed Web ServerPlease be advised that LiteSpeed Technologies Inc. is not a web hosting company and, as such, has no control over content found on this site. ",
        "comments.comment_id": [21511952, 21512921],
        "comments.comment_author": ["sqs", "jasonkester"],
        "comments.comment_descendants": [1, 17],
        "comments.comment_time": [
          "2019-11-12T06:23:15Z",
          "2019-11-12T10:01:39Z"
        ],
        "comments.comment_text": [
          "I would also recommend dev tools founders (really any founder) read GitLab’s company handbook at <a href=\"https://about.gitlab.com/handbook/\" rel=\"nofollow\">https://about.gitlab.com/handbook/</a>, especially the sales and marketing sections. It’s awesome how public they are about how they operate. You’ll learn a lot.<p>We (Sourcegraph) have started our own company handbook (<a href=\"https://about.sourcegraph.com/handbook\" rel=\"nofollow\">https://about.sourcegraph.com/handbook</a>), inspired by GitLab. So far it has helped new teammates onboard more quickly and (along with other efforts to diligently document internal practices) helped us work efficiently on a growing team spanning many timezones.",
          "I find that the key thing to overcome when selling to developers is the extreme reluctance to spend money buying something that they can build themselves.  You see,<p>1.) Developers love to build things.<p>2.) Developers hate spending money.<p>3.) Developers undervalue their time.<p>If your product looks like it would have been fun to build, you'll lose the entire \"insufficiently supervised developer\" demographic.  Those guys will happily spend tens of thousands of dollars of billable hours implementing an in-house version of your thing to avoid the possibility of outgrowing your Free Tier.<p>I've seen this play out with S3stat customers (which costs $10/month, or three minutes twenty seconds of fully loaded engineer cost), where somebody will spend a week building an in-house version of the service and standing up a server to run it.  Nicely done.  You'll break even on your investment in 21 years.<p>I've had moderate success with my latest API product pointing to a \"Boss Page\" that outlines things like build-vs-buy costs, and why you really would be better off paying us for this thing rather than dedicating an in-house guy to building and maintaining it.<p>It's a tough one."
        ],
        "id": "b3ccb627-735b-4a70-b13c-a8a2377692fe",
        "_version_": 1718527435381669889
      },
      {
        "story_id": [19311118],
        "story_author": ["darrinm"],
        "story_descendants": [34],
        "story_score": [60],
        "story_time": ["2019-03-05T15:29:56Z"],
        "story_title": "Coder (Visual Studio Code in browser) goes open source",
        "search": [
          "Coder (Visual Studio Code in browser) goes open source",
          "https://coder.com",
          "From the Developers ofcode-serverThe developer workspace platformCentralize the creation and management of cloud developer workspacesWorks with Azure, GCP, AWS, OpenShift, and anywhere you run KubernetesDevelop in VS Code, Jupyter, RStudio, IntelliJ, PyCharm, and any JetBrains IDEThe problems with development running on local machinesOnboarding delays from environment setupConfiguration drift as the project evolvesSource code on insecure endpointsLimited compute powerMove development to your cloudCoder handles the orchestration of new conformant and consistent workspaces using source-controlled Dockerfiles and workspace templates. Empower developers and data scientists to spin up self-serve workspaces that just work.Keep your workflowCoder works with the tools that you work with. No need to change your preferred editor, CI tooling, or version control systemwith support for Docker in Docker, too.Run VS Code, Jupyter Notebook, RStudio, IntelliJ, PyCharm, & other IDEs and editorsRun VS Code locally via SSH connection to your Coder workspace (local support for JetBrains IDEs coming soon)Version control with GitHub, GitLab, & BitbucketPersonalize your workspaces to fit your flowSpeed up builds and testsUse the power and scale of the cloud to offload the burden of slow builds and tests from your local machine.Deploy workspaces with the CPUs and memory you needAccommodate bursty workloads by utilizing the clusters idle CPUs and memoryHarness the parallelism of GPUs for deep learning and other intensive workloadsDevelop with any deviceAccess your workspace from anywhere with the same snappy development experience you expect from a local IDE.Work from home, the office, or wherever you areCode using any device, even an iPadOnboard to new projects from wherever, wheneverOnboarding to a new project can take days away from your productivity and hinders collaboration. Get on the same page and stay on the same page faster.Start a new project with all the required tools and dependencies in minutesCollaborate across teams with easeWork with your team from anywhere in the world without latencySecure your data and source codeSource code and data can remain secured on the cluster or in authorized repositories not sitting on a workstation or laptop.Get the security benefits of VDI with a better developer experienceReduce the risk of source code leaksIdeal for zero-trust networks and air-gapped environmentsCasestudyKazoo reduces onboarding time with Coder\"New hires have a shorter onboarding experience because theyre just spinning up a Coder workspace instead of installing locally and having to worry about whether all the dependencies are up to date.\"Joe MainwaringDirector of Infrastructure, KazooRead case studyFAQIs Coder priced per developer or environment?Is Coder SaaS?What can I expect my infrastructure costs to be?How can I procure Coder for my government agency?What does the price of Coder include?What type of support is available?Can I add users at any time?Get started with Coder todayOur commitment to open sourceLearn more about our projects and our commitment to the open-source community.Code-server: the heart of CoderCode-server is the primary open source project we maintain. It allows developers to use a browser to access remote dev environments running VS Code. Coder builds upon the success of code-server and adds features designed for enterprise teams including support for additional IDEs and advanced security features.CookiesWe use cookies to make your experience better. ",
          "Has anyone tried this and compared it to <a href=\"https://www.theia-ide.org/\" rel=\"nofollow\">https://www.theia-ide.org/</a>?<p>First thing off the bat I notice is that Coder looks harder to deploy or try out, Theia was super easy, on the landing page they had a docker one liner:<p>docker run -it -p 3000:3000 -v \"$(pwd):/home/project:cached\" theiaide/theia:next",
          "I used to work at a place where we all used the enterprise server version of RStudio, which also runs in a browser.<p>There was a lot of good thing about that setup. Nobody could walk home with code, and no code was lost on somebody laptop.<p>Execution happened on a server, much more powerful than any dev machine."
        ],
        "story_type": ["Normal"],
        "url": "https://coder.com",
        "url_text": "From the Developers ofcode-serverThe developer workspace platformCentralize the creation and management of cloud developer workspacesWorks with Azure, GCP, AWS, OpenShift, and anywhere you run KubernetesDevelop in VS Code, Jupyter, RStudio, IntelliJ, PyCharm, and any JetBrains IDEThe problems with development running on local machinesOnboarding delays from environment setupConfiguration drift as the project evolvesSource code on insecure endpointsLimited compute powerMove development to your cloudCoder handles the orchestration of new conformant and consistent workspaces using source-controlled Dockerfiles and workspace templates. Empower developers and data scientists to spin up self-serve workspaces that just work.Keep your workflowCoder works with the tools that you work with. No need to change your preferred editor, CI tooling, or version control systemwith support for Docker in Docker, too.Run VS Code, Jupyter Notebook, RStudio, IntelliJ, PyCharm, & other IDEs and editorsRun VS Code locally via SSH connection to your Coder workspace (local support for JetBrains IDEs coming soon)Version control with GitHub, GitLab, & BitbucketPersonalize your workspaces to fit your flowSpeed up builds and testsUse the power and scale of the cloud to offload the burden of slow builds and tests from your local machine.Deploy workspaces with the CPUs and memory you needAccommodate bursty workloads by utilizing the clusters idle CPUs and memoryHarness the parallelism of GPUs for deep learning and other intensive workloadsDevelop with any deviceAccess your workspace from anywhere with the same snappy development experience you expect from a local IDE.Work from home, the office, or wherever you areCode using any device, even an iPadOnboard to new projects from wherever, wheneverOnboarding to a new project can take days away from your productivity and hinders collaboration. Get on the same page and stay on the same page faster.Start a new project with all the required tools and dependencies in minutesCollaborate across teams with easeWork with your team from anywhere in the world without latencySecure your data and source codeSource code and data can remain secured on the cluster or in authorized repositories not sitting on a workstation or laptop.Get the security benefits of VDI with a better developer experienceReduce the risk of source code leaksIdeal for zero-trust networks and air-gapped environmentsCasestudyKazoo reduces onboarding time with Coder\"New hires have a shorter onboarding experience because theyre just spinning up a Coder workspace instead of installing locally and having to worry about whether all the dependencies are up to date.\"Joe MainwaringDirector of Infrastructure, KazooRead case studyFAQIs Coder priced per developer or environment?Is Coder SaaS?What can I expect my infrastructure costs to be?How can I procure Coder for my government agency?What does the price of Coder include?What type of support is available?Can I add users at any time?Get started with Coder todayOur commitment to open sourceLearn more about our projects and our commitment to the open-source community.Code-server: the heart of CoderCode-server is the primary open source project we maintain. It allows developers to use a browser to access remote dev environments running VS Code. Coder builds upon the success of code-server and adds features designed for enterprise teams including support for additional IDEs and advanced security features.CookiesWe use cookies to make your experience better. ",
        "comments.comment_id": [19313749, 19315009],
        "comments.comment_author": ["Hortinstein", "wodenokoto"],
        "comments.comment_descendants": [3, 2],
        "comments.comment_time": [
          "2019-03-05T20:07:17Z",
          "2019-03-05T22:53:48Z"
        ],
        "comments.comment_text": [
          "Has anyone tried this and compared it to <a href=\"https://www.theia-ide.org/\" rel=\"nofollow\">https://www.theia-ide.org/</a>?<p>First thing off the bat I notice is that Coder looks harder to deploy or try out, Theia was super easy, on the landing page they had a docker one liner:<p>docker run -it -p 3000:3000 -v \"$(pwd):/home/project:cached\" theiaide/theia:next",
          "I used to work at a place where we all used the enterprise server version of RStudio, which also runs in a browser.<p>There was a lot of good thing about that setup. Nobody could walk home with code, and no code was lost on somebody laptop.<p>Execution happened on a server, much more powerful than any dev machine."
        ],
        "id": "a9412812-b55b-4fb6-abf7-aada6a1addaa",
        "_version_": 1718527388832235520
      },
      {
        "story_id": [19420532],
        "story_author": ["bookofjoe"],
        "story_descendants": [169],
        "story_score": [329],
        "story_time": ["2019-03-18T12:38:43Z"],
        "story_title": "Slack enables customers to control their encryption keys in enterprise version",
        "search": [
          "Slack enables customers to control their encryption keys in enterprise version",
          "https://techcrunch.com/2019/03/18/slack-hands-over-encryption-keys-to-regulated-customers",
          "Slack announced today that it is launching Enterprise Key Management (EKM) for Slack, a new tool that enables customers to control their encryption keys in the enterprise version of the communications app. The keys are managed in the AWS KMS key management tool. Geoff Belknap, chief security officer (CSO) at Slack, says the new tool should appeal to customers in regulated industries who might need tighter control over security. Markets like financial services, healthcare and government are typically underserved in terms of which collaboration tools they can use, so we wanted to design an experience that catered to their particular security needs, Belknap told TechCrunch. Slack currently encrypts data in transit and at rest, but the new tool augments this by giving customers greater control over the encryption keys that Slack uses to encrypt messages and files being shared inside the app. He said that regulated industries in particular have been requesting the ability to control their own encryption keys, including the ability to revoke them if it was required for security reasons. EKM is a key requirement for growing enterprise companies of all sizes, and was a requested feature from many of our Enterprise Grid customers. We wanted to give these customers full control over their encryption keys, and when or if they want to revoke them, he said. Screenshot: Slack Belknap says this is especially important when customers involve people outside the organization, such as contractors, partners or vendors in Slack communications. A big benefit of EKM is that in the event of a security threat or if you ever experience suspicious activity, your security team can cut off access to the content at any time if necessary, Belknap explained. In addition to controlling the encryption keys, customers can gain greater visibility into activity inside of Slack via the Audit Logs API. Detailed activity logs tell customers exactly when and where their data is being accessed, so they can be alerted of risks and anomalies immediately, he said. If a customer finds suspicious activity, it can cut off access. EKM for Slack is generally available today for Enterprise Grid customers for an additional fee. Slack, which announced plans to go public last month, has raised more than $1 billion on a $7 billion valuation. ",
          "This is a good thing for computing freedom: it puts more control in the hands of customers instead of requiring them to outsource encryption to Slack. It's a small step, since it's Amazon KMS and since presumably Slack still sees cleartext in transit. But it goes in the direction of restoring the security profile that a customer did when they ran their own internal IRC server, and that's a good thing.",
          "Maybe this is more interesting than I initially thought. I was thinking this would just the encryption keys used to store the information for your whole organization Slack account. Instead, the keys seem to be much more granular, since you can revoke access to a single file, specific channel, workspace or organization level. [1]<p>I see one benefit of this being able to securely throw away data. Like if you are doing work for customer XYZ and chatting about it on the corresponding channel you can throw away the keys for the channel when the project ends.<p>Wonder how all this granularity works together with the search functionality. Or maybe they maintain per channel search index that can be covered by a separate key.<p>[1] <a href=\"https://slackhq.com/enterprise-key-management\" rel=\"nofollow\">https://slackhq.com/enterprise-key-management</a>"
        ],
        "story_type": ["Normal"],
        "url": "https://techcrunch.com/2019/03/18/slack-hands-over-encryption-keys-to-regulated-customers",
        "url_text": "Slack announced today that it is launching Enterprise Key Management (EKM) for Slack, a new tool that enables customers to control their encryption keys in the enterprise version of the communications app. The keys are managed in the AWS KMS key management tool. Geoff Belknap, chief security officer (CSO) at Slack, says the new tool should appeal to customers in regulated industries who might need tighter control over security. Markets like financial services, healthcare and government are typically underserved in terms of which collaboration tools they can use, so we wanted to design an experience that catered to their particular security needs, Belknap told TechCrunch. Slack currently encrypts data in transit and at rest, but the new tool augments this by giving customers greater control over the encryption keys that Slack uses to encrypt messages and files being shared inside the app. He said that regulated industries in particular have been requesting the ability to control their own encryption keys, including the ability to revoke them if it was required for security reasons. EKM is a key requirement for growing enterprise companies of all sizes, and was a requested feature from many of our Enterprise Grid customers. We wanted to give these customers full control over their encryption keys, and when or if they want to revoke them, he said. Screenshot: Slack Belknap says this is especially important when customers involve people outside the organization, such as contractors, partners or vendors in Slack communications. A big benefit of EKM is that in the event of a security threat or if you ever experience suspicious activity, your security team can cut off access to the content at any time if necessary, Belknap explained. In addition to controlling the encryption keys, customers can gain greater visibility into activity inside of Slack via the Audit Logs API. Detailed activity logs tell customers exactly when and where their data is being accessed, so they can be alerted of risks and anomalies immediately, he said. If a customer finds suspicious activity, it can cut off access. EKM for Slack is generally available today for Enterprise Grid customers for an additional fee. Slack, which announced plans to go public last month, has raised more than $1 billion on a $7 billion valuation. ",
        "comments.comment_id": [19420852, 19421683],
        "comments.comment_author": ["geofft", "jpalomaki"],
        "comments.comment_descendants": [2, 1],
        "comments.comment_time": [
          "2019-03-18T13:35:16Z",
          "2019-03-18T15:10:31Z"
        ],
        "comments.comment_text": [
          "This is a good thing for computing freedom: it puts more control in the hands of customers instead of requiring them to outsource encryption to Slack. It's a small step, since it's Amazon KMS and since presumably Slack still sees cleartext in transit. But it goes in the direction of restoring the security profile that a customer did when they ran their own internal IRC server, and that's a good thing.",
          "Maybe this is more interesting than I initially thought. I was thinking this would just the encryption keys used to store the information for your whole organization Slack account. Instead, the keys seem to be much more granular, since you can revoke access to a single file, specific channel, workspace or organization level. [1]<p>I see one benefit of this being able to securely throw away data. Like if you are doing work for customer XYZ and chatting about it on the corresponding channel you can throw away the keys for the channel when the project ends.<p>Wonder how all this granularity works together with the search functionality. Or maybe they maintain per channel search index that can be covered by a separate key.<p>[1] <a href=\"https://slackhq.com/enterprise-key-management\" rel=\"nofollow\">https://slackhq.com/enterprise-key-management</a>"
        ],
        "id": "5124ff5f-8d3d-4b09-923d-4d5926f006bb",
        "_version_": 1718527394638200832
      },
      {
        "story_id": [21642340],
        "story_author": ["sastraxi"],
        "story_descendants": [27],
        "story_score": [66],
        "story_time": ["2019-11-26T20:27:53Z"],
        "story_title": "Show HN: Pgsh – branch Postgres like Git",
        "search": [
          "Show HN: Pgsh – branch Postgres like Git",
          "https://github.com/sastraxi/pgsh",
          "pgsh: PostgreSQL tools for local development Finding database migrations painful to work with? Switching contexts a chore? Pull requests piling up? pgsh helps by managing a connection string in your .env file and allows you to branch your database just like you branch with git. Prerequisites There are only a couple requirements: your project reads its database configuration from the environment it uses a .env file to do so in development. See dotenv for more details, and The Twelve-Factor App for why this is a best practice. Language / Framework .env solution Maturity javascript dotenv high pgsh can help even more if you use knex for migrations. Installation yarn global add pgsh to make the pgsh command available everywhere pgsh init to create a .pgshrc config file in your project folder, beside your .env file (see src/pgshrc/default.js for futher configuration) You can now run pgsh anywhere in your project directory (try pgsh -a!) It is recommended to check your .pgshrc into version control. Why? URL vs split mode There are two different ways pgsh can help you manage your current connection (mode in .pgshrc): url (default) when one variable in your .env has your full database connection string (e.g. DATABASE_URL=postgres://...) split when your .env has different keys (e.g. PG_HOST=localhost, PG_DATABASE=myapp, ...) Running tests Make sure the postgres client and its associated tools (psql, pg_dump, etc.) are installed locally cp .env.example .env docker-compose up -d Run the test suite using yarn test. Note that this test suite will destroy all databases on the connected postgres server, so it will force you to send a certain environment variable to confirm this is ok. Command reference pgsh init generates a .pgshrc file for your project. pgsh url prints your connection string. pgsh psql <name?> -- <psql-options...?> connects to the current (or named) database with psql pgsh current prints the name of the database that your connection string refers to right now. pgsh or pgsh list <filter?> prints all databases, filtered by an optional filter. Output is similar to git branch. By adding the -a option you can see migration status too! Database branching Read up on the recommended branching model for more details. pgsh clone <from?> <name> clones your current (or the from) database as name, then (optionally) runs switch <name>. pgsh create <name> creates an empty database, then runs switch <name> and optionally migrates it to the latest version. pgsh switch <name> makes name your current database, changing the connection string. pgsh destroy <name> destroys the given database. This cannot be undone. You can maintain a blacklist of databases to protect from this command in .pgshrc Dump and restore pgsh dump <name?> dumps the current database (or the named one if given) to stdout pgsh restore <name> restores a previously-dumped database as name from stdin Migration management (via knex) pgsh provides a slightly-more-user-friendly interface to knex's migration system. pgsh up migrates the current database to the latest version found in your migration directory. pgsh down <version> down-migrates the current database to version. Requires your migrations to have down edges! pgsh force-up re-writes the knex_migrations table entirely based on your migration directory. In effect, running this command is saying to knex \"trust me, the database has the structure you expect\". pgsh force-down <version> re-writes the knex_migrations table to not include the record of any migration past the given version. Use this command when you manually un-migrated some migations (e.g. a bad migration or when you are trying to undo a migration with missing \"down sql\"). pgsh validate compares the knex_migrations table to the configured migrations directory and reports any inconsistencies between the two. ",
          "This looks really handy. Quite frequently I want to iterate on a migration without having to write the \"down\" migration until the \"up\" migration is done. This will save a lot of DB dropping and re-seeding.",
          "Just a note for anyone trying to download this -- please first try `pgsh@0.10.1`. The `.2` bugfix release caused a regression that I'm working through now. Thanks :)"
        ],
        "story_type": ["ShowHN"],
        "url": "https://github.com/sastraxi/pgsh",
        "url_text": "pgsh: PostgreSQL tools for local development Finding database migrations painful to work with? Switching contexts a chore? Pull requests piling up? pgsh helps by managing a connection string in your .env file and allows you to branch your database just like you branch with git. Prerequisites There are only a couple requirements: your project reads its database configuration from the environment it uses a .env file to do so in development. See dotenv for more details, and The Twelve-Factor App for why this is a best practice. Language / Framework .env solution Maturity javascript dotenv high pgsh can help even more if you use knex for migrations. Installation yarn global add pgsh to make the pgsh command available everywhere pgsh init to create a .pgshrc config file in your project folder, beside your .env file (see src/pgshrc/default.js for futher configuration) You can now run pgsh anywhere in your project directory (try pgsh -a!) It is recommended to check your .pgshrc into version control. Why? URL vs split mode There are two different ways pgsh can help you manage your current connection (mode in .pgshrc): url (default) when one variable in your .env has your full database connection string (e.g. DATABASE_URL=postgres://...) split when your .env has different keys (e.g. PG_HOST=localhost, PG_DATABASE=myapp, ...) Running tests Make sure the postgres client and its associated tools (psql, pg_dump, etc.) are installed locally cp .env.example .env docker-compose up -d Run the test suite using yarn test. Note that this test suite will destroy all databases on the connected postgres server, so it will force you to send a certain environment variable to confirm this is ok. Command reference pgsh init generates a .pgshrc file for your project. pgsh url prints your connection string. pgsh psql <name?> -- <psql-options...?> connects to the current (or named) database with psql pgsh current prints the name of the database that your connection string refers to right now. pgsh or pgsh list <filter?> prints all databases, filtered by an optional filter. Output is similar to git branch. By adding the -a option you can see migration status too! Database branching Read up on the recommended branching model for more details. pgsh clone <from?> <name> clones your current (or the from) database as name, then (optionally) runs switch <name>. pgsh create <name> creates an empty database, then runs switch <name> and optionally migrates it to the latest version. pgsh switch <name> makes name your current database, changing the connection string. pgsh destroy <name> destroys the given database. This cannot be undone. You can maintain a blacklist of databases to protect from this command in .pgshrc Dump and restore pgsh dump <name?> dumps the current database (or the named one if given) to stdout pgsh restore <name> restores a previously-dumped database as name from stdin Migration management (via knex) pgsh provides a slightly-more-user-friendly interface to knex's migration system. pgsh up migrates the current database to the latest version found in your migration directory. pgsh down <version> down-migrates the current database to version. Requires your migrations to have down edges! pgsh force-up re-writes the knex_migrations table entirely based on your migration directory. In effect, running this command is saying to knex \"trust me, the database has the structure you expect\". pgsh force-down <version> re-writes the knex_migrations table to not include the record of any migration past the given version. Use this command when you manually un-migrated some migations (e.g. a bad migration or when you are trying to undo a migration with missing \"down sql\"). pgsh validate compares the knex_migrations table to the configured migrations directory and reports any inconsistencies between the two. ",
        "comments.comment_id": [21643472, 21643890],
        "comments.comment_author": ["kevsim", "sastraxi"],
        "comments.comment_descendants": [0, 1],
        "comments.comment_time": [
          "2019-11-26T22:36:16Z",
          "2019-11-26T23:42:10Z"
        ],
        "comments.comment_text": [
          "This looks really handy. Quite frequently I want to iterate on a migration without having to write the \"down\" migration until the \"up\" migration is done. This will save a lot of DB dropping and re-seeding.",
          "Just a note for anyone trying to download this -- please first try `pgsh@0.10.1`. The `.2` bugfix release caused a regression that I'm working through now. Thanks :)"
        ],
        "id": "90ce4fba-e7a5-4d6a-a39e-b5755e6f97f5",
        "_version_": 1718527437513424896
      },
      {
        "story_id": [19253811],
        "story_author": ["th0br0"],
        "story_descendants": [2],
        "story_score": [28],
        "story_time": ["2019-02-26T12:51:27Z"],
        "story_title": "Intel’s neuro guru slams deep learning: ‘it’s not learning’",
        "search": [
          "Intel’s neuro guru slams deep learning: ‘it’s not learning’",
          "https://www.zdnet.com/article/intels-neuro-guru-slams-deep-learning-its-not-actually-learning/",
          "\"Backpropogation doesn't correlate to the brain,\" insists Mike Davies, head of Intel's neuromorphic computing unit, dismissing one of the key tools of the species of A.I. In vogue today, deep learning. \"For that reason, \"it's really an optimizations procedure, it's not actually learning.\" Davies made the comment during a talk on Thursday at the International Solid State Circuits Conference in San Francisco, a prestigious annual gathering of semiconductor designers. Davies was returning fire after Facebook's Yann LeCun, a leading apostle of deep learning, earlier in the week dismissed Davies's own technology during LeCun's opening keynote for the conference. \"The brain is the one example we have of truly intelligent computation,\" observed Davies. In contrast, so-called back-prop, invented in the 1980s, is a mathematical technique used to optimize the response of artificial neurons in a deep learning computer program. Although deep learning has proven \"very effective,\" Davies told a ballroom of attendees, \"there is no natural example of back-prop,\" he said, so it doesn't correspond to what one would consider real learning. Also: Facebook's Yann LeCun reflects on the enduring appeal of convolutions Davies then went on to give a talk about \"Loihi,\" his team's computer model of a neural network that uses so-called spiking neurons that activate only when they receive an input sample. The contention of neuromorphic computing advocates is that the approach more closely emulates the actual characteristics of the brain's functioning, such as the great economy with which the brain transmits signals. He chided LeCun for failing to value the strengths of that approach. \"It's so ironic,\" said Davies. \"Yann bashed spikes but then he said we need to deal with sparsity in hardware, and that is what spikes are about.\" Architectural overview of Intel's \"Loihi\" neuromorphic computing chip. Intel. The point-counterpoint between the two researchers is not merely academic. Intel is in the business of chips. LeCun told ZDNet in an interview Monday that Facebook has its own \"internal activities\" ongoing in the development of chips for A.I. Hence, some of Intel's chip business with large customers like Facebook might be in jeopardy if the company faces competition from those very customers because they have a different vision. The Facebook AI researcher during his talk Monday morning had critiqued spiking neuron work broadly, saying that the technology had not yielded algorithms that could produce practical results. \"Yann said, Why build chips for algorithms that don't work; I think these algorithms do work,\" said Davies. Still, he conceded that there was at least an element of truth to LeCun's criticism. While the hardware of spiking neurons \"is well adequate and provides a lot of tools to map interesting algorithms,\" said Davies, his department has a \"goal to make progress on the algorithmic front, which is what is holding back the field.\" But Davies rebuked what he called \"this knee-jerk reaction that spikes are inefficient.\" Also: Facebook's Yann LeCun says 'internal activity' proceeds on AI chips Instead, he proposed, \"let's look at the data.\" Intel says Loihi bests conventional neural network performance on conventional processors, especially in speed at which results are computed. Intel. Davies cited a report put out in December by an organization called Applied Brain Research of Waterloo, Ontario, which compared performance of a speech detection algorithm running on various chips. The software was trained to recognize the keyword \"aloha\" while also rejecting nonsense phrases. The firm ran the model on a CPU and a GPU, and also made a version that ran on the Loihi chip. Must read 'AI is very, very stupid,' says Google's AI leader(CNET)How to get all of Google Assistant's new voices right now(CNET)Unified Google AI division a clear signal of AI's future(TechRepublic)Top 5: Things to know about AI(TechRepublic)The Loihi chip seems to come out with much better energy efficiency, as measured by the number of joules required to perform inference on the task. (It's worth noting that the Loihi chip, however, actually delivers lower accuracy, as described in the full version of the Applied Science paper, which is available on the arXiv pre-print server.) In another example, Davies noted that a basic classifier task performed by Loihi was forty times as fast as a conventional GPU-based deep learning network, with 8% greater accuracy. Davies told the audience that Loihi gets \"much more efficient\" as it is tasked with larger and larger networks. He observed that control of robots would be the killer app for neuromorphic chips such as Loihi. \"Brains evolved to control limbs\" he pointed out, making robotic control a natural. Wrapping up his talk, Davies said \"there's a lot of work left to be done before we can put it out in a commercial form that's applicable to a broad range of problems,\" but he ended on an optimistic note, saying \"We can get algorithms that work that will address Yann's skepticism.\" Previous and related coverage: What is AI? Everything you need to know An executive guide to artificial intelligence, from machine learning and general AI to neural networks. What is deep learning? Everything you need to know The lowdown on deep learning: from how it relates to the wider field of machine learning through to how to get started with it. What is machine learning? Everything you need to know This guide explains what machine learning is, how it is related to artificial intelligence, how it works and why it matters. What is cloud computing? Everything you need to know about An introduction to cloud computing right from the basics up to IaaS and PaaS, hybrid, public, and private cloud. Related stories: Google's AI surfs the \"gamescape\" to conquer game theory This is what AI looks like (as sketched by AI) Google's DeepMind teams with leading 3D game dev platform DeepMind's AI spots early signs of eye disease ",
          "Gee, that read a lot like a story from a tech version of The Onion.",
          "Between the autoplay ad video, the modal popover, the request for notification privileges, the fixed topbar with ads, the interstitial ads, and the huge sidebar ads, I couldn't get past the first sentence."
        ],
        "story_type": ["Normal"],
        "url": "https://www.zdnet.com/article/intels-neuro-guru-slams-deep-learning-its-not-actually-learning/",
        "comments.comment_id": [19254336, 19257703],
        "comments.comment_author": ["yesenadam", "JHonaker"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-02-26T14:03:49Z",
          "2019-02-26T19:49:52Z"
        ],
        "comments.comment_text": [
          "Gee, that read a lot like a story from a tech version of The Onion.",
          "Between the autoplay ad video, the modal popover, the request for notification privileges, the fixed topbar with ads, the interstitial ads, and the huge sidebar ads, I couldn't get past the first sentence."
        ],
        "id": "caf0a723-e81e-4092-aa28-3a79e523253a",
        "url_text": "\"Backpropogation doesn't correlate to the brain,\" insists Mike Davies, head of Intel's neuromorphic computing unit, dismissing one of the key tools of the species of A.I. In vogue today, deep learning. \"For that reason, \"it's really an optimizations procedure, it's not actually learning.\" Davies made the comment during a talk on Thursday at the International Solid State Circuits Conference in San Francisco, a prestigious annual gathering of semiconductor designers. Davies was returning fire after Facebook's Yann LeCun, a leading apostle of deep learning, earlier in the week dismissed Davies's own technology during LeCun's opening keynote for the conference. \"The brain is the one example we have of truly intelligent computation,\" observed Davies. In contrast, so-called back-prop, invented in the 1980s, is a mathematical technique used to optimize the response of artificial neurons in a deep learning computer program. Although deep learning has proven \"very effective,\" Davies told a ballroom of attendees, \"there is no natural example of back-prop,\" he said, so it doesn't correspond to what one would consider real learning. Also: Facebook's Yann LeCun reflects on the enduring appeal of convolutions Davies then went on to give a talk about \"Loihi,\" his team's computer model of a neural network that uses so-called spiking neurons that activate only when they receive an input sample. The contention of neuromorphic computing advocates is that the approach more closely emulates the actual characteristics of the brain's functioning, such as the great economy with which the brain transmits signals. He chided LeCun for failing to value the strengths of that approach. \"It's so ironic,\" said Davies. \"Yann bashed spikes but then he said we need to deal with sparsity in hardware, and that is what spikes are about.\" Architectural overview of Intel's \"Loihi\" neuromorphic computing chip. Intel. The point-counterpoint between the two researchers is not merely academic. Intel is in the business of chips. LeCun told ZDNet in an interview Monday that Facebook has its own \"internal activities\" ongoing in the development of chips for A.I. Hence, some of Intel's chip business with large customers like Facebook might be in jeopardy if the company faces competition from those very customers because they have a different vision. The Facebook AI researcher during his talk Monday morning had critiqued spiking neuron work broadly, saying that the technology had not yielded algorithms that could produce practical results. \"Yann said, Why build chips for algorithms that don't work; I think these algorithms do work,\" said Davies. Still, he conceded that there was at least an element of truth to LeCun's criticism. While the hardware of spiking neurons \"is well adequate and provides a lot of tools to map interesting algorithms,\" said Davies, his department has a \"goal to make progress on the algorithmic front, which is what is holding back the field.\" But Davies rebuked what he called \"this knee-jerk reaction that spikes are inefficient.\" Also: Facebook's Yann LeCun says 'internal activity' proceeds on AI chips Instead, he proposed, \"let's look at the data.\" Intel says Loihi bests conventional neural network performance on conventional processors, especially in speed at which results are computed. Intel. Davies cited a report put out in December by an organization called Applied Brain Research of Waterloo, Ontario, which compared performance of a speech detection algorithm running on various chips. The software was trained to recognize the keyword \"aloha\" while also rejecting nonsense phrases. The firm ran the model on a CPU and a GPU, and also made a version that ran on the Loihi chip. Must read 'AI is very, very stupid,' says Google's AI leader(CNET)How to get all of Google Assistant's new voices right now(CNET)Unified Google AI division a clear signal of AI's future(TechRepublic)Top 5: Things to know about AI(TechRepublic)The Loihi chip seems to come out with much better energy efficiency, as measured by the number of joules required to perform inference on the task. (It's worth noting that the Loihi chip, however, actually delivers lower accuracy, as described in the full version of the Applied Science paper, which is available on the arXiv pre-print server.) In another example, Davies noted that a basic classifier task performed by Loihi was forty times as fast as a conventional GPU-based deep learning network, with 8% greater accuracy. Davies told the audience that Loihi gets \"much more efficient\" as it is tasked with larger and larger networks. He observed that control of robots would be the killer app for neuromorphic chips such as Loihi. \"Brains evolved to control limbs\" he pointed out, making robotic control a natural. Wrapping up his talk, Davies said \"there's a lot of work left to be done before we can put it out in a commercial form that's applicable to a broad range of problems,\" but he ended on an optimistic note, saying \"We can get algorithms that work that will address Yann's skepticism.\" Previous and related coverage: What is AI? Everything you need to know An executive guide to artificial intelligence, from machine learning and general AI to neural networks. What is deep learning? Everything you need to know The lowdown on deep learning: from how it relates to the wider field of machine learning through to how to get started with it. What is machine learning? Everything you need to know This guide explains what machine learning is, how it is related to artificial intelligence, how it works and why it matters. What is cloud computing? Everything you need to know about An introduction to cloud computing right from the basics up to IaaS and PaaS, hybrid, public, and private cloud. Related stories: Google's AI surfs the \"gamescape\" to conquer game theory This is what AI looks like (as sketched by AI) Google's DeepMind teams with leading 3D game dev platform DeepMind's AI spots early signs of eye disease ",
        "_version_": 1718527387688239104
      },
      {
        "story_id": [20590247],
        "story_author": ["stablemap"],
        "story_descendants": [9],
        "story_score": [48],
        "story_time": ["2019-08-02T02:15:18Z"],
        "story_title": "Experiment, Simplify, Ship",
        "search": [
          "Experiment, Simplify, Ship",
          "https://blog.golang.org/experiment",
          "The Go Blog Introduction This is the blog post version of my talk last week at GopherCon 2019. We are all on the path to Go 2, together, but none of us know exactly where that path leads or sometimes even which direction the path goes. This post discusses how we actually find and follow the path to Go 2. Heres what the process looks like. We experiment with Go as it exists now, to understand it better, learning what works well and what doesnt. Then we experiment with possible changes, to understand them better, again learning what works well and what doesnt. Based on what we learn from those experiments, we simplify. And then we experiment again. And then we simplify again. And so on. And so on. The Four Rs of Simplifying During this process, there are four main ways that we can simplify the overall experience of writing Go programs: reshaping, redefining, removing, and restricting. Simplify by Reshaping The first way we simplify is by reshaping what exists into a new form, one that ends up being simpler overall. Every Go program we write serves as an experiment to test Go itself. In the early days of Go, we quickly learned that it was common to write code like this addToList function: func addToList(list []int, x int) []int { n := len(list) if n+1 > cap(list) { big := make([]int, n, (n+5)*2) copy(big, list) list = big } list = list[:n+1] list[n] = x return list } Wed write the same code for slices of bytes, and slices of strings, and so on. Our programs were too complex, because Go was too simple. So we took the many functions like addToList in our programs and reshaped them into one function provided by Go itself. Adding append made the Go language a little more complex, but on balance it made the overall experience of writing Go programs simpler, even after accounting for the cost of learning about append. Heres another example. For Go 1, we looked at the very many development tools in the Go distribution, and we reshaped them into one new command. 5a 8g 5g 8l 5l cgo 6a gobuild 6cov gofix go 6g goinstall 6l gomake 6nm gopack 8a govet The go command is so central now that it is easy to forget that we went so long without it and how much extra work that involved. We added code and complexity to the Go distribution, but on balance we simplified the experience of writing Go programs. The new structure also created space for other interesting experiments, which well see later. Simplify by Redefining A second way we simplify is by redefining functionality we already have, allowing it to do more. Like simplifying by reshaping, simplifying by redefining makes programs simpler to write, but now with nothing new to learn. For example, append was originally defined to read only from slices. When appending to a byte slice, you could append the bytes from another byte slice, but not the bytes from a string. We redefined append to allow appending from a string, without adding anything new to the language. var b []byte var more []byte b = append(b, more...) // ok var b []byte var more string b = append(b, more...) // ok later Simplify by Removing A third way we simplify is by removing functionality when it has turned out to be less useful or less important than we expected. Removing functionality means one less thing to learn, one less thing to fix bugs in, one less thing to be distracted by or use incorrectly. Of course, removing also forces users to update existing programs, perhaps making them more complex, to make up for the removal. But the overall result can still be that the process of writing Go programs becomes simpler. An example of this is when we removed the boolean forms of non-blocking channel operations from the language: ok := c <- x // before Go 1, was non-blocking send x, ok := <-c // before Go 1, was non-blocking receive These operations were also possible to do using select, making it confusing to need to decide which form to use. Removing them simplified the language without reducing its power. Simplify by Restricting We can also simplify by restricting what is allowed. From day one, Go has restricted the encoding of Go source files: they must be UTF-8. This restriction makes every program that tries to read Go source files simpler. Those programs dont have to worry about Go source files encoded in Latin-1 or UTF-16 or UTF-7 or anything else. Another important restriction is gofmt for program formatting. Nothing rejects Go code that isnt formatted using gofmt, but we have established a convention that tools that rewrite Go programs leave them in gofmt form. If you keep your programs in gofmt form too, then these rewriters dont make any formatting changes. When you compare before and after, the only diffs you see are real changes. This restriction has simplified program rewriters and led to successful experiments like goimports, gorename, and many others. Go Development Process This cycle of experiment and simplify is a good model for what weve been doing the past ten years. but it has a problem: its too simple. We cant only experiment and simplify. We have to ship the result. We have to make it available to use. Of course, using it enables more experiments, and possibly more simplifying, and the process cycles on and on. We shipped Go to all of you for the first time on November 10, 2009. Then, with your help, we shipped Go 1 together in March 2012. And weve shipped twelve Go releases since then. All of these were important milestones, to enable more experimentation, to help us learn more about Go, and of course to make Go available for production use. When we shipped Go 1, we explicitly shifted our focus to using Go, to understand this version of the language much better before trying any more simplifications involving language changes. We needed to take time to experiment, to really understand what works and what doesnt. Of course, weve had twelve releases since Go 1, so we have still been experimenting and simplifying and shipping. But weve focused on ways to simplify Go development without significant language changes and without breaking existing Go programs. For example, Go 1.5 shipped the first concurrent garbage collector and then the following releases improved it, simplifying Go development by removing pause times as an ongoing concern. At Gophercon in 2017, we announced that after five years of experimentation, it was again time to think about significant changes that would simplify Go development. Our path to Go 2 is really the same as the path to Go 1: experiment and simplify and ship, towards an overall goal of simplifying Go development. For Go 2, the concrete topics that we believed were most important to address are error handling, generics, and dependencies. Since then we have realized that another important topic is developer tooling. The rest of this post discusses how our work in each of these areas follows that path. Along the way, well take one detour, stopping to inspect the technical detail of what will be shipping soon in Go 1.13 for error handling. Errors It is hard enough to write a program that works the right way in all cases when all the inputs are valid and correct and nothing the program depends on is failing. When you add errors into the mix, writing a program that works the right way no matter what goes wrong is even harder. As part of thinking about Go 2, we want to understand better whether Go can help make that job any simpler. There are two different aspects that could potentially be simplified: error values and error syntax. Well look at each in turn, with the technical detour I promised focusing on the Go 1.13 error value changes. Error Values Error values had to start somewhere. Here is the Read function from the first version of the os package: export func Read(fd int64, b *[]byte) (ret int64, errno int64) { r, e := syscall.read(fd, &b[0], int64(len(b))); return r, e } There was no File type yet, and also no error type. Read and the other functions in the package returned an errno int64 directly from the underlying Unix system call. This code was checked in on September 10, 2008 at 12:14pm. Like everything back then, it was an experiment, and code changed quickly. Two hours and five minutes later, the API changed: export type Error struct { s string } func (e *Error) Print() { } // to standard error! func (e *Error) String() string { } export func Read(fd int64, b *[]byte) (ret int64, err *Error) { r, e := syscall.read(fd, &b[0], int64(len(b))); return r, ErrnoToError(e) } This new API introduced the first Error type. An error held a string and could return that string and also print it to standard error. The intent here was to generalize beyond integer codes. We knew from past experience that operating system error numbers were too limited a representation, that it would simplify programs not to have to shoehorn all detail about an error into 64 bits. Using error strings had worked reasonably well for us in the past, so we did the same here. This new API lasted seven months. The next April, after more experience using interfaces, we decided to generalize further and allow user-defined error implementations, by making the os.Error type itself an interface. We simplified by removing the Print method. For Go 1 two years later, based on a suggestion by Roger Peppe, os.Error became the built-in error type, and the String method was renamed to Error. Nothing has changed since then. But we have written many Go programs, and as a result we have experimented a lot with how best to implement and use errors. Errors Are Values Making error a simple interface and allowing many different implementations means we have the entire Go language available to define and inspect errors. We like to say that errors are values, the same as any other Go value. Heres an example. On Unix, an attempt to dial a network connection ends up using the connect system call. That system call returns a syscall.Errno, which is a named integer type that represents a system call error number and implements the error interface: package syscall type Errno int64 func (e Errno) Error() string { ... } const ECONNREFUSED = Errno(61) ... err == ECONNREFUSED ... The syscall package also defines named constants for the host operating systems defined error numbers. In this case, on this system, ECONNREFUSED is number 61. Code that gets an error from a function can test whether the error is ECONNREFUSED using ordinary value equality. Moving up a level, in package os, any system call failure is reported using a larger error structure that records what operation was attempted in addition to the error. There are a handful of these structures. This one, SyscallError, describes an error invoking a specific system call with no additional information recorded: package os type SyscallError struct { Syscall string Err error } func (e *SyscallError) Error() string { return e.Syscall + \": \" + e.Err.Error() } Moving up another level, in package net, any network failure is reported using an even larger error structure that records the details of the surrounding network operation, such as dial or listen, and the network and addresses involved: package net type OpError struct { Op string Net string Source Addr Addr Addr Err error } func (e *OpError) Error() string { ... } Putting these together, the errors returned by operations like net.Dial can format as strings, but they are also structured Go data values. In this case, the error is a net.OpError, which adds context to an os.SyscallError, which adds context to a syscall.Errno: c, err := net.Dial(\"tcp\", \"localhost:50001\") // \"dial tcp [::1]:50001: connect: connection refused\" err is &net.OpError{ Op: \"dial\", Net: \"tcp\", Addr: &net.TCPAddr{IP: ParseIP(\"::1\"), Port: 50001}, Err: &os.SyscallError{ Syscall: \"connect\", Err: syscall.Errno(61), // == ECONNREFUSED }, } When we say errors are values, we mean both that the entire Go language is available to define them and also that the entire Go language is available to inspect them. Here is an example from package net. It turns out that when you attempt a socket connection, most of the time you will get connected or get connection refused, but sometimes you can get a spurious EADDRNOTAVAIL, for no good reason. Go shields user programs from this failure mode by retrying. To do this, it has to inspect the error structure to find out whether the syscall.Errno deep inside is EADDRNOTAVAIL. Here is the code: func spuriousENOTAVAIL(err error) bool { if op, ok := err.(*OpError); ok { err = op.Err } if sys, ok := err.(*os.SyscallError); ok { err = sys.Err } return err == syscall.EADDRNOTAVAIL } A type assertion peels away any net.OpError wrapping. And then a second type assertion peels away any os.SyscallError wrapping. And then the function checks the unwrapped error for equality with EADDRNOTAVAIL. What weve learned from years of experience, from this experimenting with Go errors, is that it is very powerful to be able to define arbitrary implementations of the error interface, to have the full Go language available both to construct and to deconstruct errors, and not to require the use of any single implementation. These propertiesthat errors are values, and that there is not one required error implementationare important to preserve. Not mandating one error implementation enabled everyone to experiment with additional functionality that an error might provide, leading to many packages, such as github.com/pkg/errors, gopkg.in/errgo.v2, github.com/hashicorp/errwrap, upspin.io/errors, github.com/spacemonkeygo/errors, and more. One problem with unconstrained experimentation, though, is that as a client you have to program to the union of all the possible implementations you might encounter. A simplification that seemed worth exploring for Go 2 was to define a standard version of commonly-added functionality, in the form of agreed-upon optional interfaces, so that different implementations could interoperate. Unwrap The most commonly-added functionality in these packages is some method that can be called to remove context from an error, returning the error inside. Packages use different names and meanings for this operation, and sometimes it removes one level of context, while sometimes it removes as many levels as possible. For Go 1.13, we have introduced a convention that an error implementation adding removable context to an inner error should implement an Unwrap method that returns the inner error, unwrapping the context. If there is no inner error appropriate to expose to callers, either the error shouldnt have an Unwrap method, or the Unwrap method should return nil. // Go 1.13 optional method for error implementations. interface { // Unwrap removes one layer of context, // returning the inner error if any, or else nil. Unwrap() error } The way to call this optional method is to invoke the helper function errors.Unwrap, which handles cases like the error itself being nil or not having an Unwrap method at all. package errors // Unwrap returns the result of calling // the Unwrap method on err, // if errs type defines an Unwrap method. // Otherwise, Unwrap returns nil. func Unwrap(err error) error We can use the Unwrap method to write a simpler, more general version of spuriousENOTAVAIL. Instead of looking for specific error wrapper implementations like net.OpError or os.SyscallError, the general version can loop, calling Unwrap to remove context, until either it reaches EADDRNOTAVAIL or theres no error left: func spuriousENOTAVAIL(err error) bool { for err != nil { if err == syscall.EADDRNOTAVAIL { return true } err = errors.Unwrap(err) } return false } This loop is so common, though, that Go 1.13 defines a second function, errors.Is, that repeatedly unwraps an error looking for a specific target. So we can replace the entire loop with a single call to errors.Is: func spuriousENOTAVAIL(err error) bool { return errors.Is(err, syscall.EADDRNOTAVAIL) } At this point we probably wouldnt even define the function; it would be equally clear, and simpler, to call errors.Is directly at the call sites. Go 1.13 also introduces a function errors.As that unwraps until it finds a specific implementation type. If you want to write code that works with arbitrarily-wrapped errors, errors.Is is the wrapper-aware version of an error equality check: err == target errors.Is(err, target) And errors.As is the wrapper-aware version of an error type assertion: target, ok := err.(*Type) if ok { ... } var target *Type if errors.As(err, &target) { ... } To Unwrap Or Not To Unwrap? Whether to make it possible to unwrap an error is an API decision, the same way that whether to export a struct field is an API decision. Sometimes it is appropriate to expose that detail to calling code, and sometimes it isnt. When it is, implement Unwrap. When it isnt, dont implement Unwrap. Until now, fmt.Errorf has not exposed an underlying error formatted with %v to caller inspection. That is, the result of fmt.Errorf has not been possible to unwrap. Consider this example: // errors.Unwrap(err2) == nil // err1 is not available (same as earlier Go versions) err2 := fmt.Errorf(\"connect: %v\", err1) If err2 is returned to a caller, that caller has never had any way to open up err2 and access err1. We preserved that property in Go 1.13. For the times when you do want to allow unwrapping the result of fmt.Errorf, we also added a new printing verb %w, which formats like %v, requires an error value argument, and makes the resulting errors Unwrap method return that argument. In our example, suppose we replace %v with %w: // errors.Unwrap(err4) == err3 // (%w is new in Go 1.13) err4 := fmt.Errorf(\"connect: %w\", err3) Now, if err4 is returned to a caller, the caller can use Unwrap to retrieve err3. It is important to note that absolute rules like always use %v (or never implement Unwrap) or always use %w (or always implement Unwrap) are as wrong as absolute rules like never export struct fields or always export struct fields. Instead, the right decision depends on whether callers should be able to inspect and depend on the additional information that using %w or implementing Unwrap exposes. As an illustration of this point, every error-wrapping type in the standard library that already had an exported Err field now also has an Unwrap method returning that field, but implementations with unexported error fields do not, and existing uses of fmt.Errorf with %v still use %v, not %w. Error Value Printing (Abandoned) Along with the design draft for Unwrap, we also published a design draft for an optional method for richer error printing, including stack frame information and support for localized, translated errors. // Optional method for error implementations type Formatter interface { Format(p Printer) (next error) } // Interface passed to Format type Printer interface { Print(args ...interface{}) Printf(format string, args ...interface{}) Detail() bool } This one is not as simple as Unwrap, and I wont go into the details here. As we discussed the design with the Go community over the winter, we learned that the design wasnt simple enough. It was too hard for individual error types to implement, and it did not help existing programs enough. On balance, it did not simplify Go development. As a result of this community discussion, we abandoned this printing design. Error Syntax That was error values. Lets look briefly at error syntax, another abandoned experiment. Here is some code from compress/lzw/writer.go in the standard library: // Write the savedCode if valid. if e.savedCode != invalidCode { if err := e.write(e, e.savedCode); err != nil { return err } if err := e.incHi(); err != nil && err != errOutOfCodes { return err } } // Write the eof code. eof := uint32(1)<<e.litWidth + 1 if err := e.write(e, eof); err != nil { return err } At a glance, this code is about half error checks. My eyes glaze over when I read it. And we know that code that is tedious to write and tedious to read is easy to misread, making it a good home for hard-to-find bugs. For example, one of these three error checks is not like the others, a fact that is easy to miss on a quick skim. If you were debugging this code, how long would it take to notice that? At Gophercon last year we presented a draft design for a new control flow construct marked by the keyword check. Check consumes the error result from a function call or expression. If the error is non-nil, the check returns that error. Otherwise the check evaluates to the other results from the call. We can use check to simplify the lzw code: // Write the savedCode if valid. if e.savedCode != invalidCode { check e.write(e, e.savedCode) if err := e.incHi(); err != errOutOfCodes { check err } } // Write the eof code. eof := uint32(1)<<e.litWidth + 1 check e.write(e, eof) This version of the same code uses check, which removes four lines of code and more importantly highlights that the call to e.incHi is allowed to return errOutOfCodes. Maybe most importantly, the design also allowed defining error handler blocks to be run when later checks failed. That would let you write shared context-adding code just once, like in this snippet: handle err { err = fmt.Errorf(\"closing writer: %w\", err) } // Write the savedCode if valid. if e.savedCode != invalidCode { check e.write(e, e.savedCode) if err := e.incHi(); err != errOutOfCodes { check err } } // Write the eof code. eof := uint32(1)<<e.litWidth + 1 check e.write(e, eof) In essence, check was a short way to write the if statement, and handle was like defer but only for error return paths. In contrast to exceptions in other languages, this design retained Gos important property that every potential failing call was marked explicitly in the code, now using the check keyword instead of if err != nil. The big problem with this design was that handle overlapped too much, and in confusing ways, with defer. In May we posted a new design with three simplifications: to avoid the confusion with defer, the design dropped handle in favor of just using defer; to match a similar idea in Rust and Swift, the design renamed check to try; and to allow experimentation in a way that existing parsers like gofmt would recognize, it changed check (now try) from a keyword to a built-in function. Now the same code would look like this: defer errd.Wrapf(&err, \"closing writer\") // Write the savedCode if valid. if e.savedCode != invalidCode { try(e.write(e, e.savedCode)) if err := e.incHi(); err != errOutOfCodes { try(err) } } // Write the eof code. eof := uint32(1)<<e.litWidth + 1 try(e.write(e, eof)) We spent most of June discussing this proposal publicly on GitHub. The fundamental idea of check or try was to shorten the amount of syntax repeated at each error check, and in particular to remove the return statement from view, keeping the error check explicit and better highlighting interesting variations. One interesting point raised during the public feedback discussion, however, was that without an explicit if statement and return, theres nowhere to put a debugging print, theres nowhere to put a breakpoint, and theres no code to show as unexecuted in code coverage results. The benefits we were after came at the cost of making these situations more complex. On balance, from this as well as other considerations, it was not at all clear that the overall result would be simpler Go development, so we abandoned this experiment. Thats everything about error handling, which was one of the main focuses for this year. Generics Now for something a little less controversial: generics. The second big topic we identified for Go 2 was some kind of way to write code with type parameters. This would enable writing generic data structures and also writing generic functions that work with any kind of slice, or any kind of channel, or any kind of map. For example, here is a generic channel filter: // Filter copies values from c to the returned channel, // passing along only those values satisfying f. func Filter(type value)(f func(value) bool, c <-chan value) <-chan value { out := make(chan value) go func() { for v := range c { if f(v) { out <- v } } close(out) }() return out } Weve been thinking about generics since work on Go began, and we wrote and rejected our first concrete design in 2010. We wrote and rejected three more designs by the end of 2013. Four abandoned experiments, but not failed experiments, We learned from them, like we learned from check and try. Each time, we learned that the path to Go 2 is not in that exact direction, and we noticed other directions that might be interesting to explore. But by 2013 we had decided that we needed to focus on other concerns, so we put the entire topic aside for a few years. Last year we started exploring and experimenting again, and we presented a new design, based on the idea of a contract, at Gophercon last summer. Weve continued to experiment and simplify, and weve been working with programming language theory experts to understand the design better. Overall, I am hopeful that were headed in a good direction, toward a design that will simplify Go development. Even so, we might find that this design doesnt work either. We might have to abandon this experiment and adjust our path based on what we learned. Well find out. At Gophercon 2019, Ian Lance Taylor talked about why we might want to add generics to Go and briefly previewed the latest design draft. For details, see his blog post Why Generics? Dependencies The third big topic we identified for Go 2 was dependency management. In 2010 we published a tool called goinstall, which we called an experiment in package installation. It downloaded dependencies and stored them in your Go distribution tree, in GOROOT. As we experimented with goinstall, we learned that the Go distribution and the installed packages should be kept separate, so that it was possible to change to a new Go distribution without losing all your Go packages. So in 2011 we introduced GOPATH, an environment variable that specified where to look for packages not found in the main Go distribution. Adding GOPATH created more places for Go packages but simplified Go development overall, by separating your Go distribution from your Go libraries. Compatibility The goinstall experiment intentionally left out an explicit concept of package versioning. Instead, goinstall always downloaded the latest copy. We did this so we could focus on the other design problems for package installation. Goinstall became go get as part of Go 1. When people asked about versions, we encouraged them to experiment by creating additional tools, and they did. And we encouraged package AUTHORS to provide their USERS with the same backwards compatibility we did for the Go 1 libraries. Quoting the Go FAQ: Packages intended for public use should try to maintain backwards compatibility as they evolve. If different functionality is required, add a new name instead of changing an old one. If a complete break is required, create a new package with a new import path. This convention simplifies the overall experience of using a package by restricting what authors can do: avoid breaking changes to APIs; give new functionality a new name; and give a whole new package design a new import path. Of course, people kept experimenting. One of the most interesting experiments was started by Gustavo Niemeyer. He created a Git redirector called gopkg.in, which provided different import paths for different API versions, to help package authors follow the convention of giving a new package design a new import path. For example, the Go source code in the GitHub repository go-yaml/yaml has different APIs in the v1 and v2 semantic version tags. The gopkg.in server provides these with different import paths gopkg.in/yaml.v1 and gopkg.in/yaml.v2. The convention of providing backwards compatibility, so that a newer version of a package can be used in place of an older version, is what makes go gets very simple rulealways download the latest copywork well even today. Versioning And Vendoring But in production contexts you need to be more precise about dependency versions, to make builds reproducible. Many people experimented with what that should look like, building tools that served their needs, including Keith Raricks goven (2012) and godep (2013), Matt Butchers glide (2014), and Dave Cheneys gb (2015). All of these tools use the model that you copy dependency packages into your own source control repository. The exact mechanisms used to make those packages available for import varied, but they were all more complex than it seemed they should be. After a community-wide discussion, we adopted a proposal by Keith Rarick to add explicit support for referring to copied dependencies without GOPATH tricks. This was simplifying by reshaping: like with addToList and append, these tools were already implementing the concept, but it was more awkward than it needed to be. Adding explicit support for vendor directories made these uses simpler overall. Shipping vendor directories in the go command led to more experimentation with vendoring itself, and we realized that we had introduced a few problems. The most serious was that we lost package uniqueness. Before, during any given build, an import path might appear in lots of different packages, and all the imports referred to the same target. Now with vendoring, the same import path in different packages might refer to different vendored copies of the package, all of which would appear in the final resulting binary. At the time, we didnt have a name for this property: package uniqueness. It was just how the GOPATH model worked. We didnt completely appreciate it until it went away. There is a parallel here with the check and try error syntax proposals. In that case, we were relying on how the visible return statement worked in ways we didnt appreciate until we considered removing it. When we added vendor directory support, there were many different tools for managing dependencies. We thought that a clear agreement about the format of vendor directories and vendoring metadata would allow the various tools to interoperate, the same way that agreement about how Go programs are stored in text files enables interoperation between the Go compiler, text editors, and tools like goimports and gorename. This turned out to be naively optimistic. The vendoring tools all differed in subtle semantic ways. Interoperation would require changing them all to agree about the semantics, likely breaking their respective users. Convergence did not happen. Dep At Gophercon in 2016, we started an effort to define a single tool to manage dependencies. As part of that effort, we conducted surveys with many different kinds of users to understand what they needed as far as dependency management, and a team started work on a new tool, which became dep. Dep aimed to be able to replace all the existing dependency management tools. The goal was to simplify by reshaping the existing different tools into a single one. It partly accomplished that. Dep also restored package uniqueness for its users, by having only one vendor directory at the top of the project tree. But dep also introduced a serious problem that took us a while to fully appreciate. The problem was that dep embraced a design choice from glide, to support and encourage incompatible changes to a given package without changing the import path. Here is an example. Suppose you are building your own program, and you need to have a configuration file, so you use version 2 of a popular Go YAML package: Now suppose your program imports the Kubernetes client. It turns out that Kubernetes uses YAML extensively, and it uses version 1 of the same popular package: Version 1 and version 2 have incompatible APIs, but they also have different import paths, so there is no ambiguity about which is meant by a given import. Kubernetes gets version 1, your config parser gets version 2, and everything works. Dep abandoned this model. Version 1 and version 2 of the yaml package would now have the same import path, producing a conflict. Using the same import path for two incompatible versions, combined with package uniqueness, makes it impossible to build this program that you could build before: It took us a while to understand this problem, because we had been applying the new API means new import path convention for so long that we took it for granted. The dep experiment helped us appreciate that convention better, and we gave it a name: the import compatibility rule: If an old package and a new package have the same import path, the new package must be backwards compatible with the old package. Go Modules We took what worked well in the dep experiment and what we learned about what didnt work well, and we experimented with a new design, called vgo. In vgo, packages followed the import compatibility rule, so that we can provide package uniqueness but still not break builds like the one we just looked at. This let us simplify other parts of the design as well. Besides restoring the import compatibility rule, another important part of the vgo design was to give the concept of a group of packages a name and to allow that grouping to be separated from source code repository boundaries. The name of a group of Go packages is a module, so we refer to the system now as Go modules. Go modules are now integrated with the go command, which avoids needing to copy around vendor directories at all. Replacing GOPATH With Go modules comes the end of GOPATH as a global name space. Nearly all the hard work of converting existing Go usage and tools to modules is caused by this change, from moving away from GOPATH. The fundamental idea of GOPATH is that the GOPATH directory tree is the global source of truth for what versions are being used, and the versions being used dont change as you move around between directories. But the global GOPATH mode is in direct conflict with the production requirement of per-project reproducible builds, which itself simplifies the Go development and deployment experience in many important ways. Per-project reproducible builds means that when you are working in a checkout of project A, you get the same set of dependency versions that the other developers of project A get at that commit, as defined by the go.mod file. When you switch to working in a checkout of project B, now you get that projects chosen dependency versions, the same set that the other developers of project B get. But those are likely different from project A. The set of dependency versions changing when you move from project A to project B is necessary to keep your development in sync with that of the other developers on A and on B. There cant be a single global GOPATH anymore. Most of the complexity of adopting modules arises directly from the loss of the one global GOPATH. Where is the source code for a package? Before, the answer depended only on your GOPATH environment variable, which most people rarely changed. Now, the answer depends on what project you are working on, which may change often. Everything needs updating for this new convention. Most development tools use the go/build package to find and load Go source code. Weve kept that package working, but the API did not anticipate modules, and the workarounds we added to avoid API changes are slower than wed like. Weve published a replacement, golang.org/x/tools/go/packages. Developer tools should now use that instead. It supports both GOPATH and Go modules, and it is faster and easier to use. In a release or two we may move it into the standard library, but for now golang.org/x/tools/go/packages is stable and ready for use. Go Module Proxies One of the ways modules simplify Go development is by separating the concept of a group of packages from the underlying source control repository where they are stored. When we talked to Go users about dependencies, almost everyone using Go at their companies asked how to route go get package fetches through their own servers, to better control what code can be used. And even open-source developers were concerned about dependencies disappearing or changing unexpectedly, breaking their builds. Before modules, users had attempted complex solutions to these problems, including intercepting the version control commands that the go command runs. The Go modules design makes it easy to introduce the idea of a module proxy that can be asked for a specific module version. Companies can now easily run their own module proxy, with custom rules about what is allowed and where cached copies are stored. The open-source Athens project has built just such a proxy, and Aaron Schlesinger gave a talk about it at Gophercon 2019. (Well add a link here when the video becomes available.) And for individual developers and open source teams, the Go team at Google has launched a proxy that serves as a public mirror of all open-source Go packages, and Go 1.13 will use that proxy by default when in module mode. Katie Hockman gave a talk about this system at Gophercon 2019. Go Modules Status Go 1.11 introduced modules as an experimental, opt-in preview. We keep experimenting and simplifying. Go 1.12 shipped improvements, and Go 1.13 will ship more improvements. Modules are now at the point where we believe that they will serve most users, but we arent ready to shut down GOPATH just yet. We will keep experimenting, simplifying, and revising. We fully recognize that the Go user community built up almost a decade of experience and tooling and workflows around GOPATH, and it will take a while to convert all of that to Go modules. But again, we think that modules will now work very well for most users, and I encourage you to take a look when Go 1.13 is released. As one data point, the Kubernetes project has a lot of dependencies, and they have migrated to using Go modules to manage them. You probably can too. And if you cant, please let us know whats not working for you or whats too complex, by filing a bug report, and we will experiment and simplify. Error handling, generics, and dependency management are going to take a few more years at least, and were going to focus on them for now. Error handling is close to done, modules will be next after that, and maybe generics after that. But suppose we look a couple years out, to when we are done experimenting and simplifying and have shipped error handling, modules, and generics. Then what? Its very difficult to predict the future, but I think that once these three have shipped, that may mark the start of a new quiet period for major changes. Our focus at that point will likely shift to simplifying Go development with improved tools. Some of the tool work is already underway, so this post finishes by looking at that. While we helped update all the Go communitys existing tools to understand Go modules, we noticed that having a ton of development helper tools that each do one small job is not serving users well. The individual tools are too hard to combine, too slow to invoke, and too different to use. We began an effort to unify the most commonly-required development helpers into a single tool, now called gopls (pronounced go, please). Gopls speaks the Language Server Protocol, LSP, and works with any integrated development environment or text editor with LSP support, which is essentially everything at this point. Gopls marks an expansion in focus for the Go project, from delivering standalone compiler-like, command-line tools like go vet or gorename to also delivering a complete IDE service. Rebecca Stambler gave a talk with more details about gopls and IDEs at Gophercon 2019. (Well add a link here when the video becomes available.) After gopls, we also have ideas for reviving go fix in an extensible way and for making go vet even more helpful. Coda So theres the path to Go 2. We will experiment and simplify. And experiment and simplify. And ship. And experiment and simplify. And do it all again. It may look or even feel like the path goes around in circles. But each time we experiment and simplify we learn a little more about what Go 2 should look like and move another step closer to it. Even abandoned experiments like try or our first four generics designs or dep are not wasted time. They help us learn what needs to be simplified before we can ship, and in some cases they help us better understand something we took for granted. At some point we will realize we have experimented enough, and simplified enough, and shipped enough, and we will have Go 2. Thanks to all of you in the Go community for helping us experiment and simplify and ship and find our way on this path. ",
          "I personally think they missed one big pain point of Go: the logging. The standard logging should be an interface for structured logs that can be implemented by various vendors instead of the current really basic logger. This is especially bad for third party libraries that either force you to use one logging library (logrus, zap, etc) or built their own custom interface which you then have to implement yourself for your logger.\nThat is something they should have stolen from Java...",
          "This was an exhausting read because Rich Hickeys \"Simple Made Easy\" presentation from years ago transmitted a mind-virus to me where I have to stop and have an internal conflict every time I read simpl(e|ify) in a programming context.<p>It's notable how so many independent programming communities seem enamored with Simple being the goal, yet probably don't even agree on the definition of the term."
        ],
        "story_type": ["Normal"],
        "url": "https://blog.golang.org/experiment",
        "comments.comment_id": [20590824, 20591245],
        "comments.comment_author": ["Sytten", "frou_dh"],
        "comments.comment_descendants": [1, 2],
        "comments.comment_time": [
          "2019-08-02T05:31:54Z",
          "2019-08-02T07:39:27Z"
        ],
        "comments.comment_text": [
          "I personally think they missed one big pain point of Go: the logging. The standard logging should be an interface for structured logs that can be implemented by various vendors instead of the current really basic logger. This is especially bad for third party libraries that either force you to use one logging library (logrus, zap, etc) or built their own custom interface which you then have to implement yourself for your logger.\nThat is something they should have stolen from Java...",
          "This was an exhausting read because Rich Hickeys \"Simple Made Easy\" presentation from years ago transmitted a mind-virus to me where I have to stop and have an internal conflict every time I read simpl(e|ify) in a programming context.<p>It's notable how so many independent programming communities seem enamored with Simple being the goal, yet probably don't even agree on the definition of the term."
        ],
        "id": "759e47aa-be95-4980-a761-1b9974f8936a",
        "url_text": "The Go Blog Introduction This is the blog post version of my talk last week at GopherCon 2019. We are all on the path to Go 2, together, but none of us know exactly where that path leads or sometimes even which direction the path goes. This post discusses how we actually find and follow the path to Go 2. Heres what the process looks like. We experiment with Go as it exists now, to understand it better, learning what works well and what doesnt. Then we experiment with possible changes, to understand them better, again learning what works well and what doesnt. Based on what we learn from those experiments, we simplify. And then we experiment again. And then we simplify again. And so on. And so on. The Four Rs of Simplifying During this process, there are four main ways that we can simplify the overall experience of writing Go programs: reshaping, redefining, removing, and restricting. Simplify by Reshaping The first way we simplify is by reshaping what exists into a new form, one that ends up being simpler overall. Every Go program we write serves as an experiment to test Go itself. In the early days of Go, we quickly learned that it was common to write code like this addToList function: func addToList(list []int, x int) []int { n := len(list) if n+1 > cap(list) { big := make([]int, n, (n+5)*2) copy(big, list) list = big } list = list[:n+1] list[n] = x return list } Wed write the same code for slices of bytes, and slices of strings, and so on. Our programs were too complex, because Go was too simple. So we took the many functions like addToList in our programs and reshaped them into one function provided by Go itself. Adding append made the Go language a little more complex, but on balance it made the overall experience of writing Go programs simpler, even after accounting for the cost of learning about append. Heres another example. For Go 1, we looked at the very many development tools in the Go distribution, and we reshaped them into one new command. 5a 8g 5g 8l 5l cgo 6a gobuild 6cov gofix go 6g goinstall 6l gomake 6nm gopack 8a govet The go command is so central now that it is easy to forget that we went so long without it and how much extra work that involved. We added code and complexity to the Go distribution, but on balance we simplified the experience of writing Go programs. The new structure also created space for other interesting experiments, which well see later. Simplify by Redefining A second way we simplify is by redefining functionality we already have, allowing it to do more. Like simplifying by reshaping, simplifying by redefining makes programs simpler to write, but now with nothing new to learn. For example, append was originally defined to read only from slices. When appending to a byte slice, you could append the bytes from another byte slice, but not the bytes from a string. We redefined append to allow appending from a string, without adding anything new to the language. var b []byte var more []byte b = append(b, more...) // ok var b []byte var more string b = append(b, more...) // ok later Simplify by Removing A third way we simplify is by removing functionality when it has turned out to be less useful or less important than we expected. Removing functionality means one less thing to learn, one less thing to fix bugs in, one less thing to be distracted by or use incorrectly. Of course, removing also forces users to update existing programs, perhaps making them more complex, to make up for the removal. But the overall result can still be that the process of writing Go programs becomes simpler. An example of this is when we removed the boolean forms of non-blocking channel operations from the language: ok := c <- x // before Go 1, was non-blocking send x, ok := <-c // before Go 1, was non-blocking receive These operations were also possible to do using select, making it confusing to need to decide which form to use. Removing them simplified the language without reducing its power. Simplify by Restricting We can also simplify by restricting what is allowed. From day one, Go has restricted the encoding of Go source files: they must be UTF-8. This restriction makes every program that tries to read Go source files simpler. Those programs dont have to worry about Go source files encoded in Latin-1 or UTF-16 or UTF-7 or anything else. Another important restriction is gofmt for program formatting. Nothing rejects Go code that isnt formatted using gofmt, but we have established a convention that tools that rewrite Go programs leave them in gofmt form. If you keep your programs in gofmt form too, then these rewriters dont make any formatting changes. When you compare before and after, the only diffs you see are real changes. This restriction has simplified program rewriters and led to successful experiments like goimports, gorename, and many others. Go Development Process This cycle of experiment and simplify is a good model for what weve been doing the past ten years. but it has a problem: its too simple. We cant only experiment and simplify. We have to ship the result. We have to make it available to use. Of course, using it enables more experiments, and possibly more simplifying, and the process cycles on and on. We shipped Go to all of you for the first time on November 10, 2009. Then, with your help, we shipped Go 1 together in March 2012. And weve shipped twelve Go releases since then. All of these were important milestones, to enable more experimentation, to help us learn more about Go, and of course to make Go available for production use. When we shipped Go 1, we explicitly shifted our focus to using Go, to understand this version of the language much better before trying any more simplifications involving language changes. We needed to take time to experiment, to really understand what works and what doesnt. Of course, weve had twelve releases since Go 1, so we have still been experimenting and simplifying and shipping. But weve focused on ways to simplify Go development without significant language changes and without breaking existing Go programs. For example, Go 1.5 shipped the first concurrent garbage collector and then the following releases improved it, simplifying Go development by removing pause times as an ongoing concern. At Gophercon in 2017, we announced that after five years of experimentation, it was again time to think about significant changes that would simplify Go development. Our path to Go 2 is really the same as the path to Go 1: experiment and simplify and ship, towards an overall goal of simplifying Go development. For Go 2, the concrete topics that we believed were most important to address are error handling, generics, and dependencies. Since then we have realized that another important topic is developer tooling. The rest of this post discusses how our work in each of these areas follows that path. Along the way, well take one detour, stopping to inspect the technical detail of what will be shipping soon in Go 1.13 for error handling. Errors It is hard enough to write a program that works the right way in all cases when all the inputs are valid and correct and nothing the program depends on is failing. When you add errors into the mix, writing a program that works the right way no matter what goes wrong is even harder. As part of thinking about Go 2, we want to understand better whether Go can help make that job any simpler. There are two different aspects that could potentially be simplified: error values and error syntax. Well look at each in turn, with the technical detour I promised focusing on the Go 1.13 error value changes. Error Values Error values had to start somewhere. Here is the Read function from the first version of the os package: export func Read(fd int64, b *[]byte) (ret int64, errno int64) { r, e := syscall.read(fd, &b[0], int64(len(b))); return r, e } There was no File type yet, and also no error type. Read and the other functions in the package returned an errno int64 directly from the underlying Unix system call. This code was checked in on September 10, 2008 at 12:14pm. Like everything back then, it was an experiment, and code changed quickly. Two hours and five minutes later, the API changed: export type Error struct { s string } func (e *Error) Print() { } // to standard error! func (e *Error) String() string { } export func Read(fd int64, b *[]byte) (ret int64, err *Error) { r, e := syscall.read(fd, &b[0], int64(len(b))); return r, ErrnoToError(e) } This new API introduced the first Error type. An error held a string and could return that string and also print it to standard error. The intent here was to generalize beyond integer codes. We knew from past experience that operating system error numbers were too limited a representation, that it would simplify programs not to have to shoehorn all detail about an error into 64 bits. Using error strings had worked reasonably well for us in the past, so we did the same here. This new API lasted seven months. The next April, after more experience using interfaces, we decided to generalize further and allow user-defined error implementations, by making the os.Error type itself an interface. We simplified by removing the Print method. For Go 1 two years later, based on a suggestion by Roger Peppe, os.Error became the built-in error type, and the String method was renamed to Error. Nothing has changed since then. But we have written many Go programs, and as a result we have experimented a lot with how best to implement and use errors. Errors Are Values Making error a simple interface and allowing many different implementations means we have the entire Go language available to define and inspect errors. We like to say that errors are values, the same as any other Go value. Heres an example. On Unix, an attempt to dial a network connection ends up using the connect system call. That system call returns a syscall.Errno, which is a named integer type that represents a system call error number and implements the error interface: package syscall type Errno int64 func (e Errno) Error() string { ... } const ECONNREFUSED = Errno(61) ... err == ECONNREFUSED ... The syscall package also defines named constants for the host operating systems defined error numbers. In this case, on this system, ECONNREFUSED is number 61. Code that gets an error from a function can test whether the error is ECONNREFUSED using ordinary value equality. Moving up a level, in package os, any system call failure is reported using a larger error structure that records what operation was attempted in addition to the error. There are a handful of these structures. This one, SyscallError, describes an error invoking a specific system call with no additional information recorded: package os type SyscallError struct { Syscall string Err error } func (e *SyscallError) Error() string { return e.Syscall + \": \" + e.Err.Error() } Moving up another level, in package net, any network failure is reported using an even larger error structure that records the details of the surrounding network operation, such as dial or listen, and the network and addresses involved: package net type OpError struct { Op string Net string Source Addr Addr Addr Err error } func (e *OpError) Error() string { ... } Putting these together, the errors returned by operations like net.Dial can format as strings, but they are also structured Go data values. In this case, the error is a net.OpError, which adds context to an os.SyscallError, which adds context to a syscall.Errno: c, err := net.Dial(\"tcp\", \"localhost:50001\") // \"dial tcp [::1]:50001: connect: connection refused\" err is &net.OpError{ Op: \"dial\", Net: \"tcp\", Addr: &net.TCPAddr{IP: ParseIP(\"::1\"), Port: 50001}, Err: &os.SyscallError{ Syscall: \"connect\", Err: syscall.Errno(61), // == ECONNREFUSED }, } When we say errors are values, we mean both that the entire Go language is available to define them and also that the entire Go language is available to inspect them. Here is an example from package net. It turns out that when you attempt a socket connection, most of the time you will get connected or get connection refused, but sometimes you can get a spurious EADDRNOTAVAIL, for no good reason. Go shields user programs from this failure mode by retrying. To do this, it has to inspect the error structure to find out whether the syscall.Errno deep inside is EADDRNOTAVAIL. Here is the code: func spuriousENOTAVAIL(err error) bool { if op, ok := err.(*OpError); ok { err = op.Err } if sys, ok := err.(*os.SyscallError); ok { err = sys.Err } return err == syscall.EADDRNOTAVAIL } A type assertion peels away any net.OpError wrapping. And then a second type assertion peels away any os.SyscallError wrapping. And then the function checks the unwrapped error for equality with EADDRNOTAVAIL. What weve learned from years of experience, from this experimenting with Go errors, is that it is very powerful to be able to define arbitrary implementations of the error interface, to have the full Go language available both to construct and to deconstruct errors, and not to require the use of any single implementation. These propertiesthat errors are values, and that there is not one required error implementationare important to preserve. Not mandating one error implementation enabled everyone to experiment with additional functionality that an error might provide, leading to many packages, such as github.com/pkg/errors, gopkg.in/errgo.v2, github.com/hashicorp/errwrap, upspin.io/errors, github.com/spacemonkeygo/errors, and more. One problem with unconstrained experimentation, though, is that as a client you have to program to the union of all the possible implementations you might encounter. A simplification that seemed worth exploring for Go 2 was to define a standard version of commonly-added functionality, in the form of agreed-upon optional interfaces, so that different implementations could interoperate. Unwrap The most commonly-added functionality in these packages is some method that can be called to remove context from an error, returning the error inside. Packages use different names and meanings for this operation, and sometimes it removes one level of context, while sometimes it removes as many levels as possible. For Go 1.13, we have introduced a convention that an error implementation adding removable context to an inner error should implement an Unwrap method that returns the inner error, unwrapping the context. If there is no inner error appropriate to expose to callers, either the error shouldnt have an Unwrap method, or the Unwrap method should return nil. // Go 1.13 optional method for error implementations. interface { // Unwrap removes one layer of context, // returning the inner error if any, or else nil. Unwrap() error } The way to call this optional method is to invoke the helper function errors.Unwrap, which handles cases like the error itself being nil or not having an Unwrap method at all. package errors // Unwrap returns the result of calling // the Unwrap method on err, // if errs type defines an Unwrap method. // Otherwise, Unwrap returns nil. func Unwrap(err error) error We can use the Unwrap method to write a simpler, more general version of spuriousENOTAVAIL. Instead of looking for specific error wrapper implementations like net.OpError or os.SyscallError, the general version can loop, calling Unwrap to remove context, until either it reaches EADDRNOTAVAIL or theres no error left: func spuriousENOTAVAIL(err error) bool { for err != nil { if err == syscall.EADDRNOTAVAIL { return true } err = errors.Unwrap(err) } return false } This loop is so common, though, that Go 1.13 defines a second function, errors.Is, that repeatedly unwraps an error looking for a specific target. So we can replace the entire loop with a single call to errors.Is: func spuriousENOTAVAIL(err error) bool { return errors.Is(err, syscall.EADDRNOTAVAIL) } At this point we probably wouldnt even define the function; it would be equally clear, and simpler, to call errors.Is directly at the call sites. Go 1.13 also introduces a function errors.As that unwraps until it finds a specific implementation type. If you want to write code that works with arbitrarily-wrapped errors, errors.Is is the wrapper-aware version of an error equality check: err == target errors.Is(err, target) And errors.As is the wrapper-aware version of an error type assertion: target, ok := err.(*Type) if ok { ... } var target *Type if errors.As(err, &target) { ... } To Unwrap Or Not To Unwrap? Whether to make it possible to unwrap an error is an API decision, the same way that whether to export a struct field is an API decision. Sometimes it is appropriate to expose that detail to calling code, and sometimes it isnt. When it is, implement Unwrap. When it isnt, dont implement Unwrap. Until now, fmt.Errorf has not exposed an underlying error formatted with %v to caller inspection. That is, the result of fmt.Errorf has not been possible to unwrap. Consider this example: // errors.Unwrap(err2) == nil // err1 is not available (same as earlier Go versions) err2 := fmt.Errorf(\"connect: %v\", err1) If err2 is returned to a caller, that caller has never had any way to open up err2 and access err1. We preserved that property in Go 1.13. For the times when you do want to allow unwrapping the result of fmt.Errorf, we also added a new printing verb %w, which formats like %v, requires an error value argument, and makes the resulting errors Unwrap method return that argument. In our example, suppose we replace %v with %w: // errors.Unwrap(err4) == err3 // (%w is new in Go 1.13) err4 := fmt.Errorf(\"connect: %w\", err3) Now, if err4 is returned to a caller, the caller can use Unwrap to retrieve err3. It is important to note that absolute rules like always use %v (or never implement Unwrap) or always use %w (or always implement Unwrap) are as wrong as absolute rules like never export struct fields or always export struct fields. Instead, the right decision depends on whether callers should be able to inspect and depend on the additional information that using %w or implementing Unwrap exposes. As an illustration of this point, every error-wrapping type in the standard library that already had an exported Err field now also has an Unwrap method returning that field, but implementations with unexported error fields do not, and existing uses of fmt.Errorf with %v still use %v, not %w. Error Value Printing (Abandoned) Along with the design draft for Unwrap, we also published a design draft for an optional method for richer error printing, including stack frame information and support for localized, translated errors. // Optional method for error implementations type Formatter interface { Format(p Printer) (next error) } // Interface passed to Format type Printer interface { Print(args ...interface{}) Printf(format string, args ...interface{}) Detail() bool } This one is not as simple as Unwrap, and I wont go into the details here. As we discussed the design with the Go community over the winter, we learned that the design wasnt simple enough. It was too hard for individual error types to implement, and it did not help existing programs enough. On balance, it did not simplify Go development. As a result of this community discussion, we abandoned this printing design. Error Syntax That was error values. Lets look briefly at error syntax, another abandoned experiment. Here is some code from compress/lzw/writer.go in the standard library: // Write the savedCode if valid. if e.savedCode != invalidCode { if err := e.write(e, e.savedCode); err != nil { return err } if err := e.incHi(); err != nil && err != errOutOfCodes { return err } } // Write the eof code. eof := uint32(1)<<e.litWidth + 1 if err := e.write(e, eof); err != nil { return err } At a glance, this code is about half error checks. My eyes glaze over when I read it. And we know that code that is tedious to write and tedious to read is easy to misread, making it a good home for hard-to-find bugs. For example, one of these three error checks is not like the others, a fact that is easy to miss on a quick skim. If you were debugging this code, how long would it take to notice that? At Gophercon last year we presented a draft design for a new control flow construct marked by the keyword check. Check consumes the error result from a function call or expression. If the error is non-nil, the check returns that error. Otherwise the check evaluates to the other results from the call. We can use check to simplify the lzw code: // Write the savedCode if valid. if e.savedCode != invalidCode { check e.write(e, e.savedCode) if err := e.incHi(); err != errOutOfCodes { check err } } // Write the eof code. eof := uint32(1)<<e.litWidth + 1 check e.write(e, eof) This version of the same code uses check, which removes four lines of code and more importantly highlights that the call to e.incHi is allowed to return errOutOfCodes. Maybe most importantly, the design also allowed defining error handler blocks to be run when later checks failed. That would let you write shared context-adding code just once, like in this snippet: handle err { err = fmt.Errorf(\"closing writer: %w\", err) } // Write the savedCode if valid. if e.savedCode != invalidCode { check e.write(e, e.savedCode) if err := e.incHi(); err != errOutOfCodes { check err } } // Write the eof code. eof := uint32(1)<<e.litWidth + 1 check e.write(e, eof) In essence, check was a short way to write the if statement, and handle was like defer but only for error return paths. In contrast to exceptions in other languages, this design retained Gos important property that every potential failing call was marked explicitly in the code, now using the check keyword instead of if err != nil. The big problem with this design was that handle overlapped too much, and in confusing ways, with defer. In May we posted a new design with three simplifications: to avoid the confusion with defer, the design dropped handle in favor of just using defer; to match a similar idea in Rust and Swift, the design renamed check to try; and to allow experimentation in a way that existing parsers like gofmt would recognize, it changed check (now try) from a keyword to a built-in function. Now the same code would look like this: defer errd.Wrapf(&err, \"closing writer\") // Write the savedCode if valid. if e.savedCode != invalidCode { try(e.write(e, e.savedCode)) if err := e.incHi(); err != errOutOfCodes { try(err) } } // Write the eof code. eof := uint32(1)<<e.litWidth + 1 try(e.write(e, eof)) We spent most of June discussing this proposal publicly on GitHub. The fundamental idea of check or try was to shorten the amount of syntax repeated at each error check, and in particular to remove the return statement from view, keeping the error check explicit and better highlighting interesting variations. One interesting point raised during the public feedback discussion, however, was that without an explicit if statement and return, theres nowhere to put a debugging print, theres nowhere to put a breakpoint, and theres no code to show as unexecuted in code coverage results. The benefits we were after came at the cost of making these situations more complex. On balance, from this as well as other considerations, it was not at all clear that the overall result would be simpler Go development, so we abandoned this experiment. Thats everything about error handling, which was one of the main focuses for this year. Generics Now for something a little less controversial: generics. The second big topic we identified for Go 2 was some kind of way to write code with type parameters. This would enable writing generic data structures and also writing generic functions that work with any kind of slice, or any kind of channel, or any kind of map. For example, here is a generic channel filter: // Filter copies values from c to the returned channel, // passing along only those values satisfying f. func Filter(type value)(f func(value) bool, c <-chan value) <-chan value { out := make(chan value) go func() { for v := range c { if f(v) { out <- v } } close(out) }() return out } Weve been thinking about generics since work on Go began, and we wrote and rejected our first concrete design in 2010. We wrote and rejected three more designs by the end of 2013. Four abandoned experiments, but not failed experiments, We learned from them, like we learned from check and try. Each time, we learned that the path to Go 2 is not in that exact direction, and we noticed other directions that might be interesting to explore. But by 2013 we had decided that we needed to focus on other concerns, so we put the entire topic aside for a few years. Last year we started exploring and experimenting again, and we presented a new design, based on the idea of a contract, at Gophercon last summer. Weve continued to experiment and simplify, and weve been working with programming language theory experts to understand the design better. Overall, I am hopeful that were headed in a good direction, toward a design that will simplify Go development. Even so, we might find that this design doesnt work either. We might have to abandon this experiment and adjust our path based on what we learned. Well find out. At Gophercon 2019, Ian Lance Taylor talked about why we might want to add generics to Go and briefly previewed the latest design draft. For details, see his blog post Why Generics? Dependencies The third big topic we identified for Go 2 was dependency management. In 2010 we published a tool called goinstall, which we called an experiment in package installation. It downloaded dependencies and stored them in your Go distribution tree, in GOROOT. As we experimented with goinstall, we learned that the Go distribution and the installed packages should be kept separate, so that it was possible to change to a new Go distribution without losing all your Go packages. So in 2011 we introduced GOPATH, an environment variable that specified where to look for packages not found in the main Go distribution. Adding GOPATH created more places for Go packages but simplified Go development overall, by separating your Go distribution from your Go libraries. Compatibility The goinstall experiment intentionally left out an explicit concept of package versioning. Instead, goinstall always downloaded the latest copy. We did this so we could focus on the other design problems for package installation. Goinstall became go get as part of Go 1. When people asked about versions, we encouraged them to experiment by creating additional tools, and they did. And we encouraged package AUTHORS to provide their USERS with the same backwards compatibility we did for the Go 1 libraries. Quoting the Go FAQ: Packages intended for public use should try to maintain backwards compatibility as they evolve. If different functionality is required, add a new name instead of changing an old one. If a complete break is required, create a new package with a new import path. This convention simplifies the overall experience of using a package by restricting what authors can do: avoid breaking changes to APIs; give new functionality a new name; and give a whole new package design a new import path. Of course, people kept experimenting. One of the most interesting experiments was started by Gustavo Niemeyer. He created a Git redirector called gopkg.in, which provided different import paths for different API versions, to help package authors follow the convention of giving a new package design a new import path. For example, the Go source code in the GitHub repository go-yaml/yaml has different APIs in the v1 and v2 semantic version tags. The gopkg.in server provides these with different import paths gopkg.in/yaml.v1 and gopkg.in/yaml.v2. The convention of providing backwards compatibility, so that a newer version of a package can be used in place of an older version, is what makes go gets very simple rulealways download the latest copywork well even today. Versioning And Vendoring But in production contexts you need to be more precise about dependency versions, to make builds reproducible. Many people experimented with what that should look like, building tools that served their needs, including Keith Raricks goven (2012) and godep (2013), Matt Butchers glide (2014), and Dave Cheneys gb (2015). All of these tools use the model that you copy dependency packages into your own source control repository. The exact mechanisms used to make those packages available for import varied, but they were all more complex than it seemed they should be. After a community-wide discussion, we adopted a proposal by Keith Rarick to add explicit support for referring to copied dependencies without GOPATH tricks. This was simplifying by reshaping: like with addToList and append, these tools were already implementing the concept, but it was more awkward than it needed to be. Adding explicit support for vendor directories made these uses simpler overall. Shipping vendor directories in the go command led to more experimentation with vendoring itself, and we realized that we had introduced a few problems. The most serious was that we lost package uniqueness. Before, during any given build, an import path might appear in lots of different packages, and all the imports referred to the same target. Now with vendoring, the same import path in different packages might refer to different vendored copies of the package, all of which would appear in the final resulting binary. At the time, we didnt have a name for this property: package uniqueness. It was just how the GOPATH model worked. We didnt completely appreciate it until it went away. There is a parallel here with the check and try error syntax proposals. In that case, we were relying on how the visible return statement worked in ways we didnt appreciate until we considered removing it. When we added vendor directory support, there were many different tools for managing dependencies. We thought that a clear agreement about the format of vendor directories and vendoring metadata would allow the various tools to interoperate, the same way that agreement about how Go programs are stored in text files enables interoperation between the Go compiler, text editors, and tools like goimports and gorename. This turned out to be naively optimistic. The vendoring tools all differed in subtle semantic ways. Interoperation would require changing them all to agree about the semantics, likely breaking their respective users. Convergence did not happen. Dep At Gophercon in 2016, we started an effort to define a single tool to manage dependencies. As part of that effort, we conducted surveys with many different kinds of users to understand what they needed as far as dependency management, and a team started work on a new tool, which became dep. Dep aimed to be able to replace all the existing dependency management tools. The goal was to simplify by reshaping the existing different tools into a single one. It partly accomplished that. Dep also restored package uniqueness for its users, by having only one vendor directory at the top of the project tree. But dep also introduced a serious problem that took us a while to fully appreciate. The problem was that dep embraced a design choice from glide, to support and encourage incompatible changes to a given package without changing the import path. Here is an example. Suppose you are building your own program, and you need to have a configuration file, so you use version 2 of a popular Go YAML package: Now suppose your program imports the Kubernetes client. It turns out that Kubernetes uses YAML extensively, and it uses version 1 of the same popular package: Version 1 and version 2 have incompatible APIs, but they also have different import paths, so there is no ambiguity about which is meant by a given import. Kubernetes gets version 1, your config parser gets version 2, and everything works. Dep abandoned this model. Version 1 and version 2 of the yaml package would now have the same import path, producing a conflict. Using the same import path for two incompatible versions, combined with package uniqueness, makes it impossible to build this program that you could build before: It took us a while to understand this problem, because we had been applying the new API means new import path convention for so long that we took it for granted. The dep experiment helped us appreciate that convention better, and we gave it a name: the import compatibility rule: If an old package and a new package have the same import path, the new package must be backwards compatible with the old package. Go Modules We took what worked well in the dep experiment and what we learned about what didnt work well, and we experimented with a new design, called vgo. In vgo, packages followed the import compatibility rule, so that we can provide package uniqueness but still not break builds like the one we just looked at. This let us simplify other parts of the design as well. Besides restoring the import compatibility rule, another important part of the vgo design was to give the concept of a group of packages a name and to allow that grouping to be separated from source code repository boundaries. The name of a group of Go packages is a module, so we refer to the system now as Go modules. Go modules are now integrated with the go command, which avoids needing to copy around vendor directories at all. Replacing GOPATH With Go modules comes the end of GOPATH as a global name space. Nearly all the hard work of converting existing Go usage and tools to modules is caused by this change, from moving away from GOPATH. The fundamental idea of GOPATH is that the GOPATH directory tree is the global source of truth for what versions are being used, and the versions being used dont change as you move around between directories. But the global GOPATH mode is in direct conflict with the production requirement of per-project reproducible builds, which itself simplifies the Go development and deployment experience in many important ways. Per-project reproducible builds means that when you are working in a checkout of project A, you get the same set of dependency versions that the other developers of project A get at that commit, as defined by the go.mod file. When you switch to working in a checkout of project B, now you get that projects chosen dependency versions, the same set that the other developers of project B get. But those are likely different from project A. The set of dependency versions changing when you move from project A to project B is necessary to keep your development in sync with that of the other developers on A and on B. There cant be a single global GOPATH anymore. Most of the complexity of adopting modules arises directly from the loss of the one global GOPATH. Where is the source code for a package? Before, the answer depended only on your GOPATH environment variable, which most people rarely changed. Now, the answer depends on what project you are working on, which may change often. Everything needs updating for this new convention. Most development tools use the go/build package to find and load Go source code. Weve kept that package working, but the API did not anticipate modules, and the workarounds we added to avoid API changes are slower than wed like. Weve published a replacement, golang.org/x/tools/go/packages. Developer tools should now use that instead. It supports both GOPATH and Go modules, and it is faster and easier to use. In a release or two we may move it into the standard library, but for now golang.org/x/tools/go/packages is stable and ready for use. Go Module Proxies One of the ways modules simplify Go development is by separating the concept of a group of packages from the underlying source control repository where they are stored. When we talked to Go users about dependencies, almost everyone using Go at their companies asked how to route go get package fetches through their own servers, to better control what code can be used. And even open-source developers were concerned about dependencies disappearing or changing unexpectedly, breaking their builds. Before modules, users had attempted complex solutions to these problems, including intercepting the version control commands that the go command runs. The Go modules design makes it easy to introduce the idea of a module proxy that can be asked for a specific module version. Companies can now easily run their own module proxy, with custom rules about what is allowed and where cached copies are stored. The open-source Athens project has built just such a proxy, and Aaron Schlesinger gave a talk about it at Gophercon 2019. (Well add a link here when the video becomes available.) And for individual developers and open source teams, the Go team at Google has launched a proxy that serves as a public mirror of all open-source Go packages, and Go 1.13 will use that proxy by default when in module mode. Katie Hockman gave a talk about this system at Gophercon 2019. Go Modules Status Go 1.11 introduced modules as an experimental, opt-in preview. We keep experimenting and simplifying. Go 1.12 shipped improvements, and Go 1.13 will ship more improvements. Modules are now at the point where we believe that they will serve most users, but we arent ready to shut down GOPATH just yet. We will keep experimenting, simplifying, and revising. We fully recognize that the Go user community built up almost a decade of experience and tooling and workflows around GOPATH, and it will take a while to convert all of that to Go modules. But again, we think that modules will now work very well for most users, and I encourage you to take a look when Go 1.13 is released. As one data point, the Kubernetes project has a lot of dependencies, and they have migrated to using Go modules to manage them. You probably can too. And if you cant, please let us know whats not working for you or whats too complex, by filing a bug report, and we will experiment and simplify. Error handling, generics, and dependency management are going to take a few more years at least, and were going to focus on them for now. Error handling is close to done, modules will be next after that, and maybe generics after that. But suppose we look a couple years out, to when we are done experimenting and simplifying and have shipped error handling, modules, and generics. Then what? Its very difficult to predict the future, but I think that once these three have shipped, that may mark the start of a new quiet period for major changes. Our focus at that point will likely shift to simplifying Go development with improved tools. Some of the tool work is already underway, so this post finishes by looking at that. While we helped update all the Go communitys existing tools to understand Go modules, we noticed that having a ton of development helper tools that each do one small job is not serving users well. The individual tools are too hard to combine, too slow to invoke, and too different to use. We began an effort to unify the most commonly-required development helpers into a single tool, now called gopls (pronounced go, please). Gopls speaks the Language Server Protocol, LSP, and works with any integrated development environment or text editor with LSP support, which is essentially everything at this point. Gopls marks an expansion in focus for the Go project, from delivering standalone compiler-like, command-line tools like go vet or gorename to also delivering a complete IDE service. Rebecca Stambler gave a talk with more details about gopls and IDEs at Gophercon 2019. (Well add a link here when the video becomes available.) After gopls, we also have ideas for reviving go fix in an extensible way and for making go vet even more helpful. Coda So theres the path to Go 2. We will experiment and simplify. And experiment and simplify. And ship. And experiment and simplify. And do it all again. It may look or even feel like the path goes around in circles. But each time we experiment and simplify we learn a little more about what Go 2 should look like and move another step closer to it. Even abandoned experiments like try or our first four generics designs or dep are not wasted time. They help us learn what needs to be simplified before we can ship, and in some cases they help us better understand something we took for granted. At some point we will realize we have experimented enough, and simplified enough, and shipped enough, and we will have Go 2. Thanks to all of you in the Go community for helping us experiment and simplify and ship and find our way on this path. ",
        "_version_": 1718527417883033600
      },
      {
        "story_id": [21714735],
        "story_author": ["gilfillan9"],
        "story_descendants": [3],
        "story_score": [38],
        "story_time": ["2019-12-05T18:22:22Z"],
        "story_title": "Welcome to Space",
        "search": [
          "Welcome to Space",
          "https://blog.jetbrains.com/blog/2019/12/05/welcome-to-space/",
          "New Products NewsWelcome to Space! Today at KotlinConf, we announced our brand new product Space, and we have already opened the Early Access Program. What is Space Space is an integrated team environment that provides teams and organizations with the tools they need to collaborate effectively and efficiently. It has Git-based Version Control, Code Review, Automation (CI/CD) based on Kotlin Scripting, Package Repositories, Planning tools, Issue Tracker, Chats, Blogs, Meetings, and Team Directory, among other features. Space was born out of our own needs at JetBrains. As a company, weve grown from a team of 3 developers to over 1200 people, 60% of whom are technical. With this growth, weve found our current use of independent tools has often created silos, leading to miscommunication, less efficient collaboration, and loss of information. Space is about people and teams In Space, the concept of a team is a first-class citizen. When you join a team, you are automatically included in everything related to it, be it meetings, blogs, source control, calendars, vacations, etc. This eliminates the need for creating concepts such as groups and then making sure that every team member is also part of the corresponding group. Tight integration between the areas of the system provides for numerous advantages such as knowing a persons availability, which is useful if you want to have a chat or assign a code review to them. Space is a platform Space allows you to build on the platform in multiple ways. Whether you do so by using webhooks, HTTP API, or even plugins (on the self-hosted version), you can extend the existing functionality of Space and make use of the information available to you without needing to hook up many different solutions that create silos of data. Space as a service or self-hosted We will be offering Space either as a service, hosted and managed entirely by JetBrains, or as a self-hosted version (available in the near future). The pricing plan has four levels, starting at the free tier, which is ideal for small teams, and progressing up to the Enterprise tier, which meets the needs of large organizations. Space is available everywhere Built from the ground up with Kotlin multiplatform technology, Space clients are available for web, desktop, and mobile platforms, and offer full IDE integration with the IntelliJ Platform. Space Launch and roadmap Today we already provide a ton of functionality, such as: Version Control Code Reviews Blogs Chats Team Directory Package Registry Planning Issue Tracker IDE Integration We have a lot more planned for Space, including: Knowledge Base Automation CI/CD Pipelines Personal To-Do lists and notification management The Early Access Program will be accepting requests on a first-come-first-served basis, and well be gradually increasing the number of teams that can gain access. So dont wait! Sign up now to get your team Space! ",
          "This looks interesting, I’m curious how the “free” tier interacts with the “self hosted” part - storage limits and compute limits don’t make a lot of sense if you’re providing those resources yourself.",
          "Interesting, but it smells a lot of enterprise bloat. All-in-one solutions, in the long run, tend to feel inferior to dynamic setups.<p>It’s true that that particular market is long-suffering. If this thing can keep admin overhead to a minimum and actually make teams communicate more, it’s welcome."
        ],
        "story_type": ["Normal"],
        "url": "https://blog.jetbrains.com/blog/2019/12/05/welcome-to-space/",
        "url_text": "New Products NewsWelcome to Space! Today at KotlinConf, we announced our brand new product Space, and we have already opened the Early Access Program. What is Space Space is an integrated team environment that provides teams and organizations with the tools they need to collaborate effectively and efficiently. It has Git-based Version Control, Code Review, Automation (CI/CD) based on Kotlin Scripting, Package Repositories, Planning tools, Issue Tracker, Chats, Blogs, Meetings, and Team Directory, among other features. Space was born out of our own needs at JetBrains. As a company, weve grown from a team of 3 developers to over 1200 people, 60% of whom are technical. With this growth, weve found our current use of independent tools has often created silos, leading to miscommunication, less efficient collaboration, and loss of information. Space is about people and teams In Space, the concept of a team is a first-class citizen. When you join a team, you are automatically included in everything related to it, be it meetings, blogs, source control, calendars, vacations, etc. This eliminates the need for creating concepts such as groups and then making sure that every team member is also part of the corresponding group. Tight integration between the areas of the system provides for numerous advantages such as knowing a persons availability, which is useful if you want to have a chat or assign a code review to them. Space is a platform Space allows you to build on the platform in multiple ways. Whether you do so by using webhooks, HTTP API, or even plugins (on the self-hosted version), you can extend the existing functionality of Space and make use of the information available to you without needing to hook up many different solutions that create silos of data. Space as a service or self-hosted We will be offering Space either as a service, hosted and managed entirely by JetBrains, or as a self-hosted version (available in the near future). The pricing plan has four levels, starting at the free tier, which is ideal for small teams, and progressing up to the Enterprise tier, which meets the needs of large organizations. Space is available everywhere Built from the ground up with Kotlin multiplatform technology, Space clients are available for web, desktop, and mobile platforms, and offer full IDE integration with the IntelliJ Platform. Space Launch and roadmap Today we already provide a ton of functionality, such as: Version Control Code Reviews Blogs Chats Team Directory Package Registry Planning Issue Tracker IDE Integration We have a lot more planned for Space, including: Knowledge Base Automation CI/CD Pipelines Personal To-Do lists and notification management The Early Access Program will be accepting requests on a first-come-first-served basis, and well be gradually increasing the number of teams that can gain access. So dont wait! Sign up now to get your team Space! ",
        "comments.comment_id": [21714981, 21716371],
        "comments.comment_author": ["stephenr", "toyg"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-12-05T18:44:21Z",
          "2019-12-05T20:54:23Z"
        ],
        "comments.comment_text": [
          "This looks interesting, I’m curious how the “free” tier interacts with the “self hosted” part - storage limits and compute limits don’t make a lot of sense if you’re providing those resources yourself.",
          "Interesting, but it smells a lot of enterprise bloat. All-in-one solutions, in the long run, tend to feel inferior to dynamic setups.<p>It’s true that that particular market is long-suffering. If this thing can keep admin overhead to a minimum and actually make teams communicate more, it’s welcome."
        ],
        "id": "8f57d2d9-6b9e-48a7-9084-877b639ea730",
        "_version_": 1718527438898593792
      },
      {
        "story_id": [20087168],
        "story_author": ["fphilipe"],
        "story_descendants": [379],
        "story_score": [765],
        "story_time": ["2019-06-03T19:20:28Z"],
        "story_title": "SwiftUI",
        "search": [
          "SwiftUI",
          "https://developer.apple.com/xcode/swiftui/",
          "SwiftUI helps you build great-looking apps across all Apple platforms with the power of Swift and as little code as possible. With SwiftUI, you can bring even better experiences to all users, on any Apple device, using just one set of tools andAPIs. Whats new in SwiftUI Advanced app experiences and tools Enhance your apps with new features, such as improved list views, better search experiences, and support for control focus areas. And gain more control over lower-level drawing primitives with the new CanvasAPI, a modern, GPU-accelerated equivalent of drawRect. Accessibility improvements Speed up interactions by exposing the most relevant items on a screen in a simple list using the new RotorAPI. The current accessibility focus state, such as the VoiceOver cursor, can now be read and even changed programmatically. And with the new Accessibility Representation API, your custom controls easily inherit full accessibility support from existing standard SwiftUI controls. SwiftUI improvements onmacOS New performance and API availability improvements, including support for multicolumn tables, make your macOS apps even better. Always-On Retina Display support On Apple Watch Series 5 and later, the Always-On Retina Display allows watchOS apps to stay visible, even when the watch face is dimmed, making key information available ataglance. Widgets for iPadOS Now widgets can be placed anywhere on the Home screen and increased to a new, extra-large widgetsize. Declarative syntax SwiftUI uses a declarative syntax, so you can simply state what your user interface should do. For example, you can write that you want a list of items consisting of text fields, then describe alignment, font, and color for each field. Your code is simpler and easier to read than ever before, saving you time and maintenance. This declarative style even applies to complex concepts like animation. Easily add animation to almost any control and choose a collection of ready-to-use effects with only a few lines of code. At runtime, the system handles all of the steps needed to create a smooth movement, and even deals with interruption to keep your app stable. With animation this easy, youll be looking for new ways to make your app come alive. Design tools Xcode includes intuitive design tools that make building interfaces with SwiftUI as easy as dragging and dropping. As you work in the design canvas, everything you edit is completely in sync with the code in the adjoining editor. Code is instantly visible as a preview as you type, and any change you make to that preview immediately appears in your code. Xcode recompiles your changes instantly and inserts them into a running version of your app visible, and editable at alltimes. Drag and drop. Arrange components within your user interface by simply dragging controls on the canvas. Click to open an inspector to select font, color, alignment, and other design options, and easily rearrange controls with your cursor. Many of these visual editors are also available within the code editor, so you can use inspectors to discover new modifiers for each control, even if you prefer hand-coding parts of your interface. You can also drag controls from your library and drop them on the design canvas or directly on the code. Dynamic replacement. The Swift compiler and runtime are fully embedded throughout Xcode, so your app is constantly being built and run. The design canvas you see isnt just an approximation of your user interface its your live app. And Xcode can swap edited code directly in your live app with dynamic replacement, a new feature in Swift. Previews. You can now create one or many previews of any SwiftUI views to get sample data, and configure almost anything your users might see, such as large fonts, localizations, or DarkMode. Previews can also display your UI in any device and any orientation. Get started Download Xcode and use these resources to build apps with SwiftUI for all Appleplatforms. Download Xcode ",
          "I’m one of the engineers that spearheaded this initiative inside of Apple. I just wanted to thank the HN community—I’ve been reading HN for 10 years now and it’s been formative in my development as a software engineer.<p>If you’re at WWDC stop by the labs and say hi!",
          "Lots of engineers are suggesting that SwiftUI, plus other declarative frameworks, might be \"the future\" of app development. However, I can't help but feel that this paradigm would work best when your app is a fairly basic CRUD thing. If you're working with highly interactive interfaces, complex animations, or dense, layered documents (DAWs, video editors), it seems that you would <i>need</i> explicit state and imperative code at the center of it all, and that a declarative approach would require hacks and workarounds at every turn. In my humble opinion, some of the most interesting, ground-breaking, and creative software has these properties; whereas this reactive stuff seems tailored to bog-standard utility software.<p>However: I've never used React or any of its derivatives, so this is mostly inference. Is this take accurate or not? In theory, could you scale SwiftUI to build something like Logic, Blender, or Photoshop (for example)?<p>Also, have any declarative UI frameworks been released that feature a \"platform\" layer, where you get to define how your declarative code actually turns into UI, widgets, and behaviors? It seems that SwiftUI relies on (encoded) assumptions of what an ideal UIKit or AppKit app is supposed to look and feel like, and it would be really powerful if we could mess with this foundation or even swap it out entirely.<p>(It's very possible I'm mixing up reactive/declarative/reactive-UI concepts since I'm not too familiar with the territory.)"
        ],
        "story_type": ["Normal"],
        "url": "https://developer.apple.com/xcode/swiftui/",
        "url_text": "SwiftUI helps you build great-looking apps across all Apple platforms with the power of Swift and as little code as possible. With SwiftUI, you can bring even better experiences to all users, on any Apple device, using just one set of tools andAPIs. Whats new in SwiftUI Advanced app experiences and tools Enhance your apps with new features, such as improved list views, better search experiences, and support for control focus areas. And gain more control over lower-level drawing primitives with the new CanvasAPI, a modern, GPU-accelerated equivalent of drawRect. Accessibility improvements Speed up interactions by exposing the most relevant items on a screen in a simple list using the new RotorAPI. The current accessibility focus state, such as the VoiceOver cursor, can now be read and even changed programmatically. And with the new Accessibility Representation API, your custom controls easily inherit full accessibility support from existing standard SwiftUI controls. SwiftUI improvements onmacOS New performance and API availability improvements, including support for multicolumn tables, make your macOS apps even better. Always-On Retina Display support On Apple Watch Series 5 and later, the Always-On Retina Display allows watchOS apps to stay visible, even when the watch face is dimmed, making key information available ataglance. Widgets for iPadOS Now widgets can be placed anywhere on the Home screen and increased to a new, extra-large widgetsize. Declarative syntax SwiftUI uses a declarative syntax, so you can simply state what your user interface should do. For example, you can write that you want a list of items consisting of text fields, then describe alignment, font, and color for each field. Your code is simpler and easier to read than ever before, saving you time and maintenance. This declarative style even applies to complex concepts like animation. Easily add animation to almost any control and choose a collection of ready-to-use effects with only a few lines of code. At runtime, the system handles all of the steps needed to create a smooth movement, and even deals with interruption to keep your app stable. With animation this easy, youll be looking for new ways to make your app come alive. Design tools Xcode includes intuitive design tools that make building interfaces with SwiftUI as easy as dragging and dropping. As you work in the design canvas, everything you edit is completely in sync with the code in the adjoining editor. Code is instantly visible as a preview as you type, and any change you make to that preview immediately appears in your code. Xcode recompiles your changes instantly and inserts them into a running version of your app visible, and editable at alltimes. Drag and drop. Arrange components within your user interface by simply dragging controls on the canvas. Click to open an inspector to select font, color, alignment, and other design options, and easily rearrange controls with your cursor. Many of these visual editors are also available within the code editor, so you can use inspectors to discover new modifiers for each control, even if you prefer hand-coding parts of your interface. You can also drag controls from your library and drop them on the design canvas or directly on the code. Dynamic replacement. The Swift compiler and runtime are fully embedded throughout Xcode, so your app is constantly being built and run. The design canvas you see isnt just an approximation of your user interface its your live app. And Xcode can swap edited code directly in your live app with dynamic replacement, a new feature in Swift. Previews. You can now create one or many previews of any SwiftUI views to get sample data, and configure almost anything your users might see, such as large fonts, localizations, or DarkMode. Previews can also display your UI in any device and any orientation. Get started Download Xcode and use these resources to build apps with SwiftUI for all Appleplatforms. Download Xcode ",
        "comments.comment_id": [20087286, 20090260],
        "comments.comment_author": ["kylemacomber", "archagon"],
        "comments.comment_descendants": [27, 10],
        "comments.comment_time": [
          "2019-06-03T19:27:39Z",
          "2019-06-03T23:47:39Z"
        ],
        "comments.comment_text": [
          "I’m one of the engineers that spearheaded this initiative inside of Apple. I just wanted to thank the HN community—I’ve been reading HN for 10 years now and it’s been formative in my development as a software engineer.<p>If you’re at WWDC stop by the labs and say hi!",
          "Lots of engineers are suggesting that SwiftUI, plus other declarative frameworks, might be \"the future\" of app development. However, I can't help but feel that this paradigm would work best when your app is a fairly basic CRUD thing. If you're working with highly interactive interfaces, complex animations, or dense, layered documents (DAWs, video editors), it seems that you would <i>need</i> explicit state and imperative code at the center of it all, and that a declarative approach would require hacks and workarounds at every turn. In my humble opinion, some of the most interesting, ground-breaking, and creative software has these properties; whereas this reactive stuff seems tailored to bog-standard utility software.<p>However: I've never used React or any of its derivatives, so this is mostly inference. Is this take accurate or not? In theory, could you scale SwiftUI to build something like Logic, Blender, or Photoshop (for example)?<p>Also, have any declarative UI frameworks been released that feature a \"platform\" layer, where you get to define how your declarative code actually turns into UI, widgets, and behaviors? It seems that SwiftUI relies on (encoded) assumptions of what an ideal UIKit or AppKit app is supposed to look and feel like, and it would be really powerful if we could mess with this foundation or even swap it out entirely.<p>(It's very possible I'm mixing up reactive/declarative/reactive-UI concepts since I'm not too familiar with the territory.)"
        ],
        "id": "44bb7291-a782-48a6-b902-55f712acf9d8",
        "_version_": 1718527407303950336
      },
      {
        "story_id": [21112394],
        "story_author": ["jlward4th"],
        "story_descendants": [4],
        "story_score": [57],
        "story_time": ["2019-09-30T06:57:20Z"],
        "story_title": "Observations on Observability",
        "search": [
          "Observations on Observability",
          "https://blog.colinbreck.com/observations-on-observability/",
          "02 Jun 2019 This article expands on one section of my talk What Lies Between: The Challenges of Operationalising Microservices from QCon London 2019. Over the past few years, observability has become a prominent topic in distributed computing. Observability means different things to different people and the use of the term is still evolving, but, essentially, observability refers to the practice of using instrumentation to understand software systems in order to derive superior operational outcomes, like a superior customer experience, reduced operational costs, greater reliability, or product improvement. I do not like the term observability as it is currently used in the software industry. This article is my attempt to reflect on the emerging observability practices and explain why I do not like the term observability being applied to them. I was somewhat hesitant to publish this essay, as I think some people will find it pedanticlike I am splitting hairs. If that is the case, feel free to stop reading and these words can remain my personal indulgence. In this essay, I do not reference prominent tools, writing, or presentations on the subject of observability. This is intentional. I think much of the work in this space is well intentioned and serves a purpose. Based on where I think the software industry is heading, however, it is my opinion that the way we are using the term observability now will be different from how we will want to use it in the future. Observability: The Current Narrative From my perspective, the current observability narrative goes something like this. Observability has its origins in control theory, where observability describes whether the internal state-variables of the system can be externally measured or estimated. In a similar manner, we can gain insight into software systems through direct or indirect measurements. Most software systems achieve observability by instrumenting and recording requests and responses, either through logging, tracing, or metrics. At scale, instrumenting every request becomes prohibitive in terms of both performance and cost, so sampling is used to reduce the impact, while still providing statistically valuable insights, and/or the data are only retained for a short period of time. Observability is similar to software security, performance, and quality in that it must be considered holistically and not just tacked on after the fact. This leads to a strong argument for developers operating the systems that they buildrather than turning them over to a separate operations teamso that there is a feedback loop in terms of what needs to be observable in order to operate the system effectively. Most observability data is augmented with rich metadata that includes things like the unique request-identifier, the customer, the software version, the service name, the datacenter, and so on. This rich metadata allows us to ask arbitrary questions about the system, since we can slice and dice the data after the fact and we do not need to know all of the questions that we want to ask, upfront. This can provide tremendous operational flexibility, but it can require sophisticated analysis tools, and involves a lot more than just producing and storing the raw data. Why Do I Dislike the Term? Based on this narrative, observability sounds great. So why do I dislike the term? In control theory, the dual of observability is controllability. Controllability is the question of whether an input can be found such that the system states can be steered from an initial condition to a final value in a finite time-interval. Rather than asking arbitrary questions, observability, in relation to controllability, asks very specific questions. In a state-space representation of the system, observability is determined by the rank of the observability matrixa strict, mathematical definition. It seems to me that people are borrowing the word observability from the well-established discipline of control theory to give what we are doing in computing more legitimacy, just by association. In software systems, what is currently referred to as observabilityusually some combination of logging, tracing, and metricsprovides varying levels of visibility, debuggability, traceability, discoverability, but not observability, especially as it relates to controllability. Perhaps we can call what we are currently doing visibility? Or if we want to emphasize the importance of inference through indirect measurements, maybe we can call it discernibility?[1] Current observability tools have a very narrow scope, mainly limited to request-response systems. We need tools that move beyond the focus on requests and responses. While HTTP request-response forms the backbone of many services, the majority of the systems that I have worked on in my career have been time-series systems, which have streaming and publish-subscribe messaging as the dominant communication patterns. We need tools that can provide insight into the dynamics of streaming-data and event-based systems, like systems designed for interfacing with millions of IoT devices. We also need insight into events at rest, in persistent storage, so that we can verify the integrity of the data when we read it back,[2] or identify specific customer-data so that it can be deleted to satisfy regulatory requirements. Pigging Pigging is a technique used in oil and gas pipelines. A pig is a large, cylindrical device that is the same diameter as the pipe. The pig is inserted into the pipe and moves along the pipeline with the flow of product. Pigs support maintenance operations, including cleaning, inspecting, and preventing leaks, without necessarily stopping the flow of product in the pipeline. The current focus in software observability on request-response messaging and tracing how a single request is satisfied by many collaborating systems, is our equivalent of pigging. Logging, tracing, and metrics can be invaluable tools for understanding the genealogy of a single requestfor example, someone in customer service might be able to identify why a customer's specific transaction failed or is delayed; or an aggregation of anomalous requests to a single service may help identify that a service has failedbut trying to make sense of the dynamics of a complex system, in aggregate, at scale, by sorting through a huge number pigs, is very limiting. Pigs are not used to operate oil or gas pipelines on an ongoing basis. Continuous operation relies on continuous signals: flow rates, pressures, temperatures, mass balances. Similarly, an oil refinery, which is arguably much more complex than most software systems, is not operated by observing how every single hydrocarbon flows through the system. Empirical methods that rely on continuous signals are used for control and continuous process-improvement. For a distillation column, the external dimensions of the towerthe computing equivalent of tagging events with static metadatado not even tell us much. We need to understand the desired split of components, the operating pressure, the location of the feed tray. In other words, we need to understand the dynamics, operating conditions, and control objectives for the entire system. At scale, digital becomes analog. Michael Feathers Fortunately, as discrete pieces aggregate, they start to look continuous. At scale, we can take advantage of this, embracing higher-level metrics that can be treated as continuous signals. Where I Think We Are Heading When I look at the dynamics of software systems, they remind me of the mechanical, chemical, and electrical systems studied in engineering. Processing a stream of messages from a message queue is similar to a flow rate of product. Understanding if messages are accumulating in the queue is similar to a mass balance.[3] We might control a process with a bounded queue of messages in a similar manner to how one would control the level in a holding tank, manipulating the input and output flow rates, tuning the controller to track the setpoint, or reject disturbances. I believe we will eventually view software systems as dynamical systems and we will embrace models that rely on essentially continuous signals in order to control these systems. Operating software systems at scale will look like process engineering and systems engineering. This transition to taking a more systems-engineering approach and using the right semantics will help us operate our systems more efficiently and reliably. We will identify stochastic processes and we will be more careful in understanding if the measurements that we are using are independent, or if they are auto-correlated, or cross-correlated. We will study the systems and controllers we build to understand their dynamic response and stability. We will develop more rigour in understanding what can be observed, what can be modelled, and what can be controlled.[4] There has been some mention of closed-loop control in software systems in recent years.[5] Certainly desired-state configuration, managed by systems like the Kubernetes Control Plane, is a loose form of proportional and integral control. However, we will need to borrow more sophisticated models from control theory to deal with the complex dynamics of software systems and ensure they obey operating objectives. Controllers can be very sensitive to measurement lag. Perhaps first-order plus dead-time models will prove useful. To me, scaling up worker nodes in a cluster looks like a first-order plus dead-time process: there is a dead-time as the servers are provisioned, then the response is first-order as the workers handle more and more load. Or maybe we will use techniques like Model Predictive Control (MPC) to perform optimal control, based on past measurements, while respecting constraints. Our constraints might be things like response time, the number of servers, queue size, energy consumption, and cost. The state-space that software systems operate in is non-linear and there are local and global maxima in terms of the most effective operating conditions. We will need to use techniques for continuous improvement that have been used in the process industries and in manufacturing for decades. Techniques like Design of Experiments, including Factorial Designs; or Fractional Factorial Designs for large state spaces. We will use these experiments to find more optimal operating conditions, maximizing an objective like message throughput, or minimizing latency or energy consumption, while adjusting the number of threads, CPU allocation, server instance-type, or container affinity. Just like in the process industries, this testing will happen in production and it will be continuous. Telemetry, Correlation, Cardinality I find log messages and traces as useful as the next person, but the most valuable thing to me in operating software systems and understanding their dynamics is having fine-grained telemetry from each service. I think of instrumenting a software system as one would instrument an industrial process, thinking in terms of flow rates, holding times, mass balances, counts, and so on, and whether or not I can observe the system dynamics that I want to study. For example, I might want to understand the overall message rate for a service, as well as breakdowns for the different types of messages it processes, or the number of discreet error conditions that have been encountered. This type of telemetry can be reported using very light-weight counters, then computed and aggregated after the fact.[6] This type of telemetry has a lot of advantages in terms of performance, scalability, testability, as well as composability.[7] The data can also be retained cost-effectively for a long period of time. In addition to providing great operational insight, I think this type of telemetry will eventually become the basis for controlling the dynamics of software systems in more sophisticated ways. In developing software observability, many people recommend measuring only what you need, not necessarily what you can. For example, why measure every step in a message-processing pipeline, when the vast majority of messages are transmitted through the entire pipeline? Avoiding redundant measurementseither direct or indirectis a way to tame the huge volumes of data and focus our attention on what is most valuable. This makes sense for redundant logs statements or traces, but it is not good advice for telemetry, for a number of reasons. We usually do not know all of the questions that we want to ask upfront. How many times have you been troubleshooting a problem with the data that are available, only to realize that you need more detailed telemetry or logging in order to make sense of what is going on? This is why I am of the opinion that we should record as much data as reasonably possible, upfront, because it allows the greatest flexibility in asking questions of the data later. This is one of the reasons that I like using fine-grained telemetry, because it is cheap to produce, collect, store, and analyze. It is cost effective to collect a large amount of telemetry from every service at a high frequency. Tagging this kind of telemetry with rich metatdata is also more stable in terms of cardinality. For example, the aggregate message-rate for a streaming-service tagged with metadata like the service name, the software version, the datacenter, and so on, will produce data with much lower cardinality as compared to recording every single request with a unique identifier. In addition, collecting what might look like redundant signals can be advantageous when the correlation among these signals starts to break down, allowing us to identify failures or changes in behaviour. Drawing correlations, or the lack there of, among series, is often what we are doing when looking at dashboards. A colleague of mine has developed a mental model where he can infer most dynamics and operating statesnormal and abnormalof a messaging system, just by looking at the relative network-out for each sever in the cluster. We can formalize this using multivariate approaches, like those that have been adopted in the process industries, in a similar shift from operating systems based on our intuition, to using more formal methods for monitoring and control. Multivariate methods may even derive insight from signals that we disregard as unimportant. Consider the following set of measurements in a multi-dimensional space. The majority of the variation can be described by a lower dimensional, latent-variable space, that could be described by using a multivariate, statistical technique like Principal Component Analysis (PCA). Not only would this give more insight into highly correlated signals, it could also help identify new, unique operating conditions, or predict the onset of failures, before they become critical, as the correlation among these variables starts to change over time. Using statistical-process-control charts will allow us to judge whether services are meeting service-level objectives more effectively than looking at dashboards and using our intuition. Conclusion Many organizations are developing observability tools. There is also a feeding frenzy of vendors trying to corner the market on observability toolsevery vendor has their own observability guideand it seems every conference presentation must mention observability. Most of these efforts are well intended, but the tools, while improving, remain nascent, narrowly focused, and somewhat naive. I believe there must be people experimenting with more sophisticated techniques, but these are not the people that are getting the attention of the observability space dominated by logging, metrics, and tracing. At a small scale, like when we are tracing a single request, or interpreting logs from a single container, these tools are necessarily powerful. Understanding the rich detail gives us a sense of control. But as we start to zoom out, at scale, trying to understand all of the detail is impossibledetails do not compose. Our understanding of the system and our sense of control erodes. The more details we can see, the less we have a sense of control. Mark Burgess I think the future of operating software systems at scale will look like process engineering. We will rely on continuous signals and look at software systems as dynamical systems. We will embrace similar techniques for process control and continuous improvement. This is why I do not like the term observability as it is currently used. In operating software systems at scale, we may want to reserve the traditional definition of observability as it relates to controllability. Interestingly, the term observability has also been used in Cognitive Systems Engineering in relation to helping people discern the behaviours of autonomous control systems: \"Increasing autonomy and authority of machine agents without an increase in observability create automation surprises.\" See Observations on Errors, Corrections, & Trust of Dependent Systems by James Hamilton. Certainly queuing theory is a well-developed field in computing, but these types of models do not seem to be prominent in most observability tools. My calculus professor in my first year of university, Dr. Boyd, always joked that he \"like to avoid the rigorous method, because with rigour comes mortis.\" Colm MacCrthaigh gave a very interesting presentation entitled Close Loops & Opening Minds: How to Take Control of Systems, Big & Small where he hints at the possibilities of using proportional-integral-derivative (PID) controllers for software systems. He doesn't go into too much detail and most of what he does go on to present is relatively simpletechniques like rebooting, throttling, full-synchronization, and immutability. There is, however, beauty in this simplicity. It should be noted that I am not suggesting that we build systems or controllers that are more complex than they need to be. Many physical systems, or systems in the process industries, have analogous, simple, fail-safe mechanisms, that just work. For example, there is no need to record ratesjust record counts and differentiate after the fact. Instrumenting software with counters like this is not new. This is how Windows Performance Counters work and they have been around for decades. I wrote an article on testing performance counters: Writing Automated Tests for Custom Windows Performance Counters. ",
          "> At scale, digital becomes analog. — Michael Feathers<p>> Fortunately, as discrete pieces aggregate, they start to look continuous. At scale, we can take advantage of this, embracing higher-level metrics that can be treated as continuous signals.<p>He's saying that rather than look at request-level metrics in your Big Data Message Queue (request ID, etc.), your message queue <i>in aggregate</i> can be seen as an industrial process, and there's an entire discipline built around managing those called Statistical Process Control.<p>Seen in this way, the next NewRelic is going to be all about creating p- and c-charts [1] where the limits algorithmically turn knobs on your kubernetes cluster.<p>In other words, the next big dev-ops trend is going to be the return of Six Sigma.<p>[1] <a href=\"https://en.wikipedia.org/wiki/Control_chart\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Control_chart</a>",
          "What I wish I had is a book that introduces control engineering in some depth for software folks like me.<p>I wanted to write such a book, or half of such a book, as a second part of <i>Knative in Action</i>. I wrote a simulator[0] and did a ton of reading which felt like I'd barely scratched the surface of the surface.<p>But really, there's just no room for what I wanted to do. Based on a simple estimate it would have run to about 200 pages on top of another 250 pages of actually-about-Knative material. So that whole section wound on the cutting room floor. Maybe another time.<p>In terms of books I felt helped me the <i>most</i> down this road, here's a reasonable reading list I'd point to:<p><i>Feedback Control for Computer Systems: Introducing Control Theory to Enterprise Programmers</i>, by Philipp Janert. Short, to-the-point introduction to basic classical control theory using PID controllers.<p><i>Matching Supply with Demand: An Introduction to Operations Management</i> by Cachon and Terweisch. A short, meant-for-MBAs textbook on operations management (ie applied operations research). Simple and approachable. Very good for warming up to...<p><i>Factory Physics</i> by Hopp and Spearman. Magisterial in scope, also magisterial in grumpiness and idiosyncracy. I've directly applied material from this book and <i>Matching</i> to research work.<p><i>Business Dynamics</i> by Sterman. Still the best system dynamics book I've ever read. I'm anxiously waiting for the 2nd edition, expected circa 2021.<p><i>Performance Modeling and Design of Computer Systems: Queueing Theory in Action</i> by Harchol-Balter. Approachable and enlightening. I stumbled on a few of the proofs and got lost once or twice in thickets of notation, but that's due to my own mathematical immaturity more than the book itself.<p>But I still don't have <i>a</i> book to refer to for the specifics of software dynamics, spanning its physics and economics. And I wish I did. I would love to read that book, if folks can make recommendations.<p>[0] <a href=\"https://github.com/pivotal/skenario\" rel=\"nofollow\">https://github.com/pivotal/skenario</a>"
        ],
        "story_type": ["Normal"],
        "url": "https://blog.colinbreck.com/observations-on-observability/",
        "comments.comment_id": [21132327, 21132666],
        "comments.comment_author": ["floatrock", "jacques_chester"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-10-02T00:24:28Z",
          "2019-10-02T01:25:18Z"
        ],
        "comments.comment_text": [
          "> At scale, digital becomes analog. — Michael Feathers<p>> Fortunately, as discrete pieces aggregate, they start to look continuous. At scale, we can take advantage of this, embracing higher-level metrics that can be treated as continuous signals.<p>He's saying that rather than look at request-level metrics in your Big Data Message Queue (request ID, etc.), your message queue <i>in aggregate</i> can be seen as an industrial process, and there's an entire discipline built around managing those called Statistical Process Control.<p>Seen in this way, the next NewRelic is going to be all about creating p- and c-charts [1] where the limits algorithmically turn knobs on your kubernetes cluster.<p>In other words, the next big dev-ops trend is going to be the return of Six Sigma.<p>[1] <a href=\"https://en.wikipedia.org/wiki/Control_chart\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Control_chart</a>",
          "What I wish I had is a book that introduces control engineering in some depth for software folks like me.<p>I wanted to write such a book, or half of such a book, as a second part of <i>Knative in Action</i>. I wrote a simulator[0] and did a ton of reading which felt like I'd barely scratched the surface of the surface.<p>But really, there's just no room for what I wanted to do. Based on a simple estimate it would have run to about 200 pages on top of another 250 pages of actually-about-Knative material. So that whole section wound on the cutting room floor. Maybe another time.<p>In terms of books I felt helped me the <i>most</i> down this road, here's a reasonable reading list I'd point to:<p><i>Feedback Control for Computer Systems: Introducing Control Theory to Enterprise Programmers</i>, by Philipp Janert. Short, to-the-point introduction to basic classical control theory using PID controllers.<p><i>Matching Supply with Demand: An Introduction to Operations Management</i> by Cachon and Terweisch. A short, meant-for-MBAs textbook on operations management (ie applied operations research). Simple and approachable. Very good for warming up to...<p><i>Factory Physics</i> by Hopp and Spearman. Magisterial in scope, also magisterial in grumpiness and idiosyncracy. I've directly applied material from this book and <i>Matching</i> to research work.<p><i>Business Dynamics</i> by Sterman. Still the best system dynamics book I've ever read. I'm anxiously waiting for the 2nd edition, expected circa 2021.<p><i>Performance Modeling and Design of Computer Systems: Queueing Theory in Action</i> by Harchol-Balter. Approachable and enlightening. I stumbled on a few of the proofs and got lost once or twice in thickets of notation, but that's due to my own mathematical immaturity more than the book itself.<p>But I still don't have <i>a</i> book to refer to for the specifics of software dynamics, spanning its physics and economics. And I wish I did. I would love to read that book, if folks can make recommendations.<p>[0] <a href=\"https://github.com/pivotal/skenario\" rel=\"nofollow\">https://github.com/pivotal/skenario</a>"
        ],
        "id": "4db1d965-3ac7-4d7e-bcdf-7f0c77accf02",
        "url_text": "02 Jun 2019 This article expands on one section of my talk What Lies Between: The Challenges of Operationalising Microservices from QCon London 2019. Over the past few years, observability has become a prominent topic in distributed computing. Observability means different things to different people and the use of the term is still evolving, but, essentially, observability refers to the practice of using instrumentation to understand software systems in order to derive superior operational outcomes, like a superior customer experience, reduced operational costs, greater reliability, or product improvement. I do not like the term observability as it is currently used in the software industry. This article is my attempt to reflect on the emerging observability practices and explain why I do not like the term observability being applied to them. I was somewhat hesitant to publish this essay, as I think some people will find it pedanticlike I am splitting hairs. If that is the case, feel free to stop reading and these words can remain my personal indulgence. In this essay, I do not reference prominent tools, writing, or presentations on the subject of observability. This is intentional. I think much of the work in this space is well intentioned and serves a purpose. Based on where I think the software industry is heading, however, it is my opinion that the way we are using the term observability now will be different from how we will want to use it in the future. Observability: The Current Narrative From my perspective, the current observability narrative goes something like this. Observability has its origins in control theory, where observability describes whether the internal state-variables of the system can be externally measured or estimated. In a similar manner, we can gain insight into software systems through direct or indirect measurements. Most software systems achieve observability by instrumenting and recording requests and responses, either through logging, tracing, or metrics. At scale, instrumenting every request becomes prohibitive in terms of both performance and cost, so sampling is used to reduce the impact, while still providing statistically valuable insights, and/or the data are only retained for a short period of time. Observability is similar to software security, performance, and quality in that it must be considered holistically and not just tacked on after the fact. This leads to a strong argument for developers operating the systems that they buildrather than turning them over to a separate operations teamso that there is a feedback loop in terms of what needs to be observable in order to operate the system effectively. Most observability data is augmented with rich metadata that includes things like the unique request-identifier, the customer, the software version, the service name, the datacenter, and so on. This rich metadata allows us to ask arbitrary questions about the system, since we can slice and dice the data after the fact and we do not need to know all of the questions that we want to ask, upfront. This can provide tremendous operational flexibility, but it can require sophisticated analysis tools, and involves a lot more than just producing and storing the raw data. Why Do I Dislike the Term? Based on this narrative, observability sounds great. So why do I dislike the term? In control theory, the dual of observability is controllability. Controllability is the question of whether an input can be found such that the system states can be steered from an initial condition to a final value in a finite time-interval. Rather than asking arbitrary questions, observability, in relation to controllability, asks very specific questions. In a state-space representation of the system, observability is determined by the rank of the observability matrixa strict, mathematical definition. It seems to me that people are borrowing the word observability from the well-established discipline of control theory to give what we are doing in computing more legitimacy, just by association. In software systems, what is currently referred to as observabilityusually some combination of logging, tracing, and metricsprovides varying levels of visibility, debuggability, traceability, discoverability, but not observability, especially as it relates to controllability. Perhaps we can call what we are currently doing visibility? Or if we want to emphasize the importance of inference through indirect measurements, maybe we can call it discernibility?[1] Current observability tools have a very narrow scope, mainly limited to request-response systems. We need tools that move beyond the focus on requests and responses. While HTTP request-response forms the backbone of many services, the majority of the systems that I have worked on in my career have been time-series systems, which have streaming and publish-subscribe messaging as the dominant communication patterns. We need tools that can provide insight into the dynamics of streaming-data and event-based systems, like systems designed for interfacing with millions of IoT devices. We also need insight into events at rest, in persistent storage, so that we can verify the integrity of the data when we read it back,[2] or identify specific customer-data so that it can be deleted to satisfy regulatory requirements. Pigging Pigging is a technique used in oil and gas pipelines. A pig is a large, cylindrical device that is the same diameter as the pipe. The pig is inserted into the pipe and moves along the pipeline with the flow of product. Pigs support maintenance operations, including cleaning, inspecting, and preventing leaks, without necessarily stopping the flow of product in the pipeline. The current focus in software observability on request-response messaging and tracing how a single request is satisfied by many collaborating systems, is our equivalent of pigging. Logging, tracing, and metrics can be invaluable tools for understanding the genealogy of a single requestfor example, someone in customer service might be able to identify why a customer's specific transaction failed or is delayed; or an aggregation of anomalous requests to a single service may help identify that a service has failedbut trying to make sense of the dynamics of a complex system, in aggregate, at scale, by sorting through a huge number pigs, is very limiting. Pigs are not used to operate oil or gas pipelines on an ongoing basis. Continuous operation relies on continuous signals: flow rates, pressures, temperatures, mass balances. Similarly, an oil refinery, which is arguably much more complex than most software systems, is not operated by observing how every single hydrocarbon flows through the system. Empirical methods that rely on continuous signals are used for control and continuous process-improvement. For a distillation column, the external dimensions of the towerthe computing equivalent of tagging events with static metadatado not even tell us much. We need to understand the desired split of components, the operating pressure, the location of the feed tray. In other words, we need to understand the dynamics, operating conditions, and control objectives for the entire system. At scale, digital becomes analog. Michael Feathers Fortunately, as discrete pieces aggregate, they start to look continuous. At scale, we can take advantage of this, embracing higher-level metrics that can be treated as continuous signals. Where I Think We Are Heading When I look at the dynamics of software systems, they remind me of the mechanical, chemical, and electrical systems studied in engineering. Processing a stream of messages from a message queue is similar to a flow rate of product. Understanding if messages are accumulating in the queue is similar to a mass balance.[3] We might control a process with a bounded queue of messages in a similar manner to how one would control the level in a holding tank, manipulating the input and output flow rates, tuning the controller to track the setpoint, or reject disturbances. I believe we will eventually view software systems as dynamical systems and we will embrace models that rely on essentially continuous signals in order to control these systems. Operating software systems at scale will look like process engineering and systems engineering. This transition to taking a more systems-engineering approach and using the right semantics will help us operate our systems more efficiently and reliably. We will identify stochastic processes and we will be more careful in understanding if the measurements that we are using are independent, or if they are auto-correlated, or cross-correlated. We will study the systems and controllers we build to understand their dynamic response and stability. We will develop more rigour in understanding what can be observed, what can be modelled, and what can be controlled.[4] There has been some mention of closed-loop control in software systems in recent years.[5] Certainly desired-state configuration, managed by systems like the Kubernetes Control Plane, is a loose form of proportional and integral control. However, we will need to borrow more sophisticated models from control theory to deal with the complex dynamics of software systems and ensure they obey operating objectives. Controllers can be very sensitive to measurement lag. Perhaps first-order plus dead-time models will prove useful. To me, scaling up worker nodes in a cluster looks like a first-order plus dead-time process: there is a dead-time as the servers are provisioned, then the response is first-order as the workers handle more and more load. Or maybe we will use techniques like Model Predictive Control (MPC) to perform optimal control, based on past measurements, while respecting constraints. Our constraints might be things like response time, the number of servers, queue size, energy consumption, and cost. The state-space that software systems operate in is non-linear and there are local and global maxima in terms of the most effective operating conditions. We will need to use techniques for continuous improvement that have been used in the process industries and in manufacturing for decades. Techniques like Design of Experiments, including Factorial Designs; or Fractional Factorial Designs for large state spaces. We will use these experiments to find more optimal operating conditions, maximizing an objective like message throughput, or minimizing latency or energy consumption, while adjusting the number of threads, CPU allocation, server instance-type, or container affinity. Just like in the process industries, this testing will happen in production and it will be continuous. Telemetry, Correlation, Cardinality I find log messages and traces as useful as the next person, but the most valuable thing to me in operating software systems and understanding their dynamics is having fine-grained telemetry from each service. I think of instrumenting a software system as one would instrument an industrial process, thinking in terms of flow rates, holding times, mass balances, counts, and so on, and whether or not I can observe the system dynamics that I want to study. For example, I might want to understand the overall message rate for a service, as well as breakdowns for the different types of messages it processes, or the number of discreet error conditions that have been encountered. This type of telemetry can be reported using very light-weight counters, then computed and aggregated after the fact.[6] This type of telemetry has a lot of advantages in terms of performance, scalability, testability, as well as composability.[7] The data can also be retained cost-effectively for a long period of time. In addition to providing great operational insight, I think this type of telemetry will eventually become the basis for controlling the dynamics of software systems in more sophisticated ways. In developing software observability, many people recommend measuring only what you need, not necessarily what you can. For example, why measure every step in a message-processing pipeline, when the vast majority of messages are transmitted through the entire pipeline? Avoiding redundant measurementseither direct or indirectis a way to tame the huge volumes of data and focus our attention on what is most valuable. This makes sense for redundant logs statements or traces, but it is not good advice for telemetry, for a number of reasons. We usually do not know all of the questions that we want to ask upfront. How many times have you been troubleshooting a problem with the data that are available, only to realize that you need more detailed telemetry or logging in order to make sense of what is going on? This is why I am of the opinion that we should record as much data as reasonably possible, upfront, because it allows the greatest flexibility in asking questions of the data later. This is one of the reasons that I like using fine-grained telemetry, because it is cheap to produce, collect, store, and analyze. It is cost effective to collect a large amount of telemetry from every service at a high frequency. Tagging this kind of telemetry with rich metatdata is also more stable in terms of cardinality. For example, the aggregate message-rate for a streaming-service tagged with metadata like the service name, the software version, the datacenter, and so on, will produce data with much lower cardinality as compared to recording every single request with a unique identifier. In addition, collecting what might look like redundant signals can be advantageous when the correlation among these signals starts to break down, allowing us to identify failures or changes in behaviour. Drawing correlations, or the lack there of, among series, is often what we are doing when looking at dashboards. A colleague of mine has developed a mental model where he can infer most dynamics and operating statesnormal and abnormalof a messaging system, just by looking at the relative network-out for each sever in the cluster. We can formalize this using multivariate approaches, like those that have been adopted in the process industries, in a similar shift from operating systems based on our intuition, to using more formal methods for monitoring and control. Multivariate methods may even derive insight from signals that we disregard as unimportant. Consider the following set of measurements in a multi-dimensional space. The majority of the variation can be described by a lower dimensional, latent-variable space, that could be described by using a multivariate, statistical technique like Principal Component Analysis (PCA). Not only would this give more insight into highly correlated signals, it could also help identify new, unique operating conditions, or predict the onset of failures, before they become critical, as the correlation among these variables starts to change over time. Using statistical-process-control charts will allow us to judge whether services are meeting service-level objectives more effectively than looking at dashboards and using our intuition. Conclusion Many organizations are developing observability tools. There is also a feeding frenzy of vendors trying to corner the market on observability toolsevery vendor has their own observability guideand it seems every conference presentation must mention observability. Most of these efforts are well intended, but the tools, while improving, remain nascent, narrowly focused, and somewhat naive. I believe there must be people experimenting with more sophisticated techniques, but these are not the people that are getting the attention of the observability space dominated by logging, metrics, and tracing. At a small scale, like when we are tracing a single request, or interpreting logs from a single container, these tools are necessarily powerful. Understanding the rich detail gives us a sense of control. But as we start to zoom out, at scale, trying to understand all of the detail is impossibledetails do not compose. Our understanding of the system and our sense of control erodes. The more details we can see, the less we have a sense of control. Mark Burgess I think the future of operating software systems at scale will look like process engineering. We will rely on continuous signals and look at software systems as dynamical systems. We will embrace similar techniques for process control and continuous improvement. This is why I do not like the term observability as it is currently used. In operating software systems at scale, we may want to reserve the traditional definition of observability as it relates to controllability. Interestingly, the term observability has also been used in Cognitive Systems Engineering in relation to helping people discern the behaviours of autonomous control systems: \"Increasing autonomy and authority of machine agents without an increase in observability create automation surprises.\" See Observations on Errors, Corrections, & Trust of Dependent Systems by James Hamilton. Certainly queuing theory is a well-developed field in computing, but these types of models do not seem to be prominent in most observability tools. My calculus professor in my first year of university, Dr. Boyd, always joked that he \"like to avoid the rigorous method, because with rigour comes mortis.\" Colm MacCrthaigh gave a very interesting presentation entitled Close Loops & Opening Minds: How to Take Control of Systems, Big & Small where he hints at the possibilities of using proportional-integral-derivative (PID) controllers for software systems. He doesn't go into too much detail and most of what he does go on to present is relatively simpletechniques like rebooting, throttling, full-synchronization, and immutability. There is, however, beauty in this simplicity. It should be noted that I am not suggesting that we build systems or controllers that are more complex than they need to be. Many physical systems, or systems in the process industries, have analogous, simple, fail-safe mechanisms, that just work. For example, there is no need to record ratesjust record counts and differentiate after the fact. Instrumenting software with counters like this is not new. This is how Windows Performance Counters work and they have been around for decades. I wrote an article on testing performance counters: Writing Automated Tests for Custom Windows Performance Counters. ",
        "_version_": 1718527429094408192
      },
      {
        "story_id": [21585050],
        "story_author": ["redm"],
        "story_descendants": [3],
        "story_score": [14],
        "story_time": ["2019-11-20T16:06:12Z"],
        "story_title": "Launch HN: Fast.io, Simple Enterprise File Hosting Alternative to S3",
        "search": [
          "Launch HN: Fast.io, Simple Enterprise File Hosting Alternative to S3",
          "The Fast.io team is excited to be officially launching today!<p>Our platform is a solution for designers, developers, and marketers looking to automate everything needed to host and track files and static websites at an enterprise scale.<p>We developed Fast.io because we recognized that the process of deploying static content today is complicated and time-consuming. Solutions like S3 require uploading, a separate CDN configuration, a manual review of raw data logs, and tedious cache flushing each time you make an update.<p>We want to get content online quickly with the simplicity and ease of use of cloud storage without sacrificing the reliable scalability and performance of a CDN.<p>Fast.io tightly integrates with your current workflows and preferred cloud storage service (Google Drive, Dropbox, OneDrive, MediaFire, Box, and GitHub) to manage files. It includes an integrated CDN, using Cloudflare and Akamai for lightning-fast global deploys, a visual dashboard, and detailed, accurate analytics data sent to Google Analytics and Mixpanel.<p>We’d love for you to check it out and we appreciate any and all feedback - https://www.producthunt.com/posts/fast-io<p>Here is an overview of what our simple enterprise file distribution network delivers for free:<p>- Continuous integration from cloud storage or version control<p>- Files up to 500MB each<p>- 100GB free transfer per month<p>- Foolproof Analytics collected directly on the CDN and reported to Google Analytics<p>- Custom domains and free included SSL (HTTPS)<p>- Automatic updates and global deployment from cloud storage or version control<p>- Automatic image optimization<p>- Automatic code minification<p>- Automatic directory listings<p>- Slack integration",
          "This is pretty useful! Putting blinders on and working with a project on my laptop and knowing the folder it's in is automagically getting synced with an actual CDN as well as everything being updated in real-time as I make changes??? Sounds pretty sweet. On my last project I wasted an entire damn day dealing with setting up GH-Pages and another project with Coudflair, a regular hosting company and a few tools to automate my workflow. I really like that all of this is out of the box in this product!",
          "\"Solutions like S3 require uploading, a separate CDN configuration, a manual review of raw data logs, and tedious cache flushing each time you make an update.\"<p>I love all types of automation that lets me avoid doing any of the above. The pricing plans are also very generous... Time to start playing with the free tier :)"
        ],
        "story_text": "The Fast.io team is excited to be officially launching today!<p>Our platform is a solution for designers, developers, and marketers looking to automate everything needed to host and track files and static websites at an enterprise scale.<p>We developed Fast.io because we recognized that the process of deploying static content today is complicated and time-consuming. Solutions like S3 require uploading, a separate CDN configuration, a manual review of raw data logs, and tedious cache flushing each time you make an update.<p>We want to get content online quickly with the simplicity and ease of use of cloud storage without sacrificing the reliable scalability and performance of a CDN.<p>Fast.io tightly integrates with your current workflows and preferred cloud storage service (Google Drive, Dropbox, OneDrive, MediaFire, Box, and GitHub) to manage files. It includes an integrated CDN, using Cloudflare and Akamai for lightning-fast global deploys, a visual dashboard, and detailed, accurate analytics data sent to Google Analytics and Mixpanel.<p>We’d love for you to check it out and we appreciate any and all feedback - https://www.producthunt.com/posts/fast-io<p>Here is an overview of what our simple enterprise file distribution network delivers for free:<p>- Continuous integration from cloud storage or version control<p>- Files up to 500MB each<p>- 100GB free transfer per month<p>- Foolproof Analytics collected directly on the CDN and reported to Google Analytics<p>- Custom domains and free included SSL (HTTPS)<p>- Automatic updates and global deployment from cloud storage or version control<p>- Automatic image optimization<p>- Automatic code minification<p>- Automatic directory listings<p>- Slack integration",
        "story_type": ["LanchHN"],
        "comments.comment_id": [21587894, 21590425],
        "comments.comment_author": ["telaport", "pineapplecake"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-11-20T20:05:41Z",
          "2019-11-21T00:57:36Z"
        ],
        "comments.comment_text": [
          "This is pretty useful! Putting blinders on and working with a project on my laptop and knowing the folder it's in is automagically getting synced with an actual CDN as well as everything being updated in real-time as I make changes??? Sounds pretty sweet. On my last project I wasted an entire damn day dealing with setting up GH-Pages and another project with Coudflair, a regular hosting company and a few tools to automate my workflow. I really like that all of this is out of the box in this product!",
          "\"Solutions like S3 require uploading, a separate CDN configuration, a manual review of raw data logs, and tedious cache flushing each time you make an update.\"<p>I love all types of automation that lets me avoid doing any of the above. The pricing plans are also very generous... Time to start playing with the free tier :)"
        ],
        "id": "0e829860-72f9-4385-93a3-925c106e96da",
        "_version_": 1718527436527763456
      },
      {
        "story_id": [21522522],
        "story_author": ["mr_golyadkin"],
        "story_descendants": [78],
        "story_score": [277],
        "story_time": ["2019-11-13T09:58:14Z"],
        "story_title": "Developing open-source FPGA tools",
        "search": [
          "Developing open-source FPGA tools",
          "https://twitter.com/TinyFPGA/status/1177940755530207232",
          "Something went wrong, but dont fret lets give it another shot. ",
          "For those who are not familiar with silicon vendors' toolchain, it really is very poor quality especially when they started pushing for GUI based Graphical/System development.<p>Vendors make money selling silicon, they see the toolchain as a necessary evil.<p>There are 2018 tools that are still unable to fully support VHDL-2008 standard. Many bugs reported years again remain open. And often the GUI centric\n approach means you are left manually changing ad nauseam several GUI fields and ticking boxes, until you eventually determine what is the tcl 'equivalent' to at least try to ease your pain (tcl scripting often is another pain all together but arguably a lesser evil).<p>Also specifically with Xilinx they seem to have zero consideration for basic version control and create/duplicate/modify an explosion of files and cached versions of files.<p>Projects like GHDL and this are a breath of fresh are.",
          "I'm surprised no one has mentioned Icarus Verilog [0]. I used this extensively when I was in college around 2002. Open source, used to only support FreeBSD (my first BSD exposure), but is cross platform. It saved my ass, as I didnt have to compete for limited lab time on the few Solaris boxes with the proprietary tools we used in my VLSI class while studying EE.<p>I was sitting on my living room floor with my FreeBSD laptop punching out my project while others were waiting in line at 2am for lab time.<p>Fun times.<p>Wrote all of the components for a basic 8-bit ALU while watching with Mothman Propechies and sipping some Bourbon. Wrote a C++ program in that time, too, to generate exhaustive tests for all components. Icarus was fucking awesome for all this. As a commuter student my last 2 years, not having to hang out waiting for time was awesome. I got a lot more sleep, and a lot more done.<p>I've not kept up with its development, but its apparently now cross platform and still under active development. Any amateur interested in Verilog should definitely give it a look.<p>[0] <a href=\"http://iverilog.icarus.com\" rel=\"nofollow\">http://iverilog.icarus.com</a>"
        ],
        "story_type": ["Normal"],
        "url": "https://twitter.com/TinyFPGA/status/1177940755530207232",
        "url_text": "Something went wrong, but dont fret lets give it another shot. ",
        "comments.comment_id": [21523219, 21531956],
        "comments.comment_author": ["DoingIsLearning", "hermitdev"],
        "comments.comment_descendants": [8, 1],
        "comments.comment_time": [
          "2019-11-13T12:14:32Z",
          "2019-11-14T01:40:20Z"
        ],
        "comments.comment_text": [
          "For those who are not familiar with silicon vendors' toolchain, it really is very poor quality especially when they started pushing for GUI based Graphical/System development.<p>Vendors make money selling silicon, they see the toolchain as a necessary evil.<p>There are 2018 tools that are still unable to fully support VHDL-2008 standard. Many bugs reported years again remain open. And often the GUI centric\n approach means you are left manually changing ad nauseam several GUI fields and ticking boxes, until you eventually determine what is the tcl 'equivalent' to at least try to ease your pain (tcl scripting often is another pain all together but arguably a lesser evil).<p>Also specifically with Xilinx they seem to have zero consideration for basic version control and create/duplicate/modify an explosion of files and cached versions of files.<p>Projects like GHDL and this are a breath of fresh are.",
          "I'm surprised no one has mentioned Icarus Verilog [0]. I used this extensively when I was in college around 2002. Open source, used to only support FreeBSD (my first BSD exposure), but is cross platform. It saved my ass, as I didnt have to compete for limited lab time on the few Solaris boxes with the proprietary tools we used in my VLSI class while studying EE.<p>I was sitting on my living room floor with my FreeBSD laptop punching out my project while others were waiting in line at 2am for lab time.<p>Fun times.<p>Wrote all of the components for a basic 8-bit ALU while watching with Mothman Propechies and sipping some Bourbon. Wrote a C++ program in that time, too, to generate exhaustive tests for all components. Icarus was fucking awesome for all this. As a commuter student my last 2 years, not having to hang out waiting for time was awesome. I got a lot more sleep, and a lot more done.<p>I've not kept up with its development, but its apparently now cross platform and still under active development. Any amateur interested in Verilog should definitely give it a look.<p>[0] <a href=\"http://iverilog.icarus.com\" rel=\"nofollow\">http://iverilog.icarus.com</a>"
        ],
        "id": "1df14778-6301-4502-a311-4916eace3fa2",
        "_version_": 1718527435530567680
      },
      {
        "story_id": [20556382],
        "story_author": ["gklitt"],
        "story_descendants": [186],
        "story_score": [320],
        "story_time": ["2019-07-29T16:31:32Z"],
        "story_title": "Browser extensions are underrated: the promise of hackable software",
        "search": [
          "Browser extensions are underrated: the promise of hackable software",
          "https://www.geoffreylitt.com/2019/07/29/browser-extensions.html",
          "Photo by Rick Mason on Unsplash Recent conversations about web browser extensions have focused on controversy: malicious browser extensions capturing web history, and Google limiting the capabilities used by ad blockers. These are important discussions, but we shouldnt lose sight of the big picture: browser extensions are a special ecosystem worth celebrating. Among major software platforms today, browser extensions are the rare exception that allow and encourage users to modify the apps that we use, in creative ways not intended by their original developers. On smartphone and desktop platforms, this sort of behavior ranges from unusual to impossible, but in the browser its an everyday activity. Browser extensions remind us what its like to have deep control over how we use our computers. Assembling our own software Once a software platform reaches a certain level of openness, it can fundamentally change the way that normal users relate to their software. By installing four different Gmail extensions that modify everything from the visual design to the core functionality, in some sense, Ive put together my own email client. Instead of being a passive user of pre-built applications, I can start assembling my own personalized way of using my computer. The popularity of browser extensions proves that many people are interested in customizing their software, and its not just a hobby for power users. There are over 180,000 extensions on the Chrome store, and nearly half of all Chrome users have browser extensions installed.1 When people have an easy way to extend their software with useful functionality, they apparently take advantage. Hackable platforms, not custom APIs Browser extensions have remarkably broad use cases. I personally use Chrome extensions that fill in my passwords, help me read Japanese kanji, simplify the visual design of Gmail, let me highlight and annotate articles, save articles for later reading, play videos at 2x speed, and of course, block ads. The key to this breadth is that most extensions modify applications in ways that the original developers didnt specifically plan for. When Japanese newspapers publish articles, theyre not thinking about compatibility with the kanji reading extension. Extension authors gain creative freedom because they dont need to use application-specific APIs that reflect the original developers view of how people might want to extend their application. The web platform has a few qualities that enable this sort of unplanned extensibility. The foundational one is that the classic web deployment style is to ship all the client code to the browser in human-readable form. (Source maps are a key to preserving this advantage as we ship more code thats minified or compiled from other languages.) The webs layout model also promotes extensibility by encouraging standardized semantic markupmy password manager extension works because web pages reliably use form tags for password submissions instead of building their own version. Even with these advantages, it can still require clever tricks to modify a site in ways that it wasnt built for. But its often a reasonable amount of work, not a years-long reverse engineering effort. The sheer variety of extensions available shows that extension authors are willing to jump through a few hoops to create useful software. Occasionally there are tensions between website developers and extension authors, but it seems far more common that developers are fine with their sites being extended in creative ways, as long as they dont have to do any extra work. Extensions can even make life easier for application developers: if theres a niche request that a small minority of users want, a motivated community member can just build an extension to support it. By building on a hackable platform, developers allow their users to get even more value out of their applications. Many browser extensions are generic tools designed to enhance my use of all websites. I can use my annotation extension on every website everywhere, instead of needing a different highlighting tool for each article I read. Just like using a physical highlighter with paper articles, I can master the tool once, and get a lot of leverage by applying it in different contexts. In many software platforms, we think of the operating system as providing the cross-cutting tools, and third parties as providing standalone apps that are used in isolation. With browser extensions, third parties are also adding tools; a single piece of software has the leverage to change my experience across all the apps I use. When software is built in small units, it also changes the economics. Most extensions I use are free, and are perhaps too small in their feature set to support a full-blown business. And yet, people still choose to make them, and I benefit immensely from these little bits of software. Browsing the extension store feels more like going to a local flea market than going to a supermarket. Massive software built by huge companies isnt the only way. The origins of openness Its not an accident that this openness emerged on the web platform. Since the beginning of personal computing, theres been a philosophical tradition that encourages using computers as an interactive medium where people contribute their own ideas and build their own toolsauthorship over consumption. This idea is reflected in systems like Smalltalk, Hypercard, and more recently, Dynamicland. When Tim Berners-Lee created the World Wide Web, he imagined it fitting into this tradition. My vision was a system in which sharing what you knew or thought should be as easy as learning what someone else knew.2 There were some hiccups along the way3, but eventually that vision largely won out, and the Web became a place where anyone can publish their opinions or photos through social media platforms. Still, theres a catch. When youre using Facebook, youre operating within a confined experience. Youre forced to publish in a certain format, and to use their app in a certain way (that includes, of course, seeing all the ads). Theres more room for authorship than just browsing a news website, but only within the strict lines the app has painted for you. Browser extensions offer a deeper type of control. Instead of merely typing into the provided text box, we can color outside the lines and deeply modify the way we use any application on the web. Browser extensions offer a kind of decentralization: large companies building major websites dont get to dictate all the details of our experience. Improving on extensions We clearly need to work on protecting people from malicious extensions that invade their privacy. But beyond that, here are some bigger picture opportunities I see for improving on extensions: Accessibility: Today, it requires a big jump to go from using browser extensions to creating them: you need to learn a fair amount of web development to get started, and you cant easily develop extensions in the browser itself. What if there were a quick way to get started developing and sharing extensions in the browser? You could imagine smoothly transitioning from editing a website in the developer tools to publishing a small extension. Update: Ive started working on a system called Wildcard to work towards this vision. Compatibility: Because extensions hook into websites in unsupported ways, updates to websites often result in extensions temporarily breaking, and extension authors scrambling to fix them. Can we make it easier for website developers and extension authors to form stable connections between their software, without necessarily resorting to using explicit extension APIs? There are existing practices that fit into this category alreadyfor example, using clean semantic markup, human-readable CSS, and source maps makes it easier to develop an extension. A simple change that would allow for more stable extensions would be to give users more control over when they upgrade to new versions of cloud software. If I have a 3 month window to continue using an old version after the new one is released, that would give extension authors more time to upgrade their software for the new version. Power: Web extensions are limited in their power by the typical architecture of web applications: they have broad rights to modify the browser client, but the server is off limits. For example, if my social media apps server only provides an endpoint for querying my posts in chronological order, no browser extension can ever search through all my posts by keyword. How could we rethink the client-server boundary to enable extensions to make even deeper modifications? This raises tough questions around security and privacy. The modern browser extension API has done a good job balancing extensibility with security, and yet were still grappling with the consequences of browser extensions invading peoples privacy. Giving extensions more power would raise the stakes further. Still, we shouldnt give up in the name of securitywe should fight for extensibility as a value and find ways to balance these interests. The next platform Im intrigued by a couple projects that are rethinking the web in ways that might make it more extensible: The Beaker Browser and the decentralized web community are exploring how the web works without centralized servers. It seems like their proposed architecture would give users fuller control over modifying the server side of web applications. Tim Berners-Lee is working on a new project called SOLID. I dont yet understand precisely what theyre up to, but given Tims involvement I figure its worth paying attention. A key principle is giving users more ownership over their data, which would enable people to use extensions and other software to manipulate their data in flexible ways beyond what application server APIs allow. Computing is still young, and platforms are changing quickly. Modern browser extensions and smartphone platforms have only been around for about a decade. These platforms will evolve, and there will be new platforms after them, and we will get to collectively decide how open they will be. Browser extensions give us one example to strive for: a place where we routinely hack the software we use and make it our own. Discuss on Hacker News ",
          "I believe many people should attempt to create their own web extension, even if they don't publish it.<p>In my younger years, I used to crack and hack software just for fun. Those were my Softice years. Later, when Opera was not Chromium based, I also had several site customisations, since it was very easy to add my own JS and CSS to any web site.<p>Nowadays, I have 4 extensions created and tailored for my needs. One that deals with cookies (mostly \"delete everything\" outside of my white list) and three that add functionalities to specific sites (automating, managing lists, hiding or highlighting content, etc). Building them was fun, though not as much fun as playing against \"copy protections\" long ago: like going from competitive chess to creative DIY.<p>The only pain with custom made extensions is that Firefox is very reluctant to load them. I don't want to upload them them on some Mozilla server, so I have to enter some cryptic \"about:...\" URL, then click and navigate to my extension, for every extension at every browser start. This is one of the main reasons I'm using more Vivaldi than Firefox these past months.",
          "> The modern browser extension API has done a good job balancing extensibility with security<p>No, it hasn't. Almost every single extension I install tells me some variant of \"This extension can intercept and modify all of your browsing traffic\". That's not \"well balanced\", it's completely broken. This is happening clearly for extensions by well intentioned people that <i>do not need those permissions</i>. I can't help but cynically interpret the current situation as intentional on Google's part because having the security model be \"trust Google to vet the extensions\" happens to centralise all the power with them. If you can't trust an extension from the wild then they might as well not exist, right?<p>People laughed Java out of the browser because it took 500ms to start, but at least it had an actual security model."
        ],
        "story_type": ["Normal"],
        "url": "https://www.geoffreylitt.com/2019/07/29/browser-extensions.html",
        "comments.comment_id": [20557232, 20560821],
        "comments.comment_author": ["idoubtit", "zmmmmm"],
        "comments.comment_descendants": [8, 6],
        "comments.comment_time": [
          "2019-07-29T17:44:21Z",
          "2019-07-30T00:02:29Z"
        ],
        "comments.comment_text": [
          "I believe many people should attempt to create their own web extension, even if they don't publish it.<p>In my younger years, I used to crack and hack software just for fun. Those were my Softice years. Later, when Opera was not Chromium based, I also had several site customisations, since it was very easy to add my own JS and CSS to any web site.<p>Nowadays, I have 4 extensions created and tailored for my needs. One that deals with cookies (mostly \"delete everything\" outside of my white list) and three that add functionalities to specific sites (automating, managing lists, hiding or highlighting content, etc). Building them was fun, though not as much fun as playing against \"copy protections\" long ago: like going from competitive chess to creative DIY.<p>The only pain with custom made extensions is that Firefox is very reluctant to load them. I don't want to upload them them on some Mozilla server, so I have to enter some cryptic \"about:...\" URL, then click and navigate to my extension, for every extension at every browser start. This is one of the main reasons I'm using more Vivaldi than Firefox these past months.",
          "> The modern browser extension API has done a good job balancing extensibility with security<p>No, it hasn't. Almost every single extension I install tells me some variant of \"This extension can intercept and modify all of your browsing traffic\". That's not \"well balanced\", it's completely broken. This is happening clearly for extensions by well intentioned people that <i>do not need those permissions</i>. I can't help but cynically interpret the current situation as intentional on Google's part because having the security model be \"trust Google to vet the extensions\" happens to centralise all the power with them. If you can't trust an extension from the wild then they might as well not exist, right?<p>People laughed Java out of the browser because it took 500ms to start, but at least it had an actual security model."
        ],
        "id": "c83f1872-f624-4aa6-972f-a21ecfa02c4b",
        "url_text": "Photo by Rick Mason on Unsplash Recent conversations about web browser extensions have focused on controversy: malicious browser extensions capturing web history, and Google limiting the capabilities used by ad blockers. These are important discussions, but we shouldnt lose sight of the big picture: browser extensions are a special ecosystem worth celebrating. Among major software platforms today, browser extensions are the rare exception that allow and encourage users to modify the apps that we use, in creative ways not intended by their original developers. On smartphone and desktop platforms, this sort of behavior ranges from unusual to impossible, but in the browser its an everyday activity. Browser extensions remind us what its like to have deep control over how we use our computers. Assembling our own software Once a software platform reaches a certain level of openness, it can fundamentally change the way that normal users relate to their software. By installing four different Gmail extensions that modify everything from the visual design to the core functionality, in some sense, Ive put together my own email client. Instead of being a passive user of pre-built applications, I can start assembling my own personalized way of using my computer. The popularity of browser extensions proves that many people are interested in customizing their software, and its not just a hobby for power users. There are over 180,000 extensions on the Chrome store, and nearly half of all Chrome users have browser extensions installed.1 When people have an easy way to extend their software with useful functionality, they apparently take advantage. Hackable platforms, not custom APIs Browser extensions have remarkably broad use cases. I personally use Chrome extensions that fill in my passwords, help me read Japanese kanji, simplify the visual design of Gmail, let me highlight and annotate articles, save articles for later reading, play videos at 2x speed, and of course, block ads. The key to this breadth is that most extensions modify applications in ways that the original developers didnt specifically plan for. When Japanese newspapers publish articles, theyre not thinking about compatibility with the kanji reading extension. Extension authors gain creative freedom because they dont need to use application-specific APIs that reflect the original developers view of how people might want to extend their application. The web platform has a few qualities that enable this sort of unplanned extensibility. The foundational one is that the classic web deployment style is to ship all the client code to the browser in human-readable form. (Source maps are a key to preserving this advantage as we ship more code thats minified or compiled from other languages.) The webs layout model also promotes extensibility by encouraging standardized semantic markupmy password manager extension works because web pages reliably use form tags for password submissions instead of building their own version. Even with these advantages, it can still require clever tricks to modify a site in ways that it wasnt built for. But its often a reasonable amount of work, not a years-long reverse engineering effort. The sheer variety of extensions available shows that extension authors are willing to jump through a few hoops to create useful software. Occasionally there are tensions between website developers and extension authors, but it seems far more common that developers are fine with their sites being extended in creative ways, as long as they dont have to do any extra work. Extensions can even make life easier for application developers: if theres a niche request that a small minority of users want, a motivated community member can just build an extension to support it. By building on a hackable platform, developers allow their users to get even more value out of their applications. Many browser extensions are generic tools designed to enhance my use of all websites. I can use my annotation extension on every website everywhere, instead of needing a different highlighting tool for each article I read. Just like using a physical highlighter with paper articles, I can master the tool once, and get a lot of leverage by applying it in different contexts. In many software platforms, we think of the operating system as providing the cross-cutting tools, and third parties as providing standalone apps that are used in isolation. With browser extensions, third parties are also adding tools; a single piece of software has the leverage to change my experience across all the apps I use. When software is built in small units, it also changes the economics. Most extensions I use are free, and are perhaps too small in their feature set to support a full-blown business. And yet, people still choose to make them, and I benefit immensely from these little bits of software. Browsing the extension store feels more like going to a local flea market than going to a supermarket. Massive software built by huge companies isnt the only way. The origins of openness Its not an accident that this openness emerged on the web platform. Since the beginning of personal computing, theres been a philosophical tradition that encourages using computers as an interactive medium where people contribute their own ideas and build their own toolsauthorship over consumption. This idea is reflected in systems like Smalltalk, Hypercard, and more recently, Dynamicland. When Tim Berners-Lee created the World Wide Web, he imagined it fitting into this tradition. My vision was a system in which sharing what you knew or thought should be as easy as learning what someone else knew.2 There were some hiccups along the way3, but eventually that vision largely won out, and the Web became a place where anyone can publish their opinions or photos through social media platforms. Still, theres a catch. When youre using Facebook, youre operating within a confined experience. Youre forced to publish in a certain format, and to use their app in a certain way (that includes, of course, seeing all the ads). Theres more room for authorship than just browsing a news website, but only within the strict lines the app has painted for you. Browser extensions offer a deeper type of control. Instead of merely typing into the provided text box, we can color outside the lines and deeply modify the way we use any application on the web. Browser extensions offer a kind of decentralization: large companies building major websites dont get to dictate all the details of our experience. Improving on extensions We clearly need to work on protecting people from malicious extensions that invade their privacy. But beyond that, here are some bigger picture opportunities I see for improving on extensions: Accessibility: Today, it requires a big jump to go from using browser extensions to creating them: you need to learn a fair amount of web development to get started, and you cant easily develop extensions in the browser itself. What if there were a quick way to get started developing and sharing extensions in the browser? You could imagine smoothly transitioning from editing a website in the developer tools to publishing a small extension. Update: Ive started working on a system called Wildcard to work towards this vision. Compatibility: Because extensions hook into websites in unsupported ways, updates to websites often result in extensions temporarily breaking, and extension authors scrambling to fix them. Can we make it easier for website developers and extension authors to form stable connections between their software, without necessarily resorting to using explicit extension APIs? There are existing practices that fit into this category alreadyfor example, using clean semantic markup, human-readable CSS, and source maps makes it easier to develop an extension. A simple change that would allow for more stable extensions would be to give users more control over when they upgrade to new versions of cloud software. If I have a 3 month window to continue using an old version after the new one is released, that would give extension authors more time to upgrade their software for the new version. Power: Web extensions are limited in their power by the typical architecture of web applications: they have broad rights to modify the browser client, but the server is off limits. For example, if my social media apps server only provides an endpoint for querying my posts in chronological order, no browser extension can ever search through all my posts by keyword. How could we rethink the client-server boundary to enable extensions to make even deeper modifications? This raises tough questions around security and privacy. The modern browser extension API has done a good job balancing extensibility with security, and yet were still grappling with the consequences of browser extensions invading peoples privacy. Giving extensions more power would raise the stakes further. Still, we shouldnt give up in the name of securitywe should fight for extensibility as a value and find ways to balance these interests. The next platform Im intrigued by a couple projects that are rethinking the web in ways that might make it more extensible: The Beaker Browser and the decentralized web community are exploring how the web works without centralized servers. It seems like their proposed architecture would give users fuller control over modifying the server side of web applications. Tim Berners-Lee is working on a new project called SOLID. I dont yet understand precisely what theyre up to, but given Tims involvement I figure its worth paying attention. A key principle is giving users more ownership over their data, which would enable people to use extensions and other software to manipulate their data in flexible ways beyond what application server APIs allow. Computing is still young, and platforms are changing quickly. Modern browser extensions and smartphone platforms have only been around for about a decade. These platforms will evolve, and there will be new platforms after them, and we will get to collectively decide how open they will be. Browser extensions give us one example to strive for: a place where we routinely hack the software we use and make it our own. Discuss on Hacker News ",
        "_version_": 1718527417365037056
      },
      {
        "story_id": [20069596],
        "story_author": ["methusala8"],
        "story_descendants": [15],
        "story_score": [18],
        "story_time": ["2019-06-01T15:20:33Z"],
        "story_title": "Ask HN:Skills/Tools to Stand Out as a Data Scientist",
        "search": [
          "Ask HN:Skills/Tools to Stand Out as a Data Scientist",
          "Apart from Conventional tools like Python/R and Knowledge  of Machine Learning/Statistics/SQL are there any other skills that I can pick up in order to up skill myself as a Data Scientist?<p>I have nearly three years experience in this field and would like to level up. \nThanks.",
          "A provable ability to conduct Bayesian data analysis: from experiment design, to modelling, to evaluation and back again.<p>I think it’s what makes a “data scientist” legitimate.",
          "Depending on the role, data science is generally either [data analysis (with very little modelling) + business understanding + communication and presentation skills], OR it's [statistics + software development]. There can be some deviation and mixing between the two, but to help with the latter:<p>- linear algebra<p>- calculus<p>- software development - best practices, version control, design patterns etc."
        ],
        "story_text": "Apart from Conventional tools like Python/R and Knowledge  of Machine Learning/Statistics/SQL are there any other skills that I can pick up in order to up skill myself as a Data Scientist?<p>I have nearly three years experience in this field and would like to level up. \nThanks.",
        "story_type": ["AskHN"],
        "comments.comment_id": [20069867, 20074208],
        "comments.comment_author": ["usgroup", "vxpzx"],
        "comments.comment_descendants": [2, 1],
        "comments.comment_time": [
          "2019-06-01T16:14:12Z",
          "2019-06-02T08:18:27Z"
        ],
        "comments.comment_text": [
          "A provable ability to conduct Bayesian data analysis: from experiment design, to modelling, to evaluation and back again.<p>I think it’s what makes a “data scientist” legitimate.",
          "Depending on the role, data science is generally either [data analysis (with very little modelling) + business understanding + communication and presentation skills], OR it's [statistics + software development]. There can be some deviation and mixing between the two, but to help with the latter:<p>- linear algebra<p>- calculus<p>- software development - best practices, version control, design patterns etc."
        ],
        "id": "080b56d2-0060-48d0-9f62-36e31f2a8625",
        "_version_": 1718527406961065985
      },
      {
        "story_id": [19256059],
        "story_author": ["Tomte"],
        "story_descendants": [11],
        "story_score": [34],
        "story_time": ["2019-02-26T17:00:43Z"],
        "story_title": "Semantic Linefeeds (2012)",
        "search": [
          "Semantic Linefeeds (2012)",
          "https://rhodesmill.org/brandon/2012/one-sentence-per-line/",
          "Date: 3 April 2012 Tags:python, computing, document-processing I give some advice each year in my annual Sphinx tutorial at PyCon. Agrateful student asked where I myself had learned the tip. Ihave done some archology and finally have an answer. Let me share what I teach them about semantic linefeeds, then I will reveal its source which turns out to have been written when I was only a few months old! In the tutorial, I ask students whether or not the Sphinx text files in their project will be read by end-users. If not, then I encourage students to treat the files as private source code that they are free to format semantically. Instead of fussing with the lines of each paragraph so that they all end near the right margin, they can add linefeeds anywhere that there is a break between ideas. The result can be spectacular. By starting a new line at the end of each sentence, and splitting sentences themselves at natural breaks between clauses, a text file becomes far easier to edit and version control. Text editors are very good at manipulating lines so when each sentence is a contiguous block of lines, your editor suddenly becomes a very powerful mechanism for quickly rearranging clauses and ideas. And your version-control system will love semantic linefeeds. Have you ever changed a few words at the beginning of a paragraph, only to discover that version control now thinks the whole text has changed? ... the definition in place of it. -The beauteous scheme is that now, if you change -your mind about what a paragraph should look -like, you can change the formatted output merely -by changing the definition of .PP and -re-running the formatter. +The beauty of this scheme is that now, if you +change your mind about what a paragraph should +look like, you can change the formatted output +merely by changing the definition of .PP +and re-running the formatter. As a rule of thumb, for all but the most ... With every sentence and clause on its own line, you can make exactly the same change to the same paragraph without the rest of the paragraph even noticing: ... the definition in place of it. -The beauteous scheme is that now, +The beauty of this scheme is that now, if you change your mind about what a paragraph should look like, you can change the formatted output merely by changing the definition of .PP and re-running the formatter. As a rule of thumb, for all but the most ... Semantic linefeeds, as I call them, have been making my life easier for more than twenty years, and have governed how my text files look behind-the-scenes whether my markup format is HTML, TeX, RST, or the venerable troff macro typesetter. So where did I learn the trick? For a long time I believed that my source must have been the UNIX Documenter's Workbench manual. The Workbench was an attempt by AT&T to market the operating system that had become such a cult hit internally among Bell Labs engineers, by bundling the system with its most powerful typesetting tools. The attempt failed, of course I am told that AT&T was terrible at marketing computers, just as Xerox had no idea what to do with the ideas that were bubbling at PARC in the 1970s but my father worked at Bell Labs and had a copy of the Workbench documentation around the house. (Icannot find a copy on the Internet were all public copies destroyed during the devastating copyright battle that justly brought SCO to its ruin?) But after an extensive search, I have found an earlier source and I could not be any happier to discover that my inspiration is none other than Brian W. Kernighan! He published UNIX for Beginners [PDF] as Bell Labs Technical Memorandum 74-1273-18 on 29 October 1974. It describes a far more primitive version of the operating system than his more famous and more widely available UNIX for Beginners Second Edition from 1978. After a long search I have found the lone copy linked above, hosted on an obscure Japanese web page about UNIX 6th Edition which has now disappeared but can still be viewed on the Internet Archives Wayback Machine (to which both of the links above point). In the section Hints for Preparing Documents, Kernighan shares this wisdom: Hints for Preparing Documents Most documents go through several versions (always more than you expected) before they are finally finished. Accordingly, you should do whatever possible to make the job of changing them easy. First, when you do the purely mechanical operations of typing, type so subsequent editing will be easy. Start each sentence on a new line. Make lines short, and break lines at natural places, such as after commas and semicolons, rather than randomly. Since most people change documents by rewriting phrases and adding, deleting and rearranging sentences, these precautions simplify any editing you have to do later. Brian W. Kernighan, 1974 Note how Pythonic his advice sounds he replaces the fiction of write-once documents with a realistic focus on making text that is easy to edit later! I must have read this when I was first learning UNIX and somehow carried it with me all of these years. It says something very powerful about the UNIX plain-text approach that advice given in 1974, and basically targeted at making text easier to edit in the terribly cramped ed text editor, applies just as well to our modern world of colorful full-screen editors like Emacs and Vim and distributed version control systems that were not even imagined in the 1970s. If you are interested in more early UNIX documentation including the Second Edition of Kernighan's Beginners guide check out the 7th Edition manuals which Bell Labs has kindly made available online, both as PDF files and also as plain-text files marked up for the troff typesetter. Note that you can still compile the troff files successfully on a modern system try that with any other richly-formatted text from the1970s! comments powered by 2021 ",
          "Previous discussion: <a href=\"https://news.ycombinator.com/item?id=4642395\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4642395</a>",
          "Maybe I'm misunderstanding the point of this article, but I always put linefeeds where they should logically be. If people would like to view it in a certain way, it's always easy to put it through a text-processing program, which will do a much better job on semantically separated content rather that that which has had formatting hard-coded into it.<p>(Incidentally, this is also why I use tabs for indentation, since it's a lot easier for tooling to relayout. But I'm not trying to start the holy war here.)"
        ],
        "story_type": ["Normal"],
        "url": "https://rhodesmill.org/brandon/2012/one-sentence-per-line/",
        "comments.comment_id": [19258490, 19259344],
        "comments.comment_author": ["emddudley", "saagarjha"],
        "comments.comment_descendants": [0, 2],
        "comments.comment_time": [
          "2019-02-26T21:33:26Z",
          "2019-02-26T23:18:07Z"
        ],
        "comments.comment_text": [
          "Previous discussion: <a href=\"https://news.ycombinator.com/item?id=4642395\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4642395</a>",
          "Maybe I'm misunderstanding the point of this article, but I always put linefeeds where they should logically be. If people would like to view it in a certain way, it's always easy to put it through a text-processing program, which will do a much better job on semantically separated content rather that that which has had formatting hard-coded into it.<p>(Incidentally, this is also why I use tabs for indentation, since it's a lot easier for tooling to relayout. But I'm not trying to start the holy war here.)"
        ],
        "id": "bd3dbcea-73b1-4f89-a4e8-84b7e3cad8fc",
        "url_text": "Date: 3 April 2012 Tags:python, computing, document-processing I give some advice each year in my annual Sphinx tutorial at PyCon. Agrateful student asked where I myself had learned the tip. Ihave done some archology and finally have an answer. Let me share what I teach them about semantic linefeeds, then I will reveal its source which turns out to have been written when I was only a few months old! In the tutorial, I ask students whether or not the Sphinx text files in their project will be read by end-users. If not, then I encourage students to treat the files as private source code that they are free to format semantically. Instead of fussing with the lines of each paragraph so that they all end near the right margin, they can add linefeeds anywhere that there is a break between ideas. The result can be spectacular. By starting a new line at the end of each sentence, and splitting sentences themselves at natural breaks between clauses, a text file becomes far easier to edit and version control. Text editors are very good at manipulating lines so when each sentence is a contiguous block of lines, your editor suddenly becomes a very powerful mechanism for quickly rearranging clauses and ideas. And your version-control system will love semantic linefeeds. Have you ever changed a few words at the beginning of a paragraph, only to discover that version control now thinks the whole text has changed? ... the definition in place of it. -The beauteous scheme is that now, if you change -your mind about what a paragraph should look -like, you can change the formatted output merely -by changing the definition of .PP and -re-running the formatter. +The beauty of this scheme is that now, if you +change your mind about what a paragraph should +look like, you can change the formatted output +merely by changing the definition of .PP +and re-running the formatter. As a rule of thumb, for all but the most ... With every sentence and clause on its own line, you can make exactly the same change to the same paragraph without the rest of the paragraph even noticing: ... the definition in place of it. -The beauteous scheme is that now, +The beauty of this scheme is that now, if you change your mind about what a paragraph should look like, you can change the formatted output merely by changing the definition of .PP and re-running the formatter. As a rule of thumb, for all but the most ... Semantic linefeeds, as I call them, have been making my life easier for more than twenty years, and have governed how my text files look behind-the-scenes whether my markup format is HTML, TeX, RST, or the venerable troff macro typesetter. So where did I learn the trick? For a long time I believed that my source must have been the UNIX Documenter's Workbench manual. The Workbench was an attempt by AT&T to market the operating system that had become such a cult hit internally among Bell Labs engineers, by bundling the system with its most powerful typesetting tools. The attempt failed, of course I am told that AT&T was terrible at marketing computers, just as Xerox had no idea what to do with the ideas that were bubbling at PARC in the 1970s but my father worked at Bell Labs and had a copy of the Workbench documentation around the house. (Icannot find a copy on the Internet were all public copies destroyed during the devastating copyright battle that justly brought SCO to its ruin?) But after an extensive search, I have found an earlier source and I could not be any happier to discover that my inspiration is none other than Brian W. Kernighan! He published UNIX for Beginners [PDF] as Bell Labs Technical Memorandum 74-1273-18 on 29 October 1974. It describes a far more primitive version of the operating system than his more famous and more widely available UNIX for Beginners Second Edition from 1978. After a long search I have found the lone copy linked above, hosted on an obscure Japanese web page about UNIX 6th Edition which has now disappeared but can still be viewed on the Internet Archives Wayback Machine (to which both of the links above point). In the section Hints for Preparing Documents, Kernighan shares this wisdom: Hints for Preparing Documents Most documents go through several versions (always more than you expected) before they are finally finished. Accordingly, you should do whatever possible to make the job of changing them easy. First, when you do the purely mechanical operations of typing, type so subsequent editing will be easy. Start each sentence on a new line. Make lines short, and break lines at natural places, such as after commas and semicolons, rather than randomly. Since most people change documents by rewriting phrases and adding, deleting and rearranging sentences, these precautions simplify any editing you have to do later. Brian W. Kernighan, 1974 Note how Pythonic his advice sounds he replaces the fiction of write-once documents with a realistic focus on making text that is easy to edit later! I must have read this when I was first learning UNIX and somehow carried it with me all of these years. It says something very powerful about the UNIX plain-text approach that advice given in 1974, and basically targeted at making text easier to edit in the terribly cramped ed text editor, applies just as well to our modern world of colorful full-screen editors like Emacs and Vim and distributed version control systems that were not even imagined in the 1970s. If you are interested in more early UNIX documentation including the Second Edition of Kernighan's Beginners guide check out the 7th Edition manuals which Bell Labs has kindly made available online, both as PDF files and also as plain-text files marked up for the troff typesetter. Note that you can still compile the troff files successfully on a modern system try that with any other richly-formatted text from the1970s! comments powered by 2021 ",
        "_version_": 1718527387720744960
      },
      {
        "story_id": [21668975],
        "story_author": ["networked"],
        "story_descendants": [55],
        "story_score": [195],
        "story_time": ["2019-11-30T07:20:56Z"],
        "story_title": "GitHub Trip Report",
        "search": [
          "GitHub Trip Report",
          "https://fossil-scm.org/forum/forumpost/536ce98d85",
          "(1.2) By Richard Hipp (drh) on 2019-12-02 01:07:22 edited from 1.1 [source] I, the founder of Fossil, was invited and attended the first meeting of the GitHub Open Source Advisory Board in San Francisco on 2019-11-12. This post is a report of my adventures. The only skeptic in the room GitHub was a very gracious host, providing lunch before and dinner after the 5-hour meeting. There were 24 people in the room, 5 or 6 of whom were GitHub employees. To nobody's surprise, I was the only Git skeptic present. But everyone was very cordial and welcoming. It was a productive meeting. It is not about Git The GitHub employees seem to have all read at least some of my writings concerning Git and rebase and comparing Git and Fossil and so they knew where I stand on those issues. They are not in 100% agreement with me, but they also seem to understand and appreciate my point of view. They all seemed to agree that Git has UX issues and that it is not the optimal Distributed Version Control System (DVCS) design. Their argument is that none of that matters, at least not any more. Git is used by nearly everyone and is the defacto standard, and so that is what they use. Though I disagree, it is still a reasonable argument. I offered a number of suggestions on how Git might be improved (for example, by adding the ability to check-in revisions to comments on prior check-ins) and their response was mostly \"We don't control Git\". I replied that GitHub designed and implement the v2-http sync protocol used by Git which has since become the only sync protocol that most Git user will ever experience and which significantly improved the usability of Git. I encouraged them to continue to try to push Git to improve. I'm not sure how persuasive my arguments were though. Because: It isn't really about Git anymore. GitHub used to be a Git repository hosting company. But now, they are about providing other software development and management infrastructure that augments the version control. They might as well change their name to Hub at this point. For example, the main topics of discussion at this meeting where: Issue tracking and tickets Code review Automated Continuous Integration Maintainer workflow and triage The GitHub staff says that the four pillars of their organization are DevOps Security Collaboration Insights You will notice that version control is not on either of those lists. There has been a culture shift at GitHub. They are now all about the tooling that supports Git and not Git itself. On the other hand, GitHub showed no hint of any desire to support alternative version control systems like Mercurial or Fossil. They openly assume that Open Source (mostly) runs on Git. It is just that the version control is no longer their focus. They have moved on to providing other infrastructure in support of Open Source. Other Take-Aways Documentation Is The Best Way To Say \"No\" One of the most important and also the hardest jobs of a project maintainer is saying \"no\" to enhancement requests. If you try to take on every requested enhancement, your project will quickly loss focus and become too unwieldy to maintain. Participant \"Paris\" (whose full contact information I was unable to obtain) says: \"Documentation is the best way to say 'no'.\" In other words, it is important to document why a project does things the way it does, as this will tend to prevent enhancement requests that cause the project to diverge from its original intent. I have also found that writing about the \"why\" tends to help one focus on the real purpose of the project as well. Separately it was observed that documentation is the single most important factor in open-source adaption. The better your documentation, the more likely people are to use and contribute to your project. Implications for Fossil: I'd like to add the ability to include PIC and EQN markup in the middle of both Markdown and Fossil-Wiki documents. I think this Forum feature has been very successful, but there are still many opportunities to improve the UX. Additional ideas on how Fossil might be enhanced to support better project documentation are welcomed. \"The Tools Make The Rules\" This quote (from Tom Ted Kremenek of the Swift project) is saying that your workflow will be defined and constrained by the tools you use. If you use inferior tools your productivity will suffer. This is the argument I've made for years about the importance of being about to amend check-in comments and to view the descendants of check-ins. Git partisans offer all kinds of excuses about how their workflow does not need that. I tend to counter with \"I never needed bisect until I had the capability.\" The point is that once you have the ability to amend check-in comments and view the descendants of a check-in, your workflow becomes more efficient and you wonder how you ever survived without those capabilities. What limitations or restrictions in Fossil are limiting productivity? Some possible ideas: Poor tracking of files across renames Inability to do shallow clones Poor ticket UX Inability to subscribe to notifications for changes to specific tickets or files. Difficulty implementing actions that are triggered by new check-ins and/or new tickets. This is needed for Continuous Integration (CI). Enhancements to the backoffice feature of Fossil would be welcomed. General agreement that rebasing is not a good thing Even among Git partisans, there seems to be a general agreement that rebase ought to be avoided. The Rebase Considered Harmful document is not especially controversial. An interesting insight from the LLVM developers: They use rebase extensively. But the reason is that downstream tooling controlled by third-parties and which was developed back when LLVM was still using SVN expects a linear sequence of changes. The LLVM developers would like to do more with branching, but that would break the downstream tools over which they have no control, and so they are stuck with having to rebase everything. Thus rebase supports historical compatibility of legacy tooling. It's a reasonable argument in support of rebase, not one that I necessarily agree with, but one that I understand. Summary And Concluding Thoughts GitHub is changing. It is important to understand that GitHub is moving away from being a Git Repository hosting company and towards a company that provides an entire ecosystem for software development. Git is still at the core, but the focus is no longer on Git. Fossil already provides many of the features that GitHub wraps around Git. Prior to the meeting, someone told me that \"GitHub is what make Git usable.\" Fossil has a lot of integrated capabilities that make GitHub unnecessary. Even so, there is always room for improvement and Fossil should be adapting and integrating ideas from other software development systems. One GitHub-er asked me: \"What would it take to get SQLite to move off of Fossil and on to Git.\" Just to be clear to everyone reading this: That will never happen. Fossil was specifically designed to support SQLite development and does so remarkably well. Fossil fills a different niche than does Git/GitHub. Fossil will continue to be supported and enhanced by me (as well as others) well into the foreseeable future. (2) By Richard Hipp (drh) on 2019-11-27 15:30:32 in reply to 1.0 [link] [source] Prior discussion of PIC markup in Fossil documentation was at post db217184de. The EQN markup would work much the same way. Since posting that a couple of months ago, I did actually attempt to write a PIC language interpreter in Javascript. But I discovered that Javascript is not a particularly good language with which to write a parser/compiler - or at least my limited knowledge of Javascript was insufficient for the task. Input from readers with better Javascript skills would be appreciated here. I have heard that it is now possible to compile C code into WASM. In particular, I hear that people are compiling \"sqlite3.c\" into WASM and running it in browsers. Perhaps an alternative approach would be to write a compact PIC translator in C (using Lemon to implement the parser) then compile the result into WASM for download to the browser. But I don't understand how all of that works. If anybody has any insights and can help guide me through the process, that would be appreciated. (4) By Andreas Kupries (aku) on 2019-11-27 17:52:34 in reply to 2 [link] [source] A question about lemon. IIRC it is a LALR parser generator similar to Bison. Is the generated parser able to execute semantic actions as the AST is built ? This and the ability to reach back to the lexer to control its execution will be necessary for PIC. The reason for that is PIC's macro system, which it has in lieu of proper procedures/functions. Macros can recursively call each other, and to not have this end in tears of infinite recursion this expansion has to be/is curtailed in code not executed, i.e. the untaken branches of if/else conditionals. For the implementation of the language this means that semantic actions have to be executed as the input is lexed and parsed, and then reach back to the lexer with the outcomes of conditions to selectively skip/suppress macro expansion. See pages 6-8 of the dpic docs. With that in mind I wonder if it might be suitable to reuse an existing parser like dpic, instead of writing a new PIC parser, for compilation to WASM. (5) By Richard Hipp (drh) on 2019-11-27 18:05:49 in reply to 4 [link] [source] If you can compile dpic down into a dpic.js file that we can source and use to render PIC on the client-side, that would be great! (7) By Warren Young (wyoung) on 2019-11-27 18:44:20 in reply to 2 [link] [source] it is now possible to compile C code into WASM Yes, and more to the point, WebAssembly support is approaching universal adoption. It won't run in ELinks, but this is a non-core feature that those with such niche needs can ignore. I do wonder if server-side rendering to a virtual SVG would be a better plan, though. (13) By aitap on 2019-11-27 21:35:13 in reply to 2 [link] [source] Is there a place (someone's repo on ChiselApp?) where we could collaborate on the PIC renderer? I seem to have created a Lemon grammar for PIC, but I cannot test it on real inputs yet, because first I'll have to hack the lexer to understand defines and substitute macros. (According to Kernighan and Raymond, they have to be done outside the parser, by pure sting substitution.) Also, I haven't started the work on the \"virtual machine\" to interpret the AST and output SVG. (14) By Stephan Beal (stephan) on 2019-11-27 21:45:42 in reply to 13 [link] [source] If you can wait about 12 hours i can set up a repo for you on fossil.wanderinghorse.net and make you the admin so you can manage collaborators. i have no idea whether the backoffice (forum) parts will work on that site, though, so no guarantees on that. If you want to go ahead and set up a repo, set up your user(s), and email me a link (see https://wanderinghorse.net/home/stephan) where i can download it, i can have it installed in no time tomorrow. My only requirement is that i be allowed to set myself up as an admin/superuser solely for maintenance purposes. (15) By Stephan Beal (stephan) on 2019-11-27 22:07:17 in reply to 13 [link] [source] Alternately, i can set you up an empty repo. Just email me (see previous post) with your preference. (3) By Stephan Beal (stephan) on 2019-11-27 15:41:14 in reply to 1.0 [link] [source] The only skeptic in the room But you are not alone! You will notice that version control is not on either of those lists. That's... kind of mind-blowing. Perhaps they consider it an implicit entry in the list, since it's a level of infrastructure which underpins every aspect of what they do. A \"can't see the forest for the trees\" kind of thing. What limitations or restrictions in Fossil are limiting productivity? My own personal entry in that list is not new: the lack of a library interface. Obviously (for those who don't know this tidbit of history) i intended to tackle that with libfossil, and was well on the way to feature-parity with the fossil binary when chronic RSI (specifically, C8 nerve inflammation/damage in both arms) effectively kicked me out of the game in late 2014 (even now, my typing capacity is still reduced to about 10-15% of what it previously was, and i am still in forced early retirement due to the years-long duration of my medical leave). Coincidentally enough, since yesterday i've been milling over a reboot of that work, focusing initially on features which would make it easier/possible to create custom read-only clients, ignoring the ability to update/merge/etc. for the time being, simply to try to keep the scope in check. The motivation for that came from user elechak, who posted a query based on the mlink table. Being able to plug that type of feature one's own app/page/whatever would empower all sorts of apps we don't/can't currently have. Obviously once we have a library, the gateway is opened for any number of clients, e.g. tooling for reports, CI, and IDEs. That said... my hands cannot reasonably commit (as it were) to that scope of project for the foreseeable future :(. Getting the library up and running again, now that fossil supports multiple hash types (libfossil predates those changes), would require an as-yet-unknown amount of effort. That said, i don't think it would take too awful much to get the read-only functionality working again. So... i'll take this opportunity to bang the drum again and call for \"seriously interested\" coders to get in touch and help make that happen. (Forewarning: it wouldn't be a small amount of code.) While i cannot personally commit to more than \"casual\" amounts of coding, my capacity for technical support is far less bounded and my capacity for moral support is practically limitless ;). One GitHub-er asked me: \"What would it take to get SQLite to move off of Fossil and on to Git.\" LOL! i haven't laughed this hard since you explained the logic behind you getting taller with age! (8) By Warren Young (wyoung) on 2019-11-27 18:59:43 in reply to 3 [link] [source] i intended to tackle that with libfossil Let's not make the same mistake twice: libfossil fell into disrepair because it's a second implementation of the formats and protocols within an ecosystem that's not able to progress as fast as it wants to on a single implementation. (Thus this thread.) Rather than resurrect libfossil, I'd rather see that work went into refactoring the internals of Fossil to extract a library, divorcing the CGI and CLI interfaces from a core that only deals with the repo formats. Some of this is already done well enough that all that's needed is to draw the lines more explicitly, but I want to get to a world where the build process results in a static libfossil2.a that links to the Fossil binary, which can optionally be built as a shared object and linked into an app without linking to libnss, libssl, etc. because the UI is a native GUI tool that only works on local repos and thus doesn't need network clone and sync support. The resulting library should be modular, so that if the third-party app also wants clone and sync support, it can include that as well. Or, Plan C: expand the current web APIs to the point that one could write a full-function Fossil library that did nothing other than make HTTP API calls to a running Fossil instance. (9) By Richard Hipp (drh) on 2019-11-27 19:15:14 in reply to 8 [link] [source] Proposed Forum Enhancement I would like to start a new Forum thread named something like \"Libfossil Reboot\" based on the previous comment. The new \"Libfossil Reboot\" thread would link back to Warren's post, and Warren's post should link forward to the new thread. But it would be a completely new thread, in as much as it is a tangent off of the original. In other words, I'd like to create a simple mechanism to prevent thread hijacking. Somebody with good UX sense, please suggest a suitable interface for this new feature, or if the new idea makes no sense from a UX standpoint, please talk me out of it. (11) By Warren Young (wyoung) on 2019-11-27 19:23:24 in reply to 9 [link] [source] At risk of creating a hijack of a hijack () what if a Moderator or higher level user could mark a post as being a topic fork, which then prompts the user for a new thread title. The existing post becomes the starting post of the new thread under the new title, and unlike current first-post-in-thread, it has an \"in reply to\" in its header pointing back to the other thread. For normal users, all they see is that the Reply button changes to Moved or similar. Clicking it takes them to the start of the other thread, where they can now click Reply. Don't send the user straight to a reply: they need to see the new context first. They might then choose to reply further down-thread in the child topic. (10.1) By Stephan Beal (stephan) on 2019-11-27 19:24:51 edited from 10.0 in reply to 8 [link] [source] (Edit: sorry, Richard - i didn't see your note about the threadjacking until posting this :/. Further discussion will take place in the new thread if/when it starts.) Fair enough. Rather than resurrect libfossil, I'd rather see that work went into refactoring the internals of Fossil to extract a library, divorcing the CGI and CLI interfaces from a core that only deals with the repo formats. That's more or less originally what the goal was. There are, however, fundamental design decisions in fossil which make \"bending it\" into a library unfeasible. First and foremost is its allocation failure policy: simply die. That drastically reduces the amount of code needed (as can be seen when comparing the same algos in fossil core to libfossil - a huge number of lines of code in libfossil are checking/handling result codes and allocation results, where fossil would simply die in most such cases). A library cannot feasibly have a die-on-OOM policy. (That is to say: i wouldn't personally use a library which i knew would outright kill my app if the library encountered an error code or couldn't allocate memory.) That said: i would be completely on-board with such an effort. As Richard put it when it was proposed it in 2011, though, it would require \"a Herculean effort.\" (Though dictionaries/style guides differ on whether Herculean is capitalized or not when used that way, it would certainly have a massive capital H in our context!) Or, Plan C: expand the current web APIs to the point that one could write a full-function Fossil library that did nothing other than make HTTP API calls to a running Fossil instance. That's kinda/sorta what the JSON bits do, and an HTTP-based API is essentially a link-at-call-time DLL. However, a JSON-based API would be a particularly poor choice for certain functionality, most notably transferring binary files of arbitrary sizes, as doing so requires encoding into a form JSON can use (huge strings or massive arrays of integers (byte values), both of which are larger than the binaries themselves). (12) By Warren Young (wyoung) on 2019-11-27 19:37:21 in reply to 10.0 [link] [source] Understand, I don't mean that your intent in creating the library was a mistake, only that we now know that it clearly didn't attract a big enough user community that you had to fend off the tendency to fork the libary when you had to give up on it, because you had multiple people all wanting to continue it. Knowing this, we shouldn't repeat the decision. Contrast Fossil: I can see this project forking if something drastic happened to drh or to the project direction. We've got enough people actively interested in working on it that we would have to fight off a tendency to create multiple forks and not just one new successor project. I point that out not to show a danger to Fossil, but to show that it has a bus factor above 1. Years now after libfossil stopped moving, I think we can say that it had a bus factor of exactly 1. :( Therefore, efforts going forward should make Fossil itself more powerful, not divide precious resources. As to the matter of difficulty, that's why I proposed my previously-unnamed Plan B as a refactoring effort, rather than reimplementation. It's something we can simply choose to strive toward, one bite at a time. On the efficiency of a JSON API, many HTTP APIs switch content types to match the needs of the call. JSON returns may be the default, but binary and HTML returns are also legal based on context. Some calls may even have multiple forms. In that sense, Fossil UI is the HTML API to this library already, and /raw is the binary interface. All I'm proposing is wrapping this in a way that you can use inside, say, a Fossil plugin for a programmer's text editor. (6) By Warren Young (wyoung) on 2019-11-27 18:40:00 in reply to 1.1 [link] [source] Git is used by nearly everyone and is the defacto standard, and so that is what they use. That's pretty much the \"Windows\" argument. Windows hasn't gone away in the face of smartphones, Chromebooks, and the cloud, but it has been pushed to the margins wherever practical. It's probably a large part of the reason that the Windows phone and tablet efforts didn't go anywhere. It's the point of view of an entrenched victor who becomes blind to the fact that they are now entrenched, and consequently can no longer move. I'm not even sure this is a conscious choice. Perhaps a better metaphor is Gulliver strapped down by thousands of Lilliputians using ropes woven from process, standardization, and interdependency. Gulliver could choose to rise, but he'd destroy all the ropes in doing so. 1. Poor tracking of files across renames That's not the number one issue, just the most recent request. :) Fixing this would be a nice quality of life improvement for those of us already using Fossil, but I doubt it factors very high in anyone's decision about whether to use Fossil, or whether to leave Fossil for something else. 2. Inability to do shallow clones That goes to scalability, and it probably does have a lot to do with Fossil's adoption curve. It is true that most software projects aren't Linux-scale in terms of SLOC and complexity of development and thus don't benefit from elements of Git's design that serve the rare characteristics of the Linux kernel development project. Nevertheless, it is also true that there are a lot of software projects bigger and more complex than SQLite yet still much smaller than Linux and thus could benefit if Fossil supported large-scale projects better. 3. Poor ticket UX Now that we have Markdown in tickets (!!) my only serious complaint about it is the two-step filing process. I see the value of the simplified filing interface for normal users, but when I'm logged in as a Setup user, it's annoying to not have all of the ticket fields available to me to fill out at the same time as the initial filing. This could be controlled by a user cap saying whether that user or category of users sees the full ticket UI on filing. Speaking of user caps, another thing I anticipate is support for various SSO technologies: LDAP, OpenID, Oauth, SAML, etc. That in turn would require some way to map groups in those systems to user categories in Fossil, which would probably have to expand beyond the current hard-coded set of four. 4. Inability to subscribe to notifications for changes to specific tickets or files. That sounds like it'd be pretty easy to implement. All it wants is someone to scratch the itch. Not me; don't care. :) 5. Difficulty implementing actions that are triggered by new check-ins and/or new tickets. Server-side Tcl hooks? I think the hardest part is making such hooks enforcing in the face of distributed clones. If I check something into my local repo but a push to the parent repo fails because of a Tcl hook refusing the new material, what then? LLVM...downstream tools...stuck with having to rebase everything. Wouldn't a release branch (as opposed to release and version tags) solve this? Downstream projects could point at the branch they care about, which would get only a linear sequence of changes, pushed there by a release manager. You can implement development, feature freeze, slush, and code freeze branches in this way within Fossil today, progressively restricting changes until a frozen blob of code lands on the release branch. The only thing that might be nice in this regard is locking branches to particular users. Fossil's DVCS nature makes absolute locking impossible, but part of such organized thinking is an assumption that the users aren't actively malicious. If low-level Dev A checks something into the feature freeze branch directly, Fossil already has tools to fix that, and beyond that, it's a social/admin issue, not a tech issue. GitHub is moving away from being a Git Repository hosting company They're just moving up-stack, which is necessary to avoid becoming another commodity provider of Git service. If they'd continued to focus on Git, they'd be in direct competition with Digital Ocean and /usr/bin/git. They have to move up-stack to continue propping up those Silicon Valley salaries. Fossil doesn't have the same need. It merely needs to remain useful to its users. I think this discussion should remain focused on that and possibly on attracting new users. But Fossil doesn't have the same business needs as GitHub, Inc., a Subsidiary of Microsoft Corporation. (24) By anonymous on 2020-01-27 22:04:38 in reply to 6 [link] [source] If I check something into my local repo but a push to the parent repo fails because of a Tcl hook refusing the new material, what then? What happens when you push to a parent repo where you don't have commit privilege? (25) By Warren Young (wyetr) on 2020-01-27 22:31:29 in reply to 24 [link] [source] I see where you're trying to go with that point, but the difference is that when you try to push without check-in capability it's expected to be permanent until you get an account with that capability, then permanently allowed as long as your account remains active. To compare that with the denial from hooks, you're saying that a remote piece of software can effectively activate or deactivate my check-in capability on each artifact pushed. Keep in mind that Fossil writes its check-ins durably first to the local blockchain, then syncs the result to the remote system. This means that once local artifacts are denied by the remote, they'll be denied until the remote changes its rules. If we assume that the rules implemented by the Tcl commit hook are good and wisely crafted, then this means the local repo will forever have durable artifacts that can never be pushed. The local repo will keep pushing them, and the remote will keep running the commit hook logic over them, which will keep rejecting them. All of this is fairly anti-Fossil, which tries very hard to make all clones identical. If Fossil gets anything like this feature, I think it must be done by the hook scripts being somehow sent down with the clone and kept up to date there, so that the hook can run locally before the local blockchain gets modified. (26) By Warren Young (wyoung) on 2020-01-28 19:15:48 in reply to 25 [link] [source] I think it must be done by the hook scripts being somehow sent down with the clone ...which of course introduces portability problems: you have to ensure that the hook scripts will in fact run correctly on all of the clones, else you break those clones. \"But it worked on my machine!\" doesn't fly when your remote contributors stop being able to check work in just because they're on a different host OS, or are missing some third-party Tcl package, or whatever. However, a night's sleep and a shower have provided an answer: instead of the hook script outright rejecting \"bad\" checkins, what if Fossil treats commit hook rejection as a signal to automatically create a branch for the new artifacts so that other work can continue on the original branch? Then the problem can be corrected in public, which is very much pro-Fossil. We could obviously do this in a post-facto way, running such problem-detecting scripts on the server and having it automatically move artifacts, but making it part of the sync protocol with integrated hook scripts means the remote clone could be notified immediately that their check-in has been moved to a branch. The hook script could even return a detailed message explaining the problem, which could guide the remote user to a fix. This new scheme means the content is never lost, it is committed locally and sync'd remotely, but it's set aside until the problem can be resolved. (27) By anonymous on 2020-01-28 21:18:18 in reply to 26 [link] [source] what if Fossil treats commit hook rejection as a signal to automatically create a branch for the new artifacts so that other work can continue on the original branch? I like this idea a lot. but making it part of the sync protocol with integrated hook scripts means the remote clone could be notified immediately that their check-in has been moved to a branch. I would hope that the hook script would be able to supply the new branch name, \"calculated\" by whatever rules the core project team deems appropriate. Maybe the hook script could return a string with a branch name and a message separated by some reasonable delimiter. (28) By Warren Young (wyetr) on 2020-01-28 21:51:28 in reply to 27 [link] [source] the hook script would be able to supply the new branch name Sure. It'd allow a form of namespacing, where each hook script gets its own branch naming prefix so you can tell which script shunted the commit aside. Examples: code-style-abcd1234: code rejected by a site-configured automatic code style checker ci-cd-abcd1234: this commit breaks the CI/CD build; implicitly an async hook temp-code-abcd1234: this commit appears to contain some sort of temporary code: debug logging, FIXME comments, #if 0 blocks, etc. Maybe the hook script could return a string with a branch name and a message separated by some reasonable delimiter. Tcl lets you return a list of values; it isn't limited to returning a single scalar value: return { 401, \"code style check failed\", \"code-style-$nonce\", $output_from_style_checker } That is, an error code, a brief message, a branch name to move the commit to if not null, and an optional long message that expands on the brief message. (16) By Kevin (KevinYouren) on 2019-11-29 23:17:40 in reply to 1.1 [link] [source] Richard, I think \"rebase\" may be useful to entities who want to remove or hide errors or worse. (22) By anonymous on 2019-12-05 19:34:16 in reply to 16 [link] [source] I think \"rebase\" may be useful to entities who want to remove or hide errors or worse. If you develop on a private branch, then merge the final commit to trunk then no one need ever see any mess there might be on the branch. (Down side: Fossil doesn't track merges from a private branch, not even in the repo where the private branch exists.) (23) By Kevin (KevinYouren) on 2019-12-06 01:28:25 in reply to 22 [link] [source] Thank you, I didn't know that. (29.1) Originally by anonymous with edits by Stephan Beal (stephan) on 2020-12-15 16:37:59 from 29.0 in reply to 23 [link] [source] Deleted (30) By Stephan Beal (stephan) on 2020-12-15 16:43:07 in reply to 23 [link] [source] Reminder to moderators: We have had one spammer today who's tried to sneak in hyperlinks to a \"bathroom mold removal\" product as a sneaky link on a single period, making it effectively invisible when reading the markdown-processed post but clearly visible in the unparsed content. Please always check the unparsed content of posts for new and anonymous users before approving them. This particular piece of spam was first posted by a new account (which i deleted) and then again, later in the day, as an anonymous post (which snuck past moderation but i coincidentally recognized as identical to the previous post, so deleted it). The devious part is that such posts appear to be topical, but they try to sneak in spam links which are easy for the eye to overlook. (17) By Saagar Jha (saagarjha) on 2019-11-30 18:11:51 in reply to 1.1 [link] [source] This quote (from Tom Kremenek of the Swift project) is saying that your workflow will be defined and constrained by the tools you use. Might you be talking about Ted, the Swift Project Lead? (18) By Richard Hipp (drh) on 2019-11-30 18:49:20 in reply to 17 [link] [source] Yes, it seems likely that I wrote his name down incorrectly in my notes. Sorry about that, Ted. (19) By anonymous on 2019-12-01 23:45:14 in reply to 1.1 [link] [source] I'd like to copy this post for a personal blogpost (of course with reference) is such doing welcomed and permitted? (20) By Richard Hipp (drh) on 2019-12-02 01:07:45 in reply to 19 [link] [source] (21) By Offray (offray) on 2019-12-02 21:41:56 in reply to 1.2 [link] [source] One GitHub-er asked me: \"What would it take to get SQLite to move off of Fossil and on to Git.\" Just to be clear to everyone reading this: That will never happen. Fossil was specifically designed to support SQLite development and does so remarkably well. Fossil fills a different niche than does Git/GitHub. Fossil will continue to be supported and enhanced by me (as well as others) well into the foreseeable future. You could counter ask him/her: \"What would it take to get full support to Fossil inside GitHub and make it a first class citizen there?\" ;-) Cheers, ",
          "Maybe i’m crazy but i find it completely unremarkable that a GitHub strategy session leaves core git completely alone and instead focuses on improving the ecosystem around it.",
          "This is a pretty interesting writeup, even though I don’t necessarily agree with everything. Makes you think, at least.<p>I’ve been out of version control for a bit, and GitHub itself for quite a bit longer, but this part was interesting to me:<p>> Because: It isn't really about Git anymore. GitHub used to be a Git repository hosting company. But now, they are about providing other software development and management infrastructure that augments the version control. They might as well change their name to Hub at this point.<p>Just for added color: starting around 2010 or 2011 or so (around when we added Subversion to GitHub), we had a pretty solid idea that version control wasn’t “the thing”. Mostly because Mercurial was a strong alternative, and there always felt like there might be something new in the wings that could dominate in the next few years. Version control felt really dynamic, and deeply susceptible to change. And if something was better, we’d totally ditch Git for the new thing. The name was a bit of a misnomer, from that perspective.<p>I think that changed over time — I had a hell of a time trying to get movement on basic version control improvements back in 2014 — and now they’re clearly much more about the ecosystem rather than version control itself. It’s where the money is, of course, but it’s also where the bigger innovation is, at least for the time being.<p>I think the author is right to say that Microsoft is targeting a different area than version control specifically, though you could argue if the outcome of that is good or bad. It’s certainly different, though- they’re especially growing headcount right now, and the company makeup is wildly different than what many customers tend think it is today, imo."
        ],
        "story_type": ["Normal"],
        "url": "https://fossil-scm.org/forum/forumpost/536ce98d85",
        "comments.comment_id": [21669341, 21669354],
        "comments.comment_author": ["jcims", "holman"],
        "comments.comment_descendants": [1, 2],
        "comments.comment_time": [
          "2019-11-30T10:03:22Z",
          "2019-11-30T10:07:56Z"
        ],
        "comments.comment_text": [
          "Maybe i’m crazy but i find it completely unremarkable that a GitHub strategy session leaves core git completely alone and instead focuses on improving the ecosystem around it.",
          "This is a pretty interesting writeup, even though I don’t necessarily agree with everything. Makes you think, at least.<p>I’ve been out of version control for a bit, and GitHub itself for quite a bit longer, but this part was interesting to me:<p>> Because: It isn't really about Git anymore. GitHub used to be a Git repository hosting company. But now, they are about providing other software development and management infrastructure that augments the version control. They might as well change their name to Hub at this point.<p>Just for added color: starting around 2010 or 2011 or so (around when we added Subversion to GitHub), we had a pretty solid idea that version control wasn’t “the thing”. Mostly because Mercurial was a strong alternative, and there always felt like there might be something new in the wings that could dominate in the next few years. Version control felt really dynamic, and deeply susceptible to change. And if something was better, we’d totally ditch Git for the new thing. The name was a bit of a misnomer, from that perspective.<p>I think that changed over time — I had a hell of a time trying to get movement on basic version control improvements back in 2014 — and now they’re clearly much more about the ecosystem rather than version control itself. It’s where the money is, of course, but it’s also where the bigger innovation is, at least for the time being.<p>I think the author is right to say that Microsoft is targeting a different area than version control specifically, though you could argue if the outcome of that is good or bad. It’s certainly different, though- they’re especially growing headcount right now, and the company makeup is wildly different than what many customers tend think it is today, imo."
        ],
        "id": "0185f36f-2e35-4162-a053-a39a3a250e75",
        "url_text": "(1.2) By Richard Hipp (drh) on 2019-12-02 01:07:22 edited from 1.1 [source] I, the founder of Fossil, was invited and attended the first meeting of the GitHub Open Source Advisory Board in San Francisco on 2019-11-12. This post is a report of my adventures. The only skeptic in the room GitHub was a very gracious host, providing lunch before and dinner after the 5-hour meeting. There were 24 people in the room, 5 or 6 of whom were GitHub employees. To nobody's surprise, I was the only Git skeptic present. But everyone was very cordial and welcoming. It was a productive meeting. It is not about Git The GitHub employees seem to have all read at least some of my writings concerning Git and rebase and comparing Git and Fossil and so they knew where I stand on those issues. They are not in 100% agreement with me, but they also seem to understand and appreciate my point of view. They all seemed to agree that Git has UX issues and that it is not the optimal Distributed Version Control System (DVCS) design. Their argument is that none of that matters, at least not any more. Git is used by nearly everyone and is the defacto standard, and so that is what they use. Though I disagree, it is still a reasonable argument. I offered a number of suggestions on how Git might be improved (for example, by adding the ability to check-in revisions to comments on prior check-ins) and their response was mostly \"We don't control Git\". I replied that GitHub designed and implement the v2-http sync protocol used by Git which has since become the only sync protocol that most Git user will ever experience and which significantly improved the usability of Git. I encouraged them to continue to try to push Git to improve. I'm not sure how persuasive my arguments were though. Because: It isn't really about Git anymore. GitHub used to be a Git repository hosting company. But now, they are about providing other software development and management infrastructure that augments the version control. They might as well change their name to Hub at this point. For example, the main topics of discussion at this meeting where: Issue tracking and tickets Code review Automated Continuous Integration Maintainer workflow and triage The GitHub staff says that the four pillars of their organization are DevOps Security Collaboration Insights You will notice that version control is not on either of those lists. There has been a culture shift at GitHub. They are now all about the tooling that supports Git and not Git itself. On the other hand, GitHub showed no hint of any desire to support alternative version control systems like Mercurial or Fossil. They openly assume that Open Source (mostly) runs on Git. It is just that the version control is no longer their focus. They have moved on to providing other infrastructure in support of Open Source. Other Take-Aways Documentation Is The Best Way To Say \"No\" One of the most important and also the hardest jobs of a project maintainer is saying \"no\" to enhancement requests. If you try to take on every requested enhancement, your project will quickly loss focus and become too unwieldy to maintain. Participant \"Paris\" (whose full contact information I was unable to obtain) says: \"Documentation is the best way to say 'no'.\" In other words, it is important to document why a project does things the way it does, as this will tend to prevent enhancement requests that cause the project to diverge from its original intent. I have also found that writing about the \"why\" tends to help one focus on the real purpose of the project as well. Separately it was observed that documentation is the single most important factor in open-source adaption. The better your documentation, the more likely people are to use and contribute to your project. Implications for Fossil: I'd like to add the ability to include PIC and EQN markup in the middle of both Markdown and Fossil-Wiki documents. I think this Forum feature has been very successful, but there are still many opportunities to improve the UX. Additional ideas on how Fossil might be enhanced to support better project documentation are welcomed. \"The Tools Make The Rules\" This quote (from Tom Ted Kremenek of the Swift project) is saying that your workflow will be defined and constrained by the tools you use. If you use inferior tools your productivity will suffer. This is the argument I've made for years about the importance of being about to amend check-in comments and to view the descendants of check-ins. Git partisans offer all kinds of excuses about how their workflow does not need that. I tend to counter with \"I never needed bisect until I had the capability.\" The point is that once you have the ability to amend check-in comments and view the descendants of a check-in, your workflow becomes more efficient and you wonder how you ever survived without those capabilities. What limitations or restrictions in Fossil are limiting productivity? Some possible ideas: Poor tracking of files across renames Inability to do shallow clones Poor ticket UX Inability to subscribe to notifications for changes to specific tickets or files. Difficulty implementing actions that are triggered by new check-ins and/or new tickets. This is needed for Continuous Integration (CI). Enhancements to the backoffice feature of Fossil would be welcomed. General agreement that rebasing is not a good thing Even among Git partisans, there seems to be a general agreement that rebase ought to be avoided. The Rebase Considered Harmful document is not especially controversial. An interesting insight from the LLVM developers: They use rebase extensively. But the reason is that downstream tooling controlled by third-parties and which was developed back when LLVM was still using SVN expects a linear sequence of changes. The LLVM developers would like to do more with branching, but that would break the downstream tools over which they have no control, and so they are stuck with having to rebase everything. Thus rebase supports historical compatibility of legacy tooling. It's a reasonable argument in support of rebase, not one that I necessarily agree with, but one that I understand. Summary And Concluding Thoughts GitHub is changing. It is important to understand that GitHub is moving away from being a Git Repository hosting company and towards a company that provides an entire ecosystem for software development. Git is still at the core, but the focus is no longer on Git. Fossil already provides many of the features that GitHub wraps around Git. Prior to the meeting, someone told me that \"GitHub is what make Git usable.\" Fossil has a lot of integrated capabilities that make GitHub unnecessary. Even so, there is always room for improvement and Fossil should be adapting and integrating ideas from other software development systems. One GitHub-er asked me: \"What would it take to get SQLite to move off of Fossil and on to Git.\" Just to be clear to everyone reading this: That will never happen. Fossil was specifically designed to support SQLite development and does so remarkably well. Fossil fills a different niche than does Git/GitHub. Fossil will continue to be supported and enhanced by me (as well as others) well into the foreseeable future. (2) By Richard Hipp (drh) on 2019-11-27 15:30:32 in reply to 1.0 [link] [source] Prior discussion of PIC markup in Fossil documentation was at post db217184de. The EQN markup would work much the same way. Since posting that a couple of months ago, I did actually attempt to write a PIC language interpreter in Javascript. But I discovered that Javascript is not a particularly good language with which to write a parser/compiler - or at least my limited knowledge of Javascript was insufficient for the task. Input from readers with better Javascript skills would be appreciated here. I have heard that it is now possible to compile C code into WASM. In particular, I hear that people are compiling \"sqlite3.c\" into WASM and running it in browsers. Perhaps an alternative approach would be to write a compact PIC translator in C (using Lemon to implement the parser) then compile the result into WASM for download to the browser. But I don't understand how all of that works. If anybody has any insights and can help guide me through the process, that would be appreciated. (4) By Andreas Kupries (aku) on 2019-11-27 17:52:34 in reply to 2 [link] [source] A question about lemon. IIRC it is a LALR parser generator similar to Bison. Is the generated parser able to execute semantic actions as the AST is built ? This and the ability to reach back to the lexer to control its execution will be necessary for PIC. The reason for that is PIC's macro system, which it has in lieu of proper procedures/functions. Macros can recursively call each other, and to not have this end in tears of infinite recursion this expansion has to be/is curtailed in code not executed, i.e. the untaken branches of if/else conditionals. For the implementation of the language this means that semantic actions have to be executed as the input is lexed and parsed, and then reach back to the lexer with the outcomes of conditions to selectively skip/suppress macro expansion. See pages 6-8 of the dpic docs. With that in mind I wonder if it might be suitable to reuse an existing parser like dpic, instead of writing a new PIC parser, for compilation to WASM. (5) By Richard Hipp (drh) on 2019-11-27 18:05:49 in reply to 4 [link] [source] If you can compile dpic down into a dpic.js file that we can source and use to render PIC on the client-side, that would be great! (7) By Warren Young (wyoung) on 2019-11-27 18:44:20 in reply to 2 [link] [source] it is now possible to compile C code into WASM Yes, and more to the point, WebAssembly support is approaching universal adoption. It won't run in ELinks, but this is a non-core feature that those with such niche needs can ignore. I do wonder if server-side rendering to a virtual SVG would be a better plan, though. (13) By aitap on 2019-11-27 21:35:13 in reply to 2 [link] [source] Is there a place (someone's repo on ChiselApp?) where we could collaborate on the PIC renderer? I seem to have created a Lemon grammar for PIC, but I cannot test it on real inputs yet, because first I'll have to hack the lexer to understand defines and substitute macros. (According to Kernighan and Raymond, they have to be done outside the parser, by pure sting substitution.) Also, I haven't started the work on the \"virtual machine\" to interpret the AST and output SVG. (14) By Stephan Beal (stephan) on 2019-11-27 21:45:42 in reply to 13 [link] [source] If you can wait about 12 hours i can set up a repo for you on fossil.wanderinghorse.net and make you the admin so you can manage collaborators. i have no idea whether the backoffice (forum) parts will work on that site, though, so no guarantees on that. If you want to go ahead and set up a repo, set up your user(s), and email me a link (see https://wanderinghorse.net/home/stephan) where i can download it, i can have it installed in no time tomorrow. My only requirement is that i be allowed to set myself up as an admin/superuser solely for maintenance purposes. (15) By Stephan Beal (stephan) on 2019-11-27 22:07:17 in reply to 13 [link] [source] Alternately, i can set you up an empty repo. Just email me (see previous post) with your preference. (3) By Stephan Beal (stephan) on 2019-11-27 15:41:14 in reply to 1.0 [link] [source] The only skeptic in the room But you are not alone! You will notice that version control is not on either of those lists. That's... kind of mind-blowing. Perhaps they consider it an implicit entry in the list, since it's a level of infrastructure which underpins every aspect of what they do. A \"can't see the forest for the trees\" kind of thing. What limitations or restrictions in Fossil are limiting productivity? My own personal entry in that list is not new: the lack of a library interface. Obviously (for those who don't know this tidbit of history) i intended to tackle that with libfossil, and was well on the way to feature-parity with the fossil binary when chronic RSI (specifically, C8 nerve inflammation/damage in both arms) effectively kicked me out of the game in late 2014 (even now, my typing capacity is still reduced to about 10-15% of what it previously was, and i am still in forced early retirement due to the years-long duration of my medical leave). Coincidentally enough, since yesterday i've been milling over a reboot of that work, focusing initially on features which would make it easier/possible to create custom read-only clients, ignoring the ability to update/merge/etc. for the time being, simply to try to keep the scope in check. The motivation for that came from user elechak, who posted a query based on the mlink table. Being able to plug that type of feature one's own app/page/whatever would empower all sorts of apps we don't/can't currently have. Obviously once we have a library, the gateway is opened for any number of clients, e.g. tooling for reports, CI, and IDEs. That said... my hands cannot reasonably commit (as it were) to that scope of project for the foreseeable future :(. Getting the library up and running again, now that fossil supports multiple hash types (libfossil predates those changes), would require an as-yet-unknown amount of effort. That said, i don't think it would take too awful much to get the read-only functionality working again. So... i'll take this opportunity to bang the drum again and call for \"seriously interested\" coders to get in touch and help make that happen. (Forewarning: it wouldn't be a small amount of code.) While i cannot personally commit to more than \"casual\" amounts of coding, my capacity for technical support is far less bounded and my capacity for moral support is practically limitless ;). One GitHub-er asked me: \"What would it take to get SQLite to move off of Fossil and on to Git.\" LOL! i haven't laughed this hard since you explained the logic behind you getting taller with age! (8) By Warren Young (wyoung) on 2019-11-27 18:59:43 in reply to 3 [link] [source] i intended to tackle that with libfossil Let's not make the same mistake twice: libfossil fell into disrepair because it's a second implementation of the formats and protocols within an ecosystem that's not able to progress as fast as it wants to on a single implementation. (Thus this thread.) Rather than resurrect libfossil, I'd rather see that work went into refactoring the internals of Fossil to extract a library, divorcing the CGI and CLI interfaces from a core that only deals with the repo formats. Some of this is already done well enough that all that's needed is to draw the lines more explicitly, but I want to get to a world where the build process results in a static libfossil2.a that links to the Fossil binary, which can optionally be built as a shared object and linked into an app without linking to libnss, libssl, etc. because the UI is a native GUI tool that only works on local repos and thus doesn't need network clone and sync support. The resulting library should be modular, so that if the third-party app also wants clone and sync support, it can include that as well. Or, Plan C: expand the current web APIs to the point that one could write a full-function Fossil library that did nothing other than make HTTP API calls to a running Fossil instance. (9) By Richard Hipp (drh) on 2019-11-27 19:15:14 in reply to 8 [link] [source] Proposed Forum Enhancement I would like to start a new Forum thread named something like \"Libfossil Reboot\" based on the previous comment. The new \"Libfossil Reboot\" thread would link back to Warren's post, and Warren's post should link forward to the new thread. But it would be a completely new thread, in as much as it is a tangent off of the original. In other words, I'd like to create a simple mechanism to prevent thread hijacking. Somebody with good UX sense, please suggest a suitable interface for this new feature, or if the new idea makes no sense from a UX standpoint, please talk me out of it. (11) By Warren Young (wyoung) on 2019-11-27 19:23:24 in reply to 9 [link] [source] At risk of creating a hijack of a hijack () what if a Moderator or higher level user could mark a post as being a topic fork, which then prompts the user for a new thread title. The existing post becomes the starting post of the new thread under the new title, and unlike current first-post-in-thread, it has an \"in reply to\" in its header pointing back to the other thread. For normal users, all they see is that the Reply button changes to Moved or similar. Clicking it takes them to the start of the other thread, where they can now click Reply. Don't send the user straight to a reply: they need to see the new context first. They might then choose to reply further down-thread in the child topic. (10.1) By Stephan Beal (stephan) on 2019-11-27 19:24:51 edited from 10.0 in reply to 8 [link] [source] (Edit: sorry, Richard - i didn't see your note about the threadjacking until posting this :/. Further discussion will take place in the new thread if/when it starts.) Fair enough. Rather than resurrect libfossil, I'd rather see that work went into refactoring the internals of Fossil to extract a library, divorcing the CGI and CLI interfaces from a core that only deals with the repo formats. That's more or less originally what the goal was. There are, however, fundamental design decisions in fossil which make \"bending it\" into a library unfeasible. First and foremost is its allocation failure policy: simply die. That drastically reduces the amount of code needed (as can be seen when comparing the same algos in fossil core to libfossil - a huge number of lines of code in libfossil are checking/handling result codes and allocation results, where fossil would simply die in most such cases). A library cannot feasibly have a die-on-OOM policy. (That is to say: i wouldn't personally use a library which i knew would outright kill my app if the library encountered an error code or couldn't allocate memory.) That said: i would be completely on-board with such an effort. As Richard put it when it was proposed it in 2011, though, it would require \"a Herculean effort.\" (Though dictionaries/style guides differ on whether Herculean is capitalized or not when used that way, it would certainly have a massive capital H in our context!) Or, Plan C: expand the current web APIs to the point that one could write a full-function Fossil library that did nothing other than make HTTP API calls to a running Fossil instance. That's kinda/sorta what the JSON bits do, and an HTTP-based API is essentially a link-at-call-time DLL. However, a JSON-based API would be a particularly poor choice for certain functionality, most notably transferring binary files of arbitrary sizes, as doing so requires encoding into a form JSON can use (huge strings or massive arrays of integers (byte values), both of which are larger than the binaries themselves). (12) By Warren Young (wyoung) on 2019-11-27 19:37:21 in reply to 10.0 [link] [source] Understand, I don't mean that your intent in creating the library was a mistake, only that we now know that it clearly didn't attract a big enough user community that you had to fend off the tendency to fork the libary when you had to give up on it, because you had multiple people all wanting to continue it. Knowing this, we shouldn't repeat the decision. Contrast Fossil: I can see this project forking if something drastic happened to drh or to the project direction. We've got enough people actively interested in working on it that we would have to fight off a tendency to create multiple forks and not just one new successor project. I point that out not to show a danger to Fossil, but to show that it has a bus factor above 1. Years now after libfossil stopped moving, I think we can say that it had a bus factor of exactly 1. :( Therefore, efforts going forward should make Fossil itself more powerful, not divide precious resources. As to the matter of difficulty, that's why I proposed my previously-unnamed Plan B as a refactoring effort, rather than reimplementation. It's something we can simply choose to strive toward, one bite at a time. On the efficiency of a JSON API, many HTTP APIs switch content types to match the needs of the call. JSON returns may be the default, but binary and HTML returns are also legal based on context. Some calls may even have multiple forms. In that sense, Fossil UI is the HTML API to this library already, and /raw is the binary interface. All I'm proposing is wrapping this in a way that you can use inside, say, a Fossil plugin for a programmer's text editor. (6) By Warren Young (wyoung) on 2019-11-27 18:40:00 in reply to 1.1 [link] [source] Git is used by nearly everyone and is the defacto standard, and so that is what they use. That's pretty much the \"Windows\" argument. Windows hasn't gone away in the face of smartphones, Chromebooks, and the cloud, but it has been pushed to the margins wherever practical. It's probably a large part of the reason that the Windows phone and tablet efforts didn't go anywhere. It's the point of view of an entrenched victor who becomes blind to the fact that they are now entrenched, and consequently can no longer move. I'm not even sure this is a conscious choice. Perhaps a better metaphor is Gulliver strapped down by thousands of Lilliputians using ropes woven from process, standardization, and interdependency. Gulliver could choose to rise, but he'd destroy all the ropes in doing so. 1. Poor tracking of files across renames That's not the number one issue, just the most recent request. :) Fixing this would be a nice quality of life improvement for those of us already using Fossil, but I doubt it factors very high in anyone's decision about whether to use Fossil, or whether to leave Fossil for something else. 2. Inability to do shallow clones That goes to scalability, and it probably does have a lot to do with Fossil's adoption curve. It is true that most software projects aren't Linux-scale in terms of SLOC and complexity of development and thus don't benefit from elements of Git's design that serve the rare characteristics of the Linux kernel development project. Nevertheless, it is also true that there are a lot of software projects bigger and more complex than SQLite yet still much smaller than Linux and thus could benefit if Fossil supported large-scale projects better. 3. Poor ticket UX Now that we have Markdown in tickets (!!) my only serious complaint about it is the two-step filing process. I see the value of the simplified filing interface for normal users, but when I'm logged in as a Setup user, it's annoying to not have all of the ticket fields available to me to fill out at the same time as the initial filing. This could be controlled by a user cap saying whether that user or category of users sees the full ticket UI on filing. Speaking of user caps, another thing I anticipate is support for various SSO technologies: LDAP, OpenID, Oauth, SAML, etc. That in turn would require some way to map groups in those systems to user categories in Fossil, which would probably have to expand beyond the current hard-coded set of four. 4. Inability to subscribe to notifications for changes to specific tickets or files. That sounds like it'd be pretty easy to implement. All it wants is someone to scratch the itch. Not me; don't care. :) 5. Difficulty implementing actions that are triggered by new check-ins and/or new tickets. Server-side Tcl hooks? I think the hardest part is making such hooks enforcing in the face of distributed clones. If I check something into my local repo but a push to the parent repo fails because of a Tcl hook refusing the new material, what then? LLVM...downstream tools...stuck with having to rebase everything. Wouldn't a release branch (as opposed to release and version tags) solve this? Downstream projects could point at the branch they care about, which would get only a linear sequence of changes, pushed there by a release manager. You can implement development, feature freeze, slush, and code freeze branches in this way within Fossil today, progressively restricting changes until a frozen blob of code lands on the release branch. The only thing that might be nice in this regard is locking branches to particular users. Fossil's DVCS nature makes absolute locking impossible, but part of such organized thinking is an assumption that the users aren't actively malicious. If low-level Dev A checks something into the feature freeze branch directly, Fossil already has tools to fix that, and beyond that, it's a social/admin issue, not a tech issue. GitHub is moving away from being a Git Repository hosting company They're just moving up-stack, which is necessary to avoid becoming another commodity provider of Git service. If they'd continued to focus on Git, they'd be in direct competition with Digital Ocean and /usr/bin/git. They have to move up-stack to continue propping up those Silicon Valley salaries. Fossil doesn't have the same need. It merely needs to remain useful to its users. I think this discussion should remain focused on that and possibly on attracting new users. But Fossil doesn't have the same business needs as GitHub, Inc., a Subsidiary of Microsoft Corporation. (24) By anonymous on 2020-01-27 22:04:38 in reply to 6 [link] [source] If I check something into my local repo but a push to the parent repo fails because of a Tcl hook refusing the new material, what then? What happens when you push to a parent repo where you don't have commit privilege? (25) By Warren Young (wyetr) on 2020-01-27 22:31:29 in reply to 24 [link] [source] I see where you're trying to go with that point, but the difference is that when you try to push without check-in capability it's expected to be permanent until you get an account with that capability, then permanently allowed as long as your account remains active. To compare that with the denial from hooks, you're saying that a remote piece of software can effectively activate or deactivate my check-in capability on each artifact pushed. Keep in mind that Fossil writes its check-ins durably first to the local blockchain, then syncs the result to the remote system. This means that once local artifacts are denied by the remote, they'll be denied until the remote changes its rules. If we assume that the rules implemented by the Tcl commit hook are good and wisely crafted, then this means the local repo will forever have durable artifacts that can never be pushed. The local repo will keep pushing them, and the remote will keep running the commit hook logic over them, which will keep rejecting them. All of this is fairly anti-Fossil, which tries very hard to make all clones identical. If Fossil gets anything like this feature, I think it must be done by the hook scripts being somehow sent down with the clone and kept up to date there, so that the hook can run locally before the local blockchain gets modified. (26) By Warren Young (wyoung) on 2020-01-28 19:15:48 in reply to 25 [link] [source] I think it must be done by the hook scripts being somehow sent down with the clone ...which of course introduces portability problems: you have to ensure that the hook scripts will in fact run correctly on all of the clones, else you break those clones. \"But it worked on my machine!\" doesn't fly when your remote contributors stop being able to check work in just because they're on a different host OS, or are missing some third-party Tcl package, or whatever. However, a night's sleep and a shower have provided an answer: instead of the hook script outright rejecting \"bad\" checkins, what if Fossil treats commit hook rejection as a signal to automatically create a branch for the new artifacts so that other work can continue on the original branch? Then the problem can be corrected in public, which is very much pro-Fossil. We could obviously do this in a post-facto way, running such problem-detecting scripts on the server and having it automatically move artifacts, but making it part of the sync protocol with integrated hook scripts means the remote clone could be notified immediately that their check-in has been moved to a branch. The hook script could even return a detailed message explaining the problem, which could guide the remote user to a fix. This new scheme means the content is never lost, it is committed locally and sync'd remotely, but it's set aside until the problem can be resolved. (27) By anonymous on 2020-01-28 21:18:18 in reply to 26 [link] [source] what if Fossil treats commit hook rejection as a signal to automatically create a branch for the new artifacts so that other work can continue on the original branch? I like this idea a lot. but making it part of the sync protocol with integrated hook scripts means the remote clone could be notified immediately that their check-in has been moved to a branch. I would hope that the hook script would be able to supply the new branch name, \"calculated\" by whatever rules the core project team deems appropriate. Maybe the hook script could return a string with a branch name and a message separated by some reasonable delimiter. (28) By Warren Young (wyetr) on 2020-01-28 21:51:28 in reply to 27 [link] [source] the hook script would be able to supply the new branch name Sure. It'd allow a form of namespacing, where each hook script gets its own branch naming prefix so you can tell which script shunted the commit aside. Examples: code-style-abcd1234: code rejected by a site-configured automatic code style checker ci-cd-abcd1234: this commit breaks the CI/CD build; implicitly an async hook temp-code-abcd1234: this commit appears to contain some sort of temporary code: debug logging, FIXME comments, #if 0 blocks, etc. Maybe the hook script could return a string with a branch name and a message separated by some reasonable delimiter. Tcl lets you return a list of values; it isn't limited to returning a single scalar value: return { 401, \"code style check failed\", \"code-style-$nonce\", $output_from_style_checker } That is, an error code, a brief message, a branch name to move the commit to if not null, and an optional long message that expands on the brief message. (16) By Kevin (KevinYouren) on 2019-11-29 23:17:40 in reply to 1.1 [link] [source] Richard, I think \"rebase\" may be useful to entities who want to remove or hide errors or worse. (22) By anonymous on 2019-12-05 19:34:16 in reply to 16 [link] [source] I think \"rebase\" may be useful to entities who want to remove or hide errors or worse. If you develop on a private branch, then merge the final commit to trunk then no one need ever see any mess there might be on the branch. (Down side: Fossil doesn't track merges from a private branch, not even in the repo where the private branch exists.) (23) By Kevin (KevinYouren) on 2019-12-06 01:28:25 in reply to 22 [link] [source] Thank you, I didn't know that. (29.1) Originally by anonymous with edits by Stephan Beal (stephan) on 2020-12-15 16:37:59 from 29.0 in reply to 23 [link] [source] Deleted (30) By Stephan Beal (stephan) on 2020-12-15 16:43:07 in reply to 23 [link] [source] Reminder to moderators: We have had one spammer today who's tried to sneak in hyperlinks to a \"bathroom mold removal\" product as a sneaky link on a single period, making it effectively invisible when reading the markdown-processed post but clearly visible in the unparsed content. Please always check the unparsed content of posts for new and anonymous users before approving them. This particular piece of spam was first posted by a new account (which i deleted) and then again, later in the day, as an anonymous post (which snuck past moderation but i coincidentally recognized as identical to the previous post, so deleted it). The devious part is that such posts appear to be topical, but they try to sneak in spam links which are easy for the eye to overlook. (17) By Saagar Jha (saagarjha) on 2019-11-30 18:11:51 in reply to 1.1 [link] [source] This quote (from Tom Kremenek of the Swift project) is saying that your workflow will be defined and constrained by the tools you use. Might you be talking about Ted, the Swift Project Lead? (18) By Richard Hipp (drh) on 2019-11-30 18:49:20 in reply to 17 [link] [source] Yes, it seems likely that I wrote his name down incorrectly in my notes. Sorry about that, Ted. (19) By anonymous on 2019-12-01 23:45:14 in reply to 1.1 [link] [source] I'd like to copy this post for a personal blogpost (of course with reference) is such doing welcomed and permitted? (20) By Richard Hipp (drh) on 2019-12-02 01:07:45 in reply to 19 [link] [source] (21) By Offray (offray) on 2019-12-02 21:41:56 in reply to 1.2 [link] [source] One GitHub-er asked me: \"What would it take to get SQLite to move off of Fossil and on to Git.\" Just to be clear to everyone reading this: That will never happen. Fossil was specifically designed to support SQLite development and does so remarkably well. Fossil fills a different niche than does Git/GitHub. Fossil will continue to be supported and enhanced by me (as well as others) well into the foreseeable future. You could counter ask him/her: \"What would it take to get full support to Fossil inside GitHub and make it a first class citizen there?\" ;-) Cheers, ",
        "_version_": 1718527437996818432
      },
      {
        "story_id": [21423612],
        "story_author": ["researchiteng"],
        "story_descendants": [31],
        "story_score": [122],
        "story_time": ["2019-11-01T20:56:10Z"],
        "story_title": "Show HN: Own your Kubernetes: installation, addons, best practices – as code",
        "search": [
          "Show HN: Own your Kubernetes: installation, addons, best practices – as code",
          "https://github.com/ReSearchITEng/kubeadm-playbook/",
          "Update Status of the project: Stable kubeadm-playboook ansible project's code is on Github Quick explanation https://medium.com/@re.search.it.eng/batteries-included-kubernetes-for-everyone-bccf9b8558dd What is it: For 3 years we keep on gathering best guidelines and growing this project for best kubernetes cluster installation + addons. It's gluing: pure kubeadm, offical helm charts for various addons, fine-tunings from docs and best practices. All based purely on kubeadm and official helm charts. It tries to bring together most (if not all) the steps to get from a freshly installed linux to a working k8s cluster. Its vision is to find and integrate the best tools out there (while using KISS principle). Why Going beyond minikube, making your own (usually on prem) k8s cluster (with the usuall addons installed) is still too hard or needlessly complex. Kubeadm is so strong now, that complex projects don't make sense. We felt that what was missing was getting things before and after the cluster installation, to get an initial (but reasonable) platform up. What it makes it different: pure kubeadm based (all needless complexity removed); the stronger kubeadm will be, the smaller this project! kubernetes cluster platform: not only k8s, but also the important addons this project does not hold any \"custom\" addon, everything that is installed is fetched directly their official repos (mostly helm repos) drives users towards good practices: e.g. segregate nodes in 3 categories (when possible): masters, infra, compute; (infra holds ingress controller, prometheus, grafana, and similar support tools) optionally, when docker_setup enabled, this project will also setup the docker with known kernel params for os (those from the k8s docs). focused on \"on-prem\" deployments (but still accepts anything kubeadm can do); (vmware vsphere storage integration is actively used). generates any cluster size, from 1 machine cluster (dev env) to productions sizes: all controlled by the provided inventory. scale UP or DOWN post deployment (e.g. start small with 1 vm, then add nodes, then make multi-master) -> all without downtime thanks to kubeadm. Master HA & ingress setups accepts either: VIPs (using keepalived) or Hardware LB (when available); enterprise-friendly: fully tested with http_proxy and private docker registry (usually private nexus registry proxy registry of docker.io, quay.io, k8s.gcr.io, etc; private mirror hostname&port fully configurable in this project) actively tested on both Ubuntu/Debian and CentOS/RHEL. any helm chart can be configured/added/removed via addons.yml (more detailed comparison with other solutions towards the end of this readme) What is in plan Authentication via LDAP (in plan KeyCloak); integrate it in dashboard, grafana, etc. Move from heapster to metrics server (once it will be stable) Logging stack (e.g. EFK - currently helm charts are not fully stable) (PRs are welcome :) Since when Started years back. Battle tested on for all Centos/RHEL 7.2+ till 7.6 and Ubuntu 16.04,18.04,19.10,20.04 (both with overlay2 and automatic docker_setup). Actively used on a daily basis and tested with k8s starting 1.7 till 1.19. Targets/pros&cons Kubeadm simplifies drastically the installation, so for BYO (vms,desktops,baremetal), complex projects like kubespray/kops are not required any longer. Major difference from other projects: it uses kubeadm for all activities, and kubernetes is running in containers. The project is for those who want to create&recreate k8s cluster using the official method (kubeadm), with all production features: creates Highly Available (HA cluster - multi master) (using VIPs) - using kubeadm KISS: it's build for kubeadm only (no other complexities arount it) plays nicely for corporate env: allows use of internal registry for images (insted of using internet connection) plays nicely for corporate env: works via proxy prepares your machines (e.g. kernel params like: net.bridge.bridge-nf-call-iptables, etc.) it tries to use modern methods of deploying the \"addons\". E.g. heapster, ingress, prometheus, etc -> all via helm. Pure and clean: Ingresses (via helm chart) Persistent storage (vsphere/ceph/nfs) (vsphere up to date, rook.io (ceph) needs updates; NFS not actively tested) dashboard 2.0 (via helm chart) metrics-server (via helm chart) supports corporate http proxy supports (corporate/intranet) docker registry mirrors (which should mirror: k8s.gcr.io,docker.io,quay.io) modular, clean code, supporting multiple activies by using ansible tags (e.g. add/reset a subgroup of nodes). optionally help configuring container engine (e.g. docker) This project targets to get a fully working environment in matter of minutes on any hw: baremetal, vms (vsphere, virtualbox), etc. What it does not do: k8s version upgrades: while many of its roles can be used for an upgrade, upgrade should be done using kubeadm tool. Kubeadm upgrade is pretty clear and simple, there is no need for much automation around it. If you think otherwise, let us know. PROS: quick (~10 min) full cluster installation all in one shop for a cluster which you can start working right away, without mastering the details applies fixes for quite few issues currently k8s installers have deploys plugins to all creation of dynamical persistent volumes via: vsphere, rook or self deployed NFS kubeadm is the only official tool specialized to install k8s proxy is supported; It can work even no internet access required (when there is internal registry) CONS/future versions: old k8s versions (13 and older): for HA Master, Only VIP is supported -> LB support for HA Master was not tested (try to use v1.14 and above). While for installing the cluster there is no need for internet access, the addons which come as helm charts by default look for their images on the internet (but charts have to be either cached or come from an internal helm repo). To take images from on-prem, please update the group_vars/all/addons.yaml to point to local registry version of the image. Prerequisites: ansible min. 2.5 (but higher is recommeneded. Tested on 2.5-2.8+) For a perfect experience, one should at least define a wildcard dns subdomain, to easily access the ingresses. The wildcard can pointed to the master (as it's quaranteed to exists). Note: dashboard will by default use the master machine, but also deploy under the provided domain (in parallel, only additional ingress rule) if docker_setup is True, it will also attempt to define your docker and set it up with overlay2 storage driver (one needs CentOS 7.4+) it will set required kernel modules (if desired) if one needs ceph(rook) persistent storage, disks or folders should be prepared and properly sized (e.g. /storage/rook) This playbook will: pre-sanity: docker sanity kernel modules (load & setup for every restart) Install ntp (to keep time in sync within cluster) (control via group_vars/all) Install the kubeadm repo Install kubeadm, kubelet, kubernetes-cni, and kubectl If desired, manipulate SELinux setting (control via group_vars/all) Control/set kubelet cgroup driver, swap-off, and many other settings required by kubelet to work (control via group_vars/all) Reset activities (like kubeadm reset, unmount of /var/lib/kubelet/* mounts, ip link delete cbr0, cni0 , etc.) - important for reinstallations. Initialize the cluster on master with kubeadm init Install user specified pod network from group_vars/all (flannel, calico, weave, etc) Join the nodes to the cluster with 'kubeadm join' and full set of params. Install helm Install nginx ingress controller via helm (control via group_vars/all) Install kubernetes dashboard (via helm) Installs any listed helm charts in the config (via helm) Installs any yaml listed in the config Planned: Install prometheus via Helm (control via group_vars/all) -> prometheus operator helm chart is expected soon, Sanity: checks if nodes are ready and if all pods are running, and provides details of the cluster. when enabled, it will create ceph storage cluster using rook operator when enabled, it will create vsphere persistent storage class and all required setup. Please fill in vcenter u/p/url,etc group_vars/all, and follow all initial steps there. it will define a set of handy aliases NOTE: It does support http_proxy configuration cases. Simply update the your proxy in the group_vars/all. This has been tested with RHEL&CentOS 7.3-7.6 and Ubuntu 16.04 and Kubernetes v1.6.1 - v1.13.4 In general, keep the kube* tools at the same minor version with the desired k8s cluster. (e.g. For installing k8s v1.7 one must also use kubeadm 1.7 (kubeadm limitation).) FYI, higher kube* are usually supported with 1 minor version older cluster (e.g. kube[adm/ctl/let] 1.8.* accepts kubernetes cluster 1.7.*). If for any reason anyone needs to relax RBAC, they can do: kubectl create -f https://github.com/ReSearchITEng/kubeadm-playbook/blob/master/allow-all-all-rbac.yml How To Use: Use the right release/branch Use the release/branch that fits your k8s version needs. While master may have additinal features, it's as tested as the releases. Full cluster (re)installation (reset + install) git clone https://github.com/ReSearchITEng/kubeadm-playbook.git cd kubeadm-playbook/ cp hosts.example hosts vi hosts <add hosts> # Setul vars in group_vars vi group_vars/all/* <modify vars as needed> ansible-playbook -i hosts site.yml [--skip-tags \"docker,prepull_images,kubelet\"] [-f1] If there are any issues, you may want to run only some of the steps, by choosing the appropriate tags to run. Read the site.yml. Here are also some explanations of important steps: reset any previous cluster, delete etcd, cleanup network, etc. (role/tag: reset) common section which prepares all machines (e.g. docker if required, kernel modules, etc) (role: common) install etcd (role/tag: etcd) (requried only when you have HA only) install master (role/tag: master) install nodes (role/tag: node) install network, helm, ingresses, (role/tag: post_deploy) read the docs/Troubleshooting.md Add nodes: modify inventory (hosts file), and leave the primary-master intact, but for nodes, keep ONLY the nodes to be managed (added/reset) ansible-playbook -i hosts only_nodes_only_install.yml --tags node ; More in the docs section. Add nodes in 2 steps: reset node + install node: modify inventory (hosts file), and leave the primary-master intact, but for nodes, keep ONLY the nodes to be managed (added/reset) ansible-playbook -i hosts site.yml --tags node ; More in the docs section. To remove a specific node (drains and afterwards kube resets, etc) modify inventory (hosts file), and leave the master intact, but for nodes, keep ONLY the nodes to be removed ansible-playbook -i hosts site.yml --tags node_reset Other activities possible: There are other operations possible against the cluster, look at the file: site.yml and decide. Few more examples of useful tags: \"--tags reset\" -> which resets the cluster in a safe matter (first removes all helm chars, then cleans all PVs/NFS, drains nodes, etc.) \"--tags helm_reset\" -> which removes all helm charts, and resets the helm. \"--tags cluster_sanity\" -> which does, of course, cluster_sanity and prints cluster details (no changes performed) Playbooks site.yml -> holds all tasks, including reset, install, post_deploy (overlay network, charts install), sanity; This way, site.yml should be for install install of the cluster (where all steps are required). One may use site.yml for maintenance, but always use the tags for the desired actions (on top of keeping only primary-master and desired machines for which actions are targeted) The below playbooks are subsets of the site.yml: all_install.yml -> holds install tasks only (no reset), but for all types of machines ( all_reset.yml -> reset kubernetes related packages and k8s setups in all machines in the inventory) only_nodes_only_install.yml -> runs only install actions only on nodes in the inventory (nothing on masters) only_secondaryMasters_only_install.yml -> runs install actions only on secondary-masters present in the inventory (and nothing on primary-masters or nodes) Check the installation of dashboard The output should have already presented the required info (or run again: ansible-playbook -i hosts site.yml --tags cluster_sanity). The Dashboard is set on the master host, and, additionally, if it was set, also at something like: http://dashboard.cloud.corp.example.com (depending on the configured selected domain entry), and if the wildcard DNS was properly set up *.k8s.cloud.corp.example.com pointing to master machine public IP). e.g. curl -SLk 'http://k8s-master.example.com/#!/overview?namespace=_all' | grep browsehappy Dashboard is also listening on primary hostname, port 443 (or similar if ingress helm params were changed). E.g., if your primary-master is vm01.com, browse: https://vm01.com:443/ Note: The http version (http://vm01.com:80/) will ask for token. For testing the Persistent volume, one may use/tune the files in the demo folder. kubectl exec -it demo-pod -- bash -c \"echo Hello TEST >> /usr/share/nginx/html/index.html \" and check the http://pv.cloud.corp.example.com page. load-ballancing For LB, one may want to check also: github.com/cloudnativelabs/kube-router/wiki & https://github.com/cloudnativelabs/kube-router/blob/master/docs/kubeadm.md & https://github.com/cloudnativelabs/kube-router/blob/master/docs/how-it-works.md https://github.com/google/metallb/ (implements a LB type) https://github.com/kubernetes/contrib/tree/master/keepalived-vip (HA) https://github.com/kubernetes/contrib/tree/master/service-loadbalancer DEMO: Installation demo k8s 1.16 on Ubuntu 18.04: Vagrant For using vagrant on one or multiple machines with bridged interface (public_network and ports accessible) all machines must have 1st interface as the bridged interface (so k8s processes will bind automatically to it). For this, use this script: vagrant_bridged_demo.sh. Steps to start Vagrant deployment: edit ./Vagrant file and set desired number of machines, sizing, etc. run: ./vagrant_bridged_demo.sh --full [ --bridged_adapter <desired host interface|auto> ] # bridged_adapter defaults to ip route | grep default | head -1 After preparations (edit group_vars/all, etc.), run the ansible installation normally. Using vagrant keeping NAT as 1st interface (usually with only one machine) was not tested and the Vagrantfile may requires some changes. There was no focus on this option as it's more complicated to use afterwards: one must export the ports manually to access ingresses like dashboard from the browser, and usually does not support more than one machine. kubeadm-ha Starting 1.14/1.15, kubeadm supports multimaster (aka HA) setup easy (out of the box), so no special setup. (Our playbook supports master HA also for older v1.11-v1.13, thanks to projects like: https://github.com/mbert/kubeadm2ha ( and https://github.com/sv01a/ansible-kubeadm-ha-cluster and/or github.com/cookeem/kubeadm-ha ). How does it compare to other projects: Kubeadm -> the official k8s installer With kubeadm-playbook we are focus only kubeadm. Pros: as it's the official k8s installation tool kubeadm is released with every k8s release, and you have a guarantee to be in sync with the official code. self hosted deployment, making upgrades very smooth ; Here is a KubeCon talk presenting even more reasons to go with self-hosted k8s: https://www.youtube.com/watch?v=jIZ8NaR7msI Cons: k8s cluster ugprades are not (yet) in plan, (as kubeadm upgrade is too simple (and sensitive) to need automation) when you run the playbook against an existing cluster, by default it will rebuild the entire cluster. Alternativelly, one has to use the ansible \"--tags\" to specify what exactly is desired (E.g. ansible-playbook -i hosts -v site.yml --tags post_deploy ) Other k8s installers Similar k8s install on physical/vagrant/vms (byo - on premises) projects you may want to check, but all below are without kubeadm (as opposed to this project) https://github.com/kubernetes/contrib/tree/master/ansible -> the official k8s ansible, but without kubeadm, therefore the processes will run on the nodes, not in docker containers https://github.com/dcj/ansible-kubeadm-cluster -> very simple cluster, does not (currently) have: ingresses, helm, addons, proxy support, vagrant support, persistent volumes, etc. https://github.com/apprenda/kismatic -> very big project by apprenda, it supports cluster upgrades https://github.com/kubernetes-incubator/kargo -> plans to use kubeadm in the future, for the activities kubeadm can do. https://github.com/gluster/gluster-kubernetes/blob/master/vagrant/ -> it's much more simple, no ingress, helm, addons, proxy support, and persistent volumes only using glusterfs. Entire project is only focused on CentOS. https://github.com/kubernetes-incubator/kubespray & https://github.com/kubernetes/kops (amazon) -> Neither of them used the official installtion tool: kubeadm; Updates: as of 2019 kubespray accepts kubeadm (to be checked if kubespray was fully redesigned around kubeadm or adopted as an option). As of May 2019: our projects accepts also master-HA using only kubeadm 1.14, with no other \"magic\" around. Bonus goodies: other_tools/ hold scripts like k8s cli which installs easily kubectx, krew, kubeval, etc. The docs folder hold info on how to secure cluster using operators in an elegant manner (along with aqua's set of security tests) PRs are accepted and welcome. PS: work inspired from: @sjenning - and the master ha part from @mbert. PRs & suggestions from: @carlosedp - Thanks. URL page of kubeadm-playboook ansible project kubeadm-playboook ansible project's code is on Github Our story: https://medium.com/@re.search.it.eng/batteries-included-kubernetes-for-everyone-bccf9b8558dd License: Public Domain ",
          "Getting (aka renting) a k8s from a cloud vendor is probably the most common case, but does not cover everyone's scenarios => hence this project.<p>Along the years we actively looked for best tools and practices and incorporated them in this project. We try to bring it closer to a common/initial generic k8s based platform. (it does not plan to compete with RH's OKD, but only it's basic features: out of the box network, ingress controller, monitoring, HighAvailability, etc).\nAuthentication, better security hardening, logging are in future plans.<p>Do you find it useful now, when one can simply pay jump to gke/eks/aks/pks ?<p>If so, what should be the next steps to make it a successful project (measured by users and community around it)",
          "An alternative is Rancher. It can provision clusters with the major cloud providers' managed Kubernetes services as well as with virtually any infrastructure. I wrote a post on setting up a cluster with code using Terraform, Ansible and Rancher <a href=\"https://vitobotta.com/2019/10/14/kubernetes-hetzner-cloud-terraform-ansible-rancher/\" rel=\"nofollow\">https://vitobotta.com/2019/10/14/kubernetes-hetzner-cloud-te...</a> - with this approach you can have more resources for less money compared to managed services. Having said that, I am now using Digital Ocean's managed Kubernetes service because managing a cluster can be quite a bit of work for a single person like me. By the way there's also K3s (also from Rancher) which makes it easier to spin up a cluster on your own servers."
        ],
        "story_type": ["ShowHN"],
        "url": "https://github.com/ReSearchITEng/kubeadm-playbook/",
        "comments.comment_id": [21423822, 21427290],
        "comments.comment_author": ["researchiteng", "SkyLinx"],
        "comments.comment_descendants": [0, 1],
        "comments.comment_time": [
          "2019-11-01T21:21:18Z",
          "2019-11-02T12:34:28Z"
        ],
        "comments.comment_text": [
          "Getting (aka renting) a k8s from a cloud vendor is probably the most common case, but does not cover everyone's scenarios => hence this project.<p>Along the years we actively looked for best tools and practices and incorporated them in this project. We try to bring it closer to a common/initial generic k8s based platform. (it does not plan to compete with RH's OKD, but only it's basic features: out of the box network, ingress controller, monitoring, HighAvailability, etc).\nAuthentication, better security hardening, logging are in future plans.<p>Do you find it useful now, when one can simply pay jump to gke/eks/aks/pks ?<p>If so, what should be the next steps to make it a successful project (measured by users and community around it)",
          "An alternative is Rancher. It can provision clusters with the major cloud providers' managed Kubernetes services as well as with virtually any infrastructure. I wrote a post on setting up a cluster with code using Terraform, Ansible and Rancher <a href=\"https://vitobotta.com/2019/10/14/kubernetes-hetzner-cloud-terraform-ansible-rancher/\" rel=\"nofollow\">https://vitobotta.com/2019/10/14/kubernetes-hetzner-cloud-te...</a> - with this approach you can have more resources for less money compared to managed services. Having said that, I am now using Digital Ocean's managed Kubernetes service because managing a cluster can be quite a bit of work for a single person like me. By the way there's also K3s (also from Rancher) which makes it easier to spin up a cluster on your own servers."
        ],
        "id": "c4c57c33-eccc-4a5a-b40e-94f4f4f2ceb9",
        "url_text": "Update Status of the project: Stable kubeadm-playboook ansible project's code is on Github Quick explanation https://medium.com/@re.search.it.eng/batteries-included-kubernetes-for-everyone-bccf9b8558dd What is it: For 3 years we keep on gathering best guidelines and growing this project for best kubernetes cluster installation + addons. It's gluing: pure kubeadm, offical helm charts for various addons, fine-tunings from docs and best practices. All based purely on kubeadm and official helm charts. It tries to bring together most (if not all) the steps to get from a freshly installed linux to a working k8s cluster. Its vision is to find and integrate the best tools out there (while using KISS principle). Why Going beyond minikube, making your own (usually on prem) k8s cluster (with the usuall addons installed) is still too hard or needlessly complex. Kubeadm is so strong now, that complex projects don't make sense. We felt that what was missing was getting things before and after the cluster installation, to get an initial (but reasonable) platform up. What it makes it different: pure kubeadm based (all needless complexity removed); the stronger kubeadm will be, the smaller this project! kubernetes cluster platform: not only k8s, but also the important addons this project does not hold any \"custom\" addon, everything that is installed is fetched directly their official repos (mostly helm repos) drives users towards good practices: e.g. segregate nodes in 3 categories (when possible): masters, infra, compute; (infra holds ingress controller, prometheus, grafana, and similar support tools) optionally, when docker_setup enabled, this project will also setup the docker with known kernel params for os (those from the k8s docs). focused on \"on-prem\" deployments (but still accepts anything kubeadm can do); (vmware vsphere storage integration is actively used). generates any cluster size, from 1 machine cluster (dev env) to productions sizes: all controlled by the provided inventory. scale UP or DOWN post deployment (e.g. start small with 1 vm, then add nodes, then make multi-master) -> all without downtime thanks to kubeadm. Master HA & ingress setups accepts either: VIPs (using keepalived) or Hardware LB (when available); enterprise-friendly: fully tested with http_proxy and private docker registry (usually private nexus registry proxy registry of docker.io, quay.io, k8s.gcr.io, etc; private mirror hostname&port fully configurable in this project) actively tested on both Ubuntu/Debian and CentOS/RHEL. any helm chart can be configured/added/removed via addons.yml (more detailed comparison with other solutions towards the end of this readme) What is in plan Authentication via LDAP (in plan KeyCloak); integrate it in dashboard, grafana, etc. Move from heapster to metrics server (once it will be stable) Logging stack (e.g. EFK - currently helm charts are not fully stable) (PRs are welcome :) Since when Started years back. Battle tested on for all Centos/RHEL 7.2+ till 7.6 and Ubuntu 16.04,18.04,19.10,20.04 (both with overlay2 and automatic docker_setup). Actively used on a daily basis and tested with k8s starting 1.7 till 1.19. Targets/pros&cons Kubeadm simplifies drastically the installation, so for BYO (vms,desktops,baremetal), complex projects like kubespray/kops are not required any longer. Major difference from other projects: it uses kubeadm for all activities, and kubernetes is running in containers. The project is for those who want to create&recreate k8s cluster using the official method (kubeadm), with all production features: creates Highly Available (HA cluster - multi master) (using VIPs) - using kubeadm KISS: it's build for kubeadm only (no other complexities arount it) plays nicely for corporate env: allows use of internal registry for images (insted of using internet connection) plays nicely for corporate env: works via proxy prepares your machines (e.g. kernel params like: net.bridge.bridge-nf-call-iptables, etc.) it tries to use modern methods of deploying the \"addons\". E.g. heapster, ingress, prometheus, etc -> all via helm. Pure and clean: Ingresses (via helm chart) Persistent storage (vsphere/ceph/nfs) (vsphere up to date, rook.io (ceph) needs updates; NFS not actively tested) dashboard 2.0 (via helm chart) metrics-server (via helm chart) supports corporate http proxy supports (corporate/intranet) docker registry mirrors (which should mirror: k8s.gcr.io,docker.io,quay.io) modular, clean code, supporting multiple activies by using ansible tags (e.g. add/reset a subgroup of nodes). optionally help configuring container engine (e.g. docker) This project targets to get a fully working environment in matter of minutes on any hw: baremetal, vms (vsphere, virtualbox), etc. What it does not do: k8s version upgrades: while many of its roles can be used for an upgrade, upgrade should be done using kubeadm tool. Kubeadm upgrade is pretty clear and simple, there is no need for much automation around it. If you think otherwise, let us know. PROS: quick (~10 min) full cluster installation all in one shop for a cluster which you can start working right away, without mastering the details applies fixes for quite few issues currently k8s installers have deploys plugins to all creation of dynamical persistent volumes via: vsphere, rook or self deployed NFS kubeadm is the only official tool specialized to install k8s proxy is supported; It can work even no internet access required (when there is internal registry) CONS/future versions: old k8s versions (13 and older): for HA Master, Only VIP is supported -> LB support for HA Master was not tested (try to use v1.14 and above). While for installing the cluster there is no need for internet access, the addons which come as helm charts by default look for their images on the internet (but charts have to be either cached or come from an internal helm repo). To take images from on-prem, please update the group_vars/all/addons.yaml to point to local registry version of the image. Prerequisites: ansible min. 2.5 (but higher is recommeneded. Tested on 2.5-2.8+) For a perfect experience, one should at least define a wildcard dns subdomain, to easily access the ingresses. The wildcard can pointed to the master (as it's quaranteed to exists). Note: dashboard will by default use the master machine, but also deploy under the provided domain (in parallel, only additional ingress rule) if docker_setup is True, it will also attempt to define your docker and set it up with overlay2 storage driver (one needs CentOS 7.4+) it will set required kernel modules (if desired) if one needs ceph(rook) persistent storage, disks or folders should be prepared and properly sized (e.g. /storage/rook) This playbook will: pre-sanity: docker sanity kernel modules (load & setup for every restart) Install ntp (to keep time in sync within cluster) (control via group_vars/all) Install the kubeadm repo Install kubeadm, kubelet, kubernetes-cni, and kubectl If desired, manipulate SELinux setting (control via group_vars/all) Control/set kubelet cgroup driver, swap-off, and many other settings required by kubelet to work (control via group_vars/all) Reset activities (like kubeadm reset, unmount of /var/lib/kubelet/* mounts, ip link delete cbr0, cni0 , etc.) - important for reinstallations. Initialize the cluster on master with kubeadm init Install user specified pod network from group_vars/all (flannel, calico, weave, etc) Join the nodes to the cluster with 'kubeadm join' and full set of params. Install helm Install nginx ingress controller via helm (control via group_vars/all) Install kubernetes dashboard (via helm) Installs any listed helm charts in the config (via helm) Installs any yaml listed in the config Planned: Install prometheus via Helm (control via group_vars/all) -> prometheus operator helm chart is expected soon, Sanity: checks if nodes are ready and if all pods are running, and provides details of the cluster. when enabled, it will create ceph storage cluster using rook operator when enabled, it will create vsphere persistent storage class and all required setup. Please fill in vcenter u/p/url,etc group_vars/all, and follow all initial steps there. it will define a set of handy aliases NOTE: It does support http_proxy configuration cases. Simply update the your proxy in the group_vars/all. This has been tested with RHEL&CentOS 7.3-7.6 and Ubuntu 16.04 and Kubernetes v1.6.1 - v1.13.4 In general, keep the kube* tools at the same minor version with the desired k8s cluster. (e.g. For installing k8s v1.7 one must also use kubeadm 1.7 (kubeadm limitation).) FYI, higher kube* are usually supported with 1 minor version older cluster (e.g. kube[adm/ctl/let] 1.8.* accepts kubernetes cluster 1.7.*). If for any reason anyone needs to relax RBAC, they can do: kubectl create -f https://github.com/ReSearchITEng/kubeadm-playbook/blob/master/allow-all-all-rbac.yml How To Use: Use the right release/branch Use the release/branch that fits your k8s version needs. While master may have additinal features, it's as tested as the releases. Full cluster (re)installation (reset + install) git clone https://github.com/ReSearchITEng/kubeadm-playbook.git cd kubeadm-playbook/ cp hosts.example hosts vi hosts <add hosts> # Setul vars in group_vars vi group_vars/all/* <modify vars as needed> ansible-playbook -i hosts site.yml [--skip-tags \"docker,prepull_images,kubelet\"] [-f1] If there are any issues, you may want to run only some of the steps, by choosing the appropriate tags to run. Read the site.yml. Here are also some explanations of important steps: reset any previous cluster, delete etcd, cleanup network, etc. (role/tag: reset) common section which prepares all machines (e.g. docker if required, kernel modules, etc) (role: common) install etcd (role/tag: etcd) (requried only when you have HA only) install master (role/tag: master) install nodes (role/tag: node) install network, helm, ingresses, (role/tag: post_deploy) read the docs/Troubleshooting.md Add nodes: modify inventory (hosts file), and leave the primary-master intact, but for nodes, keep ONLY the nodes to be managed (added/reset) ansible-playbook -i hosts only_nodes_only_install.yml --tags node ; More in the docs section. Add nodes in 2 steps: reset node + install node: modify inventory (hosts file), and leave the primary-master intact, but for nodes, keep ONLY the nodes to be managed (added/reset) ansible-playbook -i hosts site.yml --tags node ; More in the docs section. To remove a specific node (drains and afterwards kube resets, etc) modify inventory (hosts file), and leave the master intact, but for nodes, keep ONLY the nodes to be removed ansible-playbook -i hosts site.yml --tags node_reset Other activities possible: There are other operations possible against the cluster, look at the file: site.yml and decide. Few more examples of useful tags: \"--tags reset\" -> which resets the cluster in a safe matter (first removes all helm chars, then cleans all PVs/NFS, drains nodes, etc.) \"--tags helm_reset\" -> which removes all helm charts, and resets the helm. \"--tags cluster_sanity\" -> which does, of course, cluster_sanity and prints cluster details (no changes performed) Playbooks site.yml -> holds all tasks, including reset, install, post_deploy (overlay network, charts install), sanity; This way, site.yml should be for install install of the cluster (where all steps are required). One may use site.yml for maintenance, but always use the tags for the desired actions (on top of keeping only primary-master and desired machines for which actions are targeted) The below playbooks are subsets of the site.yml: all_install.yml -> holds install tasks only (no reset), but for all types of machines ( all_reset.yml -> reset kubernetes related packages and k8s setups in all machines in the inventory) only_nodes_only_install.yml -> runs only install actions only on nodes in the inventory (nothing on masters) only_secondaryMasters_only_install.yml -> runs install actions only on secondary-masters present in the inventory (and nothing on primary-masters or nodes) Check the installation of dashboard The output should have already presented the required info (or run again: ansible-playbook -i hosts site.yml --tags cluster_sanity). The Dashboard is set on the master host, and, additionally, if it was set, also at something like: http://dashboard.cloud.corp.example.com (depending on the configured selected domain entry), and if the wildcard DNS was properly set up *.k8s.cloud.corp.example.com pointing to master machine public IP). e.g. curl -SLk 'http://k8s-master.example.com/#!/overview?namespace=_all' | grep browsehappy Dashboard is also listening on primary hostname, port 443 (or similar if ingress helm params were changed). E.g., if your primary-master is vm01.com, browse: https://vm01.com:443/ Note: The http version (http://vm01.com:80/) will ask for token. For testing the Persistent volume, one may use/tune the files in the demo folder. kubectl exec -it demo-pod -- bash -c \"echo Hello TEST >> /usr/share/nginx/html/index.html \" and check the http://pv.cloud.corp.example.com page. load-ballancing For LB, one may want to check also: github.com/cloudnativelabs/kube-router/wiki & https://github.com/cloudnativelabs/kube-router/blob/master/docs/kubeadm.md & https://github.com/cloudnativelabs/kube-router/blob/master/docs/how-it-works.md https://github.com/google/metallb/ (implements a LB type) https://github.com/kubernetes/contrib/tree/master/keepalived-vip (HA) https://github.com/kubernetes/contrib/tree/master/service-loadbalancer DEMO: Installation demo k8s 1.16 on Ubuntu 18.04: Vagrant For using vagrant on one or multiple machines with bridged interface (public_network and ports accessible) all machines must have 1st interface as the bridged interface (so k8s processes will bind automatically to it). For this, use this script: vagrant_bridged_demo.sh. Steps to start Vagrant deployment: edit ./Vagrant file and set desired number of machines, sizing, etc. run: ./vagrant_bridged_demo.sh --full [ --bridged_adapter <desired host interface|auto> ] # bridged_adapter defaults to ip route | grep default | head -1 After preparations (edit group_vars/all, etc.), run the ansible installation normally. Using vagrant keeping NAT as 1st interface (usually with only one machine) was not tested and the Vagrantfile may requires some changes. There was no focus on this option as it's more complicated to use afterwards: one must export the ports manually to access ingresses like dashboard from the browser, and usually does not support more than one machine. kubeadm-ha Starting 1.14/1.15, kubeadm supports multimaster (aka HA) setup easy (out of the box), so no special setup. (Our playbook supports master HA also for older v1.11-v1.13, thanks to projects like: https://github.com/mbert/kubeadm2ha ( and https://github.com/sv01a/ansible-kubeadm-ha-cluster and/or github.com/cookeem/kubeadm-ha ). How does it compare to other projects: Kubeadm -> the official k8s installer With kubeadm-playbook we are focus only kubeadm. Pros: as it's the official k8s installation tool kubeadm is released with every k8s release, and you have a guarantee to be in sync with the official code. self hosted deployment, making upgrades very smooth ; Here is a KubeCon talk presenting even more reasons to go with self-hosted k8s: https://www.youtube.com/watch?v=jIZ8NaR7msI Cons: k8s cluster ugprades are not (yet) in plan, (as kubeadm upgrade is too simple (and sensitive) to need automation) when you run the playbook against an existing cluster, by default it will rebuild the entire cluster. Alternativelly, one has to use the ansible \"--tags\" to specify what exactly is desired (E.g. ansible-playbook -i hosts -v site.yml --tags post_deploy ) Other k8s installers Similar k8s install on physical/vagrant/vms (byo - on premises) projects you may want to check, but all below are without kubeadm (as opposed to this project) https://github.com/kubernetes/contrib/tree/master/ansible -> the official k8s ansible, but without kubeadm, therefore the processes will run on the nodes, not in docker containers https://github.com/dcj/ansible-kubeadm-cluster -> very simple cluster, does not (currently) have: ingresses, helm, addons, proxy support, vagrant support, persistent volumes, etc. https://github.com/apprenda/kismatic -> very big project by apprenda, it supports cluster upgrades https://github.com/kubernetes-incubator/kargo -> plans to use kubeadm in the future, for the activities kubeadm can do. https://github.com/gluster/gluster-kubernetes/blob/master/vagrant/ -> it's much more simple, no ingress, helm, addons, proxy support, and persistent volumes only using glusterfs. Entire project is only focused on CentOS. https://github.com/kubernetes-incubator/kubespray & https://github.com/kubernetes/kops (amazon) -> Neither of them used the official installtion tool: kubeadm; Updates: as of 2019 kubespray accepts kubeadm (to be checked if kubespray was fully redesigned around kubeadm or adopted as an option). As of May 2019: our projects accepts also master-HA using only kubeadm 1.14, with no other \"magic\" around. Bonus goodies: other_tools/ hold scripts like k8s cli which installs easily kubectx, krew, kubeval, etc. The docs folder hold info on how to secure cluster using operators in an elegant manner (along with aqua's set of security tests) PRs are accepted and welcome. PS: work inspired from: @sjenning - and the master ha part from @mbert. PRs & suggestions from: @carlosedp - Thanks. URL page of kubeadm-playboook ansible project kubeadm-playboook ansible project's code is on Github Our story: https://medium.com/@re.search.it.eng/batteries-included-kubernetes-for-everyone-bccf9b8558dd License: Public Domain ",
        "_version_": 1718527433853894656
      },
      {
        "story_id": [20574797],
        "story_author": ["ingve"],
        "story_descendants": [49],
        "story_score": [103],
        "story_time": ["2019-07-31T14:46:21Z"],
        "story_title": "Lefthook: Knock your team’s code back into shape",
        "search": [
          "Lefthook: Knock your team’s code back into shape",
          "https://evilmartians.com/chronicles/lefthook-knock-your-teams-code-back-into-shape",
          "Meet Lefthook, thefastest polyglot Git hooks manager out there, andmake sure not asingle line ofunruly code makes it into production. See how easy it istoinstall Lefthook, recently adopted by Discourse, Logux, andOpenstax, for most common frontend andbackend environments andensure all developers onyour team can rely onasingle flexible tool. Andit also has emojis Days, when asingle piece ofsoftware that millions rely onwas created by asingle developer inanivory tower, are long gone. Even Git, universally believed tobethebrainchild ofLinus Torvalds alone, was created with thehelp ofcontributors andisnow being maintained by ateam ofdozens. No matter ifyou work onanopen source project with thewhole world being your oyster, oryou are blooming inawalled garden ofproprietary commercial softwareyou still work inateam. Andeven with awell-organized system ofpull requests andcode reviews maintaining thecode quality across thelarge codebase with dozens ofcontributors isnot aneasy task. Hook me up Hooksways tofire off custom scripts when certain important actions (commit, push, etc.) occurare baked right into Git, so ifyou are comfortable with Bash andtheinternals oftheworld most popular version control systemyou dont need any external tools per se: just edit ./.git/hooks/pre-commit andput insome well-formed script that will, for instance, lint your files before you commit. However, when you work onaproject, you are most interested inwriting projects codenot thecode that checks it. Intheworld ofmodern web development tooling iseverything, andmyriads oftools exist for asingle reason: reducing overhead andcomplexity. Git hooks are not theexception: inJavaScript community, theweapon ofchoice isHusky with Webpack, Babel, andcreate-react-app relying onthis Node-based tool; Rails-centric backend world, however, ismostly ruled by Overcommit that comes as aRuby gem. Find detailed comparisons ofLefthook with other tools intheprojects wiki. Both tools are excellent intheir regard, but inamixed team offrontend andbackend developers, as Evil Martians are, you will often end up having two separate setups for Ruby andJavaScript with frontenders andbackenders linting their commits each intheir way. With Lefthook, you dont need tothink twiceits asingle Go binary that has wrappers both for JavaScript andfor Ruby. It can also beused as astandalone tool for any other environment. For most common use cases, Lefthook requires zero setup. Go language makes Lefthook lightning-fast andprovides support for concurrently executed scripts out ofthebox. Thefact that theexecutable isasingle machine code binary also removes theneed for minding external dependencies (Husky + lint-staged add roughly fifteen hundred dependencies toyour node_modules). It also removes theheadache ofreinstalling dependencies after each update ofyour development environment (try running aglobally installed Ruby gem with another version ofRuby). With Lefthook mentioned either inpackage.json orGemfile, andalefthook.yml configured intheprojects root (see examples below) thetool will beinstalled andused against your code automatically onthenext git pull, yarn install/ bundle install andgit add/git commitwith zero overhead for new contributors. An extensive README describes all possible usage scenarios. Thestraightforward syntax for configuration does not hide actual commands being run by Lefthookmaking sure that nothing funny ishappening below thebelt. Discourse with apunch Discoursean incredibly popular open source platform for forum-style discussionshas recently transitioned from Overcommit toLefthook andnever looked back. With almost 700 contributors authoring 34K commits andcounting, running linters onall new contributions isapriority. With Overcommit though, team members had toremind newcomers toinstall required tools continually Now with @arkweid/lefthook being adev dependency intheprojects package.json, nosetup isnecessary for new contributors. Lefthook allowed tohalf theamount oftime that pre-commit scripts take onlocalhost. The PR that changed theGit hook manager required, inessence, changing .overcommit.yml tolefthook.yml. Ifyou compare themyou will see that Lefthooks configuration ismuch more explicit while theOvercommits one relies mostly onthemagic ofplugins. Discourses CI output before andafter Lefthook Besides changing theway theoutput looksLefthook offers anice summary ofeverything it doesLefthook allowed tohalf theamount oftime that pre-commit scripts take onlocalhost, andincrease theCI run by 20% (on CI environments with better support for parallel execution thegain can beconsiderably more). A pretty bonus Round one Everything Lefthook needs tofunctionis alefthook binary installed somewhere inyour system (either globally orlocally), andalefthook.yml file inaproject root. The binary can either beinstalled globally (with Homebrew for macOS, snap for Ubuntu, AUR for Arch, orgo get anywhere), orlisted as adevelopment dependency either inRubys Gemfile, orNode.js package.json. If you are configuring Lefthook for thefirst time inyour projectyou need tochoose between these options, depending onyour preferences. Themain upside ofputting lefthook inyour Gemfile or@arkweid/lefthook inyour package.json isthat you dont need toworry that your contributors may not have lefthook installed system-wideafter thenext bundle install oryarn install thebinary will beinplace. After you have lefthook inyour systemrun lefthook install intheprojects root togenerate lefthook.yml andvisit theprojects repo for examples onhow touse thesyntax, here isaparticularly full one. Here ishow thecode describing actions oneach pre-commit (right after you type git commit -m \"new feature\" but right before it gets committed) might look like: pre-commit: commands: stylelint: tags: frontend style glob: \"*.{js}\" run: yarn stylelint {staged_files} rubocop: tags: backend style glob: \"*.{rb}\" exclude: \"application.rb|routes.rb\" run: bundle exec rubocop {all_files} scripts: \"good_job.js\": runner: node Then commit lefthook.yml toyour repository andpush it toyour favorite remotenow everyone onyour team will see it inaction every time they commit code. Blow by blow If you want tocheck Lefthook out onademo project quicklywe recommend cloning theevil_chat repositorya project we build inour celebrated Modern Frontend inRails series ofposts (Pt. 1, Pt. 2, Pt. 3). In this project, we use pre-commit hooks configured with Lefthook for formatting JavaScript andCSS files with Prettier, andlinting them with ESlint andstylelint. Heres how toquickly see Lefthook inaction. First, clone therepo andrun package managers: $ git clone git@github.com:demiazz/evil_chat.git $ bundle && yarn Now, go andbreak some CSS orJS inany .pcss or.js files. $ git add . && git commit -m \"Now I am become death, destroyer of worlds\" Wait for it! If all goes well (meaning you succeeded inbreaking thecode), this iswhat you are going tosee: A bad commit The failing scripts are shown with aboxing glove emoji ontheoutputgiving you areal left hook tobring back your attention! For now, our linting covers only thefrontend part oftheapp. What about adding some good old Rubocop tothemix? Edit your lefthook.yml toinclude thefollowing lines: # lefthook.yml pre-commit: parallel: true # tell Lefthook to utilise all cores commands: js: glob: \"*.js\" run: yarn prettier --write {staged_files} && yarn eslint {staged_files} && git add {staged_files} css: glob: \"*.{css,pcss}\" run: yarn prettier --write {staged_files} && yarn stylelint --fix {staged_files} && git add {staged_files} # Add these lines rubocop: glob: \"*.{rb}\" run: rubocop {staged_files} --parallel Note thehandy {staged_files} shortcut that allows you totarget only thefiles that are staged for acurrent commit. Now go back tofix your JS andCSS andcommit astyle offense inany oftheRuby files (yes, you have our permission). Feel free tothrow insome comments here andthere so that git picks up changes for different filetypes. Rubocop comes into play Now CSS andJS are fine, but Ruby needs another look, hence theleft hook! Roll with thepunches Here isthesummary ofthefeatures that make Lefthook stand out ofcompetition, andbring flexibility toyour workflow, see thecomplete list here. Speed Lefthook squeezes out every bit ofparallelism from your machine (or aCI server), you only need totoggle one setting: parallel: true. Heres theconfig file that describes alint series ofcommands that you can run with lefthook run lint from your command line. These are thesame commands that Discourse used torun onTravis. Lefthook gives you anability torun custom tasks like that. As analternative, you can set thesame commands toberun onpre-commit, pre-push, post-checkout, post-merge, orany other available Git hooks. # lefthook.yml lint: # parallel: true commands: rubocop: run: bundle exec rubocop --parallel prettier: run: yarn prettier --list-different \"app/assets/stylesheets/**/*.scss\" \"app/assets/javascripts/**/*.es6\" \"test/javascripts/**/*.es6\" eslint-assets: run: yarn eslint --ext .es6 app/assets/javascripts eslint-test: run: yarn eslint --ext .es6 test/javascripts eslint-plugins-assets: run: yarn eslint --ext .es6 plugins/**/assets/javascripts eslint-plugins-test: run: yarn eslint --ext .es6 plugins/**/test/javascripts eslint-assets-tests: run: yarn eslint app/assets/javascripts test/javascripts With parallel: true commented out, onmy system, this task takes over 30 seconds. With theparallel feature turned on, it takes 15.5 secondstwice as fast! Flexibility Direct control If you want torun your hook directly, without waiting for aGit action: $ lefthook run pre-commit Flexible lists offiles You can use built-in shortcuts {staged_files} and{all_files}, ordefine your own lists according tospecific selection. pre-commit: commands: frontend-linter: run: yarn eslint {staged_files} backend-linter: run: bundle exec rubocop {all_files} frontend-style: files: git diff --name-only HEAD @{push} run: yarn stylelint {files} Glob/Regex filters If you want tofilter alist offiles onthefly with aglob oraRegex. pre-commit: commands: backend-linter: glob: \"*.{rb}\" # glob filter exclude: \"application.rb|routes.rb\" # regexp filter run: bundle exec rubocop {all_files} Run your own scripts If one-liners are not enough, you can tell Lefthook toexecute custom scripts. commit-msg: scripts: \"good_job\": runner: bash Tags andlocal config for even more flexibility You can group your tasks by tags andthen exclude them when you run hooks locally (e.g., you are abackend developer andnot interested inrunning tasks onfrontend code). With Lefthook, you can create alefthook-local.yml file inyour project root (dont forget toadd it toyour .gitignore): all thesettings described here would override theones from themain lefthook.yml. Now you can assign tags todifferent series ofcommands # lefthook.yml pre-push: commands: stylelint: tags: frontend-style # a tag files: git diff --name-only master glob: \"*.{js}\" run: yarn stylelint {files} rubocop: tags: backend-style # a tag files: git diff --name-only master glob: \"*.{rb}\" run: bundle exec rubocop {files} andexclude them from being run locally: # lefthook-local.yml pre-push: exclude_tags: - frontend-style K.O. Do you use Docker for local development? Perhaps you do, perhaps not yet, but there isabig chance that someone else onyour team does otherwise. Some people embrace containerized development fully, while others prefer torely onwell-groomed local environments. Your main lefthook.yml may contain thefollowing: post-push: scripts: \"good_job.js\": runner: bash However, you want torun thesame task inaDocker container, but you dont want tomess up thesetup for everyone else. By using alefthook-local.yml file (the one you dont check ininto version control), you can alter thecommand just slightly, andjust for your local setup, by using a{cmd} shortcut, just like that: # lefthook-local.yml pre-commit: scripts: \"good_job.js\": runner: docker exec -it --rm <container_id_or_name> {cmd} {cmd} will bereplaced by acommand from themain config. The resulting command will look like this: docker exec -it --rm <container_id_or_name> node good_job.js 8 9 10 Knockout! We are confident that Lefthook iscurrently thefastest andmost flexible Git hook manager inour galaxy, so we encourage everyone tofollow theexample ofDiscourse andeither add Lefthook toyour projects orcreate apull request proposing thechange inyour favorite open source repositories. The polyglot nature ofLefthook allows it tobeused inpure frontend, pure backend, ormixed full-stack teams, andwith all common development setups onall major operating systems, including Windows. Find thebest way toinstall it, depending onyour stack, andgive it ago! See how we use Lefthook intandem with Crystalball inour commercial projects by checking out this post onDev.to. If you see our work onLefthook as yet another perfect example ofreinventing thewheel, still give it atryyou will soon realize that Lefthook ismore ofajet pack, then yet another good old wheel for git hooks management. And never, never throw inthetowel onautomating your Git andGitHub workflow! ",
          "Love these kinds of tools.  What is different between husky / git-staged and this?<p>It’s faster and smaller?<p>Quite happy with husky and running tslint/tests with husky on precommit currently.  Curious what the biggest benefits are from left hook and if it’s a pain to switch to if they are amazing benefits.<p>The biggest annoyance with current setup is on prem ci with Jenkins and GitHub Enterprise and complex docker bash scripts embedded in Jenkins.  It’s a pain to get lint/test results to show up in the GitHub pull request ui.  Doesn’t seem like this tool would help make that use case faster to set up.",
          "Generally I'm of the opinion that git hooks are not the right place to place linters or formatters.<p>For a JVM language, it means that you'll spin up a JVM for each step (and you can well have 15 formatters + linters in your stack).<p>So, the formatter better lives in a dev-only part of your codebase, so it's always loaded, interacting with your codebase using _runtime insights_ otherwise impossible to gather, and it's easily hackable.<p>I've authored this kind of solution in the past. It works great, and a proper CI pipeline (i.e. including linters) kills the other half of the problem."
        ],
        "story_type": ["Normal"],
        "url": "https://evilmartians.com/chronicles/lefthook-knock-your-teams-code-back-into-shape",
        "comments.comment_id": [20576448, 20579361],
        "comments.comment_author": ["davidjnelson", "vemv"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-07-31T17:34:50Z",
          "2019-07-31T22:42:48Z"
        ],
        "comments.comment_text": [
          "Love these kinds of tools.  What is different between husky / git-staged and this?<p>It’s faster and smaller?<p>Quite happy with husky and running tslint/tests with husky on precommit currently.  Curious what the biggest benefits are from left hook and if it’s a pain to switch to if they are amazing benefits.<p>The biggest annoyance with current setup is on prem ci with Jenkins and GitHub Enterprise and complex docker bash scripts embedded in Jenkins.  It’s a pain to get lint/test results to show up in the GitHub pull request ui.  Doesn’t seem like this tool would help make that use case faster to set up.",
          "Generally I'm of the opinion that git hooks are not the right place to place linters or formatters.<p>For a JVM language, it means that you'll spin up a JVM for each step (and you can well have 15 formatters + linters in your stack).<p>So, the formatter better lives in a dev-only part of your codebase, so it's always loaded, interacting with your codebase using _runtime insights_ otherwise impossible to gather, and it's easily hackable.<p>I've authored this kind of solution in the past. It works great, and a proper CI pipeline (i.e. including linters) kills the other half of the problem."
        ],
        "id": "23db6869-420f-4a2a-bf66-21b6508853d3",
        "url_text": "Meet Lefthook, thefastest polyglot Git hooks manager out there, andmake sure not asingle line ofunruly code makes it into production. See how easy it istoinstall Lefthook, recently adopted by Discourse, Logux, andOpenstax, for most common frontend andbackend environments andensure all developers onyour team can rely onasingle flexible tool. Andit also has emojis Days, when asingle piece ofsoftware that millions rely onwas created by asingle developer inanivory tower, are long gone. Even Git, universally believed tobethebrainchild ofLinus Torvalds alone, was created with thehelp ofcontributors andisnow being maintained by ateam ofdozens. No matter ifyou work onanopen source project with thewhole world being your oyster, oryou are blooming inawalled garden ofproprietary commercial softwareyou still work inateam. Andeven with awell-organized system ofpull requests andcode reviews maintaining thecode quality across thelarge codebase with dozens ofcontributors isnot aneasy task. Hook me up Hooksways tofire off custom scripts when certain important actions (commit, push, etc.) occurare baked right into Git, so ifyou are comfortable with Bash andtheinternals oftheworld most popular version control systemyou dont need any external tools per se: just edit ./.git/hooks/pre-commit andput insome well-formed script that will, for instance, lint your files before you commit. However, when you work onaproject, you are most interested inwriting projects codenot thecode that checks it. Intheworld ofmodern web development tooling iseverything, andmyriads oftools exist for asingle reason: reducing overhead andcomplexity. Git hooks are not theexception: inJavaScript community, theweapon ofchoice isHusky with Webpack, Babel, andcreate-react-app relying onthis Node-based tool; Rails-centric backend world, however, ismostly ruled by Overcommit that comes as aRuby gem. Find detailed comparisons ofLefthook with other tools intheprojects wiki. Both tools are excellent intheir regard, but inamixed team offrontend andbackend developers, as Evil Martians are, you will often end up having two separate setups for Ruby andJavaScript with frontenders andbackenders linting their commits each intheir way. With Lefthook, you dont need tothink twiceits asingle Go binary that has wrappers both for JavaScript andfor Ruby. It can also beused as astandalone tool for any other environment. For most common use cases, Lefthook requires zero setup. Go language makes Lefthook lightning-fast andprovides support for concurrently executed scripts out ofthebox. Thefact that theexecutable isasingle machine code binary also removes theneed for minding external dependencies (Husky + lint-staged add roughly fifteen hundred dependencies toyour node_modules). It also removes theheadache ofreinstalling dependencies after each update ofyour development environment (try running aglobally installed Ruby gem with another version ofRuby). With Lefthook mentioned either inpackage.json orGemfile, andalefthook.yml configured intheprojects root (see examples below) thetool will beinstalled andused against your code automatically onthenext git pull, yarn install/ bundle install andgit add/git commitwith zero overhead for new contributors. An extensive README describes all possible usage scenarios. Thestraightforward syntax for configuration does not hide actual commands being run by Lefthookmaking sure that nothing funny ishappening below thebelt. Discourse with apunch Discoursean incredibly popular open source platform for forum-style discussionshas recently transitioned from Overcommit toLefthook andnever looked back. With almost 700 contributors authoring 34K commits andcounting, running linters onall new contributions isapriority. With Overcommit though, team members had toremind newcomers toinstall required tools continually Now with @arkweid/lefthook being adev dependency intheprojects package.json, nosetup isnecessary for new contributors. Lefthook allowed tohalf theamount oftime that pre-commit scripts take onlocalhost. The PR that changed theGit hook manager required, inessence, changing .overcommit.yml tolefthook.yml. Ifyou compare themyou will see that Lefthooks configuration ismuch more explicit while theOvercommits one relies mostly onthemagic ofplugins. Discourses CI output before andafter Lefthook Besides changing theway theoutput looksLefthook offers anice summary ofeverything it doesLefthook allowed tohalf theamount oftime that pre-commit scripts take onlocalhost, andincrease theCI run by 20% (on CI environments with better support for parallel execution thegain can beconsiderably more). A pretty bonus Round one Everything Lefthook needs tofunctionis alefthook binary installed somewhere inyour system (either globally orlocally), andalefthook.yml file inaproject root. The binary can either beinstalled globally (with Homebrew for macOS, snap for Ubuntu, AUR for Arch, orgo get anywhere), orlisted as adevelopment dependency either inRubys Gemfile, orNode.js package.json. If you are configuring Lefthook for thefirst time inyour projectyou need tochoose between these options, depending onyour preferences. Themain upside ofputting lefthook inyour Gemfile or@arkweid/lefthook inyour package.json isthat you dont need toworry that your contributors may not have lefthook installed system-wideafter thenext bundle install oryarn install thebinary will beinplace. After you have lefthook inyour systemrun lefthook install intheprojects root togenerate lefthook.yml andvisit theprojects repo for examples onhow touse thesyntax, here isaparticularly full one. Here ishow thecode describing actions oneach pre-commit (right after you type git commit -m \"new feature\" but right before it gets committed) might look like: pre-commit: commands: stylelint: tags: frontend style glob: \"*.{js}\" run: yarn stylelint {staged_files} rubocop: tags: backend style glob: \"*.{rb}\" exclude: \"application.rb|routes.rb\" run: bundle exec rubocop {all_files} scripts: \"good_job.js\": runner: node Then commit lefthook.yml toyour repository andpush it toyour favorite remotenow everyone onyour team will see it inaction every time they commit code. Blow by blow If you want tocheck Lefthook out onademo project quicklywe recommend cloning theevil_chat repositorya project we build inour celebrated Modern Frontend inRails series ofposts (Pt. 1, Pt. 2, Pt. 3). In this project, we use pre-commit hooks configured with Lefthook for formatting JavaScript andCSS files with Prettier, andlinting them with ESlint andstylelint. Heres how toquickly see Lefthook inaction. First, clone therepo andrun package managers: $ git clone git@github.com:demiazz/evil_chat.git $ bundle && yarn Now, go andbreak some CSS orJS inany .pcss or.js files. $ git add . && git commit -m \"Now I am become death, destroyer of worlds\" Wait for it! If all goes well (meaning you succeeded inbreaking thecode), this iswhat you are going tosee: A bad commit The failing scripts are shown with aboxing glove emoji ontheoutputgiving you areal left hook tobring back your attention! For now, our linting covers only thefrontend part oftheapp. What about adding some good old Rubocop tothemix? Edit your lefthook.yml toinclude thefollowing lines: # lefthook.yml pre-commit: parallel: true # tell Lefthook to utilise all cores commands: js: glob: \"*.js\" run: yarn prettier --write {staged_files} && yarn eslint {staged_files} && git add {staged_files} css: glob: \"*.{css,pcss}\" run: yarn prettier --write {staged_files} && yarn stylelint --fix {staged_files} && git add {staged_files} # Add these lines rubocop: glob: \"*.{rb}\" run: rubocop {staged_files} --parallel Note thehandy {staged_files} shortcut that allows you totarget only thefiles that are staged for acurrent commit. Now go back tofix your JS andCSS andcommit astyle offense inany oftheRuby files (yes, you have our permission). Feel free tothrow insome comments here andthere so that git picks up changes for different filetypes. Rubocop comes into play Now CSS andJS are fine, but Ruby needs another look, hence theleft hook! Roll with thepunches Here isthesummary ofthefeatures that make Lefthook stand out ofcompetition, andbring flexibility toyour workflow, see thecomplete list here. Speed Lefthook squeezes out every bit ofparallelism from your machine (or aCI server), you only need totoggle one setting: parallel: true. Heres theconfig file that describes alint series ofcommands that you can run with lefthook run lint from your command line. These are thesame commands that Discourse used torun onTravis. Lefthook gives you anability torun custom tasks like that. As analternative, you can set thesame commands toberun onpre-commit, pre-push, post-checkout, post-merge, orany other available Git hooks. # lefthook.yml lint: # parallel: true commands: rubocop: run: bundle exec rubocop --parallel prettier: run: yarn prettier --list-different \"app/assets/stylesheets/**/*.scss\" \"app/assets/javascripts/**/*.es6\" \"test/javascripts/**/*.es6\" eslint-assets: run: yarn eslint --ext .es6 app/assets/javascripts eslint-test: run: yarn eslint --ext .es6 test/javascripts eslint-plugins-assets: run: yarn eslint --ext .es6 plugins/**/assets/javascripts eslint-plugins-test: run: yarn eslint --ext .es6 plugins/**/test/javascripts eslint-assets-tests: run: yarn eslint app/assets/javascripts test/javascripts With parallel: true commented out, onmy system, this task takes over 30 seconds. With theparallel feature turned on, it takes 15.5 secondstwice as fast! Flexibility Direct control If you want torun your hook directly, without waiting for aGit action: $ lefthook run pre-commit Flexible lists offiles You can use built-in shortcuts {staged_files} and{all_files}, ordefine your own lists according tospecific selection. pre-commit: commands: frontend-linter: run: yarn eslint {staged_files} backend-linter: run: bundle exec rubocop {all_files} frontend-style: files: git diff --name-only HEAD @{push} run: yarn stylelint {files} Glob/Regex filters If you want tofilter alist offiles onthefly with aglob oraRegex. pre-commit: commands: backend-linter: glob: \"*.{rb}\" # glob filter exclude: \"application.rb|routes.rb\" # regexp filter run: bundle exec rubocop {all_files} Run your own scripts If one-liners are not enough, you can tell Lefthook toexecute custom scripts. commit-msg: scripts: \"good_job\": runner: bash Tags andlocal config for even more flexibility You can group your tasks by tags andthen exclude them when you run hooks locally (e.g., you are abackend developer andnot interested inrunning tasks onfrontend code). With Lefthook, you can create alefthook-local.yml file inyour project root (dont forget toadd it toyour .gitignore): all thesettings described here would override theones from themain lefthook.yml. Now you can assign tags todifferent series ofcommands # lefthook.yml pre-push: commands: stylelint: tags: frontend-style # a tag files: git diff --name-only master glob: \"*.{js}\" run: yarn stylelint {files} rubocop: tags: backend-style # a tag files: git diff --name-only master glob: \"*.{rb}\" run: bundle exec rubocop {files} andexclude them from being run locally: # lefthook-local.yml pre-push: exclude_tags: - frontend-style K.O. Do you use Docker for local development? Perhaps you do, perhaps not yet, but there isabig chance that someone else onyour team does otherwise. Some people embrace containerized development fully, while others prefer torely onwell-groomed local environments. Your main lefthook.yml may contain thefollowing: post-push: scripts: \"good_job.js\": runner: bash However, you want torun thesame task inaDocker container, but you dont want tomess up thesetup for everyone else. By using alefthook-local.yml file (the one you dont check ininto version control), you can alter thecommand just slightly, andjust for your local setup, by using a{cmd} shortcut, just like that: # lefthook-local.yml pre-commit: scripts: \"good_job.js\": runner: docker exec -it --rm <container_id_or_name> {cmd} {cmd} will bereplaced by acommand from themain config. The resulting command will look like this: docker exec -it --rm <container_id_or_name> node good_job.js 8 9 10 Knockout! We are confident that Lefthook iscurrently thefastest andmost flexible Git hook manager inour galaxy, so we encourage everyone tofollow theexample ofDiscourse andeither add Lefthook toyour projects orcreate apull request proposing thechange inyour favorite open source repositories. The polyglot nature ofLefthook allows it tobeused inpure frontend, pure backend, ormixed full-stack teams, andwith all common development setups onall major operating systems, including Windows. Find thebest way toinstall it, depending onyour stack, andgive it ago! See how we use Lefthook intandem with Crystalball inour commercial projects by checking out this post onDev.to. If you see our work onLefthook as yet another perfect example ofreinventing thewheel, still give it atryyou will soon realize that Lefthook ismore ofajet pack, then yet another good old wheel for git hooks management. And never, never throw inthetowel onautomating your Git andGitHub workflow! ",
        "_version_": 1718527417624035329
      },
      {
        "story_id": [21695658],
        "story_author": ["crawshaw"],
        "story_descendants": [87],
        "story_score": [174],
        "story_time": ["2019-12-03T19:22:20Z"],
        "story_title": "The Principles of Versioning in Go",
        "search": [
          "The Principles of Versioning in Go",
          "https://research.swtch.com/vgo-principles",
          "The Principles of Versioning in Go Posted on Tuesday, December 3, 2019. PDF This blog post is about how we added package versioning to Go, in the form of Go modules, and the reasons we made the choices we did. It is adapted and updated from a talk I gave at GopherCon Singapore in 2018. Why Versions? To start, lets make sure were all on the same page, by taking a look at the ways the GOPATH-based go get breaks. Suppose we have a fresh Go installation and we want to write a program that imports D. We run go get D. Remember that we are using the original GOPATH-based go get, not Go modules. $ go get D That looks up and downloads the latest version of D, which right now is D1.0. It builds. Were happy. Now suppose a few months later we need C. We run go get C. That looks up and downloads the latest version of C, which is C1.8. $ go get C C imports D, but go get finds that it has already downloaded a copy of D, so it reuses that copy. Unfortunately, that copy is still D1.0. The latest copy of C was written using D1.4, which contains a feature or maybe a bug fix that C needs and which was missing from D1.0. So C is broken, because the dependency D is too old. Since the build failed, we try again, with go get -u C. $ go get -u C Unfortunately, an hour ago Ds author published D1.6. Because go get -u uses the latest version of every dependency, including D, it turns out that C is still broken. Cs author used D1.4, which worked fine, but D1.6 has introduced a bug that keeps C from working properly. Before, C was broken because D was too old. Now, C is broken because D is too new. Those are the two ways that go get fails when using GOPATH. Sometimes it uses dependencies that are too old. Other times it uses dependencies that are too new. What we really want in this case is the version of D that Cs author used and tested against. But GOPATH-based go get cant do that, because it has no awareness of package versions at all. Go programmers started asking for better handling of package versions as soon as we published goinstall, the original name for go get. Various tools were written over many years, separate from the Go distribution, to help make installing specific versions easier. But because those tools did not agree on a single approach, they didnt work as a base for creating other version-aware tools, such as a version-aware godoc or a version-aware vulnerability checker. We needed to add the concept of package versions to Go for many reasons. The most pressing reason was to help go get stop using code thats too old or too new, but having an agreed-upon meaning of versions in the vocabulary of Go developers and tools enables the entire Go ecosystem to become version-aware. The Go module mirror and checksum database, which safely speed up Go package downloads, and the new version-aware Go package discovery site are both made possible by an ecosystem-wide understanding of what a version is. Versions for Software Engineering Over the past two years, we have added support for package versions to Go itself, in the form of Go modules, built into the go command. Go modules introduce a new import path syntax called semantic import versioning, along with a new algorithm for selecting which versions to use, called minimal version selection. You might wonder: Why not do what other languages do? Java has Maven, Node has NPM, Ruby has Bundler, Rust has Cargo. How is this not a solved problem? You might also wonder: We introduced a new, experimental Go tool called Dep in early 2018 that implemented the general approach pioneered by Bundler and Cargo. Why did Go modules not reuse Deps design? The answer is that we learned from Dep that the general Bundler/Cargo/Dep approach includes some decisions that make software engineering more complex and more challenging. Thanks to learning about the problems were in Deps design, the Go modules design made different decisions, to make software engineering simpler and easier instead. But what is software engineering? How is software engineering different from programming? I like the following definition: Software engineering is what happens to programming when you add time and other programmers. Programming means getting a program working. You have a problem to solve, you write some Go code, you run it, you get your answer, youre done. Thats programming, and thats difficult enough by itself. But what if that code has to keep working, day after day? What if five other programmers need to work on the code too? What if the code must adapt gracefully as requirements change? Then you start to think about version control systems, to track how the code changes over time and to coordinate with the other programmers. You add unit tests, to make sure bugs you fix are not reintroduced over time, not by you six months from now, and not by that new team member whos unfamiliar with the code. You think about modularity and design patterns, to divide the program into parts that team members can work on mostly independently. You use tools to help you find bugs earlier. You look for ways to make programs as clear as possible, so that bugs are less likely. You make sure that small changes can be tested quickly, even in large programs. Youre doing all of this because your programming has turned into software engineering. (This definition and explanation of software engineering is my riff on an original theme by my Google colleague Titus Winters, whose preferred phrasing is software engineering is programming integrated over time. Its worth seven minutes of your time to see his presentation of this idea at CppCon 2017, from 8:17 to 15:00 in the video.) Nearly all of Gos distinctive design decisions were motivated by concerns about software engineering. For example, most people think that we format Go code with gofmt to make code look nicer or to end debates among team members about program layout. And to some degree we do. But the more important reason for gofmt is that if an algorithm defines how Go source code is formatted, then programs, like goimports or gorename or go fix, can edit the source code more easily. This helps you maintain code over time. As another example, Go import paths are URLs. If code imported \"uuid\", youd have to ask which uuid package. Searching for uuid on pkg.go.dev turns up dozens of packages with that name. If instead the code imports \"github.com/google/uuid\", now its clear which package we mean. Using URLs avoids ambiguity and also reuses an existing mechanism for giving out names, making it simpler and easier to coordinate with other programmers. Continuing the example, Go import paths are written in Go source files, not in a separate build configuration file. This makes Go source files self-contained, which makes it easier to understand, modify, and copy them. These decisions were all made toward the goal of simplifying software engineering. There are three broad principles behind the changes from Deps design to Go modules, all motivated by wanting to simplify software engineering. These are the principles of compatibility, repeatability, and cooperation. The rest of this post explains each principle, shows how it led us to make a different decision for Go modules than in Dep, and then responds, as fairly as I can, to objections against making that change. Principle #1: Compatibility The meaning of a name in a program should not change over time. The first principle is compatibility. Compatibilityor, if you prefer, stabilityis the idea that, in a program, the meaning of a name should not change over time. If a name meant one thing last year, it should mean the same thing this year and next year. For example, programmers are sometimes confused by a detail of strings.Split. We all expect that splitting hello world produces two strings hello and world. But if the input has leading, trailing, or repeated spaces, the result contains empty strings too. Example: strings.Split(x, \" \") \"hello world\" => {\"hello\", \"world\"} \"hello world\" => {\"hello\", \"\", \"world\"} \" hello world\" => {\"\", \"hello\", \"world\"} \"hello world \" => {\"hello\", \"world\", \"\"} Suppose we decide that it would be better overall to change the behavior of strings.Split to omit those empty strings. Can we do that? No. Weve given strings.Split a specific meaning. The documentation and the implementation agree on that meaning. Programs depend on that meaning. Changing the meaning would break those programs. It would break the principle of compatibility. We can implement the new meaning; we just need to give a new name too. In fact, years ago, to solve this exact problem, we introduced strings.Fields, which is tailored to space-separated fields and never returns empty strings. Example: strings.Fields(x) \"hello world\" => {\"hello\", \"world\"} \"hello world\" => {\"hello\", \"world\"} \" hello world\" => {\"hello\", \"world\"} \"hello world \" => {\"hello\", \"world\"} We didnt redefine strings.Split, because we were concerned about compatibility. Following the principle of compatibility simplifies software engineering, because it lets you ignore time when trying to understand programming. People dont have to think, well this package was written in 2015, back when strings.Split returned empty strings, but this other package was written last week, so it expects strings.Split to leave them out. And not just people. Tools dont have to worry about time either. For example, a refactoring tool can always move a strings.Split call from one package to another without worrying that it will change its meaning. In fact, the most important feature of Go 1 was not a language change or a new library feature. It was the declaration of compatibility: It is intended that programs written to the Go 1 specification will continue to compile and run correctly, unchanged, over the lifetime of that specification. Go programs that work today should continue to work even as future point releases of Go 1 arise (Go 1.1, Go 1.2, etc.). golang.org/doc/go1compat We committed that we would stop changing the meaning of names in the standard library, so that programs working with Go 1.1 could be expected to continue working in Go 1.2, and so on. That ongoing commitment makes it easy for users to write code and keep it working even as they upgrade to newer Go versions to get faster implementations and new features. What does compatibility have to do with versioning? Its important to think about compatibility because the most popular approach to versioning todaysemantic versioninginstead encourages incompatibility. That is, semantic versioning has the unfortunate effect of making incompatible changes seem easy. Every semantic version takes the form vMAJOR.MINOR.PATCH. If two versions have the same major number, the later (if you like, greater) version is expected to be backwards compatible with the earlier (lesser) one. But if two versions have different major numbers, they have no expected compatibility relationship. Semantic versioning seems to suggest, Its okay to make incompatible changes to your packages. Tell your users about them by incrementing the major version number. Everything will be fine. But this is an empty promise. Incrementing the major version number isnt enough. Everything is not fine. If strings.Split has one meaning today and a different meaning tomorrow, simply reading your code is now software engineering, not programming, because you need to think about time. It gets worse. Suppose B is written to expect strings.Split v1, while C is written to expect strings.Split v2. Thats fine if you build each by itself. But what happens when your package A imports both B and C? If strings.Split has to have just one meaning, theres no way to build a working program. For the Go modules design, we realized that the principle of compatibility is absolutely essential to simplifying software engineering and must be supported, encouraged, and followed. The Go FAQ has encouraged compatibility since Go 1.2 in November 2013: Packages intended for public use should try to maintain backwards compatibility as they evolve. The Go 1 compatibility guidelines are a good reference here: dont remove exported names, encourage tagged composite literals, and so on. If different functionality is required, add a new name instead of changing an old one. If a complete break is required, create a new package with a new import path. For Go modules, we gave this old advice a new name, the import compatibility rule: If an old package and a new package have the same import path, the new package must be backwards compatible with the old package. But then what do we do about semantic versioning? If we still want to use semantic versioning, as many users expect, then the import compatibility rule requires that different semantic major versions, which by definition have no compatibility relationship, must use different import paths. The way to do that in Go modules is to put the major version in the import path. We call this semantic import versioning. In this example, my/thing/v2 identifies semantic version 2 of a particular module. Version 1 was just my/thing, with no explicit version in the module path. But when you introduce major version 2 or larger, you have to add the version after the module name, to distinguish from version 1 and other major versions, so version 2 is my/thing/v2, version 3 is my/thing/v3, and so on. If the strings package were its own module, and if for some reason we really needed to redefine Split instead of adding a new function Fields, then we could create strings (major version 1) and strings/v2 (major version 2), with different Split functions. Then the unbuildable program from before can be built: B says import strings\" while C says import \"strings/v2\". Those are different packages, so its okay to build both into the program. And now B and C can each have the Split function they expect. Because strings and strings/v2 have different import paths, people and tools automatically understand that they name different packages, just as people already understand that crypto/rand and math/rand name different packages. No one needs to learn a new disambiguation rule. Lets return to the unbuildable program, not using semantic import versioning. If we replace strings in this example with an arbitrary package D, then we have a classic diamond dependency problem. Both B and C build fine by themselves, but with different, conflicting requirements for D. If we try to use both in a build of A, then theres no single choice of D that works. Semantic import versioning cuts through diamond dependencies. Theres no such thing as conflicting requirements for D. D version 1.3 must be backwards compatible with D version 1.2, and D version 2.0 has a different import path, D/v2. A program using both major versions keeps them as separate as any two package with different import paths and builds fine. Objection: Aesthetics The most common objection to semantic import versioning is that people dont like seeing the major versions in the import paths. In short, theyre ugly. Of course, what this really means is only that people are not used to seeing the major version in import paths. I can think of two examples of major aesthetic shifts in Go code that seemed ugly at the time but were adopted because they simplified software engineering and now look completely natural. The first example is export syntax. Back in early 2009, Go used an export keyword to mark a function as exported. We knew we needed something more lightweight to mark individual struct fields, and we were casting about for ideas, considering things like leading underscore means unexported or leading plus in declaration means export. Eventually we hit on the upper-case for export idea. Using an upper-case letter as the export signal looked strange to us, but that was the only drawback we could find. Otherwise, the idea was sound, it satisfied our goals, and it was more appealing than the other choices wed been considering. So we adopted it. I remember thinking that changing fmt.printf to fmt.Printf in my code was ugly, or at least jarring: to me, fmt.Printf didnt look like Go, at least not the Go I had been writing. But I had no good argument against it, so I went along with (and implemented) the change. After a few weeks, I got used to it, and now it is fmt.printf that doesnt look like Go to me. Whats more, I came to appreciate the precision about what is and isnt exported when reading code. When I go back to C++ or Java code now and I see a call like x.dangerous() I miss being able to tell at a glance whether the dangerous method is a public method that anyone can call. The second example is import paths, which I mentioned briefly earlier. In the early days of Go, before goinstall and go get, import paths were not full URLs. A developer had to manually download and install a package named uuid and then would write import \"uuid\". Changing to URLs for import paths (import \"github.com/google/uuid\") eliminated this ambiguity, and the added precision made go get possible. People did complain at first, but now the longer paths are second nature to us. We rely on and appreciate their precision, because it makes our software engineering work simpler. Both these changesupper-case for export and full URLs for import pathswere motivated by good software engineering arguments to which the only real objection was visual aesthetics. Over time we came to appreciate the benefits, and our aesthetic judgements adapted. I expect the same to happen with major versions in import paths. Well get used to them, and well come to value the precision and simplicity they bring. Objection: Updating Import Paths Another common objection is that upgrading from (say) v2 of a module to v3 of the same module requires changing all the import paths referring to that module, even if the client code doesnt need any other changes. Its true that the upgrade requires rewriting import paths, but its also easy to write a tool to do a global search and replace. We intend to make it possible to handle such upgrades with go fix, although we havent implemented that yet. Both the previous objection and this one implicitly suggest keeping the major version information only in a separate version metadata file. If we do that, then an import path wont be precise enough to identify semantics, like back when import \"uuid\" might have meant any one of dozens of different packages. All programmers and tools will have to look in the metadata file to answer the question: which major version is this? Which strings.Split am I calling? What happens when I copy a file from one module to another and forget to check the metadata file? If instead we keep import paths semantically precise, then programmers and tools dont need to be taught a new way to keep different major versions of a package separate. Another benefit of having the major version in the import path is that when you do update from v2 to v3 of a package, you can update your program gradually, in stages, maybe one package at a time, and its always clear which code has been converted and which has not. Objection: Multiple Major Versions in a Build Another common objection is that having D v1 and D v2 in the same build should be disallowed entirely. That way, Ds author wont have to think about the complexities that arise from that situation. For example, maybe package D defines a command line flag or registers an HTTP handler, so that building both D v1 and D v2 into a single program would fail without explicit coordination between those versions. Dep enforces exactly this restriction, and some people say it is simpler. But this is simplicity only for Ds author. Its not simplicity for Ds users, and normally users outnumber authors. If D v1 and D v2 cannot coexist in a single build, then diamond dependencies are back. You cant convert a large program from D v1 to D v2 gradually, the way I just explained. In internet-scale projects, this will fragment the Go package ecosystem into incompatible groups of packages: those that use D v1 and those that use D v2. For a detailed example, see my 2018 blog post, Semantic Import Versioning. Dep was forced to disallow multiple major versions in a build because the Go build system requires each import path to name a unique package (and Dep did not consider semantic import versioning). In contrast, Cargo and other systems do allow multiple major versions in a build. As I understand it, the reason these systems allow multiple versions is the same reason that Go modules does: not allowing them makes it too hard to work on large programs. Objection: Too Hard to Experiment A final objection is that versions in import paths are unnecessary overhead when youre just starting to design a package, you have no users, and youre making frequent backwards-incompatible changes. Thats absolutely true. Semantic versioning makes an exception for exactly that situation. In major version 0, there are no compatibility expectations at all, so that you can iterate quickly when youre first starting out and not worry about compatibility. For example, v0.3.4 doesnt need to be backwards compatible with anything else: not v0.3.3, not v0.0.1, not v1.0.0. Semantic import versioning makes a similar exception: major version 0 is not mentioned in import paths. In both cases, the rationale is that time has not entered the picture. Youre not doing software engineering yet. Youre just programming. Of course, this means that if you use v0 versions of other peoples packages, then you are accepting that new versions of those packages might include breaking API changes without a corresponding import path change, and you take on the responsibility to update your code when that happens. Principle #2: Repeatability The result of a build of a given version of a package should not change over time. The second principle is repeatability for program builds. By repeatability I mean that when you are building a specific version of a package, the build should decide which dependency versions to use in a way thats repeatable, that doesnt change over time. My build today should match your build of my code tomorrow and any other programmers build next year. Most package management systems dont make that guarantee. We saw earlier how GOPATH-based go get doesnt provide repeatability. First go get used a version of D that was too old: Then go get -u used a version of D that was too new: You might think, of course go get makes this mistake: it doesnt know anything about versions at all. But most other systems make the same mistake. Im going to use Dep as my example here, but at least Bundler and Cargo work the same way. Dep asks every package to include a metadata file called a manifest, which lists requirements for dependency versions. When Dep downloads C, it reads Cs manifest and learns that C needs D1.4 or later. Then Dep downloads the newest version of D satisfying that constraint. Yesterday, that meant D1.5: Today, that means D1.6: The decision is time-dependent. It changes from day to day. The build is not repeatable. The developers of Dep (and Bundler and Cargo and ...) understood the importance of repeatability, so they introduced a second metadata file called a lock file. If C is a whole program, what Go calls package main, then the lock file lists the exact version to use for every dependency of C, and Dep lets the lock file override the decisions it would normally make. Locking in those decisions ensures that they stop changing over time and makes the build repeatable. But lock files only apply to whole programs, to package main. What if C is a library, being built as part of a larger program? Then a lock file meant for building only C might not satisfy the additional constraints in the larger program. So Dep and the others must ignore lock files associated with libraries and fall back to the default time-based decisions. When you add C1.8 to a larger build, the exact packages you get depends on what day it is. In summary, Dep starts with a time-based decision about which version of D to use. Then it adds a lock file, to override that time-based decision, for repeatability, but that lock file can only be applied to whole programs. In Go modules, the go command instead makes its decision about which version of D to use in a way that does not change over time. Then builds are repeatable all the time, without the added complexity of a lock file override, and this repeatability applies to libraries, not just whole programs. The algorithm used for Go modules is very simple, despite the imposing name minimal version selection. It works like this. Each package specifies a minimum version of each dependency. For example, suppose B1.3 requests D1.3 or later, and C1.8 requests D1.4 or later. In Go modules, the go command prefers to use those exact versions, not the latest versions. If were building B by itself, well use D1.3. If were building C by itself, well use D1.4. The builds of these libraries are repeatable. Also shown in the figure, if different parts of a build request different minimum versions, the go command uses the latest requested version. The build of A sees requests for D1.3 and D1.4, and 1.4 is later than 1.3, so the build chooses D1.4. That decision does not depend on whether D1.5 and D1.6 exist, so it does not change over time. I call this minimal version selection for two reasons. First, for each package it selects the minimum version satisfying the requests (equivalently, the maximum of the requests). And second, it seems to be just about the simplest approach that could possibly work. Minimal version selection provides repeatability, for whole programs and for libraries, always, without any lock files. It removes time from consideration. Every chosen version is always one of the versions mentioned explicitly by some package already chosen for the build. Objection: Using the Latest Version is a Feature The usual first objection to prioritizing repeatability is to claim that preferring the latest version of a dependency is a feature, not a bug. The claim is that programmers either dont want to or are too lazy to update their dependencies regularly, so tools like Dep should use the latest dependencies automatically. The argument is that the benefits of having the latest versions outweigh the loss of repeatability. But this argument doesnt hold up to scrutiny. Tools like Dep provide lock files, which then require programmers to update dependencies themselves, exactly because repeatable builds are more important than using the latest version. When you deploy a 1-line bug fix, you want to be sure that your bug fix is the only change, that youre not also picking up different, newer versions of your dependencies. You want to delay upgrades until you ask for them, so that you can be ready to run all your unit tests, all your integration tests, and maybe even production canaries, before you start using those upgraded dependencies in production. Everyone agrees about this. Lock files exist because everyone agrees about this: repeatability is more important than automatic upgrades. Objection: Using the Latest Version is a Feature When Building a Library The more nuanced argument you could make against minimal version selection would be to admit that repeatability matters for whole program builds, but then argue that, for libraries, the balance is different, and having the latest dependencies is more important than a repeatable build. I disagree. As programming increasingly means connecting large libraries together, and those large libraries are increasingly organized as collections of smaller libraries, all the reasons to prefer repeatability of whole-program builds become just as important for library builds. The extreme limit of this trend is the recent move in cloud computing to serverless hosting, like Amazon Lambda, Google Cloud Functions, or Microsoft Azure Functions. The code we upload to those systems is a library, not a whole program. We certainly want the production builds on those servers to use the same versions of dependencies as on our development machines. Of course, no matter what, its important to make it easy for programmers to update their dependencies regularly. We also need tools to report which versions of a package are in a given build or a given binary, including reporting when updates are available and when there are known security problems in the versions being used. Principle #3: Cooperation To maintain the Go package ecosystem, we must all work together. Tools cannot work around a lack of cooperation. The third principle is cooperation. We often talk about the Go community and the Go open source ecosystem. The words community and ecosystem emphasize that all our work is interconnected, that were building ondepending oneach others contributions. The goal is one unified system that works as a coherent whole. The opposite, what we want to avoid, is an ecosystem that is fragmented, split into groups of packages that cant work with each other. The principle of cooperation recognizes that the only way to keep the ecosystem healthy and thriving is for us all to work together. If we dont, then no matter how technically sophisticated our tools are, the Go open source ecosystem is guaranteed to fragment and eventually fail. By implication, then, its okay if fixing incompatibilities requires cooperation. We cant avoid cooperation anyway. For example, once again we have C1.8, which requires D1.4 or later. Thanks to repeatability, a build of C1.8 by itself will use D1.4. If we build C as part of a larger build that needs D1.5, thats okay too. Then D1.6 is released, and some larger build, maybe continuous integration testing, discovers that C1.8 does not work with D1.6. No matter what, the solution is for Cs author and Ds author to cooperate and release a fix. The exact fix depends on what exactly went wrong. Maybe C depends on buggy behavior fixed in D1.6, or maybe C depends on unspecified behavior changed in D1.6. Then the solution is for Cs author to release a new C version 1.9, cooperating with the evolution of D. Or maybe D1.6 simply has a bug. Then the solution is for Ds author to release a fixed D1.7, cooperating by respecting the principle of compatibility, at which point Cs author can release C version 1.9 that specifies that it requires D1.7. Take a minute to look at what just happened. The latest C and the latest D didnt work together. That introduced a small fracture in the Go package ecosystem. Cs author or Ds author worked to fix the bug, cooperating with each other and the rest of the ecosystem to repair the fracture. This cooperation is essential to keeping the ecosystem healthy. There is no adequate technical substitute. The repeatable builds in Go modules mean that a buggy D1.6 wont be picked up until users explicitly ask to upgrade. That creates time for Cs author and Ds author to cooperate on a real solution. The Go modules system makes no other attempt to work around these temporary incompatibilities. Objection: Use Declared Incompatibilities and SAT Solvers The most common objection to this approach of depending on cooperation is that it is unreasonable to expect developers to cooperate. Developers need some way to fix problems alone. the argument goes: they can only truly depend on themselves, not others. The solution offered by package managers like Bundler, Cargo, and Dep is to allow developers to declare incompatibilities between their packages and others and then employ a SAT solver to find a package combination not ruled out by the constraints. This argument breaks down for a few reasons. First, the algorithm used by Go modules to select versions already gives the developer of a particular module complete control over which versions are selected for that module, more control in fact than SAT constraints. The developer can force the use of any specific version of any dependency, saying use this exact version no matter what anyone else says. But that power is limited to the build of that specific module, to avoid giving other developers the same control over your builds. Second, repeatability of library builds in Go modules means that the release of a new, incompatible version of a dependency has no immediate effect on builds, as we saw in the previous section. The breakage only surfaces when someone takes some step to add that version to their own build, at which point they can step back again. Third, if version selection is phrased as a problem for a SAT solver, there are often many possible satisfying selections: the SAT solver must choose between them, and there is no clear criteria for doing so. As we saw earlier, SAT-based package managers choose between multiple valid possible selections by preferring newer versions. In the case where using the newest version of everything satisfies the constraints, thats the clear most preferred answer. But what if the two possible selections are latest of B, older C and older B, latest of C? Which should be preferred? How can the developer predict the outcome? The resulting system is difficult to understand. Fourth, the output of a SAT solver is only as good as its inputs: if any incompatibilities have been omitted, the SAT solver may well arrive at a combination that is still broken, just not declared as such. Incompatibility information is likely to be particularly incomplete for combinations involving dependencies with a significant age difference that may well never have been put together before. Indeed, an analysis of Rusts Cargo ecosystem in 2018 found that Cargos preference for the latest version was masking many missing constraints in Cargo manifests. If the latest version does not work, exploring old versions seems as likely to produce a combination that is not yet known to be broken as it is to produce a working one. Overall, once you step off the happy path of selecting the newest version of every dependency, SAT solver-based package managers are not more likely to choose a working configuration than Go modules is. If anything, SAT solvers may well be less likely to find a working configuration. Example: Go Modules versus SAT Solving The counter-arguments given in the previous section are a bit abstract. Lets make them concrete by continuing the example weve been working with and looking at what happens when using a SAT solver, like in Dep. Im using Dep for concreteness, because it is the immediate predecessor of Go modules, but the behaviors here are not specific to Dep and I dont mean to single it out. For the purposes of this example, Dep works the same way as many other package managers, and they all share the problems detailed here. To set the stage, remember that C1.8 works fine with D1.4 and D1.5, but the combination of C1.8 and D1.6 is broken. That gets noticed, perhaps by continuous integration testing, and the question is what happens next. When Cs author finds out that C1.8 doesnt work with D1.6, Dep allows and encourages issuing a new version, C1.9. C1.9 documents that it needs D later than 1.4 but before 1.6. The idea is that documenting the incompatibility helps Dep avoid it in future builds. In Dep, avoiding the incompatibility is importanteven urgent!because the lack of repeatability in library builds means that as soon as D1.6 is released, all future fresh builds of C will use D1.6 and break. This is a build emergency: all of Cs new users are broken. If Ds author is unavailable, or Cs author doesnt have time to fix the actual bug, the argument is that Cs author must be able to take some step to protect users from the breakage. That step is to release C1.9, documenting the incompatibility with D1.6. That fixes new builds of C by preventing the use of D1.6. This emergency doesnt happen when using Go modules, because of minimal version selection and repeatable builds. Using Go modules, the release of D1.6 does not affect Cs users, because nothing is explicitly requesting D1.6 yet. Users keep using the older versions of D they already use. Theres no need to document the incompatibility, because nothing is breaking. Theres time to cooperate on a real fix. Looking at Deps approach of documenting incompatibility again, releasing C1.9 is not a great solution. For one thing, the premise was that Ds author created a build emergency by releasing D1.6 and then was unavailable to release a fix, so it was important to give Cs author a way to fix things, by releasing C1.9. But if Ds author might be unavailable, what happens if Cs author is unavailable too? Then the emergency caused by automatic upgrades continues and all of Cs new users stay broken. Repeatable builds in Go modules avoid the emergency entirely. Also, suppose that the bug is in D, and Ds author issues a fixed D1.7. The workaround C1.9 requires D before 1.6, so it wont use the fixed D1.7. Cs author has to issue C1.10 to allow use of D1.7. In contrast, if were using Go modules, Cs author doesnt have to issue C1.9 and then also doesnt have to undo it by issuing C1.10. In this simple example, Go modules end up working more smoothly for users than Dep. They avoid the build breakage automatically, creating time for cooperation on the real fix. Ideally, C or D gets fixed before any of Cs users even notice. But what about more complex examples? Maybe Deps approach of documenting incompatibilities is better in more complicated situations, or maybe it keeps things working even when the real fix takes a long time to arrive. Lets take a look. To do that, lets rewind the clock a little, to before the buggy D1.6 is released, and compare the decisions made by Dep and Go modules. This figure shows the documented requirements for all the relevant package versions, along with the way both Dep and Go modules would build the latest C and the latest A. Dep is using D1.5 while the Go module system is using D1.4, but both tools have found working builds. Everyone is happy. But now suppose the buggy D1.6 is released. Dep builds pick up D1.6 automatically and break. Go modules builds keep using D1.4 and keep working. This is the simple situation we were just looking at. Before we move on, though, lets fix the Dep builds. We release C1.9, which documents the incompatibility with D1.6: Now Dep builds pick up C1.9 automatically, and builds start working again. Go modules cant document incompatibility in this way, but Go modules builds also arent broken, so no fix is needed. Now lets create a build complex enough to break Go modules. We can do this in two steps. First, we will release a new B that requires D1.6. Second, we will release a new A that requires the new B, at which point As build will use C with D1.6 and break. We start by releasing the new B1.4 that requires D1.6. Go modules builds are unaffected so far, thanks to repeatability. But look! Dep builds of A pick up B1.4 automatically and now they are broken again. What happened? Dep prefers to build A using the latest B and the latest C, but thats not possible: the latest B wants D1.6 and the latest C wants D before 1.6. But does Dep give up? No. It looks for alternate versions of B and C that do agree on an acceptable D. In this case, Dep decided to keep the latest B, which means using D1.6, which means not using C1.9. Since Dep cant use the latest C, it tries older versions of C. C1.8 looks good: it says it needs D1.4 or later, and that allows D1.6. So Dep uses C1.8, and it breaks. We know that C1.8 and D1.6 are incompatible, but Dep does not. Dep cant know it, because C1.8 was released before D1.6: Cs author couldnt have predicted that D1.6 would be a problem. And all package management systems agree that package contents must be immutable once they are published, which means theres no way for Cs author to retroactively document that C1.8 doesnt work with D1.6. (And if there were some way to change C1.8s requirements retroactively, that would violate repeatability.) Releasing C1.9 with the updated requirement was the fix. Because Dep prefers to use the latest C, most of the time it will use C1.9 and know to avoid D1.6. But if Dep cant use the latest of everything, it will start trying earlier versions of some things, including maybe C1.8. And using C1.8 makes it look like D1.6 is okayeven though we know betterand the build breaks. Or it might not break. Strictly speaking, Dep didnt have to make that decision. When Dep realized that it couldnt use both the latest B and the latest C, it had many options for how it might proceed. We assumed Dep kept the latest B. But if instead Dep kept the latest C, then it would need to use an older D and then an older B, producing a working build, as shown in the third column of the diagram. So maybe Deps builds are broken or maybe not, depending on the arbitrary decisions it makes in its SAT-solver-based version selection. (Last I checked, given a choice between a newer version of one package versus another, Dep prioritizes the one with the alphabetically earlier import path, at least in small test cases.) This example demonstrates another way that Dep and systems like it (nearly all package managers besides Go modules) can produce surprising results: when the one most preferred answer (use the latest of everything) does not apply, there are often many choices with no clear preferences between them. The exact answer depends on the details of the SAT solving algorithm, heuristics, and often the input order of the packages are presented to the solver. This underspecification and non-determinism in their solvers is another reason these systems need lock files. In any event, for the sake of Dep users, lets assume Dep lucked into the choice that keeps builds working. After all, were still trying to break the Go modules users builds. To break Go modules builds, lets release a new version of A, version 1.21, which requires the latest B, which in turn requires the latest D. Now, when the go command builds the latest A, it is forced to use the latest B and the latest D. In Go modules, there is no C1.9, so the go command uses C1.8, and the combination of C1.8 and D1.6 does not work. Finally, we have broken the Go modules builds! But look! The Dep builds are using C1.8 and D1.6 too, so theyre also broken. Before, Dep had to make a choice between the latest B and the latest C. If it chose the latest B, the build broke. If it chose the latest C, the build worked. The new requirement in A is forcing Dep to choose the latest B and the latest D, taking away Deps choice of latest C. So Dep uses the older C1.8, and the build breaks just like before. What should we conclude from all this? First of all, documenting an incompatibility for Dep does not guarantee to avoid that incompatibility. Second, a repeatable build like in Go modules also does not guarantee to avoid the incompatibility. Both tools can end up building the incompatible pair of packages. But as we saw, it takes multiple intentional steps to lead Go modules to a broken build, steps that lead Dep to the same broken build. And along the way the Dep-based build broke two other times when the Go modules build did not. Ive been using Dep in these examples because it is the immediate predecessor of Go modules, but I dont mean to single out Dep. In this respect, it works the same way as nearly every other package manager in every other language. They all have this problem. Theyre not even really broken or misbehaving so much as unfortunately designed. They are designed to try to work around a lack of cooperation among the various package maintainers, and tools cannot work around a lack of cooperation. The only real solution for the C versus D incompatibility is to release a new, fixed version of either C or D. Trying to avoid the incompatibility is useful only because it creates more time for Cs author and Ds author to cooperate on a fix. Compared to the Dep approach of preferring latest versions and documenting incompatibilities, the Go modules approach of repeatable builds with minimal version selection and no documented incompatibilities creates time for cooperation automatically, with no build emergencies, no declared incompatibilities, and no explicit work by users. Then we can rely on cooperation for the real fix. Conclusion These are the three principles of versioning in Go, the reasons that the design of Go modules deviates from the design of Dep, Cargo, Bundler, and others. Compatibility. The meaning of a name in a program should not change over time. Repeatability. The result of a build of a given version of a package should not change over time. Cooperation. To maintain the Go package ecosystem, we must all work together. Tools cannot work around a lack of cooperation. These principles are motivated by concerns about software engineering, which is what happens to programming when you add time and other programmers. Compatibility eliminates the effects of time on the meaning of a program. Repeatability eliminates the effects of time on the result of a build. Cooperation is an explicit recognition that, no matter how advanced our tools are, we do have to work with the other programmers. We cant work around them. The three principles also reinforce each other, in a virtuous cycle. Compatibility enables a new version selection algorithm, which provides repeatability. Repeatability makes sure that buggy, new releases are ignored until explicitly requested, which creates more time to cooperate on fixes. That cooperation in turn reestablishes compatibility. And the cycle goes around. As of Go 1.13, Go modules are ready for production use, and many companies, including Google, have adopted them. The Go 1.14 and Go 1.15 releases will bring additional ergonomic improvements, toward eventually deprecating and removing support for GOPATH. For more about adopting modules, see the blog post series on the Go blog, starting with Using Go Modules. ",
          "A very large issue with go tooling around dependency management is that it's based around concepts like this:<p>><i>If an old package and a new package have the same import path, the new package must be backwards compatible with the old package.</i><p>Which are effectively fine conceptually.  Every dependency system also supports that, you just make a new project named \"thing-v2\" instead of bumping a major version.  Making it explicit and encouraging it when possible (instead of forcing breaking changes) is good.<p>The problem is that there's absolutely nothing to help you achieve that.  Or detect failures to achieve it.  Or anything.  You can't even reliably rename <i>a single package</i> with existing refactoring tools, good luck copying and refactoring most of an entire repository correctly.  If you do it wrong, you might have to do it all over again because it's probably another breaking change.<p>Without fairly strong tooling to push people towards correct behavior, \"every change is breaking\" is <i>the default behavior</i>.  And the frequent accidental behavior even if you're being careful.",
          "I understand why this rubs some people the wrong way, but it really is consistent with Go's vision since the start. I don't think Russ is trying to imply that the issues they are facing when designing the language (dependency management, generics, etc.) are unique or impossible to solve in any way. I think he is simply saying that they are complex problems whose solutions introduce complexity to the language and ecosystem, and when presented with such a trade-off, the Go team has always preferred to lean towards simplicity, even if it means not implementing a feature that some (or even many) developers want/need.<p>While I don't personally agree with many of the decisions they've made, I still respect the choices they've made and I think there is plenty of room for languages with competing philosophies. Because of Go's philosophy of not catering to every demand, it's never going to become the next Java or take over all of Software Engineering, but I think the Go team is fine with that as long as it continues to be useful as a tool for solving problems."
        ],
        "story_type": ["Normal"],
        "url": "https://research.swtch.com/vgo-principles",
        "comments.comment_id": [21696188, 21698903],
        "comments.comment_author": ["Groxx", "nyc640"],
        "comments.comment_descendants": [1, 3],
        "comments.comment_time": [
          "2019-12-03T20:10:32Z",
          "2019-12-04T01:47:56Z"
        ],
        "comments.comment_text": [
          "A very large issue with go tooling around dependency management is that it's based around concepts like this:<p>><i>If an old package and a new package have the same import path, the new package must be backwards compatible with the old package.</i><p>Which are effectively fine conceptually.  Every dependency system also supports that, you just make a new project named \"thing-v2\" instead of bumping a major version.  Making it explicit and encouraging it when possible (instead of forcing breaking changes) is good.<p>The problem is that there's absolutely nothing to help you achieve that.  Or detect failures to achieve it.  Or anything.  You can't even reliably rename <i>a single package</i> with existing refactoring tools, good luck copying and refactoring most of an entire repository correctly.  If you do it wrong, you might have to do it all over again because it's probably another breaking change.<p>Without fairly strong tooling to push people towards correct behavior, \"every change is breaking\" is <i>the default behavior</i>.  And the frequent accidental behavior even if you're being careful.",
          "I understand why this rubs some people the wrong way, but it really is consistent with Go's vision since the start. I don't think Russ is trying to imply that the issues they are facing when designing the language (dependency management, generics, etc.) are unique or impossible to solve in any way. I think he is simply saying that they are complex problems whose solutions introduce complexity to the language and ecosystem, and when presented with such a trade-off, the Go team has always preferred to lean towards simplicity, even if it means not implementing a feature that some (or even many) developers want/need.<p>While I don't personally agree with many of the decisions they've made, I still respect the choices they've made and I think there is plenty of room for languages with competing philosophies. Because of Go's philosophy of not catering to every demand, it's never going to become the next Java or take over all of Software Engineering, but I think the Go team is fine with that as long as it continues to be useful as a tool for solving problems."
        ],
        "id": "20926251-97d5-4d01-b4d0-c59f94858397",
        "url_text": "The Principles of Versioning in Go Posted on Tuesday, December 3, 2019. PDF This blog post is about how we added package versioning to Go, in the form of Go modules, and the reasons we made the choices we did. It is adapted and updated from a talk I gave at GopherCon Singapore in 2018. Why Versions? To start, lets make sure were all on the same page, by taking a look at the ways the GOPATH-based go get breaks. Suppose we have a fresh Go installation and we want to write a program that imports D. We run go get D. Remember that we are using the original GOPATH-based go get, not Go modules. $ go get D That looks up and downloads the latest version of D, which right now is D1.0. It builds. Were happy. Now suppose a few months later we need C. We run go get C. That looks up and downloads the latest version of C, which is C1.8. $ go get C C imports D, but go get finds that it has already downloaded a copy of D, so it reuses that copy. Unfortunately, that copy is still D1.0. The latest copy of C was written using D1.4, which contains a feature or maybe a bug fix that C needs and which was missing from D1.0. So C is broken, because the dependency D is too old. Since the build failed, we try again, with go get -u C. $ go get -u C Unfortunately, an hour ago Ds author published D1.6. Because go get -u uses the latest version of every dependency, including D, it turns out that C is still broken. Cs author used D1.4, which worked fine, but D1.6 has introduced a bug that keeps C from working properly. Before, C was broken because D was too old. Now, C is broken because D is too new. Those are the two ways that go get fails when using GOPATH. Sometimes it uses dependencies that are too old. Other times it uses dependencies that are too new. What we really want in this case is the version of D that Cs author used and tested against. But GOPATH-based go get cant do that, because it has no awareness of package versions at all. Go programmers started asking for better handling of package versions as soon as we published goinstall, the original name for go get. Various tools were written over many years, separate from the Go distribution, to help make installing specific versions easier. But because those tools did not agree on a single approach, they didnt work as a base for creating other version-aware tools, such as a version-aware godoc or a version-aware vulnerability checker. We needed to add the concept of package versions to Go for many reasons. The most pressing reason was to help go get stop using code thats too old or too new, but having an agreed-upon meaning of versions in the vocabulary of Go developers and tools enables the entire Go ecosystem to become version-aware. The Go module mirror and checksum database, which safely speed up Go package downloads, and the new version-aware Go package discovery site are both made possible by an ecosystem-wide understanding of what a version is. Versions for Software Engineering Over the past two years, we have added support for package versions to Go itself, in the form of Go modules, built into the go command. Go modules introduce a new import path syntax called semantic import versioning, along with a new algorithm for selecting which versions to use, called minimal version selection. You might wonder: Why not do what other languages do? Java has Maven, Node has NPM, Ruby has Bundler, Rust has Cargo. How is this not a solved problem? You might also wonder: We introduced a new, experimental Go tool called Dep in early 2018 that implemented the general approach pioneered by Bundler and Cargo. Why did Go modules not reuse Deps design? The answer is that we learned from Dep that the general Bundler/Cargo/Dep approach includes some decisions that make software engineering more complex and more challenging. Thanks to learning about the problems were in Deps design, the Go modules design made different decisions, to make software engineering simpler and easier instead. But what is software engineering? How is software engineering different from programming? I like the following definition: Software engineering is what happens to programming when you add time and other programmers. Programming means getting a program working. You have a problem to solve, you write some Go code, you run it, you get your answer, youre done. Thats programming, and thats difficult enough by itself. But what if that code has to keep working, day after day? What if five other programmers need to work on the code too? What if the code must adapt gracefully as requirements change? Then you start to think about version control systems, to track how the code changes over time and to coordinate with the other programmers. You add unit tests, to make sure bugs you fix are not reintroduced over time, not by you six months from now, and not by that new team member whos unfamiliar with the code. You think about modularity and design patterns, to divide the program into parts that team members can work on mostly independently. You use tools to help you find bugs earlier. You look for ways to make programs as clear as possible, so that bugs are less likely. You make sure that small changes can be tested quickly, even in large programs. Youre doing all of this because your programming has turned into software engineering. (This definition and explanation of software engineering is my riff on an original theme by my Google colleague Titus Winters, whose preferred phrasing is software engineering is programming integrated over time. Its worth seven minutes of your time to see his presentation of this idea at CppCon 2017, from 8:17 to 15:00 in the video.) Nearly all of Gos distinctive design decisions were motivated by concerns about software engineering. For example, most people think that we format Go code with gofmt to make code look nicer or to end debates among team members about program layout. And to some degree we do. But the more important reason for gofmt is that if an algorithm defines how Go source code is formatted, then programs, like goimports or gorename or go fix, can edit the source code more easily. This helps you maintain code over time. As another example, Go import paths are URLs. If code imported \"uuid\", youd have to ask which uuid package. Searching for uuid on pkg.go.dev turns up dozens of packages with that name. If instead the code imports \"github.com/google/uuid\", now its clear which package we mean. Using URLs avoids ambiguity and also reuses an existing mechanism for giving out names, making it simpler and easier to coordinate with other programmers. Continuing the example, Go import paths are written in Go source files, not in a separate build configuration file. This makes Go source files self-contained, which makes it easier to understand, modify, and copy them. These decisions were all made toward the goal of simplifying software engineering. There are three broad principles behind the changes from Deps design to Go modules, all motivated by wanting to simplify software engineering. These are the principles of compatibility, repeatability, and cooperation. The rest of this post explains each principle, shows how it led us to make a different decision for Go modules than in Dep, and then responds, as fairly as I can, to objections against making that change. Principle #1: Compatibility The meaning of a name in a program should not change over time. The first principle is compatibility. Compatibilityor, if you prefer, stabilityis the idea that, in a program, the meaning of a name should not change over time. If a name meant one thing last year, it should mean the same thing this year and next year. For example, programmers are sometimes confused by a detail of strings.Split. We all expect that splitting hello world produces two strings hello and world. But if the input has leading, trailing, or repeated spaces, the result contains empty strings too. Example: strings.Split(x, \" \") \"hello world\" => {\"hello\", \"world\"} \"hello world\" => {\"hello\", \"\", \"world\"} \" hello world\" => {\"\", \"hello\", \"world\"} \"hello world \" => {\"hello\", \"world\", \"\"} Suppose we decide that it would be better overall to change the behavior of strings.Split to omit those empty strings. Can we do that? No. Weve given strings.Split a specific meaning. The documentation and the implementation agree on that meaning. Programs depend on that meaning. Changing the meaning would break those programs. It would break the principle of compatibility. We can implement the new meaning; we just need to give a new name too. In fact, years ago, to solve this exact problem, we introduced strings.Fields, which is tailored to space-separated fields and never returns empty strings. Example: strings.Fields(x) \"hello world\" => {\"hello\", \"world\"} \"hello world\" => {\"hello\", \"world\"} \" hello world\" => {\"hello\", \"world\"} \"hello world \" => {\"hello\", \"world\"} We didnt redefine strings.Split, because we were concerned about compatibility. Following the principle of compatibility simplifies software engineering, because it lets you ignore time when trying to understand programming. People dont have to think, well this package was written in 2015, back when strings.Split returned empty strings, but this other package was written last week, so it expects strings.Split to leave them out. And not just people. Tools dont have to worry about time either. For example, a refactoring tool can always move a strings.Split call from one package to another without worrying that it will change its meaning. In fact, the most important feature of Go 1 was not a language change or a new library feature. It was the declaration of compatibility: It is intended that programs written to the Go 1 specification will continue to compile and run correctly, unchanged, over the lifetime of that specification. Go programs that work today should continue to work even as future point releases of Go 1 arise (Go 1.1, Go 1.2, etc.). golang.org/doc/go1compat We committed that we would stop changing the meaning of names in the standard library, so that programs working with Go 1.1 could be expected to continue working in Go 1.2, and so on. That ongoing commitment makes it easy for users to write code and keep it working even as they upgrade to newer Go versions to get faster implementations and new features. What does compatibility have to do with versioning? Its important to think about compatibility because the most popular approach to versioning todaysemantic versioninginstead encourages incompatibility. That is, semantic versioning has the unfortunate effect of making incompatible changes seem easy. Every semantic version takes the form vMAJOR.MINOR.PATCH. If two versions have the same major number, the later (if you like, greater) version is expected to be backwards compatible with the earlier (lesser) one. But if two versions have different major numbers, they have no expected compatibility relationship. Semantic versioning seems to suggest, Its okay to make incompatible changes to your packages. Tell your users about them by incrementing the major version number. Everything will be fine. But this is an empty promise. Incrementing the major version number isnt enough. Everything is not fine. If strings.Split has one meaning today and a different meaning tomorrow, simply reading your code is now software engineering, not programming, because you need to think about time. It gets worse. Suppose B is written to expect strings.Split v1, while C is written to expect strings.Split v2. Thats fine if you build each by itself. But what happens when your package A imports both B and C? If strings.Split has to have just one meaning, theres no way to build a working program. For the Go modules design, we realized that the principle of compatibility is absolutely essential to simplifying software engineering and must be supported, encouraged, and followed. The Go FAQ has encouraged compatibility since Go 1.2 in November 2013: Packages intended for public use should try to maintain backwards compatibility as they evolve. The Go 1 compatibility guidelines are a good reference here: dont remove exported names, encourage tagged composite literals, and so on. If different functionality is required, add a new name instead of changing an old one. If a complete break is required, create a new package with a new import path. For Go modules, we gave this old advice a new name, the import compatibility rule: If an old package and a new package have the same import path, the new package must be backwards compatible with the old package. But then what do we do about semantic versioning? If we still want to use semantic versioning, as many users expect, then the import compatibility rule requires that different semantic major versions, which by definition have no compatibility relationship, must use different import paths. The way to do that in Go modules is to put the major version in the import path. We call this semantic import versioning. In this example, my/thing/v2 identifies semantic version 2 of a particular module. Version 1 was just my/thing, with no explicit version in the module path. But when you introduce major version 2 or larger, you have to add the version after the module name, to distinguish from version 1 and other major versions, so version 2 is my/thing/v2, version 3 is my/thing/v3, and so on. If the strings package were its own module, and if for some reason we really needed to redefine Split instead of adding a new function Fields, then we could create strings (major version 1) and strings/v2 (major version 2), with different Split functions. Then the unbuildable program from before can be built: B says import strings\" while C says import \"strings/v2\". Those are different packages, so its okay to build both into the program. And now B and C can each have the Split function they expect. Because strings and strings/v2 have different import paths, people and tools automatically understand that they name different packages, just as people already understand that crypto/rand and math/rand name different packages. No one needs to learn a new disambiguation rule. Lets return to the unbuildable program, not using semantic import versioning. If we replace strings in this example with an arbitrary package D, then we have a classic diamond dependency problem. Both B and C build fine by themselves, but with different, conflicting requirements for D. If we try to use both in a build of A, then theres no single choice of D that works. Semantic import versioning cuts through diamond dependencies. Theres no such thing as conflicting requirements for D. D version 1.3 must be backwards compatible with D version 1.2, and D version 2.0 has a different import path, D/v2. A program using both major versions keeps them as separate as any two package with different import paths and builds fine. Objection: Aesthetics The most common objection to semantic import versioning is that people dont like seeing the major versions in the import paths. In short, theyre ugly. Of course, what this really means is only that people are not used to seeing the major version in import paths. I can think of two examples of major aesthetic shifts in Go code that seemed ugly at the time but were adopted because they simplified software engineering and now look completely natural. The first example is export syntax. Back in early 2009, Go used an export keyword to mark a function as exported. We knew we needed something more lightweight to mark individual struct fields, and we were casting about for ideas, considering things like leading underscore means unexported or leading plus in declaration means export. Eventually we hit on the upper-case for export idea. Using an upper-case letter as the export signal looked strange to us, but that was the only drawback we could find. Otherwise, the idea was sound, it satisfied our goals, and it was more appealing than the other choices wed been considering. So we adopted it. I remember thinking that changing fmt.printf to fmt.Printf in my code was ugly, or at least jarring: to me, fmt.Printf didnt look like Go, at least not the Go I had been writing. But I had no good argument against it, so I went along with (and implemented) the change. After a few weeks, I got used to it, and now it is fmt.printf that doesnt look like Go to me. Whats more, I came to appreciate the precision about what is and isnt exported when reading code. When I go back to C++ or Java code now and I see a call like x.dangerous() I miss being able to tell at a glance whether the dangerous method is a public method that anyone can call. The second example is import paths, which I mentioned briefly earlier. In the early days of Go, before goinstall and go get, import paths were not full URLs. A developer had to manually download and install a package named uuid and then would write import \"uuid\". Changing to URLs for import paths (import \"github.com/google/uuid\") eliminated this ambiguity, and the added precision made go get possible. People did complain at first, but now the longer paths are second nature to us. We rely on and appreciate their precision, because it makes our software engineering work simpler. Both these changesupper-case for export and full URLs for import pathswere motivated by good software engineering arguments to which the only real objection was visual aesthetics. Over time we came to appreciate the benefits, and our aesthetic judgements adapted. I expect the same to happen with major versions in import paths. Well get used to them, and well come to value the precision and simplicity they bring. Objection: Updating Import Paths Another common objection is that upgrading from (say) v2 of a module to v3 of the same module requires changing all the import paths referring to that module, even if the client code doesnt need any other changes. Its true that the upgrade requires rewriting import paths, but its also easy to write a tool to do a global search and replace. We intend to make it possible to handle such upgrades with go fix, although we havent implemented that yet. Both the previous objection and this one implicitly suggest keeping the major version information only in a separate version metadata file. If we do that, then an import path wont be precise enough to identify semantics, like back when import \"uuid\" might have meant any one of dozens of different packages. All programmers and tools will have to look in the metadata file to answer the question: which major version is this? Which strings.Split am I calling? What happens when I copy a file from one module to another and forget to check the metadata file? If instead we keep import paths semantically precise, then programmers and tools dont need to be taught a new way to keep different major versions of a package separate. Another benefit of having the major version in the import path is that when you do update from v2 to v3 of a package, you can update your program gradually, in stages, maybe one package at a time, and its always clear which code has been converted and which has not. Objection: Multiple Major Versions in a Build Another common objection is that having D v1 and D v2 in the same build should be disallowed entirely. That way, Ds author wont have to think about the complexities that arise from that situation. For example, maybe package D defines a command line flag or registers an HTTP handler, so that building both D v1 and D v2 into a single program would fail without explicit coordination between those versions. Dep enforces exactly this restriction, and some people say it is simpler. But this is simplicity only for Ds author. Its not simplicity for Ds users, and normally users outnumber authors. If D v1 and D v2 cannot coexist in a single build, then diamond dependencies are back. You cant convert a large program from D v1 to D v2 gradually, the way I just explained. In internet-scale projects, this will fragment the Go package ecosystem into incompatible groups of packages: those that use D v1 and those that use D v2. For a detailed example, see my 2018 blog post, Semantic Import Versioning. Dep was forced to disallow multiple major versions in a build because the Go build system requires each import path to name a unique package (and Dep did not consider semantic import versioning). In contrast, Cargo and other systems do allow multiple major versions in a build. As I understand it, the reason these systems allow multiple versions is the same reason that Go modules does: not allowing them makes it too hard to work on large programs. Objection: Too Hard to Experiment A final objection is that versions in import paths are unnecessary overhead when youre just starting to design a package, you have no users, and youre making frequent backwards-incompatible changes. Thats absolutely true. Semantic versioning makes an exception for exactly that situation. In major version 0, there are no compatibility expectations at all, so that you can iterate quickly when youre first starting out and not worry about compatibility. For example, v0.3.4 doesnt need to be backwards compatible with anything else: not v0.3.3, not v0.0.1, not v1.0.0. Semantic import versioning makes a similar exception: major version 0 is not mentioned in import paths. In both cases, the rationale is that time has not entered the picture. Youre not doing software engineering yet. Youre just programming. Of course, this means that if you use v0 versions of other peoples packages, then you are accepting that new versions of those packages might include breaking API changes without a corresponding import path change, and you take on the responsibility to update your code when that happens. Principle #2: Repeatability The result of a build of a given version of a package should not change over time. The second principle is repeatability for program builds. By repeatability I mean that when you are building a specific version of a package, the build should decide which dependency versions to use in a way thats repeatable, that doesnt change over time. My build today should match your build of my code tomorrow and any other programmers build next year. Most package management systems dont make that guarantee. We saw earlier how GOPATH-based go get doesnt provide repeatability. First go get used a version of D that was too old: Then go get -u used a version of D that was too new: You might think, of course go get makes this mistake: it doesnt know anything about versions at all. But most other systems make the same mistake. Im going to use Dep as my example here, but at least Bundler and Cargo work the same way. Dep asks every package to include a metadata file called a manifest, which lists requirements for dependency versions. When Dep downloads C, it reads Cs manifest and learns that C needs D1.4 or later. Then Dep downloads the newest version of D satisfying that constraint. Yesterday, that meant D1.5: Today, that means D1.6: The decision is time-dependent. It changes from day to day. The build is not repeatable. The developers of Dep (and Bundler and Cargo and ...) understood the importance of repeatability, so they introduced a second metadata file called a lock file. If C is a whole program, what Go calls package main, then the lock file lists the exact version to use for every dependency of C, and Dep lets the lock file override the decisions it would normally make. Locking in those decisions ensures that they stop changing over time and makes the build repeatable. But lock files only apply to whole programs, to package main. What if C is a library, being built as part of a larger program? Then a lock file meant for building only C might not satisfy the additional constraints in the larger program. So Dep and the others must ignore lock files associated with libraries and fall back to the default time-based decisions. When you add C1.8 to a larger build, the exact packages you get depends on what day it is. In summary, Dep starts with a time-based decision about which version of D to use. Then it adds a lock file, to override that time-based decision, for repeatability, but that lock file can only be applied to whole programs. In Go modules, the go command instead makes its decision about which version of D to use in a way that does not change over time. Then builds are repeatable all the time, without the added complexity of a lock file override, and this repeatability applies to libraries, not just whole programs. The algorithm used for Go modules is very simple, despite the imposing name minimal version selection. It works like this. Each package specifies a minimum version of each dependency. For example, suppose B1.3 requests D1.3 or later, and C1.8 requests D1.4 or later. In Go modules, the go command prefers to use those exact versions, not the latest versions. If were building B by itself, well use D1.3. If were building C by itself, well use D1.4. The builds of these libraries are repeatable. Also shown in the figure, if different parts of a build request different minimum versions, the go command uses the latest requested version. The build of A sees requests for D1.3 and D1.4, and 1.4 is later than 1.3, so the build chooses D1.4. That decision does not depend on whether D1.5 and D1.6 exist, so it does not change over time. I call this minimal version selection for two reasons. First, for each package it selects the minimum version satisfying the requests (equivalently, the maximum of the requests). And second, it seems to be just about the simplest approach that could possibly work. Minimal version selection provides repeatability, for whole programs and for libraries, always, without any lock files. It removes time from consideration. Every chosen version is always one of the versions mentioned explicitly by some package already chosen for the build. Objection: Using the Latest Version is a Feature The usual first objection to prioritizing repeatability is to claim that preferring the latest version of a dependency is a feature, not a bug. The claim is that programmers either dont want to or are too lazy to update their dependencies regularly, so tools like Dep should use the latest dependencies automatically. The argument is that the benefits of having the latest versions outweigh the loss of repeatability. But this argument doesnt hold up to scrutiny. Tools like Dep provide lock files, which then require programmers to update dependencies themselves, exactly because repeatable builds are more important than using the latest version. When you deploy a 1-line bug fix, you want to be sure that your bug fix is the only change, that youre not also picking up different, newer versions of your dependencies. You want to delay upgrades until you ask for them, so that you can be ready to run all your unit tests, all your integration tests, and maybe even production canaries, before you start using those upgraded dependencies in production. Everyone agrees about this. Lock files exist because everyone agrees about this: repeatability is more important than automatic upgrades. Objection: Using the Latest Version is a Feature When Building a Library The more nuanced argument you could make against minimal version selection would be to admit that repeatability matters for whole program builds, but then argue that, for libraries, the balance is different, and having the latest dependencies is more important than a repeatable build. I disagree. As programming increasingly means connecting large libraries together, and those large libraries are increasingly organized as collections of smaller libraries, all the reasons to prefer repeatability of whole-program builds become just as important for library builds. The extreme limit of this trend is the recent move in cloud computing to serverless hosting, like Amazon Lambda, Google Cloud Functions, or Microsoft Azure Functions. The code we upload to those systems is a library, not a whole program. We certainly want the production builds on those servers to use the same versions of dependencies as on our development machines. Of course, no matter what, its important to make it easy for programmers to update their dependencies regularly. We also need tools to report which versions of a package are in a given build or a given binary, including reporting when updates are available and when there are known security problems in the versions being used. Principle #3: Cooperation To maintain the Go package ecosystem, we must all work together. Tools cannot work around a lack of cooperation. The third principle is cooperation. We often talk about the Go community and the Go open source ecosystem. The words community and ecosystem emphasize that all our work is interconnected, that were building ondepending oneach others contributions. The goal is one unified system that works as a coherent whole. The opposite, what we want to avoid, is an ecosystem that is fragmented, split into groups of packages that cant work with each other. The principle of cooperation recognizes that the only way to keep the ecosystem healthy and thriving is for us all to work together. If we dont, then no matter how technically sophisticated our tools are, the Go open source ecosystem is guaranteed to fragment and eventually fail. By implication, then, its okay if fixing incompatibilities requires cooperation. We cant avoid cooperation anyway. For example, once again we have C1.8, which requires D1.4 or later. Thanks to repeatability, a build of C1.8 by itself will use D1.4. If we build C as part of a larger build that needs D1.5, thats okay too. Then D1.6 is released, and some larger build, maybe continuous integration testing, discovers that C1.8 does not work with D1.6. No matter what, the solution is for Cs author and Ds author to cooperate and release a fix. The exact fix depends on what exactly went wrong. Maybe C depends on buggy behavior fixed in D1.6, or maybe C depends on unspecified behavior changed in D1.6. Then the solution is for Cs author to release a new C version 1.9, cooperating with the evolution of D. Or maybe D1.6 simply has a bug. Then the solution is for Ds author to release a fixed D1.7, cooperating by respecting the principle of compatibility, at which point Cs author can release C version 1.9 that specifies that it requires D1.7. Take a minute to look at what just happened. The latest C and the latest D didnt work together. That introduced a small fracture in the Go package ecosystem. Cs author or Ds author worked to fix the bug, cooperating with each other and the rest of the ecosystem to repair the fracture. This cooperation is essential to keeping the ecosystem healthy. There is no adequate technical substitute. The repeatable builds in Go modules mean that a buggy D1.6 wont be picked up until users explicitly ask to upgrade. That creates time for Cs author and Ds author to cooperate on a real solution. The Go modules system makes no other attempt to work around these temporary incompatibilities. Objection: Use Declared Incompatibilities and SAT Solvers The most common objection to this approach of depending on cooperation is that it is unreasonable to expect developers to cooperate. Developers need some way to fix problems alone. the argument goes: they can only truly depend on themselves, not others. The solution offered by package managers like Bundler, Cargo, and Dep is to allow developers to declare incompatibilities between their packages and others and then employ a SAT solver to find a package combination not ruled out by the constraints. This argument breaks down for a few reasons. First, the algorithm used by Go modules to select versions already gives the developer of a particular module complete control over which versions are selected for that module, more control in fact than SAT constraints. The developer can force the use of any specific version of any dependency, saying use this exact version no matter what anyone else says. But that power is limited to the build of that specific module, to avoid giving other developers the same control over your builds. Second, repeatability of library builds in Go modules means that the release of a new, incompatible version of a dependency has no immediate effect on builds, as we saw in the previous section. The breakage only surfaces when someone takes some step to add that version to their own build, at which point they can step back again. Third, if version selection is phrased as a problem for a SAT solver, there are often many possible satisfying selections: the SAT solver must choose between them, and there is no clear criteria for doing so. As we saw earlier, SAT-based package managers choose between multiple valid possible selections by preferring newer versions. In the case where using the newest version of everything satisfies the constraints, thats the clear most preferred answer. But what if the two possible selections are latest of B, older C and older B, latest of C? Which should be preferred? How can the developer predict the outcome? The resulting system is difficult to understand. Fourth, the output of a SAT solver is only as good as its inputs: if any incompatibilities have been omitted, the SAT solver may well arrive at a combination that is still broken, just not declared as such. Incompatibility information is likely to be particularly incomplete for combinations involving dependencies with a significant age difference that may well never have been put together before. Indeed, an analysis of Rusts Cargo ecosystem in 2018 found that Cargos preference for the latest version was masking many missing constraints in Cargo manifests. If the latest version does not work, exploring old versions seems as likely to produce a combination that is not yet known to be broken as it is to produce a working one. Overall, once you step off the happy path of selecting the newest version of every dependency, SAT solver-based package managers are not more likely to choose a working configuration than Go modules is. If anything, SAT solvers may well be less likely to find a working configuration. Example: Go Modules versus SAT Solving The counter-arguments given in the previous section are a bit abstract. Lets make them concrete by continuing the example weve been working with and looking at what happens when using a SAT solver, like in Dep. Im using Dep for concreteness, because it is the immediate predecessor of Go modules, but the behaviors here are not specific to Dep and I dont mean to single it out. For the purposes of this example, Dep works the same way as many other package managers, and they all share the problems detailed here. To set the stage, remember that C1.8 works fine with D1.4 and D1.5, but the combination of C1.8 and D1.6 is broken. That gets noticed, perhaps by continuous integration testing, and the question is what happens next. When Cs author finds out that C1.8 doesnt work with D1.6, Dep allows and encourages issuing a new version, C1.9. C1.9 documents that it needs D later than 1.4 but before 1.6. The idea is that documenting the incompatibility helps Dep avoid it in future builds. In Dep, avoiding the incompatibility is importanteven urgent!because the lack of repeatability in library builds means that as soon as D1.6 is released, all future fresh builds of C will use D1.6 and break. This is a build emergency: all of Cs new users are broken. If Ds author is unavailable, or Cs author doesnt have time to fix the actual bug, the argument is that Cs author must be able to take some step to protect users from the breakage. That step is to release C1.9, documenting the incompatibility with D1.6. That fixes new builds of C by preventing the use of D1.6. This emergency doesnt happen when using Go modules, because of minimal version selection and repeatable builds. Using Go modules, the release of D1.6 does not affect Cs users, because nothing is explicitly requesting D1.6 yet. Users keep using the older versions of D they already use. Theres no need to document the incompatibility, because nothing is breaking. Theres time to cooperate on a real fix. Looking at Deps approach of documenting incompatibility again, releasing C1.9 is not a great solution. For one thing, the premise was that Ds author created a build emergency by releasing D1.6 and then was unavailable to release a fix, so it was important to give Cs author a way to fix things, by releasing C1.9. But if Ds author might be unavailable, what happens if Cs author is unavailable too? Then the emergency caused by automatic upgrades continues and all of Cs new users stay broken. Repeatable builds in Go modules avoid the emergency entirely. Also, suppose that the bug is in D, and Ds author issues a fixed D1.7. The workaround C1.9 requires D before 1.6, so it wont use the fixed D1.7. Cs author has to issue C1.10 to allow use of D1.7. In contrast, if were using Go modules, Cs author doesnt have to issue C1.9 and then also doesnt have to undo it by issuing C1.10. In this simple example, Go modules end up working more smoothly for users than Dep. They avoid the build breakage automatically, creating time for cooperation on the real fix. Ideally, C or D gets fixed before any of Cs users even notice. But what about more complex examples? Maybe Deps approach of documenting incompatibilities is better in more complicated situations, or maybe it keeps things working even when the real fix takes a long time to arrive. Lets take a look. To do that, lets rewind the clock a little, to before the buggy D1.6 is released, and compare the decisions made by Dep and Go modules. This figure shows the documented requirements for all the relevant package versions, along with the way both Dep and Go modules would build the latest C and the latest A. Dep is using D1.5 while the Go module system is using D1.4, but both tools have found working builds. Everyone is happy. But now suppose the buggy D1.6 is released. Dep builds pick up D1.6 automatically and break. Go modules builds keep using D1.4 and keep working. This is the simple situation we were just looking at. Before we move on, though, lets fix the Dep builds. We release C1.9, which documents the incompatibility with D1.6: Now Dep builds pick up C1.9 automatically, and builds start working again. Go modules cant document incompatibility in this way, but Go modules builds also arent broken, so no fix is needed. Now lets create a build complex enough to break Go modules. We can do this in two steps. First, we will release a new B that requires D1.6. Second, we will release a new A that requires the new B, at which point As build will use C with D1.6 and break. We start by releasing the new B1.4 that requires D1.6. Go modules builds are unaffected so far, thanks to repeatability. But look! Dep builds of A pick up B1.4 automatically and now they are broken again. What happened? Dep prefers to build A using the latest B and the latest C, but thats not possible: the latest B wants D1.6 and the latest C wants D before 1.6. But does Dep give up? No. It looks for alternate versions of B and C that do agree on an acceptable D. In this case, Dep decided to keep the latest B, which means using D1.6, which means not using C1.9. Since Dep cant use the latest C, it tries older versions of C. C1.8 looks good: it says it needs D1.4 or later, and that allows D1.6. So Dep uses C1.8, and it breaks. We know that C1.8 and D1.6 are incompatible, but Dep does not. Dep cant know it, because C1.8 was released before D1.6: Cs author couldnt have predicted that D1.6 would be a problem. And all package management systems agree that package contents must be immutable once they are published, which means theres no way for Cs author to retroactively document that C1.8 doesnt work with D1.6. (And if there were some way to change C1.8s requirements retroactively, that would violate repeatability.) Releasing C1.9 with the updated requirement was the fix. Because Dep prefers to use the latest C, most of the time it will use C1.9 and know to avoid D1.6. But if Dep cant use the latest of everything, it will start trying earlier versions of some things, including maybe C1.8. And using C1.8 makes it look like D1.6 is okayeven though we know betterand the build breaks. Or it might not break. Strictly speaking, Dep didnt have to make that decision. When Dep realized that it couldnt use both the latest B and the latest C, it had many options for how it might proceed. We assumed Dep kept the latest B. But if instead Dep kept the latest C, then it would need to use an older D and then an older B, producing a working build, as shown in the third column of the diagram. So maybe Deps builds are broken or maybe not, depending on the arbitrary decisions it makes in its SAT-solver-based version selection. (Last I checked, given a choice between a newer version of one package versus another, Dep prioritizes the one with the alphabetically earlier import path, at least in small test cases.) This example demonstrates another way that Dep and systems like it (nearly all package managers besides Go modules) can produce surprising results: when the one most preferred answer (use the latest of everything) does not apply, there are often many choices with no clear preferences between them. The exact answer depends on the details of the SAT solving algorithm, heuristics, and often the input order of the packages are presented to the solver. This underspecification and non-determinism in their solvers is another reason these systems need lock files. In any event, for the sake of Dep users, lets assume Dep lucked into the choice that keeps builds working. After all, were still trying to break the Go modules users builds. To break Go modules builds, lets release a new version of A, version 1.21, which requires the latest B, which in turn requires the latest D. Now, when the go command builds the latest A, it is forced to use the latest B and the latest D. In Go modules, there is no C1.9, so the go command uses C1.8, and the combination of C1.8 and D1.6 does not work. Finally, we have broken the Go modules builds! But look! The Dep builds are using C1.8 and D1.6 too, so theyre also broken. Before, Dep had to make a choice between the latest B and the latest C. If it chose the latest B, the build broke. If it chose the latest C, the build worked. The new requirement in A is forcing Dep to choose the latest B and the latest D, taking away Deps choice of latest C. So Dep uses the older C1.8, and the build breaks just like before. What should we conclude from all this? First of all, documenting an incompatibility for Dep does not guarantee to avoid that incompatibility. Second, a repeatable build like in Go modules also does not guarantee to avoid the incompatibility. Both tools can end up building the incompatible pair of packages. But as we saw, it takes multiple intentional steps to lead Go modules to a broken build, steps that lead Dep to the same broken build. And along the way the Dep-based build broke two other times when the Go modules build did not. Ive been using Dep in these examples because it is the immediate predecessor of Go modules, but I dont mean to single out Dep. In this respect, it works the same way as nearly every other package manager in every other language. They all have this problem. Theyre not even really broken or misbehaving so much as unfortunately designed. They are designed to try to work around a lack of cooperation among the various package maintainers, and tools cannot work around a lack of cooperation. The only real solution for the C versus D incompatibility is to release a new, fixed version of either C or D. Trying to avoid the incompatibility is useful only because it creates more time for Cs author and Ds author to cooperate on a fix. Compared to the Dep approach of preferring latest versions and documenting incompatibilities, the Go modules approach of repeatable builds with minimal version selection and no documented incompatibilities creates time for cooperation automatically, with no build emergencies, no declared incompatibilities, and no explicit work by users. Then we can rely on cooperation for the real fix. Conclusion These are the three principles of versioning in Go, the reasons that the design of Go modules deviates from the design of Dep, Cargo, Bundler, and others. Compatibility. The meaning of a name in a program should not change over time. Repeatability. The result of a build of a given version of a package should not change over time. Cooperation. To maintain the Go package ecosystem, we must all work together. Tools cannot work around a lack of cooperation. These principles are motivated by concerns about software engineering, which is what happens to programming when you add time and other programmers. Compatibility eliminates the effects of time on the meaning of a program. Repeatability eliminates the effects of time on the result of a build. Cooperation is an explicit recognition that, no matter how advanced our tools are, we do have to work with the other programmers. We cant work around them. The three principles also reinforce each other, in a virtuous cycle. Compatibility enables a new version selection algorithm, which provides repeatability. Repeatability makes sure that buggy, new releases are ignored until explicitly requested, which creates more time to cooperate on fixes. That cooperation in turn reestablishes compatibility. And the cycle goes around. As of Go 1.13, Go modules are ready for production use, and many companies, including Google, have adopted them. The Go 1.14 and Go 1.15 releases will bring additional ergonomic improvements, toward eventually deprecating and removing support for GOPATH. For more about adopting modules, see the blog post series on the Go blog, starting with Using Go Modules. ",
        "_version_": 1718527438551515138
      },
      {
        "story_id": [21415488],
        "story_author": ["9nGQluzmnq3M"],
        "story_descendants": [63],
        "story_score": [228],
        "story_time": ["2019-11-01T01:22:07Z"],
        "story_title": "Twelve-factor app development on Google Cloud",
        "search": [
          "Twelve-factor app development on Google Cloud",
          "https://cloud.google.com/solutions/twelve-factor-app-development-on-gcp",
          "This document describes the popular twelve-factor app methodology and how to apply it when you develop apps that run on Google Cloud. If you use this methodology, you can make scalable and resilient apps that can be continuously deployed with maximum agility. This document is intended for developers who are familiar with Google Cloud, version control, continuous integration, and container technology. Introduction Developers are moving apps to the cloud, and in doing so, they become more experienced at designing and deploying cloud-native apps. From that experience, a set of best practices, commonly known as the twelve factors, has emerged. Designing an app with these factors in mind lets you deploy apps to the cloud that are more portable and resilient when compared to apps deployed to on-premises environments where it takes longer to provision new resources. However, designing modern, cloud-native apps requires a change in how you think about software engineering, configuration, and deployment, when compared to designing apps for on-premises environments. This document helps you understand how to apply the twelve factors to your app design. Advantages of twelve-factor apps Twelve-factor design also helps you decouple components of the app, so that each component can be replaced easily, or scaled up or down seamlessly. Because the factors are independent of any programming language or software stack, twelve-factor design can be applied to a wide variety of apps. The twelve factors 1. Codebase You should track code for your app in a version control system such as Git or Mercurial. You work on the app by checking out the code to your local development environment. Storing the code in a version control system enables your team to work together by providing an audit trail of changes to the code, a systematic way of resolving merge conflicts, and the ability to roll back the code to a previous version. It also provides a place from which to do continuous integration (CI) and continuous deployment (CD). While developers might be working on different versions of the code in their development environments, at any given time the source of truth is the code in the version control system. The code in the repository is what gets built, tested, and deployed, and the number of repositories is independent of the number of environments. The code in the repository is used to produce a single build, which is combined with environment-specific configuration to produce an immutable releasea release where no changes can be made, including to the configurationthat can then be deployed to an environment. (Any changes required for the release should result in a new release.) Cloud Source Repositories enables you to collaborate and manage your code in a fully featured, scalable, private Git repository. It comes with code-search functionality across all repositories. You can also connect to other Google Cloud products such as Cloud Build, App Engine, Cloud Logging, and Pub/Sub. 2. Dependencies There are two considerations when it comes to dependencies for twelve-factor apps: dependency declaration and dependency isolation. Twelve-factor apps should never have implicit dependencies. You should declare any dependencies explicitly and check these dependencies into version control. This enables you to get started with the code quickly in a repeatable way and makes it easy to track changes to dependencies. Many programming languages offer a way to explicitly declare dependencies, such as pip for Python and Bundler for Ruby. You should also isolate an app and its dependencies by packaging them into a container. Containers allow you to isolate an app and its dependencies from its environment and ensure that the app works uniformly despite any differences between development and staging environments. Container Registry is a single place for your team to manage container images and perform vulnerability analysis. It also lets you decide who can access what, using fine-grained access control to the container images. Because Container Registry uses a Cloud Storage bucket as the backend for serving container images, you can control who has access to your Container Registry images by adjusting permissions for this Cloud Storage bucket. Existing CI/CD integrations also let you set up fully automated pipelines to get fast feedback. You can push images to their registry, and then pull images using an HTTP endpoint from any machine, whether it's a Compute Engine instance or your own hardware. Container Analysis can then provide vulnerability information for the images in Container Registry. 3. Configuration Every modern app requires some form of configuration. You usually have different configurations for each environment, such as development, test, and production. These configurations usually include service account credentials and resource handles to backing services such as databases. The configurations for each environment should be external to the code and should not be checked into version control. Everyone works on only one version of the code, but you have multiple configurations. The deployment environment determines which configuration to use. This enables one version of the binary to be deployed to each environment, where the only difference is the runtime configuration. An easy way to check whether the configuration has been externalized correctly is to see if the code can be made public without revealing any credentials. One way of externalizing configuration is to create configuration files. However, configuration files are usually specific to a language or development framework. A better approach is to store configuration in environment variables. These are easy to change for each environment at runtime, are not likely to be checked into version control, and are independent of programming language and development framework. In Google Kubernetes Engine (GKE), you can use ConfigMaps. This lets you bind environment variables, port numbers, configuration files, command-line arguments, and other configuration artifacts to your pods' containers and system components at runtime. 4. Backing services Every service that the app consumes as part of its normal operation, such as file systems, databases, caching systems, and message queues, should be accessed as a service and externalized in the configuration. You should think of these backing services as abstractions for the underlying resource. For example, when the app writes data to storage, treating the storage as a backing service allows you to seamlessly change the underlying storage type, because it's decoupled from the app. You can then perform a change such as switching from a local PostgreSQL database to Cloud SQL for PostgreSQL without changing the app code. 5. Build, release, run It's important to separate the software deployment process into three distinct stages: build, release, and run. Each stage should result in an artifact that's uniquely identifiable. Every deployment should be linked to a specific release that's a result of combining an environment's configuration with a build. This allows easy rollbacks and a visible audit trail of the history of every production deployment. You can manually trigger the build stage, but it's usually triggered automatically when you commit code that has passed all of the required tests. The build stage takes the code, fetches the required libraries and assets, and packages these into a self-contained binary or container. The result of the build stage is a build artifact. When the build stage is complete, the release stage combines the build artifact with the configuration of a specific environment. This produces a release. The release can be automatically deployed into the environment by a continuous deployment app. Or you can trigger the release through the same continuous deployment app. Finally, the run stage launches the release and starts it. For example, if you're deploying to GKE, Cloud Build can call the gke-deploy build step to deploy to your GKE cluster. Cloud Build can manage and automate the build, release, and run stages across multiple languages and environments using build configuration files in YAML or JSON format. 6. Processes You run twelve-factor apps in the environment as one or more processes. These processes should be stateless and should not share data with each other. This allows the apps to scale up through replication of their processes. Creating stateless apps also make processes portable across the computing infrastructure. If you're used to the concept of \"sticky\" sessions, this requires a change in how you think about handling and persisting data. Because processes can go away at any time, you can't rely on the contents of local storage being available, or that any subsequent request will be handled by the same process. Therefore, you must explicitly persist any data that needs to be reused in an external backing service such as a database. If you need to persist data, you can use Memorystore as a backing service to cache the state for your apps, and to share common data between processes to encourage loose coupling. 7. Port binding In non-cloud environments, web apps are often written to run in app containers such as GlassFish, Apache Tomcat, and Apache HTTP Server. In contrast, twelve-factor apps don't rely on external app containers. Instead, they bundle the webserver library as a part of the app itself. It's an architectural best practices for services to expose a port number, specified by the PORT environment variable. Apps that export port binding are able to consume port binding information externally (as environment variables) when using the platform-as-a-service model. In Google Cloud, you can deploy apps on platform services such as Compute Engine, GKE, App Engine, or Cloud Run. In these services, a routing layer routes requests from a public-facing hostname to your port-bound web processes. For example, when you deploy your apps to App Engine, you declare dependencies to add a webserver library to the app, such as Express (for Node.js), Flask and Gunicorn (for Python), or Jetty (for Java). You should not hard-code port numbers in your code. Instead, you should provide the port numbers in the environment, such as in an environment variable. This makes your apps portable when you run them on Google Cloud. Because Kubernetes has built-in service discovery, in Kubernetes you can abstract port bindings by mapping service ports to containers. Service discovery is accomplished using internal DNS names. Instead of hard-coding the port that the webserver listens on, the configuration uses an environment variable. The following code snippet from an App Engine app shows how to accept a port value that's passed in an environment variable. const express = require('express') const request = require('got') const app = express() app.enable('trust proxy') const PORT = process.env.PORT || 8080 app.listen(PORT, () => { console.log('App listening on port ${PORT}') console.log('Press Ctrl+C to quit.') }) 8. Concurrency You should decompose your app into independent processes based on process types such as background, web, and worker processes. This enables your app to scale up and down based on individual workload requirements. Most cloud-native apps let you scale on demand. You should design the app as multiple distributed processes that are able to independently execute blocks of work and scale out by adding more processes. The following sections describe some constructs to make it possible for apps to scale. Apps that are built with the principles of disposability and statelessness at their core are well positioned to gain from these constructs of horizontal scaling. Using App Engine You can host your apps on Google Cloud's managed infrastructure using App Engine. Instances are the computing units that App Engine uses to automatically scale your app. At any given time, your app can be running on one instance or on many instances, with requests being spread across all of them. The App Engine scheduler decides how to serve each new request. The scheduler might use an existing instance (either one that's idle or one that accepts concurrent requests), put the request in a pending request queue, or start a new instance for that request. The decision takes into account the number of available instances, how quickly your app has been serving requests (its latency), and how long it takes to start a new instance. If you use automatic scaling, you can create a balance between performance and cost by setting target CPU utilization, target throughput, and maximum concurrent requests. You can specify the type of scaling in the app.yaml file, which you upload for your service version. Based on this configuration input, the App Engine infrastructure uses dynamic or resident instances. For more information on scaling types, see the App Engine documentation. Using Compute Engine Alternatively, you can deploy and manage your apps on Compute Engine. In that case, you can scale your app to respond to variable loads using managed instance groups (MIG) based on CPU utilization, requests being handled, or other telemetry signals from your app. The following figure illustrates the key features that managed instance groups provide. Using managed instance groups lets your app scale to incoming demand and be highly available. This concept works inherently well for stateless apps such as web front-ends and for batch-based, high-performance workloads. Using Cloud Functions Cloud Functions are stateless, single-purpose functions that run on Google Cloud, where the underlying architecture on which they run is managed for you by Google. Cloud Functions respond to event triggers such as an upload into a Cloud Storage bucket or a Pub/Sub message. Each function invocation responds to a single event or request. Cloud Functions handles incoming requests by assigning them to instances of your function. When inbound request volume exceeds the number of existing instances, Cloud Functions might start new instances to handle requests. This automatic, fully managed scaling behavior allows Cloud Functions to handle many requests in parallel, each using a different instance of your function. Using GKE autoscaling There are some key Kubernetes constructs that apply to scaling processes: Horizontal Pod Autoscaling (HPA). Kubernetes can be configured to scale up or down the number of pods running in the cluster based on standard or custom metrics. This comes in handy when you need to respond to a variable load on your GKE cluster. The following sample HPA YAML file shows how to configure scaling for the deployment by setting up to 10 pods based on the average CPU utilization. apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: my-sample-web-app-hpa namespace: dev spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: my-sample-web-app minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 60 Node autoscaling. In times of increased demand, you might need to scale your cluster to accommodate more pods. With GKE, you can declaratively configure your cluster to scale. With autoscaling enabled, GKE automatically scales nodes when additional pods need to be scheduled and when existing nodes are unable to accommodate them. GKE also scales down nodes when the load on the cluster decreases, according to the thresholds you've configured. Jobs. GKE supports Kubernetes jobs. A job can be broadly defined as a task that needs one or more pods to run in order to execute the task. The job might run one time or on a schedule. The pods in which the job runs are disposed of when the job completes. The YAML file that configures the job specifies details on error handling, parallelism, how restarts are handled, and so on. 9. Disposability For apps that run on cloud infrastructure, you should treat them and the underlying infrastructure as disposable resources. Your apps should be able to handle the temporary loss of underlying infrastructure and should be able to gracefully shut down and restart. Key tenets to consider include the following: Decouple functionality such as state management and storage of transactional data using backing services. For more information, see Backing services earlier in this document. Manage environment variables outside of the app so that they can be used at runtime. Make sure that the startup time is minimal. This means that you must decide how much layering to build into images when you use virtual machines, such as public versus custom images. This decision is specific to each app and should be based on the tasks that startup scripts perform. For example, if you're downloading several packages or binaries and initializing them during startup, a sizeable portion of your start-up time will be dedicated to completing these tasks. Use native features of Google Cloud to perform infrastructure tasks. For example, you can use rolling updates in GKE and manage your security keys using Cloud Key Management Service (Cloud KMS). Use the SIGTERM signal (when it's available) to initiate a clean shutdown. For example, when App Engine Flex shuts down an instance, it normally sends a STOP (SIGTERM) signal to the app container. Your app can use this signal to perform any clean-up actions before the container shuts down. (Your app does not need to respond to the SIGTERM event.) Under normal conditions, the system waits up to 30 seconds for the app to stop and then sends a KILL (SIGKILL) signal. The following snippet in an App Engine app shows you how you can intercept the SIGTERM signal to close open database connections. const express = require('express') const dbConnection = require('./db') // Other business logic related code app.listen(PORT, () => { console.log('App listening on port ${PORT}') console.log('Press Ctrl+C to quit.') }) process.on('SIGTERM', () => { console.log('App Shutting down') dbConnection.close() // Other closing of database connection }) 10. Environment parity Enterprise apps move across different environments during their development lifecycle. Typically, these environments are development, testing and staging, and production. It's a good practice to keep these environments as similar as possible. Environment parity is a feature that most developers consider a given. Nonetheless, as enterprises grow and their IT ecosystems evolve, environment parity becomes more difficult to maintain. Maintaining environment parity has become easier in the last few years because developers have embraced source control, configuration management, and templated configuration files. This makes it easier to deploy an app to multiple environments consistently. As an example, using Docker and Docker Compose, you can ensure that the app stack retains its shape and instrumentation across environments. The following table lists Google Cloud services and tools that you can use when you design apps to run on Google Cloud. These components serve different purposes and collectively help you build workflows that make your environments more consistent. GCP component Purpose Cloud Source Repositories A single place for your team to store, manage, and track code. Cloud Storage, Cloud Source Repositories Store build artifacts. Cloud KMS Store your cryptographic keys in one central cloud service for direct use by other cloud resources and applications. Cloud Storage Store custom images that you create from from source disks, images, snapshots, or images stored in Cloud Storage. You can use these images to create virtual machine (VM) instances tailored for your apps. Container Registry Store, manage, and secure your Docker container images. Deployment Manager Write flexible template and configuration files and use them to create deployments that use a variety of Google Cloud products. 11. Logs Logs provide you with awareness of the health of your apps. It's important to decouple the collection, processing, and analysis of logs from the core logic of your apps. Decoupling logging is particularly useful when your apps require dynamic scaling and are running on public clouds, because it eliminates the overhead of managing the storage location for logs and the aggregation from distributed (and often ephemeral) VMs. Google Cloud offers a suite of tools that help with the collection, processing, and structured analysis of logs. It's a good practice to install the Cloud Logging Agent in your Compute Engine VMs. (The agent is preinstalled in App Engine and GKE VM images by default.) The agent monitors a preconfigured set of logging locations. The logs generated by your apps running in the VM are collected and streamed to Cloud Logging. When logging is enabled for a GKE cluster, a logging agent is deployed on every node that's part of the cluster. The agent collects logs, enriches the logs with relevant metadata, and persists them in a data store. These logs are available for review using Cloud Logging. If you need more control over what's logged, you can use Fluentd daemonsets. For more information, see Customizing Logging logs for Google Kubernetes Engine with Fluentd. 12. Admin processes Administrative processes usually consist of one-off tasks or timed, repeatable tasks such as generating reports, executing batch scripts, starting database backups, and migrating schemas. The admin processes factor in the twelve-factor manifesto was written with one-off tasks in mind. For cloud-native apps, this factor becomes more relevant when you're creating repeatable tasks, and the guidance in this section is oriented towards tasks like those. Timed triggers are often built as cron jobs and handled inherently by the apps themselves. This model works, but it introduces logic that's tightly coupled to the app and that requires maintenance and coordination, especially if your apps are distributed across time zones. Therefore, when you design for admin processes, you should decouple the management of these tasks from the app itself. Depending on the tools and infrastructure that your app runs on, use the following suggestions: For running apps on GKE, start separate containers for admin tasks. You can take advantage of CronJobs in GKE. CronJobs run in ephemeral containers and let you control the timing, execution frequency, and retries if jobs fail or if they take too long to complete. For hosting apps on App Engine or Compute Engine, you can externalize the triggering mechanism and create an endpoint for the trigger to invoke. This approach helps define a boundary around what the apps are responsible for, in contrast to the single-purpose focus of the endpoint. Cloud Tasks is a fully managed, asynchronous task execution service that you can use to implement this pattern with App Engine. You can also use Cloud Scheduler, an enterprise-grade, fully managed scheduler on Google Cloud, to trigger timed operations. Going beyond the twelve factors The twelve factors described in this document offer guidance for how you should approach building cloud-native apps. These apps are the foundational building blocks of an enterprise. A typical enterprise has many apps like these, often developed by several teams collaboratively to deliver business capability. It's important to establish some additional principles during the app development lifecycle, and not as an afterthought, to address how apps communicate with each other, and how they are secured and access controlled. The following sections outline some of these additional considerations that you should make during app design and development. Think API-first Apps communicate using APIs. When you're building apps, think about how the app will be consumed by your app's ecosystem, and start by designing an API strategy. A good API design makes the API easy to consume by app developers and external stakeholders. It's a good practice to start by documenting the API using the OpenAPI specification before you implement any code. APIs abstract the underlying app functionality. A well-designed API endpoint should isolate and decouple the consuming applications from the app infrastructure that provides the service. This decoupling gives you the ability to change the underlying service and its infrastructure independently, without impacting the app's consumers. It's important to catalog, document, and publish the APIs that you develop so that the consumers of the APIs are able to discover and use the APIs. Ideally, you want the API consumers to serve themselves. You can accomplish this by setting up a developer portal. A developer portal serves as an entry point for all API consumers internal for the enterprise, or external for consumers such as developers from your partner ecosystem. Apigee, Google's API management product suite, helps you manage the entire lifecycle of your APIs from design, to build, to publish. Security The realm of security is wide, and includes operating systems, networks and firewalls, data and database security, app security, and identity and access management. It's of paramount importance to address all the dimensions of security in an enterprise's ecosystem. From an app's point of view, APIs provide access to the apps in your enterprise ecosystem. You should therefore ensure that these building blocks address security considerations during the app design and build process. Considerations for helping to protect access to your app include the following: Transport Layer Security (TLS). Use TLS to help protect data in transit. You might want to use mutual TLS for your business apps; this is made easier if you use service meshes like Istio on Google Kubernetes Engine. It's also common for some use cases to create allow lists and deny lists based on IP addresses as an additional layer of security. Transport security also involves protecting your services against DDoS and bot attacks. App and end-user security. Transport security helps provide security for data in transit and establishes trust. But it's a best practice to add app-level security to control access to your app based on who the consumer of the app is. The consumers can be other apps, employees, partners, or your enterprise's end customers. You can enforce security using API keys (for consuming apps), certification-based authentication and authorization, JSON Web Tokens (JWTs) exchange, or Security Assertion Markup Language (SAML). The security landscape constantly evolves within an enterprise, making it harder for you to code security constructs in your apps. API management products like Apigee help secure APIs at all the layers mentioned in this section. What's next Review the microservices demo app that employs twelve-factor app principles and is built using Google Cloud products and services. Review the Google Cloud product suite for logging and monitoring; see the Logging documentation. Explore reference architectures, diagrams, tutorials, and best practices about Google Cloud. Take a look at our Cloud Architecture Center. ",
          "12 factor seems to have stood the test of time really well—I was introduced via Heroku (who I think invented it?) quite a long time ago in tech years, and yet it still seems to be probably the most popular ‘framework’ for devops.<p>In fact, my startup EnvKey[1] was heavily inspired by the 12 factor approach. While it has always worked well for me, one bit that always felt thorny was using the environment for configuration and secrets. It’s obviously great to get this stuff out of code, but then you face new issues on how to keep it all in sync across many environments and pass potentially highly sensitive data around securely.<p>EnvKey fills in this gap using end-to-end encryption and a seamless integration that builds on top of environment variables—it’s a drop-in replacement if you already use the environment for config. Check it out if you’re looking for something to smooth out this aspect of 12 factor! We have lots of folks using it with GCP/GKE.<p>1 - <a href=\"https://www.envkey.com\" rel=\"nofollow\">https://www.envkey.com</a>",
          "I'm the author of 12factor (although really it is an aggregation of the work and insights from many people at Heroku). It continues to surprise and please me that this piece continues to be relevant eight years later—a virtual eternity in software/internet time.<p>Fun fact: I debated whether to call it \"the Heroku way\" or somesuch. Glad I went with a standalone name, feel like that allowed it to take on a life beyond that product. For example I doubt Google would have wanted a page about \"Heroku Way app development on GCP\" in their documentation. :-)"
        ],
        "story_type": ["Normal"],
        "url": "https://cloud.google.com/solutions/twelve-factor-app-development-on-gcp",
        "comments.comment_id": [21416076, 21416881],
        "comments.comment_author": ["danenania", "adamwiggins"],
        "comments.comment_descendants": [4, 3],
        "comments.comment_time": [
          "2019-11-01T03:34:20Z",
          "2019-11-01T07:37:19Z"
        ],
        "comments.comment_text": [
          "12 factor seems to have stood the test of time really well—I was introduced via Heroku (who I think invented it?) quite a long time ago in tech years, and yet it still seems to be probably the most popular ‘framework’ for devops.<p>In fact, my startup EnvKey[1] was heavily inspired by the 12 factor approach. While it has always worked well for me, one bit that always felt thorny was using the environment for configuration and secrets. It’s obviously great to get this stuff out of code, but then you face new issues on how to keep it all in sync across many environments and pass potentially highly sensitive data around securely.<p>EnvKey fills in this gap using end-to-end encryption and a seamless integration that builds on top of environment variables—it’s a drop-in replacement if you already use the environment for config. Check it out if you’re looking for something to smooth out this aspect of 12 factor! We have lots of folks using it with GCP/GKE.<p>1 - <a href=\"https://www.envkey.com\" rel=\"nofollow\">https://www.envkey.com</a>",
          "I'm the author of 12factor (although really it is an aggregation of the work and insights from many people at Heroku). It continues to surprise and please me that this piece continues to be relevant eight years later—a virtual eternity in software/internet time.<p>Fun fact: I debated whether to call it \"the Heroku way\" or somesuch. Glad I went with a standalone name, feel like that allowed it to take on a life beyond that product. For example I doubt Google would have wanted a page about \"Heroku Way app development on GCP\" in their documentation. :-)"
        ],
        "id": "fd53cc04-558f-4c5b-90b7-cad679b14e15",
        "url_text": "This document describes the popular twelve-factor app methodology and how to apply it when you develop apps that run on Google Cloud. If you use this methodology, you can make scalable and resilient apps that can be continuously deployed with maximum agility. This document is intended for developers who are familiar with Google Cloud, version control, continuous integration, and container technology. Introduction Developers are moving apps to the cloud, and in doing so, they become more experienced at designing and deploying cloud-native apps. From that experience, a set of best practices, commonly known as the twelve factors, has emerged. Designing an app with these factors in mind lets you deploy apps to the cloud that are more portable and resilient when compared to apps deployed to on-premises environments where it takes longer to provision new resources. However, designing modern, cloud-native apps requires a change in how you think about software engineering, configuration, and deployment, when compared to designing apps for on-premises environments. This document helps you understand how to apply the twelve factors to your app design. Advantages of twelve-factor apps Twelve-factor design also helps you decouple components of the app, so that each component can be replaced easily, or scaled up or down seamlessly. Because the factors are independent of any programming language or software stack, twelve-factor design can be applied to a wide variety of apps. The twelve factors 1. Codebase You should track code for your app in a version control system such as Git or Mercurial. You work on the app by checking out the code to your local development environment. Storing the code in a version control system enables your team to work together by providing an audit trail of changes to the code, a systematic way of resolving merge conflicts, and the ability to roll back the code to a previous version. It also provides a place from which to do continuous integration (CI) and continuous deployment (CD). While developers might be working on different versions of the code in their development environments, at any given time the source of truth is the code in the version control system. The code in the repository is what gets built, tested, and deployed, and the number of repositories is independent of the number of environments. The code in the repository is used to produce a single build, which is combined with environment-specific configuration to produce an immutable releasea release where no changes can be made, including to the configurationthat can then be deployed to an environment. (Any changes required for the release should result in a new release.) Cloud Source Repositories enables you to collaborate and manage your code in a fully featured, scalable, private Git repository. It comes with code-search functionality across all repositories. You can also connect to other Google Cloud products such as Cloud Build, App Engine, Cloud Logging, and Pub/Sub. 2. Dependencies There are two considerations when it comes to dependencies for twelve-factor apps: dependency declaration and dependency isolation. Twelve-factor apps should never have implicit dependencies. You should declare any dependencies explicitly and check these dependencies into version control. This enables you to get started with the code quickly in a repeatable way and makes it easy to track changes to dependencies. Many programming languages offer a way to explicitly declare dependencies, such as pip for Python and Bundler for Ruby. You should also isolate an app and its dependencies by packaging them into a container. Containers allow you to isolate an app and its dependencies from its environment and ensure that the app works uniformly despite any differences between development and staging environments. Container Registry is a single place for your team to manage container images and perform vulnerability analysis. It also lets you decide who can access what, using fine-grained access control to the container images. Because Container Registry uses a Cloud Storage bucket as the backend for serving container images, you can control who has access to your Container Registry images by adjusting permissions for this Cloud Storage bucket. Existing CI/CD integrations also let you set up fully automated pipelines to get fast feedback. You can push images to their registry, and then pull images using an HTTP endpoint from any machine, whether it's a Compute Engine instance or your own hardware. Container Analysis can then provide vulnerability information for the images in Container Registry. 3. Configuration Every modern app requires some form of configuration. You usually have different configurations for each environment, such as development, test, and production. These configurations usually include service account credentials and resource handles to backing services such as databases. The configurations for each environment should be external to the code and should not be checked into version control. Everyone works on only one version of the code, but you have multiple configurations. The deployment environment determines which configuration to use. This enables one version of the binary to be deployed to each environment, where the only difference is the runtime configuration. An easy way to check whether the configuration has been externalized correctly is to see if the code can be made public without revealing any credentials. One way of externalizing configuration is to create configuration files. However, configuration files are usually specific to a language or development framework. A better approach is to store configuration in environment variables. These are easy to change for each environment at runtime, are not likely to be checked into version control, and are independent of programming language and development framework. In Google Kubernetes Engine (GKE), you can use ConfigMaps. This lets you bind environment variables, port numbers, configuration files, command-line arguments, and other configuration artifacts to your pods' containers and system components at runtime. 4. Backing services Every service that the app consumes as part of its normal operation, such as file systems, databases, caching systems, and message queues, should be accessed as a service and externalized in the configuration. You should think of these backing services as abstractions for the underlying resource. For example, when the app writes data to storage, treating the storage as a backing service allows you to seamlessly change the underlying storage type, because it's decoupled from the app. You can then perform a change such as switching from a local PostgreSQL database to Cloud SQL for PostgreSQL without changing the app code. 5. Build, release, run It's important to separate the software deployment process into three distinct stages: build, release, and run. Each stage should result in an artifact that's uniquely identifiable. Every deployment should be linked to a specific release that's a result of combining an environment's configuration with a build. This allows easy rollbacks and a visible audit trail of the history of every production deployment. You can manually trigger the build stage, but it's usually triggered automatically when you commit code that has passed all of the required tests. The build stage takes the code, fetches the required libraries and assets, and packages these into a self-contained binary or container. The result of the build stage is a build artifact. When the build stage is complete, the release stage combines the build artifact with the configuration of a specific environment. This produces a release. The release can be automatically deployed into the environment by a continuous deployment app. Or you can trigger the release through the same continuous deployment app. Finally, the run stage launches the release and starts it. For example, if you're deploying to GKE, Cloud Build can call the gke-deploy build step to deploy to your GKE cluster. Cloud Build can manage and automate the build, release, and run stages across multiple languages and environments using build configuration files in YAML or JSON format. 6. Processes You run twelve-factor apps in the environment as one or more processes. These processes should be stateless and should not share data with each other. This allows the apps to scale up through replication of their processes. Creating stateless apps also make processes portable across the computing infrastructure. If you're used to the concept of \"sticky\" sessions, this requires a change in how you think about handling and persisting data. Because processes can go away at any time, you can't rely on the contents of local storage being available, or that any subsequent request will be handled by the same process. Therefore, you must explicitly persist any data that needs to be reused in an external backing service such as a database. If you need to persist data, you can use Memorystore as a backing service to cache the state for your apps, and to share common data between processes to encourage loose coupling. 7. Port binding In non-cloud environments, web apps are often written to run in app containers such as GlassFish, Apache Tomcat, and Apache HTTP Server. In contrast, twelve-factor apps don't rely on external app containers. Instead, they bundle the webserver library as a part of the app itself. It's an architectural best practices for services to expose a port number, specified by the PORT environment variable. Apps that export port binding are able to consume port binding information externally (as environment variables) when using the platform-as-a-service model. In Google Cloud, you can deploy apps on platform services such as Compute Engine, GKE, App Engine, or Cloud Run. In these services, a routing layer routes requests from a public-facing hostname to your port-bound web processes. For example, when you deploy your apps to App Engine, you declare dependencies to add a webserver library to the app, such as Express (for Node.js), Flask and Gunicorn (for Python), or Jetty (for Java). You should not hard-code port numbers in your code. Instead, you should provide the port numbers in the environment, such as in an environment variable. This makes your apps portable when you run them on Google Cloud. Because Kubernetes has built-in service discovery, in Kubernetes you can abstract port bindings by mapping service ports to containers. Service discovery is accomplished using internal DNS names. Instead of hard-coding the port that the webserver listens on, the configuration uses an environment variable. The following code snippet from an App Engine app shows how to accept a port value that's passed in an environment variable. const express = require('express') const request = require('got') const app = express() app.enable('trust proxy') const PORT = process.env.PORT || 8080 app.listen(PORT, () => { console.log('App listening on port ${PORT}') console.log('Press Ctrl+C to quit.') }) 8. Concurrency You should decompose your app into independent processes based on process types such as background, web, and worker processes. This enables your app to scale up and down based on individual workload requirements. Most cloud-native apps let you scale on demand. You should design the app as multiple distributed processes that are able to independently execute blocks of work and scale out by adding more processes. The following sections describe some constructs to make it possible for apps to scale. Apps that are built with the principles of disposability and statelessness at their core are well positioned to gain from these constructs of horizontal scaling. Using App Engine You can host your apps on Google Cloud's managed infrastructure using App Engine. Instances are the computing units that App Engine uses to automatically scale your app. At any given time, your app can be running on one instance or on many instances, with requests being spread across all of them. The App Engine scheduler decides how to serve each new request. The scheduler might use an existing instance (either one that's idle or one that accepts concurrent requests), put the request in a pending request queue, or start a new instance for that request. The decision takes into account the number of available instances, how quickly your app has been serving requests (its latency), and how long it takes to start a new instance. If you use automatic scaling, you can create a balance between performance and cost by setting target CPU utilization, target throughput, and maximum concurrent requests. You can specify the type of scaling in the app.yaml file, which you upload for your service version. Based on this configuration input, the App Engine infrastructure uses dynamic or resident instances. For more information on scaling types, see the App Engine documentation. Using Compute Engine Alternatively, you can deploy and manage your apps on Compute Engine. In that case, you can scale your app to respond to variable loads using managed instance groups (MIG) based on CPU utilization, requests being handled, or other telemetry signals from your app. The following figure illustrates the key features that managed instance groups provide. Using managed instance groups lets your app scale to incoming demand and be highly available. This concept works inherently well for stateless apps such as web front-ends and for batch-based, high-performance workloads. Using Cloud Functions Cloud Functions are stateless, single-purpose functions that run on Google Cloud, where the underlying architecture on which they run is managed for you by Google. Cloud Functions respond to event triggers such as an upload into a Cloud Storage bucket or a Pub/Sub message. Each function invocation responds to a single event or request. Cloud Functions handles incoming requests by assigning them to instances of your function. When inbound request volume exceeds the number of existing instances, Cloud Functions might start new instances to handle requests. This automatic, fully managed scaling behavior allows Cloud Functions to handle many requests in parallel, each using a different instance of your function. Using GKE autoscaling There are some key Kubernetes constructs that apply to scaling processes: Horizontal Pod Autoscaling (HPA). Kubernetes can be configured to scale up or down the number of pods running in the cluster based on standard or custom metrics. This comes in handy when you need to respond to a variable load on your GKE cluster. The following sample HPA YAML file shows how to configure scaling for the deployment by setting up to 10 pods based on the average CPU utilization. apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: my-sample-web-app-hpa namespace: dev spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: my-sample-web-app minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 60 Node autoscaling. In times of increased demand, you might need to scale your cluster to accommodate more pods. With GKE, you can declaratively configure your cluster to scale. With autoscaling enabled, GKE automatically scales nodes when additional pods need to be scheduled and when existing nodes are unable to accommodate them. GKE also scales down nodes when the load on the cluster decreases, according to the thresholds you've configured. Jobs. GKE supports Kubernetes jobs. A job can be broadly defined as a task that needs one or more pods to run in order to execute the task. The job might run one time or on a schedule. The pods in which the job runs are disposed of when the job completes. The YAML file that configures the job specifies details on error handling, parallelism, how restarts are handled, and so on. 9. Disposability For apps that run on cloud infrastructure, you should treat them and the underlying infrastructure as disposable resources. Your apps should be able to handle the temporary loss of underlying infrastructure and should be able to gracefully shut down and restart. Key tenets to consider include the following: Decouple functionality such as state management and storage of transactional data using backing services. For more information, see Backing services earlier in this document. Manage environment variables outside of the app so that they can be used at runtime. Make sure that the startup time is minimal. This means that you must decide how much layering to build into images when you use virtual machines, such as public versus custom images. This decision is specific to each app and should be based on the tasks that startup scripts perform. For example, if you're downloading several packages or binaries and initializing them during startup, a sizeable portion of your start-up time will be dedicated to completing these tasks. Use native features of Google Cloud to perform infrastructure tasks. For example, you can use rolling updates in GKE and manage your security keys using Cloud Key Management Service (Cloud KMS). Use the SIGTERM signal (when it's available) to initiate a clean shutdown. For example, when App Engine Flex shuts down an instance, it normally sends a STOP (SIGTERM) signal to the app container. Your app can use this signal to perform any clean-up actions before the container shuts down. (Your app does not need to respond to the SIGTERM event.) Under normal conditions, the system waits up to 30 seconds for the app to stop and then sends a KILL (SIGKILL) signal. The following snippet in an App Engine app shows you how you can intercept the SIGTERM signal to close open database connections. const express = require('express') const dbConnection = require('./db') // Other business logic related code app.listen(PORT, () => { console.log('App listening on port ${PORT}') console.log('Press Ctrl+C to quit.') }) process.on('SIGTERM', () => { console.log('App Shutting down') dbConnection.close() // Other closing of database connection }) 10. Environment parity Enterprise apps move across different environments during their development lifecycle. Typically, these environments are development, testing and staging, and production. It's a good practice to keep these environments as similar as possible. Environment parity is a feature that most developers consider a given. Nonetheless, as enterprises grow and their IT ecosystems evolve, environment parity becomes more difficult to maintain. Maintaining environment parity has become easier in the last few years because developers have embraced source control, configuration management, and templated configuration files. This makes it easier to deploy an app to multiple environments consistently. As an example, using Docker and Docker Compose, you can ensure that the app stack retains its shape and instrumentation across environments. The following table lists Google Cloud services and tools that you can use when you design apps to run on Google Cloud. These components serve different purposes and collectively help you build workflows that make your environments more consistent. GCP component Purpose Cloud Source Repositories A single place for your team to store, manage, and track code. Cloud Storage, Cloud Source Repositories Store build artifacts. Cloud KMS Store your cryptographic keys in one central cloud service for direct use by other cloud resources and applications. Cloud Storage Store custom images that you create from from source disks, images, snapshots, or images stored in Cloud Storage. You can use these images to create virtual machine (VM) instances tailored for your apps. Container Registry Store, manage, and secure your Docker container images. Deployment Manager Write flexible template and configuration files and use them to create deployments that use a variety of Google Cloud products. 11. Logs Logs provide you with awareness of the health of your apps. It's important to decouple the collection, processing, and analysis of logs from the core logic of your apps. Decoupling logging is particularly useful when your apps require dynamic scaling and are running on public clouds, because it eliminates the overhead of managing the storage location for logs and the aggregation from distributed (and often ephemeral) VMs. Google Cloud offers a suite of tools that help with the collection, processing, and structured analysis of logs. It's a good practice to install the Cloud Logging Agent in your Compute Engine VMs. (The agent is preinstalled in App Engine and GKE VM images by default.) The agent monitors a preconfigured set of logging locations. The logs generated by your apps running in the VM are collected and streamed to Cloud Logging. When logging is enabled for a GKE cluster, a logging agent is deployed on every node that's part of the cluster. The agent collects logs, enriches the logs with relevant metadata, and persists them in a data store. These logs are available for review using Cloud Logging. If you need more control over what's logged, you can use Fluentd daemonsets. For more information, see Customizing Logging logs for Google Kubernetes Engine with Fluentd. 12. Admin processes Administrative processes usually consist of one-off tasks or timed, repeatable tasks such as generating reports, executing batch scripts, starting database backups, and migrating schemas. The admin processes factor in the twelve-factor manifesto was written with one-off tasks in mind. For cloud-native apps, this factor becomes more relevant when you're creating repeatable tasks, and the guidance in this section is oriented towards tasks like those. Timed triggers are often built as cron jobs and handled inherently by the apps themselves. This model works, but it introduces logic that's tightly coupled to the app and that requires maintenance and coordination, especially if your apps are distributed across time zones. Therefore, when you design for admin processes, you should decouple the management of these tasks from the app itself. Depending on the tools and infrastructure that your app runs on, use the following suggestions: For running apps on GKE, start separate containers for admin tasks. You can take advantage of CronJobs in GKE. CronJobs run in ephemeral containers and let you control the timing, execution frequency, and retries if jobs fail or if they take too long to complete. For hosting apps on App Engine or Compute Engine, you can externalize the triggering mechanism and create an endpoint for the trigger to invoke. This approach helps define a boundary around what the apps are responsible for, in contrast to the single-purpose focus of the endpoint. Cloud Tasks is a fully managed, asynchronous task execution service that you can use to implement this pattern with App Engine. You can also use Cloud Scheduler, an enterprise-grade, fully managed scheduler on Google Cloud, to trigger timed operations. Going beyond the twelve factors The twelve factors described in this document offer guidance for how you should approach building cloud-native apps. These apps are the foundational building blocks of an enterprise. A typical enterprise has many apps like these, often developed by several teams collaboratively to deliver business capability. It's important to establish some additional principles during the app development lifecycle, and not as an afterthought, to address how apps communicate with each other, and how they are secured and access controlled. The following sections outline some of these additional considerations that you should make during app design and development. Think API-first Apps communicate using APIs. When you're building apps, think about how the app will be consumed by your app's ecosystem, and start by designing an API strategy. A good API design makes the API easy to consume by app developers and external stakeholders. It's a good practice to start by documenting the API using the OpenAPI specification before you implement any code. APIs abstract the underlying app functionality. A well-designed API endpoint should isolate and decouple the consuming applications from the app infrastructure that provides the service. This decoupling gives you the ability to change the underlying service and its infrastructure independently, without impacting the app's consumers. It's important to catalog, document, and publish the APIs that you develop so that the consumers of the APIs are able to discover and use the APIs. Ideally, you want the API consumers to serve themselves. You can accomplish this by setting up a developer portal. A developer portal serves as an entry point for all API consumers internal for the enterprise, or external for consumers such as developers from your partner ecosystem. Apigee, Google's API management product suite, helps you manage the entire lifecycle of your APIs from design, to build, to publish. Security The realm of security is wide, and includes operating systems, networks and firewalls, data and database security, app security, and identity and access management. It's of paramount importance to address all the dimensions of security in an enterprise's ecosystem. From an app's point of view, APIs provide access to the apps in your enterprise ecosystem. You should therefore ensure that these building blocks address security considerations during the app design and build process. Considerations for helping to protect access to your app include the following: Transport Layer Security (TLS). Use TLS to help protect data in transit. You might want to use mutual TLS for your business apps; this is made easier if you use service meshes like Istio on Google Kubernetes Engine. It's also common for some use cases to create allow lists and deny lists based on IP addresses as an additional layer of security. Transport security also involves protecting your services against DDoS and bot attacks. App and end-user security. Transport security helps provide security for data in transit and establishes trust. But it's a best practice to add app-level security to control access to your app based on who the consumer of the app is. The consumers can be other apps, employees, partners, or your enterprise's end customers. You can enforce security using API keys (for consuming apps), certification-based authentication and authorization, JSON Web Tokens (JWTs) exchange, or Security Assertion Markup Language (SAML). The security landscape constantly evolves within an enterprise, making it harder for you to code security constructs in your apps. API management products like Apigee help secure APIs at all the layers mentioned in this section. What's next Review the microservices demo app that employs twelve-factor app principles and is built using Google Cloud products and services. Review the Google Cloud product suite for logging and monitoring; see the Logging documentation. Explore reference architectures, diagrams, tutorials, and best practices about Google Cloud. Take a look at our Cloud Architecture Center. ",
        "_version_": 1718527433713385472
      },
      {
        "story_id": [18977461],
        "story_author": ["fantasticfears"],
        "story_descendants": [119],
        "story_score": [313],
        "story_time": ["2019-01-23T11:47:11Z"],
        "story_title": "Mendeley encrypts users' database after Zotero provides an importer",
        "search": [
          "Mendeley encrypts users' database after Zotero provides an importer",
          "https://www.zotero.org/support/kb/mendeley_import",
          "How do I import a Mendeley library into Zotero? Zotero can directly import from an online Mendeley library. Due to changes by Mendeley, it's not possible to import from a local Mendeley installation on your computer. To import your Mendeley library, follow these steps: Make sure that all data and files have been synced to Mendeley servers. If you use Mendeley Desktop, check your sync settings to make sure that data and files are being synced, and confirm that you can open PDFs in your online Mendeley library. If you use Mendeley Reference Manager, your data and files are already all online. Install the Zotero Beta, which contains the latest version of the importer as well as a new PDF reader that can display PDF annotations imported from Mendeley. Go to File Import within Zotero and choose the Mendeley Reference Manager (online import) option. You'll be asked to log in to Mendeley to allow Zotero to perform the import. Your Mendeley password is never seen or stored. Alternative Method If for some reason you're not able to perform a direct online import, it's possible to import from a local Mendeley database by installing an old version of Mendeley Desktop from before Mendeley began encrypting the local database. See the local import instructions for more information. Known Issues There are a few issues to be aware of when importing from Mendeley. To import PDF highlight and note annotations, you currently must import using the Zotero Beta, which adds support for annotations. If youve previously imported from Mendeley using Zoteros importer, you can repeat the process with the beta to bring in your highlights and notes. Its not possible to directly import group libraries. To import items in group libraries, simply copy the group items to a collection in your Mendeley library before importing. You can then create a Zotero group and drag imported collections or items to that group. Mendeley allows any field to be added to any type. When importing into Zotero, if a field isnt valid for a given item type, the field is placed into the Extra field. When possible, those will be used automatically in citations (e.g., Original Date), and future versions of Zotero will automatically convert those to any real fields that become available. When using the Zotero word processor plugins, document citations created with Mendeley wont currently be linked to imported citations in your Zotero database. Zoteros word processor plugins can, however, read Mendeley citations and their embedded metadata, so you can continue using the same documents with Zotero. Troubleshooting Make sure youre running the latest version of Zotero available via Help Check for Updates. If youre running the latest version and something doesnt come through how you expect or you run into any trouble, let us know in the Zotero Forums. Mendeley Database Encryption Starting in Mendeley Desktop 1.19, Elsevier began encrypting the local Mendeley database, making it unreadable by Zotero and other standard database tools. Elsevier made this change a few months after Zotero publicly announced work on an importer, despite having long touted the openness of its database format as a guarantee against lock-in and explaining in its documentation that the database could be accessed using standard tools. Until recently, Mendeley Desktop imported data from Zoteros own open database, as it had since 2009. The Mendeley 1.19 release notes claimed that the encryption was for improved security on shared machines, yet applications rarely encrypt their local data files, as file protections are generally handled by the operating system with account permissions and full-disk encryption, and anyone using the same operating system account or an admin account can already install a keylogger to capture passwords. Elsevier later stated that the change was required by new European privacy regulations a bizarre claim, given that those regulations are designed to give people control over their data and guarantee data portability, not the opposite and continued to assert, falsely, that full local export was still possible, while repeatedly dismissing reports of the change as #fakenews. Direct access to the Mendeley database is the only local way to export the full contents of ones own research. The export formats supported by Mendeley dont contain folders, various metadata fields (date added, favorite, and others), or PDF annotations. Mendeley offers a web-based API, but it contains only uploaded data, so relying on it means that anyone wanting to export their own data first needs to upload all their data and files to Elseviers servers. The API is under Elseviers control and can be changed or discontinued at any time. Since making this change, Elsevier has released its replacement for Mendeley Desktop, Mendeley Reference Manager, which is essentially a wrapper around the website and doesnt contain a real local database at all. ",
          "Zotero has improved a lot, while Mendeley has repeatedly regressed.<p>Mendeley used to be quite a good program, but recently you can not export annotated PDFs meaningfully. For example, sending a folder of annotated PDFs to a co-author during a literature review is impossible. This is obviously the case since Elsevier does not want you to trade research papers, whether you have lawfull access or not.<p>The updates that took away features were silent. What happened to me some time ago was these updates occured during a high-stress phase with a short deadline until conference submissions (if you are a researcher, you know what I mean).<p>I had used Mendeley for years to annotate and categorize literature. I was now in need to send my categorized PDFs to a central repository for my co-authors to evaluate and add to. After some update, without me noticing it, it was no longer possible to export folders of PDFs or PDFs in general!<p>I had everything in Mendeley, weeks of work. I was completely f'ed - deadline approaching. I had to re-aquire all PDFs and go through all annotations by hand.<p>ELSEVIER IS SIMPLY ANTI SCIENCE. Collaboration is a key in science. Sharing results, research and literature is crucial.<p>Mendeley makes this impossible. It does NOT allow you to fully access your own work!<p>So in conclusion, USE ZOTERO. It's good now, better than before. You can use a PDF reader with annotations to open and save the PDF and Zotero will keep those annotations. You can export Bibliographies, including notes AND files. You can not do that with Mendeley.<p>So again, as a researcher, I emplore you to drop Mendeley completely, as I have done.<p>Thank you.",
          "For those wondering, here's what I gathered as some context.<p>Zotero = Your personal research assistant.  Zotero is a free, easy-to-use tool to help you collect, organize, cite, and share research.  <a href=\"https://www.zotero.org/\" rel=\"nofollow\">https://www.zotero.org/</a><p>Mendeley = Reference Management Software, produced by Elsevier who also happens to be the publisher of many peer-reviewed journals.  Elsevier come under fire for it's high costs and gateway actions to restrict access to information they've published in journals and host in archives.  This most recent action of making the database of references in Mendeley difficult to export is a continuation of their attempt to protect what they, and some legal systems, would see as their IP.  Others disagree.<p>The battle continues..."
        ],
        "story_type": ["Normal"],
        "url": "https://www.zotero.org/support/kb/mendeley_import",
        "comments.comment_id": [18977834, 18977917],
        "comments.comment_author": ["zwaps", "mwexler"],
        "comments.comment_descendants": [9, 3],
        "comments.comment_time": [
          "2019-01-23T13:17:42Z",
          "2019-01-23T13:32:57Z"
        ],
        "comments.comment_text": [
          "Zotero has improved a lot, while Mendeley has repeatedly regressed.<p>Mendeley used to be quite a good program, but recently you can not export annotated PDFs meaningfully. For example, sending a folder of annotated PDFs to a co-author during a literature review is impossible. This is obviously the case since Elsevier does not want you to trade research papers, whether you have lawfull access or not.<p>The updates that took away features were silent. What happened to me some time ago was these updates occured during a high-stress phase with a short deadline until conference submissions (if you are a researcher, you know what I mean).<p>I had used Mendeley for years to annotate and categorize literature. I was now in need to send my categorized PDFs to a central repository for my co-authors to evaluate and add to. After some update, without me noticing it, it was no longer possible to export folders of PDFs or PDFs in general!<p>I had everything in Mendeley, weeks of work. I was completely f'ed - deadline approaching. I had to re-aquire all PDFs and go through all annotations by hand.<p>ELSEVIER IS SIMPLY ANTI SCIENCE. Collaboration is a key in science. Sharing results, research and literature is crucial.<p>Mendeley makes this impossible. It does NOT allow you to fully access your own work!<p>So in conclusion, USE ZOTERO. It's good now, better than before. You can use a PDF reader with annotations to open and save the PDF and Zotero will keep those annotations. You can export Bibliographies, including notes AND files. You can not do that with Mendeley.<p>So again, as a researcher, I emplore you to drop Mendeley completely, as I have done.<p>Thank you.",
          "For those wondering, here's what I gathered as some context.<p>Zotero = Your personal research assistant.  Zotero is a free, easy-to-use tool to help you collect, organize, cite, and share research.  <a href=\"https://www.zotero.org/\" rel=\"nofollow\">https://www.zotero.org/</a><p>Mendeley = Reference Management Software, produced by Elsevier who also happens to be the publisher of many peer-reviewed journals.  Elsevier come under fire for it's high costs and gateway actions to restrict access to information they've published in journals and host in archives.  This most recent action of making the database of references in Mendeley difficult to export is a continuation of their attempt to protect what they, and some legal systems, would see as their IP.  Others disagree.<p>The battle continues..."
        ],
        "id": "7092e6ed-c846-42ab-8a91-0a780faab7dd",
        "url_text": "How do I import a Mendeley library into Zotero? Zotero can directly import from an online Mendeley library. Due to changes by Mendeley, it's not possible to import from a local Mendeley installation on your computer. To import your Mendeley library, follow these steps: Make sure that all data and files have been synced to Mendeley servers. If you use Mendeley Desktop, check your sync settings to make sure that data and files are being synced, and confirm that you can open PDFs in your online Mendeley library. If you use Mendeley Reference Manager, your data and files are already all online. Install the Zotero Beta, which contains the latest version of the importer as well as a new PDF reader that can display PDF annotations imported from Mendeley. Go to File Import within Zotero and choose the Mendeley Reference Manager (online import) option. You'll be asked to log in to Mendeley to allow Zotero to perform the import. Your Mendeley password is never seen or stored. Alternative Method If for some reason you're not able to perform a direct online import, it's possible to import from a local Mendeley database by installing an old version of Mendeley Desktop from before Mendeley began encrypting the local database. See the local import instructions for more information. Known Issues There are a few issues to be aware of when importing from Mendeley. To import PDF highlight and note annotations, you currently must import using the Zotero Beta, which adds support for annotations. If youve previously imported from Mendeley using Zoteros importer, you can repeat the process with the beta to bring in your highlights and notes. Its not possible to directly import group libraries. To import items in group libraries, simply copy the group items to a collection in your Mendeley library before importing. You can then create a Zotero group and drag imported collections or items to that group. Mendeley allows any field to be added to any type. When importing into Zotero, if a field isnt valid for a given item type, the field is placed into the Extra field. When possible, those will be used automatically in citations (e.g., Original Date), and future versions of Zotero will automatically convert those to any real fields that become available. When using the Zotero word processor plugins, document citations created with Mendeley wont currently be linked to imported citations in your Zotero database. Zoteros word processor plugins can, however, read Mendeley citations and their embedded metadata, so you can continue using the same documents with Zotero. Troubleshooting Make sure youre running the latest version of Zotero available via Help Check for Updates. If youre running the latest version and something doesnt come through how you expect or you run into any trouble, let us know in the Zotero Forums. Mendeley Database Encryption Starting in Mendeley Desktop 1.19, Elsevier began encrypting the local Mendeley database, making it unreadable by Zotero and other standard database tools. Elsevier made this change a few months after Zotero publicly announced work on an importer, despite having long touted the openness of its database format as a guarantee against lock-in and explaining in its documentation that the database could be accessed using standard tools. Until recently, Mendeley Desktop imported data from Zoteros own open database, as it had since 2009. The Mendeley 1.19 release notes claimed that the encryption was for improved security on shared machines, yet applications rarely encrypt their local data files, as file protections are generally handled by the operating system with account permissions and full-disk encryption, and anyone using the same operating system account or an admin account can already install a keylogger to capture passwords. Elsevier later stated that the change was required by new European privacy regulations a bizarre claim, given that those regulations are designed to give people control over their data and guarantee data portability, not the opposite and continued to assert, falsely, that full local export was still possible, while repeatedly dismissing reports of the change as #fakenews. Direct access to the Mendeley database is the only local way to export the full contents of ones own research. The export formats supported by Mendeley dont contain folders, various metadata fields (date added, favorite, and others), or PDF annotations. Mendeley offers a web-based API, but it contains only uploaded data, so relying on it means that anyone wanting to export their own data first needs to upload all their data and files to Elseviers servers. The API is under Elseviers control and can be changed or discontinued at any time. Since making this change, Elsevier has released its replacement for Mendeley Desktop, Mendeley Reference Manager, which is essentially a wrapper around the website and doesnt contain a real local database at all. ",
        "_version_": 1718527383297851392
      },
      {
        "story_id": [20513368],
        "story_author": ["dominikaner"],
        "story_descendants": [50],
        "story_score": [101],
        "story_time": ["2019-07-24T08:54:26Z"],
        "story_title": "EvilGnome: Rare Malware Spying on Linux Desktop Users",
        "search": [
          "EvilGnome: Rare Malware Spying on Linux Desktop Users",
          "https://www.intezer.com/blog-evilgnome-rare-malware-spying-on-linux-desktop-users/",
          "Join our free communityGet started Top Blogs Introduction Linux desktop remains an unpopular choice among mainstream desktop users, making up a little more than 2%of the desktop operating system market share. This is in contrast tothe web server market share, which consists of 70%of Linux-based operating systems. Consequently, the Linux malware ecosystem is plagued by financial driven crypto-miners and DDoS botnet tools which mostly target vulnerable servers. This explains our surprise when in the beginning of July, we discovered a new, fully undetected Linux backdoor implant, containing rarely seen functionalitieswith regards to Linux malware, targeting desktop users. Throughout our investigation, we have found evidence that shows operational similaritiesbetween this implant and GamaredonGroup. We have investigated this connectionand in this blog we will present a technical analysis of the tool. We have named the implant EvilGnome,for its disguise as a Gnome extension.The malware is currently fully undetected across all major security solutions: Figure 1: VirusTotal detectionsof an EvilGnome sample We believe this is a test version that was uploaded to VirusTotal, perhapsby mistake. The implant contains an unfinished keylogger functionality, comments, symbol names and compilation metadata which typically do not appear in production versions. EvilGnomes functionalities include desktop screenshots, file stealing, allowing capturing audio recording from the users microphone and the ability to download and execute further modules. Gamaredon Group Connection Gamaredon Groupis an alleged Russian threat group. It has been active since at least 2013, and has targeted individuals likely involved with the Ukrainian government. Gamaredon Group infects victims using malicious attachments, delivered via spearphishingtechniques. The groups implants are characterized by the employmentof information stealing toolsamong them being screenshot and document stealers delivered via a SFX, and made to achieve persistencethrough a scheduled task. Gamaredon Group primarily makes use of Russian hosting providers in order to distribute its malware. Our investigation into EvilGnome yielded several similarities between the threat actors behind EvilGnome and GamaredonGroup: Hosting Similarities The operators of EvilGnome use a hosting provider that has been used by Gamaredon Group for years, and continues to be used by the group. More specifically, EvilGnomesC2 IP address (195.62.52.101) was resolved two months ago by the domains gamework.ddns.netand workan.ddns.net, associatedwith the Gamaredon Group: Figure 2: RiskIQ EvilGnome C2 IP query We used RiskIQto map the history of the gamework.ddns.netdomain: Figure 3: gamework.ddns.net DNS timeline The finding shows that EvilGnome operates on an IP address that was controlled by the Gamaredon group two months ago. Infrastructure Similarities While investigating the EvilGnome C2, we observed that it served SSH over port 3436. We then checked for the 3436 port over three currently operating Gamaredon Group C2 servers, and found one server with this port open, serving SSH: Figure 4: SSH served on port 3436 both on EvilGnome C2 and Gamaredons rnbo-ua.ddns.net We proceeded to scan for this network fingerprint under EvilGnomes host provider and we identified two additional servers with domain names similar to the naming pattern of Gamaredon domains (the use of the .space TTLD and ddns): 185.158.115.44 -> kotl.space 185.158.115.154 -> clsass.ddns.net Tool Similarities Gamaredon Group does not use any known Linux implants. It is difficult to make comparisons between tools built for different operating systems because they are developed with different challenges and objectives in mind. We can, however, observe similarities at a high-level. The techniques and modulesemployed by EvilGnomethat is the use of SFX, persistence with task scheduler and the deployment of information stealing toolsremind us of Gamaredon Groups Windows tools. We present a thorough analysis of EvilGnome in the following section. Technical Analysis Deployment with Makeself SFX This implant is delivered in the form of a self-extracting archive shell script created with makeself: makeself.shis a small shell script that generates a self-extractable compressed tar archive from a directory. The resulting file appears as a shell script (many of those have a .runsuffix), and can be launched as is. The archive will then uncompress itself to a temporary directory and an optional arbitrary command will be executed (for example an installation script). This is pretty similar to archives generated with WinZip Self-Extractor in the Windows world. Interestingly, the tools operator did not omit metadata from the generated makeself SFX. The packaging date, development paths and the tools filename were all left exposed. We can observe that the sample is very recent, created on Thursday, July 4: Figure 5: Makeself packaging metadata and the archived files metadata As can be observed in the illustration above, the makeself script is instructed to run ./setup.sh after unpacking. Using makeselfs options, we are able to instruct the script to unpack itself without executing: Figure 6: Unpacking Makeself The archive contains four files: gnome-shell-ext the spy agent executable gnome-shell-ext.sh checks if gnome-shell-extis already running and if not, executes it rtp.dat configuration file for gnome-shell-ext setup.sh the setup script that is run by makeself after unpacking The setup script installs the agent to ~/.cache/gnome-software/gnome-shell-extensions/,in an attempt to masquerade itself as a Gnomeshell extension. Gnome shell extensions allow tweaking the Gnome desktop and add functionalities. They are the desktop equivalent to browser extensions. Persistence is achieved by registering gnome-shell-ext.shto run every minute in crontab. Finally, the script executes gnome-shell-ext.sh,which in turn launches the main executable gnome-shell-ext: Figure 7: setup.sh The Spy Agent Analyzing the agent with Intezer Analyze demonstrated to us that the code was never seen before by the system: Figure 8: Intezer Analyze report of the Spy Agent sample This large amount of unique genes located within this file is not a trend we regularly see in Linux files and therefore it seems suspicious. The Spy Agent was built in C++, using classes with an object oriented structure. The binary was not stripped, which allowed us to read symbols and understand the developers intentions. At launch, the agent forksto run in a new process. The agent then reads the rtp.datconfiguration file and loads it directly into memory: Figure 9: Loading configuration from rtp.dat We marked interesting fields within the configuration file: Figure 10: Configuration dissection The first four bytes are a hexadecimal representation of the C2s IP address: 0x65343ec3 -> 0xc3.0x3e.0x34.0x65 -> 195.62.52.101 Modules The spy agent contains five modules called Shooters: Figure 11: Shooter modules ShooterSound captures audio from the users microphone and uploads to C2 ShooterImage captures screenshots and uploads to C2 ShooterFile scans the file system for newly created files and uploads them to C2 ShooterPing receives new commands from C2 ShooterKey unimplemented and unused, most likely an unfinished keylogging module Each module is run in a separate thread, and access to shared resources (such as the configuration) is safeguarded by mutexes. The modules encrypt their output and decrypt data from the C2 with RC5 with the key sdg62_AS.sa$die3,using a modified version of a Russian open source library https://webhamster.ru/site/page/index/articles/projectcode/157: Figure 12: RC5 library On connection failure, or if instructed by the C2, these modules store their output at ~/.cache/gnome-software/gnome-shell-extensions/tmp/: Figure 13: Stored files We will now dive into each of the five modules and their options: ShooterPing The ShooterPing module processes commands received from the C2: Figure 14: C2 commands These include: Download & execute new files Set new filters for file scanning Download & set new runtime configuration Exfiltrate stored output to C2 Stop the shooter modules from running The other modules run at a constant interval between each run, as defined by one of the configuration parameters. The C2 is able to control this interval via downloading new parameters through ShooterPing. ShooterFile The ShooterFile module uses a filter list to scan the filesystem, while ignoring specific files and folders as shown in the following illustration: Figure 15: File scanning filter We can see from the filter_accepted_files list that the agents purpose is to steal document related files. However, the list is not usedby the malware and further indicates that this is a work in progress. ShooterAudio Figure 16: Capturing audio with PulseAudio The ShooterAudio module uses PulseAudio to capture audio from the users microphone. Using default configuration from rtp.dat, the module records only a size of 80,000 bytes of audio per iteration. Consequently, the module only records audio for abrief moment, making this module non-functional until a larger recording size is set by the C2. ShooterImage This module opens a connection to the XOrg Display Server, which is the backend to the Gnome desktop. It uses the Cairo open source library to take screenshots of the users desktop. Figure 17: Screenshot capturing using XOrg Server Prevention and Response We recommend to Linux users who want to check whether they are infected to check the ~/.cache/gnome-software/gnome-shell-extensions directory for the gnome-shell-ext executable. We have also created a customYARA rule, based on code reuse technology, for detecting future variants of EvilGnome. Conclusion EvilGnome is a rare type of malware due to its appetite for Linux desktop users. Throughout this post, we have presented detailed infrastructure-related evidence to connect EvilGnome to the actors behind the Gamaredon Group. We believe this is a premature test version. We anticipate newer versions to be discovered and reviewed in the future, which could potentially shed more light into the groups operations. Genetic Analysis The EvilGnome malware variant is now indexed in Intezers genetic database. If you have a suspicious file that you suspect to be EvilGnome, you can upload it to Intezer Analyze in order to detect code reuse to this threat family and many others. You are welcome totry it for free in our community edition. Figure 18: Intezer Analyze report of the Spy Agent sample IOCs EvilGnome: a21acbe7ee77c721f1adc76e7a7799c936e74348d32b4c38f3bf6357ed7e8032 82b69954410c83315dfe769eed4b6cfc7d11f0f62e26ff546542e35dcd7106b7 7ffab36b2fa68d0708c82f01a70c8d10614ca742d838b69007f5104337a4b869 195.62.52[.]101 Gamaredon Group: 185.158.115[.]44 185.158.115[.]154 clsass.ddns[.]net kotl[.]space Paul LitvakPaul is a malware analyst and reverse engineer at Intezer. He previously served as a developer in the Israel Defense Force (IDF) Intelligence Corps for three years. ",
          ">EvilGnome’s functionalities include desktop screenshots, file stealing, allowing capturing audio recording from the user’s microphone and the ability to download and execute further modules.<p>I'm glad this is still considered malware in the Linux world at least, and not just \"analytics\"",
          "This incident made me think that the hackers planned to hijack some popular extension. this would make the more damage.<p>My (unpopular) opinion is that GNOME should see what are the most used extensions, accept that people want those feature and  bring those features into GNOME or make those official extension and not third party, so you at least control what most people would install."
        ],
        "story_type": ["Normal"],
        "url": "https://www.intezer.com/blog-evilgnome-rare-malware-spying-on-linux-desktop-users/",
        "comments.comment_id": [20513734, 20513765],
        "comments.comment_author": ["swebs", "simion314"],
        "comments.comment_descendants": [1, 3],
        "comments.comment_time": [
          "2019-07-24T10:18:05Z",
          "2019-07-24T10:25:48Z"
        ],
        "comments.comment_text": [
          ">EvilGnome’s functionalities include desktop screenshots, file stealing, allowing capturing audio recording from the user’s microphone and the ability to download and execute further modules.<p>I'm glad this is still considered malware in the Linux world at least, and not just \"analytics\"",
          "This incident made me think that the hackers planned to hijack some popular extension. this would make the more damage.<p>My (unpopular) opinion is that GNOME should see what are the most used extensions, accept that people want those feature and  bring those features into GNOME or make those official extension and not third party, so you at least control what most people would install."
        ],
        "id": "a22472e3-62ca-406b-a06f-d02ccd857ec4",
        "url_text": "Join our free communityGet started Top Blogs Introduction Linux desktop remains an unpopular choice among mainstream desktop users, making up a little more than 2%of the desktop operating system market share. This is in contrast tothe web server market share, which consists of 70%of Linux-based operating systems. Consequently, the Linux malware ecosystem is plagued by financial driven crypto-miners and DDoS botnet tools which mostly target vulnerable servers. This explains our surprise when in the beginning of July, we discovered a new, fully undetected Linux backdoor implant, containing rarely seen functionalitieswith regards to Linux malware, targeting desktop users. Throughout our investigation, we have found evidence that shows operational similaritiesbetween this implant and GamaredonGroup. We have investigated this connectionand in this blog we will present a technical analysis of the tool. We have named the implant EvilGnome,for its disguise as a Gnome extension.The malware is currently fully undetected across all major security solutions: Figure 1: VirusTotal detectionsof an EvilGnome sample We believe this is a test version that was uploaded to VirusTotal, perhapsby mistake. The implant contains an unfinished keylogger functionality, comments, symbol names and compilation metadata which typically do not appear in production versions. EvilGnomes functionalities include desktop screenshots, file stealing, allowing capturing audio recording from the users microphone and the ability to download and execute further modules. Gamaredon Group Connection Gamaredon Groupis an alleged Russian threat group. It has been active since at least 2013, and has targeted individuals likely involved with the Ukrainian government. Gamaredon Group infects victims using malicious attachments, delivered via spearphishingtechniques. The groups implants are characterized by the employmentof information stealing toolsamong them being screenshot and document stealers delivered via a SFX, and made to achieve persistencethrough a scheduled task. Gamaredon Group primarily makes use of Russian hosting providers in order to distribute its malware. Our investigation into EvilGnome yielded several similarities between the threat actors behind EvilGnome and GamaredonGroup: Hosting Similarities The operators of EvilGnome use a hosting provider that has been used by Gamaredon Group for years, and continues to be used by the group. More specifically, EvilGnomesC2 IP address (195.62.52.101) was resolved two months ago by the domains gamework.ddns.netand workan.ddns.net, associatedwith the Gamaredon Group: Figure 2: RiskIQ EvilGnome C2 IP query We used RiskIQto map the history of the gamework.ddns.netdomain: Figure 3: gamework.ddns.net DNS timeline The finding shows that EvilGnome operates on an IP address that was controlled by the Gamaredon group two months ago. Infrastructure Similarities While investigating the EvilGnome C2, we observed that it served SSH over port 3436. We then checked for the 3436 port over three currently operating Gamaredon Group C2 servers, and found one server with this port open, serving SSH: Figure 4: SSH served on port 3436 both on EvilGnome C2 and Gamaredons rnbo-ua.ddns.net We proceeded to scan for this network fingerprint under EvilGnomes host provider and we identified two additional servers with domain names similar to the naming pattern of Gamaredon domains (the use of the .space TTLD and ddns): 185.158.115.44 -> kotl.space 185.158.115.154 -> clsass.ddns.net Tool Similarities Gamaredon Group does not use any known Linux implants. It is difficult to make comparisons between tools built for different operating systems because they are developed with different challenges and objectives in mind. We can, however, observe similarities at a high-level. The techniques and modulesemployed by EvilGnomethat is the use of SFX, persistence with task scheduler and the deployment of information stealing toolsremind us of Gamaredon Groups Windows tools. We present a thorough analysis of EvilGnome in the following section. Technical Analysis Deployment with Makeself SFX This implant is delivered in the form of a self-extracting archive shell script created with makeself: makeself.shis a small shell script that generates a self-extractable compressed tar archive from a directory. The resulting file appears as a shell script (many of those have a .runsuffix), and can be launched as is. The archive will then uncompress itself to a temporary directory and an optional arbitrary command will be executed (for example an installation script). This is pretty similar to archives generated with WinZip Self-Extractor in the Windows world. Interestingly, the tools operator did not omit metadata from the generated makeself SFX. The packaging date, development paths and the tools filename were all left exposed. We can observe that the sample is very recent, created on Thursday, July 4: Figure 5: Makeself packaging metadata and the archived files metadata As can be observed in the illustration above, the makeself script is instructed to run ./setup.sh after unpacking. Using makeselfs options, we are able to instruct the script to unpack itself without executing: Figure 6: Unpacking Makeself The archive contains four files: gnome-shell-ext the spy agent executable gnome-shell-ext.sh checks if gnome-shell-extis already running and if not, executes it rtp.dat configuration file for gnome-shell-ext setup.sh the setup script that is run by makeself after unpacking The setup script installs the agent to ~/.cache/gnome-software/gnome-shell-extensions/,in an attempt to masquerade itself as a Gnomeshell extension. Gnome shell extensions allow tweaking the Gnome desktop and add functionalities. They are the desktop equivalent to browser extensions. Persistence is achieved by registering gnome-shell-ext.shto run every minute in crontab. Finally, the script executes gnome-shell-ext.sh,which in turn launches the main executable gnome-shell-ext: Figure 7: setup.sh The Spy Agent Analyzing the agent with Intezer Analyze demonstrated to us that the code was never seen before by the system: Figure 8: Intezer Analyze report of the Spy Agent sample This large amount of unique genes located within this file is not a trend we regularly see in Linux files and therefore it seems suspicious. The Spy Agent was built in C++, using classes with an object oriented structure. The binary was not stripped, which allowed us to read symbols and understand the developers intentions. At launch, the agent forksto run in a new process. The agent then reads the rtp.datconfiguration file and loads it directly into memory: Figure 9: Loading configuration from rtp.dat We marked interesting fields within the configuration file: Figure 10: Configuration dissection The first four bytes are a hexadecimal representation of the C2s IP address: 0x65343ec3 -> 0xc3.0x3e.0x34.0x65 -> 195.62.52.101 Modules The spy agent contains five modules called Shooters: Figure 11: Shooter modules ShooterSound captures audio from the users microphone and uploads to C2 ShooterImage captures screenshots and uploads to C2 ShooterFile scans the file system for newly created files and uploads them to C2 ShooterPing receives new commands from C2 ShooterKey unimplemented and unused, most likely an unfinished keylogging module Each module is run in a separate thread, and access to shared resources (such as the configuration) is safeguarded by mutexes. The modules encrypt their output and decrypt data from the C2 with RC5 with the key sdg62_AS.sa$die3,using a modified version of a Russian open source library https://webhamster.ru/site/page/index/articles/projectcode/157: Figure 12: RC5 library On connection failure, or if instructed by the C2, these modules store their output at ~/.cache/gnome-software/gnome-shell-extensions/tmp/: Figure 13: Stored files We will now dive into each of the five modules and their options: ShooterPing The ShooterPing module processes commands received from the C2: Figure 14: C2 commands These include: Download & execute new files Set new filters for file scanning Download & set new runtime configuration Exfiltrate stored output to C2 Stop the shooter modules from running The other modules run at a constant interval between each run, as defined by one of the configuration parameters. The C2 is able to control this interval via downloading new parameters through ShooterPing. ShooterFile The ShooterFile module uses a filter list to scan the filesystem, while ignoring specific files and folders as shown in the following illustration: Figure 15: File scanning filter We can see from the filter_accepted_files list that the agents purpose is to steal document related files. However, the list is not usedby the malware and further indicates that this is a work in progress. ShooterAudio Figure 16: Capturing audio with PulseAudio The ShooterAudio module uses PulseAudio to capture audio from the users microphone. Using default configuration from rtp.dat, the module records only a size of 80,000 bytes of audio per iteration. Consequently, the module only records audio for abrief moment, making this module non-functional until a larger recording size is set by the C2. ShooterImage This module opens a connection to the XOrg Display Server, which is the backend to the Gnome desktop. It uses the Cairo open source library to take screenshots of the users desktop. Figure 17: Screenshot capturing using XOrg Server Prevention and Response We recommend to Linux users who want to check whether they are infected to check the ~/.cache/gnome-software/gnome-shell-extensions directory for the gnome-shell-ext executable. We have also created a customYARA rule, based on code reuse technology, for detecting future variants of EvilGnome. Conclusion EvilGnome is a rare type of malware due to its appetite for Linux desktop users. Throughout this post, we have presented detailed infrastructure-related evidence to connect EvilGnome to the actors behind the Gamaredon Group. We believe this is a premature test version. We anticipate newer versions to be discovered and reviewed in the future, which could potentially shed more light into the groups operations. Genetic Analysis The EvilGnome malware variant is now indexed in Intezers genetic database. If you have a suspicious file that you suspect to be EvilGnome, you can upload it to Intezer Analyze in order to detect code reuse to this threat family and many others. You are welcome totry it for free in our community edition. Figure 18: Intezer Analyze report of the Spy Agent sample IOCs EvilGnome: a21acbe7ee77c721f1adc76e7a7799c936e74348d32b4c38f3bf6357ed7e8032 82b69954410c83315dfe769eed4b6cfc7d11f0f62e26ff546542e35dcd7106b7 7ffab36b2fa68d0708c82f01a70c8d10614ca742d838b69007f5104337a4b869 195.62.52[.]101 Gamaredon Group: 185.158.115[.]44 185.158.115[.]154 clsass.ddns[.]net kotl[.]space Paul LitvakPaul is a malware analyst and reverse engineer at Intezer. He previously served as a developer in the Israel Defense Force (IDF) Intelligence Corps for three years. ",
        "_version_": 1718527416662491136
      },
      {
        "story_id": [21604825],
        "story_author": ["nextdns"],
        "story_descendants": [196],
        "story_score": [264],
        "story_time": ["2019-11-22T12:06:10Z"],
        "story_title": "Cname cloaking, a disguise of third-party trackers",
        "search": [
          "Cname cloaking, a disguise of third-party trackers",
          "https://medium.com/nextdns/cname-cloaking-the-dangerous-disguise-of-third-party-trackers-195205dc522a",
          "Note: If you are using NextDNS as your DNS resolver, you are already automatically protected from this. You can read more here.How come AdBlock, Adblock Plus, uBlock Origin, Ghostery, Brave and Firefox are letting a third-party tracker from Eulerian, a leading tracking company, execute their script freely on fortuneo.fr, one of the biggest online bank in France?How come the same thing is happening on thousands of other popular websites worldwide?What has started to happen in the last few months in the world of third-party tracking is having a major impact on peoples privacy, and it all stayed pretty much under the radar.For one of those websites, it all started with an email like this one:Apologies for the french, key sentences are translated below. SourceThis email is from Criteo, a leading tracking company, asking the website to make a quick change (it takes 2 minutes) to adapt to the evolution of browsers (i.e., work around tracking restrictions), and to be able to track people in a more optimal way.Criteo is requesting the website to add a CNAME for domains like kvejdd.website.com (note the randomness of the subdomain, we will talk about it later) to dnsdelegation.ioOR ELSE, you may lose 11,64% of your sales, 11,53% of your gross turnover and 20,82% of your audience. Scary stuff.A suitable name for this method would be CNAME Cloaking, and it is used to disguise a third-party tracker as first-party tracker. In this case, they are also purposely obfuscating this behind a random subdomain, with a CNAME to a generic and unbranded domain.Some tracking companies, like AT Internet (formerly XiTi), are even going to great lengths to completely distance themselves from the domain used as CNAME destination. Try figuring out which company at-o.net belongs to (hidden WHOIS information and AWS IPs). This is live right now on lemonde.fr, one of the top news websites in the world, and on many other websites.Why classic privacy-protection tools and restrictions will have a hard time protecting you from this?The way it used to work is website1.com would load a third-party tracker by calling something that looks like this: https://tracker.trackingcompany.com/j23jsak.js.Another unrelated website (website2.com) would also be calling something that looks like this: https://tracker.trackingcompany.com/k2j4vs.js.Privacy-protection tools would then simply need to automatically block any calls made to tracker.trackingcompany.com, and it would automatically protect you from Tracking Company third-party tracking. Done, its that easy.With CNAME Cloaking, many problems arise that makes it realistically impossible to block this:Browser extensions are not allowed access to the DNS layer of the request i.e., they cant see the CNAMEs. (1)When each website loads third party trackers by calling something like a3ksbl.website.com, privacy-protection tools now have to figure out which subdomain is a front for CNAME Cloaking, for tens of thousands of websites. Thats a LOT of work.With each website now having its own subdomain cloaking the third-party tracker, those tools need to include as many rules as there are websites using this CNAME Cloaking method. Blocking a third-party tracker went from one rule to thousands.Here is the problem though: those tools are already reaching the maximum number of rules allowed on each platform (50,000 for Safari, and 30,000 in the soon-to-be-released Google Chrome version with Manifest V3).The Criteo representative in the email was not lying, it does take 2 minutes to set this up. It also only takes 2 minutes to change dg3fkn.website.com to 3j4vdl.website.com (Hell, you can probably automate this). We mentioned above how much work it takes to gather all subdomains being used as a front for CNAME Cloaking. Now imagine they change every week, every day, or every hour. Its just impossible to keep track.This means that as CNAME Cloaking is being used on more and more websites, more and more people are suddenly being tracked (again).How bad is it for privacy?Lets assume you visited website1.com that includes a third-party tracker from Tracking Company, then website2.com that also includes that tracker. Tracking Company would know that you visited both sites, and each websites would send as much personal information to Tracking Company about who you are (rarely your name or email, but usually everything else they know about you your age, gender, where you live, etc.), and what you did on the website.From there, a website could agree to enrich the profiles Tracking Company has on their users by allowing them to combine the data gathered from their website to other websites (either as a mutual exchange, by paying, or both).Quoting Criteos Twitter profile description:Criteo is a global tech company that enables brands & retailers to leverage terabytes of collaborative data to connect shoppers to the things they need & loveNow, add data gathering companies to the pot that have been known to create or buy Facebook games requiring a Facebook Connect, with the sole purpose of enriching Tracking Companys profiles with even more data.This is what happened during the 2016 United States presidential election: political campaigns and foreign state actors, allegedly were able to target specific types of voters very precisely and adapt their message to those voters accordingly thanks to profiles built by tracking companies.There are 2 ways to track someone (i.e., being able to uniquely identify someone on different websites, and in time):CookiesThird-party trackers used to be able to set a unique identifier in your browser that they could read at will on the different websites you visit, as long as this third-party tracker was included by the website.Most browsers now include protections against this (i.e., third party tracker cookies and caches are sandboxed by Origin, and sometimes more), and tracking companies have publicly (or privately) switched to a method called fingerprinting, which is perfectly compatible with CNAME Cloaking.FingerprintingFingerprinting is building a unique identifier by combining multiple properties that by themselves are not unique to you, bypassing browser restrictions on cookies, and even being able to track you between devices (what cookies cant do). Some of these properties are your IP address, your operating system version, your browser version, your computer language, your computer time, the size of your screen, the pixel density of your screen, how fast your computer is, and the list goes on and on.Being able to directly execute JavaScript (and WebAssembly) in your browser gives the third-party tracker access to many (many) different properties.There is not a more ideal situation for a third-party tracker that wants to fingerprint you than being able to execute their own script from a subdomain of the website itself, as putting restrictions in place against this would negatively affect the websites themselves.And here are a few websites with a large audience that are disguising third-party trackers as first-party trackers using this method:foxnews.com, walmart.com, bbc.co.uk, go.com, webmd.com, washingtonpost.com, weather.com, fnac.com, fortuneo.fr, liberation.fr, lemonde.fr, oui.sncf, rueducommerce.fr, sfr.fr, pmu.fr, laredoute.fr, boulanger.fr, coach.com, gap.com, anntaylor.com, cnn.com, boursorama.com, arstechnica.com, saksfifthavenue.com, brandalley.fr, greenweez.com, habitat.fr, maeva.com, younited-credit.com, mathon.fr, destinia.com, vente-unique.com, nrjmobile.fr, t-mobile.com, statefarm.com, A quick look at the non-exhaustive list of domains currently CNAMEing to dnsdelegation.io (thats the one from Criteo) shows that this is being applied to quite a few websites already.Security implications of CNAME CloakingWhile this is considered bad practice for a website to set cookies as accessible to all subdomains (i.e., *.website.com), many do this. In that case, those cookies are automatically sent to the cloaked third-party tracker.One of those cookies could be an authentication cookie. Anyone in possession of this cookie can impersonate the user and access private user information.Case in point, liberation.fr:Yep, thats my session cookies right there, being sent automatically to the third-party tracker from Eulerian.Can CNAME Cloaking also be used to serve ads?Yes.DNS-level blocking to the rescueThere is good news though. This can easily be detected and blocked at the DNS level, and at NextDNS we released protections against CNAME Cloaking as soon as this started spreading, and we are continuously monitoring the situation to adapt quickly to methods like this.In the spirit of open discussion and full disclosure, we will automatically copy/paste below any comments made by the mentioned companies to team@nextdns.io.No comment has been made at this time.(1) Firefox for desktop does allow extensions to make DNS queries themselves, and extensions like uBlock Origin already apply their blocking rules to intermediary CNAMEs as well. Its worth noting that unfortunately, Firefoxs current market share is below 5%. ",
          "Use a Pihole + your adblocker of choice - defense in depth. It's easy to set up, brainless to keep updated, and helps to protect all devices on your network, not just the things that can run uBlock. I've got mine running in a Docker container, which upstreams to a stubby container, which gets DNS-over-TLS, so I get adblocking <i>and</i> DNS query encryption out to Cloudflare for the whole network, and it's really not all that hard to set up. (Edit: Here's the bash script I used. docker-compose would probably be better, but whatever. <a href=\"https://gist.github.com/cheald/23da384908404b0757eadda74124a602\" rel=\"nofollow\">https://gist.github.com/cheald/23da384908404b0757eadda74124a...</a>)<p>If you're unwilling to do that, just set your DNS servers to the Adguard servers (<a href=\"https://adguard.com/en/adguard-dns/overview.html\" rel=\"nofollow\">https://adguard.com/en/adguard-dns/overview.html</a>) and you get most of the same benefit, though obviously without the control that the Pihole offers you. On Android devices, you can go to Settings - > Wifi & Internet - > Private DNS and set  \"Private DNS provider hostname\" to dns.adguard.com (or your own exposed Pihole server, if you're so inclined) and get the same benefit when you're on LTE.",
          "The easiest way for site-owners to delegate control has been to include third-party javascript.  With new browser restrictions, we're starting to see companies switching to loading JS via CNAMEd subdomains, because that's nearly as easy.  The next step is probably reverse proxies, though, where the third-party JS comes from the same server that gives you the rest of the site's JS.<p>(Disclosure: I work in ads; speaking only for myself)"
        ],
        "story_type": ["Normal"],
        "url": "https://medium.com/nextdns/cname-cloaking-the-dangerous-disguise-of-third-party-trackers-195205dc522a",
        "comments.comment_id": [21616085, 21616410],
        "comments.comment_author": ["cheald", "jefftk"],
        "comments.comment_descendants": [6, 7],
        "comments.comment_time": [
          "2019-11-23T19:48:14Z",
          "2019-11-23T20:44:43Z"
        ],
        "comments.comment_text": [
          "Use a Pihole + your adblocker of choice - defense in depth. It's easy to set up, brainless to keep updated, and helps to protect all devices on your network, not just the things that can run uBlock. I've got mine running in a Docker container, which upstreams to a stubby container, which gets DNS-over-TLS, so I get adblocking <i>and</i> DNS query encryption out to Cloudflare for the whole network, and it's really not all that hard to set up. (Edit: Here's the bash script I used. docker-compose would probably be better, but whatever. <a href=\"https://gist.github.com/cheald/23da384908404b0757eadda74124a602\" rel=\"nofollow\">https://gist.github.com/cheald/23da384908404b0757eadda74124a...</a>)<p>If you're unwilling to do that, just set your DNS servers to the Adguard servers (<a href=\"https://adguard.com/en/adguard-dns/overview.html\" rel=\"nofollow\">https://adguard.com/en/adguard-dns/overview.html</a>) and you get most of the same benefit, though obviously without the control that the Pihole offers you. On Android devices, you can go to Settings - > Wifi & Internet - > Private DNS and set  \"Private DNS provider hostname\" to dns.adguard.com (or your own exposed Pihole server, if you're so inclined) and get the same benefit when you're on LTE.",
          "The easiest way for site-owners to delegate control has been to include third-party javascript.  With new browser restrictions, we're starting to see companies switching to loading JS via CNAMEd subdomains, because that's nearly as easy.  The next step is probably reverse proxies, though, where the third-party JS comes from the same server that gives you the rest of the site's JS.<p>(Disclosure: I work in ads; speaking only for myself)"
        ],
        "id": "d92db84d-08c9-426f-b5dd-92379e9ccc62",
        "url_text": "Note: If you are using NextDNS as your DNS resolver, you are already automatically protected from this. You can read more here.How come AdBlock, Adblock Plus, uBlock Origin, Ghostery, Brave and Firefox are letting a third-party tracker from Eulerian, a leading tracking company, execute their script freely on fortuneo.fr, one of the biggest online bank in France?How come the same thing is happening on thousands of other popular websites worldwide?What has started to happen in the last few months in the world of third-party tracking is having a major impact on peoples privacy, and it all stayed pretty much under the radar.For one of those websites, it all started with an email like this one:Apologies for the french, key sentences are translated below. SourceThis email is from Criteo, a leading tracking company, asking the website to make a quick change (it takes 2 minutes) to adapt to the evolution of browsers (i.e., work around tracking restrictions), and to be able to track people in a more optimal way.Criteo is requesting the website to add a CNAME for domains like kvejdd.website.com (note the randomness of the subdomain, we will talk about it later) to dnsdelegation.ioOR ELSE, you may lose 11,64% of your sales, 11,53% of your gross turnover and 20,82% of your audience. Scary stuff.A suitable name for this method would be CNAME Cloaking, and it is used to disguise a third-party tracker as first-party tracker. In this case, they are also purposely obfuscating this behind a random subdomain, with a CNAME to a generic and unbranded domain.Some tracking companies, like AT Internet (formerly XiTi), are even going to great lengths to completely distance themselves from the domain used as CNAME destination. Try figuring out which company at-o.net belongs to (hidden WHOIS information and AWS IPs). This is live right now on lemonde.fr, one of the top news websites in the world, and on many other websites.Why classic privacy-protection tools and restrictions will have a hard time protecting you from this?The way it used to work is website1.com would load a third-party tracker by calling something that looks like this: https://tracker.trackingcompany.com/j23jsak.js.Another unrelated website (website2.com) would also be calling something that looks like this: https://tracker.trackingcompany.com/k2j4vs.js.Privacy-protection tools would then simply need to automatically block any calls made to tracker.trackingcompany.com, and it would automatically protect you from Tracking Company third-party tracking. Done, its that easy.With CNAME Cloaking, many problems arise that makes it realistically impossible to block this:Browser extensions are not allowed access to the DNS layer of the request i.e., they cant see the CNAMEs. (1)When each website loads third party trackers by calling something like a3ksbl.website.com, privacy-protection tools now have to figure out which subdomain is a front for CNAME Cloaking, for tens of thousands of websites. Thats a LOT of work.With each website now having its own subdomain cloaking the third-party tracker, those tools need to include as many rules as there are websites using this CNAME Cloaking method. Blocking a third-party tracker went from one rule to thousands.Here is the problem though: those tools are already reaching the maximum number of rules allowed on each platform (50,000 for Safari, and 30,000 in the soon-to-be-released Google Chrome version with Manifest V3).The Criteo representative in the email was not lying, it does take 2 minutes to set this up. It also only takes 2 minutes to change dg3fkn.website.com to 3j4vdl.website.com (Hell, you can probably automate this). We mentioned above how much work it takes to gather all subdomains being used as a front for CNAME Cloaking. Now imagine they change every week, every day, or every hour. Its just impossible to keep track.This means that as CNAME Cloaking is being used on more and more websites, more and more people are suddenly being tracked (again).How bad is it for privacy?Lets assume you visited website1.com that includes a third-party tracker from Tracking Company, then website2.com that also includes that tracker. Tracking Company would know that you visited both sites, and each websites would send as much personal information to Tracking Company about who you are (rarely your name or email, but usually everything else they know about you your age, gender, where you live, etc.), and what you did on the website.From there, a website could agree to enrich the profiles Tracking Company has on their users by allowing them to combine the data gathered from their website to other websites (either as a mutual exchange, by paying, or both).Quoting Criteos Twitter profile description:Criteo is a global tech company that enables brands & retailers to leverage terabytes of collaborative data to connect shoppers to the things they need & loveNow, add data gathering companies to the pot that have been known to create or buy Facebook games requiring a Facebook Connect, with the sole purpose of enriching Tracking Companys profiles with even more data.This is what happened during the 2016 United States presidential election: political campaigns and foreign state actors, allegedly were able to target specific types of voters very precisely and adapt their message to those voters accordingly thanks to profiles built by tracking companies.There are 2 ways to track someone (i.e., being able to uniquely identify someone on different websites, and in time):CookiesThird-party trackers used to be able to set a unique identifier in your browser that they could read at will on the different websites you visit, as long as this third-party tracker was included by the website.Most browsers now include protections against this (i.e., third party tracker cookies and caches are sandboxed by Origin, and sometimes more), and tracking companies have publicly (or privately) switched to a method called fingerprinting, which is perfectly compatible with CNAME Cloaking.FingerprintingFingerprinting is building a unique identifier by combining multiple properties that by themselves are not unique to you, bypassing browser restrictions on cookies, and even being able to track you between devices (what cookies cant do). Some of these properties are your IP address, your operating system version, your browser version, your computer language, your computer time, the size of your screen, the pixel density of your screen, how fast your computer is, and the list goes on and on.Being able to directly execute JavaScript (and WebAssembly) in your browser gives the third-party tracker access to many (many) different properties.There is not a more ideal situation for a third-party tracker that wants to fingerprint you than being able to execute their own script from a subdomain of the website itself, as putting restrictions in place against this would negatively affect the websites themselves.And here are a few websites with a large audience that are disguising third-party trackers as first-party trackers using this method:foxnews.com, walmart.com, bbc.co.uk, go.com, webmd.com, washingtonpost.com, weather.com, fnac.com, fortuneo.fr, liberation.fr, lemonde.fr, oui.sncf, rueducommerce.fr, sfr.fr, pmu.fr, laredoute.fr, boulanger.fr, coach.com, gap.com, anntaylor.com, cnn.com, boursorama.com, arstechnica.com, saksfifthavenue.com, brandalley.fr, greenweez.com, habitat.fr, maeva.com, younited-credit.com, mathon.fr, destinia.com, vente-unique.com, nrjmobile.fr, t-mobile.com, statefarm.com, A quick look at the non-exhaustive list of domains currently CNAMEing to dnsdelegation.io (thats the one from Criteo) shows that this is being applied to quite a few websites already.Security implications of CNAME CloakingWhile this is considered bad practice for a website to set cookies as accessible to all subdomains (i.e., *.website.com), many do this. In that case, those cookies are automatically sent to the cloaked third-party tracker.One of those cookies could be an authentication cookie. Anyone in possession of this cookie can impersonate the user and access private user information.Case in point, liberation.fr:Yep, thats my session cookies right there, being sent automatically to the third-party tracker from Eulerian.Can CNAME Cloaking also be used to serve ads?Yes.DNS-level blocking to the rescueThere is good news though. This can easily be detected and blocked at the DNS level, and at NextDNS we released protections against CNAME Cloaking as soon as this started spreading, and we are continuously monitoring the situation to adapt quickly to methods like this.In the spirit of open discussion and full disclosure, we will automatically copy/paste below any comments made by the mentioned companies to team@nextdns.io.No comment has been made at this time.(1) Firefox for desktop does allow extensions to make DNS queries themselves, and extensions like uBlock Origin already apply their blocking rules to intermediary CNAMEs as well. Its worth noting that unfortunately, Firefoxs current market share is below 5%. ",
        "_version_": 1718527436834996224
      },
      {
        "story_id": [21437037],
        "story_author": ["protontypes"],
        "story_descendants": [54],
        "story_score": [150],
        "story_time": ["2019-11-03T22:07:25Z"],
        "story_title": "Robotics Development Environment with ROS in C++ and Python",
        "search": [
          "Robotics Development Environment with ROS in C++ and Python",
          "https://github.com/Ly0n/awesome-robotic-tooling",
          "Awesome Robotic Tooling Robotic resources and tools for professional development in C++ or Python with a touch of ROS, autonomous driving and aerospace. To stop reinventing the wheel you need to know about the wheel. This list is an attempt to show the variety of open and free tools in software and hardware development, which are useful in professional robotic development. Since the development processes are of crucial importance for the approval of such systems, the interaction of development processes and tools plays a central role. Your contribution is necessary to keep this list alive, increase the quality and to expand it. You can read more about it's origin and how you can participate in the contribution guide and related blog post. Contents Communication and Coordination Documentation and Presentation Requirements and Safety Architecture and Design Frameworks and Stacks Development Environment Code and Run Template Build and Deploy Unit and Integration Test Lint and Format Debugging and Tracing Version Control Simulation Electronics and Mechanics Sensor Processing Calibration and Transformation Perception Pipeline Machine Learning Parallel Processing Image Processing Radar Processing Lidar and Point Cloud Processing Localization and State Estimation Simultaneous Localization and Mapping Lidar Visual Vector Map Prediction Behavior and Decision Planning and Control User Interaction Graphical User Interface Acoustic User Interface Command Line Interface Data Visualization and Mission Control Annotation Point Cloud RViz Operation System Monitoring Database and Record Network Distributed File System Server Infrastructure and High Performance Computing Embedded Operation System Real-Time Kernel Network and Middleware Ethernet and Wireless Networking Controller Area Network Sensor and Acuator Interfaces Security Datasets Communication and Coordination Agile Development - Manifesto for Agile Software Development. Gitflow - Makes parallel development very easy, by isolating new development from finished work. DeepL - An online translator that outperforms Google, Microsoft and Facebook. Taiga - Agile Projectmanagment Tool. Kanboard - Minimalistic Kanban Board. kanban - Free, open source, self-hosted, Kanban board for GitLab issues. Gitlab - Simple Selfhosted Gitlab Server with Docker. Gogs - Build a simple, stable and extensible self-hosted Git service that can be setup in the most painless way. Wekan - Meteor based Kanban Board. JIRA API - Python Library for REST API of Jira. Taiga API - Python Library for REST API of Taiga. Chronos-Timetracker - Desktop client for JIRA. Track time, upload worklogs without a hassle. Grge - Grge is a daemon and command line utility augmenting GitLab. gitlab-triage - Gitlab's issues and merge requests triage, automated. Helpy - A modern, open source helpdesk customer support application. ONLYOFFICE - A free open source collaborative system developed to manage documents, projects, customer relationship and email correspondence, all in one place. discourse - A platform for community discussion. Free, open, simple. Gerrit - A code review and project management tool for Git based projects. jitsi-meet - Secure, Simple and Scalable Video Conferences that you use as a standalone app or embed in your web application. mattermost - An open source, private cloud, Slack-alternative. openproject - The leading open source project management software. leantime - Leantime is a lean project management system for innovators. gitter - Gitter is a chat and networking platform that helps to manage, grow and connect communities through messaging, content and discovery. Documentation and Presentation Typora - A Minimalist Markdown Editor. Markor - A Simple Markdown Editor for your Android Device. Pandoc - Universal markup converter. Yaspeller - Command line tool for spell checking. ReadtheDocs - Build your local ReadtheDocs Server. Doxygen - Doxygen is the de facto standard tool for generating documentation from annotated C++ sources. Sphinx - A tool that makes it easy to create intelligent and beautiful documentation for Python projects. Word-to-Markdown - A ruby gem to liberate content from Microsoft Word document. paperless - Index and archive all of your scanned paper documents. carbon - Share beautiful images of your source code. undraw - Free Professional business SVGs easy to customize. asciinema - Lets you easily record terminal sessions and replay them in a terminal as well as in a web browser. inkscape - Inkscape is a professional vector graphics editor for Linux, Windows and macOS. Reveal-Hugo - A Hugo theme for Reveal.js that makes authoring and customization a breeze. With it, you can turn any properly-formatted Hugo content into a HTML presentation. Hugo-Webslides - This is a Hugo template to create WebSlides presentation using markdown. jupyter2slides - Cloud Native Presentation Slides with Jupyter Notebook + Reveal.js. patat - Terminal-based presentations using Pandoc. github-changelog-generator - Automatically generate change log from your tags, issues, labels and pull requests on GitHub. GitLab-Release-Note-Generator - A Gitlab release note generator that generates release note on latest tag. OCRmyPDF - Adds an OCR text layer to scanned PDF files, allowing them to be searched. papermill - A tool for parameterizing, executing, and analyzing Jupyter Notebooks. docsy - An example documentation site using the Docsy Hugo theme. actions-hugo - Deploy website based on Hugo to GitHub Pages. overleaf - An open-source online real-time collaborative LaTeX editor. landslide - Generate HTML5 slideshows from markdown, ReST, or textile. libreoffice-impress-templates - Freely-licensed LibreOffice Impress templates. opensourcedesign - Community and Resources for Free Design and Logo Creation. olive - A free non-linear video editor aiming to provide a fully-featured alternative to high-end professional video editing software. buku - Browser-independent bookmark manager. swiftlatex - A WYSIWYG Browser-based LaTeX Editor. ReLaXed - Allows complex PDF layouts to be defined with CSS and JavaScript, while writing the content in a friendly, minimal syntax close to Markdown or LaTeX. foam - Foam is a personal knowledge management and sharing system inspired by Roam Research, built on Visual Studio Code and GitHub. CodiMD - Open Source Online Real-time collaborate on team documentation in markdown. jupyter-book - Build interactive, publication-quality documents from Jupyter Notebooks. InvoiceNet - Deep neural network to extract intelligent information from invoice documents. tesseract - Open Source OCR Engine. Requirements and Safety awesome-safety-critical - List of resources about programming practices for writing safety-critical software. open-autonomous-safety - OAS is a fully open-source library of Voyage's safety processes and testing procedures, designed to supplement existing safety programs at self-driving car startups across the world. CarND-Functional-Safety-Project - Create functional safety documents in this Udacity project. Automated Valet Parking Safety Documents - Created to support the safe testing of the Automated Valet Parking function using the StreetDrone test vehicle in a car park. safe_numerics - Replacements to standard numeric types which throw exceptions on errors. Air Vehicle C++ development coding standards - Provide direction and guidance to C++ programmers that will enable them to employ good programming style and proven programming practices leading to safe, reliable, testable, and maintainable code. AUTOSAR Coding Standard - Guidelines for the use of the C++14 language in critical and safety-related system. The W-Model and Lean Scaled Agility for Engineering - Ford applied an agile V-Model method from Vector that can be used in safety related project management. doorstop - Requirements management using version control. capella - Comprehensive, extensible and field-proven MBSE tool and method to successfully design systems architecture. robmosys - RobMoSys envisions an integrated approach built on top of the current code-centric robotic platforms, by applying model-driven methods and tools. Papyrus for Robotics - A graphical editing tool for robotic applications that complies with the RobMoSys approach. fossology - A toolkit you can run license, copyright and export control scans from the command line. ScenarioArchitect - The Scenario Architect is a basic python tool to generate, import and export short scene snapshots. Architecture and Design Guidelines - How to architect ROS-based systems. yEd - A powerful desktop application that can be used to quickly and effectively generate high-quality diagrams. yed_py - Generates graphML that can be opened in yEd. Plantuml - Web application to generate UML diagrams on-the-fly in your live documentation. rqt_graph - Provides a GUI plugin for visualizing the ROS computation graph. rqt_launchtree - An RQT plugin for hierarchical launchfile configuration introspection. cpp-dependencies - Tool to check C++ #include dependencies (dependency graphs created in .dot format). pydeps - Python Module Dependency graphs. aztarna - A footprinting tool for robots. draw.io - A free online diagram software for making flowcharts, process diagrams, org charts, UML, ER and network diagrams. vscode-drawio - This extension integrates Draw.io into VS Code. Frameworks and Stacks ROS - (Robot Operating System) provides libraries and tools to help software developers create robot applications. awesome-ros2 - A curated list of awesome Robot Operating System Version 2.0 (ROS 2) resources and libraries. Autoware.Auto - Autoware.Auto applies best-in-class software engineering for autonomous driving. Autoware.ai - Autoware.AI is the world's first \"All-in-One\" open-source software for autonomous driving technology. OpenPilot - Open Source Adaptive Cruise Control (ACC) and Lane Keeping Assist System (LKAS). Apollo - High performance, flexible architecture which accelerates the development, testing, and deployment of Autonomous Vehicles. PythonRobotics - This is a Python code collection of robotics algorithms, especially for autonomous navigation. Stanford Self Driving Car Code - Stanford Code From Cars That Entered DARPA Grand Challenges. astrobee - Astrobee is a free-flying robot designed to operate as a payload inside the International Space Station (ISS). CARMAPlatform - Enables cooperative automated driving plug-in. Automotive Grade Linux - Automotive Grade Linux is a collaborative open source project that is bringing together automakers, suppliers and technology companies to accelerate the development and adoption of a fully open software stack for the connected car. PX4 - An open source flight control software for drones and other unmanned vehicles. KubOS - An open-source software stack for satellites. mod_vehicle_dynamics_control - TUM Roborace Team Software Stack - Path tracking control, velocity control, curvature control and state estimation. Aslan - Open source self-driving software for low speed environments. open-source-rover - A build-it-yourself, 6-wheel rover based on the rovers on Mars from JPL. pybotics - An open-source and peer-reviewed Python toolbox for robot kinematics and calibration. makani - Contains the working Makani flight simulator, controller (autopilot), visualizer, and command center flight monitoring tools. mir_robot - This is a community project to use the MiR Robots with ROS. Development Environment Code and Run Vim-ros - Vim plugin for ROS development. Visual Studio Code - Code editor for edit-build-debug cycle. atom - Hackable text editor for the 21st century. Teletype - Share your workspace with team members and collaborate on code in real time in Atom. Sublime - A sophisticated text editor for code, markup and prose. ade-cli - The ADE Development Environment (ADE) uses docker and Gitlab to manage environments of per project development tools and optional volume images. recipe-wizard - A Dockerfile generator for running OpenGL (GLX) applications with nvidia-docker2, CUDA, ROS, and Gazebo on a remote headless server system. Jupyter ROS - Jupyter widget helpers for ROS, the Robot Operating System. ros_rqt_plugin - The ROS Qt Creator Plug-in for Python. xeus-cling - Jupyter kernel for the C++ programming language. ROS IDEs - This page collects experience and advice on using integrated development environments (IDEs) with ROS. TabNine - The all-language autocompleter. kite - Use machine learning to give you useful code completions for Python. jedi - Autocompletion and static analysis library for python. roslibpy - Python ROS Bridge library allows to use Python and IronPython to interact with ROS, the open-source robotic middleware. pybind11 - Seamless operability between C++11 and Python. Sourcetrail - Free and open-source cross-platform source explorer. rebound - Command-line tool that instantly fetches Stack Overflow results when an exception is thrown. mybinder - Open notebooks in an executable environment, making your code immediately reproducible by anyone, anywhere. ROSOnWindows - An experimental release of ROS1 for Windows. live-share - Real-time collaborative development from the comfort of your favorite tools. cocalc - Collaborative Calculation in the Cloud. EasyClangComplete - Robust C/C++ code completion for Sublime Text 3. vscode-ros - Visual Studio Code extension for Robot Operating System (ROS) development. awesome-hpp - A curated list of awesome header-only C++ libraries. Template ROS - Template for ROS node standardization in C++. Launch - Templates on how to create launch files for larger projects. Bash - A bash scripting template incorporating best practices & several useful functions. URDF - Examples on how to create Unified Robot Description Format (URDF) for different kinds of robots. Python - Style guide to be followed in writing Python code for ROS. Docker - The Dockerfile in the minimal-ade project shows a minimal example of how to create a custom base image. VS Code ROS2 Workspace Template - Template for using VSCode as an IDE for ROS2 development. Build and Deploy qemu-user-static - Enable an execution of different multi-architecture containers by QEMU and binfmt_misc. Cross compile ROS 2 on QNX - Introduces how to cross compile ROS 2 on QNX. bloom - A release automation tool which makes releasing catkin packages easier. superflore - An extended platform release manager for Robot Operating System. catkin_tools - Command line tools for working with catkin. industrial_ci - Easy continuous integration repository for ROS repositories. ros_gitlab_ci - Contains helper scripts and instructions on how to use Continuous Integration (CI) for ROS projects hosted on a GitLab instance. gitlab-runner - Runs tests and sends the results to GitLab. colcon-core - Command line tool to improve the workflow of building, testing and using multiple software packages. gitlab-release - Simple python3 script to upload files (from ci) to the current projects release (tag). clang - This is a compiler front-end for the C family of languages (C, C++, Objective-C, and Objective-C++) which is built as part of the LLVM compiler infrastructure project. catkin_virtualenv - Bundle python requirements in a catkin package via virtualenv. pyenv - Simple Python version management. aptly - Debian repository management tool. cross_compile - Assets used for ROS2 cross-compilation. docker_images - Official Docker images maintained by OSRF on ROS(2) and Gazebo. robot_upstart - Presents a suite of scripts to assist with launching background ROS processes on Ubuntu Linux PCs. robot_systemd - Units for managing startup and shutdown of roscore and roslaunch. ryo-iso - A modern ISO builder that streamlines the process of deploying a complete robot operating system from a yaml config file. network_autoconfig - Automatic configuration of ROS networking for most use cases without impacting usage that require manual configuration. rosbuild - The ROS build farm. cros - A single thread pure C implementation of the ROS framework. Unit and Integration Test setup-ros - This action sets up a ROS and ROS 2 environment for use in GitHub actions. UnitTesting - This page lays out the rationale, best practices, and policies for writing and running unit tests and integration tests for ROS. googletest - Google's C++ test framework. pytest - The pytest framework makes it easy to write small tests, yet scales to support complex functional testing. doctest - The fastest feature-rich C++11/14/17/20 single-header testing framework for unit tests and TDD. osrf_testing_tools_cpp - Contains testing tools for C++, and is used in OSRF projects. code_coverage - ROS package to run coverage testing. action-ros-ci - GitHub Action to build and test ROS 2 packages using colcon. Lint and Format action-ros-lint - GitHub action to run linters on ROS 2 packages. cppcheck - Static analysis of C/C++ code. hadolint - Dockerfile linter, validate inline bash, written in Haskell. shellcheck - A static analysis tool for shell scripts. catkin_lint - Checks package configurations for the catkin build system of ROS. pylint - Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. black - The uncompromising Python code formatter. pydocstyle - A static analysis tool for checking compliance with Python docstring conventions. haros - Static analysis of ROS application code. pydantic - Data parsing and validation using Python type hints. Debugging and Tracing heaptrack - Traces all memory allocations and annotates these events with stack traces. ros2_tracing - Tracing tools for ROS 2. Linuxperf - Various Linux performance material. lptrace - It lets you see in real-time what functions a Python program is running. pyre-check - Performant type-checking for python. FlameGraph - Visualize profiled code. gpuvis - GPU Trace Visualizer. sanitizer - AddressSanitizer, ThreadSanitizer, MemorySanitizer. cppinsights - C++ Insights - See your source code with the eyes of a compiler. inspect - The inspect module provides functions for learning about live objects, including modules, classes, instances, functions, and methods. Roslaunch Nodes in Valgrind or GDB - When debugging roscpp nodes that you are launching with roslaunch, you may wish to launch the node in a debugging program like gdb or valgrind instead. pyperformance - Python Performance Benchmark Suite. qira - QIRA is a competitor to strace and gdb. gdb-frontend - GDBFrontend is an easy, flexible and extensionable gui debugger. lttng - An open source software toolkit which you can use to simultaneously trace the Linux kernel, user applications, and user libraries. ros2-performance - Allows to easily create arbitrary ROS2 systems and then measures their performance. bcc - Tools for BPF-based Linux IO analysis, networking, monitoring, and more. tracy - A real time, nanosecond resolution, remote telemetry frame profiler for games and other applications. bpftrace - High-level tracing language for Linux eBPF. pudb - Full-screen console debugger for Python. backward-cpp - A beautiful stack trace pretty printer for C++. gdb-dashboard - GDB dashboard is a standalone .gdbinit file written using the Python API that enables a modular interface showing relevant information about the program being debugged. hotspot - The Linux perf GUI for performance analysis. memory_profiler - A python module for monitoring memory consumption of a process as well as line-by-line analysis of memory consumption for python programs. ros1_fuzzer - This fuzzer aims to help developers and researchers to find bugs and vulnerabilities in ROS nodes by performing fuzz tests over topics that the target nodes process. vscode-debug-visualizer - An extension for VS Code that visualizes data during debugging. action-tmate - Debug your GitHub Actions via SSH by using tmate to get access to the runner system itself. libstatistics_collector - ROS 2 library providing classes to collect measurements and calculate statistics across them. system_metrics_collector - Lightweight, real-time system metrics collector for ROS2 systems. Version Control git-fuzzy - A CLI interface to git that relies heavily on fzf. meld - Meld is a visual diff and merge tool that helps you compare files, directories, and version controlled projects. tig - Text-mode interface for git. gitg - A graphical user interface for git. git-cola - The highly caffeinated Git GUI. python-gitlab - A Python package providing access to the GitLab server API. bfg-repo-cleaner - Removes large or troublesome blobs like git-filter-branch does, but faster. nbdime - Tools for diffing and merging of Jupyter notebooks. semantic-release - Fully automated version management and package publishing. go-semrel-gitab - Automate version management for Gitlab. Git-repo - Git-Repo helps manage many Git repositories, does the uploads to revision control systems, and automates parts of the development workflow. dive - A tool for exploring each layer in a docker image. dvc - Management and versioning of datasets and machine learning models. learnGitBranching - A git repository visualizer, sandbox, and a series of educational tutorials and challenges. gitfs - You can mount a remote repository's branch locally, and any subsequent changes made to the files will be automatically committed to the remote. git-secret - Encrypts files with permitted users' public keys, allowing users you trust to access encrypted data using pgp and their secret keys. git-sweep - A command-line tool that helps you clean up Git branches that have been merged into master. lazygit - A simple terminal UI for git commands, written in Go with the gocui library. glab - An open-source GitLab command line tool. Simulation Drake - Drake aims to simulate even very complex dynamics of robots. Webots - Webots is an open source robot simulator compatible (among others) with ROS and ROS2. lgsv - LG Electronics America R&D Center has developed an HDRP Unity-based multi-robot simulator for autonomous vehicle developers. carla - Open-source simulator for autonomous driving research. awesome-CARLA - A curated list of awesome CARLA tutorials, blogs, and related projects. ros-bridge - ROS bridge for CARLA Simulator. scenario_runner - Traffic scenario definition and execution engine. deepdive - End-to-end simulation for self-driving cars. uuv_simulator - Gazebo/ROS packages for underwater robotics simulation. AirSim - Open source simulator for autonomous vehicles built on Unreal Engine. self-driving-car-sim - A self-driving car simulator built with Unity. ROSIntegration - Unreal Engine Plugin to enable ROS Support. gym-gazebo - An OpenAI gym extension for using Gazebo known as gym-gazebo. highway-env - A collection of environments for autonomous driving and tactical decision-making tasks. VREP Interface - ROS Bridge for the VREP simulator. car_demo - This is a simulation of a Prius in gazebo 9 with sensor data being published using ROS kinetic. sumo - Eclipse SUMO is an open source, highly portable, microscopic and continuous road traffic simulation package designed to handle large road networks. open-simulation-interface - A generic interface for the environmental perception of automated driving functions in virtual scenarios. ESIM - An Open Event Camera Simulator. Menge - Crowd Simulation Framework. pedsim_ros - Pedestrian simulator powered by the social force model for Gazebo. opencrg - Open file formats and open source tools for the detailed description, creation and evaluation of road surfaces. esmini - A basic OpenSCENARIO player. OpenSceneGraph - An open source high performance 3D graphics toolkit, used by application developers in fields such as visual simulation, games, virtual reality, scientific visualization and modelling. morse - An academic robotic simulator, based on the Blender Game Engine and the Bullet Physics engine. ROSIntegrationVision - Support for ROS-enabled RGBD data acquisition in Unreal Engine Projects. fetch_gazebo - Contains the Gazebo simulation for Fetch Robotics Fetch and Freight Research Edition Robots. rotors_simulator - Provides some multirotor models. flow - A computational framework for deep RL and control experiments for traffic microsimulation. gnss-ins-sim - GNSS + inertial navigation, sensor fusion simulator. Motion trajectory generator, sensor models, and navigation. Ignition Robotics - Test control strategies in safety, and take advantage of simulation in continuous integration tests. simulation assets for the SubT - This collection contains simulation assets for the SubT Challenge Virtual Competition in Gazebo. gazebo_ros_motors - Contains currently two motor plugins for Gazebo, one with an ideal speed controller and one without a controller that models a DC motor. map2gazebo - ROS package for creating Gazebo environments from 2D maps. sim_vehicle_dynamics - Vehicle Dynamics Simulation Software of TUM Roborace Team. gym-carla - An OpenAI gym wrapper for CARLA simulator. simbody - High-performance C++ multibody dynamics/physics library for simulating articulated biomechanical and mechanical systems like vehicles, robots, and the human skeleton. gazebo_models - This repository holds the Gazebo model database. pylot - Autonomous driving platform running on the CARLA simulator. flightmare - Flightmare is composed of two main components: a configurable rendering engine built on Unity and a flexible physics engine for dynamics simulation. champ - ROS Packages for CHAMP Quadruped Controller. rex-gym - OpenAI Gym environments for an open-source quadruped robot (SpotMicro). Trick - Developed at the NASA Johnson Space Center, is a powerful simulation development framework that enables users to build applications for all phases of space vehicle development. usv_sim_lsa - Unmanned Surface Vehicle simulation on Gazebo with water current and winds. 42 - Simulation for spacecraft attitude control system analysis and design. Complete_Street_Rule - A scenario oriented design tool intended to enable users to quickly create procedurally generated multimodal streets in ArcGIS CityEngine. AutoCore simulation - Provides test environment for Autoware and still during early development, contents below may changed during updates. fields-ignition - Generate random crop fields for Ignition Gazebo. Electronics and Mechanics HRIM - An information model for robot hardware. URDF - Repository for Unified Robot Description Format (URDF) parsing code. phobos - An add-on for Blender allowing to create URDF, SDF and SMURF robot models in a WYSIWYG environment. urdf-viz - Visualize URDF/XACRO file, URDF Viewer works on Windows/macOS/Linux. solidworks_urdf_exporter - SolidWorks to URDF Exporter. FreeCAD - Your own 3D parametric modeler. kicad - A Cross Platform and Open Source Electronics Design Automation Suite. PcbDraw - Convert your KiCAD board into a nice looking 2D drawing suitable for pinout diagrams. kicad-3rd-party-tools - Tools made by others to augment the KiCad PCB EDA suite. PandaPower - An easy to use open source tool for power system modeling, analysis and optimization with a high degree of automation. LibrePCB - A powerful, innovative and intuitive EDA tool for everyone. openscad - A software for creating solid 3D CAD models. ngspice - A open source spice simulator for electric and electronic circuits. GNSS-SDR - GNSS-SDR provides interfaces for a wide range of radio frequency front-ends and raw sample file formats, generates processing outputs in standard formats. riscv - The Free and Open RISC Instruction Set Architecture. urdfpy - A simple and easy-to-use library for loading, manipulating, saving, and visualizing URDF files. FMPy - Simulate Functional Mockup Units (FMUs) in Python. FMIKit-Simulink - Import and export Functional Mock-up Units with Simulink. oemof-solph - A modular open source framework to model energy supply systems. NASA-3D-Resources - Here you'll find a growing collection of 3D models, textures, and images from inside NASA. SUAVE - An Aircraft Design Toolbox. opem - The Open-Source PEMFC Simulation Tool (OPEM) is a modeling tool for evaluating the performance of proton exchange membrane fuel cells. pvlib-python - A community supported tool that provides a set of functions and classes for simulating the performance of photovoltaic energy systems. WireViz - A tool for easily documenting cables, wiring harnesses and connector pinouts. Horizon - EDA is an Electronic Design Automation package supporting an integrated end-to-end workflow for printed circuit board design including parts management and schematic entry. tigl - The TiGL Geometry Library can be used for the computation and processing of aircraft geometries stored inside CPACS files. foxBMS - A free, open and flexible development environment to design battery management systems. cadCAD - A Python package that assists in the processes of designing, testing and validating complex systems through simulation, with support for Monte Carlo methods, A/B testing and parameter sweeping. OpenMDAO - An open-source framework for efficient multidisciplinary optimization. ODrive - The aim is to make it possible to use inexpensive brushless motors in high performance robotics projects. OpenTirePython - An open-source mathematical tire modelling library. Sensor Processing Calibration and Transformation tf2 - Transform library, which lets the user keep track of multiple coordinate frames over time. lidar_align - A simple method for finding the extrinsic calibration between a 3D lidar and a 6-dof pose sensor. kalibr - The Kalibr visual-inertial calibration toolbox. Calibnet - Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks. lidar_camera_calibration - ROS package to find a rigid-body transformation between a LiDAR and a camera. ILCC - Reflectance Intensity Assisted Automatic and Accurate Extrinsic Calibration of 3D LiDAR. easy_handeye - Simple, straighforward ROS library for hand-eye calibration. imu_utils - A ROS package tool to analyze the IMU performance. kalibr_allan - IMU Allan standard deviation charts for use with Kalibr and inertial kalman filters. pyquaternion - A full-featured Python module for representing and using quaternions. robot_calibration - This package offers calibration of a number of parameters of a robot, such as: 3D Camera intrinsics, extrinsics Joint angle offsets and robot frame offsets. multi_sensor_calibration - Contains a calibration tool to calibrate a sensor setup consisting of lidars, radars and cameras. LiDARTag - A Real-Time Fiducial Tag using Point Clouds Lidar Data. multicam_calibration - Extrinsic and intrinsic calbration of cameras. ikpy - An Inverse Kinematics library aiming performance and modularity. livox_camera_lidar_calibration - Calibrate the extrinsic parameters between Livox LiDAR and camera. lidar_camera_calibration - Camera LiDAR Calibration using ROS, OpenCV, and PCL. Perception Pipeline SARosPerceptionKitti - ROS package for the Perception (Sensor Processing, Detection, Tracking and Evaluation) of the KITTI Vision Benchmark Suite. multiple-object-tracking-lidar - C++ implementation to Detect, track and classify multiple objects using LIDAR scans or point cloud. cadrl_ros - ROS package for dynamic obstacle avoidance for ground robots trained with deep RL. AugmentedAutoencoder - RGB-based pipeline for object detection and 6D pose estimation. jsk_recognition - A stack for the perception packages which are used in JSK lab. GibsonEnv - Gibson Environments: Real-World Perception for Embodied Agents. morefusion - Multi-object Reasoning for 6D Pose Estimation from Volumetric Fusion. Machine Learning DLIB - A toolkit for making real world machine learning and data analysis applications in C++. fastai - The fastai library simplifies training fast and accurate neural nets using modern best practices. tpot - A Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. deap - Distributed Evolutionary Algorithms in Python. gym - A toolkit for developing and comparing reinforcement learning algorithms. tensorflow_ros_cpp - A ROS package that allows to do Tensorflow inference in C++ without the need to compile TF yourself. Tensorflow Federated - TensorFlow Federated (TFF) is an open-source framework for machine learning and other computations on decentralized data. finn - Fast, Scalable Quantized Neural Network Inference on FPGAs. neuropod - Neuropod is a library that provides a uniform interface to run deep learning models from multiple frameworks in C++ and Python. leela-zero - This is a fairly faithful reimplementation of the system described in the Alpha Go Zero paper \"Mastering the Game of Go without Human Knowledge\". Trax - A library for deep learning that focuses on sequence models and reinforcement learning. mlflow - A platform to streamline machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models. Netron - Visualizer for neural network, deep learning and machine learning models. MNN - A blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba. Tensorforce - An open-source deep reinforcement learning framework, with an emphasis on modularized flexible library design and straightforward usability for applications in research and practice. Dopamine - A research framework for fast prototyping of reinforcement learning algorithms. catalyst - Was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. ray - A fast and simple framework for building and running distributed applications. tf-agents - A reliable, scalable and easy to use TensorFlow library for Contextual Bandits and Reinforcement Learning. ReAgent - An open source end-to-end platform for applied reinforcement learning (RL) developed and used at Facebook. Awesome-Mobile-Machine-Learning - A curated list of awesome mobile machine learning resources for iOS, Android, and edge devices. cnn-explainer - Learning Convolutional Neural Networks with Interactive Visualization. modelzoo - A collection of machine-learned models for use in autonomous driving applications. Parallel Processing dask - Parallel computing with task scheduling for Python. cupy - NumPy-like API accelerated with CUDA. Thrust - A C++ parallel programming library which resembles the C++ Standard Library. ArrayFire - A general purpose GPU library. OpenMP - An application programming interface that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran. VexCL - VexCL is a C++ vector expression template library for OpenCL/CUDA/OpenMP. PYNQ - An open-source project from Xilinx that makes it easy to design embedded systems with Zynq All Programmable Systems on Chips. numba - NumPy aware dynamic Python compiler using LLVM. TensorRT - A C++ library for high performance inference on NVIDIA GPUs and deep learning accelerators. libcudacxx - Provides a heterogeneous implementation of the C++ Standard Library that can be used in and between CPU and GPU code. Image Processing CV-pretrained-model - A collection of computer vision pre-trained models. image_pipeline - Fills the gap between getting raw images from a camera driver and higher-level vision processing. gstreamer - A pipeline-based multimedia framework that links together a wide variety of media processing systems to complete complex workflows. ros2_openvino_toolkit - Provides a ROS-adaptered runtime framework of neural network which quickly deploys applications and solutions for vision inference. vision_visp - Wraps the ViSP moving edge tracker provided by the ViSP visual servoing library into a ROS package. apriltag_ros - A ROS wrapper of the AprilTag 3 visual fiducial detector. deep_object_pose - Deep Object Pose Estimation. DetectAndTrack - Detect-and-Track: Efficient Pose. SfMLearner - An unsupervised learning framework for depth and ego-motion estimation. imgaug - Image augmentation for machine learning experiments. vision_opencv - Packages for interfacing ROS with OpenCV, a library of programming functions for real time computer vision. darknet_ros - YOLO ROS: Real-Time Object Detection for ROS. ros_ncnn - YOLACT / YOLO ( among other things ) on NCNN inference engine for ROS. tf-pose-estimation - Deep Pose Estimation implemented using Tensorflow with Custom Architectures for fast inference. find-object - Simple Qt interface to try OpenCV implementations of SIFT, SURF, FAST, BRIEF and other feature detectors and descriptors. yolact - A simple, fully convolutional model for real-time instance segmentation. Kimera-Semantics - Real-Time 3D Semantic Reconstruction from 2D data. detectron2 - A next-generation research platform for object detection and segmentation. OpenVX - Enables performance and power-optimized computer vision processing, especially important in embedded and real-time use cases. 3d-vehicle-tracking - Official implementation of Joint Monocular 3D Vehicle Detection and Tracking. pysot - The goal of PySOT is to provide a high-quality, high-performance codebase for visual tracking research. semantic_slam - Real time semantic slam in ROS with a hand held RGB-D camera. kitti_scan_unfolding - We propose KITTI scan unfolding in our paper Scan-based Semantic Segmentation of LiDAR Point Clouds: An Experimental Study. packnet-sfm - Official PyTorch implementation of self-supervised monocular depth estimation methods invented by the ML Team at Toyota Research Institute (TRI). AB3DMOT - This work proposes a simple yet accurate real-time baseline 3D multi-object tracking system. monoloco - Official implementation of \"MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation\" in PyTorch. Poly-YOLO - Builds on the original ideas of YOLOv3 and removes two of its weaknesses: a large amount of rewritten labels and inefficient distribution of anchors. satellite-image-deep-learning - Resources for deep learning with satellite & aerial imagery. robosat - Semantic segmentation on aerial and satellite imagery. big_transfer - Model for General Visual Representation Learning created by Google Research. LEDNet - A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation. TorchSeg - This project aims at providing a fast, modular reference implementation for semantic segmentation models using PyTorch. simpledet - A Simple and Versatile Framework for Object Detection and Instance Recognition. meshroom - Meshroom is a free, open-source 3D Reconstruction Software based on the AliceVision Photogrammetric Computer Vision framework. EasyOCR - Ready-to-use Optical character recognition (OCR) with 40+ languages supported including Chinese, Japanese, Korean and Thai. pytracking - A general python framework for visual object tracking and video object segmentation, based on PyTorch. ros_deep_learning - Deep learning inference nodes for ROS with support for NVIDIA Jetson TX1/TX2/Xavier and TensorRT. hyperpose - HyperPose: A Flexible Library for Real-time Human Pose Estimation. fawkes - Privacy preserving tool against facial recognition systems. anonymizer - An anonymizer to obfuscate faces and license plates. opendatacam - Only saves surveyed meta-data, in particular the path an object moved or number of counted objects at a certain point. Cam2BEV - TensorFlow Implementation for Computing a Semantically Segmented Bird's Eye View (BEV) Image Given the Images of Multiple Vehicle-Mounted Cameras. satpy - Python package for earth-observing satellite data processing. flownet2-pytorch - Pytorch implementation of FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks. Simd - C++ image processing and machine learning library with using of SIMD: SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, AVX, AVX2, AVX-512, VMX(Altivec) and VSX(Power7), NEON for ARM. Radar Processing pyroSAR - Framework for large-scale SAR satellite data processing. CameraRadarFusionNet - TUM Roborace Team Software Stack - Path tracking control, velocity control, curvature control and state estimation. Lidar and Point Cloud Processing cilantro - A lean C++ library for working with point cloud data. open3d - Open3D: A Modern Library for 3D Data Processing. SqueezeSeg - Implementation of SqueezeSeg, convolutional neural networks for LiDAR point clout segmentation. point_cloud_io - ROS nodes to read and write point clouds from and to files (e.g. ply, vtk). python-pcl - Python bindings to the pointcloud library. libpointmatcher - An \"Iterative Closest Point\" library for 2-D/3-D mapping in Robotics. depth_clustering - Fast and robust clustering of point clouds generated with a Velodyne sensor. lidar-bonnetal - Semantic and Instance Segmentation of LiDAR point clouds for autonomous driving. CSF - LiDAR point cloud ground filtering / segmentation (bare earth extraction) method based on cloth simulation. robot_body_filter - A highly configurable LaserScan/PointCloud2 filter that allows to dynamically remove the 3D body of the robot from the measurements. grid_map - Universal grid map library for mobile robotic mapping. elevation_mapping - Robot-centric elevation mapping for rough terrain navigation. rangenet_lib - Contains simple usage explanations of how the RangeNet++ inference works with the TensorRT and C++ interface. pointcloud_to_laserscan - Converts a 3D Point Cloud into a 2D laser scan. octomap - An Efficient Probabilistic 3D Mapping Framework Based on Octrees. pptk - Point Processing Toolkit from HEREMaps. gpu-voxels - GPU-Voxels is a CUDA based library which allows high resolution volumetric collision detection between animated 3D models and live pointclouds from 3D sensors of all kinds. spatio_temporal_voxel_layer - A new voxel layer leveraging modern 3D graphics tools to modernize navigation environmental representations. LAStools - Award-winning software for efficient LiDAR processing. PCDet - A general PyTorch-based codebase for 3D object detection from point cloud. PDAL - A C++ BSD library for translating and manipulating point cloud data. PotreeConverter - Builds a potree octree from las, laz, binary ply, xyz or ptx files. fast_gicp - A collection of GICP-based fast point cloud registration algorithms. ndt_omp - Multi-threaded and SSE friendly NDT algorithm. laser_line_extraction - A ROS packages that extracts line segments from LaserScan messages. Go-ICP - Implementation of the Go-ICP algorithm for globally optimal 3D pointset registration. PointCNN - A simple and general framework for feature learning from point clouds. segmenters_lib - The LiDAR segmenters library, for segmentation-based detection. MotionNet - Joint Perception and Motion Prediction for Autonomous Driving Based on Bird's Eye View Maps. PolarSeg - An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation. traversability_mapping - Takes in point cloud from a Velodyne VLP-16 Lidar and outputs a traversability map for autonomous navigation in real-time. lidar_super_resolution - Simulation-based Lidar Super-resolution for Ground Vehicles. Cupoch - A library that implements rapid 3D data processing and robotics computation using CUDA. linefit_ground_segmentation - Implementation of the ground segmentation algorithm. Draco - A library for compressing and decompressing 3D geometric meshes and point clouds. Votenet - Deep Hough Voting for 3D Object Detection in Point Clouds. lidar_undistortion - Provides lidar motion undistortion based on an external 6DoF pose estimation input. superpoint_graph - Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs. RandLA-Net - Efficient Semantic Segmentation of Large-Scale Point Clouds. Det3D - A first 3D Object Detection toolbox which provides off the box implementations of many 3D object detection algorithms such as PointPillars, SECOND, PIXOR. OverlapNet - A modified Siamese Network that predicts the overlap and relative yaw angle of a pair of range images generated by 3D LiDAR scans. mp2p_icp - A repertory of multi primitive-to-primitive (MP2P) ICP algorithms in C++. OpenPCDet - A Toolbox for LiDAR-based 3D Object Detection. torch-points3d - Pytorch framework for doing deep learning on point clouds. PolyFit - Polygonal Surface Reconstruction from Point Clouds. mmdetection3d - Next-generation platform for general 3D object detection. gpd - Takes a point cloud as input and produces pose estimates of viable grasps as output. SalsaNext - Uncertainty-aware Semantic Segmentation of LiDAR Point Clouds for Autonomous Driving. Super-Fast-Accurate-3D-Object-Detection - Super Fast and Accurate 3D Object Detection based on 3D LiDAR Point Clouds (The PyTorch implementation). Localization and State Estimation evo - Python package for the evaluation of odometry and SLAM. robot_localization - A package of nonlinear state estimation nodes. fuse - General architecture for performing sensor fusion live on a robot. GeographicLib - A C++ library for geographic projections. ntripbrowser - A Python API for browsing NTRIP (Networked Transport of RTCM via Internet Protocol). imu_tools - IMU-related filters and visualizers. RTKLIB - A version of RTKLIB optimized for single and dual frequency low cost GPS receivers, especially u-blox receivers. gLAB - Performs precise modeling of GNSS observables (pseudorange and carrier phase) at the centimetre level, allowing standalone GPS positioning, PPP, SBAS and DGNSS. ai-imu-dr - Contains the code of our novel accurate method for dead reckoning of wheeled vehicles based only on an IMU. Kalman-and-Bayesian-Filters-in-Python - Kalman Filter book using Jupyter Notebook. mcl_3dl - A ROS node to perform a probabilistic 3-D/6-DOF localization system for mobile robots with 3-D LIDAR(s). se2lam - On-SE(2) Localization and Mapping for Ground Vehicles by Fusing Odometry and Vision. mmWave-localization-learning - ML-based positioning method from mmWave transmissions - with high accuracy and energy efficiency. dynamic_robot_localization - A ROS package that offers 3 DoF and 6 DoF localization using PCL and allows dynamic map update using OctoMap. eagleye - An open-source software for vehicle localization utilizing GNSS and IMU. python-sgp4 - Python version of the SGP4 satellite position library. PROJ - Cartographic Projections and Coordinate Transformations Library. rpg_trajectory_evaluation - Implements common used trajectory evaluation methods for visual(-inertial) odometry. pymap3d - Pure-Python (Numpy optional) 3D coordinate conversions for geospace ecef enu eci. Simultaneous Localization and Mapping Lidar loam_velodyne - Laser Odometry and Mapping (Loam) is a realtime method for state estimation and mapping using a 3D lidar. lio-mapping - Implementation of Tightly Coupled 3D Lidar Inertial Odometry and Mapping (LIO-mapping). A-LOAM - Advanced implementation of LOAM. Fast LOAM - Fast and Optimized Lidar Odometry And Mapping. LIO_SAM - Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping. cartographer_ros - Provides ROS integration for Cartographer. loam_livox - A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR. StaticMapping - Use LiDAR to map the static world. semantic_suma - Semantic Mapping using Surfel Mapping and Semantic Segmentation. slam_toolbox - Slam Toolbox for lifelong mapping and localization in potentially massive maps with ROS . maplab - An open visual-inertial mapping framework. hdl_graph_slam - An open source ROS package for real-time 6DOF SLAM using a 3D LIDAR. interactive_slam - In contrast to existing automatic SLAM packages, we with minimal human effort. LeGO-LOAM - Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain. pyslam - Contains a monocular Visual Odometry (VO) pipeline in Python. Kitware SLAM - LiDAR-only visual SLAM developped by Kitware, as well as ROS and ParaView wrappings for easier use. horizon_highway_slam - A robust, low drift, and real time highway SLAM package suitable for Livox Horizon lidar. mola - A Modular System for Localization and Mapping. DH3D - Deep Hierarchical 3D Descriptors for Robust Large-Scale 6DOF Relocalization. LaMa - LaMa is a C++11 software library for robotic localization and mapping. LIO-SAM - Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping. Scan Context - Global LiDAR descriptor for place recognition and long-term localization. Visual orb_slam_2_ros - A ROS implementation of ORB_SLAM2. orbslam-map-saving-extension - In this extensions the map of ORB-features be saved to the disk as a reference for future runs along the same track. dso - Direct Sparse Odometry. viso2 - A ROS wrapper for libviso2, a library for visual odometry. xivo - X Inertial-aided Visual Odometry. rovio - Robust Visual Inertial Odometry Framework. LSD-SLAM - Large-Scale Direct Monocular SLAM is a real-time monocular SLAM. CubeSLAM and ORB SLAM - Monocular 3D Object Detection and SLAM Package of CubeSLAM and ORB SLAM. VINS-Fusion - A Robust and Versatile Multi-Sensor Visual-Inertial State Estimator. openvslam - OpenVSLAM: A Versatile Visual SLAM Framework. basalt - Visual-Inertial Mapping with Non-Linear Factor Recovery. Kimera - A C++ library for real-time metric-semantic simultaneous localization and mapping, which uses camera images and inertial data to build a semantically annotated 3D mesh of the environment. tagslam - A ROS-based package for Simultaneous Localization and Mapping using AprilTag fiducial markers. LARVIO - A lightweight, accurate and robust monocular visual inertial odometry based on Multi-State Constraint Kalman Filter. fiducials - Simultaneous localization and mapping using fiducial markers. open_vins - An open source platform for visual-inertial navigation research. ORB_SLAM3 - ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM. Atlas - End-to-End 3D Scene Reconstruction from Posed Images. vilib - This library focuses on the front-end of VIO pipelines with CUDA. hloc - A modular toolbox for state-of-the-art 6-DoF visual localization. It implements Hierarchical Localization, leveraging image retrieval and feature matching, and is fast, accurate, and scalable. ESVO - A novel pipeline for real-time visual odometry using a stereo event-based camera. Vector Map OpenDRIVE - An open file format for the logical description of road networks. MapsModelsImporter - A Blender add-on to import models from google maps. Lanelet2 - Map handling framework for automated driving. barefoot - Online and Offline map matching that can be used stand-alone and in the cloud. iD - The easy-to-use OpenStreetMap editor in JavaScript. RapiD - An enhanced version of iD for mapping with AI created by Facebook. segmap - A map representation based on 3D segments. Mapbox - A JavaScript library for interactive, customizable vector maps on the web. osrm-backend - Open Source Routing Machine - C++ backend. assuremapingtools - Desktop based tool for viewing, editing and saving road network maps for autonomous vehicle platforms such as Autoware. geopandas - A project to add support for geographic data to pandas objects. MapToolbox - Plugins to make Autoware vector maps in Unity. imagery-index - An index of aerial and satellite imagery useful for mapping. mapillary_tools - A library for processing and uploading images to Mapillary. mapnik - Combines pixel-perfect image output with lightning-fast cartographic algorithms, and exposes interfaces in C++, Python, and Node. gdal - GDAL is an open source X/MIT licensed translator library for raster and vector geospatial data formats. grass - GRASS GIS - free and open source Geographic Information System (GIS). 3d-tiles - Specification for streaming massive heterogeneous 3D geospatial datasets. osmnx - Python for street networks. Retrieve, model, analyze, and visualize street networks and other spatial data from OpenStreetMap. Prediction Awesome-Interaction-aware-Trajectory-Prediction - A selection of state-of-the-art research materials on trajectory prediction. sgan - Socially Acceptable Trajectories with Generative Adversarial Networks. Behavior and Decision Groot - Graphical Editor to create BehaviorTrees. Compliant with BehaviorTree.CPP. BehaviorTree.CPP - Behavior Trees Library in C++. RAFCON - Uses hierarchical state machines, featuring concurrent state execution, to represent robot programs. ROSPlan - Generic framework for task planning in a ROS system. ad-rss-lib - Library implementing the Responsibility Sensitive Safety model (RSS) for Autonomous Vehicles. FlexBE - Graphical editor for hierarchical state machines, based on ROS's smach. sts_bt_library - This library provides the functionality to set up your own behavior tree logic by using the defined tree structures like Fallback, Sequence or Parallel Nodes. SMACC - An Event-Driven, Asynchronous, Behavioral State Machine Library for real-time ROS (Robotic Operating System) applications written in C++ . py_trees_ros - Behaviours, trees and utilities that extend py_trees for use with ROS. Planning and Control pacmod - Designed to allow the user to control a vehicle with the PACMod drive-by-wire system. mpcc - Model Predictive Contouring Controller for Autonomous Racing. rrt - C++ RRT (Rapidly-exploring Random Tree) implementation. HypridAStarTrailer - A path planning algorithm based on Hybrid A* for trailer truck. path_planner - Hybrid A* Path Planner for the KTH Research Concept Vehicle. open_street_map - ROS packages for working with Open Street Map geographic information. Open Source Car Control - An assemblage of software and hardware designs that enable computer control of modern cars in order to facilitate the development of autonomous vehicle technology. fastrack - A ROS implementation of Fast and Safe Tracking (FaSTrack). commonroad - Composable benchmarks for motion planning on roads. traffic-editor - A graphical editor for robot traffic flows. steering_functions - Contains a C++ library that implements steering functions for car-like robots with limited turning radius. moveit - Easy-to-use robotics manipulation platform for developing applications, evaluating designs, and building integrated products. flexible-collision-library - A library for performing three types of proximity queries on a pair of geometric models composed of triangles. aikido - Artificial Intelligence for Kinematics, Dynamics, and Optimization. casADi - A symbolic framework for numeric optimization implementing automatic differentiation in forward and reverse modes on sparse matrix-valued computational graphs. ACADO Toolkit - A software environment and algorithm collection for automatic control and dynamic optimization. control-toolbox - An efficient C++ library for control, estimation, optimization and motion planning in robotics. CrowdNav - Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning. ompl - Consists of many state-of-the-art sampling-based motion planning algorithms. openrave - Open Robotics Automation Virtual Environment: An environment for testing, developing, and deploying robotics motion planning algorithms. teb_local_planner - An optimal trajectory planner considering distinctive topologies for mobile robots based on Timed-Elastic-Bands. pinocchio - A fast and flexible implementation of Rigid Body Dynamics algorithms and their analytical derivatives. rmf_core - The rmf_core packages provide the centralized functions of the Robotics Middleware Framework (RMF). OpEn - A solver for Fast & Accurate Embedded Optimization for next-generation Robotics and Autonomous Systems. autogenu-jupyter - This project provides the continuation/GMRES method (C/GMRES method) based solvers for nonlinear model predictive control (NMPC) and an automatic code generator for NMPC. global_racetrajectory_optimization - This repository contains multiple approaches for generating global racetrajectories. toppra - A library for computing the time-optimal path parametrization for robots subject to kinematic and dynamic constraints. tinyspline - TinySpline is a small, yet powerful library for interpolating, transforming, and querying arbitrary NURBS, B-Splines, and Bzier curves. dual quaternions ros - ROS python package for dual quaternion SLERP. mb planner - Aerial vehicle planner for tight spaces. Used in DARPA SubT Challenge. ilqr - Iterative Linear Quadratic Regulator with auto-differentiatiable dynamics models. EGO-Planner - A lightweight gradient-based local planner without ESDF construction, which significantly reduces computation time compared to some state-of-the-art methods. pykep - A scientific library providing basic tools for research in interplanetary trajectory design. am_traj - Alternating Minimization Based Trajectory Generation for Quadrotor Aggressive Flight. GraphBasedLocalTrajectoryPlanner - Was used on a real race vehicle during the Roborace Season Alpha and achieved speeds above 200km/h. User Interaction Graphical User Interface imgui - Designed to enable fast iterations and to empower programmers to create content creation tools and visualization / debug tools. qtpy - Provides an uniform layer to support PyQt5, PySide2, PyQt4 and PySide with a single codebase. mir - Mir is set of libraries for building Wayland based shells. rqt - A Qt-based framework for GUI development for ROS. It consists of three parts/metapackages. cage - This is Cage, a Wayland kiosk. A kiosk runs a single, maximized application. chilipie - Easy-to-use Raspberry Pi image for booting directly into full-screen Chrome. pencil - A tool for making diagrams and GUI prototyping that everyone can use. dynamic_reconfigure - The focus of dynamic_reconfigure is on providing a standard way to expose a subset of a node's parameters to external reconfiguration. ddynamic_reconfigure - Allows modifying parameters of a ROS node using the dynamic_reconfigure framework without having to write cfg files. elements - A lightweight, fine-grained, resolution independent, modular GUI library. NanoGUI - A minimalistic cross-platform widget library for OpenGL 3.x or higher. Acoustic User Interface pyo - A Python module written in C containing classes for a wide variety of audio signal processing types. rhasspy - Rhasspy (pronounced RAH-SPEE) is an offline, multilingual voice assistant toolkit inspired by Jasper that works well with Home Assistant, Hass.io, and Node-RED. mycroft-core - Mycroft is a hackable open source voice assistant. DDSP - A library of differentiable versions of common DSP functions (such as synthesizers, waveshapers, and filters). NoiseTorch - Creates a virtual microphone that suppresses noise, in any application. DeepSpeech - An open source Speech-To-Text engine, using a model trained by machine learning techniques based on Baidu's Deep Speech research paper. waveglow - A Flow-based Generative Network for Speech Synthesis. Command Line Interface the-art-of-command-line - Master the command line, in one page. dotfiles of cornerman - Powerful zsh and vim dotfiles. dotbot - A tool that bootstraps your dotfiles. prompt-hjem - A beautiful zsh prompt. ag - A code-searching tool similar to ack, but faster. fzf - A command-line fuzzy finder. pkgtop - Interactive package manager and resource monitor designed for the GNU/Linux. asciimatics - A cross platform package to do curses-like operations, plus higher level APIs and widgets to create text UIs and ASCII art animations. gocui - Minimalist Go package aimed at creating Console User Interfaces. TerminalImageViewer - Small C++ program to display images in a (modern) terminal using RGB ANSI codes and unicode block graphics characters. rosshow - Visualize ROS topics inside a terminal with Unicode/ASCII art. python-prompt-toolkit - Library for building powerful interactive command line applications in Python. guake - Drop-down terminal for GNOME. wemux - Multi-User Tmux Made Easy. tmuxp - A session manager built on libtmux. mapscii - World map renderer for your console. terminator - The goal of this project is to produce a useful tool for arranging terminals. bat - A cat(1) clone with wings. fx - Command-line tool and terminal JSON viewer. tmate - Instant terminal sharing. Data Visualization and Mission Control xdot - Interactive viewer for graphs written in Graphviz's dot language. guacamole - Clientless remote desktop gateway. It supports standard protocols like VNC, RDP, and SSH. ros3djs - 3D Visualization Library for use with the ROS JavaScript Libraries. webviz - Web-based visualization libraries like rviz. plotly.py - An open-source, interactive graphing library for Python. PlotJuggler - The timeseries visualization tool that you deserve. bokeh - Interactive Data Visualization in the browser, from Python. voila - From Jupyter notebooks to standalone web applications and dashboards. Pangolin - Pangolin is a lightweight portable rapid development library for managing OpenGL display / interaction and abstracting video input. rqt_bag - Provides a GUI plugin for displaying and replaying ROS bag files. kepler.gl - Kepler.gl is a powerful open source geospatial analysis tool for large-scale data sets. qgis_ros - Access bagged and live topic data in a highly featured GIS environment. openmct - A web based mission control framework. web_video_server - HTTP Streaming of ROS Image Topics in Multiple Formats. RVizWeb - Provides a convenient way of building and launching a web application with features similar to RViz. marvros - MAVLink to ROS gateway with proxy for Ground Control Station. octave - Provides a convenient command line interface for solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with Matlab. streetscape.gl - Streetscape.gl is a toolkit for visualizing autonomous and robotics data in the XVIZ protocol. urdf-loaders - URDF Loaders for Unity and THREE.js with example ATHLETE URDF File. obs-studio - Free and open source software for live streaming and screen recording. Annotation labelbox - The fastest way to annotate data to build and ship artificial intelligence applications. PixelAnnotationTool - Annotate quickly images. LabelImg - A graphical image annotation tool and label object bounding boxes in images. cvat - Powerful and efficient Computer Vision Annotation Tool (CVAT). point_labeler - Tool for labeling of a single point clouds or a stream of point clouds. label-studio - Label Studio is a multi-type data labeling and annotation tool with standardized output format. napari - A fast, interactive, multi-dimensional image viewer for python. semantic-segmentation-editor - A web based labeling tool for creating AI training data sets (2D and 3D). 3d-bat - 3D Bounding Box Annotation Tool for Point cloud and Image Labeling. labelme - Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation). universal-data-tool - Collaborate & label any type of data, images, text, or documents, in an easy web interface or desktop app. BMW-Labeltool-Lite - Provides you with a easy to use labeling tool for State-of-the-art Deep Learning training purposes. Point Cloud CloudCompare - CloudCompare is a 3D point cloud (and triangular mesh) processing software. Potree - WebGL point cloud viewer for large datasets. point_cloud_viewer - Makes viewing massive point clouds easy and convenient. LidarView - Performs real-time visualization and easy processing of live captured 3D LiDAR data from Lidar sensors. VeloView - Performs real-time visualization of live captured 3D LiDAR data from Velodyne's HDL sensors. entwine - A data organization library for massive point clouds, designed to conquer datasets of trillions of points as well as desktop-scale point clouds. polyscope - A C++ & Python viewer for 3D data like meshes and point clouds. Pcx - Point cloud importer & renderer for Unity. ImmersivePoints - A web-application for virtual reality devices to explore 3D data in the most natural way possible. RViz mapviz - Modular ROS visualization tool for 2D data. rviz_cinematographer - Easy to use tools to create and edit trajectories for the rviz camera. rviz_satellite - Display internet satellite imagery in RViz. rviz_visual_tools - C++ API wrapper for displaying shapes and meshes in Rviz. xpp - Visualization of motion-plans for legged robots. rviz stereo - 3D stereo rendering displays a different view to each eye so that the scene appears to have depth. jsk_visualization - Jsk visualization ros packages for rviz and rqt. moveit_visual_tools - Helper functions for displaying and debugging MoveIt! data in Rviz via published markers. Operation System Monitoring rosmon - ROS node launcher & monitoring daemon. multimaster_fkie - GUI-based management environment that is very useful to manage ROS-launch configurations and control running nodes. collectd - A small daemon which collects system information periodically and provides mechanisms to store and monitor the values in a variety of ways. lnav - An enhanced log file viewer that takes advantage of any semantic information that can be gleaned from the files being viewed, such as timestamps and log levels. htop - An interactive text-mode process viewer for Unix systems. It aims to be a better 'top'. atop - System and process monitor for Linux with logging and replay function. psutil - Cross-platform lib for process and system monitoring in Python. gputil - A Python module for getting the GPU status from NVIDA GPUs using nvidia-smi programmically in Python. gpustat - A simple command-line utility for querying and monitoring GPU status. nvtop - NVIDIA GPUs htop like monitoring tool. spdlog - Very fast, header-only/compiled, C++ logging library. ctop - Top-like interface for container metrics. ntop - Web-based Traffic and Security Network Traffic Monitoring. jupyterlab-nvdashboard - A JupyterLab extension for displaying dashboards of GPU usage. Database and Record ncdu - Ncdu is a disk usage analyzer with an ncurses interface. borg - Deduplicating archiver with compression and authenticated encryption. bag-database - A server that catalogs bag files and provides a web-based UI for accessing them. marv-robotics - MARV Robotics is a powerful and extensible data management platform. kitti2bag - Convert KITTI dataset to ROS bag file the easy way. pykitti - Python tools for working with KITTI data. rosbag_editor - Create a rosbag from a given one, using a simple GUI. nextcloud - Nextcloud is a suite of client-server software for creating and using file hosting services. ros_type_introspection - Deserialize ROS messages that are unknown at compilation time. syncthing - A continuous file synchronization program. rqt_bag_exporter - Qt GUI to export ROS bag topics to files (CSV and/or video). xviz - A protocol for real-time transfer and visualization of autonomy data. kitti_to_rosbag - A Dataset tools for working with the KITTI dataset raw data and converting it to a ROS bag. Also allows a library for direct access to poses, velodyne scans, and images. ros_numpy - Tools for converting ROS messages to and from numpy arrays. kitti_ros - A ROS-based player to replay KiTTI dataset. DuckDB - An embeddable SQL OLAP Database Management System. Network Distributed File System sshfs - File system based on the SSH File Transfer Protocol. moosefs - A scalable distributed storage system. ceph - A distributed object, block, and file storage platform. nfs - A distributed file system protocol originally developed by Sun Microsystems. ansible-role-nfs - Installs NFS utilities on RedHat/CentOS or Debian/Ubuntu. Server Infrastructure and High Performance Computing mass - Self-service, remote installation of Windows, CentOS, ESXi and Ubuntu on real servers turns your data centre into a bare metal cloud. polyaxon - A platform for reproducing and managing the whole life cycle of machine learning and deep learning applications. localstack - A fully functional local AWS cloud stack. Develop and test your cloud & Serverless apps offline. nvidia-docker - Build and run Docker containers leveraging NVIDIA GPUs. kubeflow - Machine Learning Toolkit for Kubernetes. log-pilot - Collect logs for docker containers. traefik - The Cloud Native Edge Router. graylog2-server - Free and open source log management. ansible - Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy. pyinfra - It can be used for ad-hoc command execution, service deployment, configuration management and more. docker-py - A Python library for the Docker Engine API. noVNC - VNC client using HTML5. Slurm - Slurm: A Highly Scalable Workload Manager. jupyterhub - Multi-user server for Jupyter notebooks. Portainer - Making Docker management easy. enroot - A simple, yet powerful tool to turn traditional container/OS images into unprivileged sandboxes. docker-firefox - Run a Docker Container with Firefox and noVNC for remote access to headless servers. luigi - A Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in. triton-inference-server - NVIDIA Triton Inference Server provides a cloud inferencing solution optimized for NVIDIA GPUs. cudf - Provides a pandas-like API that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming. Embedded Operation System vxworks7-ros2-build - Build system to automate the build of VxWorks 7 and ROS2. Yocto - Produce tools and processes that enable the creation of Linux distributions for embedded software that are independent of the underlying architecture of the embedded hardware. Automotive Graded Linux - A collaborative open source project that is bringing together automakers, suppliers and technology companies to build a Linux-based, open software platform for automotive applications that can serve as the de facto industry standard. bitbake - A generic task execution engine that allows shell and Python tasks to be run efficiently and in parallel while working within complex inter-task dependency constraints. Jailhouse - Jailhouse is a partitioning Hypervisor based on Linux. Xen - An open-source (GPL) type-1 or baremetal hypervisor. QEMU - A generic and open source machine emulator and virtualizer. qemu-xilinx - A fork of Quick EMUlator (QEMU) with improved support and modelling for the Xilinx platforms. rosserial - A ROS client library for small, embedded devices, such as Arduino. meta-ros - OpenEmbedded Layer for ROS Applications. meta-balena - Run Docker containers on embedded devices. micro-ros - The major changes compared to \"regular\" ROS 2 is that micro-ROS uses a Real-Time Operating System (RTOS) instead of Linux, and DDS for eXtremely Resource Constrained Environments. nvidia-container-runtime - NVIDIA Container Runtime is a GPU aware container runtime, compatible with the Open Containers Initiative (OCI) specification used by Docker, CRI-O, and other popular container technologie. fusesoc - Package manager and build abstraction tool for FPGA/ASIC development. jetson_easy - Automatically script to setup and configure your NVIDIA Jetson. docker-jetpack-sdk - Allows for usage of the NVIDIA JetPack SDK within a docker container for download, flashing, and install. Pressed - Provides a way to set answers to questions asked during the installation process of debian, without having to manually enter the answers while the installation is running. jetson_stats - A package to monitoring and control your NVIDIA Jetson (Xavier NX, Nano, AGX Xavier, TX1, TX2) Works with all NVIDIA Jetson ecosystem. ros_jetson_stats - The ROS jetson-stats wrapper. The status of your NVIDIA jetson in diagnostic messages. OpenCR - Open-source Control Module for ROS. acrn-hypervisor - Defines a device hypervisor reference stack and an architecture for running multiple software subsystems, managed securely, on a consolidated system by means of a virtual machine manager. jetson-containers - Machine Learning Containers for Jetson and JetPack 4.4. Real-Time Kernel ELISA - Project is to make it easier for companies to build and certify Linux-based safety-critical applications systems whose failure could result in loss of human life, significant property damage or environmental damage. PREEMPT_RT kernel patch - Aim of the PREEMPT_RT kernel patch is to minimize the amount of kernel code that is non-preemptible. Network and Middleware performance_test - Tool to test the performance of pub/sub based communication frameworks. realtime_support - Minimal real-time testing utility for measuring jitter and latency. ros1_bridge - ROS 2 package that provides bidirectional communication between ROS 1 and ROS 2. Fast-RTPS - A Protocol, which provides publisher-subscriber communications over unreliable transports such as UDP, as defined and maintained by the Object Management Group (OMG) consortium. protobuf - Google's data interchange format. opensplice - Vortex OpenSplice Community Edition. cyclonedds - Eclipse Cyclone DDS is a very performant and robust open-source DDS implementation. iceoryx - An IPC middleware for POSIX-based systems. rosbridge_suite - Provides a JSON interface to ROS, allowing any client to send JSON to publish or subscribe to ROS topics, call ROS services, and more. ros2arduino - This library helps the Arduino board communicate with the ROS2 using XRCE-DDS. eCAL - The enhanced communication abstraction layer (eCAL) is a middleware that enables scalable, high performance interprocess communication on a single computer node or between different nodes in a computer network. AUTOSAR-Adaptive - The implementation of AUTOSAR Adaptive Platform based on the R19-11. ocpp - The Open Charge Point Protocol (OCPP) is a network protocol for communication between electric vehicle chargers and a central backoffice system. Ethernet and Wireless Networking SOES - SOES is an EtherCAT slave stack written in C. netplan - Simply create a YAML description of the required network interfaces and what each should be configured to do. airalab - AIRA is reference Robonomics network client for ROS-enabled cyber-physical systems. rdbox - RDBOX is a IT infrastructure for ROS robots. ros_ethercat - This is a reimplementation of the main loop of pr2_ethercat without dependencies on PR2 software. wavemon - An ncurses-based monitoring application for wireless network devices. wireless - Making info about wireless networks available to ROS. ptpd - PTP daemon (PTPd) is an implementation the Precision Time Protocol (PTP) version 2 as defined by 'IEEE Std 1588-2008'. PTP provides precise time coordination of Ethernet LAN connected computers. iperf - A TCP, UDP, and SCTP network bandwidth measurement tool. tcpreplay - Pcap editing and replay tools. nethogs - It groups bandwidth by process. pyshark - Python wrapper for tshark, allowing python packet parsing using wireshark dissectors. pingtop - Ping multiple servers and show results in a top-like terminal UI. termshark - A terminal UI for tshark, inspired by Wireshark. udpreplay - Replay UDP packets from a pcap file. openwifi - Linux mac80211 compatible full-stack IEEE802.11/Wi-Fi design based on Software Defined Radio. Controller Area Network AndrOBD - Android OBD diagnostics with any ELM327 adapter. ddt4all - DDT4All is a tool to create your own ECU parameters screens and connect to a CAN network with a cheap ELM327 interface. cabana - CAN visualizer and DBC maker. opendbc - The project to democratize access to the decoder ring of your car. libuavcan - An open lightweight protocol designed for reliable communication in aerospace and robotic applications over robust vehicular networks such as CAN bus. python-can - The can package provides controller area network support for Python developers. CANopenNode - The internationally standardized (EN 50325-4) (CiA301) CAN-based higher-layer protocol for embedded control system. python-udsoncan - Python implementation of UDS (ISO-14229) standard. uds-c - Unified Diagnostics Service (UDS) and OBD-II (On Board Diagnostics for Vehicles) C Library. cantools - CAN BUS tools in Python 3. CANdevStudio - CANdevStudio aims to be cost-effective replacement for CAN simulation software. It can work with variety of CAN hardware interfaces. can-utils - Linux-CAN / SocketCAN user space applications. ros_canopen - CANopen driver framework for ROS. decanstructor - The definitive ROS CAN analysis tool. kvaser_interface - This package was developed as a standardized way to access Kvaser CAN devices from ROS. canmatrix - Converting CAN Database Formats .arxml .dbc .dbf .kcd. autosar - A set of python modules for working with AUTOSAR XML files. canopen - A Python implementation of the CANopen standard. The aim of the project is to support the most common parts of the CiA 301 standard in a Pythonic interface. SavvyCAN - A Qt5 based cross platform tool which can be used to load, save, and capture canbus frames. Open-Vehicle-Monitoring-System-3 - The system provides live monitoring of vehicle metrics like state of charge, temperatures, tyre pressures and diagnostic fault conditions. Sensor and Acuator Interfaces Tesla-API - Provides functionality to monitor and control the Model S (and future Tesla vehicles) remotely. flirpy - A Python library to interact with FLIR thermal imaging cameras and images. nerian_stereo - ROS node for Nerian's SceneScan and SP1 stereo vision sensors. pymmw - This is a toolbox composed of Python scripts to interact with TI's evaluation module (BoosterPack) for the IWR1443 mmWave sensing device. ti_mmwave_rospkg - TI mmWave radar ROS driver (with sensor fusion and hybrid). pacmod3 - This ROS node is designed to allow the user to control a vehicle with the PACMod drive-by-wire system, board revision 3. ros2_intel_realsense - These are packages for using Intel RealSense cameras (D400 series) with ROS2. sick_scan - This stack provides a ROS2 driver for the SICK TiM series of laser scanners. ouster_example - Sample code for connecting to and configuring the OS1, reading and visualizing data, and interfacing with ROS. ros2_ouster_drivers - These are an implementation of ROS2 drivers for the Ouster OS-1 3D lidars. livox_ros_driver - A new ROS package, specially used to connect LiDAR products produced by Livox. velodyne - A collection of ROS packages supporting Velodyne high definition 3D LIDARs. ublox - Provides support for u-blox GPS receivers. crazyflie_ros - ROS Driver for Bitcraze Crazyflie. pointgrey_camera_driver - ROS driver for Pt. Grey cameras, based on the official FlyCapture2 SDK. novatel_gps_driver - ROS driver for NovAtel GPS / GNSS receivers. pylon-ros-camera - The official pylon ROS driver for Basler GigE Vision and USB3 Vision cameras. ethz_piksi_ros - Contains (python) ROS drivers, tools, launch files, and wikis about how to use Piksi Real Time Kinematic (RTK) GPS device in ROS. sick_safetyscanners - A ROS Driver which reads the raw data from the SICK Safety Scanners and publishes the data as a laser_scan msg. bosch_imu_driver - A driver for the sensor IMU Bosch BNO055. It was implemented only the UART communication interface (correct sensor mode should be selected). oxford_gps_eth - Ethernet interface to OxTS GPS receivers using the NCOM packet structure. ifm3d - Library and Utilities for working with ifm pmd-based 3D ToF Cameras. cepton_sdk_redist - Provides ROS support for Cepton LiDAR. jetson_csi_cam - A ROS package making it simple to use CSI cameras on the Nvidia Jetson TK1, TX1, or TX2 with ROS. ros_astra_camera - A ROS driver for Orbbec 3D cameras. spot_ros - ROS Driver for Spot. Security owasp-threat-dragon-desktop - Threat Dragon is a free, open-source, cross-platform threat modeling application including system diagramming and a rule engine to auto-generate threats/mitigations. launch_ros_sandbox - Can define launch files running nodes in restrained environments, such as Docker containers or separate user accounts with limited privileges. wolfssl - A small, fast, portable implementation of TLS/SSL for embedded devices to the cloud. CANalyzat0r - Security analysis toolkit for proprietary car protocols. RSF - Robot Security Framework (RSF) is a standardized methodology to perform security assessments in robotics. How-to-Secure-A-Linux-Server - An evolving how-to guide for securing a Linux server. lynis - Security auditing tool for Linux, macOS, and UNIX-based systems. Assists with compliance testing (HIPAA/ISO27001/PCI DSS) and system hardening. OpenVPN - An open source VPN daemon. openfortivpn - A client for PPP+SSL VPN tunnel services and compatible with Fortinet VPNs. WireGuard - WireGuard is a novel VPN that runs inside the Linux Kernel and utilizes state-of-the-art cryptography. ssh-auditor - Scans for weak ssh passwords on your network. vulscan - Advanced vulnerability scanning with Nmap NSE. nmap-vulners - NSE script based on Vulners.com API. brutespray - Automatically attempts default creds on found services. fail2ban - Daemon to ban hosts that cause multiple authentication errors. DependencyCheck - A software composition analysis utility that detects publicly disclosed vulnerabilities in application dependencies. Firejail - A SUID sandbox program that reduces the risk of security breaches by restricting the running environment of untrusted applications using Linux namespaces, seccomp-bpf and Linux capabilities. RVD - Robot Vulnerability Database. Community-contributed archive of robot vulnerabilities and weaknesses. ros2_dds_security - Adding security enhancements by defining a Service Plugin Interface (SPI) architecture, a set of builtin implementations of the SPIs, and the security model enforced by the SPIs. Security-Enhanced Linux - A Linux kernel security module that provides a mechanism for supporting access control security policies, including mandatory access controls (MAC). OpenTitan - Will make the silicon Root of Trust design and implementation more transparent, trustworthy, and secure for enterprises, platform providers, and chip manufacturers. OpenTitan is administered by lowRISC CIC as a collaborative project to produce high quality, open IP for instantiation as a full-featured product. bandit - A tool designed to find common security issues in Python code. hardening - A quick way to make a Ubuntu server a bit more secure. Passbolt - Passbolt is a free and open source password manager that allows team members to store and share credentials securely. gopass - A password manager for the command line written in Go. pass - The standard unix password manager. Vault - A tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, certificates, and more. legion - An open source, easy-to-use, super-extensible and semi-automated network penetration testing framework that aids in discovery, reconnaissance and exploitation of information systems. openscap - The oscap program is a command line tool that allows users to load, scan, validate, edit, and export SCAP documents. Datasets KITTI-360 - This large-scale dataset contains 320k images and 100k laser scans in a driving distance of 73.7km. waymo_ros - This is a ROS package to connect Waymo open dataset to ROS. waymo-open-dataset - The Waymo Open Dataset is comprised of high-resolution sensor data collected by Waymo self-driving cars in a wide variety of conditions. Ford Autonomous Vehicle Dataset - Ford presents a challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times. awesome-robotics-datasets - A collection of useful datasets for robotics and computer vision. nuscenes-devkit - The devkit of the nuScenes dataset. dataset-api - This is a repo of toolkit for ApolloScape Dataset, CVPR 2019 Workshop on Autonomous Driving Challenge and ECCV 2018 challenge. utbm_robocar_dataset - EU Long-term Dataset with Multiple Sensors for Autonomous Driving. DBNet - A Large-Scale Dataset for Driving Behavior Learning. argoverse-api - Official GitHub repository for Argoverse dataset. DDAD - A new autonomous driving benchmark from TRI (Toyota Research Institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions. pandaset-devkit - Public large-scale dataset for autonomous driving provided by Hesai & Scale. a2d2_to_ros - Utilities for converting A2D2 data sets to ROS bags. awesome-satellite-imagery-datasets - List of satellite image training datasets with annotations for computer vision and deep learning. sentinelsat - Search and download Copernicus Sentinel satellite images. adas-dataset-form - Thermal Dataset for Algorithm Training. h3d - The H3D is a large scale full-surround 3D multi-object detection and tracking dataset from Honda. Mapillary Vistas Dataset - A diverse street-level imagery dataset with pixelaccurate and instancespecific human annotations for understanding street scenes around the world. TensorFlow Datasets - TensorFlow Datasets provides many public datasets as tf.data.Datasets. racetrack-database - Contains center lines (x- and y-coordinates), track widths and race lines for over 20 race tracks (mainly F1 and DTM) all over the world. BlenderProc - A procedural Blender pipeline for photorealistic training image generation. Atlatec Sample Map Data - 3D map for autonomous driving and simulation created from nothing but two cameras and GPS in downtown San Francisco. Lyft Level 5 Dataset - Level 5 is developing a self-driving system for the Lyft network. We're collecting and processing data from our autonomous fleet and sharing it with you. holicity - A City-Scale Data Platform for Learning Holistic 3D Structures. UTD19 - Largest multi-city traffic dataset publically available. ASTYX HIRES2019 DATASET - Automotive Radar Dataset for Deep Learning Based 3D Object Detection. Objectron - A collection of short, object-centric video clips, which are accompanied by AR session metadata that includes camera poses, sparse point-clouds and characterization of the planar surfaces in the surrounding environment. ",
          "If you want to really enjoy the ingenuity of people that otherwise might be considered \"simple\", check out the ROS for Agriculture group and slack channels.  People converting 70s and 80s era farm equipment to be automated, and it's pure gold.",
          "ROS is really great for getting started with robotics, but from my experience (and from others who I've talked to) tends to buckle under its own weight in larger projects. This isn't to say you shouldn't use it, just be careful with it. It has some really questionable design decisions and implementation details that can bite you later."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/Ly0n/awesome-robotic-tooling",
        "comments.comment_id": [21437583, 21438738],
        "comments.comment_author": ["heyflyguy", "a_t48"],
        "comments.comment_descendants": [3, 4],
        "comments.comment_time": [
          "2019-11-03T23:40:59Z",
          "2019-11-04T03:16:05Z"
        ],
        "comments.comment_text": [
          "If you want to really enjoy the ingenuity of people that otherwise might be considered \"simple\", check out the ROS for Agriculture group and slack channels.  People converting 70s and 80s era farm equipment to be automated, and it's pure gold.",
          "ROS is really great for getting started with robotics, but from my experience (and from others who I've talked to) tends to buckle under its own weight in larger projects. This isn't to say you shouldn't use it, just be careful with it. It has some really questionable design decisions and implementation details that can bite you later."
        ],
        "id": "fd84a63b-dc1f-4340-af88-238099d9b73e",
        "url_text": "Awesome Robotic Tooling Robotic resources and tools for professional development in C++ or Python with a touch of ROS, autonomous driving and aerospace. To stop reinventing the wheel you need to know about the wheel. This list is an attempt to show the variety of open and free tools in software and hardware development, which are useful in professional robotic development. Since the development processes are of crucial importance for the approval of such systems, the interaction of development processes and tools plays a central role. Your contribution is necessary to keep this list alive, increase the quality and to expand it. You can read more about it's origin and how you can participate in the contribution guide and related blog post. Contents Communication and Coordination Documentation and Presentation Requirements and Safety Architecture and Design Frameworks and Stacks Development Environment Code and Run Template Build and Deploy Unit and Integration Test Lint and Format Debugging and Tracing Version Control Simulation Electronics and Mechanics Sensor Processing Calibration and Transformation Perception Pipeline Machine Learning Parallel Processing Image Processing Radar Processing Lidar and Point Cloud Processing Localization and State Estimation Simultaneous Localization and Mapping Lidar Visual Vector Map Prediction Behavior and Decision Planning and Control User Interaction Graphical User Interface Acoustic User Interface Command Line Interface Data Visualization and Mission Control Annotation Point Cloud RViz Operation System Monitoring Database and Record Network Distributed File System Server Infrastructure and High Performance Computing Embedded Operation System Real-Time Kernel Network and Middleware Ethernet and Wireless Networking Controller Area Network Sensor and Acuator Interfaces Security Datasets Communication and Coordination Agile Development - Manifesto for Agile Software Development. Gitflow - Makes parallel development very easy, by isolating new development from finished work. DeepL - An online translator that outperforms Google, Microsoft and Facebook. Taiga - Agile Projectmanagment Tool. Kanboard - Minimalistic Kanban Board. kanban - Free, open source, self-hosted, Kanban board for GitLab issues. Gitlab - Simple Selfhosted Gitlab Server with Docker. Gogs - Build a simple, stable and extensible self-hosted Git service that can be setup in the most painless way. Wekan - Meteor based Kanban Board. JIRA API - Python Library for REST API of Jira. Taiga API - Python Library for REST API of Taiga. Chronos-Timetracker - Desktop client for JIRA. Track time, upload worklogs without a hassle. Grge - Grge is a daemon and command line utility augmenting GitLab. gitlab-triage - Gitlab's issues and merge requests triage, automated. Helpy - A modern, open source helpdesk customer support application. ONLYOFFICE - A free open source collaborative system developed to manage documents, projects, customer relationship and email correspondence, all in one place. discourse - A platform for community discussion. Free, open, simple. Gerrit - A code review and project management tool for Git based projects. jitsi-meet - Secure, Simple and Scalable Video Conferences that you use as a standalone app or embed in your web application. mattermost - An open source, private cloud, Slack-alternative. openproject - The leading open source project management software. leantime - Leantime is a lean project management system for innovators. gitter - Gitter is a chat and networking platform that helps to manage, grow and connect communities through messaging, content and discovery. Documentation and Presentation Typora - A Minimalist Markdown Editor. Markor - A Simple Markdown Editor for your Android Device. Pandoc - Universal markup converter. Yaspeller - Command line tool for spell checking. ReadtheDocs - Build your local ReadtheDocs Server. Doxygen - Doxygen is the de facto standard tool for generating documentation from annotated C++ sources. Sphinx - A tool that makes it easy to create intelligent and beautiful documentation for Python projects. Word-to-Markdown - A ruby gem to liberate content from Microsoft Word document. paperless - Index and archive all of your scanned paper documents. carbon - Share beautiful images of your source code. undraw - Free Professional business SVGs easy to customize. asciinema - Lets you easily record terminal sessions and replay them in a terminal as well as in a web browser. inkscape - Inkscape is a professional vector graphics editor for Linux, Windows and macOS. Reveal-Hugo - A Hugo theme for Reveal.js that makes authoring and customization a breeze. With it, you can turn any properly-formatted Hugo content into a HTML presentation. Hugo-Webslides - This is a Hugo template to create WebSlides presentation using markdown. jupyter2slides - Cloud Native Presentation Slides with Jupyter Notebook + Reveal.js. patat - Terminal-based presentations using Pandoc. github-changelog-generator - Automatically generate change log from your tags, issues, labels and pull requests on GitHub. GitLab-Release-Note-Generator - A Gitlab release note generator that generates release note on latest tag. OCRmyPDF - Adds an OCR text layer to scanned PDF files, allowing them to be searched. papermill - A tool for parameterizing, executing, and analyzing Jupyter Notebooks. docsy - An example documentation site using the Docsy Hugo theme. actions-hugo - Deploy website based on Hugo to GitHub Pages. overleaf - An open-source online real-time collaborative LaTeX editor. landslide - Generate HTML5 slideshows from markdown, ReST, or textile. libreoffice-impress-templates - Freely-licensed LibreOffice Impress templates. opensourcedesign - Community and Resources for Free Design and Logo Creation. olive - A free non-linear video editor aiming to provide a fully-featured alternative to high-end professional video editing software. buku - Browser-independent bookmark manager. swiftlatex - A WYSIWYG Browser-based LaTeX Editor. ReLaXed - Allows complex PDF layouts to be defined with CSS and JavaScript, while writing the content in a friendly, minimal syntax close to Markdown or LaTeX. foam - Foam is a personal knowledge management and sharing system inspired by Roam Research, built on Visual Studio Code and GitHub. CodiMD - Open Source Online Real-time collaborate on team documentation in markdown. jupyter-book - Build interactive, publication-quality documents from Jupyter Notebooks. InvoiceNet - Deep neural network to extract intelligent information from invoice documents. tesseract - Open Source OCR Engine. Requirements and Safety awesome-safety-critical - List of resources about programming practices for writing safety-critical software. open-autonomous-safety - OAS is a fully open-source library of Voyage's safety processes and testing procedures, designed to supplement existing safety programs at self-driving car startups across the world. CarND-Functional-Safety-Project - Create functional safety documents in this Udacity project. Automated Valet Parking Safety Documents - Created to support the safe testing of the Automated Valet Parking function using the StreetDrone test vehicle in a car park. safe_numerics - Replacements to standard numeric types which throw exceptions on errors. Air Vehicle C++ development coding standards - Provide direction and guidance to C++ programmers that will enable them to employ good programming style and proven programming practices leading to safe, reliable, testable, and maintainable code. AUTOSAR Coding Standard - Guidelines for the use of the C++14 language in critical and safety-related system. The W-Model and Lean Scaled Agility for Engineering - Ford applied an agile V-Model method from Vector that can be used in safety related project management. doorstop - Requirements management using version control. capella - Comprehensive, extensible and field-proven MBSE tool and method to successfully design systems architecture. robmosys - RobMoSys envisions an integrated approach built on top of the current code-centric robotic platforms, by applying model-driven methods and tools. Papyrus for Robotics - A graphical editing tool for robotic applications that complies with the RobMoSys approach. fossology - A toolkit you can run license, copyright and export control scans from the command line. ScenarioArchitect - The Scenario Architect is a basic python tool to generate, import and export short scene snapshots. Architecture and Design Guidelines - How to architect ROS-based systems. yEd - A powerful desktop application that can be used to quickly and effectively generate high-quality diagrams. yed_py - Generates graphML that can be opened in yEd. Plantuml - Web application to generate UML diagrams on-the-fly in your live documentation. rqt_graph - Provides a GUI plugin for visualizing the ROS computation graph. rqt_launchtree - An RQT plugin for hierarchical launchfile configuration introspection. cpp-dependencies - Tool to check C++ #include dependencies (dependency graphs created in .dot format). pydeps - Python Module Dependency graphs. aztarna - A footprinting tool for robots. draw.io - A free online diagram software for making flowcharts, process diagrams, org charts, UML, ER and network diagrams. vscode-drawio - This extension integrates Draw.io into VS Code. Frameworks and Stacks ROS - (Robot Operating System) provides libraries and tools to help software developers create robot applications. awesome-ros2 - A curated list of awesome Robot Operating System Version 2.0 (ROS 2) resources and libraries. Autoware.Auto - Autoware.Auto applies best-in-class software engineering for autonomous driving. Autoware.ai - Autoware.AI is the world's first \"All-in-One\" open-source software for autonomous driving technology. OpenPilot - Open Source Adaptive Cruise Control (ACC) and Lane Keeping Assist System (LKAS). Apollo - High performance, flexible architecture which accelerates the development, testing, and deployment of Autonomous Vehicles. PythonRobotics - This is a Python code collection of robotics algorithms, especially for autonomous navigation. Stanford Self Driving Car Code - Stanford Code From Cars That Entered DARPA Grand Challenges. astrobee - Astrobee is a free-flying robot designed to operate as a payload inside the International Space Station (ISS). CARMAPlatform - Enables cooperative automated driving plug-in. Automotive Grade Linux - Automotive Grade Linux is a collaborative open source project that is bringing together automakers, suppliers and technology companies to accelerate the development and adoption of a fully open software stack for the connected car. PX4 - An open source flight control software for drones and other unmanned vehicles. KubOS - An open-source software stack for satellites. mod_vehicle_dynamics_control - TUM Roborace Team Software Stack - Path tracking control, velocity control, curvature control and state estimation. Aslan - Open source self-driving software for low speed environments. open-source-rover - A build-it-yourself, 6-wheel rover based on the rovers on Mars from JPL. pybotics - An open-source and peer-reviewed Python toolbox for robot kinematics and calibration. makani - Contains the working Makani flight simulator, controller (autopilot), visualizer, and command center flight monitoring tools. mir_robot - This is a community project to use the MiR Robots with ROS. Development Environment Code and Run Vim-ros - Vim plugin for ROS development. Visual Studio Code - Code editor for edit-build-debug cycle. atom - Hackable text editor for the 21st century. Teletype - Share your workspace with team members and collaborate on code in real time in Atom. Sublime - A sophisticated text editor for code, markup and prose. ade-cli - The ADE Development Environment (ADE) uses docker and Gitlab to manage environments of per project development tools and optional volume images. recipe-wizard - A Dockerfile generator for running OpenGL (GLX) applications with nvidia-docker2, CUDA, ROS, and Gazebo on a remote headless server system. Jupyter ROS - Jupyter widget helpers for ROS, the Robot Operating System. ros_rqt_plugin - The ROS Qt Creator Plug-in for Python. xeus-cling - Jupyter kernel for the C++ programming language. ROS IDEs - This page collects experience and advice on using integrated development environments (IDEs) with ROS. TabNine - The all-language autocompleter. kite - Use machine learning to give you useful code completions for Python. jedi - Autocompletion and static analysis library for python. roslibpy - Python ROS Bridge library allows to use Python and IronPython to interact with ROS, the open-source robotic middleware. pybind11 - Seamless operability between C++11 and Python. Sourcetrail - Free and open-source cross-platform source explorer. rebound - Command-line tool that instantly fetches Stack Overflow results when an exception is thrown. mybinder - Open notebooks in an executable environment, making your code immediately reproducible by anyone, anywhere. ROSOnWindows - An experimental release of ROS1 for Windows. live-share - Real-time collaborative development from the comfort of your favorite tools. cocalc - Collaborative Calculation in the Cloud. EasyClangComplete - Robust C/C++ code completion for Sublime Text 3. vscode-ros - Visual Studio Code extension for Robot Operating System (ROS) development. awesome-hpp - A curated list of awesome header-only C++ libraries. Template ROS - Template for ROS node standardization in C++. Launch - Templates on how to create launch files for larger projects. Bash - A bash scripting template incorporating best practices & several useful functions. URDF - Examples on how to create Unified Robot Description Format (URDF) for different kinds of robots. Python - Style guide to be followed in writing Python code for ROS. Docker - The Dockerfile in the minimal-ade project shows a minimal example of how to create a custom base image. VS Code ROS2 Workspace Template - Template for using VSCode as an IDE for ROS2 development. Build and Deploy qemu-user-static - Enable an execution of different multi-architecture containers by QEMU and binfmt_misc. Cross compile ROS 2 on QNX - Introduces how to cross compile ROS 2 on QNX. bloom - A release automation tool which makes releasing catkin packages easier. superflore - An extended platform release manager for Robot Operating System. catkin_tools - Command line tools for working with catkin. industrial_ci - Easy continuous integration repository for ROS repositories. ros_gitlab_ci - Contains helper scripts and instructions on how to use Continuous Integration (CI) for ROS projects hosted on a GitLab instance. gitlab-runner - Runs tests and sends the results to GitLab. colcon-core - Command line tool to improve the workflow of building, testing and using multiple software packages. gitlab-release - Simple python3 script to upload files (from ci) to the current projects release (tag). clang - This is a compiler front-end for the C family of languages (C, C++, Objective-C, and Objective-C++) which is built as part of the LLVM compiler infrastructure project. catkin_virtualenv - Bundle python requirements in a catkin package via virtualenv. pyenv - Simple Python version management. aptly - Debian repository management tool. cross_compile - Assets used for ROS2 cross-compilation. docker_images - Official Docker images maintained by OSRF on ROS(2) and Gazebo. robot_upstart - Presents a suite of scripts to assist with launching background ROS processes on Ubuntu Linux PCs. robot_systemd - Units for managing startup and shutdown of roscore and roslaunch. ryo-iso - A modern ISO builder that streamlines the process of deploying a complete robot operating system from a yaml config file. network_autoconfig - Automatic configuration of ROS networking for most use cases without impacting usage that require manual configuration. rosbuild - The ROS build farm. cros - A single thread pure C implementation of the ROS framework. Unit and Integration Test setup-ros - This action sets up a ROS and ROS 2 environment for use in GitHub actions. UnitTesting - This page lays out the rationale, best practices, and policies for writing and running unit tests and integration tests for ROS. googletest - Google's C++ test framework. pytest - The pytest framework makes it easy to write small tests, yet scales to support complex functional testing. doctest - The fastest feature-rich C++11/14/17/20 single-header testing framework for unit tests and TDD. osrf_testing_tools_cpp - Contains testing tools for C++, and is used in OSRF projects. code_coverage - ROS package to run coverage testing. action-ros-ci - GitHub Action to build and test ROS 2 packages using colcon. Lint and Format action-ros-lint - GitHub action to run linters on ROS 2 packages. cppcheck - Static analysis of C/C++ code. hadolint - Dockerfile linter, validate inline bash, written in Haskell. shellcheck - A static analysis tool for shell scripts. catkin_lint - Checks package configurations for the catkin build system of ROS. pylint - Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. black - The uncompromising Python code formatter. pydocstyle - A static analysis tool for checking compliance with Python docstring conventions. haros - Static analysis of ROS application code. pydantic - Data parsing and validation using Python type hints. Debugging and Tracing heaptrack - Traces all memory allocations and annotates these events with stack traces. ros2_tracing - Tracing tools for ROS 2. Linuxperf - Various Linux performance material. lptrace - It lets you see in real-time what functions a Python program is running. pyre-check - Performant type-checking for python. FlameGraph - Visualize profiled code. gpuvis - GPU Trace Visualizer. sanitizer - AddressSanitizer, ThreadSanitizer, MemorySanitizer. cppinsights - C++ Insights - See your source code with the eyes of a compiler. inspect - The inspect module provides functions for learning about live objects, including modules, classes, instances, functions, and methods. Roslaunch Nodes in Valgrind or GDB - When debugging roscpp nodes that you are launching with roslaunch, you may wish to launch the node in a debugging program like gdb or valgrind instead. pyperformance - Python Performance Benchmark Suite. qira - QIRA is a competitor to strace and gdb. gdb-frontend - GDBFrontend is an easy, flexible and extensionable gui debugger. lttng - An open source software toolkit which you can use to simultaneously trace the Linux kernel, user applications, and user libraries. ros2-performance - Allows to easily create arbitrary ROS2 systems and then measures their performance. bcc - Tools for BPF-based Linux IO analysis, networking, monitoring, and more. tracy - A real time, nanosecond resolution, remote telemetry frame profiler for games and other applications. bpftrace - High-level tracing language for Linux eBPF. pudb - Full-screen console debugger for Python. backward-cpp - A beautiful stack trace pretty printer for C++. gdb-dashboard - GDB dashboard is a standalone .gdbinit file written using the Python API that enables a modular interface showing relevant information about the program being debugged. hotspot - The Linux perf GUI for performance analysis. memory_profiler - A python module for monitoring memory consumption of a process as well as line-by-line analysis of memory consumption for python programs. ros1_fuzzer - This fuzzer aims to help developers and researchers to find bugs and vulnerabilities in ROS nodes by performing fuzz tests over topics that the target nodes process. vscode-debug-visualizer - An extension for VS Code that visualizes data during debugging. action-tmate - Debug your GitHub Actions via SSH by using tmate to get access to the runner system itself. libstatistics_collector - ROS 2 library providing classes to collect measurements and calculate statistics across them. system_metrics_collector - Lightweight, real-time system metrics collector for ROS2 systems. Version Control git-fuzzy - A CLI interface to git that relies heavily on fzf. meld - Meld is a visual diff and merge tool that helps you compare files, directories, and version controlled projects. tig - Text-mode interface for git. gitg - A graphical user interface for git. git-cola - The highly caffeinated Git GUI. python-gitlab - A Python package providing access to the GitLab server API. bfg-repo-cleaner - Removes large or troublesome blobs like git-filter-branch does, but faster. nbdime - Tools for diffing and merging of Jupyter notebooks. semantic-release - Fully automated version management and package publishing. go-semrel-gitab - Automate version management for Gitlab. Git-repo - Git-Repo helps manage many Git repositories, does the uploads to revision control systems, and automates parts of the development workflow. dive - A tool for exploring each layer in a docker image. dvc - Management and versioning of datasets and machine learning models. learnGitBranching - A git repository visualizer, sandbox, and a series of educational tutorials and challenges. gitfs - You can mount a remote repository's branch locally, and any subsequent changes made to the files will be automatically committed to the remote. git-secret - Encrypts files with permitted users' public keys, allowing users you trust to access encrypted data using pgp and their secret keys. git-sweep - A command-line tool that helps you clean up Git branches that have been merged into master. lazygit - A simple terminal UI for git commands, written in Go with the gocui library. glab - An open-source GitLab command line tool. Simulation Drake - Drake aims to simulate even very complex dynamics of robots. Webots - Webots is an open source robot simulator compatible (among others) with ROS and ROS2. lgsv - LG Electronics America R&D Center has developed an HDRP Unity-based multi-robot simulator for autonomous vehicle developers. carla - Open-source simulator for autonomous driving research. awesome-CARLA - A curated list of awesome CARLA tutorials, blogs, and related projects. ros-bridge - ROS bridge for CARLA Simulator. scenario_runner - Traffic scenario definition and execution engine. deepdive - End-to-end simulation for self-driving cars. uuv_simulator - Gazebo/ROS packages for underwater robotics simulation. AirSim - Open source simulator for autonomous vehicles built on Unreal Engine. self-driving-car-sim - A self-driving car simulator built with Unity. ROSIntegration - Unreal Engine Plugin to enable ROS Support. gym-gazebo - An OpenAI gym extension for using Gazebo known as gym-gazebo. highway-env - A collection of environments for autonomous driving and tactical decision-making tasks. VREP Interface - ROS Bridge for the VREP simulator. car_demo - This is a simulation of a Prius in gazebo 9 with sensor data being published using ROS kinetic. sumo - Eclipse SUMO is an open source, highly portable, microscopic and continuous road traffic simulation package designed to handle large road networks. open-simulation-interface - A generic interface for the environmental perception of automated driving functions in virtual scenarios. ESIM - An Open Event Camera Simulator. Menge - Crowd Simulation Framework. pedsim_ros - Pedestrian simulator powered by the social force model for Gazebo. opencrg - Open file formats and open source tools for the detailed description, creation and evaluation of road surfaces. esmini - A basic OpenSCENARIO player. OpenSceneGraph - An open source high performance 3D graphics toolkit, used by application developers in fields such as visual simulation, games, virtual reality, scientific visualization and modelling. morse - An academic robotic simulator, based on the Blender Game Engine and the Bullet Physics engine. ROSIntegrationVision - Support for ROS-enabled RGBD data acquisition in Unreal Engine Projects. fetch_gazebo - Contains the Gazebo simulation for Fetch Robotics Fetch and Freight Research Edition Robots. rotors_simulator - Provides some multirotor models. flow - A computational framework for deep RL and control experiments for traffic microsimulation. gnss-ins-sim - GNSS + inertial navigation, sensor fusion simulator. Motion trajectory generator, sensor models, and navigation. Ignition Robotics - Test control strategies in safety, and take advantage of simulation in continuous integration tests. simulation assets for the SubT - This collection contains simulation assets for the SubT Challenge Virtual Competition in Gazebo. gazebo_ros_motors - Contains currently two motor plugins for Gazebo, one with an ideal speed controller and one without a controller that models a DC motor. map2gazebo - ROS package for creating Gazebo environments from 2D maps. sim_vehicle_dynamics - Vehicle Dynamics Simulation Software of TUM Roborace Team. gym-carla - An OpenAI gym wrapper for CARLA simulator. simbody - High-performance C++ multibody dynamics/physics library for simulating articulated biomechanical and mechanical systems like vehicles, robots, and the human skeleton. gazebo_models - This repository holds the Gazebo model database. pylot - Autonomous driving platform running on the CARLA simulator. flightmare - Flightmare is composed of two main components: a configurable rendering engine built on Unity and a flexible physics engine for dynamics simulation. champ - ROS Packages for CHAMP Quadruped Controller. rex-gym - OpenAI Gym environments for an open-source quadruped robot (SpotMicro). Trick - Developed at the NASA Johnson Space Center, is a powerful simulation development framework that enables users to build applications for all phases of space vehicle development. usv_sim_lsa - Unmanned Surface Vehicle simulation on Gazebo with water current and winds. 42 - Simulation for spacecraft attitude control system analysis and design. Complete_Street_Rule - A scenario oriented design tool intended to enable users to quickly create procedurally generated multimodal streets in ArcGIS CityEngine. AutoCore simulation - Provides test environment for Autoware and still during early development, contents below may changed during updates. fields-ignition - Generate random crop fields for Ignition Gazebo. Electronics and Mechanics HRIM - An information model for robot hardware. URDF - Repository for Unified Robot Description Format (URDF) parsing code. phobos - An add-on for Blender allowing to create URDF, SDF and SMURF robot models in a WYSIWYG environment. urdf-viz - Visualize URDF/XACRO file, URDF Viewer works on Windows/macOS/Linux. solidworks_urdf_exporter - SolidWorks to URDF Exporter. FreeCAD - Your own 3D parametric modeler. kicad - A Cross Platform and Open Source Electronics Design Automation Suite. PcbDraw - Convert your KiCAD board into a nice looking 2D drawing suitable for pinout diagrams. kicad-3rd-party-tools - Tools made by others to augment the KiCad PCB EDA suite. PandaPower - An easy to use open source tool for power system modeling, analysis and optimization with a high degree of automation. LibrePCB - A powerful, innovative and intuitive EDA tool for everyone. openscad - A software for creating solid 3D CAD models. ngspice - A open source spice simulator for electric and electronic circuits. GNSS-SDR - GNSS-SDR provides interfaces for a wide range of radio frequency front-ends and raw sample file formats, generates processing outputs in standard formats. riscv - The Free and Open RISC Instruction Set Architecture. urdfpy - A simple and easy-to-use library for loading, manipulating, saving, and visualizing URDF files. FMPy - Simulate Functional Mockup Units (FMUs) in Python. FMIKit-Simulink - Import and export Functional Mock-up Units with Simulink. oemof-solph - A modular open source framework to model energy supply systems. NASA-3D-Resources - Here you'll find a growing collection of 3D models, textures, and images from inside NASA. SUAVE - An Aircraft Design Toolbox. opem - The Open-Source PEMFC Simulation Tool (OPEM) is a modeling tool for evaluating the performance of proton exchange membrane fuel cells. pvlib-python - A community supported tool that provides a set of functions and classes for simulating the performance of photovoltaic energy systems. WireViz - A tool for easily documenting cables, wiring harnesses and connector pinouts. Horizon - EDA is an Electronic Design Automation package supporting an integrated end-to-end workflow for printed circuit board design including parts management and schematic entry. tigl - The TiGL Geometry Library can be used for the computation and processing of aircraft geometries stored inside CPACS files. foxBMS - A free, open and flexible development environment to design battery management systems. cadCAD - A Python package that assists in the processes of designing, testing and validating complex systems through simulation, with support for Monte Carlo methods, A/B testing and parameter sweeping. OpenMDAO - An open-source framework for efficient multidisciplinary optimization. ODrive - The aim is to make it possible to use inexpensive brushless motors in high performance robotics projects. OpenTirePython - An open-source mathematical tire modelling library. Sensor Processing Calibration and Transformation tf2 - Transform library, which lets the user keep track of multiple coordinate frames over time. lidar_align - A simple method for finding the extrinsic calibration between a 3D lidar and a 6-dof pose sensor. kalibr - The Kalibr visual-inertial calibration toolbox. Calibnet - Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks. lidar_camera_calibration - ROS package to find a rigid-body transformation between a LiDAR and a camera. ILCC - Reflectance Intensity Assisted Automatic and Accurate Extrinsic Calibration of 3D LiDAR. easy_handeye - Simple, straighforward ROS library for hand-eye calibration. imu_utils - A ROS package tool to analyze the IMU performance. kalibr_allan - IMU Allan standard deviation charts for use with Kalibr and inertial kalman filters. pyquaternion - A full-featured Python module for representing and using quaternions. robot_calibration - This package offers calibration of a number of parameters of a robot, such as: 3D Camera intrinsics, extrinsics Joint angle offsets and robot frame offsets. multi_sensor_calibration - Contains a calibration tool to calibrate a sensor setup consisting of lidars, radars and cameras. LiDARTag - A Real-Time Fiducial Tag using Point Clouds Lidar Data. multicam_calibration - Extrinsic and intrinsic calbration of cameras. ikpy - An Inverse Kinematics library aiming performance and modularity. livox_camera_lidar_calibration - Calibrate the extrinsic parameters between Livox LiDAR and camera. lidar_camera_calibration - Camera LiDAR Calibration using ROS, OpenCV, and PCL. Perception Pipeline SARosPerceptionKitti - ROS package for the Perception (Sensor Processing, Detection, Tracking and Evaluation) of the KITTI Vision Benchmark Suite. multiple-object-tracking-lidar - C++ implementation to Detect, track and classify multiple objects using LIDAR scans or point cloud. cadrl_ros - ROS package for dynamic obstacle avoidance for ground robots trained with deep RL. AugmentedAutoencoder - RGB-based pipeline for object detection and 6D pose estimation. jsk_recognition - A stack for the perception packages which are used in JSK lab. GibsonEnv - Gibson Environments: Real-World Perception for Embodied Agents. morefusion - Multi-object Reasoning for 6D Pose Estimation from Volumetric Fusion. Machine Learning DLIB - A toolkit for making real world machine learning and data analysis applications in C++. fastai - The fastai library simplifies training fast and accurate neural nets using modern best practices. tpot - A Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. deap - Distributed Evolutionary Algorithms in Python. gym - A toolkit for developing and comparing reinforcement learning algorithms. tensorflow_ros_cpp - A ROS package that allows to do Tensorflow inference in C++ without the need to compile TF yourself. Tensorflow Federated - TensorFlow Federated (TFF) is an open-source framework for machine learning and other computations on decentralized data. finn - Fast, Scalable Quantized Neural Network Inference on FPGAs. neuropod - Neuropod is a library that provides a uniform interface to run deep learning models from multiple frameworks in C++ and Python. leela-zero - This is a fairly faithful reimplementation of the system described in the Alpha Go Zero paper \"Mastering the Game of Go without Human Knowledge\". Trax - A library for deep learning that focuses on sequence models and reinforcement learning. mlflow - A platform to streamline machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models. Netron - Visualizer for neural network, deep learning and machine learning models. MNN - A blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba. Tensorforce - An open-source deep reinforcement learning framework, with an emphasis on modularized flexible library design and straightforward usability for applications in research and practice. Dopamine - A research framework for fast prototyping of reinforcement learning algorithms. catalyst - Was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. ray - A fast and simple framework for building and running distributed applications. tf-agents - A reliable, scalable and easy to use TensorFlow library for Contextual Bandits and Reinforcement Learning. ReAgent - An open source end-to-end platform for applied reinforcement learning (RL) developed and used at Facebook. Awesome-Mobile-Machine-Learning - A curated list of awesome mobile machine learning resources for iOS, Android, and edge devices. cnn-explainer - Learning Convolutional Neural Networks with Interactive Visualization. modelzoo - A collection of machine-learned models for use in autonomous driving applications. Parallel Processing dask - Parallel computing with task scheduling for Python. cupy - NumPy-like API accelerated with CUDA. Thrust - A C++ parallel programming library which resembles the C++ Standard Library. ArrayFire - A general purpose GPU library. OpenMP - An application programming interface that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran. VexCL - VexCL is a C++ vector expression template library for OpenCL/CUDA/OpenMP. PYNQ - An open-source project from Xilinx that makes it easy to design embedded systems with Zynq All Programmable Systems on Chips. numba - NumPy aware dynamic Python compiler using LLVM. TensorRT - A C++ library for high performance inference on NVIDIA GPUs and deep learning accelerators. libcudacxx - Provides a heterogeneous implementation of the C++ Standard Library that can be used in and between CPU and GPU code. Image Processing CV-pretrained-model - A collection of computer vision pre-trained models. image_pipeline - Fills the gap between getting raw images from a camera driver and higher-level vision processing. gstreamer - A pipeline-based multimedia framework that links together a wide variety of media processing systems to complete complex workflows. ros2_openvino_toolkit - Provides a ROS-adaptered runtime framework of neural network which quickly deploys applications and solutions for vision inference. vision_visp - Wraps the ViSP moving edge tracker provided by the ViSP visual servoing library into a ROS package. apriltag_ros - A ROS wrapper of the AprilTag 3 visual fiducial detector. deep_object_pose - Deep Object Pose Estimation. DetectAndTrack - Detect-and-Track: Efficient Pose. SfMLearner - An unsupervised learning framework for depth and ego-motion estimation. imgaug - Image augmentation for machine learning experiments. vision_opencv - Packages for interfacing ROS with OpenCV, a library of programming functions for real time computer vision. darknet_ros - YOLO ROS: Real-Time Object Detection for ROS. ros_ncnn - YOLACT / YOLO ( among other things ) on NCNN inference engine for ROS. tf-pose-estimation - Deep Pose Estimation implemented using Tensorflow with Custom Architectures for fast inference. find-object - Simple Qt interface to try OpenCV implementations of SIFT, SURF, FAST, BRIEF and other feature detectors and descriptors. yolact - A simple, fully convolutional model for real-time instance segmentation. Kimera-Semantics - Real-Time 3D Semantic Reconstruction from 2D data. detectron2 - A next-generation research platform for object detection and segmentation. OpenVX - Enables performance and power-optimized computer vision processing, especially important in embedded and real-time use cases. 3d-vehicle-tracking - Official implementation of Joint Monocular 3D Vehicle Detection and Tracking. pysot - The goal of PySOT is to provide a high-quality, high-performance codebase for visual tracking research. semantic_slam - Real time semantic slam in ROS with a hand held RGB-D camera. kitti_scan_unfolding - We propose KITTI scan unfolding in our paper Scan-based Semantic Segmentation of LiDAR Point Clouds: An Experimental Study. packnet-sfm - Official PyTorch implementation of self-supervised monocular depth estimation methods invented by the ML Team at Toyota Research Institute (TRI). AB3DMOT - This work proposes a simple yet accurate real-time baseline 3D multi-object tracking system. monoloco - Official implementation of \"MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation\" in PyTorch. Poly-YOLO - Builds on the original ideas of YOLOv3 and removes two of its weaknesses: a large amount of rewritten labels and inefficient distribution of anchors. satellite-image-deep-learning - Resources for deep learning with satellite & aerial imagery. robosat - Semantic segmentation on aerial and satellite imagery. big_transfer - Model for General Visual Representation Learning created by Google Research. LEDNet - A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation. TorchSeg - This project aims at providing a fast, modular reference implementation for semantic segmentation models using PyTorch. simpledet - A Simple and Versatile Framework for Object Detection and Instance Recognition. meshroom - Meshroom is a free, open-source 3D Reconstruction Software based on the AliceVision Photogrammetric Computer Vision framework. EasyOCR - Ready-to-use Optical character recognition (OCR) with 40+ languages supported including Chinese, Japanese, Korean and Thai. pytracking - A general python framework for visual object tracking and video object segmentation, based on PyTorch. ros_deep_learning - Deep learning inference nodes for ROS with support for NVIDIA Jetson TX1/TX2/Xavier and TensorRT. hyperpose - HyperPose: A Flexible Library for Real-time Human Pose Estimation. fawkes - Privacy preserving tool against facial recognition systems. anonymizer - An anonymizer to obfuscate faces and license plates. opendatacam - Only saves surveyed meta-data, in particular the path an object moved or number of counted objects at a certain point. Cam2BEV - TensorFlow Implementation for Computing a Semantically Segmented Bird's Eye View (BEV) Image Given the Images of Multiple Vehicle-Mounted Cameras. satpy - Python package for earth-observing satellite data processing. flownet2-pytorch - Pytorch implementation of FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks. Simd - C++ image processing and machine learning library with using of SIMD: SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, AVX, AVX2, AVX-512, VMX(Altivec) and VSX(Power7), NEON for ARM. Radar Processing pyroSAR - Framework for large-scale SAR satellite data processing. CameraRadarFusionNet - TUM Roborace Team Software Stack - Path tracking control, velocity control, curvature control and state estimation. Lidar and Point Cloud Processing cilantro - A lean C++ library for working with point cloud data. open3d - Open3D: A Modern Library for 3D Data Processing. SqueezeSeg - Implementation of SqueezeSeg, convolutional neural networks for LiDAR point clout segmentation. point_cloud_io - ROS nodes to read and write point clouds from and to files (e.g. ply, vtk). python-pcl - Python bindings to the pointcloud library. libpointmatcher - An \"Iterative Closest Point\" library for 2-D/3-D mapping in Robotics. depth_clustering - Fast and robust clustering of point clouds generated with a Velodyne sensor. lidar-bonnetal - Semantic and Instance Segmentation of LiDAR point clouds for autonomous driving. CSF - LiDAR point cloud ground filtering / segmentation (bare earth extraction) method based on cloth simulation. robot_body_filter - A highly configurable LaserScan/PointCloud2 filter that allows to dynamically remove the 3D body of the robot from the measurements. grid_map - Universal grid map library for mobile robotic mapping. elevation_mapping - Robot-centric elevation mapping for rough terrain navigation. rangenet_lib - Contains simple usage explanations of how the RangeNet++ inference works with the TensorRT and C++ interface. pointcloud_to_laserscan - Converts a 3D Point Cloud into a 2D laser scan. octomap - An Efficient Probabilistic 3D Mapping Framework Based on Octrees. pptk - Point Processing Toolkit from HEREMaps. gpu-voxels - GPU-Voxels is a CUDA based library which allows high resolution volumetric collision detection between animated 3D models and live pointclouds from 3D sensors of all kinds. spatio_temporal_voxel_layer - A new voxel layer leveraging modern 3D graphics tools to modernize navigation environmental representations. LAStools - Award-winning software for efficient LiDAR processing. PCDet - A general PyTorch-based codebase for 3D object detection from point cloud. PDAL - A C++ BSD library for translating and manipulating point cloud data. PotreeConverter - Builds a potree octree from las, laz, binary ply, xyz or ptx files. fast_gicp - A collection of GICP-based fast point cloud registration algorithms. ndt_omp - Multi-threaded and SSE friendly NDT algorithm. laser_line_extraction - A ROS packages that extracts line segments from LaserScan messages. Go-ICP - Implementation of the Go-ICP algorithm for globally optimal 3D pointset registration. PointCNN - A simple and general framework for feature learning from point clouds. segmenters_lib - The LiDAR segmenters library, for segmentation-based detection. MotionNet - Joint Perception and Motion Prediction for Autonomous Driving Based on Bird's Eye View Maps. PolarSeg - An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation. traversability_mapping - Takes in point cloud from a Velodyne VLP-16 Lidar and outputs a traversability map for autonomous navigation in real-time. lidar_super_resolution - Simulation-based Lidar Super-resolution for Ground Vehicles. Cupoch - A library that implements rapid 3D data processing and robotics computation using CUDA. linefit_ground_segmentation - Implementation of the ground segmentation algorithm. Draco - A library for compressing and decompressing 3D geometric meshes and point clouds. Votenet - Deep Hough Voting for 3D Object Detection in Point Clouds. lidar_undistortion - Provides lidar motion undistortion based on an external 6DoF pose estimation input. superpoint_graph - Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs. RandLA-Net - Efficient Semantic Segmentation of Large-Scale Point Clouds. Det3D - A first 3D Object Detection toolbox which provides off the box implementations of many 3D object detection algorithms such as PointPillars, SECOND, PIXOR. OverlapNet - A modified Siamese Network that predicts the overlap and relative yaw angle of a pair of range images generated by 3D LiDAR scans. mp2p_icp - A repertory of multi primitive-to-primitive (MP2P) ICP algorithms in C++. OpenPCDet - A Toolbox for LiDAR-based 3D Object Detection. torch-points3d - Pytorch framework for doing deep learning on point clouds. PolyFit - Polygonal Surface Reconstruction from Point Clouds. mmdetection3d - Next-generation platform for general 3D object detection. gpd - Takes a point cloud as input and produces pose estimates of viable grasps as output. SalsaNext - Uncertainty-aware Semantic Segmentation of LiDAR Point Clouds for Autonomous Driving. Super-Fast-Accurate-3D-Object-Detection - Super Fast and Accurate 3D Object Detection based on 3D LiDAR Point Clouds (The PyTorch implementation). Localization and State Estimation evo - Python package for the evaluation of odometry and SLAM. robot_localization - A package of nonlinear state estimation nodes. fuse - General architecture for performing sensor fusion live on a robot. GeographicLib - A C++ library for geographic projections. ntripbrowser - A Python API for browsing NTRIP (Networked Transport of RTCM via Internet Protocol). imu_tools - IMU-related filters and visualizers. RTKLIB - A version of RTKLIB optimized for single and dual frequency low cost GPS receivers, especially u-blox receivers. gLAB - Performs precise modeling of GNSS observables (pseudorange and carrier phase) at the centimetre level, allowing standalone GPS positioning, PPP, SBAS and DGNSS. ai-imu-dr - Contains the code of our novel accurate method for dead reckoning of wheeled vehicles based only on an IMU. Kalman-and-Bayesian-Filters-in-Python - Kalman Filter book using Jupyter Notebook. mcl_3dl - A ROS node to perform a probabilistic 3-D/6-DOF localization system for mobile robots with 3-D LIDAR(s). se2lam - On-SE(2) Localization and Mapping for Ground Vehicles by Fusing Odometry and Vision. mmWave-localization-learning - ML-based positioning method from mmWave transmissions - with high accuracy and energy efficiency. dynamic_robot_localization - A ROS package that offers 3 DoF and 6 DoF localization using PCL and allows dynamic map update using OctoMap. eagleye - An open-source software for vehicle localization utilizing GNSS and IMU. python-sgp4 - Python version of the SGP4 satellite position library. PROJ - Cartographic Projections and Coordinate Transformations Library. rpg_trajectory_evaluation - Implements common used trajectory evaluation methods for visual(-inertial) odometry. pymap3d - Pure-Python (Numpy optional) 3D coordinate conversions for geospace ecef enu eci. Simultaneous Localization and Mapping Lidar loam_velodyne - Laser Odometry and Mapping (Loam) is a realtime method for state estimation and mapping using a 3D lidar. lio-mapping - Implementation of Tightly Coupled 3D Lidar Inertial Odometry and Mapping (LIO-mapping). A-LOAM - Advanced implementation of LOAM. Fast LOAM - Fast and Optimized Lidar Odometry And Mapping. LIO_SAM - Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping. cartographer_ros - Provides ROS integration for Cartographer. loam_livox - A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR. StaticMapping - Use LiDAR to map the static world. semantic_suma - Semantic Mapping using Surfel Mapping and Semantic Segmentation. slam_toolbox - Slam Toolbox for lifelong mapping and localization in potentially massive maps with ROS . maplab - An open visual-inertial mapping framework. hdl_graph_slam - An open source ROS package for real-time 6DOF SLAM using a 3D LIDAR. interactive_slam - In contrast to existing automatic SLAM packages, we with minimal human effort. LeGO-LOAM - Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain. pyslam - Contains a monocular Visual Odometry (VO) pipeline in Python. Kitware SLAM - LiDAR-only visual SLAM developped by Kitware, as well as ROS and ParaView wrappings for easier use. horizon_highway_slam - A robust, low drift, and real time highway SLAM package suitable for Livox Horizon lidar. mola - A Modular System for Localization and Mapping. DH3D - Deep Hierarchical 3D Descriptors for Robust Large-Scale 6DOF Relocalization. LaMa - LaMa is a C++11 software library for robotic localization and mapping. LIO-SAM - Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping. Scan Context - Global LiDAR descriptor for place recognition and long-term localization. Visual orb_slam_2_ros - A ROS implementation of ORB_SLAM2. orbslam-map-saving-extension - In this extensions the map of ORB-features be saved to the disk as a reference for future runs along the same track. dso - Direct Sparse Odometry. viso2 - A ROS wrapper for libviso2, a library for visual odometry. xivo - X Inertial-aided Visual Odometry. rovio - Robust Visual Inertial Odometry Framework. LSD-SLAM - Large-Scale Direct Monocular SLAM is a real-time monocular SLAM. CubeSLAM and ORB SLAM - Monocular 3D Object Detection and SLAM Package of CubeSLAM and ORB SLAM. VINS-Fusion - A Robust and Versatile Multi-Sensor Visual-Inertial State Estimator. openvslam - OpenVSLAM: A Versatile Visual SLAM Framework. basalt - Visual-Inertial Mapping with Non-Linear Factor Recovery. Kimera - A C++ library for real-time metric-semantic simultaneous localization and mapping, which uses camera images and inertial data to build a semantically annotated 3D mesh of the environment. tagslam - A ROS-based package for Simultaneous Localization and Mapping using AprilTag fiducial markers. LARVIO - A lightweight, accurate and robust monocular visual inertial odometry based on Multi-State Constraint Kalman Filter. fiducials - Simultaneous localization and mapping using fiducial markers. open_vins - An open source platform for visual-inertial navigation research. ORB_SLAM3 - ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM. Atlas - End-to-End 3D Scene Reconstruction from Posed Images. vilib - This library focuses on the front-end of VIO pipelines with CUDA. hloc - A modular toolbox for state-of-the-art 6-DoF visual localization. It implements Hierarchical Localization, leveraging image retrieval and feature matching, and is fast, accurate, and scalable. ESVO - A novel pipeline for real-time visual odometry using a stereo event-based camera. Vector Map OpenDRIVE - An open file format for the logical description of road networks. MapsModelsImporter - A Blender add-on to import models from google maps. Lanelet2 - Map handling framework for automated driving. barefoot - Online and Offline map matching that can be used stand-alone and in the cloud. iD - The easy-to-use OpenStreetMap editor in JavaScript. RapiD - An enhanced version of iD for mapping with AI created by Facebook. segmap - A map representation based on 3D segments. Mapbox - A JavaScript library for interactive, customizable vector maps on the web. osrm-backend - Open Source Routing Machine - C++ backend. assuremapingtools - Desktop based tool for viewing, editing and saving road network maps for autonomous vehicle platforms such as Autoware. geopandas - A project to add support for geographic data to pandas objects. MapToolbox - Plugins to make Autoware vector maps in Unity. imagery-index - An index of aerial and satellite imagery useful for mapping. mapillary_tools - A library for processing and uploading images to Mapillary. mapnik - Combines pixel-perfect image output with lightning-fast cartographic algorithms, and exposes interfaces in C++, Python, and Node. gdal - GDAL is an open source X/MIT licensed translator library for raster and vector geospatial data formats. grass - GRASS GIS - free and open source Geographic Information System (GIS). 3d-tiles - Specification for streaming massive heterogeneous 3D geospatial datasets. osmnx - Python for street networks. Retrieve, model, analyze, and visualize street networks and other spatial data from OpenStreetMap. Prediction Awesome-Interaction-aware-Trajectory-Prediction - A selection of state-of-the-art research materials on trajectory prediction. sgan - Socially Acceptable Trajectories with Generative Adversarial Networks. Behavior and Decision Groot - Graphical Editor to create BehaviorTrees. Compliant with BehaviorTree.CPP. BehaviorTree.CPP - Behavior Trees Library in C++. RAFCON - Uses hierarchical state machines, featuring concurrent state execution, to represent robot programs. ROSPlan - Generic framework for task planning in a ROS system. ad-rss-lib - Library implementing the Responsibility Sensitive Safety model (RSS) for Autonomous Vehicles. FlexBE - Graphical editor for hierarchical state machines, based on ROS's smach. sts_bt_library - This library provides the functionality to set up your own behavior tree logic by using the defined tree structures like Fallback, Sequence or Parallel Nodes. SMACC - An Event-Driven, Asynchronous, Behavioral State Machine Library for real-time ROS (Robotic Operating System) applications written in C++ . py_trees_ros - Behaviours, trees and utilities that extend py_trees for use with ROS. Planning and Control pacmod - Designed to allow the user to control a vehicle with the PACMod drive-by-wire system. mpcc - Model Predictive Contouring Controller for Autonomous Racing. rrt - C++ RRT (Rapidly-exploring Random Tree) implementation. HypridAStarTrailer - A path planning algorithm based on Hybrid A* for trailer truck. path_planner - Hybrid A* Path Planner for the KTH Research Concept Vehicle. open_street_map - ROS packages for working with Open Street Map geographic information. Open Source Car Control - An assemblage of software and hardware designs that enable computer control of modern cars in order to facilitate the development of autonomous vehicle technology. fastrack - A ROS implementation of Fast and Safe Tracking (FaSTrack). commonroad - Composable benchmarks for motion planning on roads. traffic-editor - A graphical editor for robot traffic flows. steering_functions - Contains a C++ library that implements steering functions for car-like robots with limited turning radius. moveit - Easy-to-use robotics manipulation platform for developing applications, evaluating designs, and building integrated products. flexible-collision-library - A library for performing three types of proximity queries on a pair of geometric models composed of triangles. aikido - Artificial Intelligence for Kinematics, Dynamics, and Optimization. casADi - A symbolic framework for numeric optimization implementing automatic differentiation in forward and reverse modes on sparse matrix-valued computational graphs. ACADO Toolkit - A software environment and algorithm collection for automatic control and dynamic optimization. control-toolbox - An efficient C++ library for control, estimation, optimization and motion planning in robotics. CrowdNav - Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning. ompl - Consists of many state-of-the-art sampling-based motion planning algorithms. openrave - Open Robotics Automation Virtual Environment: An environment for testing, developing, and deploying robotics motion planning algorithms. teb_local_planner - An optimal trajectory planner considering distinctive topologies for mobile robots based on Timed-Elastic-Bands. pinocchio - A fast and flexible implementation of Rigid Body Dynamics algorithms and their analytical derivatives. rmf_core - The rmf_core packages provide the centralized functions of the Robotics Middleware Framework (RMF). OpEn - A solver for Fast & Accurate Embedded Optimization for next-generation Robotics and Autonomous Systems. autogenu-jupyter - This project provides the continuation/GMRES method (C/GMRES method) based solvers for nonlinear model predictive control (NMPC) and an automatic code generator for NMPC. global_racetrajectory_optimization - This repository contains multiple approaches for generating global racetrajectories. toppra - A library for computing the time-optimal path parametrization for robots subject to kinematic and dynamic constraints. tinyspline - TinySpline is a small, yet powerful library for interpolating, transforming, and querying arbitrary NURBS, B-Splines, and Bzier curves. dual quaternions ros - ROS python package for dual quaternion SLERP. mb planner - Aerial vehicle planner for tight spaces. Used in DARPA SubT Challenge. ilqr - Iterative Linear Quadratic Regulator with auto-differentiatiable dynamics models. EGO-Planner - A lightweight gradient-based local planner without ESDF construction, which significantly reduces computation time compared to some state-of-the-art methods. pykep - A scientific library providing basic tools for research in interplanetary trajectory design. am_traj - Alternating Minimization Based Trajectory Generation for Quadrotor Aggressive Flight. GraphBasedLocalTrajectoryPlanner - Was used on a real race vehicle during the Roborace Season Alpha and achieved speeds above 200km/h. User Interaction Graphical User Interface imgui - Designed to enable fast iterations and to empower programmers to create content creation tools and visualization / debug tools. qtpy - Provides an uniform layer to support PyQt5, PySide2, PyQt4 and PySide with a single codebase. mir - Mir is set of libraries for building Wayland based shells. rqt - A Qt-based framework for GUI development for ROS. It consists of three parts/metapackages. cage - This is Cage, a Wayland kiosk. A kiosk runs a single, maximized application. chilipie - Easy-to-use Raspberry Pi image for booting directly into full-screen Chrome. pencil - A tool for making diagrams and GUI prototyping that everyone can use. dynamic_reconfigure - The focus of dynamic_reconfigure is on providing a standard way to expose a subset of a node's parameters to external reconfiguration. ddynamic_reconfigure - Allows modifying parameters of a ROS node using the dynamic_reconfigure framework without having to write cfg files. elements - A lightweight, fine-grained, resolution independent, modular GUI library. NanoGUI - A minimalistic cross-platform widget library for OpenGL 3.x or higher. Acoustic User Interface pyo - A Python module written in C containing classes for a wide variety of audio signal processing types. rhasspy - Rhasspy (pronounced RAH-SPEE) is an offline, multilingual voice assistant toolkit inspired by Jasper that works well with Home Assistant, Hass.io, and Node-RED. mycroft-core - Mycroft is a hackable open source voice assistant. DDSP - A library of differentiable versions of common DSP functions (such as synthesizers, waveshapers, and filters). NoiseTorch - Creates a virtual microphone that suppresses noise, in any application. DeepSpeech - An open source Speech-To-Text engine, using a model trained by machine learning techniques based on Baidu's Deep Speech research paper. waveglow - A Flow-based Generative Network for Speech Synthesis. Command Line Interface the-art-of-command-line - Master the command line, in one page. dotfiles of cornerman - Powerful zsh and vim dotfiles. dotbot - A tool that bootstraps your dotfiles. prompt-hjem - A beautiful zsh prompt. ag - A code-searching tool similar to ack, but faster. fzf - A command-line fuzzy finder. pkgtop - Interactive package manager and resource monitor designed for the GNU/Linux. asciimatics - A cross platform package to do curses-like operations, plus higher level APIs and widgets to create text UIs and ASCII art animations. gocui - Minimalist Go package aimed at creating Console User Interfaces. TerminalImageViewer - Small C++ program to display images in a (modern) terminal using RGB ANSI codes and unicode block graphics characters. rosshow - Visualize ROS topics inside a terminal with Unicode/ASCII art. python-prompt-toolkit - Library for building powerful interactive command line applications in Python. guake - Drop-down terminal for GNOME. wemux - Multi-User Tmux Made Easy. tmuxp - A session manager built on libtmux. mapscii - World map renderer for your console. terminator - The goal of this project is to produce a useful tool for arranging terminals. bat - A cat(1) clone with wings. fx - Command-line tool and terminal JSON viewer. tmate - Instant terminal sharing. Data Visualization and Mission Control xdot - Interactive viewer for graphs written in Graphviz's dot language. guacamole - Clientless remote desktop gateway. It supports standard protocols like VNC, RDP, and SSH. ros3djs - 3D Visualization Library for use with the ROS JavaScript Libraries. webviz - Web-based visualization libraries like rviz. plotly.py - An open-source, interactive graphing library for Python. PlotJuggler - The timeseries visualization tool that you deserve. bokeh - Interactive Data Visualization in the browser, from Python. voila - From Jupyter notebooks to standalone web applications and dashboards. Pangolin - Pangolin is a lightweight portable rapid development library for managing OpenGL display / interaction and abstracting video input. rqt_bag - Provides a GUI plugin for displaying and replaying ROS bag files. kepler.gl - Kepler.gl is a powerful open source geospatial analysis tool for large-scale data sets. qgis_ros - Access bagged and live topic data in a highly featured GIS environment. openmct - A web based mission control framework. web_video_server - HTTP Streaming of ROS Image Topics in Multiple Formats. RVizWeb - Provides a convenient way of building and launching a web application with features similar to RViz. marvros - MAVLink to ROS gateway with proxy for Ground Control Station. octave - Provides a convenient command line interface for solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with Matlab. streetscape.gl - Streetscape.gl is a toolkit for visualizing autonomous and robotics data in the XVIZ protocol. urdf-loaders - URDF Loaders for Unity and THREE.js with example ATHLETE URDF File. obs-studio - Free and open source software for live streaming and screen recording. Annotation labelbox - The fastest way to annotate data to build and ship artificial intelligence applications. PixelAnnotationTool - Annotate quickly images. LabelImg - A graphical image annotation tool and label object bounding boxes in images. cvat - Powerful and efficient Computer Vision Annotation Tool (CVAT). point_labeler - Tool for labeling of a single point clouds or a stream of point clouds. label-studio - Label Studio is a multi-type data labeling and annotation tool with standardized output format. napari - A fast, interactive, multi-dimensional image viewer for python. semantic-segmentation-editor - A web based labeling tool for creating AI training data sets (2D and 3D). 3d-bat - 3D Bounding Box Annotation Tool for Point cloud and Image Labeling. labelme - Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation). universal-data-tool - Collaborate & label any type of data, images, text, or documents, in an easy web interface or desktop app. BMW-Labeltool-Lite - Provides you with a easy to use labeling tool for State-of-the-art Deep Learning training purposes. Point Cloud CloudCompare - CloudCompare is a 3D point cloud (and triangular mesh) processing software. Potree - WebGL point cloud viewer for large datasets. point_cloud_viewer - Makes viewing massive point clouds easy and convenient. LidarView - Performs real-time visualization and easy processing of live captured 3D LiDAR data from Lidar sensors. VeloView - Performs real-time visualization of live captured 3D LiDAR data from Velodyne's HDL sensors. entwine - A data organization library for massive point clouds, designed to conquer datasets of trillions of points as well as desktop-scale point clouds. polyscope - A C++ & Python viewer for 3D data like meshes and point clouds. Pcx - Point cloud importer & renderer for Unity. ImmersivePoints - A web-application for virtual reality devices to explore 3D data in the most natural way possible. RViz mapviz - Modular ROS visualization tool for 2D data. rviz_cinematographer - Easy to use tools to create and edit trajectories for the rviz camera. rviz_satellite - Display internet satellite imagery in RViz. rviz_visual_tools - C++ API wrapper for displaying shapes and meshes in Rviz. xpp - Visualization of motion-plans for legged robots. rviz stereo - 3D stereo rendering displays a different view to each eye so that the scene appears to have depth. jsk_visualization - Jsk visualization ros packages for rviz and rqt. moveit_visual_tools - Helper functions for displaying and debugging MoveIt! data in Rviz via published markers. Operation System Monitoring rosmon - ROS node launcher & monitoring daemon. multimaster_fkie - GUI-based management environment that is very useful to manage ROS-launch configurations and control running nodes. collectd - A small daemon which collects system information periodically and provides mechanisms to store and monitor the values in a variety of ways. lnav - An enhanced log file viewer that takes advantage of any semantic information that can be gleaned from the files being viewed, such as timestamps and log levels. htop - An interactive text-mode process viewer for Unix systems. It aims to be a better 'top'. atop - System and process monitor for Linux with logging and replay function. psutil - Cross-platform lib for process and system monitoring in Python. gputil - A Python module for getting the GPU status from NVIDA GPUs using nvidia-smi programmically in Python. gpustat - A simple command-line utility for querying and monitoring GPU status. nvtop - NVIDIA GPUs htop like monitoring tool. spdlog - Very fast, header-only/compiled, C++ logging library. ctop - Top-like interface for container metrics. ntop - Web-based Traffic and Security Network Traffic Monitoring. jupyterlab-nvdashboard - A JupyterLab extension for displaying dashboards of GPU usage. Database and Record ncdu - Ncdu is a disk usage analyzer with an ncurses interface. borg - Deduplicating archiver with compression and authenticated encryption. bag-database - A server that catalogs bag files and provides a web-based UI for accessing them. marv-robotics - MARV Robotics is a powerful and extensible data management platform. kitti2bag - Convert KITTI dataset to ROS bag file the easy way. pykitti - Python tools for working with KITTI data. rosbag_editor - Create a rosbag from a given one, using a simple GUI. nextcloud - Nextcloud is a suite of client-server software for creating and using file hosting services. ros_type_introspection - Deserialize ROS messages that are unknown at compilation time. syncthing - A continuous file synchronization program. rqt_bag_exporter - Qt GUI to export ROS bag topics to files (CSV and/or video). xviz - A protocol for real-time transfer and visualization of autonomy data. kitti_to_rosbag - A Dataset tools for working with the KITTI dataset raw data and converting it to a ROS bag. Also allows a library for direct access to poses, velodyne scans, and images. ros_numpy - Tools for converting ROS messages to and from numpy arrays. kitti_ros - A ROS-based player to replay KiTTI dataset. DuckDB - An embeddable SQL OLAP Database Management System. Network Distributed File System sshfs - File system based on the SSH File Transfer Protocol. moosefs - A scalable distributed storage system. ceph - A distributed object, block, and file storage platform. nfs - A distributed file system protocol originally developed by Sun Microsystems. ansible-role-nfs - Installs NFS utilities on RedHat/CentOS or Debian/Ubuntu. Server Infrastructure and High Performance Computing mass - Self-service, remote installation of Windows, CentOS, ESXi and Ubuntu on real servers turns your data centre into a bare metal cloud. polyaxon - A platform for reproducing and managing the whole life cycle of machine learning and deep learning applications. localstack - A fully functional local AWS cloud stack. Develop and test your cloud & Serverless apps offline. nvidia-docker - Build and run Docker containers leveraging NVIDIA GPUs. kubeflow - Machine Learning Toolkit for Kubernetes. log-pilot - Collect logs for docker containers. traefik - The Cloud Native Edge Router. graylog2-server - Free and open source log management. ansible - Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy. pyinfra - It can be used for ad-hoc command execution, service deployment, configuration management and more. docker-py - A Python library for the Docker Engine API. noVNC - VNC client using HTML5. Slurm - Slurm: A Highly Scalable Workload Manager. jupyterhub - Multi-user server for Jupyter notebooks. Portainer - Making Docker management easy. enroot - A simple, yet powerful tool to turn traditional container/OS images into unprivileged sandboxes. docker-firefox - Run a Docker Container with Firefox and noVNC for remote access to headless servers. luigi - A Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in. triton-inference-server - NVIDIA Triton Inference Server provides a cloud inferencing solution optimized for NVIDIA GPUs. cudf - Provides a pandas-like API that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming. Embedded Operation System vxworks7-ros2-build - Build system to automate the build of VxWorks 7 and ROS2. Yocto - Produce tools and processes that enable the creation of Linux distributions for embedded software that are independent of the underlying architecture of the embedded hardware. Automotive Graded Linux - A collaborative open source project that is bringing together automakers, suppliers and technology companies to build a Linux-based, open software platform for automotive applications that can serve as the de facto industry standard. bitbake - A generic task execution engine that allows shell and Python tasks to be run efficiently and in parallel while working within complex inter-task dependency constraints. Jailhouse - Jailhouse is a partitioning Hypervisor based on Linux. Xen - An open-source (GPL) type-1 or baremetal hypervisor. QEMU - A generic and open source machine emulator and virtualizer. qemu-xilinx - A fork of Quick EMUlator (QEMU) with improved support and modelling for the Xilinx platforms. rosserial - A ROS client library for small, embedded devices, such as Arduino. meta-ros - OpenEmbedded Layer for ROS Applications. meta-balena - Run Docker containers on embedded devices. micro-ros - The major changes compared to \"regular\" ROS 2 is that micro-ROS uses a Real-Time Operating System (RTOS) instead of Linux, and DDS for eXtremely Resource Constrained Environments. nvidia-container-runtime - NVIDIA Container Runtime is a GPU aware container runtime, compatible with the Open Containers Initiative (OCI) specification used by Docker, CRI-O, and other popular container technologie. fusesoc - Package manager and build abstraction tool for FPGA/ASIC development. jetson_easy - Automatically script to setup and configure your NVIDIA Jetson. docker-jetpack-sdk - Allows for usage of the NVIDIA JetPack SDK within a docker container for download, flashing, and install. Pressed - Provides a way to set answers to questions asked during the installation process of debian, without having to manually enter the answers while the installation is running. jetson_stats - A package to monitoring and control your NVIDIA Jetson (Xavier NX, Nano, AGX Xavier, TX1, TX2) Works with all NVIDIA Jetson ecosystem. ros_jetson_stats - The ROS jetson-stats wrapper. The status of your NVIDIA jetson in diagnostic messages. OpenCR - Open-source Control Module for ROS. acrn-hypervisor - Defines a device hypervisor reference stack and an architecture for running multiple software subsystems, managed securely, on a consolidated system by means of a virtual machine manager. jetson-containers - Machine Learning Containers for Jetson and JetPack 4.4. Real-Time Kernel ELISA - Project is to make it easier for companies to build and certify Linux-based safety-critical applications systems whose failure could result in loss of human life, significant property damage or environmental damage. PREEMPT_RT kernel patch - Aim of the PREEMPT_RT kernel patch is to minimize the amount of kernel code that is non-preemptible. Network and Middleware performance_test - Tool to test the performance of pub/sub based communication frameworks. realtime_support - Minimal real-time testing utility for measuring jitter and latency. ros1_bridge - ROS 2 package that provides bidirectional communication between ROS 1 and ROS 2. Fast-RTPS - A Protocol, which provides publisher-subscriber communications over unreliable transports such as UDP, as defined and maintained by the Object Management Group (OMG) consortium. protobuf - Google's data interchange format. opensplice - Vortex OpenSplice Community Edition. cyclonedds - Eclipse Cyclone DDS is a very performant and robust open-source DDS implementation. iceoryx - An IPC middleware for POSIX-based systems. rosbridge_suite - Provides a JSON interface to ROS, allowing any client to send JSON to publish or subscribe to ROS topics, call ROS services, and more. ros2arduino - This library helps the Arduino board communicate with the ROS2 using XRCE-DDS. eCAL - The enhanced communication abstraction layer (eCAL) is a middleware that enables scalable, high performance interprocess communication on a single computer node or between different nodes in a computer network. AUTOSAR-Adaptive - The implementation of AUTOSAR Adaptive Platform based on the R19-11. ocpp - The Open Charge Point Protocol (OCPP) is a network protocol for communication between electric vehicle chargers and a central backoffice system. Ethernet and Wireless Networking SOES - SOES is an EtherCAT slave stack written in C. netplan - Simply create a YAML description of the required network interfaces and what each should be configured to do. airalab - AIRA is reference Robonomics network client for ROS-enabled cyber-physical systems. rdbox - RDBOX is a IT infrastructure for ROS robots. ros_ethercat - This is a reimplementation of the main loop of pr2_ethercat without dependencies on PR2 software. wavemon - An ncurses-based monitoring application for wireless network devices. wireless - Making info about wireless networks available to ROS. ptpd - PTP daemon (PTPd) is an implementation the Precision Time Protocol (PTP) version 2 as defined by 'IEEE Std 1588-2008'. PTP provides precise time coordination of Ethernet LAN connected computers. iperf - A TCP, UDP, and SCTP network bandwidth measurement tool. tcpreplay - Pcap editing and replay tools. nethogs - It groups bandwidth by process. pyshark - Python wrapper for tshark, allowing python packet parsing using wireshark dissectors. pingtop - Ping multiple servers and show results in a top-like terminal UI. termshark - A terminal UI for tshark, inspired by Wireshark. udpreplay - Replay UDP packets from a pcap file. openwifi - Linux mac80211 compatible full-stack IEEE802.11/Wi-Fi design based on Software Defined Radio. Controller Area Network AndrOBD - Android OBD diagnostics with any ELM327 adapter. ddt4all - DDT4All is a tool to create your own ECU parameters screens and connect to a CAN network with a cheap ELM327 interface. cabana - CAN visualizer and DBC maker. opendbc - The project to democratize access to the decoder ring of your car. libuavcan - An open lightweight protocol designed for reliable communication in aerospace and robotic applications over robust vehicular networks such as CAN bus. python-can - The can package provides controller area network support for Python developers. CANopenNode - The internationally standardized (EN 50325-4) (CiA301) CAN-based higher-layer protocol for embedded control system. python-udsoncan - Python implementation of UDS (ISO-14229) standard. uds-c - Unified Diagnostics Service (UDS) and OBD-II (On Board Diagnostics for Vehicles) C Library. cantools - CAN BUS tools in Python 3. CANdevStudio - CANdevStudio aims to be cost-effective replacement for CAN simulation software. It can work with variety of CAN hardware interfaces. can-utils - Linux-CAN / SocketCAN user space applications. ros_canopen - CANopen driver framework for ROS. decanstructor - The definitive ROS CAN analysis tool. kvaser_interface - This package was developed as a standardized way to access Kvaser CAN devices from ROS. canmatrix - Converting CAN Database Formats .arxml .dbc .dbf .kcd. autosar - A set of python modules for working with AUTOSAR XML files. canopen - A Python implementation of the CANopen standard. The aim of the project is to support the most common parts of the CiA 301 standard in a Pythonic interface. SavvyCAN - A Qt5 based cross platform tool which can be used to load, save, and capture canbus frames. Open-Vehicle-Monitoring-System-3 - The system provides live monitoring of vehicle metrics like state of charge, temperatures, tyre pressures and diagnostic fault conditions. Sensor and Acuator Interfaces Tesla-API - Provides functionality to monitor and control the Model S (and future Tesla vehicles) remotely. flirpy - A Python library to interact with FLIR thermal imaging cameras and images. nerian_stereo - ROS node for Nerian's SceneScan and SP1 stereo vision sensors. pymmw - This is a toolbox composed of Python scripts to interact with TI's evaluation module (BoosterPack) for the IWR1443 mmWave sensing device. ti_mmwave_rospkg - TI mmWave radar ROS driver (with sensor fusion and hybrid). pacmod3 - This ROS node is designed to allow the user to control a vehicle with the PACMod drive-by-wire system, board revision 3. ros2_intel_realsense - These are packages for using Intel RealSense cameras (D400 series) with ROS2. sick_scan - This stack provides a ROS2 driver for the SICK TiM series of laser scanners. ouster_example - Sample code for connecting to and configuring the OS1, reading and visualizing data, and interfacing with ROS. ros2_ouster_drivers - These are an implementation of ROS2 drivers for the Ouster OS-1 3D lidars. livox_ros_driver - A new ROS package, specially used to connect LiDAR products produced by Livox. velodyne - A collection of ROS packages supporting Velodyne high definition 3D LIDARs. ublox - Provides support for u-blox GPS receivers. crazyflie_ros - ROS Driver for Bitcraze Crazyflie. pointgrey_camera_driver - ROS driver for Pt. Grey cameras, based on the official FlyCapture2 SDK. novatel_gps_driver - ROS driver for NovAtel GPS / GNSS receivers. pylon-ros-camera - The official pylon ROS driver for Basler GigE Vision and USB3 Vision cameras. ethz_piksi_ros - Contains (python) ROS drivers, tools, launch files, and wikis about how to use Piksi Real Time Kinematic (RTK) GPS device in ROS. sick_safetyscanners - A ROS Driver which reads the raw data from the SICK Safety Scanners and publishes the data as a laser_scan msg. bosch_imu_driver - A driver for the sensor IMU Bosch BNO055. It was implemented only the UART communication interface (correct sensor mode should be selected). oxford_gps_eth - Ethernet interface to OxTS GPS receivers using the NCOM packet structure. ifm3d - Library and Utilities for working with ifm pmd-based 3D ToF Cameras. cepton_sdk_redist - Provides ROS support for Cepton LiDAR. jetson_csi_cam - A ROS package making it simple to use CSI cameras on the Nvidia Jetson TK1, TX1, or TX2 with ROS. ros_astra_camera - A ROS driver for Orbbec 3D cameras. spot_ros - ROS Driver for Spot. Security owasp-threat-dragon-desktop - Threat Dragon is a free, open-source, cross-platform threat modeling application including system diagramming and a rule engine to auto-generate threats/mitigations. launch_ros_sandbox - Can define launch files running nodes in restrained environments, such as Docker containers or separate user accounts with limited privileges. wolfssl - A small, fast, portable implementation of TLS/SSL for embedded devices to the cloud. CANalyzat0r - Security analysis toolkit for proprietary car protocols. RSF - Robot Security Framework (RSF) is a standardized methodology to perform security assessments in robotics. How-to-Secure-A-Linux-Server - An evolving how-to guide for securing a Linux server. lynis - Security auditing tool for Linux, macOS, and UNIX-based systems. Assists with compliance testing (HIPAA/ISO27001/PCI DSS) and system hardening. OpenVPN - An open source VPN daemon. openfortivpn - A client for PPP+SSL VPN tunnel services and compatible with Fortinet VPNs. WireGuard - WireGuard is a novel VPN that runs inside the Linux Kernel and utilizes state-of-the-art cryptography. ssh-auditor - Scans for weak ssh passwords on your network. vulscan - Advanced vulnerability scanning with Nmap NSE. nmap-vulners - NSE script based on Vulners.com API. brutespray - Automatically attempts default creds on found services. fail2ban - Daemon to ban hosts that cause multiple authentication errors. DependencyCheck - A software composition analysis utility that detects publicly disclosed vulnerabilities in application dependencies. Firejail - A SUID sandbox program that reduces the risk of security breaches by restricting the running environment of untrusted applications using Linux namespaces, seccomp-bpf and Linux capabilities. RVD - Robot Vulnerability Database. Community-contributed archive of robot vulnerabilities and weaknesses. ros2_dds_security - Adding security enhancements by defining a Service Plugin Interface (SPI) architecture, a set of builtin implementations of the SPIs, and the security model enforced by the SPIs. Security-Enhanced Linux - A Linux kernel security module that provides a mechanism for supporting access control security policies, including mandatory access controls (MAC). OpenTitan - Will make the silicon Root of Trust design and implementation more transparent, trustworthy, and secure for enterprises, platform providers, and chip manufacturers. OpenTitan is administered by lowRISC CIC as a collaborative project to produce high quality, open IP for instantiation as a full-featured product. bandit - A tool designed to find common security issues in Python code. hardening - A quick way to make a Ubuntu server a bit more secure. Passbolt - Passbolt is a free and open source password manager that allows team members to store and share credentials securely. gopass - A password manager for the command line written in Go. pass - The standard unix password manager. Vault - A tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, certificates, and more. legion - An open source, easy-to-use, super-extensible and semi-automated network penetration testing framework that aids in discovery, reconnaissance and exploitation of information systems. openscap - The oscap program is a command line tool that allows users to load, scan, validate, edit, and export SCAP documents. Datasets KITTI-360 - This large-scale dataset contains 320k images and 100k laser scans in a driving distance of 73.7km. waymo_ros - This is a ROS package to connect Waymo open dataset to ROS. waymo-open-dataset - The Waymo Open Dataset is comprised of high-resolution sensor data collected by Waymo self-driving cars in a wide variety of conditions. Ford Autonomous Vehicle Dataset - Ford presents a challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times. awesome-robotics-datasets - A collection of useful datasets for robotics and computer vision. nuscenes-devkit - The devkit of the nuScenes dataset. dataset-api - This is a repo of toolkit for ApolloScape Dataset, CVPR 2019 Workshop on Autonomous Driving Challenge and ECCV 2018 challenge. utbm_robocar_dataset - EU Long-term Dataset with Multiple Sensors for Autonomous Driving. DBNet - A Large-Scale Dataset for Driving Behavior Learning. argoverse-api - Official GitHub repository for Argoverse dataset. DDAD - A new autonomous driving benchmark from TRI (Toyota Research Institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions. pandaset-devkit - Public large-scale dataset for autonomous driving provided by Hesai & Scale. a2d2_to_ros - Utilities for converting A2D2 data sets to ROS bags. awesome-satellite-imagery-datasets - List of satellite image training datasets with annotations for computer vision and deep learning. sentinelsat - Search and download Copernicus Sentinel satellite images. adas-dataset-form - Thermal Dataset for Algorithm Training. h3d - The H3D is a large scale full-surround 3D multi-object detection and tracking dataset from Honda. Mapillary Vistas Dataset - A diverse street-level imagery dataset with pixelaccurate and instancespecific human annotations for understanding street scenes around the world. TensorFlow Datasets - TensorFlow Datasets provides many public datasets as tf.data.Datasets. racetrack-database - Contains center lines (x- and y-coordinates), track widths and race lines for over 20 race tracks (mainly F1 and DTM) all over the world. BlenderProc - A procedural Blender pipeline for photorealistic training image generation. Atlatec Sample Map Data - 3D map for autonomous driving and simulation created from nothing but two cameras and GPS in downtown San Francisco. Lyft Level 5 Dataset - Level 5 is developing a self-driving system for the Lyft network. We're collecting and processing data from our autonomous fleet and sharing it with you. holicity - A City-Scale Data Platform for Learning Holistic 3D Structures. UTD19 - Largest multi-city traffic dataset publically available. ASTYX HIRES2019 DATASET - Automotive Radar Dataset for Deep Learning Based 3D Object Detection. Objectron - A collection of short, object-centric video clips, which are accompanied by AR session metadata that includes camera poses, sparse point-clouds and characterization of the planar surfaces in the surrounding environment. ",
        "_version_": 1718527434047881216
      },
      {
        "story_id": [20058174],
        "story_author": ["markusbk"],
        "story_descendants": [6],
        "story_score": [29],
        "story_time": ["2019-05-31T04:16:37Z"],
        "story_title": "Amazon MSK – managed streaming for Apache Kafka – now generally available",
        "search": [
          "Amazon MSK – managed streaming for Apache Kafka – now generally available",
          "https://aws.amazon.com/blogs/aws/amazon-managed-streaming-for-apache-kafka-msk-now-generally-available/",
          "September 8, 2021: Amazon Elasticsearch Service has been renamed to Amazon OpenSearch Service. See details. I am always amazed at how our customers are using streaming data. For example,Thomson Reuters, one of the worlds most trusted news organizations for businesses and professionals, built a solution to capture, analyze, and visualize analytics data to help product teams continuously improve the user experience. Supercell, the social game company providing games such asHay Day, Clash of Clans, and Boom Beach, is delivering in-game data in real-time, handling 45 billion events per day. Since we launched Amazon Kinesis at re:Invent 2013, we have continually expanded the ways in in which customers work with streaming data on AWS. Some of the availabletools are: Kinesis Data Streams, to capture, store, and process data streams with your own applications. Kinesis Data Firehose, totransform and collect data into destinations such as Amazon S3, Amazon Elasticsearch Service, and Amazon Redshift. Kinesis Data Analytics, to continuously analyze data using SQL or Java (viaApache Flink applications), for example to detect anomalies or for time series aggregation. Kinesis Video Streams, to simplify processing of media streams. At re:Invent 2018, we introduced in open previewAmazon Managed Streaming for Apache Kafka (MSK), a fully managed service that makes it easy to build and run applications that use Apache Kafka to process streaming data. I am excited to announce that Amazon MSK is generally available today! How it works Apache Kafka (Kafka) is an open-source platform that enables customers to capture streaming data like click stream events, transactions, IoT events, application and machine logs, and have applications that perform real-time analytics, run continuous transformations, and distribute this data to data lakes and databases in real time.You can use Kafka as a streaming data store to decouple applications producing streaming data (producers) from those consuming streaming data (consumers). While Kafka is a popular enterprise data streaming and messaging framework, it can be difficult to setup, scale, and manage in production.Amazon MSK takes care of these managing tasks and makes it easy to set up, configure, and run Kafka, along with Apache ZooKeeper, in an environment following best practices for high availability and security. Your MSK clusters always run within an Amazon VPC managed by the MSK service. Your MSK resources are made available to your own VPC, subnet, and security group through elastic network interfaces (ENIs) which will appear in your account, as described in the following architectural diagram: Customers can create a cluster in minutes, use AWS Identity and Access Management (IAM) to control cluster actions, authorize clients using TLS private certificate authorities fully managed by AWS Certificate Manager (ACM), encrypt data in-transit using TLS, and encrypt data at rest using AWS Key Management Service (KMS)encryption keys. Amazon MSK continuously monitors server health and automatically replaces servers when they fail, automates server patching, and operates highly available ZooKeeper nodes as a part of the service at no additional cost. Key Kafka performance metrics are published in the console and in Amazon CloudWatch. Amazon MSK is fully compatible with Kafka versions 1.1.1 and 2.1.0, so that you can continue to run your applications, use Kafkas admin tools, and and use Kafka compatible tools and frameworks without having to change your code. Based on our customer feedback during the open preview, Amazon MSK added may features such as: Encryption in-transit via TLS between clients and brokers, and between brokers Mutual TLS authentication using ACM private certificate authorities Support for Kafka version 2.1.0 99.9% availability SLA HIPAA eligible Cluster-wide storage scale up Integration with AWS CloudTrail for MSK API logging Cluster tagging and tag-based IAM policy application Defining custom, cluster-wide configurations for topics and brokers AWS CloudFormationsupport is coming in the next few weeks. Creating a cluster Lets create a cluster using the AWS management console. I give the cluster a name, select the VPC I want to use the cluster from, and the Kafka version. I then choose the Availability Zones (AZs) and the corresponding subnets to use in the VPC. In the next step,I select how many Kafka brokers to deploy in each AZ. More brokers allow you to scale the throughtput of a cluster by allocating partitions to different brokers. I can add tagsto search and filter my resources, apply IAM policies to the Amazon MSK API, and track my costs. For storage, I leave the default storage volume size per broker. I select to use encryption within the cluster and to allow both TLS and plaintext traffic between clients and brokers. For data at rest, I use the AWS-managed customer master key (CMK), but you can select a CMK in your account, using KMS, to have further control. Youcan use private TLS certificates to authenticate the identity of clients that connect to your cluster. This feature is usingPrivate Certificate Authorities (CA) from ACM. For now, I leave this option unchecked. In the advanced setting, I leave the default values. For example, I could have chosen here a different instance type for my brokers. Some of these settings can be updated using the AWS CLI. I create the cluster and monitor the status from the cluster summary, including the Amazon Resource Name (ARN) that I can use when interacting via CLI or SDKs. When the status is active, the client information section provides specific details to connect to the cluster, such as: The bootstrap servers I can use with Kafka tools to connect to the cluster. The Zookeper connect list of hosts and ports. I can get similar information using the AWS CLI: aws kafka list-clusters to see the ARNs of your clusters in a specific region aws kafka get-bootstrap-brokers --cluster-arn <ClusterArn> to get the Kafka bootstrap servers aws kafka describe-cluster --cluster-arn <ClusterArn> to see more details on the cluster, including the Zookeeper connect string Quick demo of using Kafka To start using Kafka, I create two EC2 instances in the same VPC, one will be a producer and one a consumer. To set them up as client machines, I download and extract theKafka toolsfrom the Apache website or any mirror.Kafka requires Java 8 to run, so I install Amazon Corretto 8. On the producer instance, in the Kafka directory, I create a topic to send data from the producer to the consumer: bin/kafka-topics.sh --create --zookeeper <ZookeeperConnectString> \\ --replication-factor 3 --partitions 1 --topic MyTopic Then I start a console-based producer: bin/kafka-console-producer.sh --broker-list <BootstrapBrokerString> \\ --topic MyTopic On the consumer instance, in the Kafka directory, I start a console-based consumer: bin/kafka-console-consumer.sh --bootstrap-server <BootstrapBrokerString> \\ --topic MyTopic --from-beginning Heres a recording of a quick demo where I create the topic and then send messages from a producer (top terminal) to a consumer of that topic (bottom terminal): Pricing and availability Pricing is per Kafka broker-hour and per provisioned storage-hour. There is no costfor the Zookeeper nodes used by your clusters.AWS data transfer rates apply for data transfer in and out of MSK. You will not be charged for data transfer within the cluster in a region, including data transfer between brokers and data transfer between brokers and ZooKeeper nodes. You can migrate your existing Kafka cluster to MSK using tools like MirrorMaker (that comes with open source Kafka) to replicate data from your clusters into a MSK cluster. Upstream compatibility is a core tenet of Amazon MSK. Our code changes to the Kafka platform are released back to open source. Amazon MSK is available in US East (N. Virginia), US East (Ohio), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Singapore), Asia Pacific (Sydney), EU (Frankfurt), EU (Ireland), EU (Paris), and EU (London). I look forward to see how are you going to use Amazon MSK to simplify building and migrating streaming applications to the cloud! Danilo ",
          "Introduced just weeks after Confluent introduced their similar service [1]. Has anyone done an in depth comparison yet?<p>[1] <a href=\"https://www.confluent.io/confluent-cloud/\" rel=\"nofollow\">https://www.confluent.io/confluent-cloud/</a>",
          "I wonder how it differs from the managed Kafka of Confluent since they also support AWS as the backend. It sounds like the same story of Elasticsearch, MongoDB and Redis though."
        ],
        "story_type": ["Normal"],
        "url": "https://aws.amazon.com/blogs/aws/amazon-managed-streaming-for-apache-kafka-msk-now-generally-available/",
        "comments.comment_id": [20058561, 20058710],
        "comments.comment_author": ["pul", "buremba"],
        "comments.comment_descendants": [3, 0],
        "comments.comment_time": [
          "2019-05-31T06:00:17Z",
          "2019-05-31T06:45:07Z"
        ],
        "comments.comment_text": [
          "Introduced just weeks after Confluent introduced their similar service [1]. Has anyone done an in depth comparison yet?<p>[1] <a href=\"https://www.confluent.io/confluent-cloud/\" rel=\"nofollow\">https://www.confluent.io/confluent-cloud/</a>",
          "I wonder how it differs from the managed Kafka of Confluent since they also support AWS as the backend. It sounds like the same story of Elasticsearch, MongoDB and Redis though."
        ],
        "id": "bb87a2d0-58c3-4e1e-ae7f-7aea56c1871a",
        "url_text": "September 8, 2021: Amazon Elasticsearch Service has been renamed to Amazon OpenSearch Service. See details. I am always amazed at how our customers are using streaming data. For example,Thomson Reuters, one of the worlds most trusted news organizations for businesses and professionals, built a solution to capture, analyze, and visualize analytics data to help product teams continuously improve the user experience. Supercell, the social game company providing games such asHay Day, Clash of Clans, and Boom Beach, is delivering in-game data in real-time, handling 45 billion events per day. Since we launched Amazon Kinesis at re:Invent 2013, we have continually expanded the ways in in which customers work with streaming data on AWS. Some of the availabletools are: Kinesis Data Streams, to capture, store, and process data streams with your own applications. Kinesis Data Firehose, totransform and collect data into destinations such as Amazon S3, Amazon Elasticsearch Service, and Amazon Redshift. Kinesis Data Analytics, to continuously analyze data using SQL or Java (viaApache Flink applications), for example to detect anomalies or for time series aggregation. Kinesis Video Streams, to simplify processing of media streams. At re:Invent 2018, we introduced in open previewAmazon Managed Streaming for Apache Kafka (MSK), a fully managed service that makes it easy to build and run applications that use Apache Kafka to process streaming data. I am excited to announce that Amazon MSK is generally available today! How it works Apache Kafka (Kafka) is an open-source platform that enables customers to capture streaming data like click stream events, transactions, IoT events, application and machine logs, and have applications that perform real-time analytics, run continuous transformations, and distribute this data to data lakes and databases in real time.You can use Kafka as a streaming data store to decouple applications producing streaming data (producers) from those consuming streaming data (consumers). While Kafka is a popular enterprise data streaming and messaging framework, it can be difficult to setup, scale, and manage in production.Amazon MSK takes care of these managing tasks and makes it easy to set up, configure, and run Kafka, along with Apache ZooKeeper, in an environment following best practices for high availability and security. Your MSK clusters always run within an Amazon VPC managed by the MSK service. Your MSK resources are made available to your own VPC, subnet, and security group through elastic network interfaces (ENIs) which will appear in your account, as described in the following architectural diagram: Customers can create a cluster in minutes, use AWS Identity and Access Management (IAM) to control cluster actions, authorize clients using TLS private certificate authorities fully managed by AWS Certificate Manager (ACM), encrypt data in-transit using TLS, and encrypt data at rest using AWS Key Management Service (KMS)encryption keys. Amazon MSK continuously monitors server health and automatically replaces servers when they fail, automates server patching, and operates highly available ZooKeeper nodes as a part of the service at no additional cost. Key Kafka performance metrics are published in the console and in Amazon CloudWatch. Amazon MSK is fully compatible with Kafka versions 1.1.1 and 2.1.0, so that you can continue to run your applications, use Kafkas admin tools, and and use Kafka compatible tools and frameworks without having to change your code. Based on our customer feedback during the open preview, Amazon MSK added may features such as: Encryption in-transit via TLS between clients and brokers, and between brokers Mutual TLS authentication using ACM private certificate authorities Support for Kafka version 2.1.0 99.9% availability SLA HIPAA eligible Cluster-wide storage scale up Integration with AWS CloudTrail for MSK API logging Cluster tagging and tag-based IAM policy application Defining custom, cluster-wide configurations for topics and brokers AWS CloudFormationsupport is coming in the next few weeks. Creating a cluster Lets create a cluster using the AWS management console. I give the cluster a name, select the VPC I want to use the cluster from, and the Kafka version. I then choose the Availability Zones (AZs) and the corresponding subnets to use in the VPC. In the next step,I select how many Kafka brokers to deploy in each AZ. More brokers allow you to scale the throughtput of a cluster by allocating partitions to different brokers. I can add tagsto search and filter my resources, apply IAM policies to the Amazon MSK API, and track my costs. For storage, I leave the default storage volume size per broker. I select to use encryption within the cluster and to allow both TLS and plaintext traffic between clients and brokers. For data at rest, I use the AWS-managed customer master key (CMK), but you can select a CMK in your account, using KMS, to have further control. Youcan use private TLS certificates to authenticate the identity of clients that connect to your cluster. This feature is usingPrivate Certificate Authorities (CA) from ACM. For now, I leave this option unchecked. In the advanced setting, I leave the default values. For example, I could have chosen here a different instance type for my brokers. Some of these settings can be updated using the AWS CLI. I create the cluster and monitor the status from the cluster summary, including the Amazon Resource Name (ARN) that I can use when interacting via CLI or SDKs. When the status is active, the client information section provides specific details to connect to the cluster, such as: The bootstrap servers I can use with Kafka tools to connect to the cluster. The Zookeper connect list of hosts and ports. I can get similar information using the AWS CLI: aws kafka list-clusters to see the ARNs of your clusters in a specific region aws kafka get-bootstrap-brokers --cluster-arn <ClusterArn> to get the Kafka bootstrap servers aws kafka describe-cluster --cluster-arn <ClusterArn> to see more details on the cluster, including the Zookeeper connect string Quick demo of using Kafka To start using Kafka, I create two EC2 instances in the same VPC, one will be a producer and one a consumer. To set them up as client machines, I download and extract theKafka toolsfrom the Apache website or any mirror.Kafka requires Java 8 to run, so I install Amazon Corretto 8. On the producer instance, in the Kafka directory, I create a topic to send data from the producer to the consumer: bin/kafka-topics.sh --create --zookeeper <ZookeeperConnectString> \\ --replication-factor 3 --partitions 1 --topic MyTopic Then I start a console-based producer: bin/kafka-console-producer.sh --broker-list <BootstrapBrokerString> \\ --topic MyTopic On the consumer instance, in the Kafka directory, I start a console-based consumer: bin/kafka-console-consumer.sh --bootstrap-server <BootstrapBrokerString> \\ --topic MyTopic --from-beginning Heres a recording of a quick demo where I create the topic and then send messages from a producer (top terminal) to a consumer of that topic (bottom terminal): Pricing and availability Pricing is per Kafka broker-hour and per provisioned storage-hour. There is no costfor the Zookeeper nodes used by your clusters.AWS data transfer rates apply for data transfer in and out of MSK. You will not be charged for data transfer within the cluster in a region, including data transfer between brokers and data transfer between brokers and ZooKeeper nodes. You can migrate your existing Kafka cluster to MSK using tools like MirrorMaker (that comes with open source Kafka) to replicate data from your clusters into a MSK cluster. Upstream compatibility is a core tenet of Amazon MSK. Our code changes to the Kafka platform are released back to open source. Amazon MSK is available in US East (N. Virginia), US East (Ohio), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Singapore), Asia Pacific (Sydney), EU (Frankfurt), EU (Ireland), EU (Paris), and EU (London). I look forward to see how are you going to use Amazon MSK to simplify building and migrating streaming applications to the cloud! Danilo ",
        "_version_": 1718527406788050944
      },
      {
        "story_id": [21244352],
        "story_author": ["zdw"],
        "story_descendants": [92],
        "story_score": [318],
        "story_time": ["2019-10-14T00:39:22Z"],
        "story_title": "How to fuck up software releases",
        "search": [
          "How to fuck up software releases",
          "https://drewdevault.com/2019/10/12/how-to-fuck-up-releases.html",
          "I manage releases for a bunch of free & open-source software. Just about every time I ship a release, I find a novel way to fuck it up. Enough of these fuck-ups have accumulated now that I wanted to share some of my mistakes and how I (try to) prevent them from happening twice. At first, I did everything manually. This is fine enough for stuff with simple release processes - stuff that basically amounts to tagging a commit, pushing it, and calling it a day. But even this gets tedious, and Id often make a mistake when picking the correct version number. So, I wrote a small script: semver. semver patch bumps the patch version, semver minor bumps the minor version, and semver major bumps the major version, based on semantic versioning. I got into the habit of using this script instead of making the tags manually. The next fuckup soon presented itself: when preparing the shortlog, I would often feed it the wrong commits, and the changelog would be messed up. So, I updated the script to run the appropriate shortlog command and pre-populate the annotated tag with it, launching the editor to adjust the changelog as necessary. Soon I wanted to apply this script to other projects, but not all of them used semantic versioning. I updated it to work for projects which just use major.minor versions as well. However, another problem arose: some projects have the version number specified in the Makefile or meson.build. I would frequently fuck this up in many creative ways: forgetting it entirely; updating it but not committing it; updating it and committing it, but tagging the wrong commit; etc. wlroots in particular was difficult because I also had to update the soversion, which had special requirements. To address these issues, I added a custom .git/_incr_version script which can add additional logic on a per-repo basis, and updated semver to call this script if present.1 Eventually, I went on vacation and shipped a release while I was there. The _incr_version script I had put into .git on my home workstation wasnt checked into version control and didnt come with me on vacation, leading to yet another fucked up release. I moved it from .git/_incr_version to contrib/_incr_version. I made the mistake, however, of leaving the old path in as a fallback, which meant that I never noticed that another projects script was still in .git until I went on another vacation and fucked up another release. Add a warning which detects if the script is at the old path Some of my projects dont use semantic versioning at all, but still have all of these other gotchas, so I added an option to just override the automatic version increment with a user-specified override. For a while, this worked well. But, inevitably, no matter how much I scripted away my mistakes I would always find a new and novel way of screwing up. The next one came when I shipped a release while on an Alpine Linux machine, which ships Busybox instead of GNU tools. Turns out Busybox gzip produces output which does not match the GNU output, which means the tarballs I signed locally differed from the ones generated by Github. Update the signing script to save the tarball to disk (previously, it lived in a pipe) and upload these alongside the releases2 Surely, there are no additional ways to fuck it up at this point. I must have every base covered, right? Wrong. Dead wrong. On the very next release I shipped, I mistakenly did everything from a feature branch, and shipped experimental, incomplete code in a stable release. Update the script to warn if the master branch isnt checked out Then, of course, another fuckup: I tagged a release without pulling first, and when I pushed, git happily rejected my branch and accepted the tag - shipping an outdated commit as the release. Update the script to git pull first I am doomed to creatively outsmart my tools in releases. If youd like to save yourself from some of the mistakes Ive made, you can find my semver script here. Each of these _incr_version scripts proved to have many bugs of their own, of course. Eli Schwartz of Arch Linux also sent a patch to Busybox which made their gzip implementation consistent with GNUs. Articles from blogs I read Generated by openring Summary of changes for October Hey everyone! This is the list of all the changes we've done to our projects and apps during the month of October. We'll also be reporting in our on position in the world, and on our future plans. Summary Of Changes donsol, ported our Famicom(65 via Hundred Rabbits November 3, 2021 Announcing the 2021 Go Developer Survey Please take the 2021 Go Developer Survey. We want to hear from you! via The Go Blog October 26, 2021 Willingness to look stupid People frequently1 think that I'm very stupid. I don't find this surprising, since I don't mind if other people think I'm stupid, which means that I don't adjust my behavior to avoid seeming stupid, which results in people thinking tha via Dan Luu October 21, 2021 ",
          "I keep a release TODO list and about 50% of every release, I fuck up and need to add something to the list. It's about 40 items long now (containing various terminal commands and activities), which includes making sure manual entries, documentation, the website, social media, dependencies, etc are handled correctly.",
          "Another \"fun\" one: typo in git tag names.<p>The rakudo project has monthly releases with tags like 2019.07. Once I mixed up the year and month in e release, so I created a tag 2018.06 instead of 2016.08 (don't remember the exact numbers), and pushed it.<p>I deleted the wrong tag as soon as I noticed, and pushed the correct one, but of course when the date of the typo'ed release month came around some years later, things blew up for everybody who still had the wrong tag in their local repo (which turned out to be quite some developers)."
        ],
        "story_type": ["Normal"],
        "url": "https://drewdevault.com/2019/10/12/how-to-fuck-up-releases.html",
        "comments.comment_id": [21244594, 21245419],
        "comments.comment_author": ["vortico", "perlgeek"],
        "comments.comment_descendants": [4, 3],
        "comments.comment_time": [
          "2019-10-14T01:33:35Z",
          "2019-10-14T05:12:29Z"
        ],
        "comments.comment_text": [
          "I keep a release TODO list and about 50% of every release, I fuck up and need to add something to the list. It's about 40 items long now (containing various terminal commands and activities), which includes making sure manual entries, documentation, the website, social media, dependencies, etc are handled correctly.",
          "Another \"fun\" one: typo in git tag names.<p>The rakudo project has monthly releases with tags like 2019.07. Once I mixed up the year and month in e release, so I created a tag 2018.06 instead of 2016.08 (don't remember the exact numbers), and pushed it.<p>I deleted the wrong tag as soon as I noticed, and pushed the correct one, but of course when the date of the typo'ed release month came around some years later, things blew up for everybody who still had the wrong tag in their local repo (which turned out to be quite some developers)."
        ],
        "id": "34d2b7e3-5364-4cf9-91d8-41c1104aa2b0",
        "url_text": "I manage releases for a bunch of free & open-source software. Just about every time I ship a release, I find a novel way to fuck it up. Enough of these fuck-ups have accumulated now that I wanted to share some of my mistakes and how I (try to) prevent them from happening twice. At first, I did everything manually. This is fine enough for stuff with simple release processes - stuff that basically amounts to tagging a commit, pushing it, and calling it a day. But even this gets tedious, and Id often make a mistake when picking the correct version number. So, I wrote a small script: semver. semver patch bumps the patch version, semver minor bumps the minor version, and semver major bumps the major version, based on semantic versioning. I got into the habit of using this script instead of making the tags manually. The next fuckup soon presented itself: when preparing the shortlog, I would often feed it the wrong commits, and the changelog would be messed up. So, I updated the script to run the appropriate shortlog command and pre-populate the annotated tag with it, launching the editor to adjust the changelog as necessary. Soon I wanted to apply this script to other projects, but not all of them used semantic versioning. I updated it to work for projects which just use major.minor versions as well. However, another problem arose: some projects have the version number specified in the Makefile or meson.build. I would frequently fuck this up in many creative ways: forgetting it entirely; updating it but not committing it; updating it and committing it, but tagging the wrong commit; etc. wlroots in particular was difficult because I also had to update the soversion, which had special requirements. To address these issues, I added a custom .git/_incr_version script which can add additional logic on a per-repo basis, and updated semver to call this script if present.1 Eventually, I went on vacation and shipped a release while I was there. The _incr_version script I had put into .git on my home workstation wasnt checked into version control and didnt come with me on vacation, leading to yet another fucked up release. I moved it from .git/_incr_version to contrib/_incr_version. I made the mistake, however, of leaving the old path in as a fallback, which meant that I never noticed that another projects script was still in .git until I went on another vacation and fucked up another release. Add a warning which detects if the script is at the old path Some of my projects dont use semantic versioning at all, but still have all of these other gotchas, so I added an option to just override the automatic version increment with a user-specified override. For a while, this worked well. But, inevitably, no matter how much I scripted away my mistakes I would always find a new and novel way of screwing up. The next one came when I shipped a release while on an Alpine Linux machine, which ships Busybox instead of GNU tools. Turns out Busybox gzip produces output which does not match the GNU output, which means the tarballs I signed locally differed from the ones generated by Github. Update the signing script to save the tarball to disk (previously, it lived in a pipe) and upload these alongside the releases2 Surely, there are no additional ways to fuck it up at this point. I must have every base covered, right? Wrong. Dead wrong. On the very next release I shipped, I mistakenly did everything from a feature branch, and shipped experimental, incomplete code in a stable release. Update the script to warn if the master branch isnt checked out Then, of course, another fuckup: I tagged a release without pulling first, and when I pushed, git happily rejected my branch and accepted the tag - shipping an outdated commit as the release. Update the script to git pull first I am doomed to creatively outsmart my tools in releases. If youd like to save yourself from some of the mistakes Ive made, you can find my semver script here. Each of these _incr_version scripts proved to have many bugs of their own, of course. Eli Schwartz of Arch Linux also sent a patch to Busybox which made their gzip implementation consistent with GNUs. Articles from blogs I read Generated by openring Summary of changes for October Hey everyone! This is the list of all the changes we've done to our projects and apps during the month of October. We'll also be reporting in our on position in the world, and on our future plans. Summary Of Changes donsol, ported our Famicom(65 via Hundred Rabbits November 3, 2021 Announcing the 2021 Go Developer Survey Please take the 2021 Go Developer Survey. We want to hear from you! via The Go Blog October 26, 2021 Willingness to look stupid People frequently1 think that I'm very stupid. I don't find this surprising, since I don't mind if other people think I'm stupid, which means that I don't adjust my behavior to avoid seeming stupid, which results in people thinking tha via Dan Luu October 21, 2021 ",
        "_version_": 1718527431192608768
      },
      {
        "story_id": [19100986],
        "story_author": ["Boulth"],
        "story_descendants": [79],
        "story_score": [289],
        "story_time": ["2019-02-06T22:55:16Z"],
        "story_title": "Sr.ht becomes Sourcehut",
        "search": [
          "Sr.ht becomes Sourcehut",
          "https://sourcehut.org/",
          "Welcome to sourcehut! This suite of open source tools is the software development platform you've been waiting for. We've taken the wisdom of the most successful open-source communities and turned it into a platform of efficient engineering tools. Absolutely no tracking or advertising All features work without JavaScript Many features work without an account The fastest & lightest software forge 100% free and open source software Sourcehut is currently available as a public alpha. What should I expect? \"Small internet\" protocols? The Plan 9 renaissance? Esoteric programming languages for music creation, and novel smartphone operating systems? These projects and more are waiting to be found on the sourcehut project index. Browse projects Hosted git repositories Public, private, and \"unlisted\" repositories Fine grained access control, including access for users without accounts First-class Mercurial support also available We've completely migrated our repo hosting, both git and Mercurial, to SourceHut. The speed, functionality, integrations, and minimal-yet-friendly UI makes it easy to use and work with. Peter Sanchez, Netlandish Inc. Powerful continuous integration Runs fully virtualised builds on various Linux distros and BSDs Submit ad-hoc jobs without pushing to your repository Post-build triggers for email, webhooks, etc Log in with SSH after build failures to investigate further This CI experience is leagues ahead of all others. Resubmitting builds and SSH'ing in is saving me multiple hours. Andrew Kelley, author of the Zig programming language Mailing lists & code review tools Patch review tools on the web Threaded, searchable mail archives Tools for working with third party mailing lists Powered by git send-email SourceHut mailing lists are the best thing since the invention of reviewing patches. Martijn Braam, postmarketOS developer Focused ticket tracking Actionable tasks only no discussions, questions, or duplicates Private bug reports and bug trackers for security issues Participation via email, with or without an account I think it is really convenient that you can send a plaintext email with your bug report, whether or not you have an account. Cadence Ember, author of Bibliogram Sophisticated account management & security PGP encrypted and signed emails from each service Two-factor authentication with TOTP Detailed audit logs of account activity Fine-grained third-party OAuth access controls I really appreciate the option to get encrypted mail with a PGP key that I provide why don't more companies have this?! Cadence Ember Markdown- and git-driven wikis Use git to version control and manage your wiki Use any organizational hierarchy you like, a flat wiki is not imposed Hosts the detailed sourcehut manual And more! Integrations with third-party services via dispatch.sr.ht Ad-hoc source code hosting via paste.sr.ht Static web hosting via srht.site ",
          "As of posting, the info on this site is very lean, but going forward sourcehut seems to be the name for the software suite of the service 'sr.ht'. The linkage is implied by the quote on the site that \"<i>sr.ht is a hosted instance of sourcehut provided for your convenience</i>\".<p>The impetus behind the branding clarification seems to be this HN thread [1]. For more history, see the the debut announcement [2] and its corresponding HN thread [3].<p>[1] <a href=\"https://news.ycombinator.com/item?id=18929709#18930413\" rel=\"nofollow\">https://news.ycombinator.com/item?id=18929709#18930413</a> [2] <a href=\"https://drewdevault.com/2018/11/15/sr.ht-general-availability.html\" rel=\"nofollow\">https://drewdevault.com/2018/11/15/sr.ht-general-availabilit...</a> [3] <a href=\"https://news.ycombinator.com/item?id=18458908\" rel=\"nofollow\">https://news.ycombinator.com/item?id=18458908</a>",
          "Mercurial support, fuck yeah:<p><a href=\"https://hg.sr.ht/\" rel=\"nofollow\">https://hg.sr.ht/</a><p>This is the killer feature for me.<p>Why Mercurial?<p>Here’s a list of Mercurial features that I think are really cool:<p>Revsets – a domain-specific language for querying your commits<p>Templates – a domain-specific language for altering the output of almost every command. Putting together these two you can do things like this: <a href=\"http://jordi.inversethought.com/blog/customising-mercurial-like-a-pro/\" rel=\"nofollow\">http://jordi.inversethought.com/blog/customising-mercurial-l...</a><p>Evolution – a distributed and safe way to share rewrites (think automatically recovering from upstream rebase without any git reset --hard and no git push --force).<p>Absorb – automatically amends an entire stack of WIP commits at once by picking the right diffs from your working directory based on which commits’ contexts they fit best.<p>Curses interface for hunk-picking – a unified interface for splitting diffs for any purpose: whether to revert working-directory changes, write a new commit, uncommit parts of a commit, or amend a commit with more diffs. Just add --interactive to any of those commands and start picking hunks!<p>A fairly rich built-in web interface – hg serve and point your browser to <a href=\"http://localhost:8000\" rel=\"nofollow\">http://localhost:8000</a> and you’re good to go.<p>Lots of support for monorepos – indeed, this is the main reason that Google, Facebook, and Mozilla are all pouring work into hg and are using it.<p>A consistent UI – this one is more subjective but often-touted feature of Mercurial. If a command accepts a revision/hash/tag/bookmark; it always uses the -r/--rev flag for it (and you can also always use a revset for any command that accepts a revision). If a command allows hunk selection, it always uses the -i/--interactive flag for it. The help for every command fits in about a screenful.<p>Give it a try! Mercurial is neat!"
        ],
        "story_type": ["Normal"],
        "url": "https://sourcehut.org/",
        "url_text": "Welcome to sourcehut! This suite of open source tools is the software development platform you've been waiting for. We've taken the wisdom of the most successful open-source communities and turned it into a platform of efficient engineering tools. Absolutely no tracking or advertising All features work without JavaScript Many features work without an account The fastest & lightest software forge 100% free and open source software Sourcehut is currently available as a public alpha. What should I expect? \"Small internet\" protocols? The Plan 9 renaissance? Esoteric programming languages for music creation, and novel smartphone operating systems? These projects and more are waiting to be found on the sourcehut project index. Browse projects Hosted git repositories Public, private, and \"unlisted\" repositories Fine grained access control, including access for users without accounts First-class Mercurial support also available We've completely migrated our repo hosting, both git and Mercurial, to SourceHut. The speed, functionality, integrations, and minimal-yet-friendly UI makes it easy to use and work with. Peter Sanchez, Netlandish Inc. Powerful continuous integration Runs fully virtualised builds on various Linux distros and BSDs Submit ad-hoc jobs without pushing to your repository Post-build triggers for email, webhooks, etc Log in with SSH after build failures to investigate further This CI experience is leagues ahead of all others. Resubmitting builds and SSH'ing in is saving me multiple hours. Andrew Kelley, author of the Zig programming language Mailing lists & code review tools Patch review tools on the web Threaded, searchable mail archives Tools for working with third party mailing lists Powered by git send-email SourceHut mailing lists are the best thing since the invention of reviewing patches. Martijn Braam, postmarketOS developer Focused ticket tracking Actionable tasks only no discussions, questions, or duplicates Private bug reports and bug trackers for security issues Participation via email, with or without an account I think it is really convenient that you can send a plaintext email with your bug report, whether or not you have an account. Cadence Ember, author of Bibliogram Sophisticated account management & security PGP encrypted and signed emails from each service Two-factor authentication with TOTP Detailed audit logs of account activity Fine-grained third-party OAuth access controls I really appreciate the option to get encrypted mail with a PGP key that I provide why don't more companies have this?! Cadence Ember Markdown- and git-driven wikis Use git to version control and manage your wiki Use any organizational hierarchy you like, a flat wiki is not imposed Hosts the detailed sourcehut manual And more! Integrations with third-party services via dispatch.sr.ht Ad-hoc source code hosting via paste.sr.ht Static web hosting via srht.site ",
        "comments.comment_id": [19101220, 19101723],
        "comments.comment_author": ["niftich", "jordigh"],
        "comments.comment_descendants": [3, 6],
        "comments.comment_time": [
          "2019-02-06T23:23:36Z",
          "2019-02-07T00:23:06Z"
        ],
        "comments.comment_text": [
          "As of posting, the info on this site is very lean, but going forward sourcehut seems to be the name for the software suite of the service 'sr.ht'. The linkage is implied by the quote on the site that \"<i>sr.ht is a hosted instance of sourcehut provided for your convenience</i>\".<p>The impetus behind the branding clarification seems to be this HN thread [1]. For more history, see the the debut announcement [2] and its corresponding HN thread [3].<p>[1] <a href=\"https://news.ycombinator.com/item?id=18929709#18930413\" rel=\"nofollow\">https://news.ycombinator.com/item?id=18929709#18930413</a> [2] <a href=\"https://drewdevault.com/2018/11/15/sr.ht-general-availability.html\" rel=\"nofollow\">https://drewdevault.com/2018/11/15/sr.ht-general-availabilit...</a> [3] <a href=\"https://news.ycombinator.com/item?id=18458908\" rel=\"nofollow\">https://news.ycombinator.com/item?id=18458908</a>",
          "Mercurial support, fuck yeah:<p><a href=\"https://hg.sr.ht/\" rel=\"nofollow\">https://hg.sr.ht/</a><p>This is the killer feature for me.<p>Why Mercurial?<p>Here’s a list of Mercurial features that I think are really cool:<p>Revsets – a domain-specific language for querying your commits<p>Templates – a domain-specific language for altering the output of almost every command. Putting together these two you can do things like this: <a href=\"http://jordi.inversethought.com/blog/customising-mercurial-like-a-pro/\" rel=\"nofollow\">http://jordi.inversethought.com/blog/customising-mercurial-l...</a><p>Evolution – a distributed and safe way to share rewrites (think automatically recovering from upstream rebase without any git reset --hard and no git push --force).<p>Absorb – automatically amends an entire stack of WIP commits at once by picking the right diffs from your working directory based on which commits’ contexts they fit best.<p>Curses interface for hunk-picking – a unified interface for splitting diffs for any purpose: whether to revert working-directory changes, write a new commit, uncommit parts of a commit, or amend a commit with more diffs. Just add --interactive to any of those commands and start picking hunks!<p>A fairly rich built-in web interface – hg serve and point your browser to <a href=\"http://localhost:8000\" rel=\"nofollow\">http://localhost:8000</a> and you’re good to go.<p>Lots of support for monorepos – indeed, this is the main reason that Google, Facebook, and Mozilla are all pouring work into hg and are using it.<p>A consistent UI – this one is more subjective but often-touted feature of Mercurial. If a command accepts a revision/hash/tag/bookmark; it always uses the -r/--rev flag for it (and you can also always use a revset for any command that accepts a revision). If a command allows hunk selection, it always uses the -i/--interactive flag for it. The help for every command fits in about a screenful.<p>Give it a try! Mercurial is neat!"
        ],
        "id": "ee1f8c9e-6620-49a0-b4b2-c2de7011d468",
        "_version_": 1718527385189482496
      },
      {
        "story_id": [19367379],
        "story_author": ["kanishkdudeja"],
        "story_descendants": [3],
        "story_score": [10],
        "story_time": ["2019-03-12T12:15:08Z"],
        "story_title": "Manifesto for Async Software Development",
        "search": [
          "Manifesto for Async Software Development",
          "http://asyncmanifesto.org/",
          "It's time for a 21st century successor to Agile And its most popular incarnation, Scrum After many years of developing software using Agile methodologies like Scrum, the time has come to value: Modern tools and flexible work environments over meetings and office hours Flexibility in prioritization over detailed planning Comprehensive documentation over tribal knowledge That is, while there is value in the latter items, there is more value in the former items. Principles of Async Software Development Modern tools Whether you're a fan of GitLab, GitHub, Bitbucket, or something else, good async collaboration tools like these all share a few important things in common: Each deeply integrates version control with issue tracking. Each offers rich prioritization and assignment features. Each wraps all that up into a slick user experience that anyone on the team can use, including nontechnical people. Meetings only as a last resort Meetings are very costly to your business. That's because creative professionals need long stretches of uninterrupted time to get meaningful work done. Thus, async communication should be your default, because it prevents context switching. Forget the planning meetings. Product owners can replace planning meetings by simply filing issues in the issue tracker, assigning priority, assigning them to people, and setting a release milestone. People will know what to work on by simply working on whatever the highest priority issue is in their queue. Skip the daily standups. Product owners can ascertain status by reading the comment threads of issues currently being worked on and posting questions as needed. Retire the backlog grooming sessions. Product owners should own the issue queue and frequently reassess priority on their own. They should loop in other people on an as-needed basis for advice. Call a meeting only when all other channels of communication aren't suitable for a specific issue. Flexible work environments Since modern tools and async communication makes 1950s-style meetings-centric office cultures obsolete, we can enjoy much more flexible work environments now. Adopt a hotelling policy at your office. Don't assign desks to anyone by default. Anyone who requests an assigned desk should get to choose whether it's a private office or in a communal space. Discourage one-size-fits-all space management. Some people work better in crowds, others work better at home. Let people decide for themselves. Document everything The better documented your workflow, the less your workers will need to interrupt each other to seek out tribal knowledge. A question answered in a FAQ or some other form of async communication is much better than one answered by a shoulder tap. ",
          "> A question answered in a FAQ or some other form of async communication is much better than one answered by a shoulder tap.<p>This doesn't scale. Eventually, you get to a point where you don't know where to look for context and context ends up getting spread around multiple areas. Complexity cannot be wrapped up in a wiki obviously enough, that you can fully grasp every possible issue without comprehending the whole wiki.",
          "This is entirely from an engineer's perspective. The reason agile and scrum are so popular is because it works for managers.<p>Managers need to herd all the cats, keep them on task, and give them continuous deadlines to report status in order to keep a low level fire burning under their employees' butts.<p>For open source software projects and hobby projects, this type of \"asynchronous\" development is perfectly fine.<p>For corporate environments, the problem is that employees are not necessarily intrinsically motivated and also need a way to communicate regularly up a chain of authority. This is the point of meetings.<p>Furthermore, managers generally succeed based on social intelligence. By regularly keeping tabs on employees using real meetings, they can assess the emotional state of the team... such as the morale or whether anyone is struggling or going through personal problems that they are reluctant to broadcast."
        ],
        "story_type": ["Normal"],
        "url": "http://asyncmanifesto.org/",
        "url_text": "It's time for a 21st century successor to Agile And its most popular incarnation, Scrum After many years of developing software using Agile methodologies like Scrum, the time has come to value: Modern tools and flexible work environments over meetings and office hours Flexibility in prioritization over detailed planning Comprehensive documentation over tribal knowledge That is, while there is value in the latter items, there is more value in the former items. Principles of Async Software Development Modern tools Whether you're a fan of GitLab, GitHub, Bitbucket, or something else, good async collaboration tools like these all share a few important things in common: Each deeply integrates version control with issue tracking. Each offers rich prioritization and assignment features. Each wraps all that up into a slick user experience that anyone on the team can use, including nontechnical people. Meetings only as a last resort Meetings are very costly to your business. That's because creative professionals need long stretches of uninterrupted time to get meaningful work done. Thus, async communication should be your default, because it prevents context switching. Forget the planning meetings. Product owners can replace planning meetings by simply filing issues in the issue tracker, assigning priority, assigning them to people, and setting a release milestone. People will know what to work on by simply working on whatever the highest priority issue is in their queue. Skip the daily standups. Product owners can ascertain status by reading the comment threads of issues currently being worked on and posting questions as needed. Retire the backlog grooming sessions. Product owners should own the issue queue and frequently reassess priority on their own. They should loop in other people on an as-needed basis for advice. Call a meeting only when all other channels of communication aren't suitable for a specific issue. Flexible work environments Since modern tools and async communication makes 1950s-style meetings-centric office cultures obsolete, we can enjoy much more flexible work environments now. Adopt a hotelling policy at your office. Don't assign desks to anyone by default. Anyone who requests an assigned desk should get to choose whether it's a private office or in a communal space. Discourage one-size-fits-all space management. Some people work better in crowds, others work better at home. Let people decide for themselves. Document everything The better documented your workflow, the less your workers will need to interrupt each other to seek out tribal knowledge. A question answered in a FAQ or some other form of async communication is much better than one answered by a shoulder tap. ",
        "comments.comment_id": [19371539, 19373363],
        "comments.comment_author": ["reallydude", "nwah1"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-03-12T18:41:55Z",
          "2019-03-12T21:43:42Z"
        ],
        "comments.comment_text": [
          "> A question answered in a FAQ or some other form of async communication is much better than one answered by a shoulder tap.<p>This doesn't scale. Eventually, you get to a point where you don't know where to look for context and context ends up getting spread around multiple areas. Complexity cannot be wrapped up in a wiki obviously enough, that you can fully grasp every possible issue without comprehending the whole wiki.",
          "This is entirely from an engineer's perspective. The reason agile and scrum are so popular is because it works for managers.<p>Managers need to herd all the cats, keep them on task, and give them continuous deadlines to report status in order to keep a low level fire burning under their employees' butts.<p>For open source software projects and hobby projects, this type of \"asynchronous\" development is perfectly fine.<p>For corporate environments, the problem is that employees are not necessarily intrinsically motivated and also need a way to communicate regularly up a chain of authority. This is the point of meetings.<p>Furthermore, managers generally succeed based on social intelligence. By regularly keeping tabs on employees using real meetings, they can assess the emotional state of the team... such as the morale or whether anyone is struggling or going through personal problems that they are reluctant to broadcast."
        ],
        "id": "547f9d3d-5c75-4886-bf4f-8d2d523d1208",
        "_version_": 1718527393754251264
      },
      {
        "story_id": [19654603],
        "story_author": ["runningmike"],
        "story_descendants": [6],
        "story_score": [7],
        "story_time": ["2019-04-13T17:58:44Z"],
        "story_title": "The holy grail for solving IT problems?",
        "search": [
          "The holy grail for solving IT problems?",
          "https://nocomplexity.com/nocode-solutions/",
          "Within the field of business IT problems it is hard to resit the temptation to use a tool that promise to solve all your complex IT problems. Despite the progress and technology innovation in the last 25 years some IT related problems are still very hard to solve. To name of few typical business IT problems: Too high development and operational cost.Inflexible IT solutions when business needs change.Vendor lock-ins.Security, privacy and safety risks.Availability issues.Disasters for business operations due to the fact that minimum quality standards are not met. E.g. quality aspects like data integrity, configuration management or restoring systems still cause real disasters for business continuity. The track record for successful large IT projects is still very miserable, also in 2019. So every sensible company is still fearful for starting a new big IT project. Since no one is immune for marketing you should be suspicious solving your real complex problems using a new holy grail IT solution. Good marketing has an emphasis that there is something real new on the table. A current trend is using low-code or no-code solutions to solve all complex IT problems that will occur during and after large IT projects are finished. Since developing IT solutions involves high cost and takes often more time than estimate the new holy grail solutions claim to solve this productivity and cost problem. But this is not all: the new low-code solutions also claim to solve all typical IT life cycle management problems. I love new solutions that solve real problems. New IT solutions can bring real advantages for business at large. But is the holy grail promised by the new low-code tools really true and proven? The new holy grail solutions can be categorized as the new era of MDA solution tools. Model driven architecture (MDA) is a software design approach for the development of software systems. Model driven architecture provides models written in well-defined design language. The models consist of multiple components including model, transformation and meta-models. Tools that use a MDA approach claim to generate working software out of a model. So no more coding, just model your problem. So the promise is not to create software manual anymore, but the solution will generate software based on the model directly. So bye bye high skilled traditional software engineers? MDA is not new. From 1980 several names have been given to the MDA idea of generating software based on a model, e.g.: Low code toolingNo code toolingModel Driven Engineering (MDE)Model Driven Design (MDD)4GL tools The promises made by MDA software solutions, no-code solutions or low-code software vendors are high. So you should be aware of key aspects when evaluating if a MDA software solution solves your problem. Key factors to evaluate MDA like tools are: MDA is a concept with strong underlying believes of a problem situation it tries to solve. This means that every MDA solution tool is biased and does not work for ever problem situation. So always determine first your exact problem situation and the problems or complexity problems you want to solve before evaluating solutions. An software tool alone will never solve your organizational problems that make software development costly in your organization. The productivity gain promised by MDA tools is not always met. When trying to solve complex problems with a MDA tool-based approach, the productivity gain is at best doubtful. Most productivity problems are not related to the software design, delivery tools or process. The hard and complex problems that impact performance in large business IT projects are related with having no general agreement and perception of the goals, requirements and principles for the IT system that must be created to solve some fuzzy problem definition.MDA is has a strong focus on initial productivity gain. But a continuous fast changing business context with changing requirements requires an approach and toolset that is suited for giving long term productivity benefits.Personal that is able to analyse, model and create and update a model in the required model language for the chosen low-code tool is not widely available. Most business problems are not modelled and solved using the BPMN standard (Business Process Model and Notation). Software engineers are good at creating software and some are also great in developing software architectures. Creating BPM models or other type of models to address business problems are no core competence of software engineers. And remember: Adjusting parameters using check-boxes or drawing lines on a canvas to outline a process is just engineering. Most real business people will never use or have the time or knowledge for making changing within business administration interfaces. Configuration using no-code tools is most of the time still programming, but with painful limitations.MDA tools and products developed have difficulties to keep up with the continuously changing IT technology world. Browser technology and mobile technology are constantly changing. However this is not always an evolutionary path.General DSLs (Domain Specific Language) used within MDA tools rarely bring the productivity gain. DSLs like BPMN, UML, SYSML are known to be complex to model a problem fast and correct for software generation.Custom defined DSLs used by several MDA tools are not open so you are locked in by the tool and the vendor.Full model testing, so business process testing using the generated software is often lacking. In order to prevent surprises automated correctness testing of the model is a severe challenge for MDA tools.Nonfunctional requirements like performance, security and stability are hard to incorporate within a model or generated code.Discipline in relation with your business culture is crucial for long term success with MDA tools. Simple changes on data or model parameters require the tool process to be followed exactly. Every step in the way the tool vendor has implemented it. So you need to follow the rules of the tools to implement changes correctly.MDA tools are not strong on versioning and dealing with multiple parallel development tracks and teams simultaneously. Most tools are in fact based on a big design upfront paradigm, like an overall data model. Versioning on models, meta data and data of all created and generated software assets is poorly supported, if supported at all. Common practices seen in mature agile software development tools using micro services paradigms and advanced distributed version control systems are often lacking in the new low-code MDA family of tools. In large companies it is not uncommon to encounter models with hundreds (or even thousands) of entities / classes. That kind of models not only doesnt help with developing software faster and more cost-efficiently but even has an adverse effect. Simple solutions for complex business IT problems rarely exist. Within 2019 creating complex software always means that minimum quality levels for e.g. security and privacy are mandatory. This is not simple at all. Creating a simple solution means you have to do a lot of difficult and inconvenient work. E.g. solid and proven business and IT architectures and designs are needed. For some problem areas you should create software to generate software to solve the problem needed. E.g. creating software to test a new part of an airplane control system. There is definitely a category of business problems that is well suited for using new latest MDA category of no-code solutions. Innovation within the field of business IT and the way business IT software is created is always needed. The latest family of MDA no-code and low-code tools do trigger innovation processes. E.g. a new tool will force you to look at your problem context from a different perspective. But never fall in love with an IT solution if you do not fully understand the problem and the problem context you want to solve. The productivity loss of solving business IT problems is seldom directly related to your current software engineering tools or process. Software is binary. Only a problem that is well stated and constant can be solved efficient using traditional software development tools. Else use or try new machine learning techniques. Business problems where organizational and human factors have large impact on the problem situation are by nature harder to solve. The new family of MDA tools will not change this. ",
          "Am I having a stroke? This article has a lot of words but doesn't seem to have any actual meaning. A parody about business buzzwords?",
          "The holy grail is no complexity.<p>It's definitely light on details and the Object Management Group was and is def not the holy grail for solving IT problems."
        ],
        "story_type": ["Normal"],
        "url": "https://nocomplexity.com/nocode-solutions/",
        "comments.comment_id": [19655539, 19657548],
        "comments.comment_author": ["mpoteat", "ekovarski"],
        "comments.comment_descendants": [3, 0],
        "comments.comment_time": [
          "2019-04-13T20:30:39Z",
          "2019-04-14T04:02:00Z"
        ],
        "comments.comment_text": [
          "Am I having a stroke? This article has a lot of words but doesn't seem to have any actual meaning. A parody about business buzzwords?",
          "The holy grail is no complexity.<p>It's definitely light on details and the Object Management Group was and is def not the holy grail for solving IT problems."
        ],
        "id": "414c19ba-eba6-4da5-b811-d1a0ea752739",
        "url_text": "Within the field of business IT problems it is hard to resit the temptation to use a tool that promise to solve all your complex IT problems. Despite the progress and technology innovation in the last 25 years some IT related problems are still very hard to solve. To name of few typical business IT problems: Too high development and operational cost.Inflexible IT solutions when business needs change.Vendor lock-ins.Security, privacy and safety risks.Availability issues.Disasters for business operations due to the fact that minimum quality standards are not met. E.g. quality aspects like data integrity, configuration management or restoring systems still cause real disasters for business continuity. The track record for successful large IT projects is still very miserable, also in 2019. So every sensible company is still fearful for starting a new big IT project. Since no one is immune for marketing you should be suspicious solving your real complex problems using a new holy grail IT solution. Good marketing has an emphasis that there is something real new on the table. A current trend is using low-code or no-code solutions to solve all complex IT problems that will occur during and after large IT projects are finished. Since developing IT solutions involves high cost and takes often more time than estimate the new holy grail solutions claim to solve this productivity and cost problem. But this is not all: the new low-code solutions also claim to solve all typical IT life cycle management problems. I love new solutions that solve real problems. New IT solutions can bring real advantages for business at large. But is the holy grail promised by the new low-code tools really true and proven? The new holy grail solutions can be categorized as the new era of MDA solution tools. Model driven architecture (MDA) is a software design approach for the development of software systems. Model driven architecture provides models written in well-defined design language. The models consist of multiple components including model, transformation and meta-models. Tools that use a MDA approach claim to generate working software out of a model. So no more coding, just model your problem. So the promise is not to create software manual anymore, but the solution will generate software based on the model directly. So bye bye high skilled traditional software engineers? MDA is not new. From 1980 several names have been given to the MDA idea of generating software based on a model, e.g.: Low code toolingNo code toolingModel Driven Engineering (MDE)Model Driven Design (MDD)4GL tools The promises made by MDA software solutions, no-code solutions or low-code software vendors are high. So you should be aware of key aspects when evaluating if a MDA software solution solves your problem. Key factors to evaluate MDA like tools are: MDA is a concept with strong underlying believes of a problem situation it tries to solve. This means that every MDA solution tool is biased and does not work for ever problem situation. So always determine first your exact problem situation and the problems or complexity problems you want to solve before evaluating solutions. An software tool alone will never solve your organizational problems that make software development costly in your organization. The productivity gain promised by MDA tools is not always met. When trying to solve complex problems with a MDA tool-based approach, the productivity gain is at best doubtful. Most productivity problems are not related to the software design, delivery tools or process. The hard and complex problems that impact performance in large business IT projects are related with having no general agreement and perception of the goals, requirements and principles for the IT system that must be created to solve some fuzzy problem definition.MDA is has a strong focus on initial productivity gain. But a continuous fast changing business context with changing requirements requires an approach and toolset that is suited for giving long term productivity benefits.Personal that is able to analyse, model and create and update a model in the required model language for the chosen low-code tool is not widely available. Most business problems are not modelled and solved using the BPMN standard (Business Process Model and Notation). Software engineers are good at creating software and some are also great in developing software architectures. Creating BPM models or other type of models to address business problems are no core competence of software engineers. And remember: Adjusting parameters using check-boxes or drawing lines on a canvas to outline a process is just engineering. Most real business people will never use or have the time or knowledge for making changing within business administration interfaces. Configuration using no-code tools is most of the time still programming, but with painful limitations.MDA tools and products developed have difficulties to keep up with the continuously changing IT technology world. Browser technology and mobile technology are constantly changing. However this is not always an evolutionary path.General DSLs (Domain Specific Language) used within MDA tools rarely bring the productivity gain. DSLs like BPMN, UML, SYSML are known to be complex to model a problem fast and correct for software generation.Custom defined DSLs used by several MDA tools are not open so you are locked in by the tool and the vendor.Full model testing, so business process testing using the generated software is often lacking. In order to prevent surprises automated correctness testing of the model is a severe challenge for MDA tools.Nonfunctional requirements like performance, security and stability are hard to incorporate within a model or generated code.Discipline in relation with your business culture is crucial for long term success with MDA tools. Simple changes on data or model parameters require the tool process to be followed exactly. Every step in the way the tool vendor has implemented it. So you need to follow the rules of the tools to implement changes correctly.MDA tools are not strong on versioning and dealing with multiple parallel development tracks and teams simultaneously. Most tools are in fact based on a big design upfront paradigm, like an overall data model. Versioning on models, meta data and data of all created and generated software assets is poorly supported, if supported at all. Common practices seen in mature agile software development tools using micro services paradigms and advanced distributed version control systems are often lacking in the new low-code MDA family of tools. In large companies it is not uncommon to encounter models with hundreds (or even thousands) of entities / classes. That kind of models not only doesnt help with developing software faster and more cost-efficiently but even has an adverse effect. Simple solutions for complex business IT problems rarely exist. Within 2019 creating complex software always means that minimum quality levels for e.g. security and privacy are mandatory. This is not simple at all. Creating a simple solution means you have to do a lot of difficult and inconvenient work. E.g. solid and proven business and IT architectures and designs are needed. For some problem areas you should create software to generate software to solve the problem needed. E.g. creating software to test a new part of an airplane control system. There is definitely a category of business problems that is well suited for using new latest MDA category of no-code solutions. Innovation within the field of business IT and the way business IT software is created is always needed. The latest family of MDA no-code and low-code tools do trigger innovation processes. E.g. a new tool will force you to look at your problem context from a different perspective. But never fall in love with an IT solution if you do not fully understand the problem and the problem context you want to solve. The productivity loss of solving business IT problems is seldom directly related to your current software engineering tools or process. Software is binary. Only a problem that is well stated and constant can be solved efficient using traditional software development tools. Else use or try new machine learning techniques. Business problems where organizational and human factors have large impact on the problem situation are by nature harder to solve. The new family of MDA tools will not change this. ",
        "_version_": 1718527399205797889
      },
      {
        "story_id": [21766560],
        "story_author": ["jmsflknr"],
        "story_descendants": [3],
        "story_score": [43],
        "story_time": ["2019-12-11T20:38:54Z"],
        "story_title": "Apple Used the DMCA to Take Down a Tweet Containing an iPhone Encryption Key",
        "search": [
          "Apple Used the DMCA to Take Down a Tweet Containing an iPhone Encryption Key",
          "https://www.vice.com/en_us/article/pkeeay/apple-dmca-take-down-tweet-containing-an-iphone-encryption-key",
          "Image: Sean Gallup/Getty ImagesSecurity researchers are accusing Apple of abusing the Digital Millennium Copyright Act (DMCA) to take down a viral tweet and several Reddit posts that discuss techniques and tools to hack iPhones.On Sunday, a security researcher who focuses on iOS and goes by the name Siguza posted a tweet containing what appears to be an encryption key that could be used to reverse engineer the Secure Enclave Processor, the part of the iPhone that handles data encryption and stores other sensitive data.Two days later, a law firm that has worked for Apple in the past sent a DMCA Takedown Notice to Twitter, asking for the tweet to be removed. The company complied, and the tweet became unavailable until today, when it reappeared. In a tweet, Siguza said that the DMCA claim was retracted.Apple confirmed that it sent the original DMCA takedown request. The company said that it retracted the request but Twitter had already complied with it and had taken the tweet down. Apple then asked Twitter to put the tweet back online.At the same time, Reddit received several DMCA takedown requests for posts on r/jailbreak, a popular subreddit where iPhone security researchers and hackers discuss techniques to jailbreak Apple devices, according to the subreddits moderators.Admins have not reached out to us in regards to these removals. We have no idea who is submitting these copyright claims, one moderator wrote.Twitter and Reddit did not immediately respond to emails asking for comment. Motherboard was not able to verify who sent the DMCA takedown requests to Reddit.iPhone security researchers and jailbreakers see these actions as Apple trying to clamp down on the jailbreaking community. Some in the community have questioned whether an encryption key, or posts linking to jailbreaking tools, are subject to copyright at all.In September, an independent iPhone security researcher that goes by the name axi0mX released Checkm8, a hacking tool that allows anyone to remove restrictions and jailbreak iOS devices as recent as the 2017 iPhone X. Weeks later, other researchers released checkra1n, a full-fledged jailbreak for devices running iOS 13, the latest version of Apples mobile operating software. For now, both Checkm8 and Checkra1n dont work on recent iOS devices, such as the iPhones released this year and last year. These developments came after years of there being no publicly available iPhone jailbreak for up-to-date phones.They just completely lost control of the battle (Jailbreaking) on iPhone X and older, Pwn20wnd, an iPhone security researcher who said his posts on Reddit were taken down, told Motherboard in an online chat. So they are trying to pick up a legal fight and waste our time, thus money / resources.Do you work or used to work at Apple? Wed love to hear from you. Using a non-work phone or computer, you can contact Lorenzo Franceschi-Bicchierai securely on Signal at +1 917 257 1382, OTR chat at lorenzofb@jabber.ccc.de, or email lorenzofb@vice.comIn the last few months Apple has flexed its legal muscles against iOS security researchers. In August, the company sued Corellium, a startup that offers a product that virtualizes iPhones. Security researchers criticized the lawsuit as an attempt by Apple to control the market of iPhone vulnerabilities, and restrict hackers ability to find them.This post was updated with more information on Apple's request to Twitter.Subscribe to our new cybersecurity podcast, CYBER.ORIGINAL REPORTING ON EVERYTHING THAT MATTERS IN YOUR INBOX.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content. ",
          "I wonder why they backtracked and asked for the Tweet to be restored. I don’t think key material is copyrightable. Besides, once it’s posted the cats out of the bag.<p>I’d rather see Apple continue to strengthen iOS security and leave the legal bs for other problems.",
          ">Kilpatrick Townsend & Stockton<p>Same guys wanted a Youtuber to remove a video where he shows location of a fuse on macbook motherboard."
        ],
        "story_type": ["Normal"],
        "url": "https://www.vice.com/en_us/article/pkeeay/apple-dmca-take-down-tweet-containing-an-iphone-encryption-key",
        "url_text": "Image: Sean Gallup/Getty ImagesSecurity researchers are accusing Apple of abusing the Digital Millennium Copyright Act (DMCA) to take down a viral tweet and several Reddit posts that discuss techniques and tools to hack iPhones.On Sunday, a security researcher who focuses on iOS and goes by the name Siguza posted a tweet containing what appears to be an encryption key that could be used to reverse engineer the Secure Enclave Processor, the part of the iPhone that handles data encryption and stores other sensitive data.Two days later, a law firm that has worked for Apple in the past sent a DMCA Takedown Notice to Twitter, asking for the tweet to be removed. The company complied, and the tweet became unavailable until today, when it reappeared. In a tweet, Siguza said that the DMCA claim was retracted.Apple confirmed that it sent the original DMCA takedown request. The company said that it retracted the request but Twitter had already complied with it and had taken the tweet down. Apple then asked Twitter to put the tweet back online.At the same time, Reddit received several DMCA takedown requests for posts on r/jailbreak, a popular subreddit where iPhone security researchers and hackers discuss techniques to jailbreak Apple devices, according to the subreddits moderators.Admins have not reached out to us in regards to these removals. We have no idea who is submitting these copyright claims, one moderator wrote.Twitter and Reddit did not immediately respond to emails asking for comment. Motherboard was not able to verify who sent the DMCA takedown requests to Reddit.iPhone security researchers and jailbreakers see these actions as Apple trying to clamp down on the jailbreaking community. Some in the community have questioned whether an encryption key, or posts linking to jailbreaking tools, are subject to copyright at all.In September, an independent iPhone security researcher that goes by the name axi0mX released Checkm8, a hacking tool that allows anyone to remove restrictions and jailbreak iOS devices as recent as the 2017 iPhone X. Weeks later, other researchers released checkra1n, a full-fledged jailbreak for devices running iOS 13, the latest version of Apples mobile operating software. For now, both Checkm8 and Checkra1n dont work on recent iOS devices, such as the iPhones released this year and last year. These developments came after years of there being no publicly available iPhone jailbreak for up-to-date phones.They just completely lost control of the battle (Jailbreaking) on iPhone X and older, Pwn20wnd, an iPhone security researcher who said his posts on Reddit were taken down, told Motherboard in an online chat. So they are trying to pick up a legal fight and waste our time, thus money / resources.Do you work or used to work at Apple? Wed love to hear from you. Using a non-work phone or computer, you can contact Lorenzo Franceschi-Bicchierai securely on Signal at +1 917 257 1382, OTR chat at lorenzofb@jabber.ccc.de, or email lorenzofb@vice.comIn the last few months Apple has flexed its legal muscles against iOS security researchers. In August, the company sued Corellium, a startup that offers a product that virtualizes iPhones. Security researchers criticized the lawsuit as an attempt by Apple to control the market of iPhone vulnerabilities, and restrict hackers ability to find them.This post was updated with more information on Apple's request to Twitter.Subscribe to our new cybersecurity podcast, CYBER.ORIGINAL REPORTING ON EVERYTHING THAT MATTERS IN YOUR INBOX.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content. ",
        "comments.comment_id": [21768252, 21770384],
        "comments.comment_author": ["t34543", "rasz"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-12-12T00:25:23Z",
          "2019-12-12T08:52:40Z"
        ],
        "comments.comment_text": [
          "I wonder why they backtracked and asked for the Tweet to be restored. I don’t think key material is copyrightable. Besides, once it’s posted the cats out of the bag.<p>I’d rather see Apple continue to strengthen iOS security and leave the legal bs for other problems.",
          ">Kilpatrick Townsend & Stockton<p>Same guys wanted a Youtuber to remove a video where he shows location of a fuse on macbook motherboard."
        ],
        "id": "7e1142d2-d45b-4c67-ad24-8422a421f008",
        "_version_": 1718527439899983872
      },
      {
        "story_id": [19286885],
        "story_author": ["djrobstep"],
        "story_descendants": [89],
        "story_score": [149],
        "story_time": ["2019-03-02T01:31:45Z"],
        "story_title": "Database schema changes are hard (2017)",
        "search": [
          "Database schema changes are hard (2017)",
          "https://djrobstep.com/talks/your-migrations-are-bad-and-you-should-feel-bad",
          "This is the slides and commentary from a talk I presented at PostgresOpen 2017, with minor revisions and updates to maintain correctness Robert Lechte / @djrobstep on twitter and github This talk about making changes to the structure of your database. Schema changes, schema migrations, whatever you want to call them. The title of this talk is just a futurama reference, I don't want anybody to actually feel bad, rather I hope by the end of this talk you feel better about schema migrations than you did before. Let's begin with an inspirational quote. Linus on data: I'm a huge proponent of designing your code around the data, rather than the other way around, and I think it's one of the reasons git has been fairly successful... Bad programmers worry about the code. Good programmers worry about data structures and their relationships. So this is Linus Torvalds, talking about the importance of structuring your data in the right way. I'm not a huge fan of the mean-spirited way Linus Torvalds sometimes says things, but I'm absolutely a huge fan of this quote and what he's saying here. Bad database bad backend bad frontend I probably don't need to say this at a database conference, but getting the structure of our database right is really important. It isn't just an issue for DBAs and backend developers. What users of software want on the frontend is reliability and fast, responsive performance, and that's impossible without a well-structured and well-functioning backend. Schema changes are hard Working with database schemas always seem a little more difficult than it should be. It always seems like a chore to make changes to your tables, and even more of a chore to roll them out. Even if it's just changing a column or two, it's time-consuming and error-prone. NoSQL TFW when you make a whole new category of database because you hate making schema changes I have a pet theory that the main reason for the sudden rise in popularity of NoSQL databases like Mongo a few years back was frustration at schema management. NoSQL databases don't really enforce much structure onto your data: You could start writing code to save your data without having to deal with schema changes. And that advantage was enough to get thousands of developers to use it! Misguided maybe (after all, Mongo sucks and you should never use it) but a totally understandable reaction. Schemas are actually good Enforcing consistency and structure is good But in most cases, having a strongly enforced schema is really important and a really good thing. Fundamentally, a schema is a guarantee that your data conforms to the structure you intend. And being able to know that and trust that is really good, and really important. It seems foolish to just throw that away. Solution Keep schemas, make better tools for working with schemas But the tools aren't great, so how can we make them better? Schema changes aren't conceptually complex. It's just an extra table here, one less column there, whatever. So why are schema changes so hard to work with? The problematic status quo Let's take a look at the way the tools work now, so we can see what needs fixing. Rails migrations, Django migrations, Alembic migrations, Go migrations, etc etc You might have worked with one of these before, or something similar in another language. What's common to almost all of them is that every time you make a schema change, you've got to create a new migration file, and each new version of your schema gets a version number, used for tracking what version your database is up to. Problem 1 A history of mistakes Over time you build up a long chain of these files, and after a while you have a folder that looks something like this. After a while: 01_make_a_table.py 02_make_another_table.py 03_add_a_column.py 04_rename_that_column.py bla bla bla 97_drop_that_old_table.py #realtalk Nobody cares about the old files The trouble is that most of these files are totally irrelevant. It's very unlikely you'll want to go back to a version of an app and database from three months ago. I'm usually pretty embarrassed about the code I wrote three months ago, and never want to see it again. History is already in version control. Why keep it hanging around? We have established tools for managing code history. Why keep old migration files hanging around cluttering up the working copy of your code? Problem 2 A file every time you make a change The overall assumption these tools make is that migrations are a Big Deal and a fairly rare operation. A heavyweight process for crafting migration files This is totally against the ethos of modern day iterative development, particularly if you're in the early stages of a project and doing rapid prototyping. It doesn't let you experiment, try out different configurations, it makes it harder change your mind. Not easy, not convenient, not intuitive, not agile. Unnecessary version number management. None of this feels very easy, or intuitive. The version numbers in particular are really annoying, and just another thing you need to keep track of and worry about. Problem 3 Testability Another problem is that these migrations aren't very testable. There are many things we want to be sure of when we make changes. So many questions Does our development schema match live? Does our test schema match live? Will our migration run on staging and live without breaking? How long will our migration take to run on the production data set? Will our migration cause downtime between deploy and migration? Usual answerWe don't know It's possible to test and check these things, but it's hard and not included out of the box. Ain't nobody got time for that In practice, the proper checks tend to be skipped. People just deploy and hope for the best. Problem 4 Framework lock-in Object-relational mappers: Maybe good, maybe bad, but shouldn't be compulsory. The next issue is that existing migration tools are usually built to work with one particular object-relational mapper library. So if you want to use that tool's migration functionality, you're coupled to that particular tool. Conscious uncoupling Some people are fans of ORMs, some aren't. But it shouldn't matter. Good migration tools should work with any setup, ORM or not, or a mix. The right tools should be available no matter how you're otherwise interacting with the database. Problem 5 All-or-nothing design assumption The tools all assume you're using them to manage the entire lifetime of your database. And, you must manage the entire database with them. Say you want to do a one-off schema change with django's migration tools, but you haven't built that database with django from the start. You're going to have to spend time retrofitting django models onto that database before you can take advantage of django's migration features. Problem 6 Database as primitive table store Also, they all assume you're using the database as nothing more than a primitive table store and not using any of the really powerful features PostgreSQL has. Party like it's SQL99? SQL has come a long way since the 90s, but we're not taking advantage of the modern features. PostgreSQL has more than just tables Views! Functions! Upserts! Hypercubes! Extensions! When you want to use these features, you're on your own. Even basic SQL views aren't handled by most of these tools. I'll stop complaining now Let's rethink this It's all very well to complain, but how could we actually solve these problems and improve the situation? Let's look at some ideas. Before The status quo Schema 1 Schema 2 Schema 3 Schema 4 Schema 5 Schema 5 Schema 6 Schema 7 Schema 8 Schema 9 Schema 10 Schema 11 ... Schema 591 Schema 592 Schema 593 Schema 594 What can we do about the messy chain of schema versions we talked about before? Instead of this enormous sequence of versioned schemas, we can simplify the problem to just a few. After Simplified Empty schema Dev schema Live schema There are the only three states we really need to care about. The current live schema is obviously pretty important. So is the development schema, which every so often becomes our live schema. And so is the empty schema: We need to be able to set up an empty database with the development schema so we can work with and test the database and the application. And that's it. How to get rid of migration files and version numbers? What about all these migration files and version numbers? How could we replace these with something better? It turns out here's something more straightforward and fundamental we can use to define database changes. A familiar concept Diffs You've almost certainly heard of diffs before. Diffs are a ubiquitous concept in software development. Diffs show changes between versions of text, code, maps, and lots of other things. Text diffs look like this: + the new text - the old text Plain old text diffs look like this. But database schemas can also be represented as diffs. Database schema diffs look like this: alter table \"author\" add column \"date_of_birth\" date; A database diff is a list of the SQL statements you need to run to get from one database state to another. For this diff we have one database where the author table has a date of birth column, and another that doesn't. This diff is the statements required to change that first database to make it match the structure of the second one. A basic equation: old database + diff = new database Fundamentally it works the same as any other diff: Old plus diff equals new. Apply the diff to the current database and you've got the new database. migra A diff tool for your database schemas So what I thought would be really good would be a diff tool for PostgreSQL. In fact I wanted one so badly that I wrote one in Python. Command line tool, Python library $ pip install migra It's a command line tool and also a python library. The way it works is best illustrated with a simple example. Basic example First create two databases... $ psql # create database a; create database b; # \\c b # create table t(id serial primary key, name varchar); To demonstrate, let's start with two databases, \"a\" and \"b\", and create a table in \"b\". Basic example Compare \"a\" to \"b\" $ migra postgresql:///a postgresql:///b Compare \"a\" to \"b\" $ migra postgresql:///a postgresql:///b create sequence \"public\".\"t_id_seq\"; create table \"public\".\"t\" ( \"id\" integer not null default nextval('t_id_seq'::regclass), \"name\" character varying ); CREATE UNIQUE INDEX t_pkey ON t USING btree (id); alter table \"public\".\"t\" add constraint \"t_pkey\" PRIMARY KEY using index \"t_pkey\"; Migra outputs the sql needed to make the schema in database \"a\" match database \"b\". In this case it's equivalent to the create table statement from before, but decomposed, with the primary key constraint, sequence, and index created separately. Migration script example First decide on a goal schema Goal schema can be created by ORM models, setup scripts, whatever Now we move on to a more comprehensive example. Here we'll generate a real migration script for a live database. Migration script example Goal setup script (again into database \"b\") create table author(id serial primary key, name varchar, dob varchar); create table book(id serial primary key, title varchar, page_count integer, author_id integer references author(id)); Migration script example: dump your live schema pg_dump -no-owner --no-privileges --schema-only prod -f prod.schema.dump.sql Migration script example: compare current to target, generate a migration file Result: $ migra postgresql:///a postgresql:///b create sequence \"public\".\"book_id_seq\"; create table \"public\".\"book\" ( \"id\" integer not null default nextval('book_id_seq'::regclass), \"title\" character varying, \"page_count\" integer, \"author_id\" integer ); alter table \"public\".\"author\" alter column \"dob\" set data type date; CREATE UNIQUE INDEX book_pkey ON book USING btree (id); alter table \"public\".\"book\" add constraint \"book_pkey\" PRIMARY KEY using index \"book_pkey\"; alter table \"public\".\"book\" add constraint \"book_author_id_fkey\" FOREIGN KEY (author_id) REFERENCES author(id); Check it matches $ psql a -1 -f migration_script.sql $ migra postgresql:///a postgresql:///b (EMPTY OUTPUT) Migration script example: review the autogenerated migration file, edit as necessary, apply $ psql production -1 -f migration_script.sql And that's the migration done. Because we've compared directly to the production schema, there's no need for version numbers, and no need to build up an enormous cluttered chain of migration files: Just a single script to review. Autosync: Changing your schema during development Another thing you might want to do is make changes to your database models, but keep your local development database in sync. And it turns out we can script this up fairly easily. Autosync example: script it up def sync(): with temporary_db() as TEMP_DB_URL: with S(TEMP_DB_URL) as s_goal: set_up_goal_database(s_goal) with S(DB_URL) as s_current, S(TEMP_DB_URL) as s_goal: m = Migration(s_current, s_goal) m.add_all_changes() if m.statements: print('CHANGES ARE PENDING:\\n\\n {}'.format(m.sql)) if prompt('Apply these changes?'): m.apply() else: print('Already synced.') No files, no version numbers: One operation Instead of creating a migration file with your changes, you just run this sync command, confirm, and your database is in sync. Fast enough to keep you in your flow That means you can keep working and keep focussed on the feature you're actually working on, rather than being distracted by version numbers. Testing: Before and after! I was complaining before about testing. If you want to carry out a migration with no errors and no downtime, you really need to know that your app works with both the before and after versions of the database. You can write a test for this pretty easily. Testing example pending_sql = Path('pending.sql').read_text().strip() if pending_sql: DATABASE_SETUPS_TO_TEST = [ load_pre_migration, load_post_migration ] else: DATABASE_SETUPS_TO_TEST = [ load_post_migration ] @pytest.fixture(params=DATABASE_SETUPS_TO_TEST) def db(request): with temporary_database() as test_db_url: setup_method = request.param setup_method(test_db_url) yield test_db_url Here's how that might work in practical terms. If we have a migration we need to do, we want to run every test twice, before and after. The way you do this will depend on a lot of things, but here's how you do it in a typical python project using postgres. If we use the convention that the pending migration code is stored in pending.sql, then we can check that file for changes. And if we have changes, then our feature code uses two different features, pre and post. Our fixture code creates a temporary database for each test, so each test runs against an isolated database: once with the old schema, and once with the migrated schema. Once we make both sets of tests pass, we know that we can deploy our app, check that's successful, and then run our migration, and we won't get any errors in either state. Migrations we can trust This test setup should give us a lot more confidence that we won't have any errors while migrating, and encourage us to make our apps a lot more robust. Talk directly to PostgreSQL ORM independence, tracking of views, functions, extensions, and other sweet PostgreSQL features. You'll note that in all these examples we're connecting to postgres directly, and working beneath the layer of any object-relational mappers. This means that the setup works independently from your application layer. You could change frontends or even have multiple frontends and your migration code doesn't need to change. Links: Docs: djrobstep.com/docs/migra Code: github.com/djrobstep/migra Sample app: github.com/djrobstep/bookapp Thanks for listening! ",
          "So how do you deal with it when you actually need to \"migrate\" data?<p>Getting a correct schema is only part of the migration process. Many times a refactoring requires a new table or field to be populated with data from the old table or field.<p>For example, a single table has a one to one relationship that we must select from to insert data into a newly created table so we can have a one to many relationship.",
          "So much of what is bad about tooling comes from this one assumption:  Database schema changes are hard.<p>If changing the schema is hard, then you come up with silly rules about when the schema can change and who can do it.  You make migration tools in other languages to avoid writing the line of SQL that would change the schema.  You use 3rd party tools to compare two databases and spit out change scripts automatically (and keep two databases versions up to date just for that purpose).  You adopt entire schemaless databases so that you never need to change the schema.<p>But that's silly.  Because Database Schema Changes are <i>Not Hard.</i><p>You do the thing the author is scared of: SQL Change Scripts.  Insert column, massage the data, flip on the null constraint, add relationships.  It's all really basic stuff, and if you don't know the SQL syntax for it you can just ask the db tool you're using to make the change.  It'll have a little \"Script This Out\" button next to the Save button.<p>If you do that, then you get to live in a world where Database Schema Changes are Easy.  You get to have a build that just runs new change scripts in order rather than involving Ruby or some wacky 3rd party tool.<p>And you can move as fast as you like."
        ],
        "story_type": ["Normal"],
        "url": "https://djrobstep.com/talks/your-migrations-are-bad-and-you-should-feel-bad",
        "comments.comment_id": [19287564, 19287951],
        "comments.comment_author": ["jjeaff", "jasonkester"],
        "comments.comment_descendants": [3, 7],
        "comments.comment_time": [
          "2019-03-02T04:26:19Z",
          "2019-03-02T06:45:34Z"
        ],
        "comments.comment_text": [
          "So how do you deal with it when you actually need to \"migrate\" data?<p>Getting a correct schema is only part of the migration process. Many times a refactoring requires a new table or field to be populated with data from the old table or field.<p>For example, a single table has a one to one relationship that we must select from to insert data into a newly created table so we can have a one to many relationship.",
          "So much of what is bad about tooling comes from this one assumption:  Database schema changes are hard.<p>If changing the schema is hard, then you come up with silly rules about when the schema can change and who can do it.  You make migration tools in other languages to avoid writing the line of SQL that would change the schema.  You use 3rd party tools to compare two databases and spit out change scripts automatically (and keep two databases versions up to date just for that purpose).  You adopt entire schemaless databases so that you never need to change the schema.<p>But that's silly.  Because Database Schema Changes are <i>Not Hard.</i><p>You do the thing the author is scared of: SQL Change Scripts.  Insert column, massage the data, flip on the null constraint, add relationships.  It's all really basic stuff, and if you don't know the SQL syntax for it you can just ask the db tool you're using to make the change.  It'll have a little \"Script This Out\" button next to the Save button.<p>If you do that, then you get to live in a world where Database Schema Changes are Easy.  You get to have a build that just runs new change scripts in order rather than involving Ruby or some wacky 3rd party tool.<p>And you can move as fast as you like."
        ],
        "id": "65d61b93-5ba0-4211-8229-1550017ee7d5",
        "url_text": "This is the slides and commentary from a talk I presented at PostgresOpen 2017, with minor revisions and updates to maintain correctness Robert Lechte / @djrobstep on twitter and github This talk about making changes to the structure of your database. Schema changes, schema migrations, whatever you want to call them. The title of this talk is just a futurama reference, I don't want anybody to actually feel bad, rather I hope by the end of this talk you feel better about schema migrations than you did before. Let's begin with an inspirational quote. Linus on data: I'm a huge proponent of designing your code around the data, rather than the other way around, and I think it's one of the reasons git has been fairly successful... Bad programmers worry about the code. Good programmers worry about data structures and their relationships. So this is Linus Torvalds, talking about the importance of structuring your data in the right way. I'm not a huge fan of the mean-spirited way Linus Torvalds sometimes says things, but I'm absolutely a huge fan of this quote and what he's saying here. Bad database bad backend bad frontend I probably don't need to say this at a database conference, but getting the structure of our database right is really important. It isn't just an issue for DBAs and backend developers. What users of software want on the frontend is reliability and fast, responsive performance, and that's impossible without a well-structured and well-functioning backend. Schema changes are hard Working with database schemas always seem a little more difficult than it should be. It always seems like a chore to make changes to your tables, and even more of a chore to roll them out. Even if it's just changing a column or two, it's time-consuming and error-prone. NoSQL TFW when you make a whole new category of database because you hate making schema changes I have a pet theory that the main reason for the sudden rise in popularity of NoSQL databases like Mongo a few years back was frustration at schema management. NoSQL databases don't really enforce much structure onto your data: You could start writing code to save your data without having to deal with schema changes. And that advantage was enough to get thousands of developers to use it! Misguided maybe (after all, Mongo sucks and you should never use it) but a totally understandable reaction. Schemas are actually good Enforcing consistency and structure is good But in most cases, having a strongly enforced schema is really important and a really good thing. Fundamentally, a schema is a guarantee that your data conforms to the structure you intend. And being able to know that and trust that is really good, and really important. It seems foolish to just throw that away. Solution Keep schemas, make better tools for working with schemas But the tools aren't great, so how can we make them better? Schema changes aren't conceptually complex. It's just an extra table here, one less column there, whatever. So why are schema changes so hard to work with? The problematic status quo Let's take a look at the way the tools work now, so we can see what needs fixing. Rails migrations, Django migrations, Alembic migrations, Go migrations, etc etc You might have worked with one of these before, or something similar in another language. What's common to almost all of them is that every time you make a schema change, you've got to create a new migration file, and each new version of your schema gets a version number, used for tracking what version your database is up to. Problem 1 A history of mistakes Over time you build up a long chain of these files, and after a while you have a folder that looks something like this. After a while: 01_make_a_table.py 02_make_another_table.py 03_add_a_column.py 04_rename_that_column.py bla bla bla 97_drop_that_old_table.py #realtalk Nobody cares about the old files The trouble is that most of these files are totally irrelevant. It's very unlikely you'll want to go back to a version of an app and database from three months ago. I'm usually pretty embarrassed about the code I wrote three months ago, and never want to see it again. History is already in version control. Why keep it hanging around? We have established tools for managing code history. Why keep old migration files hanging around cluttering up the working copy of your code? Problem 2 A file every time you make a change The overall assumption these tools make is that migrations are a Big Deal and a fairly rare operation. A heavyweight process for crafting migration files This is totally against the ethos of modern day iterative development, particularly if you're in the early stages of a project and doing rapid prototyping. It doesn't let you experiment, try out different configurations, it makes it harder change your mind. Not easy, not convenient, not intuitive, not agile. Unnecessary version number management. None of this feels very easy, or intuitive. The version numbers in particular are really annoying, and just another thing you need to keep track of and worry about. Problem 3 Testability Another problem is that these migrations aren't very testable. There are many things we want to be sure of when we make changes. So many questions Does our development schema match live? Does our test schema match live? Will our migration run on staging and live without breaking? How long will our migration take to run on the production data set? Will our migration cause downtime between deploy and migration? Usual answerWe don't know It's possible to test and check these things, but it's hard and not included out of the box. Ain't nobody got time for that In practice, the proper checks tend to be skipped. People just deploy and hope for the best. Problem 4 Framework lock-in Object-relational mappers: Maybe good, maybe bad, but shouldn't be compulsory. The next issue is that existing migration tools are usually built to work with one particular object-relational mapper library. So if you want to use that tool's migration functionality, you're coupled to that particular tool. Conscious uncoupling Some people are fans of ORMs, some aren't. But it shouldn't matter. Good migration tools should work with any setup, ORM or not, or a mix. The right tools should be available no matter how you're otherwise interacting with the database. Problem 5 All-or-nothing design assumption The tools all assume you're using them to manage the entire lifetime of your database. And, you must manage the entire database with them. Say you want to do a one-off schema change with django's migration tools, but you haven't built that database with django from the start. You're going to have to spend time retrofitting django models onto that database before you can take advantage of django's migration features. Problem 6 Database as primitive table store Also, they all assume you're using the database as nothing more than a primitive table store and not using any of the really powerful features PostgreSQL has. Party like it's SQL99? SQL has come a long way since the 90s, but we're not taking advantage of the modern features. PostgreSQL has more than just tables Views! Functions! Upserts! Hypercubes! Extensions! When you want to use these features, you're on your own. Even basic SQL views aren't handled by most of these tools. I'll stop complaining now Let's rethink this It's all very well to complain, but how could we actually solve these problems and improve the situation? Let's look at some ideas. Before The status quo Schema 1 Schema 2 Schema 3 Schema 4 Schema 5 Schema 5 Schema 6 Schema 7 Schema 8 Schema 9 Schema 10 Schema 11 ... Schema 591 Schema 592 Schema 593 Schema 594 What can we do about the messy chain of schema versions we talked about before? Instead of this enormous sequence of versioned schemas, we can simplify the problem to just a few. After Simplified Empty schema Dev schema Live schema There are the only three states we really need to care about. The current live schema is obviously pretty important. So is the development schema, which every so often becomes our live schema. And so is the empty schema: We need to be able to set up an empty database with the development schema so we can work with and test the database and the application. And that's it. How to get rid of migration files and version numbers? What about all these migration files and version numbers? How could we replace these with something better? It turns out here's something more straightforward and fundamental we can use to define database changes. A familiar concept Diffs You've almost certainly heard of diffs before. Diffs are a ubiquitous concept in software development. Diffs show changes between versions of text, code, maps, and lots of other things. Text diffs look like this: + the new text - the old text Plain old text diffs look like this. But database schemas can also be represented as diffs. Database schema diffs look like this: alter table \"author\" add column \"date_of_birth\" date; A database diff is a list of the SQL statements you need to run to get from one database state to another. For this diff we have one database where the author table has a date of birth column, and another that doesn't. This diff is the statements required to change that first database to make it match the structure of the second one. A basic equation: old database + diff = new database Fundamentally it works the same as any other diff: Old plus diff equals new. Apply the diff to the current database and you've got the new database. migra A diff tool for your database schemas So what I thought would be really good would be a diff tool for PostgreSQL. In fact I wanted one so badly that I wrote one in Python. Command line tool, Python library $ pip install migra It's a command line tool and also a python library. The way it works is best illustrated with a simple example. Basic example First create two databases... $ psql # create database a; create database b; # \\c b # create table t(id serial primary key, name varchar); To demonstrate, let's start with two databases, \"a\" and \"b\", and create a table in \"b\". Basic example Compare \"a\" to \"b\" $ migra postgresql:///a postgresql:///b Compare \"a\" to \"b\" $ migra postgresql:///a postgresql:///b create sequence \"public\".\"t_id_seq\"; create table \"public\".\"t\" ( \"id\" integer not null default nextval('t_id_seq'::regclass), \"name\" character varying ); CREATE UNIQUE INDEX t_pkey ON t USING btree (id); alter table \"public\".\"t\" add constraint \"t_pkey\" PRIMARY KEY using index \"t_pkey\"; Migra outputs the sql needed to make the schema in database \"a\" match database \"b\". In this case it's equivalent to the create table statement from before, but decomposed, with the primary key constraint, sequence, and index created separately. Migration script example First decide on a goal schema Goal schema can be created by ORM models, setup scripts, whatever Now we move on to a more comprehensive example. Here we'll generate a real migration script for a live database. Migration script example Goal setup script (again into database \"b\") create table author(id serial primary key, name varchar, dob varchar); create table book(id serial primary key, title varchar, page_count integer, author_id integer references author(id)); Migration script example: dump your live schema pg_dump -no-owner --no-privileges --schema-only prod -f prod.schema.dump.sql Migration script example: compare current to target, generate a migration file Result: $ migra postgresql:///a postgresql:///b create sequence \"public\".\"book_id_seq\"; create table \"public\".\"book\" ( \"id\" integer not null default nextval('book_id_seq'::regclass), \"title\" character varying, \"page_count\" integer, \"author_id\" integer ); alter table \"public\".\"author\" alter column \"dob\" set data type date; CREATE UNIQUE INDEX book_pkey ON book USING btree (id); alter table \"public\".\"book\" add constraint \"book_pkey\" PRIMARY KEY using index \"book_pkey\"; alter table \"public\".\"book\" add constraint \"book_author_id_fkey\" FOREIGN KEY (author_id) REFERENCES author(id); Check it matches $ psql a -1 -f migration_script.sql $ migra postgresql:///a postgresql:///b (EMPTY OUTPUT) Migration script example: review the autogenerated migration file, edit as necessary, apply $ psql production -1 -f migration_script.sql And that's the migration done. Because we've compared directly to the production schema, there's no need for version numbers, and no need to build up an enormous cluttered chain of migration files: Just a single script to review. Autosync: Changing your schema during development Another thing you might want to do is make changes to your database models, but keep your local development database in sync. And it turns out we can script this up fairly easily. Autosync example: script it up def sync(): with temporary_db() as TEMP_DB_URL: with S(TEMP_DB_URL) as s_goal: set_up_goal_database(s_goal) with S(DB_URL) as s_current, S(TEMP_DB_URL) as s_goal: m = Migration(s_current, s_goal) m.add_all_changes() if m.statements: print('CHANGES ARE PENDING:\\n\\n {}'.format(m.sql)) if prompt('Apply these changes?'): m.apply() else: print('Already synced.') No files, no version numbers: One operation Instead of creating a migration file with your changes, you just run this sync command, confirm, and your database is in sync. Fast enough to keep you in your flow That means you can keep working and keep focussed on the feature you're actually working on, rather than being distracted by version numbers. Testing: Before and after! I was complaining before about testing. If you want to carry out a migration with no errors and no downtime, you really need to know that your app works with both the before and after versions of the database. You can write a test for this pretty easily. Testing example pending_sql = Path('pending.sql').read_text().strip() if pending_sql: DATABASE_SETUPS_TO_TEST = [ load_pre_migration, load_post_migration ] else: DATABASE_SETUPS_TO_TEST = [ load_post_migration ] @pytest.fixture(params=DATABASE_SETUPS_TO_TEST) def db(request): with temporary_database() as test_db_url: setup_method = request.param setup_method(test_db_url) yield test_db_url Here's how that might work in practical terms. If we have a migration we need to do, we want to run every test twice, before and after. The way you do this will depend on a lot of things, but here's how you do it in a typical python project using postgres. If we use the convention that the pending migration code is stored in pending.sql, then we can check that file for changes. And if we have changes, then our feature code uses two different features, pre and post. Our fixture code creates a temporary database for each test, so each test runs against an isolated database: once with the old schema, and once with the migrated schema. Once we make both sets of tests pass, we know that we can deploy our app, check that's successful, and then run our migration, and we won't get any errors in either state. Migrations we can trust This test setup should give us a lot more confidence that we won't have any errors while migrating, and encourage us to make our apps a lot more robust. Talk directly to PostgreSQL ORM independence, tracking of views, functions, extensions, and other sweet PostgreSQL features. You'll note that in all these examples we're connecting to postgres directly, and working beneath the layer of any object-relational mappers. This means that the setup works independently from your application layer. You could change frontends or even have multiple frontends and your migration code doesn't need to change. Links: Docs: djrobstep.com/docs/migra Code: github.com/djrobstep/migra Sample app: github.com/djrobstep/bookapp Thanks for listening! ",
        "_version_": 1718527388317384704
      },
      {
        "story_id": [20377175],
        "story_author": ["azhenley"],
        "story_descendants": [2],
        "story_score": [64],
        "story_time": ["2019-07-07T19:21:10Z"],
        "story_title": "Quick start guide to research on human factors of software engineering",
        "search": [
          "Quick start guide to research on human factors of software engineering",
          "http://web.eecs.utk.edu/~azh/blog/guidehciseresearch.html",
          "Austin Z. Henley Assistant Professor azh@utk.edu @austinzhenley github/AZHenley This guide is meant to help new graduate students get a short introduction to research at the intersection of human-computer interaction (HCI) and software engineering (SE). By reading the materials listed below, you will get a small taste of the field. ",
          "In my experience for most developers the behaviors boil down to just a few questions?<p>1. Are the developers willing to write original code or must they be limited to configurations or tooling?<p>2. Are the developers willing to accept any API (RTFM) or must the developers be limited to prior known APIs?<p>3. Are the developers willing to accept failure for missing unspecified, but commonly known, requirements such as accessibility, usability, security, or domain specific knowledge?<p>4. Are the developers willing to alter their personal routines to accommodate a shift in requirements and delivery dates or do the developers shift requirements to accommodate their routines, such as work-life balance?<p>5. Will the developers write technical documentation to describe process and approach or will the developers shift requirements in anticipation of forth-coming technical documentation?<p>6. Will the developers refactor code to achieve simplicity (imperative), descriptive concepts (declarative), code style, or not at all? The motivations generally do not overlap.<p>7. Will the developers accept code that better achieves business concerns or end-user preferences (object measures) in violation to preferred structures or styles (subjective measures)?<p>8. Are the developers willing to read the existing code (RTFC) before recommending new tools or solutions?",
          "I really want this kind of research to flourish, but the first click I made on an interesting looking paper led me to: \"To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support.  We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers.\"<p>To be fair, I haven't read the paper and the rest of the abstract looks reasonable: \"To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits.\"<p>But what the heck is the first bit for?  Of 12 people I know, the vast majority don't use refactoring tools.  That's not the basis on which to launch a study.  I've skimmed the abstracts of the other papers as well and I'm not all that impressed.  Everything seems to be doing comparisons against Eclipse.  While Eclipse has a variety of different usability features, I'm actually not convinced any of them help at all (which is why I don't use it ;-) ).  So at the very least, I'd like to see a baseline against a traditional text editor like Emacs or Vim.  The \"improvements\" in usability that they are measuring may simply be the avoidance of problems in Eclipse.  I'm not trying to start an editor war here, I'm just saying you can't assume that any single editor is a good baseline.  I'd argue strenuously that feature rich editors like Eclipse, IntelliJ, VS Code, etc, etc are particularly bad candidates because nobody has really measured the effectiveness of their features.<p>Without being too negative, I hope there are better papers on these kinds of topics because I think it's incredibly important."
        ],
        "story_type": ["Normal"],
        "url": "http://web.eecs.utk.edu/~azh/blog/guidehciseresearch.html",
        "url_text": "Austin Z. Henley Assistant Professor azh@utk.edu @austinzhenley github/AZHenley This guide is meant to help new graduate students get a short introduction to research at the intersection of human-computer interaction (HCI) and software engineering (SE). By reading the materials listed below, you will get a small taste of the field. ",
        "comments.comment_id": [20379783, 20380549],
        "comments.comment_author": ["austincheney", "mikekchar"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-07-08T03:56:33Z",
          "2019-07-08T07:11:40Z"
        ],
        "comments.comment_text": [
          "In my experience for most developers the behaviors boil down to just a few questions?<p>1. Are the developers willing to write original code or must they be limited to configurations or tooling?<p>2. Are the developers willing to accept any API (RTFM) or must the developers be limited to prior known APIs?<p>3. Are the developers willing to accept failure for missing unspecified, but commonly known, requirements such as accessibility, usability, security, or domain specific knowledge?<p>4. Are the developers willing to alter their personal routines to accommodate a shift in requirements and delivery dates or do the developers shift requirements to accommodate their routines, such as work-life balance?<p>5. Will the developers write technical documentation to describe process and approach or will the developers shift requirements in anticipation of forth-coming technical documentation?<p>6. Will the developers refactor code to achieve simplicity (imperative), descriptive concepts (declarative), code style, or not at all? The motivations generally do not overlap.<p>7. Will the developers accept code that better achieves business concerns or end-user preferences (object measures) in violation to preferred structures or styles (subjective measures)?<p>8. Are the developers willing to read the existing code (RTFC) before recommending new tools or solutions?",
          "I really want this kind of research to flourish, but the first click I made on an interesting looking paper led me to: \"To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support.  We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers.\"<p>To be fair, I haven't read the paper and the rest of the abstract looks reasonable: \"To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits.\"<p>But what the heck is the first bit for?  Of 12 people I know, the vast majority don't use refactoring tools.  That's not the basis on which to launch a study.  I've skimmed the abstracts of the other papers as well and I'm not all that impressed.  Everything seems to be doing comparisons against Eclipse.  While Eclipse has a variety of different usability features, I'm actually not convinced any of them help at all (which is why I don't use it ;-) ).  So at the very least, I'd like to see a baseline against a traditional text editor like Emacs or Vim.  The \"improvements\" in usability that they are measuring may simply be the avoidance of problems in Eclipse.  I'm not trying to start an editor war here, I'm just saying you can't assume that any single editor is a good baseline.  I'd argue strenuously that feature rich editors like Eclipse, IntelliJ, VS Code, etc, etc are particularly bad candidates because nobody has really measured the effectiveness of their features.<p>Without being too negative, I hope there are better papers on these kinds of topics because I think it's incredibly important."
        ],
        "id": "b96c8ad7-6cf5-4a77-b55f-8b52761ba4d7",
        "_version_": 1718527414103965697
      },
      {
        "story_id": [20794991],
        "story_author": ["kerng"],
        "story_descendants": [66],
        "story_score": [81],
        "story_time": ["2019-08-25T19:34:07Z"],
        "story_title": "Android 10: Google Confirms 193 Security Vulnerabilities Need Fixing",
        "search": [
          "Android 10: Google Confirms 193 Security Vulnerabilities Need Fixing",
          "https://www.forbes.com/sites/daveywinder/2019/08/23/android-10-google-confirms-193-security-vulnerabilities-need-fixing/#3a94f6c9616b",
          "Google and Android logos are seen on Android mobile devices LightRocket via Getty Images The guessing as to what Android Q could stand for is over. As, indeed, is Android Q. Google has announced that Android is evolving, and as part of that evolution the new version of the operating system that will be released in just a few weeks time will be called Android 10. \"While there were many tempting 'Q' desserts out there,\" Sameer Samat, vice president of product management for Android, said \"we think that at version 10 and 2.5 billion active devices, it was time to make this change.\" It was also time to address a total of 193 Android security vulnerabilities that Google has confirmed need fixing with the Android 10 release. Android 10 security vulnerabilities That surprising Google vulnerability confirmation came by way of the official security release notes that were published to the Android Open Source Project (AOSP) security bulletin update on August 20. The bad news is that 193 Android security vulnerabilities needed to be fixed, covering a broad swathe of elevation of privilege, remote code execution, information disclosure and denial of service categories. Two of these are in the Android runtime itself, another two in the library and 24 in the framework. The bulk, however, is split between the Android media framework with 68 vulnerabilities and the Android system with 97. All have been scored as \"moderate\" severity. The good news is that all will be fixed by the default Android 10 patch level of 2019-09-01 on release of the new OS. Also on the positive news front, the security bulletin update stated that \"we have had no reports of active customer exploitation or abuse of these newly reported issues.\" Android 10 privacy improvements The good news for 2.5 billion Android fans doesn't stop there. Earlier this year, Stephanie Cuthbertson, director of product management for Android, stated that the then Android Q would bring \"almost 50 new features and changes focused on security and privacy.\" True to her word, a whole host of new security and privacy features are indeed included as part of the Android 10 release. Details of some of the main privacy changes can be found at the Android \"Q\" developer website, where a statement reads \"Android Q extends the transparency and control that users have over data and app capabilities.\" The top changes include \"scoped storage\" to give users more control over files by only allowing Android 10 apps a filtered view of their app-specific directory and specific types of media. Users will also have more control over when apps can use device location, by offering two options when an app asks for this access: while using the app only, or all the time (in the background, in other words.) Android 10 will also restrict when background activity can start, so as to minimize interruptions for the user who can maintain better \"control of what's shown on their screen\" as a result. There are also changes in how the camera can be accessed by apps. Android 10 requires apps to have been granted camera access permission to get \"potentially device-specific metadata.\" With the introduction of Android 10, apps will not be able to enable or disable Wi-Fi, but must use a settings panel to prompt the user to do so instead. Furthermore, \"to protect user privacy, manual configuration of the list of Wi-Fi networks is now restricted to system apps and device policy controllers.\" According to a report in Wired, \"Google will now require developers to use resettable identifiers to keep track of users. That way, if these digital fingerprints are ever compromised, or if you want to wipe your digital slate clean, there's a mechanism to do that.\" Android 10 security evolution Android 10 will also bring a quiet security evolution rather than revolution it seems. An encryption scheme by the name of Adiantum is to be introduced as part of the Android 10 platform. This is good news as Google will require all new devices running the latest Android OS, including internet-of-things devices, to be encrypted using either the established AES option or Adiantum which has sufficient performance to run on lower-end ARM processor-powered devices. That Wired report also references how a new security library for Android 10 can be used with the Google Jetpack tools package to \"help developers get security right in their apps, even if they don't have extensive expertise in the field.\" Alongside the introduction of newly hardened sandboxes, including mini-sandboxes that isolate system process and app components, the leaking of data between apps should be less of an issue. And finally, but certainly not at the bottom of the security importance list, I'm pleased that Google is making changes to the way that Android 10 will handle security updates. Important OS components will now be updated in the background, in much the same way that apps are updated, to bring the latest security fixes onto your device as soon as they are available and without having to reboot the phone! ",
          "A revocable permission for network access on Android would be a great step towards giving users more control over which apps can transmit their personal data.<p>An entire class of apps could be rendered safe by disallowing network access, especially the ones that do work offline, but are keen to phone home.",
          "This headline is grossly misleading - Android 10 has fixed those 193 vulnerabilities. The title as it is, implies they still need to be fixed.<p>This kind of content is usually never worded like this for other products, can the moderators fix it?"
        ],
        "story_type": ["Normal"],
        "url": "https://www.forbes.com/sites/daveywinder/2019/08/23/android-10-google-confirms-193-security-vulnerabilities-need-fixing/#3a94f6c9616b",
        "comments.comment_id": [20795476, 20795786],
        "comments.comment_author": ["dessant", "izacus"],
        "comments.comment_descendants": [6, 2],
        "comments.comment_time": [
          "2019-08-25T20:48:55Z",
          "2019-08-25T21:43:03Z"
        ],
        "comments.comment_text": [
          "A revocable permission for network access on Android would be a great step towards giving users more control over which apps can transmit their personal data.<p>An entire class of apps could be rendered safe by disallowing network access, especially the ones that do work offline, but are keen to phone home.",
          "This headline is grossly misleading - Android 10 has fixed those 193 vulnerabilities. The title as it is, implies they still need to be fixed.<p>This kind of content is usually never worded like this for other products, can the moderators fix it?"
        ],
        "id": "d9a2ec7a-93cc-4754-8a2b-60baa095acb7",
        "url_text": "Google and Android logos are seen on Android mobile devices LightRocket via Getty Images The guessing as to what Android Q could stand for is over. As, indeed, is Android Q. Google has announced that Android is evolving, and as part of that evolution the new version of the operating system that will be released in just a few weeks time will be called Android 10. \"While there were many tempting 'Q' desserts out there,\" Sameer Samat, vice president of product management for Android, said \"we think that at version 10 and 2.5 billion active devices, it was time to make this change.\" It was also time to address a total of 193 Android security vulnerabilities that Google has confirmed need fixing with the Android 10 release. Android 10 security vulnerabilities That surprising Google vulnerability confirmation came by way of the official security release notes that were published to the Android Open Source Project (AOSP) security bulletin update on August 20. The bad news is that 193 Android security vulnerabilities needed to be fixed, covering a broad swathe of elevation of privilege, remote code execution, information disclosure and denial of service categories. Two of these are in the Android runtime itself, another two in the library and 24 in the framework. The bulk, however, is split between the Android media framework with 68 vulnerabilities and the Android system with 97. All have been scored as \"moderate\" severity. The good news is that all will be fixed by the default Android 10 patch level of 2019-09-01 on release of the new OS. Also on the positive news front, the security bulletin update stated that \"we have had no reports of active customer exploitation or abuse of these newly reported issues.\" Android 10 privacy improvements The good news for 2.5 billion Android fans doesn't stop there. Earlier this year, Stephanie Cuthbertson, director of product management for Android, stated that the then Android Q would bring \"almost 50 new features and changes focused on security and privacy.\" True to her word, a whole host of new security and privacy features are indeed included as part of the Android 10 release. Details of some of the main privacy changes can be found at the Android \"Q\" developer website, where a statement reads \"Android Q extends the transparency and control that users have over data and app capabilities.\" The top changes include \"scoped storage\" to give users more control over files by only allowing Android 10 apps a filtered view of their app-specific directory and specific types of media. Users will also have more control over when apps can use device location, by offering two options when an app asks for this access: while using the app only, or all the time (in the background, in other words.) Android 10 will also restrict when background activity can start, so as to minimize interruptions for the user who can maintain better \"control of what's shown on their screen\" as a result. There are also changes in how the camera can be accessed by apps. Android 10 requires apps to have been granted camera access permission to get \"potentially device-specific metadata.\" With the introduction of Android 10, apps will not be able to enable or disable Wi-Fi, but must use a settings panel to prompt the user to do so instead. Furthermore, \"to protect user privacy, manual configuration of the list of Wi-Fi networks is now restricted to system apps and device policy controllers.\" According to a report in Wired, \"Google will now require developers to use resettable identifiers to keep track of users. That way, if these digital fingerprints are ever compromised, or if you want to wipe your digital slate clean, there's a mechanism to do that.\" Android 10 security evolution Android 10 will also bring a quiet security evolution rather than revolution it seems. An encryption scheme by the name of Adiantum is to be introduced as part of the Android 10 platform. This is good news as Google will require all new devices running the latest Android OS, including internet-of-things devices, to be encrypted using either the established AES option or Adiantum which has sufficient performance to run on lower-end ARM processor-powered devices. That Wired report also references how a new security library for Android 10 can be used with the Google Jetpack tools package to \"help developers get security right in their apps, even if they don't have extensive expertise in the field.\" Alongside the introduction of newly hardened sandboxes, including mini-sandboxes that isolate system process and app components, the leaking of data between apps should be less of an issue. And finally, but certainly not at the bottom of the security importance list, I'm pleased that Google is making changes to the way that Android 10 will handle security updates. Important OS components will now be updated in the background, in much the same way that apps are updated, to bring the latest security fixes onto your device as soon as they are available and without having to reboot the phone! ",
        "_version_": 1718527421639032833
      }
    ]
  }
}
