{
  "responseHeader":{
    "status":0,
    "QTime":1},
  "response":{"numFound":1915,"start":0,"numFoundExact":true,"docs":[
      {
        "story_id":19083580,
        "story_author":"briansack35",
        "story_descendants":3,
        "story_score":27,
        "story_time":"2019-02-05T07:32:49Z",
        "story_title":"What is GitOps and why you should know about it",
        "search":["What is GitOps and why you should know about it",
          "Normal",
          "https://venturebeat.com/2019/02/02/what-is-gitops-and-why-you-should-know-about-it/",
          "February 2, 2019 12:10 PM Developing technology is far more expansive than just writing code. Technology teams need to consider a number of different aspects of the companys technology stack, including its compute infrastructure, storage, development pipeline, security, and more. Companies often purchase various third-party products to complete their technology stack so they dont have to build it all on their own. Each of these tools comes with some form of management console or dashboard that enables companies to tailor that specific tool to the their own needs as well as integrate it within their product. The onboarding, configuration, and ongoing management will likely occur directly from within that console. In fact, managing these third-party products and dashboards actually becomes a significant portion of a technology teams workload and today often falls under the responsibility of the DevOps team. Above: Dashboard overdose: IT organisations are bombarded by dashboards across their entire tech stack To increase automation and efficiency, some companies now are bypassing dashboards and instead preferring to manage these products directly from within their code base. These products are then deployed and configured not as another management console, but rather by developers, as code, in the companys Git (GitHub, GitLab, or Bitbucket). This trend has been coined GitOps or Git Centric, as increasingly products are being codified and deployed within Git environments. In a somewhat unusual twist of events, Git has become the tool that software companies use to manage their entire technology stack. This means they are deploying their infrastructure, security, and the automation of their development pipeline all from within their Git. Technology blog Dzone put it nicely: Increasingly, companies are using Git as their single source of truth for their code, configurations and infrastructure. All their business logic lives in Git, and automated processes can turn their Git repositories into built and deployed software Weve entered the world of GitOps. A great example of a Git Centric tool is Terraform by Hashicorp. Terraform allows developers to define their infrastructure as code within their Git environment and to consistently and automatically spin up servers with a consistent set of configurations in a scalable manner. It also helps teams of developers understand the underlying server configurations by simply looking at the code rather than having to enter their cloud providers dashboard. The GitOps market is growing exceptionally fast, with companies announcing the codification of their products on a daily basis. At the same time, it is giving developers a perhaps unconsidered and threatening gateway into their production environments. So well also see tools likeDatree.iothat provide automatic policy compliance checks for every code commit. (Disclosure: Datree.io is one of my firms investments). Other tools are emerging to allow GitOps manage and automate more than application deployments. For example, Gitpitch enables teams to build, edit, change, and publish slide presentations. In other words, developers can build entire slide decks and presentations from within their Git provider. They can then manage changes to these presentations and collaborate around them just as they would over their application code base. And theres Waffle.io, a project management tool built within Git for engineering teams. By operating from within Git, Waffle can automatically determine what features and tasks developers have been completed and what is still in progress. It can then automatically communicate status updates to the rest of the team so that everyone is on the same page. I believe Git is likely to become the de-facto tool for automating many company operations. Obviously, the first use cases are focused on the deployment and configuration of applications. But it can also be used effectively for other business processes. The image at the top of this story shows the beginnings of a GitOps landscape. Since we are still in the early days of this market, I may have missed a number of relevant companies. If so, please reach out to me at brian@tlv.partners and let me know. Brian Sack is on the investment team at TLV Partners, an early stage VC based in Tel Aviv, Israel.VentureBeat VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative technology and transact. Our site delivers essential information on data technologies and strategies to guide you as you lead your organizations. We invite you to become a member of our community, to access: up-to-date information on the subjects of interest to you our newsletters gated thought-leader content and discounted access to our prized events, such as Transform 2021: Learn More networking features, and more Become a member ",
          "Great article, explains why Git can be used as the primary tool for interface between developers and infrastructure.",
          "Git is the bridge between development and production."],
        "story_type":"Normal",
        "url_raw":"https://venturebeat.com/2019/02/02/what-is-gitops-and-why-you-should-know-about-it/",
        "comments.comment_id":[19087094,
          19087577],
        "comments.comment_author":["mameshini",
          "h668"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-02-05T16:56:14Z",
          "2019-02-05T17:47:34Z"],
        "comments.comment_text":["Great article, explains why Git can be used as the primary tool for interface between developers and infrastructure.",
          "Git is the bridge between development and production."],
        "id":"05a00d5c-8db6-4573-af9c-b78bf20530bf",
        "url_text":"February 2, 2019 12:10 PM Developing technology is far more expansive than just writing code. Technology teams need to consider a number of different aspects of the companys technology stack, including its compute infrastructure, storage, development pipeline, security, and more. Companies often purchase various third-party products to complete their technology stack so they dont have to build it all on their own. Each of these tools comes with some form of management console or dashboard that enables companies to tailor that specific tool to the their own needs as well as integrate it within their product. The onboarding, configuration, and ongoing management will likely occur directly from within that console. In fact, managing these third-party products and dashboards actually becomes a significant portion of a technology teams workload and today often falls under the responsibility of the DevOps team. Above: Dashboard overdose: IT organisations are bombarded by dashboards across their entire tech stack To increase automation and efficiency, some companies now are bypassing dashboards and instead preferring to manage these products directly from within their code base. These products are then deployed and configured not as another management console, but rather by developers, as code, in the companys Git (GitHub, GitLab, or Bitbucket). This trend has been coined GitOps or Git Centric, as increasingly products are being codified and deployed within Git environments. In a somewhat unusual twist of events, Git has become the tool that software companies use to manage their entire technology stack. This means they are deploying their infrastructure, security, and the automation of their development pipeline all from within their Git. Technology blog Dzone put it nicely: Increasingly, companies are using Git as their single source of truth for their code, configurations and infrastructure. All their business logic lives in Git, and automated processes can turn their Git repositories into built and deployed software Weve entered the world of GitOps. A great example of a Git Centric tool is Terraform by Hashicorp. Terraform allows developers to define their infrastructure as code within their Git environment and to consistently and automatically spin up servers with a consistent set of configurations in a scalable manner. It also helps teams of developers understand the underlying server configurations by simply looking at the code rather than having to enter their cloud providers dashboard. The GitOps market is growing exceptionally fast, with companies announcing the codification of their products on a daily basis. At the same time, it is giving developers a perhaps unconsidered and threatening gateway into their production environments. So well also see tools likeDatree.iothat provide automatic policy compliance checks for every code commit. (Disclosure: Datree.io is one of my firms investments). Other tools are emerging to allow GitOps manage and automate more than application deployments. For example, Gitpitch enables teams to build, edit, change, and publish slide presentations. In other words, developers can build entire slide decks and presentations from within their Git provider. They can then manage changes to these presentations and collaborate around them just as they would over their application code base. And theres Waffle.io, a project management tool built within Git for engineering teams. By operating from within Git, Waffle can automatically determine what features and tasks developers have been completed and what is still in progress. It can then automatically communicate status updates to the rest of the team so that everyone is on the same page. I believe Git is likely to become the de-facto tool for automating many company operations. Obviously, the first use cases are focused on the deployment and configuration of applications. But it can also be used effectively for other business processes. The image at the top of this story shows the beginnings of a GitOps landscape. Since we are still in the early days of this market, I may have missed a number of relevant companies. If so, please reach out to me at brian@tlv.partners and let me know. Brian Sack is on the investment team at TLV Partners, an early stage VC based in Tel Aviv, Israel.VentureBeat VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative technology and transact. Our site delivers essential information on data technologies and strategies to guide you as you lead your organizations. We invite you to become a member of our community, to access: up-to-date information on the subjects of interest to you our newsletters gated thought-leader content and discounted access to our prized events, such as Transform 2021: Learn More networking features, and more Become a member ",
        "_version_":1718938144549634048},
      {
        "story_id":20618427,
        "story_author":"jgraham",
        "story_descendants":8,
        "story_score":52,
        "story_time":"2019-08-05T20:16:49Z",
        "story_title":"Git-Revise",
        "search":["Git-Revise",
          "Normal",
          "https://mystor.github.io/git-revise.html",
          "NIKA:\\git-revise\\> list (Aug. 6, 2019): Added the \"What git-revise is not\" section. At Mozilla I often end up building my changes in a patch stack, and used git rebase -i1 to make changes to commits in response to review comments etc. Unfortunately, with a repository as large as mozilla-central2, git rebase -i has some downsides: It's slow! Rebase operates directly on the worktree, so it performs a full checkout of each commit in the stack, and frequently refreshes worktree state. On large repositories (especially on NTFS) that can take a long time. It triggers rebuilds! Because rebase touches the file tree, some build systems (like gecko's recursive-make backend) rebuild unnecessarially. It's stateful! If the rebase fails, the repository is in a weird mid-rebase state, and in edge cases I've accidentally dropped commits due to other processes racing on the repository lock. It's clunky! Common tasks (like splitting & rewording commits) require multiple steps and are unintuitive. Naturally, I did the only reasonable thing: Build a brand-new tool. source: xkcd git-revise is a history editing tool designed for the patch-stack workflow. It's fast, non-destructive, and aims to provide a familiar, powerful, and easy to use re-imagining of the patch stack workflow. It's fast I would never claim to be a benchmarking expert 3, but git-revise performs substantially better than rebase for small history editing tasks 4. In a test applying a single-line change to a mozilla-central commit 20 patches up the stack I saw a 15x speed improvement. $ time bash -c 'git commit --fixup=$TARGET; EDITOR=true git rebase -i --autosquash $TARGET~' <snip> real 0m10.733s $ time git revise $TARGET <snip> real 0m0.685s git-revise accomplishes this using an in-memory rebase algorithm operating directly on git's trees, meaning it never has to touch your index or working directory, avoiding expensive disk I/O! It's handy git-revise isn't just a faster git rebase -i, it provides helpful commands, flags, and tools which make common changes faster, and easier: Fixup Fast $ git add . $ git revise HEAD~~ Running git revise $COMMIT directly collects changes staged in the index, and directly applies them to the specified commit. Conflicts are resolved interactively, and a warning will be shown if the final state of the tree is different from what you started with! With an extra -e, you can update the commit message at the same time, and -a will stage your changes, so you don't have to! 5 Split Commits $ git revise -c $COMMIT Select changes to be included in part [1]: diff --git b/file.txt a/file.txt <snip> Apply this hunk to index [y,n,q,a,d,e,?]? Sometimes, a commit needs to be split in two, perhaps because a change ended up in the wrong commit. The --cut flag (and cut interactive command) provides a fast way to split a commit in-place. Running git revise --cut $COMMIT will start a git add -p-style hunk selector, allowing you to pick changes for part 1, and the rest will end up in part 2. No more tinkering around with edit during a rebase to split off that comment you accidentally added to the wrong commit! Interactive Mode git-revise has a git rebase -i-style interactive mode, but with some quality-of-life improvements, on top of being fast: Implicit Base Commit If a base commit isn't provided, --interactive will implicitly locate a safe base commit to start from, walking up from HEAD, and stopping at published & merge commits. Often git revise -i is all you need! The index Todo Staged changes in the index automatically appear in interactive mode, and can be moved around and treated like any other commit in range. No need to turn it into a commit with a dummy name before you pop open interactive mode & squash it into another commit! Bulk Commit Rewording Ever wanted to update a bunch of commit messages at once? Perhaps they're all missing the bug number? Well, git revise -ie has you covered. It'll open a special Interactive Mode where each command is prefixed with a ++, and the full commit message is present after it. Changes made to these commit messages will be applied before executing the TODOs, meaning you can edit them in bulk. I use this constantly to add bug numbers, elaborate on commit details, and add reviewer information to commit messages. ++ pick f5a02a16731a Bug ??? - My commit summary, r=? The full commit message body follows! ++ pick fef1aeddd6fb Bug ??? - Another commit, r=? Another commit's body! Autosquash Support $ git revise --autosquash If you're used to git rebase -i --autosquash, revise works with you. Running git revise --autosquash will automatically reorder and apply fixup commits created with git commit --fixup=$COMMIT and similar tools, and thanks to the implicit base commit, you don't even need to specify it. You can even pass the -i flag if you want to edit the generated todo list before running it. It's non-destructive git-revise doesn't touch either your working directory, or your index. This means that if it's killed while running, your repository won't be changed, and you can't end up in a mid-rebase state while using it. Problems like conflicts are resolved interactively, while the command is running, without changing the actual files you've been working on. And, as no files are touched, git-revise won't trigger any unnecessary rebuilds! What git-revise is not (Section Added: Aug. 6, 2019) git-revise does not aim to be a complete replacement for git rebase -i. It has a specific use-case in mind, namely incremental changes to a patch stack, and excludes features which rebase supports. In my personal workflow, I still reach for git rebase [-i] when I need to rebase my local commits due to new upstream changes, and I imagine there are people with advanced workflows who cannot use git revise. Working directory changes: git-revise does not modify your working directory or index while it's running. This is part of what allows it to be so fast. However, it also means that certain rebase features, such as the edit interactive command, are not possible. This also is why git revise -i does not support removing commits from within a patch series: doing so would require changing the state of your working directory due to the now-missing commit. If you want to drop a commit you can instead move it to the end of the list and mark it as index. The commit will disappear from history, but your index and working directory won't be changed. A quick git reset --hard HEAD will update your index and working directory. These restrictions may change in the future. Features like this have been requested, and it might be useful to allow opting-in to dropping commits on the floor or pausing mid-revise. Merging through renames & copies: git-revise uses a custom merge backend, which doesn't attempt to handle file renames or copies. For changes which need to be merged or rebased through file renames and copies, git rebase is a better option. Complex history rewriting: git rebase supports rebasing complex commits, such as merges. In contrast, git-revise does not currently aim to support these more advanced features of git rebase. Interested? Awesome! git-revise is a MIT-licensed pure-Python 3.6+ package, and can be installed with pip: $ python3 -m pip install --user git-revise You can also check out the source on GitHub, and read the manpage online, or by running man git revise in your terminal. I'll leave you with some handy links to resources to learn more about git-revise, how it works, and how you can contribute! Repository: https://github.com/mystor/git-revise Bug Tracker: https://github.com/mystor/git-revise/issues Manpage: https://git-revise.readthedocs.io/en/latest/man.html Installing: https://git-revise.readthedocs.io/en/latest/install.html Contributing: https://git-revise.readthedocs.io/en/latest/contributing.html ",
          "Reminds me some of Raymond Chen's \"Stupid Git Tricks\" series  [0-6] of blog posts where he used a lot of git commit-tree and similar low level tools to avoid worktree changes and minimize GC churn (partly because of working in the humongous Windows git, which of course seems to have similar issues to the Mozilla ones mentioned here such as auto-rebuild tools).<p>It makes a bunch of sense to build nicer porcelain tools for such low level git magic when it becomes semi-routine.<p>(I couldn't find a good permalink for the entire series as a whole, so linked are all the individual posts.)<p>[0] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190506-00/?p=102478\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190506-00/?p=10...</a><p>[1] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190507-00/?p=102480\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190507-00/?p=10...</a><p>[2] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190508-00/?p=102482\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190508-00/?p=10...</a><p>[3] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190509-00/?p=102485\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190509-00/?p=10...</a><p>[4] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190510-00/?p=102488\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190510-00/?p=10...</a><p>[5] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190513-00/?p=102490\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190513-00/?p=10...</a><p>[6] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190515-00/?p=102495\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190515-00/?p=10...</a>",
          "This sounds exactly like my dream tool, the very thing I've been meaning to write myself and haven't gotten around to it. Thank you for writing this!<p>Suggestion: I'd love to see a `revise.autoSquash` git config flag (like `rebase.autoSquash`) to always autosquash in interactive mode. Maybe you already support it, but if so, the manpage doesn't list it."],
        "story_type":"Normal",
        "url_raw":"https://mystor.github.io/git-revise.html",
        "comments.comment_id":[20620441,
          20620524],
        "comments.comment_author":["WorldMaker",
          "eridius"],
        "comments.comment_descendants":[2,
          1],
        "comments.comment_time":["2019-08-05T23:37:35Z",
          "2019-08-05T23:47:19Z"],
        "comments.comment_text":["Reminds me some of Raymond Chen's \"Stupid Git Tricks\" series  [0-6] of blog posts where he used a lot of git commit-tree and similar low level tools to avoid worktree changes and minimize GC churn (partly because of working in the humongous Windows git, which of course seems to have similar issues to the Mozilla ones mentioned here such as auto-rebuild tools).<p>It makes a bunch of sense to build nicer porcelain tools for such low level git magic when it becomes semi-routine.<p>(I couldn't find a good permalink for the entire series as a whole, so linked are all the individual posts.)<p>[0] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190506-00/?p=102478\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190506-00/?p=10...</a><p>[1] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190507-00/?p=102480\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190507-00/?p=10...</a><p>[2] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190508-00/?p=102482\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190508-00/?p=10...</a><p>[3] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190509-00/?p=102485\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190509-00/?p=10...</a><p>[4] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190510-00/?p=102488\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190510-00/?p=10...</a><p>[5] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190513-00/?p=102490\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190513-00/?p=10...</a><p>[6] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190515-00/?p=102495\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190515-00/?p=10...</a>",
          "This sounds exactly like my dream tool, the very thing I've been meaning to write myself and haven't gotten around to it. Thank you for writing this!<p>Suggestion: I'd love to see a `revise.autoSquash` git config flag (like `rebase.autoSquash`) to always autosquash in interactive mode. Maybe you already support it, but if so, the manpage doesn't list it."],
        "id":"9599e1d7-ef9e-4795-856c-5276ca2f2f02",
        "url_text":"NIKA:\\git-revise\\> list (Aug. 6, 2019): Added the \"What git-revise is not\" section. At Mozilla I often end up building my changes in a patch stack, and used git rebase -i1 to make changes to commits in response to review comments etc. Unfortunately, with a repository as large as mozilla-central2, git rebase -i has some downsides: It's slow! Rebase operates directly on the worktree, so it performs a full checkout of each commit in the stack, and frequently refreshes worktree state. On large repositories (especially on NTFS) that can take a long time. It triggers rebuilds! Because rebase touches the file tree, some build systems (like gecko's recursive-make backend) rebuild unnecessarially. It's stateful! If the rebase fails, the repository is in a weird mid-rebase state, and in edge cases I've accidentally dropped commits due to other processes racing on the repository lock. It's clunky! Common tasks (like splitting & rewording commits) require multiple steps and are unintuitive. Naturally, I did the only reasonable thing: Build a brand-new tool. source: xkcd git-revise is a history editing tool designed for the patch-stack workflow. It's fast, non-destructive, and aims to provide a familiar, powerful, and easy to use re-imagining of the patch stack workflow. It's fast I would never claim to be a benchmarking expert 3, but git-revise performs substantially better than rebase for small history editing tasks 4. In a test applying a single-line change to a mozilla-central commit 20 patches up the stack I saw a 15x speed improvement. $ time bash -c 'git commit --fixup=$TARGET; EDITOR=true git rebase -i --autosquash $TARGET~' <snip> real 0m10.733s $ time git revise $TARGET <snip> real 0m0.685s git-revise accomplishes this using an in-memory rebase algorithm operating directly on git's trees, meaning it never has to touch your index or working directory, avoiding expensive disk I/O! It's handy git-revise isn't just a faster git rebase -i, it provides helpful commands, flags, and tools which make common changes faster, and easier: Fixup Fast $ git add . $ git revise HEAD~~ Running git revise $COMMIT directly collects changes staged in the index, and directly applies them to the specified commit. Conflicts are resolved interactively, and a warning will be shown if the final state of the tree is different from what you started with! With an extra -e, you can update the commit message at the same time, and -a will stage your changes, so you don't have to! 5 Split Commits $ git revise -c $COMMIT Select changes to be included in part [1]: diff --git b/file.txt a/file.txt <snip> Apply this hunk to index [y,n,q,a,d,e,?]? Sometimes, a commit needs to be split in two, perhaps because a change ended up in the wrong commit. The --cut flag (and cut interactive command) provides a fast way to split a commit in-place. Running git revise --cut $COMMIT will start a git add -p-style hunk selector, allowing you to pick changes for part 1, and the rest will end up in part 2. No more tinkering around with edit during a rebase to split off that comment you accidentally added to the wrong commit! Interactive Mode git-revise has a git rebase -i-style interactive mode, but with some quality-of-life improvements, on top of being fast: Implicit Base Commit If a base commit isn't provided, --interactive will implicitly locate a safe base commit to start from, walking up from HEAD, and stopping at published & merge commits. Often git revise -i is all you need! The index Todo Staged changes in the index automatically appear in interactive mode, and can be moved around and treated like any other commit in range. No need to turn it into a commit with a dummy name before you pop open interactive mode & squash it into another commit! Bulk Commit Rewording Ever wanted to update a bunch of commit messages at once? Perhaps they're all missing the bug number? Well, git revise -ie has you covered. It'll open a special Interactive Mode where each command is prefixed with a ++, and the full commit message is present after it. Changes made to these commit messages will be applied before executing the TODOs, meaning you can edit them in bulk. I use this constantly to add bug numbers, elaborate on commit details, and add reviewer information to commit messages. ++ pick f5a02a16731a Bug ??? - My commit summary, r=? The full commit message body follows! ++ pick fef1aeddd6fb Bug ??? - Another commit, r=? Another commit's body! Autosquash Support $ git revise --autosquash If you're used to git rebase -i --autosquash, revise works with you. Running git revise --autosquash will automatically reorder and apply fixup commits created with git commit --fixup=$COMMIT and similar tools, and thanks to the implicit base commit, you don't even need to specify it. You can even pass the -i flag if you want to edit the generated todo list before running it. It's non-destructive git-revise doesn't touch either your working directory, or your index. This means that if it's killed while running, your repository won't be changed, and you can't end up in a mid-rebase state while using it. Problems like conflicts are resolved interactively, while the command is running, without changing the actual files you've been working on. And, as no files are touched, git-revise won't trigger any unnecessary rebuilds! What git-revise is not (Section Added: Aug. 6, 2019) git-revise does not aim to be a complete replacement for git rebase -i. It has a specific use-case in mind, namely incremental changes to a patch stack, and excludes features which rebase supports. In my personal workflow, I still reach for git rebase [-i] when I need to rebase my local commits due to new upstream changes, and I imagine there are people with advanced workflows who cannot use git revise. Working directory changes: git-revise does not modify your working directory or index while it's running. This is part of what allows it to be so fast. However, it also means that certain rebase features, such as the edit interactive command, are not possible. This also is why git revise -i does not support removing commits from within a patch series: doing so would require changing the state of your working directory due to the now-missing commit. If you want to drop a commit you can instead move it to the end of the list and mark it as index. The commit will disappear from history, but your index and working directory won't be changed. A quick git reset --hard HEAD will update your index and working directory. These restrictions may change in the future. Features like this have been requested, and it might be useful to allow opting-in to dropping commits on the floor or pausing mid-revise. Merging through renames & copies: git-revise uses a custom merge backend, which doesn't attempt to handle file renames or copies. For changes which need to be merged or rebased through file renames and copies, git rebase is a better option. Complex history rewriting: git rebase supports rebasing complex commits, such as merges. In contrast, git-revise does not currently aim to support these more advanced features of git rebase. Interested? Awesome! git-revise is a MIT-licensed pure-Python 3.6+ package, and can be installed with pip: $ python3 -m pip install --user git-revise You can also check out the source on GitHub, and read the manpage online, or by running man git revise in your terminal. I'll leave you with some handy links to resources to learn more about git-revise, how it works, and how you can contribute! Repository: https://github.com/mystor/git-revise Bug Tracker: https://github.com/mystor/git-revise/issues Manpage: https://git-revise.readthedocs.io/en/latest/man.html Installing: https://git-revise.readthedocs.io/en/latest/install.html Contributing: https://git-revise.readthedocs.io/en/latest/contributing.html ",
        "_version_":1718938205674274816},
      {
        "story_id":21836184,
        "story_author":"kmf",
        "story_descendants":129,
        "story_score":316,
        "story_time":"2019-12-19T16:14:21Z",
        "story_title":"GitHub Actions is my new favorite free programming tool [video]",
        "search":["GitHub Actions is my new favorite free programming tool [video]",
          "Normal",
          "https://www.bytesized.xyz/github-actions-tutorial",
          "Enjoying these posts? Subscribe for more Subscribe to be notified of new content and support Bytesized! You'll be a part of the community helping keep this site independent and ad-free. Already a member? Sign in ",
          "I would hesitate to build on anything that relies on GitHub native tooling. GitHub support is absolutely the worst. Their tools fail in weird ways at times, and without support you will be stuck. For personal/non production apps, fine. But beware of using it as a core part of your infrastructure.",
          "I have spent the last two days fighting with these new Github tools. I've been trying to do a simple Hello-World Maven app, built on Github Actions and deployed to Github Packages. It does not work.<p>Github documentation is a disaster. They leave out critical parts. They don't provide examples. Everything they write is terse, confusing, and incomplete.<p>They have short little articles on how to do things, and for each sub-task they have a link to docs somewhere else. This would be fine, except the links don't point to anything useful.<p>To give an example: they say you can use the Github API to talk to Github Packages, but the link goes to their generic GraphQL documentation. They don't point to any reference material on the actual calls to the packages service. If it exists, I can't find it.<p>If you go to the main page in your account for Github Packages, it says that all you have to do is this:<p>mvn deploy -Dregistry=<a href=\"https://maven.pkg.github.com/mycompany\" rel=\"nofollow\">https://maven.pkg.github.com/mycompany</a> -Dtoken=GH_TOKEN<p>That is just straight-out wrong. It does not work.<p>Seriously, Github, you need to fire your documentation team and hire some people who know how to write. Perhaps you should hire people who have actually used your tools to write the docs. Or just provide some freakin' working examples."],
        "story_type":"Normal",
        "url_raw":"https://www.bytesized.xyz/github-actions-tutorial",
        "url_text":"Enjoying these posts? Subscribe for more Subscribe to be notified of new content and support Bytesized! You'll be a part of the community helping keep this site independent and ad-free. Already a member? Sign in ",
        "comments.comment_id":[21836945,
          21837957],
        "comments.comment_author":["pm90",
          "ccleve"],
        "comments.comment_descendants":[1,
          12],
        "comments.comment_time":["2019-12-19T17:21:27Z",
          "2019-12-19T18:55:23Z"],
        "comments.comment_text":["I would hesitate to build on anything that relies on GitHub native tooling. GitHub support is absolutely the worst. Their tools fail in weird ways at times, and without support you will be stuck. For personal/non production apps, fine. But beware of using it as a core part of your infrastructure.",
          "I have spent the last two days fighting with these new Github tools. I've been trying to do a simple Hello-World Maven app, built on Github Actions and deployed to Github Packages. It does not work.<p>Github documentation is a disaster. They leave out critical parts. They don't provide examples. Everything they write is terse, confusing, and incomplete.<p>They have short little articles on how to do things, and for each sub-task they have a link to docs somewhere else. This would be fine, except the links don't point to anything useful.<p>To give an example: they say you can use the Github API to talk to Github Packages, but the link goes to their generic GraphQL documentation. They don't point to any reference material on the actual calls to the packages service. If it exists, I can't find it.<p>If you go to the main page in your account for Github Packages, it says that all you have to do is this:<p>mvn deploy -Dregistry=<a href=\"https://maven.pkg.github.com/mycompany\" rel=\"nofollow\">https://maven.pkg.github.com/mycompany</a> -Dtoken=GH_TOKEN<p>That is just straight-out wrong. It does not work.<p>Seriously, Github, you need to fire your documentation team and hire some people who know how to write. Perhaps you should hire people who have actually used your tools to write the docs. Or just provide some freakin' working examples."],
        "id":"425a3af7-bd4a-4149-aca5-07c8a24f0292",
        "_version_":1718938253277528064},
      {
        "story_id":18896422,
        "story_author":"tosh",
        "story_descendants":12,
        "story_score":54,
        "story_time":"2019-01-13T12:31:44Z",
        "story_title":"Awesome Dotfiles",
        "search":["Awesome Dotfiles",
          "Normal",
          "https://github.com/webpro/awesome-dotfiles",
          "Awesome dotfiles A curated list of dotfiles resources. Inspired by the awesome list thing. Note that some articles or tools may look old or old-fashioned, but this usually means they're battle-tested and mature (like dotfiles themselves). Feel free to propose new articles, projects or tools! Articles Introductions Getting started with dotfiles (L. Kappert) Getting started with dotfiles (D. Vints) Managing your dotfiles Dotfiles Are Meant to Be Forked Dotfile discovery I do Dotfiles! Tutorials Setting up a new (OS X) development machine: Part 3 - Dotfiles and custom SSH config Setting Up a Mac Dev Machine From Zero to Hero With Dotfiles; Part 2 Using Git and GitHub to manage your dotfiles conf.d like directories for zsh/bash dotfiles Managing your dotfiles The best way to store your dotfiles: A bare Git repository Shell startup Shell startup scripts Zsh/Bash startup files loading order Using specific tools Using GNU Stow to manage your dotfiles Managing Dotfile Symlinks with GNU Stow Dotfiles and dev tools provisioned by Ansible Manage a development machine with Ansible Find dotfiles repos There are many great dotfiles repos out there, each containing their own inspiration and gems. One way to go through them is to search GitHub for \"dotfiles\". Also see: Google for \"dotfiles\" Archlinux collection Tip: search for a filename on GitHub, e.g. in:path .gitconfig. Example dotfiles repos A collection of the most popular, well-maintained, and collaborative dotfiles repositories & frameworks. Some projects contain just the dotfiles. Others go further by allowing you to easily add your own custom dotfiles, and some include scripts to manage dotfiles and plugins. Bash Title Description Focus Bash it Community bash framework. Autocompletion, themes, aliases, custom functions. Well-structured framework. Mathiass dotfiles Sensible hacker defaults for macOS Lots of goodness here, great collaborative community effort. Maximum Awesome Config files for vim and tmux Vim, tmux. Built for Mac OS X. webpro's dotfiles macOS dotfiles Bash, Homebrew, Brew Cask, Git, Node.js, Hammerspoon. rootbeersoup's dotfiles Effortless Bash, Vim and macOS configuration A curl | sh installer and a Makefile offer portable and effortless setup for either permanent or temporary configuration. Luke's voidrice Arch linux dotfile bootstrap Bloatless, often suckless software. Vim config for editing documents in markdown or latex Zsh Title Description Focus thoughtbot dotfiles Set of vim, zsh, git, and tmux configuration files Zsh, vim, tmux, git, homebrew. Uses rcm. oh-my-zsh Community-driven framework for managing your zsh configuration. Includes 200+ optional plugins (rails, git, OSX, hub, capistrano, brew, ant, php, python, etc), over 140 themes to spice up your morning, and an auto-update tool. Prezto The configuration framework for Zsh. Enriches the command line interface environment with sane defaults, aliases, functions, auto completion, and prompt themes. YADR The best vim, git, zsh plugins and the cleanest vimrc you've ever seen Homebrew, zsh, git, vim, and more. Active repository. antigen Plugin manager for zsh, inspired by oh-my-zsh and vundle. Antigen is a small set of functions that help you easily manage your shell (zsh) plugins. Antigen is to zsh, what Vundle is to vim. Dries's dotfiles Simplified approach to dotfiles for macOS Zsh, Oh My Zsh, macOS, Homebrew, Mackup sobolevn's dotfiles Dotfiles for the developer happiness Zsh, Brew, Sublime, Python, Node, Elixir Fish Title Description Focus oh-my-fish Community Fish framework. Includes many plugins and themes, with installation, auto-update, and scaffolding tools. Paul's dotfiles Abundant dotfiles with a plethora of cool custom functions Fish, macOS, Homebrew, Custom Shell functions rkalis's dotfiles Well-maintained dotfiles featuring Fish, repository management and Hammerspoon Fish, macOS, Homebrew, Repository management, Hammerspoon Ansible Title Description Focus .dots New and upgraded dotfiles, now with Ansible! Completely automated desktop setup, configuration and maintenance using Ansible sloria's dotfiles sloria's dotfiles as Ansible roles Sets up a full local development environment with a single command Tools Ansible - Radically simple configuration-management, application deployment, task-execution, and multinode orchestration engine. bashdot - Minimalist dotfile management framework written entirely in bash. chezmoi - Manage your dotfiles securely across multiple machines. comtrya - Configuration management for localhost, written in Rust, for Linux, BSD, macOS, and Windows dotbare - Manage dotfiles interactively with fzf. dotbot - Tool that bootstraps your dotfiles. dotdrop - Save your dotfiles once, deploy them everywhere. dotstow - Manage dotfiles with stow. emplace - Synchronize installed packages on multiple machines using a dotfiles repository. Fisher - A package manager for Fish fresh - Keep your dotfiles fresh. Fresh is a tool to source shell configuration (aliases, functions, etc) from others into your own configuration files. GNU Stow - Symlink farm manager which takes distinct packages of software and/or data located in separate directories on the filesystem, and makes them appear to be installed in the same place. homeshick - Git dotfile synchronizer written in Bash. homesick - Your home directory is your castle. Don't leave your dotfiles behind (article). mackup - Keep your application settings in sync (OS X/Linux). Pearl - Package manager that allows to control, sync, share dotfiles as packages automatically activated during shells or editors startup. There is a wide range of packages already available in the Official Pearl Hub (for Linux and OSX). rcm - rc file (dotfile) management. themer - Manage and generate themes across your development tools from within your dotfiles. toml-bombadil - Templatize and manage your dotfiles. yadm - Tool for managing a collection of files across multiple computers, using a shared Git repository and some additional features. macOS dockutil - Command line tool for managing dock items mas - Mac App Store command line interface zero - Radically simple personal bootstrapping tool for macOS. Miscellaneous dotfiles.github.io - Your unofficial guide to dotfiles on GitHub. OS X Defaults - Centralized place for the awesome work started by @mathiasbynens on .macos. Filesystem Hierarchy Standard - Directory structure and directory contents in Linux distributions. XDG Base Directory Specification - Summary A lesson in shortcuts - How the idea of \"hidden\" or \"dot\" files was born, by Rob Pike (originally posted on Google+) Related Lists Awesome Dev Env - Curated list of awesome tools, resources and workflow tips making an awesome development environment. Awesome Fish - Curated list of packages, prompts, and resources for the fish shell. Awesome Shell - Curated list of awesome command-line frameworks, toolkits, guides and gizmos. Awesome Sysadmin - A curated list of amazingly awesome open source sysadmin resources. Awesome Zsh Plugins - List of Zsh plugins suitable for use with oh-my-zsh, antigen & Prezto. Terminals Are Sexy - A curated list of Terminal frameworks, plugins & resources for CLI lovers. Archive/abandoned projects Bashstrap battleschool Bork Cider dev-setup dotfiles Eduardo's dotfiles ellipsis holman does dotfiles Kevin's dotfiles kody osxc vcsh (article, article) License To the extent possible under law, Lars Kappert has waived all copyright and related or neighboring rights to this work. ",
          "The best way I've found to organize dotfiles is to simply use a bare git repo.<p>> git init --bare $HOME/.dotfiles`<p>> alias dot=\"git --git-dir=$HOME/.dotfiles/ --work-tree=$HOME\"<p>> dot config --local status.showUntrackedFiles no",
          "I have a totally awesome .bash_aliases file! I've taken all the best things I've seen from all the best .bash_aliases files around the web. I bet it's like 1000 lines long.<p>I can remember about 10 of the things I have in there."],
        "story_type":"Normal",
        "url_raw":"https://github.com/webpro/awesome-dotfiles",
        "comments.comment_id":[18898081,
          18898412],
        "comments.comment_author":["anschwa",
          "blakesterz"],
        "comments.comment_descendants":[3,
          1],
        "comments.comment_time":["2019-01-13T18:59:57Z",
          "2019-01-13T20:12:20Z"],
        "comments.comment_text":["The best way I've found to organize dotfiles is to simply use a bare git repo.<p>> git init --bare $HOME/.dotfiles`<p>> alias dot=\"git --git-dir=$HOME/.dotfiles/ --work-tree=$HOME\"<p>> dot config --local status.showUntrackedFiles no",
          "I have a totally awesome .bash_aliases file! I've taken all the best things I've seen from all the best .bash_aliases files around the web. I bet it's like 1000 lines long.<p>I can remember about 10 of the things I have in there."],
        "id":"336ad6f1-ec43-450a-95e5-a928dceb61b9",
        "url_text":"Awesome dotfiles A curated list of dotfiles resources. Inspired by the awesome list thing. Note that some articles or tools may look old or old-fashioned, but this usually means they're battle-tested and mature (like dotfiles themselves). Feel free to propose new articles, projects or tools! Articles Introductions Getting started with dotfiles (L. Kappert) Getting started with dotfiles (D. Vints) Managing your dotfiles Dotfiles Are Meant to Be Forked Dotfile discovery I do Dotfiles! Tutorials Setting up a new (OS X) development machine: Part 3 - Dotfiles and custom SSH config Setting Up a Mac Dev Machine From Zero to Hero With Dotfiles; Part 2 Using Git and GitHub to manage your dotfiles conf.d like directories for zsh/bash dotfiles Managing your dotfiles The best way to store your dotfiles: A bare Git repository Shell startup Shell startup scripts Zsh/Bash startup files loading order Using specific tools Using GNU Stow to manage your dotfiles Managing Dotfile Symlinks with GNU Stow Dotfiles and dev tools provisioned by Ansible Manage a development machine with Ansible Find dotfiles repos There are many great dotfiles repos out there, each containing their own inspiration and gems. One way to go through them is to search GitHub for \"dotfiles\". Also see: Google for \"dotfiles\" Archlinux collection Tip: search for a filename on GitHub, e.g. in:path .gitconfig. Example dotfiles repos A collection of the most popular, well-maintained, and collaborative dotfiles repositories & frameworks. Some projects contain just the dotfiles. Others go further by allowing you to easily add your own custom dotfiles, and some include scripts to manage dotfiles and plugins. Bash Title Description Focus Bash it Community bash framework. Autocompletion, themes, aliases, custom functions. Well-structured framework. Mathiass dotfiles Sensible hacker defaults for macOS Lots of goodness here, great collaborative community effort. Maximum Awesome Config files for vim and tmux Vim, tmux. Built for Mac OS X. webpro's dotfiles macOS dotfiles Bash, Homebrew, Brew Cask, Git, Node.js, Hammerspoon. rootbeersoup's dotfiles Effortless Bash, Vim and macOS configuration A curl | sh installer and a Makefile offer portable and effortless setup for either permanent or temporary configuration. Luke's voidrice Arch linux dotfile bootstrap Bloatless, often suckless software. Vim config for editing documents in markdown or latex Zsh Title Description Focus thoughtbot dotfiles Set of vim, zsh, git, and tmux configuration files Zsh, vim, tmux, git, homebrew. Uses rcm. oh-my-zsh Community-driven framework for managing your zsh configuration. Includes 200+ optional plugins (rails, git, OSX, hub, capistrano, brew, ant, php, python, etc), over 140 themes to spice up your morning, and an auto-update tool. Prezto The configuration framework for Zsh. Enriches the command line interface environment with sane defaults, aliases, functions, auto completion, and prompt themes. YADR The best vim, git, zsh plugins and the cleanest vimrc you've ever seen Homebrew, zsh, git, vim, and more. Active repository. antigen Plugin manager for zsh, inspired by oh-my-zsh and vundle. Antigen is a small set of functions that help you easily manage your shell (zsh) plugins. Antigen is to zsh, what Vundle is to vim. Dries's dotfiles Simplified approach to dotfiles for macOS Zsh, Oh My Zsh, macOS, Homebrew, Mackup sobolevn's dotfiles Dotfiles for the developer happiness Zsh, Brew, Sublime, Python, Node, Elixir Fish Title Description Focus oh-my-fish Community Fish framework. Includes many plugins and themes, with installation, auto-update, and scaffolding tools. Paul's dotfiles Abundant dotfiles with a plethora of cool custom functions Fish, macOS, Homebrew, Custom Shell functions rkalis's dotfiles Well-maintained dotfiles featuring Fish, repository management and Hammerspoon Fish, macOS, Homebrew, Repository management, Hammerspoon Ansible Title Description Focus .dots New and upgraded dotfiles, now with Ansible! Completely automated desktop setup, configuration and maintenance using Ansible sloria's dotfiles sloria's dotfiles as Ansible roles Sets up a full local development environment with a single command Tools Ansible - Radically simple configuration-management, application deployment, task-execution, and multinode orchestration engine. bashdot - Minimalist dotfile management framework written entirely in bash. chezmoi - Manage your dotfiles securely across multiple machines. comtrya - Configuration management for localhost, written in Rust, for Linux, BSD, macOS, and Windows dotbare - Manage dotfiles interactively with fzf. dotbot - Tool that bootstraps your dotfiles. dotdrop - Save your dotfiles once, deploy them everywhere. dotstow - Manage dotfiles with stow. emplace - Synchronize installed packages on multiple machines using a dotfiles repository. Fisher - A package manager for Fish fresh - Keep your dotfiles fresh. Fresh is a tool to source shell configuration (aliases, functions, etc) from others into your own configuration files. GNU Stow - Symlink farm manager which takes distinct packages of software and/or data located in separate directories on the filesystem, and makes them appear to be installed in the same place. homeshick - Git dotfile synchronizer written in Bash. homesick - Your home directory is your castle. Don't leave your dotfiles behind (article). mackup - Keep your application settings in sync (OS X/Linux). Pearl - Package manager that allows to control, sync, share dotfiles as packages automatically activated during shells or editors startup. There is a wide range of packages already available in the Official Pearl Hub (for Linux and OSX). rcm - rc file (dotfile) management. themer - Manage and generate themes across your development tools from within your dotfiles. toml-bombadil - Templatize and manage your dotfiles. yadm - Tool for managing a collection of files across multiple computers, using a shared Git repository and some additional features. macOS dockutil - Command line tool for managing dock items mas - Mac App Store command line interface zero - Radically simple personal bootstrapping tool for macOS. Miscellaneous dotfiles.github.io - Your unofficial guide to dotfiles on GitHub. OS X Defaults - Centralized place for the awesome work started by @mathiasbynens on .macos. Filesystem Hierarchy Standard - Directory structure and directory contents in Linux distributions. XDG Base Directory Specification - Summary A lesson in shortcuts - How the idea of \"hidden\" or \"dot\" files was born, by Rob Pike (originally posted on Google+) Related Lists Awesome Dev Env - Curated list of awesome tools, resources and workflow tips making an awesome development environment. Awesome Fish - Curated list of packages, prompts, and resources for the fish shell. Awesome Shell - Curated list of awesome command-line frameworks, toolkits, guides and gizmos. Awesome Sysadmin - A curated list of amazingly awesome open source sysadmin resources. Awesome Zsh Plugins - List of Zsh plugins suitable for use with oh-my-zsh, antigen & Prezto. Terminals Are Sexy - A curated list of Terminal frameworks, plugins & resources for CLI lovers. Archive/abandoned projects Bashstrap battleschool Bork Cider dev-setup dotfiles Eduardo's dotfiles ellipsis holman does dotfiles Kevin's dotfiles kody osxc vcsh (article, article) License To the extent possible under law, Lars Kappert has waived all copyright and related or neighboring rights to this work. ",
        "_version_":1718938137697189888},
      {
        "story_id":20936679,
        "story_author":"marlynm",
        "story_descendants":1,
        "story_score":11,
        "story_time":"2019-09-11T04:57:48Z",
        "story_title":"Show HN: GitHub action to setup PHP with required extensions and Composer",
        "search":["Show HN: GitHub action to setup PHP with required extensions and Composer",
          "ShowHN",
          "https://github.com/shivammathur/setup-php",
          "Setup PHP in GitHub Actions Setup PHP with required extensions, php.ini configuration, code-coverage support and various tools like composer in GitHub Actions. This action gives you a cross platform interface to set up the PHP environment you need to test your application. Refer to Usage section and examples to see how to use this. Contents OS/Platform Support GitHub-Hosted Runners Self-Hosted Runners PHP Support PHP Extension Support Tools Support Coverage Support Xdebug PCOV Disable Coverage Usage Inputs Outputs Flags Basic Setup Matrix Setup Nightly Build Setup Thread Safe Setup Force Update Setup Verbose Setup Multi-Arch Setup Self Hosted Setup Local Testing Setup JIT Configuration Cache Extensions Cache Composer Dependencies Composer GitHub OAuth Inline PHP Scripts Problem Matchers Examples Versioning License Contributions Support This Project Dependencies Further Reading OS/Platform Support Both GitHub-hosted and self-hosted runners are suppported by setup-php on the following OS/Platforms. GitHub-Hosted Runners Virtual environment YAML workflow label Pre-installed PHP Ubuntu 18.04 ubuntu-18.04 PHP 7.1 to PHP 8.0 Ubuntu 20.04 ubuntu-latest or ubuntu-20.04 PHP 7.4 to PHP 8.0 Windows Server 2019 windows-latest or windows-2019 PHP 8.0 Windows Server 2022 windows-2022 PHP 8.0 macOS Catalina 10.15 macos-latest or macos-10.15 PHP 8.0 macOS Big Sur 11.x macos-11 PHP 8.0 Self-Hosted Runners Host OS/Virtual environment YAML workflow label Ubuntu 18.04 self-hosted or Linux Ubuntu 20.04 self-hosted or Linux Ubuntu 21.04 self-hosted or Linux Debian 9 self-hosted or Linux Debian 10 self-hosted or Linux Debian 11 self-hosted or Linux Windows 7 and newer self-hosted or Windows Windows Server 2012 R2 and newer self-hosted or Windows macOS Catalina 10.15 self-hosted or macOS macOS Big Sur 11.x x86_64/arm64 self-hosted or macOS Refer to the self-hosted setup to use the action on self-hosted runners. Operating systems based on the above Ubuntu and Debian versions are also supported on a best effort basis. If the requested PHP version is pre-installed, setup-php switches to it, otherwise it installs the PHP version. PHP Support On all supported OS/Platforms the following PHP versions are supported as per the runner. PHP 5.3 to PHP 8.2 on GitHub-hosted runners. PHP 5.6 to PHP 8.2 on self-hosted runners. PHP Version Stability Release Support Runner Support 5.3 Stable End of life GitHub-hosted 5.4 Stable End of life GitHub-hosted 5.5 Stable End of life GitHub-hosted 5.6 Stable End of life GitHub-hosted, self-hosted 7.0 Stable End of life GitHub-hosted, self-hosted 7.1 Stable End of life GitHub-hosted, self-hosted 7.2 Stable End of life GitHub-hosted, self-hosted 7.3 Stable Security fixes only GitHub-hosted, self-hosted 7.4 Stable Active GitHub-hosted, self-hosted 8.0 Stable Active GitHub-hosted, self-hosted 8.1 Nightly In development GitHub-hosted, self-hosted 8.2 Nightly In development GitHub-hosted, self-hosted Notes: Specifying 8.1 and 8.2 in php-version input installs a nightly build of PHP 8.1.0-dev and PHP 8.2.0-dev respectively. See nightly build setup for more information. To use JIT on PHP 8.0 and above, refer to the JIT configuration section. PHP Extension Support PHP extensions can be set up using the extensions input. It accepts a string in csv-format. On Ubuntu, extensions which are available as a package, available on PECL or a git repository can be set up. - name: Setup PHP with PECL extension uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: imagick, swoole On Windows, extensions available on PECL which have the DLL binary can be set up. On macOS, extensions available on PECL or a git repository can be set up. On Ubuntu and macOS to compile and install an extension from a git repository follow this guide. Extensions installed along with PHP if specified are enabled. Specific versions of extensions available on PECL can be set up by suffixing the extension's name with the version. This is useful for installing old versions of extensions which support end of life PHP versions. - name: Setup PHP with specific version of PECL extension uses: shivammathur/setup-php@v2 with: php-version: '5.4' extensions: swoole-1.9.3 Pre-release versions extensions available on PECL can be set up by suffixing the extension's name with its state i.e alpha, beta, devel or snapshot. - name: Setup PHP with pre-release PECL extension uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: xdebug-beta Shared extensions can be disabled by prefixing them with a :. All extensions depending on the specified extension will also be disabled. - name: Setup PHP and disable opcache uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: :opcache All shared extensions can be disabled by specifying none. When none is specified along with other extensions, it is hoisted to the start of the input. So, all the shared extensions will be disabled first, then rest of the extensions in the input will be processed. Note: This disables all core and third-party shared extensions and thus, can break some tools which need them. So, make sure you add the required extensions after none in the extensions input. - name: Setup PHP without any shared extensions except mbstring uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: none, mbstring Extension intl can be set up with specific ICU version for PHP 5.6 and above in Ubuntu workflows by suffixing intl with the ICU version. ICU 50.2 and newer versions are supported. Refer to ICU builds for the specific versions supported. - name: Setup PHP with intl uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: intl-69.1 Extensions loaded by default after setup-php runs can be found on the wiki. These extensions have custom support: cubrid, pdo_cubrid and gearman on Ubuntu. geos on Ubuntu and macOS. blackfire, couchbase, ioncube, oci8, pdo_firebird, pdo_oci, pecl_http, phalcon3 and phalcon4 on all supported OS. By default, extensions which cannot be added or disabled gracefully leave an error message in the logs, the action is not interrupted. To change this behaviour you can set fail-fast flag to true. - name: Setup PHP with fail-fast uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: oci8 env: fail-fast: true Tools Support These tools can be set up globally using the tools input. It accepts a string in csv-format. behat, blackfire, blackfire-player, codeception, composer, composer-normalize, composer-prefetcher, composer-require-checker, composer-unused, cs2pr, deployer, flex, grpc_php_plugin, infection, parallel-lint, pecl, phan, phing, phinx, phive, php-config, php-cs-fixer, phpcbf, phpcpd, phpcs, phpdoc or phpDocumentor, phpize, phplint, phpmd, phpspec, phpstan, phpunit, phpunit-bridge, phpunit-polyfills, prestissimo, protoc, psalm, symfony or symfony-cli, vapor or vapor-cli, wp or wp-cli - name: Setup PHP with tools uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: php-cs-fixer, phpunit In addition to above tools any composer tool or package can also be set up globally by specifying it as vendor/package matching the listing on Packagist. This format accepts the same version constraints as composer. - name: Setup PHP with tools uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: vimeo/psalm To set up a particular version of a tool, specify it in the form tool:version. Version can be in the following format: Semver. For example tool:1.2.3 or tool:1.2.3-beta1. Major version. For example tool:1 or tool:1.x. Major and minor version. For example tool:1.2 or tool:1.2.x. When you specify just the major version or the version in major.minor format, the latest patch version matching the input will be setup. Except for major versions of composer, For other tools when you specify only the major version or the version in major.minor format for any tool you can get rate limited by GitHub's API. To avoid this, it is recommended to provide a GitHub OAuth token. You can do that by setting COMPOSER_TOKEN environment variable. - name: Setup PHP with tools uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: php-cs-fixer:3, phpunit:8.5 env: COMPOSER_TOKEN: ${{ secrets.GITHUB_TOKEN }} The latest stable version of composer is set up by default. You can set up the required composer version by specifying the major version v1 or v2, or the version in major.minor or semver format, Additionally for composer snapshot and preview can also be specified to set up the respective releases. - name: Setup PHP with composer v2 uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: composer:v2 If you do not use composer in your workflow, you can specify tools: none to skip it. - name: Setup PHP without composer uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: none Scripts phpize and php-config are set up with the same version as of the input PHP version. The latest version of blackfire cli is set up when blackfire is specified in tools input. Please refer to the official documentation for using blackfire with GitHub Actions. Tools prestissimo and composer-prefetcher will be skipped unless composer:v1 is also specified in tools input. It is recommended to drop prestissimo and use composer v2. By default, tools which cannot be set up gracefully leave an error message in the logs, the action is not interrupted. To change this behaviour you can set fail-fast flag to true. - name: Setup PHP with fail-fast uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: deployer env: fail-fast: true Notes Input tools is useful to set up tools which you only use in GitHub Actions, thus keeping your composer.json tidy. If you do not want to use all your dev-dependencies in GitHub Actions workflow, you can run composer with --no-dev and install required tools using tools input to speed up your workflow. If you have a tool in your composer.json, do not set up it with tools input as the two instances of the tool might conflict. Coverage Support Xdebug Specify coverage: xdebug to use Xdebug and disable PCOV. Runs on all PHP versions supported. - name: Setup PHP with Xdebug uses: shivammathur/setup-php@v2 with: php-version: '8.0' coverage: xdebug The latest version of Xdebug compatible with the PHP version is set up by default. If you need Xdebug 2.x on PHP 7.2, 7.3 or 7.4, you can specify coverage: xdebug2. - name: Setup PHP with Xdebug 2.x uses: shivammathur/setup-php@v2 with: php-version: '7.4' coverage: xdebug2 PCOV Specify coverage: pcov to use PCOV and disable Xdebug. Runs on PHP 7.1 and newer PHP versions. If your source code directory is other than src, lib or, app, specify pcov.directory using the ini-values input. - name: Setup PHP with PCOV uses: shivammathur/setup-php@v2 with: php-version: '8.0' ini-values: pcov.directory=api #optional, see above for usage. coverage: pcov PHPUnit 8.x and above supports PCOV out of the box. If you are using PHPUnit 5.x, 6.x or 7.x, you need to set up pcov/clobber before executing your tests. - name: Setup PCOV run: | composer require pcov/clobber vendor/bin/pcov clobber Disable Coverage Specify coverage: none to disable both Xdebug and PCOV. Disable coverage for these reasons: You are not generating coverage reports while testing. It will disable Xdebug, which will have a positive impact on PHP performance. You are using phpdbg for running your tests. You are profiling your code using blackfire. You are using PHP in JIT mode. Please refer to JIT configuration section for more details. - name: Setup PHP with no coverage driver uses: shivammathur/setup-php@v2 with: php-version: '8.0' coverage: none Usage Inputs Specify using with keyword php-version (required) Specify the PHP version you want to set up. Accepts a string. For example '8.0'. Accepts latest to set up the latest stable PHP version. Accepts nightly to set up a nightly build from the master branch of PHP. Accepts the format d.x, where d is the major version. For example 5.x, 7.x and 8.x. See PHP support for supported PHP versions. extensions (optional) Specify the extensions you want to add or disable. Accepts a string in csv-format. For example mbstring, :opcache. Accepts none to disable all shared extensions. Shared extensions prefixed with : are disabled. See PHP extension support for more info. ini-values (optional) Specify the values you want to add to php.ini. Accepts a string in csv-format. For example post_max_size=256M, max_execution_time=180. Accepts ini values with commas if wrapped in quotes. For example xdebug.mode=\"develop,coverage\". coverage (optional) Specify the code-coverage driver you want to set up. Accepts xdebug, pcov or none. See coverage support for more info. tools (optional) Specify the tools you want to set up. Accepts a string in csv-format. For example: phpunit, phpcs See tools Support for tools supported. Outputs php-version To use outputs, give the setup-php step an id, you can use the same to get the outputs in a later step. Provides the PHP version in semver format. - name: Setup PHP id: setup-php uses: shivammathur/setup-php@v2 with: php-version: '8.0' - name: Print PHP version run: echo ${{ steps.setup-php.outputs.php-version }} Flags Specify using env keyword fail-fast (optional) Specify to mark the workflow as failed if an extension or tool fails to set up. This changes the default mode from graceful warnings to fail-fast. By default, it is set to false. Accepts true and false. phpts (optional) Specify to set up thread-safe version of PHP on Windows. Accepts ts and nts. By default, it is set to nts. See thread safe setup for more info. update (optional) Specify to update PHP on the runner to the latest patch version. Accepts true and false. By default, it is set to false. See force update setup for more info. See below for more info. Basic Setup Setup a particular PHP version. steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: mbstring, intl ini-values: post_max_size=256M, max_execution_time=180 coverage: xdebug tools: php-cs-fixer, phpunit Matrix Setup Setup multiple PHP versions on multiple operating systems. jobs: run: runs-on: ${{ matrix.operating-system }} strategy: matrix: operating-system: ['ubuntu-latest', 'windows-latest', 'macos-latest'] php-versions: ['7.3', '7.4', '8.0'] phpunit-versions: ['latest'] include: - operating-system: 'ubuntu-latest' php-versions: '7.2' phpunit-versions: '8.5.21' steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: ${{ matrix.php-versions }} extensions: mbstring, intl ini-values: post_max_size=256M, max_execution_time=180 coverage: xdebug tools: php-cs-fixer, phpunit:${{ matrix.phpunit-versions }} Nightly Build Setup Setup a nightly build of PHP 8.1 or PHP 8.2. This version is currently in development. Some user space extensions might not support this version currently. steps: - name: Setup nightly PHP uses: shivammathur/setup-php@v2 with: php-version: '8.1' extensions: mbstring ini-values: post_max_size=256M, max_execution_time=180 coverage: xdebug tools: php-cs-fixer, phpunit Thread Safe Setup Setup TS or NTS PHP on Windows. NTS versions are set up by default. On Ubuntu and macOS only NTS versions are supported. On Windows both TS and NTS versions are supported. jobs: run: runs-on: windows-latest name: Setup PHP TS on Windows steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' env: phpts: ts # specify ts or nts Force Update Setup Update to the latest patch of PHP versions. Pre-installed PHP versions on the GitHub Actions images are not updated to their latest patch release by default. You can specify the update environment variable to true for updating to the latest release. - name: Setup PHP with latest versions uses: shivammathur/setup-php@v2 with: php-version: '8.0' env: update: true # specify true or false Verbose Setup Debug your workflow To debug any issues, you can use the verbose tag instead of v2. - name: Setup PHP with logs uses: shivammathur/setup-php@verbose with: php-version: '8.0' Multi-Arch Setup Setup PHP on multiple architecture on Ubuntu GitHub Runners. PHP 5.6 to PHP 8.0 are supported by setup-php on multiple architecture on Ubuntu. For this, you can use shivammathur/node images as containers. These have compatible Nodejs installed for JavaScript based GitHub Actions. Currently, for ARM based setup, you will need self-hosted runners. jobs: run: runs-on: ubuntu-latest container: shivammathur/node:latest-${{ matrix.arch }} strategy: matrix: arch: [\"amd64\", \"i386\"] steps: - name: Install PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' Self Hosted Setup Setup PHP on a self-hosted runner. To set up a containerised self-hosted runner, refer to the following guides as per your base operating system. Linux Windows To set up the runner directly on the host OS or in a virtual machine, follow this requirements guide before setting up the self-hosted runner. If your workflow uses service containers, then set up the runner on a Linux host or in a Linux virtual machine. GitHub Actions does not support nested virtualization on Linux, so services will not work in a dockerized container. Specify the environment variable runner with the value self-hosted. Without this your workflow will fail. jobs: run: runs-on: self-hosted strategy: matrix: php-versions: ['5.6', '7.0', '7.1', '7.2', '7.3', '7.4', '8.0'] name: PHP ${{ matrix.php-versions }} steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: ${{ matrix.php-versions }} env: runner: self-hosted # Specify the runner. Notes Do not set up multiple self-hosted runners on a single server instance as parallel workflow will conflict with each other. Do not set up self-hosted runners on the side on your development environment or your production server. Avoid using the same labels for your self-hosted runners which are used by GitHub-hosted runners. Local Testing Setup Test your Ubuntu workflow locally using nektos/act. jobs: run: runs-on: ubuntu-latest steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' Run the workflow locally with act using shivammathur/node docker images. Choose the image tag which matches the runs-on property in your workflow. For example, if you are using ubuntu-20.04 in your workflow, run act -P ubuntu-20.04=shivammathur/node:20.04. # For runs-on: ubuntu-latest act -P ubuntu-latest=shivammathur/node:latest # For runs-on: ubuntu-20.04 act -P ubuntu-20.04=shivammathur/node:2004 # For runs-on: ubuntu-18.04 act -P ubuntu-18.04=shivammathur/node:1804 JIT Configuration Enable Just-in-time(JIT) on PHP 8.0 and above. To enable JIT, enable opcache in cli mode by setting opcache.enable_cli=1. JIT conflicts with Xdebug, PCOV, and other extensions which override zend_execute_ex function, so set coverage: none and disable any such extension if added. By default, opcache.jit=1235 and opcache.jit_buffer_size=256M are set which can be changed using ini-values input. For detailed information about JIT related directives refer to the official PHP documentation. For example to enable JIT in tracing mode with buffer size of 64 MB. - name: Setup PHP with JIT in tracing mode uses: shivammathur/setup-php@v2 with: php-version: '8.0' coverage: none ini-values: opcache.enable_cli=1, opcache.jit=tracing, opcache.jit_buffer_size=64M Cache Extensions You can cache PHP extensions using shivammathur/cache-extensions and action/cache GitHub Actions. Extensions which take very long to set up when cached are available in the next workflow run and are enabled directly. This reduces the workflow execution time. Refer to shivammathur/cache-extensions for details. Cache Composer Dependencies If your project uses composer, you can persist the composer's internal cache directory. Dependencies cached are loaded directly instead of downloading them while installation. The files cached are available across check-runs and will reduce the workflow execution time. - name: Get composer cache directory id: composer-cache run: echo \"::set-output name=dir::$(composer config cache-files-dir)\" - name: Cache dependencies uses: actions/cache@v2 with: path: ${{ steps.composer-cache.outputs.dir }} key: ${{ runner.os }}-composer-${{ hashFiles('**/composer.lock') }} restore-keys: ${{ runner.os }}-composer- - name: Install dependencies run: composer install --prefer-dist Notes Please do not cache vendor directory using action/cache as that will have side effects. If you do not commit composer.lock, you can use the hash of composer.json as the key for your cache. key: ${{ runner.os }}-composer-${{ hashFiles('**/composer.json') }} If you support a range of composer dependencies and use prefer-lowest and prefer-stable options, you can store them in your matrix and add them to the keys. key: ${{ runner.os }}-composer-${{ hashFiles('**/composer.lock') }}-${{ matrix.prefer }}- restore-keys: ${{ runner.os }}-composer-${{ matrix.prefer }}- Composer GitHub OAuth If you have a number of workflows which set up multiple tools or have many composer dependencies, you might hit the GitHub's rate limit for composer. Also, if you specify only the major version or the version in major.minor format, you can hit the rate limit. To avoid this you can specify an OAuth token by setting COMPOSER_TOKEN environment variable. You can use GITHUB_TOKEN secret for this purpose. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' env: COMPOSER_TOKEN: ${{ secrets.GITHUB_TOKEN }} Inline PHP Scripts If you have to run multiple lines of PHP code in your workflow, you can do that easily without saving it to a file. Put the code in the run property of a step and specify the shell as php {0}. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' - name: Run PHP code shell: php {0} run: | <?php $welcome = \"Hello, world\"; echo $welcome; Problem Matchers Problem matchers are json configurations which identify errors and warnings in your logs and surface them prominently in the GitHub Actions UI by highlighting them and creating code annotations. PHP Setup problem matchers for your PHP output by adding this step after the setup-php step. - name: Setup problem matchers for PHP run: echo \"::add-matcher::${{ runner.tool_cache }}/php.json\" PHPUnit Setup problem matchers for your PHPUnit output by adding this step after the setup-php step. - name: Setup problem matchers for PHPUnit run: echo \"::add-matcher::${{ runner.tool_cache }}/phpunit.json\" PHPStan PHPStan supports error reporting in GitHub Actions, so it does not require problem matchers. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: phpstan - name: Run PHPStan run: phpstan analyse src Psalm Psalm supports error reporting in GitHub Actions with an output format github. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: psalm - name: Run Psalm run: psalm --output-format=github Tools with checkstyle support For tools that support checkstyle reporting like phpstan, psalm, php-cs-fixer and phpcs you can use cs2pr to annotate your code. For examples refer to cs2pr documentation. Here is an example with phpcs. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: cs2pr, phpcs - name: Run phpcs run: phpcs -q --report=checkstyle src | cs2pr Examples Examples of using setup-php with various PHP Frameworks and Packages. Framework/Package Runs on Workflow Blackfire macOS, ubuntu and windows blackfire.yml Blackfire Player macOS, ubuntu and windows blackfire-player.yml CakePHP with MySQL and Redis ubuntu cakephp-mysql.yml CakePHP with PostgreSQL and Redis ubuntu cakephp-postgres.yml CakePHP without services macOS, ubuntu and windows cakephp.yml CodeIgniter macOS, ubuntu and windows codeigniter.yml Laravel with MySQL and Redis ubuntu laravel-mysql.yml Laravel with PostgreSQL and Redis ubuntu laravel-postgres.yml Laravel without services macOS, ubuntu and windows laravel.yml Lumen with MySQL and Redis ubuntu lumen-mysql.yml Lumen with PostgreSQL and Redis ubuntu lumen-postgres.yml Lumen without services macOS, ubuntu and windows lumen.yml Phalcon with MySQL ubuntu phalcon-mysql.yml Phalcon with PostgreSQL ubuntu phalcon-postgres.yml Roots/bedrock ubuntu bedrock.yml Roots/sage ubuntu sage.yml Slim Framework macOS, ubuntu and windows slim-framework.yml Symfony with MySQL ubuntu symfony-mysql.yml Symfony with PostgreSQL ubuntu symfony-postgres.yml Symfony without services macOS, ubuntu and windows symfony.yml Yii2 Starter Kit with MySQL ubuntu yii2-mysql.yml Yii2 Starter Kit with PostgreSQL ubuntu yii2-postgres.yml Zend Framework macOS, ubuntu and windows zend-framework.yml Versioning Use the v2 tag as setup-php version. It is a rolling tag and is synced with the latest minor and patch releases. With v2 you automatically get the bug fixes, security patches, new features and support for latest PHP releases. For debugging any issues verbose tag can be used temporarily. It outputs all the logs and is also synced with the latest releases. Semantic release versions can also be used. It is recommended to use dependabot with semantic versioning to keep the actions in your workflows up to date. Commit SHA can also be used, but are not recommended. They have to be updated with every release manually, without which you will not get any bug fixes, security patches or new features. It is highly discouraged to use the master branch as version, it might break your workflow after major releases as they have breaking changes. If you are using the v1 tag or a 1.x.y version, you should switch to v2 as v1 only gets critical bug fixes. Maintenance support for v1 will be dropped with the last PHP 8.0 release. License The scripts and documentation in this project are under the MIT License. This project has multiple dependencies. Their licenses can be found in their respective repositories. The logo for setup-php is a derivative work of php.net logo and is licensed under the CC BY-SA 4.0 License. Contributions Contributions are welcome! See Contributor's Guide before you start. If you face any issues or want to suggest a feature/improvement, start a discussion here. Contributors of setup-php and other related projects Support This Project Please star the project and share it. If you blog, please share your experience of using this action. Please sponsor setup-php using GitHub sponsors. Please reach out if you have any questions about sponsoring setup-php. Corporate Sponsors Individual Sponsors Dependencies Node.js dependencies aaronparker/VcRedist mlocati/powershell-phpmanager ppa:ondrej/php shivammathur/cache-extensions shivammathur/composer-cache shivammathur/homebrew-extensions shivammathur/homebrew-php shivammathur/icu-intl shivammathur/php-builder shivammathur/php-builder-windows shivammathur/php-ubuntu shivammathur/php5-darwin shivammathur/php5-ubuntu Further Reading About GitHub Actions GitHub Actions Syntax Other Awesome Actions "],
        "story_type":"ShowHN",
        "url_raw":"https://github.com/shivammathur/setup-php",
        "id":"8b055c6b-26b7-4285-ad71-a062602f65f7",
        "url_text":"Setup PHP in GitHub Actions Setup PHP with required extensions, php.ini configuration, code-coverage support and various tools like composer in GitHub Actions. This action gives you a cross platform interface to set up the PHP environment you need to test your application. Refer to Usage section and examples to see how to use this. Contents OS/Platform Support GitHub-Hosted Runners Self-Hosted Runners PHP Support PHP Extension Support Tools Support Coverage Support Xdebug PCOV Disable Coverage Usage Inputs Outputs Flags Basic Setup Matrix Setup Nightly Build Setup Thread Safe Setup Force Update Setup Verbose Setup Multi-Arch Setup Self Hosted Setup Local Testing Setup JIT Configuration Cache Extensions Cache Composer Dependencies Composer GitHub OAuth Inline PHP Scripts Problem Matchers Examples Versioning License Contributions Support This Project Dependencies Further Reading OS/Platform Support Both GitHub-hosted and self-hosted runners are suppported by setup-php on the following OS/Platforms. GitHub-Hosted Runners Virtual environment YAML workflow label Pre-installed PHP Ubuntu 18.04 ubuntu-18.04 PHP 7.1 to PHP 8.0 Ubuntu 20.04 ubuntu-latest or ubuntu-20.04 PHP 7.4 to PHP 8.0 Windows Server 2019 windows-latest or windows-2019 PHP 8.0 Windows Server 2022 windows-2022 PHP 8.0 macOS Catalina 10.15 macos-latest or macos-10.15 PHP 8.0 macOS Big Sur 11.x macos-11 PHP 8.0 Self-Hosted Runners Host OS/Virtual environment YAML workflow label Ubuntu 18.04 self-hosted or Linux Ubuntu 20.04 self-hosted or Linux Ubuntu 21.04 self-hosted or Linux Debian 9 self-hosted or Linux Debian 10 self-hosted or Linux Debian 11 self-hosted or Linux Windows 7 and newer self-hosted or Windows Windows Server 2012 R2 and newer self-hosted or Windows macOS Catalina 10.15 self-hosted or macOS macOS Big Sur 11.x x86_64/arm64 self-hosted or macOS Refer to the self-hosted setup to use the action on self-hosted runners. Operating systems based on the above Ubuntu and Debian versions are also supported on a best effort basis. If the requested PHP version is pre-installed, setup-php switches to it, otherwise it installs the PHP version. PHP Support On all supported OS/Platforms the following PHP versions are supported as per the runner. PHP 5.3 to PHP 8.2 on GitHub-hosted runners. PHP 5.6 to PHP 8.2 on self-hosted runners. PHP Version Stability Release Support Runner Support 5.3 Stable End of life GitHub-hosted 5.4 Stable End of life GitHub-hosted 5.5 Stable End of life GitHub-hosted 5.6 Stable End of life GitHub-hosted, self-hosted 7.0 Stable End of life GitHub-hosted, self-hosted 7.1 Stable End of life GitHub-hosted, self-hosted 7.2 Stable End of life GitHub-hosted, self-hosted 7.3 Stable Security fixes only GitHub-hosted, self-hosted 7.4 Stable Active GitHub-hosted, self-hosted 8.0 Stable Active GitHub-hosted, self-hosted 8.1 Nightly In development GitHub-hosted, self-hosted 8.2 Nightly In development GitHub-hosted, self-hosted Notes: Specifying 8.1 and 8.2 in php-version input installs a nightly build of PHP 8.1.0-dev and PHP 8.2.0-dev respectively. See nightly build setup for more information. To use JIT on PHP 8.0 and above, refer to the JIT configuration section. PHP Extension Support PHP extensions can be set up using the extensions input. It accepts a string in csv-format. On Ubuntu, extensions which are available as a package, available on PECL or a git repository can be set up. - name: Setup PHP with PECL extension uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: imagick, swoole On Windows, extensions available on PECL which have the DLL binary can be set up. On macOS, extensions available on PECL or a git repository can be set up. On Ubuntu and macOS to compile and install an extension from a git repository follow this guide. Extensions installed along with PHP if specified are enabled. Specific versions of extensions available on PECL can be set up by suffixing the extension's name with the version. This is useful for installing old versions of extensions which support end of life PHP versions. - name: Setup PHP with specific version of PECL extension uses: shivammathur/setup-php@v2 with: php-version: '5.4' extensions: swoole-1.9.3 Pre-release versions extensions available on PECL can be set up by suffixing the extension's name with its state i.e alpha, beta, devel or snapshot. - name: Setup PHP with pre-release PECL extension uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: xdebug-beta Shared extensions can be disabled by prefixing them with a :. All extensions depending on the specified extension will also be disabled. - name: Setup PHP and disable opcache uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: :opcache All shared extensions can be disabled by specifying none. When none is specified along with other extensions, it is hoisted to the start of the input. So, all the shared extensions will be disabled first, then rest of the extensions in the input will be processed. Note: This disables all core and third-party shared extensions and thus, can break some tools which need them. So, make sure you add the required extensions after none in the extensions input. - name: Setup PHP without any shared extensions except mbstring uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: none, mbstring Extension intl can be set up with specific ICU version for PHP 5.6 and above in Ubuntu workflows by suffixing intl with the ICU version. ICU 50.2 and newer versions are supported. Refer to ICU builds for the specific versions supported. - name: Setup PHP with intl uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: intl-69.1 Extensions loaded by default after setup-php runs can be found on the wiki. These extensions have custom support: cubrid, pdo_cubrid and gearman on Ubuntu. geos on Ubuntu and macOS. blackfire, couchbase, ioncube, oci8, pdo_firebird, pdo_oci, pecl_http, phalcon3 and phalcon4 on all supported OS. By default, extensions which cannot be added or disabled gracefully leave an error message in the logs, the action is not interrupted. To change this behaviour you can set fail-fast flag to true. - name: Setup PHP with fail-fast uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: oci8 env: fail-fast: true Tools Support These tools can be set up globally using the tools input. It accepts a string in csv-format. behat, blackfire, blackfire-player, codeception, composer, composer-normalize, composer-prefetcher, composer-require-checker, composer-unused, cs2pr, deployer, flex, grpc_php_plugin, infection, parallel-lint, pecl, phan, phing, phinx, phive, php-config, php-cs-fixer, phpcbf, phpcpd, phpcs, phpdoc or phpDocumentor, phpize, phplint, phpmd, phpspec, phpstan, phpunit, phpunit-bridge, phpunit-polyfills, prestissimo, protoc, psalm, symfony or symfony-cli, vapor or vapor-cli, wp or wp-cli - name: Setup PHP with tools uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: php-cs-fixer, phpunit In addition to above tools any composer tool or package can also be set up globally by specifying it as vendor/package matching the listing on Packagist. This format accepts the same version constraints as composer. - name: Setup PHP with tools uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: vimeo/psalm To set up a particular version of a tool, specify it in the form tool:version. Version can be in the following format: Semver. For example tool:1.2.3 or tool:1.2.3-beta1. Major version. For example tool:1 or tool:1.x. Major and minor version. For example tool:1.2 or tool:1.2.x. When you specify just the major version or the version in major.minor format, the latest patch version matching the input will be setup. Except for major versions of composer, For other tools when you specify only the major version or the version in major.minor format for any tool you can get rate limited by GitHub's API. To avoid this, it is recommended to provide a GitHub OAuth token. You can do that by setting COMPOSER_TOKEN environment variable. - name: Setup PHP with tools uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: php-cs-fixer:3, phpunit:8.5 env: COMPOSER_TOKEN: ${{ secrets.GITHUB_TOKEN }} The latest stable version of composer is set up by default. You can set up the required composer version by specifying the major version v1 or v2, or the version in major.minor or semver format, Additionally for composer snapshot and preview can also be specified to set up the respective releases. - name: Setup PHP with composer v2 uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: composer:v2 If you do not use composer in your workflow, you can specify tools: none to skip it. - name: Setup PHP without composer uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: none Scripts phpize and php-config are set up with the same version as of the input PHP version. The latest version of blackfire cli is set up when blackfire is specified in tools input. Please refer to the official documentation for using blackfire with GitHub Actions. Tools prestissimo and composer-prefetcher will be skipped unless composer:v1 is also specified in tools input. It is recommended to drop prestissimo and use composer v2. By default, tools which cannot be set up gracefully leave an error message in the logs, the action is not interrupted. To change this behaviour you can set fail-fast flag to true. - name: Setup PHP with fail-fast uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: deployer env: fail-fast: true Notes Input tools is useful to set up tools which you only use in GitHub Actions, thus keeping your composer.json tidy. If you do not want to use all your dev-dependencies in GitHub Actions workflow, you can run composer with --no-dev and install required tools using tools input to speed up your workflow. If you have a tool in your composer.json, do not set up it with tools input as the two instances of the tool might conflict. Coverage Support Xdebug Specify coverage: xdebug to use Xdebug and disable PCOV. Runs on all PHP versions supported. - name: Setup PHP with Xdebug uses: shivammathur/setup-php@v2 with: php-version: '8.0' coverage: xdebug The latest version of Xdebug compatible with the PHP version is set up by default. If you need Xdebug 2.x on PHP 7.2, 7.3 or 7.4, you can specify coverage: xdebug2. - name: Setup PHP with Xdebug 2.x uses: shivammathur/setup-php@v2 with: php-version: '7.4' coverage: xdebug2 PCOV Specify coverage: pcov to use PCOV and disable Xdebug. Runs on PHP 7.1 and newer PHP versions. If your source code directory is other than src, lib or, app, specify pcov.directory using the ini-values input. - name: Setup PHP with PCOV uses: shivammathur/setup-php@v2 with: php-version: '8.0' ini-values: pcov.directory=api #optional, see above for usage. coverage: pcov PHPUnit 8.x and above supports PCOV out of the box. If you are using PHPUnit 5.x, 6.x or 7.x, you need to set up pcov/clobber before executing your tests. - name: Setup PCOV run: | composer require pcov/clobber vendor/bin/pcov clobber Disable Coverage Specify coverage: none to disable both Xdebug and PCOV. Disable coverage for these reasons: You are not generating coverage reports while testing. It will disable Xdebug, which will have a positive impact on PHP performance. You are using phpdbg for running your tests. You are profiling your code using blackfire. You are using PHP in JIT mode. Please refer to JIT configuration section for more details. - name: Setup PHP with no coverage driver uses: shivammathur/setup-php@v2 with: php-version: '8.0' coverage: none Usage Inputs Specify using with keyword php-version (required) Specify the PHP version you want to set up. Accepts a string. For example '8.0'. Accepts latest to set up the latest stable PHP version. Accepts nightly to set up a nightly build from the master branch of PHP. Accepts the format d.x, where d is the major version. For example 5.x, 7.x and 8.x. See PHP support for supported PHP versions. extensions (optional) Specify the extensions you want to add or disable. Accepts a string in csv-format. For example mbstring, :opcache. Accepts none to disable all shared extensions. Shared extensions prefixed with : are disabled. See PHP extension support for more info. ini-values (optional) Specify the values you want to add to php.ini. Accepts a string in csv-format. For example post_max_size=256M, max_execution_time=180. Accepts ini values with commas if wrapped in quotes. For example xdebug.mode=\"develop,coverage\". coverage (optional) Specify the code-coverage driver you want to set up. Accepts xdebug, pcov or none. See coverage support for more info. tools (optional) Specify the tools you want to set up. Accepts a string in csv-format. For example: phpunit, phpcs See tools Support for tools supported. Outputs php-version To use outputs, give the setup-php step an id, you can use the same to get the outputs in a later step. Provides the PHP version in semver format. - name: Setup PHP id: setup-php uses: shivammathur/setup-php@v2 with: php-version: '8.0' - name: Print PHP version run: echo ${{ steps.setup-php.outputs.php-version }} Flags Specify using env keyword fail-fast (optional) Specify to mark the workflow as failed if an extension or tool fails to set up. This changes the default mode from graceful warnings to fail-fast. By default, it is set to false. Accepts true and false. phpts (optional) Specify to set up thread-safe version of PHP on Windows. Accepts ts and nts. By default, it is set to nts. See thread safe setup for more info. update (optional) Specify to update PHP on the runner to the latest patch version. Accepts true and false. By default, it is set to false. See force update setup for more info. See below for more info. Basic Setup Setup a particular PHP version. steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: mbstring, intl ini-values: post_max_size=256M, max_execution_time=180 coverage: xdebug tools: php-cs-fixer, phpunit Matrix Setup Setup multiple PHP versions on multiple operating systems. jobs: run: runs-on: ${{ matrix.operating-system }} strategy: matrix: operating-system: ['ubuntu-latest', 'windows-latest', 'macos-latest'] php-versions: ['7.3', '7.4', '8.0'] phpunit-versions: ['latest'] include: - operating-system: 'ubuntu-latest' php-versions: '7.2' phpunit-versions: '8.5.21' steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: ${{ matrix.php-versions }} extensions: mbstring, intl ini-values: post_max_size=256M, max_execution_time=180 coverage: xdebug tools: php-cs-fixer, phpunit:${{ matrix.phpunit-versions }} Nightly Build Setup Setup a nightly build of PHP 8.1 or PHP 8.2. This version is currently in development. Some user space extensions might not support this version currently. steps: - name: Setup nightly PHP uses: shivammathur/setup-php@v2 with: php-version: '8.1' extensions: mbstring ini-values: post_max_size=256M, max_execution_time=180 coverage: xdebug tools: php-cs-fixer, phpunit Thread Safe Setup Setup TS or NTS PHP on Windows. NTS versions are set up by default. On Ubuntu and macOS only NTS versions are supported. On Windows both TS and NTS versions are supported. jobs: run: runs-on: windows-latest name: Setup PHP TS on Windows steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' env: phpts: ts # specify ts or nts Force Update Setup Update to the latest patch of PHP versions. Pre-installed PHP versions on the GitHub Actions images are not updated to their latest patch release by default. You can specify the update environment variable to true for updating to the latest release. - name: Setup PHP with latest versions uses: shivammathur/setup-php@v2 with: php-version: '8.0' env: update: true # specify true or false Verbose Setup Debug your workflow To debug any issues, you can use the verbose tag instead of v2. - name: Setup PHP with logs uses: shivammathur/setup-php@verbose with: php-version: '8.0' Multi-Arch Setup Setup PHP on multiple architecture on Ubuntu GitHub Runners. PHP 5.6 to PHP 8.0 are supported by setup-php on multiple architecture on Ubuntu. For this, you can use shivammathur/node images as containers. These have compatible Nodejs installed for JavaScript based GitHub Actions. Currently, for ARM based setup, you will need self-hosted runners. jobs: run: runs-on: ubuntu-latest container: shivammathur/node:latest-${{ matrix.arch }} strategy: matrix: arch: [\"amd64\", \"i386\"] steps: - name: Install PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' Self Hosted Setup Setup PHP on a self-hosted runner. To set up a containerised self-hosted runner, refer to the following guides as per your base operating system. Linux Windows To set up the runner directly on the host OS or in a virtual machine, follow this requirements guide before setting up the self-hosted runner. If your workflow uses service containers, then set up the runner on a Linux host or in a Linux virtual machine. GitHub Actions does not support nested virtualization on Linux, so services will not work in a dockerized container. Specify the environment variable runner with the value self-hosted. Without this your workflow will fail. jobs: run: runs-on: self-hosted strategy: matrix: php-versions: ['5.6', '7.0', '7.1', '7.2', '7.3', '7.4', '8.0'] name: PHP ${{ matrix.php-versions }} steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: ${{ matrix.php-versions }} env: runner: self-hosted # Specify the runner. Notes Do not set up multiple self-hosted runners on a single server instance as parallel workflow will conflict with each other. Do not set up self-hosted runners on the side on your development environment or your production server. Avoid using the same labels for your self-hosted runners which are used by GitHub-hosted runners. Local Testing Setup Test your Ubuntu workflow locally using nektos/act. jobs: run: runs-on: ubuntu-latest steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' Run the workflow locally with act using shivammathur/node docker images. Choose the image tag which matches the runs-on property in your workflow. For example, if you are using ubuntu-20.04 in your workflow, run act -P ubuntu-20.04=shivammathur/node:20.04. # For runs-on: ubuntu-latest act -P ubuntu-latest=shivammathur/node:latest # For runs-on: ubuntu-20.04 act -P ubuntu-20.04=shivammathur/node:2004 # For runs-on: ubuntu-18.04 act -P ubuntu-18.04=shivammathur/node:1804 JIT Configuration Enable Just-in-time(JIT) on PHP 8.0 and above. To enable JIT, enable opcache in cli mode by setting opcache.enable_cli=1. JIT conflicts with Xdebug, PCOV, and other extensions which override zend_execute_ex function, so set coverage: none and disable any such extension if added. By default, opcache.jit=1235 and opcache.jit_buffer_size=256M are set which can be changed using ini-values input. For detailed information about JIT related directives refer to the official PHP documentation. For example to enable JIT in tracing mode with buffer size of 64 MB. - name: Setup PHP with JIT in tracing mode uses: shivammathur/setup-php@v2 with: php-version: '8.0' coverage: none ini-values: opcache.enable_cli=1, opcache.jit=tracing, opcache.jit_buffer_size=64M Cache Extensions You can cache PHP extensions using shivammathur/cache-extensions and action/cache GitHub Actions. Extensions which take very long to set up when cached are available in the next workflow run and are enabled directly. This reduces the workflow execution time. Refer to shivammathur/cache-extensions for details. Cache Composer Dependencies If your project uses composer, you can persist the composer's internal cache directory. Dependencies cached are loaded directly instead of downloading them while installation. The files cached are available across check-runs and will reduce the workflow execution time. - name: Get composer cache directory id: composer-cache run: echo \"::set-output name=dir::$(composer config cache-files-dir)\" - name: Cache dependencies uses: actions/cache@v2 with: path: ${{ steps.composer-cache.outputs.dir }} key: ${{ runner.os }}-composer-${{ hashFiles('**/composer.lock') }} restore-keys: ${{ runner.os }}-composer- - name: Install dependencies run: composer install --prefer-dist Notes Please do not cache vendor directory using action/cache as that will have side effects. If you do not commit composer.lock, you can use the hash of composer.json as the key for your cache. key: ${{ runner.os }}-composer-${{ hashFiles('**/composer.json') }} If you support a range of composer dependencies and use prefer-lowest and prefer-stable options, you can store them in your matrix and add them to the keys. key: ${{ runner.os }}-composer-${{ hashFiles('**/composer.lock') }}-${{ matrix.prefer }}- restore-keys: ${{ runner.os }}-composer-${{ matrix.prefer }}- Composer GitHub OAuth If you have a number of workflows which set up multiple tools or have many composer dependencies, you might hit the GitHub's rate limit for composer. Also, if you specify only the major version or the version in major.minor format, you can hit the rate limit. To avoid this you can specify an OAuth token by setting COMPOSER_TOKEN environment variable. You can use GITHUB_TOKEN secret for this purpose. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' env: COMPOSER_TOKEN: ${{ secrets.GITHUB_TOKEN }} Inline PHP Scripts If you have to run multiple lines of PHP code in your workflow, you can do that easily without saving it to a file. Put the code in the run property of a step and specify the shell as php {0}. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' - name: Run PHP code shell: php {0} run: | <?php $welcome = \"Hello, world\"; echo $welcome; Problem Matchers Problem matchers are json configurations which identify errors and warnings in your logs and surface them prominently in the GitHub Actions UI by highlighting them and creating code annotations. PHP Setup problem matchers for your PHP output by adding this step after the setup-php step. - name: Setup problem matchers for PHP run: echo \"::add-matcher::${{ runner.tool_cache }}/php.json\" PHPUnit Setup problem matchers for your PHPUnit output by adding this step after the setup-php step. - name: Setup problem matchers for PHPUnit run: echo \"::add-matcher::${{ runner.tool_cache }}/phpunit.json\" PHPStan PHPStan supports error reporting in GitHub Actions, so it does not require problem matchers. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: phpstan - name: Run PHPStan run: phpstan analyse src Psalm Psalm supports error reporting in GitHub Actions with an output format github. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: psalm - name: Run Psalm run: psalm --output-format=github Tools with checkstyle support For tools that support checkstyle reporting like phpstan, psalm, php-cs-fixer and phpcs you can use cs2pr to annotate your code. For examples refer to cs2pr documentation. Here is an example with phpcs. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: cs2pr, phpcs - name: Run phpcs run: phpcs -q --report=checkstyle src | cs2pr Examples Examples of using setup-php with various PHP Frameworks and Packages. Framework/Package Runs on Workflow Blackfire macOS, ubuntu and windows blackfire.yml Blackfire Player macOS, ubuntu and windows blackfire-player.yml CakePHP with MySQL and Redis ubuntu cakephp-mysql.yml CakePHP with PostgreSQL and Redis ubuntu cakephp-postgres.yml CakePHP without services macOS, ubuntu and windows cakephp.yml CodeIgniter macOS, ubuntu and windows codeigniter.yml Laravel with MySQL and Redis ubuntu laravel-mysql.yml Laravel with PostgreSQL and Redis ubuntu laravel-postgres.yml Laravel without services macOS, ubuntu and windows laravel.yml Lumen with MySQL and Redis ubuntu lumen-mysql.yml Lumen with PostgreSQL and Redis ubuntu lumen-postgres.yml Lumen without services macOS, ubuntu and windows lumen.yml Phalcon with MySQL ubuntu phalcon-mysql.yml Phalcon with PostgreSQL ubuntu phalcon-postgres.yml Roots/bedrock ubuntu bedrock.yml Roots/sage ubuntu sage.yml Slim Framework macOS, ubuntu and windows slim-framework.yml Symfony with MySQL ubuntu symfony-mysql.yml Symfony with PostgreSQL ubuntu symfony-postgres.yml Symfony without services macOS, ubuntu and windows symfony.yml Yii2 Starter Kit with MySQL ubuntu yii2-mysql.yml Yii2 Starter Kit with PostgreSQL ubuntu yii2-postgres.yml Zend Framework macOS, ubuntu and windows zend-framework.yml Versioning Use the v2 tag as setup-php version. It is a rolling tag and is synced with the latest minor and patch releases. With v2 you automatically get the bug fixes, security patches, new features and support for latest PHP releases. For debugging any issues verbose tag can be used temporarily. It outputs all the logs and is also synced with the latest releases. Semantic release versions can also be used. It is recommended to use dependabot with semantic versioning to keep the actions in your workflows up to date. Commit SHA can also be used, but are not recommended. They have to be updated with every release manually, without which you will not get any bug fixes, security patches or new features. It is highly discouraged to use the master branch as version, it might break your workflow after major releases as they have breaking changes. If you are using the v1 tag or a 1.x.y version, you should switch to v2 as v1 only gets critical bug fixes. Maintenance support for v1 will be dropped with the last PHP 8.0 release. License The scripts and documentation in this project are under the MIT License. This project has multiple dependencies. Their licenses can be found in their respective repositories. The logo for setup-php is a derivative work of php.net logo and is licensed under the CC BY-SA 4.0 License. Contributions Contributions are welcome! See Contributor's Guide before you start. If you face any issues or want to suggest a feature/improvement, start a discussion here. Contributors of setup-php and other related projects Support This Project Please star the project and share it. If you blog, please share your experience of using this action. Please sponsor setup-php using GitHub sponsors. Please reach out if you have any questions about sponsoring setup-php. Corporate Sponsors Individual Sponsors Dependencies Node.js dependencies aaronparker/VcRedist mlocati/powershell-phpmanager ppa:ondrej/php shivammathur/cache-extensions shivammathur/composer-cache shivammathur/homebrew-extensions shivammathur/homebrew-php shivammathur/icu-intl shivammathur/php-builder shivammathur/php-builder-windows shivammathur/php-ubuntu shivammathur/php5-darwin shivammathur/php5-ubuntu Further Reading About GitHub Actions GitHub Actions Syntax Other Awesome Actions ",
        "_version_":1718938218122969088},
      {
        "story_id":18854644,
        "story_author":"severine",
        "story_descendants":1,
        "story_score":25,
        "story_time":"2019-01-08T10:41:22Z",
        "story_title":"KDE is considering a migration to GitLab",
        "search":["KDE is considering a migration to GitLab",
          "Normal",
          "https://gitlab.com/gitlab-org/gitlab-ce/issues/53206",
          "Background KDE is considering a migration to GitLab: members from the KDE Board of Directors, the KDE Sysadmin team and the KDE Onboarding Initiative have been following the GNOME migration as a model and have been in touch with the GNOME Foundation and GitLab. GitLab will be initially assisting them with a Proof of Concept to facilitate making an assessment and a decision that will ultimately be consulted with the KDE community. The KDE Community is a free software community dedicated to creating an open and user-friendly computing experience, offering an advanced graphical desktop, a wide variety of applications for communication, work, education and entertainment and a platform to easily build new applications upon. We have a strong focus on finding innovative solutions to old and new problems, creating a vibrant atmosphere open for experimentation. Goals The main goals for a successful migration would be: More accessible infrastructure for contributors Code review integration with git Streamlined infrastructure and tooling Good relationship and open communication channel with upstream (GitLab in this case) Migration issues tracker KDE keeps a list of issues with priorities relevant to the migration at https://gitlab.com/gitlab-org/gitlab-ce/issues/57338 Discussion We are at the initial stages of discussion, which can be followed on the notes from our regular calls. KDE is currently looking at a self-hosted solution rather than hosting at gitlab.com. Due to their policy of only hosting Free Software on their servers the present consideration is to use gitlab-ce with the Core subscription Current KDE tooling and infrastructure Gitolite Authentication and Authorization (some repositories are push restricted) Repository management Custom hooks providing notifications via email and IRC, as well as updating statuses of tasks and bugs on Phabricator and Bugzilla Phabricator Code and asset reviews Task tracking Kanban boards Wikis Bugzilla Bug reports Automated crash reports LDAP-based SSI + management Web UI Jenkins CI: Linux, FreeBSD, Windows, Android CD: Linux (Appimage & Flatpak), Windows, MacOS and Android GitLab replacements Tool Feature GitLab feature GitLab edition Gitolite Authentication LDAP and Omniauth CE Gitolite Repository management Gitaly CE Gitolite Notification (E-Mail) E-mail notification CE Gitolite Notification (IRC). Important. TBD TBC Gitolite Set ticket status IssuesSee Closing issues and quick actions CE Phabricator Code review Discussions CE Phabricator Asset reviews (additional tool in Phabricator - Pholio) Image discussionsAlso see issue #53587 CE Phabricator Task-tracking Time tracking CE Phabricator Kanban boards Issue boards CE Bugzilla Bug reports Issues CE Bugzilla Automated crash reports (not trivial to move out of Bugzilla) Issues or an external tool CE KDE identity LDAP-based SSI (SSH key management) SSH management on GitLab or syncing keys to GitLab CE KDE identity Web UI (keep for more user-facing resources, e.g. Forum) N/A? N/A? Jenkins CI: Linux, FreeBSD, Windows, Android GitLab CI or APIJenkins plugin is EE-only CE Jenkins CD: Linux (Appimage & FlatPak), Windows, MacOS and Android GitLab CI or APIJenkins plugin is EE-only CE Test instance https://invent.kde.org/kde/ Collaborators Eike Hein (@hein), Treasurer and Vice President, Board of Directors, KDE Aleix Pol (@apol), Vice President, Board of Directors, KDE Ben Cooksley (@bcooksley), Lead Sysadmin, KDE Neofytos Kolokotronis (@tetris4), Onboarding Team Lead, KDE David Planella (@dplanella), Director of Community Relations, GitLab ",
          "Looking forward to this"],
        "story_type":"Normal",
        "url_raw":"https://gitlab.com/gitlab-org/gitlab-ce/issues/53206",
        "url_text":"Background KDE is considering a migration to GitLab: members from the KDE Board of Directors, the KDE Sysadmin team and the KDE Onboarding Initiative have been following the GNOME migration as a model and have been in touch with the GNOME Foundation and GitLab. GitLab will be initially assisting them with a Proof of Concept to facilitate making an assessment and a decision that will ultimately be consulted with the KDE community. The KDE Community is a free software community dedicated to creating an open and user-friendly computing experience, offering an advanced graphical desktop, a wide variety of applications for communication, work, education and entertainment and a platform to easily build new applications upon. We have a strong focus on finding innovative solutions to old and new problems, creating a vibrant atmosphere open for experimentation. Goals The main goals for a successful migration would be: More accessible infrastructure for contributors Code review integration with git Streamlined infrastructure and tooling Good relationship and open communication channel with upstream (GitLab in this case) Migration issues tracker KDE keeps a list of issues with priorities relevant to the migration at https://gitlab.com/gitlab-org/gitlab-ce/issues/57338 Discussion We are at the initial stages of discussion, which can be followed on the notes from our regular calls. KDE is currently looking at a self-hosted solution rather than hosting at gitlab.com. Due to their policy of only hosting Free Software on their servers the present consideration is to use gitlab-ce with the Core subscription Current KDE tooling and infrastructure Gitolite Authentication and Authorization (some repositories are push restricted) Repository management Custom hooks providing notifications via email and IRC, as well as updating statuses of tasks and bugs on Phabricator and Bugzilla Phabricator Code and asset reviews Task tracking Kanban boards Wikis Bugzilla Bug reports Automated crash reports LDAP-based SSI + management Web UI Jenkins CI: Linux, FreeBSD, Windows, Android CD: Linux (Appimage & Flatpak), Windows, MacOS and Android GitLab replacements Tool Feature GitLab feature GitLab edition Gitolite Authentication LDAP and Omniauth CE Gitolite Repository management Gitaly CE Gitolite Notification (E-Mail) E-mail notification CE Gitolite Notification (IRC). Important. TBD TBC Gitolite Set ticket status IssuesSee Closing issues and quick actions CE Phabricator Code review Discussions CE Phabricator Asset reviews (additional tool in Phabricator - Pholio) Image discussionsAlso see issue #53587 CE Phabricator Task-tracking Time tracking CE Phabricator Kanban boards Issue boards CE Bugzilla Bug reports Issues CE Bugzilla Automated crash reports (not trivial to move out of Bugzilla) Issues or an external tool CE KDE identity LDAP-based SSI (SSH key management) SSH management on GitLab or syncing keys to GitLab CE KDE identity Web UI (keep for more user-facing resources, e.g. Forum) N/A? N/A? Jenkins CI: Linux, FreeBSD, Windows, Android GitLab CI or APIJenkins plugin is EE-only CE Jenkins CD: Linux (Appimage & FlatPak), Windows, MacOS and Android GitLab CI or APIJenkins plugin is EE-only CE Test instance https://invent.kde.org/kde/ Collaborators Eike Hein (@hein), Treasurer and Vice President, Board of Directors, KDE Aleix Pol (@apol), Vice President, Board of Directors, KDE Ben Cooksley (@bcooksley), Lead Sysadmin, KDE Neofytos Kolokotronis (@tetris4), Onboarding Team Lead, KDE David Planella (@dplanella), Director of Community Relations, GitLab ",
        "comments.comment_id":[18868066],
        "comments.comment_author":["s_chaudhary"],
        "comments.comment_descendants":[0],
        "comments.comment_time":["2019-01-09T19:41:40Z"],
        "comments.comment_text":["Looking forward to this"],
        "id":"11054b8e-2a25-4efe-a117-c660cabff959",
        "_version_":1718938136087625728},
      {
        "story_id":21624739,
        "story_author":"pcr910303",
        "story_descendants":28,
        "story_score":55,
        "story_time":"2019-11-25T01:20:38Z",
        "story_title":"Legit: A CLI tool to make Git more accessible",
        "search":["Legit: A CLI tool to make Git more accessible",
          "Normal",
          "https://frostming.github.io/legit/",
          "Welcome github // pypi // issue tracker Legit is a complementary command-line interface for Git, optimized for workflow simplicity. It is heavily inspired by GitHub for Mac. Feature branch workflows are dead simple. $ git switch <branch> # Switches to branch. Stashes and restores unstaged changes. $ git sync # Synchronizes current branch. Auto-merge/rebase, un/stash. $ git publish <branch> # Publishes branch to remote server. $ git unpublish <branch> # Removes branch from remote server. $ git branches [wildcard pattern] # Nice & pretty list of branches + publication status. $ git undo [--hard] # Removes the last commit from history. Installing Legit If you are using Homebrew: $ brew install legit Or install legit via pip: $ pip install legit To enable the git aliases: $ legit --install Nice and simple the way it should be. ",
          "Quoting the README:<p>> ## The Concept<p>> GitHub for Mac is not just a Git client.<p>> This comment[0] on Hacker News says it best:<p>>> They haven't re-created the git CLI tool in a GUI, they've created something\n>> different. They've created a tool that makes Git more accessible. Little\n>> things like auto-stashing when you switch branches will confuse git\n>> veterans, but it will make Git much easier to grok for newcomers because of\n>> the assumptions it makes about your Git workflow. Why not bring this\n>> innovation back to the command line?<p>It provides a few interfaces like switch <branch> which switches to specified branch by automatically stashing any changes, or sync [<branch>] which (smartly) synchronizes the given branch. Etc...<p>[0] <a href=\"https://news.ycombinator.com/item?id=2684483\" rel=\"nofollow\">https://news.ycombinator.com/item?id=2684483</a>",
          "Wouldn't it be enough to add some git aliases for this?\nLike:<p><pre><code>  [alias]\n  hist = log --pretty=format:\\\"%h %ad | %s%d [%an]\\\" --graph --decorate --date=short\n  quick-push = \"!f() { git add . && git commit -m \\\"$1\\\" && git push; }; f\"\n  grep-log = log -E -i --grep\n  grep-hist = log --pretty=format:\\\"%h %ad | %s%d [%an]\\\" --graph --decorate --date=short -E -i --grep</code></pre>"],
        "story_type":"Normal",
        "url_raw":"https://frostming.github.io/legit/",
        "url_text":"Welcome github // pypi // issue tracker Legit is a complementary command-line interface for Git, optimized for workflow simplicity. It is heavily inspired by GitHub for Mac. Feature branch workflows are dead simple. $ git switch <branch> # Switches to branch. Stashes and restores unstaged changes. $ git sync # Synchronizes current branch. Auto-merge/rebase, un/stash. $ git publish <branch> # Publishes branch to remote server. $ git unpublish <branch> # Removes branch from remote server. $ git branches [wildcard pattern] # Nice & pretty list of branches + publication status. $ git undo [--hard] # Removes the last commit from history. Installing Legit If you are using Homebrew: $ brew install legit Or install legit via pip: $ pip install legit To enable the git aliases: $ legit --install Nice and simple the way it should be. ",
        "comments.comment_id":[21625877,
          21626344],
        "comments.comment_author":["pcr910303",
          "sandreas"],
        "comments.comment_descendants":[2,
          0],
        "comments.comment_time":["2019-11-25T06:11:06Z",
          "2019-11-25T08:26:58Z"],
        "comments.comment_text":["Quoting the README:<p>> ## The Concept<p>> GitHub for Mac is not just a Git client.<p>> This comment[0] on Hacker News says it best:<p>>> They haven't re-created the git CLI tool in a GUI, they've created something\n>> different. They've created a tool that makes Git more accessible. Little\n>> things like auto-stashing when you switch branches will confuse git\n>> veterans, but it will make Git much easier to grok for newcomers because of\n>> the assumptions it makes about your Git workflow. Why not bring this\n>> innovation back to the command line?<p>It provides a few interfaces like switch <branch> which switches to specified branch by automatically stashing any changes, or sync [<branch>] which (smartly) synchronizes the given branch. Etc...<p>[0] <a href=\"https://news.ycombinator.com/item?id=2684483\" rel=\"nofollow\">https://news.ycombinator.com/item?id=2684483</a>",
          "Wouldn't it be enough to add some git aliases for this?\nLike:<p><pre><code>  [alias]\n  hist = log --pretty=format:\\\"%h %ad | %s%d [%an]\\\" --graph --decorate --date=short\n  quick-push = \"!f() { git add . && git commit -m \\\"$1\\\" && git push; }; f\"\n  grep-log = log -E -i --grep\n  grep-hist = log --pretty=format:\\\"%h %ad | %s%d [%an]\\\" --graph --decorate --date=short -E -i --grep</code></pre>"],
        "id":"bb1f0498-6bbc-4fac-b319-565e4f8ebf0f",
        "_version_":1718938244651941888},
      {
        "story_id":19991764,
        "story_author":"owenwil",
        "story_descendants":30,
        "story_score":75,
        "story_time":"2019-05-23T13:51:24Z",
        "story_title":"GitHub's new features show it’s finally listening to developers",
        "search":["GitHub's new features show it’s finally listening to developers",
          "Normal",
          "https://char.gd/blog/2019/github-is-building-the-developer-platform-we-always-needed",
          "When GitHub was acquired by Microsoft in 2018, the company was in crisis. Internally, its culture was toxicand the company was struggling to ship meaningful features or improvements at all. In 2016, I wrote about how frustrated developers, exasperated by the company's ignorance of their pleas to build better tools, took to the platform to write an open letter, a final attempt at getting someone to care. The letter, \"Dear GitHub,\" was a brutal look at how little the company had done to foster the giant, open community, and was opaque about what it paid attention to:Those of us who run some of the most popular projects on GitHub feel completely ignored by you. Weve gone through the only support channel that you have given us either to receive an empty response or even no response at all. We have no visibility into what has happened with our requests, or whether GitHub is working on them. Since our own work is usually done in the open and everyone has input into the process, it seems strange for us to be in the dark about one of our most important project dependencies.At this inflection point, things looked badand it felt like GitHub was going to disappear. There was a time when I wondered what would happen if the world's largest open source community just went out business...but Microsoft has changed that narrative in a short period of time.GitHub, it seems, is thriving again. It just showed the fruits of that labor, and what it looks like when a company is participating in the discussion in the open, listening to the developers that know it best.At an event called GitHub Satellite, the company unveiled the biggest set of new features in memory, all designed to address glaring problems the platform has faced for years. They're designed to help make GitHub a better place to work, and contribute to the open source community as a whole. It started with the small stuff that mattershelping surface who's actually building the packages we use every day with new contributor and dependency tools, that show the people giving back to open source in a bigger way Then, it moved on to the big stuff: how GitHub is building a new way forward. Starting today, there are ways to actually financially support the people building things that we use, and support the work they've often been doing invisibly for free, or for a tiny amount of donations.Open source was always in a dangerous place: many of the things critical technology rely on come from work made available for free on GitHub, and the world's largest companies have built their fortunes atop of these projects without giving back a dollar. Many of these contributors building those tools burn out, or simply can't deal with the load, and sometimes it goes wrong when they step back, ultimating giving the reins to someone else who may not have the same intention. Long term, it wasn't sustainablenor was it a way to build the positive, diverse ecosystem of contributors that open source desperately needs. The tool, called GitHub Sponsor, allow contributors to support a specific person or project directlyand the company doesn't take any sort of cut, commission or fee. Yes, GitHub is swallowing the cost of those transactions entirely to give the creator as much of the money as possible. But, what's even wilder is that it's matching every dollar contributed in the first year, an outrageously bold commitment that's only possible with the backing of a company like Microsoft.With a way to directly support the people behind open source work, built directly into the world's most popular developer platform, we finally have the crucial missing piece to create a healthier way forward for open source. Most importantly, this helps to change the open source narrative: you shouldn't feel like you need to work for free, especially if companies are actually making money off of your work. Sponsorship right there on the page with the installation instructions helps pave the way for companies to actually fund the work they rely on, and that matters. There were a slew of other announcements to push the platform forward, too, from enterprise features for \"internal\" packages, hosted enterprise accounts, better permissions, automated security fixes for vulnerable repositories and much, much more.GitHub finally realized it has an opportunityand a responsibilityto define the way we build the internet's infrastructure. It was always best placed to build a more healthy community, but never stepped up to the platenow it has, and I can't wait to put my money behind the projects that have made my own work possible.But, the most important thing? GitHub showed it's listening, and it's acting. Alongside Microsoft's continued genuine efforts to build the best open developer experience regardless of the tools you use, it's putting its money where its mouth is, and it's good for all of us.GitHub's back. And it's building something for all of us again. Read these next ",
          "The main thread seems better:<p><a href=\"https://news.ycombinator.com/item?id=19989684\" rel=\"nofollow\">https://news.ycombinator.com/item?id=19989684</a><p>This lacks additional content or context. It is simply the author promoting some 2016 letter/article criticising GitHub, and then very loosely tying it to recent changes. It is self-promoting blogspam.",
          ">the world's largest open source community<p>Is github even considered an open source community? Open source projects work on github, sure, (so do closed source projects) but github is not the community. It is a tool."],
        "story_type":"Normal",
        "url_raw":"https://char.gd/blog/2019/github-is-building-the-developer-platform-we-always-needed",
        "comments.comment_id":[19992416,
          19992793],
        "comments.comment_author":["Someone1234",
          "CameronNemo"],
        "comments.comment_descendants":[0,
          2],
        "comments.comment_time":["2019-05-23T15:05:30Z",
          "2019-05-23T15:46:53Z"],
        "comments.comment_text":["The main thread seems better:<p><a href=\"https://news.ycombinator.com/item?id=19989684\" rel=\"nofollow\">https://news.ycombinator.com/item?id=19989684</a><p>This lacks additional content or context. It is simply the author promoting some 2016 letter/article criticising GitHub, and then very loosely tying it to recent changes. It is self-promoting blogspam.",
          ">the world's largest open source community<p>Is github even considered an open source community? Open source projects work on github, sure, (so do closed source projects) but github is not the community. It is a tool."],
        "id":"3085722d-3fe5-4737-b765-116a99d55e7c",
        "url_text":"When GitHub was acquired by Microsoft in 2018, the company was in crisis. Internally, its culture was toxicand the company was struggling to ship meaningful features or improvements at all. In 2016, I wrote about how frustrated developers, exasperated by the company's ignorance of their pleas to build better tools, took to the platform to write an open letter, a final attempt at getting someone to care. The letter, \"Dear GitHub,\" was a brutal look at how little the company had done to foster the giant, open community, and was opaque about what it paid attention to:Those of us who run some of the most popular projects on GitHub feel completely ignored by you. Weve gone through the only support channel that you have given us either to receive an empty response or even no response at all. We have no visibility into what has happened with our requests, or whether GitHub is working on them. Since our own work is usually done in the open and everyone has input into the process, it seems strange for us to be in the dark about one of our most important project dependencies.At this inflection point, things looked badand it felt like GitHub was going to disappear. There was a time when I wondered what would happen if the world's largest open source community just went out business...but Microsoft has changed that narrative in a short period of time.GitHub, it seems, is thriving again. It just showed the fruits of that labor, and what it looks like when a company is participating in the discussion in the open, listening to the developers that know it best.At an event called GitHub Satellite, the company unveiled the biggest set of new features in memory, all designed to address glaring problems the platform has faced for years. They're designed to help make GitHub a better place to work, and contribute to the open source community as a whole. It started with the small stuff that mattershelping surface who's actually building the packages we use every day with new contributor and dependency tools, that show the people giving back to open source in a bigger way Then, it moved on to the big stuff: how GitHub is building a new way forward. Starting today, there are ways to actually financially support the people building things that we use, and support the work they've often been doing invisibly for free, or for a tiny amount of donations.Open source was always in a dangerous place: many of the things critical technology rely on come from work made available for free on GitHub, and the world's largest companies have built their fortunes atop of these projects without giving back a dollar. Many of these contributors building those tools burn out, or simply can't deal with the load, and sometimes it goes wrong when they step back, ultimating giving the reins to someone else who may not have the same intention. Long term, it wasn't sustainablenor was it a way to build the positive, diverse ecosystem of contributors that open source desperately needs. The tool, called GitHub Sponsor, allow contributors to support a specific person or project directlyand the company doesn't take any sort of cut, commission or fee. Yes, GitHub is swallowing the cost of those transactions entirely to give the creator as much of the money as possible. But, what's even wilder is that it's matching every dollar contributed in the first year, an outrageously bold commitment that's only possible with the backing of a company like Microsoft.With a way to directly support the people behind open source work, built directly into the world's most popular developer platform, we finally have the crucial missing piece to create a healthier way forward for open source. Most importantly, this helps to change the open source narrative: you shouldn't feel like you need to work for free, especially if companies are actually making money off of your work. Sponsorship right there on the page with the installation instructions helps pave the way for companies to actually fund the work they rely on, and that matters. There were a slew of other announcements to push the platform forward, too, from enterprise features for \"internal\" packages, hosted enterprise accounts, better permissions, automated security fixes for vulnerable repositories and much, much more.GitHub finally realized it has an opportunityand a responsibilityto define the way we build the internet's infrastructure. It was always best placed to build a more healthy community, but never stepped up to the platenow it has, and I can't wait to put my money behind the projects that have made my own work possible.But, the most important thing? GitHub showed it's listening, and it's acting. Alongside Microsoft's continued genuine efforts to build the best open developer experience regardless of the tools you use, it's putting its money where its mouth is, and it's good for all of us.GitHub's back. And it's building something for all of us again. Read these next ",
        "_version_":1718938182778617857},
      {
        "story_id":19914380,
        "story_author":"amatt189",
        "story_descendants":3,
        "story_score":6,
        "story_time":"2019-05-14T22:19:31Z",
        "story_title":"Detect credentials & secrets in code via machine learning",
        "search":["Detect credentials & secrets in code via machine learning",
          "Normal",
          "https://medium.com/@watchtowerai/introducing-radar-api-detect-credentials-secrets-in-code-via-machine-learning-fe402b818bf1",
          "In 2016, hackers gained access to Ubers private code repositories and used hard-coded credentials to exfiltrate 57 million driver records from an AWS S3 bucket. As a result of this breach, and its subsequent cover-up, Uber was fined $148 million. Could they have prevented such an incident? If this can happen to Uber, it can happen to any other company.Source code hosting providers such as GitHub and GitLab have socialized software development, making it easier to collaborate on projects & ship code quickly. But this hasnt been without consequences: its led to an increase in the accidental publication of sensitive information such as API keys, secrets, and credentials. This sensitive information can range from SSH keys to API keys, and even passwords. After analyzing millions of commits, our research team found that this problem is widespread and occurs daily as new commits are pushed up as part of the software development lifecycle. Abuse of these leaked credentials can cause salient security & compliance risks, such as catastrophic loss of sensitive customer data, harming an organization both financially and reputationally, while putting consumers at risk.When asked about the Uber leak, GitHub had the following comment:Our recommendation is to never store access tokens, passwords, or other authentication or encryption keys in the code. If the developer must include them in the code, we recommend they implement additional operational safeguards to prevent unauthorized access or misuse.Even AWS, the largest cloud provider by market share, has been financially impacted by leaked AWS keys and was motivated to create Git-secrets to help combat this problem. However, they acknowledge the difficulty of this problem, and even they cannot guarantee fool-proof detection of their own AWS keys. Many organizations have become hyper-aware of the potential risks of credential leakage, so related open source tools are gaining popularity on platforms like GitHub despite their low accuracy rates.Our team has been evaluating existing tools, and weve built a solution that leverages machine learning to dramatically improve the chances of accurately detecting credentials in your source code. Read on to learn more.We evaluated several popular tools used today to protect and secure GitHub repositories:truffleHog, 3k+ stars on GitHub Searches through git repositories for high entropy strings and secrets, digging deep into commit historyGitrob, 3k+ stars on GitHub Reconnaissance tool for GitHub organizationsGitleaks, 4k+ start on GitHub Audit git repos for secretsGit-secrets, 5k+ stars on GitHub Prevents you from committing secrets and credentials into git repositoriesIts worth noting that we focused on the most popular open source projects there are other projects like Yelps detect-secrets and Auth0s repo-supervisor that serve a similar purpose, though we didnt include them in this analysis to avoid redundancy. Well provide a brief introduction of each tool, an analysis of their pros & cons, followed by a more detailed comparison with examples. If youre already familiar with common tools and are curious about our approach, feel free to skip ahead to the section titled Our Approach: Radar below.The two main algorithms used in these tools are entropy and regular expressions:Entropy: Shannon entropy, or information entropy, is the average rate at which information is produced by a stochastic source of data. A high entropy score is the result of high variability of information, e.g. a string with only one repetitive character will have low entropy, since there is only one state, and can be converted to 1 bit of information. On the other hand, a string with a diverse set of characters that appears highly random, such as API keys, will require more bits to transmit, having much higher entropy. See here for more info.Regular Expressions (Regex): A regular expression, or regex, is a sequence of characters that defines a search pattern. Regexes are used to perform any lexical searching to match a variable name or API key pattern. See here for more info.truffleHogtruffleHog is an open-source tool written in Python that searches through git commit history for credentials/secrets that are accidentally committed.Pros:truffleHog scans the whole history of branches and commits, thus nothing will be missed if committed.truffleHog allows for both regex based and high entropy based flagging.Users can provide custom regexes to suit their needs accordingly.Cons:High entropy is a commonly used method to detect randomly generated API keys in many tools. However, because of the lack of contextual awareness, this method tends to be noisy. For example, it can be hard to distinguish between long variable names and credentials, e.g. TestScrapeLoopRunReportsTargetDownOnInvalidUTF8 which is a high-entropy string and would likely get flagged as a false positive.Regular expressions are a powerful but limited method that searches for generic patterns. Thus, they only work well on finding keys with explicitly defined & repeatable patterns, e.g. starting with some fixed characters, or very lengthy keys. Requiring these unique characteristics dramatically reduces the pool of potential tokens that can be flagged accurately.GitrobGitrob is an open-source tool written in Go that helps find potentially sensitive files. Different than the other tools listed, it has a broader range of object detection beyond API keys.Pros:Similar to truffleHog, it drills deep into the commit history of a repository, and the user can adjust how far back in the commit history to scan.The UI is very friendly for users to manipulate and analyze results.Also provides regex for searching for generic keys, e.g. /([a-f09\\-\\$\\/]{20,})/gmi.Cons:The search mechanisms are fairly simple mainly keyword searching, which yields many false positives. Users have to manually go through all detected files and tokens to check if they contain valid sensitive info or not, which can be very time-consuming.The regex for API keys does not have a cap on the length of the key, which can lead to a high number of false positives and is not comprehensive enough to capture keys with more diverse character sets.GitleaksGitleaks is an open-source tool written in Go that provides a way to find unwanted data types in code checked in to git. Imprecisely, its a Go version of truffleHog, although algorithm-wise, there are unique aspects.Pros:It combines a regex list similar to truffleHog and a high entropy method to do credential detection.It offers the user options to adjust their regex list and entropy range.Cons:Similar to truffleHog.Git-secretsGit-secrets is a tool written in bash to prevent users from committing AWS API keys.Pros:The tools main methods only occupy one file, so the code is easy to understand, use, and extend.Cons:The searching domain is limited to several regexes for AWS keys.Poses potential risks missing real AWS API keys if the users variable name does not match Git-secrets regexes, which is a common problem for all regex based searching methods. In other words, Git-secrets detection mandates that the AWS secret key must have key in the variable name, so a change of the variable name to aws_secret or secret_token would not trigger the regex, leading to a false negative.While all the tools mentioned have their respective differentiators, they all have common limitations in their search mechanisms that severely limit their accuracy. On the one hand, searching for keys using regexes might provide high signal for distinctive API key patterns such as Google ( AZia*) or Slack ( xoxp-*) keys, however, other patterns such as MongoDBs are indistinguishable from a universally unique identifier ( UUID). The regex results can also be quite noisy in that they match a lot of hashes/SHAs as well. On the other hand, Shannon entropy is a more comprehensive search method, but due to the sheer quantity of its output, it yields a high degree of false positives, making it untenable to use at scale.Since most approaches in this domain have mainly been a mixture of regexes and Shannon entropy, which each have their respective shortcomings, our team sought to develop a novel method leveraging deep learning to overcome the limitations of these methods.Here well introduce our deep learning based approach, called Radar, trained on features extracted from a broad set of API key patterns and their surrounding context in code. Utilizing contextual information is not a novel idea even regex based methods attempt this by looking backward for high-value words. However, our model approaches this problem in a more comprehensive way. Other solutions are based upon detection techniques that leverage heuristics around variable naming conventions, but this approach is rigid and brittle.For example, consider the generic API key regex in truffleHog:/[a|A][p|P][i|I][_]?[k|K][e|E][y|Y].*['|\\\"][0-9a-zA-Z]{32,45}['|\\\"]/Now, consider the following sample code:api_key = '12345678901234567890123456789012' api_token = '12345678901234567890123456789012'In this code example, when applying truffleHogs regex, the API key in the first line will be captured, while the second will not. This demonstrates that regex searching is limited to the lexical format of the code. Because variable naming conventions differ from developer to developer, this can easily lead to situations where designed regexes do not match the variable names that a regex is searching for. Abiding by naming conventions to be compliant with a regexs rules would not be too difficult, but as a prerequisite, developers would need to read the source code to understand and identify all situations that the tool would work well in.We dont believe tools should dictate and constrain how developers work instead the optimal tool should fit their existing workflow. The way that we deal with the problem of naming variation is that we dont require situations to be lexically similar they only need to be semantically similar. In other words, it shouldnt matter if the variable name is access_token or access_key since they have the same meaning.A deeper technical dive of our model will be done in a subsequent post. If youre eager to try out Radar on GitHub repos, feel free to jump ahead to Announcing the Radar API below.In this section, we evaluate the performance of our model against the tools mentioned above by scanning a sample code repository that mimics the potential occurrence of API keys in the real world. This was done to confirm our understanding of the algorithm in each tool, and to present visual examples of their advantages and limitations.We copied over ten real-world examples that we collected during the scanning process and also added nine ambiguous examples, which represent different misleading situations that can yield false positive detections.We highlighted the results found from each detector. True positives are highlighted in green. False positives are highlighted in red. We evaluated precision, recall and F1 score to quantify & compare model performance.For reference, precision is the fraction of correctly predicted positive instances among all predicted instances. Recall is the fraction of correctly predicted instances over total number of positive instances. F1 score is an overall measure of a models accuracy that combines precision and recall.truffleHogFirst, we scanned the repo with truffleHogs regex method.Precision: 3/3, Recall: 3/10.F1: 46%As mentioned, truffleHog has two regex patterns for capturing generic/application agnostic API keys, this pattern captures an alphanumeric token between 32 and 45 characters with a variable name containing apikey or secret.We next scanned the repo with truffleHogs Entropy method.Precision: 10/17, Recall: 10/10.F1: 74%truffleHogs high entropy method doesnt check variable names, it only calculates the Shannon entropy for each token and therefore, is quite noisy. When considering the low ratio of real API keys to false positives in real-world scenarios (<1:100), the number of results from this algorithm can be overwhelming to review.GitleaksCommand: gitleaks repo-path=./ verbosePrecision: 1/1, Recall: 1/10.F1: 18%Gitleaks is similar to truffleHog, in that it mainly uses regexes to detect credentials; however, their regexes are slightly non-traditional. For example, Gitleaks only captures api_key_github, which is an uncommon variable naming convention. The name github_api_key is the more common variable name people would assign to a GitHub API key. Nonetheless, almost all available tools support adding custom regexes into their regex list so this will not be a major issue if users adjust or input regexes to meet their needs.GitrobPrecision: 8/14, Recall: 8/10.F1: 66%Its hard to directly compare Gitrob to other tools because it effectively captures all strings longer than 20 characters, within the character set [a-fA-F09], and requires users to manually go through the results to check if they are valid. Gitrob provides a nice UI so that the user can easily go through each file.Watchtower RadarPrecision: 9/10, Recall: 9/10.F1: 90%Because Radar incorporates more comprehensive features as noted above, the false negative rate and false positive rate are both quite low, enhancing the user experience when verifying results.Its important to note that this test repository was created internally, and results will vary across repositories that are scanned. There are trade-offs to any of the approaches above, each with their own merits. As such, we have released Radar as an API that you can use to scan GitHub repositories for sensitive credentials. The service is free to use for the first 5 scans, so please feel free to try it here radar.watchtower.ai by logging in with GitHub. Note that the service scans both public and private repos, and does not store or track sensitive findings. Radar scans the full commit history for repos that have 1000 unique commits or fewer. For larger repos, Radar scans the current working directory (i.e. the latest version of the files in the default branch). To scan the full commit history of repos larger than 1000 commits, or to increase your scan limit, please contact us via email at support@watchtower.ai. You can also schedule a demo via our website at www.watchtower.ai.Start a ScanFor example, after logging in, you can start a new scan of public GitHub repo via the command line:curl https://radar.watchtower.ai/api/v1/scans/new \\-u API_KEY: \\-d 'public_github_url=https://github.com/watchtowerdlp/sample'This will run the scan asynchronously, and youll be notified when the results are ready to view:{ \"id\": \"0b6b08cf-f1ff-436b-a69f-7a1cb0d06e44\", \"url\": \"https://github.com/watchtowerdlp/sample\", \"duration\": \"0.822404097\", \"created_at\": \"2019-04-28 22:51:10 UTC\", \"scanned_files\": 24, \"status_code\": 200, \"results_count\": 1, \"results\": [ { \"result_id\": \"db2d2f85-8c06-41ef-85bf-2ea2dc786ca3\", \"repo_path\": \"sample\", \"file_path\": \"sample.rb\", \"branch\": \"origin/master\", \"commit_hash\": \"3a07b08d2461b6906376081d1f13b303215bf55d\", \"author_email\": \"49463194+watchtowerdlp@users.noreply.github.com\", \"context\": \"a4300836696c47b4f2d7c'\\n+\\n+auth_token = '\", \"token\": \"db\", \"token_length\": 32, \"permalink\": \"https://github.com/watchtowerdlp/sample/blob/3a07b08d2461b6906376081d1f13b303215bf55d/sample.rb#L7\", \"created_at\": \"2019-04-28 22:51:13 UTC\" } ]}Likewise, you can configure a webhook endpoint to be programmatically notified when the scan is complete and results are available for review. If you prefer, you can also view scan results in the dashboard, screenshots below. You can read the complete API docs here: radar.watchtower.ai.View scans in the dashboard:As well as their results:Its worth noting that during the course of our research we noticed that most keys that we found were stored deep in the commit history of a repo or were included in a now-deleted file. In git, deleting files with sensitive data, or deleting the tokens themselves, is actually only at the surface level, and the secrets can still be dug up in the commit history. As such, the only effective way to remove these keys is to delete the commit that introduced the key or to delete the entire commit history and start a brand new history. As such a detection tool like Radar could be applied as part of an engineers CI/CD workflow before any code gets pushed to a remote repo to prevent the need to edit the git history. This proactive approach would be instrumental in reducing the accidental exposure of secrets. Contact us if youre interested in leveraging Radar in this way.To increase your scan limit, or scan the full commit history of larger repos, please email us at support@watchtower.ai. Likewise, we would welcome your feedback on the API please dont hesitate to reach out via email with your thoughts.About WatchtowerWatchtower is a data security platform that uses machine learning to identify business-critical data, like customer PII, across SaaS and data infrastructure. Our team, based in San Francisco & Palo Alto and backed by leading Silicon Valley investors, has experience building cloud services & APIs and the software to protect the data flowing into & through those systems at some of the fastest growing platform companies in the world. Were hiring!Ive been in the security industry for a while and was looking for a strong, automated solution for data discovery, classification, and protection. I was very impressed with the accuracy of the classification on my unstructured data nothing on the market comes close to this.Shahar Ben-HadorCIO, ExabeamTo ensure I had the proper context to reduce risk at our company, I needed a solution that was cloud first and able to provide visibility into my SaaS providers & infrastructure without slowing down the business. There were plenty of proxy solutions out on the market but none that were able to provide the same visibility and frictionless experience for our team members, enabling us to combat data spray and classification issues. That was until we started leveraging Watchtower their API-driven solution accurately monitored and was able to take immediate action, which was a game changer for us.CISO Hyper-Growth Tech CompanyWatchtower saves us substantial time and is more effective than doing this manually We need this capability and Im grateful that Watchtower does such a good job. It is better than I expected and I typically have expectations that are high.CISOFortune 100 Company ",
          "interesting use case of nlp and impressive results!",
          "How many types of credentials it can cover?"],
        "story_type":"Normal",
        "url_raw":"https://medium.com/@watchtowerai/introducing-radar-api-detect-credentials-secrets-in-code-via-machine-learning-fe402b818bf1",
        "comments.comment_id":[19914505,
          19914686],
        "comments.comment_author":["sjegler91",
          "yiang"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-05-14T22:39:27Z",
          "2019-05-14T23:02:29Z"],
        "comments.comment_text":["interesting use case of nlp and impressive results!",
          "How many types of credentials it can cover?"],
        "id":"397bdcf2-72d6-481e-a757-7d8035bd1780",
        "url_text":"In 2016, hackers gained access to Ubers private code repositories and used hard-coded credentials to exfiltrate 57 million driver records from an AWS S3 bucket. As a result of this breach, and its subsequent cover-up, Uber was fined $148 million. Could they have prevented such an incident? If this can happen to Uber, it can happen to any other company.Source code hosting providers such as GitHub and GitLab have socialized software development, making it easier to collaborate on projects & ship code quickly. But this hasnt been without consequences: its led to an increase in the accidental publication of sensitive information such as API keys, secrets, and credentials. This sensitive information can range from SSH keys to API keys, and even passwords. After analyzing millions of commits, our research team found that this problem is widespread and occurs daily as new commits are pushed up as part of the software development lifecycle. Abuse of these leaked credentials can cause salient security & compliance risks, such as catastrophic loss of sensitive customer data, harming an organization both financially and reputationally, while putting consumers at risk.When asked about the Uber leak, GitHub had the following comment:Our recommendation is to never store access tokens, passwords, or other authentication or encryption keys in the code. If the developer must include them in the code, we recommend they implement additional operational safeguards to prevent unauthorized access or misuse.Even AWS, the largest cloud provider by market share, has been financially impacted by leaked AWS keys and was motivated to create Git-secrets to help combat this problem. However, they acknowledge the difficulty of this problem, and even they cannot guarantee fool-proof detection of their own AWS keys. Many organizations have become hyper-aware of the potential risks of credential leakage, so related open source tools are gaining popularity on platforms like GitHub despite their low accuracy rates.Our team has been evaluating existing tools, and weve built a solution that leverages machine learning to dramatically improve the chances of accurately detecting credentials in your source code. Read on to learn more.We evaluated several popular tools used today to protect and secure GitHub repositories:truffleHog, 3k+ stars on GitHub Searches through git repositories for high entropy strings and secrets, digging deep into commit historyGitrob, 3k+ stars on GitHub Reconnaissance tool for GitHub organizationsGitleaks, 4k+ start on GitHub Audit git repos for secretsGit-secrets, 5k+ stars on GitHub Prevents you from committing secrets and credentials into git repositoriesIts worth noting that we focused on the most popular open source projects there are other projects like Yelps detect-secrets and Auth0s repo-supervisor that serve a similar purpose, though we didnt include them in this analysis to avoid redundancy. Well provide a brief introduction of each tool, an analysis of their pros & cons, followed by a more detailed comparison with examples. If youre already familiar with common tools and are curious about our approach, feel free to skip ahead to the section titled Our Approach: Radar below.The two main algorithms used in these tools are entropy and regular expressions:Entropy: Shannon entropy, or information entropy, is the average rate at which information is produced by a stochastic source of data. A high entropy score is the result of high variability of information, e.g. a string with only one repetitive character will have low entropy, since there is only one state, and can be converted to 1 bit of information. On the other hand, a string with a diverse set of characters that appears highly random, such as API keys, will require more bits to transmit, having much higher entropy. See here for more info.Regular Expressions (Regex): A regular expression, or regex, is a sequence of characters that defines a search pattern. Regexes are used to perform any lexical searching to match a variable name or API key pattern. See here for more info.truffleHogtruffleHog is an open-source tool written in Python that searches through git commit history for credentials/secrets that are accidentally committed.Pros:truffleHog scans the whole history of branches and commits, thus nothing will be missed if committed.truffleHog allows for both regex based and high entropy based flagging.Users can provide custom regexes to suit their needs accordingly.Cons:High entropy is a commonly used method to detect randomly generated API keys in many tools. However, because of the lack of contextual awareness, this method tends to be noisy. For example, it can be hard to distinguish between long variable names and credentials, e.g. TestScrapeLoopRunReportsTargetDownOnInvalidUTF8 which is a high-entropy string and would likely get flagged as a false positive.Regular expressions are a powerful but limited method that searches for generic patterns. Thus, they only work well on finding keys with explicitly defined & repeatable patterns, e.g. starting with some fixed characters, or very lengthy keys. Requiring these unique characteristics dramatically reduces the pool of potential tokens that can be flagged accurately.GitrobGitrob is an open-source tool written in Go that helps find potentially sensitive files. Different than the other tools listed, it has a broader range of object detection beyond API keys.Pros:Similar to truffleHog, it drills deep into the commit history of a repository, and the user can adjust how far back in the commit history to scan.The UI is very friendly for users to manipulate and analyze results.Also provides regex for searching for generic keys, e.g. /([a-f09\\-\\$\\/]{20,})/gmi.Cons:The search mechanisms are fairly simple mainly keyword searching, which yields many false positives. Users have to manually go through all detected files and tokens to check if they contain valid sensitive info or not, which can be very time-consuming.The regex for API keys does not have a cap on the length of the key, which can lead to a high number of false positives and is not comprehensive enough to capture keys with more diverse character sets.GitleaksGitleaks is an open-source tool written in Go that provides a way to find unwanted data types in code checked in to git. Imprecisely, its a Go version of truffleHog, although algorithm-wise, there are unique aspects.Pros:It combines a regex list similar to truffleHog and a high entropy method to do credential detection.It offers the user options to adjust their regex list and entropy range.Cons:Similar to truffleHog.Git-secretsGit-secrets is a tool written in bash to prevent users from committing AWS API keys.Pros:The tools main methods only occupy one file, so the code is easy to understand, use, and extend.Cons:The searching domain is limited to several regexes for AWS keys.Poses potential risks missing real AWS API keys if the users variable name does not match Git-secrets regexes, which is a common problem for all regex based searching methods. In other words, Git-secrets detection mandates that the AWS secret key must have key in the variable name, so a change of the variable name to aws_secret or secret_token would not trigger the regex, leading to a false negative.While all the tools mentioned have their respective differentiators, they all have common limitations in their search mechanisms that severely limit their accuracy. On the one hand, searching for keys using regexes might provide high signal for distinctive API key patterns such as Google ( AZia*) or Slack ( xoxp-*) keys, however, other patterns such as MongoDBs are indistinguishable from a universally unique identifier ( UUID). The regex results can also be quite noisy in that they match a lot of hashes/SHAs as well. On the other hand, Shannon entropy is a more comprehensive search method, but due to the sheer quantity of its output, it yields a high degree of false positives, making it untenable to use at scale.Since most approaches in this domain have mainly been a mixture of regexes and Shannon entropy, which each have their respective shortcomings, our team sought to develop a novel method leveraging deep learning to overcome the limitations of these methods.Here well introduce our deep learning based approach, called Radar, trained on features extracted from a broad set of API key patterns and their surrounding context in code. Utilizing contextual information is not a novel idea even regex based methods attempt this by looking backward for high-value words. However, our model approaches this problem in a more comprehensive way. Other solutions are based upon detection techniques that leverage heuristics around variable naming conventions, but this approach is rigid and brittle.For example, consider the generic API key regex in truffleHog:/[a|A][p|P][i|I][_]?[k|K][e|E][y|Y].*['|\\\"][0-9a-zA-Z]{32,45}['|\\\"]/Now, consider the following sample code:api_key = '12345678901234567890123456789012' api_token = '12345678901234567890123456789012'In this code example, when applying truffleHogs regex, the API key in the first line will be captured, while the second will not. This demonstrates that regex searching is limited to the lexical format of the code. Because variable naming conventions differ from developer to developer, this can easily lead to situations where designed regexes do not match the variable names that a regex is searching for. Abiding by naming conventions to be compliant with a regexs rules would not be too difficult, but as a prerequisite, developers would need to read the source code to understand and identify all situations that the tool would work well in.We dont believe tools should dictate and constrain how developers work instead the optimal tool should fit their existing workflow. The way that we deal with the problem of naming variation is that we dont require situations to be lexically similar they only need to be semantically similar. In other words, it shouldnt matter if the variable name is access_token or access_key since they have the same meaning.A deeper technical dive of our model will be done in a subsequent post. If youre eager to try out Radar on GitHub repos, feel free to jump ahead to Announcing the Radar API below.In this section, we evaluate the performance of our model against the tools mentioned above by scanning a sample code repository that mimics the potential occurrence of API keys in the real world. This was done to confirm our understanding of the algorithm in each tool, and to present visual examples of their advantages and limitations.We copied over ten real-world examples that we collected during the scanning process and also added nine ambiguous examples, which represent different misleading situations that can yield false positive detections.We highlighted the results found from each detector. True positives are highlighted in green. False positives are highlighted in red. We evaluated precision, recall and F1 score to quantify & compare model performance.For reference, precision is the fraction of correctly predicted positive instances among all predicted instances. Recall is the fraction of correctly predicted instances over total number of positive instances. F1 score is an overall measure of a models accuracy that combines precision and recall.truffleHogFirst, we scanned the repo with truffleHogs regex method.Precision: 3/3, Recall: 3/10.F1: 46%As mentioned, truffleHog has two regex patterns for capturing generic/application agnostic API keys, this pattern captures an alphanumeric token between 32 and 45 characters with a variable name containing apikey or secret.We next scanned the repo with truffleHogs Entropy method.Precision: 10/17, Recall: 10/10.F1: 74%truffleHogs high entropy method doesnt check variable names, it only calculates the Shannon entropy for each token and therefore, is quite noisy. When considering the low ratio of real API keys to false positives in real-world scenarios (<1:100), the number of results from this algorithm can be overwhelming to review.GitleaksCommand: gitleaks repo-path=./ verbosePrecision: 1/1, Recall: 1/10.F1: 18%Gitleaks is similar to truffleHog, in that it mainly uses regexes to detect credentials; however, their regexes are slightly non-traditional. For example, Gitleaks only captures api_key_github, which is an uncommon variable naming convention. The name github_api_key is the more common variable name people would assign to a GitHub API key. Nonetheless, almost all available tools support adding custom regexes into their regex list so this will not be a major issue if users adjust or input regexes to meet their needs.GitrobPrecision: 8/14, Recall: 8/10.F1: 66%Its hard to directly compare Gitrob to other tools because it effectively captures all strings longer than 20 characters, within the character set [a-fA-F09], and requires users to manually go through the results to check if they are valid. Gitrob provides a nice UI so that the user can easily go through each file.Watchtower RadarPrecision: 9/10, Recall: 9/10.F1: 90%Because Radar incorporates more comprehensive features as noted above, the false negative rate and false positive rate are both quite low, enhancing the user experience when verifying results.Its important to note that this test repository was created internally, and results will vary across repositories that are scanned. There are trade-offs to any of the approaches above, each with their own merits. As such, we have released Radar as an API that you can use to scan GitHub repositories for sensitive credentials. The service is free to use for the first 5 scans, so please feel free to try it here radar.watchtower.ai by logging in with GitHub. Note that the service scans both public and private repos, and does not store or track sensitive findings. Radar scans the full commit history for repos that have 1000 unique commits or fewer. For larger repos, Radar scans the current working directory (i.e. the latest version of the files in the default branch). To scan the full commit history of repos larger than 1000 commits, or to increase your scan limit, please contact us via email at support@watchtower.ai. You can also schedule a demo via our website at www.watchtower.ai.Start a ScanFor example, after logging in, you can start a new scan of public GitHub repo via the command line:curl https://radar.watchtower.ai/api/v1/scans/new \\-u API_KEY: \\-d 'public_github_url=https://github.com/watchtowerdlp/sample'This will run the scan asynchronously, and youll be notified when the results are ready to view:{ \"id\": \"0b6b08cf-f1ff-436b-a69f-7a1cb0d06e44\", \"url\": \"https://github.com/watchtowerdlp/sample\", \"duration\": \"0.822404097\", \"created_at\": \"2019-04-28 22:51:10 UTC\", \"scanned_files\": 24, \"status_code\": 200, \"results_count\": 1, \"results\": [ { \"result_id\": \"db2d2f85-8c06-41ef-85bf-2ea2dc786ca3\", \"repo_path\": \"sample\", \"file_path\": \"sample.rb\", \"branch\": \"origin/master\", \"commit_hash\": \"3a07b08d2461b6906376081d1f13b303215bf55d\", \"author_email\": \"49463194+watchtowerdlp@users.noreply.github.com\", \"context\": \"a4300836696c47b4f2d7c'\\n+\\n+auth_token = '\", \"token\": \"db\", \"token_length\": 32, \"permalink\": \"https://github.com/watchtowerdlp/sample/blob/3a07b08d2461b6906376081d1f13b303215bf55d/sample.rb#L7\", \"created_at\": \"2019-04-28 22:51:13 UTC\" } ]}Likewise, you can configure a webhook endpoint to be programmatically notified when the scan is complete and results are available for review. If you prefer, you can also view scan results in the dashboard, screenshots below. You can read the complete API docs here: radar.watchtower.ai.View scans in the dashboard:As well as their results:Its worth noting that during the course of our research we noticed that most keys that we found were stored deep in the commit history of a repo or were included in a now-deleted file. In git, deleting files with sensitive data, or deleting the tokens themselves, is actually only at the surface level, and the secrets can still be dug up in the commit history. As such, the only effective way to remove these keys is to delete the commit that introduced the key or to delete the entire commit history and start a brand new history. As such a detection tool like Radar could be applied as part of an engineers CI/CD workflow before any code gets pushed to a remote repo to prevent the need to edit the git history. This proactive approach would be instrumental in reducing the accidental exposure of secrets. Contact us if youre interested in leveraging Radar in this way.To increase your scan limit, or scan the full commit history of larger repos, please email us at support@watchtower.ai. Likewise, we would welcome your feedback on the API please dont hesitate to reach out via email with your thoughts.About WatchtowerWatchtower is a data security platform that uses machine learning to identify business-critical data, like customer PII, across SaaS and data infrastructure. Our team, based in San Francisco & Palo Alto and backed by leading Silicon Valley investors, has experience building cloud services & APIs and the software to protect the data flowing into & through those systems at some of the fastest growing platform companies in the world. Were hiring!Ive been in the security industry for a while and was looking for a strong, automated solution for data discovery, classification, and protection. I was very impressed with the accuracy of the classification on my unstructured data nothing on the market comes close to this.Shahar Ben-HadorCIO, ExabeamTo ensure I had the proper context to reduce risk at our company, I needed a solution that was cloud first and able to provide visibility into my SaaS providers & infrastructure without slowing down the business. There were plenty of proxy solutions out on the market but none that were able to provide the same visibility and frictionless experience for our team members, enabling us to combat data spray and classification issues. That was until we started leveraging Watchtower their API-driven solution accurately monitored and was able to take immediate action, which was a game changer for us.CISO Hyper-Growth Tech CompanyWatchtower saves us substantial time and is more effective than doing this manually We need this capability and Im grateful that Watchtower does such a good job. It is better than I expected and I typically have expectations that are high.CISOFortune 100 Company ",
        "_version_":1718938180163469313},
      {
        "story_id":20269142,
        "story_author":"nullish",
        "story_descendants":14,
        "story_score":13,
        "story_time":"2019-06-24T22:22:09Z",
        "story_title":"Ask HN: GitHub or Gitlab",
        "search":["Ask HN: GitHub or Gitlab",
          "Migrating from an existing Git provider, curious what other folks have to say.<p>We're a building a SaaS app with no open source components at the moment.",
          "AskHN",
          "I have been enjoying Gitlab's built-in continuous integration service. If your app does not already use something else for continuous integration, it might be worthwhile to try out.",
          "We currently use Bitbucket (do not recommend) and we're evaluating GitLab.<p>First impression is that it's a great package. The only downside for us is that we have a lot of legacy baggage, mostly rather opinionated build tools set up the way we liked it. Getting that to work with GitLab, which itself has a very opinionated view of the CI/CD pipeline is a bit of effort. But for a greenfield project I would go with GitLab in a second!<p>GitLab is a \"batteries included\" kind of tool. They've put together a lot of excellent stuff and integrated different tools very well together. For example, we need to perform security scans on our code and artifacts and GitLab offers that with minimal effort and awesome integrations into your pull request (which they call merge requests) mechanism.<p>Some may not like their approach, especially if they already have something that doesn't quite fit. But if you're starting from scratch, you really should give them a chance. They really know what they're doing ;-)"],
        "story_text":"Migrating from an existing Git provider, curious what other folks have to say.<p>We're a building a SaaS app with no open source components at the moment.",
        "story_type":"AskHN",
        "comments.comment_id":[20269519,
          20273783],
        "comments.comment_author":["stockkid",
          "stfs"],
        "comments.comment_descendants":[0,
          1],
        "comments.comment_time":["2019-06-24T23:15:51Z",
          "2019-06-25T13:07:46Z"],
        "comments.comment_text":["I have been enjoying Gitlab's built-in continuous integration service. If your app does not already use something else for continuous integration, it might be worthwhile to try out.",
          "We currently use Bitbucket (do not recommend) and we're evaluating GitLab.<p>First impression is that it's a great package. The only downside for us is that we have a lot of legacy baggage, mostly rather opinionated build tools set up the way we liked it. Getting that to work with GitLab, which itself has a very opinionated view of the CI/CD pipeline is a bit of effort. But for a greenfield project I would go with GitLab in a second!<p>GitLab is a \"batteries included\" kind of tool. They've put together a lot of excellent stuff and integrated different tools very well together. For example, we need to perform security scans on our code and artifacts and GitLab offers that with minimal effort and awesome integrations into your pull request (which they call merge requests) mechanism.<p>Some may not like their approach, especially if they already have something that doesn't quite fit. But if you're starting from scratch, you really should give them a chance. They really know what they're doing ;-)"],
        "id":"e0dd6f69-5bc2-4553-933e-14628ddd9552",
        "_version_":1718938193035788288},
      {
        "story_id":20665955,
        "story_author":"throwaway2048",
        "story_descendants":2,
        "story_score":5,
        "story_time":"2019-08-11T03:04:00Z",
        "story_title":"Mini review of tog: the new OpenBSD git browser",
        "search":["Mini review of tog: the new OpenBSD git browser",
          "Normal",
          "http://akpoff.com/archive/2019/mini_review_of_tog.html",
          "Just tried tog(1), the \"interactive read-only browser for Git repositories\" included with Game of Trees. May I just say swoon? tog has log, diff, blame, and tree views. You can start in any of the views by calling tog with appropriate sub-command, or start from default log mode and switch to tree by pressing t, blame by pressing enter, or diff by pressing enter on a line with a hash. If you're already in a repo, tog log will launch with the current repo log in view, otherwise pass one or more of the repository path (-r), commit (-c), or path (file or directory). tog blame accepts the same parameters except path must be a file. tog diff accepts a repository path and SHA1 ids for the objects, while tog tree accepts only an optional commit (-c), or repository path. If the terminal is wide enough, tog has an automatic split screen mode. Pressing enter on any line in the log view will open the diff pane to the right of the log pane. Press tab to switch between the two. Split view works everywhere that tog would normally switch to a new screen. In blame mode tog makes it easy to see file in its entirety and the commit. For those of us who work at the command line, tog is a welcome addition to the toolset. With tog you'll spend less time in git log | more, and more time reviewing code and changes. Congratulations and thanks to Stefan Sperling and Joshua Stein for producing a useful and usable tool! On OpenBSD got(1) is in the -current pkg repo, or the source is available on the Game of Trees site. ",
          "Would appreciate it if mod didn't change the title on this, because as of presently, nobody has any idea what \"tog\" is.",
          "I wanted to try it out just for 5 minutes, but alas, I'm on Arch Linux and can't compile. GNU Make complains about dots in the Makefile (things like .include and .if) and there are probably more issues like BSD specific .include <bsd.subdir.mk>.<p>It's the early days for the tool but hopefully it will become available on Linux in the future. Because the world totally needs yet another git wrapper and visualisation tool. I jest of course, but there are already a few tools I wanted to learn but never dedicated the time for. For example <i>hub</i> command line utility for working with github. <i>git-extras</i> for lots of small things like repository statistics. <i>fugitive.vim</i>, a vim plugin that claims to be a git wrapper so awesome it should be illegal.<p>Recently I had to find out why a section of code was written a certain way and decided to hunt for the commit that introduced it. The only way to do it that I'm currently familiar with is by using github blame interface. The code was moved around so regular blame showed the commit that moved the code instead of introduced it. Github allows you to jump to a commit before the change, so I did that a couple of times while looking at the diff of where the code was removed from, because this was the place I actually needed. Overall I felt rather clumsy doing that and wondered if learning to do it on command line would be easier in the future. How can tog help here? Avoiding learning to use git on command line by itself isn't a good idea in any case, but a proper browser that allows to easily dive through history without shell trickery or a dozen of HTTP requests is a welcome addition.<p>Also, for tools like this I think video demonstration is best."],
        "story_type":"Normal",
        "url_raw":"http://akpoff.com/archive/2019/mini_review_of_tog.html",
        "url_text":"Just tried tog(1), the \"interactive read-only browser for Git repositories\" included with Game of Trees. May I just say swoon? tog has log, diff, blame, and tree views. You can start in any of the views by calling tog with appropriate sub-command, or start from default log mode and switch to tree by pressing t, blame by pressing enter, or diff by pressing enter on a line with a hash. If you're already in a repo, tog log will launch with the current repo log in view, otherwise pass one or more of the repository path (-r), commit (-c), or path (file or directory). tog blame accepts the same parameters except path must be a file. tog diff accepts a repository path and SHA1 ids for the objects, while tog tree accepts only an optional commit (-c), or repository path. If the terminal is wide enough, tog has an automatic split screen mode. Pressing enter on any line in the log view will open the diff pane to the right of the log pane. Press tab to switch between the two. Split view works everywhere that tog would normally switch to a new screen. In blame mode tog makes it easy to see file in its entirety and the commit. For those of us who work at the command line, tog is a welcome addition to the toolset. With tog you'll spend less time in git log | more, and more time reviewing code and changes. Congratulations and thanks to Stefan Sperling and Joshua Stein for producing a useful and usable tool! On OpenBSD got(1) is in the -current pkg repo, or the source is available on the Game of Trees site. ",
        "comments.comment_id":[20665958,
          20666331],
        "comments.comment_author":["throwaway2048",
          "paulriddle"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-08-11T03:04:34Z",
          "2019-08-11T05:00:42Z"],
        "comments.comment_text":["Would appreciate it if mod didn't change the title on this, because as of presently, nobody has any idea what \"tog\" is.",
          "I wanted to try it out just for 5 minutes, but alas, I'm on Arch Linux and can't compile. GNU Make complains about dots in the Makefile (things like .include and .if) and there are probably more issues like BSD specific .include <bsd.subdir.mk>.<p>It's the early days for the tool but hopefully it will become available on Linux in the future. Because the world totally needs yet another git wrapper and visualisation tool. I jest of course, but there are already a few tools I wanted to learn but never dedicated the time for. For example <i>hub</i> command line utility for working with github. <i>git-extras</i> for lots of small things like repository statistics. <i>fugitive.vim</i>, a vim plugin that claims to be a git wrapper so awesome it should be illegal.<p>Recently I had to find out why a section of code was written a certain way and decided to hunt for the commit that introduced it. The only way to do it that I'm currently familiar with is by using github blame interface. The code was moved around so regular blame showed the commit that moved the code instead of introduced it. Github allows you to jump to a commit before the change, so I did that a couple of times while looking at the diff of where the code was removed from, because this was the place I actually needed. Overall I felt rather clumsy doing that and wondered if learning to do it on command line would be easier in the future. How can tog help here? Avoiding learning to use git on command line by itself isn't a good idea in any case, but a proper browser that allows to easily dive through history without shell trickery or a dozen of HTTP requests is a welcome addition.<p>Also, for tools like this I think video demonstration is best."],
        "id":"171125f0-0aa0-435d-be20-11101680e2af",
        "_version_":1718938207667617792},
      {
        "story_id":20242820,
        "story_author":"keufran",
        "story_descendants":58,
        "story_score":74,
        "story_time":"2019-06-21T14:34:59Z",
        "story_title":"Ask HN: How to handle code reviews with a visually impaired coworker?",
        "search":["Ask HN: How to handle code reviews with a visually impaired coworker?",
          "Hello World,<p>My dev team has switched to a workflow using merge requests and code reviews (mostly by commenting the merge request). Our tool is Gitlab.<p>I needed to embed a visually impaired co-worker in my team but I face some difficulties:<p>- Gitlab accessibility seems to be really bad and my co-worker is unable to use the interface  to create Merge Request (he uses accessibility tools, of course). I dont'even speak about reading and writing comments in merge request.<p>- I don't know how to handle the code review process with him. We could do physical Code Review sessions, but it's difficult because I've a very chaotic schedule and so it's difficult to find a common timeslot. Furthermore, it's very difficult for my co-worker to handle all the remarks in one session for any Merge Request with a significant amount of code.<p>- I need to keep in place the existing tooling for the rest of the team<p>Does anybody knows of tools interfacing with Gitlab (or the git repository) or methodologies that could help us ?",
          "AskHN",
          "I'm a totally blind developer and I find the easiest way to do code reviews is to use git format-patch on the branch containing the code. I read the patch files in a text editor. Perhaps comments in the pull request referencing a commit and line would allow the developer to get the required context from the patch files?",
          "I work with a blind developer. We create personal feature branches, file a ticket for merge requests in our ticketing system, code review them directly in the ticket or via email, and then rebase the feature branch onto master. We don't, but you could plausibly do reviews directly in git via editing the file with any comments.<p>I've been wanting to try gerrit however: <a href=\"https://gerrit-review.googlesource.com/Documentation/dev-design.html#_accessibility_considerations\" rel=\"nofollow\">https://gerrit-review.googlesource.com/Documentation/dev-des...</a><p>Please report back on whatever you end up doing.<p>One caution I have is that long term you should consider moving away from the gitlab tools if they are unable to fix accessibility concerns, so that everyone is on equal ground.<p>I also don't know how flexible he is on which browser or operating system he uses; he may want to try some others to see if they work any better."],
        "story_text":"Hello World,<p>My dev team has switched to a workflow using merge requests and code reviews (mostly by commenting the merge request). Our tool is Gitlab.<p>I needed to embed a visually impaired co-worker in my team but I face some difficulties:<p>- Gitlab accessibility seems to be really bad and my co-worker is unable to use the interface  to create Merge Request (he uses accessibility tools, of course). I dont'even speak about reading and writing comments in merge request.<p>- I don't know how to handle the code review process with him. We could do physical Code Review sessions, but it's difficult because I've a very chaotic schedule and so it's difficult to find a common timeslot. Furthermore, it's very difficult for my co-worker to handle all the remarks in one session for any Merge Request with a significant amount of code.<p>- I need to keep in place the existing tooling for the rest of the team<p>Does anybody knows of tools interfacing with Gitlab (or the git repository) or methodologies that could help us ?",
        "story_type":"AskHN",
        "comments.comment_id":[20243514,
          20244110],
        "comments.comment_author":["jareds",
          "sn"],
        "comments.comment_descendants":[3,
          2],
        "comments.comment_time":["2019-06-21T15:45:30Z",
          "2019-06-21T16:41:11Z"],
        "comments.comment_text":["I'm a totally blind developer and I find the easiest way to do code reviews is to use git format-patch on the branch containing the code. I read the patch files in a text editor. Perhaps comments in the pull request referencing a commit and line would allow the developer to get the required context from the patch files?",
          "I work with a blind developer. We create personal feature branches, file a ticket for merge requests in our ticketing system, code review them directly in the ticket or via email, and then rebase the feature branch onto master. We don't, but you could plausibly do reviews directly in git via editing the file with any comments.<p>I've been wanting to try gerrit however: <a href=\"https://gerrit-review.googlesource.com/Documentation/dev-design.html#_accessibility_considerations\" rel=\"nofollow\">https://gerrit-review.googlesource.com/Documentation/dev-des...</a><p>Please report back on whatever you end up doing.<p>One caution I have is that long term you should consider moving away from the gitlab tools if they are unable to fix accessibility concerns, so that everyone is on equal ground.<p>I also don't know how flexible he is on which browser or operating system he uses; he may want to try some others to see if they work any better."],
        "id":"36f1ae00-e0ee-4457-80cb-3a36c5dda927",
        "_version_":1718938192024961024},
      {
        "story_id":20993747,
        "story_author":"arzzen",
        "story_descendants":9,
        "story_score":120,
        "story_time":"2019-09-17T11:29:15Z",
        "story_title":"Show HN: Statistical tool for analyzing a Git repository",
        "search":["Show HN: Statistical tool for analyzing a Git repository",
          "ShowHN",
          "https://github.com/arzzen/git-quick-stats/",
          "GIT quick statistics git-quick-stats is a simple and efficient way to access various statistics in a git repository. Any git repository may contain tons of information about commits, contributors, and files. Extracting this information is not always trivial, mostly because there are a gadzillion options to a gadzillion git commands I dont think there is a single person alive who knows them all. Probably not even Linus Torvalds himself :). Table of Contents Screenshots Usage Interactive Non-interactive Command-line arguments Git log since and until Git log limit Git log options Git pathspec Git merge view strategy Color themes Installation UNIX and Linux macOS Windows Docker System requirements Dependencies FAQ Contribution Code reviews Some tips for good pull requests Formatting Tests Licensing Contributors Backers Sponsors Screenshots Usage Interactive git-quick-stats has a built-in interactive menu that can be executed as such: Or Non-interactive For those who prefer to utilize command-line options, git-quick-stats also has a non-interactive mode supporting both short and long options: git-quick-stats <optional-command-to-execute-directly> Or git quick-stats <optional-command-to-execute-directly> Command-line arguments Possible arguments in short and long form: GENERATE OPTIONS -T, --detailed-git-stats give a detailed list of git stats -R, --git-stats-by-branch see detailed list of git stats by branch -c, --changelogs see changelogs -L, --changelogs-by-author see changelogs by author -S, --my-daily-stats see your current daily stats -V, --csv-output-by-branch output daily stats by branch in CSV format -j, --json-output save git log as a JSON formatted file to a specified area LIST OPTIONS -b, --branch-tree show an ASCII graph of the git repo branch history -D, --branches-by-date show branches by date -C, --contributors see a list of everyone who contributed to the repo -a, --commits-per-author displays a list of commits per author -d, --commits-per-day displays a list of commits per day -m, --commits-by-month displays a list of commits per month -w, --commits-by-weekday displays a list of commits per weekday -o, --commits-by-hour displays a list of commits per hour -A, --commits-by-author-by-hour displays a list of commits per hour by author -z, --commits-by-timezone displays a list of commits per timezone -Z, --commits-by-author-by-timezone displays a list of commits per timezone by author SUGGEST OPTIONS -r, --suggest-reviewers show the best people to contact to review code -h, -?, --help display this help text in the terminal Git log since and until You can set the variables _GIT_SINCE and/or _GIT_UNTIL before running git-quick-stats to limit the git log. These work similar to git's built-in --since and --until log options. export _GIT_SINCE=\"2017-01-20\" export _GIT_UNTIL=\"2017-01-22\" Once set, run git quick-stats as normal. Note that this affects all stats that parse the git log history until unset. Git log limit You can set variable _GIT_LIMIT for limited output. It will affect the \"changelogs\" and \"branch tree\" options. Git log options You can set _GIT_LOG_OPTIONS for git log options: export _GIT_LOG_OPTIONS=\"--ignore-all-space --ignore-blank-lines\" Git pathspec You can exclude a directory from the stats by using pathspec export _GIT_PATHSPEC=':!directory' You can also exclude files from the stats. Note that it works with any alphanumeric, glob, or regex that git respects. export _GIT_PATHSPEC=':!package-lock.json' Git merge view strategy You can set the variable _GIT_MERGE_VIEW to enable merge commits to be part of the stats by setting _GIT_MERGE_VIEW to enable. You can also choose to only show merge commits by setting _GIT_MERGE_VIEW to exclusive. Default is to not show merge commits. These work similar to git's built-in --merges and --no-merges log options. export _GIT_MERGE_VIEW=\"enable\" export _GIT_MERGE_VIEW=\"exclusive\" Git branch You can set the variable _GIT_BRANCH to set the branch of the stats. Works with commands --git-stats-by-branch and --csv-output-by-branch. export _GIT_BRANCH=\"master\" Color themes You can change to the legacy color scheme by toggling the variable _MENU_THEME between default and legacy export _MENU_THEME=\"legacy\" Installation Debian and Ubuntu If you are on at least Debian Bullseye or Ubuntu Focal you can use apt for installation: apt install git-quick-stats UNIX and Linux git clone https://github.com/arzzen/git-quick-stats.git && cd git-quick-stats sudo make install For uninstalling, open up the cloned directory and run For update/reinstall macOS (homebrew) brew install git-quick-stats Or you can follow the UNIX and Linux instructions if you wish. Windows If you are installing with Cygwin, use these scripts: installer uninstaller If you are wishing to use this with WSL, follow the UNIX and Linux instructions. Docker You can use the Docker image provided: Build: docker build -t arzzen/git-quick-stats . Run interactive menu: docker run --rm -it -v $(pwd):/git arzzen/git-quick-stats Docker pull command: docker pull arzzen/git-quick-stats docker repository System requirements An OS with a Bash shell Tools we use: awk basename cat column echo git grep head printf seq sort tput tr uniq wc Dependencies bsdmainutils apt install bsdmainutils FAQ Q: I get some errors after run git-quick-stats in cygwin like /usr/local/bin/git-quick-stats: line 2: $'\\r': command not found A: You can run the dos2unix app in cygwin as follows: /bin/dos2unix.exe /usr/local/bin/git-quick-stats. This will convert the script from the CR-LF convention that Microsoft uses to the LF convention that UNIX, OS X, and Linux use. You should then should be able to run it as normal. Q: How they could be used in a project with many git projects and statistics would show a summary of all git projects? A: If you want to include submodule logs, you can try using the following: export _GIT_LOG_OPTIONS=\"-p --submodule=log\" (more info about git log --submodule) Contribution Want to contribute? Great! First, read this page. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Some tips for good pull requests Use our code When in doubt, try to stay true to the existing code of the project. Write a descriptive commit message. What problem are you solving and what are the consequences? Where and what did you test? Some good tips: here and here. If your PR consists of multiple commits which are successive improvements / fixes to your first commit, consider squashing them into a single commit (git rebase -i) such that your PR is a single commit on top of the current HEAD. This make reviewing the code so much easier, and our history more readable. Formatting This documentation is written using standard markdown syntax. Please submit your changes using the same syntax. Tests Licensing MIT see LICENSE for the full license text. Contributors This project exists thanks to all the people who contribute. Backers Thank you to all our backers! [Become a backer] Sponsors Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor] ",
          "Pull Panda [0] is another tool we've been using that is offered as a SaaS and is now free since it was acquired by Github this year. It tells you average PR review time, average PR diff size, who is most requested for review, review-comment ratio, etc. I can't believe it's taken Github so long to make progress on dashboards like this for engineering managers, but looking forward to the time Pull Panda is fully integrated.<p>[0] <a href=\"https://pullreminders.com\" rel=\"nofollow\">https://pullreminders.com</a>",
          "I made a tool myself [0] with slightly different tools. It answers two questions: Who are the relevant coders and what parts of the code are the hotspots?<p>Here is an example run on sqlite:<p><pre><code>    Top Committers (of 28 authors):\n    D. Richard Hipp      13359 commits during 19 years until 2019-09-17\n    Dan Kennedy          5813 commits during 17 years until 2019-09-16\n     together these authors have 80+% of the commits (19172/20987)\n\n    Files with most commits:\n    1143 commits: src/sqlite.h.in      during 19 years until 2019-09-16\n    1331 commits: src/where.c          during 19 years until 2019-09-03\n    1360 commits: src/btree.c          during 18 years until 2019-08-24\n    1650 commits: src/vdbe.c           during 19 years until 2019-09-16\n    1893 commits: src/sqliteInt.h      during 19 years until 2019-09-14\n\n    Files with most authors:\n    11 authors: src/main.c          \n    11 authors: src/sqliteInt.h     \n    12 authors: configure.ac        \n    12 authors: src/shell.c         \n    15 authors: Makefile.in         \n\n    By file extension:\n    .test: 1333 files\n       .c: 379 files\n     together these make up 80+% of the files (1712/2138)\n</code></pre>\n[0] <a href=\"https://github.com/qznc/dot/blob/master/bin/git-overview\" rel=\"nofollow\">https://github.com/qznc/dot/blob/master/bin/git-overview</a>"],
        "story_type":"ShowHN",
        "url_raw":"https://github.com/arzzen/git-quick-stats/",
        "comments.comment_id":[20994452,
          20999219],
        "comments.comment_author":["eatonphil",
          "qznc"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-09-17T12:59:08Z",
          "2019-09-17T19:36:32Z"],
        "comments.comment_text":["Pull Panda [0] is another tool we've been using that is offered as a SaaS and is now free since it was acquired by Github this year. It tells you average PR review time, average PR diff size, who is most requested for review, review-comment ratio, etc. I can't believe it's taken Github so long to make progress on dashboards like this for engineering managers, but looking forward to the time Pull Panda is fully integrated.<p>[0] <a href=\"https://pullreminders.com\" rel=\"nofollow\">https://pullreminders.com</a>",
          "I made a tool myself [0] with slightly different tools. It answers two questions: Who are the relevant coders and what parts of the code are the hotspots?<p>Here is an example run on sqlite:<p><pre><code>    Top Committers (of 28 authors):\n    D. Richard Hipp      13359 commits during 19 years until 2019-09-17\n    Dan Kennedy          5813 commits during 17 years until 2019-09-16\n     together these authors have 80+% of the commits (19172/20987)\n\n    Files with most commits:\n    1143 commits: src/sqlite.h.in      during 19 years until 2019-09-16\n    1331 commits: src/where.c          during 19 years until 2019-09-03\n    1360 commits: src/btree.c          during 18 years until 2019-08-24\n    1650 commits: src/vdbe.c           during 19 years until 2019-09-16\n    1893 commits: src/sqliteInt.h      during 19 years until 2019-09-14\n\n    Files with most authors:\n    11 authors: src/main.c          \n    11 authors: src/sqliteInt.h     \n    12 authors: configure.ac        \n    12 authors: src/shell.c         \n    15 authors: Makefile.in         \n\n    By file extension:\n    .test: 1333 files\n       .c: 379 files\n     together these make up 80+% of the files (1712/2138)\n</code></pre>\n[0] <a href=\"https://github.com/qznc/dot/blob/master/bin/git-overview\" rel=\"nofollow\">https://github.com/qznc/dot/blob/master/bin/git-overview</a>"],
        "id":"985667d2-c5c4-4000-98b5-c7b9c52d0ba1",
        "url_text":"GIT quick statistics git-quick-stats is a simple and efficient way to access various statistics in a git repository. Any git repository may contain tons of information about commits, contributors, and files. Extracting this information is not always trivial, mostly because there are a gadzillion options to a gadzillion git commands I dont think there is a single person alive who knows them all. Probably not even Linus Torvalds himself :). Table of Contents Screenshots Usage Interactive Non-interactive Command-line arguments Git log since and until Git log limit Git log options Git pathspec Git merge view strategy Color themes Installation UNIX and Linux macOS Windows Docker System requirements Dependencies FAQ Contribution Code reviews Some tips for good pull requests Formatting Tests Licensing Contributors Backers Sponsors Screenshots Usage Interactive git-quick-stats has a built-in interactive menu that can be executed as such: Or Non-interactive For those who prefer to utilize command-line options, git-quick-stats also has a non-interactive mode supporting both short and long options: git-quick-stats <optional-command-to-execute-directly> Or git quick-stats <optional-command-to-execute-directly> Command-line arguments Possible arguments in short and long form: GENERATE OPTIONS -T, --detailed-git-stats give a detailed list of git stats -R, --git-stats-by-branch see detailed list of git stats by branch -c, --changelogs see changelogs -L, --changelogs-by-author see changelogs by author -S, --my-daily-stats see your current daily stats -V, --csv-output-by-branch output daily stats by branch in CSV format -j, --json-output save git log as a JSON formatted file to a specified area LIST OPTIONS -b, --branch-tree show an ASCII graph of the git repo branch history -D, --branches-by-date show branches by date -C, --contributors see a list of everyone who contributed to the repo -a, --commits-per-author displays a list of commits per author -d, --commits-per-day displays a list of commits per day -m, --commits-by-month displays a list of commits per month -w, --commits-by-weekday displays a list of commits per weekday -o, --commits-by-hour displays a list of commits per hour -A, --commits-by-author-by-hour displays a list of commits per hour by author -z, --commits-by-timezone displays a list of commits per timezone -Z, --commits-by-author-by-timezone displays a list of commits per timezone by author SUGGEST OPTIONS -r, --suggest-reviewers show the best people to contact to review code -h, -?, --help display this help text in the terminal Git log since and until You can set the variables _GIT_SINCE and/or _GIT_UNTIL before running git-quick-stats to limit the git log. These work similar to git's built-in --since and --until log options. export _GIT_SINCE=\"2017-01-20\" export _GIT_UNTIL=\"2017-01-22\" Once set, run git quick-stats as normal. Note that this affects all stats that parse the git log history until unset. Git log limit You can set variable _GIT_LIMIT for limited output. It will affect the \"changelogs\" and \"branch tree\" options. Git log options You can set _GIT_LOG_OPTIONS for git log options: export _GIT_LOG_OPTIONS=\"--ignore-all-space --ignore-blank-lines\" Git pathspec You can exclude a directory from the stats by using pathspec export _GIT_PATHSPEC=':!directory' You can also exclude files from the stats. Note that it works with any alphanumeric, glob, or regex that git respects. export _GIT_PATHSPEC=':!package-lock.json' Git merge view strategy You can set the variable _GIT_MERGE_VIEW to enable merge commits to be part of the stats by setting _GIT_MERGE_VIEW to enable. You can also choose to only show merge commits by setting _GIT_MERGE_VIEW to exclusive. Default is to not show merge commits. These work similar to git's built-in --merges and --no-merges log options. export _GIT_MERGE_VIEW=\"enable\" export _GIT_MERGE_VIEW=\"exclusive\" Git branch You can set the variable _GIT_BRANCH to set the branch of the stats. Works with commands --git-stats-by-branch and --csv-output-by-branch. export _GIT_BRANCH=\"master\" Color themes You can change to the legacy color scheme by toggling the variable _MENU_THEME between default and legacy export _MENU_THEME=\"legacy\" Installation Debian and Ubuntu If you are on at least Debian Bullseye or Ubuntu Focal you can use apt for installation: apt install git-quick-stats UNIX and Linux git clone https://github.com/arzzen/git-quick-stats.git && cd git-quick-stats sudo make install For uninstalling, open up the cloned directory and run For update/reinstall macOS (homebrew) brew install git-quick-stats Or you can follow the UNIX and Linux instructions if you wish. Windows If you are installing with Cygwin, use these scripts: installer uninstaller If you are wishing to use this with WSL, follow the UNIX and Linux instructions. Docker You can use the Docker image provided: Build: docker build -t arzzen/git-quick-stats . Run interactive menu: docker run --rm -it -v $(pwd):/git arzzen/git-quick-stats Docker pull command: docker pull arzzen/git-quick-stats docker repository System requirements An OS with a Bash shell Tools we use: awk basename cat column echo git grep head printf seq sort tput tr uniq wc Dependencies bsdmainutils apt install bsdmainutils FAQ Q: I get some errors after run git-quick-stats in cygwin like /usr/local/bin/git-quick-stats: line 2: $'\\r': command not found A: You can run the dos2unix app in cygwin as follows: /bin/dos2unix.exe /usr/local/bin/git-quick-stats. This will convert the script from the CR-LF convention that Microsoft uses to the LF convention that UNIX, OS X, and Linux use. You should then should be able to run it as normal. Q: How they could be used in a project with many git projects and statistics would show a summary of all git projects? A: If you want to include submodule logs, you can try using the following: export _GIT_LOG_OPTIONS=\"-p --submodule=log\" (more info about git log --submodule) Contribution Want to contribute? Great! First, read this page. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Some tips for good pull requests Use our code When in doubt, try to stay true to the existing code of the project. Write a descriptive commit message. What problem are you solving and what are the consequences? Where and what did you test? Some good tips: here and here. If your PR consists of multiple commits which are successive improvements / fixes to your first commit, consider squashing them into a single commit (git rebase -i) such that your PR is a single commit on top of the current HEAD. This make reviewing the code so much easier, and our history more readable. Formatting This documentation is written using standard markdown syntax. Please submit your changes using the same syntax. Tests Licensing MIT see LICENSE for the full license text. Contributors This project exists thanks to all the people who contribute. Backers Thank you to all our backers! [Become a backer] Sponsors Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor] ",
        "_version_":1718938220544131072},
      {
        "story_id":21403906,
        "story_author":"guessmyname",
        "story_descendants":32,
        "story_score":59,
        "story_time":"2019-10-30T23:02:02Z",
        "story_title":"GitHub Student Developer Pack",
        "search":["GitHub Student Developer Pack",
          "Normal",
          "https://github.blog/2019-10-30-get-over-100k-worth-of-tools-with-the-github-student-developer-pack/",
          "The GitHub Student Developer Pack now offers over $100k worth of tools and training to every student developer, anywhere that GitHub is available. If youre new to the Pack, it provides verified students with GitHub Pro at no charge while they are in school, plus free access to the best developer tools and trainingthanks to our partnersso they can learn by doing. As the Pack continues to grow and shape the next generation of developers, we continue to listen. Were here to better understand how youre using these tools and whats missing that you hope to see included. Whether youre developing your portfolio, building a new desktop app, or creating an interactive mapthe goal of the Pack is to provide you with the tools you need to be successful. This year, the value of the Pack tripled during the Fall semester by adding nineteen new partners in October plus a dozen in September to the twenty-one who joined our long-time partners for Back-to-School. Lets highlight the partners who joined us since our last post in August. A few words from our new partners We ask all of our new partners what motivated them to join the Pack. Heres a sample of what theyve told us: When I was a student, I actually used the GitHub Student Developer Pack myself! It allowed me to test and learn how to use tools that I wouldnt have been able to otherwise. The Pack was a great help! -Floran Pagliai, Weglot The way that developers learn best is by getting their hands dirty, trying different things, and experimenting with a variety of tools. The Pack allows developers to do that and get exposure creating awesome projects outside the confines of the classroom. -Naeem ul Haq, Educative Not many high school and even college students would ordinarily be able to afford [our tool.] Letting students try it will give us valuable feedback about what up-and-coming developers are looking for. -Levie Rufenacht, Wisej The Pack opens new doors to students that were not accessible before. It basically gives them whats needed to build a project from A to Z, lets them cultivate their curiosity, an essential quality for a developer, and free their creativity. Learning should know no boundaries. -Julien Lehuraux, USE Together Partner details The following are our new partners and the tools and training they are providing, for free, to students: October Appfigures, app store analytics, optimization, and intelligence Astra Security, security suite for your website, including firewall, malware scanner, and a managed bug bounty platform BoltFlare, reliable, secure, and scalable managed WordPress hosting BrowserStack, test your web apps, providing instant access to 2,000+ browsers and real iOS and Android devices Codecov, implement code coverage easier to develop healthier code Educative, level up on trending coding skills at your own pace with interactive, text-based courses EverSQL, boost your database performance by automatically optimizing your SQL queries HazeOver, get focused while working on projects or studying Iconscout, design resources marketplace with high quality icons, illustrations, and stock images Interview Cake, makes coding interviews a piece of cake with practice questions, data structures and algorithms reference pages, cheat sheets, and more Kaltura, build interactive video experiences and advanced media applications MNX.io, managed Cloud hosting for developers NetLicensing, cost-effective and integrated Licensing-as-a-Service (LaaS) solution for your software on any platform from Desktop to IoT and SaaS Scrapinghub, battle-tested cloud platform for running web crawlers where you can manage and automate your web spiders at scale Testmail, unlimited email addresses and mailboxes for automating email tests with powerful APIs Typeform, interactive forms, surveys, and quizzes to engage and grow your audience USE-Together, provides a remote pair programming and team collaboration tool Weglot, make any website multilingual and manage your translations through a single platform Wisej, build powerful web applications in Visual Studio with C# or VB.NET September Blackfire, Code performance measurement tool where you can find and fix bottlenecks Canva, create professional looking graphics and designs, featuring thousands of templates and an easy to use editor Covalence, provides an exclusive developer community and allows you to learn Full Stack web-development with no long-term commitments Crowdin, cloud-based solution that streamlines localization management Education Host, web hosting platform to host assignment and project work GoRails, tutorials for web developers learning Ruby, Rails, Javascript, Turbolinks, Stimulus.js, Vue.js, and more Honeybadger, exception, uptime, and cron monitoring Mailgun, APIs that enable you to send, receive, and track email One Month, learn HTML, CSS, JavaScript and Python in just 30 days Repl.it, an online IDE that lets you instantly code in over fifty languages so you can start learning, building, collaborating, and hosting all in one place Storyscript, top-level, cloud native programming language that helps you orchestrate data flow seamlessly between microservices Vaadin, open source Java framework for building Progressive Web Applications What can I do with the Pack? Looking for ideas for how to use all the new tools? Take a look at these projects or check out GitHub Educations Twitter and Facebook for suggestions. Make a request Have a tool youd like to see included in the Pack? Let the developer know (and tag us) on social media using the hashtag #GitHubPack. Companies interested in including an offer in the Pack can apply to be a partner. Not yet a member of the Pack? Its available for all verified students ages 13+, anywhere in the world where GitHub is available. Join today using your school-issued email, student ID, or other proof of current academic enrollment. Explore the latest offers in the GitHub Student Developer Pack ",
          "For those who are part of Hack Club (<a href=\"https://hackclub.com\" rel=\"nofollow\">https://hackclub.com</a>), we partnered with GitHub to provide speedy access to the Student Developer Pack (48 hrs vs. 2-3 weeks). Redeem at <a href=\"https://hack.af/pack\" rel=\"nofollow\">https://hack.af/pack</a>.<p>More on the partnership at <a href=\"https://medium.com/hackclub/github-hack-club-grants-for-your-club-custom-merch-more-f64d6da0d782\" rel=\"nofollow\">https://medium.com/hackclub/github-hack-club-grants-for-your...</a>",
          "Just a heads up:<p>Do NOT use AWS Educate (regardless if you get it from some hackathon or Github).<p>The amount of pain and suffering you have to go through just to get permission to spin up a EC2 instance is unbearable. Compounded with a ton of restrictions and 60-minute sessions, it'll make you unlike AWS real-quick.<p>I would instead suggest to use the $300 GCP credit (since there's no restrictions for that) if you need cloud credits."],
        "story_type":"Normal",
        "url_raw":"https://github.blog/2019-10-30-get-over-100k-worth-of-tools-with-the-github-student-developer-pack/",
        "comments.comment_id":[21404840,
          21405132],
        "comments.comment_author":["zachlatta",
          "sdan"],
        "comments.comment_descendants":[0,
          1],
        "comments.comment_time":["2019-10-31T01:28:13Z",
          "2019-10-31T02:24:53Z"],
        "comments.comment_text":["For those who are part of Hack Club (<a href=\"https://hackclub.com\" rel=\"nofollow\">https://hackclub.com</a>), we partnered with GitHub to provide speedy access to the Student Developer Pack (48 hrs vs. 2-3 weeks). Redeem at <a href=\"https://hack.af/pack\" rel=\"nofollow\">https://hack.af/pack</a>.<p>More on the partnership at <a href=\"https://medium.com/hackclub/github-hack-club-grants-for-your-club-custom-merch-more-f64d6da0d782\" rel=\"nofollow\">https://medium.com/hackclub/github-hack-club-grants-for-your...</a>",
          "Just a heads up:<p>Do NOT use AWS Educate (regardless if you get it from some hackathon or Github).<p>The amount of pain and suffering you have to go through just to get permission to spin up a EC2 instance is unbearable. Compounded with a ton of restrictions and 60-minute sessions, it'll make you unlike AWS real-quick.<p>I would instead suggest to use the $300 GCP credit (since there's no restrictions for that) if you need cloud credits."],
        "id":"95c33fa5-9ca8-4a93-85a1-becc1795b384",
        "url_text":"The GitHub Student Developer Pack now offers over $100k worth of tools and training to every student developer, anywhere that GitHub is available. If youre new to the Pack, it provides verified students with GitHub Pro at no charge while they are in school, plus free access to the best developer tools and trainingthanks to our partnersso they can learn by doing. As the Pack continues to grow and shape the next generation of developers, we continue to listen. Were here to better understand how youre using these tools and whats missing that you hope to see included. Whether youre developing your portfolio, building a new desktop app, or creating an interactive mapthe goal of the Pack is to provide you with the tools you need to be successful. This year, the value of the Pack tripled during the Fall semester by adding nineteen new partners in October plus a dozen in September to the twenty-one who joined our long-time partners for Back-to-School. Lets highlight the partners who joined us since our last post in August. A few words from our new partners We ask all of our new partners what motivated them to join the Pack. Heres a sample of what theyve told us: When I was a student, I actually used the GitHub Student Developer Pack myself! It allowed me to test and learn how to use tools that I wouldnt have been able to otherwise. The Pack was a great help! -Floran Pagliai, Weglot The way that developers learn best is by getting their hands dirty, trying different things, and experimenting with a variety of tools. The Pack allows developers to do that and get exposure creating awesome projects outside the confines of the classroom. -Naeem ul Haq, Educative Not many high school and even college students would ordinarily be able to afford [our tool.] Letting students try it will give us valuable feedback about what up-and-coming developers are looking for. -Levie Rufenacht, Wisej The Pack opens new doors to students that were not accessible before. It basically gives them whats needed to build a project from A to Z, lets them cultivate their curiosity, an essential quality for a developer, and free their creativity. Learning should know no boundaries. -Julien Lehuraux, USE Together Partner details The following are our new partners and the tools and training they are providing, for free, to students: October Appfigures, app store analytics, optimization, and intelligence Astra Security, security suite for your website, including firewall, malware scanner, and a managed bug bounty platform BoltFlare, reliable, secure, and scalable managed WordPress hosting BrowserStack, test your web apps, providing instant access to 2,000+ browsers and real iOS and Android devices Codecov, implement code coverage easier to develop healthier code Educative, level up on trending coding skills at your own pace with interactive, text-based courses EverSQL, boost your database performance by automatically optimizing your SQL queries HazeOver, get focused while working on projects or studying Iconscout, design resources marketplace with high quality icons, illustrations, and stock images Interview Cake, makes coding interviews a piece of cake with practice questions, data structures and algorithms reference pages, cheat sheets, and more Kaltura, build interactive video experiences and advanced media applications MNX.io, managed Cloud hosting for developers NetLicensing, cost-effective and integrated Licensing-as-a-Service (LaaS) solution for your software on any platform from Desktop to IoT and SaaS Scrapinghub, battle-tested cloud platform for running web crawlers where you can manage and automate your web spiders at scale Testmail, unlimited email addresses and mailboxes for automating email tests with powerful APIs Typeform, interactive forms, surveys, and quizzes to engage and grow your audience USE-Together, provides a remote pair programming and team collaboration tool Weglot, make any website multilingual and manage your translations through a single platform Wisej, build powerful web applications in Visual Studio with C# or VB.NET September Blackfire, Code performance measurement tool where you can find and fix bottlenecks Canva, create professional looking graphics and designs, featuring thousands of templates and an easy to use editor Covalence, provides an exclusive developer community and allows you to learn Full Stack web-development with no long-term commitments Crowdin, cloud-based solution that streamlines localization management Education Host, web hosting platform to host assignment and project work GoRails, tutorials for web developers learning Ruby, Rails, Javascript, Turbolinks, Stimulus.js, Vue.js, and more Honeybadger, exception, uptime, and cron monitoring Mailgun, APIs that enable you to send, receive, and track email One Month, learn HTML, CSS, JavaScript and Python in just 30 days Repl.it, an online IDE that lets you instantly code in over fifty languages so you can start learning, building, collaborating, and hosting all in one place Storyscript, top-level, cloud native programming language that helps you orchestrate data flow seamlessly between microservices Vaadin, open source Java framework for building Progressive Web Applications What can I do with the Pack? Looking for ideas for how to use all the new tools? Take a look at these projects or check out GitHub Educations Twitter and Facebook for suggestions. Make a request Have a tool youd like to see included in the Pack? Let the developer know (and tag us) on social media using the hashtag #GitHubPack. Companies interested in including an offer in the Pack can apply to be a partner. Not yet a member of the Pack? Its available for all verified students ages 13+, anywhere in the world where GitHub is available. Join today using your school-issued email, student ID, or other proof of current academic enrollment. Explore the latest offers in the GitHub Student Developer Pack ",
        "_version_":1718938235635236864},
      {
        "story_id":20330324,
        "story_author":"myroon5",
        "story_descendants":39,
        "story_score":225,
        "story_time":"2019-07-01T22:24:01Z",
        "story_title":"Scaling from 2k to 25k engineers on GitHub at Microsoft",
        "search":["Scaling from 2k to 25k engineers on GitHub at Microsoft",
          "Normal",
          "https://jeffwilcox.blog/2019/06/scaling-25k/",
          "At Microsoft today we have almost 25,000 engineers participating in our official GitHub organizations for open source, a great number of them contributing to open source communities throughout GitHub. Its been quite a ride: thats 10X the engineers we were working with when I posted in 2015 about our experience scaling from 20 to 2,000 engineers in the Azure open source org. As a member of Microsofts Open Source Programs Office (OSPO) team, its been exciting to be a part of this growth, and I wanted to take some time to write down some of the investments we have made so that others could get a peek inside. Using data gathered through an open source project Ill mention in this post, I was able to query our GitHub entity data to create this chart of Microsofts public repos created since 2011: Some of our employees have always been on GitHub, contributing to projects, sharing their ideas and insights. Many of our teams are new to social coding and working in the open, and look to OSPO to help provide documentation and guidance. We have engineers who are busy contributing to projects all over GitHub, donating their time and code or entire projects, many I am sure enjoy working with open source in an official capacity, others after hours, or just hacking away. Looking at the contributors to virtual-kubelet, a project thats part of the Cloud Native Computing Foundation, I see familiar names of people Ive worked with. The Visual Studio Code team has been on fire, moving at a fast pace for years. Theres an entire community of awesome people from around the world on GitHub every day opening issues, performing code reviews, and contributing code to VSCode. These teams are finding new ways to communicate, to build consensus and governance models, and to invite maintainers into the fold. In this post, I will cover: Core principles the Open Source Programs Office uses to guide the open source and GitHub experience Technical investments we have made to scale GitHub Program investments Key learnings Looking to the future Resources including the open source projects mentioned in this post Im going to be focusing more on the tactical approach we took to enabling GitHub at scale and not any specific projects experience - though I hope to track down and share the experiences that projects like the Terminal, TypeScript, Accessibility Insights, and the Windows Calculator have had. We work hard to build the right experiences so that engineers have everything they need, without the OSPO team getting in their way. It should be no surprise that open source is a big part of what has helped us to scale, and we continue to give back and contribute where we can: weve adopted CLA Assistant, an open source project started by SAP, and now contribute to the project (we threw away our home-built, aging Contributor License Agreement (CLAs) bot!) were using an open source attribution engine built by Amazon our self-service GitHub management tooling is open source were collaborating on Clearly Defined, an OSI project, to crawl all the open source projects we can find to discover license, copyright, and other data, curating and sharing the resulting data in the open our team has invested in moving to more common open services and systems such as containers and Postgres and MongoDB to make it easier to collaborate with other companies and their preferred tech stacks Looking forward, we have a lot more work to do to focus on developing our capabilities - evolving our maturity models around what healthy and awesome projects look like, helping graduate work into the community and to foundations, and continuing rapid iteration, experiments, and learning from all of this. Im so excited to see where we are at in another few years, and encouraged by the collaboration happening on open source projects across the industry, the developing communities around specific Microsoft technologies, and all the random contributions that Microsoft engineers are making to open source all over GitHub as their teams take dependencies on and get involved in the associated communities. Principles weve adopted To encourage the right behaviors and teach Microsoft employees how to use GitHub and participate in communities, weve identified a number of tenants or principles that we try and use in everything that OSPO does specific to our tooling and approach. For example, we encourage teams to work in the open on GitHub, getting to learn the tools and lingo. Eliminate + Simplify Wed rather remove a complex process to reduce the work our engineers need to do if it is not providing the right return or value. If we need to ask teams questions about the project they want to release as open source, we focus on what problem were trying to solve, and we have been able to eliminate questions that we used to ask, after thinking through the outcomes with stakeholders and advisors. Looking back five years, we used to have a number of manual registration systems where engineers would let us know what open source they use. These were often free-form text fields, and many teams would only take a best-faith effort to share that data. Today weve been able to eliminate many of the registration scenarios by detecting the use of open source in many scenarios across the company, just asking follow-up questions or going through reviews for certain projects when necessary or needing more information. Eliminating process is not always possible, but if we continually ask questions about the workflows and guidance, and ask the teams using these systems to provide feedback and suggestions, hopefully well test the edges and eliminate where possible. Self-service At scale, we cannot be a roadblock, and we trust and encourage our engineers to learn through experience. We want to make sure users learn about GitHub teams, and how to request to join them. There is no possible way that we would have been able to get 25,000 people collaborating on GitHub so quickly without building a self-service experience whereby our engineers could join our GitHub orgs at their pace as they have a need to learn and participate. Traditionally, a GitHub invitation needs to be sent by an org owner to a specific username or e-mail address, and that just would not work well at our scale, without being in the way of our engineers. Thanks to the GitHub API and our GitHub management portal, things are smooth. Whenever possible, we want to provide documents, guidance, and other reusable resources, instead of having to rely on special knowledge or process that has manual steps. Delegation Our office provides guidance and resources to help advise Microsoft businesses in their approach to contributing to, using, and releasing open source, but we leave the business decisions with the particular business. Decisions such as whether to release open source go through a question/answer experience, and the outcome is either automatic approval, or a business approval workflow that kicks off, allowing a teams business and legal representatives make the final decisions and choices. We also have begun deputizing open source champs: people who can help spread the good word and provide opinions and advice to their teams about how to think about open source. Transparency Open source centers around collaboration, but one of the challenges today with GitHub orgs full of many repos is identifying what teams have access to accept pull requests or administer settings. While GitHub shows a lot of this data if you start with the Teams view in an org and drill into a specific team, theres no view for a given repos teams, unless youre an admin for that repo. For all of our repos, releases and reviews, Our portal exposes the given teams that control each repo All the release requests and data are stored in work items available to any employee Our portal for GitHub management shows all GitHub Teams, including secret or hidden teams We enable cross-org search in our portal, to more easily locate similarly named teams and repos across orgs, reducing confusion and internal support costs Thanks to this, out of the 1000s of repos, its relatively painless to find the right contacts for a project inside the company. Authentic GitHub Engineers should learn how the GitHub interface works, how pull requests and reviews, issues, forks, organization teams, and collaborators function. Whenever possible, we hope that our users go directly to GitHub to manage their teams, configure team maintainers, welcome new community members into their projects as maintainers or contributors. While we do have separate internal interfaces and tools, these ideally augment the native experience. Weve built a browser extension that our users can install to help light up GitHub to make it easier to find employees, resources, or internal tools. Its important to us that engineers learn the GitHub fork and pull request model for contributing, as we strive to use a similar experience for inner source work. Technical investments Our engineering team has made many technical investments to help scale the company to be able to participate in open source better. Whenever possible we want to use open source to make open source better and share those learnings and contributions for other companies and individuals to use. Well invest in our own tooling when we must, but look to the marketplace and work that others are doing on the horizon: we are excited to see GitHubs Enterprise Cloud product evolve and drive new features and capabilities into the product to make enterprise-scale open source easier. Adopting CLA Assistant Today we host an instance of an open source project for Contributor License Agreement (CLA) management, integrated with GitHub, called CLA Assistant. This allows us to make sure that people contributing to our projects have signed the appropriate CLA before we accept a contribution. Once the CLA for an entity is signed once by a community member, they can then contribute to all other repos, so its really a relatively low burden that helps keep our lawyer friends happy. CLA Assistant is an open source project that was started by SAP and is licensed as Apache 2.0. Weve contributed to the project functionality to help with scale issues and rate limiting, automatic signing to handle when employees join and leave the company, and are excited to be able to support a new class of corporate CLAs thanks to contributions made by others to that project. In 2017 we migrated to this new open solution from the in-house CLA bot that we abandoned. I really like how the VSCode team messaged this to their community with a GitHub issue (#34239). The system is powered by GitHub org-level webhooks, so it is always on and teams do not need to worry about whether their repos are protected with the CLA and ready for contribution. Data and Insights GitHub provides useful information including traffic stats, contributor info and other breakdowns at the repository level. Across Microsofts open source projects, however, we have a need to be able to slice and dice data looking for trends over time, analyzing investments at scale, and so realized early on that we needed to import all of the available data we can from our GitHub open source releases into our own big data systems such as Azure Data Lake and Azure Data Explorer. Newer GitHub Enterprise Cloud features look to help provide organization insights, and were super excited to give those a try to augment the other data and insight methods we are using today. Here are some of the projects weve used, created, or collaborated on. GHTorrent Our team is one of the sponsors of the GHTorrent Project, an effort to create a scalable, queryable, offline mirror of data offered through the GitHub REST API. The repo is at github.com/gousiosg/github-mirror. This data, similar to GHArchive, helps learn about the broad collaboration happening on GitHub. We donate cloud compute resources to the project run by Georgios Gousios who kicked off the project with ideas and collaboration with Diomidis Spinellis. At Microsoft, we ingest the data into Azure Data Lake to be able to run interesting queries and learn more about trends and happenings beyond our campus. GHCrawler GHCrawler is a robust GitHub API crawler that walks a queue of GitHub entities transitively, retrieving and storing their contents. The initial project launched in 2016 and evolved significantly at the TODO Group tools hackathon in June 2017 while working with other companies to abstract the data stores to support other technologies and stacks. The crawler taught us a lot about the GitHub API and how to be friendly citizens by focusing on the caching of entities, the use of e-tags, and being careful to not repeatedly fetch the same resource. Different from GHTorrent, the crawler is able to traverse Microsofts own open source organizations using tokens that can peer into our current GitHub team memberships, retrieve info about maintainers, configuration, and private repos, and so is very useful in answering operational questions about the growth of GitHub at Microsoft. We ingest the data from GHCrawler into both Azure Data Lake Azure Data Explorer and use that data in Power BI dashboards and reports, live data display on internal sites, and other resources. The chart of new repos at the top of this post was created by querying this crawler data as stored in Azure Data Explorer. Heres a query that returns repos created in the official Microsoft organization in the last 30 days (that are public): Since the data comes from GitHub but is stored internally, teams can make use of the data for their own business needs and interests, without having to worry about GitHub API rate limiting, the specifics around collecting that data, and just being able to focus on using the data effectively to solve business problems. A favorite resource for many teams looking to build new communities is the data around pull requests and issues, and also collected traffic API data. Since GitHub only provides a 2-week window of consolidated traffic info, storing the data in our big data tools helps us look at trends more easily. ClearlyDefined ClearlyDefined is an Open Source Initiative (OSI) project that has a mission to help FOSS projects thrive by being clearly defined. The lack of clarity around licenses reduces engagement that ends up meaning fewer users, fewer contributors, and a smaller community. Communities choose a license for their project with the terms that they like. Defining and knowing the license for an open source project is essential to a successful community partnership, and so ClearlyDefined helps clarify that by identifying key data such as license set, attribution parties, code location, and when the data is missing, curation can help. Microsoft contributes to the effort and is making use of the data to help provide license clarity in the tooling used to help teams understand their use of open source. As of June 2019, ClearlyDefined has over 6.3 million definitions. These definitions are created by running tools such as licensee, scancode-toolkit, and fossology tools across oodles of open source. repolinter Another TODO Group project OSPO collaborates on is repolinter. Given a source repository, you can run repolinter to learn basic information that can help inform whether a project has all that is necessary to incubate and build a healthy community, such as license files README files Code of Conduct, governance or contribution information No binaries Licenses detectable by licensee Source headers have license information We hope to be able to share this data in a more visible way to help teams see where they can make simple improvements, or even by automatically opening pull requests if they are missing the essentials such as a mention of the Code of Conduct. oss-attribution-builder Weve collaborated with Amazon and use their amzn/oss-attribution-builder project to help build open source license notice files to help teams meet their legal obligations. GitHub management portal Microsofts GitHub management portal that employees to use, detailed in my 2015 post on the portal, handles: self-service org joining for employees cross-organization search of people, repos, and teams support jobs to maintain data generating digests and reports caching GitHub REST API responses processing web hook events Here you can see an experience where you can search, sort and filter repos across all of the many GitHub orgs in one place, improving discoverability. The portal itself is open source and has been rebuilt and refactored over the past few years to try and be more useful and to allow other companies to use it: Node service and site, now implemented in TypeScript Backing stores supported include Postgres and others backed by interfaces An out-of-the-box run local in memory experience is now available The repo is on GitHub at microsoft/opensource-portal. New repo wizard While we have a very liberal and friendly policy to make it easy for an engineering team to decide to share some code such as a sample, we do want to make sure that theres a process in place that includes a business decision and legal team approval if theres a major release being considered. Microsoft has chosen to turn off the Allow Members to Create Repositories option on our open source GitHub organizations. Instead, we have our own new repository wizard within our GitHub management tooling. This site collects information about the purpose of the repo, the common GitHub Team permissions that will help a group of engineers get off and running, and then populates the repo with the standard Microsoft open source content such as Code of Conduct info in the README, an appropriate LICENSE file for the project, and the original purpose and use of the repo, for data reporting purposes internally. To make this experience easier to find, we work to educate people through documentation and tools such as the previously mentioned Open Source Assistant web browser extension. The extension is able to light up the green new repo button on our orgs and link directly to the wizard, providing a better user experience than the you have insufficient permission to create a new repository message that our users would receive on GitHub otherwise. The outcome of the repo wizard will be the creation of a public or private repo on GitHub. We ask, by policy, that teams only use our open source GitHub orgs for work they are going to ship within the next 30 days. Release reviews The outcome of the new repo wizard is either auto-approval to make a project public, or the kick-off of a business and legal review process, to help comply with policy, and inform leaders about important open source decisions that are being made. We have an internal service called the Usage Service that creates a work item in our work item tracking system (Azure Boards), and assigns that work item to the appropriate legal and business reviewer. The work item is then passed along, with fields or more info being filled out, discussions are had, and eventually the review will be final approved and ready for teams to ship their project to the world and go build that open community. Here is a screenshot, redacted of the details and names, showing the work item for the release approval of the Windows Calculator: After the final approval, an automated e-mail is sent to the original requester with any guidance from the various reviewers involved in the process, to help them understand any notifications, legal obligations, or other work that needs to be done to support their release. Open Source Assistant Browser Extension Internally we ship a browser extension that lights up GitHub with Microsoft-specific information and guidance. Since a GitHub username may not be related to a persons recognized corporate identity, the extension lights up the more common corporate alias inline throughout GitHub - pull requests, issues, people profiles. By highlighting other Microsoft employees on GitHub, people who use the extension are able to dedicate additional focus on a good collaboration experience with the community, while easily being able to better identify their coworkers on the site. The extension helps people to continue to use the native GitHub experience, while augmenting bits and pieces where weve made decisions around our GitHub org settings, such as disabling New Repo creation direct on GitHub. The extension also is also to provide links to policy around open source, link to the new repository wizard. Since we disable native repo creation on GitHub in order to ask engineers to complete our wizard to learn more about their intent with their open source release, we often would get support questions about how to create repos, since GitHub would display the message that the user could not create repos. Now, this is what they see: In another sceenshot, you can see a Join button that lets an employee self-service join the GitHub organization through our GitHub management portal: Finally, since we want teams to use our official GitHub org that is configured automatically with the CLA system, with compliance tools, audit logs, and the ability to manage the lifecycle of when users join and leave, we do not allow teams to create new GitHub orgs for open source. By updating GitHubs new organization page, we have a chance to let people know about the policy and other resources before they get too far down the road of trying to setup yet-another-org. The extension works Firefox, Chrome, and the new Edge based on Chromium. Microsoft employees can learn more and install the extension at aka.ms/1esassistant. While this extenson is not open source today, if others think it could help their organization, I can see what we can do. Regular automation jobs Weve implemented jobs that do everything from keeping data and caches up-to-date to helping make sure that we have confidence in the membership of our GitHub organizations. These jobs are all open source, implemented in TypeScript, as a part of the opensource-portal repo. Removing former employees Connected to HR data sources, we can react when an employee leaves the company. In a big company, people are always changing roles, trying new things, and sometimes they leave for other opportunities. We unlink people when they leave: removing the association between their GitHub account and their corporate identity, and removing their GitHub organization membership(s). We welcome former employees to continue to contribute and participate to projects where they should continue to be maintainers, but for clarity, do not allow them to remain as org members. We also send an e-mail to their former manager, so they have confidence that the right things have happened with permissions and system access. Enforced system teams To help enable operations, support, meet security obligations, and light up bots such as our CLA system, we have a set of GitHub teams that are automatically and permanently added to every repository in our GitHub orgs, when the job is configured. By monitoring the GitHub event bus, the job adds what we call system teams to a repo the instant a new repo is created. And, if a repo admin tries to remove or change the permissions that such a system team has, the job automatically restores its permission. An occassional cronjob also runs to validate and reenforce these teams as needed. Preventing big permission grants As the number of members in a GitHub organization grows high, youll also start to see very large teams. We have teams such as All Employee Members that are designed to give easy read access to private repos that are being finished up and polished before going open source. An unintended side effect we found: some people wanted to just give all members administrative access over their repo, to make the act of onboarding get much easier. Trouble is, with admin comes great power, and any of the thousands of members of the org could then delete the repo, change whatever they want, override branch protections, etc. This also would automatically subscribe thousands of people to spammy notification messages, depending on their GitHub notification preferences. As a result, we have a large permissions job that monitors for grants of write or admin access to an org-defined large number of people. Once such a grant is made, and the team is too large, the job will automatically downgrade the permissions to try and prevent mistakes, and then send an e-mail address to the person who made the permission choice, hoping to educate them about the risks and help them understand what changes were made. GitHub username changes GitHub allows users to change their username. This is excellent: many people start with a cute screen name or other indicator, then realize theyre professionally developing software with their peers in the industry, and often want to change that username. Not all users know that they can rename their account, so we do get people who delete an account, just to create a new one. However, the majority of GitHub apps and APIs work off of GitHub usernames instead of the integer-based user ID. To help stay ahead of this, we have a cronjob that looks up GitHub users by ID and then refreshes our internal data store of those usernames, in the hope that we can improve the API success rate and reduce support costs when renames happen. For critical operations, additional logic has been added to GitHub apps to anticipate the potential of a user rename happening, and being able to smartly attempt to fallback to looking up the user first by ID, or alerting and sending the issue to operational support. Weekly and daily digests We have a cronjob that sends updates about new changes and interesting happenings on GitHub repos. These are typically only send to repo admins, to not spam too many people. The digests are personalized to the receipient, and cover scenarios such as: notifying the creator of a repo, and their manager, of the new repo notifying when the # of admins becomes rather high, so that team maintainers can cleanup permissions abandoned repos that havent been touched in years when a team is down to a single maintainer, suggesting they appoint another maintainer or two when a repository has been flipped from private to public Program investments More important than the technical tools and hacks we put in place, the programs that we run in the Open Source Programs Office help drive process improvements, awareness of features and capabilities, roll out new functionality, and to emphasis the work that is happening in the open in new ways. Heres a set of the OSPO investments we have been making or put in place to help drive improvement in the open source maturity level of the company. Docs We have a central documentation repository that is easily accessible to employees where they can read through guides on how to go about releasing open source, or contributing to it. The hope is that teams new to many of the tools and approaches in open source can refer to the material, share it with others, and help us to scale this to others. Heres a rough look at the outline of some of that material. In time I hope we can prepare a version of these docs to share with the world - General Information - What exactly is Open Source? - When and why Open Source - Can I use it? - Misconceptions - Bad Practices - Benefits - Use - Overview - Security and vulnerability management - Copyleft obligations - Code signing - Attribution guidelines - Business review process - Required notice generation - Automated registration of use - Contribute - Contribution overview - Forking repositories - Business review, if required - Release - Release overview - Checklist - Copyright headers - Foster your community - Build and publish your project - Code signing - Business review, if required - Data and Insights - GitHub insights - Registrations and request insights - GitHub information - GitHub overview - Accounts and linking - Repositories - Teams - Organizations - Transferring and migrating repos - Service accounts - Two-factor authentication - Apps, services and integrations - Large files, LFS Monthly newsletter The team prepares a monthly newsletter to all interested parties in the company, including all linked GitHub users, with a distribution north of 25,000 members. As a low-frequency resource, the hope is that the newsletter helps provide useful information without inundating teams. Typical newsletters will include 3-5 topics and updates of note, some data or graphs and charts of adoption and community health, and the date for the next open source meetup. Open Source Meetup A monthly gathering on the Microsoft campus (or online), the meetup is a quick event that starts off with networking, a quick lunch, and then 3-4 speakers who share their recent open source experiences. This helps connect people, have interesting conversations, and build community. Open Source Champs A cross-company group of people who get it, these are some of the best open source minds at Microsoft, who have connections, experience, data, and can help drive best practices and advise on situations. The champs primarily get involved in a specific discussion list in the company today, but hopefully in time they will also be able to help share information with their organizations, and build a two-way conversation channel with more stakeholders and businesses across the company. The champs will be a key part of helping us continue to scale and share and get subject matter experts connected with the right teams as they open up. Internal support OSPO offers an internal support experience, with internal mail, a Teams channel, and other places, to help support people as needed. The most common issues tend to be helping people find the documentation for what they are looking to do, pointing them at the GitHub account linking experience or new repo wizard, or helping answer policy clarifications and questions. Not all questions can be answered by OSPO - we do at times need people to reach out to their legal contact for their organization to support their request. It is also super important to us that we update internal docs and tools when we learn of gaps or improvements that can be made, so that the next group of people with the same need will not need an escalation or support incident to help answer their question. We look to our principles around delegation, self-service, and elimination to help reduce support cost, solving problems before they grow too large. A clever hack or targeted fix often helps a lot. Executive council and briefings Appointed executives from across Microsofts businesses make up the majority of the OSS Exec Council that meets at least quarterly to discuss progress on evolving the culture, being available to advise how their groups are contributing to open source, and otherwise helping provide a conduit with leadership, the open source champs, and our open source office. In many ways, this group is the board of directors of our OSPO team and we look forward to helping to tackle whatever is next as a company in this space. Having this in place helps us take the temperature of leadership of where to spend our effort and how to think about complicated issues and how to make things friction-free for our engineering teams who want to contribute to and use open source everywhere. Were also available to brief business leaders and others on their approach, share the learnings others have had, and strive to remove roadblocks with all teams. Learnings Weve learned a lot, and continue to learn as we scale the open source adoption across the company. Many of the specifics learnings Ive had relate to how we operate at scale, how much we can set teams free with tools and access, and how to help others learn from the experiences. Just a few short stories Our GitHub Bedlam moment GitHub is super real: we finally had our own Bedlam incident earlier this year we had a GitHub team inside one of our largest GitHub orgs called All Members, to make it easy to quickly give access to private repos ahead of the launch of new work at events. The idea is that as long as you give all members read access to the repo, they can then fork and submit pull requests, making it easier than trying to figure out what teams may grant them the rights they need. It also encourages the more open contribution model we love. GitHub Team Discussions shipped in late 2017 and essentially were not very frequently used by our open source teams for quite a while. Finally, one quiet day in January, it finally happened someone posted a message to the all employees in this org team discussion, immediately going out to a lot of people and then someone posted about how to update your notification settings and then it spiraled out from there. Along the way we learned a few things: People learned about GitHub discussions We may have found a few minor bugs in the notification settings on GitHub that were corrected We realized there are downsides to massive teams We had some fun, too Old time Microsoft people smiled a lot. Bedlam was a classic Microsoft learning lesson. Its fun that we can still have similar experiences with the modern developer toolkit, too Two-factor authentication A surprising amount of our internal support traffic comes from employees who unfortunately lose access to their GitHub accounts. Since we require our users to use GitHubs two-factor authentication, and this is a separate two-factor system than the Azure Active Directory multi-factor auth that our employees also use, its easy for people to get confused, or to lose access to the GitHub side of things. Thankfully, GitHub does a great job of helping remind people to save their two-factor authentication codes and to encourage other technology such as the use of U2F devices. Everytime a new iPhone model comes out, our support volume spikes a lot of people use two-factor authentication apps on their phone, but then forget to store their backup codes or anticipate that wiping their phone and upgrading to a new one may not always restore what they think it may. I strongly recommend that everyone have a YubiKey to help make things easier when using GitHub on a daily basis. GitHub API: user renames Most GitHub REST v3 API calls operate on GitHub username as opposed to user IDs. GitHub allows users to change their username, which means that if you are not also validating what their current username is, you could find that a few API calls are not working for a particular set of users who have changed their usernames. There is a GitHub API that can take a GitHub user ID and provide you the current user information response, so you can hit that occassionally, such as in a daily job which also respects e-tags and caches responses, to make sure you have the accurate username before calling operational APIs. At scale, we tend to see about 25 user renames per week at Microsoft right now. GitHub API: conditional requests The GitHub APIs conditional request guidance is good, but many libraries or users of their APIs do not seem to use them by default. As long as you keep a cached version of the entities you request, you can use the e-tag and an If-None-Match header to help reduce your load on the rate limits for GitHub. A request with the existing e-tag, to check if the entity has changed, will not count against your rolling API limit if it has not changed. Its great seeing more and more libraries such as octokit/rest.js supporting this concept in various ways, or having extensibility for it. Be nice to GitHubs API. Team vs Individual repo permissions Whenever possible we strongly encourage that the repositories that Microsoft governs use team permissions instead of granting individual permissions. GitHub Teams support multiple team maintainers, helping projects evolve over time, and making it easier to keep access and admin permission assigned as needed. Weve had to do a lot of user education on this topic: if someone opens a support ticket needing access permissions, we will never grant them individual permissions (Collaborator on GitHub) to a repo, but instead will ask them to help us find or create a new Team on GitHub for that permission assignment to go to. ** Team permissions. Teams with maintainers. Avoid individual permissions.** This helps keep things sane. Very sane. Transparency around permissions As previously mentioned, our GitHub management portal shows all employees all the permissions for teams, including secret teams. Since we are focused on enabling open source on GitHub, this helps answer questions people have about who has access to administer or change settings on a repo and reduces support volume. On GitHub you can natively see, given a team, which repos and permissions it has, if you are a member of the org. However, you cannot see easily from a given repo who the teams are. Transparent data in the portal reduces support costs around the 404 these arent the droids you are looking for experience on GitHub: since GitHub will return a 404 for a repo that an org member does not have access to, we used to get a decent amount of support traffic from people asking if the URL they were sent by a team member was real or not. Paying GitHub Before we spent a few billion dollars on GitHub, we had paid GitHub organization accounts at various pricing tiers for our open source GitHub orgs: we needed private repo access for people getting ready to release their open source. Early on we would have a bunch of people running around with corporate cards and expensing GitHub. Over the years we were able to consolidate this spend and bring sanity as the GitHub use scaled, and also to help use that as a forcing function: by centrally funding the official GitHub organizations for open source in the OSPO team, we were able to make it easy for teams to get going without having to worry about billing or setting up new organizations and systems. GitHub offered a really useful annual invoice payment method, so instead of having to worry about using a corporate card or other payment method each month, we would just submit a single payment for a set of GitHub orgs, then process that for payment. GitHub sales was super helpful and friendly. The only minor issue weve had with this is the few times that we had to make changes to the LFS data packs for an org during the year we would solve this by approving the purchase order for GitHub to include some additional buffer, so that GitHub sales could just invoice us for the additional data packs as needed. Before we used the invoice payment method, we did use corporate credit cards as needed and approved by our finance team; it was nice that we could associate a Billing Manager or managers to the orgs to help manage that without having to worry about permission to be org owners or control the resource itself. Kudos to the friendly GitHub sales and their supporting staff - they were always willing to jump on a Zoom video conference call and work through the details, such as when we were signing the Corporate Terms of Service agreement for some of our orgs. GitHubs annual invoice payment option is really straightforward and can help you reduce random one-off corporate card expenses. Org proliferation Many Microsoft teams love to ship the org chart, but when it comes to reinforcing Microsofts open source releases, we prefer all repos to go in our few main organizations. This also helps make it easier for engineers to get going: they do not need to identify both an org and a team to work on a repo, they just need to know the repo name or team details. By policy and reinforced in our tooling and other systems, we have asked people to just use the official Microsoft GitHub org since early 2017. This has been super important by providing a really straightforward way for people to get access and not worry about cross-org permissions or finding things. We also know that product names and teams change so often that these things just would not scale. While the GitHub rename features provide some flexibility to orgs and repos, we would prefer not to change too many things at once. Newer GitHub features such as an enterprise account announced in May sound really useful to help address this in the future: if you can combine compliance and billing together, perhaps it will be easier to have additional organizations where it makes sense. Build a strategy around how many GitHub organizations your organization will have for open source. The answer might be 1. Human challenges Were continually learning from people! People are great. Products vs Projects Microsoft is super crisp on the requirements to ship products and services: the Microsoft Security Development Lifecycle (SDL) helps team to be mindful of security and how to build and ship software. As a company, our products also have requirements around code signing, packaging, localization, globalization, servicing, accessibility, etc. Shipping an open source code project (a repo) is a little less involved, but teams still need to do the right thing. Our policies around releasing source code are more about business purpose and approval, making sure governance information and LICENSE files and READMEs are in place, and that teams are ready to support, build and evolve a friendly open community. Weve had a few learning situations where teams thought that the full set of requirements to release a product - such as localizing in many languages - were required to release an open source repo. For now, we emphasize to teams that projects are not products, but often, they compliment one another, and sometimes, they literally are the same thing. Products and services are very different from open source code. Thats OK. Tell your friends. Forking guidance Forking is a big deal in the open source world. There are times where a fork is the right evolution or move for a community to evolve, but weve found that some teams view forks as the way to contribute upstream. For example: if someone wants to contribute to CLA Assistant, one approach would be to fork the repo to their individual GitHub account, and then to submit a pull request to that upstream project. Another approach would be to fork the project to the official Microsoft organization, prepare changes, and then submit it as a pull request. While the second example makes it clear that this is very much a Microsoft contribution, it creates confusion, because Microsoft just wants to participate in the upstream project, and not fork it in any hard way. We strongly encourage teams to simply fork and submit pull requests, the GitHub way, from their individual accounts, not from the official organization account. We want teams to always contribute to the upstream when possible and only fork as a last resort. Forking can be a big deal. Upstream is the right place to contribute. Support volume (e-mail) Since we set the e-mail address opensource at microsoft as the e-mail address associated with the official Microsoft organization on GitHub, we get a lot of e-mail traffic from people looking for help with issues and products. Within GitHub, if youre browsing a repo and click Contact GitHub in the footer of the web page, it essentially asks whether you are looking to report a GitHub issue or an issue with the repo. This helps reduce issue/support traffic to GitHub for open source repos. GitHub then offers the e-mail address associated with the SUPPORT files for the repo, or falls back to the org-wide e-mail address. So we get a lot of e-mail. Weve learned to improve spam filters and use templates and work to address issues, but we do get a lot of mail. Expect that support will need to happen. People want more open source from us! On the entertaining side, many passionate users of long-time Microsoft software regularly write in to ask us to open source their favorite projects Flight Simulator Microsoft Money Age of Empires our operating system Its great that people really want to see this, but I think a lot of folks do not always understand that commercial software often has many dependencies or licensed components that may not be open source, or the code requires a very specialized build environment, etc. I do love seeing the historical releases to share code, such as the original winfile.exe or old-school MS-DOS 1.25 and 2.0! Hopefully teams will find the time to share when it makes sense. Its also a reality that historical software releases are not a place that will be easy to build an active collaborative community around unless a clean mechanism exists to release and build modern bits. Open source all the things Exciting new GitHub features As outlined in Build like an open source community with GitHub Enterprise, Mario Rodriguez from GitHub highlights how the features that help make GitHub great for open source can also be super useful for any organization. Weve started moving to GitHub Enterprise Cloud for more of our open source organizations and Im excited to see what our engineering teams will do with these new capabilities. A few capabilities in particular that were starting to use include: Org insights Available in beta now for Enterprise Cloud accounts, you can take a look at the high-level trends around new issues, closed issues, and other key specs and data points for all of your repositories across the org. Another nice view is the where members are getting work done look at where time is being spent on GitHub by everyone contributing to the org. Maintainer roles Especially interesting to teams like .NET which have a large amount of activity and maintainers, new roles beyond the classic read/write/admin on GitHub means that they can appoint Triage users (can manage issues and PRs, but not push code directly to the repo) or Maintain users (additional settings on top of issues and PR management). Transferring issues Moving issues between repos used to be done by third-party, unofficial, or random tooling, and now that GitHub directly supports issue movement, were pretty happy to see this additional option open for communities who have issues spanning multiple repos. Audit logs GitHub has an audit log API beta for Enterprise Cloud customers now. Using GraphQL, this means that we will be able to keep an audit log copy in our systems for review, analysis, and without the manual or cumbersome approach required today to either ingest and store webhook information in an append-only data store, and/or parse the JSON and CSV versions of the audit log for a GitHub organization. Looking forward We have a lot left to do and our journey will continue to get more interesting as Microsoft continues its open source adventure. Some of the things the Open Source Programs Office will be thinking through will include As a company we will continue to work to evolve our take on a maturity model for open source organizations, and I cant wait to see what is next. Sharing guidance and guides Similar to the Open Source Guides at //opensource.guide created by GitHub, wed love to share, to help others in the industry learn from our progress. Playbooks around how to think about open source, making business decisions related to open source, all are fun topics we would love to share our learnings and take on. We may also be able to share our docs, or a version of them, with the world. Major kudos to Google who already shares their open source guidance and policies on their site. Identifying contribution opportunities We are starting to look at ways to draw attention to contribution opportunities, such as highlighting up-for-grabs issues across our releases, and also recognizing when our employees contribute to open projects outside control of the company, too. By updating the opensource.microsoft.com site, we hope to be able to tell good stories and share useful information about what our teams are up to. As a short-term experiment, we are listing up for grabs issues right on the homepage of our open source site, to learn about whether people find that interesting. Open Source Resources Here are open source GitHub repos mentioned in this post. Check them out! cla-assistant/cla-assistant amzn/oss-attribution-builder clearlydefined/crawler todogroup/repolinter clearlydefined/curated-data microsoft/opensource-portal microsoft/ghcrawler fossology/fossology nexB/scancode-toolkit licensee/licensee octokit/rest.js Hope you found this interesting, let me know what you think on Twitter. If you were expecting a short or concise post, you may have clicked on the wrong thing Jeff Wilcox Principal Software Engineer Microsoft Open Source Programs Office ",
          "The title makes it sound like the number of employees at Github scaled to 25k. The actual meaning though is that the number of Github contributers / participants at Microsoft increased to 25k.<p>Quote:\n\"At Microsoft today we have almost 25,000 engineers participating in our official GitHub organizations for open source, a great number of them contributing to open source communities throughout GitHub.\"<p>Quite misleading IMO.",
          "Nice to see that MS is very serious about open source. This is a huge change from back when Steve Balmer was still in charge and open source was a dirty word.<p>Employing this many people to work on open source costs billions. I'm sure not all of those people are doing this full time. But still, this represents an enormous investment. That just goes to show how incredibly valuable OSS is these days. That's similar to the net worth of some open core companies that have been debated on HN recently (Mongo, Elastic, Redis, etc.).<p>MS is getting plenty of return on investment. For example a lot of their development tooling is getting significant external contributions. Co-developing software with externals makes economic sense when the primary function of that software is to help people find their way to your for profit services and software. This reduces their cost, makes the software more valuable, and grows the user base of the software. More users means more opportunity for these people to find their way to for profit MS stuff. Even as a recruiting tool this is super valuable since no doubt many external contributors are on the radar for hiring.<p>Just having a lot of developers give up their mac books and instead choosing to run windows again increases the chance that they might end up in Azure, which as we learned recently mostly runs Linux stuff these days. MS is relearning that staying on friendly terms with developers is important for them. This is something that e.g. Google or Apple may want to consider. They do lots of OSS as well of course but they seem a lot more focused on internals lately and some of what they do seems a bit hostile even. Especially Google is starting to act a lot like MS used to act.<p>Maybe buying Gitlab would not be a bad idea for them ..."],
        "story_type":"Normal",
        "url_raw":"https://jeffwilcox.blog/2019/06/scaling-25k/",
        "comments.comment_id":[20331878,
          20332519],
        "comments.comment_author":["cshg",
          "jillesvangurp"],
        "comments.comment_descendants":[5,
          1],
        "comments.comment_time":["2019-07-02T03:22:11Z",
          "2019-07-02T06:00:32Z"],
        "comments.comment_text":["The title makes it sound like the number of employees at Github scaled to 25k. The actual meaning though is that the number of Github contributers / participants at Microsoft increased to 25k.<p>Quote:\n\"At Microsoft today we have almost 25,000 engineers participating in our official GitHub organizations for open source, a great number of them contributing to open source communities throughout GitHub.\"<p>Quite misleading IMO.",
          "Nice to see that MS is very serious about open source. This is a huge change from back when Steve Balmer was still in charge and open source was a dirty word.<p>Employing this many people to work on open source costs billions. I'm sure not all of those people are doing this full time. But still, this represents an enormous investment. That just goes to show how incredibly valuable OSS is these days. That's similar to the net worth of some open core companies that have been debated on HN recently (Mongo, Elastic, Redis, etc.).<p>MS is getting plenty of return on investment. For example a lot of their development tooling is getting significant external contributions. Co-developing software with externals makes economic sense when the primary function of that software is to help people find their way to your for profit services and software. This reduces their cost, makes the software more valuable, and grows the user base of the software. More users means more opportunity for these people to find their way to for profit MS stuff. Even as a recruiting tool this is super valuable since no doubt many external contributors are on the radar for hiring.<p>Just having a lot of developers give up their mac books and instead choosing to run windows again increases the chance that they might end up in Azure, which as we learned recently mostly runs Linux stuff these days. MS is relearning that staying on friendly terms with developers is important for them. This is something that e.g. Google or Apple may want to consider. They do lots of OSS as well of course but they seem a lot more focused on internals lately and some of what they do seems a bit hostile even. Especially Google is starting to act a lot like MS used to act.<p>Maybe buying Gitlab would not be a bad idea for them ..."],
        "id":"c4bb41f3-9542-43b1-9d27-7e0bad1d297c",
        "url_text":"At Microsoft today we have almost 25,000 engineers participating in our official GitHub organizations for open source, a great number of them contributing to open source communities throughout GitHub. Its been quite a ride: thats 10X the engineers we were working with when I posted in 2015 about our experience scaling from 20 to 2,000 engineers in the Azure open source org. As a member of Microsofts Open Source Programs Office (OSPO) team, its been exciting to be a part of this growth, and I wanted to take some time to write down some of the investments we have made so that others could get a peek inside. Using data gathered through an open source project Ill mention in this post, I was able to query our GitHub entity data to create this chart of Microsofts public repos created since 2011: Some of our employees have always been on GitHub, contributing to projects, sharing their ideas and insights. Many of our teams are new to social coding and working in the open, and look to OSPO to help provide documentation and guidance. We have engineers who are busy contributing to projects all over GitHub, donating their time and code or entire projects, many I am sure enjoy working with open source in an official capacity, others after hours, or just hacking away. Looking at the contributors to virtual-kubelet, a project thats part of the Cloud Native Computing Foundation, I see familiar names of people Ive worked with. The Visual Studio Code team has been on fire, moving at a fast pace for years. Theres an entire community of awesome people from around the world on GitHub every day opening issues, performing code reviews, and contributing code to VSCode. These teams are finding new ways to communicate, to build consensus and governance models, and to invite maintainers into the fold. In this post, I will cover: Core principles the Open Source Programs Office uses to guide the open source and GitHub experience Technical investments we have made to scale GitHub Program investments Key learnings Looking to the future Resources including the open source projects mentioned in this post Im going to be focusing more on the tactical approach we took to enabling GitHub at scale and not any specific projects experience - though I hope to track down and share the experiences that projects like the Terminal, TypeScript, Accessibility Insights, and the Windows Calculator have had. We work hard to build the right experiences so that engineers have everything they need, without the OSPO team getting in their way. It should be no surprise that open source is a big part of what has helped us to scale, and we continue to give back and contribute where we can: weve adopted CLA Assistant, an open source project started by SAP, and now contribute to the project (we threw away our home-built, aging Contributor License Agreement (CLAs) bot!) were using an open source attribution engine built by Amazon our self-service GitHub management tooling is open source were collaborating on Clearly Defined, an OSI project, to crawl all the open source projects we can find to discover license, copyright, and other data, curating and sharing the resulting data in the open our team has invested in moving to more common open services and systems such as containers and Postgres and MongoDB to make it easier to collaborate with other companies and their preferred tech stacks Looking forward, we have a lot more work to do to focus on developing our capabilities - evolving our maturity models around what healthy and awesome projects look like, helping graduate work into the community and to foundations, and continuing rapid iteration, experiments, and learning from all of this. Im so excited to see where we are at in another few years, and encouraged by the collaboration happening on open source projects across the industry, the developing communities around specific Microsoft technologies, and all the random contributions that Microsoft engineers are making to open source all over GitHub as their teams take dependencies on and get involved in the associated communities. Principles weve adopted To encourage the right behaviors and teach Microsoft employees how to use GitHub and participate in communities, weve identified a number of tenants or principles that we try and use in everything that OSPO does specific to our tooling and approach. For example, we encourage teams to work in the open on GitHub, getting to learn the tools and lingo. Eliminate + Simplify Wed rather remove a complex process to reduce the work our engineers need to do if it is not providing the right return or value. If we need to ask teams questions about the project they want to release as open source, we focus on what problem were trying to solve, and we have been able to eliminate questions that we used to ask, after thinking through the outcomes with stakeholders and advisors. Looking back five years, we used to have a number of manual registration systems where engineers would let us know what open source they use. These were often free-form text fields, and many teams would only take a best-faith effort to share that data. Today weve been able to eliminate many of the registration scenarios by detecting the use of open source in many scenarios across the company, just asking follow-up questions or going through reviews for certain projects when necessary or needing more information. Eliminating process is not always possible, but if we continually ask questions about the workflows and guidance, and ask the teams using these systems to provide feedback and suggestions, hopefully well test the edges and eliminate where possible. Self-service At scale, we cannot be a roadblock, and we trust and encourage our engineers to learn through experience. We want to make sure users learn about GitHub teams, and how to request to join them. There is no possible way that we would have been able to get 25,000 people collaborating on GitHub so quickly without building a self-service experience whereby our engineers could join our GitHub orgs at their pace as they have a need to learn and participate. Traditionally, a GitHub invitation needs to be sent by an org owner to a specific username or e-mail address, and that just would not work well at our scale, without being in the way of our engineers. Thanks to the GitHub API and our GitHub management portal, things are smooth. Whenever possible, we want to provide documents, guidance, and other reusable resources, instead of having to rely on special knowledge or process that has manual steps. Delegation Our office provides guidance and resources to help advise Microsoft businesses in their approach to contributing to, using, and releasing open source, but we leave the business decisions with the particular business. Decisions such as whether to release open source go through a question/answer experience, and the outcome is either automatic approval, or a business approval workflow that kicks off, allowing a teams business and legal representatives make the final decisions and choices. We also have begun deputizing open source champs: people who can help spread the good word and provide opinions and advice to their teams about how to think about open source. Transparency Open source centers around collaboration, but one of the challenges today with GitHub orgs full of many repos is identifying what teams have access to accept pull requests or administer settings. While GitHub shows a lot of this data if you start with the Teams view in an org and drill into a specific team, theres no view for a given repos teams, unless youre an admin for that repo. For all of our repos, releases and reviews, Our portal exposes the given teams that control each repo All the release requests and data are stored in work items available to any employee Our portal for GitHub management shows all GitHub Teams, including secret or hidden teams We enable cross-org search in our portal, to more easily locate similarly named teams and repos across orgs, reducing confusion and internal support costs Thanks to this, out of the 1000s of repos, its relatively painless to find the right contacts for a project inside the company. Authentic GitHub Engineers should learn how the GitHub interface works, how pull requests and reviews, issues, forks, organization teams, and collaborators function. Whenever possible, we hope that our users go directly to GitHub to manage their teams, configure team maintainers, welcome new community members into their projects as maintainers or contributors. While we do have separate internal interfaces and tools, these ideally augment the native experience. Weve built a browser extension that our users can install to help light up GitHub to make it easier to find employees, resources, or internal tools. Its important to us that engineers learn the GitHub fork and pull request model for contributing, as we strive to use a similar experience for inner source work. Technical investments Our engineering team has made many technical investments to help scale the company to be able to participate in open source better. Whenever possible we want to use open source to make open source better and share those learnings and contributions for other companies and individuals to use. Well invest in our own tooling when we must, but look to the marketplace and work that others are doing on the horizon: we are excited to see GitHubs Enterprise Cloud product evolve and drive new features and capabilities into the product to make enterprise-scale open source easier. Adopting CLA Assistant Today we host an instance of an open source project for Contributor License Agreement (CLA) management, integrated with GitHub, called CLA Assistant. This allows us to make sure that people contributing to our projects have signed the appropriate CLA before we accept a contribution. Once the CLA for an entity is signed once by a community member, they can then contribute to all other repos, so its really a relatively low burden that helps keep our lawyer friends happy. CLA Assistant is an open source project that was started by SAP and is licensed as Apache 2.0. Weve contributed to the project functionality to help with scale issues and rate limiting, automatic signing to handle when employees join and leave the company, and are excited to be able to support a new class of corporate CLAs thanks to contributions made by others to that project. In 2017 we migrated to this new open solution from the in-house CLA bot that we abandoned. I really like how the VSCode team messaged this to their community with a GitHub issue (#34239). The system is powered by GitHub org-level webhooks, so it is always on and teams do not need to worry about whether their repos are protected with the CLA and ready for contribution. Data and Insights GitHub provides useful information including traffic stats, contributor info and other breakdowns at the repository level. Across Microsofts open source projects, however, we have a need to be able to slice and dice data looking for trends over time, analyzing investments at scale, and so realized early on that we needed to import all of the available data we can from our GitHub open source releases into our own big data systems such as Azure Data Lake and Azure Data Explorer. Newer GitHub Enterprise Cloud features look to help provide organization insights, and were super excited to give those a try to augment the other data and insight methods we are using today. Here are some of the projects weve used, created, or collaborated on. GHTorrent Our team is one of the sponsors of the GHTorrent Project, an effort to create a scalable, queryable, offline mirror of data offered through the GitHub REST API. The repo is at github.com/gousiosg/github-mirror. This data, similar to GHArchive, helps learn about the broad collaboration happening on GitHub. We donate cloud compute resources to the project run by Georgios Gousios who kicked off the project with ideas and collaboration with Diomidis Spinellis. At Microsoft, we ingest the data into Azure Data Lake to be able to run interesting queries and learn more about trends and happenings beyond our campus. GHCrawler GHCrawler is a robust GitHub API crawler that walks a queue of GitHub entities transitively, retrieving and storing their contents. The initial project launched in 2016 and evolved significantly at the TODO Group tools hackathon in June 2017 while working with other companies to abstract the data stores to support other technologies and stacks. The crawler taught us a lot about the GitHub API and how to be friendly citizens by focusing on the caching of entities, the use of e-tags, and being careful to not repeatedly fetch the same resource. Different from GHTorrent, the crawler is able to traverse Microsofts own open source organizations using tokens that can peer into our current GitHub team memberships, retrieve info about maintainers, configuration, and private repos, and so is very useful in answering operational questions about the growth of GitHub at Microsoft. We ingest the data from GHCrawler into both Azure Data Lake Azure Data Explorer and use that data in Power BI dashboards and reports, live data display on internal sites, and other resources. The chart of new repos at the top of this post was created by querying this crawler data as stored in Azure Data Explorer. Heres a query that returns repos created in the official Microsoft organization in the last 30 days (that are public): Since the data comes from GitHub but is stored internally, teams can make use of the data for their own business needs and interests, without having to worry about GitHub API rate limiting, the specifics around collecting that data, and just being able to focus on using the data effectively to solve business problems. A favorite resource for many teams looking to build new communities is the data around pull requests and issues, and also collected traffic API data. Since GitHub only provides a 2-week window of consolidated traffic info, storing the data in our big data tools helps us look at trends more easily. ClearlyDefined ClearlyDefined is an Open Source Initiative (OSI) project that has a mission to help FOSS projects thrive by being clearly defined. The lack of clarity around licenses reduces engagement that ends up meaning fewer users, fewer contributors, and a smaller community. Communities choose a license for their project with the terms that they like. Defining and knowing the license for an open source project is essential to a successful community partnership, and so ClearlyDefined helps clarify that by identifying key data such as license set, attribution parties, code location, and when the data is missing, curation can help. Microsoft contributes to the effort and is making use of the data to help provide license clarity in the tooling used to help teams understand their use of open source. As of June 2019, ClearlyDefined has over 6.3 million definitions. These definitions are created by running tools such as licensee, scancode-toolkit, and fossology tools across oodles of open source. repolinter Another TODO Group project OSPO collaborates on is repolinter. Given a source repository, you can run repolinter to learn basic information that can help inform whether a project has all that is necessary to incubate and build a healthy community, such as license files README files Code of Conduct, governance or contribution information No binaries Licenses detectable by licensee Source headers have license information We hope to be able to share this data in a more visible way to help teams see where they can make simple improvements, or even by automatically opening pull requests if they are missing the essentials such as a mention of the Code of Conduct. oss-attribution-builder Weve collaborated with Amazon and use their amzn/oss-attribution-builder project to help build open source license notice files to help teams meet their legal obligations. GitHub management portal Microsofts GitHub management portal that employees to use, detailed in my 2015 post on the portal, handles: self-service org joining for employees cross-organization search of people, repos, and teams support jobs to maintain data generating digests and reports caching GitHub REST API responses processing web hook events Here you can see an experience where you can search, sort and filter repos across all of the many GitHub orgs in one place, improving discoverability. The portal itself is open source and has been rebuilt and refactored over the past few years to try and be more useful and to allow other companies to use it: Node service and site, now implemented in TypeScript Backing stores supported include Postgres and others backed by interfaces An out-of-the-box run local in memory experience is now available The repo is on GitHub at microsoft/opensource-portal. New repo wizard While we have a very liberal and friendly policy to make it easy for an engineering team to decide to share some code such as a sample, we do want to make sure that theres a process in place that includes a business decision and legal team approval if theres a major release being considered. Microsoft has chosen to turn off the Allow Members to Create Repositories option on our open source GitHub organizations. Instead, we have our own new repository wizard within our GitHub management tooling. This site collects information about the purpose of the repo, the common GitHub Team permissions that will help a group of engineers get off and running, and then populates the repo with the standard Microsoft open source content such as Code of Conduct info in the README, an appropriate LICENSE file for the project, and the original purpose and use of the repo, for data reporting purposes internally. To make this experience easier to find, we work to educate people through documentation and tools such as the previously mentioned Open Source Assistant web browser extension. The extension is able to light up the green new repo button on our orgs and link directly to the wizard, providing a better user experience than the you have insufficient permission to create a new repository message that our users would receive on GitHub otherwise. The outcome of the repo wizard will be the creation of a public or private repo on GitHub. We ask, by policy, that teams only use our open source GitHub orgs for work they are going to ship within the next 30 days. Release reviews The outcome of the new repo wizard is either auto-approval to make a project public, or the kick-off of a business and legal review process, to help comply with policy, and inform leaders about important open source decisions that are being made. We have an internal service called the Usage Service that creates a work item in our work item tracking system (Azure Boards), and assigns that work item to the appropriate legal and business reviewer. The work item is then passed along, with fields or more info being filled out, discussions are had, and eventually the review will be final approved and ready for teams to ship their project to the world and go build that open community. Here is a screenshot, redacted of the details and names, showing the work item for the release approval of the Windows Calculator: After the final approval, an automated e-mail is sent to the original requester with any guidance from the various reviewers involved in the process, to help them understand any notifications, legal obligations, or other work that needs to be done to support their release. Open Source Assistant Browser Extension Internally we ship a browser extension that lights up GitHub with Microsoft-specific information and guidance. Since a GitHub username may not be related to a persons recognized corporate identity, the extension lights up the more common corporate alias inline throughout GitHub - pull requests, issues, people profiles. By highlighting other Microsoft employees on GitHub, people who use the extension are able to dedicate additional focus on a good collaboration experience with the community, while easily being able to better identify their coworkers on the site. The extension helps people to continue to use the native GitHub experience, while augmenting bits and pieces where weve made decisions around our GitHub org settings, such as disabling New Repo creation direct on GitHub. The extension also is also to provide links to policy around open source, link to the new repository wizard. Since we disable native repo creation on GitHub in order to ask engineers to complete our wizard to learn more about their intent with their open source release, we often would get support questions about how to create repos, since GitHub would display the message that the user could not create repos. Now, this is what they see: In another sceenshot, you can see a Join button that lets an employee self-service join the GitHub organization through our GitHub management portal: Finally, since we want teams to use our official GitHub org that is configured automatically with the CLA system, with compliance tools, audit logs, and the ability to manage the lifecycle of when users join and leave, we do not allow teams to create new GitHub orgs for open source. By updating GitHubs new organization page, we have a chance to let people know about the policy and other resources before they get too far down the road of trying to setup yet-another-org. The extension works Firefox, Chrome, and the new Edge based on Chromium. Microsoft employees can learn more and install the extension at aka.ms/1esassistant. While this extenson is not open source today, if others think it could help their organization, I can see what we can do. Regular automation jobs Weve implemented jobs that do everything from keeping data and caches up-to-date to helping make sure that we have confidence in the membership of our GitHub organizations. These jobs are all open source, implemented in TypeScript, as a part of the opensource-portal repo. Removing former employees Connected to HR data sources, we can react when an employee leaves the company. In a big company, people are always changing roles, trying new things, and sometimes they leave for other opportunities. We unlink people when they leave: removing the association between their GitHub account and their corporate identity, and removing their GitHub organization membership(s). We welcome former employees to continue to contribute and participate to projects where they should continue to be maintainers, but for clarity, do not allow them to remain as org members. We also send an e-mail to their former manager, so they have confidence that the right things have happened with permissions and system access. Enforced system teams To help enable operations, support, meet security obligations, and light up bots such as our CLA system, we have a set of GitHub teams that are automatically and permanently added to every repository in our GitHub orgs, when the job is configured. By monitoring the GitHub event bus, the job adds what we call system teams to a repo the instant a new repo is created. And, if a repo admin tries to remove or change the permissions that such a system team has, the job automatically restores its permission. An occassional cronjob also runs to validate and reenforce these teams as needed. Preventing big permission grants As the number of members in a GitHub organization grows high, youll also start to see very large teams. We have teams such as All Employee Members that are designed to give easy read access to private repos that are being finished up and polished before going open source. An unintended side effect we found: some people wanted to just give all members administrative access over their repo, to make the act of onboarding get much easier. Trouble is, with admin comes great power, and any of the thousands of members of the org could then delete the repo, change whatever they want, override branch protections, etc. This also would automatically subscribe thousands of people to spammy notification messages, depending on their GitHub notification preferences. As a result, we have a large permissions job that monitors for grants of write or admin access to an org-defined large number of people. Once such a grant is made, and the team is too large, the job will automatically downgrade the permissions to try and prevent mistakes, and then send an e-mail address to the person who made the permission choice, hoping to educate them about the risks and help them understand what changes were made. GitHub username changes GitHub allows users to change their username. This is excellent: many people start with a cute screen name or other indicator, then realize theyre professionally developing software with their peers in the industry, and often want to change that username. Not all users know that they can rename their account, so we do get people who delete an account, just to create a new one. However, the majority of GitHub apps and APIs work off of GitHub usernames instead of the integer-based user ID. To help stay ahead of this, we have a cronjob that looks up GitHub users by ID and then refreshes our internal data store of those usernames, in the hope that we can improve the API success rate and reduce support costs when renames happen. For critical operations, additional logic has been added to GitHub apps to anticipate the potential of a user rename happening, and being able to smartly attempt to fallback to looking up the user first by ID, or alerting and sending the issue to operational support. Weekly and daily digests We have a cronjob that sends updates about new changes and interesting happenings on GitHub repos. These are typically only send to repo admins, to not spam too many people. The digests are personalized to the receipient, and cover scenarios such as: notifying the creator of a repo, and their manager, of the new repo notifying when the # of admins becomes rather high, so that team maintainers can cleanup permissions abandoned repos that havent been touched in years when a team is down to a single maintainer, suggesting they appoint another maintainer or two when a repository has been flipped from private to public Program investments More important than the technical tools and hacks we put in place, the programs that we run in the Open Source Programs Office help drive process improvements, awareness of features and capabilities, roll out new functionality, and to emphasis the work that is happening in the open in new ways. Heres a set of the OSPO investments we have been making or put in place to help drive improvement in the open source maturity level of the company. Docs We have a central documentation repository that is easily accessible to employees where they can read through guides on how to go about releasing open source, or contributing to it. The hope is that teams new to many of the tools and approaches in open source can refer to the material, share it with others, and help us to scale this to others. Heres a rough look at the outline of some of that material. In time I hope we can prepare a version of these docs to share with the world - General Information - What exactly is Open Source? - When and why Open Source - Can I use it? - Misconceptions - Bad Practices - Benefits - Use - Overview - Security and vulnerability management - Copyleft obligations - Code signing - Attribution guidelines - Business review process - Required notice generation - Automated registration of use - Contribute - Contribution overview - Forking repositories - Business review, if required - Release - Release overview - Checklist - Copyright headers - Foster your community - Build and publish your project - Code signing - Business review, if required - Data and Insights - GitHub insights - Registrations and request insights - GitHub information - GitHub overview - Accounts and linking - Repositories - Teams - Organizations - Transferring and migrating repos - Service accounts - Two-factor authentication - Apps, services and integrations - Large files, LFS Monthly newsletter The team prepares a monthly newsletter to all interested parties in the company, including all linked GitHub users, with a distribution north of 25,000 members. As a low-frequency resource, the hope is that the newsletter helps provide useful information without inundating teams. Typical newsletters will include 3-5 topics and updates of note, some data or graphs and charts of adoption and community health, and the date for the next open source meetup. Open Source Meetup A monthly gathering on the Microsoft campus (or online), the meetup is a quick event that starts off with networking, a quick lunch, and then 3-4 speakers who share their recent open source experiences. This helps connect people, have interesting conversations, and build community. Open Source Champs A cross-company group of people who get it, these are some of the best open source minds at Microsoft, who have connections, experience, data, and can help drive best practices and advise on situations. The champs primarily get involved in a specific discussion list in the company today, but hopefully in time they will also be able to help share information with their organizations, and build a two-way conversation channel with more stakeholders and businesses across the company. The champs will be a key part of helping us continue to scale and share and get subject matter experts connected with the right teams as they open up. Internal support OSPO offers an internal support experience, with internal mail, a Teams channel, and other places, to help support people as needed. The most common issues tend to be helping people find the documentation for what they are looking to do, pointing them at the GitHub account linking experience or new repo wizard, or helping answer policy clarifications and questions. Not all questions can be answered by OSPO - we do at times need people to reach out to their legal contact for their organization to support their request. It is also super important to us that we update internal docs and tools when we learn of gaps or improvements that can be made, so that the next group of people with the same need will not need an escalation or support incident to help answer their question. We look to our principles around delegation, self-service, and elimination to help reduce support cost, solving problems before they grow too large. A clever hack or targeted fix often helps a lot. Executive council and briefings Appointed executives from across Microsofts businesses make up the majority of the OSS Exec Council that meets at least quarterly to discuss progress on evolving the culture, being available to advise how their groups are contributing to open source, and otherwise helping provide a conduit with leadership, the open source champs, and our open source office. In many ways, this group is the board of directors of our OSPO team and we look forward to helping to tackle whatever is next as a company in this space. Having this in place helps us take the temperature of leadership of where to spend our effort and how to think about complicated issues and how to make things friction-free for our engineering teams who want to contribute to and use open source everywhere. Were also available to brief business leaders and others on their approach, share the learnings others have had, and strive to remove roadblocks with all teams. Learnings Weve learned a lot, and continue to learn as we scale the open source adoption across the company. Many of the specifics learnings Ive had relate to how we operate at scale, how much we can set teams free with tools and access, and how to help others learn from the experiences. Just a few short stories Our GitHub Bedlam moment GitHub is super real: we finally had our own Bedlam incident earlier this year we had a GitHub team inside one of our largest GitHub orgs called All Members, to make it easy to quickly give access to private repos ahead of the launch of new work at events. The idea is that as long as you give all members read access to the repo, they can then fork and submit pull requests, making it easier than trying to figure out what teams may grant them the rights they need. It also encourages the more open contribution model we love. GitHub Team Discussions shipped in late 2017 and essentially were not very frequently used by our open source teams for quite a while. Finally, one quiet day in January, it finally happened someone posted a message to the all employees in this org team discussion, immediately going out to a lot of people and then someone posted about how to update your notification settings and then it spiraled out from there. Along the way we learned a few things: People learned about GitHub discussions We may have found a few minor bugs in the notification settings on GitHub that were corrected We realized there are downsides to massive teams We had some fun, too Old time Microsoft people smiled a lot. Bedlam was a classic Microsoft learning lesson. Its fun that we can still have similar experiences with the modern developer toolkit, too Two-factor authentication A surprising amount of our internal support traffic comes from employees who unfortunately lose access to their GitHub accounts. Since we require our users to use GitHubs two-factor authentication, and this is a separate two-factor system than the Azure Active Directory multi-factor auth that our employees also use, its easy for people to get confused, or to lose access to the GitHub side of things. Thankfully, GitHub does a great job of helping remind people to save their two-factor authentication codes and to encourage other technology such as the use of U2F devices. Everytime a new iPhone model comes out, our support volume spikes a lot of people use two-factor authentication apps on their phone, but then forget to store their backup codes or anticipate that wiping their phone and upgrading to a new one may not always restore what they think it may. I strongly recommend that everyone have a YubiKey to help make things easier when using GitHub on a daily basis. GitHub API: user renames Most GitHub REST v3 API calls operate on GitHub username as opposed to user IDs. GitHub allows users to change their username, which means that if you are not also validating what their current username is, you could find that a few API calls are not working for a particular set of users who have changed their usernames. There is a GitHub API that can take a GitHub user ID and provide you the current user information response, so you can hit that occassionally, such as in a daily job which also respects e-tags and caches responses, to make sure you have the accurate username before calling operational APIs. At scale, we tend to see about 25 user renames per week at Microsoft right now. GitHub API: conditional requests The GitHub APIs conditional request guidance is good, but many libraries or users of their APIs do not seem to use them by default. As long as you keep a cached version of the entities you request, you can use the e-tag and an If-None-Match header to help reduce your load on the rate limits for GitHub. A request with the existing e-tag, to check if the entity has changed, will not count against your rolling API limit if it has not changed. Its great seeing more and more libraries such as octokit/rest.js supporting this concept in various ways, or having extensibility for it. Be nice to GitHubs API. Team vs Individual repo permissions Whenever possible we strongly encourage that the repositories that Microsoft governs use team permissions instead of granting individual permissions. GitHub Teams support multiple team maintainers, helping projects evolve over time, and making it easier to keep access and admin permission assigned as needed. Weve had to do a lot of user education on this topic: if someone opens a support ticket needing access permissions, we will never grant them individual permissions (Collaborator on GitHub) to a repo, but instead will ask them to help us find or create a new Team on GitHub for that permission assignment to go to. ** Team permissions. Teams with maintainers. Avoid individual permissions.** This helps keep things sane. Very sane. Transparency around permissions As previously mentioned, our GitHub management portal shows all employees all the permissions for teams, including secret teams. Since we are focused on enabling open source on GitHub, this helps answer questions people have about who has access to administer or change settings on a repo and reduces support volume. On GitHub you can natively see, given a team, which repos and permissions it has, if you are a member of the org. However, you cannot see easily from a given repo who the teams are. Transparent data in the portal reduces support costs around the 404 these arent the droids you are looking for experience on GitHub: since GitHub will return a 404 for a repo that an org member does not have access to, we used to get a decent amount of support traffic from people asking if the URL they were sent by a team member was real or not. Paying GitHub Before we spent a few billion dollars on GitHub, we had paid GitHub organization accounts at various pricing tiers for our open source GitHub orgs: we needed private repo access for people getting ready to release their open source. Early on we would have a bunch of people running around with corporate cards and expensing GitHub. Over the years we were able to consolidate this spend and bring sanity as the GitHub use scaled, and also to help use that as a forcing function: by centrally funding the official GitHub organizations for open source in the OSPO team, we were able to make it easy for teams to get going without having to worry about billing or setting up new organizations and systems. GitHub offered a really useful annual invoice payment method, so instead of having to worry about using a corporate card or other payment method each month, we would just submit a single payment for a set of GitHub orgs, then process that for payment. GitHub sales was super helpful and friendly. The only minor issue weve had with this is the few times that we had to make changes to the LFS data packs for an org during the year we would solve this by approving the purchase order for GitHub to include some additional buffer, so that GitHub sales could just invoice us for the additional data packs as needed. Before we used the invoice payment method, we did use corporate credit cards as needed and approved by our finance team; it was nice that we could associate a Billing Manager or managers to the orgs to help manage that without having to worry about permission to be org owners or control the resource itself. Kudos to the friendly GitHub sales and their supporting staff - they were always willing to jump on a Zoom video conference call and work through the details, such as when we were signing the Corporate Terms of Service agreement for some of our orgs. GitHubs annual invoice payment option is really straightforward and can help you reduce random one-off corporate card expenses. Org proliferation Many Microsoft teams love to ship the org chart, but when it comes to reinforcing Microsofts open source releases, we prefer all repos to go in our few main organizations. This also helps make it easier for engineers to get going: they do not need to identify both an org and a team to work on a repo, they just need to know the repo name or team details. By policy and reinforced in our tooling and other systems, we have asked people to just use the official Microsoft GitHub org since early 2017. This has been super important by providing a really straightforward way for people to get access and not worry about cross-org permissions or finding things. We also know that product names and teams change so often that these things just would not scale. While the GitHub rename features provide some flexibility to orgs and repos, we would prefer not to change too many things at once. Newer GitHub features such as an enterprise account announced in May sound really useful to help address this in the future: if you can combine compliance and billing together, perhaps it will be easier to have additional organizations where it makes sense. Build a strategy around how many GitHub organizations your organization will have for open source. The answer might be 1. Human challenges Were continually learning from people! People are great. Products vs Projects Microsoft is super crisp on the requirements to ship products and services: the Microsoft Security Development Lifecycle (SDL) helps team to be mindful of security and how to build and ship software. As a company, our products also have requirements around code signing, packaging, localization, globalization, servicing, accessibility, etc. Shipping an open source code project (a repo) is a little less involved, but teams still need to do the right thing. Our policies around releasing source code are more about business purpose and approval, making sure governance information and LICENSE files and READMEs are in place, and that teams are ready to support, build and evolve a friendly open community. Weve had a few learning situations where teams thought that the full set of requirements to release a product - such as localizing in many languages - were required to release an open source repo. For now, we emphasize to teams that projects are not products, but often, they compliment one another, and sometimes, they literally are the same thing. Products and services are very different from open source code. Thats OK. Tell your friends. Forking guidance Forking is a big deal in the open source world. There are times where a fork is the right evolution or move for a community to evolve, but weve found that some teams view forks as the way to contribute upstream. For example: if someone wants to contribute to CLA Assistant, one approach would be to fork the repo to their individual GitHub account, and then to submit a pull request to that upstream project. Another approach would be to fork the project to the official Microsoft organization, prepare changes, and then submit it as a pull request. While the second example makes it clear that this is very much a Microsoft contribution, it creates confusion, because Microsoft just wants to participate in the upstream project, and not fork it in any hard way. We strongly encourage teams to simply fork and submit pull requests, the GitHub way, from their individual accounts, not from the official organization account. We want teams to always contribute to the upstream when possible and only fork as a last resort. Forking can be a big deal. Upstream is the right place to contribute. Support volume (e-mail) Since we set the e-mail address opensource at microsoft as the e-mail address associated with the official Microsoft organization on GitHub, we get a lot of e-mail traffic from people looking for help with issues and products. Within GitHub, if youre browsing a repo and click Contact GitHub in the footer of the web page, it essentially asks whether you are looking to report a GitHub issue or an issue with the repo. This helps reduce issue/support traffic to GitHub for open source repos. GitHub then offers the e-mail address associated with the SUPPORT files for the repo, or falls back to the org-wide e-mail address. So we get a lot of e-mail. Weve learned to improve spam filters and use templates and work to address issues, but we do get a lot of mail. Expect that support will need to happen. People want more open source from us! On the entertaining side, many passionate users of long-time Microsoft software regularly write in to ask us to open source their favorite projects Flight Simulator Microsoft Money Age of Empires our operating system Its great that people really want to see this, but I think a lot of folks do not always understand that commercial software often has many dependencies or licensed components that may not be open source, or the code requires a very specialized build environment, etc. I do love seeing the historical releases to share code, such as the original winfile.exe or old-school MS-DOS 1.25 and 2.0! Hopefully teams will find the time to share when it makes sense. Its also a reality that historical software releases are not a place that will be easy to build an active collaborative community around unless a clean mechanism exists to release and build modern bits. Open source all the things Exciting new GitHub features As outlined in Build like an open source community with GitHub Enterprise, Mario Rodriguez from GitHub highlights how the features that help make GitHub great for open source can also be super useful for any organization. Weve started moving to GitHub Enterprise Cloud for more of our open source organizations and Im excited to see what our engineering teams will do with these new capabilities. A few capabilities in particular that were starting to use include: Org insights Available in beta now for Enterprise Cloud accounts, you can take a look at the high-level trends around new issues, closed issues, and other key specs and data points for all of your repositories across the org. Another nice view is the where members are getting work done look at where time is being spent on GitHub by everyone contributing to the org. Maintainer roles Especially interesting to teams like .NET which have a large amount of activity and maintainers, new roles beyond the classic read/write/admin on GitHub means that they can appoint Triage users (can manage issues and PRs, but not push code directly to the repo) or Maintain users (additional settings on top of issues and PR management). Transferring issues Moving issues between repos used to be done by third-party, unofficial, or random tooling, and now that GitHub directly supports issue movement, were pretty happy to see this additional option open for communities who have issues spanning multiple repos. Audit logs GitHub has an audit log API beta for Enterprise Cloud customers now. Using GraphQL, this means that we will be able to keep an audit log copy in our systems for review, analysis, and without the manual or cumbersome approach required today to either ingest and store webhook information in an append-only data store, and/or parse the JSON and CSV versions of the audit log for a GitHub organization. Looking forward We have a lot left to do and our journey will continue to get more interesting as Microsoft continues its open source adventure. Some of the things the Open Source Programs Office will be thinking through will include As a company we will continue to work to evolve our take on a maturity model for open source organizations, and I cant wait to see what is next. Sharing guidance and guides Similar to the Open Source Guides at //opensource.guide created by GitHub, wed love to share, to help others in the industry learn from our progress. Playbooks around how to think about open source, making business decisions related to open source, all are fun topics we would love to share our learnings and take on. We may also be able to share our docs, or a version of them, with the world. Major kudos to Google who already shares their open source guidance and policies on their site. Identifying contribution opportunities We are starting to look at ways to draw attention to contribution opportunities, such as highlighting up-for-grabs issues across our releases, and also recognizing when our employees contribute to open projects outside control of the company, too. By updating the opensource.microsoft.com site, we hope to be able to tell good stories and share useful information about what our teams are up to. As a short-term experiment, we are listing up for grabs issues right on the homepage of our open source site, to learn about whether people find that interesting. Open Source Resources Here are open source GitHub repos mentioned in this post. Check them out! cla-assistant/cla-assistant amzn/oss-attribution-builder clearlydefined/crawler todogroup/repolinter clearlydefined/curated-data microsoft/opensource-portal microsoft/ghcrawler fossology/fossology nexB/scancode-toolkit licensee/licensee octokit/rest.js Hope you found this interesting, let me know what you think on Twitter. If you were expecting a short or concise post, you may have clicked on the wrong thing Jeff Wilcox Principal Software Engineer Microsoft Open Source Programs Office ",
        "_version_":1718938194718752768},
      {
        "story_id":21112632,
        "story_author":"severine",
        "story_descendants":135,
        "story_score":442,
        "story_time":"2019-09-30T07:38:00Z",
        "story_title":"KDE is adopting GitLab",
        "search":["KDE is adopting GitLab",
          "Normal",
          "https://about.gitlab.com/press/releases/2019-09-17-gitlab-adopted-by-KDE.html",
          "You are here: Press and LogosPress releasesGitLab Adopted by KDE to Foster Open Source Contributions KDE Open Source Community has access to GitLab DevOps platform increasing members software-building options MILAN, ITALY Akademy September 11, 2019 Today GitLab, the DevOps platform delivered as a single application, announced that KDE, an international technology community that creates free and open source software for desktop and portable computing, is adopting GitLab for use by its developers to further enhance infrastructure accessibility and encourage contributions. KDE is a free and open source software community dedicated to creating a user-friendly computing experience. It offers an advanced graphical desktop, a wide variety of applications for communication, work, education and entertainment, and a platform for easily building new applications. Adding access to GitLab will provide the KDE community with additional options for accessible infrastructure for contributors, code review integration with git, streamlined infrastructure and tooling, and an open communication channel with the upstream GitLab community. With the adoption of GitLab, the KDE community, one of the largest Free Software communities with more than 2.600 contributors, will have access to an even wider range of development and code review features with GitLabs DevOps platform to complement current tools used by the KDE community. The KDE community will also be able to integrate GitLabs single application for the DevOps lifecycle to their development workflow, from planning to development and deployment to monitoring. Using GitLab, KDE contributors will have access to Concurrent DevOps, and the ability to manage and secure across stages. GitLab also provides increased visibility and comprehensive governance and accelerates software lifecycles. Were thrilled that the KDE community has chosen to adopt GitLab to offer its developers with additional tooling and features for building cutting-edge applications, said David Planella, Director of Community Relations, GitLab. KDE places a strong emphasis on finding innovative solutions to old and new problems in an atmosphere of open experimentation. This thinking aligns with GitLabs goal of helping teams better collaborate on software development, and we look forward to supporting KDE as they continue to build great software for millions of users across the globe. Lydia Pintscher, President of KDE e.V., said: For an open community like KDE, having friendly, easy-to-use infrastructure is crucial. We have spent the last two years significantly reducing the barriers of entry all across KDE. Moving to GitLab is a major step in that process. Note to editors: During GitLab Commit, GitLabs inaugural user events in Brooklyn on September 17 and London on October 9, KDE will participate in a panel on the benefits of using GitLab with KDE projects. About KDE KDE is an international community that creates Free Software for desktop and portable computing. Among KDE's products are a modern desktop system for Linux and UNIX platforms, and comprehensive office productivity and groupware suites. KDE offers hundreds of software titles in many categories including web applications, multimedia, entertainment, educational, graphics and software development. KDE software is translated into more than 65 languages and is built with ease of use and modern accessibility principles in mind. KDE's full-featured applications run natively on Linux, BSD, Solaris, Windows and macOS. About GitLab GitLab is the DevOps platform built from the ground up as a single application for all stages of the DevOps lifecycle enabling Product, Development, QA, Security, and Operations teams to work concurrently on the same project. GitLab provides a single data store, one user interface, and one permission model across the DevOps lifecycle. This allows teams to significantly reduce cycle times through more efficient collaboration and enhanced focus. Built on Open Source, GitLab works alongside its growing community, which is composed of thousands of developers and millions of users, to continuously deliver new DevOps innovations. GitLab has an estimated 30 million+ users (both Paid and Free) from startups to global enterprises, including Ticketmaster, Jaguar Land Rover, NASDAQ, Dish Network, and Comcast trust GitLab to deliver great software faster. All-remote since 2014, GitLab has more than 1,300 team members in 65 countries. Media Contact Natasha Woods GitLab nwoods@gitlab.com (415) 312-5289 KDE Press Room press@kde.org Git is a trademark of Software Freedom Conservancy and our use of 'GitLab' is under license View page source Edit in Web IDE please contribute. GitLab B.V. ",
          "So far, I am happy only using pure git.<p>If you guys that use GitLab or GitHub or alike would have to switch to pure git - what is the one thing you would miss most?",
          "Couldn't see a link in the post but there seems to be an active GitLab instance here[1]. I've only done a few patches with their previous Phabricator flow and it was a very different experience from the usual GitHub/GitLab workflow. I'm really hoping that switching to something more people have experience with increases the number of contributors.<p>[1]: <a href=\"https://invent.kde.org/public/\" rel=\"nofollow\">https://invent.kde.org/public/</a>"],
        "story_type":"Normal",
        "url_raw":"https://about.gitlab.com/press/releases/2019-09-17-gitlab-adopted-by-KDE.html",
        "comments.comment_id":[21112936,
          21113256],
        "comments.comment_author":["TekMol",
          "EspadaV9"],
        "comments.comment_descendants":[17,
          3],
        "comments.comment_time":["2019-09-30T08:40:31Z",
          "2019-09-30T09:37:59Z"],
        "comments.comment_text":["So far, I am happy only using pure git.<p>If you guys that use GitLab or GitHub or alike would have to switch to pure git - what is the one thing you would miss most?",
          "Couldn't see a link in the post but there seems to be an active GitLab instance here[1]. I've only done a few patches with their previous Phabricator flow and it was a very different experience from the usual GitHub/GitLab workflow. I'm really hoping that switching to something more people have experience with increases the number of contributors.<p>[1]: <a href=\"https://invent.kde.org/public/\" rel=\"nofollow\">https://invent.kde.org/public/</a>"],
        "id":"9006fb32-342b-4857-bd35-80fa4612d93d",
        "url_text":"You are here: Press and LogosPress releasesGitLab Adopted by KDE to Foster Open Source Contributions KDE Open Source Community has access to GitLab DevOps platform increasing members software-building options MILAN, ITALY Akademy September 11, 2019 Today GitLab, the DevOps platform delivered as a single application, announced that KDE, an international technology community that creates free and open source software for desktop and portable computing, is adopting GitLab for use by its developers to further enhance infrastructure accessibility and encourage contributions. KDE is a free and open source software community dedicated to creating a user-friendly computing experience. It offers an advanced graphical desktop, a wide variety of applications for communication, work, education and entertainment, and a platform for easily building new applications. Adding access to GitLab will provide the KDE community with additional options for accessible infrastructure for contributors, code review integration with git, streamlined infrastructure and tooling, and an open communication channel with the upstream GitLab community. With the adoption of GitLab, the KDE community, one of the largest Free Software communities with more than 2.600 contributors, will have access to an even wider range of development and code review features with GitLabs DevOps platform to complement current tools used by the KDE community. The KDE community will also be able to integrate GitLabs single application for the DevOps lifecycle to their development workflow, from planning to development and deployment to monitoring. Using GitLab, KDE contributors will have access to Concurrent DevOps, and the ability to manage and secure across stages. GitLab also provides increased visibility and comprehensive governance and accelerates software lifecycles. Were thrilled that the KDE community has chosen to adopt GitLab to offer its developers with additional tooling and features for building cutting-edge applications, said David Planella, Director of Community Relations, GitLab. KDE places a strong emphasis on finding innovative solutions to old and new problems in an atmosphere of open experimentation. This thinking aligns with GitLabs goal of helping teams better collaborate on software development, and we look forward to supporting KDE as they continue to build great software for millions of users across the globe. Lydia Pintscher, President of KDE e.V., said: For an open community like KDE, having friendly, easy-to-use infrastructure is crucial. We have spent the last two years significantly reducing the barriers of entry all across KDE. Moving to GitLab is a major step in that process. Note to editors: During GitLab Commit, GitLabs inaugural user events in Brooklyn on September 17 and London on October 9, KDE will participate in a panel on the benefits of using GitLab with KDE projects. About KDE KDE is an international community that creates Free Software for desktop and portable computing. Among KDE's products are a modern desktop system for Linux and UNIX platforms, and comprehensive office productivity and groupware suites. KDE offers hundreds of software titles in many categories including web applications, multimedia, entertainment, educational, graphics and software development. KDE software is translated into more than 65 languages and is built with ease of use and modern accessibility principles in mind. KDE's full-featured applications run natively on Linux, BSD, Solaris, Windows and macOS. About GitLab GitLab is the DevOps platform built from the ground up as a single application for all stages of the DevOps lifecycle enabling Product, Development, QA, Security, and Operations teams to work concurrently on the same project. GitLab provides a single data store, one user interface, and one permission model across the DevOps lifecycle. This allows teams to significantly reduce cycle times through more efficient collaboration and enhanced focus. Built on Open Source, GitLab works alongside its growing community, which is composed of thousands of developers and millions of users, to continuously deliver new DevOps innovations. GitLab has an estimated 30 million+ users (both Paid and Free) from startups to global enterprises, including Ticketmaster, Jaguar Land Rover, NASDAQ, Dish Network, and Comcast trust GitLab to deliver great software faster. All-remote since 2014, GitLab has more than 1,300 team members in 65 countries. Media Contact Natasha Woods GitLab nwoods@gitlab.com (415) 312-5289 KDE Press Room press@kde.org Git is a trademark of Software Freedom Conservancy and our use of 'GitLab' is under license View page source Edit in Web IDE please contribute. GitLab B.V. ",
        "_version_":1718938225488166912},
      {
        "story_id":21439378,
        "story_author":"chmaynard",
        "story_descendants":1,
        "story_score":24,
        "story_time":"2019-11-04T06:11:27Z",
        "story_title":"Highlights from Git 2.24",
        "search":["Highlights from Git 2.24",
          "Normal",
          "https://github.blog/2019-11-03-highlights-from-git-2-24/",
          "The open source Git project just released Git 2.24 with features and bug fixes from over 78 contributors, 21 of them new. Heres our look at some of the most exciting features and changes introduced since Git 2.23. Feature macros Since the very early days, Git has shipped with a configuration subsystem that lets you configure different global or repository-specific settings. For example, the first time you wrote a commit on a new machine, you might have been reminded to set your user.name and user.email settings[1] if you havent already. It turns out, git config is used for many things ranging anywhere from the identity you commit with and what line endings to use all the way to configuring aliases to other Git commands and what algorithm is chosen to produce diffs. Usually, configuring some behavior requires only a single configuration change, like enabling or disabling any of the aforementioned values. But what about when it doesnt? What do you do when you dont know which configuration values to change? For example, lets say you want to live on the bleeding-edge of the latest from upstream Git, but dont have a chance to discover all the new configurable options. In Git 2.24, you can now opt into feature macrosone Git configuration that implies many others. These are hand-selected by the developers of Git, and they let you opt into a certain feature or adopt a handful of settings based on the characteristics of your repository. For example, lets pretend that you have a particularly large repository, and youre noticing some slow-downs. With enough searching, you might find that setting index.version to 4 could help, but discovering this can seem like a stretch. Instead, you can enable feature.manyFiles with: $ git config feature.manyFiles true Now youre opted into the features that will make your experience with Git the smoothest it can be. Setting this signals to Git that youre willing to adopt whichever settings Git developers feel can make your experience smoothest (right now, this means that the index.version and core.untrackedCache to enable path-prefix compression and the untracked cache, respectively). Not only that, but you can feel even better knowing that any new features in a release that might help your use case will be included in the macro. [source] Commit graphs by default You may remember commit graphs, a feature that we have discussed in some of our previous highlights. Since its introduction in Git 2.19, this feature has received a steady stream of attention. When enabled, and kept reasonably up to date, commit graphs can represent an order of magnitude improvement in the performance of loading commits. Now in Git 2.24, commit graphs are enabled by default, meaning that your repository will see an improvement the next time you run git gc. Previously, this feature was an opt-in behind an experimental core.commitGraph configuration (as well as a handful of others), but after extensive testing[2], its ready for prime time. Besides being the new standard, here are a few other changes to commit graphs: All commit-graph sub-commands (e.g., git commit-graph write, git commit-graph verify, and so on) now have support for the -[no-]progress. Now the progress meters for these commands behave in the usual way: writing only to a terminal by default, and respecting -[no-]progress to override that. A new configuration value to automatically update the commit-graph file while fetching has been introduced, which takes advantage of commit graph chains to write a portion of history onto a commit-graph chain for later compaction. This means that every time you get new commits from a remote, they are guaranteed to be in a commit-graph immediately, and you dont have to wait around for the next auto-gc. To try this out today, set the fetch.writeCommitGraph configuration variable to true. Lots of bug fixes to improve the performance and reliability of the commit-graph commands, especially when faced with corrupt repositories. The commit-graph commands now also support Gits latest tracing mechanism, trace2. [source, source, source, source, source, source, source] Adopting the Contributor Covenant Since the last release, the Git project has discussed at length adopting a code of conduct to solidify welcoming and inclusive behavior on the mailing list where Git development takes place. Because communication between Git developers happens over email, it can be intimidating or unwelcoming to new contributors who may not be familiar with the values of the people contributing to Git. The Git community has long relied on a policy of be nice, as much as possible (to quote this thread). This approach is in the right spirit, but it may not be readily apparent to new contributors unfamiliar with the existing culture. Likewise, it can make an individual feel uncomfortable engaging with a project that has not solidified its values. By adopting a code of conduct, the Git project is making it clear which behaviors it encourages and which it wont tolerate. New contributors are able to see explicitly what the projects values are, and they can put their trust in Gits choice of using the well-trusted and widely-adopted Contributor Covenant. This code of conduct is enforced by the projects leadership, who will handle any case in which an individual does not adhere to the guidelines. New contributors can be assured that the Git community is behind this adoption with the introduction of the Code of Conduct, Acked-by 16 prominent members of the Git community. [source] Alternative history rewriting tools If youve ever wanted to perform a complicated operation over the history of your repositorylike expunging a file from a repositorys history or extracting the history pertaining to just one directoryyou may have visited the documentation for git filter-branch. git filter-branch is a long-standing and powerful tool for rewriting history[3]. With git filter-branch, you can do all of those aforementioned operations and much more. However, this flexibility comes at a hefty cost: git filter-branch is notoriously complicated to use (not to mention slow), and can often lead its users towards unintended changes, including repository corruption and data loss. In other words, git filter-branch is starting to show its age. Now, as of Git 2.24, the Git project instead recommends a new, independent tool, git filter-repo. git filter-repo serves to avoid many of the pitfalls that users experienced with git filter-branch. Instead of reprocessing every commit in order, git filter-repo operates on an efficient, stream representation of history to run much faster. The tool is extremely powerful, and all of its capabilities are documented thoroughly. Here are a few highlights about how you can use git filter-repo: git filter-repo --analyzeprovides a human-readable selection of metrics profiling the size of your repository. This includes how many objects of each kind there are, which files and directories are largest, which extensions take up the most space, and so on. And this isnt your only option in this space. For additional metrics on the shape of your repository, check out another tool, git sizer. You can also filter the history of your repository to contain only certain paths, with --path-{glob,regex} and similar options. [source] Likewise, you can run a find and replace operation over history, as well as strip blobs that are larger than a fixed threshold. [source] When rewriting history, any rewritten commits (along with their ancestors) will get a new SHA-1 to identify them. By default, git filter-repo updates all other references to these SHA-1s, like other commit messages that reference them. By a similar token, git filter-repo also has options to rewrite the names of contributors using .mailmap. [source, source] Finally, git filter-repo is extensible. It provides a flexible interface for specifying callbacks in Python (e.g., calling a function when git filter-repoencounters a blob/tree/commit, new filetype, etc.), as well as defining new sub-commands entirely. View a portfolio of demo extensions, or define your own to support a complex history rewrite. [source] git filter-branch will, however, remain included in the usual distributions of Git for some time. git filter-repo is another alternative for performing complex modifications to your repositorys history, and its now the official recommendation from upstream. [source] Tidbits You might be aware that many Git commands take one or more optional reference names as arguments. For example, git log without arguments will display a log of everything thats reachable from the currently checked-out branch, but git log my-feature ^master will show you only whats on my-feature and not on master. But what if your branch is called --super-dangerous-option, you probably dont want to invoke git log since itll interpret the argument as an option, not a branch name. You could try and disambiguate by invoking git log 'refs/heads/--super-dangerous-option', but if youre scripting, you may not know under what namespace the argument youre getting belongs. Git 2.24 has a new way to prevent this sort of option injection attack using --end-of-options. When Git sees this as an argument to any command, it knows to treat the remaining arguments as such, and wont interpret them as more options. So, instead of the string previously mentioned, you could write the following to get the history of your (admittedly, pretty oddly named) branch: $ git log --end-of-options --super-dangerous-option Not using the standard -- was an intentional choice here, since this is already a widely-used mechanism in Git to separate reference names from files. In this example, you could have also written git log --end-of-options --super-dangerous-option ^master -- path/to/file to get only the history over that range which modified that specific file. [source] In a previous post, we talked about git rebases new --rebase-merges option, which allows users to perform rebases while preserving the structure of history. But when Git encounters a merge point how does it unify the two histories? By default, it uses a strategy known internally as recursive, which is most likely the merge strategy youre currently using. You might not know that you can tell Git which merge strategy to use, and picking one over the other may result in a different resolution[4]. Now, git rebase --rebase-merges supports the --strategy and --strategy-option options of git rebase, so you can rebase history while both preserving its structural integrity and specifying your own merge resolution strategy. [source] Git supports a number of hooks, which are specially-named executable files that Git will run at various points during your workflow. For example, a pre-push hook is invoked after running git push but before the push actually occurs and so on. A new hook has been added to allow callers to interact with Git after a merge has been carried out, but before the resulting commit is written. To intercept this point, callers can place an executable file of their choice in .git/hooks/pre-merge-commit. [source] Git has learned a handful of new tricks since the last release to handle partial clones. For those who arent up to date on what partial cloning in Git looks like, heres a quick primer. When cloning a repository, users can specify that they would only like some of its objects by using a filter. When doing so, the remote from which the user clones is designated as a promisor, meaning that it promises to send the remaining objects later on if the user requests them down the road. Up until 2.24, Git only supported a single promisor remote, but this latest release now supports more than one promisor remote. This is especially interesting since it means that users can configure a handful of geographically close remotes, and not all of those remotes have to have all of the objects. Theres more work planned in this area, so stay tuned for more updates on this feature in the future. [source] Last but not least, Gits command-line completion engine has learned how to complete configuration variables on per-command configurations. Git has a hierarchy of places where gitconfig files can be found: your repository (via .git/config), your home directory, and so on. But, Git also supports the top-level -c flag to specify configuration variables for each command. Heres an example: $ git -c core.autocrlf=false add path/to/my/file This invocation will disable Gits auto-CRLF conversion just for the duration of the git add (and any other internal commands that Git may run as a part of git add). However, if you forgot the name of the variable that you were trying to set Gits command-line completion engine learned how to provide a completion list of configuration variable names in Git 2.24. So, if you ever forget where you are in the middle of a -c ..., all you need to do is press tab. [source] [1]When you create a commit, these are the configuration values that Git is looking at to generate the signature; Git-parlance for the name/email-pair that is shown wherever an identity is expected. [2]Including at GitHub, where weve used the commit-graph file behind the scenes since August to achieve speed-ups of over 50 percent on operations which traverse history. [3]In fact, this tool appeared in git/git with 6f6826c52b which was first released in v1.5.3 over 12 years ago. [4]For example, the patience merge strategy is widely regarded as one that will move up chunks of your diff (these are usually known as hunks) that look textually related (for example, closing function braces), but in fact produce awkward-looking diffs. Learn more Thats just a sample of changes from the latest version. Check out the release notes for 2.24 or any previous versions in the Git repository. ",
          "Release notes: <a href=\"https://github.com/git/git/blob/v2.24.0/Documentation/RelNotes/2.24.0.txt\" rel=\"nofollow\">https://github.com/git/git/blob/v2.24.0/Documentation/RelNot...</a>"],
        "story_type":"Normal",
        "url_raw":"https://github.blog/2019-11-03-highlights-from-git-2-24/",
        "comments.comment_id":[21439402],
        "comments.comment_author":["chmaynard"],
        "comments.comment_descendants":[0],
        "comments.comment_time":["2019-11-04T06:20:38Z"],
        "comments.comment_text":["Release notes: <a href=\"https://github.com/git/git/blob/v2.24.0/Documentation/RelNotes/2.24.0.txt\" rel=\"nofollow\">https://github.com/git/git/blob/v2.24.0/Documentation/RelNot...</a>"],
        "id":"724cf90a-0a39-4a88-829d-b4a14e448674",
        "url_text":"The open source Git project just released Git 2.24 with features and bug fixes from over 78 contributors, 21 of them new. Heres our look at some of the most exciting features and changes introduced since Git 2.23. Feature macros Since the very early days, Git has shipped with a configuration subsystem that lets you configure different global or repository-specific settings. For example, the first time you wrote a commit on a new machine, you might have been reminded to set your user.name and user.email settings[1] if you havent already. It turns out, git config is used for many things ranging anywhere from the identity you commit with and what line endings to use all the way to configuring aliases to other Git commands and what algorithm is chosen to produce diffs. Usually, configuring some behavior requires only a single configuration change, like enabling or disabling any of the aforementioned values. But what about when it doesnt? What do you do when you dont know which configuration values to change? For example, lets say you want to live on the bleeding-edge of the latest from upstream Git, but dont have a chance to discover all the new configurable options. In Git 2.24, you can now opt into feature macrosone Git configuration that implies many others. These are hand-selected by the developers of Git, and they let you opt into a certain feature or adopt a handful of settings based on the characteristics of your repository. For example, lets pretend that you have a particularly large repository, and youre noticing some slow-downs. With enough searching, you might find that setting index.version to 4 could help, but discovering this can seem like a stretch. Instead, you can enable feature.manyFiles with: $ git config feature.manyFiles true Now youre opted into the features that will make your experience with Git the smoothest it can be. Setting this signals to Git that youre willing to adopt whichever settings Git developers feel can make your experience smoothest (right now, this means that the index.version and core.untrackedCache to enable path-prefix compression and the untracked cache, respectively). Not only that, but you can feel even better knowing that any new features in a release that might help your use case will be included in the macro. [source] Commit graphs by default You may remember commit graphs, a feature that we have discussed in some of our previous highlights. Since its introduction in Git 2.19, this feature has received a steady stream of attention. When enabled, and kept reasonably up to date, commit graphs can represent an order of magnitude improvement in the performance of loading commits. Now in Git 2.24, commit graphs are enabled by default, meaning that your repository will see an improvement the next time you run git gc. Previously, this feature was an opt-in behind an experimental core.commitGraph configuration (as well as a handful of others), but after extensive testing[2], its ready for prime time. Besides being the new standard, here are a few other changes to commit graphs: All commit-graph sub-commands (e.g., git commit-graph write, git commit-graph verify, and so on) now have support for the -[no-]progress. Now the progress meters for these commands behave in the usual way: writing only to a terminal by default, and respecting -[no-]progress to override that. A new configuration value to automatically update the commit-graph file while fetching has been introduced, which takes advantage of commit graph chains to write a portion of history onto a commit-graph chain for later compaction. This means that every time you get new commits from a remote, they are guaranteed to be in a commit-graph immediately, and you dont have to wait around for the next auto-gc. To try this out today, set the fetch.writeCommitGraph configuration variable to true. Lots of bug fixes to improve the performance and reliability of the commit-graph commands, especially when faced with corrupt repositories. The commit-graph commands now also support Gits latest tracing mechanism, trace2. [source, source, source, source, source, source, source] Adopting the Contributor Covenant Since the last release, the Git project has discussed at length adopting a code of conduct to solidify welcoming and inclusive behavior on the mailing list where Git development takes place. Because communication between Git developers happens over email, it can be intimidating or unwelcoming to new contributors who may not be familiar with the values of the people contributing to Git. The Git community has long relied on a policy of be nice, as much as possible (to quote this thread). This approach is in the right spirit, but it may not be readily apparent to new contributors unfamiliar with the existing culture. Likewise, it can make an individual feel uncomfortable engaging with a project that has not solidified its values. By adopting a code of conduct, the Git project is making it clear which behaviors it encourages and which it wont tolerate. New contributors are able to see explicitly what the projects values are, and they can put their trust in Gits choice of using the well-trusted and widely-adopted Contributor Covenant. This code of conduct is enforced by the projects leadership, who will handle any case in which an individual does not adhere to the guidelines. New contributors can be assured that the Git community is behind this adoption with the introduction of the Code of Conduct, Acked-by 16 prominent members of the Git community. [source] Alternative history rewriting tools If youve ever wanted to perform a complicated operation over the history of your repositorylike expunging a file from a repositorys history or extracting the history pertaining to just one directoryyou may have visited the documentation for git filter-branch. git filter-branch is a long-standing and powerful tool for rewriting history[3]. With git filter-branch, you can do all of those aforementioned operations and much more. However, this flexibility comes at a hefty cost: git filter-branch is notoriously complicated to use (not to mention slow), and can often lead its users towards unintended changes, including repository corruption and data loss. In other words, git filter-branch is starting to show its age. Now, as of Git 2.24, the Git project instead recommends a new, independent tool, git filter-repo. git filter-repo serves to avoid many of the pitfalls that users experienced with git filter-branch. Instead of reprocessing every commit in order, git filter-repo operates on an efficient, stream representation of history to run much faster. The tool is extremely powerful, and all of its capabilities are documented thoroughly. Here are a few highlights about how you can use git filter-repo: git filter-repo --analyzeprovides a human-readable selection of metrics profiling the size of your repository. This includes how many objects of each kind there are, which files and directories are largest, which extensions take up the most space, and so on. And this isnt your only option in this space. For additional metrics on the shape of your repository, check out another tool, git sizer. You can also filter the history of your repository to contain only certain paths, with --path-{glob,regex} and similar options. [source] Likewise, you can run a find and replace operation over history, as well as strip blobs that are larger than a fixed threshold. [source] When rewriting history, any rewritten commits (along with their ancestors) will get a new SHA-1 to identify them. By default, git filter-repo updates all other references to these SHA-1s, like other commit messages that reference them. By a similar token, git filter-repo also has options to rewrite the names of contributors using .mailmap. [source, source] Finally, git filter-repo is extensible. It provides a flexible interface for specifying callbacks in Python (e.g., calling a function when git filter-repoencounters a blob/tree/commit, new filetype, etc.), as well as defining new sub-commands entirely. View a portfolio of demo extensions, or define your own to support a complex history rewrite. [source] git filter-branch will, however, remain included in the usual distributions of Git for some time. git filter-repo is another alternative for performing complex modifications to your repositorys history, and its now the official recommendation from upstream. [source] Tidbits You might be aware that many Git commands take one or more optional reference names as arguments. For example, git log without arguments will display a log of everything thats reachable from the currently checked-out branch, but git log my-feature ^master will show you only whats on my-feature and not on master. But what if your branch is called --super-dangerous-option, you probably dont want to invoke git log since itll interpret the argument as an option, not a branch name. You could try and disambiguate by invoking git log 'refs/heads/--super-dangerous-option', but if youre scripting, you may not know under what namespace the argument youre getting belongs. Git 2.24 has a new way to prevent this sort of option injection attack using --end-of-options. When Git sees this as an argument to any command, it knows to treat the remaining arguments as such, and wont interpret them as more options. So, instead of the string previously mentioned, you could write the following to get the history of your (admittedly, pretty oddly named) branch: $ git log --end-of-options --super-dangerous-option Not using the standard -- was an intentional choice here, since this is already a widely-used mechanism in Git to separate reference names from files. In this example, you could have also written git log --end-of-options --super-dangerous-option ^master -- path/to/file to get only the history over that range which modified that specific file. [source] In a previous post, we talked about git rebases new --rebase-merges option, which allows users to perform rebases while preserving the structure of history. But when Git encounters a merge point how does it unify the two histories? By default, it uses a strategy known internally as recursive, which is most likely the merge strategy youre currently using. You might not know that you can tell Git which merge strategy to use, and picking one over the other may result in a different resolution[4]. Now, git rebase --rebase-merges supports the --strategy and --strategy-option options of git rebase, so you can rebase history while both preserving its structural integrity and specifying your own merge resolution strategy. [source] Git supports a number of hooks, which are specially-named executable files that Git will run at various points during your workflow. For example, a pre-push hook is invoked after running git push but before the push actually occurs and so on. A new hook has been added to allow callers to interact with Git after a merge has been carried out, but before the resulting commit is written. To intercept this point, callers can place an executable file of their choice in .git/hooks/pre-merge-commit. [source] Git has learned a handful of new tricks since the last release to handle partial clones. For those who arent up to date on what partial cloning in Git looks like, heres a quick primer. When cloning a repository, users can specify that they would only like some of its objects by using a filter. When doing so, the remote from which the user clones is designated as a promisor, meaning that it promises to send the remaining objects later on if the user requests them down the road. Up until 2.24, Git only supported a single promisor remote, but this latest release now supports more than one promisor remote. This is especially interesting since it means that users can configure a handful of geographically close remotes, and not all of those remotes have to have all of the objects. Theres more work planned in this area, so stay tuned for more updates on this feature in the future. [source] Last but not least, Gits command-line completion engine has learned how to complete configuration variables on per-command configurations. Git has a hierarchy of places where gitconfig files can be found: your repository (via .git/config), your home directory, and so on. But, Git also supports the top-level -c flag to specify configuration variables for each command. Heres an example: $ git -c core.autocrlf=false add path/to/my/file This invocation will disable Gits auto-CRLF conversion just for the duration of the git add (and any other internal commands that Git may run as a part of git add). However, if you forgot the name of the variable that you were trying to set Gits command-line completion engine learned how to provide a completion list of configuration variable names in Git 2.24. So, if you ever forget where you are in the middle of a -c ..., all you need to do is press tab. [source] [1]When you create a commit, these are the configuration values that Git is looking at to generate the signature; Git-parlance for the name/email-pair that is shown wherever an identity is expected. [2]Including at GitHub, where weve used the commit-graph file behind the scenes since August to achieve speed-ups of over 50 percent on operations which traverse history. [3]In fact, this tool appeared in git/git with 6f6826c52b which was first released in v1.5.3 over 12 years ago. [4]For example, the patience merge strategy is widely regarded as one that will move up chunks of your diff (these are usually known as hunks) that look textually related (for example, closing function braces), but in fact produce awkward-looking diffs. Learn more Thats just a sample of changes from the latest version. Check out the release notes for 2.24 or any previous versions in the Git repository. ",
        "_version_":1718938236980559872},
      {
        "story_id":19074170,
        "story_author":"nosarthur",
        "story_descendants":36,
        "story_score":79,
        "story_time":"2019-02-04T05:22:20Z",
        "story_title":"Show HN: Gita – a CLI tool to manage multiple Git repos",
        "search":["Show HN: Gita – a CLI tool to manage multiple Git repos",
          "ShowHN",
          "https://github.com/nosarthur/gita",
          "_______________________________ ( ____ \\__ __|__ __( ___ ) | ( \\/ ) ( ) ( | ( ) | | | | | | | | (___) | | | ____ | | | | | ___ | | | \\_ ) | | | | | ( ) | | (___) |__) (___ | | | ) ( | (_______)_______/ )_( |/ \\| v0.15 Gita: a command-line tool to manage multiple git repos This tool does two things display the status of multiple git repos such as branch, modification, commit message side by side (batch) delegate git commands/aliases from any working directory If several repos are related, it helps to see their status together. I also hate to change directories to execute git commands. In this screenshot, the gita ll command displays the status of all repos. The gita remote dotfiles command translates to git remote -v for the dotfiles repo, even though we are not in the repo. The gita fetch command fetches from all repos and two of them have updates. To see the pre-defined commands, run gita -h or take a look at cmds.json. To add your own commands, see the customization section. To run arbitrary git command, see the superman mode section. To run arbitrary shell command, see the shell mode section. The branch color distinguishes 5 situations between local and remote branches: color meaning white local has no remote green local is the same as remote red local has diverged from remote purple local is ahead of remote (good for push) yellow local is behind remote (good for merge) The choice of purple for ahead and yellow for behind is motivated by blueshift and redshift, using green as baseline. You can change the color scheme using the gita color command. See the customization section. The additional status symbols denote symbol meaning + staged changes * unstaged changes _ untracked files/folders The bookkeeping sub-commands are gita add <repo-path(s)> [-g <groupname>]: add repo(s) to gita, optionally into an existing group gita add -a <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively and automatically generate hierarchical groups. See the customization section for more details. gita add -b <bare-repo-path(s)>: add bare repo(s) to gita. See the customization section for more details on setting custom worktree. gita add -r <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively gita clone <config-file>: clone repos in config-file (generated by gita freeze) to current directory. gita clone -p <config-file>: clone repos in config-file to prescribed paths. gita context: context sub-command gita context: show current context gita context <group-name>: set context to group-name, all operations then only apply to repos in this group gita context auto: set context automatically according to the current working directory gita context none: remove context gita color: color sub-command gita color [ll]: Show available colors and the current coloring scheme gita color reset: Reset to the default coloring scheme gita color set <situation> <color>: Use the specified color for the local-remote situation gita flags: flags sub-command gita flags set <repo-name> <flags>: add custom flags to repo gita flags [ll]: display repos with custom flags gita freeze: print information of all repos such as URL, name, and path. Use with gita clone. gita group: group sub-command gita group add <repo-name(s)> -n <group-name>: add repo(s) to a new or existing group gita group [ll]: display existing groups with repos gita group ls: display existing group names gita group rename <group-name> <new-name>: change group name gita group rm <group-name(s)>: delete group(s) gita group rmrepo <repo-name(s)> -n <group-name>: remove repo(s) from existing group gita info: info sub-command gita info [ll]: display the used and unused information items gita info add <info-item>: enable information item gita info rm <info-item>: disable information item gita ll: display the status of all repos gita ll <group-name>: display the status of repos in a group gita ll -g: display the repo summaries by groups gita ls: display the names of all repos gita ls <repo-name>: display the absolute path of one repo gita rename <repo-name> <new-name>: rename a repo gita rm <repo-name(s)>: remove repo(s) from gita (won't remove files on disk) gita -v: display gita version The git delegating sub-commands are of two formats gita <sub-command> [repo-name(s) or group-name(s)]: optional repo or group input, and no input means all repos. gita <sub-command> <repo-name(s) or groups-name(s)>: required repo name(s) or group name(s) input They translate to git <sub-command> for the corresponding repos. By default, only fetch and pull take optional input. In other words, gita fetch and gita pull apply to all repos. To see the pre-defined sub-commands, run gita -h or take a look at cmds.json. To add your own sub-commands or override the default behaviors, see the customization section. To run arbitrary git command, see the superman mode section. If more than one repos are specified, the git command runs asynchronously, with the exception of log, difftool and mergetool, which require non-trivial user input. Repo configuration is saved in $XDG_CONFIG_HOME/gita/repos.csv (most likely ~/.config/gita/repos.csv). Installation To install the latest version, run If you prefer development mode, download the source code and run pip3 install -e <gita-source-folder> In either case, calling gita in terminal may not work, then put the following line in the .bashrc file. alias gita=\"python3 -m gita\" Windows users may need to enable the ANSI escape sequence in terminal for the branch color to work. See this stackoverflow post for details. Auto-completion Download .gita-completion.bash or .gita-completion.zsh and source it in shell. Superman mode The superman mode delegates any git command or alias. Usage: gita super [repo-name(s) or group-name(s)] <any-git-command-with-or-without-options> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita super checkout master puts all repos on the master branch gita super frontend-repo backend-repo commit -am 'implement a new feature' executes git commit -am 'implement a new feature' for frontend-repo and backend-repo Shell mode The shell mode delegates any shell command. Usage: gita shell [repo-name(s) or group-name(s)] <any-shell-command> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita shell ll lists contents for all repos gita shell repo1 repo2 mkdir docs create a new directory docs in repo1 and repo2 gita shell \"git describe --abbrev=0 --tags | xargs git checkout\": check out the latest tag for all repos Customization define repo group and context When the project contains several independent but related repos, we can define a group and execute gita command on this group. For example, gita group add repo1 repo2 -n my-group gita ll my-group gita pull my-group To save more typing, one can set a group as context, then any gita command is scoped to the group gita context my-group gita ll gita pull The most useful context maybe auto. In this mode, the context is automatically determined from the current working directory (CWD): the context is the group whose member repo's path contains CWD. To set it, run To remove the context, run It is also possible to recursively add repos within a directory and generate hierarchical groups automatically. For example, running on the following folder structure src project1 repo1 repo2 repo3 project2 repo4 repo5 repo6 gives rise to 3 groups: src:repo1,repo2,repo3,repo4,repo5,repo6 src-project1:repo1,repo2 src-project2:repo4,repo5 add user-defined sub-command using json file Custom delegating sub-commands can be defined in $XDG_CONFIG_HOME/gita/cmds.json (most likely ~/.config/gita/cmds.json) And they shadow the default ones if name collisions exist. Default delegating sub-commands are defined in cmds.json. For example, gita stat <repo-name(s)> is registered as \"stat\":{ \"cmd\": \"git diff --stat\", \"help\": \"show edit statistics\" } which executes git diff --stat for the specified repo(s). To disable asynchronous execution, set disable_async to be true. See the difftool example: \"difftool\":{ \"cmd\": \"git difftool\", \"disable_async\": true, \"help\": \"show differences using a tool\" } If you want a custom command to behave like gita fetch, i.e., to apply to all repos when no repo is specified, set allow_all to be true. For example, the following snippet creates a new command gita comaster [repo-name(s)] with optional repo name input. \"comaster\":{ \"cmd\": \"checkout master\", \"allow_all\": true, \"help\": \"checkout the master branch\" } Any command that runs in the superman mode mode or the shell mode can be defined in this json format. For example, the following command runs in shell mode and fetches only the current branch from upstream. \"fetchcrt\":{ \"cmd\": \"git rev-parse --abbrev-ref HEAD | xargs git fetch --prune upstream\", \"allow_all\": true, \"shell\": true, \"help\": \"fetch current branch only\" } customize the local/remote relationship coloring displayed by the gita ll command You can see the default color scheme and the available colors via gita color. To change the color coding, use gita color set <situation> <color>. The configuration is saved in $XDG_CONFIG_HOME/gita/color.csv. customize information displayed by the gita ll command You can customize the information displayed by gita ll. The used and unused information items are shown with gita info, and the configuration is saved in $XDG_CONFIG_HOME/gita/info.csv. For example, the default setting corresponds to branch,commit_msg,commit_time customize git command flags One can set custom flags to run git commands. For example, with gita flags set my-repo --git-dir=`gita ls dotfiles` --work-tree=$HOME any git command/alias triggered from gita on dotfiles will use these flags. Note that the flags are applied immediately after git. For example, gita st dotfiles translates to git --git-dir=$HOME/somefolder --work-tree=$HOME status running from the dotfiles directory. This feature was originally added to deal with bare repo dotfiles. Requirements Gita requires Python 3.6 or higher, due to the use of f-string and asyncio module. Under the hood, gita uses subprocess to run git commands/aliases. Thus the installed git version may matter. I have git 1.8.3.1, 2.17.2, and 2.20.1 on my machines, and their results agree. Tips effect shell command enter <repo> directory cd `gita ls <repo>` delete repos in <group> gita group ll <group> | xargs gita rm Contributing To contribute, you can report/fix bugs request/implement features star/recommend this project Read this article if you have never contribute code to open source project before. Chat room is available on To run tests locally, simply pytest in the source code folder. Note that context should be set as none. More implementation details are in design.md. A step-by-step guide to reproduce this project is here. You can also sponsor me on GitHub. Any amount is appreciated! Other multi-repo tools I haven't tried them but I heard good things about them. myrepos repo ",
          "One thing this project does really well is to start the readme with a screenshot. I open the link, scroll down to the readme, and I immediately see what sort of user interface/experience I will get. Some commenters have noted that Gita is similar to other multi-repo tools, but both Repo and wstool are more effort to evaluate, because their readmes don't have pictures.",
          "Nice, but is there a way to just run any command? I.e. just `gita <optional repo names/paths> <pass entire command line git -C repodir>`. This has advantages in that you don't have to go round via a command file, don't have to keep it synced across machines, don't have to remember what you put in the file etc, and can just use the git syntax which already took long enough to learn by heart :P<p>I've used multiple multiple repository tools and in the end all I happen to use is one (usually versioned) file to store a list of repositories and then a command which just loops over all repos and applies anything to it. If I need custom commands I use git aliases so that works both for normal git and whatever tool used."],
        "story_type":"ShowHN",
        "url_raw":"https://github.com/nosarthur/gita",
        "comments.comment_id":[19075272,
          19075476],
        "comments.comment_author":["fyhn",
          "stinos"],
        "comments.comment_descendants":[1,
          3],
        "comments.comment_time":["2019-02-04T10:43:55Z",
          "2019-02-04T11:34:37Z"],
        "comments.comment_text":["One thing this project does really well is to start the readme with a screenshot. I open the link, scroll down to the readme, and I immediately see what sort of user interface/experience I will get. Some commenters have noted that Gita is similar to other multi-repo tools, but both Repo and wstool are more effort to evaluate, because their readmes don't have pictures.",
          "Nice, but is there a way to just run any command? I.e. just `gita <optional repo names/paths> <pass entire command line git -C repodir>`. This has advantages in that you don't have to go round via a command file, don't have to keep it synced across machines, don't have to remember what you put in the file etc, and can just use the git syntax which already took long enough to learn by heart :P<p>I've used multiple multiple repository tools and in the end all I happen to use is one (usually versioned) file to store a list of repositories and then a command which just loops over all repos and applies anything to it. If I need custom commands I use git aliases so that works both for normal git and whatever tool used."],
        "id":"ed8d9197-eaf1-480e-a6a4-986f10ba95fd",
        "url_text":"_______________________________ ( ____ \\__ __|__ __( ___ ) | ( \\/ ) ( ) ( | ( ) | | | | | | | | (___) | | | ____ | | | | | ___ | | | \\_ ) | | | | | ( ) | | (___) |__) (___ | | | ) ( | (_______)_______/ )_( |/ \\| v0.15 Gita: a command-line tool to manage multiple git repos This tool does two things display the status of multiple git repos such as branch, modification, commit message side by side (batch) delegate git commands/aliases from any working directory If several repos are related, it helps to see their status together. I also hate to change directories to execute git commands. In this screenshot, the gita ll command displays the status of all repos. The gita remote dotfiles command translates to git remote -v for the dotfiles repo, even though we are not in the repo. The gita fetch command fetches from all repos and two of them have updates. To see the pre-defined commands, run gita -h or take a look at cmds.json. To add your own commands, see the customization section. To run arbitrary git command, see the superman mode section. To run arbitrary shell command, see the shell mode section. The branch color distinguishes 5 situations between local and remote branches: color meaning white local has no remote green local is the same as remote red local has diverged from remote purple local is ahead of remote (good for push) yellow local is behind remote (good for merge) The choice of purple for ahead and yellow for behind is motivated by blueshift and redshift, using green as baseline. You can change the color scheme using the gita color command. See the customization section. The additional status symbols denote symbol meaning + staged changes * unstaged changes _ untracked files/folders The bookkeeping sub-commands are gita add <repo-path(s)> [-g <groupname>]: add repo(s) to gita, optionally into an existing group gita add -a <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively and automatically generate hierarchical groups. See the customization section for more details. gita add -b <bare-repo-path(s)>: add bare repo(s) to gita. See the customization section for more details on setting custom worktree. gita add -r <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively gita clone <config-file>: clone repos in config-file (generated by gita freeze) to current directory. gita clone -p <config-file>: clone repos in config-file to prescribed paths. gita context: context sub-command gita context: show current context gita context <group-name>: set context to group-name, all operations then only apply to repos in this group gita context auto: set context automatically according to the current working directory gita context none: remove context gita color: color sub-command gita color [ll]: Show available colors and the current coloring scheme gita color reset: Reset to the default coloring scheme gita color set <situation> <color>: Use the specified color for the local-remote situation gita flags: flags sub-command gita flags set <repo-name> <flags>: add custom flags to repo gita flags [ll]: display repos with custom flags gita freeze: print information of all repos such as URL, name, and path. Use with gita clone. gita group: group sub-command gita group add <repo-name(s)> -n <group-name>: add repo(s) to a new or existing group gita group [ll]: display existing groups with repos gita group ls: display existing group names gita group rename <group-name> <new-name>: change group name gita group rm <group-name(s)>: delete group(s) gita group rmrepo <repo-name(s)> -n <group-name>: remove repo(s) from existing group gita info: info sub-command gita info [ll]: display the used and unused information items gita info add <info-item>: enable information item gita info rm <info-item>: disable information item gita ll: display the status of all repos gita ll <group-name>: display the status of repos in a group gita ll -g: display the repo summaries by groups gita ls: display the names of all repos gita ls <repo-name>: display the absolute path of one repo gita rename <repo-name> <new-name>: rename a repo gita rm <repo-name(s)>: remove repo(s) from gita (won't remove files on disk) gita -v: display gita version The git delegating sub-commands are of two formats gita <sub-command> [repo-name(s) or group-name(s)]: optional repo or group input, and no input means all repos. gita <sub-command> <repo-name(s) or groups-name(s)>: required repo name(s) or group name(s) input They translate to git <sub-command> for the corresponding repos. By default, only fetch and pull take optional input. In other words, gita fetch and gita pull apply to all repos. To see the pre-defined sub-commands, run gita -h or take a look at cmds.json. To add your own sub-commands or override the default behaviors, see the customization section. To run arbitrary git command, see the superman mode section. If more than one repos are specified, the git command runs asynchronously, with the exception of log, difftool and mergetool, which require non-trivial user input. Repo configuration is saved in $XDG_CONFIG_HOME/gita/repos.csv (most likely ~/.config/gita/repos.csv). Installation To install the latest version, run If you prefer development mode, download the source code and run pip3 install -e <gita-source-folder> In either case, calling gita in terminal may not work, then put the following line in the .bashrc file. alias gita=\"python3 -m gita\" Windows users may need to enable the ANSI escape sequence in terminal for the branch color to work. See this stackoverflow post for details. Auto-completion Download .gita-completion.bash or .gita-completion.zsh and source it in shell. Superman mode The superman mode delegates any git command or alias. Usage: gita super [repo-name(s) or group-name(s)] <any-git-command-with-or-without-options> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita super checkout master puts all repos on the master branch gita super frontend-repo backend-repo commit -am 'implement a new feature' executes git commit -am 'implement a new feature' for frontend-repo and backend-repo Shell mode The shell mode delegates any shell command. Usage: gita shell [repo-name(s) or group-name(s)] <any-shell-command> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita shell ll lists contents for all repos gita shell repo1 repo2 mkdir docs create a new directory docs in repo1 and repo2 gita shell \"git describe --abbrev=0 --tags | xargs git checkout\": check out the latest tag for all repos Customization define repo group and context When the project contains several independent but related repos, we can define a group and execute gita command on this group. For example, gita group add repo1 repo2 -n my-group gita ll my-group gita pull my-group To save more typing, one can set a group as context, then any gita command is scoped to the group gita context my-group gita ll gita pull The most useful context maybe auto. In this mode, the context is automatically determined from the current working directory (CWD): the context is the group whose member repo's path contains CWD. To set it, run To remove the context, run It is also possible to recursively add repos within a directory and generate hierarchical groups automatically. For example, running on the following folder structure src project1 repo1 repo2 repo3 project2 repo4 repo5 repo6 gives rise to 3 groups: src:repo1,repo2,repo3,repo4,repo5,repo6 src-project1:repo1,repo2 src-project2:repo4,repo5 add user-defined sub-command using json file Custom delegating sub-commands can be defined in $XDG_CONFIG_HOME/gita/cmds.json (most likely ~/.config/gita/cmds.json) And they shadow the default ones if name collisions exist. Default delegating sub-commands are defined in cmds.json. For example, gita stat <repo-name(s)> is registered as \"stat\":{ \"cmd\": \"git diff --stat\", \"help\": \"show edit statistics\" } which executes git diff --stat for the specified repo(s). To disable asynchronous execution, set disable_async to be true. See the difftool example: \"difftool\":{ \"cmd\": \"git difftool\", \"disable_async\": true, \"help\": \"show differences using a tool\" } If you want a custom command to behave like gita fetch, i.e., to apply to all repos when no repo is specified, set allow_all to be true. For example, the following snippet creates a new command gita comaster [repo-name(s)] with optional repo name input. \"comaster\":{ \"cmd\": \"checkout master\", \"allow_all\": true, \"help\": \"checkout the master branch\" } Any command that runs in the superman mode mode or the shell mode can be defined in this json format. For example, the following command runs in shell mode and fetches only the current branch from upstream. \"fetchcrt\":{ \"cmd\": \"git rev-parse --abbrev-ref HEAD | xargs git fetch --prune upstream\", \"allow_all\": true, \"shell\": true, \"help\": \"fetch current branch only\" } customize the local/remote relationship coloring displayed by the gita ll command You can see the default color scheme and the available colors via gita color. To change the color coding, use gita color set <situation> <color>. The configuration is saved in $XDG_CONFIG_HOME/gita/color.csv. customize information displayed by the gita ll command You can customize the information displayed by gita ll. The used and unused information items are shown with gita info, and the configuration is saved in $XDG_CONFIG_HOME/gita/info.csv. For example, the default setting corresponds to branch,commit_msg,commit_time customize git command flags One can set custom flags to run git commands. For example, with gita flags set my-repo --git-dir=`gita ls dotfiles` --work-tree=$HOME any git command/alias triggered from gita on dotfiles will use these flags. Note that the flags are applied immediately after git. For example, gita st dotfiles translates to git --git-dir=$HOME/somefolder --work-tree=$HOME status running from the dotfiles directory. This feature was originally added to deal with bare repo dotfiles. Requirements Gita requires Python 3.6 or higher, due to the use of f-string and asyncio module. Under the hood, gita uses subprocess to run git commands/aliases. Thus the installed git version may matter. I have git 1.8.3.1, 2.17.2, and 2.20.1 on my machines, and their results agree. Tips effect shell command enter <repo> directory cd `gita ls <repo>` delete repos in <group> gita group ll <group> | xargs gita rm Contributing To contribute, you can report/fix bugs request/implement features star/recommend this project Read this article if you have never contribute code to open source project before. Chat room is available on To run tests locally, simply pytest in the source code folder. Note that context should be set as none. More implementation details are in design.md. A step-by-step guide to reproduce this project is here. You can also sponsor me on GitHub. Any amount is appreciated! Other multi-repo tools I haven't tried them but I heard good things about them. myrepos repo ",
        "_version_":1718938144242401281},
      {
        "story_id":19738327,
        "story_author":"ariehkovler",
        "story_descendants":88,
        "story_score":75,
        "story_time":"2019-04-24T13:55:08Z",
        "story_title":"We need a new generation of source control",
        "search":["We need a new generation of source control",
          "Normal",
          "https://www.rookout.com/cant-git-no-satisfaction-why-we-need-a-new-gen-source-control/",
          "By: | January 5, 2019Liran is the Co-Founder and CTO of Rookout. Hes an Observability and Instrumentation expert with a deep understanding of Java, Python, Node, and C++. Liran has broad experience in cybersecurity and compliance from his past roles. When not coding, you can find Liran hosting his podcast, speaking at conferences, writing about his tech adventures, and trying out the local cuisine when traveling.Remember the good old days of enterprise software? When everything had to be installed on-premises? To install an application, youd have to set up a big, vertically scalable server. You would then have to execute a single process written in C/C++, Java or .NET. Well, as you know, those days are long gone.Everything has changed with the transition to the cloud and SaaS. Today, instead of comprising a single vertically scalable process, most applications comprise multiple horizontally scalable processes. This model was first pioneered by Googles borg and by Netflix on EC2. Nowadays, though, you no longer have to be a large enterprise to access microservice infrastructures. Kubernetes and serverless have made microservices viable and accessible to even small startups and lone coders.Lets Git down to businessSo where does Git fit into the picture? Git is an excellent match for single-process applications, but it starts to fail when it comes to multi-process applications. This is precisely what gave birth to the endless mono-repo vs. multi-repo flame-wars.Each side of this debate classifies the other as zealous extremists (as only developers can!), but both of them miss the crux of the matter: Git and its accompanying ecosystem are not yet fit for the task of developing modern cloud-native applications.Shots fired: multi-repos suckBefore we dive in, lets answer this: whats great about Git? Its the almighty atomic commit, the groundbreaking (at the time) branching capabilities, and the ever-useful blame. Well, these beloved features all but disappear in a multi-repo setup. Working in multiple repositories comes with significant drawbacks, which is why its not at all surprising that some of the biggest names in the tech world, including Google and Facebook, have gone down the mono-repo path at a huge investment of time and resources.Dependency management in a multi-repo setup is a nightmare. Instead of having everything in a single repository, you end up with repositories pointing to each other using two git features (git submodules and git subtree) and language-specific dependency management such as npm or Maven. The very existence of the many different methods to manage multi-repos is in itself proof that none of these tools are enough on their own. Gits source-of-truth is no longer a single folder on your computer but a mishmash of source providers and various artifactories.In developers everyday work, repository separation becomes an artificial barrier that impacts technological decisions. This creates a Conways Law effect, making early design decisions about component boundaries very hard to change. It also makes large scale refactorings a much trickier business.However, the biggest failure of the multi-repo is cultural. Instead of having all your source code readily available to all developers, they have to jump hurdles to figure out which repo they need and then clone it. These seemingly-small obstacles often become high fences: developers stop reading and updating code in components and repositories that arent directly in their responsibility.With all these engineering, operations and cultural barriers, why doesnt everyone go the mono-repo route?Take no prisoners: mono-repos suck tooOnce youve packed everything into a single repository, figuring out the connections within the repository becomes a challenge. For humans, that can chip away at the original architecture, breaking away useful abstractions and jumbling everything together.For machines, this lack of separation within the repo is even worse. When you push a code change to a repo, automated processes kick in. CI systems build and test the code, and then CD systems deploy it. Sometimes its to a test or staging environment, and sometimes directly to production.There are certain components you will need to build and deploy hundreds of times a day. At the same time, there are other more delicate and mission-critical components. These require human supervision and extra precaution. The problem with mono-repository is that it mixes all of these components into one. More surprising is the fact that todays vast Git CI ecosystem, with its impressive offerings in both the hosted and the SaaS space, doesnt even try to tackle the issue. In fact, not only will Git CI tools rebuild and redeploy your entire repo, they are often built explicitly for multi-repo projects.Another issue is large repository sizes. Git doesnt handle large repos gracefully. You can easily end up with repo sizes that dont fit in your hard-drive, or clone time that ends up in the hours. For big projects, this requires careful management and pruning of commit history. It is also essential to avoid committing dependencies, auto-generated files and other large files which may be necessary for specific scenarios.Is there still hope for multi-repos?There are new tools that seek to bring some of the benefits of mono-repos to multi-repos. These tools try to set up a configuration that would unite multiple repos under a single umbrella/abstraction layer, thus making managing multiple-repositories easier for example, TwoSigmas Git-meta, mateodelnortes meta, gitslave ,and a bunch of others.These tools bring back a bit of sanity into the complexities of managing multi-repos, reducing some of the toil and error-prone manual operations. But none of them truly give back the control and power of a single Git repo.You cant have your cake and Git it tooThe downsides of multi-repos are real. You cant deny the value of a (truly) single source of truth, (truly) atomic commits, and a (truly) single place to develop and collaborate. On the other hand, none of the downsides of mono-repos are inherent. All of them are related to the current implementation of the Git source control tool itself and its accompanying eco-system, especially CI/CD tools.Its time for a new generation of source control that wasnt purely designed for open-source projects, C and the Linux kernel. A source control designed for delivering modern applications in a polyglot cloud-native world. One that embraces code dependencies and helps the engineering team define and manage them, rather than scaring them away. A source control that treats CI, CD, and releases as first-class citizens, rather than relying on the very useful add-ons provided by GitHub and its community. ",
          "Are we mistaking a dependency control problem as a revision control problem?<p>In a previous life, before microservices, CI/CD etc. existed, we did just fine with 20-30 CVS repositories, each representing a separate component (a running process) in a very large distributed system.<p>The only difference was that we did not have to marshal a large number of 3rd party dependencies that were constantly undergoing version changes. We basically relied on C++, the standard template library and a tightly version controlled set of internal libraries with a single stable version shared across the entire org. The whole system would have been between 750,000 - 1,000,000 lines of code (libraries included).<p>I'm not saying that that's the right approach. But it's mind boggling for me that we can't solve this problem easily anymore.",
          "The source control system is not the piece of the equation that matters to most people.  The build system is the important part.  That's what prevents you from rebuilding the repository when you only change one Kubernetes config file, or what causes 100 docker images to be built because you changed a file in libc.<p>I think the tooling around this is fairly limited right now.  I feel that most people are hoping docker caches stuff intelligently, which it doesn't.  People should probably be using Bazel, but language support is hit-or-miss and it's very complicated.  (It's aggravated by the fact that every language now considers itself responsible for building its own code.  go \"just works\", which is great, but it's hard to translate that local caching to something that can be spread among multiple build workers.  Bazel attempts to make all that work, but it basically has to start from scratch, which is unfortunate.  It also means that you can't just start using some crazy new language unless you want to now support it in the build system.  We all hate Makefiles, but the whole \"foo.c becomes foo.o\" model was much more straightforward than what languages do today.)"],
        "story_type":"Normal",
        "url_raw":"https://www.rookout.com/cant-git-no-satisfaction-why-we-need-a-new-gen-source-control/",
        "comments.comment_id":[19739585,
          19739587],
        "comments.comment_author":["hliyan",
          "jrockway"],
        "comments.comment_descendants":[5,
          0],
        "comments.comment_time":["2019-04-24T15:48:25Z",
          "2019-04-24T15:48:36Z"],
        "comments.comment_text":["Are we mistaking a dependency control problem as a revision control problem?<p>In a previous life, before microservices, CI/CD etc. existed, we did just fine with 20-30 CVS repositories, each representing a separate component (a running process) in a very large distributed system.<p>The only difference was that we did not have to marshal a large number of 3rd party dependencies that were constantly undergoing version changes. We basically relied on C++, the standard template library and a tightly version controlled set of internal libraries with a single stable version shared across the entire org. The whole system would have been between 750,000 - 1,000,000 lines of code (libraries included).<p>I'm not saying that that's the right approach. But it's mind boggling for me that we can't solve this problem easily anymore.",
          "The source control system is not the piece of the equation that matters to most people.  The build system is the important part.  That's what prevents you from rebuilding the repository when you only change one Kubernetes config file, or what causes 100 docker images to be built because you changed a file in libc.<p>I think the tooling around this is fairly limited right now.  I feel that most people are hoping docker caches stuff intelligently, which it doesn't.  People should probably be using Bazel, but language support is hit-or-miss and it's very complicated.  (It's aggravated by the fact that every language now considers itself responsible for building its own code.  go \"just works\", which is great, but it's hard to translate that local caching to something that can be spread among multiple build workers.  Bazel attempts to make all that work, but it basically has to start from scratch, which is unfortunate.  It also means that you can't just start using some crazy new language unless you want to now support it in the build system.  We all hate Makefiles, but the whole \"foo.c becomes foo.o\" model was much more straightforward than what languages do today.)"],
        "id":"5704b72c-adbe-495d-a45f-56c235c0e6e6",
        "url_text":"By: | January 5, 2019Liran is the Co-Founder and CTO of Rookout. Hes an Observability and Instrumentation expert with a deep understanding of Java, Python, Node, and C++. Liran has broad experience in cybersecurity and compliance from his past roles. When not coding, you can find Liran hosting his podcast, speaking at conferences, writing about his tech adventures, and trying out the local cuisine when traveling.Remember the good old days of enterprise software? When everything had to be installed on-premises? To install an application, youd have to set up a big, vertically scalable server. You would then have to execute a single process written in C/C++, Java or .NET. Well, as you know, those days are long gone.Everything has changed with the transition to the cloud and SaaS. Today, instead of comprising a single vertically scalable process, most applications comprise multiple horizontally scalable processes. This model was first pioneered by Googles borg and by Netflix on EC2. Nowadays, though, you no longer have to be a large enterprise to access microservice infrastructures. Kubernetes and serverless have made microservices viable and accessible to even small startups and lone coders.Lets Git down to businessSo where does Git fit into the picture? Git is an excellent match for single-process applications, but it starts to fail when it comes to multi-process applications. This is precisely what gave birth to the endless mono-repo vs. multi-repo flame-wars.Each side of this debate classifies the other as zealous extremists (as only developers can!), but both of them miss the crux of the matter: Git and its accompanying ecosystem are not yet fit for the task of developing modern cloud-native applications.Shots fired: multi-repos suckBefore we dive in, lets answer this: whats great about Git? Its the almighty atomic commit, the groundbreaking (at the time) branching capabilities, and the ever-useful blame. Well, these beloved features all but disappear in a multi-repo setup. Working in multiple repositories comes with significant drawbacks, which is why its not at all surprising that some of the biggest names in the tech world, including Google and Facebook, have gone down the mono-repo path at a huge investment of time and resources.Dependency management in a multi-repo setup is a nightmare. Instead of having everything in a single repository, you end up with repositories pointing to each other using two git features (git submodules and git subtree) and language-specific dependency management such as npm or Maven. The very existence of the many different methods to manage multi-repos is in itself proof that none of these tools are enough on their own. Gits source-of-truth is no longer a single folder on your computer but a mishmash of source providers and various artifactories.In developers everyday work, repository separation becomes an artificial barrier that impacts technological decisions. This creates a Conways Law effect, making early design decisions about component boundaries very hard to change. It also makes large scale refactorings a much trickier business.However, the biggest failure of the multi-repo is cultural. Instead of having all your source code readily available to all developers, they have to jump hurdles to figure out which repo they need and then clone it. These seemingly-small obstacles often become high fences: developers stop reading and updating code in components and repositories that arent directly in their responsibility.With all these engineering, operations and cultural barriers, why doesnt everyone go the mono-repo route?Take no prisoners: mono-repos suck tooOnce youve packed everything into a single repository, figuring out the connections within the repository becomes a challenge. For humans, that can chip away at the original architecture, breaking away useful abstractions and jumbling everything together.For machines, this lack of separation within the repo is even worse. When you push a code change to a repo, automated processes kick in. CI systems build and test the code, and then CD systems deploy it. Sometimes its to a test or staging environment, and sometimes directly to production.There are certain components you will need to build and deploy hundreds of times a day. At the same time, there are other more delicate and mission-critical components. These require human supervision and extra precaution. The problem with mono-repository is that it mixes all of these components into one. More surprising is the fact that todays vast Git CI ecosystem, with its impressive offerings in both the hosted and the SaaS space, doesnt even try to tackle the issue. In fact, not only will Git CI tools rebuild and redeploy your entire repo, they are often built explicitly for multi-repo projects.Another issue is large repository sizes. Git doesnt handle large repos gracefully. You can easily end up with repo sizes that dont fit in your hard-drive, or clone time that ends up in the hours. For big projects, this requires careful management and pruning of commit history. It is also essential to avoid committing dependencies, auto-generated files and other large files which may be necessary for specific scenarios.Is there still hope for multi-repos?There are new tools that seek to bring some of the benefits of mono-repos to multi-repos. These tools try to set up a configuration that would unite multiple repos under a single umbrella/abstraction layer, thus making managing multiple-repositories easier for example, TwoSigmas Git-meta, mateodelnortes meta, gitslave ,and a bunch of others.These tools bring back a bit of sanity into the complexities of managing multi-repos, reducing some of the toil and error-prone manual operations. But none of them truly give back the control and power of a single Git repo.You cant have your cake and Git it tooThe downsides of multi-repos are real. You cant deny the value of a (truly) single source of truth, (truly) atomic commits, and a (truly) single place to develop and collaborate. On the other hand, none of the downsides of mono-repos are inherent. All of them are related to the current implementation of the Git source control tool itself and its accompanying eco-system, especially CI/CD tools.Its time for a new generation of source control that wasnt purely designed for open-source projects, C and the Linux kernel. A source control designed for delivering modern applications in a polyglot cloud-native world. One that embraces code dependencies and helps the engineering team define and manage them, rather than scaring them away. A source control that treats CI, CD, and releases as first-class citizens, rather than relying on the very useful add-ons provided by GitHub and its community. ",
        "_version_":1718938172799320064},
      {
        "story_id":21572278,
        "story_author":"timqian",
        "story_descendants":9,
        "story_score":16,
        "story_time":"2019-11-19T12:16:34Z",
        "story_title":"Show HN: I want to write resume in markdown, so I build this tool",
        "search":["Show HN: I want to write resume in markdown, so I build this tool",
          "ShowHN",
          "https://github.com/timqian/resumd",
          "Features Convert to html/pdf (example PDF) Web UI Customizable themes(GitHub theme, timqian.com theme, TUI theme) Easy to share and edit A pure frontend project, your data on your hand Aim To be the best markdown to resume tool Sponsors Your logo here Thanks toast-ui.react-editor Markdown to HTML: showdownjs Demo of showdownjs GitHub styled markdown css: github-markdown-css HTML to PDF: jsPDF Print HTML to PDF in browser Print.js Print page without header and footer Create react app Logo: Picas Logo Font: Ceviche One Change style sheets with JS Previous art Cv-maker: markdown based resume. GitHub; Hacker News disscussions; Online markdown resume editor: DeerResume Markdown-resuem-js: command line tool to turn markdown document into a resume in HTML and PDF. Markdown-resume: another command line tool. TODO i18n: https://codesandbox.io/s/1zxox032q Author timqian ",
          "I did something similar using Pandoc and a dedicated template: <a href=\"https://github.com/john-bokma/resume-pandoc\" rel=\"nofollow\">https://github.com/john-bokma/resume-pandoc</a>",
          "I don't get it. It's a markdown editor, of which there are a bajillion. I don't see anything that's obviously resume specific about it besides the stuff you happened to type into it.<p>what am i missing?"],
        "story_type":"ShowHN",
        "url_raw":"https://github.com/timqian/resumd",
        "url_text":"Features Convert to html/pdf (example PDF) Web UI Customizable themes(GitHub theme, timqian.com theme, TUI theme) Easy to share and edit A pure frontend project, your data on your hand Aim To be the best markdown to resume tool Sponsors Your logo here Thanks toast-ui.react-editor Markdown to HTML: showdownjs Demo of showdownjs GitHub styled markdown css: github-markdown-css HTML to PDF: jsPDF Print HTML to PDF in browser Print.js Print page without header and footer Create react app Logo: Picas Logo Font: Ceviche One Change style sheets with JS Previous art Cv-maker: markdown based resume. GitHub; Hacker News disscussions; Online markdown resume editor: DeerResume Markdown-resuem-js: command line tool to turn markdown document into a resume in HTML and PDF. Markdown-resume: another command line tool. TODO i18n: https://codesandbox.io/s/1zxox032q Author timqian ",
        "comments.comment_id":[21572323,
          21578956],
        "comments.comment_author":["jjjbokma",
          "masukomi"],
        "comments.comment_descendants":[0,
          1],
        "comments.comment_time":["2019-11-19T12:27:21Z",
          "2019-11-19T22:23:33Z"],
        "comments.comment_text":["I did something similar using Pandoc and a dedicated template: <a href=\"https://github.com/john-bokma/resume-pandoc\" rel=\"nofollow\">https://github.com/john-bokma/resume-pandoc</a>",
          "I don't get it. It's a markdown editor, of which there are a bajillion. I don't see anything that's obviously resume specific about it besides the stuff you happened to type into it.<p>what am i missing?"],
        "id":"01ab353c-ccc1-4c16-aeb4-ed210bed0d10",
        "_version_":1718938242631335936}]
  }}
