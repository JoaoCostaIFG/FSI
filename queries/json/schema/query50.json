{
  "responseHeader": {
    "status": 0,
    "QTime": 21
  },
  "response": {
    "numFound": 222,
    "start": 0,
    "numFoundExact": true,
    "docs": [
      {
        "story_id": 20559240,
        "story_author": "mooreds",
        "story_descendants": 7,
        "story_score": 23,
        "story_time": "2019-07-29T20:45:37Z",
        "story_title": "Learn a little jq, Awk and sed",
        "search": [
          "Learn a little jq, Awk and sed",
          "https://letterstoanewdeveloper.com/2019/07/29/learn-a-little-jq-awk-and-sed/",
          "Dear new developer, You are probably going to be dealing with text files sometime during your development career. These could be plain text, csv, or json. They may have data you want to get out, or log files you want to examine. You may be transforming from one format to another. Now, if this is a regular occurrence, you may want to build a script or a program around this problem (or use a third party service which aggregates everything together). But sometimes these files are one offs. Or you use them once in a blue moon. And it can take a little while to write a script, look at the libraries, and put it all together. Another alternative is to learn some of the unix tools available on the command line. Here are three that I consider table stakes. awk This is a multi purpose line processing utility. I often want to grab lines of a log file and figure out what is going on. Heres a few lines of a log file: 54.147.20.92 - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" \"Slackbot 1.0 (+https://api.slack.com/robots)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" If I want to see only the ip addresses (assuming these are all in a file called logs.txt), Id run something like: $ awk '{print $1}' logs.txt 54.147.20.92 185.24.234.106 185.24.234.106 Theres lots more, but you can see that youd be able to slice and dice delimited data pretty easily. Heres a great article which dives in further. sed This is another line utility. You can use it for all kinds of things, but I primarily use it to do search and replace on a file. Suppose you had the same log file, but you wanted to anonymize the the ip address and the user agent. Perhaps youre going to ship them off for long term storage or something. You can easily remove this with a couple of sed commands. $ sed 's/^[^ ]*//' logs.txt |sed 's/\"[^\"]*\"$//' - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" Yes, it looks like line noise, but this is the power of regular expressions. Theyre in every language (though with slight variations) and worth learning. sed gives you the power of regular expressions at the command line for processing files. I dont have a great sed tutorial Ive found, but googling shows a number. jq If you work on the command line with modern software at all, you have encountered json. Its used for configuration files and data transmission. Sometimes you get an array of json and you just want to pick out certain attributes of it. Tools like sed and awk fail at this, because they are used to newlines separating records, not curly braces and commas. Sure, you could use regular expressions to parse simple json, and there are times when Ive done this. But a far better tool is jq. Im not as savvy with this as with the others, but have used it whenever Im dealing with an API that delivers json (which is most modern ones). I can pull the API down with curl (another great tool) and parse it out with jq. I can put these all in a script and have the exploration be repeatable. I did this a few months ago when I was doing some exploration of an elastic search system. I crafted the queries with curl and then used jq to parse out the results so that I could make some sense of this. Yes, I could have done this with a real programming language, but it would have taken longer. I could also have used a gui tool like postman, but then it would not have been replicable. sed and awk should be on every system you run across; jq is non standard, but easy to install. Its worth spending some time getting to know these tools. So next time you are processing a text file and need to extract just a bit of it, reach for sed and awk. Next time you get a hairy json file and you are peering at it, look at jq. I think youll be happy with the result. Sincerely, Dan Published July 29, 2019October 17, 2020 "
        ],
        "story_type": "Normal",
        "url_raw": "https://letterstoanewdeveloper.com/2019/07/29/learn-a-little-jq-awk-and-sed/",
        "comments.comment_id": [20562095, 20562592],
        "comments.comment_author": ["envolt", "magoon"],
        "comments.comment_descendants": [0, 3],
        "comments.comment_time": [
          "2019-07-30T04:14:52Z",
          "2019-07-30T06:42:37Z"
        ],
        "comments.comment_text": [
          "When I first saw someone using zsh (omz), I was awe-struck.<p>Same thing happens to the person sitting next to me when I pipe an output to jq.",
          "however, I find jq not so friendly piping its output to other programs."
        ],
        "id": "3068ae11-af74-41a3-8ab6-00a85aae5ab8",
        "url_text": "Dear new developer, You are probably going to be dealing with text files sometime during your development career. These could be plain text, csv, or json. They may have data you want to get out, or log files you want to examine. You may be transforming from one format to another. Now, if this is a regular occurrence, you may want to build a script or a program around this problem (or use a third party service which aggregates everything together). But sometimes these files are one offs. Or you use them once in a blue moon. And it can take a little while to write a script, look at the libraries, and put it all together. Another alternative is to learn some of the unix tools available on the command line. Here are three that I consider table stakes. awk This is a multi purpose line processing utility. I often want to grab lines of a log file and figure out what is going on. Heres a few lines of a log file: 54.147.20.92 - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" \"Slackbot 1.0 (+https://api.slack.com/robots)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" If I want to see only the ip addresses (assuming these are all in a file called logs.txt), Id run something like: $ awk '{print $1}' logs.txt 54.147.20.92 185.24.234.106 185.24.234.106 Theres lots more, but you can see that youd be able to slice and dice delimited data pretty easily. Heres a great article which dives in further. sed This is another line utility. You can use it for all kinds of things, but I primarily use it to do search and replace on a file. Suppose you had the same log file, but you wanted to anonymize the the ip address and the user agent. Perhaps youre going to ship them off for long term storage or something. You can easily remove this with a couple of sed commands. $ sed 's/^[^ ]*//' logs.txt |sed 's/\"[^\"]*\"$//' - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" Yes, it looks like line noise, but this is the power of regular expressions. Theyre in every language (though with slight variations) and worth learning. sed gives you the power of regular expressions at the command line for processing files. I dont have a great sed tutorial Ive found, but googling shows a number. jq If you work on the command line with modern software at all, you have encountered json. Its used for configuration files and data transmission. Sometimes you get an array of json and you just want to pick out certain attributes of it. Tools like sed and awk fail at this, because they are used to newlines separating records, not curly braces and commas. Sure, you could use regular expressions to parse simple json, and there are times when Ive done this. But a far better tool is jq. Im not as savvy with this as with the others, but have used it whenever Im dealing with an API that delivers json (which is most modern ones). I can pull the API down with curl (another great tool) and parse it out with jq. I can put these all in a script and have the exploration be repeatable. I did this a few months ago when I was doing some exploration of an elastic search system. I crafted the queries with curl and then used jq to parse out the results so that I could make some sense of this. Yes, I could have done this with a real programming language, but it would have taken longer. I could also have used a gui tool like postman, but then it would not have been replicable. sed and awk should be on every system you run across; jq is non standard, but easy to install. Its worth spending some time getting to know these tools. So next time you are processing a text file and need to extract just a bit of it, reach for sed and awk. Next time you get a hairy json file and you are peering at it, look at jq. I think youll be happy with the result. Sincerely, Dan Published July 29, 2019October 17, 2020 ",
        "_version_": 1718536505738133504
      },
      {
        "story_id": 21363121,
        "story_author": "signa11",
        "story_descendants": 102,
        "story_score": 636,
        "story_time": "2019-10-26T11:52:57Z",
        "story_title": "An Illustrated Guide to Useful Command Line Tools",
        "search": [
          "An Illustrated Guide to Useful Command Line Tools",
          "https://www.wezm.net/technical/2019/10/useful-command-line-tools/",
          "Published on Sat, 26 October 2019 Inspired by a similar post by Ben Boyter this a list of useful command line tools that I use. Its not a list of every tool I use. These are tools that are new or typically not part of a standard POSIX command line environment. This post is a living document and will be updated over time. It should be obvious that I have a strong preference for fast tools without a large runtime dependency like Python or node.js. Most of these tools are portable to *BSD, Linux, macOS. Many also work on Windows. For OSes that ship up to date software many are available via the system package repository. Last updated: 31 Oct 2019 About my CLI environment: I use the zsh shell, Pragmata Pro font, and base16 default dark color scheme. My prompt is generated by promptline. Table of Contents Alacritty Terminal emulator alt Find alternate files bat cat with syntax highlighting bb System monitor chars Unicode character search dot Dot files manager dust Disk usage analyser eva Calculator exa Replacement for ls fd Replacement for find hexyl Hex viewer hyperfine Benchmarking tool jq awk/XPath for JSON mdcat Render Markdown in the terminal pass Password manager Podman Docker alternative Restic Encrypted backup tool ripgrep Fast, intelligent grep shotgun Take screenshots skim Fuzzy finder slop Graphical region selection Syncthing Decentralised file synchronisation tig TUI for git titlecase Convert text to title case Universal Ctags Maintained ctags fork watchexec Run commands in response to file system changes z Jump to directories zola Static site compiler Changelog The changelog for this page Alacritty Alacritty is fast terminal emulator. Whilst not strictly a command line tool, it does host everything I do in the command line. It is the terminal emulator in use in all the screenshots on this page. Homepage alt alt is a tool for finding the alternate to a file. E.g. the header for an implementation or the test for an implementation. I use it paired with Neovim to easily toggle between tests and implementation. $ alt app/models/page.rb spec/models/page_spec.rb Homepage bat bat is an alternative to the common (mis)use of cat to print a file to the terminal. It supports syntax highlighting and git integration. Homepage bb bb is system monitor like top. It shows overall CPU and memory usage as well as detailed information per process. Homepage chars chars shows information about Unicode characters matching a search term. Homepage dot dot is a dotfiles manager. It maintains a set of symlinks according to a mappings file. I use it to manage my dotfiles. Homepage dust dust is an alternative du -sh. It calculates the size of a directory tree, printing a summary of the largest items. Homepage exa exa is a replacement for ls with sensible defaults and added features like a tree view, git integration, and optional icons. I have ls aliased to exa in my shell. Homepage eva eva is a command line calculator similar to bc, with syntax highlighting and persistent history. Homepage fd fd is an alternative to find and has a more user friendly command line interface and respects ignore files, like .gitignore. The combination of its speed and ignore file support make it excellent for searching for files in git repositories. Homepage hexyl hexyl is a hex viewer that uses Unicode characters and colour to make the output more readable. Homepage hyperfine hyperfine command line benchmarking tool. It allows you to benchmark commands with warmup and statistical analysis. Homepage jq jq is kind of like awk for JSON. It lets you transform and extract information from JSON documents. Homepage mdcat mdcat renders Markdown files in the terminal. In supported terminals (not Alacritty) links are clickable (without the url being visible like in a web browser) and images are rendered. Homepage pass pass is a password manager that uses GPG to store the passwords. I use it with the passff Firefox extension and Pass for iOS on my phone. Homepage Podman podman is an alternative to Docker that does not require a daemon. Containers are run as the user running Podman so files written into the host dont end up owned by root. The CLI is largely compatible with the docker CLI. Homepage Restic restic is a backup tool that performs client side encryption, de-duplication and supports a variety of local and remote storage backends. Homepage ripgrep ripgrep (rg) recursively searches file trees for content in files matching a regular expression. Its extremely fast, and respects ignore files and binary files by default. Homepage shotgun shotgun is a tool for taking screenshots on X.org based environments. All the screenshots in this post were taken with it. It pairs well with slop. $ shotgun $(slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\") eva.png Homepage skim skim is a fuzzy finder. It can be used to fuzzy match input fed to it. I use it with Neovim and zsh for fuzzy matching file names. Homepage slop slop (Select Operation) presents a UI to select a region of the screen or a window and prints the region to stdout. Works well with shotgun. $ slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\" -i 8389044 -g 1464x1008+291+818 Homepage Syncthing Syncthing is a decentralised file synchronisation tool. Like Dropbox but self hosted and without the need for a central third-party file store. Homepage tig tig is a ncurses TUI for git. Its great for reviewing and staging changes, viewing history and diffs. Homepage titlecase titlecase is a little tool I wrote to format text using a title case format described by John Gruber. It correctly handles punctuation, and words like iPhone. I use it to obtain consistent titles on all my blog posts. $ echo 'an illustrated guide to useful command line tools' | titlecase An Illustrated Guide to Useful Command Line Tools I typically use it from within Neovim where selected text is piped through it in-place. This is done by creating a visual selection and then typing: :!titlecase. Homepage Universal Ctags Universal Ctags is a fork of exuberant ctags that is actively maintained. ctags is used to generate a tags file that vim and other tools can use to navigate to the definition of symbols in files. $ ctags --recurse src Homepage watchexec watchexec is a file and directory watcher that can run commands in response to file-system changes. Handy for auto running tests or restarting a development web server when source files change. # run command on file change $ watchexec -w content cobalt build # kill and restart server on file change $ watchexec -w src -s SIGINT -r 'cargo run' Homepage z z tracks your most used directories and allows you to jump to them with a partial name. Homepage zola zola is a full-featured very fast static site compiler. Homepage Changelog 31 Oct 2019 Add bb, and brief descriptions to the table of contents 28 Oct 2019 Add hyperfine Comments Comments on Lobsters Comments on Hacker News Previous Post: What I Learnt Building a Lobsters TUI in Rust "
        ],
        "story_type": "Normal",
        "url_raw": "https://www.wezm.net/technical/2019/10/useful-command-line-tools/",
        "comments.comment_id": [21363660, 21364159],
        "comments.comment_author": ["aquova", "atonalfreerider"],
        "comments.comment_descendants": [4, 2],
        "comments.comment_time": [
          "2019-10-26T13:52:33Z",
          "2019-10-26T15:18:17Z"
        ],
        "comments.comment_text": [
          "I quite like this list, there are a number of utilities here that I already use on a daily basis. There are also a few utilities that I like that weren't on this list, or some alternatives to what was shown. Some off the top of my head:<p>- nnn[0] (C) - A terminal file manager, similar to ranger. It allows you to navigate directories, manipulate files, analyze disk usage, and fuzzy open files.<p>- ncdu[1] (C) - ncurses disk analyzer. Similar to du -sh, but allows for directory navigation as well.<p>- z.lua[2] (Lua) - It's an alternative to the z utility mentioned in the article. I haven't done the benchmarks, but they claim to be faster than z. I'm mostly just including it because it's what I use.<p>[0] <a href=\"https://github.com/jarun/nnn\" rel=\"nofollow\">https://github.com/jarun/nnn</a>\n[1] <a href=\"https://dev.yorhel.nl/ncdu\" rel=\"nofollow\">https://dev.yorhel.nl/ncdu</a>\n[2] <a href=\"https://github.com/skywind3000/z.lua\" rel=\"nofollow\">https://github.com/skywind3000/z.lua</a>",
          "An observation: I have always taken the meaning of the word \"illustrated\" to specifically refer to non-lexical graphics. Searching the dictionary definition of the word, I find a looser definition that applies to \"examples\" intended to aid an explanation.<p>This is a similar cognitive dissonance to when I first learned that \"Visual Basic\" and \"Visual Studio\" meant that the syntax of the displayed code was highlighted, not graphically represented in a non-lexical way."
        ],
        "id": "8f7092d0-5220-4f32-a924-a8ab72710660",
        "url_text": "Published on Sat, 26 October 2019 Inspired by a similar post by Ben Boyter this a list of useful command line tools that I use. Its not a list of every tool I use. These are tools that are new or typically not part of a standard POSIX command line environment. This post is a living document and will be updated over time. It should be obvious that I have a strong preference for fast tools without a large runtime dependency like Python or node.js. Most of these tools are portable to *BSD, Linux, macOS. Many also work on Windows. For OSes that ship up to date software many are available via the system package repository. Last updated: 31 Oct 2019 About my CLI environment: I use the zsh shell, Pragmata Pro font, and base16 default dark color scheme. My prompt is generated by promptline. Table of Contents Alacritty Terminal emulator alt Find alternate files bat cat with syntax highlighting bb System monitor chars Unicode character search dot Dot files manager dust Disk usage analyser eva Calculator exa Replacement for ls fd Replacement for find hexyl Hex viewer hyperfine Benchmarking tool jq awk/XPath for JSON mdcat Render Markdown in the terminal pass Password manager Podman Docker alternative Restic Encrypted backup tool ripgrep Fast, intelligent grep shotgun Take screenshots skim Fuzzy finder slop Graphical region selection Syncthing Decentralised file synchronisation tig TUI for git titlecase Convert text to title case Universal Ctags Maintained ctags fork watchexec Run commands in response to file system changes z Jump to directories zola Static site compiler Changelog The changelog for this page Alacritty Alacritty is fast terminal emulator. Whilst not strictly a command line tool, it does host everything I do in the command line. It is the terminal emulator in use in all the screenshots on this page. Homepage alt alt is a tool for finding the alternate to a file. E.g. the header for an implementation or the test for an implementation. I use it paired with Neovim to easily toggle between tests and implementation. $ alt app/models/page.rb spec/models/page_spec.rb Homepage bat bat is an alternative to the common (mis)use of cat to print a file to the terminal. It supports syntax highlighting and git integration. Homepage bb bb is system monitor like top. It shows overall CPU and memory usage as well as detailed information per process. Homepage chars chars shows information about Unicode characters matching a search term. Homepage dot dot is a dotfiles manager. It maintains a set of symlinks according to a mappings file. I use it to manage my dotfiles. Homepage dust dust is an alternative du -sh. It calculates the size of a directory tree, printing a summary of the largest items. Homepage exa exa is a replacement for ls with sensible defaults and added features like a tree view, git integration, and optional icons. I have ls aliased to exa in my shell. Homepage eva eva is a command line calculator similar to bc, with syntax highlighting and persistent history. Homepage fd fd is an alternative to find and has a more user friendly command line interface and respects ignore files, like .gitignore. The combination of its speed and ignore file support make it excellent for searching for files in git repositories. Homepage hexyl hexyl is a hex viewer that uses Unicode characters and colour to make the output more readable. Homepage hyperfine hyperfine command line benchmarking tool. It allows you to benchmark commands with warmup and statistical analysis. Homepage jq jq is kind of like awk for JSON. It lets you transform and extract information from JSON documents. Homepage mdcat mdcat renders Markdown files in the terminal. In supported terminals (not Alacritty) links are clickable (without the url being visible like in a web browser) and images are rendered. Homepage pass pass is a password manager that uses GPG to store the passwords. I use it with the passff Firefox extension and Pass for iOS on my phone. Homepage Podman podman is an alternative to Docker that does not require a daemon. Containers are run as the user running Podman so files written into the host dont end up owned by root. The CLI is largely compatible with the docker CLI. Homepage Restic restic is a backup tool that performs client side encryption, de-duplication and supports a variety of local and remote storage backends. Homepage ripgrep ripgrep (rg) recursively searches file trees for content in files matching a regular expression. Its extremely fast, and respects ignore files and binary files by default. Homepage shotgun shotgun is a tool for taking screenshots on X.org based environments. All the screenshots in this post were taken with it. It pairs well with slop. $ shotgun $(slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\") eva.png Homepage skim skim is a fuzzy finder. It can be used to fuzzy match input fed to it. I use it with Neovim and zsh for fuzzy matching file names. Homepage slop slop (Select Operation) presents a UI to select a region of the screen or a window and prints the region to stdout. Works well with shotgun. $ slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\" -i 8389044 -g 1464x1008+291+818 Homepage Syncthing Syncthing is a decentralised file synchronisation tool. Like Dropbox but self hosted and without the need for a central third-party file store. Homepage tig tig is a ncurses TUI for git. Its great for reviewing and staging changes, viewing history and diffs. Homepage titlecase titlecase is a little tool I wrote to format text using a title case format described by John Gruber. It correctly handles punctuation, and words like iPhone. I use it to obtain consistent titles on all my blog posts. $ echo 'an illustrated guide to useful command line tools' | titlecase An Illustrated Guide to Useful Command Line Tools I typically use it from within Neovim where selected text is piped through it in-place. This is done by creating a visual selection and then typing: :!titlecase. Homepage Universal Ctags Universal Ctags is a fork of exuberant ctags that is actively maintained. ctags is used to generate a tags file that vim and other tools can use to navigate to the definition of symbols in files. $ ctags --recurse src Homepage watchexec watchexec is a file and directory watcher that can run commands in response to file-system changes. Handy for auto running tests or restarting a development web server when source files change. # run command on file change $ watchexec -w content cobalt build # kill and restart server on file change $ watchexec -w src -s SIGINT -r 'cargo run' Homepage z z tracks your most used directories and allows you to jump to them with a partial name. Homepage zola zola is a full-featured very fast static site compiler. Homepage Changelog 31 Oct 2019 Add bb, and brief descriptions to the table of contents 28 Oct 2019 Add hyperfine Comments Comments on Lobsters Comments on Hacker News Previous Post: What I Learnt Building a Lobsters TUI in Rust ",
        "_version_": 1718536536273715200
      },
      {
        "story_id": 21572308,
        "story_author": "dragondax",
        "story_descendants": 47,
        "story_score": 98,
        "story_time": "2019-11-19T12:23:01Z",
        "story_title": "Run an Internet Speed Test from the Command Line",
        "search": [
          "Run an Internet Speed Test from the Command Line",
          "https://www.putorius.net/speed-test-command-line.html",
          "We have all used tools like speedtest.net to test upload and download speeds. Whether it was to test the WiFi in that coffee shop (I use my own tether, never unknown hot spots), preparing for a LAN party (do people still do that?), or just a step in troubleshooting, we have all been there. For one reason or another you simply think you are being cheated of bandwidth, so you want independent verification of your speeds. This typically means opening a browser and going to a website to test your connection. But what if you want to run a speed test on a remote server? In this article we will discuss running an internet speed test from the Linux command line, and skipping the browser. There is something about the raw efficiency of the command line that I am really attracted to. As I discussed in the article 5 Command Line Tool to Break Your Dependence on the GUI, I try my best to stay away from the browser. It usually creates an unnecessary distraction. The internet is designed to grab your attention like a laser pointer does to a cat. So lets get started, and figure out one more way to stay away from the GUI. Different Speed Test Packages There are a few different tools you can use to run a speed test from the command line. To make things even more confusing the two most popular share the same exact name, but both use the speedtest.net service. Unofficial Speedtest-CLI Python Script The first one is an independently written Python script that is simple to install and use. It is available in the default repositories for some popular Linux distributions. Pros: Easy to installWide AvailabilityFull list of serversCan specify upload test, download test, or both Cons: Minimal output format optionsNo verbose output option Jump to Installing speedtest-cli Python Script or How to Use speedtest-cli Python Script. Official Ookla Speedtest CLI The second tool is built by Ookla, the people who bring you the speedtest.net website and service. Installing it requires you to add a repo for your package manager. But the maintainers offer simple instructions for installation. Pros: Official release from OoklaMore robust formatting optionsOutput easier to read, better layoutVerbose output availableHas repo making it easy to get updates Cons: Use limited to nearby serversCannot specify download or upload only Jump to Official Ookla Speedtest CLI The Speedtest-cli Python Script This is an easy way to get started running a speed test on the Linux command line. Installing the Speedtest-cli Python Script Simply use your package manager to install the package. Install on Fedora using DNF sudo dnf install speedtest-cli Ubuntu or Debian using APT sudo apt-get install speedtest-cli CentOS/Red Hat 7 / 8 Unfortunately, CentOS does not offer the rpm in their repos. It can still be easily installed. Change to /usr/bin directory to make command available to all users: cd /usr/bin Install dependencies: sudo yum install -y python wget Fetch script from github: sudo wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli Make script executable: sudo chmod +x speedtest-cli Or just copy and paste the whole thing below as a single line: cd /usr/bin; sudo yum install -y python wget && wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli && sudo chmod +x speedtest-cli How to Use the Python Script to Run a Speed Test The most basic usage is to simply run the command. It will automatically select the best server based on ping responses. Speedtest-cli Python Script Options There are several options available to change the default behavior. Here we will outline the most popular options. List Available Speed Test Servers You can use the list option to find a list of available servers to run your test against. At the time of writing this list is pretty extensive with 8829 possible servers. NOTE: The servers are sorted by distance, closest first. [[emailprotected] ~]$ speedtest-cli --list Retrieving speedtest.net configuration 4847) Hotwire Fision (Philadelphia, PA, United States) [10.92 km] 10979) School District of Philadelphia (Philadelphia, PA, United States) [10.92 km] ...OUTPUT TRUNCATED... Specify Specific Server to Test Against Once you have found the server you want to test against, you can use the server <SERVER ID> to select it. The server ID is the first column in the output of the list option above. [[emailprotected] ~]$ speedtest-cli --server 4847 Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by School District of Philadelphia (Philadelphia, PA) [10.92 km]: 25.033 ms Testing download speed.. Download: 384.07 Mbit/s Testing upload speed Upload: 417.93 Mbit/s Only Test Upload or Download Speeds The option is actually designed to exclude a test. But since there are only two options it is effectively the same as selecting only one. To run only the download test, you exclude the upload, and vice versa. [[emailprotected] ~]$ speedtest-cli --no-upload Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by KamaTera INC (New Jersey, NJ) [62.36 km]: 19.785 ms Testing download speed.. Download: 600.89 Mbit/s Skipping upload test Format Output in JSON or CSV You can specify the output format in JSON or CSV. You also have the opton to use CSV with a custom delimiter. This is handy if you are going to use the output in some other script or application. [[emailprotected] ~]$ speedtest-cli --json {\"download\": 597726146.0529929, \"upload\": 562476134.8046777, \"ping\": 17.004, \"server\": {\"url\": \"http://speedtest.us-ny2.kamatera.com:8080/speedtest/upload.php\", \"lat\": \"40.0583\", \"lon\": \"-74.4057\", \"name\": \"New Jersey, NJ\", \"country\": \"United States\", \"cc\": \"US\", \"sponsor\": \"KamaTera INC\", \"id\": \"11612\" ...OUTPUT TRUNCATED... Using CSV with a custom delimiter. The default delimiter is a comma, which is implied by the name CSV. Here we use the csv-delimiter option to change the delimiter to a pipe character. [[emailprotected] ~]$ speedtest-cli --csv --csv-delimiter \"|\" 11612|KamaTera INC|New Jersey, NJ|2019-11-17T14:51:53.636981Z|62.35865439150934|8.546|588013638.8767571|512001168.48230773||x.x.x.x The Official Ookla Speedtest CLI The official Speedtest CLI (Command Line Interface) from Ookla is a little more robust. It has all of the options of the python script and more. There are also several output formats not available with the unofficial python script. Ooklas speedtest is also a little easier on the eyes. It spreads the information out which makes it easier to read and displays a neat little progress bar. A URL you can use to share the results is also displayed by default. Installing the Official Speedtest CLI Install Speedtest CLI on Ubuntu / Debian: The Speedtest CLI from Ookla is supported on Ubuntu (xenial & bionic) and Debian (jessie, stretch, buster). $ sudo apt-get install gnupg1 apt-transport-https dirmngr $ export INSTALL_KEY=379CE192D401AB61 $ export DEB_DISTRO=$(lsb_release -sc) $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys $INSTALL_KEY $ echo \"deb https://ookla.bintray.com/debian ${DEB_DISTRO} main\" | sudo tee /etc/apt/sources.list.d/speedtest.list $ sudo apt-get update $ sudo apt-get install speedtest Install Speedtest CLI on Fedora / Redhat / CentOS: Fedora has moved on to DNF for package management, but is still compatible with YUM. These instructions were tested on Fedora 31, CentOS 7 and Red Hat 8. $ sudo yum install wget $ wget https://bintray.com/ookla/rhel/rpm -O bintray-ookla-rhel.repo $ sudo mv bintray-ookla-rhel.repo /etc/yum.repos.d/ $ sudo yum install speedtest How to Use the Official Speedtest CLI Once installed you can simply call the utility by typing speedtest at the command line. This will give you all the default information that you would see on the web version of speedtest.net. Official Speedtest CLI Options The options available in the official release are more robust. Here we will outline the popular options and how to use them. List Available Speed Test Servers Using the -L (servers) option will give you a list of servers available to run a test against. This option will only show you servers that are nearby. What exactly determines nearby is undefined. But for me it looks like they are staying in the tri-state area (PA, NJ, DE). [[emailprotected] ~]$ speedtest -L Closest servers: ID Name Location Country 4847 Hotwire Fision Philadelphia, PA United States 10979 School District of Philadelphia Philadelphia, PA United States 9840 Comcast New Castle, DE United States 11612 KamaTera INC New Jersey, NJ United States ...OUTPUT TRUNCATED... Optionally, you can use the -o (host) option and specify the FQDN of the server instead of the ID. But oddly, I dont see a way to get the FQDN of the servers on the list. I am guessing this option is available for using a custom server. I havent found a way to list all servers. If you are looking to test against a server on the other side of the country, you will have to find it another way. Select Specific Server to Run Speed Test Against You can use the -s (server-id) option to select a server to use from the list. You must supply the server ID with this option. The server ID is the number in the first column of the list output above. [[emailprotected] ~]$ speedtest -s 4847 Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) Change Unit Used for Speed Output The -u (unit) option can display the speed output in many different formats. Decimal prefix, bits per second: bps, kbps, Mbps, Gbps Decimal prefix, bytes per second: B/s, kB/s, MB/s, GB/s Binary prefix, bits per second: kibps, Mibps, Gibps Binary prefix, bytes per second: kiB/s, MiB/s, GiB/s [[emailprotected] ~]$ speedtest -u MiB/s Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) ISP: Verizon Fios Latency: 10.67 ms ( 0.95 ms jitter) Download: 63.45 MiB/s (data used: 700.2 MiB) ...OUTPUT TRUNCATED... Output Formatting Options The Ookla Speedtest CLI offers decent options for output formats. Human Readable DefaultCSV Comma Separated ValueTSV Tab Separated ValueJSON JavaScript Object NotationJSONL JSON LinesJSON-PRETTY JSON Pretty Printed Here is an example using json-pretty. [[emailprotected] ~]$ speedtest -f json-pretty { \"type\": \"result\", \"timestamp\": \"2019-11-17T16:42:06Z\", \"ping\": { \"jitter\": 0.29899999999999999, \"latency\": 17.474 }, \"download\": { \"bandwidth\": 92184614, \"bytes\": 491967724, \"elapsed\": 5303 }, \"upload\": { \"bandwidth\": 45010100, \"bytes\": 313859035, \"elapsed\": 6714 }, ...OUTPUT TRUNCATED... Conclusion Running a speed test from the command line may not be something that is needed on a daily basis for most people. However, it may prove useful in some troubleshooting situations. In this article we cover how to run a speed test from the command line using two similar tools. The unofficial python script and the official Ookla Speedtest CLI. We discussed installing, using and setting options for each one. This should be enough to get you started. For more information on these tools, visit their respective home pages found in the resources section below. Resources and Links: Ookla Speedtest CLISpeedtest-cli (Python Version) on GitHub "
        ],
        "story_type": "Normal",
        "url_raw": "https://www.putorius.net/speed-test-command-line.html",
        "comments.comment_id": [21582731, 21583805],
        "comments.comment_author": ["dspillett", "virtuallynathan"],
        "comments.comment_descendants": [5, 4],
        "comments.comment_time": [
          "2019-11-20T10:49:21Z",
          "2019-11-20T13:46:04Z"
        ],
        "comments.comment_text": [
          "Note that some ISPs prioritise traffic to known speedtest targets, turning off traffic shaping rules that might otherwise slow bulk transfers. When this happens it means you are testing the likely maximum throughput of your connection not necessarily the throughput you will see more generally.<p>This is why Netflix started fast.com - because it draws data from the same distribution points as their video streaming apps it means you can't prioritise the speedtest without also doing so for the video traffic or (more likely) you can't de-prioritise the video traffic without also getting bad scores in that particular speedtest. From Netflix's point of view it is an answer to people contacting support with \"my speedtest results are fine, the problem must be your servers\" when they are experiencing video lag/drops and other such problems and the issue is due to ISP traffic shaping or the ISP simply not having enough backhaul bandwidth.<p>A more reliable test might be taking part in a busy public torrent: that way you are testing against arbitrary locations so your ISP can't be setting different shaping rules for them. Just remember to throttle upstream when testing downstream and vice-versa or saturation in the other direction will slow control packets that will in turn give you lower results for the one you are testing. This may fall into another trap though: unless you limit the number of active streams it may be an unrealistic test as more generally most processes use a small number of streams (or just a single one), and if you limit the number of streams too much you might get a lower result because each swarm member you connect to may be fairly saturated and sharing its bandwidth amongst many connections.",
          "speedtest-cli is <i>garbage</i> if you have >100Mbps Speeds. The dev refuses to acknowledge this: <a href=\"https://github.com/sivel/speedtest-cli/issues/226\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/226</a><p>Also not just me:\n<a href=\"https://github.com/sivel/speedtest-cli/issues/649\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/649</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/648\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/648</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/641\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/641</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/616\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/616</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/601\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/601</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/588\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/588</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/546\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/546</a>"
        ],
        "id": "83a187cd-64f4-47ea-86f3-7626e5aa293c",
        "url_text": "We have all used tools like speedtest.net to test upload and download speeds. Whether it was to test the WiFi in that coffee shop (I use my own tether, never unknown hot spots), preparing for a LAN party (do people still do that?), or just a step in troubleshooting, we have all been there. For one reason or another you simply think you are being cheated of bandwidth, so you want independent verification of your speeds. This typically means opening a browser and going to a website to test your connection. But what if you want to run a speed test on a remote server? In this article we will discuss running an internet speed test from the Linux command line, and skipping the browser. There is something about the raw efficiency of the command line that I am really attracted to. As I discussed in the article 5 Command Line Tool to Break Your Dependence on the GUI, I try my best to stay away from the browser. It usually creates an unnecessary distraction. The internet is designed to grab your attention like a laser pointer does to a cat. So lets get started, and figure out one more way to stay away from the GUI. Different Speed Test Packages There are a few different tools you can use to run a speed test from the command line. To make things even more confusing the two most popular share the same exact name, but both use the speedtest.net service. Unofficial Speedtest-CLI Python Script The first one is an independently written Python script that is simple to install and use. It is available in the default repositories for some popular Linux distributions. Pros: Easy to installWide AvailabilityFull list of serversCan specify upload test, download test, or both Cons: Minimal output format optionsNo verbose output option Jump to Installing speedtest-cli Python Script or How to Use speedtest-cli Python Script. Official Ookla Speedtest CLI The second tool is built by Ookla, the people who bring you the speedtest.net website and service. Installing it requires you to add a repo for your package manager. But the maintainers offer simple instructions for installation. Pros: Official release from OoklaMore robust formatting optionsOutput easier to read, better layoutVerbose output availableHas repo making it easy to get updates Cons: Use limited to nearby serversCannot specify download or upload only Jump to Official Ookla Speedtest CLI The Speedtest-cli Python Script This is an easy way to get started running a speed test on the Linux command line. Installing the Speedtest-cli Python Script Simply use your package manager to install the package. Install on Fedora using DNF sudo dnf install speedtest-cli Ubuntu or Debian using APT sudo apt-get install speedtest-cli CentOS/Red Hat 7 / 8 Unfortunately, CentOS does not offer the rpm in their repos. It can still be easily installed. Change to /usr/bin directory to make command available to all users: cd /usr/bin Install dependencies: sudo yum install -y python wget Fetch script from github: sudo wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli Make script executable: sudo chmod +x speedtest-cli Or just copy and paste the whole thing below as a single line: cd /usr/bin; sudo yum install -y python wget && wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli && sudo chmod +x speedtest-cli How to Use the Python Script to Run a Speed Test The most basic usage is to simply run the command. It will automatically select the best server based on ping responses. Speedtest-cli Python Script Options There are several options available to change the default behavior. Here we will outline the most popular options. List Available Speed Test Servers You can use the list option to find a list of available servers to run your test against. At the time of writing this list is pretty extensive with 8829 possible servers. NOTE: The servers are sorted by distance, closest first. [[emailprotected] ~]$ speedtest-cli --list Retrieving speedtest.net configuration 4847) Hotwire Fision (Philadelphia, PA, United States) [10.92 km] 10979) School District of Philadelphia (Philadelphia, PA, United States) [10.92 km] ...OUTPUT TRUNCATED... Specify Specific Server to Test Against Once you have found the server you want to test against, you can use the server <SERVER ID> to select it. The server ID is the first column in the output of the list option above. [[emailprotected] ~]$ speedtest-cli --server 4847 Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by School District of Philadelphia (Philadelphia, PA) [10.92 km]: 25.033 ms Testing download speed.. Download: 384.07 Mbit/s Testing upload speed Upload: 417.93 Mbit/s Only Test Upload or Download Speeds The option is actually designed to exclude a test. But since there are only two options it is effectively the same as selecting only one. To run only the download test, you exclude the upload, and vice versa. [[emailprotected] ~]$ speedtest-cli --no-upload Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by KamaTera INC (New Jersey, NJ) [62.36 km]: 19.785 ms Testing download speed.. Download: 600.89 Mbit/s Skipping upload test Format Output in JSON or CSV You can specify the output format in JSON or CSV. You also have the opton to use CSV with a custom delimiter. This is handy if you are going to use the output in some other script or application. [[emailprotected] ~]$ speedtest-cli --json {\"download\": 597726146.0529929, \"upload\": 562476134.8046777, \"ping\": 17.004, \"server\": {\"url\": \"http://speedtest.us-ny2.kamatera.com:8080/speedtest/upload.php\", \"lat\": \"40.0583\", \"lon\": \"-74.4057\", \"name\": \"New Jersey, NJ\", \"country\": \"United States\", \"cc\": \"US\", \"sponsor\": \"KamaTera INC\", \"id\": \"11612\" ...OUTPUT TRUNCATED... Using CSV with a custom delimiter. The default delimiter is a comma, which is implied by the name CSV. Here we use the csv-delimiter option to change the delimiter to a pipe character. [[emailprotected] ~]$ speedtest-cli --csv --csv-delimiter \"|\" 11612|KamaTera INC|New Jersey, NJ|2019-11-17T14:51:53.636981Z|62.35865439150934|8.546|588013638.8767571|512001168.48230773||x.x.x.x The Official Ookla Speedtest CLI The official Speedtest CLI (Command Line Interface) from Ookla is a little more robust. It has all of the options of the python script and more. There are also several output formats not available with the unofficial python script. Ooklas speedtest is also a little easier on the eyes. It spreads the information out which makes it easier to read and displays a neat little progress bar. A URL you can use to share the results is also displayed by default. Installing the Official Speedtest CLI Install Speedtest CLI on Ubuntu / Debian: The Speedtest CLI from Ookla is supported on Ubuntu (xenial & bionic) and Debian (jessie, stretch, buster). $ sudo apt-get install gnupg1 apt-transport-https dirmngr $ export INSTALL_KEY=379CE192D401AB61 $ export DEB_DISTRO=$(lsb_release -sc) $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys $INSTALL_KEY $ echo \"deb https://ookla.bintray.com/debian ${DEB_DISTRO} main\" | sudo tee /etc/apt/sources.list.d/speedtest.list $ sudo apt-get update $ sudo apt-get install speedtest Install Speedtest CLI on Fedora / Redhat / CentOS: Fedora has moved on to DNF for package management, but is still compatible with YUM. These instructions were tested on Fedora 31, CentOS 7 and Red Hat 8. $ sudo yum install wget $ wget https://bintray.com/ookla/rhel/rpm -O bintray-ookla-rhel.repo $ sudo mv bintray-ookla-rhel.repo /etc/yum.repos.d/ $ sudo yum install speedtest How to Use the Official Speedtest CLI Once installed you can simply call the utility by typing speedtest at the command line. This will give you all the default information that you would see on the web version of speedtest.net. Official Speedtest CLI Options The options available in the official release are more robust. Here we will outline the popular options and how to use them. List Available Speed Test Servers Using the -L (servers) option will give you a list of servers available to run a test against. This option will only show you servers that are nearby. What exactly determines nearby is undefined. But for me it looks like they are staying in the tri-state area (PA, NJ, DE). [[emailprotected] ~]$ speedtest -L Closest servers: ID Name Location Country 4847 Hotwire Fision Philadelphia, PA United States 10979 School District of Philadelphia Philadelphia, PA United States 9840 Comcast New Castle, DE United States 11612 KamaTera INC New Jersey, NJ United States ...OUTPUT TRUNCATED... Optionally, you can use the -o (host) option and specify the FQDN of the server instead of the ID. But oddly, I dont see a way to get the FQDN of the servers on the list. I am guessing this option is available for using a custom server. I havent found a way to list all servers. If you are looking to test against a server on the other side of the country, you will have to find it another way. Select Specific Server to Run Speed Test Against You can use the -s (server-id) option to select a server to use from the list. You must supply the server ID with this option. The server ID is the number in the first column of the list output above. [[emailprotected] ~]$ speedtest -s 4847 Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) Change Unit Used for Speed Output The -u (unit) option can display the speed output in many different formats. Decimal prefix, bits per second: bps, kbps, Mbps, Gbps Decimal prefix, bytes per second: B/s, kB/s, MB/s, GB/s Binary prefix, bits per second: kibps, Mibps, Gibps Binary prefix, bytes per second: kiB/s, MiB/s, GiB/s [[emailprotected] ~]$ speedtest -u MiB/s Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) ISP: Verizon Fios Latency: 10.67 ms ( 0.95 ms jitter) Download: 63.45 MiB/s (data used: 700.2 MiB) ...OUTPUT TRUNCATED... Output Formatting Options The Ookla Speedtest CLI offers decent options for output formats. Human Readable DefaultCSV Comma Separated ValueTSV Tab Separated ValueJSON JavaScript Object NotationJSONL JSON LinesJSON-PRETTY JSON Pretty Printed Here is an example using json-pretty. [[emailprotected] ~]$ speedtest -f json-pretty { \"type\": \"result\", \"timestamp\": \"2019-11-17T16:42:06Z\", \"ping\": { \"jitter\": 0.29899999999999999, \"latency\": 17.474 }, \"download\": { \"bandwidth\": 92184614, \"bytes\": 491967724, \"elapsed\": 5303 }, \"upload\": { \"bandwidth\": 45010100, \"bytes\": 313859035, \"elapsed\": 6714 }, ...OUTPUT TRUNCATED... Conclusion Running a speed test from the command line may not be something that is needed on a daily basis for most people. However, it may prove useful in some troubleshooting situations. In this article we cover how to run a speed test from the command line using two similar tools. The unofficial python script and the official Ookla Speedtest CLI. We discussed installing, using and setting options for each one. This should be enough to get you started. For more information on these tools, visit their respective home pages found in the resources section below. Resources and Links: Ookla Speedtest CLISpeedtest-cli (Python Version) on GitHub ",
        "_version_": 1718536544026886144
      },
      {
        "story_id": 20811829,
        "story_author": "weinzierl",
        "story_descendants": 59,
        "story_score": 273,
        "story_time": "2019-08-27T16:54:20Z",
        "story_title": "Curl exercises",
        "search": [
          "Curl exercises",
          "https://jvns.ca/blog/2019/08/27/curl-exercises/",
          "Recently Ive been interested in how people learn things. I was reading Kathy Sierras great book Badass: Making Users Awesome. It talks about the idea of deliberate practice. The idea is that you find a small micro-skill that can be learned in maybe 3 sessions of 45 minutes, and focus on learning that micro-skill. So, as an exercise, I was trying to think of a computer skill that I thought could be learned in 3 45-minute sessions. I thought that making HTTP requests with curl might be a skill like that, so here are some curl exercises as an experiment! whats curl? curl is a command line tool for making HTTP requests. I like it because its an easy way to test that servers or APIs are doing what I think, but its a little confusing at first! Heres a drawing explaining curls most important command line arguments (which is page 6 of my Bite Size Networking zine). You can click to make it bigger. fluency is valuable With any command line tool, I think having fluency is really helpful. Its really nice to be able to just type in the thing you need. For example recently I was testing out the Gumroad API and I was able to just type in: curl https://api.gumroad.com/v2/sales \\ -d \"access_token=<SECRET>\" \\ -X GET -d \"before=2016-09-03\" and get things working from the command line. 21 curl exercises These exercises are about understanding how to make different kinds of HTTP requests with curl. Theyre a little repetitive on purpose. They exercise basically everything I do with curl. To keep it simple, were going to make a lot of our requests to the same website: https://httpbin.org. httpbin is a service that accepts HTTP requests and then tells you what request you made. Request https://httpbin.org Request https://httpbin.org/anything. httpbin.org/anything will look at the request you made, parse it, and echo back to you what you requested. curls default is to make a GET request. Make a POST request to https://httpbin.org/anything Make a GET request to https://httpbin.org/anything, but this time add some query parameters (set value=panda). Request googles robots.txt file (www.google.com/robots.txt) Make a GET request to https://httpbin.org/anything and set the header User-Agent: elephant. Make a DELETE request to https://httpbin.org/anything Request https://httpbin.org/anything and also get the response headers Make a POST request to https://httpbin.org/anything with the JSON body {\"value\": \"panda\"} Make the same POST request as the previous exercise, but set the Content-Type header to application/json (because POST requests need to have a content type that matches their body). Look at the json field in the response to see the difference from the previous one. Make a GET request to https://httpbin.org/anything and set the header Accept-Encoding: gzip (what happens? why?) Put a bunch of a JSON in a file and then make a POST request to https://httpbin.org/anything with the JSON in that file as the body Make a request to https://httpbin.org/image and set the header Accept: image/png. Save the output to a PNG file and open the file in an image viewer. Try the same thing with different Accept: headers. Make a PUT request to https://httpbin.org/anything Request https://httpbin.org/image/jpeg, save it to a file, and open that file in your image editor. Request https://www.twitter.com. Youll get an empty response. Get curl to show you the response headers too, and try to figure out why the response was empty. Make any request to https://httpbin.org/anything and just set some nonsense headers (like panda: elephant) Request https://httpbin.org/status/404 and https://httpbin.org/status/200. Request them again and get curl to show the response headers. Request https://httpbin.org/anything and set a username and password (with -u username:password) Download the Twitter homepage (https://twitter.com) in Spanish by setting the Accept-Language: es-ES header. Make a request to the Stripe API with curl. (see https://stripe.com/docs/development for how, they give you a test API key). Try making exactly the same request to https://httpbin.org/anything. "
        ],
        "story_type": "Normal",
        "url_raw": "https://jvns.ca/blog/2019/08/27/curl-exercises/",
        "url_text": "Recently Ive been interested in how people learn things. I was reading Kathy Sierras great book Badass: Making Users Awesome. It talks about the idea of deliberate practice. The idea is that you find a small micro-skill that can be learned in maybe 3 sessions of 45 minutes, and focus on learning that micro-skill. So, as an exercise, I was trying to think of a computer skill that I thought could be learned in 3 45-minute sessions. I thought that making HTTP requests with curl might be a skill like that, so here are some curl exercises as an experiment! whats curl? curl is a command line tool for making HTTP requests. I like it because its an easy way to test that servers or APIs are doing what I think, but its a little confusing at first! Heres a drawing explaining curls most important command line arguments (which is page 6 of my Bite Size Networking zine). You can click to make it bigger. fluency is valuable With any command line tool, I think having fluency is really helpful. Its really nice to be able to just type in the thing you need. For example recently I was testing out the Gumroad API and I was able to just type in: curl https://api.gumroad.com/v2/sales \\ -d \"access_token=<SECRET>\" \\ -X GET -d \"before=2016-09-03\" and get things working from the command line. 21 curl exercises These exercises are about understanding how to make different kinds of HTTP requests with curl. Theyre a little repetitive on purpose. They exercise basically everything I do with curl. To keep it simple, were going to make a lot of our requests to the same website: https://httpbin.org. httpbin is a service that accepts HTTP requests and then tells you what request you made. Request https://httpbin.org Request https://httpbin.org/anything. httpbin.org/anything will look at the request you made, parse it, and echo back to you what you requested. curls default is to make a GET request. Make a POST request to https://httpbin.org/anything Make a GET request to https://httpbin.org/anything, but this time add some query parameters (set value=panda). Request googles robots.txt file (www.google.com/robots.txt) Make a GET request to https://httpbin.org/anything and set the header User-Agent: elephant. Make a DELETE request to https://httpbin.org/anything Request https://httpbin.org/anything and also get the response headers Make a POST request to https://httpbin.org/anything with the JSON body {\"value\": \"panda\"} Make the same POST request as the previous exercise, but set the Content-Type header to application/json (because POST requests need to have a content type that matches their body). Look at the json field in the response to see the difference from the previous one. Make a GET request to https://httpbin.org/anything and set the header Accept-Encoding: gzip (what happens? why?) Put a bunch of a JSON in a file and then make a POST request to https://httpbin.org/anything with the JSON in that file as the body Make a request to https://httpbin.org/image and set the header Accept: image/png. Save the output to a PNG file and open the file in an image viewer. Try the same thing with different Accept: headers. Make a PUT request to https://httpbin.org/anything Request https://httpbin.org/image/jpeg, save it to a file, and open that file in your image editor. Request https://www.twitter.com. Youll get an empty response. Get curl to show you the response headers too, and try to figure out why the response was empty. Make any request to https://httpbin.org/anything and just set some nonsense headers (like panda: elephant) Request https://httpbin.org/status/404 and https://httpbin.org/status/200. Request them again and get curl to show the response headers. Request https://httpbin.org/anything and set a username and password (with -u username:password) Download the Twitter homepage (https://twitter.com) in Spanish by setting the Accept-Language: es-ES header. Make a request to the Stripe API with curl. (see https://stripe.com/docs/development for how, they give you a test API key). Try making exactly the same request to https://httpbin.org/anything. ",
        "comments.comment_id": [20813303, 20813591],
        "comments.comment_author": ["AndrejKolar", "UI_at_80x24"],
        "comments.comment_descendants": [1, 2],
        "comments.comment_time": [
          "2019-08-27T19:03:29Z",
          "2019-08-27T19:33:10Z"
        ],
        "comments.comment_text": [
          "I really like using HTTPie (<a href=\"https://httpie.org\" rel=\"nofollow\">https://httpie.org</a>). Much friendlier syntax then curl, formats the output...<p>Also works great with fx (<a href=\"https://github.com/antonmedv/fx\" rel=\"nofollow\">https://github.com/antonmedv/fx</a>). Just pipe the output from HTTPie and you get easy json processing.",
          "This is more SysAdmin related, but one power-curl function I use atleast 30 times a day is this alias I have in my .bash_aliases<p>This will output the HTTP status code for a given URL.<p><pre><code>     alias hstat=\"curl -o /dev/null --silent --head --write-out '%{http_code}\\n'\" $1  \n</code></pre>\nExample:<p><pre><code>  $ hstat google.com\n  301\n</code></pre>\nI also use curl as an 'uptime monitor' by adding onto that code section.  (a file with a list of URLs, and \"if http status code !=200\" then email me.)<p>There are variations on this all over the place but I really depend on it and I like it."
        ],
        "id": "c5511fe6-8738-4287-8464-3db62119691f",
        "_version_": 1718536516130570240
      },
      {
        "story_id": 19954195,
        "story_author": "rumcajz",
        "story_descendants": 39,
        "story_score": 126,
        "story_time": "2019-05-19T16:52:59Z",
        "story_title": "Show HN: UXY – adding structure to Unix tools",
        "search": [
          "Show HN: UXY – adding structure to Unix tools",
          "https://github.com/sustrik/uxy",
          "Treating everything as a string is the way through which the great power and versatility of UNIX tools is achieved. However, sometimes the constant parsing of strings gets a bit cumbersome. UXY is a tool to manipulate UXY format, which is basically a two-dimensional table that's both human- and machine-readable. The format is deliberately designed to be as similar to the output of standard tools, such as ls or ps, as possible. UXY tool also wraps some common UNIX tools and exports their output in UXY format. Along with converters from/to other common data formats (e.g. JSON) it is meant to allow for quick and painless access to the data. Examples $ uxy ls TYPE PERMISSIONS LINKS OWNER GROUP SIZE TIME NAME - rw-r--r-- 1 martin martin 3204 2019-05-25T15:44:46.371308721+02:00 README.md - rwxr-xr-x 1 martin martin 25535 2019-05-25T16:29:28.518397541+02:00 uxy $ uxy ls | uxy fmt \"NAME SIZE\" NAME SIZE README.md 7451 uxy 11518 $ uxy ls | uxy fmt \"NAME SIZE\" | uxy align NAME SIZE README.md 7451 uxy 11518 $ uxy top | uxy fmt \"PID CPU COMMAND\" | uxy to-json [ { \"PID\": \"4704\", \"CPU\": \"12.5\", \"COMMAND\": \"top\" }, { \"PID\": \"2903\", \"CPU\": \"6.2\", \"COMMAND\": \"Web Content\" }, { \"PID\": \"1\", \"CPU\": \"0.0\", \"COMMAND\": \"systemd\" } ] $ uxy ls | uxy grep test NAME TYPE PERMISSIONS LINKS OWNER GROUP SIZE TIME NAME - rw-r--r-- 1 martin martin 45 2019-05-25T16:09:58.755551983+02:00 test.csv - rw-r--r-- 1 martin martin 84 2019-05-25T16:09:58.755552856+02:00 test.txt - rw-r--r-- 1 martin martin 75 2019-05-25T16:09:58.755559998+02:00 test.uxy $ uxy ps | uxy to-json | jq '.[].CMD' \"bash\" \"uxy\" \"uxy\" \"jq\" \"ps\" $ cat test.csv NAME,TIME Quasimodo,14:30 Moby Dick,14:22 $ cat test.csv | uxy from-csv | uxy align NAME TIME Quasimodo 14:30 \"Moby Dick\" 14:22 TOOLS UXY tools All UXY tools take input from stdin and write the result to stdout. The tools follow the Postel's principle: \"Be liberal in what you accept, conservative in what you output.\" They accept any UXY input, but they try to align the fields in the output to make it more convenient to read. uxy align uxy from-csv uxy from-json uxy from-yaml uxy grep uxy import uxy fmt uxy to-csv uxy to-json uxy to-yaml uxy trim Wrapped UNIX tools Any argument that could be passed to the original tool can also be passed to the UXY-wrapped version of the tool. The exception are the arguments that modify how the output looks like. UXY manages those arguments itself. The only control you have over the output is to either print the default (short) set of result fields (mostly defined as \"the most useful info that fits on page\") or long set of result fields (\"all the information UXY was able to extract\"): $ uxy -l ps When running with -l option it often happens that the output exceeds the terminal width, gets wrapped and unreadable. In such cases you can either filter out just the fields you are intersed in using fmt subcommand or convert the result to YAML (uxy to-yaml) which happens to render each field on a separate line: $ uxy -l ifconfig | uxy fmt \"NAME INET-ADDR\" NAME INET-ADDR enp0s31f6 \"\" lo 127.0.0.1 wlp3s0 192.168.1.7 $ uxy -l ifconfig | uxy to-yaml - ETHER-ADDR: e4:42:a6:f4:1d:02 FLAGS: UP,BROADCAST,RUNNING,MULTICAST INET-ADDR: 192.168.1.7 INET-NETMASK: 255.255.255.0 INET6-ADDR: fe80::fd53:a17f:12ce:38a8 INET6-PREFIXLEN: '64' INET6-SCOPEID: 0x20 ... uxy du uxy ifconfig uxy ls uxy lsof uxy netstat uxy ps uxy top uxy w TESTING To test, run ./test script. "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/sustrik/uxy",
        "url_text": "Treating everything as a string is the way through which the great power and versatility of UNIX tools is achieved. However, sometimes the constant parsing of strings gets a bit cumbersome. UXY is a tool to manipulate UXY format, which is basically a two-dimensional table that's both human- and machine-readable. The format is deliberately designed to be as similar to the output of standard tools, such as ls or ps, as possible. UXY tool also wraps some common UNIX tools and exports their output in UXY format. Along with converters from/to other common data formats (e.g. JSON) it is meant to allow for quick and painless access to the data. Examples $ uxy ls TYPE PERMISSIONS LINKS OWNER GROUP SIZE TIME NAME - rw-r--r-- 1 martin martin 3204 2019-05-25T15:44:46.371308721+02:00 README.md - rwxr-xr-x 1 martin martin 25535 2019-05-25T16:29:28.518397541+02:00 uxy $ uxy ls | uxy fmt \"NAME SIZE\" NAME SIZE README.md 7451 uxy 11518 $ uxy ls | uxy fmt \"NAME SIZE\" | uxy align NAME SIZE README.md 7451 uxy 11518 $ uxy top | uxy fmt \"PID CPU COMMAND\" | uxy to-json [ { \"PID\": \"4704\", \"CPU\": \"12.5\", \"COMMAND\": \"top\" }, { \"PID\": \"2903\", \"CPU\": \"6.2\", \"COMMAND\": \"Web Content\" }, { \"PID\": \"1\", \"CPU\": \"0.0\", \"COMMAND\": \"systemd\" } ] $ uxy ls | uxy grep test NAME TYPE PERMISSIONS LINKS OWNER GROUP SIZE TIME NAME - rw-r--r-- 1 martin martin 45 2019-05-25T16:09:58.755551983+02:00 test.csv - rw-r--r-- 1 martin martin 84 2019-05-25T16:09:58.755552856+02:00 test.txt - rw-r--r-- 1 martin martin 75 2019-05-25T16:09:58.755559998+02:00 test.uxy $ uxy ps | uxy to-json | jq '.[].CMD' \"bash\" \"uxy\" \"uxy\" \"jq\" \"ps\" $ cat test.csv NAME,TIME Quasimodo,14:30 Moby Dick,14:22 $ cat test.csv | uxy from-csv | uxy align NAME TIME Quasimodo 14:30 \"Moby Dick\" 14:22 TOOLS UXY tools All UXY tools take input from stdin and write the result to stdout. The tools follow the Postel's principle: \"Be liberal in what you accept, conservative in what you output.\" They accept any UXY input, but they try to align the fields in the output to make it more convenient to read. uxy align uxy from-csv uxy from-json uxy from-yaml uxy grep uxy import uxy fmt uxy to-csv uxy to-json uxy to-yaml uxy trim Wrapped UNIX tools Any argument that could be passed to the original tool can also be passed to the UXY-wrapped version of the tool. The exception are the arguments that modify how the output looks like. UXY manages those arguments itself. The only control you have over the output is to either print the default (short) set of result fields (mostly defined as \"the most useful info that fits on page\") or long set of result fields (\"all the information UXY was able to extract\"): $ uxy -l ps When running with -l option it often happens that the output exceeds the terminal width, gets wrapped and unreadable. In such cases you can either filter out just the fields you are intersed in using fmt subcommand or convert the result to YAML (uxy to-yaml) which happens to render each field on a separate line: $ uxy -l ifconfig | uxy fmt \"NAME INET-ADDR\" NAME INET-ADDR enp0s31f6 \"\" lo 127.0.0.1 wlp3s0 192.168.1.7 $ uxy -l ifconfig | uxy to-yaml - ETHER-ADDR: e4:42:a6:f4:1d:02 FLAGS: UP,BROADCAST,RUNNING,MULTICAST INET-ADDR: 192.168.1.7 INET-NETMASK: 255.255.255.0 INET6-ADDR: fe80::fd53:a17f:12ce:38a8 INET6-PREFIXLEN: '64' INET6-SCOPEID: 0x20 ... uxy du uxy ifconfig uxy ls uxy lsof uxy netstat uxy ps uxy top uxy w TESTING To test, run ./test script. ",
        "comments.comment_id": [19957013, 19957470],
        "comments.comment_author": ["dima55", "bayareanative"],
        "comments.comment_descendants": [4, 1],
        "comments.comment_time": [
          "2019-05-20T02:12:03Z",
          "2019-05-20T04:14:54Z"
        ],
        "comments.comment_text": [
          "This is becoming a really crowded space. Some other similar tools that make slightly different design choices and that have variable envisioned use cases:<p>- <a href=\"https://github.com/dkogan/vnlog\" rel=\"nofollow\">https://github.com/dkogan/vnlog</a><p>- <a href=\"https://csvkit.readthedocs.io/\" rel=\"nofollow\">https://csvkit.readthedocs.io/</a><p>- <a href=\"https://github.com/johnkerl/miller\" rel=\"nofollow\">https://github.com/johnkerl/miller</a><p>- <a href=\"https://github.com/BurntSushi/xsv\" rel=\"nofollow\">https://github.com/BurntSushi/xsv</a><p>- <a href=\"https://github.com/eBay/tsv-utils-dlang\" rel=\"nofollow\">https://github.com/eBay/tsv-utils-dlang</a><p>- <a href=\"https://stedolan.github.io/jq/\" rel=\"nofollow\">https://stedolan.github.io/jq/</a><p>- <a href=\"http://harelba.github.io/q/\" rel=\"nofollow\">http://harelba.github.io/q/</a><p>- <a href=\"https://github.com/BatchLabs/charlatan\" rel=\"nofollow\">https://github.com/BatchLabs/charlatan</a><p>- <a href=\"https://github.com/dinedal/textql\" rel=\"nofollow\">https://github.com/dinedal/textql</a><p>- <a href=\"https://github.com/dbohdan/sqawk\" rel=\"nofollow\">https://github.com/dbohdan/sqawk</a><p>(disclaimer: vnlog is my tool)",
          "A related problem is the constant churn of logging.. taking structured data, destructuring it with a string serialization and then parsing it again.<p>This resource-wasting antipattern pops up over and over again.<p>Also, logs are message-oriented entries and serializing them as discrete, lengthy files is insane.<p>Structured data should stay structured, say a time-series / log-structured database. Destructuring should be a rare event."
        ],
        "id": "60ab6a08-5f98-487b-a8a3-43e4749f229c",
        "_version_": 1718536484085039104
      },
      {
        "story_id": 21858952,
        "story_author": "pcr910303",
        "story_descendants": 38,
        "story_score": 117,
        "story_time": "2019-12-22T19:07:21Z",
        "story_title": "JSON on the Command Line with Jq",
        "search": [
          "JSON on the Command Line with Jq",
          "https://shapeshed.com/jq-json/",
          "HomePostsContactLast updated Saturday, Nov 16, 2019A series of how to examples on using jq, a command-line JSON processorEstimated reading time: 4 minutesTable of contentsHow to pretty print JSONHow to use pipes with jqHow to find a key and valueHow to find items in an arrayHow to combine filtersHow to transform JSONHow to transform data values within JSONHow to remove keys from JSONHow to map valuesHow to wrangle JSON how you wantFurther readingjq is a fantastic command-line JSON processor. It plays nice with UNIX pipes and offers extensive functionality for interrogating, manipulating and working with JSON file.How to pretty print JSONjq can do a lot but probably the highest frequency use for most users is to pretty print JSON either from a file or after a network call. Suppose that we have a file names.json containing the following json.[{\"id\": 1, \"name\": \"Arthur\", \"age\": \"21\"},{\"id\": 2, \"name\": \"Richard\", \"age\": \"32\"}] jq can pretty print this file using the . filter. This takes the entire input and sends it to standard output. Unless told not to jq will pretty print making JSON readable.jq '.' names.json [ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] How to use pipes with jqBecause jq is UNIX friendly it is possible to pipe data in and out of it. This can be useful for using jq as a filter or interacting with other tools. In the following pipeline cat pipes the file into jq and this is piped onto less. This can be very useful for viewing large JSON files.cat names.json | jq '.' | less How to find a key and valueTo find a key and value jq can filter based on keys and return the value. Suppose we have the following simple JSON document saved as dog.json.{ \"name\": \"Buster\", \"breed\": \"Golden Retriever\", \"age\": \"4\", \"owner\": { \"name\": \"Sally\" }, \"likes\": [ \"bones\", \"balls\", \"dog biscuits\" ] } jq can retrieve values from this document by passing key names.jq '.name' \"Buster\" Multiple keys can by passed separated by commas.jq '.breed,.age' \"Golden Retriever\" \"4\" To search for nested objects chain values using the dot operator just as you would in JavaScript.jq '.owner.name' \"Sally\" How to find items in an arrayTo search for items in arrays use bracket syntax with the index starting at 0.jq '.likes[0]' \"bones\" Multiple elements of an array may also be returned.echo '[\"a\",\"b\",\"c\",\"d\",\"e\"]' | jq '.[2:4]' [ \"c\", \"d\" ] How to combine filtersjq can combine filters to search within a selection. For the following JSON document suppose that the names need to be filtered.[ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] This can be achieved with a pipe with the jq filter.jq '.[] | .name' names.json \"Arthur\" \"Richard\" How to transform JSONjq can be used for more than just reading values from a JSON object. It can also transform JSON into new data structures. Returning to the dog.json example earlier a new array can be created containing the name and likes as follows.jq '[.name, .likes[]]' dog.json [ \"Buster\", \"Bones\", \"balls\", \"dog biscuits\" ] This can be very useful in data transform pipelines to shift JSON data from one structure to another.How to transform data values within JSONjq can also operate on data within JSON objects. Suppose the following JSON file exists and is saved as inventory.json.{ \"eggs\": 2, \"cheese\": 1, \"milk\": 1 } jq can be used to perform basic arithmetic on number values.jq '.eggs + 1' inventory.json 3 How to remove keys from JSONjq can remove keys from JSON objects. This outputs the JSON object without the deleted key. Suppose the following JSON is saved as cheeses.json.{ \"maroilles\": \"stinky\", \"goat\": \"mild\" } jq can remove keys as follows leaving just wonderful stinky cheese.jq 'del(.goat)' cheeses.json { \"maroilles\": \"stinky\" } How to map valuesjq can map values and perform an operation on each one. In the following example each item in an array is mapped and has two subtracted.echo '[12,14,15]' | jq 'map(.-2)' [ 10, 12, 13 ] How to wrangle JSON how you wantjq has many more advanced features to help manipulating and wrangling JSON however you want to. For more run man jq.Further readingjq project pagejq manualjq is sed for JSONbash that JSON (jq)Parsing JSON with jqHave an update or suggestion for this article? You can edit it here and send me a pull request.TagsRecent Posts "
        ],
        "story_type": "Normal",
        "url_raw": "https://shapeshed.com/jq-json/",
        "comments.comment_id": [21859335, 21860107],
        "comments.comment_author": ["dang", "aasasd"],
        "comments.comment_descendants": [0, 5],
        "comments.comment_time": [
          "2019-12-22T20:11:17Z",
          "2019-12-22T22:34:41Z"
        ],
        "comments.comment_text": [
          "Thread from 2016: <a href=\"https://news.ycombinator.com/item?id=13090604\" rel=\"nofollow\">https://news.ycombinator.com/item?id=13090604</a><p>2015: <a href=\"https://news.ycombinator.com/item?id=9446980\" rel=\"nofollow\">https://news.ycombinator.com/item?id=9446980</a><p>2014: <a href=\"https://news.ycombinator.com/item?id=7895076\" rel=\"nofollow\">https://news.ycombinator.com/item?id=7895076</a><p>2013: <a href=\"https://news.ycombinator.com/item?id=5734683\" rel=\"nofollow\">https://news.ycombinator.com/item?id=5734683</a><p>2012: <a href=\"https://news.ycombinator.com/item?id=4985250\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4985250</a><p><a href=\"https://news.ycombinator.com/item?id=4679933\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4679933</a><p>(for the curious)",
          "Every time I try to do something involved with jq (i.e. more than a couple commands deep) and inevitably end up wandering through the manual, I get a persistent feeling that jq could be cleaned up quite a bit. That it either has structure throughout that I fail to grasp, or that it's really rather disorganized and convoluted in places. This is exacerbated by me never being able to find anything in the manual without skimming half of it each single time.<p>E.g.:<p>Filtering and transforming objects by reaching a few levels deeper in them seems to be way easier to slap together in Python's list comprehensions or some functional notation. Stuff like [{ * * x, qwe: x.qwe*2} for x in a.b if x.y.z == 1].<p>Add nested lists to the above, and I can spend another fifteen minutes writing a one-liner (i.e. `a` is a list of objects with lists in some field).<p>I once had to deal with a conditional structure where a field might be one object or a list of them. Hoo boy.<p>Every time I want to check the number of entries my filters print out, I need to wrap them in an array. Repeat this a couple dozen times during writing a complex query. Weirdly, this is where strict structure gets in the way―and while I understand the reasoning, I feel that something could be done for such basic need. (Like, aggregate filters should be able to aggregate instead of acting on individual items? Maybe it's already in the language, but I'm not in the mood to go over the manual again.)<p>Variables seem to be bolted on as an afterthought, or at least the syntax doesn't exactly accommodate them. Meanwhile, they're necessary to implement some filters omitted in the language. Compare that to Lisp's simple `(let)`. IIRC the manual also says that jq has some semblance of functions, but I'm afraid to think about them with this syntax.<p>I like the idea a lot, but execution not so much. Frankly I'll probably end up throwing together a script in Lumo or something, that will accept Lisp expressions and feed JSON structure to them. (I'd use Fennel, but JSON has actual null while Lua... doesn't.)<p>Btw, I have pretty much the same sentiment about Git. Git structures, great. Git tools, oy vey. Maybe I need Lisp for Git, or at least Python for Git."
        ],
        "id": "8f13f462-28e3-469c-b994-29bf671a1fb6",
        "url_text": "HomePostsContactLast updated Saturday, Nov 16, 2019A series of how to examples on using jq, a command-line JSON processorEstimated reading time: 4 minutesTable of contentsHow to pretty print JSONHow to use pipes with jqHow to find a key and valueHow to find items in an arrayHow to combine filtersHow to transform JSONHow to transform data values within JSONHow to remove keys from JSONHow to map valuesHow to wrangle JSON how you wantFurther readingjq is a fantastic command-line JSON processor. It plays nice with UNIX pipes and offers extensive functionality for interrogating, manipulating and working with JSON file.How to pretty print JSONjq can do a lot but probably the highest frequency use for most users is to pretty print JSON either from a file or after a network call. Suppose that we have a file names.json containing the following json.[{\"id\": 1, \"name\": \"Arthur\", \"age\": \"21\"},{\"id\": 2, \"name\": \"Richard\", \"age\": \"32\"}] jq can pretty print this file using the . filter. This takes the entire input and sends it to standard output. Unless told not to jq will pretty print making JSON readable.jq '.' names.json [ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] How to use pipes with jqBecause jq is UNIX friendly it is possible to pipe data in and out of it. This can be useful for using jq as a filter or interacting with other tools. In the following pipeline cat pipes the file into jq and this is piped onto less. This can be very useful for viewing large JSON files.cat names.json | jq '.' | less How to find a key and valueTo find a key and value jq can filter based on keys and return the value. Suppose we have the following simple JSON document saved as dog.json.{ \"name\": \"Buster\", \"breed\": \"Golden Retriever\", \"age\": \"4\", \"owner\": { \"name\": \"Sally\" }, \"likes\": [ \"bones\", \"balls\", \"dog biscuits\" ] } jq can retrieve values from this document by passing key names.jq '.name' \"Buster\" Multiple keys can by passed separated by commas.jq '.breed,.age' \"Golden Retriever\" \"4\" To search for nested objects chain values using the dot operator just as you would in JavaScript.jq '.owner.name' \"Sally\" How to find items in an arrayTo search for items in arrays use bracket syntax with the index starting at 0.jq '.likes[0]' \"bones\" Multiple elements of an array may also be returned.echo '[\"a\",\"b\",\"c\",\"d\",\"e\"]' | jq '.[2:4]' [ \"c\", \"d\" ] How to combine filtersjq can combine filters to search within a selection. For the following JSON document suppose that the names need to be filtered.[ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] This can be achieved with a pipe with the jq filter.jq '.[] | .name' names.json \"Arthur\" \"Richard\" How to transform JSONjq can be used for more than just reading values from a JSON object. It can also transform JSON into new data structures. Returning to the dog.json example earlier a new array can be created containing the name and likes as follows.jq '[.name, .likes[]]' dog.json [ \"Buster\", \"Bones\", \"balls\", \"dog biscuits\" ] This can be very useful in data transform pipelines to shift JSON data from one structure to another.How to transform data values within JSONjq can also operate on data within JSON objects. Suppose the following JSON file exists and is saved as inventory.json.{ \"eggs\": 2, \"cheese\": 1, \"milk\": 1 } jq can be used to perform basic arithmetic on number values.jq '.eggs + 1' inventory.json 3 How to remove keys from JSONjq can remove keys from JSON objects. This outputs the JSON object without the deleted key. Suppose the following JSON is saved as cheeses.json.{ \"maroilles\": \"stinky\", \"goat\": \"mild\" } jq can remove keys as follows leaving just wonderful stinky cheese.jq 'del(.goat)' cheeses.json { \"maroilles\": \"stinky\" } How to map valuesjq can map values and perform an operation on each one. In the following example each item in an array is mapped and has two subtracted.echo '[12,14,15]' | jq 'map(.-2)' [ 10, 12, 13 ] How to wrangle JSON how you wantjq has many more advanced features to help manipulating and wrangling JSON however you want to. For more run man jq.Further readingjq project pagejq manualjq is sed for JSONbash that JSON (jq)Parsing JSON with jqHave an update or suggestion for this article? You can edit it here and send me a pull request.TagsRecent Posts ",
        "_version_": 1718536554487480320
      },
      {
        "story_id": 19427332,
        "story_author": "an-tao",
        "story_descendants": 105,
        "story_score": 189,
        "story_time": "2019-03-19T01:47:40Z",
        "story_title": "Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
        "search": [
          "Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
          "https://github.com/an-tao/drogon",
          "English | | Overview Drogon is a C++14/17-based HTTP application framework. Drogon can be used to easily build various types of web application server programs using C++. Drogon is the name of a dragon in the American TV series \"Game of Thrones\" that I really like. Drogon is a cross-platform framework, It supports Linux, macOS, FreeBSD, OpenBSD, HaikuOS, and Windows. Its main features are as follows: Use a non-blocking I/O network lib based on epoll (kqueue under macOS/FreeBSD) to provide high-concurrency, high-performance network IO, please visit the TFB Tests Results for more details; Provide a completely asynchronous programming mode; Support Http1.0/1.1 (server side and client side); Based on template, a simple reflection mechanism is implemented to completely decouple the main program framework, controllers and views. Support cookies and built-in sessions; Support back-end rendering, the controller generates the data to the view to generate the Html page. Views are described by CSP template files, C++ codes are embedded into Html pages through CSP tags. And the drogon command-line tool automatically generates the C++ code files for compilation; Support view page dynamic loading (dynamic compilation and loading at runtime); Provide a convenient and flexible routing solution from the path to the controller handler; Support filter chains to facilitate the execution of unified logic (such as login verification, Http Method constraint verification, etc.) before handling HTTP requests; Support https (based on OpenSSL); Support WebSocket (server side and client side); Support JSON format request and response, very friendly to the Restful API application development; Support file download and upload; Support gzip, brotli compression transmission; Support pipelining; Provide a lightweight command line tool, drogon_ctl, to simplify the creation of various classes in Drogon and the generation of view code; Support non-blocking I/O based asynchronously reading and writing database (PostgreSQL and MySQL(MariaDB) database); Support asynchronously reading and writing sqlite3 database based on thread pool; Support Redis with asynchronous reading and writing; Support ARM Architecture; Provide a convenient lightweight ORM implementation that supports for regular object-to-database bidirectional mapping; Support plugins which can be installed by the configuration file at load time; Support AOP with build-in joinpoints. Support C++ coroutines A very simple example Unlike most C++ frameworks, the main program of the drogon application can be kept clean and simple. Drogon uses a few tricks to decouple controllers from the main program. The routing settings of controllers can be done through macros or configuration file. Below is the main program of a typical drogon application: #include <drogon/drogon.h> using namespace drogon; int main() { app().setLogPath(\"./\") .setLogLevel(trantor::Logger::kWarn) .addListener(\"0.0.0.0\", 80) .setThreadNum(16) .enableRunAsDaemon() .run(); } It can be further simplified by using configuration file as follows: #include <drogon/drogon.h> using namespace drogon; int main() { app().loadConfigFile(\"./config.json\").run(); } Drogon provides some interfaces for adding controller logic directly in the main() function, for example, user can register a handler like this in Drogon: app().registerHandler(\"/test?username={name}\", [](const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback, const std::string &name) { Json::Value json; json[\"result\"]=\"ok\"; json[\"message\"]=std::string(\"hello,\")+name; auto resp=HttpResponse::newHttpJsonResponse(json); callback(resp); }, {Get,\"LoginFilter\"}); While such interfaces look intuitive, they are not suitable for complex business logic scenarios. Assuming there are tens or even hundreds of handlers that need to be registered in the framework, isn't it a better practice to implement them separately in their respective classes? So unless your logic is very simple, we don't recommend using above interfaces. Instead, we can create an HttpSimpleController as follows: /// The TestCtrl.h file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class TestCtrl:public drogon::HttpSimpleController<TestCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN PATH_ADD(\"/test\",Get); PATH_LIST_END }; /// The TestCtrl.cc file #include \"TestCtrl.h\" void TestCtrl::asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) { //write your application logic here auto resp = HttpResponse::newHttpResponse(); resp->setBody(\"<p>Hello, world!</p>\"); resp->setExpiredTime(0); callback(resp); } Most of the above programs can be automatically generated by the command line tool drogon_ctl provided by drogon (The command is drogon_ctl create controller TestCtrl). All the user needs to do is add their own business logic. In the example, the controller returns a Hello, world! string when the client accesses the http://ip/test URL. For JSON format response, we create the controller as follows: /// The header file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class JsonCtrl : public drogon::HttpSimpleController<JsonCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN //list path definitions here; PATH_ADD(\"/json\", Get); PATH_LIST_END }; /// The source file #include \"JsonCtrl.h\" void JsonCtrl::asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) { Json::Value ret; ret[\"message\"] = \"Hello, World!\"; auto resp = HttpResponse::newHttpJsonResponse(ret); callback(resp); } Let's go a step further and create a demo RESTful API with the HttpController class, as shown below (Omit the source file): /// The header file #pragma once #include <drogon/HttpController.h> using namespace drogon; namespace api { namespace v1 { class User : public drogon::HttpController<User> { public: METHOD_LIST_BEGIN //use METHOD_ADD to add your custom processing function here; METHOD_ADD(User::getInfo, \"/{id}\", Get); //path is /api/v1/User/{arg1} METHOD_ADD(User::getDetailInfo, \"/{id}/detailinfo\", Get); //path is /api/v1/User/{arg1}/detailinfo METHOD_ADD(User::newUser, \"/{name}\", Post); //path is /api/v1/User/{arg1} METHOD_LIST_END //your declaration of processing function maybe like this: void getInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void getDetailInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void newUser(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, std::string &&userName); public: User() { LOG_DEBUG << \"User constructor!\"; } }; } // namespace v1 } // namespace api As you can see, users can use the HttpController to map paths and parameters at the same time. This is a very convenient way to create a RESTful API application. In addition, you can also find that all handler interfaces are in asynchronous mode, where the response is returned by a callback object. This design is for performance reasons because in asynchronous mode the drogon application can handle a large number of concurrent requests with a small number of threads. After compiling all of the above source files, we get a very simple web application. This is a good start. For more information, please visit the wiki or DocsForge Contributions Every contribution is welcome. Please refer to the contribution guidelines for more information. "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/an-tao/drogon",
        "comments.comment_id": [19427737, 19427925],
        "comments.comment_author": ["shakna", "KirinDave"],
        "comments.comment_descendants": [1, 8],
        "comments.comment_time": [
          "2019-03-19T03:06:56Z",
          "2019-03-19T03:45:52Z"
        ],
        "comments.comment_text": [
          "2 small things.<p>a) Show me the code to start with, don't send me digging for it. A SimpleController example as the very first thing would help give a feel for the project, and makes me more likely to consider the project.<p>b) If there's an easier way (like the drogon_ctl utility at the start of Quickstart [0]), show that first, and the more detailed way second.<p>Other than that, it looks great. I've used libmicrohttpd a few times, so a bit less of an overhead always looks great.<p>[0] <a href=\"https://github.com/an-tao/drogon/wiki/quick-start\" rel=\"nofollow\">https://github.com/an-tao/drogon/wiki/quick-start</a>",
          "This is cool, and I like it. Very Haskell like, which is a compliment in my book.<p>But one thing that surprises me is that folks are essentially sleeping on HTTP/2. HTTP/2 is just a hell of a lot better in most every dimension. It's better for handshake latency, it's better for bandwidth in most cases, it's better for eliminating excess SSL overhead and also, it's kinda easier to write client libraries for, because it's so much simpler (although the parallel and concurrent nature of connections will challenge a lot of programmers).<p>It's not bad to see a new contender in this space, but it's surprising that it isn't http/2 first. Is there a good reason for this? It's busted through 90% support on caniuse, so it's hard to make an argument that adoption holds it back."
        ],
        "id": "a112525e-9084-46ed-9e92-8fe23217ccb1",
        "url_text": "English | | Overview Drogon is a C++14/17-based HTTP application framework. Drogon can be used to easily build various types of web application server programs using C++. Drogon is the name of a dragon in the American TV series \"Game of Thrones\" that I really like. Drogon is a cross-platform framework, It supports Linux, macOS, FreeBSD, OpenBSD, HaikuOS, and Windows. Its main features are as follows: Use a non-blocking I/O network lib based on epoll (kqueue under macOS/FreeBSD) to provide high-concurrency, high-performance network IO, please visit the TFB Tests Results for more details; Provide a completely asynchronous programming mode; Support Http1.0/1.1 (server side and client side); Based on template, a simple reflection mechanism is implemented to completely decouple the main program framework, controllers and views. Support cookies and built-in sessions; Support back-end rendering, the controller generates the data to the view to generate the Html page. Views are described by CSP template files, C++ codes are embedded into Html pages through CSP tags. And the drogon command-line tool automatically generates the C++ code files for compilation; Support view page dynamic loading (dynamic compilation and loading at runtime); Provide a convenient and flexible routing solution from the path to the controller handler; Support filter chains to facilitate the execution of unified logic (such as login verification, Http Method constraint verification, etc.) before handling HTTP requests; Support https (based on OpenSSL); Support WebSocket (server side and client side); Support JSON format request and response, very friendly to the Restful API application development; Support file download and upload; Support gzip, brotli compression transmission; Support pipelining; Provide a lightweight command line tool, drogon_ctl, to simplify the creation of various classes in Drogon and the generation of view code; Support non-blocking I/O based asynchronously reading and writing database (PostgreSQL and MySQL(MariaDB) database); Support asynchronously reading and writing sqlite3 database based on thread pool; Support Redis with asynchronous reading and writing; Support ARM Architecture; Provide a convenient lightweight ORM implementation that supports for regular object-to-database bidirectional mapping; Support plugins which can be installed by the configuration file at load time; Support AOP with build-in joinpoints. Support C++ coroutines A very simple example Unlike most C++ frameworks, the main program of the drogon application can be kept clean and simple. Drogon uses a few tricks to decouple controllers from the main program. The routing settings of controllers can be done through macros or configuration file. Below is the main program of a typical drogon application: #include <drogon/drogon.h> using namespace drogon; int main() { app().setLogPath(\"./\") .setLogLevel(trantor::Logger::kWarn) .addListener(\"0.0.0.0\", 80) .setThreadNum(16) .enableRunAsDaemon() .run(); } It can be further simplified by using configuration file as follows: #include <drogon/drogon.h> using namespace drogon; int main() { app().loadConfigFile(\"./config.json\").run(); } Drogon provides some interfaces for adding controller logic directly in the main() function, for example, user can register a handler like this in Drogon: app().registerHandler(\"/test?username={name}\", [](const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback, const std::string &name) { Json::Value json; json[\"result\"]=\"ok\"; json[\"message\"]=std::string(\"hello,\")+name; auto resp=HttpResponse::newHttpJsonResponse(json); callback(resp); }, {Get,\"LoginFilter\"}); While such interfaces look intuitive, they are not suitable for complex business logic scenarios. Assuming there are tens or even hundreds of handlers that need to be registered in the framework, isn't it a better practice to implement them separately in their respective classes? So unless your logic is very simple, we don't recommend using above interfaces. Instead, we can create an HttpSimpleController as follows: /// The TestCtrl.h file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class TestCtrl:public drogon::HttpSimpleController<TestCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN PATH_ADD(\"/test\",Get); PATH_LIST_END }; /// The TestCtrl.cc file #include \"TestCtrl.h\" void TestCtrl::asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) { //write your application logic here auto resp = HttpResponse::newHttpResponse(); resp->setBody(\"<p>Hello, world!</p>\"); resp->setExpiredTime(0); callback(resp); } Most of the above programs can be automatically generated by the command line tool drogon_ctl provided by drogon (The command is drogon_ctl create controller TestCtrl). All the user needs to do is add their own business logic. In the example, the controller returns a Hello, world! string when the client accesses the http://ip/test URL. For JSON format response, we create the controller as follows: /// The header file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class JsonCtrl : public drogon::HttpSimpleController<JsonCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN //list path definitions here; PATH_ADD(\"/json\", Get); PATH_LIST_END }; /// The source file #include \"JsonCtrl.h\" void JsonCtrl::asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) { Json::Value ret; ret[\"message\"] = \"Hello, World!\"; auto resp = HttpResponse::newHttpJsonResponse(ret); callback(resp); } Let's go a step further and create a demo RESTful API with the HttpController class, as shown below (Omit the source file): /// The header file #pragma once #include <drogon/HttpController.h> using namespace drogon; namespace api { namespace v1 { class User : public drogon::HttpController<User> { public: METHOD_LIST_BEGIN //use METHOD_ADD to add your custom processing function here; METHOD_ADD(User::getInfo, \"/{id}\", Get); //path is /api/v1/User/{arg1} METHOD_ADD(User::getDetailInfo, \"/{id}/detailinfo\", Get); //path is /api/v1/User/{arg1}/detailinfo METHOD_ADD(User::newUser, \"/{name}\", Post); //path is /api/v1/User/{arg1} METHOD_LIST_END //your declaration of processing function maybe like this: void getInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void getDetailInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void newUser(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, std::string &&userName); public: User() { LOG_DEBUG << \"User constructor!\"; } }; } // namespace v1 } // namespace api As you can see, users can use the HttpController to map paths and parameters at the same time. This is a very convenient way to create a RESTful API application. In addition, you can also find that all handler interfaces are in asynchronous mode, where the response is returned by a callback object. This design is for performance reasons because in asynchronous mode the drogon application can handle a large number of concurrent requests with a small number of threads. After compiling all of the above source files, we get a very simple web application. This is a good start. For more information, please visit the wiki or DocsForge Contributions Every contribution is welcome. Please refer to the contribution guidelines for more information. ",
        "_version_": 1718536461178896384
      },
      {
        "story_id": 21240641,
        "story_author": "sharkdp",
        "story_descendants": 25,
        "story_score": 96,
        "story_time": "2019-10-13T14:50:12Z",
        "story_title": "Show HN: Hyperfine – a command-line benchmarking tool",
        "search": [
          "Show HN: Hyperfine – a command-line benchmarking tool",
          "https://github.com/sharkdp/hyperfine",
          " A command-line benchmarking tool. Demo: Benchmarking fd and find: Features Statistical analysis across multiple runs. Support for arbitrary shell commands. Constant feedback about the benchmark progress and current estimates. Warmup runs can be executed before the actual benchmark. Cache-clearing commands can be set up before each timing run. Statistical outlier detection to detect interference from other programs and caching effects. Export results to various formats: CSV, JSON, Markdown, AsciiDoc. Parameterized benchmarks (e.g. vary the number of threads). Cross-platform Usage Basic benchmark To run a benchmark, you can simply call hyperfine <command>.... The argument(s) can be any shell command. For example: Hyperfine will automatically determine the number of runs to perform for each command. By default, it will perform at least 10 benchmarking runs. To change this, you can use the -m/--min-runs option: hyperfine --min-runs 5 'sleep 0.2' 'sleep 3.2' Warmup runs and preparation commands If the program execution time is limited by disk I/O, the benchmarking results can be heavily influenced by disk caches and whether they are cold or warm. If you want to run the benchmark on a warm cache, you can use the -w/--warmup option to perform a certain number of program executions before the actual benchmark: hyperfine --warmup 3 'grep -R TODO *' Conversely, if you want to run the benchmark for a cold cache, you can use the -p/--prepare option to run a special command before each timing run. For example, to clear harddisk caches on Linux, you can run sync; echo 3 | sudo tee /proc/sys/vm/drop_caches To use this specific command with Hyperfine, call sudo -v to temporarily gain sudo permissions and then call: hyperfine --prepare 'sync; echo 3 | sudo tee /proc/sys/vm/drop_caches' 'grep -R TODO *' Parameterized benchmarks If you want to run a benchmark where only a single parameter is varied (say, the number of threads), you can use the -P/--parameter-scan option and call: hyperfine --prepare 'make clean' --parameter-scan num_threads 1 12 'make -j {num_threads}' This also works with decimal numbers. The -D/--parameter-step-size option can be used to control the step size: hyperfine --parameter-scan delay 0.3 0.7 -D 0.2 'sleep {delay}' This runs sleep 0.3, sleep 0.5 and sleep 0.7. Shell functions and aliases If you are using bash, you can export shell functions to directly benchmark them with hyperfine: $ my_function() { sleep 1; } $ export -f my_function $ hyperfine my_function If you are using a different shell, or if you want to benchmark shell aliases, you may try to put them in a separate file: echo 'my_function() { sleep 1 }' > /tmp/my_function.sh echo 'alias my_alias=\"sleep 1\"' > /tmp/my_alias.sh hyperfine 'source /tmp/my_function.sh; eval my_function' hyperfine 'source /tmp/my_alias.sh; eval my_alias' Export results Hyperfine has multiple options for exporting benchmark results: CSV, JSON, Markdown (see --help text for details). To export results to Markdown, for example, you can use the --export-markdown option that will create tables like this: Command Mean [s] Min [s] Max [s] Relative find . -iregex '.*[0-9]\\.jpg$' 2.275 0.046 2.243 2.397 9.79 0.22 find . -iname '*[0-9].jpg' 1.427 0.026 1.405 1.468 6.14 0.13 fd -HI '.*[0-9]\\.jpg$' 0.232 0.002 0.230 0.236 1.00 The JSON output is useful if you want to analyze the benchmark results in more detail. See the scripts/ folder for some examples. Installation On Ubuntu Download the appropriate .deb package from the Release page and install it via dpkg: wget https://github.com/sharkdp/hyperfine/releases/download/v1.12.0/hyperfine_1.12.0_amd64.deb sudo dpkg -i hyperfine_1.12.0_amd64.deb On Fedora On Fedora, hyperfine can be installed from the official repositories: On Alpine Linux On Alpine Linux, hyperfine can be installed from the official repositories: On Arch Linux On Arch Linux, hyperfine can be installed from the official repositories: On Funtoo Linux On Funtoo Linux, hyperfine can be installed from core-kit: emerge app-benchmarks/hyperfine On NixOS On NixOS, hyperfine can be installed from the official repositories: On Void Linux Hyperfine can be installed via xbps xbps-install -S hyperfine On macOS Hyperfine can be installed via Homebrew: Or you can install using MacPorts: sudo port selfupdate sudo port install hyperfine On FreeBSD Hyperfine can be installed via pkg: On OpenBSD With conda Hyperfine can be installed via conda from the conda-forge channel: conda install -c conda-forge hyperfine With cargo (Linux, macOS, Windows) Hyperfine can be installed via cargo: Make sure that you use Rust 1.46 or higher. From binaries (Linux, macOS, Windows) Download the corresponding archive from the Release page. Alternative tools Hyperfine is inspired by bench. Integration with other tools Chronologer is a tool that uses hyperfine to visualize changes in benchmark timings across your Git history. Make sure to check out the scripts folder in this repository for a set of tools to work with hyperfine benchmark results. Origin of the name The name hyperfine was chosen in reference to the hyperfine levels of caesium 133 which play a crucial role in the definition of our base unit of time the second. License hyperfine is dual-licensed under the terms of the MIT License and the Apache License 2.0. See the LICENSE-APACHE and LICENSE-MIT files for details. "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/sharkdp/hyperfine",
        "comments.comment_id": [21240684, 21251645],
        "comments.comment_author": ["sharkdp", "kqr"],
        "comments.comment_descendants": [1, 5],
        "comments.comment_time": [
          "2019-10-13T14:57:05Z",
          "2019-10-14T19:09:09Z"
        ],
        "comments.comment_text": [
          "I have submitted \"hyperfine\" 1.5 years ago when it just came out. Since then, the program has gained functionality (statistical outlier detection, result export, parametrized benchmarks) and maturity.<p>Old discussion: <a href=\"https://news.ycombinator.com/item?id=16193225\" rel=\"nofollow\">https://news.ycombinator.com/item?id=16193225</a><p>Looking forward to your feedback!",
          "Most -- nearly all -- benchmarking tools like this work from a normality assumption, i.e. assume that results follow the normal distribution, or is close to it. Some do this on blind faith, others argue from the CLT that \"with infinite samples, the mean is normally distributed, so surely it must be also with finite number of samples, at least a little?\"<p>In fact, performance numbers (latencies) often follow a heavy-tailed distribution. For these, you need a literal shitload of samples to get even a slightly normal mean. For these, the sample mean, the sample variance, the sample centiles -- they all severely underestimate the true values.<p>What's worse is when these tools start to remove \"outliers\". With a heavy-tailed distribution, the majority of samples don't contribute very much at all to the expectation. The strongest signal is found in the extreme values. The strongest signal is found in the stuff that is thrown out. The junk that's left is the noise, the stuff that doesn't tell you very much about what you're dealing with.<p>I stand firm in my belief that unless you can prove how CLT applies to your input distributions, you should not assume normality.<p>And if you don't know what you are doing, stop reporting means. Stop reporting centiles. Report the maximum value. That's a really boring thing to hear, but it is nearly always statistically  and analytically meaningful, so it is a good default."
        ],
        "id": "ff1d53d8-aadf-4db4-8227-4e1893326ee8",
        "url_text": " A command-line benchmarking tool. Demo: Benchmarking fd and find: Features Statistical analysis across multiple runs. Support for arbitrary shell commands. Constant feedback about the benchmark progress and current estimates. Warmup runs can be executed before the actual benchmark. Cache-clearing commands can be set up before each timing run. Statistical outlier detection to detect interference from other programs and caching effects. Export results to various formats: CSV, JSON, Markdown, AsciiDoc. Parameterized benchmarks (e.g. vary the number of threads). Cross-platform Usage Basic benchmark To run a benchmark, you can simply call hyperfine <command>.... The argument(s) can be any shell command. For example: Hyperfine will automatically determine the number of runs to perform for each command. By default, it will perform at least 10 benchmarking runs. To change this, you can use the -m/--min-runs option: hyperfine --min-runs 5 'sleep 0.2' 'sleep 3.2' Warmup runs and preparation commands If the program execution time is limited by disk I/O, the benchmarking results can be heavily influenced by disk caches and whether they are cold or warm. If you want to run the benchmark on a warm cache, you can use the -w/--warmup option to perform a certain number of program executions before the actual benchmark: hyperfine --warmup 3 'grep -R TODO *' Conversely, if you want to run the benchmark for a cold cache, you can use the -p/--prepare option to run a special command before each timing run. For example, to clear harddisk caches on Linux, you can run sync; echo 3 | sudo tee /proc/sys/vm/drop_caches To use this specific command with Hyperfine, call sudo -v to temporarily gain sudo permissions and then call: hyperfine --prepare 'sync; echo 3 | sudo tee /proc/sys/vm/drop_caches' 'grep -R TODO *' Parameterized benchmarks If you want to run a benchmark where only a single parameter is varied (say, the number of threads), you can use the -P/--parameter-scan option and call: hyperfine --prepare 'make clean' --parameter-scan num_threads 1 12 'make -j {num_threads}' This also works with decimal numbers. The -D/--parameter-step-size option can be used to control the step size: hyperfine --parameter-scan delay 0.3 0.7 -D 0.2 'sleep {delay}' This runs sleep 0.3, sleep 0.5 and sleep 0.7. Shell functions and aliases If you are using bash, you can export shell functions to directly benchmark them with hyperfine: $ my_function() { sleep 1; } $ export -f my_function $ hyperfine my_function If you are using a different shell, or if you want to benchmark shell aliases, you may try to put them in a separate file: echo 'my_function() { sleep 1 }' > /tmp/my_function.sh echo 'alias my_alias=\"sleep 1\"' > /tmp/my_alias.sh hyperfine 'source /tmp/my_function.sh; eval my_function' hyperfine 'source /tmp/my_alias.sh; eval my_alias' Export results Hyperfine has multiple options for exporting benchmark results: CSV, JSON, Markdown (see --help text for details). To export results to Markdown, for example, you can use the --export-markdown option that will create tables like this: Command Mean [s] Min [s] Max [s] Relative find . -iregex '.*[0-9]\\.jpg$' 2.275 0.046 2.243 2.397 9.79 0.22 find . -iname '*[0-9].jpg' 1.427 0.026 1.405 1.468 6.14 0.13 fd -HI '.*[0-9]\\.jpg$' 0.232 0.002 0.230 0.236 1.00 The JSON output is useful if you want to analyze the benchmark results in more detail. See the scripts/ folder for some examples. Installation On Ubuntu Download the appropriate .deb package from the Release page and install it via dpkg: wget https://github.com/sharkdp/hyperfine/releases/download/v1.12.0/hyperfine_1.12.0_amd64.deb sudo dpkg -i hyperfine_1.12.0_amd64.deb On Fedora On Fedora, hyperfine can be installed from the official repositories: On Alpine Linux On Alpine Linux, hyperfine can be installed from the official repositories: On Arch Linux On Arch Linux, hyperfine can be installed from the official repositories: On Funtoo Linux On Funtoo Linux, hyperfine can be installed from core-kit: emerge app-benchmarks/hyperfine On NixOS On NixOS, hyperfine can be installed from the official repositories: On Void Linux Hyperfine can be installed via xbps xbps-install -S hyperfine On macOS Hyperfine can be installed via Homebrew: Or you can install using MacPorts: sudo port selfupdate sudo port install hyperfine On FreeBSD Hyperfine can be installed via pkg: On OpenBSD With conda Hyperfine can be installed via conda from the conda-forge channel: conda install -c conda-forge hyperfine With cargo (Linux, macOS, Windows) Hyperfine can be installed via cargo: Make sure that you use Rust 1.46 or higher. From binaries (Linux, macOS, Windows) Download the corresponding archive from the Release page. Alternative tools Hyperfine is inspired by bench. Integration with other tools Chronologer is a tool that uses hyperfine to visualize changes in benchmark timings across your Git history. Make sure to check out the scripts folder in this repository for a set of tools to work with hyperfine benchmark results. Origin of the name The name hyperfine was chosen in reference to the hyperfine levels of caesium 133 which play a crucial role in the definition of our base unit of time the second. License hyperfine is dual-licensed under the terms of the MIT License and the Apache License 2.0. See the LICENSE-APACHE and LICENSE-MIT files for details. ",
        "_version_": 1718536532512473088
      },
      {
        "story_id": 20062064,
        "story_author": "lelf",
        "story_descendants": 25,
        "story_score": 193,
        "story_time": "2019-05-31T15:57:18Z",
        "story_title": "Semantic: Parsing, analyzing, and comparing source code across many languages",
        "search": [
          "Semantic: Parsing, analyzing, and comparing source code across many languages",
          "https://github.com/github/semantic",
          "semantic is a Haskell library and command line tool for parsing, analyzing, and comparing source code. In a hurry? Check out our documentation of example uses for the semantic command line tool. Table of Contents Usage Language support Development Technology and architecture Licensing Usage Run semantic --help for complete list of up-to-date options. Parse Usage: semantic parse [--sexpression | (--json-symbols|--symbols) | --proto-symbols | --show | --quiet] [FILES...] Generate parse trees for path(s) Available options: --sexpression Output s-expression parse trees (default) --json-symbols,--symbols Output JSON symbol list --proto-symbols Output protobufs symbol list --show Output using the Show instance (debug only, format subject to change without notice) --quiet Don't produce output, but show timing stats -h,--help Show this help text Language support Language Parse AST Symbols Stack graphs Ruby JavaScript TypeScript Python Go PHP Java JSON JSX TSX CodeQL Haskell Used for code navigation on github.com. Supported Partial support Under development - N/A Development semantic requires at least GHC 8.10.1 and Cabal 3.0. We strongly recommend using ghcup to sandbox GHC versions, as GHC packages installed through your OS's package manager may not install statically-linked versions of the GHC boot libraries. semantic currently builds only on Unix systems; users of other operating systems may wish to use the Docker images. We use cabal's Nix-style local builds for development. To get started quickly: git clone git@github.com:github/semantic.git cd semantic script/bootstrap cabal v2-build all cabal v2-run semantic:test cabal v2-run semantic:semantic -- --help You can also use the Bazel build system for development. To learn more about Bazel and why it might give you a better development experience, check the build documentation. git clone git@github.com:github/semantic.git cd semantic script/bootstrap-bazel bazel build //... stack as a build tool is not officially supported; there is unofficial stack.yaml support available, though we cannot make guarantees as to its stability. Technology and architecture Architecturally, semantic: Generates per-language Haskell syntax types based on tree-sitter grammar definitions. Reads blobs from a filesystem or provided via a protocol buffer request. Returns blobs or performs analysis. Renders output in one of many supported formats. Throughout its lifestyle, semantic has leveraged a number of interesting algorithms and techniques, including: Myers' algorithm (SES) as described in the paper An O(ND) Difference Algorithm and Its Variations RWS as described in the paper RWS-Diff: Flexible and Efficient Change Detection in Hierarchical Data. Open unions and data types la carte. An implementation of Abstracting Definitional Interpreters extended to work with an la carte representation of syntax terms. Contributions Contributions are welcome! Please see our contribution guidelines and our code of conduct for details on how to participate in our community. Licensing Semantic is licensed under the MIT license. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/github/semantic",
        "url_text": "semantic is a Haskell library and command line tool for parsing, analyzing, and comparing source code. In a hurry? Check out our documentation of example uses for the semantic command line tool. Table of Contents Usage Language support Development Technology and architecture Licensing Usage Run semantic --help for complete list of up-to-date options. Parse Usage: semantic parse [--sexpression | (--json-symbols|--symbols) | --proto-symbols | --show | --quiet] [FILES...] Generate parse trees for path(s) Available options: --sexpression Output s-expression parse trees (default) --json-symbols,--symbols Output JSON symbol list --proto-symbols Output protobufs symbol list --show Output using the Show instance (debug only, format subject to change without notice) --quiet Don't produce output, but show timing stats -h,--help Show this help text Language support Language Parse AST Symbols Stack graphs Ruby JavaScript TypeScript Python Go PHP Java JSON JSX TSX CodeQL Haskell Used for code navigation on github.com. Supported Partial support Under development - N/A Development semantic requires at least GHC 8.10.1 and Cabal 3.0. We strongly recommend using ghcup to sandbox GHC versions, as GHC packages installed through your OS's package manager may not install statically-linked versions of the GHC boot libraries. semantic currently builds only on Unix systems; users of other operating systems may wish to use the Docker images. We use cabal's Nix-style local builds for development. To get started quickly: git clone git@github.com:github/semantic.git cd semantic script/bootstrap cabal v2-build all cabal v2-run semantic:test cabal v2-run semantic:semantic -- --help You can also use the Bazel build system for development. To learn more about Bazel and why it might give you a better development experience, check the build documentation. git clone git@github.com:github/semantic.git cd semantic script/bootstrap-bazel bazel build //... stack as a build tool is not officially supported; there is unofficial stack.yaml support available, though we cannot make guarantees as to its stability. Technology and architecture Architecturally, semantic: Generates per-language Haskell syntax types based on tree-sitter grammar definitions. Reads blobs from a filesystem or provided via a protocol buffer request. Returns blobs or performs analysis. Renders output in one of many supported formats. Throughout its lifestyle, semantic has leveraged a number of interesting algorithms and techniques, including: Myers' algorithm (SES) as described in the paper An O(ND) Difference Algorithm and Its Variations RWS as described in the paper RWS-Diff: Flexible and Efficient Change Detection in Hierarchical Data. Open unions and data types la carte. An implementation of Abstracting Definitional Interpreters extended to work with an la carte representation of syntax terms. Contributions Contributions are welcome! Please see our contribution guidelines and our code of conduct for details on how to participate in our community. Licensing Semantic is licensed under the MIT license. ",
        "comments.comment_id": [20062368, 20062741],
        "comments.comment_author": ["anentropic", "stcredzero"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-05-31T16:27:42Z",
          "2019-05-31T17:01:44Z"
        ],
        "comments.comment_text": [
          "Looks very interesting - would benefit from showing some examples and/or use cases",
          "The late 20th century, early 00's version:<p><a href=\"http://www.program-transformation.org/Transform/CodeCrawler\" rel=\"nofollow\">http://www.program-transformation.org/Transform/CodeCrawler</a><p>(And MOOSE)"
        ],
        "id": "649642de-3037-44fa-a1f9-419d010a2e4d",
        "_version_": 1718536487511785472
      },
      {
        "story_id": 21196177,
        "story_author": "vinnyglennon",
        "story_descendants": 1,
        "story_score": 13,
        "story_time": "2019-10-08T19:29:58Z",
        "story_title": "I wrote a command-line Ruby program to manage EC2 instances for me",
        "search": [
          "I wrote a command-line Ruby program to manage EC2 instances for me",
          "https://www.codewithjason.com/wrote-command-line-ruby-program-manage-ec2-instances/",
          "Why I did this Heroku is great, but not in 100% of cases When I want to quickly deploy a Rails application, my go-to choice is Heroku. Im a big fan of the idea that I can just run heroku create and have a production application online in just a matter of seconds. Unfortunately, Heroku isnt always a desirable option. If Im just messing around, I dont usually want to pay for Heroku features, but I also dont always want my dynos to fall asleep after 30 minutes like on the free tier. (Im aware that there are ways around this but I dont necessarily want to deal with the hassle of all that.) Also, sometimes I want finer control than what Heroku provides. I want to be closer to the metal with the ability to directly manage my EC2 instances, RDS instances, and other AWS services. Sometimes I desire this for cost reasons. Sometimes I just want to learn what I think is the valuable developer skill of knowing how to manage AWS infrastructure. Unfortunately, using AWS by itself isnt very easy. Setting up Rails on bare EC2 is a time-consuming and brain-consuming hassle Getting a Rails app standing up on AWS is pretty hard and time-consuming. Im actually not even going to get into Rails-related stuff in this post because even the small task of getting an EC2 instance up and runningwithout no additional software installed on that instanceis a lot harder than I think it should be, and theres a lot to discuss and improve just inside that step. Just to briefly illustrate what a pain in the ass it is to get an EC2 instance launched and to SSH into it, here are the steps. The steps that follow are the command-line steps. I find the AWS GUI console steps roughly equally painful. 1. Use the AWS CLI create-key-pair command to create a key pair. This step is necessary for later when I want to SSH into my instance. 2. Think of a name for the key pair and save it somewhere. Thinking of a name might seem like a trivially small hurdle, but every tiny bit of mental friction adds up. I dont want to have to think of a name, and I dont want to have to think about where to put the file (even if that means just remembering that I want to put the key in ~/.ssh, which is the most likely case. 3. Use the run-instances command, using an AMI ID (AMI == Amazon Machine Image) and passing in my key name. Now I have to go look up the run-instances (because I sure as hell dont remember it) and, look up my AMI ID, and remember what my key name is. (If you dont know what an AMI ID is, thats what determines whether the instance will be Ubuntu, Amazon Linux, Windows, etc.) 4. Use the describe-instances command to find out the public DNS name of the instance I just launched. This means I either have to search the JSON response of describe-instances for the PublicDnsName entry or apply a filter. Just like with every AWS CLI command, Id have to go look up the exact syntax for this. 5. Run the ssh command, passing in my instances DNS and the path to my key. This step is probably the easiest, although it took me a long time to commit the exact ssh -i syntax to memory. For the record, the command is ssh -i ~/.ssh/my_key.pem ubuntu@mypublicdns.com. Its a small pain in the ass to have to look up the public DNS for my instance again and remember whether my EC2 user is going to be ubuntu or ec2-user (it depends on what AMI I used). My goals for my AWS command-line tool All this fuckery was a big hassle so I decided to write my own command-line tool to manage EC2 instances. I call the tool Exosuit. You can actually try it out yourself by following these instructions. There were four specific capabilities I wanted Exosuit to have. Launch an instance By running bin/exo launch, it should launch an EC2 instance for me. It should assume I want Ubuntu. It should let me know when the instance is ready, and what its instance ID and public DNS are. SSH into an instance I should be able to run bin/exo ssh, get prompted for which instance I want to SSH into, and then get SSHd into that instance. List all running instances I should be able to run bin/exo instances to see all my running instances. It should show the instance ID and public DNS for each. Terminate instances I should be able to run bin/exo terminate which will show me all my instance IDs and allow me to select one or more of them for termination. How I did it Side note: when I first wrote this, I forgot that the AWS SDK for Ruby existed, so I reinvented some wheels. Whoops. After I wrote this I refactored the project to use AWS SDK instead of shell out to AWS CLI. For brevity Ill focus on the bin/exo launch command. Using the AW CLI run-instances command The AWS CLI command for launching an instance looks like this: aws ec2 run-instances \\ --count 1 \\ --image-id ami-05c1fa8df71875112 \\ --instance-type t2.micro \\ --key-name normal-quiet-carrot \\ --profile personal Hopefully most of these flags are self-explanatory. You might wonder where the key name of normal-quiet-carrot came from. When the bin/exo launch command is run, Exosuit asks Is there a file defined at .exosuit/config.yml that contains a key pair name and path? If not, create that file, create a new key pair with a random phrase for a name, and save the name and path to that file. Heres what my .exosuit/config.yml looks like: --- aws_profile_name: personal key_pair: name: normal-quiet-carrot path: \"~/.ssh/normal-quiet-carrot.pem\" The aws_profile_name is something that I imagine most users arent likely to need. I personally happen to have multiple AWS accounts, so its necessary for me to send a --profile flag when using AWS CLI commands so AWS knows which account of mine to use. If a profile isnt specified in .exosuit/config.yml, Exosuit will just leave the --profile flag off and everything will still work fine. Abstracting the run-instances command Once I had coded Exosuit to construct a few different AWS CLI commands (e.g. run-instances, terminate-instances), I noticed that things were getting a little repetitive. Most troubling, I had to always remember to include the --profile flag (just as I would if I were typing all this on the command line manually), and I didnt always remember to do so. In those cases my command would get sent to the wrong account. Thats bad. So I created an abstraction called AWSCommand. Heres what a usage of it looks like: command = AWSCommand.new( :run_instances, count: 1, image_id: IMAGE_ID, instance_type: INSTANCE_TYPE, key_name: key_pair.name ) JSON.parse(command.run) You can probably see the resemblance it bears to the bare run-instances usage. Note the conspicuous absence of the profile flag, which is now automatically included every single time. Listening for launch success One of my least favorite things about manually launching EC2 instances is having to check periodically to see when theyve started running. So I wanted Exosuit to tell me when my EC2 instance was running. I achieved this by writing a loop that hits AWS once per second, checking the state of my new instance each time. module Exosuit def self.launch_instance response = Instance.launch(self.key_pair) instance_id = response['Instances'][0]['InstanceId'] print \"Launching instance #{instance_id}...\" while true sleep(1) print '.' instance = Instance.find(instance_id) if instance && instance.running? puts break end end puts 'Instance is now running' puts \"Public DNS: #{instance.public_dns_name}\" end end You might wonder what Instance.find and instance.running? do. The Instance.find method will run the aws ec2 describe-instances command, parse the JSON response, then grab the relevant JSON data for whatever instance_id I passed to it. The return value is an instance of the Instance class. When an instance of Instance is instantiated, an instance variable gets set (pardon all the instances) with all the JSON data for that instance that was returned by the AWS CLI. The instance.running? method simply looks at that JSON data (which has since been converted to a Ruby hash) and checks to see what the value of ['State']['Name'] is. Heres an abbreviated version of the Instance class for reference. module Exosuit class Instance def initialize(info) @info = info end def state @info['State']['Name'] end def running? state == 'running' end end end (By the way, all the Exosuit code is available on GitHub if youd like to take a look.) Success notification As you can see from the code a couple snippets above, Exosuit lets me know once my instances has entered a running state. At this point I can run bin/exo ssh, bin/exo instances or bin/exo terminate to mess with my instance(s) as I please. Demo video Heres a small sample of Exosuit in action: Try it out yourself If youd like to try out Exosuit, just visit the Getting Started with Exosuit guide. If you think this idea is cool and useful, please let me know by opening a GitHub issue for a feature youd like to see, or tweeting at me, or simply starring the project on GitHub so I can gage interest. I hope you enjoyed this explanation and I look forward to sharing the next steps I take with this project. "
        ],
        "story_type": "Normal",
        "url_raw": "https://www.codewithjason.com/wrote-command-line-ruby-program-manage-ec2-instances/",
        "comments.comment_id": [21196575],
        "comments.comment_author": ["rpmisms"],
        "comments.comment_descendants": [0],
        "comments.comment_time": ["2019-10-08T20:04:40Z"],
        "comments.comment_text": [
          "Dude, thank you so much. This looks like exactly what I need for a project I'm on."
        ],
        "id": "29303460-7239-41eb-b2d0-b298a0118d61",
        "url_text": "Why I did this Heroku is great, but not in 100% of cases When I want to quickly deploy a Rails application, my go-to choice is Heroku. Im a big fan of the idea that I can just run heroku create and have a production application online in just a matter of seconds. Unfortunately, Heroku isnt always a desirable option. If Im just messing around, I dont usually want to pay for Heroku features, but I also dont always want my dynos to fall asleep after 30 minutes like on the free tier. (Im aware that there are ways around this but I dont necessarily want to deal with the hassle of all that.) Also, sometimes I want finer control than what Heroku provides. I want to be closer to the metal with the ability to directly manage my EC2 instances, RDS instances, and other AWS services. Sometimes I desire this for cost reasons. Sometimes I just want to learn what I think is the valuable developer skill of knowing how to manage AWS infrastructure. Unfortunately, using AWS by itself isnt very easy. Setting up Rails on bare EC2 is a time-consuming and brain-consuming hassle Getting a Rails app standing up on AWS is pretty hard and time-consuming. Im actually not even going to get into Rails-related stuff in this post because even the small task of getting an EC2 instance up and runningwithout no additional software installed on that instanceis a lot harder than I think it should be, and theres a lot to discuss and improve just inside that step. Just to briefly illustrate what a pain in the ass it is to get an EC2 instance launched and to SSH into it, here are the steps. The steps that follow are the command-line steps. I find the AWS GUI console steps roughly equally painful. 1. Use the AWS CLI create-key-pair command to create a key pair. This step is necessary for later when I want to SSH into my instance. 2. Think of a name for the key pair and save it somewhere. Thinking of a name might seem like a trivially small hurdle, but every tiny bit of mental friction adds up. I dont want to have to think of a name, and I dont want to have to think about where to put the file (even if that means just remembering that I want to put the key in ~/.ssh, which is the most likely case. 3. Use the run-instances command, using an AMI ID (AMI == Amazon Machine Image) and passing in my key name. Now I have to go look up the run-instances (because I sure as hell dont remember it) and, look up my AMI ID, and remember what my key name is. (If you dont know what an AMI ID is, thats what determines whether the instance will be Ubuntu, Amazon Linux, Windows, etc.) 4. Use the describe-instances command to find out the public DNS name of the instance I just launched. This means I either have to search the JSON response of describe-instances for the PublicDnsName entry or apply a filter. Just like with every AWS CLI command, Id have to go look up the exact syntax for this. 5. Run the ssh command, passing in my instances DNS and the path to my key. This step is probably the easiest, although it took me a long time to commit the exact ssh -i syntax to memory. For the record, the command is ssh -i ~/.ssh/my_key.pem ubuntu@mypublicdns.com. Its a small pain in the ass to have to look up the public DNS for my instance again and remember whether my EC2 user is going to be ubuntu or ec2-user (it depends on what AMI I used). My goals for my AWS command-line tool All this fuckery was a big hassle so I decided to write my own command-line tool to manage EC2 instances. I call the tool Exosuit. You can actually try it out yourself by following these instructions. There were four specific capabilities I wanted Exosuit to have. Launch an instance By running bin/exo launch, it should launch an EC2 instance for me. It should assume I want Ubuntu. It should let me know when the instance is ready, and what its instance ID and public DNS are. SSH into an instance I should be able to run bin/exo ssh, get prompted for which instance I want to SSH into, and then get SSHd into that instance. List all running instances I should be able to run bin/exo instances to see all my running instances. It should show the instance ID and public DNS for each. Terminate instances I should be able to run bin/exo terminate which will show me all my instance IDs and allow me to select one or more of them for termination. How I did it Side note: when I first wrote this, I forgot that the AWS SDK for Ruby existed, so I reinvented some wheels. Whoops. After I wrote this I refactored the project to use AWS SDK instead of shell out to AWS CLI. For brevity Ill focus on the bin/exo launch command. Using the AW CLI run-instances command The AWS CLI command for launching an instance looks like this: aws ec2 run-instances \\ --count 1 \\ --image-id ami-05c1fa8df71875112 \\ --instance-type t2.micro \\ --key-name normal-quiet-carrot \\ --profile personal Hopefully most of these flags are self-explanatory. You might wonder where the key name of normal-quiet-carrot came from. When the bin/exo launch command is run, Exosuit asks Is there a file defined at .exosuit/config.yml that contains a key pair name and path? If not, create that file, create a new key pair with a random phrase for a name, and save the name and path to that file. Heres what my .exosuit/config.yml looks like: --- aws_profile_name: personal key_pair: name: normal-quiet-carrot path: \"~/.ssh/normal-quiet-carrot.pem\" The aws_profile_name is something that I imagine most users arent likely to need. I personally happen to have multiple AWS accounts, so its necessary for me to send a --profile flag when using AWS CLI commands so AWS knows which account of mine to use. If a profile isnt specified in .exosuit/config.yml, Exosuit will just leave the --profile flag off and everything will still work fine. Abstracting the run-instances command Once I had coded Exosuit to construct a few different AWS CLI commands (e.g. run-instances, terminate-instances), I noticed that things were getting a little repetitive. Most troubling, I had to always remember to include the --profile flag (just as I would if I were typing all this on the command line manually), and I didnt always remember to do so. In those cases my command would get sent to the wrong account. Thats bad. So I created an abstraction called AWSCommand. Heres what a usage of it looks like: command = AWSCommand.new( :run_instances, count: 1, image_id: IMAGE_ID, instance_type: INSTANCE_TYPE, key_name: key_pair.name ) JSON.parse(command.run) You can probably see the resemblance it bears to the bare run-instances usage. Note the conspicuous absence of the profile flag, which is now automatically included every single time. Listening for launch success One of my least favorite things about manually launching EC2 instances is having to check periodically to see when theyve started running. So I wanted Exosuit to tell me when my EC2 instance was running. I achieved this by writing a loop that hits AWS once per second, checking the state of my new instance each time. module Exosuit def self.launch_instance response = Instance.launch(self.key_pair) instance_id = response['Instances'][0]['InstanceId'] print \"Launching instance #{instance_id}...\" while true sleep(1) print '.' instance = Instance.find(instance_id) if instance && instance.running? puts break end end puts 'Instance is now running' puts \"Public DNS: #{instance.public_dns_name}\" end end You might wonder what Instance.find and instance.running? do. The Instance.find method will run the aws ec2 describe-instances command, parse the JSON response, then grab the relevant JSON data for whatever instance_id I passed to it. The return value is an instance of the Instance class. When an instance of Instance is instantiated, an instance variable gets set (pardon all the instances) with all the JSON data for that instance that was returned by the AWS CLI. The instance.running? method simply looks at that JSON data (which has since been converted to a Ruby hash) and checks to see what the value of ['State']['Name'] is. Heres an abbreviated version of the Instance class for reference. module Exosuit class Instance def initialize(info) @info = info end def state @info['State']['Name'] end def running? state == 'running' end end end (By the way, all the Exosuit code is available on GitHub if youd like to take a look.) Success notification As you can see from the code a couple snippets above, Exosuit lets me know once my instances has entered a running state. At this point I can run bin/exo ssh, bin/exo instances or bin/exo terminate to mess with my instance(s) as I please. Demo video Heres a small sample of Exosuit in action: Try it out yourself If youd like to try out Exosuit, just visit the Getting Started with Exosuit guide. If you think this idea is cool and useful, please let me know by opening a GitHub issue for a feature youd like to see, or tweeting at me, or simply starring the project on GitHub so I can gage interest. I hope you enjoyed this explanation and I look forward to sharing the next steps I take with this project. ",
        "_version_": 1718536530743525376
      },
      {
        "story_id": 21370525,
        "story_author": "o2sh",
        "story_descendants": 26,
        "story_score": 155,
        "story_time": "2019-10-27T15:57:25Z",
        "story_title": "Git repository summary on your terminal",
        "search": [
          "Git repository summary on your terminal",
          "https://github.com/o2sh/onefetch",
          "A command-line Git information tool written in Rust | | | Onefetch is a command-line Git information tool written in Rust that displays project information and code statistics for a local Git repository directly on your terminal. The tool is completely offline - no network access is required. By default, the repo's information is displayed alongside the dominant language's logo, but you can further configure onefetch to instead use an image - on supported terminals -, a text input or nothing at all. It automatically detects open source licenses from texts and provides the user with valuable information like code distribution, pending changes, number of dependencies (by package manager), top contributors (by number of commits), size on disk, creation date, LOC (lines of code), etc. Onefetch can be configured via command-line flags to display exactly what you want, the way you want it to: you can customize ASCII/Text formatting, disable info lines, ignore files & directories, output in multiple formats (Json, Yaml), etc. As of now, onefetch supports more than 50 different programming languages; if your language of choice isn't supported: Open up an issue and support will be added. Contributions are very welcome! See CONTRIBUTING for more info. More: [Wiki] [Installation] [Getting Started] "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/o2sh/onefetch",
        "url_text": "A command-line Git information tool written in Rust | | | Onefetch is a command-line Git information tool written in Rust that displays project information and code statistics for a local Git repository directly on your terminal. The tool is completely offline - no network access is required. By default, the repo's information is displayed alongside the dominant language's logo, but you can further configure onefetch to instead use an image - on supported terminals -, a text input or nothing at all. It automatically detects open source licenses from texts and provides the user with valuable information like code distribution, pending changes, number of dependencies (by package manager), top contributors (by number of commits), size on disk, creation date, LOC (lines of code), etc. Onefetch can be configured via command-line flags to display exactly what you want, the way you want it to: you can customize ASCII/Text formatting, disable info lines, ignore files & directories, output in multiple formats (Json, Yaml), etc. As of now, onefetch supports more than 50 different programming languages; if your language of choice isn't supported: Open up an issue and support will be added. Contributions are very welcome! See CONTRIBUTING for more info. More: [Wiki] [Installation] [Getting Started] ",
        "comments.comment_id": [21371667, 21372106],
        "comments.comment_author": ["babuskov", "mncharity"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-10-27T18:54:22Z",
          "2019-10-27T20:17:36Z"
        ],
        "comments.comment_text": [
          "Reminds me of Ohloh (now OpenHub) but for your own projects and on command line.",
          "Patterns of commit activity might be shown using unicode sparklines and/or a color scale (foreground and/or background).<p>\"C++ (41.75 %)\" seems a lot of ink and space for \"C++ 42%\"."
        ],
        "id": "9bff0a31-9d45-4f97-9272-05cb934640bb",
        "_version_": 1718536536506499072
      },
      {
        "story_id": 18937195,
        "story_author": "kenshaw",
        "story_descendants": 39,
        "story_score": 73,
        "story_time": "2019-01-18T05:13:09Z",
        "story_title": "Gunk: Modern front end and syntax for Protocol Buffers",
        "search": [
          "Gunk: Modern front end and syntax for Protocol Buffers",
          "https://github.com/gunk/gunk",
          "Gunk is a modern frontend and syntax for Protocol Buffers. Quickstart | Installing | Syntax | Configuring | About | Releases Overview Gunk provides a modern project-based workflow along with a Go-derived syntax for defining types and services for use with Protocol Buffers. Gunk is designed to integrate cleanly with existing protoc based build pipelines, while standardizing workflows in a way that is familiar/accessible to Go developers. Quickstart Create a working directory for a project: $ mkdir -p ~/src/example && cd ~/src/example Install gunk and place the following Gunk definitions in example/util.gunk: package util // Util is a utility service. type Util interface { // Echo returns the passed message. Echo(Message) Message } // Message contains an echo message. type Message struct { // Msg is a message from a client. Msg string `pb:\"1\"` } Create the corresponding project configuration in example/.gunkconfig: [generate go] [generate js] import_style=commonjs binary Then, generate protocol buffer definitions/code: $ ls -A .gunkconfig util.gunk $ gunk generate $ ls -A all.pb.go all_pb.js .gunkconfig util.gunk As seen above, gunk generated the corresponding Go and JavaScript protobuf code using the options defined in the .gunkconfig. End-to-end Example A end-to-end example gRPC server implementation, using Gunk definitions is available for review. Debugging protoc commands Underlying commands executed by gunk can be viewed with the following: $ gunk generate -x protoc-gen-go protoc --js_out=import_style=commonjs,binary:/home/user/example --descriptor_set_in=/dev/stdin all.proto Installing The gunk command-line tool can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the gunk or gunk.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) Installing via Homebrew (macOS) gunk is available in the gunk/gunk tap, and can be installed in the usual way with the brew command: # add tap $ brew tap gunk/gunk # install gunk $ brew install gunk Installing via Scoop (Windows) gunk can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install gunk Installing via Go gunk can be installed in the usual Go fashion: # install gunk $ go get -u github.com/gunk/gunk Protobuf Dependency and Caching The gunk command-line tool uses the protoc command-line tool. gunk can be configured to use protoc at a specified path. If it isn't available, gunk will download the latest protobuf release to the user's cache, for use. It's also possible to pin a specific version, see the section on protoc configuration. Protocol Types and Messages Gunk provides an alternate, Go-derived syntax for defining protocol buffers. As such, Gunk definitions are a subset of the Go programming language. Additionally, a special +gunk annotation is recognized by gunk, to allow the declaration of protocol buffer options: package message import \"github.com/gunk/opt/http\" // Message is a Echo message. type Message struct { // Msg holds a message. Msg string `pb:\"1\" json:\"msg\"` Code int `pb:\"2\" json:\"code\"` } // Util is a utility service. type Util interface { // Echo echoes a message. // // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo(Message) Message } Technically speaking, gunk is not actually strict subset of go, as gunk allows unused imports; it actually requires them for some features. See the example above; in pure go, this would not be a valid go code, as http is not used outside of the comment. Scalars Gunk's Go-derived syntax uses the canonical Go scalar types of the proto3 syntax, defined by the protocol buffer project: Proto3 Type Gunk Type double float64 float float32 int32 int int32 int32 int64 int64 uint32 uint uint32 uint32 uint64 uint64 bool bool string string bytes []byte Note: Variable-length scalars will be enabled in the future using a tag parameter. Messages Gunk's Go-derived syntax uses Go's struct type declarations for declaring messages, and require a pb:\"<field_number>\" tag to indicate the field number: type Message struct { FieldA string `pb:\"1\"` } type Envelope struct { Message Message `pb:\"1\" json:\"msg\"` } There are additional tags (for example, the json: tag above), that will be recognized by gunk format, and passed on to generators, where possible. Note: When using gunk format, a valid pb:\"<field_number>\" tag will be automatically inserted if not declared. Services Gunk's Go-derived syntax uses Go's interface syntax for declaring services: type SearchService interface { Search(SearchRequest) SearchResponse } The above is equivalent to the following protobuf syntax: service SearchService { rpc Search (SearchRequest) returns (SearchResponse); } Enums Gunk's Go-derived syntax uses Go const's for declaring enums: type MyEnum int const ( MYENUM MyEnum = iota MYENUM2 ) Note: values can also be fixed numeric values or a calculated value (using iota). Maps Gunk's Go-derived syntax uses Go map's for declaring map fields: type Project struct { ProjectID string `pb:\"1\" json:\"project_id\"` } type GetProjectResponse struct { Projects map[string]Project `pb:\"1\"` } Repeated Values Gunk's Go-derived syntax uses Go's slice syntax ([]) for declaring a repeated field: type MyMessage struct { FieldA []string `pb:\"1\"` } Message Streams Gunk's Go-derived syntax uses Go chan syntax for declaring streams: type MessageService interface { List(chan Message) chan Message } The above is equivalent to the following protobuf syntax: service MessageService { rpc List(stream Message) returns (stream Message); } Protocol Options Protocol buffer options are standard messages (ie, a struct), and can be attached to any service, message, enum, or other other type declaration in a Gunk file via the doccomment preceding the type, field, or service: // MyOption is an option. type MyOption struct { Name string `pb:\"1\"` } // +gunk MyOption { // Name: \"test\", // } type MyMessage struct { /* ... */ } Project Configuration Files Gunk uses a top-level .gunkconfig configuration file for managing the Gunk protocol definitons for a project: # Example .gunkconfig for Go, grpc-gateway, Python and JS [generate go] out=v1/go plugins=grpc [generate] out=v1/go command=protoc-gen-grpc-gateway logtostderr=true [generate python] out=v1/python [generate js] out=v1/js import_style=commonjs binary Project Search Path When gunk is invoked from the command-line, it searches the passed package spec (or current working directory) for a .gunkconfig file, and walks up the directory hierarchy until a .gunkconfig is found, or the project's root is encountered. The project root is defined as the top-most directory containing a .git subdirectory, or where a go.mod file is located. Format The .gunkconfig file format is compatible with Git config syntax, and in turn is compatible with the INI file format: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Global section import_path - see \"Converting Existing Protobuf Files\" strip_enum_type_names - with this option on, enums with their type prefixed will be renamed to the version without prefix. Note that this might produce invalid protobuf that stops compiling in 1.4.* protoc-gen-go, if the enum names clash. Section [protoc] The path where to check for (or where to download) the protoc binary can be configured. The version can also be pinned. Parameters version - the version of protoc to use. If unspecified, defaults to the latest release available. Otherwise, gunk will either download the specified version, or check that the version of protoc at the specified path matches what was configured. path - the path to check for the protoc binary. If unspecified, defaults appropriate user cache directory for the user's OS. If no file exists at the path, gunk will attempt to download protoc. Section [generate[ <type>]] Each [generate] or [generate <type>] section in a .gunkconfig corresponds to a invocation of the protoc-gen-<type> tool. Parameters Each name[=value] parameter defined within a [generate] section will be passed as a parameter to the protoc-gen-<type> tool, with the exception of the following special parameters that override the behavior of the gunk generate tool: command - overrides the protoc-gen-* command executable used by gunk generate. The executable must be findable on $PATH (Linux/macOS) or %PATH% (Windows), or may be the full path to the executable. If not defined, then command will be protoc-gen-<type>, when <type> is the value in [generate <type>]. protoc - overrides the <type> value, causing gunk generate to use the protoc value in place of <type>. out - overrides the output path of protoc. If not defined, output will be the same directory as the location of the .gunk files. plugin_version - specify version of plugin. The plugin is downloaded from github/maven, built in cache and used. It is not installed in $PATH. This currently works with the following plugins: protoc-gen-go protoc-gen-grpc-java protoc-gen-grpc-gateway protoc-gen-openapiv2 (protoc-gen-swagger support is deprecated) protoc-gen-swift (installing swift itself first is necessary) protoc-gen-grpc-swift (installing swift itself first is necessary) protoc-gen-ts (installing node and npm first is necessary) protoc-gen-grpc-python (cmake, gcc is necessary; takes ~10 minutes to clone build) It is recommended to use this function everywhere, for reproducible builds, together with version for protoc. json_tag_postproc - uses json tags defined in gunk file also for go-generated file fix_paths_postproc - for js and ts - by default, gunk generates wrong paths for other imported gunk packages, because of the way gunk moves files around. Works only if js also has import_style=commonjs option. All other name[=value] pairs specified within the generate section will be passed as plugin parameters to protoc and the protoc-gen-<type> generators. Short Form The following .gunkconfig: [generate go] [generate js] out=v1/js is equivalent to: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Different forms of invocation There are three different forms of gunkconfig sections that have three different semantics. [generate] command=protoc-gen-go [generate] protoc=go [generate go] The first one uses protoc-gen-go plugin directly, without using protoc. It also attempts to move files to the same directory as the gunk file. The second one uses protoc and does not attempt to move any files. Protoc attempts to load plugin from $PATH, if it is not one of the built-in protoc plugins; this will not work together with pinned version and other gunk features and is not recommended outside of built-in protoc generators. The third version is reccomended. It will try to detect whether language is one of built-in protoc generators, in that case behaves like the second way, otherwise behaves like the first. The built-in protoc generators are: cpp java python php ruby csharp objc js Third-Party Protobuf Options Gunk provides the +gunk annotation syntax for declaring protobuf options, and specially recognizes some third-party API annotations, such as Google HTTP options, including all builtin/standard protoc options for code generation: // +gunk java.Package(\"com.example.message\") // +gunk java.MultipleFiles(true) package message import ( \"github.com/gunk/opt/http\" \"github.com/gunk/opt/file/java\" ) type Util interface { // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo() } Further documentation on available options can be found at the Gunk options project. Formatting Gunk Files Gunk provides the gunk format command to format .gunk files (akin to gofmt): $ gunk format /path/to/file.gunk $ gunk format <pathspec> Converting Existing Protobuf Files Gunk provides the gunk convert command that will converting existing .proto files (or a directory) to the Go-derived Gunk syntax: $ gunk convert /path/to/file.proto $ gunk convert /path/to/protobuf/directory If your .proto is referencing another .proto from another directory, you can add import_path in the global section of your .gunkconfig. If you don't provide import_path it will only search in the root directory. import_path=relative/path/to/protobuf/directory The path to provide is relative from the .gunkconfig location. Furthermore, the referenced files must contain: option go_package=\"path/of/go/package\"; The resulting .gunk file will contain the import path as defined in go_package: import ( name \"path/of/go/package\" ) About Gunk is developed by the team at Brankas, and was designed to streamline API design and development. History From the beginning of the company, the Brankas team defined API types and services in .proto files, leveraging ad-hoc Makefile's, shell scripts, and other non-standardized mechanisms for generating Protocol Buffer code. As development exploded in 2017 (and beyond) with continued addition of backend microservices/APIs, more code repositories and projects, and team members, it became necessary to standardize tooling for the organization as well as reduce the cognitive load of developers (who for the most part were working almost exclusively with Go) when declaring gRPC and REST services. Naming The Gunk name has a cheeky, backronym \"Gunk Unified N-terface Kompiler\", however the name was chosen because it was possible to secure the GitHub gunk project name, was short, concise, and not used by other projects. Additionally, \"gunk\" is an apt description for the \"gunk\" surrounding protocol definition, generation, compilation, and delivery. Contributing Issues, Pull Requests, and other contributions are greatly welcomed and appreciated! Get started with building and running gunk: # clone source repository $ git clone https://github.com/gunk/gunk.git && cd gunk # force GO111MODULES $ export GO111MODULE=on # build and run $ go build && ./gunk Dependency Management Gunk uses Go modules for dependency management, and as such requires Go 1.11+. Please run go mod tidy before submitting any PRs: $ export GO111MODULE=on $ cd gunk && go mod tidy "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/gunk/gunk",
        "comments.comment_id": [18945203, 18945486],
        "comments.comment_author": ["djur", "grogenaut"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-01-19T03:44:30Z",
          "2019-01-19T05:20:46Z"
        ],
        "comments.comment_text": [
          "I'm not seeing what makes this \"modern\". proto3 is only a few years old and nothing about it strikes me as unusually archaic. Protobuf in general isn't that much older than Go. I can see why Go-compatible syntax would be attractive to Go developers, so maybe that should be in the description rather than \"modern\"?",
          "Not sure why I’d want to define a language independent interchange format in a language specific way and remove all of the tooling help at the same time. Why is this better? A why section/motivations would help greatly."
        ],
        "id": "298bc63f-2491-4cbe-a038-e4d42ea2a0e9",
        "url_text": "Gunk is a modern frontend and syntax for Protocol Buffers. Quickstart | Installing | Syntax | Configuring | About | Releases Overview Gunk provides a modern project-based workflow along with a Go-derived syntax for defining types and services for use with Protocol Buffers. Gunk is designed to integrate cleanly with existing protoc based build pipelines, while standardizing workflows in a way that is familiar/accessible to Go developers. Quickstart Create a working directory for a project: $ mkdir -p ~/src/example && cd ~/src/example Install gunk and place the following Gunk definitions in example/util.gunk: package util // Util is a utility service. type Util interface { // Echo returns the passed message. Echo(Message) Message } // Message contains an echo message. type Message struct { // Msg is a message from a client. Msg string `pb:\"1\"` } Create the corresponding project configuration in example/.gunkconfig: [generate go] [generate js] import_style=commonjs binary Then, generate protocol buffer definitions/code: $ ls -A .gunkconfig util.gunk $ gunk generate $ ls -A all.pb.go all_pb.js .gunkconfig util.gunk As seen above, gunk generated the corresponding Go and JavaScript protobuf code using the options defined in the .gunkconfig. End-to-end Example A end-to-end example gRPC server implementation, using Gunk definitions is available for review. Debugging protoc commands Underlying commands executed by gunk can be viewed with the following: $ gunk generate -x protoc-gen-go protoc --js_out=import_style=commonjs,binary:/home/user/example --descriptor_set_in=/dev/stdin all.proto Installing The gunk command-line tool can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the gunk or gunk.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) Installing via Homebrew (macOS) gunk is available in the gunk/gunk tap, and can be installed in the usual way with the brew command: # add tap $ brew tap gunk/gunk # install gunk $ brew install gunk Installing via Scoop (Windows) gunk can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install gunk Installing via Go gunk can be installed in the usual Go fashion: # install gunk $ go get -u github.com/gunk/gunk Protobuf Dependency and Caching The gunk command-line tool uses the protoc command-line tool. gunk can be configured to use protoc at a specified path. If it isn't available, gunk will download the latest protobuf release to the user's cache, for use. It's also possible to pin a specific version, see the section on protoc configuration. Protocol Types and Messages Gunk provides an alternate, Go-derived syntax for defining protocol buffers. As such, Gunk definitions are a subset of the Go programming language. Additionally, a special +gunk annotation is recognized by gunk, to allow the declaration of protocol buffer options: package message import \"github.com/gunk/opt/http\" // Message is a Echo message. type Message struct { // Msg holds a message. Msg string `pb:\"1\" json:\"msg\"` Code int `pb:\"2\" json:\"code\"` } // Util is a utility service. type Util interface { // Echo echoes a message. // // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo(Message) Message } Technically speaking, gunk is not actually strict subset of go, as gunk allows unused imports; it actually requires them for some features. See the example above; in pure go, this would not be a valid go code, as http is not used outside of the comment. Scalars Gunk's Go-derived syntax uses the canonical Go scalar types of the proto3 syntax, defined by the protocol buffer project: Proto3 Type Gunk Type double float64 float float32 int32 int int32 int32 int64 int64 uint32 uint uint32 uint32 uint64 uint64 bool bool string string bytes []byte Note: Variable-length scalars will be enabled in the future using a tag parameter. Messages Gunk's Go-derived syntax uses Go's struct type declarations for declaring messages, and require a pb:\"<field_number>\" tag to indicate the field number: type Message struct { FieldA string `pb:\"1\"` } type Envelope struct { Message Message `pb:\"1\" json:\"msg\"` } There are additional tags (for example, the json: tag above), that will be recognized by gunk format, and passed on to generators, where possible. Note: When using gunk format, a valid pb:\"<field_number>\" tag will be automatically inserted if not declared. Services Gunk's Go-derived syntax uses Go's interface syntax for declaring services: type SearchService interface { Search(SearchRequest) SearchResponse } The above is equivalent to the following protobuf syntax: service SearchService { rpc Search (SearchRequest) returns (SearchResponse); } Enums Gunk's Go-derived syntax uses Go const's for declaring enums: type MyEnum int const ( MYENUM MyEnum = iota MYENUM2 ) Note: values can also be fixed numeric values or a calculated value (using iota). Maps Gunk's Go-derived syntax uses Go map's for declaring map fields: type Project struct { ProjectID string `pb:\"1\" json:\"project_id\"` } type GetProjectResponse struct { Projects map[string]Project `pb:\"1\"` } Repeated Values Gunk's Go-derived syntax uses Go's slice syntax ([]) for declaring a repeated field: type MyMessage struct { FieldA []string `pb:\"1\"` } Message Streams Gunk's Go-derived syntax uses Go chan syntax for declaring streams: type MessageService interface { List(chan Message) chan Message } The above is equivalent to the following protobuf syntax: service MessageService { rpc List(stream Message) returns (stream Message); } Protocol Options Protocol buffer options are standard messages (ie, a struct), and can be attached to any service, message, enum, or other other type declaration in a Gunk file via the doccomment preceding the type, field, or service: // MyOption is an option. type MyOption struct { Name string `pb:\"1\"` } // +gunk MyOption { // Name: \"test\", // } type MyMessage struct { /* ... */ } Project Configuration Files Gunk uses a top-level .gunkconfig configuration file for managing the Gunk protocol definitons for a project: # Example .gunkconfig for Go, grpc-gateway, Python and JS [generate go] out=v1/go plugins=grpc [generate] out=v1/go command=protoc-gen-grpc-gateway logtostderr=true [generate python] out=v1/python [generate js] out=v1/js import_style=commonjs binary Project Search Path When gunk is invoked from the command-line, it searches the passed package spec (or current working directory) for a .gunkconfig file, and walks up the directory hierarchy until a .gunkconfig is found, or the project's root is encountered. The project root is defined as the top-most directory containing a .git subdirectory, or where a go.mod file is located. Format The .gunkconfig file format is compatible with Git config syntax, and in turn is compatible with the INI file format: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Global section import_path - see \"Converting Existing Protobuf Files\" strip_enum_type_names - with this option on, enums with their type prefixed will be renamed to the version without prefix. Note that this might produce invalid protobuf that stops compiling in 1.4.* protoc-gen-go, if the enum names clash. Section [protoc] The path where to check for (or where to download) the protoc binary can be configured. The version can also be pinned. Parameters version - the version of protoc to use. If unspecified, defaults to the latest release available. Otherwise, gunk will either download the specified version, or check that the version of protoc at the specified path matches what was configured. path - the path to check for the protoc binary. If unspecified, defaults appropriate user cache directory for the user's OS. If no file exists at the path, gunk will attempt to download protoc. Section [generate[ <type>]] Each [generate] or [generate <type>] section in a .gunkconfig corresponds to a invocation of the protoc-gen-<type> tool. Parameters Each name[=value] parameter defined within a [generate] section will be passed as a parameter to the protoc-gen-<type> tool, with the exception of the following special parameters that override the behavior of the gunk generate tool: command - overrides the protoc-gen-* command executable used by gunk generate. The executable must be findable on $PATH (Linux/macOS) or %PATH% (Windows), or may be the full path to the executable. If not defined, then command will be protoc-gen-<type>, when <type> is the value in [generate <type>]. protoc - overrides the <type> value, causing gunk generate to use the protoc value in place of <type>. out - overrides the output path of protoc. If not defined, output will be the same directory as the location of the .gunk files. plugin_version - specify version of plugin. The plugin is downloaded from github/maven, built in cache and used. It is not installed in $PATH. This currently works with the following plugins: protoc-gen-go protoc-gen-grpc-java protoc-gen-grpc-gateway protoc-gen-openapiv2 (protoc-gen-swagger support is deprecated) protoc-gen-swift (installing swift itself first is necessary) protoc-gen-grpc-swift (installing swift itself first is necessary) protoc-gen-ts (installing node and npm first is necessary) protoc-gen-grpc-python (cmake, gcc is necessary; takes ~10 minutes to clone build) It is recommended to use this function everywhere, for reproducible builds, together with version for protoc. json_tag_postproc - uses json tags defined in gunk file also for go-generated file fix_paths_postproc - for js and ts - by default, gunk generates wrong paths for other imported gunk packages, because of the way gunk moves files around. Works only if js also has import_style=commonjs option. All other name[=value] pairs specified within the generate section will be passed as plugin parameters to protoc and the protoc-gen-<type> generators. Short Form The following .gunkconfig: [generate go] [generate js] out=v1/js is equivalent to: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Different forms of invocation There are three different forms of gunkconfig sections that have three different semantics. [generate] command=protoc-gen-go [generate] protoc=go [generate go] The first one uses protoc-gen-go plugin directly, without using protoc. It also attempts to move files to the same directory as the gunk file. The second one uses protoc and does not attempt to move any files. Protoc attempts to load plugin from $PATH, if it is not one of the built-in protoc plugins; this will not work together with pinned version and other gunk features and is not recommended outside of built-in protoc generators. The third version is reccomended. It will try to detect whether language is one of built-in protoc generators, in that case behaves like the second way, otherwise behaves like the first. The built-in protoc generators are: cpp java python php ruby csharp objc js Third-Party Protobuf Options Gunk provides the +gunk annotation syntax for declaring protobuf options, and specially recognizes some third-party API annotations, such as Google HTTP options, including all builtin/standard protoc options for code generation: // +gunk java.Package(\"com.example.message\") // +gunk java.MultipleFiles(true) package message import ( \"github.com/gunk/opt/http\" \"github.com/gunk/opt/file/java\" ) type Util interface { // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo() } Further documentation on available options can be found at the Gunk options project. Formatting Gunk Files Gunk provides the gunk format command to format .gunk files (akin to gofmt): $ gunk format /path/to/file.gunk $ gunk format <pathspec> Converting Existing Protobuf Files Gunk provides the gunk convert command that will converting existing .proto files (or a directory) to the Go-derived Gunk syntax: $ gunk convert /path/to/file.proto $ gunk convert /path/to/protobuf/directory If your .proto is referencing another .proto from another directory, you can add import_path in the global section of your .gunkconfig. If you don't provide import_path it will only search in the root directory. import_path=relative/path/to/protobuf/directory The path to provide is relative from the .gunkconfig location. Furthermore, the referenced files must contain: option go_package=\"path/of/go/package\"; The resulting .gunk file will contain the import path as defined in go_package: import ( name \"path/of/go/package\" ) About Gunk is developed by the team at Brankas, and was designed to streamline API design and development. History From the beginning of the company, the Brankas team defined API types and services in .proto files, leveraging ad-hoc Makefile's, shell scripts, and other non-standardized mechanisms for generating Protocol Buffer code. As development exploded in 2017 (and beyond) with continued addition of backend microservices/APIs, more code repositories and projects, and team members, it became necessary to standardize tooling for the organization as well as reduce the cognitive load of developers (who for the most part were working almost exclusively with Go) when declaring gRPC and REST services. Naming The Gunk name has a cheeky, backronym \"Gunk Unified N-terface Kompiler\", however the name was chosen because it was possible to secure the GitHub gunk project name, was short, concise, and not used by other projects. Additionally, \"gunk\" is an apt description for the \"gunk\" surrounding protocol definition, generation, compilation, and delivery. Contributing Issues, Pull Requests, and other contributions are greatly welcomed and appreciated! Get started with building and running gunk: # clone source repository $ git clone https://github.com/gunk/gunk.git && cd gunk # force GO111MODULES $ export GO111MODULE=on # build and run $ go build && ./gunk Dependency Management Gunk uses Go modules for dependency management, and as such requires Go 1.11+. Please run go mod tidy before submitting any PRs: $ export GO111MODULE=on $ cd gunk && go mod tidy ",
        "_version_": 1718536438489808896
      },
      {
        "story_id": 21676256,
        "story_author": "osprojects",
        "story_descendants": 4,
        "story_score": 32,
        "story_time": "2019-12-01T16:42:50Z",
        "story_title": "Open Source Webhook Server",
        "search": [
          "Open Source Webhook Server",
          "https://github.com/adnanh/webhook",
          "What is webhook? webhook is a lightweight configurable tool written in Go, that allows you to easily create HTTP endpoints (hooks) on your server, which you can use to execute configured commands. You can also pass data from the HTTP request (such as headers, payload or query variables) to your commands. webhook also allows you to specify rules which have to be satisfied in order for the hook to be triggered. For example, if you're using Github or Bitbucket, you can use webhook to set up a hook that runs a redeploy script for your project on your staging server, whenever you push changes to the master branch of your project. If you use Mattermost or Slack, you can set up an \"Outgoing webhook integration\" or \"Slash command\" to run various commands on your server, which can then report back directly to you or your channels using the \"Incoming webhook integrations\", or the appropriate response body. webhook aims to do nothing more than it should do, and that is: receive the request, parse the headers, payload and query variables, check if the specified rules for the hook are satisfied, and finally, pass the specified arguments to the specified command via command line arguments or via environment variables. Everything else is the responsibility of the command's author. Hookdoo If you don't have time to waste configuring, hosting, debugging and maintaining your webhook instance, we offer a SaaS solution that has all of the capabilities webhook provides, plus a lot more, and all that packaged in a nice friendly web interface. If you are interested, find out more at hookdoo website. If you have any questions, you can contact us at info@hookdoo.com If you need a way of inspecting, monitoring and replaying webhooks without the back and forth troubleshooting, give Hookdeck a try! Getting started Installation Building from source To get started, first make sure you've properly set up your Go 1.14 or newer environment and then run $ go build github.com/adnanh/webhook to build the latest version of the webhook. Using package manager Snap store Ubuntu If you are using Ubuntu linux (17.04 or later), you can install webhook using sudo apt-get install webhook which will install community packaged version. Debian If you are using Debian linux (\"stretch\" or later), you can install webhook using sudo apt-get install webhook which will install community packaged version (thanks @freeekanayaka) from https://packages.debian.org/sid/webhook Download prebuilt binaries Prebuilt binaries for different architectures are available at GitHub Releases. Configuration Next step is to define some hooks you want webhook to serve. webhook supports JSON or YAML configuration files, but we'll focus primarily on JSON in the following example. Begin by creating an empty file named hooks.json. This file will contain an array of hooks the webhook will serve. Check Hook definition page to see the detailed description of what properties a hook can contain, and how to use them. Let's define a simple hook named redeploy-webhook that will run a redeploy script located in /var/scripts/redeploy.sh. Make sure that your bash script has #!/bin/sh shebang on top. Our hooks.json file will now look like this: [ { \"id\": \"redeploy-webhook\", \"execute-command\": \"/var/scripts/redeploy.sh\", \"command-working-directory\": \"/var/webhook\" } ] NOTE: If you prefer YAML, the equivalent hooks.yaml file would be: - id: redeploy-webhook execute-command: \"/var/scripts/redeploy.sh\" command-working-directory: \"/var/webhook\" You can now run webhook using $ /path/to/webhook -hooks hooks.json -verbose It will start up on default port 9000 and will provide you with one HTTP endpoint http://yourserver:9000/hooks/redeploy-webhook Check webhook parameters page to see how to override the ip, port and other settings such as hook hotreload, verbose output, etc, when starting the webhook. By performing a simple HTTP GET or POST request to that endpoint, your specified redeploy script would be executed. Neat! However, hook defined like that could pose a security threat to your system, because anyone who knows your endpoint, can send a request and execute your command. To prevent that, you can use the \"trigger-rule\" property for your hook, to specify the exact circumstances under which the hook would be triggered. For example, you can use them to add a secret that you must supply as a parameter in order to successfully trigger the hook. Please check out the Hook rules page for detailed list of available rules and their usage. Multipart Form Data webhook provides limited support the parsing of multipart form data. Multipart form data can contain two types of parts: values and files. All form values are automatically added to the payload scope. Use the parse-parameters-as-json settings to parse a given value as JSON. All files are ignored unless they match one of the following criteria: The Content-Type header is application/json. The part is named in the parse-parameters-as-json setting. In either case, the given file part will be parsed as JSON and added to the payload map. Templates webhook can parse the hooks configuration file as a Go template when given the -template CLI parameter. See the Templates page for more details on template usage. Using HTTPS webhook by default serves hooks using http. If you want webhook to serve secure content using https, you can use the -secure flag while starting webhook. Files containing a certificate and matching private key for the server must be provided using the -cert /path/to/cert.pem and -key /path/to/key.pem flags. If the certificate is signed by a certificate authority, the cert file should be the concatenation of the server's certificate followed by the CA's certificate. TLS version and cipher suite selection flags are available from the command line. To list available cipher suites, use the -list-cipher-suites flag. The -tls-min-version flag can be used with -list-cipher-suites. CORS Headers If you want to set CORS headers, you can use the -header name=value flag while starting webhook to set the appropriate CORS headers that will be returned with each response. Interested in running webhook inside of a Docker container? You can use one of the following Docker images, or create your own (please read this discussion): almir/webhook roxedus/webhook thecatlady/webhook Examples Check out Hook examples page for more complex examples of hooks. Guides featuring webhook Plex 2 Telegram by @psyhomb Webhook & JIRA by @perfecto25 Trigger Ansible AWX job runs on SCM (e.g. git) commit by @jpmens Deploy using GitHub webhooks by @awea Setting up Automatic Deployment and Builds Using Webhooks by Will Browning Auto deploy your Node.js app on push to GitHub in 3 simple steps by Karolis Rusenas Automate Static Site Deployments with Salt, Git, and Webhooks by Linode Using Prometheus to Automatically Scale WebLogic Clusters on Kubernetes by Marina Kogan Github Pages and Jekyll - A New Platform for LACNIC Labs by Carlos Martnez Cagnazzo How to Deploy React Apps Using Webhooks and Integrating Slack on Ubuntu by Arslan Ud Din Shafiq Private webhooks by Thomas Adventures in webhooks by Drake GitHub pro tips by Spencer Lyon XiaoMi Vacuum + Amazon Button = Dash Cleaning by c0mmensal Set up Automated Deployments From Github With Webhook by Maxim Orlov VIDEO: Gitlab CI/CD configuration using Docker and adnanh/webhook to deploy on VPS - Tutorial #1 by Yes! Let's Learn Software Engineering Integrate automatic deployment in 20 minutes using webhooks + Nginx setup by Anksus ... Want to add your own? Open an Issue or create a PR :-) Community Contributions See the webhook-contrib repository for a collections of tools and helpers related to webhook that have been contributed by the webhook community. Need help? Check out existing issues to see if someone else also had the same problem, or open a new one. Support active development Sponsors DigitalOcean is a simple and robust cloud computing platform, designed for developers. BrowserStack is a cloud-based cross-browser testing tool that enables developers to test their websites across various browsers on different operating systems and mobile devices, without requiring users to install virtual machines, devices or emulators. Support this project by becoming a sponsor. Your logo will show up here with a link to your website. By contributing This project exists thanks to all the people who contribute. Contribute!. By giving money OpenCollective Backer OpenCollective Sponsor PayPal Patreon Faircode Flattr Thank you to all our backers! License The MIT License (MIT) Copyright (c) 2015 Adnan Hajdarevic adnanh@gmail.com Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/adnanh/webhook",
        "comments.comment_id": [21679205, 21688092],
        "comments.comment_author": ["m-p-3", "AtomicOrbital"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-12-02T01:32:48Z",
          "2019-12-02T23:09:42Z"
        ],
        "comments.comment_text": [
          "Interesting, could be useful to interface some of my stuff with IFTTT or Integromat.",
          "I have been using this webhook server in prod for a few years and its been easy to setup/maintain ... you just define  github.com repo to publish `git push` or whatever then this webhook server listens to every git push my team makes to launch a code recompile/redeploy ...  foolproof and solid ... I highly recommend"
        ],
        "id": "d4c7df02-9e7a-462d-8b7b-5910674b5674",
        "url_text": "What is webhook? webhook is a lightweight configurable tool written in Go, that allows you to easily create HTTP endpoints (hooks) on your server, which you can use to execute configured commands. You can also pass data from the HTTP request (such as headers, payload or query variables) to your commands. webhook also allows you to specify rules which have to be satisfied in order for the hook to be triggered. For example, if you're using Github or Bitbucket, you can use webhook to set up a hook that runs a redeploy script for your project on your staging server, whenever you push changes to the master branch of your project. If you use Mattermost or Slack, you can set up an \"Outgoing webhook integration\" or \"Slash command\" to run various commands on your server, which can then report back directly to you or your channels using the \"Incoming webhook integrations\", or the appropriate response body. webhook aims to do nothing more than it should do, and that is: receive the request, parse the headers, payload and query variables, check if the specified rules for the hook are satisfied, and finally, pass the specified arguments to the specified command via command line arguments or via environment variables. Everything else is the responsibility of the command's author. Hookdoo If you don't have time to waste configuring, hosting, debugging and maintaining your webhook instance, we offer a SaaS solution that has all of the capabilities webhook provides, plus a lot more, and all that packaged in a nice friendly web interface. If you are interested, find out more at hookdoo website. If you have any questions, you can contact us at info@hookdoo.com If you need a way of inspecting, monitoring and replaying webhooks without the back and forth troubleshooting, give Hookdeck a try! Getting started Installation Building from source To get started, first make sure you've properly set up your Go 1.14 or newer environment and then run $ go build github.com/adnanh/webhook to build the latest version of the webhook. Using package manager Snap store Ubuntu If you are using Ubuntu linux (17.04 or later), you can install webhook using sudo apt-get install webhook which will install community packaged version. Debian If you are using Debian linux (\"stretch\" or later), you can install webhook using sudo apt-get install webhook which will install community packaged version (thanks @freeekanayaka) from https://packages.debian.org/sid/webhook Download prebuilt binaries Prebuilt binaries for different architectures are available at GitHub Releases. Configuration Next step is to define some hooks you want webhook to serve. webhook supports JSON or YAML configuration files, but we'll focus primarily on JSON in the following example. Begin by creating an empty file named hooks.json. This file will contain an array of hooks the webhook will serve. Check Hook definition page to see the detailed description of what properties a hook can contain, and how to use them. Let's define a simple hook named redeploy-webhook that will run a redeploy script located in /var/scripts/redeploy.sh. Make sure that your bash script has #!/bin/sh shebang on top. Our hooks.json file will now look like this: [ { \"id\": \"redeploy-webhook\", \"execute-command\": \"/var/scripts/redeploy.sh\", \"command-working-directory\": \"/var/webhook\" } ] NOTE: If you prefer YAML, the equivalent hooks.yaml file would be: - id: redeploy-webhook execute-command: \"/var/scripts/redeploy.sh\" command-working-directory: \"/var/webhook\" You can now run webhook using $ /path/to/webhook -hooks hooks.json -verbose It will start up on default port 9000 and will provide you with one HTTP endpoint http://yourserver:9000/hooks/redeploy-webhook Check webhook parameters page to see how to override the ip, port and other settings such as hook hotreload, verbose output, etc, when starting the webhook. By performing a simple HTTP GET or POST request to that endpoint, your specified redeploy script would be executed. Neat! However, hook defined like that could pose a security threat to your system, because anyone who knows your endpoint, can send a request and execute your command. To prevent that, you can use the \"trigger-rule\" property for your hook, to specify the exact circumstances under which the hook would be triggered. For example, you can use them to add a secret that you must supply as a parameter in order to successfully trigger the hook. Please check out the Hook rules page for detailed list of available rules and their usage. Multipart Form Data webhook provides limited support the parsing of multipart form data. Multipart form data can contain two types of parts: values and files. All form values are automatically added to the payload scope. Use the parse-parameters-as-json settings to parse a given value as JSON. All files are ignored unless they match one of the following criteria: The Content-Type header is application/json. The part is named in the parse-parameters-as-json setting. In either case, the given file part will be parsed as JSON and added to the payload map. Templates webhook can parse the hooks configuration file as a Go template when given the -template CLI parameter. See the Templates page for more details on template usage. Using HTTPS webhook by default serves hooks using http. If you want webhook to serve secure content using https, you can use the -secure flag while starting webhook. Files containing a certificate and matching private key for the server must be provided using the -cert /path/to/cert.pem and -key /path/to/key.pem flags. If the certificate is signed by a certificate authority, the cert file should be the concatenation of the server's certificate followed by the CA's certificate. TLS version and cipher suite selection flags are available from the command line. To list available cipher suites, use the -list-cipher-suites flag. The -tls-min-version flag can be used with -list-cipher-suites. CORS Headers If you want to set CORS headers, you can use the -header name=value flag while starting webhook to set the appropriate CORS headers that will be returned with each response. Interested in running webhook inside of a Docker container? You can use one of the following Docker images, or create your own (please read this discussion): almir/webhook roxedus/webhook thecatlady/webhook Examples Check out Hook examples page for more complex examples of hooks. Guides featuring webhook Plex 2 Telegram by @psyhomb Webhook & JIRA by @perfecto25 Trigger Ansible AWX job runs on SCM (e.g. git) commit by @jpmens Deploy using GitHub webhooks by @awea Setting up Automatic Deployment and Builds Using Webhooks by Will Browning Auto deploy your Node.js app on push to GitHub in 3 simple steps by Karolis Rusenas Automate Static Site Deployments with Salt, Git, and Webhooks by Linode Using Prometheus to Automatically Scale WebLogic Clusters on Kubernetes by Marina Kogan Github Pages and Jekyll - A New Platform for LACNIC Labs by Carlos Martnez Cagnazzo How to Deploy React Apps Using Webhooks and Integrating Slack on Ubuntu by Arslan Ud Din Shafiq Private webhooks by Thomas Adventures in webhooks by Drake GitHub pro tips by Spencer Lyon XiaoMi Vacuum + Amazon Button = Dash Cleaning by c0mmensal Set up Automated Deployments From Github With Webhook by Maxim Orlov VIDEO: Gitlab CI/CD configuration using Docker and adnanh/webhook to deploy on VPS - Tutorial #1 by Yes! Let's Learn Software Engineering Integrate automatic deployment in 20 minutes using webhooks + Nginx setup by Anksus ... Want to add your own? Open an Issue or create a PR :-) Community Contributions See the webhook-contrib repository for a collections of tools and helpers related to webhook that have been contributed by the webhook community. Need help? Check out existing issues to see if someone else also had the same problem, or open a new one. Support active development Sponsors DigitalOcean is a simple and robust cloud computing platform, designed for developers. BrowserStack is a cloud-based cross-browser testing tool that enables developers to test their websites across various browsers on different operating systems and mobile devices, without requiring users to install virtual machines, devices or emulators. Support this project by becoming a sponsor. Your logo will show up here with a link to your website. By contributing This project exists thanks to all the people who contribute. Contribute!. By giving money OpenCollective Backer OpenCollective Sponsor PayPal Patreon Faircode Flattr Thank you to all our backers! License The MIT License (MIT) Copyright (c) 2015 Adnan Hajdarevic adnanh@gmail.com Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ",
        "_version_": 1718536547948560384
      },
      {
        "story_id": 21912838,
        "story_author": "lastmjs",
        "story_descendants": 126,
        "story_score": 103,
        "story_time": "2019-12-30T12:22:53Z",
        "story_title": "Zwitterion: a web dev server that lets you import anything",
        "search": [
          "Zwitterion: a web dev server that lets you import anything",
          "https://github.com/lastmjs/zwitterion",
          "Zwitterion A web dev server that lets you import anything* * If by anything you mean: JavaScript ES2015+, TypeScript, JSON, JSX, TSX, AssemblyScript, Rust, C, C++, WebAssembly, and in the future anything that compiles to JavaScript or WebAssembly. Zwitterion is designed to be an instant replacement for your current web development static file server. Production deployments are also possible through the static build. For example, you can write stuff like the following and it just works: ./index.html: <!DOCTYPE html> <html> <head> <script type=\"module\" src=\"app.ts\"></script> </head> <body> This is the simplest developer experience I've ever had! </body> </html> ./app.ts: import { getHelloWorld } from './hello-world.ts'; const helloWorld: string = getHelloWorld(); console.log(helloWorld); ./hello-world.ts: export function getHelloWorld(): string { return 'Why hello there world!'; } Really, it just works. Zwitterion lets you get back to the good old days of web development. Just write your source code in any supported language and run it in the browser. Also...Zwitterion is NOT a bundler. It eschews bundling for a simpler experience. Current Features ES2015+ TypeScript JSON JSX TSX AssemblyScript Rust (basic support) C (basic support) C++ (basic support) WebAssembly Text Format (Wat) WebAssembly (Wasm) Bare imports (import * as stuff from 'library'; instead of import * as stuff from '../node_modules/library/index.js';) Single Page Application routing (by default the server returns index.html on unhandled routes) Static build for production deployment Upcoming Features More robust Rust integration (i.e. automatic local Rust installation during npm installation) More robust C integration More robust C++ integration Import maps HTTP2 optimizations Documentation Examples Installation and Basic Use Production Use Languages JavaScript TypeScript JSON JSX TSX AssemblyScript Rust C C++ WebAssembly Text Format (Wat) WebAssembly (Wasm) Command Line Options Special Considerations Under the Hood Installation and Basic Use Local Installation and Use Install Zwitterion in the directory that you would like to serve files from: Run Zwitterion by accessing its executable directly from the terminal: node_modules/.bin/zwitterion or from an npm script: { ... \"scripts\": { \"start\": \"zwitterion\" } ... } Global Installation and Use Install Zwitterion globally to use across projects: npm install -g zwitterion Run Zwitterion from the terminal: or from an npm script: { ... \"scripts\": { \"start\": \"zwitterion\" } ... } Production Use It is recommended to use Zwitterion in production by creating a static build of your project. A static build essentially runs all relevant files through Zwitterion, and copies those and all other files in your project to a dist directory. You can take this directory and upload it to a Content Delivery Network (CDN), or another static file hosting service. You may also use a running Zwitterion server in production, but for performance and potential security reasons it is not recommended. To create a static build, run Zwitterion with the --build-static option. You will probably need to add the application/javascript MIME type to your hosting provider for your TypeScript, AssemblyScript, Rust, Wasm, and Wat files. From the terminal: zwitterion --build-static From an npm script: { ... \"scripts\": { \"build-static\": \"zwitterion --build-static\" } ... } The static build will be located in a directory called dist, in the same directory that you ran the --build-static command from. Languages JavaScript JavaScript is the language of the web. You can learn more here. Importing JavaScript ES2015+ is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.js: import { helloWorld } from './hello-world.js'; console.log(helloWorld()); ./hello-world.js: export function helloWorld() { return 'Hello world!'; } JavaScript transpilation is done by the TypeScript compiler. By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json TypeScript TypeScript is a typed superset of JavaScript. You can learn more here. Importing TypeScript is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.ts: import { helloWorld } from './hello-world.ts'; console.log(helloWorld()); ./hello-world.ts: export function helloWorld(): string { return 'Hello world!'; } By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json JSON JSON is provided as a default export. It is recommended to use explicit file extensions: ./app.js: import helloWorld from './hello-world.json'; console.log(helloWorld); ./hello-world.json: JSX Importing JSX is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.js: import { helloWorldElement } from './hello-world.jsx'; ReactDOM.render( helloWorldElement, document.getElementById('root') ); ./hello-world.jsx: export const hellowWorldElement = <h1>Hello, world!</h1>; JSX transpilation is done by the TypeScript compiler. By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json TSX Importing TSX is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.js: import { helloWorldElement } from './hello-world.tsx'; ReactDOM.render( helloWorldElement, document.getElementById('root') ); ./hello-world.tsx: const helloWorld: string = 'Hello, world!'; export const hellowWorldElement = <h1>{ helloWorld }</h1>; TSX transpilation is done by the TypeScript compiler. By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json AssemblyScript AssemblyScript is a new language that compiles a strict subset of TypeScript to WebAssembly. You can learn more about it in The AssemblyScript Book. Zwitterion assumes that AssemblyScript files have the .as file extension. This is a Zwitterion-specific extension choice, as the AssemblyScript project has not yet chosen its own official file extension. You can follow that discussion here. Zwitterion will follow the official extension choice once it is made. Importing AssemblyScript is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry AssemblyScript module is a function that returns a promise. This function takes as its one parameter an object containing imports to the AssemblyScript module. Passing values to and from functions exported from AssemblyScript modules should be straightforward, but there are some limitations. Zwitterion uses as-bind under the hood to broker values to and from AssemblyScript modules. Look there if you need more information. You can import AssemblyScript from JavaScript or TypeScript files like this: ./app.js: import addModuleInit from './add.as'; runAssemblyScript(); async function runAssemblyScript() { const adddModule = await addModuleInit(); console.log(addModule.add(1, 1)); } ./add.as: export function add(x: i32, y: i32): i32 { return x + y; } If you want to pass in imports from outside of the AssemblyScript environment, you create a file with export declarations defining the types of the imports. You then pass your imports in as an object to the AssemblyScript module init function. The name of the property that defines your imports for a module must be the exact filename of the file exporting the import declarations. For example: ./app.js: import addModuleInit from './add.as'; runAssemblyScript(); async function runAssemblyScript() { const adddModule = await addModuleInit({ 'env.as': { log: console.log } }); console.log(addModule.add(1, 1)); } ./env.as: export declare function log(x: number): void; ./add.as: import { log } from './env.as'; export function add(x: i32, y: i32): i32 { log(x + y); return x + y; } You can also import AssemblyScript from within AssemblyScript files, like so: ./add.as: import { subtract } from './subtract.as'; export function add(x: i32, y: i32): i32 { return subtract(x + y, 0); } ./subtract.as: export function subtract(x: i32, y: i32): i32 { return x - y; } By default, no compiler options have been set. The available options can be found here. You can add options by creating a .json file with an array of option names and values, and telling Zwitterion where to locate it with the --asc-options-file command line option. For example: ./asc-options.json: [ \"--optimizeLevel\", \"3\", \"--runtime\", \"none\", \"--shrinkLevel\", \"2\" ] Tell Zwitterion where to locate it: zwitterion --asc-options-file asc-options.json Rust Rust is a low-level language focused on performance, reliability, and productivity. Learn more here. Rust support is currently very basic (i.e. no wasm-bindgen support). You must have Rust installed on your machine. You can find instructions for installing Rust here. It is a goal of Zwitterion to automatically install a local version of the necessary Rust tooling when Zwitterion is installed, but that is currently a work in progress. Importing Rust is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry Rust module is a function that returns a promise. This function takes as its one parameter an object containing imports to the Rust module. You can import Rust from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.rs'; runRust(); async function runRust() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.rs #![no_main] #[no_mangle] pub fn add(x: i32, y: i32) -> i32 { return x + y; } C C support is currently very basic. You must have Emscripten installed on your machine. You can find instructions for installing Emscripten here. It is a goal of Zwitterion to automatically install a local version of the necessary C tooling when Zwitterion is installed, but that is currently a work in progress. Importing C is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry C module is a function that returns a promise. This function takes as its one parameter an object containing imports to the C module. You can import C from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.c'; runC(); async function runC() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.c int add(int x, int y) { return x + y; } C++ C++ support is currently very basic. You must have Emscripten installed on your machine. You can find instructions for installing Emscripten here. It is a goal of Zwitterion to automatically install a local version of the necessary C++ tooling when Zwitterion is installed, but that is currently a work in progress. Importing C++ is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry C++ module is a function that returns a promise. This function takes as its one parameter an object containing imports to the C++ module. You can import C++ from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.cpp'; runCPP(); async function runCPP() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.cpp extern \"C\" { int add(int x, int y) { return x + y; } } WebAssembly Text Format (Wat) Wat is a textual representation of the Wasm binary format. It allows Wasm to be more easily written by hand. Learn more here. Importing Wat is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry Wat module is a function that returns a promise. This function takes as its one parameter an object containing imports to the Wat module. You can import Wat from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.wat'; runWat(); async function runWat() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.wat (module (func $add (param $x i32) (param $y i32) (result i32) (i32.add (get_local $x) (get_local $y)) ) (export \"add\" (func $add)) ) WebAssembly (Wasm) Wasm is a binary instruction format built to be efficient, safe, portable, and open. Learn more here. Importing Wasm is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry Wasm module is a function that returns a promise. This function takes as its one parameter an object containing imports to the Wasm module. You can import Wasm from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.wasm'; runWasm(); async function runWasm() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.wasm Imagine this is a compiled Wasm binary file with a function called `add` Command Line Options Port Specify the server's port: Build Static Create a static build of the current working directory. The output will be in a directory called dist in the current working directory: Exclude A comma-separated list of paths, relative to the current directory, to exclude from the static build: Include A comma-separated list of paths, relative to the current directory, to include in the static build SPA Root A path to a file, relative to the current directory, to serve as the SPA root. It will be returned for the root path and when a file cannot be found: Disable SPA Disable the SPA redirect to index.html: Headers File A path to a JSON file, relative to the current directory, for custom HTTP headers: --headers-file [headersFile] Custom HTTP headers are specified as a JSON object with the following shape: type CustomHTTPHeaders = { [regexp: string]: HTTPHeaders; } type HTTPHeaders = { [key: string]: string; } For example: ./headers.json { \"^service-worker.ts$\": { \"Service-Worker-Allowed\": \"/\" } } TSC Options File A path to a JSON file, relative to the current directory, for tsc compiler options: --tsc-options-file [tscOptionsFile] The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: ASC Options File A path to a JSON file, relative to the current directory, for asc compiler options: --asc-options-file [ascOptionsFile] By default, no compiler options have been set. The available options can be found here. Options are specified as an array of option names and values. For example: ./asc-options.json: [ \"--optimizeLevel\", \"3\", \"--runtime\", \"none\", \"--shrinkLevel\", \"2\" ] Special Considerations Third-party Packages Third-party packages must be authored as if they were using Zwitterion. Essentially this means they should be authored in standard JavaScript or TypeScript, and AssemblyScript, Rust, C, and C++ must be authored according to their WebAssembly documentation. Notable exceptions will be explained in this documentation. CommonJS (the require syntax), JSON, HTML, or CSS ES Module imports, and other non-standard features that bundlers commonly support are not suppored in source code. Root File It's important to note that Zwitterion assumes that the root file (the file found at /) of your web application is always an index.html file. ES Module script elements Zwitterion depends on native browser support for ES modules (import/export syntax). You must add the type=\"module\" attribute to script elements that reference modules, for example: <script type=\"module\" src=\"amazing-module.ts\"></script> Performance It's important to note that Zwitterion does not bundle files nor engage in tree shaking. This may impact the performance of your application. HTTP2 and ES modules may help with performance, but at this point in time signs tend to point toward worse performance. Zwitterion has plans to improve performance by automatically generating HTTP2 server push information from the static build, and looking into tree shaking, but it is unclear what affect this will have. Stay tuned for more information about performance as Zwitterion matures. With all of the above being said, the performance implications are unclear. Measure for yourself. Read the following for more information on bundling versus not bundling with HTTP2: https://medium.com/@asyncmax/the-right-way-to-bundle-your-assets-for-faster-sites-over-http-2-437c37efe3ff https://stackoverflow.com/questions/30861591/why-bundle-optimizations-are-no-longer-a-concern-in-http-2 http://engineering.khanacademy.org/posts/js-packaging-http2.htm https://blog.newrelic.com/2016/02/09/http2-best-practices-web-performance/ https://mattwilcox.net/web-development/http2-for-front-end-web-developers https://news.ycombinator.com/item?id=9137690 https://www.sitepoint.com/file-bundling-and-http2/ https://medium.freecodecamp.org/javascript-modules-part-2-module-bundling-5020383cf306 https://css-tricks.com/musings-on-http2-and-bundling/ Under the Hood Zwitterion is simple. It is more or less a static file server, but it rewrites requested files in memory as necessary to return to the client. For example, if a TypeScript file is requested from the client, Zwitterion will retrieve the text of the file, compile it to JavaScript, and then return the compiled text to the client. The same thing is done for JavaScript files. In fact, nearly the same process will be used for any file extension that we want to support in the future. For example, in the future, if a C file is requested it will be read into memory, the text will be compiled to WebAssembly, and the WebAssembly will be returned to the client. All of this compilation is done server-side and hidden from the user. To the user, it's just a static file server. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/lastmjs/zwitterion",
        "comments.comment_id": [21913339, 21914342],
        "comments.comment_author": ["andybak", "JMTQp8lwXL"],
        "comments.comment_descendants": [11, 1],
        "comments.comment_time": [
          "2019-12-30T13:58:58Z",
          "2019-12-30T16:17:47Z"
        ],
        "comments.comment_text": [
          "Can anyone point me to a workflow where the cognitive load is similar to how it was in the good old days?<p>i.e. some <i>thing</i> that does everything for me apart from the bit where I write application and presentation code? That mostly just works and doesn't require me to understand the whole stack.<p>Because when I switch to other languages/environments I can most remain blissfully unaware of the plumbing that keeps the thing running. I learned all I needed to learn about pip and virtualenv in under an hour. My C# editor just lets me edit code and hit play.<p>Is there any chance of getting to this state of nirvana for web development?",
          "It doesn't appear to support CSS files (or other flavors like SCSS), so calling it a \"webpack killer\" seems extra. Also, a lot of people need polyfills for node modules written for NodeJS that are used in browser; for example, Buffer. Webpack takes care of all of those nuances for you. I wouldn't call this a killer for my workflows, where I need Webpack features such as aliasing, etc.<p>That being said, I support the effort to make a simpler, superior bundler. My disagreement is more so with the marketing in the title. Though, those sort of claims are what drive clicks today."
        ],
        "id": "09a01b3f-6300-450a-b0d2-a77b2906633f",
        "url_text": "Zwitterion A web dev server that lets you import anything* * If by anything you mean: JavaScript ES2015+, TypeScript, JSON, JSX, TSX, AssemblyScript, Rust, C, C++, WebAssembly, and in the future anything that compiles to JavaScript or WebAssembly. Zwitterion is designed to be an instant replacement for your current web development static file server. Production deployments are also possible through the static build. For example, you can write stuff like the following and it just works: ./index.html: <!DOCTYPE html> <html> <head> <script type=\"module\" src=\"app.ts\"></script> </head> <body> This is the simplest developer experience I've ever had! </body> </html> ./app.ts: import { getHelloWorld } from './hello-world.ts'; const helloWorld: string = getHelloWorld(); console.log(helloWorld); ./hello-world.ts: export function getHelloWorld(): string { return 'Why hello there world!'; } Really, it just works. Zwitterion lets you get back to the good old days of web development. Just write your source code in any supported language and run it in the browser. Also...Zwitterion is NOT a bundler. It eschews bundling for a simpler experience. Current Features ES2015+ TypeScript JSON JSX TSX AssemblyScript Rust (basic support) C (basic support) C++ (basic support) WebAssembly Text Format (Wat) WebAssembly (Wasm) Bare imports (import * as stuff from 'library'; instead of import * as stuff from '../node_modules/library/index.js';) Single Page Application routing (by default the server returns index.html on unhandled routes) Static build for production deployment Upcoming Features More robust Rust integration (i.e. automatic local Rust installation during npm installation) More robust C integration More robust C++ integration Import maps HTTP2 optimizations Documentation Examples Installation and Basic Use Production Use Languages JavaScript TypeScript JSON JSX TSX AssemblyScript Rust C C++ WebAssembly Text Format (Wat) WebAssembly (Wasm) Command Line Options Special Considerations Under the Hood Installation and Basic Use Local Installation and Use Install Zwitterion in the directory that you would like to serve files from: Run Zwitterion by accessing its executable directly from the terminal: node_modules/.bin/zwitterion or from an npm script: { ... \"scripts\": { \"start\": \"zwitterion\" } ... } Global Installation and Use Install Zwitterion globally to use across projects: npm install -g zwitterion Run Zwitterion from the terminal: or from an npm script: { ... \"scripts\": { \"start\": \"zwitterion\" } ... } Production Use It is recommended to use Zwitterion in production by creating a static build of your project. A static build essentially runs all relevant files through Zwitterion, and copies those and all other files in your project to a dist directory. You can take this directory and upload it to a Content Delivery Network (CDN), or another static file hosting service. You may also use a running Zwitterion server in production, but for performance and potential security reasons it is not recommended. To create a static build, run Zwitterion with the --build-static option. You will probably need to add the application/javascript MIME type to your hosting provider for your TypeScript, AssemblyScript, Rust, Wasm, and Wat files. From the terminal: zwitterion --build-static From an npm script: { ... \"scripts\": { \"build-static\": \"zwitterion --build-static\" } ... } The static build will be located in a directory called dist, in the same directory that you ran the --build-static command from. Languages JavaScript JavaScript is the language of the web. You can learn more here. Importing JavaScript ES2015+ is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.js: import { helloWorld } from './hello-world.js'; console.log(helloWorld()); ./hello-world.js: export function helloWorld() { return 'Hello world!'; } JavaScript transpilation is done by the TypeScript compiler. By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json TypeScript TypeScript is a typed superset of JavaScript. You can learn more here. Importing TypeScript is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.ts: import { helloWorld } from './hello-world.ts'; console.log(helloWorld()); ./hello-world.ts: export function helloWorld(): string { return 'Hello world!'; } By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json JSON JSON is provided as a default export. It is recommended to use explicit file extensions: ./app.js: import helloWorld from './hello-world.json'; console.log(helloWorld); ./hello-world.json: JSX Importing JSX is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.js: import { helloWorldElement } from './hello-world.jsx'; ReactDOM.render( helloWorldElement, document.getElementById('root') ); ./hello-world.jsx: export const hellowWorldElement = <h1>Hello, world!</h1>; JSX transpilation is done by the TypeScript compiler. By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json TSX Importing TSX is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.js: import { helloWorldElement } from './hello-world.tsx'; ReactDOM.render( helloWorldElement, document.getElementById('root') ); ./hello-world.tsx: const helloWorld: string = 'Hello, world!'; export const hellowWorldElement = <h1>{ helloWorld }</h1>; TSX transpilation is done by the TypeScript compiler. By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json AssemblyScript AssemblyScript is a new language that compiles a strict subset of TypeScript to WebAssembly. You can learn more about it in The AssemblyScript Book. Zwitterion assumes that AssemblyScript files have the .as file extension. This is a Zwitterion-specific extension choice, as the AssemblyScript project has not yet chosen its own official file extension. You can follow that discussion here. Zwitterion will follow the official extension choice once it is made. Importing AssemblyScript is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry AssemblyScript module is a function that returns a promise. This function takes as its one parameter an object containing imports to the AssemblyScript module. Passing values to and from functions exported from AssemblyScript modules should be straightforward, but there are some limitations. Zwitterion uses as-bind under the hood to broker values to and from AssemblyScript modules. Look there if you need more information. You can import AssemblyScript from JavaScript or TypeScript files like this: ./app.js: import addModuleInit from './add.as'; runAssemblyScript(); async function runAssemblyScript() { const adddModule = await addModuleInit(); console.log(addModule.add(1, 1)); } ./add.as: export function add(x: i32, y: i32): i32 { return x + y; } If you want to pass in imports from outside of the AssemblyScript environment, you create a file with export declarations defining the types of the imports. You then pass your imports in as an object to the AssemblyScript module init function. The name of the property that defines your imports for a module must be the exact filename of the file exporting the import declarations. For example: ./app.js: import addModuleInit from './add.as'; runAssemblyScript(); async function runAssemblyScript() { const adddModule = await addModuleInit({ 'env.as': { log: console.log } }); console.log(addModule.add(1, 1)); } ./env.as: export declare function log(x: number): void; ./add.as: import { log } from './env.as'; export function add(x: i32, y: i32): i32 { log(x + y); return x + y; } You can also import AssemblyScript from within AssemblyScript files, like so: ./add.as: import { subtract } from './subtract.as'; export function add(x: i32, y: i32): i32 { return subtract(x + y, 0); } ./subtract.as: export function subtract(x: i32, y: i32): i32 { return x - y; } By default, no compiler options have been set. The available options can be found here. You can add options by creating a .json file with an array of option names and values, and telling Zwitterion where to locate it with the --asc-options-file command line option. For example: ./asc-options.json: [ \"--optimizeLevel\", \"3\", \"--runtime\", \"none\", \"--shrinkLevel\", \"2\" ] Tell Zwitterion where to locate it: zwitterion --asc-options-file asc-options.json Rust Rust is a low-level language focused on performance, reliability, and productivity. Learn more here. Rust support is currently very basic (i.e. no wasm-bindgen support). You must have Rust installed on your machine. You can find instructions for installing Rust here. It is a goal of Zwitterion to automatically install a local version of the necessary Rust tooling when Zwitterion is installed, but that is currently a work in progress. Importing Rust is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry Rust module is a function that returns a promise. This function takes as its one parameter an object containing imports to the Rust module. You can import Rust from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.rs'; runRust(); async function runRust() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.rs #![no_main] #[no_mangle] pub fn add(x: i32, y: i32) -> i32 { return x + y; } C C support is currently very basic. You must have Emscripten installed on your machine. You can find instructions for installing Emscripten here. It is a goal of Zwitterion to automatically install a local version of the necessary C tooling when Zwitterion is installed, but that is currently a work in progress. Importing C is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry C module is a function that returns a promise. This function takes as its one parameter an object containing imports to the C module. You can import C from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.c'; runC(); async function runC() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.c int add(int x, int y) { return x + y; } C++ C++ support is currently very basic. You must have Emscripten installed on your machine. You can find instructions for installing Emscripten here. It is a goal of Zwitterion to automatically install a local version of the necessary C++ tooling when Zwitterion is installed, but that is currently a work in progress. Importing C++ is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry C++ module is a function that returns a promise. This function takes as its one parameter an object containing imports to the C++ module. You can import C++ from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.cpp'; runCPP(); async function runCPP() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.cpp extern \"C\" { int add(int x, int y) { return x + y; } } WebAssembly Text Format (Wat) Wat is a textual representation of the Wasm binary format. It allows Wasm to be more easily written by hand. Learn more here. Importing Wat is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry Wat module is a function that returns a promise. This function takes as its one parameter an object containing imports to the Wat module. You can import Wat from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.wat'; runWat(); async function runWat() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.wat (module (func $add (param $x i32) (param $y i32) (result i32) (i32.add (get_local $x) (get_local $y)) ) (export \"add\" (func $add)) ) WebAssembly (Wasm) Wasm is a binary instruction format built to be efficient, safe, portable, and open. Learn more here. Importing Wasm is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry Wasm module is a function that returns a promise. This function takes as its one parameter an object containing imports to the Wasm module. You can import Wasm from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.wasm'; runWasm(); async function runWasm() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.wasm Imagine this is a compiled Wasm binary file with a function called `add` Command Line Options Port Specify the server's port: Build Static Create a static build of the current working directory. The output will be in a directory called dist in the current working directory: Exclude A comma-separated list of paths, relative to the current directory, to exclude from the static build: Include A comma-separated list of paths, relative to the current directory, to include in the static build SPA Root A path to a file, relative to the current directory, to serve as the SPA root. It will be returned for the root path and when a file cannot be found: Disable SPA Disable the SPA redirect to index.html: Headers File A path to a JSON file, relative to the current directory, for custom HTTP headers: --headers-file [headersFile] Custom HTTP headers are specified as a JSON object with the following shape: type CustomHTTPHeaders = { [regexp: string]: HTTPHeaders; } type HTTPHeaders = { [key: string]: string; } For example: ./headers.json { \"^service-worker.ts$\": { \"Service-Worker-Allowed\": \"/\" } } TSC Options File A path to a JSON file, relative to the current directory, for tsc compiler options: --tsc-options-file [tscOptionsFile] The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: ASC Options File A path to a JSON file, relative to the current directory, for asc compiler options: --asc-options-file [ascOptionsFile] By default, no compiler options have been set. The available options can be found here. Options are specified as an array of option names and values. For example: ./asc-options.json: [ \"--optimizeLevel\", \"3\", \"--runtime\", \"none\", \"--shrinkLevel\", \"2\" ] Special Considerations Third-party Packages Third-party packages must be authored as if they were using Zwitterion. Essentially this means they should be authored in standard JavaScript or TypeScript, and AssemblyScript, Rust, C, and C++ must be authored according to their WebAssembly documentation. Notable exceptions will be explained in this documentation. CommonJS (the require syntax), JSON, HTML, or CSS ES Module imports, and other non-standard features that bundlers commonly support are not suppored in source code. Root File It's important to note that Zwitterion assumes that the root file (the file found at /) of your web application is always an index.html file. ES Module script elements Zwitterion depends on native browser support for ES modules (import/export syntax). You must add the type=\"module\" attribute to script elements that reference modules, for example: <script type=\"module\" src=\"amazing-module.ts\"></script> Performance It's important to note that Zwitterion does not bundle files nor engage in tree shaking. This may impact the performance of your application. HTTP2 and ES modules may help with performance, but at this point in time signs tend to point toward worse performance. Zwitterion has plans to improve performance by automatically generating HTTP2 server push information from the static build, and looking into tree shaking, but it is unclear what affect this will have. Stay tuned for more information about performance as Zwitterion matures. With all of the above being said, the performance implications are unclear. Measure for yourself. Read the following for more information on bundling versus not bundling with HTTP2: https://medium.com/@asyncmax/the-right-way-to-bundle-your-assets-for-faster-sites-over-http-2-437c37efe3ff https://stackoverflow.com/questions/30861591/why-bundle-optimizations-are-no-longer-a-concern-in-http-2 http://engineering.khanacademy.org/posts/js-packaging-http2.htm https://blog.newrelic.com/2016/02/09/http2-best-practices-web-performance/ https://mattwilcox.net/web-development/http2-for-front-end-web-developers https://news.ycombinator.com/item?id=9137690 https://www.sitepoint.com/file-bundling-and-http2/ https://medium.freecodecamp.org/javascript-modules-part-2-module-bundling-5020383cf306 https://css-tricks.com/musings-on-http2-and-bundling/ Under the Hood Zwitterion is simple. It is more or less a static file server, but it rewrites requested files in memory as necessary to return to the client. For example, if a TypeScript file is requested from the client, Zwitterion will retrieve the text of the file, compile it to JavaScript, and then return the compiled text to the client. The same thing is done for JavaScript files. In fact, nearly the same process will be used for any file extension that we want to support in the future. For example, in the future, if a C file is requested it will be read into memory, the text will be compiled to WebAssembly, and the WebAssembly will be returned to the client. All of this compilation is done server-side and hidden from the user. To the user, it's just a static file server. ",
        "_version_": 1718536556518572032
      },
      {
        "story_id": 18855503,
        "story_author": "javinpaul",
        "story_descendants": 2,
        "story_score": 8,
        "story_time": "2019-01-08T14:00:07Z",
        "story_title": "Master npm",
        "search": [
          "Master npm",
          "https://hashnode.com/post/master-npm-in-under-10-minutes-or-get-your-money-back-cjqmak392001i7vs2ufdlvcqb",
          "Weve all heard about Gulp, Grunt, Webpack or whatever tool that's cool at the time you're reading this article. All these tools help with automating your project's build tasks in one way or another. We all know that JavaScript is an interpreted language, so there is no actual code translation into bytecode done during this phase. However, it is also true that given JavaScripts ecosystem, depending on the project, there can be quite a lot of tasks to perform prior, during, and after preparing our code for deployment. These tasks could include things like transpiling your TypeScript files into the actual JavasScript that will get executed, or your SCSS files into CSS, maybe even compile all your images into a sprite sheet for better web performance. And that is just to name a few, there are tons more. With that being said, it is normal that many tools have been created during the past few years to help developers deal with the associated deployment pains. However, one particular tool thats been there since the beginning has been vastly ignored. Im referring to npm, and in this mastering npm tutorial, Im going to show you a set of tips and tricks you can use to cover most of your orchestration needs with it and become an npm master. Please see this is a beginner's guide and if you are just getting started you are in the right place. Let's start! Tip No.1 - Initializing New Packages To start you off with a simple one, you dont need generators for your new projects. Yes, of course, if youre planning on creating a new Express-based project, using Express own generator might be a good idea. But if youre starting something new, you might want to use npms own project generator. Yes, you read right, npm comes with a built-in generator, which helps you complete the basic information every npm project should have. This is the command you should use: your-folder> $ npm init Thats it, simple, direct, to the point and you get the following as a response: You basically get asked a set of questions to fill in your package.json file, which acts as a basic manifest for your project containing a basic description and some extra useful metadata about it e.g. the authors name, what kind of license its being published under, and so on. Note that some of the questions have an option between parenthesis. That means you can simply hit ENTER and that text will be entered by default, which comes extra handy when youre not yet sure how to answer some of the questions. Finally, at the end, you get a preview of how your new package.json file will look like. Then, hit ENTER again and itll be saved in your projects folder. Remember that anything and everything youve entered here can be changed in the future by editing the JSON file. Tip No.2 - Freezing Dependency Versions This is another simple, yet a forgotten feature. Did you know that if you install a new package without specifying its version, itll save it to your package.json file as version ^X.Y.Z? Meaning, at least, make sure you install version X.Y.Z of the package, but if there is a new one that matches X, then install that one (so it could match 1.0.0 and 1.9.9 as well). Maybe you dont see a big issue with that, but in my years of working with Node.js, I cant tell you how many times a poorly managed version change in one of my dependencies ended up preventing my code from working. It sounds crazy, but there is nothing out there that prevents OSS maintainers from making these types of mistakes. In fact, there was recently a problem with a hacker who gained access to the repository of a highly used package and after adding some malicious code, bumped the version up, so projects without version lock would install its new version without even asking for it. I believe this default behavior from npm comes from an innate trust in OSS maintainers, but as weve come to learn in the past, this is not a very good practice. And you could be fooled into thinking this is easily fixed by removing the caret or any other symbol from the version number. That would tell the package manager to download exactly that version, and no other. In that case, the problem comes from your dependencies, if theyve not done the same and version locked their own dependencies, then once you install them, youll run into the same problem. So, how do we fix this problem? Enter npm shrinkwrap $ npm shrinkwrap The above command will rename your package.json file into npm-shrinkwrap.json, and inside the dependencies element, youll notice every single dependency the project and its dependencies have. And from now on, that file will be used whenever you run npm install. To give you an example, for a single dependency such as express, you end up with a 350+ lines JSON file. So yeah, there are a lot of dependencies to deal with. Youll notice that in your new JSON file, youll have a version that starts with ~, which although not an exact match, will only allow updates on minor versions, so going back to the example, you could match 1.0.0 all the way to 1.0.X (with X being any valid number, of course). The point here is that minor version updates are only related to bug fixes that dont break backward compatibility. With that being said, the version bump is still something that needs to be done manually, so if you want to be 100% sure, you should remove those ~ from versions everywhere. Tip No.3 - Running Scripts Let's now move on to more interesting orchestration opportunities, shall we? Running scripts is one of the key feats you can achieve with npm that will give you a chance to start worrying about Grunt, Gulp or any other alternative. Thanks to this feature, you can even use npm as a language agnostic task runner! Its as simple as doing this: By adding the scripts element to your JSON file, you gain the ability to add extra functionality to npm. Out of the box, there are some commands that npm already expects such as start and test. So you can use those directly like this: $ npm start And the command you specify (node index.js in this example) will be executed. If on the other hand, you want to go ahead and add custom commands (which is the whole point of doing this), you can run them using npm run, like this: $ npm run <command-name> So, for example, on our screenshot, if we wanted to do the production preparations, we can do: $ npm run prod-pred And our prod-prep.sh script, is just a simple Bash script that could look like the following or as complex as you need it: #!/bin/bash echo \"== Doing the production preps ==\" echo \"Running tests...\" mocha test echo \"Linting the code...\" jshint **.js Its a very simple mechanic, but powerful at the same time, without the need for an extra task runner. We can even use hooks to automate scripts depending on what were trying to do, so lets look at that now. You can also get the list of available commands by typing: $ npm run With no arguments, npm will return the full list of commands, grouped by Lifecycle related and those available through npm run, like this: Lifecycle scripts included in test2-npm: test mocha test start node index.js available via `npm run-script`: prod-pred sh prod-prep.sh Tip No.4 - Running Scripts Before and After Other Scripts Although running scripts is one of the most powerful features of npm, giving it the ability to come much more than simply a package manager. It goes one step further by providing you with hooks. Long story short, every single command you can configure for your project will have a pre and post hook. Even those which you dont have full control over, such as install, uninstall, publish or update. So you can do things like: { \"name\": \"test\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"prepublish\": \"mocha test\", \"build\": \"sh build-project.sh\", \"postbuild\": \"rm -rf ./tmp\" }, \"author\": \"\", \"license\": \"ISC\", \"dependencies\": { \"mocha\": \"^5.2.0\" } } Notice how Ive added a pre-publish hook, to make sure all tests pass before publishing the package (that single line will make sure you dont forget about doing that ever again!). And Ive also added a simple post-build hook, showing you how you can add hooks to your own commands as well, in this case, it will remove any temporary folder or files created by a generic build process. The main takeaway here is that you can add hooks to existing and custom commands and the execution flow will look at their exit codes, meaning that if the tests fail for our first hook, the publish command will not be executed. Tip No.5 - Automatically Increasing Package Versions Another little-known feature of npm is that it allows you to automatically increase your packages version number. The versioning system used by npm is semver, so you have three variants to run whenever you need to bump up your version number: $ npm version patch $ npm version minor $ npm version major To make sure were all on the same page: semver works by having 3 numbers for each version, distributed like this: major.minor.patch And their meanings are: Major: references the main package version number. Changes to this number imply that non-backward compatible changes have been made, either to a feature or to the entire thing. Either way, rest assured that if you were using it and jump to the next major version, unless you update your own code, something will break. Minor: similar to the previous one, but a change in the minor version references a backward-compatible one. Meaning that most likely new features have been added to the package without breaking existing ones. If this number changes, it is recommended that you go through the documentation just to be sure. Patch: as already discussed, changes to this number on the version reference small fixes, usually bug fixes. Although it might seem like a small feature, you could pair it with scripts and hooks and come up with something like this: { \"name\": \"test\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"publish:fix\": \"npm publish\", \"publish:minor\": \"npm publish\", \"publish:major\": \"npm publish\", \"prepublish:fix\": \"npm version patch\", \"prepublish:minor\": \"npm version minor\", \"prepublish:major\": \"npm version major\" }, \"author\": \"\", \"license\": \"ISC\", \"dependencies\": { \"mocha\": \"^5.2.0\" } } And with that, you can simply choose what youre publishing and forget about the version number. For example: $ npm run publish:fix Tip No.6 - Using npx Sometimes your packages or others will add command line tools for you to use, Ive been using the example of mocha, for instance, which adds the mocha command that needs to be executed in order to run and verify the tests. Usually, these commands are either installed in the global cache or in the local node_modules folder, in which case theyre not readily accessible for you to use by simply typing their name. Since version 5.2, npm added the npx command, which simplifies that task for you. Instead of having to know where the binary is located (if at all), itll look for it in your $PATH and then (if not found) in your local node_modules/.bin . Not only that, but if it cant find it anywhere, itll install the package for you and then run the binary, all in one step. Using npx comes in handy when you need the power of a cli tool available from npms global registry, but you dont want to have it installed on your system. Tip No.7 - Adding npm Autocomplete to Your Bash This one is strictly for *nix based systems or Bash for Windows 10, but I think its pretty useful to have under your tool belt. One thing that is very useful when using Bash or any command line interface is the autocomplete feature they usually provide. The problem comes when youre using custom command line tools, they dont always provide auto-complete for their own arguments and (or) commands. Luckily for us, npm allows you to add this feature to your bash, simply with a single line of code: $ npm completion >> ~/.bashrc Warning: Make sure youre doing the double >, otherwise youd be replacing your .bashrc file content, instead of adding the required code for autocomplete. After that, you can either open a new terminal or run: $ source ~/.bashrc Thatll reload your Bash configuration and the autocomplete with it. Once this is done, you can test it by typing npm ins and hitting TAB. Note that the autocomplete feature also works for custom commands too, so you can also get that extra help if youre heavy on the scripts! Final tips In this npm tutorial, I covered several ways to improve your \"npm-fu\", such as using scripts and hooks to run custom actions, automating your version handling and locking dependencies versions. That being said, if you liked what you read, go check out npms official documentation, there are many things you can do with this tool that will probably help you get rid of some extra dependencies you might already have in your project. Leave a comment if any of these tips where useful to you or send me a message on Hashnode if you have any questions about them! "
        ],
        "story_type": "Normal",
        "url_raw": "https://hashnode.com/post/master-npm-in-under-10-minutes-or-get-your-money-back-cjqmak392001i7vs2ufdlvcqb",
        "comments.comment_id": [18855656, 18855672],
        "comments.comment_author": ["matharmin", "orf"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-01-08T14:24:23Z",
          "2019-01-08T14:26:56Z"
        ],
        "comments.comment_text": [
          "The article covers the basics, but not even close to mastering.<p>shrinkwrap should really not be used anymore - package-lock is mostly automatic, and much better than shrinkwrap for what you want to do in most projects.<p>If you want to talk about mastering npm, there are way more topics to cover, such as resolving issues when upgrading (`npm ls` helps here) and checking for vulnerabilities (`npm audit`).",
          "I'm not sure if any of this can be considered mastering NPM. In reality this is a uber-basic introduction to a handful of the most simple commands."
        ],
        "id": "3d964580-fbd5-4161-8701-42fc9ed84f68",
        "url_text": "Weve all heard about Gulp, Grunt, Webpack or whatever tool that's cool at the time you're reading this article. All these tools help with automating your project's build tasks in one way or another. We all know that JavaScript is an interpreted language, so there is no actual code translation into bytecode done during this phase. However, it is also true that given JavaScripts ecosystem, depending on the project, there can be quite a lot of tasks to perform prior, during, and after preparing our code for deployment. These tasks could include things like transpiling your TypeScript files into the actual JavasScript that will get executed, or your SCSS files into CSS, maybe even compile all your images into a sprite sheet for better web performance. And that is just to name a few, there are tons more. With that being said, it is normal that many tools have been created during the past few years to help developers deal with the associated deployment pains. However, one particular tool thats been there since the beginning has been vastly ignored. Im referring to npm, and in this mastering npm tutorial, Im going to show you a set of tips and tricks you can use to cover most of your orchestration needs with it and become an npm master. Please see this is a beginner's guide and if you are just getting started you are in the right place. Let's start! Tip No.1 - Initializing New Packages To start you off with a simple one, you dont need generators for your new projects. Yes, of course, if youre planning on creating a new Express-based project, using Express own generator might be a good idea. But if youre starting something new, you might want to use npms own project generator. Yes, you read right, npm comes with a built-in generator, which helps you complete the basic information every npm project should have. This is the command you should use: your-folder> $ npm init Thats it, simple, direct, to the point and you get the following as a response: You basically get asked a set of questions to fill in your package.json file, which acts as a basic manifest for your project containing a basic description and some extra useful metadata about it e.g. the authors name, what kind of license its being published under, and so on. Note that some of the questions have an option between parenthesis. That means you can simply hit ENTER and that text will be entered by default, which comes extra handy when youre not yet sure how to answer some of the questions. Finally, at the end, you get a preview of how your new package.json file will look like. Then, hit ENTER again and itll be saved in your projects folder. Remember that anything and everything youve entered here can be changed in the future by editing the JSON file. Tip No.2 - Freezing Dependency Versions This is another simple, yet a forgotten feature. Did you know that if you install a new package without specifying its version, itll save it to your package.json file as version ^X.Y.Z? Meaning, at least, make sure you install version X.Y.Z of the package, but if there is a new one that matches X, then install that one (so it could match 1.0.0 and 1.9.9 as well). Maybe you dont see a big issue with that, but in my years of working with Node.js, I cant tell you how many times a poorly managed version change in one of my dependencies ended up preventing my code from working. It sounds crazy, but there is nothing out there that prevents OSS maintainers from making these types of mistakes. In fact, there was recently a problem with a hacker who gained access to the repository of a highly used package and after adding some malicious code, bumped the version up, so projects without version lock would install its new version without even asking for it. I believe this default behavior from npm comes from an innate trust in OSS maintainers, but as weve come to learn in the past, this is not a very good practice. And you could be fooled into thinking this is easily fixed by removing the caret or any other symbol from the version number. That would tell the package manager to download exactly that version, and no other. In that case, the problem comes from your dependencies, if theyve not done the same and version locked their own dependencies, then once you install them, youll run into the same problem. So, how do we fix this problem? Enter npm shrinkwrap $ npm shrinkwrap The above command will rename your package.json file into npm-shrinkwrap.json, and inside the dependencies element, youll notice every single dependency the project and its dependencies have. And from now on, that file will be used whenever you run npm install. To give you an example, for a single dependency such as express, you end up with a 350+ lines JSON file. So yeah, there are a lot of dependencies to deal with. Youll notice that in your new JSON file, youll have a version that starts with ~, which although not an exact match, will only allow updates on minor versions, so going back to the example, you could match 1.0.0 all the way to 1.0.X (with X being any valid number, of course). The point here is that minor version updates are only related to bug fixes that dont break backward compatibility. With that being said, the version bump is still something that needs to be done manually, so if you want to be 100% sure, you should remove those ~ from versions everywhere. Tip No.3 - Running Scripts Let's now move on to more interesting orchestration opportunities, shall we? Running scripts is one of the key feats you can achieve with npm that will give you a chance to start worrying about Grunt, Gulp or any other alternative. Thanks to this feature, you can even use npm as a language agnostic task runner! Its as simple as doing this: By adding the scripts element to your JSON file, you gain the ability to add extra functionality to npm. Out of the box, there are some commands that npm already expects such as start and test. So you can use those directly like this: $ npm start And the command you specify (node index.js in this example) will be executed. If on the other hand, you want to go ahead and add custom commands (which is the whole point of doing this), you can run them using npm run, like this: $ npm run <command-name> So, for example, on our screenshot, if we wanted to do the production preparations, we can do: $ npm run prod-pred And our prod-prep.sh script, is just a simple Bash script that could look like the following or as complex as you need it: #!/bin/bash echo \"== Doing the production preps ==\" echo \"Running tests...\" mocha test echo \"Linting the code...\" jshint **.js Its a very simple mechanic, but powerful at the same time, without the need for an extra task runner. We can even use hooks to automate scripts depending on what were trying to do, so lets look at that now. You can also get the list of available commands by typing: $ npm run With no arguments, npm will return the full list of commands, grouped by Lifecycle related and those available through npm run, like this: Lifecycle scripts included in test2-npm: test mocha test start node index.js available via `npm run-script`: prod-pred sh prod-prep.sh Tip No.4 - Running Scripts Before and After Other Scripts Although running scripts is one of the most powerful features of npm, giving it the ability to come much more than simply a package manager. It goes one step further by providing you with hooks. Long story short, every single command you can configure for your project will have a pre and post hook. Even those which you dont have full control over, such as install, uninstall, publish or update. So you can do things like: { \"name\": \"test\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"prepublish\": \"mocha test\", \"build\": \"sh build-project.sh\", \"postbuild\": \"rm -rf ./tmp\" }, \"author\": \"\", \"license\": \"ISC\", \"dependencies\": { \"mocha\": \"^5.2.0\" } } Notice how Ive added a pre-publish hook, to make sure all tests pass before publishing the package (that single line will make sure you dont forget about doing that ever again!). And Ive also added a simple post-build hook, showing you how you can add hooks to your own commands as well, in this case, it will remove any temporary folder or files created by a generic build process. The main takeaway here is that you can add hooks to existing and custom commands and the execution flow will look at their exit codes, meaning that if the tests fail for our first hook, the publish command will not be executed. Tip No.5 - Automatically Increasing Package Versions Another little-known feature of npm is that it allows you to automatically increase your packages version number. The versioning system used by npm is semver, so you have three variants to run whenever you need to bump up your version number: $ npm version patch $ npm version minor $ npm version major To make sure were all on the same page: semver works by having 3 numbers for each version, distributed like this: major.minor.patch And their meanings are: Major: references the main package version number. Changes to this number imply that non-backward compatible changes have been made, either to a feature or to the entire thing. Either way, rest assured that if you were using it and jump to the next major version, unless you update your own code, something will break. Minor: similar to the previous one, but a change in the minor version references a backward-compatible one. Meaning that most likely new features have been added to the package without breaking existing ones. If this number changes, it is recommended that you go through the documentation just to be sure. Patch: as already discussed, changes to this number on the version reference small fixes, usually bug fixes. Although it might seem like a small feature, you could pair it with scripts and hooks and come up with something like this: { \"name\": \"test\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"publish:fix\": \"npm publish\", \"publish:minor\": \"npm publish\", \"publish:major\": \"npm publish\", \"prepublish:fix\": \"npm version patch\", \"prepublish:minor\": \"npm version minor\", \"prepublish:major\": \"npm version major\" }, \"author\": \"\", \"license\": \"ISC\", \"dependencies\": { \"mocha\": \"^5.2.0\" } } And with that, you can simply choose what youre publishing and forget about the version number. For example: $ npm run publish:fix Tip No.6 - Using npx Sometimes your packages or others will add command line tools for you to use, Ive been using the example of mocha, for instance, which adds the mocha command that needs to be executed in order to run and verify the tests. Usually, these commands are either installed in the global cache or in the local node_modules folder, in which case theyre not readily accessible for you to use by simply typing their name. Since version 5.2, npm added the npx command, which simplifies that task for you. Instead of having to know where the binary is located (if at all), itll look for it in your $PATH and then (if not found) in your local node_modules/.bin . Not only that, but if it cant find it anywhere, itll install the package for you and then run the binary, all in one step. Using npx comes in handy when you need the power of a cli tool available from npms global registry, but you dont want to have it installed on your system. Tip No.7 - Adding npm Autocomplete to Your Bash This one is strictly for *nix based systems or Bash for Windows 10, but I think its pretty useful to have under your tool belt. One thing that is very useful when using Bash or any command line interface is the autocomplete feature they usually provide. The problem comes when youre using custom command line tools, they dont always provide auto-complete for their own arguments and (or) commands. Luckily for us, npm allows you to add this feature to your bash, simply with a single line of code: $ npm completion >> ~/.bashrc Warning: Make sure youre doing the double >, otherwise youd be replacing your .bashrc file content, instead of adding the required code for autocomplete. After that, you can either open a new terminal or run: $ source ~/.bashrc Thatll reload your Bash configuration and the autocomplete with it. Once this is done, you can test it by typing npm ins and hitting TAB. Note that the autocomplete feature also works for custom commands too, so you can also get that extra help if youre heavy on the scripts! Final tips In this npm tutorial, I covered several ways to improve your \"npm-fu\", such as using scripts and hooks to run custom actions, automating your version handling and locking dependencies versions. That being said, if you liked what you read, go check out npms official documentation, there are many things you can do with this tool that will probably help you get rid of some extra dependencies you might already have in your project. Leave a comment if any of these tips where useful to you or send me a message on Hashnode if you have any questions about them! ",
        "_version_": 1718536435434258432
      },
      {
        "story_id": 19108787,
        "story_author": "jaxxstorm",
        "story_descendants": 346,
        "story_score": 263,
        "story_time": "2019-02-07T21:31:59Z",
        "story_title": "Why are we templating YAML?",
        "search": [
          "Why are we templating YAML?",
          "https://leebriggs.co.uk/blog/2019/02/07/why-are-we-templating-yaml.html",
          "Published Feb 7, 2019 by Lee Briggs #kubernetes #configuration mgmt #jsonnet #helm #kr8 I was at cfgmgmtcamp 2019 in Ghent, and did a talk which I think was well received about the need for some Kubernetes configuration management as well as the solution we built for it at $work, kr8. I made a statement during the talk which ignited some fairly fierce discussion both online, and at the conference: \"If you're starting to template yaml, ask yourself the question: why am I not *generating* json?\" - @briggsl spitting straight fire at #cfgmgmtcamp eric sorenson (@ahpook) February 5, 2019 To put this into my own words: At some point, we decided it was okay for us to template yaml. When did this happen? How is this acceptable? After some conversation, I figured it was probably best to back up my claims in some way. This blog post is going to try to do that. The configuration problem Once the applications and infrastructure youre going to manage grows past a certain size, you inevitably end up in some form of configuration complexity hell. If youre only deploying 1 or maybe 2 things, you can write a yaml configuration file and be done with it. However once you grow beyond that, you need to figure out how to manage this complexity. Its incredibly likely that the reason you have multiple configuration files is because the $thing that uses that config is slightly different from its companions. Examples of this include: Applications deployed in different environments, like dev, stg and prod Applications deployed in different regions, like Europe or North American Obviously, not all the configuration is different here, but its likely the configuration differs enough that you want to be able to differentiate between the two. This configuration complexity has been well known for Operators (System Administrators, DevOps engineers, whatever you want to call them) for some years now. An entire discpline grew up around this in Configuration Management, and each tool solved this problem in their own way, but ultimately, they used YAML to get the job done. My favourite method has always been hiera which comes bundled with Puppet. Having the ability to hierarchically look up the variables of specific config needs is incredibly powerful and flexible, and has generally meant you dont actually need to do any templating of yaml at all, except perhaps for embedding Puppet facts into the yaml. Did we go backwards? Then, as our industries needs moved above the operating system and into cloud computing, we had a whole new data plane to configure. The tooling to configure this changed, and tools like CloudFormation and Helm appeared. These tools are excellent configuration tools, but I firmly believe we (as an industry) got something really, really wrong when we designed them. To examine that, lets take a look at example of a helm chart taking a custom parameter Helm Charts Helm charts can take external parameters defined by an values.yaml file which you specify when rendering the chart. A simple example might look like this: Lets say my external parameter is simple - its a string. Itd look a bit like this: image: \"{{ .Values.image }}\" Thats not so bad right? You just specify a value for image in your values.yaml and youre on your way. The real problem starts to get highlighted when you want to do more complicated and complex things. In this particular example, youre doing okay because you know you have to specify an image for a Kubernetes deployment. However, what if youre working with something like an optional field? Well, then it gets a little more unwieldy: {{- with .resourceGroup }} resourceGroup: {{ . }} {{- end }} Optional values just make things ugly in templating languages, and you cant just leave the value blank, so you have to resort to ugly loops and conditionals that are probably going to bite you later. Lets say you need to go a step further, and you need to push an array or map into the config. With helm, youd do something like this. {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 8 }} {{- end }} Firstly, lets ignore the madness of having a templating function toYaml to convert yaml to yaml and focus more on the whitespace issue here. YAML has strict requirements and whitespace implementation rules. The following, for example, is not valid or complete yaml: something: nothing hello: goodbye Generally, if youre handwriting something, this isnt necessarily a problem because you just hit backspace twice and its fixed. However, if youre generating YAML using a templating system, you cant do that - and if youre operating above 5 or 10 configuration files, you probably want to be generating your config rather than writing it. So, in the above example, you want to embed the values of .Values.podAnnotations under the annotations field, which is indented already. So youre having to not only indent your values, but indent them correctly. What makes this even more confusing is that the go parser doesnt actually know anything about YAML at all, so if you try to keep the syntax clean and indent the templates like this: {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 6 }} {{- end }} You actually cant do that, because the templating system gets confused. This is a singular example of the complexity and difficulty you end up facing when generating config data in YAML, but when you really start to do more complex work, it really starts to become obvious that this isnt the way to go. Needless to say, this isnt what I want to spend my time doing. If fiddling around with whitespace requirements in a templating system doing something its not really designed for is what suits you, then Im not going to stop you. I also dont want to spend my time writing configuration in JSON without comments and accidentally missing commas all over the shop. We (as an industry) decided a long time ago that shit wasnt going to work and thats why YAML exists. So what should we do instead? Thats where jsonnet comes in. JSON, Jsonnet & YAML Before we actually talk about Jsonnet, its worth reminding people of a very important (but oft forgotten point). YAML is a superset of JSON and converting between the two is trivial. Many applications and programming languages will parse JSON and YAML natively, and many can convert between the two very simple. For example, in Python: python -c 'import json, sys, yaml ; y=yaml.safe_load(sys.stdin.read()) ; print(json.dumps(y))' So with that in mind, lets talk about Jsonnet. Welcome to the church of Jsonnet Jsonnet is a relatively new, little known (outside the Kubernetes community?) language that calls itself a data templating language. Its definitely a good exercise to read and consume the Jsonnet design rationale page to get an idea why it exists, but if I was going to define in a nutshell what its purpose is - its to generate JSON config. So, how does it help, exactly? Well, lets take our earlier example - we want to generate some JSON config specifying a parameter (ie, the image string). We can do that very very easily with Jsonnet using external variables. Firstly, lets define some Jsonnet: { image: std.extVar('image'), } Then, we can generate it using the Jsonnet command line tool, passing in the external variable as we need to: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Easy! Optional fields Before, I noted that if you wanted to define an optional field, with YAML templating you had to define if statements for everything. With Jsonnet, youre just defining code! // define a variable - yes, jsonnet also has comments local rg = null; { image: std.extVar('image'), // if the variable is null, this will be blank [if rg != null then 'resourceGroup']: rg, } The output here, because our variable is null, means that we never actually populate resourceGroup. If you specify a value, it will appear: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Maps and parameters Okay, now lets look at our previous annotation example. We want to define some pod annotations, which takes a YAML map as its input. You want this map to be configurable by specifying external data, and obviously doing that on the command line sucks (youd be very unlikely to specify this with Helm on the command line, for example) so generally youd use Jsonnet imports to this. Im going to specify this config as a variable and then load that variable into the annotation: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { // annotations are nested under the metadata of a pod annotations: annotations, }, } This might just be my bias towards Jsonnet talking, but this is so dramatically easier than faffing about with indentation that I cant even begin to describe it. Additional goodies The final thing I wanted to quickly explore, which is something that I feel cant really be done with Helm and other yaml templating tools, is the concept of manipulating existing objects in config. Lets take our example above with the annotations, and look at the result file: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true } } } Now, lets say for example I wanted to append a set of annotations to this annotations map. In any templating system, Id probably have to rewrite the whole map. Jsonnet makes this trivial. I can simply use the + operator to add something to this. Heres a (poor) example: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { annotations: annotations, }, } + { // this adds another JSON object metadata+: { // I'm using the + operator, so we'll append to the existing metadata annotations+: { // same as above something: 'nothing', }, }, } The end result is this: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true, \"something\": \"nothing\" } } } Obviously, in this case, its more code to this, but as your example get more complex, it becomes extremely useful to be able to manipulate objects this way. Kr8 We use all of these methods in kr8 to make creating and manipulating configuration for multiple Kubernetes clusters easy and simple. I highly recommend you check it out if any of the concepts youve found here have found you nodding your head. "
        ],
        "story_type": "Normal",
        "url_raw": "https://leebriggs.co.uk/blog/2019/02/07/why-are-we-templating-yaml.html",
        "comments.comment_id": [19109135, 19109540],
        "comments.comment_author": ["BossingAround", "joeduffy"],
        "comments.comment_descendants": [12, 19],
        "comments.comment_time": [
          "2019-02-07T22:14:16Z",
          "2019-02-07T22:58:45Z"
        ],
        "comments.comment_text": [
          "I know I'm in a minority, but I really dislike YAML... I recently did a lot of Ansible and boy, at the beginning, I was just struggling a lot. Syntactic whitespace kills me.<p>I don't like it in Python either, but for some reason, when I write Python, it's a lot easier. Maybe YAML is just a bit more complex (and Python has better IDE support..?)",
          "My belief is that we've been slowly building up to using general purpose languages, one small step at a time, throughout the infrastructure as code, DevOps, and SRE journeys this past 10 years. INI files, XML, JSON, and YAML aren't sufficiently expressive -- lacking for loops, conditionals, variable references, and any sort of abstraction -- so, of course, we add templates to it. But as the author (IMHO rightfully) points out, we just end up with a funky, poor approximation of a language.<p>I think this approach is a byproduct of thinking about infrastructure and configuration -- and the cloud generally -- as an \"afterthought,\" not a core part of an application's infrastructure. Containers, Kubernetes, serverless, and more hosted services all change this, and Chef, Puppet, and others laid the groundwork to think differently about what the future looks like. More developers today than ever before need to think about how to build and configure cloud software.<p>We started the Pulumi project to solve this very problem, so I'm admittedly biased, and I hope you forgive the plug -- I only mention it here because I think it contributes to the discussion. Our approach is to simply use general purpose languages like TypeScript, Python, and Go, while still having infrastructure as code. An important thing to realize is that infrastructure as code is based on the idea of a <i>goal state</i>. Using a full blown language to generate that goal state generally doesn't threaten the repeatability, determinism, or robustness of the solution, provided you've got an engine handling state management, diffing, resource CRUD, and so on. We've been able to apply this universally across AWS, Azure, GCP, <i>and</i> Kubernetes, often mixing their configuration in the same program.<p>Again, I'm biased and want to admit that, however if you're sick of YAML, it's definitely worth checking out. We'd love your feedback:<p>- Project website: <a href=\"https://pulumi.io/\" rel=\"nofollow\">https://pulumi.io/</a><p>- All open source on GitHub: <a href=\"https://github.com/pulumi/pulumi\" rel=\"nofollow\">https://github.com/pulumi/pulumi</a><p>- Example of abstractions: <a href=\"https://blog.pulumi.com/the-fastest-path-to-deploying-kubernetes-on-aws-with-eks-and-pulumi\" rel=\"nofollow\">https://blog.pulumi.com/the-fastest-path-to-deploying-kubern...</a><p>- Example of serverless as event handlers: <a href=\"https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simple-serverless-functions\" rel=\"nofollow\">https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simp...</a><p>Pulumi may not be <i>the</i> solution for everyone, but I'm fairly optimistic that this is where we're all heading.<p>Joe"
        ],
        "id": "503e4c18-7018-418c-bb56-0eca6ec52d78",
        "url_text": "Published Feb 7, 2019 by Lee Briggs #kubernetes #configuration mgmt #jsonnet #helm #kr8 I was at cfgmgmtcamp 2019 in Ghent, and did a talk which I think was well received about the need for some Kubernetes configuration management as well as the solution we built for it at $work, kr8. I made a statement during the talk which ignited some fairly fierce discussion both online, and at the conference: \"If you're starting to template yaml, ask yourself the question: why am I not *generating* json?\" - @briggsl spitting straight fire at #cfgmgmtcamp eric sorenson (@ahpook) February 5, 2019 To put this into my own words: At some point, we decided it was okay for us to template yaml. When did this happen? How is this acceptable? After some conversation, I figured it was probably best to back up my claims in some way. This blog post is going to try to do that. The configuration problem Once the applications and infrastructure youre going to manage grows past a certain size, you inevitably end up in some form of configuration complexity hell. If youre only deploying 1 or maybe 2 things, you can write a yaml configuration file and be done with it. However once you grow beyond that, you need to figure out how to manage this complexity. Its incredibly likely that the reason you have multiple configuration files is because the $thing that uses that config is slightly different from its companions. Examples of this include: Applications deployed in different environments, like dev, stg and prod Applications deployed in different regions, like Europe or North American Obviously, not all the configuration is different here, but its likely the configuration differs enough that you want to be able to differentiate between the two. This configuration complexity has been well known for Operators (System Administrators, DevOps engineers, whatever you want to call them) for some years now. An entire discpline grew up around this in Configuration Management, and each tool solved this problem in their own way, but ultimately, they used YAML to get the job done. My favourite method has always been hiera which comes bundled with Puppet. Having the ability to hierarchically look up the variables of specific config needs is incredibly powerful and flexible, and has generally meant you dont actually need to do any templating of yaml at all, except perhaps for embedding Puppet facts into the yaml. Did we go backwards? Then, as our industries needs moved above the operating system and into cloud computing, we had a whole new data plane to configure. The tooling to configure this changed, and tools like CloudFormation and Helm appeared. These tools are excellent configuration tools, but I firmly believe we (as an industry) got something really, really wrong when we designed them. To examine that, lets take a look at example of a helm chart taking a custom parameter Helm Charts Helm charts can take external parameters defined by an values.yaml file which you specify when rendering the chart. A simple example might look like this: Lets say my external parameter is simple - its a string. Itd look a bit like this: image: \"{{ .Values.image }}\" Thats not so bad right? You just specify a value for image in your values.yaml and youre on your way. The real problem starts to get highlighted when you want to do more complicated and complex things. In this particular example, youre doing okay because you know you have to specify an image for a Kubernetes deployment. However, what if youre working with something like an optional field? Well, then it gets a little more unwieldy: {{- with .resourceGroup }} resourceGroup: {{ . }} {{- end }} Optional values just make things ugly in templating languages, and you cant just leave the value blank, so you have to resort to ugly loops and conditionals that are probably going to bite you later. Lets say you need to go a step further, and you need to push an array or map into the config. With helm, youd do something like this. {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 8 }} {{- end }} Firstly, lets ignore the madness of having a templating function toYaml to convert yaml to yaml and focus more on the whitespace issue here. YAML has strict requirements and whitespace implementation rules. The following, for example, is not valid or complete yaml: something: nothing hello: goodbye Generally, if youre handwriting something, this isnt necessarily a problem because you just hit backspace twice and its fixed. However, if youre generating YAML using a templating system, you cant do that - and if youre operating above 5 or 10 configuration files, you probably want to be generating your config rather than writing it. So, in the above example, you want to embed the values of .Values.podAnnotations under the annotations field, which is indented already. So youre having to not only indent your values, but indent them correctly. What makes this even more confusing is that the go parser doesnt actually know anything about YAML at all, so if you try to keep the syntax clean and indent the templates like this: {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 6 }} {{- end }} You actually cant do that, because the templating system gets confused. This is a singular example of the complexity and difficulty you end up facing when generating config data in YAML, but when you really start to do more complex work, it really starts to become obvious that this isnt the way to go. Needless to say, this isnt what I want to spend my time doing. If fiddling around with whitespace requirements in a templating system doing something its not really designed for is what suits you, then Im not going to stop you. I also dont want to spend my time writing configuration in JSON without comments and accidentally missing commas all over the shop. We (as an industry) decided a long time ago that shit wasnt going to work and thats why YAML exists. So what should we do instead? Thats where jsonnet comes in. JSON, Jsonnet & YAML Before we actually talk about Jsonnet, its worth reminding people of a very important (but oft forgotten point). YAML is a superset of JSON and converting between the two is trivial. Many applications and programming languages will parse JSON and YAML natively, and many can convert between the two very simple. For example, in Python: python -c 'import json, sys, yaml ; y=yaml.safe_load(sys.stdin.read()) ; print(json.dumps(y))' So with that in mind, lets talk about Jsonnet. Welcome to the church of Jsonnet Jsonnet is a relatively new, little known (outside the Kubernetes community?) language that calls itself a data templating language. Its definitely a good exercise to read and consume the Jsonnet design rationale page to get an idea why it exists, but if I was going to define in a nutshell what its purpose is - its to generate JSON config. So, how does it help, exactly? Well, lets take our earlier example - we want to generate some JSON config specifying a parameter (ie, the image string). We can do that very very easily with Jsonnet using external variables. Firstly, lets define some Jsonnet: { image: std.extVar('image'), } Then, we can generate it using the Jsonnet command line tool, passing in the external variable as we need to: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Easy! Optional fields Before, I noted that if you wanted to define an optional field, with YAML templating you had to define if statements for everything. With Jsonnet, youre just defining code! // define a variable - yes, jsonnet also has comments local rg = null; { image: std.extVar('image'), // if the variable is null, this will be blank [if rg != null then 'resourceGroup']: rg, } The output here, because our variable is null, means that we never actually populate resourceGroup. If you specify a value, it will appear: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Maps and parameters Okay, now lets look at our previous annotation example. We want to define some pod annotations, which takes a YAML map as its input. You want this map to be configurable by specifying external data, and obviously doing that on the command line sucks (youd be very unlikely to specify this with Helm on the command line, for example) so generally youd use Jsonnet imports to this. Im going to specify this config as a variable and then load that variable into the annotation: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { // annotations are nested under the metadata of a pod annotations: annotations, }, } This might just be my bias towards Jsonnet talking, but this is so dramatically easier than faffing about with indentation that I cant even begin to describe it. Additional goodies The final thing I wanted to quickly explore, which is something that I feel cant really be done with Helm and other yaml templating tools, is the concept of manipulating existing objects in config. Lets take our example above with the annotations, and look at the result file: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true } } } Now, lets say for example I wanted to append a set of annotations to this annotations map. In any templating system, Id probably have to rewrite the whole map. Jsonnet makes this trivial. I can simply use the + operator to add something to this. Heres a (poor) example: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { annotations: annotations, }, } + { // this adds another JSON object metadata+: { // I'm using the + operator, so we'll append to the existing metadata annotations+: { // same as above something: 'nothing', }, }, } The end result is this: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true, \"something\": \"nothing\" } } } Obviously, in this case, its more code to this, but as your example get more complex, it becomes extremely useful to be able to manipulate objects this way. Kr8 We use all of these methods in kr8 to make creating and manipulating configuration for multiple Kubernetes clusters easy and simple. I highly recommend you check it out if any of the concepts youve found here have found you nodding your head. ",
        "_version_": 1718536444754001920
      },
      {
        "story_id": 19032439,
        "story_author": "WalterSobchak",
        "story_descendants": 2,
        "story_score": 21,
        "story_time": "2019-01-30T03:40:28Z",
        "story_title": "Announcing .NET Core 3 Preview 2",
        "search": [
          "Announcing .NET Core 3 Preview 2",
          "https://blogs.msdn.microsoft.com/dotnet/2019/01/29/announcing-net-core-3-preview-2/",
          "Richard January 29th, 2019 Today, we are announcing .NET Core 3 Preview 2. It includes new features in .NET Core 3.0 and C# 8, in addition to the large number of new features in Preview 1. ASP.NET Core 3.0 Preview 2 is alsoreleased today. C# 8 Preview 2 is part of .NET Core 3 SDK, and was also released last week with Visual Studio 2019 Preview 2. Download and get started with .NET Core 3 Preview 2 right now on Windows, macOS and Linux. In case you missed it, we made some big announcements with Preview 1, including adding support for Windows Forms and WPF with .NET Core, on Windows, and that both UI frameworks will be open source. We also announced that we would support Entity Framework 6 on .NET Core, which will come in a later preview. ASP.NET Core is also adding many features including Razor components. Please provide feedback on the release in the comments below or at dotnet/core #2263. You can see complete details of the release in the .NET Core 3 Preview 2 release notes. .NET Core 3 will be supported in Visual Studio 2019, Visual Studio for Mac and Visual Studio Code. Visual Studio 2019 Preview 2 was released last week and has support for C# 8. The Visual Studio Code C# Extension (in pre-release channel) was also just updated to support C# 8. C# 8 C# 8 is a major release of the language, as Mads describes in Do more with patterns in C# 8.0, Take C# 8.0 for a spin and Building C# 8.0. In this post, Ill cover a few favorites that are new in Preview 2. Using Declarations Are you tired of using statements that require indenting your code? No more! You can now write the following code, which attaches a using declaration to the scope of the current statement block and then disposes the object at the end of it. Switch Expressions Anyone who uses C# probably loves the idea of a switch statement, but not the syntax. C# 8 introduces switch expressions, which enable the following: terser syntax, returns a value since it is an expression, and fully integrated with pattern matching. The switch keyword is infix, meaning the keyword sits between the tested value (here, thats o) and the list of cases, much like expression lambdas. The following examples use the lambda syntax for methods, which integrates well with switch expressions but isnt required. You can see the syntax for switch expressions in the following example: There are two patterns at play in this example. o first matches with the Point type pattern and then with the property pattern inside the {curly braces}. The _ describes the discard pattern, which is the same as default for switch statements. You can go one step further, and rely on tuple deconstruction and parameter position, as you can see in the following example: In this example, you can see you do not need to define a variable or explicit type for each of the cases. Instead, the compiler can match the tuple being testing with the tuples defined for each of the cases. All of these patterns enable you to write declarative code that captures your intent instead of procedural code that implements tests for it. The compiler becomes responsible for implementing that boring procedural code and is guaranteed to always do it correctly. There will still be cases where switch statements will be a better choice than switch expressions and patterns can be used with both syntax styles. Async streams Async streams are another major improvement in C# 8. They have been changing with each preview and require that the compiler and the framework libraries match to work correctly. You need .NET Core 3.0 Preview 2 to use async streams if you want to develop with either Visual Studio 2019 Preview 2 or the latest preview of the C# extension for Visual Studio Code. If you are using .NET Core 3.0 Preview 2 at the commandline, then everything will work as expected. IEEE Floating-point improvements Floating point APIs are in the process of being updated to comply with IEEE 754-2008 revision. The goal of this floating point project is to expose all required operations and ensure that they are behaviorly compliant with the IEEE spec. Parsing and formatting fixes: Correctly parse and round inputs of any length. Correctly parse and format negative zero. Correctly parse Infinity and NaN by performing a case-insensitive check and allowing an optional preceding + where applicable. New Math APIs: BitIncrement/BitDecrement corresponds to the nextUp and nextDown IEEE operations. They return the smallest floating-point number that compares greater or lesser than the input (respectively). For example, Math.BitIncrement(0.0) would return double.Epsilon. MaxMagnitude/MinMagnitude corresponds to the maxNumMag and minNumMag IEEE operations, they return the value that is greater or lesser in magnitude of the two inputs (respectively). For example, Math.MaxMagnitude(2.0, -3.0) would return -3.0. ILogB corresponds to the logB IEEE operation which returns an integral value, it returns the integral base-2 log of the input parameter. This is effectively the same as floor(log2(x)), but done with minimal rounding error. ScaleB corresponds to the scaleB IEEE operation which takes an integral value, it returns effectively x * pow(2, n), but is done with minimal rounding error. Log2 corresponds to the log2 IEEE operation, it returns the base-2 logarithm. It minimizes rounding error. FusedMultiplyAdd corresponds to the fma IEEE operation, it performs a fused multiply add. That is, it does (x * y) + z as a single operation, there-by minimizing the rounding error. An example would be FusedMultiplyAdd(1e308, 2.0, -1e308) which returns 1e308. The regular (1e308 * 2.0) - 1e308 returns double.PositiveInfinity. CopySign corresponds to the copySign IEEE operation, it returns the value of x, but with the sign of y. .NET Platform Dependent Intrinsics Weve added APIs that allow access to certain perf-oriented CPU instructions, such as the SIMD or Bit Manipulation instruction sets. These instructions can help achieve big performance improvements in certain scenarios, such as processing data efficiently in parallel. In addition to exposing the APIs for your programs to use, we have begun using these instructions to accelerate the .NET libraries too. The following CoreCLR PRs demonstrate a few of the intrinsics, either via implementation or use: Implement simple SSE2 hardware instrinsics Implement the SSE hardware intrinsics Arm64 Base HW Intrinsics Use TZCNT and LZCNT for Locate{First|Last}Found{Byte|Char} For more information, take a look at .NET Platform Dependent Intrinsics, which defines an approach for defining this hardware infrastructure, allowing Microsoft, chip vendors or any other company or individual to define hardware/chip APIs that should be exposed to .NET code. Introducing a fast in-box JSON Writer & JSON Document Following the introduction of the JSON reader in preview1, weve added System.Text.Json.Utf8JsonWriter and System.Text.Json.JsonDocument. As described in our System.Text.Json roadmap, we plan to provide a POCO serializer and deserializer next. Utf8JsonWriter The Utf8JsonWriter provides a high-performance, non-cached, forward-only way to write UTF-8 encoded JSON text from common .NET types like String, Int32, and DateTime. Like the reader, the writer is a foundational, low-level type, that can be leveraged to build custom serializers. Writing a JSON payload using the new Utf8JsonWriter is 30-80% faster than using the writer from Json.NET and does not allocate. Here is a sample usage of the Utf8JsonWriter that can be used as a starting point: The Utf8JsonWriter accepts IBufferWriter<byte> as the output location to synchronously write the json data into and you, as the caller, need to provide a concrete implementation. The platform does not currently include an implementation of this interface, but we plan to provide one that is backed by a resizable byte array. That implementation would enable synchronous writes, which could then be copied to any stream (either synchronously or asynchronously). If you are writing JSON over the network and include the System.IO.Pipelines package, you can leverage the Pipe-based implementation of the interface called PipeWriter to skip the need to copy the JSON from an intermediary buffer to the actual output. You can take inspiration from this sample implementation of IBufferWriter<T>. The following is a skeleton array-backed concrete implementation of the interface: JsonDocument In preview2, weve also added System.Text.Json.JsonDocument which was built on top of the Utf8JsonReader. The JsonDocument provides the ability to parse JSON data and build a read-only Document Object Model (DOM) that can be queried to support random access and enumeration. The JSON elements that compose the data can be accessed via the JsonElement type which is exposed by the JsonDocument as a property called RootElement. The JsonElement contains the JSON array and object enumerators along with APIs to convert JSON text to common .NET types. Parsing a typical JSON payload and accessing all its members using the JsonDocument is 2-3x faster than Json.NET with very little allocations for data that is reasonably sized (i.e. < 1 MB). Here is a sample usage of the JsonDocument and JsonElement that can be used as a starting point: GPIO Support for Raspberry Pi We added initial support for GPIO with Preview 1. As part of Preview 2, we have released two NuGet packages that you can use for GPIO programming. System.Device.Gpio Iot.Device.Bindings The GPIO Packages includes APIs for GPIO, SPI, I2C and PWM devices. The IoT bindings package includes device bindings for various chips and sensors, the same ones available at dotnet/iot src/devices. Updated serial port APIs were announced as part of Preview 1. They are not part of these packages but are available as part of the .NET Core platform. Local dotnet tools Local dotnet tools have been improved in Preview 2. Local tools are similar to dotnet global tools but are associated with a particular location on disk. This enables per-project and per-repository tooling. You can read more about them in the .NET Core 3.0 Preview 1 post. In this preview, we have added 2 new commands: dotnet new tool-manifest dotnet tool list To add local tools, you need to add a manifest that will define the tools and versions available. The dotnet new tool-manifest command automates creation of the manifest required by local tools. After this file is created, you can add tools to it via dotnet tool install <packageId>. The command dotnet tool list lists local tools and their corresponding manifest. The command dotnet tool list -g lists global tools. There was a change in .NET Core Local Tools between .NET Core 3.0 Preview 1 and .NET Core 3.0 Preview 2. If you tried out local tools in Preview 1 by running a command like dotnet tool restore or dotnet tool install, you need to delete your local tools cache folder before local tools will work correctly in Preview 2. This folder is located at: On mac, Linux: rm -r $HOME/.dotnet/toolResolverCache On Windows: rmdir /s %USERPROFILE%\\.dotnet\\toolResolverCache If you do not delete this folder, you will receive an error. Assembly Unloadability Assembly unloadability is a new capability of AssemblyLoaderContext. This new feature is largely transparent from an API perspective, exposed with just a few new APIs. It enables a loader context to be unloaded, releasing all memory for instantiated types, static fields and for the assembly itself. An application should be able to load and unload assemblies via this mechanism forever without experiencing a memory leak. We expect this new capability to be used for the following scenarios: Plugin scenarios where dynamic plugin loading and unloading is required. Dynamically compiling, running and then flushing code. Useful for web sites, scripting engines, etc. Loading assemblies for introspection (like ReflectionOnlyLoad), although MetadataLoadContext (released in Preview 1) will be a better choice in many cases. More information: Design doc Using Unloadability doc Assembly unloading requires significant care to ensure that all references to managed objects from outside a loader context are understood and managed. When the loader context is requested to be unloaded, any outside references need to have been unreferenced so that the loader context is self-consistent only to itself. Assembly unloadability was provided in the .NET Framework by Application Domains (AppDomains), which are not supported with .NET Core. AppDomains had both benefits and limitations compared to this new model. We consider this new loader context-based model to be more flexible and higher performance when compared to AppDomains. Windows Native Interop Windows offers a rich native API, in the form of flat C APIs, COM and WinRT. Weve had support for P/Invoke since .NET Core 1.0, and have been adding the ability to CoCreate COM APIs and Activate WinRT APIs as part of the .NET Core 3.0 release. We have had many requests for these capabilities, so we know that they will get a lot of use. Late last year, we announced that we had managed to automate Excel from .NET Core. That was a fun moment. You can now try this same demo yourself with the Excel demo sample. Under the covers, this demo is using COM interop features like NOPIA, object equivalence and custom marshallers. Managed C++ and WinRT interop are coming in a later preview. We prioritized getting COM interop built first. WPF and Windows Forms The WPF and Windows Forms teams opened up their repositories, dotnet/wpf and dotnet/winforms, respectively, on December 4th, the same day .NET Core 3.0 Preview 1 was released. Much of the last month, beyond holidays, has been spent interacting with the community, merging PRs, and responding to issues. In the background, theyve been integrating WPF and Windows Forms into the .NET Core build system, including adopting the Arcade SDK. Arcade is an MSBuild SDK that exposes functionality which is needed to build the .NET Platform. The WPF team will be publishing more of the WPF source code over the coming months. The same teams have been making a final set of changes in .NET Framework 4.8. These same changes have also been added to the .NET Core versions of WPF and Windows Forms. Visual Studio support Desktop development on .NET Core 3 requires Visual Studio 2019. We added the WPF and Windows Forms templates to the New Project Dialog to make it easier starting your new applications without using the command line. The WPF and Windows Forms designer teams are continuing to work on an updated designer for .NET Core, which will be part of a Visual Studio 2019 Update. MSIX Deployment for Desktop apps MSIX is a new Windows app package format. It can be used to deploy .NET Core 3 desktop applications to Windows 10. The Windows Application Packaging Project, available in Visual Studio 2019 preview 2, allows you to create MSIX packages with self-contained .NET Core applications. Note: The .NET Core project file must specify the supported runtimes in the <RuntimeIdentifiers> property: <RuntimeIdentifiers>win-x86;win-x64</RuntimeIdentifiers> Install .NET Core 3.0 Previews on Linux with Snap Snap is the preferred way to install and try .NET Core previews on Linux distributions that support Snap. At present, only X64 builds are supported with Snap. Well look at supporting ARM builds, too. After configuring Snap on your system, run the following command to install the .NET Core SDK 3.0 Preview SDK. sudo snap install dotnet-sdk --beta --classic When .NET Core in installed using the Snap package, the default .NET Core command is dotnet-sdk.dotnet, as opposed to just dotnet. The benefit of the namespaced command is that it will not conflict with a globally installed .NET Core version you may have. This command can be aliased to dotnet with: sudo snap alias dotnet-sdk.dotnet dotnet Some distros require an additional step to enable access to the SSL certificate. See our Linux Setup for details. Platform Support .NET Core 3 will be supported on the following operating systems: Windows Client: 7, 8.1, 10 (1607+) Windows Server: 2012 R2 SP1+ macOS: 10.12+ RHEL: 6+ Fedora: 26+ Ubuntu: 16.04+ Debian: 9+ SLES: 12+ openSUSE: 42.3+ Alpine: 3.8+ Chip support follows: x64 on Windows, macOS, and Linux x86 on Windows ARM32 on Windows and Linux ARM64 on Linux For Linux, ARM32 is supported on Debian 9+ and Ubuntu 16.04+. For ARM64, it is the same as ARM32 with the addition of Alpine 3.8. These are the same versions of those distros as is supported for X64. We made a conscious decision to make supported platforms as similar as possible between X64, ARM32 and ARM64. Docker images for .NET Core 3.0 are available at microsoft/dotnet on Docker Hub. We are in the process of adopting Microsoft Container Registry (MCR). We expect that the final .NET Core 3.0 images will only be published to MCR. Closing Take a look at the .NET Core 3.0 Preview 1 post if you missed that. It includes a broader description of the overall release including the initial set of features, which are also included and improved on in Preview 2. Thanks to everyone that installed .NET Core 3.0 Preview 1. We appreciate you trying out the new version and for your feedback. Please share any feedback you have about Preview 2. "
        ],
        "story_type": "Normal",
        "url_raw": "https://blogs.msdn.microsoft.com/dotnet/2019/01/29/announcing-net-core-3-preview-2/",
        "comments.comment_id": [19033128, 19037266],
        "comments.comment_author": ["oaiey", "tonyedgecombe"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-01-30T06:25:13Z",
          "2019-01-30T17:40:02Z"
        ],
        "comments.comment_text": [
          "C# pattern matching capabilities are starting to remind me of OCaml tutorials in the university. Sweet.",
          "<i>using var options = Parse(args);</i><p>That's going to tidy up a ton of my code, flatter is definitely better."
        ],
        "id": "32c07d2c-ab8f-43a9-b4de-571f97848998",
        "url_text": "Richard January 29th, 2019 Today, we are announcing .NET Core 3 Preview 2. It includes new features in .NET Core 3.0 and C# 8, in addition to the large number of new features in Preview 1. ASP.NET Core 3.0 Preview 2 is alsoreleased today. C# 8 Preview 2 is part of .NET Core 3 SDK, and was also released last week with Visual Studio 2019 Preview 2. Download and get started with .NET Core 3 Preview 2 right now on Windows, macOS and Linux. In case you missed it, we made some big announcements with Preview 1, including adding support for Windows Forms and WPF with .NET Core, on Windows, and that both UI frameworks will be open source. We also announced that we would support Entity Framework 6 on .NET Core, which will come in a later preview. ASP.NET Core is also adding many features including Razor components. Please provide feedback on the release in the comments below or at dotnet/core #2263. You can see complete details of the release in the .NET Core 3 Preview 2 release notes. .NET Core 3 will be supported in Visual Studio 2019, Visual Studio for Mac and Visual Studio Code. Visual Studio 2019 Preview 2 was released last week and has support for C# 8. The Visual Studio Code C# Extension (in pre-release channel) was also just updated to support C# 8. C# 8 C# 8 is a major release of the language, as Mads describes in Do more with patterns in C# 8.0, Take C# 8.0 for a spin and Building C# 8.0. In this post, Ill cover a few favorites that are new in Preview 2. Using Declarations Are you tired of using statements that require indenting your code? No more! You can now write the following code, which attaches a using declaration to the scope of the current statement block and then disposes the object at the end of it. Switch Expressions Anyone who uses C# probably loves the idea of a switch statement, but not the syntax. C# 8 introduces switch expressions, which enable the following: terser syntax, returns a value since it is an expression, and fully integrated with pattern matching. The switch keyword is infix, meaning the keyword sits between the tested value (here, thats o) and the list of cases, much like expression lambdas. The following examples use the lambda syntax for methods, which integrates well with switch expressions but isnt required. You can see the syntax for switch expressions in the following example: There are two patterns at play in this example. o first matches with the Point type pattern and then with the property pattern inside the {curly braces}. The _ describes the discard pattern, which is the same as default for switch statements. You can go one step further, and rely on tuple deconstruction and parameter position, as you can see in the following example: In this example, you can see you do not need to define a variable or explicit type for each of the cases. Instead, the compiler can match the tuple being testing with the tuples defined for each of the cases. All of these patterns enable you to write declarative code that captures your intent instead of procedural code that implements tests for it. The compiler becomes responsible for implementing that boring procedural code and is guaranteed to always do it correctly. There will still be cases where switch statements will be a better choice than switch expressions and patterns can be used with both syntax styles. Async streams Async streams are another major improvement in C# 8. They have been changing with each preview and require that the compiler and the framework libraries match to work correctly. You need .NET Core 3.0 Preview 2 to use async streams if you want to develop with either Visual Studio 2019 Preview 2 or the latest preview of the C# extension for Visual Studio Code. If you are using .NET Core 3.0 Preview 2 at the commandline, then everything will work as expected. IEEE Floating-point improvements Floating point APIs are in the process of being updated to comply with IEEE 754-2008 revision. The goal of this floating point project is to expose all required operations and ensure that they are behaviorly compliant with the IEEE spec. Parsing and formatting fixes: Correctly parse and round inputs of any length. Correctly parse and format negative zero. Correctly parse Infinity and NaN by performing a case-insensitive check and allowing an optional preceding + where applicable. New Math APIs: BitIncrement/BitDecrement corresponds to the nextUp and nextDown IEEE operations. They return the smallest floating-point number that compares greater or lesser than the input (respectively). For example, Math.BitIncrement(0.0) would return double.Epsilon. MaxMagnitude/MinMagnitude corresponds to the maxNumMag and minNumMag IEEE operations, they return the value that is greater or lesser in magnitude of the two inputs (respectively). For example, Math.MaxMagnitude(2.0, -3.0) would return -3.0. ILogB corresponds to the logB IEEE operation which returns an integral value, it returns the integral base-2 log of the input parameter. This is effectively the same as floor(log2(x)), but done with minimal rounding error. ScaleB corresponds to the scaleB IEEE operation which takes an integral value, it returns effectively x * pow(2, n), but is done with minimal rounding error. Log2 corresponds to the log2 IEEE operation, it returns the base-2 logarithm. It minimizes rounding error. FusedMultiplyAdd corresponds to the fma IEEE operation, it performs a fused multiply add. That is, it does (x * y) + z as a single operation, there-by minimizing the rounding error. An example would be FusedMultiplyAdd(1e308, 2.0, -1e308) which returns 1e308. The regular (1e308 * 2.0) - 1e308 returns double.PositiveInfinity. CopySign corresponds to the copySign IEEE operation, it returns the value of x, but with the sign of y. .NET Platform Dependent Intrinsics Weve added APIs that allow access to certain perf-oriented CPU instructions, such as the SIMD or Bit Manipulation instruction sets. These instructions can help achieve big performance improvements in certain scenarios, such as processing data efficiently in parallel. In addition to exposing the APIs for your programs to use, we have begun using these instructions to accelerate the .NET libraries too. The following CoreCLR PRs demonstrate a few of the intrinsics, either via implementation or use: Implement simple SSE2 hardware instrinsics Implement the SSE hardware intrinsics Arm64 Base HW Intrinsics Use TZCNT and LZCNT for Locate{First|Last}Found{Byte|Char} For more information, take a look at .NET Platform Dependent Intrinsics, which defines an approach for defining this hardware infrastructure, allowing Microsoft, chip vendors or any other company or individual to define hardware/chip APIs that should be exposed to .NET code. Introducing a fast in-box JSON Writer & JSON Document Following the introduction of the JSON reader in preview1, weve added System.Text.Json.Utf8JsonWriter and System.Text.Json.JsonDocument. As described in our System.Text.Json roadmap, we plan to provide a POCO serializer and deserializer next. Utf8JsonWriter The Utf8JsonWriter provides a high-performance, non-cached, forward-only way to write UTF-8 encoded JSON text from common .NET types like String, Int32, and DateTime. Like the reader, the writer is a foundational, low-level type, that can be leveraged to build custom serializers. Writing a JSON payload using the new Utf8JsonWriter is 30-80% faster than using the writer from Json.NET and does not allocate. Here is a sample usage of the Utf8JsonWriter that can be used as a starting point: The Utf8JsonWriter accepts IBufferWriter<byte> as the output location to synchronously write the json data into and you, as the caller, need to provide a concrete implementation. The platform does not currently include an implementation of this interface, but we plan to provide one that is backed by a resizable byte array. That implementation would enable synchronous writes, which could then be copied to any stream (either synchronously or asynchronously). If you are writing JSON over the network and include the System.IO.Pipelines package, you can leverage the Pipe-based implementation of the interface called PipeWriter to skip the need to copy the JSON from an intermediary buffer to the actual output. You can take inspiration from this sample implementation of IBufferWriter<T>. The following is a skeleton array-backed concrete implementation of the interface: JsonDocument In preview2, weve also added System.Text.Json.JsonDocument which was built on top of the Utf8JsonReader. The JsonDocument provides the ability to parse JSON data and build a read-only Document Object Model (DOM) that can be queried to support random access and enumeration. The JSON elements that compose the data can be accessed via the JsonElement type which is exposed by the JsonDocument as a property called RootElement. The JsonElement contains the JSON array and object enumerators along with APIs to convert JSON text to common .NET types. Parsing a typical JSON payload and accessing all its members using the JsonDocument is 2-3x faster than Json.NET with very little allocations for data that is reasonably sized (i.e. < 1 MB). Here is a sample usage of the JsonDocument and JsonElement that can be used as a starting point: GPIO Support for Raspberry Pi We added initial support for GPIO with Preview 1. As part of Preview 2, we have released two NuGet packages that you can use for GPIO programming. System.Device.Gpio Iot.Device.Bindings The GPIO Packages includes APIs for GPIO, SPI, I2C and PWM devices. The IoT bindings package includes device bindings for various chips and sensors, the same ones available at dotnet/iot src/devices. Updated serial port APIs were announced as part of Preview 1. They are not part of these packages but are available as part of the .NET Core platform. Local dotnet tools Local dotnet tools have been improved in Preview 2. Local tools are similar to dotnet global tools but are associated with a particular location on disk. This enables per-project and per-repository tooling. You can read more about them in the .NET Core 3.0 Preview 1 post. In this preview, we have added 2 new commands: dotnet new tool-manifest dotnet tool list To add local tools, you need to add a manifest that will define the tools and versions available. The dotnet new tool-manifest command automates creation of the manifest required by local tools. After this file is created, you can add tools to it via dotnet tool install <packageId>. The command dotnet tool list lists local tools and their corresponding manifest. The command dotnet tool list -g lists global tools. There was a change in .NET Core Local Tools between .NET Core 3.0 Preview 1 and .NET Core 3.0 Preview 2. If you tried out local tools in Preview 1 by running a command like dotnet tool restore or dotnet tool install, you need to delete your local tools cache folder before local tools will work correctly in Preview 2. This folder is located at: On mac, Linux: rm -r $HOME/.dotnet/toolResolverCache On Windows: rmdir /s %USERPROFILE%\\.dotnet\\toolResolverCache If you do not delete this folder, you will receive an error. Assembly Unloadability Assembly unloadability is a new capability of AssemblyLoaderContext. This new feature is largely transparent from an API perspective, exposed with just a few new APIs. It enables a loader context to be unloaded, releasing all memory for instantiated types, static fields and for the assembly itself. An application should be able to load and unload assemblies via this mechanism forever without experiencing a memory leak. We expect this new capability to be used for the following scenarios: Plugin scenarios where dynamic plugin loading and unloading is required. Dynamically compiling, running and then flushing code. Useful for web sites, scripting engines, etc. Loading assemblies for introspection (like ReflectionOnlyLoad), although MetadataLoadContext (released in Preview 1) will be a better choice in many cases. More information: Design doc Using Unloadability doc Assembly unloading requires significant care to ensure that all references to managed objects from outside a loader context are understood and managed. When the loader context is requested to be unloaded, any outside references need to have been unreferenced so that the loader context is self-consistent only to itself. Assembly unloadability was provided in the .NET Framework by Application Domains (AppDomains), which are not supported with .NET Core. AppDomains had both benefits and limitations compared to this new model. We consider this new loader context-based model to be more flexible and higher performance when compared to AppDomains. Windows Native Interop Windows offers a rich native API, in the form of flat C APIs, COM and WinRT. Weve had support for P/Invoke since .NET Core 1.0, and have been adding the ability to CoCreate COM APIs and Activate WinRT APIs as part of the .NET Core 3.0 release. We have had many requests for these capabilities, so we know that they will get a lot of use. Late last year, we announced that we had managed to automate Excel from .NET Core. That was a fun moment. You can now try this same demo yourself with the Excel demo sample. Under the covers, this demo is using COM interop features like NOPIA, object equivalence and custom marshallers. Managed C++ and WinRT interop are coming in a later preview. We prioritized getting COM interop built first. WPF and Windows Forms The WPF and Windows Forms teams opened up their repositories, dotnet/wpf and dotnet/winforms, respectively, on December 4th, the same day .NET Core 3.0 Preview 1 was released. Much of the last month, beyond holidays, has been spent interacting with the community, merging PRs, and responding to issues. In the background, theyve been integrating WPF and Windows Forms into the .NET Core build system, including adopting the Arcade SDK. Arcade is an MSBuild SDK that exposes functionality which is needed to build the .NET Platform. The WPF team will be publishing more of the WPF source code over the coming months. The same teams have been making a final set of changes in .NET Framework 4.8. These same changes have also been added to the .NET Core versions of WPF and Windows Forms. Visual Studio support Desktop development on .NET Core 3 requires Visual Studio 2019. We added the WPF and Windows Forms templates to the New Project Dialog to make it easier starting your new applications without using the command line. The WPF and Windows Forms designer teams are continuing to work on an updated designer for .NET Core, which will be part of a Visual Studio 2019 Update. MSIX Deployment for Desktop apps MSIX is a new Windows app package format. It can be used to deploy .NET Core 3 desktop applications to Windows 10. The Windows Application Packaging Project, available in Visual Studio 2019 preview 2, allows you to create MSIX packages with self-contained .NET Core applications. Note: The .NET Core project file must specify the supported runtimes in the <RuntimeIdentifiers> property: <RuntimeIdentifiers>win-x86;win-x64</RuntimeIdentifiers> Install .NET Core 3.0 Previews on Linux with Snap Snap is the preferred way to install and try .NET Core previews on Linux distributions that support Snap. At present, only X64 builds are supported with Snap. Well look at supporting ARM builds, too. After configuring Snap on your system, run the following command to install the .NET Core SDK 3.0 Preview SDK. sudo snap install dotnet-sdk --beta --classic When .NET Core in installed using the Snap package, the default .NET Core command is dotnet-sdk.dotnet, as opposed to just dotnet. The benefit of the namespaced command is that it will not conflict with a globally installed .NET Core version you may have. This command can be aliased to dotnet with: sudo snap alias dotnet-sdk.dotnet dotnet Some distros require an additional step to enable access to the SSL certificate. See our Linux Setup for details. Platform Support .NET Core 3 will be supported on the following operating systems: Windows Client: 7, 8.1, 10 (1607+) Windows Server: 2012 R2 SP1+ macOS: 10.12+ RHEL: 6+ Fedora: 26+ Ubuntu: 16.04+ Debian: 9+ SLES: 12+ openSUSE: 42.3+ Alpine: 3.8+ Chip support follows: x64 on Windows, macOS, and Linux x86 on Windows ARM32 on Windows and Linux ARM64 on Linux For Linux, ARM32 is supported on Debian 9+ and Ubuntu 16.04+. For ARM64, it is the same as ARM32 with the addition of Alpine 3.8. These are the same versions of those distros as is supported for X64. We made a conscious decision to make supported platforms as similar as possible between X64, ARM32 and ARM64. Docker images for .NET Core 3.0 are available at microsoft/dotnet on Docker Hub. We are in the process of adopting Microsoft Container Registry (MCR). We expect that the final .NET Core 3.0 images will only be published to MCR. Closing Take a look at the .NET Core 3.0 Preview 1 post if you missed that. It includes a broader description of the overall release including the initial set of features, which are also included and improved on in Preview 2. Thanks to everyone that installed .NET Core 3.0 Preview 1. We appreciate you trying out the new version and for your feedback. Please share any feedback you have about Preview 2. ",
        "_version_": 1718536442096910336
      },
      {
        "story_id": 21661013,
        "story_author": "mana99",
        "story_descendants": 32,
        "story_score": 168,
        "story_time": "2019-11-29T00:10:46Z",
        "story_title": "Making Git and Jupyter Notebooks play nice",
        "search": [
          "Making Git and Jupyter Notebooks play nice",
          "http://timstaley.co.uk/posts/making-git-and-jupyter-notebooks-play-nice/",
          "Summary: jq rocks for speedy JSON mangling. Use it to make powerful git clean filters, e.g. when stripping out unwanted cached-data from Jupyter notebooks. You can find the documentation of git 'clean' and 'smudge' filters buried in the page on git-attributes, or see my example setup below. The trouble with notebooks For a year or so now I've been using Jupyter notebooks as a means to produce tutorials and other documentation (see e.g. the voeventdb.remote tutorial). It's a powerful medium, providing a good compromise between ease-of-editing and the capability to interleave text, code, intermediate results, plots, and even nicely-typeset LaTeX-encoded equations. I've even gone to far as to urge its adoption in recent conference talks. However, this powerful interface inherits the age-old curse of WYSIWYG editors - the document-files tend to contain more than just plain-text, and therefore are not-so-easy to handle with standard version-control tools. In the case of Jupyter, the format doesn't stray too far from comfortable plain-text territory - the ipynb format is just a custom JSON data-structure, with the occasional base-64 encoded blob for images and other binary data. Which means version-control systems such as Git can handle it quite well, but diff-comparing different versions of a complex notebook quickly becomes a chore as you scroll past long blocks of unintelligible base-64 gibberish. This is a problem when working with long-lived, multiple-revision or (especially) multiple-coauthor projects. What can we do about this? First, it's worth mentioning the initial \"I'll figure this out later\" solution which has served many users sufficiently well for a while - if you're typically only working from one machine, and you just want to keep your notebooks vaguely manageable, you can get by for a long time by manually hitting Cell -> All Output -> Clear (followed by a Save) before you commit your notebooks. This wipes the slate clean with regards to cell outputs (plots, prints, whatver), so you'll need to re-run any computation next time you run the notebook. The problems with this approach are that A. it's manual, so you'll have to painstakingly open up every notebook you recently re-ran and clear it before you commit, and B. it doesn't even fully solve the 'noise-in-the-diffs' problem, since every notebook also contains a 'metadata' section, which looks a bit like this: { \"metadata\": { \"kernelspec\": { \"display_name\": \"Python 2\", \"language\": \"python\", \"name\": \"python2\" }, \"language_info\": { \"codemirror_mode\": { \"name\": \"ipython\", \"version\": 2 }, \"file_extension\": \".py\", \"mimetype\": \"text/x-python\", \"name\": \"python\", \"nbconvert_exporter\": \"python\", \"pygments_lexer\": \"ipython2\", \"version\": \"2.7.12\" } } Note the metadata section is effectively a blank slate, and has a myriad of possible uses, but for most users it will just contain the above. This is useful for checking a previously run notebook, but is mostly unwanted information when checking-in files to a multi-user project where everyone's using a slightly different Python version - it just generates more diff-noise. Possible Pythonic solutions nbdime - an nbformat diff-GUI We clearly need some tooling, and there are some Python projects out there trying to address exactly this problem. First, it's worth mentioning nbdime, which picks up the ball from where the (now defunct) nbdiff project left off and attempts to provide content-aware diffing and merging of Jupyter notebooks - a meld (GUI) diff-tool equivalent for the nbformat, if you will. I think nbdime has the potential to be a really good beginner-friendly, general purpose notebook-handling tool and I want to see it succeed. However; it's currently somewhat of a beta, and more importantly it only fills one role in the notebook editing toolbox - viewing crufty diffs. What I really want to do is automatically clear out all the cruft and minimize the diffs in the first place. nbstripout - does what it says on the tin A little searching then leads to nbstripout, which is a one-module Python script wrapping the nbformat processing functions, and adding some automagic for setting up your git config (on which more in a moment). This effectively automates the 'clear all output' manual process described above. However, this doesn't suit me for a couple of reasons; it leaves in that problematic 'metadata' section and also it's **slowww**. Running a script manually and expecting a short delay is fine, but we're going to integrate this into our git setup. That means it will run every time we hit git diff! One of the few things I love about git is that it's typically blazing fast; so a delay of nearly a fifth of a second every time I try to interact with it gets old pretty quickly: time nbstripout 01-parsing.ipynb real 0m0.174s user 0m0.152s sys 0m0.016s (Note, this is a small notebook-file, on a fairly beefy laptop with an SSD). This not a criticism of nbstripout so much as an inherent flaw in using Python for low-latency tasks - that cold-startup overhead on the CPython interpreter is a killer. (Which in turn harks back to ancient history of mercurial vs git!) Enter jq Fortunately, we have another option (thanks to Jan Schulz for the tip-off on this). Since the nbformat is just JSON, we can make use of jq, 'a lightweight and flexible command-line JSON processor' ('sed for JSON data'). There's a modicum of set-up overhead as jq has its very own query / filter language, but the documentation is good and the hard work has been done for you already. Here's the jq invocation I'm currently using: jq --indent 1 \\ ' (.cells[] | select(has(\"outputs\")) | .outputs) = [] | (.cells[] | select(has(\"execution_count\")) | .execution_count) = null | .metadata = {\"language_info\": {\"name\":\"python\", \"pygments_lexer\": \"ipython3\"}} | .cells[].metadata = {} ' 01-parsing.ipynb Each line inside the single-quotes defines a filter - the first selects any entries from the 'cells' list, and blanks any outputs. The second resets any execution counts. The third wipes the notebook metadata, replacing it with the minimum of required information for the notebook to still run without complaints [*] and work correctly when formatted with nbsphinx. The fourth filter-line, .cells[].metadata = {} is a matter of preference and situation - in recent versions of Jupyter every cell can be marked hidden / collapsed / write-protected, etc. I'm not interested in that metadata usually but of course you may want to keep it for some projects. We now have a fully stripped-down notebook that should contain only the common information needed to execute with whatever local Python installation is available (assuming Python2/3 compatibility, correctly set-up library installs and all the rest). Note you'll need jq version 1.5 or greater, since the --indent option was only recently implemented and is necessary to conform with the nbformat. Fortunately that should only be a small binary-download away, even if you're on ancient linux or OSX. That's a bit of a handful to type, but you can set it up as an alias in your .bashrc with a bit of careful quotation-escaping: alias nbstrip_jq=\"jq --indent 1 \\ '(.cells[] | select(has(\\\"outputs\\\")) | .outputs) = [] \\ | (.cells[] | select(has(\\\"execution_count\\\")) | .execution_count) = null \\ | .metadata = {\\\"language_info\\\": {\\\"name\\\": \\\"python\\\", \\\"pygments_lexer\\\": \\\"ipython3\\\"}} \\ | .cells[].metadata = {} \\ '\" Which can then be used conveniently like so: nbstrip_jq 01-parsing.ipynb > stripped.ipynb Not only does this give us full control to wipe that pesky metadata, it's pretty damn quick, taking something like a tenth of the time of nbstripout in my (admittedly ad-hoc) testing: nbstrip_jq 01-parsing.ipynb # (JSON contents omitted) real 0m0.015s user 0m0.008s sys 0m0.004s [*]Note on handling the notebook-level metadata section: Previously I had been blanking the metadata entirely, but it turns out that the pygments_lexer entry is crucial for nbsphinx to format notebooks with the correct syntax highlighting, hence the slightly awkward entry you see here. You might want to take a more careful approach and put together a jq-filter which merely removes (or normalizes) the Python version numbers, thereby lessening the risk of inadvertently wiping useful metadata. But for the purposes of this blog post I wanted to keep things as simple as possible while actually giving a usable, working setup. Automation: Integrating with git So we're all tooled up, but the question remains - how do we get git to run this automatically for us? For this, we dive into 'gitattributes' functionality, specifically the filter section. This describes how to define 'clean' and 'smudge' (reverse of clean) filters, which are operations that transform our data as it is checked in or out of the git-repository, so that (for example) our notebook-output cells are always stripped away from the JSON-data before it's added to the git repository: In the general case you can also define a smudge-filter to take your repository contents and do something with it to make it local to your system, but we'll not be needing that here - we'll just use the cat command as a placeholder. The easiest way to explain how to configure this is with an example. Personally, I want notebook-cleaning behaviour to be the default across all my git-repositories, so I have the following entries in my global ~/.gitconfig file: [core] attributesfile = ~/.gitattributes_global [filter \"nbstrip_full\"] clean = \"jq --indent 1 \\ '(.cells[] | select(has(\\\"outputs\\\")) | .outputs) = [] \\ | (.cells[] | select(has(\\\"execution_count\\\")) | .execution_count) = null \\ | .metadata = {\\\"language_info\\\": {\\\"name\\\": \\\"python\\\", \\\"pygments_lexer\\\": \\\"ipython3\\\"}} \\ | .cells[].metadata = {} \\ '\" smudge = cat required = true And then in ~/.gitattributes_global: *.ipynb filter=nbstrip_full (Note, once you've defined your filter you can just as easily assign it to files in a repository specific .gitattributes file if you prefer a fine-grained approach.) That's it! You're all set to go version control notebooks like a champ! Well, almost. Getting started and gotchas Note that we're into git-powertool territory here, so things might be a little less polished compared to the (cough) usual intuitive git interface you're used to. To start off with, assuming a pre-existing set of notebooks, you'll want to add a 'do-nothing' commit, where you simply pull in the newly-filtered versions of your notebooks and trim out any unwanted metadata. Just git add your notebooks, noting that you may need to touch them first, so git picks up on the timestamp-modification and actually looks at the files for changes. Then, to see the patch removing all the cruft. Commit that, then go ahead, run your notebooks, leave uncleaned outputs all over the place. Unless you change the actual code-cell contents, your git diff should be blank! Great. Except. If you have executed a notebook since your last commit, git status may show that file as 'modified', despite the fact that when you git diff, the filters go into action and no differences-to-HEAD are found. So you have to 'tune out' these false-positive modified flags when reading the git-status. Another issue is that if you use a diff-GUI such as meld, then beware: unlike git diff, git difftool will not apply filters to the working directory before comparing with the repo HEAD - so your command-line and GUI diffs have suddenly diverged! The logic behind this difference in behaviour is that GUI programs give the option to edit the local working-copy directly, as discussed at length in this thread. This has clearly caught out others before. If they bother you, these false-positives and diff-divergences can easily be resolved by manually applying the jq-filters before you run your diffs. For convenience, my ~/.bashrc also defines the following command to apply the filters to all notebooks in the current working directory: function nbstrip_all_cwd { for nbfile in *.ipynb; do echo \"$( nbstrip_jq $nbfile )\" > $nbfile done unset nbfile } Addtionally, let me note that clean/smudge filters often do not play well with rebase operations. Things get very confusing if you try to rebase across commits before / after applying a clean-filter. The simplest way to work around this is to simply comment out the relevant filter-assignment line in .gitattributes_global while performing a rebase, then uncomment it when done. As a parting note, if you also choose to configure your gitattributes globally, you may want to know how to 'whitelist' notebooks in a particular repository (for example, if you're checking-in executed notebooks to a github-pages documentation branch). This is dead easy, just add a local .gitattributes file to the repository and 'unset' the filter attribute, like so: Or you could replace the *.ipynb with a path to a specific notebook, etc. Hope that helps! Comments or corrections very welcome via Twitter. "
        ],
        "story_type": "Normal",
        "url_raw": "http://timstaley.co.uk/posts/making-git-and-jupyter-notebooks-play-nice/",
        "comments.comment_id": [21661478, 21663231],
        "comments.comment_author": ["FridgeSeal", "TimSAstro"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-11-29T02:12:26Z",
          "2019-11-29T09:33:20Z"
        ],
        "comments.comment_text": [
          "Things I’ve found in checked-in notebooks:<p>* database credentials.<p>* sensitive data<p>* a whole DataBricks webpage because the person didn’t understand how to export just the notebook.<p>* collections of notebooks named only what step in the process they are, and literally nothing about what they actually do.<p>* Whole base64 encoded images and zip files<p>* packages imported by manually manipulating system environment paths<p>* multi-processing/multithreading by shelling out and calling new python instances<p>* good old “don’t run these cells”",
          "Hi, HN!<p>Author here. A friend mentioned this was on the front page so I wanted to stop by and make explicit that this advice is OUT OF DATE as far as I'm concerned. It's way too much hassle (I work in a much larger team now than I did then!) and doesn't play well with rebase etc.<p>These days I either recommend the jupytext approach (not tried it but seems sensible) or personally I just use Sphinx-gallery.<p>Advantages:<p>* Plain python files play well with IDE refactoring, Black formatter, etc etc.<p>* You now have a readymade 'tutorial' page for your docs.<p>* Files are run with every docs build, so you can configure things to alert you when they're broken.<p>Disadvantages:<p>* You end up editing a throwaway notebook file. If you forget to copy-paste your edits back to the source, and rebuild, you have lost your edits. However, this forces me to keep the 'temporary, exploratory' nature at the front of my mind and not allow the notebook code to grow too large before performing some clean-up."
        ],
        "id": "73890fd1-c83b-4b65-9d1c-fa6fad1fa5c3",
        "url_text": "Summary: jq rocks for speedy JSON mangling. Use it to make powerful git clean filters, e.g. when stripping out unwanted cached-data from Jupyter notebooks. You can find the documentation of git 'clean' and 'smudge' filters buried in the page on git-attributes, or see my example setup below. The trouble with notebooks For a year or so now I've been using Jupyter notebooks as a means to produce tutorials and other documentation (see e.g. the voeventdb.remote tutorial). It's a powerful medium, providing a good compromise between ease-of-editing and the capability to interleave text, code, intermediate results, plots, and even nicely-typeset LaTeX-encoded equations. I've even gone to far as to urge its adoption in recent conference talks. However, this powerful interface inherits the age-old curse of WYSIWYG editors - the document-files tend to contain more than just plain-text, and therefore are not-so-easy to handle with standard version-control tools. In the case of Jupyter, the format doesn't stray too far from comfortable plain-text territory - the ipynb format is just a custom JSON data-structure, with the occasional base-64 encoded blob for images and other binary data. Which means version-control systems such as Git can handle it quite well, but diff-comparing different versions of a complex notebook quickly becomes a chore as you scroll past long blocks of unintelligible base-64 gibberish. This is a problem when working with long-lived, multiple-revision or (especially) multiple-coauthor projects. What can we do about this? First, it's worth mentioning the initial \"I'll figure this out later\" solution which has served many users sufficiently well for a while - if you're typically only working from one machine, and you just want to keep your notebooks vaguely manageable, you can get by for a long time by manually hitting Cell -> All Output -> Clear (followed by a Save) before you commit your notebooks. This wipes the slate clean with regards to cell outputs (plots, prints, whatver), so you'll need to re-run any computation next time you run the notebook. The problems with this approach are that A. it's manual, so you'll have to painstakingly open up every notebook you recently re-ran and clear it before you commit, and B. it doesn't even fully solve the 'noise-in-the-diffs' problem, since every notebook also contains a 'metadata' section, which looks a bit like this: { \"metadata\": { \"kernelspec\": { \"display_name\": \"Python 2\", \"language\": \"python\", \"name\": \"python2\" }, \"language_info\": { \"codemirror_mode\": { \"name\": \"ipython\", \"version\": 2 }, \"file_extension\": \".py\", \"mimetype\": \"text/x-python\", \"name\": \"python\", \"nbconvert_exporter\": \"python\", \"pygments_lexer\": \"ipython2\", \"version\": \"2.7.12\" } } Note the metadata section is effectively a blank slate, and has a myriad of possible uses, but for most users it will just contain the above. This is useful for checking a previously run notebook, but is mostly unwanted information when checking-in files to a multi-user project where everyone's using a slightly different Python version - it just generates more diff-noise. Possible Pythonic solutions nbdime - an nbformat diff-GUI We clearly need some tooling, and there are some Python projects out there trying to address exactly this problem. First, it's worth mentioning nbdime, which picks up the ball from where the (now defunct) nbdiff project left off and attempts to provide content-aware diffing and merging of Jupyter notebooks - a meld (GUI) diff-tool equivalent for the nbformat, if you will. I think nbdime has the potential to be a really good beginner-friendly, general purpose notebook-handling tool and I want to see it succeed. However; it's currently somewhat of a beta, and more importantly it only fills one role in the notebook editing toolbox - viewing crufty diffs. What I really want to do is automatically clear out all the cruft and minimize the diffs in the first place. nbstripout - does what it says on the tin A little searching then leads to nbstripout, which is a one-module Python script wrapping the nbformat processing functions, and adding some automagic for setting up your git config (on which more in a moment). This effectively automates the 'clear all output' manual process described above. However, this doesn't suit me for a couple of reasons; it leaves in that problematic 'metadata' section and also it's **slowww**. Running a script manually and expecting a short delay is fine, but we're going to integrate this into our git setup. That means it will run every time we hit git diff! One of the few things I love about git is that it's typically blazing fast; so a delay of nearly a fifth of a second every time I try to interact with it gets old pretty quickly: time nbstripout 01-parsing.ipynb real 0m0.174s user 0m0.152s sys 0m0.016s (Note, this is a small notebook-file, on a fairly beefy laptop with an SSD). This not a criticism of nbstripout so much as an inherent flaw in using Python for low-latency tasks - that cold-startup overhead on the CPython interpreter is a killer. (Which in turn harks back to ancient history of mercurial vs git!) Enter jq Fortunately, we have another option (thanks to Jan Schulz for the tip-off on this). Since the nbformat is just JSON, we can make use of jq, 'a lightweight and flexible command-line JSON processor' ('sed for JSON data'). There's a modicum of set-up overhead as jq has its very own query / filter language, but the documentation is good and the hard work has been done for you already. Here's the jq invocation I'm currently using: jq --indent 1 \\ ' (.cells[] | select(has(\"outputs\")) | .outputs) = [] | (.cells[] | select(has(\"execution_count\")) | .execution_count) = null | .metadata = {\"language_info\": {\"name\":\"python\", \"pygments_lexer\": \"ipython3\"}} | .cells[].metadata = {} ' 01-parsing.ipynb Each line inside the single-quotes defines a filter - the first selects any entries from the 'cells' list, and blanks any outputs. The second resets any execution counts. The third wipes the notebook metadata, replacing it with the minimum of required information for the notebook to still run without complaints [*] and work correctly when formatted with nbsphinx. The fourth filter-line, .cells[].metadata = {} is a matter of preference and situation - in recent versions of Jupyter every cell can be marked hidden / collapsed / write-protected, etc. I'm not interested in that metadata usually but of course you may want to keep it for some projects. We now have a fully stripped-down notebook that should contain only the common information needed to execute with whatever local Python installation is available (assuming Python2/3 compatibility, correctly set-up library installs and all the rest). Note you'll need jq version 1.5 or greater, since the --indent option was only recently implemented and is necessary to conform with the nbformat. Fortunately that should only be a small binary-download away, even if you're on ancient linux or OSX. That's a bit of a handful to type, but you can set it up as an alias in your .bashrc with a bit of careful quotation-escaping: alias nbstrip_jq=\"jq --indent 1 \\ '(.cells[] | select(has(\\\"outputs\\\")) | .outputs) = [] \\ | (.cells[] | select(has(\\\"execution_count\\\")) | .execution_count) = null \\ | .metadata = {\\\"language_info\\\": {\\\"name\\\": \\\"python\\\", \\\"pygments_lexer\\\": \\\"ipython3\\\"}} \\ | .cells[].metadata = {} \\ '\" Which can then be used conveniently like so: nbstrip_jq 01-parsing.ipynb > stripped.ipynb Not only does this give us full control to wipe that pesky metadata, it's pretty damn quick, taking something like a tenth of the time of nbstripout in my (admittedly ad-hoc) testing: nbstrip_jq 01-parsing.ipynb # (JSON contents omitted) real 0m0.015s user 0m0.008s sys 0m0.004s [*]Note on handling the notebook-level metadata section: Previously I had been blanking the metadata entirely, but it turns out that the pygments_lexer entry is crucial for nbsphinx to format notebooks with the correct syntax highlighting, hence the slightly awkward entry you see here. You might want to take a more careful approach and put together a jq-filter which merely removes (or normalizes) the Python version numbers, thereby lessening the risk of inadvertently wiping useful metadata. But for the purposes of this blog post I wanted to keep things as simple as possible while actually giving a usable, working setup. Automation: Integrating with git So we're all tooled up, but the question remains - how do we get git to run this automatically for us? For this, we dive into 'gitattributes' functionality, specifically the filter section. This describes how to define 'clean' and 'smudge' (reverse of clean) filters, which are operations that transform our data as it is checked in or out of the git-repository, so that (for example) our notebook-output cells are always stripped away from the JSON-data before it's added to the git repository: In the general case you can also define a smudge-filter to take your repository contents and do something with it to make it local to your system, but we'll not be needing that here - we'll just use the cat command as a placeholder. The easiest way to explain how to configure this is with an example. Personally, I want notebook-cleaning behaviour to be the default across all my git-repositories, so I have the following entries in my global ~/.gitconfig file: [core] attributesfile = ~/.gitattributes_global [filter \"nbstrip_full\"] clean = \"jq --indent 1 \\ '(.cells[] | select(has(\\\"outputs\\\")) | .outputs) = [] \\ | (.cells[] | select(has(\\\"execution_count\\\")) | .execution_count) = null \\ | .metadata = {\\\"language_info\\\": {\\\"name\\\": \\\"python\\\", \\\"pygments_lexer\\\": \\\"ipython3\\\"}} \\ | .cells[].metadata = {} \\ '\" smudge = cat required = true And then in ~/.gitattributes_global: *.ipynb filter=nbstrip_full (Note, once you've defined your filter you can just as easily assign it to files in a repository specific .gitattributes file if you prefer a fine-grained approach.) That's it! You're all set to go version control notebooks like a champ! Well, almost. Getting started and gotchas Note that we're into git-powertool territory here, so things might be a little less polished compared to the (cough) usual intuitive git interface you're used to. To start off with, assuming a pre-existing set of notebooks, you'll want to add a 'do-nothing' commit, where you simply pull in the newly-filtered versions of your notebooks and trim out any unwanted metadata. Just git add your notebooks, noting that you may need to touch them first, so git picks up on the timestamp-modification and actually looks at the files for changes. Then, to see the patch removing all the cruft. Commit that, then go ahead, run your notebooks, leave uncleaned outputs all over the place. Unless you change the actual code-cell contents, your git diff should be blank! Great. Except. If you have executed a notebook since your last commit, git status may show that file as 'modified', despite the fact that when you git diff, the filters go into action and no differences-to-HEAD are found. So you have to 'tune out' these false-positive modified flags when reading the git-status. Another issue is that if you use a diff-GUI such as meld, then beware: unlike git diff, git difftool will not apply filters to the working directory before comparing with the repo HEAD - so your command-line and GUI diffs have suddenly diverged! The logic behind this difference in behaviour is that GUI programs give the option to edit the local working-copy directly, as discussed at length in this thread. This has clearly caught out others before. If they bother you, these false-positives and diff-divergences can easily be resolved by manually applying the jq-filters before you run your diffs. For convenience, my ~/.bashrc also defines the following command to apply the filters to all notebooks in the current working directory: function nbstrip_all_cwd { for nbfile in *.ipynb; do echo \"$( nbstrip_jq $nbfile )\" > $nbfile done unset nbfile } Addtionally, let me note that clean/smudge filters often do not play well with rebase operations. Things get very confusing if you try to rebase across commits before / after applying a clean-filter. The simplest way to work around this is to simply comment out the relevant filter-assignment line in .gitattributes_global while performing a rebase, then uncomment it when done. As a parting note, if you also choose to configure your gitattributes globally, you may want to know how to 'whitelist' notebooks in a particular repository (for example, if you're checking-in executed notebooks to a github-pages documentation branch). This is dead easy, just add a local .gitattributes file to the repository and 'unset' the filter attribute, like so: Or you could replace the *.ipynb with a path to a specific notebook, etc. Hope that helps! Comments or corrections very welcome via Twitter. ",
        "_version_": 1718536547062513664
      },
      {
        "story_id": 21454153,
        "story_author": "askytb",
        "story_descendants": 144,
        "story_score": 706,
        "story_time": "2019-11-05T16:55:16Z",
        "story_title": "Stripe CLI",
        "search": [
          "Stripe CLI",
          "https://stripe.com/blog/stripe-cli",
          "Building and testing a Stripe integration can require frequent switching between the terminal, your code editor, and the Dashboard. Today, were excited to launch the Stripe command-line interface (CLI). It lets you interact with Stripe right from the terminal and makes it easier to build, test, and manage your integration.To start, the CLI will let you test webhooks, tail real-time API logs, and create or update API objects. Heres a preview of some of the features: Simplify webhook setup and testingStripe sends a variety ofwebhooks, which let you listen and respond to specific events programmatically. Runstripe listenwith the CLI to forward webhooks to your local web server during developmentno third-party tunneling tools required. You can also trigger and test specific webhook events withstripe trigger.Debug faster with real-time logsWhile integrating, it can be useful to look at logs to fix any issues. You can now usestripe logs tailto stream API request logs in real time in the terminal in addition to viewing these logs from the Dashboard. Quickly inspect parameters or JSON responses and debug errors as they happen.Speed up common tasks and workflowsYou can now create, retrieve, update, or delete any Stripe object directly from the CLI in both test and live mode. For example, you can usestripe customers createand specify parameters for properties inline.Since you can pipe results into other commands, this can be a simple and powerful way to automate tasks. Heres an example:The above command uses the CLI to list live subscriptions that are past due, pipes the JSON response tojqto extract the customer name and email, and exports the data in CSV format.To see a full list of supported commands, runstripe helpor visitthe docsto learn more.Getting startedThe Stripe CLI natively supportsmacOS, Windows, and Linux. You can also pull ourDocker imageto use in automated testing or a continuous integration setup.Were just getting started and well be adding a lot more features to the CLI. If you have feedback or feature requests, join the conversation onGitHub. You can also runstripe feedbackto share your ideas for which use cases we should tackle next. "
        ],
        "story_type": "Normal",
        "url_raw": "https://stripe.com/blog/stripe-cli",
        "url_text": "Building and testing a Stripe integration can require frequent switching between the terminal, your code editor, and the Dashboard. Today, were excited to launch the Stripe command-line interface (CLI). It lets you interact with Stripe right from the terminal and makes it easier to build, test, and manage your integration.To start, the CLI will let you test webhooks, tail real-time API logs, and create or update API objects. Heres a preview of some of the features: Simplify webhook setup and testingStripe sends a variety ofwebhooks, which let you listen and respond to specific events programmatically. Runstripe listenwith the CLI to forward webhooks to your local web server during developmentno third-party tunneling tools required. You can also trigger and test specific webhook events withstripe trigger.Debug faster with real-time logsWhile integrating, it can be useful to look at logs to fix any issues. You can now usestripe logs tailto stream API request logs in real time in the terminal in addition to viewing these logs from the Dashboard. Quickly inspect parameters or JSON responses and debug errors as they happen.Speed up common tasks and workflowsYou can now create, retrieve, update, or delete any Stripe object directly from the CLI in both test and live mode. For example, you can usestripe customers createand specify parameters for properties inline.Since you can pipe results into other commands, this can be a simple and powerful way to automate tasks. Heres an example:The above command uses the CLI to list live subscriptions that are past due, pipes the JSON response tojqto extract the customer name and email, and exports the data in CSV format.To see a full list of supported commands, runstripe helpor visitthe docsto learn more.Getting startedThe Stripe CLI natively supportsmacOS, Windows, and Linux. You can also pull ourDocker imageto use in automated testing or a continuous integration setup.Were just getting started and well be adding a lot more features to the CLI. If you have feedback or feature requests, join the conversation onGitHub. You can also runstripe feedbackto share your ideas for which use cases we should tackle next. ",
        "comments.comment_id": [21454954, 21456155],
        "comments.comment_author": ["knubie", "rsync"],
        "comments.comment_descendants": [9, 11],
        "comments.comment_time": [
          "2019-11-05T18:11:21Z",
          "2019-11-05T20:12:09Z"
        ],
        "comments.comment_text": [
          "Stripe is by far the best developer experience I’ve had in my career working with third party APIs/services. The attention to detail is just second to none, and documentation is a big part of this.",
          "In the Twilio world, I can do something like this, via the command line and their API:<p>- activate new phone number<p>- send SMS from that number<p>- deactivate number<p>I have not ever done <i>that particular workflow</i>, but I could.<p>Who is the Twilio of payments/fintech wherein I could perform a workflow like this:<p>- generate new CC number in my name<p>- set transaction limit(s) and expiry 10 days from now<p>- (go use that CC in real life)<p>- deactivate the CC<p>or maybe:<p>- disable existing card<p>- reenable existing card<p>I know I could do this if I wrote the API myself and had a big, complicated, sticky relationship with a bank ... but does someone let me do things like this if I am just an end-user (like Twilio does) ?"
        ],
        "id": "957e9a20-6634-4915-81bf-516b962f9a95",
        "_version_": 1718536539405811712
      },
      {
        "story_id": 19677850,
        "story_author": "whalesalad",
        "story_descendants": 230,
        "story_score": 332,
        "story_time": "2019-04-16T22:19:03Z",
        "story_title": "Panic’s Next Editor",
        "search": [
          "Panic’s Next Editor",
          "https://panic.com/next/",
          "If we're being honest, Mac apps are a bit of a lost art. There are great reasons to make cross-platform apps to start, they're cross-platform but it's just not who we are. Founded as a Mac software company in 1997, our joy at Panic comes from building things that feel truly, well, Mac-like. Long ago, we created Coda, an all-in-one Mac web editor that broke new ground. But when we started work on Nova, we looked at where the web was today, and where we needed to be. It was time for a fresh start. A powerful editor. A themeable interface. Flexible workflows. Useful tools. Robust extensions. And lots of settings. The Editor. It all starts with our first-class text-editor. It's new, hyper-fast, and flexible, with all the features you want: smart autocomplete, multiple cursors, a Minimap, editor overscroll, tag pairs and brackets, and way, way more. Autocomplete with Fuzzy Matching Minimap Issues Multiple Cursors Git Status For the curious, Nova has built-in support for CoffeeScript, CSS, Diff, ERB, Haml, HTML, INI, JavaScript, JSON, JSX, Less, Lua, Markdown, Perl, PHP, Python, Ruby, Sass, SCSS, Smarty, SQL, TSX, TypeScript, XML, and YAML. It's also very expandable, with a robust API and a built-in extension browser. (Here's a little editor story for fun. During beta we found some bugs in Apple's text layout engine that we just could not fix. Our solution? Writing our own text layout manager from scratch. Not only did this fix the bugs, but it also boosted our editor's performance. We're not messing around!) But even the best text engine in the world means nothing unless you actually enjoy spending your time in the app. So, how does Nova look? The Interface. It's beautiful. And clean. And fun. You can make Nova look exactly the way you want, while still feeling Mac-like. Bright, dark, cyberpunk, it's all you. Plus, themes are CSS-like and easy to write. Nova can even automatically change your theme when your Mac switches from light to dark mode. The Workflows. Nova doesn't just help you code. It helps your code run. You can easily create build and run tasks for your projects. We didn't have them in Coda, but boy do we have them now. They're custom scripts that can be triggered at any time by toolbar buttons or keyboard shortcuts. Imagine building content, and with the single click of a button watching as Nova fires up your local server, grabs the appropriate URL, and opens a browser for you, instantly. Just think of the time you'll save. Nova supports separate Build, Run, and Clean tasks. It can open a report when run. And the scripts can be written in a variety of languages. The Tools. Now, this is important. Editing text is just part of what Nova does. We've bundled in extremely useful tools to help you get your work done quickly and efficiently. They're all fast and native too, of course. The New Tab button doesn't just open a fresh document. although it does that, too. Click it to quickly access a feature-packed Transmit file browser, or a super-convenient Prompt terminal, all right inside Nova. Meanwhile, Nova's sidebar is packed with power. The sidebar can also be split to show multiple tools at once, on the left and/or right side of your editor. And you can drag your favorite tools into the sidebar dock at the top for one-click access. Nova also has Git source control tools built-in. Clone. Click-to-clone. Initialize a repo. Fetch and pull. Stage and unstage. Commit. Push. You know the drill. (We don't have built-in diff yet, but it's on our list!) Git status is available both in the editor and the sidebar. And a useful \"Show Last Change for Line\" pop-up explains commits. The Extensions. Nova has a robust extensions API. A Nova extension can do lots of things, like add support for new languages, extend the sidebar, draw beautiful new themes and syntax colors, validate different code, and much more. Even better, extensions are written in JavaScript, so anyone can write them. And Nova includes built-in extension templates for fast development. Check out some of this weeks popular extensions Text Tools 3.0.5 biati Sort, Transform, Filter, Delete Duplicates, Encode, Decode, Expand and Shrink Se... CSS Validator 0.9.2 Panic Integrates the W3C CSS online valiator. Prettier 2.3.0 Alexander Weiss Code formatter using prettier Beautify 1.4.1 Patrick A. Vuarnoz Format Javascript, JSON, CSS, SCSS, LESS, HTML and XML using JS-Beautify. JSON 1.1.1 Cameron Little Advanced JSON support for Nova TypeScript 2.4.0 Cameron Little Advanced TypeScript and JavaScript language support for Nova Browse Extensions The Settings. People have strong editor opinions. And we're here to help. Nova has a whole host of settings. We have easily customizable key bindings. We have custom, quickly-switchable workspace layouts. And we have loads of editor tweaks, from matching brackets to overscroll. (And if there's something you need to work that Nova doesn't have, just let us know! Nova is always changing, always growing.) Click around to see Nova's preferences! And So Much More. Command Palette Project Launcher with Custom Artwork Multiple Sidebars & Sidebar Splits Separate Editor & Window Themes Automatic Theme Changes Global & Project Clips Project-Wide Indexing Intelligent, Extendable Autocomplete Powerful Open Quickly Git Source Control Sidebar Preview Tabs Built-in Static Web Server Remote Publishing via FTP, SFTP, WebDAV, & Clouds Local & Remote Terminals Markdown Preview Customizable In-App Key Bindings Panic Sync for Servers & Keys Robust Extension API In-App Extension Library nova Command Line Tool Reopen Recently Closed Files Small or Large Sidebar Dock Sizes Project-specific Sidebar Layouts Remote-Bound Workspaces Quick Tab Overview Customizable Event Behaviors Deep-Filtering Files Sidebar Ignored Files in Sidebar Drag-to-Split Easily Merged JSON Project Settings Files Sidebar Navigation Controls Single-Click to Open Files Find & Replace in Project Powerful Find & Replace Wildcards Find Scopes Ignore Specific Files when Indexing Remote Files Sidebar Server Preferences Staged Publishing List Multiple Publishing Destinations per Project Save & Publish Rich Editor Typography & Styling Customizable Line Height Text Glow Support in Themes Multiple Insertion Point Styles Type & Function Separators Customizable Editor Overscroll Source Control Change Annotations Automatic Link Detection Spell Checking Powerful Clip Wildcard Tokens Expandable Issue Line Annotations Hierarchical Symbols List Jump To Definition Dictionary Define Popover EditorConfig Support Customizable Markdown Stylesheets Rainbow Bracket Nesting Rainbow Indentation Guides Matching Tag Highlighting Identifier Highlighting Automatic Closing-Bracket Insertion Bracket Wrapping Automatic Tag Closing Customizable Wrap Indentation Quickly Add Cursors for Successive Lines Project Issues Sidebar Git Branch & Switch Git Commit, Fetch, Push, & Pull Image, Audio, & Video Media Viewers Extension Updating Without Restarting IDE Task Output Reports Remote Tasks on Unix, Linux, Windows, & PowerShell Custom Task Environment Variables Automatic Parsing of Task Output Into Issues Auditory and Visual Terminal Beeps Customizable Terminal Tab Titles Terminal Key-Binding Escape Sequences Terminal \"Option as Meta Key\" Terminal URL and file detection Terminal Mouse Events RSA, ECDSA, & ED25519 Keys Dual-Pane File Browser Tabs Cloud Provider Files Support Transfer Transcripts Robust Transfer Settings & Rules External Preview in Browser with Live Reload Non-Interruptive Updates Install Updates On Quit Coda 2 Import & Migration Assistant Transmit 5 Import No-Fuss Analytics & Privacy Settings And Now You Know Why This Took Us a Few Years For Extensions: Develop Extensions In-App Safe, Sandboxed Environment Robust JavaScript API Project & Global Settings Rapidly Reload and Test Live Filesystem, Network, & Subprocess Access Debug Console Editor & Project Actions Linters & Validators Custom Language Grammars Expressive Completion Providers Build & Run Task Templates Custom Sidebars Syntax Inspector Language Server Protocol Support Custom Themes Workspace Notifications Text Parsing & Encoding Utilities Secure Credential Storage Submit Extensions Easily with Validation "
        ],
        "story_type": "Normal",
        "url_raw": "https://panic.com/next/",
        "comments.comment_id": [19678379, 19678473],
        "comments.comment_author": ["simonhamp", "azhenley"],
        "comments.comment_descendants": [1, 6],
        "comments.comment_time": [
          "2019-04-16T23:50:32Z",
          "2019-04-17T00:04:52Z"
        ],
        "comments.comment_text": [
          "Coda was one of my favourite apps when I first got a Mac. It was such a fresh experience when compared to the dry utilitarian stuff that was available on Windows at the time.<p>I believe that Panic will create a beautiful product and they have many supporters who will happily pay a license fee - I know I will!<p>But it’s not going to be an easy ride. There’s a lot to live up to here and they will need to have some killer features thrown in at the beginning.<p>The hardest part tho will be to make it an approachable tool that can hook the hobbyist but keep them on when they need more. I dropped Coda for a number of reasons, but it wasn’t because it wasn’t a great code editor.",
          "It looks promising. But I'm not sure what features it could add over VS Code for me. I also worry that it won't have the hackability of Electron-based editors. (Before anyone complains about Electron performance, I haven't had any issues in that regard.)<p>I do academic research on code editors."
        ],
        "id": "d8ff4b69-fc86-46e4-81cb-02aa3aee95f1",
        "url_text": "If we're being honest, Mac apps are a bit of a lost art. There are great reasons to make cross-platform apps to start, they're cross-platform but it's just not who we are. Founded as a Mac software company in 1997, our joy at Panic comes from building things that feel truly, well, Mac-like. Long ago, we created Coda, an all-in-one Mac web editor that broke new ground. But when we started work on Nova, we looked at where the web was today, and where we needed to be. It was time for a fresh start. A powerful editor. A themeable interface. Flexible workflows. Useful tools. Robust extensions. And lots of settings. The Editor. It all starts with our first-class text-editor. It's new, hyper-fast, and flexible, with all the features you want: smart autocomplete, multiple cursors, a Minimap, editor overscroll, tag pairs and brackets, and way, way more. Autocomplete with Fuzzy Matching Minimap Issues Multiple Cursors Git Status For the curious, Nova has built-in support for CoffeeScript, CSS, Diff, ERB, Haml, HTML, INI, JavaScript, JSON, JSX, Less, Lua, Markdown, Perl, PHP, Python, Ruby, Sass, SCSS, Smarty, SQL, TSX, TypeScript, XML, and YAML. It's also very expandable, with a robust API and a built-in extension browser. (Here's a little editor story for fun. During beta we found some bugs in Apple's text layout engine that we just could not fix. Our solution? Writing our own text layout manager from scratch. Not only did this fix the bugs, but it also boosted our editor's performance. We're not messing around!) But even the best text engine in the world means nothing unless you actually enjoy spending your time in the app. So, how does Nova look? The Interface. It's beautiful. And clean. And fun. You can make Nova look exactly the way you want, while still feeling Mac-like. Bright, dark, cyberpunk, it's all you. Plus, themes are CSS-like and easy to write. Nova can even automatically change your theme when your Mac switches from light to dark mode. The Workflows. Nova doesn't just help you code. It helps your code run. You can easily create build and run tasks for your projects. We didn't have them in Coda, but boy do we have them now. They're custom scripts that can be triggered at any time by toolbar buttons or keyboard shortcuts. Imagine building content, and with the single click of a button watching as Nova fires up your local server, grabs the appropriate URL, and opens a browser for you, instantly. Just think of the time you'll save. Nova supports separate Build, Run, and Clean tasks. It can open a report when run. And the scripts can be written in a variety of languages. The Tools. Now, this is important. Editing text is just part of what Nova does. We've bundled in extremely useful tools to help you get your work done quickly and efficiently. They're all fast and native too, of course. The New Tab button doesn't just open a fresh document. although it does that, too. Click it to quickly access a feature-packed Transmit file browser, or a super-convenient Prompt terminal, all right inside Nova. Meanwhile, Nova's sidebar is packed with power. The sidebar can also be split to show multiple tools at once, on the left and/or right side of your editor. And you can drag your favorite tools into the sidebar dock at the top for one-click access. Nova also has Git source control tools built-in. Clone. Click-to-clone. Initialize a repo. Fetch and pull. Stage and unstage. Commit. Push. You know the drill. (We don't have built-in diff yet, but it's on our list!) Git status is available both in the editor and the sidebar. And a useful \"Show Last Change for Line\" pop-up explains commits. The Extensions. Nova has a robust extensions API. A Nova extension can do lots of things, like add support for new languages, extend the sidebar, draw beautiful new themes and syntax colors, validate different code, and much more. Even better, extensions are written in JavaScript, so anyone can write them. And Nova includes built-in extension templates for fast development. Check out some of this weeks popular extensions Text Tools 3.0.5 biati Sort, Transform, Filter, Delete Duplicates, Encode, Decode, Expand and Shrink Se... CSS Validator 0.9.2 Panic Integrates the W3C CSS online valiator. Prettier 2.3.0 Alexander Weiss Code formatter using prettier Beautify 1.4.1 Patrick A. Vuarnoz Format Javascript, JSON, CSS, SCSS, LESS, HTML and XML using JS-Beautify. JSON 1.1.1 Cameron Little Advanced JSON support for Nova TypeScript 2.4.0 Cameron Little Advanced TypeScript and JavaScript language support for Nova Browse Extensions The Settings. People have strong editor opinions. And we're here to help. Nova has a whole host of settings. We have easily customizable key bindings. We have custom, quickly-switchable workspace layouts. And we have loads of editor tweaks, from matching brackets to overscroll. (And if there's something you need to work that Nova doesn't have, just let us know! Nova is always changing, always growing.) Click around to see Nova's preferences! And So Much More. Command Palette Project Launcher with Custom Artwork Multiple Sidebars & Sidebar Splits Separate Editor & Window Themes Automatic Theme Changes Global & Project Clips Project-Wide Indexing Intelligent, Extendable Autocomplete Powerful Open Quickly Git Source Control Sidebar Preview Tabs Built-in Static Web Server Remote Publishing via FTP, SFTP, WebDAV, & Clouds Local & Remote Terminals Markdown Preview Customizable In-App Key Bindings Panic Sync for Servers & Keys Robust Extension API In-App Extension Library nova Command Line Tool Reopen Recently Closed Files Small or Large Sidebar Dock Sizes Project-specific Sidebar Layouts Remote-Bound Workspaces Quick Tab Overview Customizable Event Behaviors Deep-Filtering Files Sidebar Ignored Files in Sidebar Drag-to-Split Easily Merged JSON Project Settings Files Sidebar Navigation Controls Single-Click to Open Files Find & Replace in Project Powerful Find & Replace Wildcards Find Scopes Ignore Specific Files when Indexing Remote Files Sidebar Server Preferences Staged Publishing List Multiple Publishing Destinations per Project Save & Publish Rich Editor Typography & Styling Customizable Line Height Text Glow Support in Themes Multiple Insertion Point Styles Type & Function Separators Customizable Editor Overscroll Source Control Change Annotations Automatic Link Detection Spell Checking Powerful Clip Wildcard Tokens Expandable Issue Line Annotations Hierarchical Symbols List Jump To Definition Dictionary Define Popover EditorConfig Support Customizable Markdown Stylesheets Rainbow Bracket Nesting Rainbow Indentation Guides Matching Tag Highlighting Identifier Highlighting Automatic Closing-Bracket Insertion Bracket Wrapping Automatic Tag Closing Customizable Wrap Indentation Quickly Add Cursors for Successive Lines Project Issues Sidebar Git Branch & Switch Git Commit, Fetch, Push, & Pull Image, Audio, & Video Media Viewers Extension Updating Without Restarting IDE Task Output Reports Remote Tasks on Unix, Linux, Windows, & PowerShell Custom Task Environment Variables Automatic Parsing of Task Output Into Issues Auditory and Visual Terminal Beeps Customizable Terminal Tab Titles Terminal Key-Binding Escape Sequences Terminal \"Option as Meta Key\" Terminal URL and file detection Terminal Mouse Events RSA, ECDSA, & ED25519 Keys Dual-Pane File Browser Tabs Cloud Provider Files Support Transfer Transcripts Robust Transfer Settings & Rules External Preview in Browser with Live Reload Non-Interruptive Updates Install Updates On Quit Coda 2 Import & Migration Assistant Transmit 5 Import No-Fuss Analytics & Privacy Settings And Now You Know Why This Took Us a Few Years For Extensions: Develop Extensions In-App Safe, Sandboxed Environment Robust JavaScript API Project & Global Settings Rapidly Reload and Test Live Filesystem, Network, & Subprocess Access Debug Console Editor & Project Actions Linters & Validators Custom Language Grammars Expressive Completion Providers Build & Run Task Templates Custom Sidebars Syntax Inspector Language Server Protocol Support Custom Themes Workspace Notifications Text Parsing & Encoding Utilities Secure Credential Storage Submit Extensions Easily with Validation ",
        "_version_": 1718536472541265920
      },
      {
        "story_id": 20518114,
        "story_author": "sergiotapia",
        "story_descendants": 32,
        "story_score": 70,
        "story_time": "2019-07-24T18:25:53Z",
        "story_title": "Panic – Nova Private Beta",
        "search": [
          "Panic – Nova Private Beta",
          "https://panic.com/nova/",
          "If we're being honest, Mac apps are a bit of a lost art. There are great reasons to make cross-platform apps to start, they're cross-platform but it's just not who we are. Founded as a Mac software company in 1997, our joy at Panic comes from building things that feel truly, well, Mac-like. Long ago, we created Coda, an all-in-one Mac web editor that broke new ground. But when we started work on Nova, we looked at where the web was today, and where we needed to be. It was time for a fresh start. A powerful editor. A themeable interface. Flexible workflows. Useful tools. Robust extensions. And lots of settings. The Editor. It all starts with our first-class text-editor. It's new, hyper-fast, and flexible, with all the features you want: smart autocomplete, multiple cursors, a Minimap, editor overscroll, tag pairs and brackets, and way, way more. Autocomplete with Fuzzy Matching Minimap Issues Multiple Cursors Git Status For the curious, Nova has built-in support for CoffeeScript, CSS, Diff, ERB, Haml, HTML, INI, JavaScript, JSON, JSX, Less, Lua, Markdown, Perl, PHP, Python, Ruby, Sass, SCSS, Smarty, SQL, TSX, TypeScript, XML, and YAML. It's also very expandable, with a robust API and a built-in extension browser. (Here's a little editor story for fun. During beta we found some bugs in Apple's text layout engine that we just could not fix. Our solution? Writing our own text layout manager from scratch. Not only did this fix the bugs, but it also boosted our editor's performance. We're not messing around!) But even the best text engine in the world means nothing unless you actually enjoy spending your time in the app. So, how does Nova look? The Interface. It's beautiful. And clean. And fun. You can make Nova look exactly the way you want, while still feeling Mac-like. Bright, dark, cyberpunk, it's all you. Plus, themes are CSS-like and easy to write. Nova can even automatically change your theme when your Mac switches from light to dark mode. The Workflows. Nova doesn't just help you code. It helps your code run. You can easily create build and run tasks for your projects. We didn't have them in Coda, but boy do we have them now. They're custom scripts that can be triggered at any time by toolbar buttons or keyboard shortcuts. Imagine building content, and with the single click of a button watching as Nova fires up your local server, grabs the appropriate URL, and opens a browser for you, instantly. Just think of the time you'll save. Nova supports separate Build, Run, and Clean tasks. It can open a report when run. And the scripts can be written in a variety of languages. The Tools. Now, this is important. Editing text is just part of what Nova does. We've bundled in extremely useful tools to help you get your work done quickly and efficiently. They're all fast and native too, of course. The New Tab button doesn't just open a fresh document. although it does that, too. Click it to quickly access a feature-packed Transmit file browser, or a super-convenient Prompt terminal, all right inside Nova. Meanwhile, Nova's sidebar is packed with power. The sidebar can also be split to show multiple tools at once, on the left and/or right side of your editor. And you can drag your favorite tools into the sidebar dock at the top for one-click access. Nova also has Git source control tools built-in. Clone. Click-to-clone. Initialize a repo. Fetch and pull. Stage and unstage. Commit. Push. You know the drill. (We don't have built-in diff yet, but it's on our list!) Git status is available both in the editor and the sidebar. And a useful \"Show Last Change for Line\" pop-up explains commits. The Extensions. Nova has a robust extensions API. A Nova extension can do lots of things, like add support for new languages, extend the sidebar, draw beautiful new themes and syntax colors, validate different code, and much more. Even better, extensions are written in JavaScript, so anyone can write them. And Nova includes built-in extension templates for fast development. Check out some of this weeks popular extensions Text Tools 3.0.5 biati Sort, Transform, Filter, Delete Duplicates, Encode, Decode, Expand and Shrink Se... CSS Validator 0.9.2 Panic Integrates the W3C CSS online valiator. Prettier 2.3.0 Alexander Weiss Code formatter using prettier Beautify 1.4.1 Patrick A. Vuarnoz Format Javascript, JSON, CSS, SCSS, LESS, HTML and XML using JS-Beautify. JSON 1.1.1 Cameron Little Advanced JSON support for Nova TypeScript 2.4.0 Cameron Little Advanced TypeScript and JavaScript language support for Nova Browse Extensions The Settings. People have strong editor opinions. And we're here to help. Nova has a whole host of settings. We have easily customizable key bindings. We have custom, quickly-switchable workspace layouts. And we have loads of editor tweaks, from matching brackets to overscroll. (And if there's something you need to work that Nova doesn't have, just let us know! Nova is always changing, always growing.) Click around to see Nova's preferences! And So Much More. Command Palette Project Launcher with Custom Artwork Multiple Sidebars & Sidebar Splits Separate Editor & Window Themes Automatic Theme Changes Global & Project Clips Project-Wide Indexing Intelligent, Extendable Autocomplete Powerful Open Quickly Git Source Control Sidebar Preview Tabs Built-in Static Web Server Remote Publishing via FTP, SFTP, WebDAV, & Clouds Local & Remote Terminals Markdown Preview Customizable In-App Key Bindings Panic Sync for Servers & Keys Robust Extension API In-App Extension Library nova Command Line Tool Reopen Recently Closed Files Small or Large Sidebar Dock Sizes Project-specific Sidebar Layouts Remote-Bound Workspaces Quick Tab Overview Customizable Event Behaviors Deep-Filtering Files Sidebar Ignored Files in Sidebar Drag-to-Split Easily Merged JSON Project Settings Files Sidebar Navigation Controls Single-Click to Open Files Find & Replace in Project Powerful Find & Replace Wildcards Find Scopes Ignore Specific Files when Indexing Remote Files Sidebar Server Preferences Staged Publishing List Multiple Publishing Destinations per Project Save & Publish Rich Editor Typography & Styling Customizable Line Height Text Glow Support in Themes Multiple Insertion Point Styles Type & Function Separators Customizable Editor Overscroll Source Control Change Annotations Automatic Link Detection Spell Checking Powerful Clip Wildcard Tokens Expandable Issue Line Annotations Hierarchical Symbols List Jump To Definition Dictionary Define Popover EditorConfig Support Customizable Markdown Stylesheets Rainbow Bracket Nesting Rainbow Indentation Guides Matching Tag Highlighting Identifier Highlighting Automatic Closing-Bracket Insertion Bracket Wrapping Automatic Tag Closing Customizable Wrap Indentation Quickly Add Cursors for Successive Lines Project Issues Sidebar Git Branch & Switch Git Commit, Fetch, Push, & Pull Image, Audio, & Video Media Viewers Extension Updating Without Restarting IDE Task Output Reports Remote Tasks on Unix, Linux, Windows, & PowerShell Custom Task Environment Variables Automatic Parsing of Task Output Into Issues Auditory and Visual Terminal Beeps Customizable Terminal Tab Titles Terminal Key-Binding Escape Sequences Terminal \"Option as Meta Key\" Terminal URL and file detection Terminal Mouse Events RSA, ECDSA, & ED25519 Keys Dual-Pane File Browser Tabs Cloud Provider Files Support Transfer Transcripts Robust Transfer Settings & Rules External Preview in Browser with Live Reload Non-Interruptive Updates Install Updates On Quit Coda 2 Import & Migration Assistant Transmit 5 Import No-Fuss Analytics & Privacy Settings And Now You Know Why This Took Us a Few Years For Extensions: Develop Extensions In-App Safe, Sandboxed Environment Robust JavaScript API Project & Global Settings Rapidly Reload and Test Live Filesystem, Network, & Subprocess Access Debug Console Editor & Project Actions Linters & Validators Custom Language Grammars Expressive Completion Providers Build & Run Task Templates Custom Sidebars Syntax Inspector Language Server Protocol Support Custom Themes Workspace Notifications Text Parsing & Encoding Utilities Secure Credential Storage Submit Extensions Easily with Validation "
        ],
        "story_type": "Normal",
        "url_raw": "https://panic.com/nova/",
        "comments.comment_id": [20519489, 20519942],
        "comments.comment_author": ["joshstrange", "steve_adams_86"],
        "comments.comment_descendants": [3, 4],
        "comments.comment_time": [
          "2019-07-24T20:46:31Z",
          "2019-07-24T21:28:28Z"
        ],
        "comments.comment_text": [
          "I LOVED Coda, it was my first \"real\" editor (a step up from NP++) but now that I'm using IDEA I can't imagine going back to something with such a small market cap/plugin developer ecosystem. I wish Panic the best (and I still love them as a company) but I just don't see Coda/Nova being worth it (price or ecosystem-wise) just to have a beautiful editor. Maybe I'll try a trial when it comes out but it just looks... less powerful... that I really need from my editor.<p>All that said Panic is awesome and the breadth of what they do (Mac Apps, iOS App, Games, handheld gaming hardware) is crazy. I wish they would show Prompt some more love as I fear that is slowly being abandoned and I'll have to find an alternative SSH iOS client.",
          "I'm a heavy user of VS Code and recently RubyMine (JetBrains does a wonderful job on their IDEs), but I'm really excited about this.<p>I wonder mainly about two things.<p>One, how can Panic compete with a free tool like VS Code? Its extensions are stellar, free, and well maintained. The core product is <i>extremely</i> well maintained and constantly improving.<p>Two, if they're competing with products like WebStorm or SublimeText, again... How?<p>I don't doubt that they can do it and that they have a solid plan, and that's exciting. Panic delivers on polish and that alone is very appealing. RubyMine for example is really great in terms of function, but it feels clunky as hell at times. It feels like the beastly Java app that it is. I respect the work JetBrains does on their platform more than enough to pay for it, but there's a ton of room for polish!<p>I'm eagerly waiting to give the beta a test run anyway. The prospect of a new tool to play with is always exciting, and I've never used a tool from Panic that I didn't enjoy."
        ],
        "id": "138f5c50-5844-41b6-94d4-c46709854eb4",
        "url_text": "If we're being honest, Mac apps are a bit of a lost art. There are great reasons to make cross-platform apps to start, they're cross-platform but it's just not who we are. Founded as a Mac software company in 1997, our joy at Panic comes from building things that feel truly, well, Mac-like. Long ago, we created Coda, an all-in-one Mac web editor that broke new ground. But when we started work on Nova, we looked at where the web was today, and where we needed to be. It was time for a fresh start. A powerful editor. A themeable interface. Flexible workflows. Useful tools. Robust extensions. And lots of settings. The Editor. It all starts with our first-class text-editor. It's new, hyper-fast, and flexible, with all the features you want: smart autocomplete, multiple cursors, a Minimap, editor overscroll, tag pairs and brackets, and way, way more. Autocomplete with Fuzzy Matching Minimap Issues Multiple Cursors Git Status For the curious, Nova has built-in support for CoffeeScript, CSS, Diff, ERB, Haml, HTML, INI, JavaScript, JSON, JSX, Less, Lua, Markdown, Perl, PHP, Python, Ruby, Sass, SCSS, Smarty, SQL, TSX, TypeScript, XML, and YAML. It's also very expandable, with a robust API and a built-in extension browser. (Here's a little editor story for fun. During beta we found some bugs in Apple's text layout engine that we just could not fix. Our solution? Writing our own text layout manager from scratch. Not only did this fix the bugs, but it also boosted our editor's performance. We're not messing around!) But even the best text engine in the world means nothing unless you actually enjoy spending your time in the app. So, how does Nova look? The Interface. It's beautiful. And clean. And fun. You can make Nova look exactly the way you want, while still feeling Mac-like. Bright, dark, cyberpunk, it's all you. Plus, themes are CSS-like and easy to write. Nova can even automatically change your theme when your Mac switches from light to dark mode. The Workflows. Nova doesn't just help you code. It helps your code run. You can easily create build and run tasks for your projects. We didn't have them in Coda, but boy do we have them now. They're custom scripts that can be triggered at any time by toolbar buttons or keyboard shortcuts. Imagine building content, and with the single click of a button watching as Nova fires up your local server, grabs the appropriate URL, and opens a browser for you, instantly. Just think of the time you'll save. Nova supports separate Build, Run, and Clean tasks. It can open a report when run. And the scripts can be written in a variety of languages. The Tools. Now, this is important. Editing text is just part of what Nova does. We've bundled in extremely useful tools to help you get your work done quickly and efficiently. They're all fast and native too, of course. The New Tab button doesn't just open a fresh document. although it does that, too. Click it to quickly access a feature-packed Transmit file browser, or a super-convenient Prompt terminal, all right inside Nova. Meanwhile, Nova's sidebar is packed with power. The sidebar can also be split to show multiple tools at once, on the left and/or right side of your editor. And you can drag your favorite tools into the sidebar dock at the top for one-click access. Nova also has Git source control tools built-in. Clone. Click-to-clone. Initialize a repo. Fetch and pull. Stage and unstage. Commit. Push. You know the drill. (We don't have built-in diff yet, but it's on our list!) Git status is available both in the editor and the sidebar. And a useful \"Show Last Change for Line\" pop-up explains commits. The Extensions. Nova has a robust extensions API. A Nova extension can do lots of things, like add support for new languages, extend the sidebar, draw beautiful new themes and syntax colors, validate different code, and much more. Even better, extensions are written in JavaScript, so anyone can write them. And Nova includes built-in extension templates for fast development. Check out some of this weeks popular extensions Text Tools 3.0.5 biati Sort, Transform, Filter, Delete Duplicates, Encode, Decode, Expand and Shrink Se... CSS Validator 0.9.2 Panic Integrates the W3C CSS online valiator. Prettier 2.3.0 Alexander Weiss Code formatter using prettier Beautify 1.4.1 Patrick A. Vuarnoz Format Javascript, JSON, CSS, SCSS, LESS, HTML and XML using JS-Beautify. JSON 1.1.1 Cameron Little Advanced JSON support for Nova TypeScript 2.4.0 Cameron Little Advanced TypeScript and JavaScript language support for Nova Browse Extensions The Settings. People have strong editor opinions. And we're here to help. Nova has a whole host of settings. We have easily customizable key bindings. We have custom, quickly-switchable workspace layouts. And we have loads of editor tweaks, from matching brackets to overscroll. (And if there's something you need to work that Nova doesn't have, just let us know! Nova is always changing, always growing.) Click around to see Nova's preferences! And So Much More. Command Palette Project Launcher with Custom Artwork Multiple Sidebars & Sidebar Splits Separate Editor & Window Themes Automatic Theme Changes Global & Project Clips Project-Wide Indexing Intelligent, Extendable Autocomplete Powerful Open Quickly Git Source Control Sidebar Preview Tabs Built-in Static Web Server Remote Publishing via FTP, SFTP, WebDAV, & Clouds Local & Remote Terminals Markdown Preview Customizable In-App Key Bindings Panic Sync for Servers & Keys Robust Extension API In-App Extension Library nova Command Line Tool Reopen Recently Closed Files Small or Large Sidebar Dock Sizes Project-specific Sidebar Layouts Remote-Bound Workspaces Quick Tab Overview Customizable Event Behaviors Deep-Filtering Files Sidebar Ignored Files in Sidebar Drag-to-Split Easily Merged JSON Project Settings Files Sidebar Navigation Controls Single-Click to Open Files Find & Replace in Project Powerful Find & Replace Wildcards Find Scopes Ignore Specific Files when Indexing Remote Files Sidebar Server Preferences Staged Publishing List Multiple Publishing Destinations per Project Save & Publish Rich Editor Typography & Styling Customizable Line Height Text Glow Support in Themes Multiple Insertion Point Styles Type & Function Separators Customizable Editor Overscroll Source Control Change Annotations Automatic Link Detection Spell Checking Powerful Clip Wildcard Tokens Expandable Issue Line Annotations Hierarchical Symbols List Jump To Definition Dictionary Define Popover EditorConfig Support Customizable Markdown Stylesheets Rainbow Bracket Nesting Rainbow Indentation Guides Matching Tag Highlighting Identifier Highlighting Automatic Closing-Bracket Insertion Bracket Wrapping Automatic Tag Closing Customizable Wrap Indentation Quickly Add Cursors for Successive Lines Project Issues Sidebar Git Branch & Switch Git Commit, Fetch, Push, & Pull Image, Audio, & Video Media Viewers Extension Updating Without Restarting IDE Task Output Reports Remote Tasks on Unix, Linux, Windows, & PowerShell Custom Task Environment Variables Automatic Parsing of Task Output Into Issues Auditory and Visual Terminal Beeps Customizable Terminal Tab Titles Terminal Key-Binding Escape Sequences Terminal \"Option as Meta Key\" Terminal URL and file detection Terminal Mouse Events RSA, ECDSA, & ED25519 Keys Dual-Pane File Browser Tabs Cloud Provider Files Support Transfer Transcripts Robust Transfer Settings & Rules External Preview in Browser with Live Reload Non-Interruptive Updates Install Updates On Quit Coda 2 Import & Migration Assistant Transmit 5 Import No-Fuss Analytics & Privacy Settings And Now You Know Why This Took Us a Few Years For Extensions: Develop Extensions In-App Safe, Sandboxed Environment Robust JavaScript API Project & Global Settings Rapidly Reload and Test Live Filesystem, Network, & Subprocess Access Debug Console Editor & Project Actions Linters & Validators Custom Language Grammars Expressive Completion Providers Build & Run Task Templates Custom Sidebars Syntax Inspector Language Server Protocol Support Custom Themes Workspace Notifications Text Parsing & Encoding Utilities Secure Credential Storage Submit Extensions Easily with Validation ",
        "_version_": 1718536504300535808
      },
      {
        "story_id": 19382838,
        "story_author": "kylehotchkiss",
        "story_descendants": 5,
        "story_score": 14,
        "story_time": "2019-03-13T19:42:33Z",
        "story_title": "Panic's Next Code Editor (Coda Successor)",
        "search": [
          "Panic's Next Code Editor (Coda Successor)",
          "https://panic.com/next/",
          "If we're being honest, Mac apps are a bit of a lost art. There are great reasons to make cross-platform apps to start, they're cross-platform but it's just not who we are. Founded as a Mac software company in 1997, our joy at Panic comes from building things that feel truly, well, Mac-like. Long ago, we created Coda, an all-in-one Mac web editor that broke new ground. But when we started work on Nova, we looked at where the web was today, and where we needed to be. It was time for a fresh start. A powerful editor. A themeable interface. Flexible workflows. Useful tools. Robust extensions. And lots of settings. The Editor. It all starts with our first-class text-editor. It's new, hyper-fast, and flexible, with all the features you want: smart autocomplete, multiple cursors, a Minimap, editor overscroll, tag pairs and brackets, and way, way more. Autocomplete with Fuzzy Matching Minimap Issues Multiple Cursors Git Status For the curious, Nova has built-in support for CoffeeScript, CSS, Diff, ERB, Haml, HTML, INI, JavaScript, JSON, JSX, Less, Lua, Markdown, Perl, PHP, Python, Ruby, Sass, SCSS, Smarty, SQL, TSX, TypeScript, XML, and YAML. It's also very expandable, with a robust API and a built-in extension browser. (Here's a little editor story for fun. During beta we found some bugs in Apple's text layout engine that we just could not fix. Our solution? Writing our own text layout manager from scratch. Not only did this fix the bugs, but it also boosted our editor's performance. We're not messing around!) But even the best text engine in the world means nothing unless you actually enjoy spending your time in the app. So, how does Nova look? The Interface. It's beautiful. And clean. And fun. You can make Nova look exactly the way you want, while still feeling Mac-like. Bright, dark, cyberpunk, it's all you. Plus, themes are CSS-like and easy to write. Nova can even automatically change your theme when your Mac switches from light to dark mode. The Workflows. Nova doesn't just help you code. It helps your code run. You can easily create build and run tasks for your projects. We didn't have them in Coda, but boy do we have them now. They're custom scripts that can be triggered at any time by toolbar buttons or keyboard shortcuts. Imagine building content, and with the single click of a button watching as Nova fires up your local server, grabs the appropriate URL, and opens a browser for you, instantly. Just think of the time you'll save. Nova supports separate Build, Run, and Clean tasks. It can open a report when run. And the scripts can be written in a variety of languages. The Tools. Now, this is important. Editing text is just part of what Nova does. We've bundled in extremely useful tools to help you get your work done quickly and efficiently. They're all fast and native too, of course. The New Tab button doesn't just open a fresh document. although it does that, too. Click it to quickly access a feature-packed Transmit file browser, or a super-convenient Prompt terminal, all right inside Nova. Meanwhile, Nova's sidebar is packed with power. The sidebar can also be split to show multiple tools at once, on the left and/or right side of your editor. And you can drag your favorite tools into the sidebar dock at the top for one-click access. Nova also has Git source control tools built-in. Clone. Click-to-clone. Initialize a repo. Fetch and pull. Stage and unstage. Commit. Push. You know the drill. (We don't have built-in diff yet, but it's on our list!) Git status is available both in the editor and the sidebar. And a useful \"Show Last Change for Line\" pop-up explains commits. The Extensions. Nova has a robust extensions API. A Nova extension can do lots of things, like add support for new languages, extend the sidebar, draw beautiful new themes and syntax colors, validate different code, and much more. Even better, extensions are written in JavaScript, so anyone can write them. And Nova includes built-in extension templates for fast development. Check out some of this weeks popular extensions Text Tools 3.0.5 biati Sort, Transform, Filter, Delete Duplicates, Encode, Decode, Expand and Shrink Se... CSS Validator 0.9.2 Panic Integrates the W3C CSS online valiator. Prettier 2.3.0 Alexander Weiss Code formatter using prettier Beautify 1.4.1 Patrick A. Vuarnoz Format Javascript, JSON, CSS, SCSS, LESS, HTML and XML using JS-Beautify. JSON 1.1.1 Cameron Little Advanced JSON support for Nova TypeScript 2.4.0 Cameron Little Advanced TypeScript and JavaScript language support for Nova Browse Extensions The Settings. People have strong editor opinions. And we're here to help. Nova has a whole host of settings. We have easily customizable key bindings. We have custom, quickly-switchable workspace layouts. And we have loads of editor tweaks, from matching brackets to overscroll. (And if there's something you need to work that Nova doesn't have, just let us know! Nova is always changing, always growing.) Click around to see Nova's preferences! And So Much More. Command Palette Project Launcher with Custom Artwork Multiple Sidebars & Sidebar Splits Separate Editor & Window Themes Automatic Theme Changes Global & Project Clips Project-Wide Indexing Intelligent, Extendable Autocomplete Powerful Open Quickly Git Source Control Sidebar Preview Tabs Built-in Static Web Server Remote Publishing via FTP, SFTP, WebDAV, & Clouds Local & Remote Terminals Markdown Preview Customizable In-App Key Bindings Panic Sync for Servers & Keys Robust Extension API In-App Extension Library nova Command Line Tool Reopen Recently Closed Files Small or Large Sidebar Dock Sizes Project-specific Sidebar Layouts Remote-Bound Workspaces Quick Tab Overview Customizable Event Behaviors Deep-Filtering Files Sidebar Ignored Files in Sidebar Drag-to-Split Easily Merged JSON Project Settings Files Sidebar Navigation Controls Single-Click to Open Files Find & Replace in Project Powerful Find & Replace Wildcards Find Scopes Ignore Specific Files when Indexing Remote Files Sidebar Server Preferences Staged Publishing List Multiple Publishing Destinations per Project Save & Publish Rich Editor Typography & Styling Customizable Line Height Text Glow Support in Themes Multiple Insertion Point Styles Type & Function Separators Customizable Editor Overscroll Source Control Change Annotations Automatic Link Detection Spell Checking Powerful Clip Wildcard Tokens Expandable Issue Line Annotations Hierarchical Symbols List Jump To Definition Dictionary Define Popover EditorConfig Support Customizable Markdown Stylesheets Rainbow Bracket Nesting Rainbow Indentation Guides Matching Tag Highlighting Identifier Highlighting Automatic Closing-Bracket Insertion Bracket Wrapping Automatic Tag Closing Customizable Wrap Indentation Quickly Add Cursors for Successive Lines Project Issues Sidebar Git Branch & Switch Git Commit, Fetch, Push, & Pull Image, Audio, & Video Media Viewers Extension Updating Without Restarting IDE Task Output Reports Remote Tasks on Unix, Linux, Windows, & PowerShell Custom Task Environment Variables Automatic Parsing of Task Output Into Issues Auditory and Visual Terminal Beeps Customizable Terminal Tab Titles Terminal Key-Binding Escape Sequences Terminal \"Option as Meta Key\" Terminal URL and file detection Terminal Mouse Events RSA, ECDSA, & ED25519 Keys Dual-Pane File Browser Tabs Cloud Provider Files Support Transfer Transcripts Robust Transfer Settings & Rules External Preview in Browser with Live Reload Non-Interruptive Updates Install Updates On Quit Coda 2 Import & Migration Assistant Transmit 5 Import No-Fuss Analytics & Privacy Settings And Now You Know Why This Took Us a Few Years For Extensions: Develop Extensions In-App Safe, Sandboxed Environment Robust JavaScript API Project & Global Settings Rapidly Reload and Test Live Filesystem, Network, & Subprocess Access Debug Console Editor & Project Actions Linters & Validators Custom Language Grammars Expressive Completion Providers Build & Run Task Templates Custom Sidebars Syntax Inspector Language Server Protocol Support Custom Themes Workspace Notifications Text Parsing & Encoding Utilities Secure Credential Storage Submit Extensions Easily with Validation "
        ],
        "story_type": "Normal",
        "url_raw": "https://panic.com/next/",
        "comments.comment_id": [19383006, 19384753],
        "comments.comment_author": ["tivert", "lurker213"],
        "comments.comment_descendants": [3, 0],
        "comments.comment_time": [
          "2019-03-13T19:55:37Z",
          "2019-03-13T22:49:19Z"
        ],
        "comments.comment_text": [
          "Why would you deliberately skew the text block like they did?  The page reads like a printout from a busted printer.<p>Definitely not the worst weird hipster web-design I've seen, but it just invokes an uncomfortable feeling of something being broken or off.  Not a good thing for a product announcement.",
          "the only way I see this having any chance is if they make it able to load textmate / vscode / sublime text extensions"
        ],
        "id": "179a19b1-f60a-4181-9b0c-6da091d40fd5",
        "url_text": "If we're being honest, Mac apps are a bit of a lost art. There are great reasons to make cross-platform apps to start, they're cross-platform but it's just not who we are. Founded as a Mac software company in 1997, our joy at Panic comes from building things that feel truly, well, Mac-like. Long ago, we created Coda, an all-in-one Mac web editor that broke new ground. But when we started work on Nova, we looked at where the web was today, and where we needed to be. It was time for a fresh start. A powerful editor. A themeable interface. Flexible workflows. Useful tools. Robust extensions. And lots of settings. The Editor. It all starts with our first-class text-editor. It's new, hyper-fast, and flexible, with all the features you want: smart autocomplete, multiple cursors, a Minimap, editor overscroll, tag pairs and brackets, and way, way more. Autocomplete with Fuzzy Matching Minimap Issues Multiple Cursors Git Status For the curious, Nova has built-in support for CoffeeScript, CSS, Diff, ERB, Haml, HTML, INI, JavaScript, JSON, JSX, Less, Lua, Markdown, Perl, PHP, Python, Ruby, Sass, SCSS, Smarty, SQL, TSX, TypeScript, XML, and YAML. It's also very expandable, with a robust API and a built-in extension browser. (Here's a little editor story for fun. During beta we found some bugs in Apple's text layout engine that we just could not fix. Our solution? Writing our own text layout manager from scratch. Not only did this fix the bugs, but it also boosted our editor's performance. We're not messing around!) But even the best text engine in the world means nothing unless you actually enjoy spending your time in the app. So, how does Nova look? The Interface. It's beautiful. And clean. And fun. You can make Nova look exactly the way you want, while still feeling Mac-like. Bright, dark, cyberpunk, it's all you. Plus, themes are CSS-like and easy to write. Nova can even automatically change your theme when your Mac switches from light to dark mode. The Workflows. Nova doesn't just help you code. It helps your code run. You can easily create build and run tasks for your projects. We didn't have them in Coda, but boy do we have them now. They're custom scripts that can be triggered at any time by toolbar buttons or keyboard shortcuts. Imagine building content, and with the single click of a button watching as Nova fires up your local server, grabs the appropriate URL, and opens a browser for you, instantly. Just think of the time you'll save. Nova supports separate Build, Run, and Clean tasks. It can open a report when run. And the scripts can be written in a variety of languages. The Tools. Now, this is important. Editing text is just part of what Nova does. We've bundled in extremely useful tools to help you get your work done quickly and efficiently. They're all fast and native too, of course. The New Tab button doesn't just open a fresh document. although it does that, too. Click it to quickly access a feature-packed Transmit file browser, or a super-convenient Prompt terminal, all right inside Nova. Meanwhile, Nova's sidebar is packed with power. The sidebar can also be split to show multiple tools at once, on the left and/or right side of your editor. And you can drag your favorite tools into the sidebar dock at the top for one-click access. Nova also has Git source control tools built-in. Clone. Click-to-clone. Initialize a repo. Fetch and pull. Stage and unstage. Commit. Push. You know the drill. (We don't have built-in diff yet, but it's on our list!) Git status is available both in the editor and the sidebar. And a useful \"Show Last Change for Line\" pop-up explains commits. The Extensions. Nova has a robust extensions API. A Nova extension can do lots of things, like add support for new languages, extend the sidebar, draw beautiful new themes and syntax colors, validate different code, and much more. Even better, extensions are written in JavaScript, so anyone can write them. And Nova includes built-in extension templates for fast development. Check out some of this weeks popular extensions Text Tools 3.0.5 biati Sort, Transform, Filter, Delete Duplicates, Encode, Decode, Expand and Shrink Se... CSS Validator 0.9.2 Panic Integrates the W3C CSS online valiator. Prettier 2.3.0 Alexander Weiss Code formatter using prettier Beautify 1.4.1 Patrick A. Vuarnoz Format Javascript, JSON, CSS, SCSS, LESS, HTML and XML using JS-Beautify. JSON 1.1.1 Cameron Little Advanced JSON support for Nova TypeScript 2.4.0 Cameron Little Advanced TypeScript and JavaScript language support for Nova Browse Extensions The Settings. People have strong editor opinions. And we're here to help. Nova has a whole host of settings. We have easily customizable key bindings. We have custom, quickly-switchable workspace layouts. And we have loads of editor tweaks, from matching brackets to overscroll. (And if there's something you need to work that Nova doesn't have, just let us know! Nova is always changing, always growing.) Click around to see Nova's preferences! And So Much More. Command Palette Project Launcher with Custom Artwork Multiple Sidebars & Sidebar Splits Separate Editor & Window Themes Automatic Theme Changes Global & Project Clips Project-Wide Indexing Intelligent, Extendable Autocomplete Powerful Open Quickly Git Source Control Sidebar Preview Tabs Built-in Static Web Server Remote Publishing via FTP, SFTP, WebDAV, & Clouds Local & Remote Terminals Markdown Preview Customizable In-App Key Bindings Panic Sync for Servers & Keys Robust Extension API In-App Extension Library nova Command Line Tool Reopen Recently Closed Files Small or Large Sidebar Dock Sizes Project-specific Sidebar Layouts Remote-Bound Workspaces Quick Tab Overview Customizable Event Behaviors Deep-Filtering Files Sidebar Ignored Files in Sidebar Drag-to-Split Easily Merged JSON Project Settings Files Sidebar Navigation Controls Single-Click to Open Files Find & Replace in Project Powerful Find & Replace Wildcards Find Scopes Ignore Specific Files when Indexing Remote Files Sidebar Server Preferences Staged Publishing List Multiple Publishing Destinations per Project Save & Publish Rich Editor Typography & Styling Customizable Line Height Text Glow Support in Themes Multiple Insertion Point Styles Type & Function Separators Customizable Editor Overscroll Source Control Change Annotations Automatic Link Detection Spell Checking Powerful Clip Wildcard Tokens Expandable Issue Line Annotations Hierarchical Symbols List Jump To Definition Dictionary Define Popover EditorConfig Support Customizable Markdown Stylesheets Rainbow Bracket Nesting Rainbow Indentation Guides Matching Tag Highlighting Identifier Highlighting Automatic Closing-Bracket Insertion Bracket Wrapping Automatic Tag Closing Customizable Wrap Indentation Quickly Add Cursors for Successive Lines Project Issues Sidebar Git Branch & Switch Git Commit, Fetch, Push, & Pull Image, Audio, & Video Media Viewers Extension Updating Without Restarting IDE Task Output Reports Remote Tasks on Unix, Linux, Windows, & PowerShell Custom Task Environment Variables Automatic Parsing of Task Output Into Issues Auditory and Visual Terminal Beeps Customizable Terminal Tab Titles Terminal Key-Binding Escape Sequences Terminal \"Option as Meta Key\" Terminal URL and file detection Terminal Mouse Events RSA, ECDSA, & ED25519 Keys Dual-Pane File Browser Tabs Cloud Provider Files Support Transfer Transcripts Robust Transfer Settings & Rules External Preview in Browser with Live Reload Non-Interruptive Updates Install Updates On Quit Coda 2 Import & Migration Assistant Transmit 5 Import No-Fuss Analytics & Privacy Settings And Now You Know Why This Took Us a Few Years For Extensions: Develop Extensions In-App Safe, Sandboxed Environment Robust JavaScript API Project & Global Settings Rapidly Reload and Test Live Filesystem, Network, & Subprocess Access Debug Console Editor & Project Actions Linters & Validators Custom Language Grammars Expressive Completion Providers Build & Run Task Templates Custom Sidebars Syntax Inspector Language Server Protocol Support Custom Themes Workspace Notifications Text Parsing & Encoding Utilities Secure Credential Storage Submit Extensions Easily with Validation ",
        "_version_": 1718536459395268608
      },
      {
        "story_id": 20012499,
        "story_author": "lwhsiao",
        "story_descendants": 94,
        "story_score": 236,
        "story_time": "2019-05-26T01:04:51Z",
        "story_title": "Hledger: Robust Plain Text Accounting",
        "search": [
          "Hledger: Robust Plain Text Accounting",
          "https://hledger.org/",
          "hledger easy, dependable plain text accounting Plain text what now ? Easy, you say ? Dependable ? Compatible Free Software Limitations Where to next ? But wait... hledger is free cross-platform accounting software for both power users and folks new to accounting. It's good for tracking money, time, investments, cryptocurrencies, inventory and more, with high accuracy, flexibility and privacy. It is a more actively maintained, largely compatible reimplementation of Ledger CLI, with a particular focus on ease of use and robustness. Quick links: install, getting started, manual, support, sponsors, A command-line tool (CLI). Transactions are stored in a journal file which you can edit with a text editor. From this hledger produces various reports, without changing your data. A live-updating terminal interface (TUI), that lets you review account balances and transactions quickly. (screencast) A zero-setup web interface (WUI), allowing terminal-free, point-and-click usage. Run it privately on your local machine, or on a server to collaborate with others. (demo). A haskell library. You can write scripts, add-on commands, or financial applications as powerful as hledger itself. Plain text what now ? hledger is a Plain Text Accounting system, where your accounting data is stored in a readable plain text file, often version-controlled. Some strengths of the PTA approach: Runs on your local computer, keeping your financial data private and under your control Simple model of operation: put a log of transactions in, get reports out Simple, expressive, human-readable, future-proof plain text format Can be version controlled, eg with Git, to safeguard your data, track changes, or collaborate Edit with your favourite text editor, or a data entry UI, or import from other formats Easy to script, automate, and integrate into custom workflows Lightweight, fast, non-distracting to use Great for learning more of double-entry bookkeeping and accounting. Easy, you say ? Well - depending on your experience, hledger or Plain Text Accounting may or may not seem easy at first encounter. (It may just be that our intro docs aren't good enough yet; we're working on that.) But certainly within its scope of Plain Text Accounting, hledger aims to be intuitive, learnable and highly usable, taking only the best from other PTA tools and leaving the rest. Here are some things it provides out of the box: Easy multi-currency double-entry accounting using only a plain text file Easy assisted data entry or [CSV import][convert] Easy zero-setup command line, terminal, and web user interfaces Easy multi-period balance sheet, income statement, and cashflow reports Easy summarising of account balances to a desired depth Easy output to text, HTML, CSV, JSON or SQL Easy import/export/co-usage with Ledger CLI or Beancount Easy to download or build on all major platforms Fast. Reports normally take a fraction of a second, and hledger-ui updates instantly as you edit. Dependable ? hledger strives to be comfortable to use, to be absolutely reliable, to provide real-world value, and to never waste your time. It provides: Robust installation: multiple options are provided for binary and source installation. Building from source is reliable and consistent across platforms. Robust execution: hledger is written in Haskell, a modern, highly-regarded programming language. Runtime failures are minimised by Haskell's memory management and strong compile-time type checking. Failures caused by user input are reported clearly and promptly. Robust testing: The software is continually tested by extensive automated tests. Robust features: built-in commands and options combine well with one another, and are expected to do something sensible in all cases, with all kinds of input. Robust calculation: results are expected to always perfectly match what you would calculate on paper, up to 255 decimal places. Robust parsing: dated items, such as balance assertions and balance assignments, are processed in date order. Assertions/assignments with the same date are processed in parse order. Multiple assertions/assignments within a single transaction work as you would expect. Robust reporting: reports are deterministic and not affected by the order of input files or data items except where that is part of their spec. Robust documentation: all functionality is documented precisely, with a mnemonic permalink. User manuals for your hledger version are available online, and built in for offline viewing. General and command-specific command line help is provided. We favour documentation-driven development. Compatible hledger is a rewrite of the pioneering Ledger CLI, aiming to build out the same core features to a higher level of quality, and to add new ones making it useful to more people. Ledger users will find the file formats and commands familiar, and with a little care can run both tools on the same data files. (You can read more about the origins and differences.) hledger can read Beancount files, or vice versa, by converting them with the beancount2ledger and ledger2beancount tools. Many tools exist for importing from other applications. Data can be exported as CSV, JSON or basic SQL. Free Software hledger is free software, with no purchase price or monthly fees. It is licensed under GNU GPLv3, providing the strongest guarantee that you will always have the right to run, inspect, modify, or share it. It is actively maintained, with regular releases and a large chat room and other support resources. Limitations What are some current limitations of hledger and Plain Text Accounting ? The \"GUIs\" are minimalist; there is no rich GUI competitive with Quicken or GNUCash. As a beginner you might feel there's too much choice, too much to read, yet not enough clear opinionated guidance. (We're working on it. A request in the chat will produce quick help.) hledger is fast, but not yet as fast as Ledger. Where to next ? Next, you could: Scan through the docs listed in the sidebar to your left. If it's not visible, click the menu button at top left. Check the FAQ. Do a tutorial: Quick Start, Accounting concepts, or Easy workflow #1, #2, or #3. Become an expert: read or skim the hledger user manual. Or just the COMMON TASKS. Check out the hledger-ui and hledger-web manuals. Browse the blog posts on plaintextaccounting.org. Watch videos, such as hledger fan's beginner lessons. Look at lots of example files. Introduce yourself in our chat room, or browse the mail list. But wait... hledger is brought to you by Simon Michael and 140+ contributors. I have been building and relying on this project continuously since 2007. I hope you too will find it useful in transforming your relationship with time and money. When you have achieved some success, please consider joining the sponsors to support the project. Thanks! "
        ],
        "story_type": "Normal",
        "url_raw": "https://hledger.org/",
        "comments.comment_id": [20018519, 20020392],
        "comments.comment_author": ["smichael", "stevesimmons"],
        "comments.comment_descendants": [6, 3],
        "comments.comment_time": [
          "2019-05-27T00:17:54Z",
          "2019-05-27T08:27:17Z"
        ],
        "comments.comment_text": [
          "I'm hledger's lead developer, thanks for the mention. If you have any questions or hit any snags trying it out, I'm ready to help. Critique of product, docs etc. is welcome. Last time (2017) we got 2 out of 41 comments about hledger, so I'm hoping for a few more. :) <a href=\"https://hledger.org/release-notes\" rel=\"nofollow\">https://hledger.org/release-notes</a> has the (software) changes since then. Our chat is #hledger on Freenode: <a href=\"http://webchat.freenode.net/?channels=hledger\" rel=\"nofollow\">http://webchat.freenode.net/?channels=hledger</a>",
          "These ledger formats have what feels to me an overly verbose multi-line structure for each transaction.<p>For 20 years, I have been using a format that combines a diary and ledger. The 'to' and 'from' accounts are either bank/asset accounts (sav, cc, hl for home loan, ...) or spend categories (snack, shopping, clothes, ... ). Each year I finish 31 December with a `# Closing balances` section for each bank/investment/pension account, then start a new text file `2019.txt` with an `# Opening balances` section before 1 January's entry.<p>Here is a typical day's format:<p><pre><code>  # 2019-05-27 Mon\n  2.50    sav>lunch    Cafe Nero - Coffee\n  100.00  sav>cc       Transfer - Credit card repayment\n  amt     from>to      forex: shop - desc [txn #] \n  # Diary\n  * <topic> - <what>\n  * Times - Sleep 01:00-06:00 5h. Run 45m. Work 08:00-18:00 10h.\n  * Work - Meeting with xxx on xxx. We agreed to xxx.\n  * Friends - Lunch with xxx\n  * Note - Running shoes are Asics GT-2000, size 48, model T500N.\n  # Links\n  * Hledger: Robust plain text accounting - https://news.ycombinator.com/item?id=20012499\n</code></pre>\nFor the first 5 years, I was single and reconciled everything down to the cent, using an AWK script to generate totals to match against my bank statements.<p>When I got married, all our accounts switched to joint ones.  I didn't want to track my my partner's side of our spending. So these logs became more a personal diary than a fully reconciled ledger. Anything I want to remember long-term goes either there or secured in a password safe."
        ],
        "id": "3e0cc261-f4cb-4248-98aa-30303f9ebfce",
        "url_text": "hledger easy, dependable plain text accounting Plain text what now ? Easy, you say ? Dependable ? Compatible Free Software Limitations Where to next ? But wait... hledger is free cross-platform accounting software for both power users and folks new to accounting. It's good for tracking money, time, investments, cryptocurrencies, inventory and more, with high accuracy, flexibility and privacy. It is a more actively maintained, largely compatible reimplementation of Ledger CLI, with a particular focus on ease of use and robustness. Quick links: install, getting started, manual, support, sponsors, A command-line tool (CLI). Transactions are stored in a journal file which you can edit with a text editor. From this hledger produces various reports, without changing your data. A live-updating terminal interface (TUI), that lets you review account balances and transactions quickly. (screencast) A zero-setup web interface (WUI), allowing terminal-free, point-and-click usage. Run it privately on your local machine, or on a server to collaborate with others. (demo). A haskell library. You can write scripts, add-on commands, or financial applications as powerful as hledger itself. Plain text what now ? hledger is a Plain Text Accounting system, where your accounting data is stored in a readable plain text file, often version-controlled. Some strengths of the PTA approach: Runs on your local computer, keeping your financial data private and under your control Simple model of operation: put a log of transactions in, get reports out Simple, expressive, human-readable, future-proof plain text format Can be version controlled, eg with Git, to safeguard your data, track changes, or collaborate Edit with your favourite text editor, or a data entry UI, or import from other formats Easy to script, automate, and integrate into custom workflows Lightweight, fast, non-distracting to use Great for learning more of double-entry bookkeeping and accounting. Easy, you say ? Well - depending on your experience, hledger or Plain Text Accounting may or may not seem easy at first encounter. (It may just be that our intro docs aren't good enough yet; we're working on that.) But certainly within its scope of Plain Text Accounting, hledger aims to be intuitive, learnable and highly usable, taking only the best from other PTA tools and leaving the rest. Here are some things it provides out of the box: Easy multi-currency double-entry accounting using only a plain text file Easy assisted data entry or [CSV import][convert] Easy zero-setup command line, terminal, and web user interfaces Easy multi-period balance sheet, income statement, and cashflow reports Easy summarising of account balances to a desired depth Easy output to text, HTML, CSV, JSON or SQL Easy import/export/co-usage with Ledger CLI or Beancount Easy to download or build on all major platforms Fast. Reports normally take a fraction of a second, and hledger-ui updates instantly as you edit. Dependable ? hledger strives to be comfortable to use, to be absolutely reliable, to provide real-world value, and to never waste your time. It provides: Robust installation: multiple options are provided for binary and source installation. Building from source is reliable and consistent across platforms. Robust execution: hledger is written in Haskell, a modern, highly-regarded programming language. Runtime failures are minimised by Haskell's memory management and strong compile-time type checking. Failures caused by user input are reported clearly and promptly. Robust testing: The software is continually tested by extensive automated tests. Robust features: built-in commands and options combine well with one another, and are expected to do something sensible in all cases, with all kinds of input. Robust calculation: results are expected to always perfectly match what you would calculate on paper, up to 255 decimal places. Robust parsing: dated items, such as balance assertions and balance assignments, are processed in date order. Assertions/assignments with the same date are processed in parse order. Multiple assertions/assignments within a single transaction work as you would expect. Robust reporting: reports are deterministic and not affected by the order of input files or data items except where that is part of their spec. Robust documentation: all functionality is documented precisely, with a mnemonic permalink. User manuals for your hledger version are available online, and built in for offline viewing. General and command-specific command line help is provided. We favour documentation-driven development. Compatible hledger is a rewrite of the pioneering Ledger CLI, aiming to build out the same core features to a higher level of quality, and to add new ones making it useful to more people. Ledger users will find the file formats and commands familiar, and with a little care can run both tools on the same data files. (You can read more about the origins and differences.) hledger can read Beancount files, or vice versa, by converting them with the beancount2ledger and ledger2beancount tools. Many tools exist for importing from other applications. Data can be exported as CSV, JSON or basic SQL. Free Software hledger is free software, with no purchase price or monthly fees. It is licensed under GNU GPLv3, providing the strongest guarantee that you will always have the right to run, inspect, modify, or share it. It is actively maintained, with regular releases and a large chat room and other support resources. Limitations What are some current limitations of hledger and Plain Text Accounting ? The \"GUIs\" are minimalist; there is no rich GUI competitive with Quicken or GNUCash. As a beginner you might feel there's too much choice, too much to read, yet not enough clear opinionated guidance. (We're working on it. A request in the chat will produce quick help.) hledger is fast, but not yet as fast as Ledger. Where to next ? Next, you could: Scan through the docs listed in the sidebar to your left. If it's not visible, click the menu button at top left. Check the FAQ. Do a tutorial: Quick Start, Accounting concepts, or Easy workflow #1, #2, or #3. Become an expert: read or skim the hledger user manual. Or just the COMMON TASKS. Check out the hledger-ui and hledger-web manuals. Browse the blog posts on plaintextaccounting.org. Watch videos, such as hledger fan's beginner lessons. Look at lots of example files. Introduce yourself in our chat room, or browse the mail list. But wait... hledger is brought to you by Simon Michael and 140+ contributors. I have been building and relying on this project continuously since 2007. I hope you too will find it useful in transforming your relationship with time and money. When you have achieved some success, please consider joining the sponsors to support the project. Thanks! ",
        "_version_": 1718536485871812608
      },
      {
        "story_id": 21045987,
        "story_author": "dhruvkar",
        "story_descendants": 25,
        "story_score": 73,
        "story_time": "2019-09-23T04:47:46Z",
        "story_title": "Bedrock – A modular, WAN-replicated, blockchain-based database",
        "search": [
          "Bedrock – A modular, WAN-replicated, blockchain-based database",
          "https://bedrockdb.com/",
          "Why Code Install Use Jobs Cache vs MySQL Replication Blockchain Multizone Chat Contact Bedrock Rock-solid distributed data Bedrock is a simple, modular, WAN-replicated, Blockchain-based data foundation for global-scale applications. Taking each of those in turn: Bedrock is simple. This means it exposes the fewest knobs necessary, with appropriate defaults at every layer. Bedrock is modular. This means its functionality is packaged into separate plugins that are decoupled and independently maintainable. Bedrock is WAN-replicated. This means it is designed to endure the myriad real-world problems that occur across slow, unreliable internet connections. Bedrock is Blockchain-based. This means it uses a private blockchain to synchronize and self organize. Bedrock is a data foundation. This means it is not just a simple database that responds to queries, but rather a platform on which data-processing applications (like databases, job queues, caches, etc) can be built. Bedrock is for global-scale applications. This means it is built to be deployed in a geo-redundant fashion spanning many datacenters around the world. Bedrock was built by Expensify, and is a networking and distributed transaction layer built atop SQLite, the fastest, most reliable, and most widely distributed database in the world. Why to use it If youre building a website or other online service, youve got to use something. Why use Bedrock rather than the alternatives? Weve provided a more detailed comparision against MySQL, but in general Bedrock is: Faster. This is true for networked queries using the Bedrock::DB plugin, but especially true for custom plugins you write yourself because SQLite is just a library that operates inside your processs memory space. That means when your plugin queries SQLite, it isnt serializing/deserializing over a network: its directly accessing the RAM of the database itself. This is great in a single node, but if you still want more (because who doesnt?) then install any number of nodes and load-balance reads across all of them. This means every CPU of every database server is available for parallel reads, each of which has direct access to the database RAM. Simpler. This is because Bedrock is written for modern hardware with large SSD-backed RAID drives and generous RAM file caches, and thereby doesnt mess with the zillion hacky tricks the other databases do to eke out high performance on largely obsolete hardware. This results in fewer esoteric knobs, and sane defaults that just work. More reliable. This is because Bedrocks synchronization engine supports active/active distributed transactions with automatic failover, and can be clustered not just inside a single datacenter, but across multiple datacenters spanning the internet. This means Bedrock continues functioning not only if a single node goes down, but even if you lose an entire datacenter. After all, it doesnt matter who you are using: your datacenter will fail, eventually. But you neednt fail along with it. More powerful. Most people dont realize just how powerful SQLite is. Indexes, triggers, foreign key constraints, native JSON support, expression indexes check the full list here. Youll be amazed, but thats just the start. On top of this Bedrock layers a robust plugin system, and includes a fully functional job queue and replicated cache all the basics you need for modern service design, wrapped into one simple package. Bedrock is not only production ready, but actively used by Expensifys many thousands of customers, and millions of users. (Curious why an expense reporting company built their own database? Read what the First Round Review has to say about it.) How to get it Bedrock can be compiled from source using the Expensify/Bedrock public repo, or installed with the commands below: Ubuntu Linux You can build from scratch as follows: # Clone out this repo: git clone https://github.com/Expensify/Bedrock.git # Install some dependencies sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt-get update sudo apt-get install build-essential gcc-9 g++-9 libpcre++-dev zlib1g-dev # Build it cd Bedrock make # Create an empty database (See: https://github.com/Expensify/Bedrock/issues/489) touch bedrock.db # Run it (press Ctrl^C to quit, or use -fork to make it run in the backgroud) ./bedrock # Connect to it in a different terminal using netcat nc localhost 8888 # Type \"Status\" and then enter twice to verify it's working # See here to use the default DB plugin: http://bedrockdb.com/db.html Arch Linux Copy/paste this command into your terminal: This will tansparently download the latest version from GitHub, compile it, package it up, and install it. MacOSX You can build from scratch as follows: # Clone out this repo: git clone https://github.com/Expensify/Bedrock.git # Install some dependencies with Brew (see: https://brew.sh/) brew update brew install gcc@6 # Configure PCRE to use C++17 and compile from source brew uninstall --ignore-dependencies pcre brew edit pcre # Add these to the end of the `system \"./configure\"` command: # \"--enable-cpp\", # \"--enable-pcre64\", # \"CXX=/usr/local/bin/g++-9\", # \"CXXFLAGS=--std=gnu++14\" brew install --build-from-source pcre # Build it cd Bedrock make # Create an empty database (See: https://github.com/Expensify/Bedrock/issues/489) touch bedrock.db # Run it (press Ctrl^C to quit, or use -fork to make it run in the backgroud) ./bedrock # Connect to it in a different terminal using netcat nc localhost 8888 # Type \"Status\" and then enter twice to verify it's working # See here to use the default DB plugin: http://bedrockdb.com/db.html How to use it Bedrock is so easy to use, youll think youre missing something. Once installed, Bedrock listens on localhost port 8888, and stores its database in /var/lib/bedrock. The easiest way to talk with Bedrock is using netcat as follows: $ nc localhost 8888 Query: SELECT 1 AS foo, 2 AS bar; That query can be any SQLite-compatible query including schema changes, foreign key constraints, partial indexes, native JSON expressions, or any of the tremendous amount of functionality SQLite offers. The result will be returned in an HTTP-like response format: 200 OK Content-Length: 16 foo | bar 1 | 2 By default, Bedrock optimizes the output for human consumption. If you are a robot, request JSON output: $ nc localhost 8888 Query query: SELECT 1 AS foo, 2 AS bar; format: json 200 OK Content-Length: 40 {\"headers\":[\"foo\",\"bar\"],\"rows\":[[1,2]]} Some people are creeped out by sockets, and prefer tools. No problem: Bedrock supports the MySQL protocol, meaning you can continue using whatever MySQL client you prefer: $ mysql -h 127.0.0.1 Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 1 Server version: bedrock 09b08f82e6eefe69f79bb8414882dd64182e3e8c Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> SELECT 1 AS foo, 2 AS bar; +------+------+ | foo | bar | +------+------+ | 1 | 2 | +------+------+ 1 row in set (0.01 sec) mysql> That also means you can continue using whatever MySQL language binding you already know and love. Alternatively, if you dont know or love any of them, Bedrock also provides a PHP binding that looks something like this: $bedrock = new Bedrock(); $result = $bedrock->db->query(\"SELECT 1 AS foo, 2 AS bar;\"); It really can be that easy. Bedrock plugins Additionally, Bedrock::DB is just one plugin to the overall Bedrock platform. Bedrock itself is less a database, and more a tool that can be used to build a wide variety of data-management applications with a database being just one example. Each plugin implements and exposes new externally-visible commands (essentially equivalent to stored procedures). However, unlike simple stored procedures, plugins can also include schema changes. Plugins can be enabled via the -plugins command line parameter. Current plugins include: Status - Provides basic status about the health the Bedrock cluster. DB - Provides direct SQL access to the underlying database. Jobs - Provides a simple job queue. Cache - Provides a simple replicated cache. MySQL - Emulates MySQL How to help and get helped So many ways! Run bedrock -? on the command line to see all the available command-line options Chat with us live on Bedrocks Gitter page Post to the Bedrock mailing list by emailing bedrock@googlegroups.com Create an issue in Bedrocks GitHub issue list Submit a PR to Bedrocks GitHub repo Email David, the CEO of Expensify (and biggest Bedrock fanboy ever) directly: dbarrett@expensify.com Join Expensify and you can work on Bedrock (and other, even cooler things) full time! "
        ],
        "story_type": "Normal",
        "url_raw": "https://bedrockdb.com/",
        "comments.comment_id": [21046442, 21046501],
        "comments.comment_author": ["prepend", "cakoose"],
        "comments.comment_descendants": [3, 5],
        "comments.comment_time": [
          "2019-09-23T06:37:06Z",
          "2019-09-23T06:50:41Z"
        ],
        "comments.comment_text": [
          "This looks promising. A typical blockchain discussion in my org (kind of averse to change with a few hundred database people, smart, but Microsoft stack) goes like this:\nSomeone: “We need a private blockchain for C use case. We’re about to pay consulting firm X a gazillion dollars to implement EthiHyperBitLedget”\nArchitect: “Silly bird, you just need a distributed database...”\nSomeone:”Fine then how do I get one of those”\nArchitect:”you just hire me and I’ll write a bunch of database stuff for you that is a one off spaghetti mess of sql server stuff”\n75% of the time they buy that blockchain thing\n25% of the time they try to spatchcock the sql thing\nSo far we are 0 for 10 for stuff being useful.<p>If there were a nice distributed database with the features of blockchain I think this could merge my excited Someones with my fuddyduddy Architects.",
          "The \"blockchain-based\" part is misleading.<p>For one, when people say \"blockchain\", they're almost always including the clever protocol that enables a currency with no central authority.  That's the interesting part.  Bedrock doesn't do any of that.<p>Without that, \"blockchain\" is just a simple technique for incremental hashing.  Bedrock uses that, but it's not substantial enough to sensibly list \"blockchain-based\" as one of the three top attributes.<p>(They could have said \"Paxos-based\".  That's the protocol they use to ensure things don't get out of sync.)<p>Just trying to capitalize on the cryptocurrency hype, I guess.  FaunaDB did something similar: <a href=\"https://fauna.com/blog/distributed-ledger-without-the-blockchain\" rel=\"nofollow\">https://fauna.com/blog/distributed-ledger-without-the-blockc...</a>"
        ],
        "id": "7d847a66-68d6-4b1b-8cea-7e15ec3e910a",
        "url_text": "Why Code Install Use Jobs Cache vs MySQL Replication Blockchain Multizone Chat Contact Bedrock Rock-solid distributed data Bedrock is a simple, modular, WAN-replicated, Blockchain-based data foundation for global-scale applications. Taking each of those in turn: Bedrock is simple. This means it exposes the fewest knobs necessary, with appropriate defaults at every layer. Bedrock is modular. This means its functionality is packaged into separate plugins that are decoupled and independently maintainable. Bedrock is WAN-replicated. This means it is designed to endure the myriad real-world problems that occur across slow, unreliable internet connections. Bedrock is Blockchain-based. This means it uses a private blockchain to synchronize and self organize. Bedrock is a data foundation. This means it is not just a simple database that responds to queries, but rather a platform on which data-processing applications (like databases, job queues, caches, etc) can be built. Bedrock is for global-scale applications. This means it is built to be deployed in a geo-redundant fashion spanning many datacenters around the world. Bedrock was built by Expensify, and is a networking and distributed transaction layer built atop SQLite, the fastest, most reliable, and most widely distributed database in the world. Why to use it If youre building a website or other online service, youve got to use something. Why use Bedrock rather than the alternatives? Weve provided a more detailed comparision against MySQL, but in general Bedrock is: Faster. This is true for networked queries using the Bedrock::DB plugin, but especially true for custom plugins you write yourself because SQLite is just a library that operates inside your processs memory space. That means when your plugin queries SQLite, it isnt serializing/deserializing over a network: its directly accessing the RAM of the database itself. This is great in a single node, but if you still want more (because who doesnt?) then install any number of nodes and load-balance reads across all of them. This means every CPU of every database server is available for parallel reads, each of which has direct access to the database RAM. Simpler. This is because Bedrock is written for modern hardware with large SSD-backed RAID drives and generous RAM file caches, and thereby doesnt mess with the zillion hacky tricks the other databases do to eke out high performance on largely obsolete hardware. This results in fewer esoteric knobs, and sane defaults that just work. More reliable. This is because Bedrocks synchronization engine supports active/active distributed transactions with automatic failover, and can be clustered not just inside a single datacenter, but across multiple datacenters spanning the internet. This means Bedrock continues functioning not only if a single node goes down, but even if you lose an entire datacenter. After all, it doesnt matter who you are using: your datacenter will fail, eventually. But you neednt fail along with it. More powerful. Most people dont realize just how powerful SQLite is. Indexes, triggers, foreign key constraints, native JSON support, expression indexes check the full list here. Youll be amazed, but thats just the start. On top of this Bedrock layers a robust plugin system, and includes a fully functional job queue and replicated cache all the basics you need for modern service design, wrapped into one simple package. Bedrock is not only production ready, but actively used by Expensifys many thousands of customers, and millions of users. (Curious why an expense reporting company built their own database? Read what the First Round Review has to say about it.) How to get it Bedrock can be compiled from source using the Expensify/Bedrock public repo, or installed with the commands below: Ubuntu Linux You can build from scratch as follows: # Clone out this repo: git clone https://github.com/Expensify/Bedrock.git # Install some dependencies sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt-get update sudo apt-get install build-essential gcc-9 g++-9 libpcre++-dev zlib1g-dev # Build it cd Bedrock make # Create an empty database (See: https://github.com/Expensify/Bedrock/issues/489) touch bedrock.db # Run it (press Ctrl^C to quit, or use -fork to make it run in the backgroud) ./bedrock # Connect to it in a different terminal using netcat nc localhost 8888 # Type \"Status\" and then enter twice to verify it's working # See here to use the default DB plugin: http://bedrockdb.com/db.html Arch Linux Copy/paste this command into your terminal: This will tansparently download the latest version from GitHub, compile it, package it up, and install it. MacOSX You can build from scratch as follows: # Clone out this repo: git clone https://github.com/Expensify/Bedrock.git # Install some dependencies with Brew (see: https://brew.sh/) brew update brew install gcc@6 # Configure PCRE to use C++17 and compile from source brew uninstall --ignore-dependencies pcre brew edit pcre # Add these to the end of the `system \"./configure\"` command: # \"--enable-cpp\", # \"--enable-pcre64\", # \"CXX=/usr/local/bin/g++-9\", # \"CXXFLAGS=--std=gnu++14\" brew install --build-from-source pcre # Build it cd Bedrock make # Create an empty database (See: https://github.com/Expensify/Bedrock/issues/489) touch bedrock.db # Run it (press Ctrl^C to quit, or use -fork to make it run in the backgroud) ./bedrock # Connect to it in a different terminal using netcat nc localhost 8888 # Type \"Status\" and then enter twice to verify it's working # See here to use the default DB plugin: http://bedrockdb.com/db.html How to use it Bedrock is so easy to use, youll think youre missing something. Once installed, Bedrock listens on localhost port 8888, and stores its database in /var/lib/bedrock. The easiest way to talk with Bedrock is using netcat as follows: $ nc localhost 8888 Query: SELECT 1 AS foo, 2 AS bar; That query can be any SQLite-compatible query including schema changes, foreign key constraints, partial indexes, native JSON expressions, or any of the tremendous amount of functionality SQLite offers. The result will be returned in an HTTP-like response format: 200 OK Content-Length: 16 foo | bar 1 | 2 By default, Bedrock optimizes the output for human consumption. If you are a robot, request JSON output: $ nc localhost 8888 Query query: SELECT 1 AS foo, 2 AS bar; format: json 200 OK Content-Length: 40 {\"headers\":[\"foo\",\"bar\"],\"rows\":[[1,2]]} Some people are creeped out by sockets, and prefer tools. No problem: Bedrock supports the MySQL protocol, meaning you can continue using whatever MySQL client you prefer: $ mysql -h 127.0.0.1 Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 1 Server version: bedrock 09b08f82e6eefe69f79bb8414882dd64182e3e8c Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> SELECT 1 AS foo, 2 AS bar; +------+------+ | foo | bar | +------+------+ | 1 | 2 | +------+------+ 1 row in set (0.01 sec) mysql> That also means you can continue using whatever MySQL language binding you already know and love. Alternatively, if you dont know or love any of them, Bedrock also provides a PHP binding that looks something like this: $bedrock = new Bedrock(); $result = $bedrock->db->query(\"SELECT 1 AS foo, 2 AS bar;\"); It really can be that easy. Bedrock plugins Additionally, Bedrock::DB is just one plugin to the overall Bedrock platform. Bedrock itself is less a database, and more a tool that can be used to build a wide variety of data-management applications with a database being just one example. Each plugin implements and exposes new externally-visible commands (essentially equivalent to stored procedures). However, unlike simple stored procedures, plugins can also include schema changes. Plugins can be enabled via the -plugins command line parameter. Current plugins include: Status - Provides basic status about the health the Bedrock cluster. DB - Provides direct SQL access to the underlying database. Jobs - Provides a simple job queue. Cache - Provides a simple replicated cache. MySQL - Emulates MySQL How to help and get helped So many ways! Run bedrock -? on the command line to see all the available command-line options Chat with us live on Bedrocks Gitter page Post to the Bedrock mailing list by emailing bedrock@googlegroups.com Create an issue in Bedrocks GitHub issue list Submit a PR to Bedrocks GitHub repo Email David, the CEO of Expensify (and biggest Bedrock fanboy ever) directly: dbarrett@expensify.com Join Expensify and you can work on Bedrock (and other, even cooler things) full time! ",
        "_version_": 1718536524694290432
      },
      {
        "story_id": 21900715,
        "story_author": "2038AD",
        "story_descendants": 70,
        "story_score": 129,
        "story_time": "2019-12-28T18:36:58Z",
        "story_title": "Unintuitive JSON Parsing",
        "search": [
          "Unintuitive JSON Parsing",
          "https://nullprogram.com/blog/2019/12/28/",
          "December 28, 2019 nullprogram.com/blog/2019/12/28/ This article was discussed on Hacker News and on reddit. Despite the goal of JSON being a subset of JavaScript which it failed to achieve (update: this was fixed) parsing JSON is quite unlike parsing a programming language. For invalid inputs, the specific cause of error is often counter-intuitive. Normally this doesnt matter, but I recently ran into a case where it does. Consider this invalid input to a JSON parser: To a human this might be interpreted as an array containing a number. Either the leading zero is ignored, or it indicates octal, as it does in many languages, including JavaScript. In either case the number in the array would be 1. However, JSON does not support leading zeros, neither ignoring them nor supporting octal notation. Heres the railroad diagram for numbers from the JSON specficaiton: Or in regular expression form: -?(0|[1-9][0-9]*)(\\.[0-9]+)?([eE][+-]?[0-9]+)? If a token starts with 0 then it can only be followed by ., e, or E. It cannot be followed by a digit. So, the natural human response to mentally parsing [01] is: This input is invalid because it contains a number with a leading zero, and leading zeros are not accepted. But this is not actually why parsing fails! A simple model for the parser is as consuming tokens from a lexer. The lexers job is to read individual code points (characters) from the input and group them into tokens. The possible tokens are string, number, left brace, right brace, left bracket, right bracket, comma, true, false, and null. The lexer skips over insignificant whitespace, and it doesnt care about structure, like matching braces and brackets. Thats the parsers job. In some instances the lexer can fail to parse a token. For example, if while looking for a new token the lexer reads the character %, then the input must be invalid. No token starts with this character. So in some cases invalid input will be detected by the lexer. The parser consumes tokens from the lexer and, using some state, ensures the sequence of tokens is valid. For example, arrays must be a well formed sequence of left bracket, value, comma, value, comma, etc., right bracket. One way to reject input with trailing garbage, is for the lexer to also produce an EOF (end of file/input) token when there are no more tokens, and the parser could specifically check for that token before accepting the input as valid. Getting back to the input [01], a JSON parser receives a left bracket token, then updates its bookkeeping to track that its parsing an array. When looking for the next token, the lexer sees the character 0 followed by 1. According to the railroad diagram, this is a number token (starts with 0), but 1 cannot be part of this token, so it produces a number token with the contents 0. Everything is still fine. Next the lexer sees 1 followed by ]. Since ] cannot be part of a number, it produces another number token with the contents 1. The parser receives this token but, since its parsing an array, it expects either a comma token or a right bracket. Since this is neither, the parser fails with an error about an unexpected number. The parser will not complain about leading zeros because JSON has no concept of leading zeros. Human intuition is right, but for the wrong reasons. Try this for yourself in your favorite JSON parser. Or even just pop up the JavaScript console in your browser and try it out: Firefox reports: SyntaxError: JSON.parse: expected , or ] after array element Chromium reports: SyntaxError: Unexpected number in JSON Edge reports (note it says number not digit): Error: Invalid number at position:3 In all cases the parsers accepted a zero as the first array element, then rejected the input after the second number token for being a bad sequence of tokens. In other words, this is a parser error rather than a lexer error, as a human might intuit. My JSON parser comes with a testing tool that shows the token stream up until the parser rejects the input, useful for understanding these situations: $ echo '[01]' | tests/stream struct expect seq[] = { {JSON_ARRAY}, {JSON_NUMBER, \"0\"}, {JSON_ERROR}, }; Theres an argument to be made here that perhaps the human readable error message should mention leading zeros, since thats likely the cause of the invalid input. That is, a human probably thought JSON allowed leading zeros, and so the clearer message would tell the human that JSON does not allow leading zeros. This is the more art than science part of parsing. Its the same story with this invalid input: From this input, the lexer unambiguously produces left bracket, true, false, right bracket. Its still up to the parser to reject this input. The only reason we never see truefalse in valid JSON is that the overall structure never allows these tokens to be adjacent, not because theyd be ambiguous. Programming languages have identifiers, and in a programming language this would parse as the identifier truefalse rather than true followed by false. From this point of view, JSON seems quite strange. Just as before, Firefox reports: SyntaxError: JSON.parse: expected , or ] after array element Chromium reports the same error as it does for [true false]: SyntaxError: Unexpected token f in JSON Edges message is probably a minor bug in their JSON parser: Error: Expected ] at position:10 Position 10 is the last character in false. The lexer consumed false from the input, produced a false token, then the parser rejected the input. When it reported the error, it chose the end of the invalid token as the error position rather than the start, despite the fact that the only two valid tokens (comma, right bracket) are both a single character. It should also say Expected ] or , (as Firefox does) rather than just ]. Concatenated JSON Thats all pretty academic. Except for producing nice error messages, nobody really cares so much why the input was rejected. The mismatch between intuition and reality isnt important. However, it does come up with concatenated JSON. Some parsers, including mine, will optionally consume multiple JSON values, one after another, from the same input. Heres an example from one of my favorite command line tools, jq: echo '{\"x\":0,\"y\":1}{\"x\":2,\"y\":3}{\"x\":4,\"y\":5}' | jq '.x + .y' 1 5 9 The input contains three unambiguously-concatenated JSON objects, so the parser produces three distinct objects. Now consider this input, this time outside of the context of an array: Is this invalid, one number, or two numbers? According to the lexer and parser model described above, this is valid and unambiguously two concatenated numbers. Heres what my parser says: $ echo '01' | tests/stream struct expect seq[] = { {JSON_NUMBER, \"0\"}, {JSON_DONE}, {JSON_NUMBER, \"1\"}, {JSON_DONE}, {JSON_ERROR}, }; Note: The JSON_DONE token indicates acceptance, and the JSON_ERROR token is an EOF indicator, not a hard error. Since jq allows leading zeros in its JSON input, its ambiguous and parses this as the number 1, so asking its opinion on this input isnt so interesting. I surveyed some other JSON parsers that accept concatenated JSON: Jackson: Reject as leading zero. Noggit: Reject as leading zero. yajl: Accept as two numbers. For my parser its the same story for truefalse: echo 'truefalse' | tests/stream struct expect seq[] = { {JSON_TRUE, \"true\"}, {JSON_DONE}, {JSON_FALSE, \"false\"}, {JSON_DONE}, {JSON_ERROR}, }; Neither rejecting nor accepting this input is wrong, per se. Concatenated JSON is outside of the scope of JSON itself, and concatenating arbitrary JSON objects without a whitespace delimiter can lead to weird and ill-formed input. This is all a great argument in favor or Newline Delimited JSON, and its two simple rules: Line separator is '\\n' Each line is a valid JSON value This solves the concatenation issue, and, even more, it works well with parsers not supporting concatenation: Split the input on newlines and pass each line to your JSON parser. "
        ],
        "story_type": "Normal",
        "url_raw": "https://nullprogram.com/blog/2019/12/28/",
        "comments.comment_id": [21901262, 21901416],
        "comments.comment_author": ["nicoburns", "ufo"],
        "comments.comment_descendants": [6, 2],
        "comments.comment_time": [
          "2019-12-28T19:49:51Z",
          "2019-12-28T20:10:12Z"
        ],
        "comments.comment_text": [
          "> The parser will not complain about leading zeros because JSON has no concept of leading zeros.<p>Of course there is no logical reason why the parser shouldn't have this concept just because the spec doesn't require. IMO, beyond basic correctness, user friendly error messages are the main differentiator between excellent parsers and crappy parsers.",
          "In cases like this, the parser and lexer can often produce a better error message if they are written to accept a more lax input and then check it for errors.<p>For example, instead of faithfully implementing the grammar from the specification, allow numbers with leading zeroes and then produce an error for them.<p>Another situation where this comes up is parsing language keywords. Instead of writing a separate lexer rule for every keyword, write a single rule for keyword-or-identifier, and then use a hash table lookup inside of that to determine if it is a keyword or identifier."
        ],
        "id": "f017d1e2-20ad-4946-a31c-7375fbe8d893",
        "url_text": "December 28, 2019 nullprogram.com/blog/2019/12/28/ This article was discussed on Hacker News and on reddit. Despite the goal of JSON being a subset of JavaScript which it failed to achieve (update: this was fixed) parsing JSON is quite unlike parsing a programming language. For invalid inputs, the specific cause of error is often counter-intuitive. Normally this doesnt matter, but I recently ran into a case where it does. Consider this invalid input to a JSON parser: To a human this might be interpreted as an array containing a number. Either the leading zero is ignored, or it indicates octal, as it does in many languages, including JavaScript. In either case the number in the array would be 1. However, JSON does not support leading zeros, neither ignoring them nor supporting octal notation. Heres the railroad diagram for numbers from the JSON specficaiton: Or in regular expression form: -?(0|[1-9][0-9]*)(\\.[0-9]+)?([eE][+-]?[0-9]+)? If a token starts with 0 then it can only be followed by ., e, or E. It cannot be followed by a digit. So, the natural human response to mentally parsing [01] is: This input is invalid because it contains a number with a leading zero, and leading zeros are not accepted. But this is not actually why parsing fails! A simple model for the parser is as consuming tokens from a lexer. The lexers job is to read individual code points (characters) from the input and group them into tokens. The possible tokens are string, number, left brace, right brace, left bracket, right bracket, comma, true, false, and null. The lexer skips over insignificant whitespace, and it doesnt care about structure, like matching braces and brackets. Thats the parsers job. In some instances the lexer can fail to parse a token. For example, if while looking for a new token the lexer reads the character %, then the input must be invalid. No token starts with this character. So in some cases invalid input will be detected by the lexer. The parser consumes tokens from the lexer and, using some state, ensures the sequence of tokens is valid. For example, arrays must be a well formed sequence of left bracket, value, comma, value, comma, etc., right bracket. One way to reject input with trailing garbage, is for the lexer to also produce an EOF (end of file/input) token when there are no more tokens, and the parser could specifically check for that token before accepting the input as valid. Getting back to the input [01], a JSON parser receives a left bracket token, then updates its bookkeeping to track that its parsing an array. When looking for the next token, the lexer sees the character 0 followed by 1. According to the railroad diagram, this is a number token (starts with 0), but 1 cannot be part of this token, so it produces a number token with the contents 0. Everything is still fine. Next the lexer sees 1 followed by ]. Since ] cannot be part of a number, it produces another number token with the contents 1. The parser receives this token but, since its parsing an array, it expects either a comma token or a right bracket. Since this is neither, the parser fails with an error about an unexpected number. The parser will not complain about leading zeros because JSON has no concept of leading zeros. Human intuition is right, but for the wrong reasons. Try this for yourself in your favorite JSON parser. Or even just pop up the JavaScript console in your browser and try it out: Firefox reports: SyntaxError: JSON.parse: expected , or ] after array element Chromium reports: SyntaxError: Unexpected number in JSON Edge reports (note it says number not digit): Error: Invalid number at position:3 In all cases the parsers accepted a zero as the first array element, then rejected the input after the second number token for being a bad sequence of tokens. In other words, this is a parser error rather than a lexer error, as a human might intuit. My JSON parser comes with a testing tool that shows the token stream up until the parser rejects the input, useful for understanding these situations: $ echo '[01]' | tests/stream struct expect seq[] = { {JSON_ARRAY}, {JSON_NUMBER, \"0\"}, {JSON_ERROR}, }; Theres an argument to be made here that perhaps the human readable error message should mention leading zeros, since thats likely the cause of the invalid input. That is, a human probably thought JSON allowed leading zeros, and so the clearer message would tell the human that JSON does not allow leading zeros. This is the more art than science part of parsing. Its the same story with this invalid input: From this input, the lexer unambiguously produces left bracket, true, false, right bracket. Its still up to the parser to reject this input. The only reason we never see truefalse in valid JSON is that the overall structure never allows these tokens to be adjacent, not because theyd be ambiguous. Programming languages have identifiers, and in a programming language this would parse as the identifier truefalse rather than true followed by false. From this point of view, JSON seems quite strange. Just as before, Firefox reports: SyntaxError: JSON.parse: expected , or ] after array element Chromium reports the same error as it does for [true false]: SyntaxError: Unexpected token f in JSON Edges message is probably a minor bug in their JSON parser: Error: Expected ] at position:10 Position 10 is the last character in false. The lexer consumed false from the input, produced a false token, then the parser rejected the input. When it reported the error, it chose the end of the invalid token as the error position rather than the start, despite the fact that the only two valid tokens (comma, right bracket) are both a single character. It should also say Expected ] or , (as Firefox does) rather than just ]. Concatenated JSON Thats all pretty academic. Except for producing nice error messages, nobody really cares so much why the input was rejected. The mismatch between intuition and reality isnt important. However, it does come up with concatenated JSON. Some parsers, including mine, will optionally consume multiple JSON values, one after another, from the same input. Heres an example from one of my favorite command line tools, jq: echo '{\"x\":0,\"y\":1}{\"x\":2,\"y\":3}{\"x\":4,\"y\":5}' | jq '.x + .y' 1 5 9 The input contains three unambiguously-concatenated JSON objects, so the parser produces three distinct objects. Now consider this input, this time outside of the context of an array: Is this invalid, one number, or two numbers? According to the lexer and parser model described above, this is valid and unambiguously two concatenated numbers. Heres what my parser says: $ echo '01' | tests/stream struct expect seq[] = { {JSON_NUMBER, \"0\"}, {JSON_DONE}, {JSON_NUMBER, \"1\"}, {JSON_DONE}, {JSON_ERROR}, }; Note: The JSON_DONE token indicates acceptance, and the JSON_ERROR token is an EOF indicator, not a hard error. Since jq allows leading zeros in its JSON input, its ambiguous and parses this as the number 1, so asking its opinion on this input isnt so interesting. I surveyed some other JSON parsers that accept concatenated JSON: Jackson: Reject as leading zero. Noggit: Reject as leading zero. yajl: Accept as two numbers. For my parser its the same story for truefalse: echo 'truefalse' | tests/stream struct expect seq[] = { {JSON_TRUE, \"true\"}, {JSON_DONE}, {JSON_FALSE, \"false\"}, {JSON_DONE}, {JSON_ERROR}, }; Neither rejecting nor accepting this input is wrong, per se. Concatenated JSON is outside of the scope of JSON itself, and concatenating arbitrary JSON objects without a whitespace delimiter can lead to weird and ill-formed input. This is all a great argument in favor or Newline Delimited JSON, and its two simple rules: Line separator is '\\n' Each line is a valid JSON value This solves the concatenation issue, and, even more, it works well with parsers not supporting concatenation: Split the input on newlines and pass each line to your JSON parser. ",
        "_version_": 1718536556024692736
      },
      {
        "story_id": 21145755,
        "story_author": "rajiv_abraham",
        "story_descendants": 18,
        "story_score": 44,
        "story_time": "2019-10-03T11:26:19Z",
        "story_title": "The Rosie Pattern Language",
        "search": [
          "The Rosie Pattern Language",
          "http://rosie-lang.org/about/",
          "Mission Statement The Rosie Pattern Language (RPL) is intended to replace regular expressions (regex) in situations where: many regex are used in an application, project, or organization; or some regex are used by many people, or by a few people over a long period of time; or regex are used in production systems, where the cost of run-time errors is high. The advantages conferred by RPL in cases 1 and 2 derive from the RPL syntax and expressive power. The syntax resembles a programming language, making expressions easier to read and understand, and compatible with tools like diff. RPL is based on Parsing Expression Grammars, which are more powerful than regex, obviating the need for recursive regex or regex grammars, both of which are ad hoc extensions and are not commonly supported in regex libraries. All three situations listed above involve maintainability. RPL is easier to maintain than regex (which are considered write only by most developers). The Rosie project provides both a command line and REPL for development and debugging. Also, RPL supports executable unit tests, making it possible to: have a suite of regression tests test expressions independent of the code that uses them compile and test expressions at build time, avoiding run-time errors in production Rosie is like regex, but better Rosie is a supercharged alternative to Regular Expressions (regex), matching patterns against any input text. Rosie ships with a standard library of patterns for matching timestamps, network addresses, email addresses, CSV files, JSON, and many more common syntactic forms. Rosie/RPL scales in ways that regex do not Scale to many patterns RPL is readable and maintainable. It is structured like a programming language, so you can: Build complex patterns out of simple ones Write patterns that others can read and understand, using whitespace, comments, and built-in test expressions Create libraries of reusable patterns Import pattern libraries built by other people Scale to big data The Rosie Pattern Engine is small and fast. The entire run-time takes less than 400KB of disk space, and around 20MB of resident memory Basic patterns take linear time to match, whereas most modern regex engines can exponentially backtrack Recursive patterns are available when needed, to match recursive data like html or json Current speed (v1.1.0 release) is approximately 5x faster than the regex-based grok Scale for productivity Rosie is flexible and extensible. Unlike most regex tools, Rosie can generate structured (JSON) output, making its output easy to store or to consume by downstream processes An alternate compressed output format can be selected to reduce data transfer volume The CLI uses different colors for dates, times, network addresses, etc., so that you dont have to read JSON when working interactively Plain text (not JSON) output can be selected when using rosie to replace grep Rosie is extensible with new patterns, libraries, color assignments, output formats, and macros Rosie has an interactive pattern development mode to help write and debug patterns Rosie supports UTF-8 natively, but input text can be in any encoding; Rosie can even handle invalid codepoints gracefully Rosie is released under the MIT license You can download Rosie from gitlab "
        ],
        "story_type": "Normal",
        "url_raw": "http://rosie-lang.org/about/",
        "url_text": "Mission Statement The Rosie Pattern Language (RPL) is intended to replace regular expressions (regex) in situations where: many regex are used in an application, project, or organization; or some regex are used by many people, or by a few people over a long period of time; or regex are used in production systems, where the cost of run-time errors is high. The advantages conferred by RPL in cases 1 and 2 derive from the RPL syntax and expressive power. The syntax resembles a programming language, making expressions easier to read and understand, and compatible with tools like diff. RPL is based on Parsing Expression Grammars, which are more powerful than regex, obviating the need for recursive regex or regex grammars, both of which are ad hoc extensions and are not commonly supported in regex libraries. All three situations listed above involve maintainability. RPL is easier to maintain than regex (which are considered write only by most developers). The Rosie project provides both a command line and REPL for development and debugging. Also, RPL supports executable unit tests, making it possible to: have a suite of regression tests test expressions independent of the code that uses them compile and test expressions at build time, avoiding run-time errors in production Rosie is like regex, but better Rosie is a supercharged alternative to Regular Expressions (regex), matching patterns against any input text. Rosie ships with a standard library of patterns for matching timestamps, network addresses, email addresses, CSV files, JSON, and many more common syntactic forms. Rosie/RPL scales in ways that regex do not Scale to many patterns RPL is readable and maintainable. It is structured like a programming language, so you can: Build complex patterns out of simple ones Write patterns that others can read and understand, using whitespace, comments, and built-in test expressions Create libraries of reusable patterns Import pattern libraries built by other people Scale to big data The Rosie Pattern Engine is small and fast. The entire run-time takes less than 400KB of disk space, and around 20MB of resident memory Basic patterns take linear time to match, whereas most modern regex engines can exponentially backtrack Recursive patterns are available when needed, to match recursive data like html or json Current speed (v1.1.0 release) is approximately 5x faster than the regex-based grok Scale for productivity Rosie is flexible and extensible. Unlike most regex tools, Rosie can generate structured (JSON) output, making its output easy to store or to consume by downstream processes An alternate compressed output format can be selected to reduce data transfer volume The CLI uses different colors for dates, times, network addresses, etc., so that you dont have to read JSON when working interactively Plain text (not JSON) output can be selected when using rosie to replace grep Rosie is extensible with new patterns, libraries, color assignments, output formats, and macros Rosie has an interactive pattern development mode to help write and debug patterns Rosie supports UTF-8 natively, but input text can be in any encoding; Rosie can even handle invalid codepoints gracefully Rosie is released under the MIT license You can download Rosie from gitlab ",
        "comments.comment_id": [21151598, 21157632],
        "comments.comment_author": ["wodenokoto", "devchix"],
        "comments.comment_descendants": [4, 0],
        "comments.comment_time": [
          "2019-10-03T20:11:53Z",
          "2019-10-04T13:49:40Z"
        ],
        "comments.comment_text": [
          "If it is so much better and easier to read than regex, why not show any examples? Even the examples page is void of any examples.",
          "I say this so often: \"I can't believe it's $year and we're still using regex!\"  So when I see \"new pattern matching language\" my eyes light up and I run over there -- but there's no example, no docs, even where it says \"examples\", and \"docs\", and it's hard to find anything I can use.  I'm sorry, am I a vulgar entitled twit for not finding the spoon right away?<p>Regex is terrible to read.  Give me an example, things we frequently match for, IP, credit card number, dates ... I read the example for date parsing, I'm ... not sure what the equiv is?  I suggest the authors put up a Rosetta stone of sort, eg. in regex-speak: [0-1][0-9]-[0-3][0-9]-201[0-9], in Rosie-speak: xyz.  What about capture group, that's what makes regex powerful, not just matching, that and the look-ahead look-behind.<p>Meta-comment: regex is buried in everything significant that I work with, it's buried in grep, the language libraries, Splunk.  It's going to be hard to dislodge, there's a deep moat because the tools and common use cases are ugly but well-understood.  Why <i>are</i> regex still being used?  Why has nothing better come along? How would I even regex-match extended Unicode?"
        ],
        "id": "c52e6b3c-c71f-4f36-8d48-a6a86c3b49a5",
        "_version_": 1718536529059512320
      },
      {
        "story_id": 19185405,
        "story_author": "sonabinu",
        "story_descendants": 18,
        "story_score": 86,
        "story_time": "2019-02-17T16:59:51Z",
        "story_title": "Awesome Python",
        "search": [
          "Awesome Python",
          "https://github.com/vinta/awesome-python",
          "Awesome Python A curated list of awesome Python frameworks, libraries, software and resources. Inspired by awesome-php. Awesome Python Admin Panels Algorithms and Design Patterns ASGI Servers Asynchronous Programming Audio Authentication Build Tools Built-in Classes Enhancement Caching ChatOps Tools CMS Code Analysis Command-line Interface Development Command-line Tools Compatibility Computer Vision Concurrency and Parallelism Configuration Cryptography Data Analysis Data Validation Data Visualization Database Drivers Database Date and Time Debugging Tools Deep Learning DevOps Tools Distributed Computing Distribution Documentation Downloader E-commerce Editor Plugins and IDEs Email Enterprise Application Integrations Environment Management Files Foreign Function Interface Forms Functional Programming Game Development Geolocation GUI Development Hardware HTML Manipulation HTTP Clients Image Processing Implementations Interactive Interpreter Internationalization Job Scheduler Logging Machine Learning Miscellaneous Natural Language Processing Network Virtualization News Feed ORM Package Management Package Repositories Penetration testing Permissions Processes Recommender Systems Refactoring RESTful API Robotics RPC Servers Science Search Serialization Serverless Frameworks Shell Specific Formats Processing Static Site Generator Tagging Task Queues Template Engine Testing Text Processing Third-party APIs URL Manipulation Video Web Asset Management Web Content Extracting Web Crawling Web Frameworks WebSocket WSGI Servers Resources Books Newsletters Podcasts Websites Contributing Admin Panels Libraries for administrative interfaces. ajenti - The admin panel your servers deserve. django-grappelli - A jazzy skin for the Django Admin-Interface. django-jet - Modern responsive template for the Django admin interface with improved functionality. django-suit - Alternative Django Admin-Interface (free only for Non-commercial use). django-xadmin - Drop-in replacement of Django admin comes with lots of goodies. flask-admin - Simple and extensible administrative interface framework for Flask. flower - Real-time monitor and web admin for Celery. jet-bridge - Admin panel framework for any application with nice UI (ex Jet Django). wooey - A Django app which creates automatic web UIs for Python scripts. Algorithms and Design Patterns Python implementation of data structures, algorithms and design patterns. Also see awesome-algorithms. Algorithms algorithms - Minimal examples of data structures and algorithms. python-ds - A collection of data structure and algorithms for coding interviews. sortedcontainers - Fast and pure-Python implementation of sorted collections. TheAlgorithms - All Algorithms implemented in Python. Design Patterns PyPattyrn - A simple yet effective library for implementing common design patterns. python-patterns - A collection of design patterns in Python. transitions - A lightweight, object-oriented finite state machine implementation. ASGI Servers ASGI-compatible web servers. daphne - A HTTP, HTTP2 and WebSocket protocol server for ASGI and ASGI-HTTP. uvicorn - A lightning-fast ASGI server implementation, using uvloop and httptools. Asynchronous Programming asyncio - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks. awesome-asyncio trio - A friendly library for async concurrency and I/O. Twisted - An event-driven networking engine. uvloop - Ultra fast asyncio event loop. Audio Libraries for manipulating audio and its metadata. Audio audioread - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding. dejavu - Audio fingerprinting and recognition. kapre - Keras Audio Preprocessors. librosa - Python library for audio and music analysis. matchering - A library for automated reference audio mastering. mingus - An advanced music theory and notation package with MIDI file and playback support. pyAudioAnalysis - Audio feature extraction, classification, segmentation and applications. pydub - Manipulate audio with a simple and easy high level interface. TimeSide - Open web audio processing framework. Metadata beets - A music library manager and MusicBrainz tagger. eyeD3 - A tool for working with audio files, specifically MP3 files containing ID3 metadata. mutagen - A Python module to handle audio metadata. tinytag - A library for reading music meta data of MP3, OGG, FLAC and Wave files. Authentication Libraries for implementing authentications schemes. OAuth authlib - JavaScript Object Signing and Encryption draft implementation. django-allauth - Authentication app for Django that \"just works.\" django-oauth-toolkit - OAuth 2 goodies for Django. oauthlib - A generic and thorough implementation of the OAuth request-signing logic. python-oauth2 - A fully tested, abstract interface to creating OAuth clients and servers. python-social-auth - An easy-to-setup social authentication mechanism. JWT pyjwt - JSON Web Token implementation in Python. python-jose - A JOSE implementation in Python. python-jwt - A module for generating and verifying JSON Web Tokens. Build Tools Compile software from source code. BitBake - A make-like build tool for embedded Linux. buildout - A build system for creating, assembling and deploying applications from multiple parts. PlatformIO - A console tool to build code with different development platforms. pybuilder - A continuous build tool written in pure Python. SCons - A software construction tool. Built-in Classes Enhancement Libraries for enhancing Python built-in classes. attrs - Replacement for __init__, __eq__, __repr__, etc. boilerplate in class definitions. bidict - Efficient, Pythonic bidirectional map data structures and related functionality.. Box - Python dictionaries with advanced dot notation access. dataclasses - (Python standard library) Data classes. DottedDict - A library that provides a method of accessing lists and dicts with a dotted path notation. CMS Content Management Systems. django-cms - An Open source enterprise CMS based on the Django. feincms - One of the most advanced Content Management Systems built on Django. indico - A feature-rich event management system, made @ CERN. Kotti - A high-level, Pythonic web application framework built on Pyramid. mezzanine - A powerful, consistent, and flexible content management platform. plone - A CMS built on top of the open source application server Zope. quokka - Flexible, extensible, small CMS powered by Flask and MongoDB. wagtail - A Django content management system. Caching Libraries for caching data. beaker - A WSGI middleware for sessions and caching. django-cache-machine - Automatic caching and invalidation for Django models. django-cacheops - A slick ORM cache with automatic granular event-driven invalidation. dogpile.cache - dogpile.cache is next generation replacement for Beaker made by same authors. HermesCache - Python caching library with tag-based invalidation and dogpile effect prevention. pylibmc - A Python wrapper around the libmemcached interface. python-diskcache - SQLite and file backed cache backend with faster lookups than memcached and redis. ChatOps Tools Libraries for chatbot development. errbot - The easiest and most popular chatbot to implement ChatOps. Code Analysis Tools of static analysis, linters and code quality checkers. Also see awesome-static-analysis. Code Analysis coala - Language independent and easily extendable code analysis application. code2flow - Turn your Python and JavaScript code into DOT flowcharts. prospector - A tool to analyse Python code. pycallgraph - A library that visualises the flow (call graph) of your Python application. vulture - A tool for finding and analysing dead Python code. Code Linters flake8 - A wrapper around pycodestyle, pyflakes and McCabe. awesome-flake8-extensions pylama - A code audit tool for Python and JavaScript. pylint - A fully customizable source code analyzer. wemake-python-styleguide - The strictest and most opinionated python linter ever. Code Formatters black - The uncompromising Python code formatter. isort - A Python utility / library to sort imports. yapf - Yet another Python code formatter from Google. Static Type Checkers, also see awesome-python-typing mypy - Check variable types during compile time. pyre-check - Performant type checking. typeshed - Collection of library stubs for Python, with static types. Static Type Annotations Generators MonkeyType - A system for Python that generates static type annotations by collecting runtime types. pyannotate - Auto-generate PEP-484 annotations. pytype - Pytype checks and infers types for Python code - without requiring type annotations. Command-line Interface Development Libraries for building command-line applications. Command-line Application Development cement - CLI Application Framework for Python. click - A package for creating beautiful command line interfaces in a composable way. cliff - A framework for creating command-line programs with multi-level commands. docopt - Pythonic command line arguments parser. python-fire - A library for creating command line interfaces from absolutely any Python object. python-prompt-toolkit - A library for building powerful interactive command lines. Terminal Rendering alive-progress - A new kind of Progress Bar, with real-time throughput, eta and very cool animations. asciimatics - A package to create full-screen text UIs (from interactive forms to ASCII animations). bashplotlib - Making basic plots in the terminal. colorama - Cross-platform colored terminal text. rich - Python library for rich text and beautiful formatting in the terminal. Also provides a great RichHandler log handler. tqdm - Fast, extensible progress bar for loops and CLI. Command-line Tools Useful CLI-based tools for productivity. Productivity Tools copier - A library and command-line utility for rendering projects templates. cookiecutter - A command-line utility that creates projects from cookiecutters (project templates). doitlive - A tool for live presentations in the terminal. howdoi - Instant coding answers via the command line. Invoke - A tool for managing shell-oriented subprocesses and organizing executable Python code into CLI-invokable tasks. PathPicker - Select files out of bash output. percol - Adds flavor of interactive selection to the traditional pipe concept on UNIX. thefuck - Correcting your previous console command. tmuxp - A tmux session manager. try - A dead simple CLI to try out python packages - it's never been easier. CLI Enhancements httpie - A command line HTTP client, a user-friendly cURL replacement. iredis - Redis CLI with autocompletion and syntax highlighting. kube-shell - An integrated shell for working with the Kubernetes CLI. litecli - SQLite CLI with autocompletion and syntax highlighting. mycli - MySQL CLI with autocompletion and syntax highlighting. pgcli - PostgreSQL CLI with autocompletion and syntax highlighting. saws - A Supercharged aws-cli. Compatibility Libraries for migrating from Python 2 to 3. python-future - The missing compatibility layer between Python 2 and Python 3. modernize - Modernizes Python code for eventual Python 3 migration. six - Python 2 and 3 compatibility utilities. Computer Vision Libraries for Computer Vision. EasyOCR - Ready-to-use OCR with 40+ languages supported. Face Recognition - Simple facial recognition library. Kornia - Open Source Differentiable Computer Vision Library for PyTorch. OpenCV - Open Source Computer Vision Library. pytesseract - A wrapper for Google Tesseract OCR. SimpleCV - An open source framework for building computer vision applications. tesserocr - Another simple, Pillow-friendly, wrapper around the tesseract-ocr API for OCR. Concurrency and Parallelism Libraries for concurrent and parallel execution. Also see awesome-asyncio. concurrent.futures - (Python standard library) A high-level interface for asynchronously executing callables. eventlet - Asynchronous framework with WSGI support. gevent - A coroutine-based Python networking library that uses greenlet. multiprocessing - (Python standard library) Process-based parallelism. scoop - Scalable Concurrent Operations in Python. uvloop - Ultra fast implementation of asyncio event loop on top of libuv. Configuration Libraries for storing and parsing configuration options. configobj - INI file parser with validation. configparser - (Python standard library) INI file parser. hydra - Hydra is a framework for elegantly configuring complex applications. profig - Config from multiple formats with value conversion. python-decouple - Strict separation of settings from code. Cryptography cryptography - A package designed to expose cryptographic primitives and recipes to Python developers. paramiko - The leading native Python SSHv2 protocol library. passlib - Secure password storage/hashing library, very high level. pynacl - Python binding to the Networking and Cryptography (NaCl) library. Data Analysis Libraries for data analyzing. AWS Data Wrangler - Pandas on AWS. Blaze - NumPy and Pandas interface to Big Data. Open Mining - Business Intelligence (BI) in Pandas interface. Optimus - Agile Data Science Workflows made easy with PySpark. Orange - Data mining, data visualization, analysis and machine learning through visual programming or scripts. Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools. Data Validation Libraries for validating data. Used for forms in many cases. Cerberus - A lightweight and extensible data validation library. colander - Validating and deserializing data obtained via XML, JSON, an HTML form post. jsonschema - An implementation of JSON Schema for Python. schema - A library for validating Python data structures. Schematics - Data Structure Validation. valideer - Lightweight extensible data validation and adaptation library. voluptuous - A Python data validation library. Data Visualization Libraries for visualizing data. Also see awesome-javascript. Altair - Declarative statistical visualization library for Python. Bokeh - Interactive Web Plotting for Python. bqplot - Interactive Plotting Library for the Jupyter Notebook. Cartopy - A cartographic python library with matplotlib support. Dash - Built on top of Flask, React and Plotly aimed at analytical web applications. awesome-dash diagrams - Diagram as Code. Matplotlib - A Python 2D plotting library. plotnine - A grammar of graphics for Python based on ggplot2. Pygal - A Python SVG Charts Creator. PyGraphviz - Python interface to Graphviz. PyQtGraph - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets. Seaborn - Statistical data visualization using Matplotlib. VisPy - High-performance scientific visualization based on OpenGL. Database Databases implemented in Python. pickleDB - A simple and lightweight key-value store for Python. tinydb - A tiny, document-oriented database. ZODB - A native object database for Python. A key-value and object graph database. Database Drivers Libraries for connecting and operating databases. MySQL - awesome-mysql mysqlclient - MySQL connector with Python 3 support (mysql-python fork). PyMySQL - A pure Python MySQL driver compatible to mysql-python. PostgreSQL - awesome-postgres psycopg2 - The most popular PostgreSQL adapter for Python. queries - A wrapper of the psycopg2 library for interacting with PostgreSQL. SQlite - awesome-sqlite sqlite3 - (Python standard library) SQlite interface compliant with DB-API 2.0 SuperSQLite - A supercharged SQLite library built on top of apsw. Other Relational Databases pymssql - A simple database interface to Microsoft SQL Server. clickhouse-driver - Python driver with native interface for ClickHouse. NoSQL Databases cassandra-driver - The Python Driver for Apache Cassandra. happybase - A developer-friendly library for Apache HBase. kafka-python - The Python client for Apache Kafka. py2neo - A client library and toolkit for working with Neo4j. pymongo - The official Python client for MongoDB. redis-py - The Python client for Redis. Asynchronous Clients motor - The async Python driver for MongoDB. Date and Time Libraries for working with dates and times. Arrow - A Python library that offers a sensible and human-friendly approach to creating, manipulating, formatting and converting dates, times and timestamps. Chronyk - A Python 3 library for parsing human-written times and dates. dateutil - Extensions to the standard Python datetime module. delorean - A library for clearing up the inconvenient truths that arise dealing with datetimes. maya - Datetimes for Humans. moment - A Python library for dealing with dates/times. Inspired by Moment.js. Pendulum - Python datetimes made easy. PyTime - An easy-to-use Python module which aims to operate date/time/datetime by string. pytz - World timezone definitions, modern and historical. Brings the tz database into Python. when.py - Providing user-friendly functions to help perform common date and time actions. Debugging Tools Libraries for debugging code. pdb-like Debugger ipdb - IPython-enabled pdb. pdb++ - Another drop-in replacement for pdb. pudb - A full-screen, console-based Python debugger. wdb - An improbable web debugger through WebSockets. Tracing lptrace - strace for Python programs. manhole - Debugging UNIX socket connections and present the stacktraces for all threads and an interactive prompt. pyringe - Debugger capable of attaching to and injecting code into Python processes. python-hunter - A flexible code tracing toolkit. Profiler line_profiler - Line-by-line profiling. memory_profiler - Monitor Memory usage of Python code. py-spy - A sampling profiler for Python programs. Written in Rust. pyflame - A ptracing profiler For Python. vprof - Visual Python profiler. Others django-debug-toolbar - Display various debug information for Django. django-devserver - A drop-in replacement for Django's runserver. flask-debugtoolbar - A port of the django-debug-toolbar to flask. icecream - Inspect variables, expressions, and program execution with a single, simple function call. pyelftools - Parsing and analyzing ELF files and DWARF debugging information. Deep Learning Frameworks for Neural Networks and Deep Learning. Also see awesome-deep-learning. caffe - A fast open framework for deep learning.. keras - A high-level neural networks library and capable of running on top of either TensorFlow or Theano. mxnet - A deep learning framework designed for both efficiency and flexibility. pytorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration. SerpentAI - Game agent framework. Use any video game as a deep learning sandbox. tensorflow - The most popular Deep Learning framework created by Google. Theano - A library for fast numerical computation. DevOps Tools Software and libraries for DevOps. Configuration Management ansible - A radically simple IT automation platform. cloudinit - A multi-distribution package that handles early initialization of a cloud instance. OpenStack - Open source software for building private and public clouds. pyinfra - A versatile CLI tools and python libraries to automate infrastructure. saltstack - Infrastructure automation and management system. SSH-style Deployment cuisine - Chef-like functionality for Fabric. fabric - A simple, Pythonic tool for remote execution and deployment. fabtools - Tools for writing awesome Fabric files. Process Management honcho - A Python clone of Foreman, for managing Procfile-based applications. supervisor - Supervisor process control system for UNIX. Monitoring psutil - A cross-platform process and system utilities module. Backup BorgBackup - A deduplicating archiver with compression and encryption. Others docker-compose - Fast, isolated development environments using Docker. Distributed Computing Frameworks and libraries for Distributed Computing. Batch Processing dask - A flexible parallel computing library for analytic computing. luigi - A module that helps you build complex pipelines of batch jobs. mrjob - Run MapReduce jobs on Hadoop or Amazon Web Services. PySpark - Apache Spark Python API. Ray - A system for parallel and distributed Python that unifies the machine learning ecosystem. Stream Processing faust - A stream processing library, porting the ideas from Kafka Streams to Python. streamparse - Run Python code against real-time streams of data via Apache Storm. Distribution Libraries to create packaged executables for release distribution. dh-virtualenv - Build and distribute a virtualenv as a Debian package. Nuitka - Compile scripts, modules, packages to an executable or extension module. py2app - Freezes Python scripts (Mac OS X). py2exe - Freezes Python scripts (Windows). pyarmor - A tool used to obfuscate python scripts, bind obfuscated scripts to fixed machine or expire obfuscated scripts. PyInstaller - Converts Python programs into stand-alone executables (cross-platform). pynsist - A tool to build Windows installers, installers bundle Python itself. shiv - A command line utility for building fully self-contained zipapps (PEP 441), but with all their dependencies included. Documentation Libraries for generating project documentation. sphinx - Python Documentation generator. awesome-sphinxdoc pdoc - Epydoc replacement to auto generate API documentation for Python libraries. pycco - The literate-programming-style documentation generator. Downloader Libraries for downloading. akshare - A financial data interface library, built for human beings! s3cmd - A command line tool for managing Amazon S3 and CloudFront. s4cmd - Super S3 command line tool, good for higher performance. you-get - A YouTube/Youku/Niconico video downloader written in Python 3. youtube-dl - A small command-line program to download videos from YouTube. E-commerce Frameworks and libraries for e-commerce and payments. alipay - Unofficial Alipay API for Python. Cartridge - A shopping cart app built using the Mezzanine. django-oscar - An open-source e-commerce framework for Django. django-shop - A Django based shop system. forex-python - Foreign exchange rates, Bitcoin price index and currency conversion. merchant - A Django app to accept payments from various payment processors. money - Money class with optional CLDR-backed locale-aware formatting and an extensible currency exchange. python-currencies - Display money format and its filthy currencies. saleor - An e-commerce storefront for Django. shoop - An open source E-Commerce platform based on Django. Editor Plugins and IDEs Emacs elpy - Emacs Python Development Environment. Sublime Text anaconda - Anaconda turns your Sublime Text 3 in a full featured Python development IDE. SublimeJEDI - A Sublime Text plugin to the awesome auto-complete library Jedi. Vim jedi-vim - Vim bindings for the Jedi auto-completion library for Python. python-mode - An all in one plugin for turning Vim into a Python IDE. YouCompleteMe - Includes Jedi-based completion engine for Python. Visual Studio PTVS - Python Tools for Visual Studio. Visual Studio Code Python - The official VSCode extension with rich support for Python. IDE PyCharm - Commercial Python IDE by JetBrains. Has free community edition available. spyder - Open Source Python IDE. Email Libraries for sending and parsing email. Mail Servers modoboa - A mail hosting and management platform including a modern Web UI. salmon - A Python Mail Server. Clients imbox - Python IMAP for Humans. yagmail - Yet another Gmail/SMTP client. Others flanker - An email address and Mime parsing library. mailer - High-performance extensible mail delivery framework. Enterprise Application Integrations Platforms and tools for systems integrations in enterprise environments Zato - ESB, SOA, REST, APIs and Cloud Integrations in Python. Environment Management Libraries for Python version and virtual environment management. pyenv - Simple Python version management. virtualenv - A tool to create isolated Python environments. Files Libraries for file manipulation and MIME type detection. mimetypes - (Python standard library) Map filenames to MIME types. path.py - A module wrapper for os.path. pathlib - (Python standard library) An cross-platform, object-oriented path library. PyFilesystem2 - Python's filesystem abstraction layer. python-magic - A Python interface to the libmagic file type identification library. Unipath - An object-oriented approach to file/directory operations. watchdog - API and shell utilities to monitor file system events. Foreign Function Interface Libraries for providing foreign function interface. cffi - Foreign Function Interface for Python calling C code. ctypes - (Python standard library) Foreign Function Interface for Python calling C code. PyCUDA - A Python wrapper for Nvidia's CUDA API. SWIG - Simplified Wrapper and Interface Generator. Forms Libraries for working with forms. Deform - Python HTML form generation library influenced by the formish form generation library. django-bootstrap3 - Bootstrap 3 integration with Django. django-bootstrap4 - Bootstrap 4 integration with Django. django-crispy-forms - A Django app which lets you create beautiful forms in a very elegant and DRY way. django-remote-forms - A platform independent Django form serializer. WTForms - A flexible forms validation and rendering library. Functional Programming Functional Programming with Python. Coconut - A variant of Python built for simple, elegant, Pythonic functional programming. CyToolz - Cython implementation of Toolz: High performance functional utilities. fn.py - Functional programming in Python: implementation of missing features to enjoy FP. funcy - A fancy and practical functional tools. more-itertools - More routines for operating on iterables, beyond itertools. returns - A set of type-safe monads, transformers, and composition utilities. Toolz - A collection of functional utilities for iterators, functions, and dictionaries. GUI Development Libraries for working with graphical user interface applications. curses - Built-in wrapper for ncurses used to create terminal GUI applications. Eel - A library for making simple Electron-like offline HTML/JS GUI apps. enaml - Creating beautiful user-interfaces with Declarative Syntax like QML. Flexx - Flexx is a pure Python toolkit for creating GUI's, that uses web technology for its rendering. Gooey - Turn command line programs into a full GUI application with one line. kivy - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS. pyglet - A cross-platform windowing and multimedia library for Python. PyGObject - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3). PyQt - Python bindings for the Qt cross-platform application and UI framework. PySimpleGUI - Wrapper for tkinter, Qt, WxPython and Remi. pywebview - A lightweight cross-platform native wrapper around a webview component. Tkinter - Tkinter is Python's de-facto standard GUI package. Toga - A Python native, OS native GUI toolkit. urwid - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc. wxPython - A blending of the wxWidgets C++ class library with the Python. DearPyGui - A Simple GPU accelerated Python GUI framework GraphQL Libraries for working with GraphQL. graphene - GraphQL framework for Python. tartiflette-aiohttp - An aiohttp-based wrapper for Tartiflette to expose GraphQL APIs over HTTP. tartiflette-asgi - ASGI support for the Tartiflette GraphQL engine. tartiflette - SDL-first GraphQL engine implementation for Python 3.6+ and asyncio. Game Development Awesome game development libraries. Arcade - Arcade is a modern Python framework for crafting games with compelling graphics and sound. Cocos2d - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications. Harfang3D - Python framework for 3D, VR and game development. Panda3D - 3D game engine developed by Disney. Pygame - Pygame is a set of Python modules designed for writing games. PyOgre - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D. PyOpenGL - Python ctypes bindings for OpenGL and it's related APIs. PySDL2 - A ctypes based wrapper for the SDL2 library. RenPy - A Visual Novel engine. Geolocation Libraries for geocoding addresses and working with latitudes and longitudes. django-countries - A Django app that provides a country field for models and forms. GeoDjango - A world-class geographic web framework. GeoIP - Python API for MaxMind GeoIP Legacy Database. geojson - Python bindings and utilities for GeoJSON. geopy - Python Geocoding Toolbox. HTML Manipulation Libraries for working with HTML and XML. BeautifulSoup - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML. bleach - A whitelist-based HTML sanitization and text linkification library. cssutils - A CSS library for Python. html5lib - A standards-compliant library for parsing and serializing HTML documents and fragments. lxml - A very fast, easy-to-use and versatile library for handling HTML and XML. MarkupSafe - Implements a XML/HTML/XHTML Markup safe string for Python. pyquery - A jQuery-like library for parsing HTML. untangle - Converts XML documents to Python objects for easy access. WeasyPrint - A visual rendering engine for HTML and CSS that can export to PDF. xmldataset - Simple XML Parsing. xmltodict - Working with XML feel like you are working with JSON. HTTP Clients Libraries for working with HTTP. grequests - requests + gevent for asynchronous HTTP requests. httplib2 - Comprehensive HTTP client library. httpx - A next generation HTTP client for Python. requests - HTTP Requests for Humans. treq - Python requests like API built on top of Twisted's HTTP client. urllib3 - A HTTP library with thread-safe connection pooling, file post support, sanity friendly. Hardware Libraries for programming with hardware. ino - Command line toolkit for working with Arduino. keyboard - Hook and simulate global keyboard events on Windows and Linux. mouse - Hook and simulate global mouse events on Windows and Linux. Pingo - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc. PyUserInput - A module for cross-platform control of the mouse and keyboard. scapy - A brilliant packet manipulation library. Image Processing Libraries for manipulating images. hmap - Image histogram remapping. imgSeek - A project for searching a collection of images using visual similarity. nude.py - Nudity detection. pagan - Retro identicon (Avatar) generation based on input string and hash. pillow - Pillow is the friendly PIL fork. python-barcode - Create barcodes in Python with no extra dependencies. pygram - Instagram-like image filters. PyMatting - A library for alpha matting. python-qrcode - A pure Python QR Code generator. pywal - A tool that generates color schemes from images. pyvips - A fast image processing library with low memory needs. Quads - Computer art based on quadtrees. scikit-image - A Python library for (scientific) image processing. thumbor - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images. wand - Python bindings for MagickWand, C API for ImageMagick. Implementations Implementations of Python. CLPython - Implementation of the Python programming language written in Common Lisp. CPython - Default, most widely used implementation of the Python programming language written in C. Cython - Optimizing Static Compiler for Python. Grumpy - More compiler than interpreter as more powerful CPython2.7 replacement (alpha). IronPython - Implementation of the Python programming language written in C#. Jython - Implementation of Python programming language written in Java for the JVM. MicroPython - A lean and efficient Python programming language implementation. Numba - Python JIT compiler to LLVM aimed at scientific Python. PeachPy - x86-64 assembler embedded in Python. Pyjion - A JIT for Python based upon CoreCLR. PyPy - A very fast and compliant implementation of the Python language. Pyston - A Python implementation using JIT techniques. Stackless Python - An enhanced version of the Python programming language. Interactive Interpreter Interactive Python interpreters (REPL). bpython - A fancy interface to the Python interpreter. Jupyter Notebook (IPython) - A rich toolkit to help you make the most out of using Python interactively. awesome-jupyter ptpython - Advanced Python REPL built on top of the python-prompt-toolkit. Internationalization Libraries for working with i18n. Babel - An internationalization library for Python. PyICU - A wrapper of International Components for Unicode C++ library (ICU). Job Scheduler Libraries for scheduling jobs. Airflow - Airflow is a platform to programmatically author, schedule and monitor workflows. APScheduler - A light but powerful in-process task scheduler that lets you schedule functions. django-schedule - A calendaring app for Django. doit - A task runner and build tool. gunnery - Multipurpose task execution tool for distributed systems with web-based interface. Joblib - A set of tools to provide lightweight pipelining in Python. Plan - Writing crontab file in Python like a charm. Prefect - A modern workflow orchestration framework that makes it easy to build, schedule and monitor robust data pipelines. schedule - Python job scheduling for humans. Spiff - A powerful workflow engine implemented in pure Python. TaskFlow - A Python library that helps to make task execution easy, consistent and reliable. Logging Libraries for generating and working with logs. logbook - Logging replacement for Python. logging - (Python standard library) Logging facility for Python. loguru - Library which aims to bring enjoyable logging in Python. sentry-python - Sentry SDK for Python. structlog - Structured logging made easy. Machine Learning Libraries for Machine Learning. Also see awesome-machine-learning. gym - A toolkit for developing and comparing reinforcement learning algorithms. H2O - Open Source Fast Scalable Machine Learning Platform. Metrics - Machine learning evaluation metrics. NuPIC - Numenta Platform for Intelligent Computing. scikit-learn - The most popular Python library for Machine Learning. Spark ML - Apache Spark's scalable Machine Learning library. vowpal_porpoise - A lightweight Python wrapper for Vowpal Wabbit. xgboost - A scalable, portable, and distributed gradient boosting library. MindsDB - MindsDB is an open source AI layer for existing databases that allows you to effortlessly develop, train and deploy state-of-the-art machine learning models using standard queries. Microsoft Windows Python programming on Microsoft Windows. Python(x,y) - Scientific-applications-oriented Python Distribution based on Qt and Spyder. pythonlibs - Unofficial Windows binaries for Python extension packages. PythonNet - Python Integration with the .NET Common Language Runtime (CLR). PyWin32 - Python Extensions for Windows. WinPython - Portable development environment for Windows 7/8. Miscellaneous Useful libraries or tools that don't fit in the categories above. blinker - A fast Python in-process signal/event dispatching system. boltons - A set of pure-Python utilities. itsdangerous - Various helpers to pass trusted data to untrusted environments. magenta - A tool to generate music and art using artificial intelligence. pluginbase - A simple but flexible plugin system for Python. tryton - A general purpose business framework. Natural Language Processing Libraries for working with human languages. General gensim - Topic Modeling for Humans. langid.py - Stand-alone language identification system. nltk - A leading platform for building Python programs to work with human language data. pattern - A web mining module. polyglot - Natural language pipeline supporting hundreds of languages. pytext - A natural language modeling framework based on PyTorch. PyTorch-NLP - A toolkit enabling rapid deep learning NLP prototyping for research. spacy - A library for industrial-strength natural language processing in Python and Cython. Stanza - The Stanford NLP Group's official Python library, supporting 60+ languages. Chinese funNLP - A collection of tools and datasets for Chinese NLP. jieba - The most popular Chinese text segmentation library. pkuseg-python - A toolkit for Chinese word segmentation in various domains. snownlp - A library for processing Chinese text. Network Virtualization Tools and libraries for Virtual Networking and SDN (Software Defined Networking). mininet - A popular network emulator and API written in Python. napalm - Cross-vendor API to manipulate network devices. pox - A Python-based SDN control applications, such as OpenFlow SDN controllers. News Feed Libraries for building user's activities. django-activity-stream - Generating generic activity streams from the actions on your site. Stream Framework - Building news feed and notification systems using Cassandra and Redis. ORM Libraries that implement Object-Relational Mapping or data mapping techniques. Relational Databases Django Models - The Django ORM. SQLAlchemy - The Python SQL Toolkit and Object Relational Mapper. awesome-sqlalchemy dataset - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL. orator - The Orator ORM provides a simple yet beautiful ActiveRecord implementation. orm - An async ORM. peewee - A small, expressive ORM. pony - ORM that provides a generator-oriented interface to SQL. pydal - A pure Python Database Abstraction Layer. NoSQL Databases hot-redis - Rich Python data types for Redis. mongoengine - A Python Object-Document-Mapper for working with MongoDB. PynamoDB - A Pythonic interface for Amazon DynamoDB. redisco - A Python Library for Simple Models and Containers Persisted in Redis. Package Management Libraries for package and dependency management. pip - The package installer for Python. pip-tools - A set of tools to keep your pinned Python dependencies fresh. PyPI conda - Cross-platform, Python-agnostic binary package manager. poetry - Python dependency management and packaging made easy. Package Repositories Local PyPI repository server and proxies. bandersnatch - PyPI mirroring tool provided by Python Packaging Authority (PyPA). devpi - PyPI server and packaging/testing/release tool. localshop - Local PyPI server (custom packages and auto-mirroring of pypi). warehouse - Next generation Python Package Repository (PyPI). Penetration Testing Frameworks and tools for penetration testing. fsociety - A Penetration testing framework. setoolkit - A toolkit for social engineering. sqlmap - Automatic SQL injection and database takeover tool. Permissions Libraries that allow or deny users access to data or functionality. django-guardian - Implementation of per object permissions for Django 1.2+ django-rules - A tiny but powerful app providing object-level permissions to Django, without requiring a database. Processes Libraries for starting and communicating with OS processes. delegator.py - Subprocesses for Humans 2.0. sarge - Yet another wrapper for subprocess. sh - A full-fledged subprocess replacement for Python. Recommender Systems Libraries for building recommender systems. annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage. fastFM - A library for Factorization Machines. implicit - A fast Python implementation of collaborative filtering for implicit datasets. libffm - A library for Field-aware Factorization Machine (FFM). lightfm - A Python implementation of a number of popular recommendation algorithms. spotlight - Deep recommender models using PyTorch. Surprise - A scikit for building and analyzing recommender systems. tensorrec - A Recommendation Engine Framework in TensorFlow. Refactoring Refactoring tools and libraries for Python Bicycle Repair Man - Bicycle Repair Man, a refactoring tool for Python. Bowler - Safe code refactoring for modern Python. Rope - Rope is a python refactoring library. RESTful API Libraries for building RESTful APIs. Django django-rest-framework - A powerful and flexible toolkit to build web APIs. django-tastypie - Creating delicious APIs for Django apps. Flask eve - REST API framework powered by Flask, MongoDB and good intentions. flask-api - Browsable Web APIs for Flask. flask-restful - Quickly building REST APIs for Flask. Pyramid cornice - A RESTful framework for Pyramid. Framework agnostic apistar - A smart Web API framework, designed for Python 3. falcon - A high-performance framework for building cloud APIs and web app backends. fastapi - A modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints. hug - A Python 3 framework for cleanly exposing APIs. sandman2 - Automated REST APIs for existing database-driven systems. sanic - A Python 3.6+ web server and web framework that's written to go fast. vibora - Fast, efficient and asynchronous Web framework inspired by Flask. Robotics Libraries for robotics. PythonRobotics - This is a compilation of various robotics algorithms with visualizations. rospy - This is a library for ROS (Robot Operating System). RPC Servers RPC-compatible servers. RPyC (Remote Python Call) - A transparent and symmetric RPC library for Python zeroRPC - zerorpc is a flexible RPC implementation based on ZeroMQ and MessagePack. Science Libraries for scientific computing. Also see Python-for-Scientists. astropy - A community Python library for Astronomy. bcbio-nextgen - Providing best-practice pipelines for fully automated high throughput sequencing analysis. bccb - Collection of useful code related to biological analysis. Biopython - Biopython is a set of freely available tools for biological computation. cclib - A library for parsing and interpreting the results of computational chemistry packages. Colour - Implementing a comprehensive number of colour theory transformations and algorithms. Karate Club - Unsupervised machine learning toolbox for graph structured data. NetworkX - A high-productivity software for complex networks. NIPY - A collection of neuroimaging toolkits. NumPy - A fundamental package for scientific computing with Python. ObsPy - A Python toolbox for seismology. Open Babel - A chemical toolbox designed to speak the many languages of chemical data. PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion. PyMC - Markov Chain Monte Carlo sampling toolkit. QuTiP - Quantum Toolbox in Python. RDKit - Cheminformatics and Machine Learning Software. SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering. SimPy - A process-based discrete-event simulation framework. statsmodels - Statistical modeling and econometrics in Python. SymPy - A Python library for symbolic mathematics. Zipline - A Pythonic algorithmic trading library. Search Libraries and software for indexing and performing search queries on data. django-haystack - Modular search for Django. elasticsearch-dsl-py - The official high-level Python client for Elasticsearch. elasticsearch-py - The official low-level Python client for Elasticsearch. pysolr - A lightweight Python wrapper for Apache Solr. whoosh - A fast, pure Python search engine library. Serialization Libraries for serializing complex data types marshmallow - A lightweight library for converting complex objects to and from simple Python datatypes. pysimdjson - A Python bindings for simdjson. python-rapidjson - A Python wrapper around RapidJSON. ultrajson - A fast JSON decoder and encoder written in C with Python bindings. Serverless Frameworks Frameworks for developing serverless Python code. python-lambda - A toolkit for developing and deploying Python code in AWS Lambda. Zappa - A tool for deploying WSGI applications on AWS Lambda and API Gateway. Shell Shells based on Python. xonsh - A Python-powered, cross-platform, Unix-gazing shell language and command prompt. Specific Formats Processing Libraries for parsing and manipulating specific text formats. General tablib - A module for Tabular Datasets in XLS, CSV, JSON, YAML. Office docxtpl - Editing a docx document by jinja2 template openpyxl - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files. pyexcel - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files. python-docx - Reads, queries and modifies Microsoft Word 2007/2008 docx files. python-pptx - Python library for creating and updating PowerPoint (.pptx) files. unoconv - Convert between any document format supported by LibreOffice/OpenOffice. XlsxWriter - A Python module for creating Excel .xlsx files. xlwings - A BSD-licensed library that makes it easy to call Python from Excel and vice versa. xlwt / xlrd - Writing and reading data and formatting information from Excel files. PDF PDFMiner - A tool for extracting information from PDF documents. PyPDF2 - A library capable of splitting, merging and transforming PDF pages. ReportLab - Allowing Rapid creation of rich PDF documents. Markdown Mistune - Fastest and full featured pure Python parsers of Markdown. Python-Markdown - A Python implementation of John Grubers Markdown. YAML PyYAML - YAML implementations for Python. CSV csvkit - Utilities for converting to and working with CSV. Archive unp - A command line tool that can unpack archives easily. Static Site Generator Static site generator is a software that takes some text + templates as input and produces HTML files on the output. lektor - An easy to use static CMS and blog engine. mkdocs - Markdown friendly documentation generator. makesite - Simple, lightweight, and magic-free static site/blog generator (< 130 lines). nikola - A static website and blog generator. pelican - Static site generator that supports Markdown and reST syntax. Tagging Libraries for tagging items. django-taggit - Simple tagging for Django. Task Queues Libraries for working with task queues. celery - An asynchronous task queue/job queue based on distributed message passing. dramatiq - A fast and reliable background task processing library for Python 3. huey - Little multi-threaded task queue. mrq - A distributed worker task queue in Python using Redis & gevent. rq - Simple job queues for Python. Template Engine Libraries and tools for templating and lexing. Genshi - Python templating toolkit for generation of web-aware output. Jinja2 - A modern and designer friendly templating language. Mako - Hyperfast and lightweight templating for the Python platform. Testing Libraries for testing codebases and generating test data. Testing Frameworks hypothesis - Hypothesis is an advanced Quickcheck style property based testing library. nose2 - The successor to nose, based on `unittest2. pytest - A mature full-featured Python testing tool. Robot Framework - A generic test automation framework. unittest - (Python standard library) Unit testing framework. Test Runners green - A clean, colorful test runner. mamba - The definitive testing tool for Python. Born under the banner of BDD. tox - Auto builds and tests distributions in multiple Python versions GUI / Web Testing locust - Scalable user load testing tool written in Python. PyAutoGUI - PyAutoGUI is a cross-platform GUI automation Python module for human beings. Schemathesis - A tool for automatic property-based testing of web applications built with Open API / Swagger specifications. Selenium - Python bindings for Selenium WebDriver. sixpack - A language-agnostic A/B Testing framework. splinter - Open source tool for testing web applications. Mock doublex - Powerful test doubles framework for Python. freezegun - Travel through time by mocking the datetime module. httmock - A mocking library for requests for Python 2.6+ and 3.2+. httpretty - HTTP request mock tool for Python. mock - (Python standard library) A mocking and patching library. mocket - A socket mock framework with gevent/asyncio/SSL support. responses - A utility library for mocking out the requests Python library. VCR.py - Record and replay HTTP interactions on your tests. Object Factories factory_boy - A test fixtures replacement for Python. mixer - Another fixtures replacement. Supports Django, Flask, SQLAlchemy, Peewee and etc. model_mommy - Creating random fixtures for testing in Django. Code Coverage coverage - Code coverage measurement. Fake Data fake2db - Fake database generator. faker - A Python package that generates fake data. mimesis - is a Python library that help you generate fake data. radar - Generate random datetime / time. Text Processing Libraries for parsing and manipulating plain texts. General chardet - Python 2/3 compatible character encoding detector. difflib - (Python standard library) Helpers for computing deltas. ftfy - Makes Unicode text less broken and more consistent automagically. fuzzywuzzy - Fuzzy String Matching. Levenshtein - Fast computation of Levenshtein distance and string similarity. pangu.py - Paranoid text spacing. pyfiglet - An implementation of figlet written in Python. pypinyin - Convert Chinese hanzi () to pinyin (). textdistance - Compute distance between sequences with 30+ algorithms. unidecode - ASCII transliterations of Unicode text. Slugify awesome-slugify - A Python slugify library that can preserve unicode. python-slugify - A Python slugify library that translates unicode to ASCII. unicode-slugify - A slugifier that generates unicode slugs with Django as a dependency. Unique identifiers hashids - Implementation of hashids in Python. shortuuid - A generator library for concise, unambiguous and URL-safe UUIDs. Parser ply - Implementation of lex and yacc parsing tools for Python. pygments - A generic syntax highlighter. pyparsing - A general purpose framework for generating parsers. python-nameparser - Parsing human names into their individual components. python-phonenumbers - Parsing, formatting, storing and validating international phone numbers. python-user-agents - Browser user agent parser. sqlparse - A non-validating SQL parser. Third-party APIs Libraries for accessing third party services APIs. Also see List of Python API Wrappers and Libraries. apache-libcloud - One Python library for all clouds. boto3 - Python interface to Amazon Web Services. django-wordpress - WordPress models and views for Django. facebook-sdk - Facebook Platform Python SDK. google-api-python-client - Google APIs Client Library for Python. gspread - Google Spreadsheets Python API. twython - A Python wrapper for the Twitter API. URL Manipulation Libraries for parsing URLs. furl - A small Python library that makes parsing and manipulating URLs easy. purl - A simple, immutable URL class with a clean API for interrogation and manipulation. pyshorteners - A pure Python URL shortening lib. webargs - A friendly library for parsing HTTP request arguments with built-in support for popular web frameworks. Video Libraries for manipulating video and GIFs. moviepy - A module for script-based movie editing with many formats, including animated GIFs. scikit-video - Video processing routines for SciPy. vidgear - Most Powerful multi-threaded Video Processing framework. Web Asset Management Tools for managing, compressing and minifying website assets. django-compressor - Compresses linked and inline JavaScript or CSS into a single cached file. django-pipeline - An asset packaging library for Django. django-storages - A collection of custom storage back ends for Django. fanstatic - Packages, optimizes, and serves static file dependencies as Python packages. fileconveyor - A daemon to detect and sync files to CDNs, S3 and FTP. flask-assets - Helps you integrate webassets into your Flask app. webassets - Bundles, optimizes, and manages unique cache-busting URLs for static resources. Web Content Extracting Libraries for extracting web contents. html2text - Convert HTML to Markdown-formatted text. lassie - Web Content Retrieval for Humans. micawber - A small library for extracting rich content from URLs. newspaper - News extraction, article extraction and content curation in Python. python-readability - Fast Python port of arc90's readability tool. requests-html - Pythonic HTML Parsing for Humans. sumy - A module for automatic summarization of text documents and HTML pages. textract - Extract text from any document, Word, PowerPoint, PDFs, etc. toapi - Every web site provides APIs. Web Crawling Libraries to automate web scraping. cola - A distributed crawling framework. feedparser - Universal feed parser. grab - Site scraping framework. MechanicalSoup - A Python library for automating interaction with websites. portia - Visual scraping for Scrapy. pyspider - A powerful spider system. robobrowser - A simple, Pythonic library for browsing the web without a standalone web browser. scrapy - A fast high-level screen scraping and web crawling framework. Web Frameworks Traditional full stack web frameworks. Also see RESTful API. Synchronous Django - The most popular web framework in Python. awesome-django awesome-django Flask - A microframework for Python. awesome-flask Pyramid - A small, fast, down-to-earth, open source Python web framework. awesome-pyramid Masonite - The modern and developer centric Python web framework. Asynchronous Tornado - A web framework and asynchronous networking library. WebSocket Libraries for working with WebSocket. autobahn-python - WebSocket & WAMP for Python on Twisted and asyncio. channels - Developer-friendly asynchrony for Django. websockets - A library for building WebSocket servers and clients with a focus on correctness and simplicity. WSGI Servers WSGI-compatible web servers. bjoern - Asynchronous, very fast and written in C. gunicorn - Pre-forked, ported from Ruby's Unicorn project. uWSGI - A project aims at developing a full stack for building hosting services, written in C. waitress - Multi-threaded, powers Pyramid. werkzeug - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects. Resources Where to discover learning resources or new Python libraries. Books Fluent Python Think Python Websites Tutorials Full Stack Python Python Cheatsheet Real Python The Hitchhikers Guide to Python Ultimate Python study guide Libraries Awesome Python @LibHunt Others Python ZEEF Pythonic News What the f*ck Python! Newsletters Awesome Python Newsletter Pycoder's Weekly Python Tricks Python Weekly Podcasts Django Chat Podcast.__init__ Python Bytes Running in Production Talk Python To Me Test and Code The Real Python Podcast Contributing Your contributions are always welcome! Please take a look at the contribution guidelines first. I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could vote for them by adding to them. Pull requests will be merged when their votes reach 20. If you have any question about this opinionated list, do not hesitate to contact me @VintaChen on Twitter or open an issue on GitHub. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/vinta/awesome-python",
        "comments.comment_id": [19186522, 19186888],
        "comments.comment_author": ["danpalmer", "js2"],
        "comments.comment_descendants": [3, 1],
        "comments.comment_time": [
          "2019-02-17T20:06:54Z",
          "2019-02-17T21:02:01Z"
        ],
        "comments.comment_text": [
          "There's lots of great stuff in the Python community, lots of very mature, high quality packages.<p>As with all ecosystems, there's also rubbish, and there's certainly some rubbish on this list. It would be wrong to name packages, but it makes me question \"Awesome X\" lists, their intentions, the skill in curation behind them, and their usefulness to newcomers.<p>I would personally not use inclusion on an \"Awesome X\" list as a signal of quality. The lists have assumed a purely discovery based role for me, which is a shame, because the idea of a curated set of packages or tools for newcomers to an ecosystem is a great one.",
          "I'm not sure I'd call this list curated. It would be nice if it were a bit more opinionated. Where there are multiple libraries in a category it provides no guidance on how to choose one over another. Also, in some of the categories, clear winners exist today. For example in the testing category, just use pytest, and run it with tox.<p>I was going to submit a PR, but the repo has hundreds of PRs and 60 issues already open.<p>So yeah, it's more a Python smörgåsbord than a curated list. In the end, I'm not sure it's much better than using a search engine."
        ],
        "id": "5ca8e62a-d18e-4bf5-bb29-b58d32fbe824",
        "url_text": "Awesome Python A curated list of awesome Python frameworks, libraries, software and resources. Inspired by awesome-php. Awesome Python Admin Panels Algorithms and Design Patterns ASGI Servers Asynchronous Programming Audio Authentication Build Tools Built-in Classes Enhancement Caching ChatOps Tools CMS Code Analysis Command-line Interface Development Command-line Tools Compatibility Computer Vision Concurrency and Parallelism Configuration Cryptography Data Analysis Data Validation Data Visualization Database Drivers Database Date and Time Debugging Tools Deep Learning DevOps Tools Distributed Computing Distribution Documentation Downloader E-commerce Editor Plugins and IDEs Email Enterprise Application Integrations Environment Management Files Foreign Function Interface Forms Functional Programming Game Development Geolocation GUI Development Hardware HTML Manipulation HTTP Clients Image Processing Implementations Interactive Interpreter Internationalization Job Scheduler Logging Machine Learning Miscellaneous Natural Language Processing Network Virtualization News Feed ORM Package Management Package Repositories Penetration testing Permissions Processes Recommender Systems Refactoring RESTful API Robotics RPC Servers Science Search Serialization Serverless Frameworks Shell Specific Formats Processing Static Site Generator Tagging Task Queues Template Engine Testing Text Processing Third-party APIs URL Manipulation Video Web Asset Management Web Content Extracting Web Crawling Web Frameworks WebSocket WSGI Servers Resources Books Newsletters Podcasts Websites Contributing Admin Panels Libraries for administrative interfaces. ajenti - The admin panel your servers deserve. django-grappelli - A jazzy skin for the Django Admin-Interface. django-jet - Modern responsive template for the Django admin interface with improved functionality. django-suit - Alternative Django Admin-Interface (free only for Non-commercial use). django-xadmin - Drop-in replacement of Django admin comes with lots of goodies. flask-admin - Simple and extensible administrative interface framework for Flask. flower - Real-time monitor and web admin for Celery. jet-bridge - Admin panel framework for any application with nice UI (ex Jet Django). wooey - A Django app which creates automatic web UIs for Python scripts. Algorithms and Design Patterns Python implementation of data structures, algorithms and design patterns. Also see awesome-algorithms. Algorithms algorithms - Minimal examples of data structures and algorithms. python-ds - A collection of data structure and algorithms for coding interviews. sortedcontainers - Fast and pure-Python implementation of sorted collections. TheAlgorithms - All Algorithms implemented in Python. Design Patterns PyPattyrn - A simple yet effective library for implementing common design patterns. python-patterns - A collection of design patterns in Python. transitions - A lightweight, object-oriented finite state machine implementation. ASGI Servers ASGI-compatible web servers. daphne - A HTTP, HTTP2 and WebSocket protocol server for ASGI and ASGI-HTTP. uvicorn - A lightning-fast ASGI server implementation, using uvloop and httptools. Asynchronous Programming asyncio - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks. awesome-asyncio trio - A friendly library for async concurrency and I/O. Twisted - An event-driven networking engine. uvloop - Ultra fast asyncio event loop. Audio Libraries for manipulating audio and its metadata. Audio audioread - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding. dejavu - Audio fingerprinting and recognition. kapre - Keras Audio Preprocessors. librosa - Python library for audio and music analysis. matchering - A library for automated reference audio mastering. mingus - An advanced music theory and notation package with MIDI file and playback support. pyAudioAnalysis - Audio feature extraction, classification, segmentation and applications. pydub - Manipulate audio with a simple and easy high level interface. TimeSide - Open web audio processing framework. Metadata beets - A music library manager and MusicBrainz tagger. eyeD3 - A tool for working with audio files, specifically MP3 files containing ID3 metadata. mutagen - A Python module to handle audio metadata. tinytag - A library for reading music meta data of MP3, OGG, FLAC and Wave files. Authentication Libraries for implementing authentications schemes. OAuth authlib - JavaScript Object Signing and Encryption draft implementation. django-allauth - Authentication app for Django that \"just works.\" django-oauth-toolkit - OAuth 2 goodies for Django. oauthlib - A generic and thorough implementation of the OAuth request-signing logic. python-oauth2 - A fully tested, abstract interface to creating OAuth clients and servers. python-social-auth - An easy-to-setup social authentication mechanism. JWT pyjwt - JSON Web Token implementation in Python. python-jose - A JOSE implementation in Python. python-jwt - A module for generating and verifying JSON Web Tokens. Build Tools Compile software from source code. BitBake - A make-like build tool for embedded Linux. buildout - A build system for creating, assembling and deploying applications from multiple parts. PlatformIO - A console tool to build code with different development platforms. pybuilder - A continuous build tool written in pure Python. SCons - A software construction tool. Built-in Classes Enhancement Libraries for enhancing Python built-in classes. attrs - Replacement for __init__, __eq__, __repr__, etc. boilerplate in class definitions. bidict - Efficient, Pythonic bidirectional map data structures and related functionality.. Box - Python dictionaries with advanced dot notation access. dataclasses - (Python standard library) Data classes. DottedDict - A library that provides a method of accessing lists and dicts with a dotted path notation. CMS Content Management Systems. django-cms - An Open source enterprise CMS based on the Django. feincms - One of the most advanced Content Management Systems built on Django. indico - A feature-rich event management system, made @ CERN. Kotti - A high-level, Pythonic web application framework built on Pyramid. mezzanine - A powerful, consistent, and flexible content management platform. plone - A CMS built on top of the open source application server Zope. quokka - Flexible, extensible, small CMS powered by Flask and MongoDB. wagtail - A Django content management system. Caching Libraries for caching data. beaker - A WSGI middleware for sessions and caching. django-cache-machine - Automatic caching and invalidation for Django models. django-cacheops - A slick ORM cache with automatic granular event-driven invalidation. dogpile.cache - dogpile.cache is next generation replacement for Beaker made by same authors. HermesCache - Python caching library with tag-based invalidation and dogpile effect prevention. pylibmc - A Python wrapper around the libmemcached interface. python-diskcache - SQLite and file backed cache backend with faster lookups than memcached and redis. ChatOps Tools Libraries for chatbot development. errbot - The easiest and most popular chatbot to implement ChatOps. Code Analysis Tools of static analysis, linters and code quality checkers. Also see awesome-static-analysis. Code Analysis coala - Language independent and easily extendable code analysis application. code2flow - Turn your Python and JavaScript code into DOT flowcharts. prospector - A tool to analyse Python code. pycallgraph - A library that visualises the flow (call graph) of your Python application. vulture - A tool for finding and analysing dead Python code. Code Linters flake8 - A wrapper around pycodestyle, pyflakes and McCabe. awesome-flake8-extensions pylama - A code audit tool for Python and JavaScript. pylint - A fully customizable source code analyzer. wemake-python-styleguide - The strictest and most opinionated python linter ever. Code Formatters black - The uncompromising Python code formatter. isort - A Python utility / library to sort imports. yapf - Yet another Python code formatter from Google. Static Type Checkers, also see awesome-python-typing mypy - Check variable types during compile time. pyre-check - Performant type checking. typeshed - Collection of library stubs for Python, with static types. Static Type Annotations Generators MonkeyType - A system for Python that generates static type annotations by collecting runtime types. pyannotate - Auto-generate PEP-484 annotations. pytype - Pytype checks and infers types for Python code - without requiring type annotations. Command-line Interface Development Libraries for building command-line applications. Command-line Application Development cement - CLI Application Framework for Python. click - A package for creating beautiful command line interfaces in a composable way. cliff - A framework for creating command-line programs with multi-level commands. docopt - Pythonic command line arguments parser. python-fire - A library for creating command line interfaces from absolutely any Python object. python-prompt-toolkit - A library for building powerful interactive command lines. Terminal Rendering alive-progress - A new kind of Progress Bar, with real-time throughput, eta and very cool animations. asciimatics - A package to create full-screen text UIs (from interactive forms to ASCII animations). bashplotlib - Making basic plots in the terminal. colorama - Cross-platform colored terminal text. rich - Python library for rich text and beautiful formatting in the terminal. Also provides a great RichHandler log handler. tqdm - Fast, extensible progress bar for loops and CLI. Command-line Tools Useful CLI-based tools for productivity. Productivity Tools copier - A library and command-line utility for rendering projects templates. cookiecutter - A command-line utility that creates projects from cookiecutters (project templates). doitlive - A tool for live presentations in the terminal. howdoi - Instant coding answers via the command line. Invoke - A tool for managing shell-oriented subprocesses and organizing executable Python code into CLI-invokable tasks. PathPicker - Select files out of bash output. percol - Adds flavor of interactive selection to the traditional pipe concept on UNIX. thefuck - Correcting your previous console command. tmuxp - A tmux session manager. try - A dead simple CLI to try out python packages - it's never been easier. CLI Enhancements httpie - A command line HTTP client, a user-friendly cURL replacement. iredis - Redis CLI with autocompletion and syntax highlighting. kube-shell - An integrated shell for working with the Kubernetes CLI. litecli - SQLite CLI with autocompletion and syntax highlighting. mycli - MySQL CLI with autocompletion and syntax highlighting. pgcli - PostgreSQL CLI with autocompletion and syntax highlighting. saws - A Supercharged aws-cli. Compatibility Libraries for migrating from Python 2 to 3. python-future - The missing compatibility layer between Python 2 and Python 3. modernize - Modernizes Python code for eventual Python 3 migration. six - Python 2 and 3 compatibility utilities. Computer Vision Libraries for Computer Vision. EasyOCR - Ready-to-use OCR with 40+ languages supported. Face Recognition - Simple facial recognition library. Kornia - Open Source Differentiable Computer Vision Library for PyTorch. OpenCV - Open Source Computer Vision Library. pytesseract - A wrapper for Google Tesseract OCR. SimpleCV - An open source framework for building computer vision applications. tesserocr - Another simple, Pillow-friendly, wrapper around the tesseract-ocr API for OCR. Concurrency and Parallelism Libraries for concurrent and parallel execution. Also see awesome-asyncio. concurrent.futures - (Python standard library) A high-level interface for asynchronously executing callables. eventlet - Asynchronous framework with WSGI support. gevent - A coroutine-based Python networking library that uses greenlet. multiprocessing - (Python standard library) Process-based parallelism. scoop - Scalable Concurrent Operations in Python. uvloop - Ultra fast implementation of asyncio event loop on top of libuv. Configuration Libraries for storing and parsing configuration options. configobj - INI file parser with validation. configparser - (Python standard library) INI file parser. hydra - Hydra is a framework for elegantly configuring complex applications. profig - Config from multiple formats with value conversion. python-decouple - Strict separation of settings from code. Cryptography cryptography - A package designed to expose cryptographic primitives and recipes to Python developers. paramiko - The leading native Python SSHv2 protocol library. passlib - Secure password storage/hashing library, very high level. pynacl - Python binding to the Networking and Cryptography (NaCl) library. Data Analysis Libraries for data analyzing. AWS Data Wrangler - Pandas on AWS. Blaze - NumPy and Pandas interface to Big Data. Open Mining - Business Intelligence (BI) in Pandas interface. Optimus - Agile Data Science Workflows made easy with PySpark. Orange - Data mining, data visualization, analysis and machine learning through visual programming or scripts. Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools. Data Validation Libraries for validating data. Used for forms in many cases. Cerberus - A lightweight and extensible data validation library. colander - Validating and deserializing data obtained via XML, JSON, an HTML form post. jsonschema - An implementation of JSON Schema for Python. schema - A library for validating Python data structures. Schematics - Data Structure Validation. valideer - Lightweight extensible data validation and adaptation library. voluptuous - A Python data validation library. Data Visualization Libraries for visualizing data. Also see awesome-javascript. Altair - Declarative statistical visualization library for Python. Bokeh - Interactive Web Plotting for Python. bqplot - Interactive Plotting Library for the Jupyter Notebook. Cartopy - A cartographic python library with matplotlib support. Dash - Built on top of Flask, React and Plotly aimed at analytical web applications. awesome-dash diagrams - Diagram as Code. Matplotlib - A Python 2D plotting library. plotnine - A grammar of graphics for Python based on ggplot2. Pygal - A Python SVG Charts Creator. PyGraphviz - Python interface to Graphviz. PyQtGraph - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets. Seaborn - Statistical data visualization using Matplotlib. VisPy - High-performance scientific visualization based on OpenGL. Database Databases implemented in Python. pickleDB - A simple and lightweight key-value store for Python. tinydb - A tiny, document-oriented database. ZODB - A native object database for Python. A key-value and object graph database. Database Drivers Libraries for connecting and operating databases. MySQL - awesome-mysql mysqlclient - MySQL connector with Python 3 support (mysql-python fork). PyMySQL - A pure Python MySQL driver compatible to mysql-python. PostgreSQL - awesome-postgres psycopg2 - The most popular PostgreSQL adapter for Python. queries - A wrapper of the psycopg2 library for interacting with PostgreSQL. SQlite - awesome-sqlite sqlite3 - (Python standard library) SQlite interface compliant with DB-API 2.0 SuperSQLite - A supercharged SQLite library built on top of apsw. Other Relational Databases pymssql - A simple database interface to Microsoft SQL Server. clickhouse-driver - Python driver with native interface for ClickHouse. NoSQL Databases cassandra-driver - The Python Driver for Apache Cassandra. happybase - A developer-friendly library for Apache HBase. kafka-python - The Python client for Apache Kafka. py2neo - A client library and toolkit for working with Neo4j. pymongo - The official Python client for MongoDB. redis-py - The Python client for Redis. Asynchronous Clients motor - The async Python driver for MongoDB. Date and Time Libraries for working with dates and times. Arrow - A Python library that offers a sensible and human-friendly approach to creating, manipulating, formatting and converting dates, times and timestamps. Chronyk - A Python 3 library for parsing human-written times and dates. dateutil - Extensions to the standard Python datetime module. delorean - A library for clearing up the inconvenient truths that arise dealing with datetimes. maya - Datetimes for Humans. moment - A Python library for dealing with dates/times. Inspired by Moment.js. Pendulum - Python datetimes made easy. PyTime - An easy-to-use Python module which aims to operate date/time/datetime by string. pytz - World timezone definitions, modern and historical. Brings the tz database into Python. when.py - Providing user-friendly functions to help perform common date and time actions. Debugging Tools Libraries for debugging code. pdb-like Debugger ipdb - IPython-enabled pdb. pdb++ - Another drop-in replacement for pdb. pudb - A full-screen, console-based Python debugger. wdb - An improbable web debugger through WebSockets. Tracing lptrace - strace for Python programs. manhole - Debugging UNIX socket connections and present the stacktraces for all threads and an interactive prompt. pyringe - Debugger capable of attaching to and injecting code into Python processes. python-hunter - A flexible code tracing toolkit. Profiler line_profiler - Line-by-line profiling. memory_profiler - Monitor Memory usage of Python code. py-spy - A sampling profiler for Python programs. Written in Rust. pyflame - A ptracing profiler For Python. vprof - Visual Python profiler. Others django-debug-toolbar - Display various debug information for Django. django-devserver - A drop-in replacement for Django's runserver. flask-debugtoolbar - A port of the django-debug-toolbar to flask. icecream - Inspect variables, expressions, and program execution with a single, simple function call. pyelftools - Parsing and analyzing ELF files and DWARF debugging information. Deep Learning Frameworks for Neural Networks and Deep Learning. Also see awesome-deep-learning. caffe - A fast open framework for deep learning.. keras - A high-level neural networks library and capable of running on top of either TensorFlow or Theano. mxnet - A deep learning framework designed for both efficiency and flexibility. pytorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration. SerpentAI - Game agent framework. Use any video game as a deep learning sandbox. tensorflow - The most popular Deep Learning framework created by Google. Theano - A library for fast numerical computation. DevOps Tools Software and libraries for DevOps. Configuration Management ansible - A radically simple IT automation platform. cloudinit - A multi-distribution package that handles early initialization of a cloud instance. OpenStack - Open source software for building private and public clouds. pyinfra - A versatile CLI tools and python libraries to automate infrastructure. saltstack - Infrastructure automation and management system. SSH-style Deployment cuisine - Chef-like functionality for Fabric. fabric - A simple, Pythonic tool for remote execution and deployment. fabtools - Tools for writing awesome Fabric files. Process Management honcho - A Python clone of Foreman, for managing Procfile-based applications. supervisor - Supervisor process control system for UNIX. Monitoring psutil - A cross-platform process and system utilities module. Backup BorgBackup - A deduplicating archiver with compression and encryption. Others docker-compose - Fast, isolated development environments using Docker. Distributed Computing Frameworks and libraries for Distributed Computing. Batch Processing dask - A flexible parallel computing library for analytic computing. luigi - A module that helps you build complex pipelines of batch jobs. mrjob - Run MapReduce jobs on Hadoop or Amazon Web Services. PySpark - Apache Spark Python API. Ray - A system for parallel and distributed Python that unifies the machine learning ecosystem. Stream Processing faust - A stream processing library, porting the ideas from Kafka Streams to Python. streamparse - Run Python code against real-time streams of data via Apache Storm. Distribution Libraries to create packaged executables for release distribution. dh-virtualenv - Build and distribute a virtualenv as a Debian package. Nuitka - Compile scripts, modules, packages to an executable or extension module. py2app - Freezes Python scripts (Mac OS X). py2exe - Freezes Python scripts (Windows). pyarmor - A tool used to obfuscate python scripts, bind obfuscated scripts to fixed machine or expire obfuscated scripts. PyInstaller - Converts Python programs into stand-alone executables (cross-platform). pynsist - A tool to build Windows installers, installers bundle Python itself. shiv - A command line utility for building fully self-contained zipapps (PEP 441), but with all their dependencies included. Documentation Libraries for generating project documentation. sphinx - Python Documentation generator. awesome-sphinxdoc pdoc - Epydoc replacement to auto generate API documentation for Python libraries. pycco - The literate-programming-style documentation generator. Downloader Libraries for downloading. akshare - A financial data interface library, built for human beings! s3cmd - A command line tool for managing Amazon S3 and CloudFront. s4cmd - Super S3 command line tool, good for higher performance. you-get - A YouTube/Youku/Niconico video downloader written in Python 3. youtube-dl - A small command-line program to download videos from YouTube. E-commerce Frameworks and libraries for e-commerce and payments. alipay - Unofficial Alipay API for Python. Cartridge - A shopping cart app built using the Mezzanine. django-oscar - An open-source e-commerce framework for Django. django-shop - A Django based shop system. forex-python - Foreign exchange rates, Bitcoin price index and currency conversion. merchant - A Django app to accept payments from various payment processors. money - Money class with optional CLDR-backed locale-aware formatting and an extensible currency exchange. python-currencies - Display money format and its filthy currencies. saleor - An e-commerce storefront for Django. shoop - An open source E-Commerce platform based on Django. Editor Plugins and IDEs Emacs elpy - Emacs Python Development Environment. Sublime Text anaconda - Anaconda turns your Sublime Text 3 in a full featured Python development IDE. SublimeJEDI - A Sublime Text plugin to the awesome auto-complete library Jedi. Vim jedi-vim - Vim bindings for the Jedi auto-completion library for Python. python-mode - An all in one plugin for turning Vim into a Python IDE. YouCompleteMe - Includes Jedi-based completion engine for Python. Visual Studio PTVS - Python Tools for Visual Studio. Visual Studio Code Python - The official VSCode extension with rich support for Python. IDE PyCharm - Commercial Python IDE by JetBrains. Has free community edition available. spyder - Open Source Python IDE. Email Libraries for sending and parsing email. Mail Servers modoboa - A mail hosting and management platform including a modern Web UI. salmon - A Python Mail Server. Clients imbox - Python IMAP for Humans. yagmail - Yet another Gmail/SMTP client. Others flanker - An email address and Mime parsing library. mailer - High-performance extensible mail delivery framework. Enterprise Application Integrations Platforms and tools for systems integrations in enterprise environments Zato - ESB, SOA, REST, APIs and Cloud Integrations in Python. Environment Management Libraries for Python version and virtual environment management. pyenv - Simple Python version management. virtualenv - A tool to create isolated Python environments. Files Libraries for file manipulation and MIME type detection. mimetypes - (Python standard library) Map filenames to MIME types. path.py - A module wrapper for os.path. pathlib - (Python standard library) An cross-platform, object-oriented path library. PyFilesystem2 - Python's filesystem abstraction layer. python-magic - A Python interface to the libmagic file type identification library. Unipath - An object-oriented approach to file/directory operations. watchdog - API and shell utilities to monitor file system events. Foreign Function Interface Libraries for providing foreign function interface. cffi - Foreign Function Interface for Python calling C code. ctypes - (Python standard library) Foreign Function Interface for Python calling C code. PyCUDA - A Python wrapper for Nvidia's CUDA API. SWIG - Simplified Wrapper and Interface Generator. Forms Libraries for working with forms. Deform - Python HTML form generation library influenced by the formish form generation library. django-bootstrap3 - Bootstrap 3 integration with Django. django-bootstrap4 - Bootstrap 4 integration with Django. django-crispy-forms - A Django app which lets you create beautiful forms in a very elegant and DRY way. django-remote-forms - A platform independent Django form serializer. WTForms - A flexible forms validation and rendering library. Functional Programming Functional Programming with Python. Coconut - A variant of Python built for simple, elegant, Pythonic functional programming. CyToolz - Cython implementation of Toolz: High performance functional utilities. fn.py - Functional programming in Python: implementation of missing features to enjoy FP. funcy - A fancy and practical functional tools. more-itertools - More routines for operating on iterables, beyond itertools. returns - A set of type-safe monads, transformers, and composition utilities. Toolz - A collection of functional utilities for iterators, functions, and dictionaries. GUI Development Libraries for working with graphical user interface applications. curses - Built-in wrapper for ncurses used to create terminal GUI applications. Eel - A library for making simple Electron-like offline HTML/JS GUI apps. enaml - Creating beautiful user-interfaces with Declarative Syntax like QML. Flexx - Flexx is a pure Python toolkit for creating GUI's, that uses web technology for its rendering. Gooey - Turn command line programs into a full GUI application with one line. kivy - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS. pyglet - A cross-platform windowing and multimedia library for Python. PyGObject - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3). PyQt - Python bindings for the Qt cross-platform application and UI framework. PySimpleGUI - Wrapper for tkinter, Qt, WxPython and Remi. pywebview - A lightweight cross-platform native wrapper around a webview component. Tkinter - Tkinter is Python's de-facto standard GUI package. Toga - A Python native, OS native GUI toolkit. urwid - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc. wxPython - A blending of the wxWidgets C++ class library with the Python. DearPyGui - A Simple GPU accelerated Python GUI framework GraphQL Libraries for working with GraphQL. graphene - GraphQL framework for Python. tartiflette-aiohttp - An aiohttp-based wrapper for Tartiflette to expose GraphQL APIs over HTTP. tartiflette-asgi - ASGI support for the Tartiflette GraphQL engine. tartiflette - SDL-first GraphQL engine implementation for Python 3.6+ and asyncio. Game Development Awesome game development libraries. Arcade - Arcade is a modern Python framework for crafting games with compelling graphics and sound. Cocos2d - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications. Harfang3D - Python framework for 3D, VR and game development. Panda3D - 3D game engine developed by Disney. Pygame - Pygame is a set of Python modules designed for writing games. PyOgre - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D. PyOpenGL - Python ctypes bindings for OpenGL and it's related APIs. PySDL2 - A ctypes based wrapper for the SDL2 library. RenPy - A Visual Novel engine. Geolocation Libraries for geocoding addresses and working with latitudes and longitudes. django-countries - A Django app that provides a country field for models and forms. GeoDjango - A world-class geographic web framework. GeoIP - Python API for MaxMind GeoIP Legacy Database. geojson - Python bindings and utilities for GeoJSON. geopy - Python Geocoding Toolbox. HTML Manipulation Libraries for working with HTML and XML. BeautifulSoup - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML. bleach - A whitelist-based HTML sanitization and text linkification library. cssutils - A CSS library for Python. html5lib - A standards-compliant library for parsing and serializing HTML documents and fragments. lxml - A very fast, easy-to-use and versatile library for handling HTML and XML. MarkupSafe - Implements a XML/HTML/XHTML Markup safe string for Python. pyquery - A jQuery-like library for parsing HTML. untangle - Converts XML documents to Python objects for easy access. WeasyPrint - A visual rendering engine for HTML and CSS that can export to PDF. xmldataset - Simple XML Parsing. xmltodict - Working with XML feel like you are working with JSON. HTTP Clients Libraries for working with HTTP. grequests - requests + gevent for asynchronous HTTP requests. httplib2 - Comprehensive HTTP client library. httpx - A next generation HTTP client for Python. requests - HTTP Requests for Humans. treq - Python requests like API built on top of Twisted's HTTP client. urllib3 - A HTTP library with thread-safe connection pooling, file post support, sanity friendly. Hardware Libraries for programming with hardware. ino - Command line toolkit for working with Arduino. keyboard - Hook and simulate global keyboard events on Windows and Linux. mouse - Hook and simulate global mouse events on Windows and Linux. Pingo - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc. PyUserInput - A module for cross-platform control of the mouse and keyboard. scapy - A brilliant packet manipulation library. Image Processing Libraries for manipulating images. hmap - Image histogram remapping. imgSeek - A project for searching a collection of images using visual similarity. nude.py - Nudity detection. pagan - Retro identicon (Avatar) generation based on input string and hash. pillow - Pillow is the friendly PIL fork. python-barcode - Create barcodes in Python with no extra dependencies. pygram - Instagram-like image filters. PyMatting - A library for alpha matting. python-qrcode - A pure Python QR Code generator. pywal - A tool that generates color schemes from images. pyvips - A fast image processing library with low memory needs. Quads - Computer art based on quadtrees. scikit-image - A Python library for (scientific) image processing. thumbor - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images. wand - Python bindings for MagickWand, C API for ImageMagick. Implementations Implementations of Python. CLPython - Implementation of the Python programming language written in Common Lisp. CPython - Default, most widely used implementation of the Python programming language written in C. Cython - Optimizing Static Compiler for Python. Grumpy - More compiler than interpreter as more powerful CPython2.7 replacement (alpha). IronPython - Implementation of the Python programming language written in C#. Jython - Implementation of Python programming language written in Java for the JVM. MicroPython - A lean and efficient Python programming language implementation. Numba - Python JIT compiler to LLVM aimed at scientific Python. PeachPy - x86-64 assembler embedded in Python. Pyjion - A JIT for Python based upon CoreCLR. PyPy - A very fast and compliant implementation of the Python language. Pyston - A Python implementation using JIT techniques. Stackless Python - An enhanced version of the Python programming language. Interactive Interpreter Interactive Python interpreters (REPL). bpython - A fancy interface to the Python interpreter. Jupyter Notebook (IPython) - A rich toolkit to help you make the most out of using Python interactively. awesome-jupyter ptpython - Advanced Python REPL built on top of the python-prompt-toolkit. Internationalization Libraries for working with i18n. Babel - An internationalization library for Python. PyICU - A wrapper of International Components for Unicode C++ library (ICU). Job Scheduler Libraries for scheduling jobs. Airflow - Airflow is a platform to programmatically author, schedule and monitor workflows. APScheduler - A light but powerful in-process task scheduler that lets you schedule functions. django-schedule - A calendaring app for Django. doit - A task runner and build tool. gunnery - Multipurpose task execution tool for distributed systems with web-based interface. Joblib - A set of tools to provide lightweight pipelining in Python. Plan - Writing crontab file in Python like a charm. Prefect - A modern workflow orchestration framework that makes it easy to build, schedule and monitor robust data pipelines. schedule - Python job scheduling for humans. Spiff - A powerful workflow engine implemented in pure Python. TaskFlow - A Python library that helps to make task execution easy, consistent and reliable. Logging Libraries for generating and working with logs. logbook - Logging replacement for Python. logging - (Python standard library) Logging facility for Python. loguru - Library which aims to bring enjoyable logging in Python. sentry-python - Sentry SDK for Python. structlog - Structured logging made easy. Machine Learning Libraries for Machine Learning. Also see awesome-machine-learning. gym - A toolkit for developing and comparing reinforcement learning algorithms. H2O - Open Source Fast Scalable Machine Learning Platform. Metrics - Machine learning evaluation metrics. NuPIC - Numenta Platform for Intelligent Computing. scikit-learn - The most popular Python library for Machine Learning. Spark ML - Apache Spark's scalable Machine Learning library. vowpal_porpoise - A lightweight Python wrapper for Vowpal Wabbit. xgboost - A scalable, portable, and distributed gradient boosting library. MindsDB - MindsDB is an open source AI layer for existing databases that allows you to effortlessly develop, train and deploy state-of-the-art machine learning models using standard queries. Microsoft Windows Python programming on Microsoft Windows. Python(x,y) - Scientific-applications-oriented Python Distribution based on Qt and Spyder. pythonlibs - Unofficial Windows binaries for Python extension packages. PythonNet - Python Integration with the .NET Common Language Runtime (CLR). PyWin32 - Python Extensions for Windows. WinPython - Portable development environment for Windows 7/8. Miscellaneous Useful libraries or tools that don't fit in the categories above. blinker - A fast Python in-process signal/event dispatching system. boltons - A set of pure-Python utilities. itsdangerous - Various helpers to pass trusted data to untrusted environments. magenta - A tool to generate music and art using artificial intelligence. pluginbase - A simple but flexible plugin system for Python. tryton - A general purpose business framework. Natural Language Processing Libraries for working with human languages. General gensim - Topic Modeling for Humans. langid.py - Stand-alone language identification system. nltk - A leading platform for building Python programs to work with human language data. pattern - A web mining module. polyglot - Natural language pipeline supporting hundreds of languages. pytext - A natural language modeling framework based on PyTorch. PyTorch-NLP - A toolkit enabling rapid deep learning NLP prototyping for research. spacy - A library for industrial-strength natural language processing in Python and Cython. Stanza - The Stanford NLP Group's official Python library, supporting 60+ languages. Chinese funNLP - A collection of tools and datasets for Chinese NLP. jieba - The most popular Chinese text segmentation library. pkuseg-python - A toolkit for Chinese word segmentation in various domains. snownlp - A library for processing Chinese text. Network Virtualization Tools and libraries for Virtual Networking and SDN (Software Defined Networking). mininet - A popular network emulator and API written in Python. napalm - Cross-vendor API to manipulate network devices. pox - A Python-based SDN control applications, such as OpenFlow SDN controllers. News Feed Libraries for building user's activities. django-activity-stream - Generating generic activity streams from the actions on your site. Stream Framework - Building news feed and notification systems using Cassandra and Redis. ORM Libraries that implement Object-Relational Mapping or data mapping techniques. Relational Databases Django Models - The Django ORM. SQLAlchemy - The Python SQL Toolkit and Object Relational Mapper. awesome-sqlalchemy dataset - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL. orator - The Orator ORM provides a simple yet beautiful ActiveRecord implementation. orm - An async ORM. peewee - A small, expressive ORM. pony - ORM that provides a generator-oriented interface to SQL. pydal - A pure Python Database Abstraction Layer. NoSQL Databases hot-redis - Rich Python data types for Redis. mongoengine - A Python Object-Document-Mapper for working with MongoDB. PynamoDB - A Pythonic interface for Amazon DynamoDB. redisco - A Python Library for Simple Models and Containers Persisted in Redis. Package Management Libraries for package and dependency management. pip - The package installer for Python. pip-tools - A set of tools to keep your pinned Python dependencies fresh. PyPI conda - Cross-platform, Python-agnostic binary package manager. poetry - Python dependency management and packaging made easy. Package Repositories Local PyPI repository server and proxies. bandersnatch - PyPI mirroring tool provided by Python Packaging Authority (PyPA). devpi - PyPI server and packaging/testing/release tool. localshop - Local PyPI server (custom packages and auto-mirroring of pypi). warehouse - Next generation Python Package Repository (PyPI). Penetration Testing Frameworks and tools for penetration testing. fsociety - A Penetration testing framework. setoolkit - A toolkit for social engineering. sqlmap - Automatic SQL injection and database takeover tool. Permissions Libraries that allow or deny users access to data or functionality. django-guardian - Implementation of per object permissions for Django 1.2+ django-rules - A tiny but powerful app providing object-level permissions to Django, without requiring a database. Processes Libraries for starting and communicating with OS processes. delegator.py - Subprocesses for Humans 2.0. sarge - Yet another wrapper for subprocess. sh - A full-fledged subprocess replacement for Python. Recommender Systems Libraries for building recommender systems. annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage. fastFM - A library for Factorization Machines. implicit - A fast Python implementation of collaborative filtering for implicit datasets. libffm - A library for Field-aware Factorization Machine (FFM). lightfm - A Python implementation of a number of popular recommendation algorithms. spotlight - Deep recommender models using PyTorch. Surprise - A scikit for building and analyzing recommender systems. tensorrec - A Recommendation Engine Framework in TensorFlow. Refactoring Refactoring tools and libraries for Python Bicycle Repair Man - Bicycle Repair Man, a refactoring tool for Python. Bowler - Safe code refactoring for modern Python. Rope - Rope is a python refactoring library. RESTful API Libraries for building RESTful APIs. Django django-rest-framework - A powerful and flexible toolkit to build web APIs. django-tastypie - Creating delicious APIs for Django apps. Flask eve - REST API framework powered by Flask, MongoDB and good intentions. flask-api - Browsable Web APIs for Flask. flask-restful - Quickly building REST APIs for Flask. Pyramid cornice - A RESTful framework for Pyramid. Framework agnostic apistar - A smart Web API framework, designed for Python 3. falcon - A high-performance framework for building cloud APIs and web app backends. fastapi - A modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints. hug - A Python 3 framework for cleanly exposing APIs. sandman2 - Automated REST APIs for existing database-driven systems. sanic - A Python 3.6+ web server and web framework that's written to go fast. vibora - Fast, efficient and asynchronous Web framework inspired by Flask. Robotics Libraries for robotics. PythonRobotics - This is a compilation of various robotics algorithms with visualizations. rospy - This is a library for ROS (Robot Operating System). RPC Servers RPC-compatible servers. RPyC (Remote Python Call) - A transparent and symmetric RPC library for Python zeroRPC - zerorpc is a flexible RPC implementation based on ZeroMQ and MessagePack. Science Libraries for scientific computing. Also see Python-for-Scientists. astropy - A community Python library for Astronomy. bcbio-nextgen - Providing best-practice pipelines for fully automated high throughput sequencing analysis. bccb - Collection of useful code related to biological analysis. Biopython - Biopython is a set of freely available tools for biological computation. cclib - A library for parsing and interpreting the results of computational chemistry packages. Colour - Implementing a comprehensive number of colour theory transformations and algorithms. Karate Club - Unsupervised machine learning toolbox for graph structured data. NetworkX - A high-productivity software for complex networks. NIPY - A collection of neuroimaging toolkits. NumPy - A fundamental package for scientific computing with Python. ObsPy - A Python toolbox for seismology. Open Babel - A chemical toolbox designed to speak the many languages of chemical data. PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion. PyMC - Markov Chain Monte Carlo sampling toolkit. QuTiP - Quantum Toolbox in Python. RDKit - Cheminformatics and Machine Learning Software. SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering. SimPy - A process-based discrete-event simulation framework. statsmodels - Statistical modeling and econometrics in Python. SymPy - A Python library for symbolic mathematics. Zipline - A Pythonic algorithmic trading library. Search Libraries and software for indexing and performing search queries on data. django-haystack - Modular search for Django. elasticsearch-dsl-py - The official high-level Python client for Elasticsearch. elasticsearch-py - The official low-level Python client for Elasticsearch. pysolr - A lightweight Python wrapper for Apache Solr. whoosh - A fast, pure Python search engine library. Serialization Libraries for serializing complex data types marshmallow - A lightweight library for converting complex objects to and from simple Python datatypes. pysimdjson - A Python bindings for simdjson. python-rapidjson - A Python wrapper around RapidJSON. ultrajson - A fast JSON decoder and encoder written in C with Python bindings. Serverless Frameworks Frameworks for developing serverless Python code. python-lambda - A toolkit for developing and deploying Python code in AWS Lambda. Zappa - A tool for deploying WSGI applications on AWS Lambda and API Gateway. Shell Shells based on Python. xonsh - A Python-powered, cross-platform, Unix-gazing shell language and command prompt. Specific Formats Processing Libraries for parsing and manipulating specific text formats. General tablib - A module for Tabular Datasets in XLS, CSV, JSON, YAML. Office docxtpl - Editing a docx document by jinja2 template openpyxl - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files. pyexcel - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files. python-docx - Reads, queries and modifies Microsoft Word 2007/2008 docx files. python-pptx - Python library for creating and updating PowerPoint (.pptx) files. unoconv - Convert between any document format supported by LibreOffice/OpenOffice. XlsxWriter - A Python module for creating Excel .xlsx files. xlwings - A BSD-licensed library that makes it easy to call Python from Excel and vice versa. xlwt / xlrd - Writing and reading data and formatting information from Excel files. PDF PDFMiner - A tool for extracting information from PDF documents. PyPDF2 - A library capable of splitting, merging and transforming PDF pages. ReportLab - Allowing Rapid creation of rich PDF documents. Markdown Mistune - Fastest and full featured pure Python parsers of Markdown. Python-Markdown - A Python implementation of John Grubers Markdown. YAML PyYAML - YAML implementations for Python. CSV csvkit - Utilities for converting to and working with CSV. Archive unp - A command line tool that can unpack archives easily. Static Site Generator Static site generator is a software that takes some text + templates as input and produces HTML files on the output. lektor - An easy to use static CMS and blog engine. mkdocs - Markdown friendly documentation generator. makesite - Simple, lightweight, and magic-free static site/blog generator (< 130 lines). nikola - A static website and blog generator. pelican - Static site generator that supports Markdown and reST syntax. Tagging Libraries for tagging items. django-taggit - Simple tagging for Django. Task Queues Libraries for working with task queues. celery - An asynchronous task queue/job queue based on distributed message passing. dramatiq - A fast and reliable background task processing library for Python 3. huey - Little multi-threaded task queue. mrq - A distributed worker task queue in Python using Redis & gevent. rq - Simple job queues for Python. Template Engine Libraries and tools for templating and lexing. Genshi - Python templating toolkit for generation of web-aware output. Jinja2 - A modern and designer friendly templating language. Mako - Hyperfast and lightweight templating for the Python platform. Testing Libraries for testing codebases and generating test data. Testing Frameworks hypothesis - Hypothesis is an advanced Quickcheck style property based testing library. nose2 - The successor to nose, based on `unittest2. pytest - A mature full-featured Python testing tool. Robot Framework - A generic test automation framework. unittest - (Python standard library) Unit testing framework. Test Runners green - A clean, colorful test runner. mamba - The definitive testing tool for Python. Born under the banner of BDD. tox - Auto builds and tests distributions in multiple Python versions GUI / Web Testing locust - Scalable user load testing tool written in Python. PyAutoGUI - PyAutoGUI is a cross-platform GUI automation Python module for human beings. Schemathesis - A tool for automatic property-based testing of web applications built with Open API / Swagger specifications. Selenium - Python bindings for Selenium WebDriver. sixpack - A language-agnostic A/B Testing framework. splinter - Open source tool for testing web applications. Mock doublex - Powerful test doubles framework for Python. freezegun - Travel through time by mocking the datetime module. httmock - A mocking library for requests for Python 2.6+ and 3.2+. httpretty - HTTP request mock tool for Python. mock - (Python standard library) A mocking and patching library. mocket - A socket mock framework with gevent/asyncio/SSL support. responses - A utility library for mocking out the requests Python library. VCR.py - Record and replay HTTP interactions on your tests. Object Factories factory_boy - A test fixtures replacement for Python. mixer - Another fixtures replacement. Supports Django, Flask, SQLAlchemy, Peewee and etc. model_mommy - Creating random fixtures for testing in Django. Code Coverage coverage - Code coverage measurement. Fake Data fake2db - Fake database generator. faker - A Python package that generates fake data. mimesis - is a Python library that help you generate fake data. radar - Generate random datetime / time. Text Processing Libraries for parsing and manipulating plain texts. General chardet - Python 2/3 compatible character encoding detector. difflib - (Python standard library) Helpers for computing deltas. ftfy - Makes Unicode text less broken and more consistent automagically. fuzzywuzzy - Fuzzy String Matching. Levenshtein - Fast computation of Levenshtein distance and string similarity. pangu.py - Paranoid text spacing. pyfiglet - An implementation of figlet written in Python. pypinyin - Convert Chinese hanzi () to pinyin (). textdistance - Compute distance between sequences with 30+ algorithms. unidecode - ASCII transliterations of Unicode text. Slugify awesome-slugify - A Python slugify library that can preserve unicode. python-slugify - A Python slugify library that translates unicode to ASCII. unicode-slugify - A slugifier that generates unicode slugs with Django as a dependency. Unique identifiers hashids - Implementation of hashids in Python. shortuuid - A generator library for concise, unambiguous and URL-safe UUIDs. Parser ply - Implementation of lex and yacc parsing tools for Python. pygments - A generic syntax highlighter. pyparsing - A general purpose framework for generating parsers. python-nameparser - Parsing human names into their individual components. python-phonenumbers - Parsing, formatting, storing and validating international phone numbers. python-user-agents - Browser user agent parser. sqlparse - A non-validating SQL parser. Third-party APIs Libraries for accessing third party services APIs. Also see List of Python API Wrappers and Libraries. apache-libcloud - One Python library for all clouds. boto3 - Python interface to Amazon Web Services. django-wordpress - WordPress models and views for Django. facebook-sdk - Facebook Platform Python SDK. google-api-python-client - Google APIs Client Library for Python. gspread - Google Spreadsheets Python API. twython - A Python wrapper for the Twitter API. URL Manipulation Libraries for parsing URLs. furl - A small Python library that makes parsing and manipulating URLs easy. purl - A simple, immutable URL class with a clean API for interrogation and manipulation. pyshorteners - A pure Python URL shortening lib. webargs - A friendly library for parsing HTTP request arguments with built-in support for popular web frameworks. Video Libraries for manipulating video and GIFs. moviepy - A module for script-based movie editing with many formats, including animated GIFs. scikit-video - Video processing routines for SciPy. vidgear - Most Powerful multi-threaded Video Processing framework. Web Asset Management Tools for managing, compressing and minifying website assets. django-compressor - Compresses linked and inline JavaScript or CSS into a single cached file. django-pipeline - An asset packaging library for Django. django-storages - A collection of custom storage back ends for Django. fanstatic - Packages, optimizes, and serves static file dependencies as Python packages. fileconveyor - A daemon to detect and sync files to CDNs, S3 and FTP. flask-assets - Helps you integrate webassets into your Flask app. webassets - Bundles, optimizes, and manages unique cache-busting URLs for static resources. Web Content Extracting Libraries for extracting web contents. html2text - Convert HTML to Markdown-formatted text. lassie - Web Content Retrieval for Humans. micawber - A small library for extracting rich content from URLs. newspaper - News extraction, article extraction and content curation in Python. python-readability - Fast Python port of arc90's readability tool. requests-html - Pythonic HTML Parsing for Humans. sumy - A module for automatic summarization of text documents and HTML pages. textract - Extract text from any document, Word, PowerPoint, PDFs, etc. toapi - Every web site provides APIs. Web Crawling Libraries to automate web scraping. cola - A distributed crawling framework. feedparser - Universal feed parser. grab - Site scraping framework. MechanicalSoup - A Python library for automating interaction with websites. portia - Visual scraping for Scrapy. pyspider - A powerful spider system. robobrowser - A simple, Pythonic library for browsing the web without a standalone web browser. scrapy - A fast high-level screen scraping and web crawling framework. Web Frameworks Traditional full stack web frameworks. Also see RESTful API. Synchronous Django - The most popular web framework in Python. awesome-django awesome-django Flask - A microframework for Python. awesome-flask Pyramid - A small, fast, down-to-earth, open source Python web framework. awesome-pyramid Masonite - The modern and developer centric Python web framework. Asynchronous Tornado - A web framework and asynchronous networking library. WebSocket Libraries for working with WebSocket. autobahn-python - WebSocket & WAMP for Python on Twisted and asyncio. channels - Developer-friendly asynchrony for Django. websockets - A library for building WebSocket servers and clients with a focus on correctness and simplicity. WSGI Servers WSGI-compatible web servers. bjoern - Asynchronous, very fast and written in C. gunicorn - Pre-forked, ported from Ruby's Unicorn project. uWSGI - A project aims at developing a full stack for building hosting services, written in C. waitress - Multi-threaded, powers Pyramid. werkzeug - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects. Resources Where to discover learning resources or new Python libraries. Books Fluent Python Think Python Websites Tutorials Full Stack Python Python Cheatsheet Real Python The Hitchhikers Guide to Python Ultimate Python study guide Libraries Awesome Python @LibHunt Others Python ZEEF Pythonic News What the f*ck Python! Newsletters Awesome Python Newsletter Pycoder's Weekly Python Tricks Python Weekly Podcasts Django Chat Podcast.__init__ Python Bytes Running in Production Talk Python To Me Test and Code The Real Python Podcast Contributing Your contributions are always welcome! Please take a look at the contribution guidelines first. I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could vote for them by adding to them. Pull requests will be merged when their votes reach 20. If you have any question about this opinionated list, do not hesitate to contact me @VintaChen on Twitter or open an issue on GitHub. ",
        "_version_": 1718536447338741760
      },
      {
        "story_id": 19481387,
        "story_author": "polm23",
        "story_descendants": 26,
        "story_score": 125,
        "story_time": "2019-03-25T10:30:49Z",
        "story_title": "Describing Chinese Characters with Recursive Radical Packing Language",
        "search": [
          "Describing Chinese Characters with Recursive Radical Packing Language",
          "https://github.com/LingDong-/rrpl",
          "Recursive Radical Packing Language (RRPL) is a proposal for a method of describing arbitrary Chinese characters concisely while retaining their structural information. Potential fields for usage include font design and machine learning. In RRPL, each Chinese character is described as a short string of numbers, symbols, and references to other characters. Its syntax is inspired by markup languages such as LaTeX, as well as the traditional \"\" grids used for calligraphy practice. 5000+ Traditional Chinese Characters and radicals are currently described using this language. You can download a .json file containing all of them (and unicode mapping) here: dist/min-trad.json Check out Chinese character & radical visualizations made with RRPL here and here. Syntax Each Chinese character is described as a combination of components. These components can be other characters or radicals, as well as building blocks, which defines the simplest shapes that make up every component. Combination can be applied recursively to describe ever more complex glyphs. Below is an overview of this syntax; You can also check out the Interactive Demo to play with it yourself. Building Blocks A building block is a string of the alphabet {0, 1, 2, 3, 4, 5, 6, 7, 8}, in which the presence of a number indicates a corresponding stroke to be drawn on a \"\" grid: 1 2 3 \\|/ 8 -+- 4 /|\\ 7 6 5 0 indicates that no stroke should be drawn in this block. Example: ResultCodeResultCode 48 24578 Packing Building blocks can be packed horizontally or vertically using the - and | symbols respectively to compose more complex glyphs. These symbols can be chained to pack more than two symbols with equal room. Example: ResultCodeResultCode 27-26-26 2468|24578 Grouping ( and ) symbols can be used to group components together so mixed horizontal and vertical packing can happen in the correct order. Example: ResultCodeResultCode (48|37)-(25678|27)-(37|15) (46-68)|(246-268)|(24-28) Referencing Other characters and radicals can be referenced directly to build a new character. The parser will dump the contents of the reference glyph directly into the string, similar to C/C++ #include feature. This makes it especially easy to describe the more complicated Chinese characters, as most of them consist of radicals. Example: ResultCodeResultCode |468||() ((|())-())|() (()-())|()| ((()-()-())|())|(()-()) Parser An baseline parser is included in rrpl_parser.js, which powers this Interactive Demo. It can be used with browser-side JavaScript as well as Node.js: //require the module: (or in html, <script src=\"./rrpl_parser.js\"></script>) var parser = require('./rrpl_parser.js'); //obtain an abstract syntax tree var ast = parser.parse(\"(48|37)-(25678|27)-(37|15)\"); //returns line segments (normalized 0.0-1.0) that can be used to render the character var lines = parser.toLines(parser.toRects(ast)); File Type RRPL data can be stored in a JSON file, whith the root object mapping unicode characters to their respective description, e.g. { \"\":\"48\", \"\":\"468|26|27\", \"\":\"246|248\", \"\":\"(48-45678-48)|(3-26-1)\", \"\":\"|\", \"\":\"(46-2468-68)|(24-2468-28)\", \"\":\"|\" } The references in these files are usually first expanded before rendering is attempted. This can be done in two ways. The first is using parser.preprocess(json_object) in rrpl_parser.js, while the second is using compile.js. More documentation can be found in the header comments of these files. The JSON files can be further compressed into (and uncompressed from) a binary file around half of the size of the original using compress.js, by using a half byte to encode each symbol in the RRPL alphabet. Downloads dist/min-trad.json contains RRPL description of 5000+ traditional Chinese characters stored in JSON format. dist/RRPL.ttf contains a True Type Font (ttf) containing 5000+ traditional Chinese characters with glyphs generated by the default parser. Below is a screenshot of the font in macOS TextEdit.app: Tools Rendering Generate a preview.html web page containing a rendering of all characters in a RRPL json file: $node render.js preview path/to/input.json Generate a realtime.html web page where user inputs can be parsed and rendered interactively: (Characters defined in the input file will be available for referencing) $node render.js realtime path/to/input.json Exporting Export a folder of SVG (Scalable Vector Graphics) rendering of each character in a RRPL json file: $node export_glyphs.js path/to/input.json path/to/output/folder 0 Contrary to what render.js generates, these SVG's contains \"outlines\" of the glyphs instead of simple strokes. More settings such as thickness can be tweaked in the source code of export_glyphs.js; Command-line API will come later. To generate a TTF (True Type) font from the aforementioned SVG's, FontForge's python library can be used for this purpose. (pip install fontforge) An example can be found in tools/forge_font.py. Applications Since RRPL reduces all Chinese characters to a short string of numbers, their structure can be learned by sequential models such as Markov chains, RNN's and LSTM's without much difficulty. I've applied RNN (Recurrent Neural Networks) to the language to hallucinate non-existent Chinese characters. Below are some characters generated by training overnight on ~1000 RRPL character descriptions, with the visuals rendered using a pix2pix model. A separate repo for that project will be created soon. Contributing rrpl.json contains the latest, work-in-progress version. There're some 5,000 characters in there, but there're over 50,000 Chinese characters in existence! So help is very much appreciated. If you'd like to help with this project, please append new characters to the file and submit a pull request. For more info, contact me by sending an email to lingdonh[at]andrew[dot]cmu[dot]edu. Below is a rendering of all 5000+ Chinese characters denoted using RRPL so far. Click on the image to enlarge. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/LingDong-/rrpl",
        "comments.comment_id": [19490888, 19491848],
        "comments.comment_author": ["userbinator", "crazygringo"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-03-26T11:58:34Z",
          "2019-03-26T13:59:58Z"
        ],
        "comments.comment_text": [
          "It's similar to this:<p><a href=\"https://en.wikipedia.org/wiki/Cangjie_input_method#Early_Cangjie_system\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Cangjie_input_method#Early_Can...</a><p><i>A particular \"feature\" of this early system is that if you send random lowercase words to the character generator, it will attempt to construct Chinese characters according to the Cangjie decomposition rules, sometimes causing strange, unknown characters to appear.</i>",
          "This is so cool!<p>Perhaps two decades ago ago I had the idea, what if ASCII/Unicode were eventually replaced with a system where the codepoints actually encoded the basic \"strokes\" of a character, much like this, then with additional context about the character (e.g. render as Latin uppercase, as Arabic, as a geometric symbol).<p>And then even if you didn't have the necessary font installed, there would still be enough information to render something close enough to be meaningful, even if it wouldn't necessarily be pretty. And that the bits and bytes in a text document wouldn't be arbitrary codepoints, but inherently meaningful.<p>And that it would solve, especially, the case of all these old Chinese characters that still aren't in Unicode.<p>I never decided to pursue it, and these days universal fonts already exist... but I love so much to see someone trying it, and the idea of associating it with a neural network to actually render in each style is <i>really</i> clever. It makes me wonder how feasible it would be to design a system using NN's that could take, say, any Latin-1 font and automatically produce an equivalent style in any other script, based on how existing semi-universal fonts have done the same."
        ],
        "id": "afd40c0f-09a0-4f9f-9861-96552553e95d",
        "url_text": "Recursive Radical Packing Language (RRPL) is a proposal for a method of describing arbitrary Chinese characters concisely while retaining their structural information. Potential fields for usage include font design and machine learning. In RRPL, each Chinese character is described as a short string of numbers, symbols, and references to other characters. Its syntax is inspired by markup languages such as LaTeX, as well as the traditional \"\" grids used for calligraphy practice. 5000+ Traditional Chinese Characters and radicals are currently described using this language. You can download a .json file containing all of them (and unicode mapping) here: dist/min-trad.json Check out Chinese character & radical visualizations made with RRPL here and here. Syntax Each Chinese character is described as a combination of components. These components can be other characters or radicals, as well as building blocks, which defines the simplest shapes that make up every component. Combination can be applied recursively to describe ever more complex glyphs. Below is an overview of this syntax; You can also check out the Interactive Demo to play with it yourself. Building Blocks A building block is a string of the alphabet {0, 1, 2, 3, 4, 5, 6, 7, 8}, in which the presence of a number indicates a corresponding stroke to be drawn on a \"\" grid: 1 2 3 \\|/ 8 -+- 4 /|\\ 7 6 5 0 indicates that no stroke should be drawn in this block. Example: ResultCodeResultCode 48 24578 Packing Building blocks can be packed horizontally or vertically using the - and | symbols respectively to compose more complex glyphs. These symbols can be chained to pack more than two symbols with equal room. Example: ResultCodeResultCode 27-26-26 2468|24578 Grouping ( and ) symbols can be used to group components together so mixed horizontal and vertical packing can happen in the correct order. Example: ResultCodeResultCode (48|37)-(25678|27)-(37|15) (46-68)|(246-268)|(24-28) Referencing Other characters and radicals can be referenced directly to build a new character. The parser will dump the contents of the reference glyph directly into the string, similar to C/C++ #include feature. This makes it especially easy to describe the more complicated Chinese characters, as most of them consist of radicals. Example: ResultCodeResultCode |468||() ((|())-())|() (()-())|()| ((()-()-())|())|(()-()) Parser An baseline parser is included in rrpl_parser.js, which powers this Interactive Demo. It can be used with browser-side JavaScript as well as Node.js: //require the module: (or in html, <script src=\"./rrpl_parser.js\"></script>) var parser = require('./rrpl_parser.js'); //obtain an abstract syntax tree var ast = parser.parse(\"(48|37)-(25678|27)-(37|15)\"); //returns line segments (normalized 0.0-1.0) that can be used to render the character var lines = parser.toLines(parser.toRects(ast)); File Type RRPL data can be stored in a JSON file, whith the root object mapping unicode characters to their respective description, e.g. { \"\":\"48\", \"\":\"468|26|27\", \"\":\"246|248\", \"\":\"(48-45678-48)|(3-26-1)\", \"\":\"|\", \"\":\"(46-2468-68)|(24-2468-28)\", \"\":\"|\" } The references in these files are usually first expanded before rendering is attempted. This can be done in two ways. The first is using parser.preprocess(json_object) in rrpl_parser.js, while the second is using compile.js. More documentation can be found in the header comments of these files. The JSON files can be further compressed into (and uncompressed from) a binary file around half of the size of the original using compress.js, by using a half byte to encode each symbol in the RRPL alphabet. Downloads dist/min-trad.json contains RRPL description of 5000+ traditional Chinese characters stored in JSON format. dist/RRPL.ttf contains a True Type Font (ttf) containing 5000+ traditional Chinese characters with glyphs generated by the default parser. Below is a screenshot of the font in macOS TextEdit.app: Tools Rendering Generate a preview.html web page containing a rendering of all characters in a RRPL json file: $node render.js preview path/to/input.json Generate a realtime.html web page where user inputs can be parsed and rendered interactively: (Characters defined in the input file will be available for referencing) $node render.js realtime path/to/input.json Exporting Export a folder of SVG (Scalable Vector Graphics) rendering of each character in a RRPL json file: $node export_glyphs.js path/to/input.json path/to/output/folder 0 Contrary to what render.js generates, these SVG's contains \"outlines\" of the glyphs instead of simple strokes. More settings such as thickness can be tweaked in the source code of export_glyphs.js; Command-line API will come later. To generate a TTF (True Type) font from the aforementioned SVG's, FontForge's python library can be used for this purpose. (pip install fontforge) An example can be found in tools/forge_font.py. Applications Since RRPL reduces all Chinese characters to a short string of numbers, their structure can be learned by sequential models such as Markov chains, RNN's and LSTM's without much difficulty. I've applied RNN (Recurrent Neural Networks) to the language to hallucinate non-existent Chinese characters. Below are some characters generated by training overnight on ~1000 RRPL character descriptions, with the visuals rendered using a pix2pix model. A separate repo for that project will be created soon. Contributing rrpl.json contains the latest, work-in-progress version. There're some 5,000 characters in there, but there're over 50,000 Chinese characters in existence! So help is very much appreciated. If you'd like to help with this project, please append new characters to the file and submit a pull request. For more info, contact me by sending an email to lingdonh[at]andrew[dot]cmu[dot]edu. Below is a rendering of all 5000+ Chinese characters denoted using RRPL so far. Click on the image to enlarge. ",
        "_version_": 1718536463235153920
      },
      {
        "story_id": 20993747,
        "story_author": "arzzen",
        "story_descendants": 9,
        "story_score": 120,
        "story_time": "2019-09-17T11:29:15Z",
        "story_title": "Show HN: Statistical tool for analyzing a Git repository",
        "search": [
          "Show HN: Statistical tool for analyzing a Git repository",
          "https://github.com/arzzen/git-quick-stats/",
          "GIT quick statistics git-quick-stats is a simple and efficient way to access various statistics in a git repository. Any git repository may contain tons of information about commits, contributors, and files. Extracting this information is not always trivial, mostly because there are a gadzillion options to a gadzillion git commands I dont think there is a single person alive who knows them all. Probably not even Linus Torvalds himself :). Table of Contents Screenshots Usage Interactive Non-interactive Command-line arguments Git log since and until Git log limit Git log options Git pathspec Git merge view strategy Color themes Installation UNIX and Linux macOS Windows Docker System requirements Dependencies FAQ Contribution Code reviews Some tips for good pull requests Formatting Tests Licensing Contributors Backers Sponsors Screenshots Usage Interactive git-quick-stats has a built-in interactive menu that can be executed as such: Or Non-interactive For those who prefer to utilize command-line options, git-quick-stats also has a non-interactive mode supporting both short and long options: git-quick-stats <optional-command-to-execute-directly> Or git quick-stats <optional-command-to-execute-directly> Command-line arguments Possible arguments in short and long form: GENERATE OPTIONS -T, --detailed-git-stats give a detailed list of git stats -R, --git-stats-by-branch see detailed list of git stats by branch -c, --changelogs see changelogs -L, --changelogs-by-author see changelogs by author -S, --my-daily-stats see your current daily stats -V, --csv-output-by-branch output daily stats by branch in CSV format -j, --json-output save git log as a JSON formatted file to a specified area LIST OPTIONS -b, --branch-tree show an ASCII graph of the git repo branch history -D, --branches-by-date show branches by date -C, --contributors see a list of everyone who contributed to the repo -a, --commits-per-author displays a list of commits per author -d, --commits-per-day displays a list of commits per day -m, --commits-by-month displays a list of commits per month -w, --commits-by-weekday displays a list of commits per weekday -o, --commits-by-hour displays a list of commits per hour -A, --commits-by-author-by-hour displays a list of commits per hour by author -z, --commits-by-timezone displays a list of commits per timezone -Z, --commits-by-author-by-timezone displays a list of commits per timezone by author SUGGEST OPTIONS -r, --suggest-reviewers show the best people to contact to review code -h, -?, --help display this help text in the terminal Git log since and until You can set the variables _GIT_SINCE and/or _GIT_UNTIL before running git-quick-stats to limit the git log. These work similar to git's built-in --since and --until log options. export _GIT_SINCE=\"2017-01-20\" export _GIT_UNTIL=\"2017-01-22\" Once set, run git quick-stats as normal. Note that this affects all stats that parse the git log history until unset. Git log limit You can set variable _GIT_LIMIT for limited output. It will affect the \"changelogs\" and \"branch tree\" options. Git log options You can set _GIT_LOG_OPTIONS for git log options: export _GIT_LOG_OPTIONS=\"--ignore-all-space --ignore-blank-lines\" Git pathspec You can exclude a directory from the stats by using pathspec export _GIT_PATHSPEC=':!directory' You can also exclude files from the stats. Note that it works with any alphanumeric, glob, or regex that git respects. export _GIT_PATHSPEC=':!package-lock.json' Git merge view strategy You can set the variable _GIT_MERGE_VIEW to enable merge commits to be part of the stats by setting _GIT_MERGE_VIEW to enable. You can also choose to only show merge commits by setting _GIT_MERGE_VIEW to exclusive. Default is to not show merge commits. These work similar to git's built-in --merges and --no-merges log options. export _GIT_MERGE_VIEW=\"enable\" export _GIT_MERGE_VIEW=\"exclusive\" Git branch You can set the variable _GIT_BRANCH to set the branch of the stats. Works with commands --git-stats-by-branch and --csv-output-by-branch. export _GIT_BRANCH=\"master\" Color themes You can change to the legacy color scheme by toggling the variable _MENU_THEME between default and legacy export _MENU_THEME=\"legacy\" Installation Debian and Ubuntu If you are on at least Debian Bullseye or Ubuntu Focal you can use apt for installation: apt install git-quick-stats UNIX and Linux git clone https://github.com/arzzen/git-quick-stats.git && cd git-quick-stats sudo make install For uninstalling, open up the cloned directory and run For update/reinstall macOS (homebrew) brew install git-quick-stats Or you can follow the UNIX and Linux instructions if you wish. Windows If you are installing with Cygwin, use these scripts: installer uninstaller If you are wishing to use this with WSL, follow the UNIX and Linux instructions. Docker You can use the Docker image provided: Build: docker build -t arzzen/git-quick-stats . Run interactive menu: docker run --rm -it -v $(pwd):/git arzzen/git-quick-stats Docker pull command: docker pull arzzen/git-quick-stats docker repository System requirements An OS with a Bash shell Tools we use: awk basename cat column echo git grep head printf seq sort tput tr uniq wc Dependencies bsdmainutils apt install bsdmainutils FAQ Q: I get some errors after run git-quick-stats in cygwin like /usr/local/bin/git-quick-stats: line 2: $'\\r': command not found A: You can run the dos2unix app in cygwin as follows: /bin/dos2unix.exe /usr/local/bin/git-quick-stats. This will convert the script from the CR-LF convention that Microsoft uses to the LF convention that UNIX, OS X, and Linux use. You should then should be able to run it as normal. Q: How they could be used in a project with many git projects and statistics would show a summary of all git projects? A: If you want to include submodule logs, you can try using the following: export _GIT_LOG_OPTIONS=\"-p --submodule=log\" (more info about git log --submodule) Contribution Want to contribute? Great! First, read this page. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Some tips for good pull requests Use our code When in doubt, try to stay true to the existing code of the project. Write a descriptive commit message. What problem are you solving and what are the consequences? Where and what did you test? Some good tips: here and here. If your PR consists of multiple commits which are successive improvements / fixes to your first commit, consider squashing them into a single commit (git rebase -i) such that your PR is a single commit on top of the current HEAD. This make reviewing the code so much easier, and our history more readable. Formatting This documentation is written using standard markdown syntax. Please submit your changes using the same syntax. Tests Licensing MIT see LICENSE for the full license text. Contributors This project exists thanks to all the people who contribute. Backers Thank you to all our backers! [Become a backer] Sponsors Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor] "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/arzzen/git-quick-stats/",
        "comments.comment_id": [20994452, 20999219],
        "comments.comment_author": ["eatonphil", "qznc"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-09-17T12:59:08Z",
          "2019-09-17T19:36:32Z"
        ],
        "comments.comment_text": [
          "Pull Panda [0] is another tool we've been using that is offered as a SaaS and is now free since it was acquired by Github this year. It tells you average PR review time, average PR diff size, who is most requested for review, review-comment ratio, etc. I can't believe it's taken Github so long to make progress on dashboards like this for engineering managers, but looking forward to the time Pull Panda is fully integrated.<p>[0] <a href=\"https://pullreminders.com\" rel=\"nofollow\">https://pullreminders.com</a>",
          "I made a tool myself [0] with slightly different tools. It answers two questions: Who are the relevant coders and what parts of the code are the hotspots?<p>Here is an example run on sqlite:<p><pre><code>    Top Committers (of 28 authors):\n    D. Richard Hipp      13359 commits during 19 years until 2019-09-17\n    Dan Kennedy          5813 commits during 17 years until 2019-09-16\n     together these authors have 80+% of the commits (19172/20987)\n\n    Files with most commits:\n    1143 commits: src/sqlite.h.in      during 19 years until 2019-09-16\n    1331 commits: src/where.c          during 19 years until 2019-09-03\n    1360 commits: src/btree.c          during 18 years until 2019-08-24\n    1650 commits: src/vdbe.c           during 19 years until 2019-09-16\n    1893 commits: src/sqliteInt.h      during 19 years until 2019-09-14\n\n    Files with most authors:\n    11 authors: src/main.c          \n    11 authors: src/sqliteInt.h     \n    12 authors: configure.ac        \n    12 authors: src/shell.c         \n    15 authors: Makefile.in         \n\n    By file extension:\n    .test: 1333 files\n       .c: 379 files\n     together these make up 80+% of the files (1712/2138)\n</code></pre>\n[0] <a href=\"https://github.com/qznc/dot/blob/master/bin/git-overview\" rel=\"nofollow\">https://github.com/qznc/dot/blob/master/bin/git-overview</a>"
        ],
        "id": "a1db772a-25ad-4dd6-a019-a414c8733ef6",
        "url_text": "GIT quick statistics git-quick-stats is a simple and efficient way to access various statistics in a git repository. Any git repository may contain tons of information about commits, contributors, and files. Extracting this information is not always trivial, mostly because there are a gadzillion options to a gadzillion git commands I dont think there is a single person alive who knows them all. Probably not even Linus Torvalds himself :). Table of Contents Screenshots Usage Interactive Non-interactive Command-line arguments Git log since and until Git log limit Git log options Git pathspec Git merge view strategy Color themes Installation UNIX and Linux macOS Windows Docker System requirements Dependencies FAQ Contribution Code reviews Some tips for good pull requests Formatting Tests Licensing Contributors Backers Sponsors Screenshots Usage Interactive git-quick-stats has a built-in interactive menu that can be executed as such: Or Non-interactive For those who prefer to utilize command-line options, git-quick-stats also has a non-interactive mode supporting both short and long options: git-quick-stats <optional-command-to-execute-directly> Or git quick-stats <optional-command-to-execute-directly> Command-line arguments Possible arguments in short and long form: GENERATE OPTIONS -T, --detailed-git-stats give a detailed list of git stats -R, --git-stats-by-branch see detailed list of git stats by branch -c, --changelogs see changelogs -L, --changelogs-by-author see changelogs by author -S, --my-daily-stats see your current daily stats -V, --csv-output-by-branch output daily stats by branch in CSV format -j, --json-output save git log as a JSON formatted file to a specified area LIST OPTIONS -b, --branch-tree show an ASCII graph of the git repo branch history -D, --branches-by-date show branches by date -C, --contributors see a list of everyone who contributed to the repo -a, --commits-per-author displays a list of commits per author -d, --commits-per-day displays a list of commits per day -m, --commits-by-month displays a list of commits per month -w, --commits-by-weekday displays a list of commits per weekday -o, --commits-by-hour displays a list of commits per hour -A, --commits-by-author-by-hour displays a list of commits per hour by author -z, --commits-by-timezone displays a list of commits per timezone -Z, --commits-by-author-by-timezone displays a list of commits per timezone by author SUGGEST OPTIONS -r, --suggest-reviewers show the best people to contact to review code -h, -?, --help display this help text in the terminal Git log since and until You can set the variables _GIT_SINCE and/or _GIT_UNTIL before running git-quick-stats to limit the git log. These work similar to git's built-in --since and --until log options. export _GIT_SINCE=\"2017-01-20\" export _GIT_UNTIL=\"2017-01-22\" Once set, run git quick-stats as normal. Note that this affects all stats that parse the git log history until unset. Git log limit You can set variable _GIT_LIMIT for limited output. It will affect the \"changelogs\" and \"branch tree\" options. Git log options You can set _GIT_LOG_OPTIONS for git log options: export _GIT_LOG_OPTIONS=\"--ignore-all-space --ignore-blank-lines\" Git pathspec You can exclude a directory from the stats by using pathspec export _GIT_PATHSPEC=':!directory' You can also exclude files from the stats. Note that it works with any alphanumeric, glob, or regex that git respects. export _GIT_PATHSPEC=':!package-lock.json' Git merge view strategy You can set the variable _GIT_MERGE_VIEW to enable merge commits to be part of the stats by setting _GIT_MERGE_VIEW to enable. You can also choose to only show merge commits by setting _GIT_MERGE_VIEW to exclusive. Default is to not show merge commits. These work similar to git's built-in --merges and --no-merges log options. export _GIT_MERGE_VIEW=\"enable\" export _GIT_MERGE_VIEW=\"exclusive\" Git branch You can set the variable _GIT_BRANCH to set the branch of the stats. Works with commands --git-stats-by-branch and --csv-output-by-branch. export _GIT_BRANCH=\"master\" Color themes You can change to the legacy color scheme by toggling the variable _MENU_THEME between default and legacy export _MENU_THEME=\"legacy\" Installation Debian and Ubuntu If you are on at least Debian Bullseye or Ubuntu Focal you can use apt for installation: apt install git-quick-stats UNIX and Linux git clone https://github.com/arzzen/git-quick-stats.git && cd git-quick-stats sudo make install For uninstalling, open up the cloned directory and run For update/reinstall macOS (homebrew) brew install git-quick-stats Or you can follow the UNIX and Linux instructions if you wish. Windows If you are installing with Cygwin, use these scripts: installer uninstaller If you are wishing to use this with WSL, follow the UNIX and Linux instructions. Docker You can use the Docker image provided: Build: docker build -t arzzen/git-quick-stats . Run interactive menu: docker run --rm -it -v $(pwd):/git arzzen/git-quick-stats Docker pull command: docker pull arzzen/git-quick-stats docker repository System requirements An OS with a Bash shell Tools we use: awk basename cat column echo git grep head printf seq sort tput tr uniq wc Dependencies bsdmainutils apt install bsdmainutils FAQ Q: I get some errors after run git-quick-stats in cygwin like /usr/local/bin/git-quick-stats: line 2: $'\\r': command not found A: You can run the dos2unix app in cygwin as follows: /bin/dos2unix.exe /usr/local/bin/git-quick-stats. This will convert the script from the CR-LF convention that Microsoft uses to the LF convention that UNIX, OS X, and Linux use. You should then should be able to run it as normal. Q: How they could be used in a project with many git projects and statistics would show a summary of all git projects? A: If you want to include submodule logs, you can try using the following: export _GIT_LOG_OPTIONS=\"-p --submodule=log\" (more info about git log --submodule) Contribution Want to contribute? Great! First, read this page. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Some tips for good pull requests Use our code When in doubt, try to stay true to the existing code of the project. Write a descriptive commit message. What problem are you solving and what are the consequences? Where and what did you test? Some good tips: here and here. If your PR consists of multiple commits which are successive improvements / fixes to your first commit, consider squashing them into a single commit (git rebase -i) such that your PR is a single commit on top of the current HEAD. This make reviewing the code so much easier, and our history more readable. Formatting This documentation is written using standard markdown syntax. Please submit your changes using the same syntax. Tests Licensing MIT see LICENSE for the full license text. Contributors This project exists thanks to all the people who contribute. Backers Thank you to all our backers! [Become a backer] Sponsors Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor] ",
        "_version_": 1718536522948411392
      },
      {
        "story_id": 19988548,
        "story_author": "axiomdata316",
        "story_descendants": 169,
        "story_score": 597,
        "story_time": "2019-05-23T04:42:59Z",
        "story_title": "The Art of Command Line (2015)",
        "search": [
          "The Art of Command Line (2015)",
          "https://github.com/jlevy/the-art-of-command-line",
          " etina Deutsch English Espaol Franais Indonesia Italiano polski Portugus Romn Slovenina The Art of Command Line Note: I'm planning to revise this and looking for a new co-author to help with expanding this into a more comprehensive guide. While it's very popular, it could be broader and a bit deeper. If you like to write and are close to being an expert on this material and willing to consider helping, please drop me a note at josh (0x40) holloway.com. jlevy, Holloway. Thank you! Meta Basics Everyday use Processing files and data System debugging One-liners Obscure but useful macOS only Windows only More resources Disclaimer Fluency on the command line is a skill often neglected or considered arcane, but it improves your flexibility and productivity as an engineer in both obvious and subtle ways. This is a selection of notes and tips on using the command-line that we've found useful when working on Linux. Some tips are elementary, and some are fairly specific, sophisticated, or obscure. This page is not long, but if you can use and recall all the items here, you know a lot. This work is the result of many authors and translators. Some of this originally appeared on Quora, but it has since moved to GitHub, where people more talented than the original author have made numerous improvements. Please submit a question if you have a question related to the command line. Please contribute if you see an error or something that could be better! Meta Scope: This guide is for both beginners and experienced users. The goals are breadth (everything important), specificity (give concrete examples of the most common case), and brevity (avoid things that aren't essential or digressions you can easily look up elsewhere). Every tip is essential in some situation or significantly saves time over alternatives. This is written for Linux, with the exception of the \"macOS only\" and \"Windows only\" sections. Many of the other items apply or can be installed on other Unices or macOS (or even Cygwin). The focus is on interactive Bash, though many tips apply to other shells and to general Bash scripting. It includes both \"standard\" Unix commands as well as ones that require special package installs -- so long as they are important enough to merit inclusion. Notes: To keep this to one page, content is implicitly included by reference. You're smart enough to look up more detail elsewhere once you know the idea or command to Google. Use apt, yum, dnf, pacman, pip or brew (as appropriate) to install new programs. Use Explainshell to get a helpful breakdown of what commands, options, pipes etc. do. Basics Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long. Alternate shells can be nice, but Bash is powerful and always available (learning only zsh, fish, etc., while tempting on your own laptop, restricts you in many situations, such as using existing servers). Learn at least one text-based editor well. The nano editor is one of the simplest for basic editing (opening, editing, saving, searching). However, for the power user in a text terminal, there is no substitute for Vim (vi), the hard-to-learn but venerable, fast, and full-featured editor. Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.) Finding documentation: Know how to read official documentation with man (for the inquisitive, man man lists the section numbers, e.g. 1 is \"regular\" commands, 5 is files/conventions, and 8 are for administration). Find man pages with apropos. Know that some commands are not executables, but Bash builtins, and that you can get help on them with help and help -d. You can find out whether a command is an executable, shell builtin or an alias by using type command. curl cheat.sh/command will give a brief \"cheat sheet\" with common examples of how to use a shell command. Learn about redirection of output and input using > and < and pipes using |. Know > overwrites the output file and >> appends. Learn about stdout and stderr. Learn about file glob expansion with * (and perhaps ? and [...]) and quoting and the difference between double \" and single ' quotes. (See more on variable expansion below.) Be familiar with Bash job management: &, ctrl-z, ctrl-c, jobs, fg, bg, kill, etc. Know ssh, and the basics of passwordless authentication, via ssh-agent, ssh-add, etc. Basic file management: ls and ls -l (in particular, learn what every column in ls -l means), less, head, tail and tail -f (or even better, less +F), ln and ln -s (learn the differences and advantages of hard versus soft links), chown, chmod, du (for a quick summary of disk usage: du -hs *). For filesystem management, df, mount, fdisk, mkfs, lsblk. Learn what an inode is (ls -i or df -i). Basic network management: ip or ifconfig, dig, traceroute, route. Learn and use a version control management system, such as git. Know regular expressions well, and the various flags to grep/egrep. The -i, -o, -v, -A, -B, and -C options are worth knowing. Learn to use apt-get, yum, dnf or pacman (depending on distro) to find and install packages. And make sure you have pip to install Python-based command-line tools (a few below are easiest to install via pip). Everyday use In Bash, use Tab to complete arguments or list all available commands and ctrl-r to search through command history (after pressing, type to search, press ctrl-r repeatedly to cycle through more matches, press Enter to execute the found command, or hit the right arrow to put the result in the current line to allow editing). In Bash, use ctrl-w to delete the last word, and ctrl-u to delete the content from current cursor back to the start of the line. Use alt-b and alt-f to move by word, ctrl-a to move cursor to beginning of line, ctrl-e to move cursor to end of line, ctrl-k to kill to the end of the line, ctrl-l to clear the screen. See man readline for all the default keybindings in Bash. There are a lot. For example alt-. cycles through previous arguments, and alt-* expands a glob. Alternatively, if you love vi-style key-bindings, use set -o vi (and set -o emacs to put it back). For editing long commands, after setting your editor (for example export EDITOR=vim), ctrl-x ctrl-e will open the current command in an editor for multi-line editing. Or in vi style, escape-v. To see recent commands, use history. Follow with !n (where n is the command number) to execute again. There are also many abbreviations you can use, the most useful probably being !$ for last argument and !! for last command (see \"HISTORY EXPANSION\" in the man page). However, these are often easily replaced with ctrl-r and alt-.. Go to your home directory with cd. Access files relative to your home directory with the ~ prefix (e.g. ~/.bashrc). In sh scripts refer to the home directory as $HOME. To go back to the previous working directory: cd -. If you are halfway through typing a command but change your mind, hit alt-# to add a # at the beginning and enter it as a comment (or use ctrl-a, #, enter). You can then return to it later via command history. Use xargs (or parallel). It's very powerful. Note you can control how many items execute per line (-L) as well as parallelism (-P). If you're not sure if it'll do the right thing, use xargs echo first. Also, -I{} is handy. Examples: find . -name '*.py' | xargs grep some_function cat hosts | xargs -I{} ssh root@{} hostname pstree -p is a helpful display of the process tree. Use pgrep and pkill to find or signal processes by name (-f is helpful). Know the various signals you can send processes. For example, to suspend a process, use kill -STOP [pid]. For the full list, see man 7 signal Use nohup or disown if you want a background process to keep running forever. Check what processes are listening via netstat -lntp or ss -plat (for TCP; add -u for UDP) or lsof -iTCP -sTCP:LISTEN -P -n (which also works on macOS). See also lsof and fuser for open sockets and files. See uptime or w to know how long the system has been running. Use alias to create shortcuts for commonly used commands. For example, alias ll='ls -latr' creates a new alias ll. Save aliases, shell settings, and functions you commonly use in ~/.bashrc, and arrange for login shells to source it. This will make your setup available in all your shell sessions. Put the settings of environment variables as well as commands that should be executed when you login in ~/.bash_profile. Separate configuration will be needed for shells you launch from graphical environment logins and cron jobs. Synchronize your configuration files (e.g. .bashrc and .bash_profile) among various computers with Git. Understand that care is needed when variables and filenames include whitespace. Surround your Bash variables with quotes, e.g. \"$FOO\". Prefer the -0 or -print0 options to enable null characters to delimit filenames, e.g. locate -0 pattern | xargs -0 ls -al or find / -print0 -type d | xargs -0 ls -al. To iterate on filenames containing whitespace in a for loop, set your IFS to be a newline only using IFS=$'\\n'. In Bash scripts, use set -x (or the variant set -v, which logs raw input, including unexpanded variables and comments) for debugging output. Use strict modes unless you have a good reason not to: Use set -e to abort on errors (nonzero exit code). Use set -u to detect unset variable usages. Consider set -o pipefail too, to abort on errors within pipes (though read up on it more if you do, as this topic is a bit subtle). For more involved scripts, also use trap on EXIT or ERR. A useful habit is to start a script like this, which will make it detect and abort on common errors and print a message: set -euo pipefail trap \"echo 'error: Script failed: see failed command above'\" ERR In Bash scripts, subshells (written with parentheses) are convenient ways to group commands. A common example is to temporarily move to a different working directory, e.g. # do something in current dir (cd /some/other/dir && other-command) # continue in original dir In Bash, note there are lots of kinds of variable expansion. Checking a variable exists: ${name:?error message}. For example, if a Bash script requires a single argument, just write input_file=${1:?usage: $0 input_file}. Using a default value if a variable is empty: ${name:-default}. If you want to have an additional (optional) parameter added to the previous example, you can use something like output_file=${2:-logfile}. If $2 is omitted and thus empty, output_file will be set to logfile. Arithmetic expansion: i=$(( (i + 1) % 5 )). Sequences: {1..10}. Trimming of strings: ${var%suffix} and ${var#prefix}. For example if var=foo.pdf, then echo ${var%.pdf}.txt prints foo.txt. Brace expansion using {...} can reduce having to re-type similar text and automate combinations of items. This is helpful in examples like mv foo.{txt,pdf} some-dir (which moves both files), cp somefile{,.bak} (which expands to cp somefile somefile.bak) or mkdir -p test-{a,b,c}/subtest-{1,2,3} (which expands all possible combinations and creates a directory tree). Brace expansion is performed before any other expansion. The order of expansions is: brace expansion; tilde expansion, parameter and variable expansion, arithmetic expansion, and command substitution (done in a left-to-right fashion); word splitting; and filename expansion. (For example, a range like {1..20} cannot be expressed with variables using {$a..$b}. Use seq or a for loop instead, e.g., seq $a $b or for((i=a; i<=b; i++)); do ... ; done.) The output of a command can be treated like a file via <(some command) (known as process substitution). For example, compare local /etc/hosts with a remote one: diff /etc/hosts <(ssh somehost cat /etc/hosts) When writing scripts you may want to put all of your code in curly braces. If the closing brace is missing, your script will be prevented from executing due to a syntax error. This makes sense when your script is going to be downloaded from the web, since it prevents partially downloaded scripts from executing: A \"here document\" allows redirection of multiple lines of input as if from a file: cat <<EOF input on multiple lines EOF In Bash, redirect both standard output and standard error via: some-command >logfile 2>&1 or some-command &>logfile. Often, to ensure a command does not leave an open file handle to standard input, tying it to the terminal you are in, it is also good practice to add </dev/null. Use man ascii for a good ASCII table, with hex and decimal values. For general encoding info, man unicode, man utf-8, and man latin1 are helpful. Use screen or tmux to multiplex the screen, especially useful on remote ssh sessions and to detach and re-attach to a session. byobu can enhance screen or tmux by providing more information and easier management. A more minimal alternative for session persistence only is dtach. In ssh, knowing how to port tunnel with -L or -D (and occasionally -R) is useful, e.g. to access web sites from a remote server. It can be useful to make a few optimizations to your ssh configuration; for example, this ~/.ssh/config contains settings to avoid dropped connections in certain network environments, uses compression (which is helpful with scp over low-bandwidth connections), and multiplex channels to the same server with a local control file: TCPKeepAlive=yes ServerAliveInterval=15 ServerAliveCountMax=6 Compression=yes ControlMaster auto ControlPath /tmp/%r@%h:%p ControlPersist yes A few other options relevant to ssh are security sensitive and should be enabled with care, e.g. per subnet or host or in trusted networks: StrictHostKeyChecking=no, ForwardAgent=yes Consider mosh an alternative to ssh that uses UDP, avoiding dropped connections and adding convenience on the road (requires server-side setup). To get the permissions on a file in octal form, which is useful for system configuration but not available in ls and easy to bungle, use something like stat -c '%A %a %n' /etc/timezone For interactive selection of values from the output of another command, use percol or fzf. For interaction with files based on the output of another command (like git), use fpp (PathPicker). For a simple web server for all files in the current directory (and subdirs), available to anyone on your network, use: python -m SimpleHTTPServer 7777 (for port 7777 and Python 2) and python -m http.server 7777 (for port 7777 and Python 3). For running a command as another user, use sudo. Defaults to running as root; use -u to specify another user. Use -i to login as that user (you will be asked for your password). For switching the shell to another user, use su username or su - username. The latter with \"-\" gets an environment as if another user just logged in. Omitting the username defaults to root. You will be asked for the password of the user you are switching to. Know about the 128K limit on command lines. This \"Argument list too long\" error is common when wildcard matching large numbers of files. (When this happens alternatives like find and xargs may help.) For a basic calculator (and of course access to Python in general), use the python interpreter. For example, Processing files and data To locate a file by name in the current directory, find . -iname '*something*' (or similar). To find a file anywhere by name, use locate something (but bear in mind updatedb may not have indexed recently created files). For general searching through source or data files, there are several options more advanced or faster than grep -r, including (in rough order from older to newer) ack, ag (\"the silver searcher\"), and rg (ripgrep). To convert HTML to text: lynx -dump -stdin For Markdown, HTML, and all kinds of document conversion, try pandoc. For example, to convert a Markdown document to Word format: pandoc README.md --from markdown --to docx -o temp.docx If you must handle XML, xmlstarlet is old but good. For JSON, use jq. For interactive use, also see jid and jiq. For YAML, use shyaml. For Excel or CSV files, csvkit provides in2csv, csvcut, csvjoin, csvgrep, etc. For Amazon S3, s3cmd is convenient and s4cmd is faster. Amazon's aws and the improved saws are essential for other AWS-related tasks. Know about sort and uniq, including uniq's -u and -d options -- see one-liners below. See also comm. Know about cut, paste, and join to manipulate text files. Many people use cut but forget about join. Know about wc to count newlines (-l), characters (-m), words (-w) and bytes (-c). Know about tee to copy from stdin to a file and also to stdout, as in ls -al | tee file.txt. For more complex calculations, including grouping, reversing fields, and statistical calculations, consider datamash. Know that locale affects a lot of command line tools in subtle ways, including sorting order (collation) and performance. Most Linux installations will set LANG or other locale variables to a local setting like US English. But be aware sorting will change if you change locale. And know i18n routines can make sort or other commands run many times slower. In some situations (such as the set operations or uniqueness operations below) you can safely ignore slow i18n routines entirely and use traditional byte-based sort order, using export LC_ALL=C. You can set a specific command's environment by prefixing its invocation with the environment variable settings, as in TZ=Pacific/Fiji date. Know basic awk and sed for simple data munging. See One-liners for examples. To replace all occurrences of a string in place, in one or more files: perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt To rename multiple files and/or search and replace within files, try repren. (In some cases the rename command also allows multiple renames, but be careful as its functionality is not the same on all Linux distributions.) # Full rename of filenames, directories, and contents foo -> bar: repren --full --preserve-case --from foo --to bar . # Recover backup files whatever.bak -> whatever: repren --renames --from '(.*)\\.bak' --to '\\1' *.bak # Same as above, using rename, if available: rename 's/\\.bak$//' *.bak As the man page says, rsync really is a fast and extraordinarily versatile file copying tool. It's known for synchronizing between machines but is equally useful locally. When security restrictions allow, using rsync instead of scp allows recovery of a transfer without restarting from scratch. It also is among the fastest ways to delete large numbers of files: mkdir empty && rsync -r --delete empty/ some-dir && rmdir some-dir For monitoring progress when processing files, use pv, pycp, pmonitor, progress, rsync --progress, or, for block-level copying, dd status=progress. Use shuf to shuffle or select random lines from a file. Know sort's options. For numbers, use -n, or -h for handling human-readable numbers (e.g. from du -h). Know how keys work (-t and -k). In particular, watch out that you need to write -k1,1 to sort by only the first field; -k1 means sort according to the whole line. Stable sort (sort -s) can be useful. For example, to sort first by field 2, then secondarily by field 1, you can use sort -k1,1 | sort -s -k2,2. If you ever need to write a tab literal in a command line in Bash (e.g. for the -t argument to sort), press ctrl-v [Tab] or write $'\\t' (the latter is better as you can copy/paste it). The standard tools for patching source code are diff and patch. See also diffstat for summary statistics of a diff and sdiff for a side-by-side diff. Note diff -r works for entire directories. Use diff -r tree1 tree2 | diffstat for a summary of changes. Use vimdiff to compare and edit files. For binary files, use hd, hexdump or xxd for simple hex dumps and bvi, hexedit or biew for binary editing. Also for binary files, strings (plus grep, etc.) lets you find bits of text. For binary diffs (delta compression), use xdelta3. To convert text encodings, try iconv. Or uconv for more advanced use; it supports some advanced Unicode things. For example: # Displays hex codes or actual names of characters (useful for debugging): uconv -f utf-8 -t utf-8 -x '::Any-Hex;' < input.txt uconv -f utf-8 -t utf-8 -x '::Any-Name;' < input.txt # Lowercase and removes all accents (by expanding and dropping them): uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] >; ::Any-NFC;' < input.txt > output.txt To split files into pieces, see split (to split by size) and csplit (to split by a pattern). Date and time: To get the current date and time in the helpful ISO 8601 format, use date -u +\"%Y-%m-%dT%H:%M:%SZ\" (other options are problematic). To manipulate date and time expressions, use dateadd, datediff, strptime etc. from dateutils. Use zless, zmore, zcat, and zgrep to operate on compressed files. File attributes are settable via chattr and offer a lower-level alternative to file permissions. For example, to protect against accidental file deletion the immutable flag: sudo chattr +i /critical/directory/or/file Use getfacl and setfacl to save and restore file permissions. For example: getfacl -R /some/path > permissions.txt setfacl --restore=permissions.txt To create empty files quickly, use truncate (creates sparse file), fallocate (ext4, xfs, btrfs and ocfs2 filesystems), xfs_mkfile (almost any filesystems, comes in xfsprogs package), mkfile (for Unix-like systems like Solaris, Mac OS). System debugging For web debugging, curl and curl -I are handy, or their wget equivalents, or the more modern httpie. To know current cpu/disk status, the classic tools are top (or the better htop), iostat, and iotop. Use iostat -mxz 15 for basic CPU and detailed per-partition disk stats and performance insight. For network connection details, use netstat and ss. For a quick overview of what's happening on a system, dstat is especially useful. For broadest overview with details, use glances. To know memory status, run and understand the output of free and vmstat. In particular, be aware the \"cached\" value is memory held by the Linux kernel as file cache, so effectively counts toward the \"free\" value. Java system debugging is a different kettle of fish, but a simple trick on Oracle's and some other JVMs is that you can run kill -3 <pid> and a full stack trace and heap summary (including generational garbage collection details, which can be highly informative) will be dumped to stderr/logs. The JDK's jps, jstat, jstack, jmap are useful. SJK tools are more advanced. Use mtr as a better traceroute, to identify network issues. For looking at why a disk is full, ncdu saves time over the usual commands like du -sh *. To find which socket or process is using bandwidth, try iftop or nethogs. The ab tool (comes with Apache) is helpful for quick-and-dirty checking of web server performance. For more complex load testing, try siege. For more serious network debugging, wireshark, tshark, or ngrep. Know about strace and ltrace. These can be helpful if a program is failing, hanging, or crashing, and you don't know why, or if you want to get a general idea of performance. Note the profiling option (-c), and the ability to attach to a running process (-p). Use trace child option (-f) to avoid missing important calls. Know about ldd to check shared libraries etc but never run it on untrusted files. Know how to connect to a running process with gdb and get its stack traces. Use /proc. It's amazingly helpful sometimes when debugging live problems. Examples: /proc/cpuinfo, /proc/meminfo, /proc/cmdline, /proc/xxx/cwd, /proc/xxx/exe, /proc/xxx/fd/, /proc/xxx/smaps (where xxx is the process id or pid). When debugging why something went wrong in the past, sar can be very helpful. It shows historic statistics on CPU, memory, network, etc. For deeper systems and performance analyses, look at stap (SystemTap), perf, and sysdig. Check what OS you're on with uname or uname -a (general Unix/kernel info) or lsb_release -a (Linux distro info). Use dmesg whenever something's acting really funny (it could be hardware or driver issues). If you delete a file and it doesn't free up expected disk space as reported by du, check whether the file is in use by a process: lsof | grep deleted | grep \"filename-of-my-big-file\" One-liners A few examples of piecing together commands: It is remarkably helpful sometimes that you can do set intersection, union, and difference of text files via sort/uniq. Suppose a and b are text files that are already uniqued. This is fast, and works on files of arbitrary size, up to many gigabytes. (Sort is not limited by memory, though you may need to use the -T option if /tmp is on a small root partition.) See also the note about LC_ALL above and sort's -u option (left out for clarity below). sort a b | uniq > c # c is a union b sort a b | uniq -d > c # c is a intersect b sort a b b | uniq -u > c # c is set difference a - b Pretty-print two JSON files, normalizing their syntax, then coloring and paginating the result: diff <(jq --sort-keys . < file1.json) <(jq --sort-keys . < file2.json) | colordiff | less -R Use grep . * to quickly examine the contents of all files in a directory (so each line is paired with the filename), or head -100 * (so each file has a heading). This can be useful for directories filled with config settings like those in /sys, /proc, /etc. Summing all numbers in the third column of a text file (this is probably 3X faster and 3X less code than equivalent Python): awk '{ x += $3 } END { print x }' myfile To see sizes/dates on a tree of files, this is like a recursive ls -l but is easier to read than ls -lR: Say you have a text file, like a web server log, and a certain value that appears on some lines, such as an acct_id parameter that is present in the URL. If you want a tally of how many requests for each acct_id: egrep -o 'acct_id=[0-9]+' access.log | cut -d= -f2 | sort | uniq -c | sort -rn To continuously monitor changes, use watch, e.g. check changes to files in a directory with watch -d -n 2 'ls -rtlh | tail' or to network settings while troubleshooting your wifi settings with watch -d -n 2 ifconfig. Run this function to get a random tip from this document (parses Markdown and extracts an item): function taocl() { curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md | sed '/cowsay[.]png/d' | pandoc -f markdown -t html | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v \"(html/body/ul/li[count(p)>0])[$RANDOM mod last()+1]\" | xmlstarlet unesc | fmt -80 | iconv -t US } Obscure but useful expr: perform arithmetic or boolean operations or evaluate regular expressions m4: simple macro processor yes: print a string a lot cal: nice calendar env: run a command (useful in scripts) printenv: print out environment variables (useful in debugging and scripts) look: find English words (or lines in a file) beginning with a string cut, paste and join: data manipulation fmt: format text paragraphs pr: format text into pages/columns fold: wrap lines of text column: format text fields into aligned, fixed-width columns or tables expand and unexpand: convert between tabs and spaces nl: add line numbers seq: print numbers bc: calculator factor: factor integers gpg: encrypt and sign files toe: table of terminfo entries nc: network debugging and data transfer socat: socket relay and tcp port forwarder (similar to netcat) slurm: network traffic visualization dd: moving data between files or devices file: identify type of a file tree: display directories and subdirectories as a nesting tree; like ls but recursive stat: file info time: execute and time a command timeout: execute a command for specified amount of time and stop the process when the specified amount of time completes. lockfile: create semaphore file that can only be removed by rm -f logrotate: rotate, compress and mail logs. watch: run a command repeatedly, showing results and/or highlighting changes when-changed: runs any command you specify whenever it sees file changed. See inotifywait and entr as well. tac: print files in reverse comm: compare sorted files line by line strings: extract text from binary files tr: character translation or manipulation iconv or uconv: conversion for text encodings split and csplit: splitting files sponge: read all input before writing it, useful for reading from then writing to the same file, e.g., grep -v something some-file | sponge some-file units: unit conversions and calculations; converts furlongs per fortnight to twips per blink (see also /usr/share/units/definitions.units) apg: generates random passwords xz: high-ratio file compression ldd: dynamic library info nm: symbols from object files ab or wrk: benchmarking web servers strace: system call debugging mtr: better traceroute for network debugging cssh: visual concurrent shell rsync: sync files and folders over SSH or in local file system wireshark and tshark: packet capture and network debugging ngrep: grep for the network layer host and dig: DNS lookups lsof: process file descriptor and socket info dstat: useful system stats glances: high level, multi-subsystem overview iostat: Disk usage stats mpstat: CPU usage stats vmstat: Memory usage stats htop: improved version of top last: login history w: who's logged on id: user/group identity info sar: historic system stats iftop or nethogs: network utilization by socket or process ss: socket statistics dmesg: boot and system error messages sysctl: view and configure Linux kernel parameters at run time hdparm: SATA/ATA disk manipulation/performance lsblk: list block devices: a tree view of your disks and disk partitions lshw, lscpu, lspci, lsusb, dmidecode: hardware information, including CPU, BIOS, RAID, graphics, devices, etc. lsmod and modinfo: List and show details of kernel modules. fortune, ddate, and sl: um, well, it depends on whether you consider steam locomotives and Zippy quotations \"useful\" macOS only These are items relevant only on macOS. Package management with brew (Homebrew) and/or port (MacPorts). These can be used to install on macOS many of the above commands. Copy output of any command to a desktop app with pbcopy and paste input from one with pbpaste. To enable the Option key in macOS Terminal as an alt key (such as used in the commands above like alt-b, alt-f, etc.), open Preferences -> Profiles -> Keyboard and select \"Use Option as Meta key\". To open a file with a desktop app, use open or open -a /Applications/Whatever.app. Spotlight: Search files with mdfind and list metadata (such as photo EXIF info) with mdls. Be aware macOS is based on BSD Unix, and many commands (for example ps, ls, tail, awk, sed) have many subtle variations from Linux, which is largely influenced by System V-style Unix and GNU tools. You can often tell the difference by noting a man page has the heading \"BSD General Commands Manual.\" In some cases GNU versions can be installed, too (such as gawk and gsed for GNU awk and sed). If writing cross-platform Bash scripts, avoid such commands (for example, consider Python or perl) or test carefully. To get macOS release information, use sw_vers. Windows only These items are relevant only on Windows. Ways to obtain Unix tools under Windows Access the power of the Unix shell under Microsoft Windows by installing Cygwin. Most of the things described in this document will work out of the box. On Windows 10, you can use Windows Subsystem for Linux (WSL), which provides a familiar Bash environment with Unix command line utilities. If you mainly want to use GNU developer tools (such as GCC) on Windows, consider MinGW and its MSYS package, which provides utilities such as bash, gawk, make and grep. MSYS doesn't have all the features compared to Cygwin. MinGW is particularly useful for creating native Windows ports of Unix tools. Another option to get Unix look and feel under Windows is Cash. Note that only very few Unix commands and command-line options are available in this environment. Useful Windows command-line tools You can perform and script most Windows system administration tasks from the command line by learning and using wmic. Native command-line Windows networking tools you may find useful include ping, ipconfig, tracert, and netstat. You can perform many useful Windows tasks by invoking the Rundll32 command. Cygwin tips and tricks Install additional Unix programs with the Cygwin's package manager. Use mintty as your command-line window. Access the Windows clipboard through /dev/clipboard. Run cygstart to open an arbitrary file through its registered application. Access the Windows registry with regtool. Note that a C:\\ Windows drive path becomes /cygdrive/c under Cygwin, and that Cygwin's / appears under C:\\cygwin on Windows. Convert between Cygwin and Windows-style file paths with cygpath. This is most useful in scripts that invoke Windows programs. More resources awesome-shell: A curated list of shell tools and resources. awesome-osx-command-line: A more in-depth guide for the macOS command line. Strict mode for writing better shell scripts. shellcheck: A shell script static analysis tool. Essentially, lint for bash/sh/zsh. Filenames and Pathnames in Shell: The sadly complex minutiae on how to handle filenames correctly in shell scripts. Data Science at the Command Line: More commands and tools helpful for doing data science, from the book of the same name Disclaimer With the exception of very small tasks, code is written so others can read it. With power comes responsibility. The fact you can do something in Bash doesn't necessarily mean you should! ;) License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/jlevy/the-art-of-command-line",
        "comments.comment_id": [19989314, 19989338],
        "comments.comment_author": ["smith-kyle", "molteanu"],
        "comments.comment_descendants": [8, 4],
        "comments.comment_time": [
          "2019-05-23T07:16:13Z",
          "2019-05-23T07:21:38Z"
        ],
        "comments.comment_text": [
          "> Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long<p>Skimming <i>The Grapes of Wrath</i> would be shorter. Thank you, but no thank you.",
          "<i>Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.)</i><p>Huh?! Unlikely?!"
        ],
        "id": "32047875-53e0-426f-ad69-c2a16cc48e54",
        "url_text": " etina Deutsch English Espaol Franais Indonesia Italiano polski Portugus Romn Slovenina The Art of Command Line Note: I'm planning to revise this and looking for a new co-author to help with expanding this into a more comprehensive guide. While it's very popular, it could be broader and a bit deeper. If you like to write and are close to being an expert on this material and willing to consider helping, please drop me a note at josh (0x40) holloway.com. jlevy, Holloway. Thank you! Meta Basics Everyday use Processing files and data System debugging One-liners Obscure but useful macOS only Windows only More resources Disclaimer Fluency on the command line is a skill often neglected or considered arcane, but it improves your flexibility and productivity as an engineer in both obvious and subtle ways. This is a selection of notes and tips on using the command-line that we've found useful when working on Linux. Some tips are elementary, and some are fairly specific, sophisticated, or obscure. This page is not long, but if you can use and recall all the items here, you know a lot. This work is the result of many authors and translators. Some of this originally appeared on Quora, but it has since moved to GitHub, where people more talented than the original author have made numerous improvements. Please submit a question if you have a question related to the command line. Please contribute if you see an error or something that could be better! Meta Scope: This guide is for both beginners and experienced users. The goals are breadth (everything important), specificity (give concrete examples of the most common case), and brevity (avoid things that aren't essential or digressions you can easily look up elsewhere). Every tip is essential in some situation or significantly saves time over alternatives. This is written for Linux, with the exception of the \"macOS only\" and \"Windows only\" sections. Many of the other items apply or can be installed on other Unices or macOS (or even Cygwin). The focus is on interactive Bash, though many tips apply to other shells and to general Bash scripting. It includes both \"standard\" Unix commands as well as ones that require special package installs -- so long as they are important enough to merit inclusion. Notes: To keep this to one page, content is implicitly included by reference. You're smart enough to look up more detail elsewhere once you know the idea or command to Google. Use apt, yum, dnf, pacman, pip or brew (as appropriate) to install new programs. Use Explainshell to get a helpful breakdown of what commands, options, pipes etc. do. Basics Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long. Alternate shells can be nice, but Bash is powerful and always available (learning only zsh, fish, etc., while tempting on your own laptop, restricts you in many situations, such as using existing servers). Learn at least one text-based editor well. The nano editor is one of the simplest for basic editing (opening, editing, saving, searching). However, for the power user in a text terminal, there is no substitute for Vim (vi), the hard-to-learn but venerable, fast, and full-featured editor. Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.) Finding documentation: Know how to read official documentation with man (for the inquisitive, man man lists the section numbers, e.g. 1 is \"regular\" commands, 5 is files/conventions, and 8 are for administration). Find man pages with apropos. Know that some commands are not executables, but Bash builtins, and that you can get help on them with help and help -d. You can find out whether a command is an executable, shell builtin or an alias by using type command. curl cheat.sh/command will give a brief \"cheat sheet\" with common examples of how to use a shell command. Learn about redirection of output and input using > and < and pipes using |. Know > overwrites the output file and >> appends. Learn about stdout and stderr. Learn about file glob expansion with * (and perhaps ? and [...]) and quoting and the difference between double \" and single ' quotes. (See more on variable expansion below.) Be familiar with Bash job management: &, ctrl-z, ctrl-c, jobs, fg, bg, kill, etc. Know ssh, and the basics of passwordless authentication, via ssh-agent, ssh-add, etc. Basic file management: ls and ls -l (in particular, learn what every column in ls -l means), less, head, tail and tail -f (or even better, less +F), ln and ln -s (learn the differences and advantages of hard versus soft links), chown, chmod, du (for a quick summary of disk usage: du -hs *). For filesystem management, df, mount, fdisk, mkfs, lsblk. Learn what an inode is (ls -i or df -i). Basic network management: ip or ifconfig, dig, traceroute, route. Learn and use a version control management system, such as git. Know regular expressions well, and the various flags to grep/egrep. The -i, -o, -v, -A, -B, and -C options are worth knowing. Learn to use apt-get, yum, dnf or pacman (depending on distro) to find and install packages. And make sure you have pip to install Python-based command-line tools (a few below are easiest to install via pip). Everyday use In Bash, use Tab to complete arguments or list all available commands and ctrl-r to search through command history (after pressing, type to search, press ctrl-r repeatedly to cycle through more matches, press Enter to execute the found command, or hit the right arrow to put the result in the current line to allow editing). In Bash, use ctrl-w to delete the last word, and ctrl-u to delete the content from current cursor back to the start of the line. Use alt-b and alt-f to move by word, ctrl-a to move cursor to beginning of line, ctrl-e to move cursor to end of line, ctrl-k to kill to the end of the line, ctrl-l to clear the screen. See man readline for all the default keybindings in Bash. There are a lot. For example alt-. cycles through previous arguments, and alt-* expands a glob. Alternatively, if you love vi-style key-bindings, use set -o vi (and set -o emacs to put it back). For editing long commands, after setting your editor (for example export EDITOR=vim), ctrl-x ctrl-e will open the current command in an editor for multi-line editing. Or in vi style, escape-v. To see recent commands, use history. Follow with !n (where n is the command number) to execute again. There are also many abbreviations you can use, the most useful probably being !$ for last argument and !! for last command (see \"HISTORY EXPANSION\" in the man page). However, these are often easily replaced with ctrl-r and alt-.. Go to your home directory with cd. Access files relative to your home directory with the ~ prefix (e.g. ~/.bashrc). In sh scripts refer to the home directory as $HOME. To go back to the previous working directory: cd -. If you are halfway through typing a command but change your mind, hit alt-# to add a # at the beginning and enter it as a comment (or use ctrl-a, #, enter). You can then return to it later via command history. Use xargs (or parallel). It's very powerful. Note you can control how many items execute per line (-L) as well as parallelism (-P). If you're not sure if it'll do the right thing, use xargs echo first. Also, -I{} is handy. Examples: find . -name '*.py' | xargs grep some_function cat hosts | xargs -I{} ssh root@{} hostname pstree -p is a helpful display of the process tree. Use pgrep and pkill to find or signal processes by name (-f is helpful). Know the various signals you can send processes. For example, to suspend a process, use kill -STOP [pid]. For the full list, see man 7 signal Use nohup or disown if you want a background process to keep running forever. Check what processes are listening via netstat -lntp or ss -plat (for TCP; add -u for UDP) or lsof -iTCP -sTCP:LISTEN -P -n (which also works on macOS). See also lsof and fuser for open sockets and files. See uptime or w to know how long the system has been running. Use alias to create shortcuts for commonly used commands. For example, alias ll='ls -latr' creates a new alias ll. Save aliases, shell settings, and functions you commonly use in ~/.bashrc, and arrange for login shells to source it. This will make your setup available in all your shell sessions. Put the settings of environment variables as well as commands that should be executed when you login in ~/.bash_profile. Separate configuration will be needed for shells you launch from graphical environment logins and cron jobs. Synchronize your configuration files (e.g. .bashrc and .bash_profile) among various computers with Git. Understand that care is needed when variables and filenames include whitespace. Surround your Bash variables with quotes, e.g. \"$FOO\". Prefer the -0 or -print0 options to enable null characters to delimit filenames, e.g. locate -0 pattern | xargs -0 ls -al or find / -print0 -type d | xargs -0 ls -al. To iterate on filenames containing whitespace in a for loop, set your IFS to be a newline only using IFS=$'\\n'. In Bash scripts, use set -x (or the variant set -v, which logs raw input, including unexpanded variables and comments) for debugging output. Use strict modes unless you have a good reason not to: Use set -e to abort on errors (nonzero exit code). Use set -u to detect unset variable usages. Consider set -o pipefail too, to abort on errors within pipes (though read up on it more if you do, as this topic is a bit subtle). For more involved scripts, also use trap on EXIT or ERR. A useful habit is to start a script like this, which will make it detect and abort on common errors and print a message: set -euo pipefail trap \"echo 'error: Script failed: see failed command above'\" ERR In Bash scripts, subshells (written with parentheses) are convenient ways to group commands. A common example is to temporarily move to a different working directory, e.g. # do something in current dir (cd /some/other/dir && other-command) # continue in original dir In Bash, note there are lots of kinds of variable expansion. Checking a variable exists: ${name:?error message}. For example, if a Bash script requires a single argument, just write input_file=${1:?usage: $0 input_file}. Using a default value if a variable is empty: ${name:-default}. If you want to have an additional (optional) parameter added to the previous example, you can use something like output_file=${2:-logfile}. If $2 is omitted and thus empty, output_file will be set to logfile. Arithmetic expansion: i=$(( (i + 1) % 5 )). Sequences: {1..10}. Trimming of strings: ${var%suffix} and ${var#prefix}. For example if var=foo.pdf, then echo ${var%.pdf}.txt prints foo.txt. Brace expansion using {...} can reduce having to re-type similar text and automate combinations of items. This is helpful in examples like mv foo.{txt,pdf} some-dir (which moves both files), cp somefile{,.bak} (which expands to cp somefile somefile.bak) or mkdir -p test-{a,b,c}/subtest-{1,2,3} (which expands all possible combinations and creates a directory tree). Brace expansion is performed before any other expansion. The order of expansions is: brace expansion; tilde expansion, parameter and variable expansion, arithmetic expansion, and command substitution (done in a left-to-right fashion); word splitting; and filename expansion. (For example, a range like {1..20} cannot be expressed with variables using {$a..$b}. Use seq or a for loop instead, e.g., seq $a $b or for((i=a; i<=b; i++)); do ... ; done.) The output of a command can be treated like a file via <(some command) (known as process substitution). For example, compare local /etc/hosts with a remote one: diff /etc/hosts <(ssh somehost cat /etc/hosts) When writing scripts you may want to put all of your code in curly braces. If the closing brace is missing, your script will be prevented from executing due to a syntax error. This makes sense when your script is going to be downloaded from the web, since it prevents partially downloaded scripts from executing: A \"here document\" allows redirection of multiple lines of input as if from a file: cat <<EOF input on multiple lines EOF In Bash, redirect both standard output and standard error via: some-command >logfile 2>&1 or some-command &>logfile. Often, to ensure a command does not leave an open file handle to standard input, tying it to the terminal you are in, it is also good practice to add </dev/null. Use man ascii for a good ASCII table, with hex and decimal values. For general encoding info, man unicode, man utf-8, and man latin1 are helpful. Use screen or tmux to multiplex the screen, especially useful on remote ssh sessions and to detach and re-attach to a session. byobu can enhance screen or tmux by providing more information and easier management. A more minimal alternative for session persistence only is dtach. In ssh, knowing how to port tunnel with -L or -D (and occasionally -R) is useful, e.g. to access web sites from a remote server. It can be useful to make a few optimizations to your ssh configuration; for example, this ~/.ssh/config contains settings to avoid dropped connections in certain network environments, uses compression (which is helpful with scp over low-bandwidth connections), and multiplex channels to the same server with a local control file: TCPKeepAlive=yes ServerAliveInterval=15 ServerAliveCountMax=6 Compression=yes ControlMaster auto ControlPath /tmp/%r@%h:%p ControlPersist yes A few other options relevant to ssh are security sensitive and should be enabled with care, e.g. per subnet or host or in trusted networks: StrictHostKeyChecking=no, ForwardAgent=yes Consider mosh an alternative to ssh that uses UDP, avoiding dropped connections and adding convenience on the road (requires server-side setup). To get the permissions on a file in octal form, which is useful for system configuration but not available in ls and easy to bungle, use something like stat -c '%A %a %n' /etc/timezone For interactive selection of values from the output of another command, use percol or fzf. For interaction with files based on the output of another command (like git), use fpp (PathPicker). For a simple web server for all files in the current directory (and subdirs), available to anyone on your network, use: python -m SimpleHTTPServer 7777 (for port 7777 and Python 2) and python -m http.server 7777 (for port 7777 and Python 3). For running a command as another user, use sudo. Defaults to running as root; use -u to specify another user. Use -i to login as that user (you will be asked for your password). For switching the shell to another user, use su username or su - username. The latter with \"-\" gets an environment as if another user just logged in. Omitting the username defaults to root. You will be asked for the password of the user you are switching to. Know about the 128K limit on command lines. This \"Argument list too long\" error is common when wildcard matching large numbers of files. (When this happens alternatives like find and xargs may help.) For a basic calculator (and of course access to Python in general), use the python interpreter. For example, Processing files and data To locate a file by name in the current directory, find . -iname '*something*' (or similar). To find a file anywhere by name, use locate something (but bear in mind updatedb may not have indexed recently created files). For general searching through source or data files, there are several options more advanced or faster than grep -r, including (in rough order from older to newer) ack, ag (\"the silver searcher\"), and rg (ripgrep). To convert HTML to text: lynx -dump -stdin For Markdown, HTML, and all kinds of document conversion, try pandoc. For example, to convert a Markdown document to Word format: pandoc README.md --from markdown --to docx -o temp.docx If you must handle XML, xmlstarlet is old but good. For JSON, use jq. For interactive use, also see jid and jiq. For YAML, use shyaml. For Excel or CSV files, csvkit provides in2csv, csvcut, csvjoin, csvgrep, etc. For Amazon S3, s3cmd is convenient and s4cmd is faster. Amazon's aws and the improved saws are essential for other AWS-related tasks. Know about sort and uniq, including uniq's -u and -d options -- see one-liners below. See also comm. Know about cut, paste, and join to manipulate text files. Many people use cut but forget about join. Know about wc to count newlines (-l), characters (-m), words (-w) and bytes (-c). Know about tee to copy from stdin to a file and also to stdout, as in ls -al | tee file.txt. For more complex calculations, including grouping, reversing fields, and statistical calculations, consider datamash. Know that locale affects a lot of command line tools in subtle ways, including sorting order (collation) and performance. Most Linux installations will set LANG or other locale variables to a local setting like US English. But be aware sorting will change if you change locale. And know i18n routines can make sort or other commands run many times slower. In some situations (such as the set operations or uniqueness operations below) you can safely ignore slow i18n routines entirely and use traditional byte-based sort order, using export LC_ALL=C. You can set a specific command's environment by prefixing its invocation with the environment variable settings, as in TZ=Pacific/Fiji date. Know basic awk and sed for simple data munging. See One-liners for examples. To replace all occurrences of a string in place, in one or more files: perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt To rename multiple files and/or search and replace within files, try repren. (In some cases the rename command also allows multiple renames, but be careful as its functionality is not the same on all Linux distributions.) # Full rename of filenames, directories, and contents foo -> bar: repren --full --preserve-case --from foo --to bar . # Recover backup files whatever.bak -> whatever: repren --renames --from '(.*)\\.bak' --to '\\1' *.bak # Same as above, using rename, if available: rename 's/\\.bak$//' *.bak As the man page says, rsync really is a fast and extraordinarily versatile file copying tool. It's known for synchronizing between machines but is equally useful locally. When security restrictions allow, using rsync instead of scp allows recovery of a transfer without restarting from scratch. It also is among the fastest ways to delete large numbers of files: mkdir empty && rsync -r --delete empty/ some-dir && rmdir some-dir For monitoring progress when processing files, use pv, pycp, pmonitor, progress, rsync --progress, or, for block-level copying, dd status=progress. Use shuf to shuffle or select random lines from a file. Know sort's options. For numbers, use -n, or -h for handling human-readable numbers (e.g. from du -h). Know how keys work (-t and -k). In particular, watch out that you need to write -k1,1 to sort by only the first field; -k1 means sort according to the whole line. Stable sort (sort -s) can be useful. For example, to sort first by field 2, then secondarily by field 1, you can use sort -k1,1 | sort -s -k2,2. If you ever need to write a tab literal in a command line in Bash (e.g. for the -t argument to sort), press ctrl-v [Tab] or write $'\\t' (the latter is better as you can copy/paste it). The standard tools for patching source code are diff and patch. See also diffstat for summary statistics of a diff and sdiff for a side-by-side diff. Note diff -r works for entire directories. Use diff -r tree1 tree2 | diffstat for a summary of changes. Use vimdiff to compare and edit files. For binary files, use hd, hexdump or xxd for simple hex dumps and bvi, hexedit or biew for binary editing. Also for binary files, strings (plus grep, etc.) lets you find bits of text. For binary diffs (delta compression), use xdelta3. To convert text encodings, try iconv. Or uconv for more advanced use; it supports some advanced Unicode things. For example: # Displays hex codes or actual names of characters (useful for debugging): uconv -f utf-8 -t utf-8 -x '::Any-Hex;' < input.txt uconv -f utf-8 -t utf-8 -x '::Any-Name;' < input.txt # Lowercase and removes all accents (by expanding and dropping them): uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] >; ::Any-NFC;' < input.txt > output.txt To split files into pieces, see split (to split by size) and csplit (to split by a pattern). Date and time: To get the current date and time in the helpful ISO 8601 format, use date -u +\"%Y-%m-%dT%H:%M:%SZ\" (other options are problematic). To manipulate date and time expressions, use dateadd, datediff, strptime etc. from dateutils. Use zless, zmore, zcat, and zgrep to operate on compressed files. File attributes are settable via chattr and offer a lower-level alternative to file permissions. For example, to protect against accidental file deletion the immutable flag: sudo chattr +i /critical/directory/or/file Use getfacl and setfacl to save and restore file permissions. For example: getfacl -R /some/path > permissions.txt setfacl --restore=permissions.txt To create empty files quickly, use truncate (creates sparse file), fallocate (ext4, xfs, btrfs and ocfs2 filesystems), xfs_mkfile (almost any filesystems, comes in xfsprogs package), mkfile (for Unix-like systems like Solaris, Mac OS). System debugging For web debugging, curl and curl -I are handy, or their wget equivalents, or the more modern httpie. To know current cpu/disk status, the classic tools are top (or the better htop), iostat, and iotop. Use iostat -mxz 15 for basic CPU and detailed per-partition disk stats and performance insight. For network connection details, use netstat and ss. For a quick overview of what's happening on a system, dstat is especially useful. For broadest overview with details, use glances. To know memory status, run and understand the output of free and vmstat. In particular, be aware the \"cached\" value is memory held by the Linux kernel as file cache, so effectively counts toward the \"free\" value. Java system debugging is a different kettle of fish, but a simple trick on Oracle's and some other JVMs is that you can run kill -3 <pid> and a full stack trace and heap summary (including generational garbage collection details, which can be highly informative) will be dumped to stderr/logs. The JDK's jps, jstat, jstack, jmap are useful. SJK tools are more advanced. Use mtr as a better traceroute, to identify network issues. For looking at why a disk is full, ncdu saves time over the usual commands like du -sh *. To find which socket or process is using bandwidth, try iftop or nethogs. The ab tool (comes with Apache) is helpful for quick-and-dirty checking of web server performance. For more complex load testing, try siege. For more serious network debugging, wireshark, tshark, or ngrep. Know about strace and ltrace. These can be helpful if a program is failing, hanging, or crashing, and you don't know why, or if you want to get a general idea of performance. Note the profiling option (-c), and the ability to attach to a running process (-p). Use trace child option (-f) to avoid missing important calls. Know about ldd to check shared libraries etc but never run it on untrusted files. Know how to connect to a running process with gdb and get its stack traces. Use /proc. It's amazingly helpful sometimes when debugging live problems. Examples: /proc/cpuinfo, /proc/meminfo, /proc/cmdline, /proc/xxx/cwd, /proc/xxx/exe, /proc/xxx/fd/, /proc/xxx/smaps (where xxx is the process id or pid). When debugging why something went wrong in the past, sar can be very helpful. It shows historic statistics on CPU, memory, network, etc. For deeper systems and performance analyses, look at stap (SystemTap), perf, and sysdig. Check what OS you're on with uname or uname -a (general Unix/kernel info) or lsb_release -a (Linux distro info). Use dmesg whenever something's acting really funny (it could be hardware or driver issues). If you delete a file and it doesn't free up expected disk space as reported by du, check whether the file is in use by a process: lsof | grep deleted | grep \"filename-of-my-big-file\" One-liners A few examples of piecing together commands: It is remarkably helpful sometimes that you can do set intersection, union, and difference of text files via sort/uniq. Suppose a and b are text files that are already uniqued. This is fast, and works on files of arbitrary size, up to many gigabytes. (Sort is not limited by memory, though you may need to use the -T option if /tmp is on a small root partition.) See also the note about LC_ALL above and sort's -u option (left out for clarity below). sort a b | uniq > c # c is a union b sort a b | uniq -d > c # c is a intersect b sort a b b | uniq -u > c # c is set difference a - b Pretty-print two JSON files, normalizing their syntax, then coloring and paginating the result: diff <(jq --sort-keys . < file1.json) <(jq --sort-keys . < file2.json) | colordiff | less -R Use grep . * to quickly examine the contents of all files in a directory (so each line is paired with the filename), or head -100 * (so each file has a heading). This can be useful for directories filled with config settings like those in /sys, /proc, /etc. Summing all numbers in the third column of a text file (this is probably 3X faster and 3X less code than equivalent Python): awk '{ x += $3 } END { print x }' myfile To see sizes/dates on a tree of files, this is like a recursive ls -l but is easier to read than ls -lR: Say you have a text file, like a web server log, and a certain value that appears on some lines, such as an acct_id parameter that is present in the URL. If you want a tally of how many requests for each acct_id: egrep -o 'acct_id=[0-9]+' access.log | cut -d= -f2 | sort | uniq -c | sort -rn To continuously monitor changes, use watch, e.g. check changes to files in a directory with watch -d -n 2 'ls -rtlh | tail' or to network settings while troubleshooting your wifi settings with watch -d -n 2 ifconfig. Run this function to get a random tip from this document (parses Markdown and extracts an item): function taocl() { curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md | sed '/cowsay[.]png/d' | pandoc -f markdown -t html | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v \"(html/body/ul/li[count(p)>0])[$RANDOM mod last()+1]\" | xmlstarlet unesc | fmt -80 | iconv -t US } Obscure but useful expr: perform arithmetic or boolean operations or evaluate regular expressions m4: simple macro processor yes: print a string a lot cal: nice calendar env: run a command (useful in scripts) printenv: print out environment variables (useful in debugging and scripts) look: find English words (or lines in a file) beginning with a string cut, paste and join: data manipulation fmt: format text paragraphs pr: format text into pages/columns fold: wrap lines of text column: format text fields into aligned, fixed-width columns or tables expand and unexpand: convert between tabs and spaces nl: add line numbers seq: print numbers bc: calculator factor: factor integers gpg: encrypt and sign files toe: table of terminfo entries nc: network debugging and data transfer socat: socket relay and tcp port forwarder (similar to netcat) slurm: network traffic visualization dd: moving data between files or devices file: identify type of a file tree: display directories and subdirectories as a nesting tree; like ls but recursive stat: file info time: execute and time a command timeout: execute a command for specified amount of time and stop the process when the specified amount of time completes. lockfile: create semaphore file that can only be removed by rm -f logrotate: rotate, compress and mail logs. watch: run a command repeatedly, showing results and/or highlighting changes when-changed: runs any command you specify whenever it sees file changed. See inotifywait and entr as well. tac: print files in reverse comm: compare sorted files line by line strings: extract text from binary files tr: character translation or manipulation iconv or uconv: conversion for text encodings split and csplit: splitting files sponge: read all input before writing it, useful for reading from then writing to the same file, e.g., grep -v something some-file | sponge some-file units: unit conversions and calculations; converts furlongs per fortnight to twips per blink (see also /usr/share/units/definitions.units) apg: generates random passwords xz: high-ratio file compression ldd: dynamic library info nm: symbols from object files ab or wrk: benchmarking web servers strace: system call debugging mtr: better traceroute for network debugging cssh: visual concurrent shell rsync: sync files and folders over SSH or in local file system wireshark and tshark: packet capture and network debugging ngrep: grep for the network layer host and dig: DNS lookups lsof: process file descriptor and socket info dstat: useful system stats glances: high level, multi-subsystem overview iostat: Disk usage stats mpstat: CPU usage stats vmstat: Memory usage stats htop: improved version of top last: login history w: who's logged on id: user/group identity info sar: historic system stats iftop or nethogs: network utilization by socket or process ss: socket statistics dmesg: boot and system error messages sysctl: view and configure Linux kernel parameters at run time hdparm: SATA/ATA disk manipulation/performance lsblk: list block devices: a tree view of your disks and disk partitions lshw, lscpu, lspci, lsusb, dmidecode: hardware information, including CPU, BIOS, RAID, graphics, devices, etc. lsmod and modinfo: List and show details of kernel modules. fortune, ddate, and sl: um, well, it depends on whether you consider steam locomotives and Zippy quotations \"useful\" macOS only These are items relevant only on macOS. Package management with brew (Homebrew) and/or port (MacPorts). These can be used to install on macOS many of the above commands. Copy output of any command to a desktop app with pbcopy and paste input from one with pbpaste. To enable the Option key in macOS Terminal as an alt key (such as used in the commands above like alt-b, alt-f, etc.), open Preferences -> Profiles -> Keyboard and select \"Use Option as Meta key\". To open a file with a desktop app, use open or open -a /Applications/Whatever.app. Spotlight: Search files with mdfind and list metadata (such as photo EXIF info) with mdls. Be aware macOS is based on BSD Unix, and many commands (for example ps, ls, tail, awk, sed) have many subtle variations from Linux, which is largely influenced by System V-style Unix and GNU tools. You can often tell the difference by noting a man page has the heading \"BSD General Commands Manual.\" In some cases GNU versions can be installed, too (such as gawk and gsed for GNU awk and sed). If writing cross-platform Bash scripts, avoid such commands (for example, consider Python or perl) or test carefully. To get macOS release information, use sw_vers. Windows only These items are relevant only on Windows. Ways to obtain Unix tools under Windows Access the power of the Unix shell under Microsoft Windows by installing Cygwin. Most of the things described in this document will work out of the box. On Windows 10, you can use Windows Subsystem for Linux (WSL), which provides a familiar Bash environment with Unix command line utilities. If you mainly want to use GNU developer tools (such as GCC) on Windows, consider MinGW and its MSYS package, which provides utilities such as bash, gawk, make and grep. MSYS doesn't have all the features compared to Cygwin. MinGW is particularly useful for creating native Windows ports of Unix tools. Another option to get Unix look and feel under Windows is Cash. Note that only very few Unix commands and command-line options are available in this environment. Useful Windows command-line tools You can perform and script most Windows system administration tasks from the command line by learning and using wmic. Native command-line Windows networking tools you may find useful include ping, ipconfig, tracert, and netstat. You can perform many useful Windows tasks by invoking the Rundll32 command. Cygwin tips and tricks Install additional Unix programs with the Cygwin's package manager. Use mintty as your command-line window. Access the Windows clipboard through /dev/clipboard. Run cygstart to open an arbitrary file through its registered application. Access the Windows registry with regtool. Note that a C:\\ Windows drive path becomes /cygdrive/c under Cygwin, and that Cygwin's / appears under C:\\cygwin on Windows. Convert between Cygwin and Windows-style file paths with cygpath. This is most useful in scripts that invoke Windows programs. More resources awesome-shell: A curated list of shell tools and resources. awesome-osx-command-line: A more in-depth guide for the macOS command line. Strict mode for writing better shell scripts. shellcheck: A shell script static analysis tool. Essentially, lint for bash/sh/zsh. Filenames and Pathnames in Shell: The sadly complex minutiae on how to handle filenames correctly in shell scripts. Data Science at the Command Line: More commands and tools helpful for doing data science, from the book of the same name Disclaimer With the exception of very small tasks, code is written so others can read it. With power comes responsibility. The fact you can do something in Bash doesn't necessarily mean you should! ;) License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. ",
        "_version_": 1718536485114740736
      },
      {
        "story_id": 20534211,
        "story_author": "javinpaul",
        "story_descendants": 6,
        "story_score": 68,
        "story_time": "2019-07-26T12:26:25Z",
        "story_title": "Milkman: An Extensible Alternative to Postman in JavaFX",
        "search": [
          "Milkman: An Extensible Alternative to Postman in JavaFX",
          "https://github.com/warmuuh/milkman",
          "Milkman is heavily inspired by Postman. But I got sick of all those electron-based applications that need ages and loads of memory to start up. Therefore i created a JavaFx-based workbench for crafting requests / responses. It is not limited to e.g. http (or more specifically rest) requests. Due to nearly everything being a plugin, other things are possible, like database-requests or GRPC, GraphQl, etc... Download Download latest version in Release Section. Or download Nightly version for latest changes. Download plugin archive as well, if you want to use any of those plugins. unzip everything into the same directory and start application via either the supplied start-scripts or milkman.exe Install via Chocolatey (Win) To install just Milkman run Choco install milkman To install a plugin run Choco install milkman-<plugin name> ex. Choco install milkman-explore Install Milkman and all plugins available run Choco install milkman-plugins Milkman will be installed under C:\\tools\\milkman. Install via Homebrew (MacOs) To install, run brew install --cask milkman. All plugins are included. To update, run brew upgrade --cask Changes latest changes can be seen in the Changelog Features Everything is a plugin: Request-types (e.g. Http Request), request-aspects (e.g. Headers, Body, etc), editors for request aspects (e.g. table-based editors for headers), importers, whatever it is, you can extend it. The core application only handles Workspaces with Environments, Collections, Requests and their aspects. Http Request Plugin: Several plugins are provided already that extend the core application to be a replacement for postman. Crafting and Executing Http/Rest requests with json highlighting. Support Proxy-server configuration and SSE. Grpc Plugin: support of Grpc Services, Server Reflection and Streaming Test Runner: run several requests as separate test-scenario JavaFX Application: as in: fast (compared to electron at least :D) and skinn-able (you can extend milkman with your own themes using simple CSS). Commandline Interface: there is a command line interface for Milkman which allows to edit/execute requests on your command line. Slack Command: you can use /milkman <privatebin-url> in slack to share requests in a better way. More Info. Some more details of the core application features, such as hotkeys etc. Existing Plugins: Note: see respective plugin folder for more details Http Request Plugin: (included in main distribution) The Http request plugin packaged with the release contains all means to do http request as well as import collections, environments or dumps from postman. To migrate from postman, just export a dump-file from postman and import it by pasting its content into the Postman (v2.1) Dump-Importer. The Http Request Plugin also comes with Proxy-support. Some proxies require credentials and support for supplying BASIC proxy credentials is built into the plugin. See Options-page to activate that as it is off by default. Additionally, the plugin supports chunked responses, enabling testing of Server-sent events (SSE). Graphql Plugin Simple Graphql Plugin to craft GraphQL requests. No Auto-complete or schema valiadtion included. Grpc Plugin Grpc plugin with server-reflection support and streaming-support. Can also query via proto-defintion. JDBC Plugin This plugin introduces SQL capability to milkman. You can query SQL databases via milkman as well, using JDBC drivers. Cassandra Plugin: Allows to execute CQL queries for given cassandra databases. Git Team Synchronization Plugin This plugin allows to synchronize workspaces via Git (using Differential Synchronization). Teams can use this plugin to have a shared workspace that synchronizes between members. Explore Plugin: This plugin extends Rest-responses by adding an Explore-Tab where you can use JMesPath queries against a JSON response. Scripting Plugin: Extends requests by executing a script after request execution. This allows to e.g. set environment variables based on results of json. PrivateBin Sharing Plugin Adds capability to share requests via PrivateBin, including Burn After Reading feature Note Plugin: This is a sample plugin that allows to add arbitrary description to every request. Serves as a starting point for learning to extend milkman. Test Plugin A testrunner plugin that allows to run multiple requests in sequence and verify the results Auth Plugin An OAuth Plugin for managing oauth credentials and transparent refreshing of tokens Websocket Plugin A websocket plugin for interacting with websocket endpoints Showcase Plugins No client fits all, so you are encouraged to write your own plugins to e.g. add headers that are necessary for your internal service structures or add importers for internal service registries. A sample plugin was provided that extends all requests with a Note tab so you can add some description to any kind of requests. More details about developing plugins can be found in the plugin development guide. Installation: all plugins are to be copied to the /plugins folder Changes latest changes can be seen in the (Changelog)[changelog.md] "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/warmuuh/milkman",
        "comments.comment_id": [20538301, 20541853],
        "comments.comment_author": ["lunias", "pjmlp"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-07-26T21:02:43Z",
          "2019-07-27T13:03:48Z"
        ],
        "comments.comment_text": [
          "Cool to see someone using JavaFX and not Electron. I quite like it and used it for a few card games and stuff.",
          "Congratulations on picking up JavaFX for doing it."
        ],
        "id": "60746db6-dcc2-4f3c-af42-cb693f0c3509",
        "url_text": "Milkman is heavily inspired by Postman. But I got sick of all those electron-based applications that need ages and loads of memory to start up. Therefore i created a JavaFx-based workbench for crafting requests / responses. It is not limited to e.g. http (or more specifically rest) requests. Due to nearly everything being a plugin, other things are possible, like database-requests or GRPC, GraphQl, etc... Download Download latest version in Release Section. Or download Nightly version for latest changes. Download plugin archive as well, if you want to use any of those plugins. unzip everything into the same directory and start application via either the supplied start-scripts or milkman.exe Install via Chocolatey (Win) To install just Milkman run Choco install milkman To install a plugin run Choco install milkman-<plugin name> ex. Choco install milkman-explore Install Milkman and all plugins available run Choco install milkman-plugins Milkman will be installed under C:\\tools\\milkman. Install via Homebrew (MacOs) To install, run brew install --cask milkman. All plugins are included. To update, run brew upgrade --cask Changes latest changes can be seen in the Changelog Features Everything is a plugin: Request-types (e.g. Http Request), request-aspects (e.g. Headers, Body, etc), editors for request aspects (e.g. table-based editors for headers), importers, whatever it is, you can extend it. The core application only handles Workspaces with Environments, Collections, Requests and their aspects. Http Request Plugin: Several plugins are provided already that extend the core application to be a replacement for postman. Crafting and Executing Http/Rest requests with json highlighting. Support Proxy-server configuration and SSE. Grpc Plugin: support of Grpc Services, Server Reflection and Streaming Test Runner: run several requests as separate test-scenario JavaFX Application: as in: fast (compared to electron at least :D) and skinn-able (you can extend milkman with your own themes using simple CSS). Commandline Interface: there is a command line interface for Milkman which allows to edit/execute requests on your command line. Slack Command: you can use /milkman <privatebin-url> in slack to share requests in a better way. More Info. Some more details of the core application features, such as hotkeys etc. Existing Plugins: Note: see respective plugin folder for more details Http Request Plugin: (included in main distribution) The Http request plugin packaged with the release contains all means to do http request as well as import collections, environments or dumps from postman. To migrate from postman, just export a dump-file from postman and import it by pasting its content into the Postman (v2.1) Dump-Importer. The Http Request Plugin also comes with Proxy-support. Some proxies require credentials and support for supplying BASIC proxy credentials is built into the plugin. See Options-page to activate that as it is off by default. Additionally, the plugin supports chunked responses, enabling testing of Server-sent events (SSE). Graphql Plugin Simple Graphql Plugin to craft GraphQL requests. No Auto-complete or schema valiadtion included. Grpc Plugin Grpc plugin with server-reflection support and streaming-support. Can also query via proto-defintion. JDBC Plugin This plugin introduces SQL capability to milkman. You can query SQL databases via milkman as well, using JDBC drivers. Cassandra Plugin: Allows to execute CQL queries for given cassandra databases. Git Team Synchronization Plugin This plugin allows to synchronize workspaces via Git (using Differential Synchronization). Teams can use this plugin to have a shared workspace that synchronizes between members. Explore Plugin: This plugin extends Rest-responses by adding an Explore-Tab where you can use JMesPath queries against a JSON response. Scripting Plugin: Extends requests by executing a script after request execution. This allows to e.g. set environment variables based on results of json. PrivateBin Sharing Plugin Adds capability to share requests via PrivateBin, including Burn After Reading feature Note Plugin: This is a sample plugin that allows to add arbitrary description to every request. Serves as a starting point for learning to extend milkman. Test Plugin A testrunner plugin that allows to run multiple requests in sequence and verify the results Auth Plugin An OAuth Plugin for managing oauth credentials and transparent refreshing of tokens Websocket Plugin A websocket plugin for interacting with websocket endpoints Showcase Plugins No client fits all, so you are encouraged to write your own plugins to e.g. add headers that are necessary for your internal service structures or add importers for internal service registries. A sample plugin was provided that extends all requests with a Note tab so you can add some description to any kind of requests. More details about developing plugins can be found in the plugin development guide. Installation: all plugins are to be copied to the /plugins folder Changes latest changes can be seen in the (Changelog)[changelog.md] ",
        "_version_": 1718536504853135360
      },
      {
        "story_id": 19589614,
        "story_author": "hu3",
        "story_descendants": 132,
        "story_score": 218,
        "story_time": "2019-04-06T06:42:14Z",
        "story_title": "Lessons learned porting 50k loc from Java to Go",
        "search": [
          "Lessons learned porting 50k loc from Java to Go",
          "https://blog.kowalczyk.info/article/19f2fe97f06a47c3b1f118fd06851fad/lessons-learned-porting-50k-loc-from-java-to-go.html",
          "I was contracted to port a large Java code base to Go. The code in question is a Java client for RavenDB, a NoSQL JSON document database. Code with tests was around 50 thousand lines. The result of the port is a Go client. This article describes what I've learn in the process. Testing, code coverage Large projects benefit greatly from automated testing and tracking code coverage. I used TravisCI and AppVeyor for testing. Codecov.io for code coverage. There are many other services. I used both AppVeyor and TravisCI because a year ago Travis didn't have Windows support and AppVeyor didn't have Linux support. Today if I was settings this up from scratch, I would stick with just AppVeyor, as it can now do both Linux and Windows testing and the future of TravisCI is murky, after it was acquired by private equity firm and reportedly fired the original dev team. Codecov is barely adequate. For Go, they count non-code lines (comments etc.) as not executed. It's impossible to get 100% code coverage as reported by the tool. Coveralls seems to have the same problem. It's better than nothing but there's an opportunity to do things better, especially for Go programs. Go's race detector is great Parts of the code use concurrency and it's really easy to get concurrency wrong. Go provides race detector that can be enabled with -race flag during compilation. It slows down the program but additional checks can detect if you're concurrently modifying the same memory location. I always run tests with -race enabled and it alerted me to numerous races, which allowed me to fix them promptly. Building custom tools for testing In a project that big it's impossible to verify correctness by inspection. Too much code to hold in your head at once. When a test fails, it can be a challenge to figure out why just from the information in the test failure. Database client driver talks to RavenDB database server over HTTP using JSON to encode commands and results. When porting Java tests to Go, it was very useful to be able to capture the HTTP traffic between Java client and server and compare it with HTTP traffic generated by Go port. I built custom tools to help me do that. For capturing HTTP traffic in Java client, I built a logging HTTP proxy in Go and directed Java client to use that HTTP proxy. For Go client, I built a hook in the library that allows to intercept HTTP requests. I used it to log the traffic to a file. I was then able to compare HTTP traffic generated by Java client to traffic generated by my Go port and spot the differences. Porting process You can't just start porting 50 thousand lines of code in random order. Without testing and validating after every little step I'm sure I would be defeated by complexity. I was new to RavenDB and Java code base. My first step was to get a high-level understanding how Java code works. At the core the client talks to the server via HTTP protocol. I captured the traffic, looked at it and wrote the simplest Go code to talk the server. When that was working it gave me confidence I'll be able to replicate the functionality. My first milestone was to port enough code to be able to port the simplest Java test. I used a combination of bottom-up and top-down approach. Bottom-up part is where I identified the code at the bottom of call chain responsible for sending commands to the server and parsing responses and ported those. The top-down part is where I stepped through the test I was porting to identify which parts of the code need to be ported to implement that part. After successfully porting the first step, the rest of the work was porting one test at a time, also porting all the necessary code needed to make the test work. After the tests were ported and passing, I did improvements to make the code more Go-ish. I believe that this step-by-step approach was crucial to completing the work. Psychologically, when faced with a year-long project, it's important to have smaller, intermediate milestones. Hitting those kept me motivated. Keeping the code compiling, running and passing tests at all times is also good. Allowing bugs to accumulate can make it very hard to fix them when you finally get to it. Challenges of porting Java to Go The objective of the port was to keep it as close as possible to Java code base, as it needs to be kept in sync with Java changes in the future. I'm somewhat surprised how much code I ported in a line-by-line fashion. The most time consuming part of the port was reversing the order of variable declaration, from Java's type name to Go's name type. I wish there was a tool that would do that part for me. String vs. string In Java, String is an object that really is a reference (a pointer). As a result, a string can be null. In Go string is a value type. It can't be nil, only empty. It wasn't a big deal and most of the time I could mechanically replace null with \"\". Errors vs. exceptions Java uses exceptions to communicate errors. Go returns values of error interface. Porting wasn't difficult but it did require changing lots of function signatures to return error values and propagate them up the call stack. Generics Go doesn't have them (yet). Porting generic APIs was the biggest challenge. Here's an example of a generic method in Java: public <T> T load(Class<T> clazz, String id) { And the caller: Foo foo = load(Foo.class, \"id\") In Go, I used two strategies. One is to use interface{}, which combines value and its type, similar to object in Java. This is not preferred approach. While it works, operating on interface{} is clumsy for the user of the library. In some cases I was able to use reflection and the above code was ported as: func Load(result interface{}, id string) error I could use reflection to query type of result and create values of that type from JSON document. And the caller side: var result *Foo err := Load(&result, \"id\") Function overloading Go doesn't have it (and most likely will never have it). I can't say I found a good solution to port those. In some cases overloading was used to create shorter helpers: void foo(int a, String b) {} void foo(int a) { foo(a, null); } Sometimes I would just drop the shorter helper. Sometimes I would write 2 functions: func foo(a int) {} func fooWithB(a int, b string) {} When number of potential arguments was large I would sometimes do: type FooArgs struct { A int B string } func foo(args *FooArgs) { } Inheritance Go is not especially object-oriented and doesn't have inheritance. Simple cases of inheritance can be ported with embedding. class B : A { } Can sometimes be ported as: type A struct { } type B struct { A } We've embedded A inside B, so B inherit all the methods and fields of A. It doesn't work for virtual functions. There is no good way to directly port code that uses virtual functions. One option to emulate virtual function is to use embedding of structs and function pointers. This essentially re-implements virtual table that Java gives you for free as part of object implementation. Another option is to write a stand-alone function that dispatches the right function for a given type by using type switch. Interfaces Both Java and Go have interfaces but they are different things, like apples and salami. A few times I did create a Go interface type that replicated Java interface. In more cases I dropped interfaces and instead exposed concrete structs in the API. Circular imports between packages Java allows circular imports between packages. Go does not. As a result I was not able to replicate the package structure of Java code in my port. For simplicity I went with a single package. Not ideal, because it ended up being very large package. So large, in fact, that Go 1.10 couldn't handle so many source files in a single package on Windows. Luckily it was fixed in Go 1.11. Private, public, protected Go's designers are under-appreciated. Their ability to simplify concepts is unmatched and access control is one example of that. Other languages gravitate to fine-grained access control: public, private, protected specified with the smallest possible granularity (per class field and method). As a result a library implementing some functionality has the same access to other classes in the same library as external code using that library. Go simplified that by only having public vs. private and scoping access to package level. That makes more sense. When I write a library to, say, parse markdown, I don't want to expose internals of the implementation to users of the library. But hiding those internals from myself is counter-productive. Java programmers noticed that issue and sometimes use an interface as a hack to fix over-exposed classes. By returning an interface instead of a a concrete class, you can hide some of the public APIs available to direct users of the class. Concurrency Go's concurrency is simply the best and a built-in race detector is of great help in repelling concurrency bugs. That being said, in my first porting pass I went with emulating Java APIs. For example, I implemented a facsimile of Java's CompletableFuture class. Only after the code was working I would re-structure it to be more idiomatic Go. Fluent function chaining RavenDB has very sophisticated querying capabilities. Java client uses method chaining for building queries: List<ReduceResult> results = session.query(User.class) .groupBy(\"name\") .selectKey() .selectCount() .orderByDescending(\"count\") .ofType(ReduceResult.class) .toList(); This only works in languages that communicate errors via exceptions. When a function additionally returns an error, it's no longer possible to chain it like that. To replicate chaining in Go I used a \"stateful error\" approach: type Query struct { err error } func (q *Query) WhereEquals(field string, val interface{}) *Query { if q.err != nil { return q } // logic that might set q.err return q } func (q *Query) GroupBy(field string) *Query { if q.err != nil { return q } // logic that might set q.err return q } func (q *Query) Execute(result inteface{}) error { if q.err != nil { return q.err } // do logic } This can be chained: var result *Foo err := NewQuery().WhereEquals(\"Name\", \"Frank\").GroupBy(\"Age\").Execute(&result) JSON marshaling Java doesn't have a built-in marshaling and the client uses Jackson JSON library. Go has JSON support in standard library but it doesn't provide as many hooks for tweaking marshaling process. I didn't try to match all of Java's functionality as what is provided by Go's built-in JSON support seems to be flexible enough. Go code is shorter This is not so much a property of Java but the culture which dictates what is considered an idiomatic code. In Java setter and getter methods are common. As a result, Java code: class Foo { private int bar; public void setBar(int bar) { this.bar = bar; } public int getBar() { return this.bar; } } ends up in Go as: type Foo struct { Bar int } 3 lines vs. 11 lines. It does add up when you have a lot of classes with lots of members. Most other code ends up being of equivalent length. Notion for organizing the work I'm a heavy user of Notion.so. Simplifying a lot, Notion is a hierarchical note taking application. Think a cross of Evernote and a wiki, exquisitely designed and implemented by top notch software designers. Here's how I used Notion to organize my work on Go port: Here's what's there: not shown above, I have a page that is a calendar view where I take short notes about what I work on on a given day and how much time I spent. This is important information since it was a hourly contract. Thanks to those notes I know that I spent 601 hours over 11 months clients like to know the progress. I had a page for each month were I summarized the work done like this: Those pages were shared with the client. A short-term todo list helps when starting work each day: I even managed invoices as Notion pages and used \"Export to PDF\" function to generate PDF version of the invoice Additional resources I've provided some additional commentary in response to questions: in Hacker News discussion in /r/golang discussion Other material: if you need a NoSQL, JSON document database, give RavenDB a try. It's chock full of advanced features if you're programming in Go, try a free Essential Go programming book if you're interested in Notion, I'm world's most advanced user of Notion: I reverse engineered Notion API I wrote an unofficial Go library for Notion API all content on this website is written in Notion and published with my custom toolchain "
        ],
        "story_type": "Normal",
        "url_raw": "https://blog.kowalczyk.info/article/19f2fe97f06a47c3b1f118fd06851fad/lessons-learned-porting-50k-loc-from-java-to-go.html",
        "comments.comment_id": [19589910, 19589963],
        "comments.comment_author": ["indogooner", "ivan_gammel"],
        "comments.comment_descendants": [6, 4],
        "comments.comment_time": [
          "2019-04-06T08:51:52Z",
          "2019-04-06T09:14:06Z"
        ],
        "comments.comment_text": [
          "The article is more about the process and not the motivation for this port. What problem with Java/JVM caused the organization to commission this expensive porting exercise? And what are the benefits they have achieved after the port.",
          "It is very interesting to see in such projects how they expose the weakness of certain Java idioms. The mentioned JavaBeans getters and setters are obsolete pattern, for which in most cases there’s no good reason to keep it in the code. Java ecosystem is probably the richest one on the design patterns, some of which receive „anti“ prefix over the years (e.g. self-contained singleton). At the same time new languages explore and verify the ideas and coding styles that might be worth to borrow and make mainstream. Porting exercise can be such source of new best practices pushing us to rethink what is worth keeping and what is pointless overengineering."
        ],
        "id": "24e0de6a-8a00-46fd-ac2a-1cf12aa6de41",
        "url_text": "I was contracted to port a large Java code base to Go. The code in question is a Java client for RavenDB, a NoSQL JSON document database. Code with tests was around 50 thousand lines. The result of the port is a Go client. This article describes what I've learn in the process. Testing, code coverage Large projects benefit greatly from automated testing and tracking code coverage. I used TravisCI and AppVeyor for testing. Codecov.io for code coverage. There are many other services. I used both AppVeyor and TravisCI because a year ago Travis didn't have Windows support and AppVeyor didn't have Linux support. Today if I was settings this up from scratch, I would stick with just AppVeyor, as it can now do both Linux and Windows testing and the future of TravisCI is murky, after it was acquired by private equity firm and reportedly fired the original dev team. Codecov is barely adequate. For Go, they count non-code lines (comments etc.) as not executed. It's impossible to get 100% code coverage as reported by the tool. Coveralls seems to have the same problem. It's better than nothing but there's an opportunity to do things better, especially for Go programs. Go's race detector is great Parts of the code use concurrency and it's really easy to get concurrency wrong. Go provides race detector that can be enabled with -race flag during compilation. It slows down the program but additional checks can detect if you're concurrently modifying the same memory location. I always run tests with -race enabled and it alerted me to numerous races, which allowed me to fix them promptly. Building custom tools for testing In a project that big it's impossible to verify correctness by inspection. Too much code to hold in your head at once. When a test fails, it can be a challenge to figure out why just from the information in the test failure. Database client driver talks to RavenDB database server over HTTP using JSON to encode commands and results. When porting Java tests to Go, it was very useful to be able to capture the HTTP traffic between Java client and server and compare it with HTTP traffic generated by Go port. I built custom tools to help me do that. For capturing HTTP traffic in Java client, I built a logging HTTP proxy in Go and directed Java client to use that HTTP proxy. For Go client, I built a hook in the library that allows to intercept HTTP requests. I used it to log the traffic to a file. I was then able to compare HTTP traffic generated by Java client to traffic generated by my Go port and spot the differences. Porting process You can't just start porting 50 thousand lines of code in random order. Without testing and validating after every little step I'm sure I would be defeated by complexity. I was new to RavenDB and Java code base. My first step was to get a high-level understanding how Java code works. At the core the client talks to the server via HTTP protocol. I captured the traffic, looked at it and wrote the simplest Go code to talk the server. When that was working it gave me confidence I'll be able to replicate the functionality. My first milestone was to port enough code to be able to port the simplest Java test. I used a combination of bottom-up and top-down approach. Bottom-up part is where I identified the code at the bottom of call chain responsible for sending commands to the server and parsing responses and ported those. The top-down part is where I stepped through the test I was porting to identify which parts of the code need to be ported to implement that part. After successfully porting the first step, the rest of the work was porting one test at a time, also porting all the necessary code needed to make the test work. After the tests were ported and passing, I did improvements to make the code more Go-ish. I believe that this step-by-step approach was crucial to completing the work. Psychologically, when faced with a year-long project, it's important to have smaller, intermediate milestones. Hitting those kept me motivated. Keeping the code compiling, running and passing tests at all times is also good. Allowing bugs to accumulate can make it very hard to fix them when you finally get to it. Challenges of porting Java to Go The objective of the port was to keep it as close as possible to Java code base, as it needs to be kept in sync with Java changes in the future. I'm somewhat surprised how much code I ported in a line-by-line fashion. The most time consuming part of the port was reversing the order of variable declaration, from Java's type name to Go's name type. I wish there was a tool that would do that part for me. String vs. string In Java, String is an object that really is a reference (a pointer). As a result, a string can be null. In Go string is a value type. It can't be nil, only empty. It wasn't a big deal and most of the time I could mechanically replace null with \"\". Errors vs. exceptions Java uses exceptions to communicate errors. Go returns values of error interface. Porting wasn't difficult but it did require changing lots of function signatures to return error values and propagate them up the call stack. Generics Go doesn't have them (yet). Porting generic APIs was the biggest challenge. Here's an example of a generic method in Java: public <T> T load(Class<T> clazz, String id) { And the caller: Foo foo = load(Foo.class, \"id\") In Go, I used two strategies. One is to use interface{}, which combines value and its type, similar to object in Java. This is not preferred approach. While it works, operating on interface{} is clumsy for the user of the library. In some cases I was able to use reflection and the above code was ported as: func Load(result interface{}, id string) error I could use reflection to query type of result and create values of that type from JSON document. And the caller side: var result *Foo err := Load(&result, \"id\") Function overloading Go doesn't have it (and most likely will never have it). I can't say I found a good solution to port those. In some cases overloading was used to create shorter helpers: void foo(int a, String b) {} void foo(int a) { foo(a, null); } Sometimes I would just drop the shorter helper. Sometimes I would write 2 functions: func foo(a int) {} func fooWithB(a int, b string) {} When number of potential arguments was large I would sometimes do: type FooArgs struct { A int B string } func foo(args *FooArgs) { } Inheritance Go is not especially object-oriented and doesn't have inheritance. Simple cases of inheritance can be ported with embedding. class B : A { } Can sometimes be ported as: type A struct { } type B struct { A } We've embedded A inside B, so B inherit all the methods and fields of A. It doesn't work for virtual functions. There is no good way to directly port code that uses virtual functions. One option to emulate virtual function is to use embedding of structs and function pointers. This essentially re-implements virtual table that Java gives you for free as part of object implementation. Another option is to write a stand-alone function that dispatches the right function for a given type by using type switch. Interfaces Both Java and Go have interfaces but they are different things, like apples and salami. A few times I did create a Go interface type that replicated Java interface. In more cases I dropped interfaces and instead exposed concrete structs in the API. Circular imports between packages Java allows circular imports between packages. Go does not. As a result I was not able to replicate the package structure of Java code in my port. For simplicity I went with a single package. Not ideal, because it ended up being very large package. So large, in fact, that Go 1.10 couldn't handle so many source files in a single package on Windows. Luckily it was fixed in Go 1.11. Private, public, protected Go's designers are under-appreciated. Their ability to simplify concepts is unmatched and access control is one example of that. Other languages gravitate to fine-grained access control: public, private, protected specified with the smallest possible granularity (per class field and method). As a result a library implementing some functionality has the same access to other classes in the same library as external code using that library. Go simplified that by only having public vs. private and scoping access to package level. That makes more sense. When I write a library to, say, parse markdown, I don't want to expose internals of the implementation to users of the library. But hiding those internals from myself is counter-productive. Java programmers noticed that issue and sometimes use an interface as a hack to fix over-exposed classes. By returning an interface instead of a a concrete class, you can hide some of the public APIs available to direct users of the class. Concurrency Go's concurrency is simply the best and a built-in race detector is of great help in repelling concurrency bugs. That being said, in my first porting pass I went with emulating Java APIs. For example, I implemented a facsimile of Java's CompletableFuture class. Only after the code was working I would re-structure it to be more idiomatic Go. Fluent function chaining RavenDB has very sophisticated querying capabilities. Java client uses method chaining for building queries: List<ReduceResult> results = session.query(User.class) .groupBy(\"name\") .selectKey() .selectCount() .orderByDescending(\"count\") .ofType(ReduceResult.class) .toList(); This only works in languages that communicate errors via exceptions. When a function additionally returns an error, it's no longer possible to chain it like that. To replicate chaining in Go I used a \"stateful error\" approach: type Query struct { err error } func (q *Query) WhereEquals(field string, val interface{}) *Query { if q.err != nil { return q } // logic that might set q.err return q } func (q *Query) GroupBy(field string) *Query { if q.err != nil { return q } // logic that might set q.err return q } func (q *Query) Execute(result inteface{}) error { if q.err != nil { return q.err } // do logic } This can be chained: var result *Foo err := NewQuery().WhereEquals(\"Name\", \"Frank\").GroupBy(\"Age\").Execute(&result) JSON marshaling Java doesn't have a built-in marshaling and the client uses Jackson JSON library. Go has JSON support in standard library but it doesn't provide as many hooks for tweaking marshaling process. I didn't try to match all of Java's functionality as what is provided by Go's built-in JSON support seems to be flexible enough. Go code is shorter This is not so much a property of Java but the culture which dictates what is considered an idiomatic code. In Java setter and getter methods are common. As a result, Java code: class Foo { private int bar; public void setBar(int bar) { this.bar = bar; } public int getBar() { return this.bar; } } ends up in Go as: type Foo struct { Bar int } 3 lines vs. 11 lines. It does add up when you have a lot of classes with lots of members. Most other code ends up being of equivalent length. Notion for organizing the work I'm a heavy user of Notion.so. Simplifying a lot, Notion is a hierarchical note taking application. Think a cross of Evernote and a wiki, exquisitely designed and implemented by top notch software designers. Here's how I used Notion to organize my work on Go port: Here's what's there: not shown above, I have a page that is a calendar view where I take short notes about what I work on on a given day and how much time I spent. This is important information since it was a hourly contract. Thanks to those notes I know that I spent 601 hours over 11 months clients like to know the progress. I had a page for each month were I summarized the work done like this: Those pages were shared with the client. A short-term todo list helps when starting work each day: I even managed invoices as Notion pages and used \"Export to PDF\" function to generate PDF version of the invoice Additional resources I've provided some additional commentary in response to questions: in Hacker News discussion in /r/golang discussion Other material: if you need a NoSQL, JSON document database, give RavenDB a try. It's chock full of advanced features if you're programming in Go, try a free Essential Go programming book if you're interested in Notion, I'm world's most advanced user of Notion: I reverse engineered Notion API I wrote an unofficial Go library for Notion API all content on this website is written in Notion and published with my custom toolchain ",
        "_version_": 1718536467950600192
      },
      {
        "story_id": 21043521,
        "story_author": "goostavos",
        "story_descendants": 88,
        "story_score": 531,
        "story_time": "2019-09-22T19:49:56Z",
        "story_title": "Using Gooey as a Universal Front End for Any Language or CLI Application",
        "search": [
          "Using Gooey as a Universal Front End for Any Language or CLI Application",
          "https://chriskiehl.com/article/gooey-as-a-universal-frontend",
          "Published: 2019-09-22Gooey is a tool for transforming command line interfaces into beautiful desktop applications. It can be used as the frontend client for any language or program. Whether you've built your application in Java, Node, or Haskell, or you just want to put a pretty interface on an existing tool like FFMPEG, Gooey can be used to create a fast, practically free UI with just a little bit of Python (about 20 lines!). Don't want to write no stinkin' Python? Good news! Gooey is actually controlled entirely by a plain ol' JSON file. We're only using Python here because it has convenient bindings for generating the JSON. Contribute bindings for your language! To show how this all fits together, and that it really works for anything, we're going to walk through building a graphical interface to one of my favorite tools of all time: FFMPEG. These steps apply to anything, though! You could swap out FFMPEG for a .jar you've written, or an arbitrary windows .exe, an OSX .app bundle, or anything on linux that's executable! Why wrap up another program at all? While FFMPEG is amazing, it is anything but friendly. Borderline hostile, in fact. The learning curve is basically just a brick wall. To make it friendly for our theoretical end user, we're going to expose a subset of it as a simple, familiar GUI in which they can point-and-click their way to success. Additionally, since FFMPEG has a staggeringly deep suite of video processing tools, to keep things simple we're going to expose just a very small subset of its functionality. We're going to build a UI in which users can specify 1. an input video file, 2. a timestamp where they'd like to extract a screenshot, and finally 3. the output filename where they want the screenshot saved.In short, it will transform a \"scary\" terminal command line this:ffmpeg -ss 300 -i /home/user/path/to/movie.mkv -frames:v 1 outfile.png into an easy to use desktop application that you could hand over to users.Getting StartedTo play along at home, grab the appropriate FFMPEG binary for your OS here and install it.If all has gone well, you should be able to invoke it from the command line and get back some useful version infoffmpeg -version With that working, let's get onto building the UI side of things.Step 1. Installing PythonIf you're coming from other languages / backgrounds, step one will be installing Python. You can find an appropriate guide for your OS here. If you're on Windows, the shortest path to done boils down to visiting python.org, clicking the big \"Download\" button, and then double clicking the downloaded file.Step 2. Create a directory and source file for the projectWe'll need a single python file to describe the GUI we want to build. To stay organized, we'll put any files we create in a directory called myproject.mkdir myproject cd myproject Inside here we'll create the source file we need.touch main.py Step 3. Installing GooeyBefore we start writing the little bits of code required, we have to install Gooey. In Python land, this is done via a tool called pip. This should have been installed automatically along side Python in step 1.pip install Gooey Note: you would normally install Python dependencies inside of a virtual environment, but we're ignoring that detail for ease of example. Step 4. Let's make a UI!Open the main.py file we created in step 2 and paste in the following.from gooey import Gooey, GooeyParser @Gooey(target=\"ffmpeg\", program_name='Frame Extraction v1.0', suppress_gooey_flag=True) def main(): parser = GooeyParser(description=\"Extracting frames from a movie using FFMPEG\") ffmpeg = parser.add_argument_group('Frame Extraction Util') ffmpeg.add_argument('-i', metavar='Input Movie', help='The movie for which you want to extract frames', widget='FileChooser') ffmpeg.add_argument('output', metavar='Output Image', help='Where to save the extracted frame', widget='FileSaver', ) ffmpeg.add_argument('-ss', metavar='Timestamp', help='Timestamp of snapshot (in seconds)') ffmpeg.add_argument('-frames:v', metavar='Timestamp', default=1, gooey_options={'visible': False}) parser.parse_args() if __name__ == '__main__': main() note: if you're familiar with Python, this code will look suspiciously similar to the Argparse library, and that's because it is Argparse code! Gooey is a drop-in replacement for all projects backed by Argparse! That's all it takes to get a UI bolted on in front of an existing CLI application! Before we walk through the details of the code, let's give it a run and enjoy the spoils of our work!python main.py Pretty awesome!How it works:A fair bit of the code is just boilerplate: function definitions, imports, bookkeeping. You can learn about them in detail via the docs. We'll focus just on the interesting bits. Going from top to bottom, the first notable line in our code is the one that begins with @Gooey@Gooey(target=\"ffmpeg\", program_name='Frame Extraction v1.0', suppress_gooey_flag=True) target is the most important argument on this line. This is how we tell Gooey that we're trying to run an ffmpeg command. If you were using Gooey with, say, a .jar, you'd similarly provide that info here (e.g. target='java -jar '). At run time, the input provided by the user in the UI gets stitched together and handed over to whatever is specified in target`. This is what enables Gooey to work with any CLI app! The next notable lines are the ones beginning with ffmpeg.add_argument. These are how we specify what shows up as a form field in the UI. Each field we want to present to the user will have an associated call to add_argument. They all have a few arguments in common:first positional argument - this is where we specify the CLI variable for which we're receiving input. In our case, we've set it to -i, which is how you declare the input file to FFMPEGmetavar - This is the human readable name we want to appear over the input field.help - This is the descriptive text that appears next to the input fieldwidget - Gooey has a bunch of different input widgets. Since we're dealing with file IO, we set the relevant fields to FileChooser/FileSaver which spawns the native file dialog used by your host OS.There are a few slightly advanced pieces of Gooey in here as well. ffmpeg.add_argument('-frames:v', metavar='Timestamp', default=1, required=True, # Look at me! gooey_options={'visible': False}) This one uses the gooey_options API to hide this specific component from the interface. The frames:v argument is a minor detail of FFMPEG that end users need not worry about, so we set it up with the values ffmpeg needs, but don't actually expose it.And that's all there is to it! In just a few lines of Python, we've build a super polished UI that's easy to use and turns the complex task of frame extraction via FFMPEG into something that can be used by anyone. We can take this a little further, though..!Bonus: ValidationOur UI is pretty good, but it has a few problems. For instance, users could click start without filling out all the fields, or they could fill out the fields with junk data! Let's be helpful citizens and give them some guidance.Make required fields required:We can add required=True to any fields we want to force the user to manage.@Gooey(target=\"ffmpeg\", program_name='Frame Extraction v1.0', suppress_gooey_flag=True) def main(): parser = GooeyParser(description=\"Extracting frames from a movie using FFMPEG\") ffmpeg = parser.add_argument_group('Frame Extraction Util') ffmpeg.add_argument('-i', metavar='Input Movie', help='The movie for which you want to extract frames', required=True, # Look at me! widget='FileChooser') ffmpeg.add_argument('output', metavar='Output Image', help='Where to save the extracted frame', widget='FileSaver', ) ffmpeg.add_argument('-ss', metavar='Timestamp', required=True, # Look at me! help='Timestamp of snapshot (in seconds)') ffmpeg.add_argument('-frames:v', metavar='Timestamp', default=1, required=True, # Look at me! gooey_options={'visible': False}) parser.parse_args() Now if we try to advance without filling out any of the forms we get a helpful notification!Input Validation:Now let's make sure that the user enters appropriate data types. Update the -ss argument to look like the following.ffmpeg.add_argument('-ss', metavar='Timestamp', required=True, # Look at me! help='Timestamp of snapshot (in seconds)', gooey_options={ # NEW! 'validator': { 'test': 'user_input.isdigit()', 'message': 'Please enter a number' } }) You can read all about validation in the docs, but in short, this tells Gooey to verify that the user's input is actually a number before passing things of to ffmpeg. With this in place, now we've gone really far towards making something that's truly user friendly!Wrapping UpWhere should we go from here? There are lots of options! You could package your new Gooey powered app as a stand alone executable, customize the look and feel of the UI, internationalize your text, and a whole lot more! "
        ],
        "story_type": "Normal",
        "url_raw": "https://chriskiehl.com/article/gooey-as-a-universal-frontend",
        "comments.comment_id": [21044987, 21045597],
        "comments.comment_author": ["icegreentea2", "jlarocco"],
        "comments.comment_descendants": [2, 0],
        "comments.comment_time": [
          "2019-09-23T00:27:25Z",
          "2019-09-23T02:56:58Z"
        ],
        "comments.comment_text": [
          "I've been using this to wrap CLIs for internal tooling at my company [it's also my first accepted contribution to a FOSS project =D ]. Works well, but subject to the usual Python packaging issues. If I needed to wrap a CLI with a GUI for my users, that probably means I don't want to walk them through installing Python and dependencies either. That means using something like PyInstaller, which I can report plays well with Gooey.",
          "Never heard of this, but I once wrote a Perl script (it was a long time ago ;-) that created Perl/Tk UIs around CLI tools by parsing the output from \"$CMD --help\" or \"man $CMD\".<p>It never worked perfect, but was \"good enough\" for the few simple things I needed it for."
        ],
        "id": "cdb5a3ee-c270-438d-806b-03e37841a527",
        "url_text": "Published: 2019-09-22Gooey is a tool for transforming command line interfaces into beautiful desktop applications. It can be used as the frontend client for any language or program. Whether you've built your application in Java, Node, or Haskell, or you just want to put a pretty interface on an existing tool like FFMPEG, Gooey can be used to create a fast, practically free UI with just a little bit of Python (about 20 lines!). Don't want to write no stinkin' Python? Good news! Gooey is actually controlled entirely by a plain ol' JSON file. We're only using Python here because it has convenient bindings for generating the JSON. Contribute bindings for your language! To show how this all fits together, and that it really works for anything, we're going to walk through building a graphical interface to one of my favorite tools of all time: FFMPEG. These steps apply to anything, though! You could swap out FFMPEG for a .jar you've written, or an arbitrary windows .exe, an OSX .app bundle, or anything on linux that's executable! Why wrap up another program at all? While FFMPEG is amazing, it is anything but friendly. Borderline hostile, in fact. The learning curve is basically just a brick wall. To make it friendly for our theoretical end user, we're going to expose a subset of it as a simple, familiar GUI in which they can point-and-click their way to success. Additionally, since FFMPEG has a staggeringly deep suite of video processing tools, to keep things simple we're going to expose just a very small subset of its functionality. We're going to build a UI in which users can specify 1. an input video file, 2. a timestamp where they'd like to extract a screenshot, and finally 3. the output filename where they want the screenshot saved.In short, it will transform a \"scary\" terminal command line this:ffmpeg -ss 300 -i /home/user/path/to/movie.mkv -frames:v 1 outfile.png into an easy to use desktop application that you could hand over to users.Getting StartedTo play along at home, grab the appropriate FFMPEG binary for your OS here and install it.If all has gone well, you should be able to invoke it from the command line and get back some useful version infoffmpeg -version With that working, let's get onto building the UI side of things.Step 1. Installing PythonIf you're coming from other languages / backgrounds, step one will be installing Python. You can find an appropriate guide for your OS here. If you're on Windows, the shortest path to done boils down to visiting python.org, clicking the big \"Download\" button, and then double clicking the downloaded file.Step 2. Create a directory and source file for the projectWe'll need a single python file to describe the GUI we want to build. To stay organized, we'll put any files we create in a directory called myproject.mkdir myproject cd myproject Inside here we'll create the source file we need.touch main.py Step 3. Installing GooeyBefore we start writing the little bits of code required, we have to install Gooey. In Python land, this is done via a tool called pip. This should have been installed automatically along side Python in step 1.pip install Gooey Note: you would normally install Python dependencies inside of a virtual environment, but we're ignoring that detail for ease of example. Step 4. Let's make a UI!Open the main.py file we created in step 2 and paste in the following.from gooey import Gooey, GooeyParser @Gooey(target=\"ffmpeg\", program_name='Frame Extraction v1.0', suppress_gooey_flag=True) def main(): parser = GooeyParser(description=\"Extracting frames from a movie using FFMPEG\") ffmpeg = parser.add_argument_group('Frame Extraction Util') ffmpeg.add_argument('-i', metavar='Input Movie', help='The movie for which you want to extract frames', widget='FileChooser') ffmpeg.add_argument('output', metavar='Output Image', help='Where to save the extracted frame', widget='FileSaver', ) ffmpeg.add_argument('-ss', metavar='Timestamp', help='Timestamp of snapshot (in seconds)') ffmpeg.add_argument('-frames:v', metavar='Timestamp', default=1, gooey_options={'visible': False}) parser.parse_args() if __name__ == '__main__': main() note: if you're familiar with Python, this code will look suspiciously similar to the Argparse library, and that's because it is Argparse code! Gooey is a drop-in replacement for all projects backed by Argparse! That's all it takes to get a UI bolted on in front of an existing CLI application! Before we walk through the details of the code, let's give it a run and enjoy the spoils of our work!python main.py Pretty awesome!How it works:A fair bit of the code is just boilerplate: function definitions, imports, bookkeeping. You can learn about them in detail via the docs. We'll focus just on the interesting bits. Going from top to bottom, the first notable line in our code is the one that begins with @Gooey@Gooey(target=\"ffmpeg\", program_name='Frame Extraction v1.0', suppress_gooey_flag=True) target is the most important argument on this line. This is how we tell Gooey that we're trying to run an ffmpeg command. If you were using Gooey with, say, a .jar, you'd similarly provide that info here (e.g. target='java -jar '). At run time, the input provided by the user in the UI gets stitched together and handed over to whatever is specified in target`. This is what enables Gooey to work with any CLI app! The next notable lines are the ones beginning with ffmpeg.add_argument. These are how we specify what shows up as a form field in the UI. Each field we want to present to the user will have an associated call to add_argument. They all have a few arguments in common:first positional argument - this is where we specify the CLI variable for which we're receiving input. In our case, we've set it to -i, which is how you declare the input file to FFMPEGmetavar - This is the human readable name we want to appear over the input field.help - This is the descriptive text that appears next to the input fieldwidget - Gooey has a bunch of different input widgets. Since we're dealing with file IO, we set the relevant fields to FileChooser/FileSaver which spawns the native file dialog used by your host OS.There are a few slightly advanced pieces of Gooey in here as well. ffmpeg.add_argument('-frames:v', metavar='Timestamp', default=1, required=True, # Look at me! gooey_options={'visible': False}) This one uses the gooey_options API to hide this specific component from the interface. The frames:v argument is a minor detail of FFMPEG that end users need not worry about, so we set it up with the values ffmpeg needs, but don't actually expose it.And that's all there is to it! In just a few lines of Python, we've build a super polished UI that's easy to use and turns the complex task of frame extraction via FFMPEG into something that can be used by anyone. We can take this a little further, though..!Bonus: ValidationOur UI is pretty good, but it has a few problems. For instance, users could click start without filling out all the fields, or they could fill out the fields with junk data! Let's be helpful citizens and give them some guidance.Make required fields required:We can add required=True to any fields we want to force the user to manage.@Gooey(target=\"ffmpeg\", program_name='Frame Extraction v1.0', suppress_gooey_flag=True) def main(): parser = GooeyParser(description=\"Extracting frames from a movie using FFMPEG\") ffmpeg = parser.add_argument_group('Frame Extraction Util') ffmpeg.add_argument('-i', metavar='Input Movie', help='The movie for which you want to extract frames', required=True, # Look at me! widget='FileChooser') ffmpeg.add_argument('output', metavar='Output Image', help='Where to save the extracted frame', widget='FileSaver', ) ffmpeg.add_argument('-ss', metavar='Timestamp', required=True, # Look at me! help='Timestamp of snapshot (in seconds)') ffmpeg.add_argument('-frames:v', metavar='Timestamp', default=1, required=True, # Look at me! gooey_options={'visible': False}) parser.parse_args() Now if we try to advance without filling out any of the forms we get a helpful notification!Input Validation:Now let's make sure that the user enters appropriate data types. Update the -ss argument to look like the following.ffmpeg.add_argument('-ss', metavar='Timestamp', required=True, # Look at me! help='Timestamp of snapshot (in seconds)', gooey_options={ # NEW! 'validator': { 'test': 'user_input.isdigit()', 'message': 'Please enter a number' } }) You can read all about validation in the docs, but in short, this tells Gooey to verify that the user's input is actually a number before passing things of to ffmpeg. With this in place, now we've gone really far towards making something that's truly user friendly!Wrapping UpWhere should we go from here? There are lots of options! You could package your new Gooey powered app as a stand alone executable, customize the look and feel of the UI, internationalize your text, and a whole lot more! ",
        "_version_": 1718536524630327296
      },
      {
        "story_id": 19325371,
        "story_author": "lerax",
        "story_descendants": 64,
        "story_score": 109,
        "story_time": "2019-03-07T03:44:16Z",
        "story_title": "Seven Unix Commands Every Data Scientist Should Know",
        "search": [
          "Seven Unix Commands Every Data Scientist Should Know",
          "http://neowaylabs.github.io/programming/unix-shell-for-data-scientists/",
          "In many situations, unprepared data scientists can spend much more time than necessary on secondary tasks. Although their attention should stay on analyzing data, checking hypothesis, engineering features, etc., they often need to get their hands dirty and code auxiliary scripts and parsers to get the information they need. Prepared data scientists prefer to use Unix commands and solve their secondary tasks in a few seconds. This post presents 7 basic Unix commands (maybe eight) that, once incorporated in the day-by-day toolset, can potentially improve the productivity of any data scientist. The beauty of these tools is the ability to easily combine them into powerful commands, executing tasks that could take a full day of coding to get them right. This is the selected list, probably in the order of most frequent usage: grep cat find head/tail wc awk shuf In addition, it is shown how two auxiliary commands (xargs and man) can improve even further the usability of the 7 commands above. grep grep searches for patterns in files or in the standard input. It is really useful to find stuff without opening files in an editor. Examples Problem: We want to find where the library ggplot was used in any script in the directory r_project. $ grep -rn ggplot r_project/ r_project/script.R:4:library(ggplot2) r_project/script.R:247:t<-ggplot(base.test.c, aes(x=score, colour=gap_dic, group=gap_dic)) r_project/script.R:265:t<-ggplot(base.test.c[base.test.c$gap_total<40000,], aes(x=gap_total, colour=corte2, group=corte2)) option -r stands for recursion option -n tells grep to show the line numbers Problem: From a big log file, we want to get only the logging messages with the pattern TRAINING -. $ grep 'TRAINING -' my_app.log Note that grep is case sensitive by default. Problem: Given a directory composed of subdirectories whose names contain a date, we want to get the complete name of one or more subdirectories containing a date like 2018_05. $ ls models_by_date/ | grep 2018_05 Here we use the pipe | operator to send the output of the first command to grep by the standard input 1. Problem: When listing all installed Python packages, we want to see only the results containing the name gpyopt (usually a single line). $ pip freeze | grep -i gpyopt GPyOpt==1.2.5 pip freeze returns a list of installed packages to the standard output and grep searches for gpyopt (-i makes it case insensitive) in the standard input. Problem: Given an arbitrary text file, we want to show only those lines containing a pattern specified by a regex. $ grep -oP \"'[\\w]+ == [\\d.]+'\" python_library/setup.py 'numpy == 1.15.0' 'fire == 0.1.3' 'gpyopt == 1.2.5' 'recsys_commons == 0.1.0' This example uses Perl regular expression to search for packages and versions in a Python setup file. cat Prints on the screen (or to the standard output) the contents of files. Simple like that. $ cat script.sh #!/bin/bash set -eo pipefail echo \"$BLEU\" find find searches for files by specifying many different (and optionally) kinds of parameters. It is also able to execute some simple actions or an entire command line using the resultant files. Examples Problem: We want to find all files with the extension json in the current directory, including subdirectories. $ find . -name '*.json' ./third-party/wiwinwlh/src/26-data-formats/example.json ./third-party/wiwinwlh/src/26-data-formats/crew.json Problem: All files with extension pyc must be removed from the directory my_library/modules, recursively. $ find my_library/modules -name '*.pyc' -delete Problem: In a directory containing multiple projects, we want to find all setup.py files that contain the text boto3 (in other words, we are looking for projects using the library boto3). $ find . -name setup.py -type f -exec grep -Hn boto3 {} \\; Here we tell it to search only for files with the exact name setup.py, ignoring directories (-type f) and executing grep (-exec) on each one of them (why dont we just use grep alone?)2. Note that: {} is used to pass a file path as an argument to grep \\; marks the end of the command grep parameter -H makes it print the filename head/tail Like working with DataFrames, head and tail print the first and the last lines of files or of the standard input. Examples Problem: Given a CSV file, we want to quickly look at its header. Problem: From a potentially huge log file, we want to read only its last 20 events. wc wc is very useful to count lines, words or even characters in files. Examples Problem: How many text lines does a file have? $ wc -l data.csv 624 data.csv Problem: We want to know the total number of CSV records in a directory containing multiple CSV files. $ wc -l data_dir/*.csv 102224 data_dir/part-00000-02aa95cd-3907-44c8-87ee-97ff44677349-c000.csv 102513 data_dir/part-00001-02aa95cd-3907-44c8-87ee-97ff44677349-c000.csv 204737 total We may need to discount the number of header lines (this could also be done just using the shell). awk awk uses a programming language (AWK) for text processing. It is powerful and might seem complicated to learn and use. However, there are a few commands that can be used very frequently. Examples Problem: Given a CSV file, we want to know the number of columns just by analyzing its header. $ head -n 1 data.csv | awk -F ',' '{print NF}' 91 First, head sends the first line to the standard output, which is consumed (as the standard input) by awk and broken up into a sequence of fields delimited by ,. NF holds the number of fields in a line. Problem: Given a big CSV file containing hundreds of columns, we want to have a quick look at the first lines of a specific column (lets say the third column). $ head data.csv | awk -F ',' '{print $3}' var_x 3.0 4.0 3.0 3.0 3.0 3.0 3.0 2.0 3.0 shuf shuf generates a random permutation of its inputs. It is very useful to sample data. Examples Problem: Given a CSV file, we want to take a random sample (50 records) from it and save this sample in another file. $ cat big_csv.csv | shuf | head -n 50 > sample_from_big_csv.csv The operator > is used to redirect the standard output to a normal file. Without using it, the result of head would be just printed on the screen. Problem: Given a directory containing multiple data files, we want to get a random sample of files (5 files) and copy these files to another directory. $ find origin_dir/ -type f | shuf | head -n 5 | xargs -i cp {} sample_dir/ First find returns a list of files in origin_dir (including this directory name in their paths), then shuf shuffles the list of file paths, head takes the first 5 file paths and, finally, cp copy each of these 5 files to the directory sample_dir (xargs, explained in the next section, is used as an auxiliary command since we couldnt just use the standard input). Auxiliary Commands xargs xargs is a kind of auxiliary program, since its role is to convert the standard input into an argument of another program. This is really useful to make a chain of processing programs that dont use the standard input. Examples Problem: We must remove all __pycache__ directories in a given project directory. $ find my_app -name '__pycache__' -type d | xargs -i rm -r {} As find action -delete cant remove nonempty directories, we use rm -r to remove all __pycache__ folders. Problem: Remove all Git branches that were already fully merged with their upstream branches. $ git branch | xargs -i git branch -d {} man man is also another auxiliary program. It provides an interface to reference manuals of almost all UNIX commands. The image below shows the result of checking the manual of man itself (man man). By Kamran Mackey - Arch Linux using the Cinnamon display manager.,GPL. Conclusion This post presented 7 Unix commands to increase the productivity of data scientists, including examples of usage and two auxiliary commands to make even more powerful combinations. By describing common problems and possible solutions, the intention is to make the post a useful reference to tackle similar scenarios. More time we spend exploring and using these commands, more productive we become by using them. "
        ],
        "story_type": "Normal",
        "url_raw": "http://neowaylabs.github.io/programming/unix-shell-for-data-scientists/",
        "comments.comment_id": [19325718, 19325787],
        "comments.comment_author": ["aviraldg", "cybersol"],
        "comments.comment_descendants": [2, 4],
        "comments.comment_time": [
          "2019-03-07T05:23:23Z",
          "2019-03-07T05:36:06Z"
        ],
        "comments.comment_text": [
          "I'd add jq (<a href=\"https://stedolan.github.io/jq/\" rel=\"nofollow\">https://stedolan.github.io/jq/</a>) to the list. JSON data is so common, and jq makes working with it a breeze.",
          "'sort' and 'uniq' should also be near the top of the list. And once your doing more on the command-line, 'join' and 'comm' can help you merge data from multiple files."
        ],
        "id": "ca26bf39-24b5-4c54-b928-13383b3fbf47",
        "url_text": "In many situations, unprepared data scientists can spend much more time than necessary on secondary tasks. Although their attention should stay on analyzing data, checking hypothesis, engineering features, etc., they often need to get their hands dirty and code auxiliary scripts and parsers to get the information they need. Prepared data scientists prefer to use Unix commands and solve their secondary tasks in a few seconds. This post presents 7 basic Unix commands (maybe eight) that, once incorporated in the day-by-day toolset, can potentially improve the productivity of any data scientist. The beauty of these tools is the ability to easily combine them into powerful commands, executing tasks that could take a full day of coding to get them right. This is the selected list, probably in the order of most frequent usage: grep cat find head/tail wc awk shuf In addition, it is shown how two auxiliary commands (xargs and man) can improve even further the usability of the 7 commands above. grep grep searches for patterns in files or in the standard input. It is really useful to find stuff without opening files in an editor. Examples Problem: We want to find where the library ggplot was used in any script in the directory r_project. $ grep -rn ggplot r_project/ r_project/script.R:4:library(ggplot2) r_project/script.R:247:t<-ggplot(base.test.c, aes(x=score, colour=gap_dic, group=gap_dic)) r_project/script.R:265:t<-ggplot(base.test.c[base.test.c$gap_total<40000,], aes(x=gap_total, colour=corte2, group=corte2)) option -r stands for recursion option -n tells grep to show the line numbers Problem: From a big log file, we want to get only the logging messages with the pattern TRAINING -. $ grep 'TRAINING -' my_app.log Note that grep is case sensitive by default. Problem: Given a directory composed of subdirectories whose names contain a date, we want to get the complete name of one or more subdirectories containing a date like 2018_05. $ ls models_by_date/ | grep 2018_05 Here we use the pipe | operator to send the output of the first command to grep by the standard input 1. Problem: When listing all installed Python packages, we want to see only the results containing the name gpyopt (usually a single line). $ pip freeze | grep -i gpyopt GPyOpt==1.2.5 pip freeze returns a list of installed packages to the standard output and grep searches for gpyopt (-i makes it case insensitive) in the standard input. Problem: Given an arbitrary text file, we want to show only those lines containing a pattern specified by a regex. $ grep -oP \"'[\\w]+ == [\\d.]+'\" python_library/setup.py 'numpy == 1.15.0' 'fire == 0.1.3' 'gpyopt == 1.2.5' 'recsys_commons == 0.1.0' This example uses Perl regular expression to search for packages and versions in a Python setup file. cat Prints on the screen (or to the standard output) the contents of files. Simple like that. $ cat script.sh #!/bin/bash set -eo pipefail echo \"$BLEU\" find find searches for files by specifying many different (and optionally) kinds of parameters. It is also able to execute some simple actions or an entire command line using the resultant files. Examples Problem: We want to find all files with the extension json in the current directory, including subdirectories. $ find . -name '*.json' ./third-party/wiwinwlh/src/26-data-formats/example.json ./third-party/wiwinwlh/src/26-data-formats/crew.json Problem: All files with extension pyc must be removed from the directory my_library/modules, recursively. $ find my_library/modules -name '*.pyc' -delete Problem: In a directory containing multiple projects, we want to find all setup.py files that contain the text boto3 (in other words, we are looking for projects using the library boto3). $ find . -name setup.py -type f -exec grep -Hn boto3 {} \\; Here we tell it to search only for files with the exact name setup.py, ignoring directories (-type f) and executing grep (-exec) on each one of them (why dont we just use grep alone?)2. Note that: {} is used to pass a file path as an argument to grep \\; marks the end of the command grep parameter -H makes it print the filename head/tail Like working with DataFrames, head and tail print the first and the last lines of files or of the standard input. Examples Problem: Given a CSV file, we want to quickly look at its header. Problem: From a potentially huge log file, we want to read only its last 20 events. wc wc is very useful to count lines, words or even characters in files. Examples Problem: How many text lines does a file have? $ wc -l data.csv 624 data.csv Problem: We want to know the total number of CSV records in a directory containing multiple CSV files. $ wc -l data_dir/*.csv 102224 data_dir/part-00000-02aa95cd-3907-44c8-87ee-97ff44677349-c000.csv 102513 data_dir/part-00001-02aa95cd-3907-44c8-87ee-97ff44677349-c000.csv 204737 total We may need to discount the number of header lines (this could also be done just using the shell). awk awk uses a programming language (AWK) for text processing. It is powerful and might seem complicated to learn and use. However, there are a few commands that can be used very frequently. Examples Problem: Given a CSV file, we want to know the number of columns just by analyzing its header. $ head -n 1 data.csv | awk -F ',' '{print NF}' 91 First, head sends the first line to the standard output, which is consumed (as the standard input) by awk and broken up into a sequence of fields delimited by ,. NF holds the number of fields in a line. Problem: Given a big CSV file containing hundreds of columns, we want to have a quick look at the first lines of a specific column (lets say the third column). $ head data.csv | awk -F ',' '{print $3}' var_x 3.0 4.0 3.0 3.0 3.0 3.0 3.0 2.0 3.0 shuf shuf generates a random permutation of its inputs. It is very useful to sample data. Examples Problem: Given a CSV file, we want to take a random sample (50 records) from it and save this sample in another file. $ cat big_csv.csv | shuf | head -n 50 > sample_from_big_csv.csv The operator > is used to redirect the standard output to a normal file. Without using it, the result of head would be just printed on the screen. Problem: Given a directory containing multiple data files, we want to get a random sample of files (5 files) and copy these files to another directory. $ find origin_dir/ -type f | shuf | head -n 5 | xargs -i cp {} sample_dir/ First find returns a list of files in origin_dir (including this directory name in their paths), then shuf shuffles the list of file paths, head takes the first 5 file paths and, finally, cp copy each of these 5 files to the directory sample_dir (xargs, explained in the next section, is used as an auxiliary command since we couldnt just use the standard input). Auxiliary Commands xargs xargs is a kind of auxiliary program, since its role is to convert the standard input into an argument of another program. This is really useful to make a chain of processing programs that dont use the standard input. Examples Problem: We must remove all __pycache__ directories in a given project directory. $ find my_app -name '__pycache__' -type d | xargs -i rm -r {} As find action -delete cant remove nonempty directories, we use rm -r to remove all __pycache__ folders. Problem: Remove all Git branches that were already fully merged with their upstream branches. $ git branch | xargs -i git branch -d {} man man is also another auxiliary program. It provides an interface to reference manuals of almost all UNIX commands. The image below shows the result of checking the manual of man itself (man man). By Kamran Mackey - Arch Linux using the Cinnamon display manager.,GPL. Conclusion This post presented 7 Unix commands to increase the productivity of data scientists, including examples of usage and two auxiliary commands to make even more powerful combinations. By describing common problems and possible solutions, the intention is to make the post a useful reference to tackle similar scenarios. More time we spend exploring and using these commands, more productive we become by using them. ",
        "_version_": 1718536457329573888
      },
      {
        "story_id": 18971044,
        "story_author": "ajr0",
        "story_descendants": 174,
        "story_score": 505,
        "story_time": "2019-01-22T18:47:15Z",
        "story_title": "AMD Open Source Driver for Vulkan",
        "search": [
          "AMD Open Source Driver for Vulkan",
          "https://github.com/GPUOpen-Drivers/AMDVLK",
          "AMD Open Source Driver for Vulkan The AMD Open Source Driver for Vulkan is an open-source Vulkan driver for Radeon graphics adapters on Linux. It is built on top of AMD's Platform Abstraction Library (PAL), a shared component that is designed to encapsulate certain hardware and OS-specific programming details for many of AMD's 3D and compute drivers. Leveraging PAL can help provide a consistent experience across platforms, including support for recently released GPUs and compatibility with AMD developer tools. Shaders that compose a particular VkPipeline object are compiled as a single entity using the LLVM-Based Pipeline Compiler (LLPC) library. LLPC builds on LLVM's existing shader compilation infrastructure for AMD GPUs to generate code objects compatible with PAL's pipeline ABI. Notably, AMD's closed-source Vulkan driver currently uses a different pipeline compiler, which is the major difference between AMD's open-source and closed-source Vulkan drivers. Product Support The AMD Open Source Driver for Vulkan is designed to support the following AMD GPUs: Radeon RX 6900/6800/6700/6600 Series Radeon RX 5700/5600/5500 Series Radeon RX Vega Series Radeon RX 400/500 Series Radeon Pro WX 9100, x200 Series Radeon Pro W5700/W5500 Series Note: For Pre-Polaris and Pre-Raven GPUs, please use v-2021.Q2.5 or older release. Operating System Support The AMD Open Source Driver for Vulkan is designed to support following distros on both the AMDGPU upstream driver stack and the AMDGPU Pro driver stack: Ubuntu 20.04 (64-bit version) Ubuntu 18.04 (64-bit version) RedHat 8.2 (64-bit version) RedHat 7.8 (64-bit version) The driver has not been tested on other distros. You may try it out on other distros of your choice. Note: To run the Vulkan driver with AMDGPU upstream driver stack on SI and CI generation GPUs, amdgpu.si_support and amdgpu.cik_support need to be enabled in kernel Feature Support and Performance The AMD Open Source Driver for Vulkan is designed to support the following features: Vulkan 1.2 More than 30 extensions Radeon GPUProfiler tracing Built-in debug and profiling tools Mid-command buffer preemption and SR-IOV virtualization The following features and improvements are planned in future releases (Please refer to Release Notes for update of each release): Upcoming versions of the Vulkan API Hardware performance counter collection through RenderDoc LLPC optimizations to improve GPU-limited performance and compile time Optimizations to improve CPU-limited performance Known Issues CTS may hang in VK.synchronization.internally_synchronized_objects.pipeline_cache_compute with Linux kernel versions lower than 4.13 The driver can only work with firmware of ME feature version >= 25 (you can check the version with command \"sudo cat /sys/kernel/debug/dri/0/amdgpu_firmware_info\"). If you are using upstream stack with GPUs of SI or CI family, you may need to upgrade the kernel to 4.19 or later version and firmware (under /lib/firmware/amdgpu/) to the right version from https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/tree/amdgpu, and then update ramfs (sudo mkinitramfs -o /boot/initrd.img-`uname -r` `uname -r`) Timeline semaphore is not fully supported in Linux kernel until version 5.5. You can install Vulkan timeline semaphore layer to enable the extension if you are using earlier version of Linux kernel How to Contribute You are welcome to submit contributions of code to the AMD Open Source Driver for Vulkan. The driver is built from source code in four repositories: LLVM, XGL, LLPC and PAL. For changes to LLVM, you should submit contribution to the LLVM trunk. Commits there will be evaluated to merge into the amd-gfx-gpuopen-master branch periodically. For changes to XGL, LLPC and PAL, please create a pull request against the dev branch. After your change is reviewed and if it is accepted, it will be evaluated to merge into the master branch in a subsequent regular promotion. IMPORTANT: By creating a pull request, you agree to allow your contribution to be licensed by the project owners under the terms of the MIT License. When contributing to XGL, LLPC and PAL, your code should: Match the style of nearby existing code. Your code may be edited to comply with our coding standards when it is merged into the master branch. Avoid adding new dependencies, including dependencies on STL. Please make each contribution reasonably small. If you would like to make a big contribution, like a new feature or extension, please raise an issue first to allow planning to evaluate and review your work. Note: Since PAL is a shared component that must support other APIs, other operating systems, and pre-production hardware, you might be asked to revise your PAL change for reasons that may not be obvious from a pure Linux Vulkan driver perspective. Build Instructions System Requirements It is recommended to install 16GB RAM in your build system. Build System CMake 3.13.4 or newer is required. Download and install proper one if the cmake is older than 3.13.4. Ninja is requred. Install Dev and Tools Packages Ubuntu sudo apt-get install build-essential curl g++-multilib gcc-multilib git pkg-config python3 64-bit sudo apt-get install libssl-dev libx11-dev libxcb1-dev x11proto-dri2-dev libxcb-dri3-dev libxcb-dri2-0-dev libxcb-present-dev libxshmfence-dev libxrandr-dev libwayland-dev 32-bit dpkg --add-architecture i386 sudo apt-get install libssl-dev:i386 libx11-dev:i386 libxcb1-dev:i386 libxcb-dri3-dev:i386 libxcb-dri2-0-dev:i386 libxcb-present-dev:i386 libxshmfence-dev:i386 libwayland-dev libwayland-dev:i386 libxrandr-dev:i386 RedHat wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum localinstall epel-release-latest-7.noarch.rpm sudo yum update 64-bit sudo yum -y install openssl-devel gcc-c++ python3 curl glibc-devel libstdc++-devel libxcb-devel libX11-devel libxshmfence-devel libXrandr-devel wayland-devel 32-bit sudo yum -y install openssl-devel.i686 gcc-c++ python3 curl glibc-devel.i686 libstdc++-devel.i686 libxcb-devel.i686 libX11-devel.i686 libxshmfence-devel.i686 libXrandr-devel.i686 wayland-devel.i686 Get Repo Tools Ubuntu 18.04 sudo apt-get install repo Ubuntu 20.04, RedHat 7.8, 8.2 mkdir ~/bin curl https://storage.googleapis.com/git-repo-downloads/repo > ~/bin/repo # Replacing python with python3 is only needed on Ubuntu 20.04 if the 'python' executable is not available sed -i s/python/python3/ ~/bin/repo chmod a+x ~/bin/repo export PATH=~/bin:\"$PATH\" Get Source Code mkdir vulkandriver cd vulkandriver repo init -u https://github.com/GPUOpen-Drivers/AMDVLK.git -b master repo sync Note: Source code in dev branch can be gotten by using \"-b dev\" in the \"repo init\" command Build Driver and Generate JSON Files Ubuntu cmake -G Ninja -S xgl -B xgl/builds/Release64 cmake --build xgl/builds/Release64 cmake -G Ninja -S xgl -B xgl/builds/Release32 -DCMAKE_C_FLAGS=-m32 -DCMAKE_CXX_FLAGS=-m32 cmake --build xgl/builds/Release32 RedHat cmake -G Ninja -S drivers/xgl -B xgl/builds/Release64 cmake --build xgl/builds/Release64 cmake -G Ninja -S drivers/xgl -B xgl/builds/Release32 -DCMAKE_C_FLAGS=-m32 -DCMAKE_CXX_FLAGS=-m32 cmake --build xgl/builds/Release32 Note: For RedHat 7.x, please use cmake3(>= 3.13.4) instead of cmake. For debug build, use -DCMAKE_BUILD_TYPE=Debug -DLLVM_PARALLEL_LINK_JOBS=2 (Linking a debug build of llvm is very memory intensive, so we use only two parallel jobs). To enable Wayland support, use -DBUILD_WAYLAND_SUPPORT=ON. Installation Instructions Install Vulkan SDK You can download and install the SDK package here. Install dependencies Ubuntu sudo apt install libssl1.1 RedHat sudo yum install openssl-libs Install Driver and JSON Files sudo cmake --install xgl/builds/Release64 --component icd sudo cmake --install xgl/builds/Release32 --component icd If you want to install driver to customized directory, you can add \"-DCMAKE_INSTALL_PREFIX={installation directory}\" in the cmake build command. JSON files will be installed to /etc/vulkan/icd.d while other files will be installed to the installation directory you specified. If RADV is also installed in the system, AMDVLK driver will be enabled by default after installation. You can switch the driver between AMDVLK and RADV by environment variable AMD_VULKAN_ICD = AMDVLK or RADV. Note: The remaining steps are only required when running the AMDGPU upstream driver stack. Turn on DRI3 and disable modesetting X driver Add following lines in /usr/share/X11/xorg.conf.d/10-amdgpu.conf: Section \"Device\" Identifier \"AMDgpu\" Option \"DRI\" \"3\" EndSection And make sure following line is NOT included in the section: Required Settings On the AMDGPU upstream driver stack with libdrm version lower than 2.4.92, the max number of IB per submission MUST be limited to 4 (the default setting in AMD Open Source driver for Vulkan is 16). This can be accomplished via the Runtime Settings mechanism by adding the following line to amdPalSettings.cfg: MaxNumCmdStreamsPerSubmit,4 CommandBufferCombineDePreambles,1 Install with pre-built driver You could generate the installation package with below command while building driver: Ubuntu cmake -G Ninja -S xgl -B xgl/builds/Release64 [-DPACKAGE_VERSION=package version] cmake --build xgl/builds/Release64 --target makePackage RedHat cmake -G Ninja -S xgl -B xgl/builds/Release64 [-DPACKAGE_VERSION=package version] cmake --build xgl/builds/Release64 --target makePackage You could also download pre-built package from https://github.com/GPUOpen-Drivers/AMDVLK/releases for each code promotion in master branch. Below is the installation instruction: Ubuntu 18.04, 20.04 sudo dpkg -r amdvlk /* If old version is installed on the machine, remove it first */ sudo dpkg -i amdvlk_x.x.x_amd64.deb sudo apt-get -f install RedHat 7.8, 8.2 sudo rpm -e amdvlk /* If old version is installed on the machine, remove it first */ sudo rpm -i amdvlk-x.x.x.x86_64.rpm For Ubuntu, you could also install the latest driver build from https://repo.radeon.com: sudo wget -qO - http://repo.radeon.com/amdvlk/apt/debian/amdvlk.gpg.key | sudo apt-key add - sudo sh -c 'echo deb [arch=amd64,i386] http://repo.radeon.com/amdvlk/apt/debian/ bionic main > /etc/apt/sources.list.d/amdvlk.list' sudo apt-get remove amdvlk /* If old version is installed on the machine, remove it first */ sudo apt update sudo apt-get install amdvlk Runtime Settings The driver exposes many settings that can customize the driver's behavior and facilitate debugging. You can add/edit settings in amdVulkanSettings.cfg or amdPalSettings.cfg file under one of below paths, formatted with one name,value pair per line: /etc/amd $AMD_CONFIG_DIR Some example settings are listed below: Setting Name Valid Values Comment AllowVkPipelineCachingToDisk 0: disallow1: default 1 is default value which enables Pal's archive-file based caching.The archive-file is stored under ~/.cache/AMD/VkCache. ShaderCacheMode 0: disable cache1: runtime cache2: cache to disk Runtime cache is the default mode. For \"cache to disk\", the cache file is generated under $AMD_SHADER_DISK_CACHE_PATH/AMD/LlpcCache or $XDG_CACHE_HOME/AMD/LlpcCache or $HOME/.cache/AMD/LlpcCache IFH 0: default1: drop all submits Infinitely Fast Hardware. Submit calls are dropped before being sent to hardware. Useful for measuring CPU-limited performance. EnableVmAlwaysValid 0: disable1: default2: force enable 1 is the default setting which enables the VM-always-valid feature for kernel 4.16 and above. The feature can reduce command buffer submission overhead related to virtual memory management. IdleAfterSubmitGpuMask Bitmask of GPUs (i.e., bit 0 is GPU0, etc.) Forces the CPU to immediately wait for each GPU submission to complete on the specified set of GPUs. All available settings can be determined by examining below source files that define them. .../xgl/icd/settings/settings.cfg (API layer settings) .../pal/src/core/settings_core.json (PAL hardware-independent settings) .../pal/src/core/hw/gfxip/gfx6/settings_gfx6.json (PAL GFX6-8 settings) .../pal/src/core/hw/gfxip/gfx9/settings_gfx9.json (PAL GFX9+ settings) Runtime settings are only read at device initialization, and cannot be changed without restarting the application. If running on a system with multiple GPUs, the same settings will apply to all of them. Lines in the settings file that start with ; will be treated as comments. Enable extensions under development The extensions under development are not enabled by default in driver. You can enable them through environment variable: export AMDVLK_ENABLE_DEVELOPING_EXT=\"<extension1-name> [<extension2-name>...]\" or export AMDVLK_ENABLE_DEVELOPING_EXT=\"all\" The extension name is case-insensitive. PAL GpuProfiler Layer The GpuProfiler is an optional layer that is designed to intercept the PAL interface to provide basic GPU profiling support. Currently, this layer is controlled exclusively through runtime settings and outputs its results to file. You can use the following Runtime Settings to generate .csv files with GPU timings of work performed during the designated frames of an application (one file for each frame): Setting Name Value Comment GpuProfilerMode 0: disable1: enable with sqtt off2: enable with sqtt for thread trace3: enable with sqtt for RGP Enables and sets the SQTT mode for the GPU performance profiler layer. Actual capture of performance data must be specified via frame number with GpuProfilerConfig_StartFrame or by pressing shift-F11. GpuProfilerConfig.LogDirectory <directory-path> The directory path is relative to $AMD_DEBUG_DIR or $TMPDIR or /var/tmp/, default value is \"amdpal/\". Your application must have write permissions to the directory. The profiling logs are output to a subdirectory that is named in the format like <AppName><yyyy-MM-dd><HH:mm:ss>. GpuProfilerConfig.Granularity 0: per-draw1: per-cmdbuf Defines what is measured/profiled. Per-draw times individual commands (such as draw, dispatch, etc.) inside command buffers, while per-cmdbuf only profiles entire command buffers in aggregate. GpuProfilerConfig.StartFrame Positive integer First frame to capture data for. If StartFrame and FrameCount are not set, all frames will be profiled. GpuProfilerConfig.FrameCount Positive integer Number of frames to capture data for. GpuProfilerConfig.RecordPipelineStats 0, 1 Gathers pipeline statistic query data per entry if enabled. You can use the script timingReport.py to analyze the profiling log: python timeReport.py <profiling_log_subdirectory> Dump Pipelines and Shaders The output of timeReport.py includes the information of top pipelines like below: Top Pipelines (>= 1%) Compiler Hash | Type | Avg. Call Count | Avg. GPU Time [us] | Avg. Frame % 1. 0xd91d15e42d62dcbb | VsPs | 43 | 11,203.15 | 10.20 % 2. 0x724e9af55f2adf1b | Cs | 1 | 9,347.50 | 8.51 % 3. 0x396e5ad6f7a789f7 | VsHsDsPs | 468 | 8,401.35 | 7.65 % You can add the following settings to amdPalSettings.cfg to dump the information of each pipeline: EnablePipelineDump,1 PipelineDumpDir,<dump_dir_path> PipelineDumpDir is a sub-path relative to $AMD_DEBUG_DIR or $TMPDIR or /var/tmp/, default value is \"spvPipeline/\". The pipeline dump file is named in the format like Pipeline<Type>_<Compiler_Hash>.pipe. For example, the above top 1 pipeline is dumped to PipelineVsFs_0xD91D15E42D62DCBB.pipe. The shaders referenced by each pipeline are also dumped to .spv files. PAL Debug Overlay PAL's debug overlay can be enabled to display real time statistics and information on top of a running application. This includes a rolling FPS average, CPU and GPU frame times, and a ledger tracking how much video memory has been allocated from each available heap. Benchmarking (i.e., \"Benchmark (F11)\") is currently unsupported. Setting Name Value Comment DebugOverlayEnabled 0, 1 Enables the debug overlay. DebugOverlayConfig.DebugOverlayLocation 0: top-left1: top-right2: bottom-left3: bottom-right Determines where the overlay text should be displayed. Can be used to avoid collision with important rendering by the application. DebugOverlayConfig.PrintFrameNumber 0, 1 Reports the current frame number. Useful when determining a good frame range for profiling with the GpuProfiler layer. DebugOverlayConfig.TimeGraphEnable 0, 1 Enables rendering of a graph of recent CPU and GPU frame times. Third Party Software The AMD Open Source Driver for Vulkan contains code written by third parties. LLVM is distributed under the Apache License v2.0 with LLVM Exceptions. See LICENSE.TXT file in the top directory of the LLVM repository. MetroHash is distributed under the terms of Apache License 2.0. See LICENSE file in the top directory of the MetroHash repository. CWPack is distributed under the terms of MITLicense. See LICENSE file in the top directory of the CWPack repository. Please see the README.md file in the PAL, LLPC and XGL repositories for information on third party software used by those libraries. DISCLAIMER The information contained herein is for informational purposes only, and is subject to change without notice. This document may contain technical inaccuracies, omissions and typographical errors, and AMD is under no obligation to update or otherwise correct this information. Advanced Micro Devices, Inc. makes no representations or warranties with respect to the accuracy or completeness of the contents of this document, and assumes no liability of any kind, including the implied warranties of noninfringement, merchantability or fitness for particular purposes, with respect to the operation or use of AMD hardware, software or other products described herein. No license, including implied or arising by estoppel, to any intellectual property rights is granted by this document. Terms and limitations applicable to the purchase or use of AMD's products are as set forth in a signed agreement between the parties or in AMD's Standard Terms and Conditions of Sale. AMD, the AMD Arrow logo, Radeon, FirePro, and combinations thereof are trademarks of Advanced Micro Devices, Inc. Other product names used in this publication are for identification purposes only and may be trademarks of their respective companies. Vega is a codename for AMD architecture, and is not a product name. Linux is the registered trademark of Linus Torvalds in the U.S. and other countries. Vulkan and the Vulkan logo are registered trademarks of the Khronos Group, Inc. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/GPUOpen-Drivers/AMDVLK",
        "comments.comment_id": [18971864, 18973652],
        "comments.comment_author": ["tombert", "zanny"],
        "comments.comment_descendants": [6, 10],
        "comments.comment_time": [
          "2019-01-22T20:03:31Z",
          "2019-01-22T23:09:14Z"
        ],
        "comments.comment_text": [
          "I can't speak for anyone else, but because AMD has been opening up their drivers, the laptop I purchased six months ago was AMD based.<p>I haven't done any kind of elaborate benchmarks, but as someone who runs Linux full-time, I want to support companies that make my life a bit easier.<p>That said, I have had some issue with my computer having some weird graphical glitches, and then crashing...I don't know if that's the drivers fault but I never had this with my NVidia or Intel cards...",
          "A lot of people are talking along the lines of \"oh AMD is nice but... Nvidia\".<p>No, in 2019 all AMD GPUs this decade support OpenGL through 4.5, support Vulkan, and still really don't have a great OpenCL situation (rocm is out of tree on every distro and only supprts parts of 2.0 still).<p>For gaming though, theres no reason not to get an AMD GPU. They are at near performance parity with Nvidia relative to their Windows performance, they work with the inbuilt drivers on every distro out of the box, and the only footgun to watch out for is that new hardware generally takes a feature release Mesa cycle to get stable after launch. You even get hardware accelerated h264 encoding and decoding (and vpx on some chips) via vaapi. All on top of the fundamental that they are much more freedom respecting than Nvidia.<p><i>Stop giving Nvidia your money to screw you over with</i>. CUDA, their RTX crap, Gsync, Physx, Nvidia \"Gameworks\", and much more are all anti-competitive monopolist exploitative user-hostile evil meant to screw over competition and customers alike. Nvidia is one of the most reprehensible companies out there among peers like Oracle. AMD isn't a selfless helpless angel of a company, but when their products <i>are</i> competitive, and in many ways better (such as supporting Wayland) stop giving such a hostile business your money."
        ],
        "id": "627ebc07-24f5-4c1a-ba69-92ea048c9698",
        "url_text": "AMD Open Source Driver for Vulkan The AMD Open Source Driver for Vulkan is an open-source Vulkan driver for Radeon graphics adapters on Linux. It is built on top of AMD's Platform Abstraction Library (PAL), a shared component that is designed to encapsulate certain hardware and OS-specific programming details for many of AMD's 3D and compute drivers. Leveraging PAL can help provide a consistent experience across platforms, including support for recently released GPUs and compatibility with AMD developer tools. Shaders that compose a particular VkPipeline object are compiled as a single entity using the LLVM-Based Pipeline Compiler (LLPC) library. LLPC builds on LLVM's existing shader compilation infrastructure for AMD GPUs to generate code objects compatible with PAL's pipeline ABI. Notably, AMD's closed-source Vulkan driver currently uses a different pipeline compiler, which is the major difference between AMD's open-source and closed-source Vulkan drivers. Product Support The AMD Open Source Driver for Vulkan is designed to support the following AMD GPUs: Radeon RX 6900/6800/6700/6600 Series Radeon RX 5700/5600/5500 Series Radeon RX Vega Series Radeon RX 400/500 Series Radeon Pro WX 9100, x200 Series Radeon Pro W5700/W5500 Series Note: For Pre-Polaris and Pre-Raven GPUs, please use v-2021.Q2.5 or older release. Operating System Support The AMD Open Source Driver for Vulkan is designed to support following distros on both the AMDGPU upstream driver stack and the AMDGPU Pro driver stack: Ubuntu 20.04 (64-bit version) Ubuntu 18.04 (64-bit version) RedHat 8.2 (64-bit version) RedHat 7.8 (64-bit version) The driver has not been tested on other distros. You may try it out on other distros of your choice. Note: To run the Vulkan driver with AMDGPU upstream driver stack on SI and CI generation GPUs, amdgpu.si_support and amdgpu.cik_support need to be enabled in kernel Feature Support and Performance The AMD Open Source Driver for Vulkan is designed to support the following features: Vulkan 1.2 More than 30 extensions Radeon GPUProfiler tracing Built-in debug and profiling tools Mid-command buffer preemption and SR-IOV virtualization The following features and improvements are planned in future releases (Please refer to Release Notes for update of each release): Upcoming versions of the Vulkan API Hardware performance counter collection through RenderDoc LLPC optimizations to improve GPU-limited performance and compile time Optimizations to improve CPU-limited performance Known Issues CTS may hang in VK.synchronization.internally_synchronized_objects.pipeline_cache_compute with Linux kernel versions lower than 4.13 The driver can only work with firmware of ME feature version >= 25 (you can check the version with command \"sudo cat /sys/kernel/debug/dri/0/amdgpu_firmware_info\"). If you are using upstream stack with GPUs of SI or CI family, you may need to upgrade the kernel to 4.19 or later version and firmware (under /lib/firmware/amdgpu/) to the right version from https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/tree/amdgpu, and then update ramfs (sudo mkinitramfs -o /boot/initrd.img-`uname -r` `uname -r`) Timeline semaphore is not fully supported in Linux kernel until version 5.5. You can install Vulkan timeline semaphore layer to enable the extension if you are using earlier version of Linux kernel How to Contribute You are welcome to submit contributions of code to the AMD Open Source Driver for Vulkan. The driver is built from source code in four repositories: LLVM, XGL, LLPC and PAL. For changes to LLVM, you should submit contribution to the LLVM trunk. Commits there will be evaluated to merge into the amd-gfx-gpuopen-master branch periodically. For changes to XGL, LLPC and PAL, please create a pull request against the dev branch. After your change is reviewed and if it is accepted, it will be evaluated to merge into the master branch in a subsequent regular promotion. IMPORTANT: By creating a pull request, you agree to allow your contribution to be licensed by the project owners under the terms of the MIT License. When contributing to XGL, LLPC and PAL, your code should: Match the style of nearby existing code. Your code may be edited to comply with our coding standards when it is merged into the master branch. Avoid adding new dependencies, including dependencies on STL. Please make each contribution reasonably small. If you would like to make a big contribution, like a new feature or extension, please raise an issue first to allow planning to evaluate and review your work. Note: Since PAL is a shared component that must support other APIs, other operating systems, and pre-production hardware, you might be asked to revise your PAL change for reasons that may not be obvious from a pure Linux Vulkan driver perspective. Build Instructions System Requirements It is recommended to install 16GB RAM in your build system. Build System CMake 3.13.4 or newer is required. Download and install proper one if the cmake is older than 3.13.4. Ninja is requred. Install Dev and Tools Packages Ubuntu sudo apt-get install build-essential curl g++-multilib gcc-multilib git pkg-config python3 64-bit sudo apt-get install libssl-dev libx11-dev libxcb1-dev x11proto-dri2-dev libxcb-dri3-dev libxcb-dri2-0-dev libxcb-present-dev libxshmfence-dev libxrandr-dev libwayland-dev 32-bit dpkg --add-architecture i386 sudo apt-get install libssl-dev:i386 libx11-dev:i386 libxcb1-dev:i386 libxcb-dri3-dev:i386 libxcb-dri2-0-dev:i386 libxcb-present-dev:i386 libxshmfence-dev:i386 libwayland-dev libwayland-dev:i386 libxrandr-dev:i386 RedHat wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum localinstall epel-release-latest-7.noarch.rpm sudo yum update 64-bit sudo yum -y install openssl-devel gcc-c++ python3 curl glibc-devel libstdc++-devel libxcb-devel libX11-devel libxshmfence-devel libXrandr-devel wayland-devel 32-bit sudo yum -y install openssl-devel.i686 gcc-c++ python3 curl glibc-devel.i686 libstdc++-devel.i686 libxcb-devel.i686 libX11-devel.i686 libxshmfence-devel.i686 libXrandr-devel.i686 wayland-devel.i686 Get Repo Tools Ubuntu 18.04 sudo apt-get install repo Ubuntu 20.04, RedHat 7.8, 8.2 mkdir ~/bin curl https://storage.googleapis.com/git-repo-downloads/repo > ~/bin/repo # Replacing python with python3 is only needed on Ubuntu 20.04 if the 'python' executable is not available sed -i s/python/python3/ ~/bin/repo chmod a+x ~/bin/repo export PATH=~/bin:\"$PATH\" Get Source Code mkdir vulkandriver cd vulkandriver repo init -u https://github.com/GPUOpen-Drivers/AMDVLK.git -b master repo sync Note: Source code in dev branch can be gotten by using \"-b dev\" in the \"repo init\" command Build Driver and Generate JSON Files Ubuntu cmake -G Ninja -S xgl -B xgl/builds/Release64 cmake --build xgl/builds/Release64 cmake -G Ninja -S xgl -B xgl/builds/Release32 -DCMAKE_C_FLAGS=-m32 -DCMAKE_CXX_FLAGS=-m32 cmake --build xgl/builds/Release32 RedHat cmake -G Ninja -S drivers/xgl -B xgl/builds/Release64 cmake --build xgl/builds/Release64 cmake -G Ninja -S drivers/xgl -B xgl/builds/Release32 -DCMAKE_C_FLAGS=-m32 -DCMAKE_CXX_FLAGS=-m32 cmake --build xgl/builds/Release32 Note: For RedHat 7.x, please use cmake3(>= 3.13.4) instead of cmake. For debug build, use -DCMAKE_BUILD_TYPE=Debug -DLLVM_PARALLEL_LINK_JOBS=2 (Linking a debug build of llvm is very memory intensive, so we use only two parallel jobs). To enable Wayland support, use -DBUILD_WAYLAND_SUPPORT=ON. Installation Instructions Install Vulkan SDK You can download and install the SDK package here. Install dependencies Ubuntu sudo apt install libssl1.1 RedHat sudo yum install openssl-libs Install Driver and JSON Files sudo cmake --install xgl/builds/Release64 --component icd sudo cmake --install xgl/builds/Release32 --component icd If you want to install driver to customized directory, you can add \"-DCMAKE_INSTALL_PREFIX={installation directory}\" in the cmake build command. JSON files will be installed to /etc/vulkan/icd.d while other files will be installed to the installation directory you specified. If RADV is also installed in the system, AMDVLK driver will be enabled by default after installation. You can switch the driver between AMDVLK and RADV by environment variable AMD_VULKAN_ICD = AMDVLK or RADV. Note: The remaining steps are only required when running the AMDGPU upstream driver stack. Turn on DRI3 and disable modesetting X driver Add following lines in /usr/share/X11/xorg.conf.d/10-amdgpu.conf: Section \"Device\" Identifier \"AMDgpu\" Option \"DRI\" \"3\" EndSection And make sure following line is NOT included in the section: Required Settings On the AMDGPU upstream driver stack with libdrm version lower than 2.4.92, the max number of IB per submission MUST be limited to 4 (the default setting in AMD Open Source driver for Vulkan is 16). This can be accomplished via the Runtime Settings mechanism by adding the following line to amdPalSettings.cfg: MaxNumCmdStreamsPerSubmit,4 CommandBufferCombineDePreambles,1 Install with pre-built driver You could generate the installation package with below command while building driver: Ubuntu cmake -G Ninja -S xgl -B xgl/builds/Release64 [-DPACKAGE_VERSION=package version] cmake --build xgl/builds/Release64 --target makePackage RedHat cmake -G Ninja -S xgl -B xgl/builds/Release64 [-DPACKAGE_VERSION=package version] cmake --build xgl/builds/Release64 --target makePackage You could also download pre-built package from https://github.com/GPUOpen-Drivers/AMDVLK/releases for each code promotion in master branch. Below is the installation instruction: Ubuntu 18.04, 20.04 sudo dpkg -r amdvlk /* If old version is installed on the machine, remove it first */ sudo dpkg -i amdvlk_x.x.x_amd64.deb sudo apt-get -f install RedHat 7.8, 8.2 sudo rpm -e amdvlk /* If old version is installed on the machine, remove it first */ sudo rpm -i amdvlk-x.x.x.x86_64.rpm For Ubuntu, you could also install the latest driver build from https://repo.radeon.com: sudo wget -qO - http://repo.radeon.com/amdvlk/apt/debian/amdvlk.gpg.key | sudo apt-key add - sudo sh -c 'echo deb [arch=amd64,i386] http://repo.radeon.com/amdvlk/apt/debian/ bionic main > /etc/apt/sources.list.d/amdvlk.list' sudo apt-get remove amdvlk /* If old version is installed on the machine, remove it first */ sudo apt update sudo apt-get install amdvlk Runtime Settings The driver exposes many settings that can customize the driver's behavior and facilitate debugging. You can add/edit settings in amdVulkanSettings.cfg or amdPalSettings.cfg file under one of below paths, formatted with one name,value pair per line: /etc/amd $AMD_CONFIG_DIR Some example settings are listed below: Setting Name Valid Values Comment AllowVkPipelineCachingToDisk 0: disallow1: default 1 is default value which enables Pal's archive-file based caching.The archive-file is stored under ~/.cache/AMD/VkCache. ShaderCacheMode 0: disable cache1: runtime cache2: cache to disk Runtime cache is the default mode. For \"cache to disk\", the cache file is generated under $AMD_SHADER_DISK_CACHE_PATH/AMD/LlpcCache or $XDG_CACHE_HOME/AMD/LlpcCache or $HOME/.cache/AMD/LlpcCache IFH 0: default1: drop all submits Infinitely Fast Hardware. Submit calls are dropped before being sent to hardware. Useful for measuring CPU-limited performance. EnableVmAlwaysValid 0: disable1: default2: force enable 1 is the default setting which enables the VM-always-valid feature for kernel 4.16 and above. The feature can reduce command buffer submission overhead related to virtual memory management. IdleAfterSubmitGpuMask Bitmask of GPUs (i.e., bit 0 is GPU0, etc.) Forces the CPU to immediately wait for each GPU submission to complete on the specified set of GPUs. All available settings can be determined by examining below source files that define them. .../xgl/icd/settings/settings.cfg (API layer settings) .../pal/src/core/settings_core.json (PAL hardware-independent settings) .../pal/src/core/hw/gfxip/gfx6/settings_gfx6.json (PAL GFX6-8 settings) .../pal/src/core/hw/gfxip/gfx9/settings_gfx9.json (PAL GFX9+ settings) Runtime settings are only read at device initialization, and cannot be changed without restarting the application. If running on a system with multiple GPUs, the same settings will apply to all of them. Lines in the settings file that start with ; will be treated as comments. Enable extensions under development The extensions under development are not enabled by default in driver. You can enable them through environment variable: export AMDVLK_ENABLE_DEVELOPING_EXT=\"<extension1-name> [<extension2-name>...]\" or export AMDVLK_ENABLE_DEVELOPING_EXT=\"all\" The extension name is case-insensitive. PAL GpuProfiler Layer The GpuProfiler is an optional layer that is designed to intercept the PAL interface to provide basic GPU profiling support. Currently, this layer is controlled exclusively through runtime settings and outputs its results to file. You can use the following Runtime Settings to generate .csv files with GPU timings of work performed during the designated frames of an application (one file for each frame): Setting Name Value Comment GpuProfilerMode 0: disable1: enable with sqtt off2: enable with sqtt for thread trace3: enable with sqtt for RGP Enables and sets the SQTT mode for the GPU performance profiler layer. Actual capture of performance data must be specified via frame number with GpuProfilerConfig_StartFrame or by pressing shift-F11. GpuProfilerConfig.LogDirectory <directory-path> The directory path is relative to $AMD_DEBUG_DIR or $TMPDIR or /var/tmp/, default value is \"amdpal/\". Your application must have write permissions to the directory. The profiling logs are output to a subdirectory that is named in the format like <AppName><yyyy-MM-dd><HH:mm:ss>. GpuProfilerConfig.Granularity 0: per-draw1: per-cmdbuf Defines what is measured/profiled. Per-draw times individual commands (such as draw, dispatch, etc.) inside command buffers, while per-cmdbuf only profiles entire command buffers in aggregate. GpuProfilerConfig.StartFrame Positive integer First frame to capture data for. If StartFrame and FrameCount are not set, all frames will be profiled. GpuProfilerConfig.FrameCount Positive integer Number of frames to capture data for. GpuProfilerConfig.RecordPipelineStats 0, 1 Gathers pipeline statistic query data per entry if enabled. You can use the script timingReport.py to analyze the profiling log: python timeReport.py <profiling_log_subdirectory> Dump Pipelines and Shaders The output of timeReport.py includes the information of top pipelines like below: Top Pipelines (>= 1%) Compiler Hash | Type | Avg. Call Count | Avg. GPU Time [us] | Avg. Frame % 1. 0xd91d15e42d62dcbb | VsPs | 43 | 11,203.15 | 10.20 % 2. 0x724e9af55f2adf1b | Cs | 1 | 9,347.50 | 8.51 % 3. 0x396e5ad6f7a789f7 | VsHsDsPs | 468 | 8,401.35 | 7.65 % You can add the following settings to amdPalSettings.cfg to dump the information of each pipeline: EnablePipelineDump,1 PipelineDumpDir,<dump_dir_path> PipelineDumpDir is a sub-path relative to $AMD_DEBUG_DIR or $TMPDIR or /var/tmp/, default value is \"spvPipeline/\". The pipeline dump file is named in the format like Pipeline<Type>_<Compiler_Hash>.pipe. For example, the above top 1 pipeline is dumped to PipelineVsFs_0xD91D15E42D62DCBB.pipe. The shaders referenced by each pipeline are also dumped to .spv files. PAL Debug Overlay PAL's debug overlay can be enabled to display real time statistics and information on top of a running application. This includes a rolling FPS average, CPU and GPU frame times, and a ledger tracking how much video memory has been allocated from each available heap. Benchmarking (i.e., \"Benchmark (F11)\") is currently unsupported. Setting Name Value Comment DebugOverlayEnabled 0, 1 Enables the debug overlay. DebugOverlayConfig.DebugOverlayLocation 0: top-left1: top-right2: bottom-left3: bottom-right Determines where the overlay text should be displayed. Can be used to avoid collision with important rendering by the application. DebugOverlayConfig.PrintFrameNumber 0, 1 Reports the current frame number. Useful when determining a good frame range for profiling with the GpuProfiler layer. DebugOverlayConfig.TimeGraphEnable 0, 1 Enables rendering of a graph of recent CPU and GPU frame times. Third Party Software The AMD Open Source Driver for Vulkan contains code written by third parties. LLVM is distributed under the Apache License v2.0 with LLVM Exceptions. See LICENSE.TXT file in the top directory of the LLVM repository. MetroHash is distributed under the terms of Apache License 2.0. See LICENSE file in the top directory of the MetroHash repository. CWPack is distributed under the terms of MITLicense. See LICENSE file in the top directory of the CWPack repository. Please see the README.md file in the PAL, LLPC and XGL repositories for information on third party software used by those libraries. DISCLAIMER The information contained herein is for informational purposes only, and is subject to change without notice. This document may contain technical inaccuracies, omissions and typographical errors, and AMD is under no obligation to update or otherwise correct this information. Advanced Micro Devices, Inc. makes no representations or warranties with respect to the accuracy or completeness of the contents of this document, and assumes no liability of any kind, including the implied warranties of noninfringement, merchantability or fitness for particular purposes, with respect to the operation or use of AMD hardware, software or other products described herein. No license, including implied or arising by estoppel, to any intellectual property rights is granted by this document. Terms and limitations applicable to the purchase or use of AMD's products are as set forth in a signed agreement between the parties or in AMD's Standard Terms and Conditions of Sale. AMD, the AMD Arrow logo, Radeon, FirePro, and combinations thereof are trademarks of Advanced Micro Devices, Inc. Other product names used in this publication are for identification purposes only and may be trademarks of their respective companies. Vega is a codename for AMD architecture, and is not a product name. Linux is the registered trademark of Linus Torvalds in the U.S. and other countries. Vulkan and the Vulkan logo are registered trademarks of the Khronos Group, Inc. ",
        "_version_": 1718536439938940928
      },
      {
        "story_id": 19894673,
        "story_author": "peter_d_sherman",
        "story_descendants": 45,
        "story_score": 185,
        "story_time": "2019-05-12T21:23:50Z",
        "story_title": "Awesome Pascal – A curated list of Delphi, FreePascal, and Pascal shiny things",
        "search": [
          "Awesome Pascal – A curated list of Delphi, FreePascal, and Pascal shiny things",
          "https://github.com/Fr0sT-Brutal/awesome-pascal",
          "Awesome Pascal A curated list of awesome Delphi, FreePascal and other *Pascal frameworks, libraries, resources, and shiny things. Inspired by awesome-xxx stuff. Note that only open-source projects are considered. Dead projects (not updated for 3 years or more) must be really awesome or unique to be included. Feel free to suggest other missing nice projects either by comments or pull requests. This awesome collection is also available on Delphi.ZEEF.com Note on compiler compatibility. There are compiler/language dialect compatibility badges for all projects based on a project's description. No real compatibility with compilers not officially supported is checked. Often a code could be used with non-supported compiler/language dialect with minor modifications but there could be exceptions. Contents General Libraries Multimedia Audio Video Graphic Game dev Communications Network Serial port GUI Control packs Single controls Editors Viewers Other GUI Database Scripting Machine Learning Non-visual Classes/Utils Compression Encryption XML/JSON/YAML/HTML Language Memory managers System Template Logging Math Command-line Other non-visual OS Report generating Unit Testing Debugging / error handling Utilities RAD Studio IDE plugins/wizards Plugins for other IDE's Documentation Code check/review, debug Setup Other General Libraries Big general-purpose libraries JCL. [Delphi] [FPC] Set of thoroughly tested and fully documented utility functions and non-visual classes which can be instantly reused in your Delphi and C++ Builder projects. The library is grouped into several categories such as Strings, Files and I/O, Security, Math and many, many more. JVCL. [Delphi] Library of over 600 Delphi components developed by \"Project JEDI\" members. // GUI, algorithms, classes, API headers etc. Alcinoe (mirror at GH). [Delphi] Library of visual and non-visual components for Delphi. // Network: FTP/Http/NNTP/POP3/SMTP, ISAPI, WinInet Http/FTP clients; DB: Firebird/MySQL/SQLite3/Memcached/MongoDb/SphinxQL; XML/JSON Parser; ZLIB; Cryptography: AES, Blowfish, MD5, SHA, secure keyed MD5/SHA; opengl video player; FireMonkey controls; Other: Self-Balancing Binary Trees, expression evaluator Fundamentals Code Library (abandoned, more recent fork is here - though it slightly differs in units set, f.ex. no XML. Recent major version 5 here). [Delphi] [FPC] Collection of Delphi / FreePascal code units. Includes libraries for Unicode, Strings, Data Structures, Sockets and Mathematics. // Utils: ZLIB compression; JSON; XML; ProtocolBuffers; Unicode routines; data structures; Hashes: XOR, CRC, Adler, MD5, SHA, secure keyed MD5/SHA, etc; Network: blocking TCP client/server, HTTP(S) via SSL3/TLS1.0/TLS1.1/TLS1.2 (fully native); SQL parser; BitCoin MtGox client; Blaise script engine; Cipher: AES, DES, FUNE, RC2, RC4, RSA, Diffie-Hellman; Maths: matrix, complex, statistics, huge numbers Spring4D. [Delphi] Open-source code library for Embarcadero Delphi 2010 and higher. It consists of a number of different modules that contain a base class library (common types, interface based collection types, reflection extensions) and a dependency injection framework. Includes Encryption Library. // Collections and other containers using Generics and based on IEnumerable, probably more accurate and featured than RTL analogs; crypto: CRC, DES, MD5, SHA; file utils etc TheUnknownOnes. [Delphi] Huge heap of classes, components, utilities for almost every purpose. Nearly undocumented and seems not very up-to-date though. CNVCL. [Delphi] CnPack Component Package. Large collection of visual components, classes and utilities. // Lots of useful stuff; documentation and comments mainly in Chinese mORMot. [Delphi] [FPC] Client-Server ORM/ODM SOA MVC framework for Delphi 6 and higher, or FPC 2.7. Direct SQL/NoSQL database access, ORM/ODM over objects, RESTful ORM and SOA services via interfaces over high performance HTTP server, MVC/MVVM web sites, testing including mocks and stubs, logging, cryptography, compression, huge documentation. MARS - Curiosity. [Delphi] Delphi REST Library. Pure REST approach, standard concepts in a familiar Delphi flavor (including a component based client library). Known compatibility: Delphi versions from XE to 10 Seattle. Some functionalities requires FireDAC. ADAPT. [Delphi] Advanced Developer Async Programming Toolkit, foundation library intended to be used at the heart of your projects for the purpose of providing extremely powerful, multi-threaded (and thread-safe) capabilities. Event Engine - a very powerful system for producing Multi-Threaded, Asynchronous and Event-Driven programs. Generics Collections - highly efficient Collection Types (Lists, Trees, Maps etc.). Math Library - a library for Unit Conversion, special calculation and other useful mathematics routines. Package Engine - extension of the Streamables Engine supporting the packaging of files together (a VFS of sorts). Shared Streams Library - 100% Thread-Safe Stream Classes (Interfaced too) allowing read/write from multiple Threads. Stream Handling Library - makes working with Streams much easier! Handles Deleting, Inserting, Reading and Writing data. Redux Delphi. [Delphi] Predictable state container for Delphi apps utilizing a unidirectional data flow. Inspired by ReduxJS. Comes with Immutable Generic List. GrijjyFoundation. [Delphi] Foundation classes and utilities that are used throughout the other Grijjy Repositories. // BSON/JSON, IOCP/EPOLL sockets, socket pools, HTTP, HTTP/2, OpenSSL, ProtocolBuffers. unRxLib. [Delphi] Effort to keep RxLibrary (library of 60+ components) actual. QuickLib. [Delphi] [FPC] Quick development library (AutoMapper, LinQ, IOC Dependency Injection, MemoryCache, Scheduled tasks, Config, Serializers, Json Serialize, Chronometer, Threads, Lists, Config, Console services etc) with crossplatform support for Delphi/Firemonkey (Windows,Linux,macOS/IOS/Android) and freepascal (Windows/Linux). KOL. [Delphi] [FPC] (KOL-CE port to FPC) KEY OBJECTS LIBRARY for Delphi (and FPC) - to make applications small and power. This library is freeware and open source. MCK is a kit of mirror classes for the VISUAL project development in Delphi environment using KOL library. cwRuntime. [Delphi] [FPC] Compiler agnostic and cross platform collection of utility libraries for Delphi and FreePascal. It is heavily interface based, offering ARC based memory management features and flexible implementation abstraction, with the goal of forming a source bridge for developers familiar with the two supported compilers. Unit testing, collections/containers, multiplatform interface for loading dynamic libraries, Unicode utils, interfaces for working with streams and buffers, logging, threading, high-precision timers, sockets. Multimedia Audio Audio Tools Library. [Delphi] For manipulating many audio formats file information. // Abandoned since 2005. Delphi ASIO & VST Project. [Delphi] Framework for writing applications using the ASIO interface and VST plugins. It comes with countless DSP algorithms all demonstrated in dozens of examples. // Not very active lately, but the trunk is in a usable state NewAC - New Audio Components (abandoned, list of forks on GH here). [Delphi] Designed to help your Delphi programs perform different sound processing tasks. With NewAC you can play audio stored in many formats (wav, Ogg Vorbis, FLAC, Monkey Audio, WavPack, MP3, Windows WMA, DTS, AC-3 (Dolby Surround), VOB (DVD files)). // Playback, recording, tag read/write, some audio editing tasks and conversions Audorra. [Delphi] [FPC] Digital audio library for Delphi and Freepascal. Using a flexible plugin architecture, it allows you to exchange the audio backend (e.g. WaveOut, OpenAL), add protocol classes (e.g. file, http) and decoders. Delphi-BASS. [Delphi] Delphi's FMX and VCL header/wrapper units for BASS audio library plus add-ons. Video DSPack (abandoned, active fork is here). [Delphi] Set of components and classes to write Multimedia Applications using MS Direct Show and DirectX technologies. Delphi-OpenCV. [Delphi] Translation of OpenCV library header files in Delphi // Includes FFMPEG headers FFmpeg Delphi/Pascal Headers. [Delphi] [FPC] Open source translation of FFMPEG headers. PasLibVlc. [Delphi] [FPC] Interface to VideoLAN libvlc.dll and VCL player component for Delphi / FreePascal based on VideoLAN fevh264. [FPC] Baseline h.264 encoder. Windows and Linux are supported Graphic Image files, free drawing, barcodes etc. There are also some drawing engines in Game dev section Graphics32. [Delphi] [FPC] Designed for fast 32-bit graphics handling on Delphi, Kylix and Lazarus. Optimized for 32-bit pixel formats, it provides fast operations with pixels and graphic primitives, and in most cases Graphics32 outperforms the standard TCanvas classes. It is almost a hundred times faster in per-pixel access and about 2-5 times faster in drawing lines. GraphicEx. [Delphi] Addendum to Delphi's Graphics.pas to enable your application to load many common image formats. This library is primarily designed to load images as background (buttons, forms, toolbars) and textures (DirectX, OpenGL) or for image browsing and editing purposes as long as you don't need to save images. Vampyre Imaging Library. [Delphi] [FPC] Cross-platform native Object Pascal (Delphi and Free Pascal) image loading, saving, and manipulation library. CCR-EXIF (seems abandoned, list of forks on GH here). [Delphi] Library to read and write Exif, IPTC and XMP metadata from JPEG, TIFF and PSD images. KIcon. [Delphi] [FPC] This component makes sense if a more complex manipulation with icons (or better icon files *.ico) than just viewing is needed. Full PNG icon image support, correct rendering, icons with alpha channel. Delphi Twain. [Delphi] [FPC] The library allows you to easily access scanning functions from Delphi and Lazarus. Synopse PDF. [Delphi] [FPC] Fully featured Open Source PDF document creation library for Delphi, embedded in one unit. Pure Delphi code, Delphi 5 up to Delphi 10.3 Rio (and latest version of FPC), for Win32 and Win64 platforms. PowerPDF. [Delphi] VCL component to create PDF document visually. Like Forms, you can design PDF document easily on Delphi or C++Builder IDE. IGDI+. [Delphi] The free open source library allows quick and easy implementations of complex GDI+ applications, in a natural Delphi-friendly code. GLScene. [Delphi] [FPC] OpenGL based 3D library for Delphi, C++Builder and Lazarus. It provides visual components and objects allowing description and rendering of 3D scenes in an easy, no-hassle, yet powerful manner. GLScene is not just an OpenGL wrapper or utility library, it has grown to become a set of founding classes for a generic 3D engine with Rapid Application Development in mind. GLScene allows you to quickly design and render 3D scenes without having to learn the intricacies of OpenGL, if you know how to design a TForm, you'll easily master the basic operations of GLScene. The library comes with a large collections of demos showcasing the ease of use, and demonstrating RAD wasn't done at the expense of CPU/GPU horsepower. SynGdiPlus. [Delphi] [FPC] Enables an application to load and save GIF, TIF, PNG and JPG pictures. It also allows anti-aliased drawing from any TMetaFile. That is, you can play a .emf content using GDI+ instead of GDI, for much better rendering result. Andorra 2D. [Delphi] [FPC] New generation 2D Engine for Delphi and Lazarus. Andorra 2D is capable to use DirectX or OpenGL through graphic plugins. Andorra 2D is built in a very modular way and is yet easy to use. Transparent-canvas. [Delphi] Delphi VCL / Windows project for drawing semi-transparent alphablended graphics. It provides a class similar to TCanvas. Fully-justified-text. [Delphi] Delphi VCL / Windows project for text output, allowing printing of fully justified text with a variety of options. AsciiImage. [Delphi] AsciiImage-Implementation for Delphi by Alexander Benikowski based on AsciiImage by Charles Parnot. Read more on his article. // Creates scalable monochrome image from ASCII pixel map PngComponents. [Delphi] PngComponents is a set of components that allows you to include in your application real PNG files. PNG files on their own do not generate an enourmous advantage, but their support for an alpha-channel does indeed have quite a charm to it. AggPasMod. [Delphi] Modernized Pascal Anti-Grain Geometry. Based on AggPas, which is itself based on the Anti-Grain Geometry, this project offers support for the latest Delphi Versions (XE and above) and contains some helper classes (VCL components and FireMonkey interface). 2D vector graphics library. Basically, you can think of AggPas as of a rendering engine that produces pixel images in memory from some vectorial data. But of course, AGG can do much more than that. // Vector graphic library, renders SVG and much more delphi-shader. [Delphi] Hundreds of graphical effects, and a library that provides GLSL functionality in pure Delphi code. This project produces an executable with more than a hundred real-time graphical effects. All that is a 100% pascal implementation, without the use of external libraries or hardware acceleration. dglOpenGL. [Delphi] [FPC] Delphi / Pascal OpenGL header translation. DelphiZXingQRCodeEx. [Delphi] [FPC] Delphi/Lazarus port of the QR Code generating functionality from ZXing, an open source barcode image processing library. ZXing.Delphi. [Delphi] Native Object Pascal library for Delphi XE to 10.2 Tokyo that is based on the well known open source Barcode Scanning Library ZXing (Zebra Crossing). It is aimed at all of the FireMonkey mobile platforms and, starting from v3.1, it fully supports also Windows VCL applications (no dependencies on FMX.Graphics unit). Zint-Barcode-Generator-for-Delphi. [Delphi] Native Delphi port of Zint-Barcode-Generator. QuickImageFX. [Delphi] Delphi library for simplifying image load/save, conversion and transformation. Load/save png, jpg, gif and bmp. get image from different resources: file, stream, http, imagelist, associated windows icon, executable file icon, etc. Rotate, flip, grayscale and many other transformations. NativeJpg. [Delphi] Fully object-oriented Pascal implementation that allows to read and write Jpeg files. You can use this software to read and write Jpeg images from files or streams. It supports baseline and progressive Jpeg, support for metadata, as well as all conceivable lossless operations. OpenGL Pascal Toolkit. [FPC] Easy to use native pascal toolkit that allows to create and manage OpenGL contexts in a platform independent way. BGRAbitmap. [Delphi] [FPC] Drawing routines with transparency and antialiasing with Lazarus. Offers also various transforms. These routines allow to manipulate 32bit images in BGRA format or RGBA format (depending on the platform). Clipper. [Delphi] Library performs line & polygon clipping - intersection, union, difference & exclusive-or, and line & polygon offsetting dexif. [Delphi] [FPC] Lazarus port of Delphi EXIF Library to extract Exif Information from Images FontIconEditor. [Delphi] Simple component editor that allow you to add icons to a TImageList from a font. You can use any font you want. IconFontsImageList. [Delphi] Extended ImageList for Delphi (VCL & FMX) to simple use and manage Icon Fonts (with GDI+ support) Mundus. [Delphi] Software renderer written in Delphi. Currently supports only Win32 as it makes use of some inline assembler. Image32. [Delphi] [FPC] (Website) 2D graphics library written in Delphi Pascal. It provides an extensive range of image manipulation functions and includes a line and polygon renderer supporting a wide range of brush filling options. SVGIconImageList. [Delphi] Four engines to render SVG (Delphi TSVG, Delphi Image32, Direct2D or Cairo) and four components to simplify use of SVG images (resize, fixedcolor, grayscale, etc). Skia4Delphi. [Delphi] Cross-platform 2D graphics API for Delphi platforms based on Google's Skia Graphics Library. It provides a comprehensive 2D API that can be used across mobile, server and desktop models to render images. Game dev There are also some drawing engines suitable for game dev in Graphic section RecastNavigation. [Delphi] Navigation mesh construction toolset for games. Recast is accompanied with Detour, path-finding and spatial reasoning toolkit. You can use any navigation mesh with Detour, but of course the data generated with Recast fits perfectly. This is a port of the original RecastNavigation written in C++. Kraft Physics Engine. [Delphi] [FPC] Open source Object Pascal physics engine library that can be used in 3D games. Compatible with: Delphi 7-XE7 (but not with the Android and iOS targets), FreePascal >= 2.6.2 (with almost all FPC-supported targets including Android and iOS) ZenGL. [Delphi] [FPC] OpenGL Cross-platform game development library written in Pascal, designed to provide necessary functionality for rendering 2D-graphics, handling input, sound output, etc. // Not updated lately, but is working ok Asphyre aka Platform eXtended Library (PXL). [Delphi] [FPC] Cross-platform framework for developing 2D/3D video games, interactive and scientific applications. It aids the developer with mathematics, hardware control, resource management, displaying real-time graphics and text, handle user input and network communication capabilities. CrystalPathFinding. [Delphi] [FPC] Simple and effective library with an open source intended for the searching of the shortest paths by algorithms A*/WA* for maps based on tiles with 4 (simple), 8 (diagonal/diagonalex) or 6 (hexagonal) neighbors. Allegro-Pas (GitHub). [Delphi] [FPC] Wrapper to use the Allegro game library with Pascal/Delphi. Castle Game Engine. [FPC] Complete Pascal Game Engine. Cross-platform 3D and 2D game engine with a lot of graphic effects and a scene graph based on X3D. TileEngine. (GitHub) [Delphi] [FPC] OOP Pascal Wrapper and bindings for Tilengine 2D retro graphics engine. Tilengine is a cross-platform 2D graphics engine for creating classic/retro games with tilemaps, sprites and palettes. Its scanline-based rendering algorithm makes raster effects a core feature, a technique used by many games running on real 2D graphics chips. SDL2 (GitHub). [Delphi] [FPC] Pascal SDL 2 Headers. Simple DirectMedia Layer is a cross-platform development library designed to provide low level access to audio, keyboard, mouse, joystick, and graphics hardware via OpenGL and Direct3D. SFML. [Delphi] [FPC] Pascal SFML Headers. SFML provides a simple interface to the various components of your PC, to ease the development of games and multimedia applications. It is composed of five modules: system, window, graphics, audio and network. Currently Delphi and FPC/Lazarus are supported. However, due to a compiler incompatibility with the Delphi compiler (solved with workarounds), FPC is recommended at the moment. pasvulkan. [Delphi] [FPC] Vulkan header generator, OOP-style API wrapper, framework and prospective Vulkan-based game engine for Object Pascal. DarkGlass. [Delphi] DarkGlass is a general purpose game engine written using Delphi. JEDI-SDL. [Delphi] [FPC] Pascal headers for SDL from JEDI. Works with Delphi, Kylix, Free Pascal, Gnu Pascal and TMT Pascal. Apus Game Engine. [Delphi] [FPC] Cross-platform library for making mostly 2D games, GUI applications and web services. Supports UI, text rendering, on-fly localization, particles, basic scripting and many lower level subsystems. Uses OpenGL/GLES and DirectX. Delphi3D Engine. [Delphi] A 3D-graphic and game engine for Delphi and Windows Communications Network Socket communication, network protocols, encodings, etc Internet Component Suite. [Delphi] Asynchronous-based library composed of various Internet components and applications. Clients/servers for TCP, UDP, raw sockets, FTP, SMTP, POP3, NNTP, HTTP, Telnet and more. Supports SSL and TLS with the help of OpenSSL. Also includes Mime Decoder, SHA1/MD4/MD5 hashes, DES encryption. Indy. [Delphi] [FPC] Network components for Delphi, C++Builder, Delphi.NET, and FreePascal // All-in-one network library based on blocking sockets and threads. Included in default RAD studio installation since 2006. Ararat Synapse. [Delphi] [FPC] Pascal TCP/IP Library for Delphi, C++Builder, Kylix and FreePascal. Deals with network communication by means of blocking (synchronous) sockets or with limited non-blocking mode. This project not using asynchronous sockets! The Project contains simple low level non-visual objects for easiest programming without problems (no required multithread synchronisation, no need for windows message processing, etc) Great for command line utilities, visual projects, NT services, etc // TCP, UDP, ICMP, RAW; ICMP, DNS, SMTP, HTTP, SNMP, NTP, FTP, LDAP, NNTP, Telnet; IPv6; SOCKS proxy; SSL/TLS (via OpenSSL or Windows CryptoApi); PING; character code transcoding; MIME coding and decoding; CRC16, CRC32, MD5 and HMAC-MD5. Internet Professional. [Delphi] Set of VCL components providing Internet connectivity for Borland Delphi & C++Builder. iPRO includes POP3, SMTP, NNTP, FTP, HTTP, Instant Messaging, & HTML viewer components, as well as components for low-level socket access. // Seems abandoned but contains pretty large set of features incl ICMP, POP, SMTP, HTTP, NNTP, NTP, FTP, SMTP; HTML parser and viewer; MIME utils; cookies, certificates, caching, encryption etc SynCrtSock. [Delphi] [FPC] Features several sockets and HTTP client-server classes, including a high-performance http.sys based server under Windows, and a new thread-pool powered socket server. // Also implements http.sys binding under Windows and cURL binding under nix TML Messaging Suite. [Delphi] [FPC] Network messaging library for rapid development of extensible and scalable interfaces. Based on the peer to peer standard protocol BEEP (Blocks Extensible Exchange Protocol), defined in RFC3080 and RFC3081. libTML is suitable for many use cases and communication patterns. Equipped with a type safe data API, TML can transport hierarchical data structures fast and reliable. // The libTML Object Pascal Components are not only a language binding to the core library but a complete set of non visual components to simplify the usage of libTML with Embarcadero RAD Studio and Lazarus. DMVCFramework. [Delphi] Popular and powerful framework for web solution in Delphi. Delphi IOCP. [Delphi] Implements several network classes based on Windows IOCP technology. Socket, HTTP, Ntrip servers and clients. // Quite well documented and good styled code but Chinese only. delphi-aws-ses. [Delphi] Amazon Simple Email Service (AWS SES) library for Delphi applications. delphi-slackbot. [Delphi] Delphi library to send messages on Slack using slackbot. Kitto. [Delphi] Allows to create Rich Internet Applications based on a data model that can be mapped onto any database. The client-side part uses ExtJS (through the ExtPascal library) to create a fully AJAX application, allowing you to build standard and advanced data-manipulating forms in a fraction of the time. Kitto is aimed at Delphi developers that need to create web application without delving into the intricacies of HTML, CSS, JavaScript or learning to use a particular library such as ExtJS, yet it allows access to the bare metal if required. Daraja Framework. [Delphi] [FPC] Lightweight HTTP server framework for Object Pascal (Delphi 2009+ / Free Pascal 3.0). Implementing RESTful services is supported via the daraja-restful extension. Alcinoe. FTP/Http/NNTP/POP3/SMTP, ISAPI, WinInet Http/FTP clients. Fundamentals Code Library. Blocking TCP client/server, HTTP(S) via SSL3/TLS1.0/TLS1.1/TLS1.2 (fully native). mORMot. RESTful ORM and SOA services via interfaces over high performance HTTP server, MVC/MVVM web sites SDriver. [Delphi] Delphi wrapper for Slack API. Hprose for Delphi/Lazarus. [Delphi] [FPC] High Performance Remote Object Service Engine. It is a modern, lightweight, cross-language, cross-platform, object-oriented, high performance, remote dynamic communication middleware. It is not only easy to use, but powerful. This project is the implementation of Hprose for Delphi/Lazarus. TelegAPI. [Delphi] Library for working with Telegram messenger Bot API in Delphi. fp-telegram. [FPC] Library for working with Telegram bots API in FreePascal/Lazarus. DelphiZeroMQ. [Delphi] Delphi implementation of ZeroMQ Majordomo protocol and CZMQ high level binding. GrijjyFoundation. IOCP/EPOLL sockets, socket pools, HTTP, HTTP/2, OpenSSL, ProtocolBuffers. STOMP Client. [Delphi] [FPC] STOMP client for Embarcadero Delphi and FreePascal. The project can use INDY (Delphi) or Synapse (Delphi or FreePascal). delphiXero. [Delphi] XERO cloud accounting API for Delphi. BesaSoap. [Delphi] The BesaSoap library is designed to help programmers develop faster and more native web service client applications. Represents C# or Java like native class support, nullable data types and custom attributes. IndySoap. [Delphi] Open Source Library for implementing Web services using Delphi/CBuilder Compilers. IndySoap isn't tied to Indy for transport services, though Indy based transport services are included. Fano Framework. [FPC] Web application framework for modern Pascal programming language. It is written in Free Pascal. Internet Tools. XPath/XQuery/JSONiq/CSS/HTML; functions to perform HTTP/S requests on Windows/Linux/macOS/Android, an XSLT-inspired webscraping language, and an auto update class. Delphi Cross Socket. [Delphi] Delphi cross platform socket library (Chinese). Uses different IO models for different platforms: IOCP (Windows), KQUEUE (FreeBSD(macOS, iOS, etc)), EPOLL (Linux(Linux, Android)). ToroKernel. [FPC] This is a library-kernel that allows freepascal applications which are specially ported to run alone in the system. Toro is compiled within the user's application thus resulting in a single binary that can boot on baremetal or as a guest in a modern hypervisor,e.g., hyperv, kvm, qemu, firecraker. ToroKernel addresses the development of microservices by providing a dedicated API. Horse. [Delphi] [FPC] Fast and minimalist web framework. Horse allows to create powerful RESTful servers without effort. Focused on microservices. Bauglir WebSocket. [Delphi] [FPC] WebSocket server/client implementation based on Ararat Synapse. Delphi-RabbitMQ. [Delphi] RabbitMQ driver for Delphi DelphiGrpc. [Delphi] Implementation of the realtime and streaming gRPC protocol Google API for Delphi. [Delphi] Google API for Delphi Delphi JOSE and JWT Library. [Delphi] Delphi implementation of JOSE (JSON Object Signing and Encryption) and JWT (JSON Web Token) WiRL. [Delphi] Project was created to simplify RESTful service implementation in Delphi but, more importantly, to enable maximum interoperability with REST clients written in other languages and tools OpenSSL. [Delphi] Delphi wrapper for OpenSSL Thrift Delphi Software Library. [Delphi] Lightweight, language-independent software stack for point-to-point RPC implementation. Thrift provides clean abstractions and implementations for data transport, data serialization, and application level processing. The code generation system takes a simple definition language as input and generates code across programming languages that uses the abstracted stack to build interoperable RPC clients and servers. Thrift makes it easy for programs written in different programming languages to share data and call remote procedures. With support for 28 programming languages, chances are Thrift supports the languages that you currently use. Delphi Modbus. [Delphi] [FPC] Implementation of a ModbusTCP protocol master and slave over TCP/IP. RESTRequest4Delphi. [Delphi] RESTRequest4Delphi is a API to consume REST services written in any programming language. Designed to facilitate development, in a simple and minimalist way. LazWebsockets. [FPC] This provides a small Websocket server and client implementation written for the FPC and Lazarus. It is fully based upon the fcl ssockets unit and therefore independent from any additional dependencies except from the FCL. NetCom7. [Delphi] This set of components is the fastest possible implementation of socket communications, in any language; this is an extremely optimised code on TCP/IP sockets. VK API. [Delphi] Library for working with Vkontakte (Russian social network) API in Delphi. Full API (with Bot samples). AWS SDK for Dephi. [Delphi] Unofficial AWS (Amazon Web Services) SDK for Delphi. WARNING! Requires paid libs from TMS Voice Communication. [Delphi] Voice Communicator Components. // Implement RTP, RTSP, SHOUT, SNTP, STUN protocols and multiple audio format endocing/deconding libPasCURL. [Delphi] [FPC] Bindings and wrapper around cURL library. libcurl is the library is using for transferring data specified with URL syntax, supporting HTTP, HTTPS, FTP, FTPS, GOPHER, TFTP, SCP, SFTP, SMB, TELNET, DICT, LDAP, LDAPS, FILE, IMAP, SMTP, POP3, RTSP and RTMP. Delphi_SChannelTLS. [Delphi] Helper functions and socket classes to perform TLS communication by means of WinAPI (SChannel). Includes Overbyte ICS TWSocket descendant class. Serial port Synaser. [Delphi] [FPC] Library for blocking communication on serial ports. It is non-visual class as in Synapse, and programmer interface is very similar to Synapse. Async Professional (Newest and maintained version for recent compiler version only). [Delphi] Comprehensive communications toolkit for Embarcadero Delphi, C++Builder, & ActiveX environments. It provides direct access to serial ports, TAPI and the Microsoft Speech API (TTS/Speech recognition). It supports faxing, terminal emulation, VOIP, RAS dial & more. // Seems outdated (last update in 2011) but adapted to XE and should be easy to use in newer versions. The project is also very thoroughly documented. Second link points to an adapted version for newest compiler versions. TComPort. [Delphi] Delphi/C++ Builder serial communications components. It is generally easy to use for basic Serial Communications purposes. // Seems abandoned since 2011 GUI Visual components Control packs Large sets of GUI controls Cindy components. [Delphi] Packages with 71 components: VCL controls (labels, buttons, panels, Edits, TabControls, StaticText) with features like background gradient, colored bevels, wallpaper, shadowText, caption orientation etc. Orpheus (Newest and maintained version for recent compiler version only). [Delphi] Award-winning UI toolkit for Borland Delphi & C++Builder. It contains over 120 components covering everything from data entry to calendars and clocks. Other noteworthy components include an Object Inspector, LookOut bar & report views. // Advanced edits, comboboxes, grids + component (de)serializers. GUI components look rather old-style, theme support might be limited. Package contains many demos but no docs seem available. Second link points to an adapted version for newest compiler versions. KControls. [Delphi] [FPC] Control components. All controls have been written with the aim to become both cross-IDE compatible (Delphi/C++Builder VCL and Lazarus LCL) and cross-platform compatible in Lazarus. // Most useful are TKGrid with its DB-aware heritage TKDBGrid a very full-featured grid implementation incl. inplace editors. There's also hex editor, print preview, editors, labels, buttons etc. D.P.F Delphi Android / D.P.F Delphi iOS native components. [Delphi] D.P.F Delphi Native Components, 100% iOS Performance and styles. Develop iPhone & iPad & iPod Touch applications with fast native performance and native styles. Use native Android controls and services. Fast native performance. Mixed with FM VCL controls. Can be quick updated with latest Android controls & features. Essentials. [Delphi] Contains 13 native VCL controls for Embarcadero Delphi and C++Builder. The controls include drop-down calendars and calculators, roll-up dialogs, 3-D labels, tiled backgrounds, scrolling messages, menu buttons, and more. FreeEsVCLComponents. [Delphi] Free library of VCL components for Delphi and C++Builder. This new controls and components to improve the appearance applications and to better user experience. Components support visual styles and has modern style. All components has best support transparency, not flicker, and has support interesting possibility for double buffering for TGraphicControl heirs. SpTBXLib. [Delphi] Add on package for Toolbar2000 components, it adds the following features: Skins, Unicode support, Custom painting events and many more. Kastri. [Delphi] Cross-platform library which builds upon the existing RTL and FMX libraries in Delphi. Supports a number of newer APIs that you won't find in FMX/RTL, and \"backfills\" for missing APIs DelphiUCL. [Delphi] UWP controls for Delphi VCL. JPPack. [Delphi] [FPC] Collection of VCL components for Delphi and LCL components for Lazarus and CodeTyphon - buttons, panels, LinkLabel, ProgressBar, ColorComboBox, ColorListBox, Timer and other DDuce. [Delphi] Components, modules, extensions and primitives using Delphi new language features like operator overloading, attributes, generics, anonymous methods and extended RTTI providing some new powerful tools to extend the developer's creativity. // Property editors, grids, XML Tree, etc Single controls EasyListView (seems abandoned, active fork on GH here). [Delphi] Part of VirtualShellTools for the Listview but can be used for a TListview Replacement that is faster and more customizable. // Feature-rich Listview implementing virtual (callback-based) MVC paradigm. VirtualTreeView. [Delphi] (VirtualTreeView-Lazarus port to FPC [FPC]). Treeview control built from ground up. Many years of development made it one of the most flexible and advanced tree controls available today. // Extremely flexible visual component implementing virtual (callback-based) MVC paradigm. Could be also used as a listview or grid. Used in RAD Studio GUI. Delphi Chromium Embedded. [Delphi] Embedding Chromium in Delphi, tested on Delphi 2010, XE, XE2, Delphi 7. // Several Chromium DLLs required TChromeTabs. [Delphi] Comprehensive implementation of Google Chrome's tabs for Delphi 6 - Delphi 10.1 Berlin TFrameStand. [Delphi] Easily use TFrame(s) in your FireMonkey (FMX) applications to gain visual consistency though the whole user experience and easily add modern looking elements like effects and transitions. TPrintPreview. [Delphi] Print Preview Component for Delphi Vcl Win32/Win64 VolgaDB. [Delphi] Pretty customizable DBgrid for Delphi. TCustomGrid descendant. CheckBox, ComboBox column styles. Also includes TVolgaDBEdit that replaces TDBEdit, TDBComboBox, TDBLookupCombo, TDBLookupTree andTDBDatePicker in one component. TVolgaDBEdit may be DB-aware and non DB-aware. // Seems abandoned since 2013 TTreeListView. [Delphi] [FPC] This component is a mix between TTreeView and TListView and can paint a tree whose nodes have additional information sorted in columns. neTabControl. [Delphi] FireMonkey control for Delphi. It builds on the native TabControl and adds a number of features. ATTabs. [Delphi] [FPC] Delphi/Lazarus component for lite tabs. OS independent, fully custom drawn. zControls. [Delphi] Contains TzObjectInspector - a powerful object inspector with many features. RiverSoftAVG Charting Component Suite. [Delphi] Free (for non-commercial use) with source charting Suite for adding charts and graphs to your programs. For Delphi 2010-Tokyo (Win32/Win64/macOS/iOS/Android) and Appmethod (Object Pascal). DzHTMLText. [Delphi] [FPC] Visual component that allows you to specify a formatted text in a label, using almost the same syntax used in HTML code. SMDBGrid component. [Delphi] The successor of TDBGrid with the extended features. Is able to display multiline wordwrap column titles, checkboxs for boolean fields, a convenient select of records from the keyboard and mouse via checkboxs, extanded Indicator column, fixing of columns, an opportunity to exclude insert and delete of records in the DBGrid, own standard PopupMenu, save/restore of a column states, processing of additional events etc. Multilanguage resources. decTreeView. [Delphi] The decTreeView library is an alternative implementation of the TreeView (SysTreeView32) control Editors SynEdit (mirror at GitHub). [Delphi] Syntax highlighting edit control, not based on the Windows common controls. SynEdit is compatible with both Delphi and Kylix LazEdit. [FPC] General text editor with syntax highlighting and tools to help edit HTML. ATSynEdit. [FPC] Multi-line editor control for Lazarus including syntax highlighting. QDSEquations. [Delphi] Equation editor for Delphi and Lazarus that allows you to enter and display math formulas of any complexity, from simple Greek symbols to matrixes and complex integral expressions. Viewers ATViewer (mirror at GitHub). [Delphi] Delphi components to view various file types: text, binary, images, multimedia, webpages, etc. // Used in Universal Viewer software. Could be used to display hex dumps, features fast display of unlimited size files/streams. Supports Total Commander Lister plugins. ATImageMap (mirror at GitHub). [Delphi] Component designed to show many images (parts of the whole image) as a single map. For example, you may have array of images, 200 by X, and 100 by Y and control will show them as a single map. Component also allows to draw paths: each path consists of many lines, points, and icons. HtmlViewer. [Delphi] [FPC] Delphi/Lazarus HtmlViewer/FrameViewer. // Html visualiser supporting majority of tags, inline styles and CSS. SciDe. [Delphi] [FPC] Sciter (Embeddable HTML/CSS/script engine) wrapper for Delphi. ATBinHex for Delphi [Delphi], ATBinHex for Laz. [FPC] Viewer for files of unlimited size like in Total Commander. ATImageBox for Delphi [Delphi], ATImageBox for Laz. [FPC] TScrollBox with embedded TImage. Control can auto position image inside. CEF4Delphi. [Delphi] [FPC] Project to embed Chromium-based browsers in applications made with Delphi or Lazarus/FPC Other GUI GMLib (Google Maps Library) (seems abandoned, active fork on GH here and here). [Delphi] Components for Delphi/C++ Builder that encapsulate the GoogleMaps API to administrate a map, markers, polygons, rectangles, polylines, etc. All objects that you can put into a map. VCL Styles Utils. [Delphi] Collection of classes and style hooks, which extend, fix QC reports and add new features to the VCL Styles. // Collection of patches/enhancements that promote stock VCL style engine to a new level. Styling for Inno Setup and NSIS also available. TaskbarListComponents. [Delphi] Set of components designed as Delphi wrappers for the Windows 7 Taskbarlist Interfaces (e.g. ITaskbarlist3) // Requires JVCL TFireMonkeyContainer. [Delphi] Delphi VCL component to host a FMX HD or 3D form. It means you can embed a FireMonkey (FMX) form as a control in a VCL form, so you can design a FMX form and use it in your VCL app. PascalSCADA. [Delphi] [FPC] Set of components (framework) for Delphi/Lazarus to make easy the development of industrial applications (HMI=Human Machine Interface/SCADA=System Control And Data Acquisition). It runs on Windows, Linux and FreeBSD. Windows Ribbon Framework for Delphi. [Delphi] This Delphi library allows Delphi developers to use of the Windows Ribbon Framework in their Delphi applications. This library uses the native Windows library to implement the Ribbon functionality. It does not emulate the Ribbon user interface like other Delphi component sets do (or Delphi's built-in Ribbon emulation components). DKLang. [Delphi] DKLang Localization Package is a set of classes intended to simplify the localization of applications written in Delphi. GNU Gettext for Delphi, C++ and Kylix. [Delphi] GNU GetText translation tools for Borland Delphi and Borland C++ Builder. OpenWire. [Delphi] The library allows writing advanced VCL and FireMonkey components for rapid codeless application development. The components developed with the library allow creation of complex applications with zero lines of program code. SynTaskDialog. [Delphi] [FPC] Implement TaskDialog window (native on Vista/Seven, emulated on XP) AnyiQuack. [Delphi] jQuery-like control animation framework. TLanguages. [Delphi] Localization tool for VCL and FMX. BitMapEditor - Delphi. [Delphi] Single-form, simple bitmap editor for Delphi. BearLibTerminal. [Delphi] Provides a pseudoterminal window with a grid of character cells and a simple yet powerful API for flexible textual output and uncomplicated input processing. // Multiplatform dynamic library that has Delphi bindings Dam. [Delphi] [FPC] Delphi and Lazarus Message Dialogs with Formatted Text. Database ZeosLib. [Delphi] [FPC] Set of database components for MySQL, PostgreSQL, Interbase, Firebird, MS SQL, Sybase, Oracle and SQLite. Unified Interbase. [Delphi] Set of components to use Interbase, FireBird and YAFFIL. These components were born from the need to use Interbase, FireBird or Yaffil indifferently as fast as possible in a Multithreading environment, a Server for example. ASQLite. [Delphi] Delphi SQLite set of DAC components from aducom software, based on their latest release for Delphi 2009, and updated to support newer editions of Delphi as included in RemObjects Data Abstract for Delphi. TxQuery. [Delphi] TDataSet descendant component that can be used to query one or more TDataSet descendant components using SQL statements. It is implemented in Delphi 100% source code, no DLL required, because it implements its own SQL syntax parser and SQL engine. Delphi-ORM. [Delphi] Object-Relational Mapping for Delphi XE2-7 (Win32). Supports for FirebirdSQL, SQLServer and SQLite3. delphimemcache. [Delphi] Implements a thread safe client for memcached. // Requires Indy 10 SynDB (docs). [Delphi] [FPC] High performance direct access to SQLite3, Oracle, MSSQL, PostgreSQL, Firebird, MySQL, ODBC, OleDB, including remote HTTP connection and direct JSON support. SynMongoDB (docs). [Delphi] [FPC] Offers direct low-level access to any MongoDB server, its custom data types, JSON or via TDocVariant custom variant document storage. DSharp. [Delphi] Small library for providing data binding in Delphi. It does not require special components to data bind to properties. It also provides dependency injection, MVVM and more interesting utilities. ghORM. [FPC] Object Relational Mapping unit to ease database access from Free Pascal, by abstracting the backend and simple data retrieval (with filtering), insertion and update. tDBF. [Delphi] [FPC] Native dBASE III+, dBase IV and dBase 2k data access component for Delphi, BCB, Kylix, FreePascal. It allows you to create very compact database programs which don't need any special installer programs. The DB engine code is compiled right into your executable. Redis client [Delphi] Delphi Redis Client version 2 is compatible with Delphi 10.1 Berlin and better. WARNING! If you use an older Delphi version you have to use Delphi Redis Client Version 1 wich works for Delphi 10 Seattle, XE8, XE7, XE6 and XE5 (should works also with older versions). This client is able to send all Redis commands and read the response using an internal parser. QDAC3 (SVN: svn://www.qdac.cc/QDAC3). [Delphi] Stands for quick data access components. Useful units such as QJson (easy to use json unit), QWorker (job delivery) etc. // Description and comments in Chinese, author is not good at English. Haven't tested this library by myself. InstantObjects. [Delphi] Integrated framework for developing object-oriented business solutions in Delphi. The framework provides the foundation for the development process as well as the engine that powers the final application. InstantObjects offers: Model realization in the Delphi IDE via integrated two-way tools; Object persistence in the most common relational databases or flat XML-based files; Object presentation via standard data-aware controls. Alcinoe. Firebird/MySQL/SQLite3/Memcached/MongoDb/SphinxQL. SynBigTable. [Delphi] [FPC] Class used to store huge amount of data with fast retrieval. tiOPF. [Delphi] [FPC] Object Persistent Framework written in Object Pascal, for use with Delphi and Free Pascal (FPC) compilers. tiOPF simplifies the mapping of an object oriented business model into a relational database. Persistence layers are available for Firebird, Oracle, MS SQL Server, MySQL, PostgreSQL, SQLite, NexusDB, XML, CSV, TAB, Remote (via HTTP) and many more. It also allows you to use your choice of database connection components, like IBX, dbExpress, DOA, SqlDB, FBLib etc. hcOPF. [Delphi] Object Persistent Framework written in Embarcadero's Delphi (Object Pascal). This Value Type Framework provides a base class (ThcObject) composed of attribute objects that can be automatically persisted to an object store (normally an RDBMS). Marshmallow. [Delphi] Object-Relational Mapping for Delphi XE2-7 (Win32) inspired by .NET micro ORM's (mostly by PetaPoco) and Java Hibernate. Developed by Linas Naginionis. Supports SQLite, Sybase ASA, SQL Server, Firebird, Oracle, MySQL, PostgreSQL, MongoDB. Uses Spring Framework. In active development. DelphiCassandra. [Delphi] Delphi driver classes to communicate with Cassandra database. DelphiCouchbase. [Delphi] Delphi driver classes to communicate with Couchbase database. DelphiMongoDB. [Delphi] Delphi driver classes to communicate with MongoDB database. QuickORM. [Delphi] [FPC] QuickORM is a simple RestServer and Restclient based on mORMot framework. Provides a fast implementation of client-server applications in few minutes. iORM. [Delphi] Delphi ORM interface based useful to develop desktop and mobile application. d-ORModel. [Delphi] ORM for Delphi, based on models and object fields. LINQ support, fully typed and compile time checks. Scripting Using script engine in your applications Pascal Script. [Delphi] [FPC] Free scripting engine that allows you to use most of the Object Pascal language within your Delphi or Free Pascal projects at runtime. Written completely in Delphi, it is composed of a set of units that can be compiled into your executable, eliminating the need to distribute any external files. Pascal Script started out as a need for a good working script, when there were none available at the time. DWScript. [Delphi] Object-oriented scripting engine for Delphi based on the Delphi language, with extensions borrowed from other Pascal languages (FreePascal, Prism, etc.). It introduces a few Pascal language extensions of its own as well. Delphi-JavaScript. [Delphi] JavaScript engine for delphi based on Mozilla's Spidermonkey. // Spidermonkey DLL required Blaise. [Delphi] Open-source object-oriented scripting language. Language features: Object-oriented; Unicode support; Optional typing, ie dynamic or static typing; Richly typed; Higher-level mathematics support, for example Complex numbers, Rational numbers and Matrices; Virtual Machine architecture; Co-routines; Familiar language syntax, influenced by Object Pascal, Python and Ada. SpiderMonkey. [Delphi] [FPC] Binding for Mozilla JavaScript engine, including JIT and multi-threading, with easy objects access via Delphi variants. // Spidermonkey DLL required BESEN. [Delphi] [FPC] Complete ECMAScript Fifth Edition Implementation in Object Pascal, which is compilable with Delphi >=7 and Free Pascal >= 2.5.1 (maybe also 2.4.1). Python for Delphi (P4D). [Delphi] [FPC] Set of free components that wrap up the Python dll into Delphi and Lazarus (FPC). They let you easily execute Python scripts, create new Python modules and new Python types. You can create Python extensions as dlls and much more CrystalLUA. [Delphi] Lua binding (Delphi6-2007). // LUA DLL required lua4delphi. [Delphi] Delphi binding for Lua 5.1 language. // LUA DLL required chakracore-delphi. [Delphi] [FPC] Delphi and Free Pascal bindings and classes for Microsoft's ChakraCore JavaScript engine library. VerySimple.Lua. [Delphi] Lua Wrapper for Delphi XE5-D10.1 which automatically creates OOP callback functions for Delphi <-> Lua. // LUA DLL required QuickJS-Engine. [Delphi] [FPC] Delphi and Free Pascal bindings for Bellard's QuickJS JavaScript Engine. Machine Learning Machine learning and neural networks noe. [FPC] Framework to build neural networks in pure object pascal. Non-visual Classes/Utils Compression FWZip. [Delphi] Classes to work with Zip archives using Store and Deflate methods, supports ZIP64, DataDescryptors, PKWARE encryption, NTFS attributes, Utf8 in filenames. // Uses stock ZLIB.obj that gets compiled into binary. Comments and description in Russian. Abbrevia (Newest and maintained version for recent compiler version only). [Delphi] Advanced data compression toolkit for Delphi and C++Builder. Supports PKZIP, Microsoft CAB, tar, gzip, and bzip2 archives, and can create self-extracting executables. On Windows it also provides Delphi wrappers for the LZMA, Bzip2, and WavPack SDKs, and PPMd decompression. Abbrevia also has several visual controls that simplify displaying and manipulating archives, including treeview and listview components. Features: Unicode filenames in all archive formats; Decompress most .zipx and legacy (PKZIP v1) zips; ZIP64 support for archives larger than 2GB; Spanned and split zip archives; Cross-platform (Windows, OS X, and Linux); No DLLs required; Includes COM component; Extensive documentation // Second link points to an adapted version for newest compiler versions. SynLZ SynLZO SynZip PasZip. [Delphi] [FPC] Several high speed compression units, featuring ZIP/LZ77 Deflate/Inflate, LZO and SynLZ algorithm, in pascal and optimized assembler. Delphi zlib. [Delphi] Wrapper for zlib.obj originally used by Borland. Delphi up to XE3 supported. DIUcl. [Delphi] DIUcl is a lossless compression library with extremely fast and small (200 bytes only!) ASM decompressor. Compression times and ratios are similar to those of deflate/zip and bzip2. Delphi port of the popular UCL Compression Library, which is also used by the popular and well known UPX Ultimate Packer for eXecutables. Encryption Delphi Encryption Compendium (DEC). [Delphi] [FPC] Cryptographic library for Delphi & C++ Builder. Symmetric cryptographic functions: Blowfish, Twofish, IDEA, Cast128, Cast256, Mars, RC2, RC4, RC5, RC6, Rijndael / AES, Square, SCOP, Sapphire, 1DES, 2DES, 3DES, 2DDES, 3DDES, 3TDES, 3Way, Gost, Misty, NewDES, Q128, SAFER, Shark, Skipjack, TEA, TEAN; Block cipher modes of operation: CTSx, CBCx, CFB8, CFBx, OFB8, OFBx, CFSx, ECBx; Hashes: MD2, MD4, MD5, RipeMD128, RipeMD160, RipeMD256, RipeMD320, SHA, SHA1, SHA224, SHA256, SHA384, SHA512, SHA3-224, SHA3-256, SHA3-384, SHA3-512, Haval128, Haval160, Haval192, Haval224, Haval256, Tiger, Panama, Whirlpool, Whirlpool1, WhirlpoolT, Square, Snefru128, Snefru256, Sapphire. LockBox (Newest and maintained version for recent compiler version only). [Delphi] Delphi library for cryptography. Currently supported Delphi XE6. It provides support for AES, DES, 3DES, Blowfish, Twofish, SHA2 (including the new SHA-512/224 & SHA-512/256), MD5; ECB, CBC, CFB8, CFB, CTR, ECB, OFB, PCBC chaining modes, RSA digital signature and verification. Has interface to OpenSSL library. // Check out this page as well for alternative version. SynCrypto. [Delphi] [FPC] Fast cryptographic routines (hashing and cypher), implementing AES, XOR, RC4, ADLER32, MD5, SHA1, SHA256 algorithms, optimized for speed (tuned assembler and VIA PADLOCK optional support). TForge. [Delphi] [FPC] Open-source crypto library written in Delphi, compatible with Free Pascal Compiler. MD5, SHA1, SHA256, CRC32, Jenkins-One-At-Time, HMAC, PBKDF1, PBKDF2, AES, DES, RC4, RC5, Salsa20. Spring4D. CRC, DES, MD5, SHA Fundamentals Code Library. Hashes: XOR, CRC, Adler, MD5, SHA, secure keyed MD5/SHA, etc; Cipher: AES, DES, FUNE, RC2/4, RSA. Alcinoe. AES, Blowfish, MD5, SHA, secure keyed MD5/SHA. DCPcrypt (fork #1), DCPcrypt (fork #2). [Delphi] Suite of cryptographic components for Delphi. bcrypt. [Delphi] A library to help you hash passwords. MurMur-Delphi. [Delphi] MurMur1/2/3 fast seeded hashing algorithms port in pure-pascal. HashLib4Pascal. [Delphi] [FPC] Object Pascal hashing library released under the permissive MIT License which provides an easy to use interface for computing hashes and checksums of data. It also supports state based (incremental) hashing. CRC, Adler, Murmur, Jenkins, MD5, SHA, Blake, many more. SimpleBaseLib4Pascal. [Delphi] [FPC] Simple to use Base Encoding Package for Delphi/FreePascal Compilers that provides at the moment support for encoding and decoding various bases such as Base16, Base32 (various variants), Base58 (various variants) and Base64 (various variants) and Base85 (various variants). CryptoLib4Pascal. [Delphi] [FPC] Object Pascal cryptographic library released under the permissive MIT License. Ciphers: AES (128, 192, and 256), Rijndael, Blowfish, Speck, ChaCha, (X)Salsa20, DSA, (DET)ECDSA (supported curves: NIST, X9.62, SEC2, Brainpool), ECNR, ECSchnorr, EdDSA (Ed25519, Ed25519Blake2B) XML/JSON/YAML/HTML dataset-serialize. [Delphi] [FPC] This component is a JSON serializer for the DataSet component. Allows you to convert JSON to DataSet, DataSet to JSON, and export and load the structure of DataSet fields in JSON format. Compatible with VCL projects, FMX and uniGUI (framework). OmniXML. [Delphi] XML parser written in Delphi. Full support for Document Object Model (DOM) Level 1 specification; Supports Extensible Markup Language (XML) 1.0 (Second Edition) specification; Has built-in support for different code pages (main 8-bit code pages, UTF-8, UTF-16); Is compatible with MS XML parser; Fast parsing even large and highly structured documents; Includes helper functions to ease processing XML documents; Simplified XPath support. SAX for Pascal. [Delphi] [FPC] Designed to implement the Simple API for XML Parsing in Pascal/Delphi. // Callback-based XML parser, useful for processing huge XML streams. Abandoned since 2004 but is almost the only SAX implementation available. KDS XML. [Delphi] Class library for streamed parsing, validating and generating XML. It is written in Object Pascal/Delphi and works on Win32 (Delphi) and Linux (Kylix). Parts of it depend on the SAX for Pascal interface specifications. // Seems dead. XML Partner. [Delphi] Helps add the power of XML to Borland Delphi, C++ Builder, and Kylix projects through native, easy to use VCL and CLX components. These powerful components simplify the process of creating, modifying, and parsing XML data documents. // Seems dead, check out this page for probably newer version. Open XML. [Delphi] Provides a wide range of methods, components and foundation classes. It can be used for Win32/Kylix as well as for .NET development. SuperObject. [Delphi] [FPC] Parser/writer for JSON data format. This toolkit is designed to work with Delphi and FreePascal (win32, win64, linux32, linux64, macOS Intel). Supports reading/writing XML as well. Libxml2 for pascal. [Delphi] [FPC] Pascal units accessing the popular XML API from Daniel Veillard. This should be usable at least from Kylix and Delphi, but hopefully also from other Pascal compilers (like freepascal). NativeXml. [Delphi] This component contains a small-footprint Object Pascal (Delphi) XML implementation that allows to read and write XML documents. You basically only need one unit and you can simply add it to the \"uses\" clause. You can use this software to read XML documents from files, streams or strings. The load routine generates events that can be used to display load progress on the fly. You can also use it to create and save XML documents. Delphi-XmlLite. [Delphi] Header translation for Microsoft XmlLite. XmlLite is a native C++ implementation of .NET XmlReader+Writer for stream-based, forward-only XML parsing and creation. XmlLite.dll is required. It is included with all new versions of Windows, and service packs for old versions. XmlReader's pull-based interface is cleaner to use than SAX's event-based interface. // Seems abandoned and reported to be somewhat buggy. Chimera. [Delphi] Open Source (MIT License) library for Delphi XE2 which provides a fast and cross platform JSON generator/parser (serializer/deserializer) under a license that doesn't suck. SynCommons. [Delphi] [FPC] High speed JSON library, using TDocVariant custom variant type for storage and access. SynCrossPlatformJSON. [Delphi] [FPC] High speed cross-platform JSON library, using TJSONVariant custom variant type for storage and access. Json Data Objects. [Delphi] This Delphi unit contains a JSON parser that supports Delphi 2009-10Seattle and the platforms Win32, Win64 and ARM Android (MacOS and iOS may work). TinyJSON (mirror at GH). [Delphi] This is a small and clean library for associative arrays with Boolean / Integer / Float / WideString values. Allows import (export) from (to) JSON text. Extensive error-checking. Uses FunHash (by Sokolov Yura), HatTrie (by Daniel C. Jones), FastInt64 and FastMove (by FastCode project). JSON delphi library. [Delphi] This is a delphi library implementing JSON data format and objects structure. Lightweight and fast. dwsJSON. [Delphi] [FPC] dwsJSON is a unit that supports JSON parsing/creating, it's part of DWScript but relatively \"standalone\", in that if you add it in your Delphi (or FPC) projects, it won't pull the whole of DWScript library, and thus can be used anywhere you need. Fundamentals Code Library. JSON, XML. Alcinoe. XML/JSON Parser. delphi-yaml. [Delphi] Delphi 7 compatible bindings for libyaml, YAML parser and emitter library implemented in C. Four layers of bindings are proposed. GrijjyFoundation. JSON/BSON. VerySimpleXML. [Delphi] Lightweight, one-unit, cross-platform XML reader/writer for Delphi 2010 - 10.2.2 Tokyo XSuperObject. [Delphi] Delphi Cross Platform Rapid JSON Internet Tools. [Delphi] [FPC] Package provides standard conformant XPath 2.0, XQuery 1.0 and XPath/XQuery 3.0 interpreters with extensions for - among others - JSONiq, pattern matching, CSS and HTML; as well as functions to perform HTTP/S requests on Windows/Linux/macOS/Android, an XSLT-inspired webscraping language, and an auto update class. Delphi-JsonToDelphiClass (Newer fork. [Delphi] Generates Delphi Classes based on JSON string (Json To Delphi Class Generator / JSON Data Binding Tool). Also includes unit for interaction with GitHub. XML Parser. [Delphi] [FPC] Lightweight ObjectPascal XML parser for Delphi and FreePascal. By leaving out syntax checking, well-formedness checks and/or validation, and by choosing a progressive scanning technique, this parser is very fast. HTML parser. [Delphi] HTML parser. Supports Windows, macOS, iOS, Android platform. Comments in Chinese Neslib. [Delphi] Ultra light-weight and cross-platform XML library for Delphi. DJSON. [Delphi] Delphi JSON object mapper fast-html-parser. [Delphi] [FPC] Fast HTML Parser THTMLWriter. [Delphi] Class library that enables the developer to create HTML and HTML documents. It uses the fluent interface to make creating HTML text easy and natural. Language Tools for Pascal and other languages Next Delphi Yacc & Lex. [Delphi] Parser generator toolset for Delphi. Abstract Syntax Tree Builder. [Delphi] With DelphiAST you can take real Delphi code and get an abstract syntax tree. One unit at time and without a symbol table though. Castalia-Delphi-Parser. [Delphi] These files make up a hand-written high speed parser for the Object Pascal dialect known as \"Delphi\". The original work was done by Martin Waldenburg in the late 1990s, and the project was abandoned sometime before 2003, when I found the code and began working on it. I have kept it updated as necessary to work with my project, called \"Castalia\". CrossPascal. [Delphi] Aims to be a Delphi 7 compatible cross-platform source-to-source compiler (together with the new unicode string types from XE3 but where ansistring is still the default string type for to be still Delphi 7 compatible) which generates intermediate C code. // Quite interesting project though seems abandoned pas2js, docs. [Delphi] [FPC] An open source Pascal to JavaScript transpiler. It parses Object Pascal and emits JavaScript. The JavaScript is currently of level ECMAScript 5 and should run in any browser or in Node.js (target \"nodejs\"). Basically, Delphi 7 syntax is supported. Used in tools like TMS WebCore and Elevate Web Builder. Memory managers Libraries that implement dynamic memory allocation FastMM. [Delphi] Lightning fast replacement memory manager for Embarcadero Delphi Win32 and Win64 applications that is not prone to memory fragmentation, and supports shared memory without the use of external .DLL files. // Used as stock memory manager since 2006 but in simplified version. Provides powerful memory leak/corruption detection instruments. ScaleMM. [Delphi] Fast scaling memory manager for Delphi BrainMM. [Delphi] Extremely fast memory manager for Delphi. // Advanced memory allocation functions for faster aligned operations. FastMM4-AVX. [Delphi] [FPC] FastMM4 fork with AVX support and multi-threaded enhancements (faster locking) FastMM5. [Delphi] Fast replacement memory manager for Embarcadero Delphi applications that scales well across multiple threads and CPU cores, is not prone to memory fragmentation, and supports shared memory without the use of external .DLL files. Version 5 is a complete rewrite of FastMM. System Low-level helper stuff: memory, threading etc OmniThreadLibrary. [Delphi] Simple to use threading library for Delphi. // Easy integration of async processes in your app Delphi Detours Library. [Delphi] [FPC] Library allowing you to hook Delphi functions and object methods and Windows API functions. It provides an easy way to insert and remove hook. // Supports x64, calling original functions, multi hooks, COM/Interfaces/win32api, object methods hooking, fully thread-safe, Delphi 7/2005-2010/XE-XE7 & Lazarus/FPC, 64 bit address is supported. MemoryModule. [Delphi] [FPC] With the MemoryModule engine you can store all required DLLs inside your binary to keep it standalone. Additional hook units allow transparent using of MM engine thus allowing switching MM/WinAPI loading as well as enabling 3rd party dynamic-load DLL interfaces that are unaware of MM (tested with Interbase Express components and Firebird client library). MemoryModule is a Pascal port of Joachim Bauch's C MemoryModule. DirectoryWatcher. [Delphi] Watch changes in directories on different platforms (Windows/Linux/Mac OS). ezthreads. [FPC] simple to use threading library AsyncCalls. [Delphi] Asynchronous function call framework Template Engines to generate text output based on templates SynMustache. [Delphi] [FPC] Delphi implementation of the Mustache template language, supporting Delphi 6 up to Delphi 10 Seattle (and FPC/Lazarus compilation). Delphi Template Engine. [Delphi] Template engine designed to be used as a library in Delphi (mainly Delphi 7) applications, allowing developers to use templating on their software with no worry about implementing it. MustaPAS. [Delphi] [FPC] Mustache implementation in simple procedural Pascal. Sempare Template Engine. [Delphi] The template engine allows for flexible text manipulation. It can be used for generating email, html, source code, xml, configuration, etc. It is very easy to use, flexible and extensible, where templates are readable and maintainable. It supports: conditions, looping, custom functions and referencing data via RTTI. XE4, XE8+ DVD Chief Template Engine. [Delphi] [FPC] Fork of abandoned implementation of PHP Smarty template engine for Delphi by DVD Chief. liquid-delphi. [Delphi] Delphi port of the popular Ruby Liquid templating language and dotLiquid implementation. It is a separate project that aims to retain the same template syntax as the original, while using delphi coding conventions where possible. Logging Log4d. [Delphi] [FPC] Implementation of logging system for Delphi, based on Log4j. TraceTool. [Delphi] C#, C++, Delphi, ActiveX and Java trace framework and a trace viewer. LoggerPro. [Delphi] A modern and pluggable logging framework for Delphi. SynLog. [Delphi] [FPC] Logging functions used by Synopse projects. slf4p. [Delphi] [FPC] A simple logging facade with support for LazLogger, Log4D, and other logging frameworks. GrijjyCloudLogger. [Delphi] Remote logging tool that allows you to send log messages over the Intranet or Internet from Windows, Linux, iOS, Android and macOS devices to a viewer running on Windows. Besides sending messages along with any data, it has numerous features including custom live watches, remote live views of objects, tracking live memory usage, object allocations, growth leaks and more. QuickLogger. [Delphi] [FPC] Delphi/freepascal/.NET (Windows/Linux) library for logging on files, console, memory, email, rest, telegram, slack, eventlog, redis, ide debug messages and throw events.. Math Big Decimal Math. [Delphi] This unit provides a arbitrary precision BCD float number type. It can be used like any numeric type and supports: At least numbers between 10-2147483647 to 102147483647 with 2147483647 decimal digit precision; All standard arithmetic and comparison operators; Rounding functions (floor, ceil, to-even, ..); Some more advanced operations, e.g. power and sqrt. TIntX. [Delphi] [FPC] Pascal port of IntX arbitrary precision Integer library with fast, about O(N * log N) multiplication/division algorithms implementation. It provides all the basic arithmetic operations on Integers, comparing, bitwise shifting etc. It also allows parsing numbers in different bases and converting them to string, also in any base. The advantage of this library is its fast multiplication, division and from base/to base conversion algorithms. all the fast versions of the algorithms are based on fast multiplication of big Integers using Fast Hartley Transform which runs for O(N * log N * log log N) time instead of classic O(N^2). DelphiBigNumberXLib. [Delphi] Arbitrary Precision Library for Delphi with Support for Integer and Floating Point Computations. FastMath. [Delphi] Delphi math library that is optimized for fast performance (sometimes at the cost of not performing error checking or losing a little accuracy). It uses hand-optimized assembly code to achieve much better performance then the equivalent functions provided by the Delphi RTL. // Floating-point, vector, matrix operations. MPArith. [Delphi] Multi precision integer, rational, real, and complex arithmetic. AMath and DAMath. [Delphi] Accurate mathematical methods without using multi precision arithmetic and double precision accurate mathematical methods without using multi precision arithmetic or assembler respectively. ALGLIB. [Delphi] [FPC] Cross-platform numerical analysis and data processing library. It supports several operating systems (Windows and POSIX, including Linux). ALGLIB features include: Data analysis (classification/regression, statistics); Optimization and nonlinear solvers; Interpolation and linear/nonlinear least-squares fitting; Linear algebra (direct algorithms, EVD/SVD), direct and iterative linear solvers; Fast Fourier Transform and many other algorithms. // Free edition is Delphi wrapper around generic C core licensed for Personal and Academic Use. CAI NEURAL API. [FPC] [Delphi] Cross-platform Neural Network API optimized for AVX, AVX2 and AVX512 instruction sets plus OpenCL capable devices including AMD, Intel and NVIDIA. Command-line Libraries for parsing command-line arguments TCommandLineReader. [Delphi] [FPC] This unit provides an advanced, platform-independent command line parser for Lazarus and Delphi. It checks for allowed options, automatically prints a help with a list of all of them, and - contrary to the parser in the rtl - behaves the same on Windows and Linux. CommandLineParser. [Delphi] Simple Command Line Options Parser - spawned from the DUnitX Project. GpCommandLineParser. [Delphi] Attribute based command line parser. JPL.CmdLineParser. [Delphi] [FPC] Command-line parser for Delphi and Free Pascal Nullpobug.ArgumentParser. [Delphi] [FPC] Command-line parser for Delphi and Free Pascal Other non-visual TRegExpr. [Delphi] [FPC] Regular expressions engine in pure Object Pascal. FLRE. [Delphi] [FPC] FLRE ( F ast L ight R egular E xpressions) is a fast, safe and efficient regular expression library, which is implemented in Object Pascal (Delphi and Free Pascal) but which is even usable from other languages like C/C++ and so on. OnGuard (Alternate and maintained version for recent compiler version only). [Delphi] Library to create demo versions of your Borland Delphi & C++Builder applications. Create demo versions that are time-limited, feature-limited, limited to a certain number of uses, or limited to a certain # of concurrent network users. // Second link points to an adapted version for newest compiler versions. StringSimilarity. [Delphi] Package designed for some fuzzy and phonetic string comparison algorithms. So far implemented are the following algorithms: DamerauLevenshtein, Koelner Phonetik, SoundEx, Metaphone, DoubleMetaphone, NGram, Dice, JaroWinkler, NeedlemanWunch, SmithWatermanGotoh, MongeElkan. PubSub Chimera. [Delphi] Open Source (MIT License) library for Delphi which provides a fast and cross platform PubSub and Message Queue implementation under a license that doesn't suck. DuckDuckDelphi. [Delphi] Adds simple duck typing to Delphi Objects and provides an RTTI helper class to simplify many common RTTI tasks. byterage. [Delphi] Object pascal class library designed to remove some of the limitations of streams. The framework is very simple to use, with only one common ancestor class (TBRBuffer) which defines a set of storage agnostic mechanisms for allocating, scaling, inserting, deleting and otherwise manipulating a segment of raw binary data. stateless. [Delphi] Simple library for creating state machines in Delphi code. GenericTree. [Delphi] Delphi implementation of a generic Tree structure. Delphi Event Bus (for short DEB). [Delphi] Event Bus framework for Delphi. DelphiEventBus. [Delphi] Yet another Event Bus framework for Delphi, with annotations and rich event filtering. DHibernate. [Delphi] Object Persistent Framework based on Hibernate and NHibernate for Delphi. // Abandoned since 2012 UniConv. [Delphi] [FPC] Universal text conversion library is a universal quick and compact library intended for conversion, comparison and change of the register of text in concordance with the latest standards of the Unicode Consortium. The librarys function greatly resembles ICU, libiconv and Windows.kernel which are de facto standard for popular operating systems. CachedBuffers. [Delphi] [FPC] The library is irreplaceable for the tasks of sequential data reading or writing, especially if the requirements for the performance are increased and there are much data. CachedTexts. [Delphi] [FPC] Powerful and compact cross-platform library aimed at parsing and generating of text data with the maximum possible performance. Depends on the two other libraries: CachedBuffers and UniConv. ZEXMLSS. [Delphi] [FPC] Lazarus/Delphi component for read/write ods, excel xml, xlsx. PasMP. [Delphi] [FPC] Parallel-processing/multi-processing library for Object Pascal. ICU4PAS. [Delphi] [FPC] Object Pascal, cross platform, Direct Class Wrapper over the mature and widely used set of C/C++ ICU libraries providing Unicode support, software internationalization (i18n) and globalization (g11n), giving applications the same results on all platforms. You can use it on Windows with Delphi and FreePascal and on Linux with Kylix and FreePascal. // Hadn't been updated since 2007 but ICU interface probably remains the same GpDelphiUnits. [Delphi] Collection of useful Delphi units. Various TList descendants, TList-compatible, and TList-similar classes. Dynamically allocated, O(1) enqueue and dequeue, threadsafe, microlocking queue. Interface to 64-bit file functions with some added functionality. String hash, table and dictionary. Collection of Win32/Win64 wrappers and helper functions. Time Zone Routines. Embedded file system. BaseNcodingPascal. [Delphi] [FPC] Library for encoding of binary data into strings using base32, base85, base128 and other algorithms for FPC and Delphi. ByteSizeLibPascal. [Delphi] [FPC] TByteSize is a utility \"record\" that makes byte size representation in code easier by removing ambiguity of the value being represented. EmailValidationPascal. [Delphi] [FPC] Simple Class for Validating Email Address Syntax in Pascal/Delphi. PRNG. [Delphi] Seven fast pseudo random number generators with period lengths much greater than Pascal's random function. All are implemented with context records, therefore several independent generators can be used simultaneously, they are not cryptographically secure. In addition there are three cryptographic generators. CSV File and String Reader. [Delphi] TnvvCSVFileReader and TnvvCSVStringReader are light weighted and fast classes that resemble unidirectional data set. HTMLBuilder. [Delphi] Build simplified html with pascal code. Marvin.IA. [Delphi] Machine learning collection of object-oriented Pascal primitives (only interfaces and classes). FreePascal Generics.Collections. [FPC] FreePascal Generics.Collections library (TList, TDictionary, THashMap and more) FuzzyWuzzy.pas. [FPC] Port of the well-known Python fuzzy string matching package that uses the Levenshtein distance to compute differences between string sequences. GS.Core. [Delphi] [FPC] Core functions shared by several projects. // Thread Pool, file operations, Key<>Value database, JSON lib, etc PascalTZ. [FPC] Pascal Time Zone allows you to convert between local times in various time zones and GMT/UTC, taking into account historical changes to time zone rules. Charset Enigma. [Delphi] Delphi charset detector Community Edition DelphiPatterns. [Delphi] Complete set of design patterns implemented in Delphi language Markdown Processor for Pascal. [Delphi] [FPC] This is a Pascal (Delphi) library that processes to markdown to HTML Coroutine-based multithreading library. [Delphi] AIO implement procedural oriented programming (POP) style in Delphi. It means developer can combine advantages of OOP and POP, splitting logic to multiple state machines, schedule them to threads, connect them by communication channels like in GoLang Rapid.Generics. [Delphi] Rapid generics/defaults equivalent classes for Delphi (XE8+) Keras4Delphi. [Delphi] High-level neural networks API, written in Pascal with Python Binding TZDB. [Delphi] [FPC] IANA Time Zone Database for Delphi/FreePascal PascalUtils. [Delphi] [FPC] Delphi and object pascal library of utils data structures libPasC-Algorithms. [Delphi] [FPC] Delphi and object pascal library of common data structures and algorithms. Library rewritten from c-algorithms repository and other sources. VSoft.Messaging. [Delphi] Libary that provides an internal synchronous/asynchronous publish/subscribe messaging system for Delphi applications. Delphi-Hunspell. [Delphi] Simple Hunspell spell checking engine wrapper for Delphi. OS Tools that help dealing with OS-specific internals GLibWMI. [Delphi] Component Library for Delphi that encapsulate the classes for access to WMI of Windows in a set of VCL. BiosInfo, PrinterInfo, DiskInfo, etc. Allow access WMI Classes: WIN32_Bios, WIN32_Printers, WIN32_DiskDrive. MemoryMap. [Delphi] Set of classes to get all the info about a memory of a running process. The Drag and Drop Component Suite. [Delphi] VCL component library that enables your Delphi and C++Builder applications to support COM based drag and drop and integrate with the Windows clipboard. TSMBIOS. [Delphi] [FPC] Allows access the System Management BIOS (SMBIOS) using the Object Pascal language (Delphi or Free Pascal). The SMBIOS (System Management BIOS) is a standard developed by the DMTF. The information stored in the SMBIOS includes devices manufacturer, model name, serial number, BIOS version, asset tag, processors, ports and device memory installed. VersionInfo for Delphi. [Delphi] The library makes it very easy to read values from the Version Info resource of Windows executables and DLLs. Optionally extends the TApplication class with a version info property via class helper. Magenta Systems WMI and SMART Component. [Delphi] Contains WMI, SMART and SCSI PassThrough functions, of particular use for getting hard disk information and configuring network adaptors, but also for many other general uses. MagWMI provides general view access to any WMI information using SQL like commands, and also a number of dedicated function relating to TCP/IP configuration, such as setting the adaptor IP addresses, the computer name, domain/workgroup, BIOS and disk drive information. madKernel. [Delphi] The package is about Kernel Objects for the biggest part. The most important object types are wrapped up in interfaces, utilizing all the specific kernel32 APIs. Has interface wrappers for: Events, Mutexes, Threads, Processes, Windows, Modules, Tray Icons, shared memory buffers. // Free with source for non-commercial usage (only) with some conditions. Available to download as part of madCollection installer. Pretty well documented. Requires madBasic package. madSecurity. [Delphi] The package makes it easily possible to handle Shares and other Security Objects like file security or registry security. To be able to do so, this package also features functionality around Accounts and ACEs and ACLs. // Free with source for non-commercial usage (only) with some conditions. Available to download as part of madCollection installer. Pretty well documented. Requires madBasic package. madShell. [Delphi] The package implements often needed shell functionality, beginning with Special Folders like the \"Windows\" folder or the \"Program Files\" folder, continuing with Shell ID Lists, Shell Objects and Shell Events. Then you'll find functionality around ShortCuts/ShellLinks and finally everything about Display Modes. // Free with source for non-commercial usage (only) with some conditions. Available to download as part of madCollection installer. Pretty well documented. Requires madBasic package. WindowsAutorun. [Delphi] Helps you manage autoload in Windows OS. ActiveDirectory4Delphi. [Delphi] Delphi basic library for validation and authentication of LDAP users in Active Directory. Report generating Report Manager. [Delphi] Report manager is a reporting application (Report Manager Designer) and a set of libraries and utilities to preview, export or print reports. Include native .Net and Delphi/C++Builder libraries, ActiveX component and also standard dynamic link library for use in any language like GNU C. FortesReport. [Delphi] The FortesReport is a powerful report generator available as a package of components for Delphi. mORMotReport (docs). [Delphi] Fast and efficient code-based reporting component, with preview form and PDF export. Unit Testing DUnitX. [Delphi] New test framework, taking ideas from DUnit, NUnit and other test frameworks. It is designed to work with Delphi 2010 or later, it makes use of language/RTL features that are not available in older versions of Delphi. DUnit. [Delphi] Unit Testing Framework, that has been the standard testing framework for years, the Delphi IDE now ships with this library. // Included since XE, deprecated since XE8 in favor of DUnitX; seems abandoned. DUnit2. [Delphi] Fork of the DUnit Project that adds several new features. // Seems abandoned, lacks some features from last DUnit version. DelphiSpec. [Delphi] Library for running automated tests written in plain language. Because they're written in plain language, they can be read by anyone on your team. Because they can be read by anyone, you can use them to help improve communication, collaboration and trust on your team. Delphi-Mocks. [Delphi] Simple mocking framework for Delphi XE2 or later. Allow you to mock both classes and interfaces for testing. DUnit-XML. [Delphi] Test runner that allows DUnit Tests to output NUnit compatible XML. Smoketest. [Delphi] Framework for writing tests and performance benchmarks using the Delphi language for Microsoft Windows. It has been tested on all versions of Delphi from 7 thru to 2010. SynTests. [Delphi] [FPC] Unit test functions including mocks and stubs. OpenCTF. [Delphi] Test framework add-on for Embarcadero Delphi which performs automatic checks of all components in Forms (or DataModules). It provides an easy way to build automatic quality checks for large projects where many components have to pass repeated tests. OpenCTF is based on the DUnit open source test framework and extends it by specialized test classes and helper functions. DelphiUIAutomation. [Delphi] Delphi classes that wrap the MS UIAutomation library. DelphiUIAutomation is a framework for automating rich client applications based on Win32 (and specifically tested with Delphi XE5). It is written in Delphi XE5 and it requires no use of scripting languages. It provides a consistent object-oriented API, hiding the complexity of Microsoft's UIAutomation library and windows messages. Debugging / error handling Delphi LeakCheck. [Delphi] Free code library to check the memory leaks in the DUnit and DUnit2 tests. Supports Delphi XE-XE7. FastMM. Provides powerful memory leak/corruption detection instruments. JclDebug (part of Project JEDI). [Delphi] [FPC] Tracing, MAP file parser, exception report generation, exception stack traces. DebugEngine. [Delphi] Collection of utilities related to debug stuff (stack trace, CPU registers snapshot, debug info, etc). Accessing Delphi debug info, Getting address of symbol from its name, Delphi map parsing and map converter to binary format, Smart stack trace, Delphi exception stack trace hook, etc. ObjectDebugger. [Delphi] Run-time Object Inspector for Delphi VCL applications. Utilities Free non-opensource products allowed here. RAD Studio IDE plugins/wizards Delphi IDE theme editor / Delphi IDE Colorizer. Tool to change the IDE color highlighting of several Object Pascal IDE's like Delphi (RAD Studio), Appmethod, Lazarus and Smart Mobile Studio. DITE supports Delphi 5-7, 2005-2010, XE-XE8, Appmethod 1.13-1.14, Lazarus v1.0.1.3 and Smart Mobile Studio IDE v1.1.2.17. The Delphi IDE Colorizer (DIC) is a plugin which allows to customize the look and feel of the workspace of the RAD Studio IDE and Appmethod. DDevExtensions. Extends the Delphi/C++Builder IDE by adding some new productivity features. // Many useful IDE tweaks, must have. VCL Fix Pack. Delphi unit that fixes VCL and RTL bugs at runtime by patching the original functions. If you want all IDE Fix Pack fixes in your application this unit is what you are looking for. Adding the unit to your project (Delphi and C++Builder) automatically installs the patches that are available for your Delphi/C++Builder version. // Actual for Delphi/C++ 6..2009 IDE Fix Pack. Collection of unofficial bug fixes and performance optimizations for the RAD Studio IDE, Win32/Win64 compiler and Win32 debugger. IDE Fix Pack is an IDE plugin for RAD Studio 2009-XE6 that fixes IDE bugs at runtime. All changes are done in memory. No files on disk are modified. None of your projects are modified or benefit from the IDE Fix Pack other than being compiled faster. Only the IDE gets the fixes and optimizations. // Supports all RAD Studio versions since 2007. Removes lots of annoying bugs that EMBT haven't fixed for years. Yay! GExperts. Free set of tools built to increase the productivity of Delphi and C++Builder programmers by adding several features to the IDE. GExperts is developed as Open Source software and we encourage user contributions to the project. Grep search and replace supporting unicode files, DFMs, etc; Automatically rename components, insert text macros, open recent files; Easily backup your projects, with custom additional file lists; Keep nested lists of favorite files for quick access; Track dependencies between units in your project; Quickly jump to any procedure in the current unit; And much, much more. CnWizards. Free Plug-in Tool Set for Delphi/C++ Builder/CodeGear RAD Studio to Improve Development Efficiency. Delphi Package Installer (DelphiPI). Tool which aids you installing components to your Delphi IDE. DelphiPI automatically resolves dependencies between packages, compiles, installs and adds source paths to your IDE. ResEd. Expert for Delphi 2005, 2006, 2007, 2009, 2010 and XE. This expert is designed for editing the resource files (.res; .resx) that are linked to the active project. It will automatically search for all occurrences of {$R xyz.res} lines and will open/create resourcefiles for them. The expert registers itself in the menubar of Delphi under View. Parnassus Bookmarks. IDE plugin that extends bookmark functionality. DelphiSettingManager. Multiple IDE profiles for Delphi (up to XE6). Allows to install multiple versions of the same component or different component sets for different projects. Delphinus. New Packagemanager which runs on Delphi XE and newer and uses GitHub as a Backend to Provide the packages. TestInsight. Unit testing IDE Plugin for Delphi. It supports all versions from XE to 10 Seattle. Supports DUnit, DUnit2, DUnitX frameworks. Delphi IDE Explorer. Wizard / expert / plugin that allows you to browser the internal fields, methods, properties and events of the IDE. // Mainly useful for developers of IDE experts Multi-RAD Studio IDE Expert Manager. Application is for editing the installed experts in all versions of RAD Studio (and older Delphi and C++ Builder) on a machine. OTA Interface Search. Application helps to find Open Tools API (OTA) interfaces, methods and properties and understand how to get to those interfaces or methods / properties of the interfaces. AutoSave. Expert that periodically auto saves all the open modified IDE files. Browse and Doc It. Plug-in allows you to document and browse your code from within the IDE. Integrated Testing Helper. Plugin for Delphi and RAD Studio that allows you to run command-line application before and after the compilation of you projects. It also provides the ability to zip you projects files into an archive on each compile/build and manage the application's version information. Project Magician. Wizard for advanced project options manipulation. Selective Debugging. Wizard that allows to tune for which units their debug version will be used. MMX Code Explorer. Feature-rich productivity enhancing plugin. Includes refactoring, class browser, advanced editing, metrict and many more. FormResource. Wizard that helps storing various data as form resources. Delphi Library Helper Tool to assist Delphi developers configuring library folders. Mobile Image Creator Creating Icons and Launcher Images for Delphi Mobile Applications (Firemonkey). This is a fork of Mobile Gfx created by Thomas Grubb of RiverSoftAVG. Delphi-Adb-WiFi. Plugin for RAD Studio, which allows launching and debugging on an Android device without connecting to a computer via USB. Works over WiFi. RADSplit. Dockable Split-Screen Editors for RAD Studio (Delphi and C++ Builder). DzNoteEditor. Delphi Property Editor for TStrings supporting formatted languages with syntax highlight. Plugins for other IDE's Delphi IDE theme editor / Delphi IDE Colorizer. Supports Appmethod, Lazarus and Smart Mobile Studio. Pascal and Pascal Formatter. Open source extensions created for Visual Studio Code that add Pascal support. Documentation SynProject (docs). Tool for code source versioning and automated documentation of Delphi projects. PasDoc. [Delphi] [FPC] Documentation tool for ObjectPascal (FreePascal and Delphi) source code. Documentation is generated from comments found in source code. Available output formats are HTML, HtmlHelp, LaTeX, latex2rtf, simplexml. More output formats may be added in the future. Code check/review, debug GpProfiler2017. [Delphi] Source code instrumenting profiler for Delphi XE and higher. Other forks support older versions. SamplingProfiler. [Delphi] Performance profiling tool for Delphi 5 to 32bits Delphi XE4. Its purpose is to help locate bottlenecks, even in final, optimized code running at full-speed. Delphi Code Coverage. [Delphi] Simple Code Coverage tool for Delphi that creates code coverage reports based on detailed MAP files. Pascal Analyzer (free Lite version available). [Delphi] Pascal Analyzer, or PAL for short, parses Delphi or Borland Pascal source code. It builds large internal tables of identifiers, and collects other information such as calls between subprograms. When the parsing is completed, extensive reports are produced. These reports contain a great deal of important information about the source code. This information will help you understand your source code better, and assist you in producing code of higher quality and reliability. madExcept. [Delphi] madExcept was built to help you locate bugs in your software. Whenever there's a crash/exception in your program, madExcept will automatically catch it, analyze it, collect lots of useful information, and give the end user the possibility to send you a full bug report. madExcept is also able to find memory leaks, resource leaks and buffer overruns for you. // Free without source for non-commercial usage (only) with some conditions. Available to download as part of madCollection installer (you'll need to install madExcept item). Pretty well documented. delphiunitsizes. [Delphi] Tool to display the sizes of each unit in a Delphi executable. Shows the size of each unit that is included in a Delphi exe-file. It also shows an approximate size of each symbol (classes, methods, procedures etc) in a unit. MapFileStats. [Delphi] Tool that provides simple binary size statistics from .MAP files (any Delphi version up to at least Delphi XE5). Spider. [Delphi] Real time profiler for Delphi applications AsmProfiler. [Delphi] Full tracing 32bit profiler (instrumenting and sampling), written in Delphi and some assembly map2pdb. [Delphi] Tool used to convert the MAP files produced by the Delphi and C++ Builder compilers to Microsoft PDB files for use in tools that support that format. Setup Lazy Delphi Builder. Build tool for Delphi. Recompile projects/packages from sources with all dependencies, without need to mess around with configs. Quickly (re-)install components from sources into IDE, with no need to change your Library Path. // Powerful automating tool. Freeware but not open source Inno Setup. Free installer for Windows programs. First introduced in 1997, Inno Setup today rivals and even surpasses many commercial installers in feature set and stability. WinSparkle and its Delphi wrapper. WinSparkle is an easy-to-use software update library for Windows developers. WinSparkle is a heavily (to the point of being its almost-port) inspired by the Sparkle framework originally by Andy Matuschak that became the de facto standard for software updates on macOS. Silverpoint MultiInstaller. Multi component package installer for Embarcadero Delphi and C++Builder, it was created to ease the components installation on the IDE. Grijjy Deployment Manager. Tool to simplify the deployment of files and folders for iOS and Android apps written in Delphi. It is especially useful if you need to deploy a lot of files, such as 3rd party SDKs. Other WMI Delphi Code Creator. Allows you to generate Object Pascal, Oxygene, C++ and C# code to access the WMI (Windows Management Instrumentation) classes, events and methods. Also includes a set of tools to explorer and Query the content of the WMI. Delphi Preview Handler. Preview handler for Windows Vista, 7 and 8 which allow you read your object pascal, C++ and Assembly code with Syntax highlighting without open in a editor Delphi Dev. Shell Tools. Windows shell extension with useful tasks for Object Pascal Developers (Delphi, Free Pascal). Delphi.gitignore. .gitignore templates for Delphi. There is also one for Lazarus. OmniPascal. Project that enables Delphi and Free Pascal developers to write and maintain code using the modern editor Visual Studio Code. Delphi Unit Tests. Set of unit tests for Delphi's libraries. Delphi community members are encouraged to fork the repository, add tests, and create a pull request. Embarcadero employees are particularly encouraged to add tests from the internal tests that are run with official Delphi builds. madDisAsm. The package features a full x86 disassembler including MMX, 3dNow enhanced, SSE and SSE2 support. The disassembler can examine a single x86 instruction (see ParseCode) or a full function (see ParseFunction) and either return a short analysis or a full text disassembly. Register contents are watched/followed if possible, this improves the analyses for jump/call targets. Case/switch jump tables are automatically detected and handled correctly. // Free without source for non-commercial usage (only) with some conditions. Available to download as part of madCollection installer (you'll need to install madExcept item). Pretty well documented. Chet - C Header Translator for Delphi. Chet is a .h-to-.pas translator powered by libclang for Delphi. Uses the Clang compiler to parse header files, resulting in more accurate translations that require fewer manual adjustments. Boss. Dependency Manager for Delphi projects. C-To-Delphi. [Delphi] This tool will convert most of your standard C code. Better Translation Manager. [Delphi] Translation Manager dzBdsLauncher. [Delphi] Launcher for the Delphi IDE that decides which of multiple IDEs to launch based on the suffix of the dproj file passed to it. DFMJSON. [Delphi] Library to convert between Delphi's .DFM (or .FMX) format and JSON. It can be used to parse a DFM file into an Abstract Syntax Tree in JSON, which can then be edited and the results turned back to DFM format. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/Fr0sT-Brutal/awesome-pascal",
        "comments.comment_id": [19895720, 19896786],
        "comments.comment_author": ["narag", "EdgarVerona"],
        "comments.comment_descendants": [1, 3],
        "comments.comment_time": [
          "2019-05-13T01:10:52Z",
          "2019-05-13T06:28:12Z"
        ],
        "comments.comment_text": [
          "Wow, that's a comprehensive list! I've been searching for the most useful resources I've used in 25 years and they're all there.<p>On the other hand, I see that Delphi usenet forums have been closed for a year... long time I had not checked.",
          "There's a tender place in my heart for Pascal.  The first language I was taught was Turbo Pascal (I had sort of taught myself QBasic before that, as much as one can from reading the help manual and stumbling through), and the built-in libraries was as close to the liberty and scope of Python as you could get in those days.  It really inspired me to continue down the path of software development.<p>I also have fond memories of BBS door games and software, a large portion of which was built with Pascal.<p>I digress: but it's nice to reminisce about those old days."
        ],
        "id": "cc5a5d0d-4d01-4bb6-8e96-045cce73c5f3",
        "url_text": "Awesome Pascal A curated list of awesome Delphi, FreePascal and other *Pascal frameworks, libraries, resources, and shiny things. Inspired by awesome-xxx stuff. Note that only open-source projects are considered. Dead projects (not updated for 3 years or more) must be really awesome or unique to be included. Feel free to suggest other missing nice projects either by comments or pull requests. This awesome collection is also available on Delphi.ZEEF.com Note on compiler compatibility. There are compiler/language dialect compatibility badges for all projects based on a project's description. No real compatibility with compilers not officially supported is checked. Often a code could be used with non-supported compiler/language dialect with minor modifications but there could be exceptions. Contents General Libraries Multimedia Audio Video Graphic Game dev Communications Network Serial port GUI Control packs Single controls Editors Viewers Other GUI Database Scripting Machine Learning Non-visual Classes/Utils Compression Encryption XML/JSON/YAML/HTML Language Memory managers System Template Logging Math Command-line Other non-visual OS Report generating Unit Testing Debugging / error handling Utilities RAD Studio IDE plugins/wizards Plugins for other IDE's Documentation Code check/review, debug Setup Other General Libraries Big general-purpose libraries JCL. [Delphi] [FPC] Set of thoroughly tested and fully documented utility functions and non-visual classes which can be instantly reused in your Delphi and C++ Builder projects. The library is grouped into several categories such as Strings, Files and I/O, Security, Math and many, many more. JVCL. [Delphi] Library of over 600 Delphi components developed by \"Project JEDI\" members. // GUI, algorithms, classes, API headers etc. Alcinoe (mirror at GH). [Delphi] Library of visual and non-visual components for Delphi. // Network: FTP/Http/NNTP/POP3/SMTP, ISAPI, WinInet Http/FTP clients; DB: Firebird/MySQL/SQLite3/Memcached/MongoDb/SphinxQL; XML/JSON Parser; ZLIB; Cryptography: AES, Blowfish, MD5, SHA, secure keyed MD5/SHA; opengl video player; FireMonkey controls; Other: Self-Balancing Binary Trees, expression evaluator Fundamentals Code Library (abandoned, more recent fork is here - though it slightly differs in units set, f.ex. no XML. Recent major version 5 here). [Delphi] [FPC] Collection of Delphi / FreePascal code units. Includes libraries for Unicode, Strings, Data Structures, Sockets and Mathematics. // Utils: ZLIB compression; JSON; XML; ProtocolBuffers; Unicode routines; data structures; Hashes: XOR, CRC, Adler, MD5, SHA, secure keyed MD5/SHA, etc; Network: blocking TCP client/server, HTTP(S) via SSL3/TLS1.0/TLS1.1/TLS1.2 (fully native); SQL parser; BitCoin MtGox client; Blaise script engine; Cipher: AES, DES, FUNE, RC2, RC4, RSA, Diffie-Hellman; Maths: matrix, complex, statistics, huge numbers Spring4D. [Delphi] Open-source code library for Embarcadero Delphi 2010 and higher. It consists of a number of different modules that contain a base class library (common types, interface based collection types, reflection extensions) and a dependency injection framework. Includes Encryption Library. // Collections and other containers using Generics and based on IEnumerable, probably more accurate and featured than RTL analogs; crypto: CRC, DES, MD5, SHA; file utils etc TheUnknownOnes. [Delphi] Huge heap of classes, components, utilities for almost every purpose. Nearly undocumented and seems not very up-to-date though. CNVCL. [Delphi] CnPack Component Package. Large collection of visual components, classes and utilities. // Lots of useful stuff; documentation and comments mainly in Chinese mORMot. [Delphi] [FPC] Client-Server ORM/ODM SOA MVC framework for Delphi 6 and higher, or FPC 2.7. Direct SQL/NoSQL database access, ORM/ODM over objects, RESTful ORM and SOA services via interfaces over high performance HTTP server, MVC/MVVM web sites, testing including mocks and stubs, logging, cryptography, compression, huge documentation. MARS - Curiosity. [Delphi] Delphi REST Library. Pure REST approach, standard concepts in a familiar Delphi flavor (including a component based client library). Known compatibility: Delphi versions from XE to 10 Seattle. Some functionalities requires FireDAC. ADAPT. [Delphi] Advanced Developer Async Programming Toolkit, foundation library intended to be used at the heart of your projects for the purpose of providing extremely powerful, multi-threaded (and thread-safe) capabilities. Event Engine - a very powerful system for producing Multi-Threaded, Asynchronous and Event-Driven programs. Generics Collections - highly efficient Collection Types (Lists, Trees, Maps etc.). Math Library - a library for Unit Conversion, special calculation and other useful mathematics routines. Package Engine - extension of the Streamables Engine supporting the packaging of files together (a VFS of sorts). Shared Streams Library - 100% Thread-Safe Stream Classes (Interfaced too) allowing read/write from multiple Threads. Stream Handling Library - makes working with Streams much easier! Handles Deleting, Inserting, Reading and Writing data. Redux Delphi. [Delphi] Predictable state container for Delphi apps utilizing a unidirectional data flow. Inspired by ReduxJS. Comes with Immutable Generic List. GrijjyFoundation. [Delphi] Foundation classes and utilities that are used throughout the other Grijjy Repositories. // BSON/JSON, IOCP/EPOLL sockets, socket pools, HTTP, HTTP/2, OpenSSL, ProtocolBuffers. unRxLib. [Delphi] Effort to keep RxLibrary (library of 60+ components) actual. QuickLib. [Delphi] [FPC] Quick development library (AutoMapper, LinQ, IOC Dependency Injection, MemoryCache, Scheduled tasks, Config, Serializers, Json Serialize, Chronometer, Threads, Lists, Config, Console services etc) with crossplatform support for Delphi/Firemonkey (Windows,Linux,macOS/IOS/Android) and freepascal (Windows/Linux). KOL. [Delphi] [FPC] (KOL-CE port to FPC) KEY OBJECTS LIBRARY for Delphi (and FPC) - to make applications small and power. This library is freeware and open source. MCK is a kit of mirror classes for the VISUAL project development in Delphi environment using KOL library. cwRuntime. [Delphi] [FPC] Compiler agnostic and cross platform collection of utility libraries for Delphi and FreePascal. It is heavily interface based, offering ARC based memory management features and flexible implementation abstraction, with the goal of forming a source bridge for developers familiar with the two supported compilers. Unit testing, collections/containers, multiplatform interface for loading dynamic libraries, Unicode utils, interfaces for working with streams and buffers, logging, threading, high-precision timers, sockets. Multimedia Audio Audio Tools Library. [Delphi] For manipulating many audio formats file information. // Abandoned since 2005. Delphi ASIO & VST Project. [Delphi] Framework for writing applications using the ASIO interface and VST plugins. It comes with countless DSP algorithms all demonstrated in dozens of examples. // Not very active lately, but the trunk is in a usable state NewAC - New Audio Components (abandoned, list of forks on GH here). [Delphi] Designed to help your Delphi programs perform different sound processing tasks. With NewAC you can play audio stored in many formats (wav, Ogg Vorbis, FLAC, Monkey Audio, WavPack, MP3, Windows WMA, DTS, AC-3 (Dolby Surround), VOB (DVD files)). // Playback, recording, tag read/write, some audio editing tasks and conversions Audorra. [Delphi] [FPC] Digital audio library for Delphi and Freepascal. Using a flexible plugin architecture, it allows you to exchange the audio backend (e.g. WaveOut, OpenAL), add protocol classes (e.g. file, http) and decoders. Delphi-BASS. [Delphi] Delphi's FMX and VCL header/wrapper units for BASS audio library plus add-ons. Video DSPack (abandoned, active fork is here). [Delphi] Set of components and classes to write Multimedia Applications using MS Direct Show and DirectX technologies. Delphi-OpenCV. [Delphi] Translation of OpenCV library header files in Delphi // Includes FFMPEG headers FFmpeg Delphi/Pascal Headers. [Delphi] [FPC] Open source translation of FFMPEG headers. PasLibVlc. [Delphi] [FPC] Interface to VideoLAN libvlc.dll and VCL player component for Delphi / FreePascal based on VideoLAN fevh264. [FPC] Baseline h.264 encoder. Windows and Linux are supported Graphic Image files, free drawing, barcodes etc. There are also some drawing engines in Game dev section Graphics32. [Delphi] [FPC] Designed for fast 32-bit graphics handling on Delphi, Kylix and Lazarus. Optimized for 32-bit pixel formats, it provides fast operations with pixels and graphic primitives, and in most cases Graphics32 outperforms the standard TCanvas classes. It is almost a hundred times faster in per-pixel access and about 2-5 times faster in drawing lines. GraphicEx. [Delphi] Addendum to Delphi's Graphics.pas to enable your application to load many common image formats. This library is primarily designed to load images as background (buttons, forms, toolbars) and textures (DirectX, OpenGL) or for image browsing and editing purposes as long as you don't need to save images. Vampyre Imaging Library. [Delphi] [FPC] Cross-platform native Object Pascal (Delphi and Free Pascal) image loading, saving, and manipulation library. CCR-EXIF (seems abandoned, list of forks on GH here). [Delphi] Library to read and write Exif, IPTC and XMP metadata from JPEG, TIFF and PSD images. KIcon. [Delphi] [FPC] This component makes sense if a more complex manipulation with icons (or better icon files *.ico) than just viewing is needed. Full PNG icon image support, correct rendering, icons with alpha channel. Delphi Twain. [Delphi] [FPC] The library allows you to easily access scanning functions from Delphi and Lazarus. Synopse PDF. [Delphi] [FPC] Fully featured Open Source PDF document creation library for Delphi, embedded in one unit. Pure Delphi code, Delphi 5 up to Delphi 10.3 Rio (and latest version of FPC), for Win32 and Win64 platforms. PowerPDF. [Delphi] VCL component to create PDF document visually. Like Forms, you can design PDF document easily on Delphi or C++Builder IDE. IGDI+. [Delphi] The free open source library allows quick and easy implementations of complex GDI+ applications, in a natural Delphi-friendly code. GLScene. [Delphi] [FPC] OpenGL based 3D library for Delphi, C++Builder and Lazarus. It provides visual components and objects allowing description and rendering of 3D scenes in an easy, no-hassle, yet powerful manner. GLScene is not just an OpenGL wrapper or utility library, it has grown to become a set of founding classes for a generic 3D engine with Rapid Application Development in mind. GLScene allows you to quickly design and render 3D scenes without having to learn the intricacies of OpenGL, if you know how to design a TForm, you'll easily master the basic operations of GLScene. The library comes with a large collections of demos showcasing the ease of use, and demonstrating RAD wasn't done at the expense of CPU/GPU horsepower. SynGdiPlus. [Delphi] [FPC] Enables an application to load and save GIF, TIF, PNG and JPG pictures. It also allows anti-aliased drawing from any TMetaFile. That is, you can play a .emf content using GDI+ instead of GDI, for much better rendering result. Andorra 2D. [Delphi] [FPC] New generation 2D Engine for Delphi and Lazarus. Andorra 2D is capable to use DirectX or OpenGL through graphic plugins. Andorra 2D is built in a very modular way and is yet easy to use. Transparent-canvas. [Delphi] Delphi VCL / Windows project for drawing semi-transparent alphablended graphics. It provides a class similar to TCanvas. Fully-justified-text. [Delphi] Delphi VCL / Windows project for text output, allowing printing of fully justified text with a variety of options. AsciiImage. [Delphi] AsciiImage-Implementation for Delphi by Alexander Benikowski based on AsciiImage by Charles Parnot. Read more on his article. // Creates scalable monochrome image from ASCII pixel map PngComponents. [Delphi] PngComponents is a set of components that allows you to include in your application real PNG files. PNG files on their own do not generate an enourmous advantage, but their support for an alpha-channel does indeed have quite a charm to it. AggPasMod. [Delphi] Modernized Pascal Anti-Grain Geometry. Based on AggPas, which is itself based on the Anti-Grain Geometry, this project offers support for the latest Delphi Versions (XE and above) and contains some helper classes (VCL components and FireMonkey interface). 2D vector graphics library. Basically, you can think of AggPas as of a rendering engine that produces pixel images in memory from some vectorial data. But of course, AGG can do much more than that. // Vector graphic library, renders SVG and much more delphi-shader. [Delphi] Hundreds of graphical effects, and a library that provides GLSL functionality in pure Delphi code. This project produces an executable with more than a hundred real-time graphical effects. All that is a 100% pascal implementation, without the use of external libraries or hardware acceleration. dglOpenGL. [Delphi] [FPC] Delphi / Pascal OpenGL header translation. DelphiZXingQRCodeEx. [Delphi] [FPC] Delphi/Lazarus port of the QR Code generating functionality from ZXing, an open source barcode image processing library. ZXing.Delphi. [Delphi] Native Object Pascal library for Delphi XE to 10.2 Tokyo that is based on the well known open source Barcode Scanning Library ZXing (Zebra Crossing). It is aimed at all of the FireMonkey mobile platforms and, starting from v3.1, it fully supports also Windows VCL applications (no dependencies on FMX.Graphics unit). Zint-Barcode-Generator-for-Delphi. [Delphi] Native Delphi port of Zint-Barcode-Generator. QuickImageFX. [Delphi] Delphi library for simplifying image load/save, conversion and transformation. Load/save png, jpg, gif and bmp. get image from different resources: file, stream, http, imagelist, associated windows icon, executable file icon, etc. Rotate, flip, grayscale and many other transformations. NativeJpg. [Delphi] Fully object-oriented Pascal implementation that allows to read and write Jpeg files. You can use this software to read and write Jpeg images from files or streams. It supports baseline and progressive Jpeg, support for metadata, as well as all conceivable lossless operations. OpenGL Pascal Toolkit. [FPC] Easy to use native pascal toolkit that allows to create and manage OpenGL contexts in a platform independent way. BGRAbitmap. [Delphi] [FPC] Drawing routines with transparency and antialiasing with Lazarus. Offers also various transforms. These routines allow to manipulate 32bit images in BGRA format or RGBA format (depending on the platform). Clipper. [Delphi] Library performs line & polygon clipping - intersection, union, difference & exclusive-or, and line & polygon offsetting dexif. [Delphi] [FPC] Lazarus port of Delphi EXIF Library to extract Exif Information from Images FontIconEditor. [Delphi] Simple component editor that allow you to add icons to a TImageList from a font. You can use any font you want. IconFontsImageList. [Delphi] Extended ImageList for Delphi (VCL & FMX) to simple use and manage Icon Fonts (with GDI+ support) Mundus. [Delphi] Software renderer written in Delphi. Currently supports only Win32 as it makes use of some inline assembler. Image32. [Delphi] [FPC] (Website) 2D graphics library written in Delphi Pascal. It provides an extensive range of image manipulation functions and includes a line and polygon renderer supporting a wide range of brush filling options. SVGIconImageList. [Delphi] Four engines to render SVG (Delphi TSVG, Delphi Image32, Direct2D or Cairo) and four components to simplify use of SVG images (resize, fixedcolor, grayscale, etc). Skia4Delphi. [Delphi] Cross-platform 2D graphics API for Delphi platforms based on Google's Skia Graphics Library. It provides a comprehensive 2D API that can be used across mobile, server and desktop models to render images. Game dev There are also some drawing engines suitable for game dev in Graphic section RecastNavigation. [Delphi] Navigation mesh construction toolset for games. Recast is accompanied with Detour, path-finding and spatial reasoning toolkit. You can use any navigation mesh with Detour, but of course the data generated with Recast fits perfectly. This is a port of the original RecastNavigation written in C++. Kraft Physics Engine. [Delphi] [FPC] Open source Object Pascal physics engine library that can be used in 3D games. Compatible with: Delphi 7-XE7 (but not with the Android and iOS targets), FreePascal >= 2.6.2 (with almost all FPC-supported targets including Android and iOS) ZenGL. [Delphi] [FPC] OpenGL Cross-platform game development library written in Pascal, designed to provide necessary functionality for rendering 2D-graphics, handling input, sound output, etc. // Not updated lately, but is working ok Asphyre aka Platform eXtended Library (PXL). [Delphi] [FPC] Cross-platform framework for developing 2D/3D video games, interactive and scientific applications. It aids the developer with mathematics, hardware control, resource management, displaying real-time graphics and text, handle user input and network communication capabilities. CrystalPathFinding. [Delphi] [FPC] Simple and effective library with an open source intended for the searching of the shortest paths by algorithms A*/WA* for maps based on tiles with 4 (simple), 8 (diagonal/diagonalex) or 6 (hexagonal) neighbors. Allegro-Pas (GitHub). [Delphi] [FPC] Wrapper to use the Allegro game library with Pascal/Delphi. Castle Game Engine. [FPC] Complete Pascal Game Engine. Cross-platform 3D and 2D game engine with a lot of graphic effects and a scene graph based on X3D. TileEngine. (GitHub) [Delphi] [FPC] OOP Pascal Wrapper and bindings for Tilengine 2D retro graphics engine. Tilengine is a cross-platform 2D graphics engine for creating classic/retro games with tilemaps, sprites and palettes. Its scanline-based rendering algorithm makes raster effects a core feature, a technique used by many games running on real 2D graphics chips. SDL2 (GitHub). [Delphi] [FPC] Pascal SDL 2 Headers. Simple DirectMedia Layer is a cross-platform development library designed to provide low level access to audio, keyboard, mouse, joystick, and graphics hardware via OpenGL and Direct3D. SFML. [Delphi] [FPC] Pascal SFML Headers. SFML provides a simple interface to the various components of your PC, to ease the development of games and multimedia applications. It is composed of five modules: system, window, graphics, audio and network. Currently Delphi and FPC/Lazarus are supported. However, due to a compiler incompatibility with the Delphi compiler (solved with workarounds), FPC is recommended at the moment. pasvulkan. [Delphi] [FPC] Vulkan header generator, OOP-style API wrapper, framework and prospective Vulkan-based game engine for Object Pascal. DarkGlass. [Delphi] DarkGlass is a general purpose game engine written using Delphi. JEDI-SDL. [Delphi] [FPC] Pascal headers for SDL from JEDI. Works with Delphi, Kylix, Free Pascal, Gnu Pascal and TMT Pascal. Apus Game Engine. [Delphi] [FPC] Cross-platform library for making mostly 2D games, GUI applications and web services. Supports UI, text rendering, on-fly localization, particles, basic scripting and many lower level subsystems. Uses OpenGL/GLES and DirectX. Delphi3D Engine. [Delphi] A 3D-graphic and game engine for Delphi and Windows Communications Network Socket communication, network protocols, encodings, etc Internet Component Suite. [Delphi] Asynchronous-based library composed of various Internet components and applications. Clients/servers for TCP, UDP, raw sockets, FTP, SMTP, POP3, NNTP, HTTP, Telnet and more. Supports SSL and TLS with the help of OpenSSL. Also includes Mime Decoder, SHA1/MD4/MD5 hashes, DES encryption. Indy. [Delphi] [FPC] Network components for Delphi, C++Builder, Delphi.NET, and FreePascal // All-in-one network library based on blocking sockets and threads. Included in default RAD studio installation since 2006. Ararat Synapse. [Delphi] [FPC] Pascal TCP/IP Library for Delphi, C++Builder, Kylix and FreePascal. Deals with network communication by means of blocking (synchronous) sockets or with limited non-blocking mode. This project not using asynchronous sockets! The Project contains simple low level non-visual objects for easiest programming without problems (no required multithread synchronisation, no need for windows message processing, etc) Great for command line utilities, visual projects, NT services, etc // TCP, UDP, ICMP, RAW; ICMP, DNS, SMTP, HTTP, SNMP, NTP, FTP, LDAP, NNTP, Telnet; IPv6; SOCKS proxy; SSL/TLS (via OpenSSL or Windows CryptoApi); PING; character code transcoding; MIME coding and decoding; CRC16, CRC32, MD5 and HMAC-MD5. Internet Professional. [Delphi] Set of VCL components providing Internet connectivity for Borland Delphi & C++Builder. iPRO includes POP3, SMTP, NNTP, FTP, HTTP, Instant Messaging, & HTML viewer components, as well as components for low-level socket access. // Seems abandoned but contains pretty large set of features incl ICMP, POP, SMTP, HTTP, NNTP, NTP, FTP, SMTP; HTML parser and viewer; MIME utils; cookies, certificates, caching, encryption etc SynCrtSock. [Delphi] [FPC] Features several sockets and HTTP client-server classes, including a high-performance http.sys based server under Windows, and a new thread-pool powered socket server. // Also implements http.sys binding under Windows and cURL binding under nix TML Messaging Suite. [Delphi] [FPC] Network messaging library for rapid development of extensible and scalable interfaces. Based on the peer to peer standard protocol BEEP (Blocks Extensible Exchange Protocol), defined in RFC3080 and RFC3081. libTML is suitable for many use cases and communication patterns. Equipped with a type safe data API, TML can transport hierarchical data structures fast and reliable. // The libTML Object Pascal Components are not only a language binding to the core library but a complete set of non visual components to simplify the usage of libTML with Embarcadero RAD Studio and Lazarus. DMVCFramework. [Delphi] Popular and powerful framework for web solution in Delphi. Delphi IOCP. [Delphi] Implements several network classes based on Windows IOCP technology. Socket, HTTP, Ntrip servers and clients. // Quite well documented and good styled code but Chinese only. delphi-aws-ses. [Delphi] Amazon Simple Email Service (AWS SES) library for Delphi applications. delphi-slackbot. [Delphi] Delphi library to send messages on Slack using slackbot. Kitto. [Delphi] Allows to create Rich Internet Applications based on a data model that can be mapped onto any database. The client-side part uses ExtJS (through the ExtPascal library) to create a fully AJAX application, allowing you to build standard and advanced data-manipulating forms in a fraction of the time. Kitto is aimed at Delphi developers that need to create web application without delving into the intricacies of HTML, CSS, JavaScript or learning to use a particular library such as ExtJS, yet it allows access to the bare metal if required. Daraja Framework. [Delphi] [FPC] Lightweight HTTP server framework for Object Pascal (Delphi 2009+ / Free Pascal 3.0). Implementing RESTful services is supported via the daraja-restful extension. Alcinoe. FTP/Http/NNTP/POP3/SMTP, ISAPI, WinInet Http/FTP clients. Fundamentals Code Library. Blocking TCP client/server, HTTP(S) via SSL3/TLS1.0/TLS1.1/TLS1.2 (fully native). mORMot. RESTful ORM and SOA services via interfaces over high performance HTTP server, MVC/MVVM web sites SDriver. [Delphi] Delphi wrapper for Slack API. Hprose for Delphi/Lazarus. [Delphi] [FPC] High Performance Remote Object Service Engine. It is a modern, lightweight, cross-language, cross-platform, object-oriented, high performance, remote dynamic communication middleware. It is not only easy to use, but powerful. This project is the implementation of Hprose for Delphi/Lazarus. TelegAPI. [Delphi] Library for working with Telegram messenger Bot API in Delphi. fp-telegram. [FPC] Library for working with Telegram bots API in FreePascal/Lazarus. DelphiZeroMQ. [Delphi] Delphi implementation of ZeroMQ Majordomo protocol and CZMQ high level binding. GrijjyFoundation. IOCP/EPOLL sockets, socket pools, HTTP, HTTP/2, OpenSSL, ProtocolBuffers. STOMP Client. [Delphi] [FPC] STOMP client for Embarcadero Delphi and FreePascal. The project can use INDY (Delphi) or Synapse (Delphi or FreePascal). delphiXero. [Delphi] XERO cloud accounting API for Delphi. BesaSoap. [Delphi] The BesaSoap library is designed to help programmers develop faster and more native web service client applications. Represents C# or Java like native class support, nullable data types and custom attributes. IndySoap. [Delphi] Open Source Library for implementing Web services using Delphi/CBuilder Compilers. IndySoap isn't tied to Indy for transport services, though Indy based transport services are included. Fano Framework. [FPC] Web application framework for modern Pascal programming language. It is written in Free Pascal. Internet Tools. XPath/XQuery/JSONiq/CSS/HTML; functions to perform HTTP/S requests on Windows/Linux/macOS/Android, an XSLT-inspired webscraping language, and an auto update class. Delphi Cross Socket. [Delphi] Delphi cross platform socket library (Chinese). Uses different IO models for different platforms: IOCP (Windows), KQUEUE (FreeBSD(macOS, iOS, etc)), EPOLL (Linux(Linux, Android)). ToroKernel. [FPC] This is a library-kernel that allows freepascal applications which are specially ported to run alone in the system. Toro is compiled within the user's application thus resulting in a single binary that can boot on baremetal or as a guest in a modern hypervisor,e.g., hyperv, kvm, qemu, firecraker. ToroKernel addresses the development of microservices by providing a dedicated API. Horse. [Delphi] [FPC] Fast and minimalist web framework. Horse allows to create powerful RESTful servers without effort. Focused on microservices. Bauglir WebSocket. [Delphi] [FPC] WebSocket server/client implementation based on Ararat Synapse. Delphi-RabbitMQ. [Delphi] RabbitMQ driver for Delphi DelphiGrpc. [Delphi] Implementation of the realtime and streaming gRPC protocol Google API for Delphi. [Delphi] Google API for Delphi Delphi JOSE and JWT Library. [Delphi] Delphi implementation of JOSE (JSON Object Signing and Encryption) and JWT (JSON Web Token) WiRL. [Delphi] Project was created to simplify RESTful service implementation in Delphi but, more importantly, to enable maximum interoperability with REST clients written in other languages and tools OpenSSL. [Delphi] Delphi wrapper for OpenSSL Thrift Delphi Software Library. [Delphi] Lightweight, language-independent software stack for point-to-point RPC implementation. Thrift provides clean abstractions and implementations for data transport, data serialization, and application level processing. The code generation system takes a simple definition language as input and generates code across programming languages that uses the abstracted stack to build interoperable RPC clients and servers. Thrift makes it easy for programs written in different programming languages to share data and call remote procedures. With support for 28 programming languages, chances are Thrift supports the languages that you currently use. Delphi Modbus. [Delphi] [FPC] Implementation of a ModbusTCP protocol master and slave over TCP/IP. RESTRequest4Delphi. [Delphi] RESTRequest4Delphi is a API to consume REST services written in any programming language. Designed to facilitate development, in a simple and minimalist way. LazWebsockets. [FPC] This provides a small Websocket server and client implementation written for the FPC and Lazarus. It is fully based upon the fcl ssockets unit and therefore independent from any additional dependencies except from the FCL. NetCom7. [Delphi] This set of components is the fastest possible implementation of socket communications, in any language; this is an extremely optimised code on TCP/IP sockets. VK API. [Delphi] Library for working with Vkontakte (Russian social network) API in Delphi. Full API (with Bot samples). AWS SDK for Dephi. [Delphi] Unofficial AWS (Amazon Web Services) SDK for Delphi. WARNING! Requires paid libs from TMS Voice Communication. [Delphi] Voice Communicator Components. // Implement RTP, RTSP, SHOUT, SNTP, STUN protocols and multiple audio format endocing/deconding libPasCURL. [Delphi] [FPC] Bindings and wrapper around cURL library. libcurl is the library is using for transferring data specified with URL syntax, supporting HTTP, HTTPS, FTP, FTPS, GOPHER, TFTP, SCP, SFTP, SMB, TELNET, DICT, LDAP, LDAPS, FILE, IMAP, SMTP, POP3, RTSP and RTMP. Delphi_SChannelTLS. [Delphi] Helper functions and socket classes to perform TLS communication by means of WinAPI (SChannel). Includes Overbyte ICS TWSocket descendant class. Serial port Synaser. [Delphi] [FPC] Library for blocking communication on serial ports. It is non-visual class as in Synapse, and programmer interface is very similar to Synapse. Async Professional (Newest and maintained version for recent compiler version only). [Delphi] Comprehensive communications toolkit for Embarcadero Delphi, C++Builder, & ActiveX environments. It provides direct access to serial ports, TAPI and the Microsoft Speech API (TTS/Speech recognition). It supports faxing, terminal emulation, VOIP, RAS dial & more. // Seems outdated (last update in 2011) but adapted to XE and should be easy to use in newer versions. The project is also very thoroughly documented. Second link points to an adapted version for newest compiler versions. TComPort. [Delphi] Delphi/C++ Builder serial communications components. It is generally easy to use for basic Serial Communications purposes. // Seems abandoned since 2011 GUI Visual components Control packs Large sets of GUI controls Cindy components. [Delphi] Packages with 71 components: VCL controls (labels, buttons, panels, Edits, TabControls, StaticText) with features like background gradient, colored bevels, wallpaper, shadowText, caption orientation etc. Orpheus (Newest and maintained version for recent compiler version only). [Delphi] Award-winning UI toolkit for Borland Delphi & C++Builder. It contains over 120 components covering everything from data entry to calendars and clocks. Other noteworthy components include an Object Inspector, LookOut bar & report views. // Advanced edits, comboboxes, grids + component (de)serializers. GUI components look rather old-style, theme support might be limited. Package contains many demos but no docs seem available. Second link points to an adapted version for newest compiler versions. KControls. [Delphi] [FPC] Control components. All controls have been written with the aim to become both cross-IDE compatible (Delphi/C++Builder VCL and Lazarus LCL) and cross-platform compatible in Lazarus. // Most useful are TKGrid with its DB-aware heritage TKDBGrid a very full-featured grid implementation incl. inplace editors. There's also hex editor, print preview, editors, labels, buttons etc. D.P.F Delphi Android / D.P.F Delphi iOS native components. [Delphi] D.P.F Delphi Native Components, 100% iOS Performance and styles. Develop iPhone & iPad & iPod Touch applications with fast native performance and native styles. Use native Android controls and services. Fast native performance. Mixed with FM VCL controls. Can be quick updated with latest Android controls & features. Essentials. [Delphi] Contains 13 native VCL controls for Embarcadero Delphi and C++Builder. The controls include drop-down calendars and calculators, roll-up dialogs, 3-D labels, tiled backgrounds, scrolling messages, menu buttons, and more. FreeEsVCLComponents. [Delphi] Free library of VCL components for Delphi and C++Builder. This new controls and components to improve the appearance applications and to better user experience. Components support visual styles and has modern style. All components has best support transparency, not flicker, and has support interesting possibility for double buffering for TGraphicControl heirs. SpTBXLib. [Delphi] Add on package for Toolbar2000 components, it adds the following features: Skins, Unicode support, Custom painting events and many more. Kastri. [Delphi] Cross-platform library which builds upon the existing RTL and FMX libraries in Delphi. Supports a number of newer APIs that you won't find in FMX/RTL, and \"backfills\" for missing APIs DelphiUCL. [Delphi] UWP controls for Delphi VCL. JPPack. [Delphi] [FPC] Collection of VCL components for Delphi and LCL components for Lazarus and CodeTyphon - buttons, panels, LinkLabel, ProgressBar, ColorComboBox, ColorListBox, Timer and other DDuce. [Delphi] Components, modules, extensions and primitives using Delphi new language features like operator overloading, attributes, generics, anonymous methods and extended RTTI providing some new powerful tools to extend the developer's creativity. // Property editors, grids, XML Tree, etc Single controls EasyListView (seems abandoned, active fork on GH here). [Delphi] Part of VirtualShellTools for the Listview but can be used for a TListview Replacement that is faster and more customizable. // Feature-rich Listview implementing virtual (callback-based) MVC paradigm. VirtualTreeView. [Delphi] (VirtualTreeView-Lazarus port to FPC [FPC]). Treeview control built from ground up. Many years of development made it one of the most flexible and advanced tree controls available today. // Extremely flexible visual component implementing virtual (callback-based) MVC paradigm. Could be also used as a listview or grid. Used in RAD Studio GUI. Delphi Chromium Embedded. [Delphi] Embedding Chromium in Delphi, tested on Delphi 2010, XE, XE2, Delphi 7. // Several Chromium DLLs required TChromeTabs. [Delphi] Comprehensive implementation of Google Chrome's tabs for Delphi 6 - Delphi 10.1 Berlin TFrameStand. [Delphi] Easily use TFrame(s) in your FireMonkey (FMX) applications to gain visual consistency though the whole user experience and easily add modern looking elements like effects and transitions. TPrintPreview. [Delphi] Print Preview Component for Delphi Vcl Win32/Win64 VolgaDB. [Delphi] Pretty customizable DBgrid for Delphi. TCustomGrid descendant. CheckBox, ComboBox column styles. Also includes TVolgaDBEdit that replaces TDBEdit, TDBComboBox, TDBLookupCombo, TDBLookupTree andTDBDatePicker in one component. TVolgaDBEdit may be DB-aware and non DB-aware. // Seems abandoned since 2013 TTreeListView. [Delphi] [FPC] This component is a mix between TTreeView and TListView and can paint a tree whose nodes have additional information sorted in columns. neTabControl. [Delphi] FireMonkey control for Delphi. It builds on the native TabControl and adds a number of features. ATTabs. [Delphi] [FPC] Delphi/Lazarus component for lite tabs. OS independent, fully custom drawn. zControls. [Delphi] Contains TzObjectInspector - a powerful object inspector with many features. RiverSoftAVG Charting Component Suite. [Delphi] Free (for non-commercial use) with source charting Suite for adding charts and graphs to your programs. For Delphi 2010-Tokyo (Win32/Win64/macOS/iOS/Android) and Appmethod (Object Pascal). DzHTMLText. [Delphi] [FPC] Visual component that allows you to specify a formatted text in a label, using almost the same syntax used in HTML code. SMDBGrid component. [Delphi] The successor of TDBGrid with the extended features. Is able to display multiline wordwrap column titles, checkboxs for boolean fields, a convenient select of records from the keyboard and mouse via checkboxs, extanded Indicator column, fixing of columns, an opportunity to exclude insert and delete of records in the DBGrid, own standard PopupMenu, save/restore of a column states, processing of additional events etc. Multilanguage resources. decTreeView. [Delphi] The decTreeView library is an alternative implementation of the TreeView (SysTreeView32) control Editors SynEdit (mirror at GitHub). [Delphi] Syntax highlighting edit control, not based on the Windows common controls. SynEdit is compatible with both Delphi and Kylix LazEdit. [FPC] General text editor with syntax highlighting and tools to help edit HTML. ATSynEdit. [FPC] Multi-line editor control for Lazarus including syntax highlighting. QDSEquations. [Delphi] Equation editor for Delphi and Lazarus that allows you to enter and display math formulas of any complexity, from simple Greek symbols to matrixes and complex integral expressions. Viewers ATViewer (mirror at GitHub). [Delphi] Delphi components to view various file types: text, binary, images, multimedia, webpages, etc. // Used in Universal Viewer software. Could be used to display hex dumps, features fast display of unlimited size files/streams. Supports Total Commander Lister plugins. ATImageMap (mirror at GitHub). [Delphi] Component designed to show many images (parts of the whole image) as a single map. For example, you may have array of images, 200 by X, and 100 by Y and control will show them as a single map. Component also allows to draw paths: each path consists of many lines, points, and icons. HtmlViewer. [Delphi] [FPC] Delphi/Lazarus HtmlViewer/FrameViewer. // Html visualiser supporting majority of tags, inline styles and CSS. SciDe. [Delphi] [FPC] Sciter (Embeddable HTML/CSS/script engine) wrapper for Delphi. ATBinHex for Delphi [Delphi], ATBinHex for Laz. [FPC] Viewer for files of unlimited size like in Total Commander. ATImageBox for Delphi [Delphi], ATImageBox for Laz. [FPC] TScrollBox with embedded TImage. Control can auto position image inside. CEF4Delphi. [Delphi] [FPC] Project to embed Chromium-based browsers in applications made with Delphi or Lazarus/FPC Other GUI GMLib (Google Maps Library) (seems abandoned, active fork on GH here and here). [Delphi] Components for Delphi/C++ Builder that encapsulate the GoogleMaps API to administrate a map, markers, polygons, rectangles, polylines, etc. All objects that you can put into a map. VCL Styles Utils. [Delphi] Collection of classes and style hooks, which extend, fix QC reports and add new features to the VCL Styles. // Collection of patches/enhancements that promote stock VCL style engine to a new level. Styling for Inno Setup and NSIS also available. TaskbarListComponents. [Delphi] Set of components designed as Delphi wrappers for the Windows 7 Taskbarlist Interfaces (e.g. ITaskbarlist3) // Requires JVCL TFireMonkeyContainer. [Delphi] Delphi VCL component to host a FMX HD or 3D form. It means you can embed a FireMonkey (FMX) form as a control in a VCL form, so you can design a FMX form and use it in your VCL app. PascalSCADA. [Delphi] [FPC] Set of components (framework) for Delphi/Lazarus to make easy the development of industrial applications (HMI=Human Machine Interface/SCADA=System Control And Data Acquisition). It runs on Windows, Linux and FreeBSD. Windows Ribbon Framework for Delphi. [Delphi] This Delphi library allows Delphi developers to use of the Windows Ribbon Framework in their Delphi applications. This library uses the native Windows library to implement the Ribbon functionality. It does not emulate the Ribbon user interface like other Delphi component sets do (or Delphi's built-in Ribbon emulation components). DKLang. [Delphi] DKLang Localization Package is a set of classes intended to simplify the localization of applications written in Delphi. GNU Gettext for Delphi, C++ and Kylix. [Delphi] GNU GetText translation tools for Borland Delphi and Borland C++ Builder. OpenWire. [Delphi] The library allows writing advanced VCL and FireMonkey components for rapid codeless application development. The components developed with the library allow creation of complex applications with zero lines of program code. SynTaskDialog. [Delphi] [FPC] Implement TaskDialog window (native on Vista/Seven, emulated on XP) AnyiQuack. [Delphi] jQuery-like control animation framework. TLanguages. [Delphi] Localization tool for VCL and FMX. BitMapEditor - Delphi. [Delphi] Single-form, simple bitmap editor for Delphi. BearLibTerminal. [Delphi] Provides a pseudoterminal window with a grid of character cells and a simple yet powerful API for flexible textual output and uncomplicated input processing. // Multiplatform dynamic library that has Delphi bindings Dam. [Delphi] [FPC] Delphi and Lazarus Message Dialogs with Formatted Text. Database ZeosLib. [Delphi] [FPC] Set of database components for MySQL, PostgreSQL, Interbase, Firebird, MS SQL, Sybase, Oracle and SQLite. Unified Interbase. [Delphi] Set of components to use Interbase, FireBird and YAFFIL. These components were born from the need to use Interbase, FireBird or Yaffil indifferently as fast as possible in a Multithreading environment, a Server for example. ASQLite. [Delphi] Delphi SQLite set of DAC components from aducom software, based on their latest release for Delphi 2009, and updated to support newer editions of Delphi as included in RemObjects Data Abstract for Delphi. TxQuery. [Delphi] TDataSet descendant component that can be used to query one or more TDataSet descendant components using SQL statements. It is implemented in Delphi 100% source code, no DLL required, because it implements its own SQL syntax parser and SQL engine. Delphi-ORM. [Delphi] Object-Relational Mapping for Delphi XE2-7 (Win32). Supports for FirebirdSQL, SQLServer and SQLite3. delphimemcache. [Delphi] Implements a thread safe client for memcached. // Requires Indy 10 SynDB (docs). [Delphi] [FPC] High performance direct access to SQLite3, Oracle, MSSQL, PostgreSQL, Firebird, MySQL, ODBC, OleDB, including remote HTTP connection and direct JSON support. SynMongoDB (docs). [Delphi] [FPC] Offers direct low-level access to any MongoDB server, its custom data types, JSON or via TDocVariant custom variant document storage. DSharp. [Delphi] Small library for providing data binding in Delphi. It does not require special components to data bind to properties. It also provides dependency injection, MVVM and more interesting utilities. ghORM. [FPC] Object Relational Mapping unit to ease database access from Free Pascal, by abstracting the backend and simple data retrieval (with filtering), insertion and update. tDBF. [Delphi] [FPC] Native dBASE III+, dBase IV and dBase 2k data access component for Delphi, BCB, Kylix, FreePascal. It allows you to create very compact database programs which don't need any special installer programs. The DB engine code is compiled right into your executable. Redis client [Delphi] Delphi Redis Client version 2 is compatible with Delphi 10.1 Berlin and better. WARNING! If you use an older Delphi version you have to use Delphi Redis Client Version 1 wich works for Delphi 10 Seattle, XE8, XE7, XE6 and XE5 (should works also with older versions). This client is able to send all Redis commands and read the response using an internal parser. QDAC3 (SVN: svn://www.qdac.cc/QDAC3). [Delphi] Stands for quick data access components. Useful units such as QJson (easy to use json unit), QWorker (job delivery) etc. // Description and comments in Chinese, author is not good at English. Haven't tested this library by myself. InstantObjects. [Delphi] Integrated framework for developing object-oriented business solutions in Delphi. The framework provides the foundation for the development process as well as the engine that powers the final application. InstantObjects offers: Model realization in the Delphi IDE via integrated two-way tools; Object persistence in the most common relational databases or flat XML-based files; Object presentation via standard data-aware controls. Alcinoe. Firebird/MySQL/SQLite3/Memcached/MongoDb/SphinxQL. SynBigTable. [Delphi] [FPC] Class used to store huge amount of data with fast retrieval. tiOPF. [Delphi] [FPC] Object Persistent Framework written in Object Pascal, for use with Delphi and Free Pascal (FPC) compilers. tiOPF simplifies the mapping of an object oriented business model into a relational database. Persistence layers are available for Firebird, Oracle, MS SQL Server, MySQL, PostgreSQL, SQLite, NexusDB, XML, CSV, TAB, Remote (via HTTP) and many more. It also allows you to use your choice of database connection components, like IBX, dbExpress, DOA, SqlDB, FBLib etc. hcOPF. [Delphi] Object Persistent Framework written in Embarcadero's Delphi (Object Pascal). This Value Type Framework provides a base class (ThcObject) composed of attribute objects that can be automatically persisted to an object store (normally an RDBMS). Marshmallow. [Delphi] Object-Relational Mapping for Delphi XE2-7 (Win32) inspired by .NET micro ORM's (mostly by PetaPoco) and Java Hibernate. Developed by Linas Naginionis. Supports SQLite, Sybase ASA, SQL Server, Firebird, Oracle, MySQL, PostgreSQL, MongoDB. Uses Spring Framework. In active development. DelphiCassandra. [Delphi] Delphi driver classes to communicate with Cassandra database. DelphiCouchbase. [Delphi] Delphi driver classes to communicate with Couchbase database. DelphiMongoDB. [Delphi] Delphi driver classes to communicate with MongoDB database. QuickORM. [Delphi] [FPC] QuickORM is a simple RestServer and Restclient based on mORMot framework. Provides a fast implementation of client-server applications in few minutes. iORM. [Delphi] Delphi ORM interface based useful to develop desktop and mobile application. d-ORModel. [Delphi] ORM for Delphi, based on models and object fields. LINQ support, fully typed and compile time checks. Scripting Using script engine in your applications Pascal Script. [Delphi] [FPC] Free scripting engine that allows you to use most of the Object Pascal language within your Delphi or Free Pascal projects at runtime. Written completely in Delphi, it is composed of a set of units that can be compiled into your executable, eliminating the need to distribute any external files. Pascal Script started out as a need for a good working script, when there were none available at the time. DWScript. [Delphi] Object-oriented scripting engine for Delphi based on the Delphi language, with extensions borrowed from other Pascal languages (FreePascal, Prism, etc.). It introduces a few Pascal language extensions of its own as well. Delphi-JavaScript. [Delphi] JavaScript engine for delphi based on Mozilla's Spidermonkey. // Spidermonkey DLL required Blaise. [Delphi] Open-source object-oriented scripting language. Language features: Object-oriented; Unicode support; Optional typing, ie dynamic or static typing; Richly typed; Higher-level mathematics support, for example Complex numbers, Rational numbers and Matrices; Virtual Machine architecture; Co-routines; Familiar language syntax, influenced by Object Pascal, Python and Ada. SpiderMonkey. [Delphi] [FPC] Binding for Mozilla JavaScript engine, including JIT and multi-threading, with easy objects access via Delphi variants. // Spidermonkey DLL required BESEN. [Delphi] [FPC] Complete ECMAScript Fifth Edition Implementation in Object Pascal, which is compilable with Delphi >=7 and Free Pascal >= 2.5.1 (maybe also 2.4.1). Python for Delphi (P4D). [Delphi] [FPC] Set of free components that wrap up the Python dll into Delphi and Lazarus (FPC). They let you easily execute Python scripts, create new Python modules and new Python types. You can create Python extensions as dlls and much more CrystalLUA. [Delphi] Lua binding (Delphi6-2007). // LUA DLL required lua4delphi. [Delphi] Delphi binding for Lua 5.1 language. // LUA DLL required chakracore-delphi. [Delphi] [FPC] Delphi and Free Pascal bindings and classes for Microsoft's ChakraCore JavaScript engine library. VerySimple.Lua. [Delphi] Lua Wrapper for Delphi XE5-D10.1 which automatically creates OOP callback functions for Delphi <-> Lua. // LUA DLL required QuickJS-Engine. [Delphi] [FPC] Delphi and Free Pascal bindings for Bellard's QuickJS JavaScript Engine. Machine Learning Machine learning and neural networks noe. [FPC] Framework to build neural networks in pure object pascal. Non-visual Classes/Utils Compression FWZip. [Delphi] Classes to work with Zip archives using Store and Deflate methods, supports ZIP64, DataDescryptors, PKWARE encryption, NTFS attributes, Utf8 in filenames. // Uses stock ZLIB.obj that gets compiled into binary. Comments and description in Russian. Abbrevia (Newest and maintained version for recent compiler version only). [Delphi] Advanced data compression toolkit for Delphi and C++Builder. Supports PKZIP, Microsoft CAB, tar, gzip, and bzip2 archives, and can create self-extracting executables. On Windows it also provides Delphi wrappers for the LZMA, Bzip2, and WavPack SDKs, and PPMd decompression. Abbrevia also has several visual controls that simplify displaying and manipulating archives, including treeview and listview components. Features: Unicode filenames in all archive formats; Decompress most .zipx and legacy (PKZIP v1) zips; ZIP64 support for archives larger than 2GB; Spanned and split zip archives; Cross-platform (Windows, OS X, and Linux); No DLLs required; Includes COM component; Extensive documentation // Second link points to an adapted version for newest compiler versions. SynLZ SynLZO SynZip PasZip. [Delphi] [FPC] Several high speed compression units, featuring ZIP/LZ77 Deflate/Inflate, LZO and SynLZ algorithm, in pascal and optimized assembler. Delphi zlib. [Delphi] Wrapper for zlib.obj originally used by Borland. Delphi up to XE3 supported. DIUcl. [Delphi] DIUcl is a lossless compression library with extremely fast and small (200 bytes only!) ASM decompressor. Compression times and ratios are similar to those of deflate/zip and bzip2. Delphi port of the popular UCL Compression Library, which is also used by the popular and well known UPX Ultimate Packer for eXecutables. Encryption Delphi Encryption Compendium (DEC). [Delphi] [FPC] Cryptographic library for Delphi & C++ Builder. Symmetric cryptographic functions: Blowfish, Twofish, IDEA, Cast128, Cast256, Mars, RC2, RC4, RC5, RC6, Rijndael / AES, Square, SCOP, Sapphire, 1DES, 2DES, 3DES, 2DDES, 3DDES, 3TDES, 3Way, Gost, Misty, NewDES, Q128, SAFER, Shark, Skipjack, TEA, TEAN; Block cipher modes of operation: CTSx, CBCx, CFB8, CFBx, OFB8, OFBx, CFSx, ECBx; Hashes: MD2, MD4, MD5, RipeMD128, RipeMD160, RipeMD256, RipeMD320, SHA, SHA1, SHA224, SHA256, SHA384, SHA512, SHA3-224, SHA3-256, SHA3-384, SHA3-512, Haval128, Haval160, Haval192, Haval224, Haval256, Tiger, Panama, Whirlpool, Whirlpool1, WhirlpoolT, Square, Snefru128, Snefru256, Sapphire. LockBox (Newest and maintained version for recent compiler version only). [Delphi] Delphi library for cryptography. Currently supported Delphi XE6. It provides support for AES, DES, 3DES, Blowfish, Twofish, SHA2 (including the new SHA-512/224 & SHA-512/256), MD5; ECB, CBC, CFB8, CFB, CTR, ECB, OFB, PCBC chaining modes, RSA digital signature and verification. Has interface to OpenSSL library. // Check out this page as well for alternative version. SynCrypto. [Delphi] [FPC] Fast cryptographic routines (hashing and cypher), implementing AES, XOR, RC4, ADLER32, MD5, SHA1, SHA256 algorithms, optimized for speed (tuned assembler and VIA PADLOCK optional support). TForge. [Delphi] [FPC] Open-source crypto library written in Delphi, compatible with Free Pascal Compiler. MD5, SHA1, SHA256, CRC32, Jenkins-One-At-Time, HMAC, PBKDF1, PBKDF2, AES, DES, RC4, RC5, Salsa20. Spring4D. CRC, DES, MD5, SHA Fundamentals Code Library. Hashes: XOR, CRC, Adler, MD5, SHA, secure keyed MD5/SHA, etc; Cipher: AES, DES, FUNE, RC2/4, RSA. Alcinoe. AES, Blowfish, MD5, SHA, secure keyed MD5/SHA. DCPcrypt (fork #1), DCPcrypt (fork #2). [Delphi] Suite of cryptographic components for Delphi. bcrypt. [Delphi] A library to help you hash passwords. MurMur-Delphi. [Delphi] MurMur1/2/3 fast seeded hashing algorithms port in pure-pascal. HashLib4Pascal. [Delphi] [FPC] Object Pascal hashing library released under the permissive MIT License which provides an easy to use interface for computing hashes and checksums of data. It also supports state based (incremental) hashing. CRC, Adler, Murmur, Jenkins, MD5, SHA, Blake, many more. SimpleBaseLib4Pascal. [Delphi] [FPC] Simple to use Base Encoding Package for Delphi/FreePascal Compilers that provides at the moment support for encoding and decoding various bases such as Base16, Base32 (various variants), Base58 (various variants) and Base64 (various variants) and Base85 (various variants). CryptoLib4Pascal. [Delphi] [FPC] Object Pascal cryptographic library released under the permissive MIT License. Ciphers: AES (128, 192, and 256), Rijndael, Blowfish, Speck, ChaCha, (X)Salsa20, DSA, (DET)ECDSA (supported curves: NIST, X9.62, SEC2, Brainpool), ECNR, ECSchnorr, EdDSA (Ed25519, Ed25519Blake2B) XML/JSON/YAML/HTML dataset-serialize. [Delphi] [FPC] This component is a JSON serializer for the DataSet component. Allows you to convert JSON to DataSet, DataSet to JSON, and export and load the structure of DataSet fields in JSON format. Compatible with VCL projects, FMX and uniGUI (framework). OmniXML. [Delphi] XML parser written in Delphi. Full support for Document Object Model (DOM) Level 1 specification; Supports Extensible Markup Language (XML) 1.0 (Second Edition) specification; Has built-in support for different code pages (main 8-bit code pages, UTF-8, UTF-16); Is compatible with MS XML parser; Fast parsing even large and highly structured documents; Includes helper functions to ease processing XML documents; Simplified XPath support. SAX for Pascal. [Delphi] [FPC] Designed to implement the Simple API for XML Parsing in Pascal/Delphi. // Callback-based XML parser, useful for processing huge XML streams. Abandoned since 2004 but is almost the only SAX implementation available. KDS XML. [Delphi] Class library for streamed parsing, validating and generating XML. It is written in Object Pascal/Delphi and works on Win32 (Delphi) and Linux (Kylix). Parts of it depend on the SAX for Pascal interface specifications. // Seems dead. XML Partner. [Delphi] Helps add the power of XML to Borland Delphi, C++ Builder, and Kylix projects through native, easy to use VCL and CLX components. These powerful components simplify the process of creating, modifying, and parsing XML data documents. // Seems dead, check out this page for probably newer version. Open XML. [Delphi] Provides a wide range of methods, components and foundation classes. It can be used for Win32/Kylix as well as for .NET development. SuperObject. [Delphi] [FPC] Parser/writer for JSON data format. This toolkit is designed to work with Delphi and FreePascal (win32, win64, linux32, linux64, macOS Intel). Supports reading/writing XML as well. Libxml2 for pascal. [Delphi] [FPC] Pascal units accessing the popular XML API from Daniel Veillard. This should be usable at least from Kylix and Delphi, but hopefully also from other Pascal compilers (like freepascal). NativeXml. [Delphi] This component contains a small-footprint Object Pascal (Delphi) XML implementation that allows to read and write XML documents. You basically only need one unit and you can simply add it to the \"uses\" clause. You can use this software to read XML documents from files, streams or strings. The load routine generates events that can be used to display load progress on the fly. You can also use it to create and save XML documents. Delphi-XmlLite. [Delphi] Header translation for Microsoft XmlLite. XmlLite is a native C++ implementation of .NET XmlReader+Writer for stream-based, forward-only XML parsing and creation. XmlLite.dll is required. It is included with all new versions of Windows, and service packs for old versions. XmlReader's pull-based interface is cleaner to use than SAX's event-based interface. // Seems abandoned and reported to be somewhat buggy. Chimera. [Delphi] Open Source (MIT License) library for Delphi XE2 which provides a fast and cross platform JSON generator/parser (serializer/deserializer) under a license that doesn't suck. SynCommons. [Delphi] [FPC] High speed JSON library, using TDocVariant custom variant type for storage and access. SynCrossPlatformJSON. [Delphi] [FPC] High speed cross-platform JSON library, using TJSONVariant custom variant type for storage and access. Json Data Objects. [Delphi] This Delphi unit contains a JSON parser that supports Delphi 2009-10Seattle and the platforms Win32, Win64 and ARM Android (MacOS and iOS may work). TinyJSON (mirror at GH). [Delphi] This is a small and clean library for associative arrays with Boolean / Integer / Float / WideString values. Allows import (export) from (to) JSON text. Extensive error-checking. Uses FunHash (by Sokolov Yura), HatTrie (by Daniel C. Jones), FastInt64 and FastMove (by FastCode project). JSON delphi library. [Delphi] This is a delphi library implementing JSON data format and objects structure. Lightweight and fast. dwsJSON. [Delphi] [FPC] dwsJSON is a unit that supports JSON parsing/creating, it's part of DWScript but relatively \"standalone\", in that if you add it in your Delphi (or FPC) projects, it won't pull the whole of DWScript library, and thus can be used anywhere you need. Fundamentals Code Library. JSON, XML. Alcinoe. XML/JSON Parser. delphi-yaml. [Delphi] Delphi 7 compatible bindings for libyaml, YAML parser and emitter library implemented in C. Four layers of bindings are proposed. GrijjyFoundation. JSON/BSON. VerySimpleXML. [Delphi] Lightweight, one-unit, cross-platform XML reader/writer for Delphi 2010 - 10.2.2 Tokyo XSuperObject. [Delphi] Delphi Cross Platform Rapid JSON Internet Tools. [Delphi] [FPC] Package provides standard conformant XPath 2.0, XQuery 1.0 and XPath/XQuery 3.0 interpreters with extensions for - among others - JSONiq, pattern matching, CSS and HTML; as well as functions to perform HTTP/S requests on Windows/Linux/macOS/Android, an XSLT-inspired webscraping language, and an auto update class. Delphi-JsonToDelphiClass (Newer fork. [Delphi] Generates Delphi Classes based on JSON string (Json To Delphi Class Generator / JSON Data Binding Tool). Also includes unit for interaction with GitHub. XML Parser. [Delphi] [FPC] Lightweight ObjectPascal XML parser for Delphi and FreePascal. By leaving out syntax checking, well-formedness checks and/or validation, and by choosing a progressive scanning technique, this parser is very fast. HTML parser. [Delphi] HTML parser. Supports Windows, macOS, iOS, Android platform. Comments in Chinese Neslib. [Delphi] Ultra light-weight and cross-platform XML library for Delphi. DJSON. [Delphi] Delphi JSON object mapper fast-html-parser. [Delphi] [FPC] Fast HTML Parser THTMLWriter. [Delphi] Class library that enables the developer to create HTML and HTML documents. It uses the fluent interface to make creating HTML text easy and natural. Language Tools for Pascal and other languages Next Delphi Yacc & Lex. [Delphi] Parser generator toolset for Delphi. Abstract Syntax Tree Builder. [Delphi] With DelphiAST you can take real Delphi code and get an abstract syntax tree. One unit at time and without a symbol table though. Castalia-Delphi-Parser. [Delphi] These files make up a hand-written high speed parser for the Object Pascal dialect known as \"Delphi\". The original work was done by Martin Waldenburg in the late 1990s, and the project was abandoned sometime before 2003, when I found the code and began working on it. I have kept it updated as necessary to work with my project, called \"Castalia\". CrossPascal. [Delphi] Aims to be a Delphi 7 compatible cross-platform source-to-source compiler (together with the new unicode string types from XE3 but where ansistring is still the default string type for to be still Delphi 7 compatible) which generates intermediate C code. // Quite interesting project though seems abandoned pas2js, docs. [Delphi] [FPC] An open source Pascal to JavaScript transpiler. It parses Object Pascal and emits JavaScript. The JavaScript is currently of level ECMAScript 5 and should run in any browser or in Node.js (target \"nodejs\"). Basically, Delphi 7 syntax is supported. Used in tools like TMS WebCore and Elevate Web Builder. Memory managers Libraries that implement dynamic memory allocation FastMM. [Delphi] Lightning fast replacement memory manager for Embarcadero Delphi Win32 and Win64 applications that is not prone to memory fragmentation, and supports shared memory without the use of external .DLL files. // Used as stock memory manager since 2006 but in simplified version. Provides powerful memory leak/corruption detection instruments. ScaleMM. [Delphi] Fast scaling memory manager for Delphi BrainMM. [Delphi] Extremely fast memory manager for Delphi. // Advanced memory allocation functions for faster aligned operations. FastMM4-AVX. [Delphi] [FPC] FastMM4 fork with AVX support and multi-threaded enhancements (faster locking) FastMM5. [Delphi] Fast replacement memory manager for Embarcadero Delphi applications that scales well across multiple threads and CPU cores, is not prone to memory fragmentation, and supports shared memory without the use of external .DLL files. Version 5 is a complete rewrite of FastMM. System Low-level helper stuff: memory, threading etc OmniThreadLibrary. [Delphi] Simple to use threading library for Delphi. // Easy integration of async processes in your app Delphi Detours Library. [Delphi] [FPC] Library allowing you to hook Delphi functions and object methods and Windows API functions. It provides an easy way to insert and remove hook. // Supports x64, calling original functions, multi hooks, COM/Interfaces/win32api, object methods hooking, fully thread-safe, Delphi 7/2005-2010/XE-XE7 & Lazarus/FPC, 64 bit address is supported. MemoryModule. [Delphi] [FPC] With the MemoryModule engine you can store all required DLLs inside your binary to keep it standalone. Additional hook units allow transparent using of MM engine thus allowing switching MM/WinAPI loading as well as enabling 3rd party dynamic-load DLL interfaces that are unaware of MM (tested with Interbase Express components and Firebird client library). MemoryModule is a Pascal port of Joachim Bauch's C MemoryModule. DirectoryWatcher. [Delphi] Watch changes in directories on different platforms (Windows/Linux/Mac OS). ezthreads. [FPC] simple to use threading library AsyncCalls. [Delphi] Asynchronous function call framework Template Engines to generate text output based on templates SynMustache. [Delphi] [FPC] Delphi implementation of the Mustache template language, supporting Delphi 6 up to Delphi 10 Seattle (and FPC/Lazarus compilation). Delphi Template Engine. [Delphi] Template engine designed to be used as a library in Delphi (mainly Delphi 7) applications, allowing developers to use templating on their software with no worry about implementing it. MustaPAS. [Delphi] [FPC] Mustache implementation in simple procedural Pascal. Sempare Template Engine. [Delphi] The template engine allows for flexible text manipulation. It can be used for generating email, html, source code, xml, configuration, etc. It is very easy to use, flexible and extensible, where templates are readable and maintainable. It supports: conditions, looping, custom functions and referencing data via RTTI. XE4, XE8+ DVD Chief Template Engine. [Delphi] [FPC] Fork of abandoned implementation of PHP Smarty template engine for Delphi by DVD Chief. liquid-delphi. [Delphi] Delphi port of the popular Ruby Liquid templating language and dotLiquid implementation. It is a separate project that aims to retain the same template syntax as the original, while using delphi coding conventions where possible. Logging Log4d. [Delphi] [FPC] Implementation of logging system for Delphi, based on Log4j. TraceTool. [Delphi] C#, C++, Delphi, ActiveX and Java trace framework and a trace viewer. LoggerPro. [Delphi] A modern and pluggable logging framework for Delphi. SynLog. [Delphi] [FPC] Logging functions used by Synopse projects. slf4p. [Delphi] [FPC] A simple logging facade with support for LazLogger, Log4D, and other logging frameworks. GrijjyCloudLogger. [Delphi] Remote logging tool that allows you to send log messages over the Intranet or Internet from Windows, Linux, iOS, Android and macOS devices to a viewer running on Windows. Besides sending messages along with any data, it has numerous features including custom live watches, remote live views of objects, tracking live memory usage, object allocations, growth leaks and more. QuickLogger. [Delphi] [FPC] Delphi/freepascal/.NET (Windows/Linux) library for logging on files, console, memory, email, rest, telegram, slack, eventlog, redis, ide debug messages and throw events.. Math Big Decimal Math. [Delphi] This unit provides a arbitrary precision BCD float number type. It can be used like any numeric type and supports: At least numbers between 10-2147483647 to 102147483647 with 2147483647 decimal digit precision; All standard arithmetic and comparison operators; Rounding functions (floor, ceil, to-even, ..); Some more advanced operations, e.g. power and sqrt. TIntX. [Delphi] [FPC] Pascal port of IntX arbitrary precision Integer library with fast, about O(N * log N) multiplication/division algorithms implementation. It provides all the basic arithmetic operations on Integers, comparing, bitwise shifting etc. It also allows parsing numbers in different bases and converting them to string, also in any base. The advantage of this library is its fast multiplication, division and from base/to base conversion algorithms. all the fast versions of the algorithms are based on fast multiplication of big Integers using Fast Hartley Transform which runs for O(N * log N * log log N) time instead of classic O(N^2). DelphiBigNumberXLib. [Delphi] Arbitrary Precision Library for Delphi with Support for Integer and Floating Point Computations. FastMath. [Delphi] Delphi math library that is optimized for fast performance (sometimes at the cost of not performing error checking or losing a little accuracy). It uses hand-optimized assembly code to achieve much better performance then the equivalent functions provided by the Delphi RTL. // Floating-point, vector, matrix operations. MPArith. [Delphi] Multi precision integer, rational, real, and complex arithmetic. AMath and DAMath. [Delphi] Accurate mathematical methods without using multi precision arithmetic and double precision accurate mathematical methods without using multi precision arithmetic or assembler respectively. ALGLIB. [Delphi] [FPC] Cross-platform numerical analysis and data processing library. It supports several operating systems (Windows and POSIX, including Linux). ALGLIB features include: Data analysis (classification/regression, statistics); Optimization and nonlinear solvers; Interpolation and linear/nonlinear least-squares fitting; Linear algebra (direct algorithms, EVD/SVD), direct and iterative linear solvers; Fast Fourier Transform and many other algorithms. // Free edition is Delphi wrapper around generic C core licensed for Personal and Academic Use. CAI NEURAL API. [FPC] [Delphi] Cross-platform Neural Network API optimized for AVX, AVX2 and AVX512 instruction sets plus OpenCL capable devices including AMD, Intel and NVIDIA. Command-line Libraries for parsing command-line arguments TCommandLineReader. [Delphi] [FPC] This unit provides an advanced, platform-independent command line parser for Lazarus and Delphi. It checks for allowed options, automatically prints a help with a list of all of them, and - contrary to the parser in the rtl - behaves the same on Windows and Linux. CommandLineParser. [Delphi] Simple Command Line Options Parser - spawned from the DUnitX Project. GpCommandLineParser. [Delphi] Attribute based command line parser. JPL.CmdLineParser. [Delphi] [FPC] Command-line parser for Delphi and Free Pascal Nullpobug.ArgumentParser. [Delphi] [FPC] Command-line parser for Delphi and Free Pascal Other non-visual TRegExpr. [Delphi] [FPC] Regular expressions engine in pure Object Pascal. FLRE. [Delphi] [FPC] FLRE ( F ast L ight R egular E xpressions) is a fast, safe and efficient regular expression library, which is implemented in Object Pascal (Delphi and Free Pascal) but which is even usable from other languages like C/C++ and so on. OnGuard (Alternate and maintained version for recent compiler version only). [Delphi] Library to create demo versions of your Borland Delphi & C++Builder applications. Create demo versions that are time-limited, feature-limited, limited to a certain number of uses, or limited to a certain # of concurrent network users. // Second link points to an adapted version for newest compiler versions. StringSimilarity. [Delphi] Package designed for some fuzzy and phonetic string comparison algorithms. So far implemented are the following algorithms: DamerauLevenshtein, Koelner Phonetik, SoundEx, Metaphone, DoubleMetaphone, NGram, Dice, JaroWinkler, NeedlemanWunch, SmithWatermanGotoh, MongeElkan. PubSub Chimera. [Delphi] Open Source (MIT License) library for Delphi which provides a fast and cross platform PubSub and Message Queue implementation under a license that doesn't suck. DuckDuckDelphi. [Delphi] Adds simple duck typing to Delphi Objects and provides an RTTI helper class to simplify many common RTTI tasks. byterage. [Delphi] Object pascal class library designed to remove some of the limitations of streams. The framework is very simple to use, with only one common ancestor class (TBRBuffer) which defines a set of storage agnostic mechanisms for allocating, scaling, inserting, deleting and otherwise manipulating a segment of raw binary data. stateless. [Delphi] Simple library for creating state machines in Delphi code. GenericTree. [Delphi] Delphi implementation of a generic Tree structure. Delphi Event Bus (for short DEB). [Delphi] Event Bus framework for Delphi. DelphiEventBus. [Delphi] Yet another Event Bus framework for Delphi, with annotations and rich event filtering. DHibernate. [Delphi] Object Persistent Framework based on Hibernate and NHibernate for Delphi. // Abandoned since 2012 UniConv. [Delphi] [FPC] Universal text conversion library is a universal quick and compact library intended for conversion, comparison and change of the register of text in concordance with the latest standards of the Unicode Consortium. The librarys function greatly resembles ICU, libiconv and Windows.kernel which are de facto standard for popular operating systems. CachedBuffers. [Delphi] [FPC] The library is irreplaceable for the tasks of sequential data reading or writing, especially if the requirements for the performance are increased and there are much data. CachedTexts. [Delphi] [FPC] Powerful and compact cross-platform library aimed at parsing and generating of text data with the maximum possible performance. Depends on the two other libraries: CachedBuffers and UniConv. ZEXMLSS. [Delphi] [FPC] Lazarus/Delphi component for read/write ods, excel xml, xlsx. PasMP. [Delphi] [FPC] Parallel-processing/multi-processing library for Object Pascal. ICU4PAS. [Delphi] [FPC] Object Pascal, cross platform, Direct Class Wrapper over the mature and widely used set of C/C++ ICU libraries providing Unicode support, software internationalization (i18n) and globalization (g11n), giving applications the same results on all platforms. You can use it on Windows with Delphi and FreePascal and on Linux with Kylix and FreePascal. // Hadn't been updated since 2007 but ICU interface probably remains the same GpDelphiUnits. [Delphi] Collection of useful Delphi units. Various TList descendants, TList-compatible, and TList-similar classes. Dynamically allocated, O(1) enqueue and dequeue, threadsafe, microlocking queue. Interface to 64-bit file functions with some added functionality. String hash, table and dictionary. Collection of Win32/Win64 wrappers and helper functions. Time Zone Routines. Embedded file system. BaseNcodingPascal. [Delphi] [FPC] Library for encoding of binary data into strings using base32, base85, base128 and other algorithms for FPC and Delphi. ByteSizeLibPascal. [Delphi] [FPC] TByteSize is a utility \"record\" that makes byte size representation in code easier by removing ambiguity of the value being represented. EmailValidationPascal. [Delphi] [FPC] Simple Class for Validating Email Address Syntax in Pascal/Delphi. PRNG. [Delphi] Seven fast pseudo random number generators with period lengths much greater than Pascal's random function. All are implemented with context records, therefore several independent generators can be used simultaneously, they are not cryptographically secure. In addition there are three cryptographic generators. CSV File and String Reader. [Delphi] TnvvCSVFileReader and TnvvCSVStringReader are light weighted and fast classes that resemble unidirectional data set. HTMLBuilder. [Delphi] Build simplified html with pascal code. Marvin.IA. [Delphi] Machine learning collection of object-oriented Pascal primitives (only interfaces and classes). FreePascal Generics.Collections. [FPC] FreePascal Generics.Collections library (TList, TDictionary, THashMap and more) FuzzyWuzzy.pas. [FPC] Port of the well-known Python fuzzy string matching package that uses the Levenshtein distance to compute differences between string sequences. GS.Core. [Delphi] [FPC] Core functions shared by several projects. // Thread Pool, file operations, Key<>Value database, JSON lib, etc PascalTZ. [FPC] Pascal Time Zone allows you to convert between local times in various time zones and GMT/UTC, taking into account historical changes to time zone rules. Charset Enigma. [Delphi] Delphi charset detector Community Edition DelphiPatterns. [Delphi] Complete set of design patterns implemented in Delphi language Markdown Processor for Pascal. [Delphi] [FPC] This is a Pascal (Delphi) library that processes to markdown to HTML Coroutine-based multithreading library. [Delphi] AIO implement procedural oriented programming (POP) style in Delphi. It means developer can combine advantages of OOP and POP, splitting logic to multiple state machines, schedule them to threads, connect them by communication channels like in GoLang Rapid.Generics. [Delphi] Rapid generics/defaults equivalent classes for Delphi (XE8+) Keras4Delphi. [Delphi] High-level neural networks API, written in Pascal with Python Binding TZDB. [Delphi] [FPC] IANA Time Zone Database for Delphi/FreePascal PascalUtils. [Delphi] [FPC] Delphi and object pascal library of utils data structures libPasC-Algorithms. [Delphi] [FPC] Delphi and object pascal library of common data structures and algorithms. Library rewritten from c-algorithms repository and other sources. VSoft.Messaging. [Delphi] Libary that provides an internal synchronous/asynchronous publish/subscribe messaging system for Delphi applications. Delphi-Hunspell. [Delphi] Simple Hunspell spell checking engine wrapper for Delphi. OS Tools that help dealing with OS-specific internals GLibWMI. [Delphi] Component Library for Delphi that encapsulate the classes for access to WMI of Windows in a set of VCL. BiosInfo, PrinterInfo, DiskInfo, etc. Allow access WMI Classes: WIN32_Bios, WIN32_Printers, WIN32_DiskDrive. MemoryMap. [Delphi] Set of classes to get all the info about a memory of a running process. The Drag and Drop Component Suite. [Delphi] VCL component library that enables your Delphi and C++Builder applications to support COM based drag and drop and integrate with the Windows clipboard. TSMBIOS. [Delphi] [FPC] Allows access the System Management BIOS (SMBIOS) using the Object Pascal language (Delphi or Free Pascal). The SMBIOS (System Management BIOS) is a standard developed by the DMTF. The information stored in the SMBIOS includes devices manufacturer, model name, serial number, BIOS version, asset tag, processors, ports and device memory installed. VersionInfo for Delphi. [Delphi] The library makes it very easy to read values from the Version Info resource of Windows executables and DLLs. Optionally extends the TApplication class with a version info property via class helper. Magenta Systems WMI and SMART Component. [Delphi] Contains WMI, SMART and SCSI PassThrough functions, of particular use for getting hard disk information and configuring network adaptors, but also for many other general uses. MagWMI provides general view access to any WMI information using SQL like commands, and also a number of dedicated function relating to TCP/IP configuration, such as setting the adaptor IP addresses, the computer name, domain/workgroup, BIOS and disk drive information. madKernel. [Delphi] The package is about Kernel Objects for the biggest part. The most important object types are wrapped up in interfaces, utilizing all the specific kernel32 APIs. Has interface wrappers for: Events, Mutexes, Threads, Processes, Windows, Modules, Tray Icons, shared memory buffers. // Free with source for non-commercial usage (only) with some conditions. Available to download as part of madCollection installer. Pretty well documented. Requires madBasic package. madSecurity. [Delphi] The package makes it easily possible to handle Shares and other Security Objects like file security or registry security. To be able to do so, this package also features functionality around Accounts and ACEs and ACLs. // Free with source for non-commercial usage (only) with some conditions. Available to download as part of madCollection installer. Pretty well documented. Requires madBasic package. madShell. [Delphi] The package implements often needed shell functionality, beginning with Special Folders like the \"Windows\" folder or the \"Program Files\" folder, continuing with Shell ID Lists, Shell Objects and Shell Events. Then you'll find functionality around ShortCuts/ShellLinks and finally everything about Display Modes. // Free with source for non-commercial usage (only) with some conditions. Available to download as part of madCollection installer. Pretty well documented. Requires madBasic package. WindowsAutorun. [Delphi] Helps you manage autoload in Windows OS. ActiveDirectory4Delphi. [Delphi] Delphi basic library for validation and authentication of LDAP users in Active Directory. Report generating Report Manager. [Delphi] Report manager is a reporting application (Report Manager Designer) and a set of libraries and utilities to preview, export or print reports. Include native .Net and Delphi/C++Builder libraries, ActiveX component and also standard dynamic link library for use in any language like GNU C. FortesReport. [Delphi] The FortesReport is a powerful report generator available as a package of components for Delphi. mORMotReport (docs). [Delphi] Fast and efficient code-based reporting component, with preview form and PDF export. Unit Testing DUnitX. [Delphi] New test framework, taking ideas from DUnit, NUnit and other test frameworks. It is designed to work with Delphi 2010 or later, it makes use of language/RTL features that are not available in older versions of Delphi. DUnit. [Delphi] Unit Testing Framework, that has been the standard testing framework for years, the Delphi IDE now ships with this library. // Included since XE, deprecated since XE8 in favor of DUnitX; seems abandoned. DUnit2. [Delphi] Fork of the DUnit Project that adds several new features. // Seems abandoned, lacks some features from last DUnit version. DelphiSpec. [Delphi] Library for running automated tests written in plain language. Because they're written in plain language, they can be read by anyone on your team. Because they can be read by anyone, you can use them to help improve communication, collaboration and trust on your team. Delphi-Mocks. [Delphi] Simple mocking framework for Delphi XE2 or later. Allow you to mock both classes and interfaces for testing. DUnit-XML. [Delphi] Test runner that allows DUnit Tests to output NUnit compatible XML. Smoketest. [Delphi] Framework for writing tests and performance benchmarks using the Delphi language for Microsoft Windows. It has been tested on all versions of Delphi from 7 thru to 2010. SynTests. [Delphi] [FPC] Unit test functions including mocks and stubs. OpenCTF. [Delphi] Test framework add-on for Embarcadero Delphi which performs automatic checks of all components in Forms (or DataModules). It provides an easy way to build automatic quality checks for large projects where many components have to pass repeated tests. OpenCTF is based on the DUnit open source test framework and extends it by specialized test classes and helper functions. DelphiUIAutomation. [Delphi] Delphi classes that wrap the MS UIAutomation library. DelphiUIAutomation is a framework for automating rich client applications based on Win32 (and specifically tested with Delphi XE5). It is written in Delphi XE5 and it requires no use of scripting languages. It provides a consistent object-oriented API, hiding the complexity of Microsoft's UIAutomation library and windows messages. Debugging / error handling Delphi LeakCheck. [Delphi] Free code library to check the memory leaks in the DUnit and DUnit2 tests. Supports Delphi XE-XE7. FastMM. Provides powerful memory leak/corruption detection instruments. JclDebug (part of Project JEDI). [Delphi] [FPC] Tracing, MAP file parser, exception report generation, exception stack traces. DebugEngine. [Delphi] Collection of utilities related to debug stuff (stack trace, CPU registers snapshot, debug info, etc). Accessing Delphi debug info, Getting address of symbol from its name, Delphi map parsing and map converter to binary format, Smart stack trace, Delphi exception stack trace hook, etc. ObjectDebugger. [Delphi] Run-time Object Inspector for Delphi VCL applications. Utilities Free non-opensource products allowed here. RAD Studio IDE plugins/wizards Delphi IDE theme editor / Delphi IDE Colorizer. Tool to change the IDE color highlighting of several Object Pascal IDE's like Delphi (RAD Studio), Appmethod, Lazarus and Smart Mobile Studio. DITE supports Delphi 5-7, 2005-2010, XE-XE8, Appmethod 1.13-1.14, Lazarus v1.0.1.3 and Smart Mobile Studio IDE v1.1.2.17. The Delphi IDE Colorizer (DIC) is a plugin which allows to customize the look and feel of the workspace of the RAD Studio IDE and Appmethod. DDevExtensions. Extends the Delphi/C++Builder IDE by adding some new productivity features. // Many useful IDE tweaks, must have. VCL Fix Pack. Delphi unit that fixes VCL and RTL bugs at runtime by patching the original functions. If you want all IDE Fix Pack fixes in your application this unit is what you are looking for. Adding the unit to your project (Delphi and C++Builder) automatically installs the patches that are available for your Delphi/C++Builder version. // Actual for Delphi/C++ 6..2009 IDE Fix Pack. Collection of unofficial bug fixes and performance optimizations for the RAD Studio IDE, Win32/Win64 compiler and Win32 debugger. IDE Fix Pack is an IDE plugin for RAD Studio 2009-XE6 that fixes IDE bugs at runtime. All changes are done in memory. No files on disk are modified. None of your projects are modified or benefit from the IDE Fix Pack other than being compiled faster. Only the IDE gets the fixes and optimizations. // Supports all RAD Studio versions since 2007. Removes lots of annoying bugs that EMBT haven't fixed for years. Yay! GExperts. Free set of tools built to increase the productivity of Delphi and C++Builder programmers by adding several features to the IDE. GExperts is developed as Open Source software and we encourage user contributions to the project. Grep search and replace supporting unicode files, DFMs, etc; Automatically rename components, insert text macros, open recent files; Easily backup your projects, with custom additional file lists; Keep nested lists of favorite files for quick access; Track dependencies between units in your project; Quickly jump to any procedure in the current unit; And much, much more. CnWizards. Free Plug-in Tool Set for Delphi/C++ Builder/CodeGear RAD Studio to Improve Development Efficiency. Delphi Package Installer (DelphiPI). Tool which aids you installing components to your Delphi IDE. DelphiPI automatically resolves dependencies between packages, compiles, installs and adds source paths to your IDE. ResEd. Expert for Delphi 2005, 2006, 2007, 2009, 2010 and XE. This expert is designed for editing the resource files (.res; .resx) that are linked to the active project. It will automatically search for all occurrences of {$R xyz.res} lines and will open/create resourcefiles for them. The expert registers itself in the menubar of Delphi under View. Parnassus Bookmarks. IDE plugin that extends bookmark functionality. DelphiSettingManager. Multiple IDE profiles for Delphi (up to XE6). Allows to install multiple versions of the same component or different component sets for different projects. Delphinus. New Packagemanager which runs on Delphi XE and newer and uses GitHub as a Backend to Provide the packages. TestInsight. Unit testing IDE Plugin for Delphi. It supports all versions from XE to 10 Seattle. Supports DUnit, DUnit2, DUnitX frameworks. Delphi IDE Explorer. Wizard / expert / plugin that allows you to browser the internal fields, methods, properties and events of the IDE. // Mainly useful for developers of IDE experts Multi-RAD Studio IDE Expert Manager. Application is for editing the installed experts in all versions of RAD Studio (and older Delphi and C++ Builder) on a machine. OTA Interface Search. Application helps to find Open Tools API (OTA) interfaces, methods and properties and understand how to get to those interfaces or methods / properties of the interfaces. AutoSave. Expert that periodically auto saves all the open modified IDE files. Browse and Doc It. Plug-in allows you to document and browse your code from within the IDE. Integrated Testing Helper. Plugin for Delphi and RAD Studio that allows you to run command-line application before and after the compilation of you projects. It also provides the ability to zip you projects files into an archive on each compile/build and manage the application's version information. Project Magician. Wizard for advanced project options manipulation. Selective Debugging. Wizard that allows to tune for which units their debug version will be used. MMX Code Explorer. Feature-rich productivity enhancing plugin. Includes refactoring, class browser, advanced editing, metrict and many more. FormResource. Wizard that helps storing various data as form resources. Delphi Library Helper Tool to assist Delphi developers configuring library folders. Mobile Image Creator Creating Icons and Launcher Images for Delphi Mobile Applications (Firemonkey). This is a fork of Mobile Gfx created by Thomas Grubb of RiverSoftAVG. Delphi-Adb-WiFi. Plugin for RAD Studio, which allows launching and debugging on an Android device without connecting to a computer via USB. Works over WiFi. RADSplit. Dockable Split-Screen Editors for RAD Studio (Delphi and C++ Builder). DzNoteEditor. Delphi Property Editor for TStrings supporting formatted languages with syntax highlight. Plugins for other IDE's Delphi IDE theme editor / Delphi IDE Colorizer. Supports Appmethod, Lazarus and Smart Mobile Studio. Pascal and Pascal Formatter. Open source extensions created for Visual Studio Code that add Pascal support. Documentation SynProject (docs). Tool for code source versioning and automated documentation of Delphi projects. PasDoc. [Delphi] [FPC] Documentation tool for ObjectPascal (FreePascal and Delphi) source code. Documentation is generated from comments found in source code. Available output formats are HTML, HtmlHelp, LaTeX, latex2rtf, simplexml. More output formats may be added in the future. Code check/review, debug GpProfiler2017. [Delphi] Source code instrumenting profiler for Delphi XE and higher. Other forks support older versions. SamplingProfiler. [Delphi] Performance profiling tool for Delphi 5 to 32bits Delphi XE4. Its purpose is to help locate bottlenecks, even in final, optimized code running at full-speed. Delphi Code Coverage. [Delphi] Simple Code Coverage tool for Delphi that creates code coverage reports based on detailed MAP files. Pascal Analyzer (free Lite version available). [Delphi] Pascal Analyzer, or PAL for short, parses Delphi or Borland Pascal source code. It builds large internal tables of identifiers, and collects other information such as calls between subprograms. When the parsing is completed, extensive reports are produced. These reports contain a great deal of important information about the source code. This information will help you understand your source code better, and assist you in producing code of higher quality and reliability. madExcept. [Delphi] madExcept was built to help you locate bugs in your software. Whenever there's a crash/exception in your program, madExcept will automatically catch it, analyze it, collect lots of useful information, and give the end user the possibility to send you a full bug report. madExcept is also able to find memory leaks, resource leaks and buffer overruns for you. // Free without source for non-commercial usage (only) with some conditions. Available to download as part of madCollection installer (you'll need to install madExcept item). Pretty well documented. delphiunitsizes. [Delphi] Tool to display the sizes of each unit in a Delphi executable. Shows the size of each unit that is included in a Delphi exe-file. It also shows an approximate size of each symbol (classes, methods, procedures etc) in a unit. MapFileStats. [Delphi] Tool that provides simple binary size statistics from .MAP files (any Delphi version up to at least Delphi XE5). Spider. [Delphi] Real time profiler for Delphi applications AsmProfiler. [Delphi] Full tracing 32bit profiler (instrumenting and sampling), written in Delphi and some assembly map2pdb. [Delphi] Tool used to convert the MAP files produced by the Delphi and C++ Builder compilers to Microsoft PDB files for use in tools that support that format. Setup Lazy Delphi Builder. Build tool for Delphi. Recompile projects/packages from sources with all dependencies, without need to mess around with configs. Quickly (re-)install components from sources into IDE, with no need to change your Library Path. // Powerful automating tool. Freeware but not open source Inno Setup. Free installer for Windows programs. First introduced in 1997, Inno Setup today rivals and even surpasses many commercial installers in feature set and stability. WinSparkle and its Delphi wrapper. WinSparkle is an easy-to-use software update library for Windows developers. WinSparkle is a heavily (to the point of being its almost-port) inspired by the Sparkle framework originally by Andy Matuschak that became the de facto standard for software updates on macOS. Silverpoint MultiInstaller. Multi component package installer for Embarcadero Delphi and C++Builder, it was created to ease the components installation on the IDE. Grijjy Deployment Manager. Tool to simplify the deployment of files and folders for iOS and Android apps written in Delphi. It is especially useful if you need to deploy a lot of files, such as 3rd party SDKs. Other WMI Delphi Code Creator. Allows you to generate Object Pascal, Oxygene, C++ and C# code to access the WMI (Windows Management Instrumentation) classes, events and methods. Also includes a set of tools to explorer and Query the content of the WMI. Delphi Preview Handler. Preview handler for Windows Vista, 7 and 8 which allow you read your object pascal, C++ and Assembly code with Syntax highlighting without open in a editor Delphi Dev. Shell Tools. Windows shell extension with useful tasks for Object Pascal Developers (Delphi, Free Pascal). Delphi.gitignore. .gitignore templates for Delphi. There is also one for Lazarus. OmniPascal. Project that enables Delphi and Free Pascal developers to write and maintain code using the modern editor Visual Studio Code. Delphi Unit Tests. Set of unit tests for Delphi's libraries. Delphi community members are encouraged to fork the repository, add tests, and create a pull request. Embarcadero employees are particularly encouraged to add tests from the internal tests that are run with official Delphi builds. madDisAsm. The package features a full x86 disassembler including MMX, 3dNow enhanced, SSE and SSE2 support. The disassembler can examine a single x86 instruction (see ParseCode) or a full function (see ParseFunction) and either return a short analysis or a full text disassembly. Register contents are watched/followed if possible, this improves the analyses for jump/call targets. Case/switch jump tables are automatically detected and handled correctly. // Free without source for non-commercial usage (only) with some conditions. Available to download as part of madCollection installer (you'll need to install madExcept item). Pretty well documented. Chet - C Header Translator for Delphi. Chet is a .h-to-.pas translator powered by libclang for Delphi. Uses the Clang compiler to parse header files, resulting in more accurate translations that require fewer manual adjustments. Boss. Dependency Manager for Delphi projects. C-To-Delphi. [Delphi] This tool will convert most of your standard C code. Better Translation Manager. [Delphi] Translation Manager dzBdsLauncher. [Delphi] Launcher for the Delphi IDE that decides which of multiple IDEs to launch based on the suffix of the dproj file passed to it. DFMJSON. [Delphi] Library to convert between Delphi's .DFM (or .FMX) format and JSON. It can be used to parse a DFM file into an Abstract Syntax Tree in JSON, which can then be edited and the results turned back to DFM format. ",
        "_version_": 1718536481922875392
      },
      {
        "story_id": 18813941,
        "story_author": "yankcrime",
        "story_descendants": 16,
        "story_score": 96,
        "story_time": "2019-01-03T09:44:48Z",
        "story_title": "The Firecracker virtual machine monitor",
        "search": [
          "The Firecracker virtual machine monitor",
          "https://lwn.net/SubscriberLink/775736/d9d6c371b9dbfc5f/",
          "This article brought to you by LWN subscribersSubscribers to LWN.net made this article and everything that surrounds it possible. If you appreciate our content, please buy a subscription and make the next set of articles possible. Cloud computing services that run customer code in short-lived processes are often called \"serverless\". But under the hood, virtual machines (VMs) are usually launched to run that isolated code on demand. The boot times for these VMs can be slow. This is the cause of noticeable start-up latency in a serverless platform like Amazon Web Services (AWS) Lambda. To address the start-up latency, AWS developed Firecracker, a lightweight virtual machine monitor (VMM), which it recently released as open-source software. Firecracker emulates a minimal device model to launch Linux guest VMs more quickly. It's an interesting exploration of improving security and hardware utilization by using a minimal VMM built with almost no legacy emulation. A pared-down VMM Firecracker began as a fork of Google's crosvm from ChromeOS. It runs Linux guest VMs using the Kernel-based Virtual Machine (KVM) API and emulates a minimal set of devices. Currently, it supports only Intel processors, but AMD and Arm are planned to follow. In contrast to the QEMU code base of well over one million lines of C, which supports much more than just qemu-kvm, Firecracker is around 50 thousand lines of Rust. The small size lets Firecracker meet its specifications for minimal overhead and fast startup times. Serverless workloads can be significantly delayed by slow cold boots, so integration tests are used to enforce the specifications. The VMM process starts up in around 12ms on AWS EC2 I3.metal instances. Though this time varies, it stays under 60ms. Once the guest VM is configured, it takes a further 125ms to launch the init process in the guest. Firecracker spawns a thread for each VM vCPU to use via the KVM API along with a separate management thread. The memory overhead of each thread (excluding guest memory) is less than 5MB. Performance aside, paring down the VMM emulation reduces the attack surface exposed to untrusted guest virtual machines. Though there were notable VM emulation bugs [PDF] before it, qemu-kvm has demonstrated this risk well. Nelson Elhage published a qemu-kvm guest-to-host breakout [PDF] in 2011. It exploited a quirk in the PCI device hotplugging emulation, which would always act on unplug requests for guest hardware devices even if the device didn't support being unplugged. Back then, Elhage correctly expected more vulnerabilities to come in the KVM user-space emulation. There have been other exploits since then, but perhaps the clearest example of the risk from obsolete device emulation is the vulnerability Jason Geffner discovered in the QEMU virtual floppy disk controller in 2015. Running Firecracker Freed from the need to support lots of legacy devices, Firecracker ships as a single static binary linked against the musl C library. Each run of Firecracker is a one-shot launch of a single VM. Firecracker VMs aren't rebooted. The VM either shuts down or ends when its Firecracker process is killed. Re-launching a VM is as simple as killing the Firecracker process and running Firecracker again. Multiple VMs are launched by running separate instances of Firecracker, each running one VM. Firecracker can be run without arguments. The VM is configured after Firecracker starts via a RESTful API over a Unix socket. The guest kernel, its boot arguments, and the root filesystem are configured over this API. The root filesystem is a raw disk image. Multiple disks can be attached to the VM, but only before the VM is started. These curl commands from the Getting Started guide configure a VM with the provided demo kernel and an Alpine Linux root filesystem image: curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/boot-source' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"kernel_image_path\": \"./hello-vmlinux.bin\", \"boot_args\": \"console=ttyS0 reboot=k panic=1 pci=off\" }' curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/drives/rootfs' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"drive_id\": \"rootfs\", \"path_on_host\": \"./hello-rootfs.ext4\", \"is_root_device\": true, \"is_read_only\": false }' The configured VM can then be started with a final call: curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/actions' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{\"action_type\": \"InstanceStart\"}' Each such Firecracker process runs a single KVM instance (a \"microVM\" in the documentation). The serial console of the guest VM is mapped to Firecracker's standard input/output. Networking can be configured for the guest via a TAP interface on the host. As an example for host-only networking, create a TAP interface on the host with: # ip tuntap add dev tap0 mode tap # ip addr add 172.17.0.1/16 dev tap0 # ip link set tap0 up Then configure the VM to use the TAP interface before starting the VM: curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/network-interfaces/eth0' \\ -H 'Content-Type: application/json' \\ -d '{ \"iface_id\": \"eth0\", \"host_dev_name\": \"tap0\", \"state\": \"Attached\" }' Finally, start the VM and configure networking inside the guest as needed: # ip addr add 172.17.100.1/16 dev eth0 # ip link set eth0 up # ping -c 3 172.17.0.1 Emulation for networking and block devices uses Virtio. The only other emulated device is an i8042 PS/2 keyboard controller supporting a single key for the guest to request a reset. No BIOS is emulated as the VMM boots a Linux kernel directly, loading the kernel into guest VM memory and starting the vCPU at the kernel's entry point. The Firecracker demo runs 4000 such microVMs on an AWS EC2 I3.metal instance with 72 vCPUs (on 36 physical cores) and 512 GB of memory. As shown by the demo, Firecracker will gladly oversubscribe host CPU and memory to maximize hardware utilization. Once a microVM has started, the API supports almost no actions. Unlike a more general purpose VMM, there's intentionally no support for live migration or VM snapshots since serverless workloads are short lived. The main supported action is triggering a block device rescan. This is useful since Firecracker doesn't support hotplugging disks; they need to be attached before the VM starts. If the disk contents are not known at boot, a secondary empty disk can be attached. Later the disk can be resized and filled on the host. A block device rescan will then let the Linux guest pick up the changes to the disk. The Firecracker VMM can rate-limit its guest VM I/O to contain misbehaving workloads. This limits bytes per second and I/O operations per second on the disk and over the network. Firecracker doesn't enforce this as a static maximum I/O rate per second. Instead, token buckets are used to bound usage. This lets guest VMs do I/O as fast as needed until the token bucket for bytes or operations empties. The buckets are continuously replenished at a fixed rate. The bucket size and replenishment rate are configurable depending on how large of bursts should be allowed in guest VM usage. This particular token bucket implementation also allows for a large initial burst on startup. Cloud computing services typically provide a metadata HTTP service reachable from inside the VM. Often it's available at the well-known non-routable IP address 169.254.169.254, like it is for AWS, Google Cloud, and Azure. The metadata HTTP service offers details specific to the cloud provider and the service on which the code is run. Typical examples are the host networking configuration and temporary credentials the virtualized code can use. The Firecracker VMM supports emulating a metadata HTTP service for its guest VM. The VMM handles traffic to the metadata IP itself, rather than via the TAP interface. This is supported by a small user-mode TCP stack and a tiny HTTP server built into Firecracker. The metadata available is entirely configurable. Deploying Firecracker in production In 2007 Tavis Ormandy studied [PDF] the security exposure of hosts running hostile virtualized code. He recommended treating VMMs as services that could be compromised. Firecracker's guide for safe production deployment shows what that looks like a decade later. Being written in Rust mitigates some risk to the Firecracker VMM process from malicious guests. But Firecracker also ships with a separate jailer used to reduce the blast radius of a compromised VMM process. The jailer isolates the VMM in a chroot, in its own namespaces, and imposes a tight seccomp filter. The filter whitelists system calls by number and optionally limits system-call arguments, such as limiting ioctl() commands to the necessary KVM calls. Control groups version1 are used to prevent PID exhaustion and to prevent workloads sharing CPU cores and NUMA nodes to reduce the likelihood of exploitable side channels. The recommendations include a list of host security configurations. These are meant to mitigate side channels enabled by CPU features, host kernel features, and recent hardware vulnerabilities. Possible future as a container runtime Originally, Firecracker was intended to be a faster way to run serverless workloads while keeping the isolation of VMs, but there are other possible uses for it. An actively developed prototype in Go uses Firecracker as a container runtime. The goal is a drop-in containerd replacement with the needed interfaces to meet the Open Container Initiative (OCI) and Container Network Interface (CNI) standards. Though there are already containerd shims like Kata Containers that can run containers in VMs, Firecracker's unusually pared-down design is hoped to be more lightweight and trustworthy. Currently, each container runs in a single VM, but the project plans to batch multiple containers into single VMs as well. Commands to manage containers get sent from front-end tools (like ctr). In the prototype's architecture, the runtime passes the commands to an agent inside the Firecracker guest VM using Firecracker's experimental vsock support. Inside the guest VM, the agent in turn proxies the commands to runc to spawn containers. The prototype also implements a snapshotter for creating and restoring container images as disk images. The initial goal of Firecracker was a faster serverless platform running code isolated in VMs. But Firecracker's use as a container runtime might prove its design more versatile than that. As an open-source project, it's a useful public exploration of what minimal application-specific KVM implementations can achieve when built without the need for legacy emulation. Index entries for this article GuestArticlesDesai, Azhar (Log in to post comments) "
        ],
        "story_type": "Normal",
        "url_raw": "https://lwn.net/SubscriberLink/775736/d9d6c371b9dbfc5f/",
        "comments.comment_id": [18820610, 18821151],
        "comments.comment_author": ["andrewstuart", "mgerdts"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-01-03T23:53:51Z",
          "2019-01-04T01:16:33Z"
        ],
        "comments.comment_text": [
          "The weird thing about Firecracker is that it doesn't run on Amazon EC2 shared host instances - but theoretically it will run on Digital Ocean shared instances, Azure shared instances and Google shared instances.<p>For the most part, Amazon EC2 is excluded from firecracker, unless you pay at the top end for bare metal EC2 instances.<p>Odd.",
          "In addition to the firecracker-containerd[0] architecture, there's also firecracker support in kata containers[1].  While firecracker aims to be as lean as possible[2], kata aims to be portable[3].  It would be swell if the bits that run inside the VM and the protocol that goes over the vsock or vserial proxy were interoperable, but I see that as rather unlikely to happen soon.<p>0. <a href=\"https://github.com/firecracker-microvm/firecracker-containerd/blob/master/docs/architecture.md\" rel=\"nofollow\">https://github.com/firecracker-microvm/firecracker-container...</a><p>1. <a href=\"https://github.com/kata-containers/documentation/wiki/Initial-release-of-Kata-Containers-with-Firecracker-support\" rel=\"nofollow\">https://github.com/kata-containers/documentation/wiki/Initia...</a><p>2. <a href=\"https://firecracker-microvm.github.io/\" rel=\"nofollow\">https://firecracker-microvm.github.io/</a><p>3. <a href=\"https://github.com/kata-containers/documentation/blob/master/design/kata-design-requirements.md#multiple-hardware-architectures-support\" rel=\"nofollow\">https://github.com/kata-containers/documentation/blob/master...</a>"
        ],
        "id": "4ca93655-2808-433c-9c9b-3ce0815058b0",
        "url_text": "This article brought to you by LWN subscribersSubscribers to LWN.net made this article and everything that surrounds it possible. If you appreciate our content, please buy a subscription and make the next set of articles possible. Cloud computing services that run customer code in short-lived processes are often called \"serverless\". But under the hood, virtual machines (VMs) are usually launched to run that isolated code on demand. The boot times for these VMs can be slow. This is the cause of noticeable start-up latency in a serverless platform like Amazon Web Services (AWS) Lambda. To address the start-up latency, AWS developed Firecracker, a lightweight virtual machine monitor (VMM), which it recently released as open-source software. Firecracker emulates a minimal device model to launch Linux guest VMs more quickly. It's an interesting exploration of improving security and hardware utilization by using a minimal VMM built with almost no legacy emulation. A pared-down VMM Firecracker began as a fork of Google's crosvm from ChromeOS. It runs Linux guest VMs using the Kernel-based Virtual Machine (KVM) API and emulates a minimal set of devices. Currently, it supports only Intel processors, but AMD and Arm are planned to follow. In contrast to the QEMU code base of well over one million lines of C, which supports much more than just qemu-kvm, Firecracker is around 50 thousand lines of Rust. The small size lets Firecracker meet its specifications for minimal overhead and fast startup times. Serverless workloads can be significantly delayed by slow cold boots, so integration tests are used to enforce the specifications. The VMM process starts up in around 12ms on AWS EC2 I3.metal instances. Though this time varies, it stays under 60ms. Once the guest VM is configured, it takes a further 125ms to launch the init process in the guest. Firecracker spawns a thread for each VM vCPU to use via the KVM API along with a separate management thread. The memory overhead of each thread (excluding guest memory) is less than 5MB. Performance aside, paring down the VMM emulation reduces the attack surface exposed to untrusted guest virtual machines. Though there were notable VM emulation bugs [PDF] before it, qemu-kvm has demonstrated this risk well. Nelson Elhage published a qemu-kvm guest-to-host breakout [PDF] in 2011. It exploited a quirk in the PCI device hotplugging emulation, which would always act on unplug requests for guest hardware devices even if the device didn't support being unplugged. Back then, Elhage correctly expected more vulnerabilities to come in the KVM user-space emulation. There have been other exploits since then, but perhaps the clearest example of the risk from obsolete device emulation is the vulnerability Jason Geffner discovered in the QEMU virtual floppy disk controller in 2015. Running Firecracker Freed from the need to support lots of legacy devices, Firecracker ships as a single static binary linked against the musl C library. Each run of Firecracker is a one-shot launch of a single VM. Firecracker VMs aren't rebooted. The VM either shuts down or ends when its Firecracker process is killed. Re-launching a VM is as simple as killing the Firecracker process and running Firecracker again. Multiple VMs are launched by running separate instances of Firecracker, each running one VM. Firecracker can be run without arguments. The VM is configured after Firecracker starts via a RESTful API over a Unix socket. The guest kernel, its boot arguments, and the root filesystem are configured over this API. The root filesystem is a raw disk image. Multiple disks can be attached to the VM, but only before the VM is started. These curl commands from the Getting Started guide configure a VM with the provided demo kernel and an Alpine Linux root filesystem image: curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/boot-source' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"kernel_image_path\": \"./hello-vmlinux.bin\", \"boot_args\": \"console=ttyS0 reboot=k panic=1 pci=off\" }' curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/drives/rootfs' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"drive_id\": \"rootfs\", \"path_on_host\": \"./hello-rootfs.ext4\", \"is_root_device\": true, \"is_read_only\": false }' The configured VM can then be started with a final call: curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/actions' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{\"action_type\": \"InstanceStart\"}' Each such Firecracker process runs a single KVM instance (a \"microVM\" in the documentation). The serial console of the guest VM is mapped to Firecracker's standard input/output. Networking can be configured for the guest via a TAP interface on the host. As an example for host-only networking, create a TAP interface on the host with: # ip tuntap add dev tap0 mode tap # ip addr add 172.17.0.1/16 dev tap0 # ip link set tap0 up Then configure the VM to use the TAP interface before starting the VM: curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/network-interfaces/eth0' \\ -H 'Content-Type: application/json' \\ -d '{ \"iface_id\": \"eth0\", \"host_dev_name\": \"tap0\", \"state\": \"Attached\" }' Finally, start the VM and configure networking inside the guest as needed: # ip addr add 172.17.100.1/16 dev eth0 # ip link set eth0 up # ping -c 3 172.17.0.1 Emulation for networking and block devices uses Virtio. The only other emulated device is an i8042 PS/2 keyboard controller supporting a single key for the guest to request a reset. No BIOS is emulated as the VMM boots a Linux kernel directly, loading the kernel into guest VM memory and starting the vCPU at the kernel's entry point. The Firecracker demo runs 4000 such microVMs on an AWS EC2 I3.metal instance with 72 vCPUs (on 36 physical cores) and 512 GB of memory. As shown by the demo, Firecracker will gladly oversubscribe host CPU and memory to maximize hardware utilization. Once a microVM has started, the API supports almost no actions. Unlike a more general purpose VMM, there's intentionally no support for live migration or VM snapshots since serverless workloads are short lived. The main supported action is triggering a block device rescan. This is useful since Firecracker doesn't support hotplugging disks; they need to be attached before the VM starts. If the disk contents are not known at boot, a secondary empty disk can be attached. Later the disk can be resized and filled on the host. A block device rescan will then let the Linux guest pick up the changes to the disk. The Firecracker VMM can rate-limit its guest VM I/O to contain misbehaving workloads. This limits bytes per second and I/O operations per second on the disk and over the network. Firecracker doesn't enforce this as a static maximum I/O rate per second. Instead, token buckets are used to bound usage. This lets guest VMs do I/O as fast as needed until the token bucket for bytes or operations empties. The buckets are continuously replenished at a fixed rate. The bucket size and replenishment rate are configurable depending on how large of bursts should be allowed in guest VM usage. This particular token bucket implementation also allows for a large initial burst on startup. Cloud computing services typically provide a metadata HTTP service reachable from inside the VM. Often it's available at the well-known non-routable IP address 169.254.169.254, like it is for AWS, Google Cloud, and Azure. The metadata HTTP service offers details specific to the cloud provider and the service on which the code is run. Typical examples are the host networking configuration and temporary credentials the virtualized code can use. The Firecracker VMM supports emulating a metadata HTTP service for its guest VM. The VMM handles traffic to the metadata IP itself, rather than via the TAP interface. This is supported by a small user-mode TCP stack and a tiny HTTP server built into Firecracker. The metadata available is entirely configurable. Deploying Firecracker in production In 2007 Tavis Ormandy studied [PDF] the security exposure of hosts running hostile virtualized code. He recommended treating VMMs as services that could be compromised. Firecracker's guide for safe production deployment shows what that looks like a decade later. Being written in Rust mitigates some risk to the Firecracker VMM process from malicious guests. But Firecracker also ships with a separate jailer used to reduce the blast radius of a compromised VMM process. The jailer isolates the VMM in a chroot, in its own namespaces, and imposes a tight seccomp filter. The filter whitelists system calls by number and optionally limits system-call arguments, such as limiting ioctl() commands to the necessary KVM calls. Control groups version1 are used to prevent PID exhaustion and to prevent workloads sharing CPU cores and NUMA nodes to reduce the likelihood of exploitable side channels. The recommendations include a list of host security configurations. These are meant to mitigate side channels enabled by CPU features, host kernel features, and recent hardware vulnerabilities. Possible future as a container runtime Originally, Firecracker was intended to be a faster way to run serverless workloads while keeping the isolation of VMs, but there are other possible uses for it. An actively developed prototype in Go uses Firecracker as a container runtime. The goal is a drop-in containerd replacement with the needed interfaces to meet the Open Container Initiative (OCI) and Container Network Interface (CNI) standards. Though there are already containerd shims like Kata Containers that can run containers in VMs, Firecracker's unusually pared-down design is hoped to be more lightweight and trustworthy. Currently, each container runs in a single VM, but the project plans to batch multiple containers into single VMs as well. Commands to manage containers get sent from front-end tools (like ctr). In the prototype's architecture, the runtime passes the commands to an agent inside the Firecracker guest VM using Firecracker's experimental vsock support. Inside the guest VM, the agent in turn proxies the commands to runc to spawn containers. The prototype also implements a snapshotter for creating and restoring container images as disk images. The initial goal of Firecracker was a faster serverless platform running code isolated in VMs. But Firecracker's use as a container runtime might prove its design more versatile than that. As an open-source project, it's a useful public exploration of what minimal application-specific KVM implementations can achieve when built without the need for legacy emulation. Index entries for this article GuestArticlesDesai, Azhar (Log in to post comments) ",
        "_version_": 1718536433440915456
      },
      {
        "story_id": 19305408,
        "story_author": "tidwall",
        "story_descendants": 24,
        "story_score": 130,
        "story_time": "2019-03-04T21:00:09Z",
        "story_title": "Show HN: Tile38 – Realtime geofencing and geospatial index",
        "search": [
          "Show HN: Tile38 – Realtime geofencing and geospatial index",
          "https://github.com/tidwall/tile38",
          "Tile38 is an open source (MIT licensed), in-memory geolocation data store, spatial index, and realtime geofence. It supports a variety of object types including lat/lon points, bounding boxes, XYZ tiles, Geohashes, and GeoJSON. Features Spatial index with search methods such as Nearby, Within, and Intersects. Realtime geofencing through webhooks or pub/sub channels. Object types of lat/lon, bbox, Geohash, GeoJSON, QuadKey, and XYZ tile. Support for lots of Clients Libraries written in many different languages. Variety of protocols, including http (curl), websockets, telnet, and the Redis RESP. Server responses are RESP or JSON. Full command line interface. Leader / follower replication. In-memory database that persists on disk. Components tile38-server - The server tile38-cli - Command line interface tool tile38-benchmark - Server benchmark tool Getting Started Getting Tile38 Perhaps the easiest way to get the latest Tile38 is to use one of the pre-built release binaries which are available for OSX, Linux, FreeBSD, and Windows. Instructions for using these binaries are on the GitHub releases page. Docker To run the latest stable version of Tile38: docker pull tile38/tile38 docker run -p 9851:9851 tile38/tile38 Visit the Tile38 hub page for more information. Homebrew (macOS) Install Tile38 using Homebrew brew install tile38 tile38-server Building Tile38 Tile38 can be compiled and used on Linux, OSX, Windows, FreeBSD, and probably others since the codebase is 100% Go. We support both 32 bit and 64 bit systems. Go must be installed on the build machine. To build everything simply: To test: Running For command line options invoke: To run a single server: $ ./tile38-server # The tile38 shell connects to localhost:9851 $ ./tile38-cli > help Prometheus Metrics Tile38 can natively export Prometheus metrics by setting the --metrics-addr command line flag (disabled by default). This example exposes the HTTP metrics server on port 4321: # start server and enable Prometheus metrics, listen on local interface only ./tile38-server --metrics-addr=127.0.0.1:4321 # access metrics curl http://127.0.0.1:4321/metrics If you need to access the /metrics endpoint from a different host you'll have to set the flag accordingly, e.g. set it to 0.0.0.0:<<port>> to listen on all interfaces. Use the redis_exporter for more advanced use cases like extracting key values or running a lua script. Playing with Tile38 Basic operations: $ ./tile38-cli # add a couple of points named 'truck1' and 'truck2' to a collection named 'fleet'. > set fleet truck1 point 33.5123 -112.2693 # on the Loop 101 in Phoenix > set fleet truck2 point 33.4626 -112.1695 # on the I-10 in Phoenix # search the 'fleet' collection. > scan fleet # returns both trucks in 'fleet' > nearby fleet point 33.462 -112.268 6000 # search 6 kilometers around a point. returns one truck. # key value operations > get fleet truck1 # returns 'truck1' > del fleet truck2 # deletes 'truck2' > drop fleet # removes all Tile38 has a ton of great commands. Fields Fields are extra data that belongs to an object. A field is always a double precision floating point. There is no limit to the number of fields that an object can have. To set a field when setting an object: > set fleet truck1 field speed 90 point 33.5123 -112.2693 > set fleet truck1 field speed 90 field age 21 point 33.5123 -112.2693 To set a field when an object already exists: > fset fleet truck1 speed 90 Searching Tile38 has support to search for objects and points that are within or intersects other objects. All object types can be searched including Polygons, MultiPolygons, GeometryCollections, etc. Within WITHIN searches a collection for objects that are fully contained inside a specified bounding area. Intersects INTERSECTS searches a collection for objects that intersect a specified bounding area. Nearby NEARBY searches a collection for objects that intersect a specified radius. Search options SPARSE - This option will distribute the results of a search evenly across the requested area. This is very helpful for example; when you have many (perhaps millions) of objects and do not want them all clustered together on a map. Sparse will limit the number of objects returned and provide them evenly distributed so that your map looks clean. You can choose a value between 1 and 8. The value 1 will result in no more than 4 items. The value 8 will result in no more than 65536. 1=4, 2=16, 3=64, 4=256, 5=1024, 6=4098, 7=16384, 8=65536. No Sparsing Sparse 1 Sparse 2 Sparse 3 Sparse 4 Sparse 5 Please note that the higher the sparse value, the slower the performance. Also, LIMIT and CURSOR are not available when using SPARSE. WHERE - This option allows for filtering out results based on field values. For examplenearby fleet where speed 70 +inf point 33.462 -112.268 6000 will return only the objects in the 'fleet' collection that are within the 6 km radius and have a field named speed that is greater than 70. Multiple WHEREs are concatenated as and clauses. WHERE speed 70 +inf WHERE age -inf 24 would be interpreted as speed is over 70 and age is less than 24.The default value for a field is always 0. Thus if you do a WHERE on the field speed and an object does not have that field set, the server will pretend that the object does and that the value is Zero. MATCH - MATCH is similar to WHERE except that it works on the object id instead of fields.nearby fleet match truck* point 33.462 -112.268 6000 will return only the objects in the 'fleet' collection that are within the 6 km radius and have an object id that starts with truck. There can be multiple MATCH options in a single search. The MATCH value is a simple glob pattern. CURSOR - CURSOR is used to iterate though many objects from the search results. An iteration begins when the CURSOR is set to Zero or not included with the request, and completes when the cursor returned by the server is Zero. NOFIELDS - NOFIELDS tells the server that you do not want field values returned with the search results. LIMIT - LIMIT can be used to limit the number of objects returned for a single search request. A geofence is a virtual boundary that can detect when an object enters or exits the area. This boundary can be a radius, bounding box, or a polygon. Tile38 can turn any standard search into a geofence monitor by adding the FENCE keyword to the search. Tile38 also allows for Webhooks to be assigned to Geofences. A simple example: > nearby fleet fence point 33.462 -112.268 6000 This command opens a geofence that monitors the 'fleet' collection. The server will respond with: And the connection will be kept open. If any object enters or exits the 6 km radius around 33.462,-112.268 the server will respond in realtime with a message such as: {\"command\":\"set\",\"detect\":\"enter\",\"id\":\"truck02\",\"object\":{\"type\":\"Point\",\"coordinates\":[-112.2695,33.4626]}} The server will notify the client if the command is del | set | drop. del notifies the client that an object has been deleted from the collection that is being fenced. drop notifies the client that the entire collection is dropped. set notifies the client that an object has been added or updated, and when it's position is detected by the fence. The detect may be one of the following values. inside is when an object is inside the specified area. outside is when an object is outside the specified area. enter is when an object that was not previously in the fence has entered the area. exit is when an object that was previously in the fence has exited the area. cross is when an object that was not previously in the fence has entered and exited the area. These can be used when establishing a geofence, to pre-filter responses. For instance, to limit responses to enter and exit detections: > nearby fleet fence detect enter,exit point 33.462 -112.268 6000 Pub/sub channels Tile38 supports delivering geofence notications over pub/sub channels. To create a static geofence that sends notifications when a bus is within 200 meters of a point and sends to the busstop channel: > setchan busstop nearby buses fence point 33.5123 -112.2693 200 Subscribe on the busstop channel: Object types All object types except for XYZ Tiles and QuadKeys can be stored in a collection. XYZ Tiles and QuadKeys are reserved for the SEARCH keyword only. Lat/lon point The most basic object type is a point that is composed of a latitude and a longitude. There is an optional z member that may be used for auxiliary data such as elevation or a timestamp. set fleet truck1 point 33.5123 -112.2693 # plain lat/lon set fleet truck1 point 33.5123 -112.2693 225 # lat/lon with z member Bounding box A bounding box consists of two points. The first being the southwestern most point and the second is the northeastern most point. set fleet truck1 bounds 30 -110 40 -100 Geohash A geohash is a string representation of a point. With the length of the string indicating the precision of the point. set fleet truck1 hash 9tbnthxzr # this would be equivalent to 'point 33.5123 -112.2693' GeoJSON GeoJSON is an industry standard format for representing a variety of object types including a point, multipoint, linestring, multilinestring, polygon, multipolygon, geometrycollection, feature, and featurecollection. * All ignored members will not persist. Important to note that all coordinates are in Longitude, Latitude order. set city tempe object {\"type\":\"Polygon\",\"coordinates\":[[[0,0],[10,10],[10,0],[0,0]]]} XYZ Tile An XYZ tile is rectangle bounding area on earth that is represented by an X, Y coordinate and a Z (zoom) level. Check out maptiler.org for an interactive example. QuadKey A QuadKey used the same coordinate system as an XYZ tile except that the string representation is a string characters composed of 0, 1, 2, or 3. For a detailed explanation checkout The Bing Maps Tile System. Network protocols It's recommended to use a client library or the Tile38 CLI, but there are times when only HTTP is available or when you need to test from a remote terminal. In those cases we provide an HTTP and telnet options. HTTP One of the simplest ways to call a tile38 command is to use HTTP. From the command line you can use curl. For example: # call with request in the body curl --data \"set fleet truck3 point 33.4762 -112.10923\" localhost:9851 # call with request in the url path curl localhost:9851/set+fleet+truck3+point+33.4762+-112.10923 Websockets Websockets can be used when you need to Geofence and keep the connection alive. It works just like the HTTP example above, with the exception that the connection stays alive and the data is sent from the server as text websocket messages. Telnet There is the option to use a plain telnet connection. The default output through telnet is RESP. telnet localhost 9851 set fleet truck3 point 33.4762 -112.10923 +OK The server will respond in JSON or RESP depending on which protocol is used when initiating the first command. HTTP and Websockets use JSON. Telnet and RESP clients use RESP. Tile38 Client Libraries The following clients are built specifically for Tile38. Clients that support most Tile38 features are marked with a . Go: xjem/t38c Node.js: node-tile38 (example code) Python: pyle38 Go: cjkreklow/t38c Python: pytile38 Rust: nazar Swift: Talon Java: tile38-client-java Java: tile38-client Redis Client Libraries Tile38 uses the Redis RESP protocol natively. Therefore most clients that support basic Redis commands will also support Tile38. C: hiredis C#: StackExchange.Redis C++: redox Clojure: carmine Common Lisp: CL-Redis Erlang: Eredis Go: go-redis (example code) Go: redigo (example code) Haskell: hedis Java: lettuce (example code) Node.js: node_redis (example code) Perl: perl-redis PHP: tinyredisclient (example code) PHP: phpredis Python: redis-py (example code) Ruby: redic (example code) Ruby: redis-rb (example code) Rust: redis-rs Scala: scala-redis Swift: Redbird Contact Josh Baker @tidwall License Tile38 source code is available under the MIT License. "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/tidwall/tile38",
        "comments.comment_id": [19306411, 19307480],
        "comments.comment_author": ["gravypod", "alleycat5000"],
        "comments.comment_descendants": [2, 0],
        "comments.comment_time": [
          "2019-03-04T22:44:27Z",
          "2019-03-05T01:58:39Z"
        ],
        "comments.comment_text": [
          "This is definitely a cool product. I've been thinking of building something like this for use in a game server backend.<p>Do you plan on adding height-support? I'm sure some clients in the professional space would like to know \"what room/floor in this building are you in?\"",
          "One thing I've found hard to find in Go is a library equivalent to GEOS/JTS; looks like there are some nice geometric operations bundled under <a href=\"https://github.com/tidwall/geojson\" rel=\"nofollow\">https://github.com/tidwall/geojson</a>"
        ],
        "id": "59e3c0db-cf68-4fe6-953d-92ddb3d914a5",
        "url_text": "Tile38 is an open source (MIT licensed), in-memory geolocation data store, spatial index, and realtime geofence. It supports a variety of object types including lat/lon points, bounding boxes, XYZ tiles, Geohashes, and GeoJSON. Features Spatial index with search methods such as Nearby, Within, and Intersects. Realtime geofencing through webhooks or pub/sub channels. Object types of lat/lon, bbox, Geohash, GeoJSON, QuadKey, and XYZ tile. Support for lots of Clients Libraries written in many different languages. Variety of protocols, including http (curl), websockets, telnet, and the Redis RESP. Server responses are RESP or JSON. Full command line interface. Leader / follower replication. In-memory database that persists on disk. Components tile38-server - The server tile38-cli - Command line interface tool tile38-benchmark - Server benchmark tool Getting Started Getting Tile38 Perhaps the easiest way to get the latest Tile38 is to use one of the pre-built release binaries which are available for OSX, Linux, FreeBSD, and Windows. Instructions for using these binaries are on the GitHub releases page. Docker To run the latest stable version of Tile38: docker pull tile38/tile38 docker run -p 9851:9851 tile38/tile38 Visit the Tile38 hub page for more information. Homebrew (macOS) Install Tile38 using Homebrew brew install tile38 tile38-server Building Tile38 Tile38 can be compiled and used on Linux, OSX, Windows, FreeBSD, and probably others since the codebase is 100% Go. We support both 32 bit and 64 bit systems. Go must be installed on the build machine. To build everything simply: To test: Running For command line options invoke: To run a single server: $ ./tile38-server # The tile38 shell connects to localhost:9851 $ ./tile38-cli > help Prometheus Metrics Tile38 can natively export Prometheus metrics by setting the --metrics-addr command line flag (disabled by default). This example exposes the HTTP metrics server on port 4321: # start server and enable Prometheus metrics, listen on local interface only ./tile38-server --metrics-addr=127.0.0.1:4321 # access metrics curl http://127.0.0.1:4321/metrics If you need to access the /metrics endpoint from a different host you'll have to set the flag accordingly, e.g. set it to 0.0.0.0:<<port>> to listen on all interfaces. Use the redis_exporter for more advanced use cases like extracting key values or running a lua script. Playing with Tile38 Basic operations: $ ./tile38-cli # add a couple of points named 'truck1' and 'truck2' to a collection named 'fleet'. > set fleet truck1 point 33.5123 -112.2693 # on the Loop 101 in Phoenix > set fleet truck2 point 33.4626 -112.1695 # on the I-10 in Phoenix # search the 'fleet' collection. > scan fleet # returns both trucks in 'fleet' > nearby fleet point 33.462 -112.268 6000 # search 6 kilometers around a point. returns one truck. # key value operations > get fleet truck1 # returns 'truck1' > del fleet truck2 # deletes 'truck2' > drop fleet # removes all Tile38 has a ton of great commands. Fields Fields are extra data that belongs to an object. A field is always a double precision floating point. There is no limit to the number of fields that an object can have. To set a field when setting an object: > set fleet truck1 field speed 90 point 33.5123 -112.2693 > set fleet truck1 field speed 90 field age 21 point 33.5123 -112.2693 To set a field when an object already exists: > fset fleet truck1 speed 90 Searching Tile38 has support to search for objects and points that are within or intersects other objects. All object types can be searched including Polygons, MultiPolygons, GeometryCollections, etc. Within WITHIN searches a collection for objects that are fully contained inside a specified bounding area. Intersects INTERSECTS searches a collection for objects that intersect a specified bounding area. Nearby NEARBY searches a collection for objects that intersect a specified radius. Search options SPARSE - This option will distribute the results of a search evenly across the requested area. This is very helpful for example; when you have many (perhaps millions) of objects and do not want them all clustered together on a map. Sparse will limit the number of objects returned and provide them evenly distributed so that your map looks clean. You can choose a value between 1 and 8. The value 1 will result in no more than 4 items. The value 8 will result in no more than 65536. 1=4, 2=16, 3=64, 4=256, 5=1024, 6=4098, 7=16384, 8=65536. No Sparsing Sparse 1 Sparse 2 Sparse 3 Sparse 4 Sparse 5 Please note that the higher the sparse value, the slower the performance. Also, LIMIT and CURSOR are not available when using SPARSE. WHERE - This option allows for filtering out results based on field values. For examplenearby fleet where speed 70 +inf point 33.462 -112.268 6000 will return only the objects in the 'fleet' collection that are within the 6 km radius and have a field named speed that is greater than 70. Multiple WHEREs are concatenated as and clauses. WHERE speed 70 +inf WHERE age -inf 24 would be interpreted as speed is over 70 and age is less than 24.The default value for a field is always 0. Thus if you do a WHERE on the field speed and an object does not have that field set, the server will pretend that the object does and that the value is Zero. MATCH - MATCH is similar to WHERE except that it works on the object id instead of fields.nearby fleet match truck* point 33.462 -112.268 6000 will return only the objects in the 'fleet' collection that are within the 6 km radius and have an object id that starts with truck. There can be multiple MATCH options in a single search. The MATCH value is a simple glob pattern. CURSOR - CURSOR is used to iterate though many objects from the search results. An iteration begins when the CURSOR is set to Zero or not included with the request, and completes when the cursor returned by the server is Zero. NOFIELDS - NOFIELDS tells the server that you do not want field values returned with the search results. LIMIT - LIMIT can be used to limit the number of objects returned for a single search request. A geofence is a virtual boundary that can detect when an object enters or exits the area. This boundary can be a radius, bounding box, or a polygon. Tile38 can turn any standard search into a geofence monitor by adding the FENCE keyword to the search. Tile38 also allows for Webhooks to be assigned to Geofences. A simple example: > nearby fleet fence point 33.462 -112.268 6000 This command opens a geofence that monitors the 'fleet' collection. The server will respond with: And the connection will be kept open. If any object enters or exits the 6 km radius around 33.462,-112.268 the server will respond in realtime with a message such as: {\"command\":\"set\",\"detect\":\"enter\",\"id\":\"truck02\",\"object\":{\"type\":\"Point\",\"coordinates\":[-112.2695,33.4626]}} The server will notify the client if the command is del | set | drop. del notifies the client that an object has been deleted from the collection that is being fenced. drop notifies the client that the entire collection is dropped. set notifies the client that an object has been added or updated, and when it's position is detected by the fence. The detect may be one of the following values. inside is when an object is inside the specified area. outside is when an object is outside the specified area. enter is when an object that was not previously in the fence has entered the area. exit is when an object that was previously in the fence has exited the area. cross is when an object that was not previously in the fence has entered and exited the area. These can be used when establishing a geofence, to pre-filter responses. For instance, to limit responses to enter and exit detections: > nearby fleet fence detect enter,exit point 33.462 -112.268 6000 Pub/sub channels Tile38 supports delivering geofence notications over pub/sub channels. To create a static geofence that sends notifications when a bus is within 200 meters of a point and sends to the busstop channel: > setchan busstop nearby buses fence point 33.5123 -112.2693 200 Subscribe on the busstop channel: Object types All object types except for XYZ Tiles and QuadKeys can be stored in a collection. XYZ Tiles and QuadKeys are reserved for the SEARCH keyword only. Lat/lon point The most basic object type is a point that is composed of a latitude and a longitude. There is an optional z member that may be used for auxiliary data such as elevation or a timestamp. set fleet truck1 point 33.5123 -112.2693 # plain lat/lon set fleet truck1 point 33.5123 -112.2693 225 # lat/lon with z member Bounding box A bounding box consists of two points. The first being the southwestern most point and the second is the northeastern most point. set fleet truck1 bounds 30 -110 40 -100 Geohash A geohash is a string representation of a point. With the length of the string indicating the precision of the point. set fleet truck1 hash 9tbnthxzr # this would be equivalent to 'point 33.5123 -112.2693' GeoJSON GeoJSON is an industry standard format for representing a variety of object types including a point, multipoint, linestring, multilinestring, polygon, multipolygon, geometrycollection, feature, and featurecollection. * All ignored members will not persist. Important to note that all coordinates are in Longitude, Latitude order. set city tempe object {\"type\":\"Polygon\",\"coordinates\":[[[0,0],[10,10],[10,0],[0,0]]]} XYZ Tile An XYZ tile is rectangle bounding area on earth that is represented by an X, Y coordinate and a Z (zoom) level. Check out maptiler.org for an interactive example. QuadKey A QuadKey used the same coordinate system as an XYZ tile except that the string representation is a string characters composed of 0, 1, 2, or 3. For a detailed explanation checkout The Bing Maps Tile System. Network protocols It's recommended to use a client library or the Tile38 CLI, but there are times when only HTTP is available or when you need to test from a remote terminal. In those cases we provide an HTTP and telnet options. HTTP One of the simplest ways to call a tile38 command is to use HTTP. From the command line you can use curl. For example: # call with request in the body curl --data \"set fleet truck3 point 33.4762 -112.10923\" localhost:9851 # call with request in the url path curl localhost:9851/set+fleet+truck3+point+33.4762+-112.10923 Websockets Websockets can be used when you need to Geofence and keep the connection alive. It works just like the HTTP example above, with the exception that the connection stays alive and the data is sent from the server as text websocket messages. Telnet There is the option to use a plain telnet connection. The default output through telnet is RESP. telnet localhost 9851 set fleet truck3 point 33.4762 -112.10923 +OK The server will respond in JSON or RESP depending on which protocol is used when initiating the first command. HTTP and Websockets use JSON. Telnet and RESP clients use RESP. Tile38 Client Libraries The following clients are built specifically for Tile38. Clients that support most Tile38 features are marked with a . Go: xjem/t38c Node.js: node-tile38 (example code) Python: pyle38 Go: cjkreklow/t38c Python: pytile38 Rust: nazar Swift: Talon Java: tile38-client-java Java: tile38-client Redis Client Libraries Tile38 uses the Redis RESP protocol natively. Therefore most clients that support basic Redis commands will also support Tile38. C: hiredis C#: StackExchange.Redis C++: redox Clojure: carmine Common Lisp: CL-Redis Erlang: Eredis Go: go-redis (example code) Go: redigo (example code) Haskell: hedis Java: lettuce (example code) Node.js: node_redis (example code) Perl: perl-redis PHP: tinyredisclient (example code) PHP: phpredis Python: redis-py (example code) Ruby: redic (example code) Ruby: redis-rb (example code) Rust: redis-rs Scala: scala-redis Swift: Redbird Contact Josh Baker @tidwall License Tile38 source code is available under the MIT License. ",
        "_version_": 1718536456598716416
      },
      {
        "story_id": 21509373,
        "story_author": "nexuist",
        "story_descendants": 26,
        "story_score": 147,
        "story_time": "2019-11-11T22:02:51Z",
        "story_title": "Usql – A Universal CLI for Databases",
        "search": [
          "Usql – A Universal CLI for Databases",
          "https://github.com/xo/usql",
          "Installing | Building | Using | Database Support | Features and Compatibility | Releases | Contributing usql is a universal command-line interface for PostgreSQL, MySQL, Oracle Database, SQLite3, Microsoft SQL Server, and many other databases including NoSQL and non-relational databases! usql provides a simple way to work with SQL and NoSQL databases via a command-line inspired by PostgreSQL's psql. usql supports most of the core psql features, such as variables, backticks, and commands and has additional features that psql does not, such as syntax highlighting, context-based completion, and multiple database support. Database administrators and developers that would prefer to work with a tool like psql with non-PostgreSQL databases, will find usql intuitive, easy-to-use, and a great replacement for the command-line clients/tools for other databases. Installing usql can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the usql or usql.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) macOS Notes The recommended installation method on macOS is via brew (see below). If the following or similar error is encountered when attempting to run usql: $ usql dyld: Library not loaded: /usr/local/opt/icu4c/lib/libicuuc.68.dylib Referenced from: /Users/user/.local/bin/usql Reason: image not found Abort trap: 6 Then the ICU lib needs to be installed. This can be accomplished using brew: Installing via Homebrew (macOS and Linux) usql is available in the xo/xo tap, and can be installed in the usual way with the brew command: # install usql with \"most\" drivers $ brew install xo/xo/usql Additional support for ODBC databases can be installed by passing --with-odbc option during install: # install usql with odbc support $ brew install --with-odbc usql Installing via Scoop (Windows) usql can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install usql Installing via Go usql can be installed in the usual Go fashion: # install usql from master branch with basic database support # includes PostgreSQL, Oracle Database, MySQL, MS SQL, and SQLite3 drivers $ go install github.com/xo/usql@master Building When building usql with Go, only drivers for PostgreSQL, MySQL, SQLite3 and Microsoft SQL Server will be enabled by default. Other databases can be enabled by specifying the build tag for their database driver. Additionally, the most and all build tags include most, and all SQL drivers, respectively: # install all drivers $ go install -tags all github.com/xo/usql@master # install with most drivers (excludes unsupported drivers) $ go install -tags most github.com/xo/usql@master # install with base drivers and additional support for Oracle Database and ODBC $ go install -tags 'godror odbc' github.com/xo/usql@master For every build tag <driver>, there is also a no_<driver> build tag disabling the driver: # install all drivers excluding avatica and couchbase $ go install -tags 'all no_avatica no_couchbase' github.com/xo/usql@master Release Builds Release builds are built with the most build tag. Additional SQLite3 build tags are also specified for releases. Embedding An effort has been made to keep usql's packages modular, and reusable by other developers wishing to leverage the usql code base. As such, it is possible to embed or create a SQL command-line interface (e.g, for use by some other project as an \"official\" client) using the core usql source tree. Please refer to main.go to see how usql puts together its packages. usql's code is also well-documented -- please refer to the Go reference for an overview of the various packages and APIs. Database Support usql works with all Go standard library compatible SQL drivers supported by github.com/xo/dburl. The list of drivers that usql was built with can be displayed using the \\drivers command: $ cd $GOPATH/src/github.com/xo/usql $ export GO111MODULE=on # build excluding the base drivers, and including cassandra and moderncsqlite $ go build -tags 'no_postgres no_oracle no_sqlserver no_sqlite3 cassandra moderncsqlite' # show built driver support $ ./usql -c '\\drivers' Available Drivers: cql [ca, scy, scylla, datastax, cassandra] memsql (mysql) [me] moderncsqlite [mq, sq, file, sqlite, sqlite3, modernsqlite] mysql [my, maria, aurora, mariadb, percona] tidb (mysql) [ti] vitess (mysql) [vt] The above shows that usql was built with only the mysql, cassandra (ie, cql), and moderncsqlite drivers. The output above reflects information about the drivers available to usql, specifically the internal driver name, its primary URL scheme, the driver's available scheme aliases (shown in [...]), and the real/underlying driver (shown in (...)) for wire compatible drivers. Supported Database Schemes and Aliases The following are the Go SQL drivers that usql supports, the associated database, scheme / build tag, and scheme aliases: Database Scheme / Tag Scheme Aliases Driver Package / Notes Microsoft SQL Server sqlserver ms, mssql github.com/denisenkom/go-mssqldb MySQL mysql my, maria, aurora, mariadb, percona github.com/go-sql-driver/mysql Oracle Database oracle or, ora, oci, oci8, odpi, odpi-c github.com/sijms/go-ora/v2 PostgreSQL postgres pg, pgsql, postgresql github.com/lib/pq SQLite3 sqlite3 sq, file, sqlite github.com/mattn/go-sqlite3 Alibaba MaxCompute maxcompute mc sqlflow.org/gomaxcompute Apache Avatica avatica av, phoenix github.com/apache/calcite-avatica-go/v5 Apache H2 h2 github.com/jmrobles/h2go Apache Ignite ignite ig, gridgain github.com/amsokol/ignite-go-client/sql AWS Athena athena s3, aws github.com/uber/athenadriver/go Cassandra cassandra ca, scy, scylla, datastax, cql github.com/MichaelS11/go-cql-driver ClickHouse clickhouse ch github.com/ClickHouse/clickhouse-go Couchbase couchbase n1, n1ql github.com/couchbase/go_n1ql CSVQ csvq cs, csv, tsv, json github.com/mithrandie/csvq-driver Cznic QL ql cznic, cznicql modernc.org/ql Exasol exasol ex, exa github.com/exasol/exasol-driver-go Firebird firebird fb, firebirdsql github.com/nakagami/firebirdsql Genji genji gj github.com/genjidb/genji/driver Google BigQuery bigquery bq gorm.io/driver/bigquery/driver Google Spanner spanner sp github.com/cloudspannerecosystem/go-sql-spanner Microsoft ADODB adodb ad, ado github.com/mattn/go-adodb ModernC SQLite3 moderncsqlite mq, modernsqlite modernc.org/sqlite MySQL MyMySQL mymysql zm, mymy github.com/ziutek/mymysql/godrv Netezza netezza nz, nzgo github.com/IBM/nzgo PostgreSQL PGX pgx px github.com/jackc/pgx/v4/stdlib Presto presto pr, prs, prestos, prestodb, prestodbs github.com/prestodb/presto-go-client/presto SAP ASE sapase ax, ase, tds github.com/thda/tds SAP HANA saphana sa, sap, hana, hdb github.com/SAP/go-hdb/driver Trino trino tr, trs, trinos github.com/trinodb/trino-go-client/trino Vertica vertica ve github.com/vertica/vertica-sql-go VoltDB voltdb vo, vdb, volt github.com/VoltDB/voltdb-client-go/voltdbclient Apache Hive hive hi sqlflow.org/gohive Apache Impala impala im github.com/bippio/go-impala Azure CosmosDB cosmos cm github.com/btnguyen2k/gocosmos GO DRiver for ORacle godror gr github.com/godror/godror ODBC odbc od github.com/alexbrainman/odbc Snowflake snowflake sf github.com/snowflakedb/gosnowflake Amazon Redshift postgres rs, redshift github.com/lib/pq CockroachDB postgres cr, cdb, crdb, cockroach, cockroachdb github.com/lib/pq OLE ODBC adodb oo, ole, oleodbc github.com/mattn/go-adodb SingleStore MemSQL mysql me, memsql github.com/go-sql-driver/mysql TiDB mysql ti, tidb github.com/go-sql-driver/mysql Vitess Database mysql vt, vitess github.com/go-sql-driver/mysql NO DRIVERS no_base no base drivers (useful for development) MOST DRIVERS most all stable drivers ALL DRIVERS all all drivers NO <TAG> no_<tag> exclude driver with <tag> Requires CGO Wire compatible (see respective driver) Any of the protocol schemes/aliases shown above can be used in conjunction when connecting to a database via the command-line or with the \\connect command: # connect to a vitess database: $ usql vt://user:pass@host:3306/mydatabase $ usql (not connected)=> \\c vitess://user:pass@host:3306/mydatabase See the section below on connecting to databases for further details building DSNs/URLs for use with usql. Using After installing, usql can be used similarly to the following: # connect to a postgres database $ usql postgres://booktest@localhost/booktest # connect to an oracle database $ usql oracle://user:pass@host/oracle.sid # connect to a postgres database and run the commands contained in script.sql $ usql pg://localhost/ -f script.sql Command-line Options Supported command-line options: $ usql --help usql, the universal command-line interface for SQL databases Usage: usql [OPTIONS]... [DSN] Arguments: DSN database url Options: -c, --command=COMMAND ... run only single command (SQL or internal) and exit -f, --file=FILE ... execute commands from file and exit -w, --no-password never prompt for password -X, --no-rc do not read start up file -o, --out=OUT output file -W, --password force password prompt (should happen automatically) -1, --single-transaction execute as a single transaction (if non-interactive) -v, --set=, --variable=NAME=VALUE ... set variable NAME to VALUE -P, --pset=VAR[=ARG] ... set printing option VAR to ARG (see \\pset command) -F, --field-separator=FIELD-SEPARATOR ... field separator for unaligned output (default, \"|\") -R, --record-separator=RECORD-SEPARATOR ... record separator for unaligned output (default, \\n) -T, --table-attr=TABLE-ATTR ... set HTML table tag attributes (e.g., width, border) -A, --no-align unaligned table output mode -H, --html HTML table output mode -t, --tuples-only print rows only -x, --expanded turn on expanded table output -z, --field-separator-zero set field separator for unaligned output to zero byte -0, --record-separator-zero set record separator for unaligned output to zero byte -J, --json JSON output mode -C, --csv CSV output mode -G, --vertical vertical output mode -V, --version display version and exit Connecting to Databases usql opens a database connection by parsing a URL and passing the resulting connection string to a database driver. Database connection strings (aka \"data source name\" or DSNs) have the same parsing rules as URLs, and can be passed to usql via command-line, or to the \\connect or \\c commands. Connection strings look like the following: driver+transport://user:pass@host/dbname?opt1=a&opt2=b driver:/path/to/file /path/to/file Where the above are: Component Description driver driver scheme name or scheme alias transport tcp, udp, unix or driver name (for ODBC and ADODB) user username pass password host hostname dbname database name, instance, or service name/ID ?opt1=a&... additional database driver options (see respective SQL driver for available options) /path/to/file a path on disk Some databases, such as Microsoft SQL Server, or Oracle Database support a path component (ie, /dbname) in the form of /instance/dbname, where /instance is the optional service identifier (aka \"SID\") or database instance Driver Aliases usql supports the same driver names and aliases from the dburl package. Most databases have at least one or more alias - please refer to the dburl documentation for all supported aliases. Short Aliases All database drivers have a two character short form that is usually the first two letters of the database driver. For example, pg for postgres, my for mysql, ms for sqlserver (formerly known as mssql), or for oracle, or sq for sqlite3. Passing Driver Options Driver options are specified as standard URL query options in the form of ?opt1=a&obt2=b. Please refer to the relevant database driver's documentation for available options. Paths on Disk If a URL does not have a driver: scheme, usql will check if it is a path on disk. If the path exists, usql will attempt to use an appropriate database driver to open the path. If the specified path is a Unix Domain Socket, usql will attempt to open it using the MySQL driver. If the path is a directory, usql will attempt to open it using the PostgreSQL driver. If the path is a regular file, usql will attempt to open the file using the SQLite3 driver. Driver Defaults As with URLs, most components in the URL are optional and many components can be left out. usql will attempt connecting using defaults where possible: # connect to postgres using the local $USER and the unix domain socket in /var/run/postgresql $ usql pg:// Please see documentation for the database driver you are connecting with for more information. Connection Examples The following are example connection strings and additional ways to connect to databases using usql: # connect to a postgres database $ usql pg://user:pass@host/dbname $ usql pgsql://user:pass@host/dbname $ usql postgres://user:pass@host:port/dbname $ usql pg:// $ usql /var/run/postgresql $ usql pg://user:pass@host/dbname?sslmode=disable # Connect without SSL # connect to a mysql database $ usql my://user:pass@host/dbname $ usql mysql://user:pass@host:port/dbname $ usql my:// $ usql /var/run/mysqld/mysqld.sock # connect to a sqlserver database $ usql sqlserver://user:pass@host/instancename/dbname $ usql ms://user:pass@host/dbname $ usql ms://user:pass@host/instancename/dbname $ usql mssql://user:pass@host:port/dbname $ usql ms:// # connect to a sqlserver database using Windows domain authentication $ runas /user:ACME\\wiley /netonly \"usql mssql://host/dbname/\" # connect to a oracle database $ usql or://user:pass@host/sid $ usql oracle://user:pass@host:port/sid $ usql or:// # connect to a cassandra database $ usql ca://user:pass@host/keyspace $ usql cassandra://host/keyspace $ usql cql://host/ $ usql ca:// # connect to a sqlite database that exists on disk $ usql dbname.sqlite3 # NOTE: when connecting to a SQLite database, if the \"<driver>://\" or # \"<driver>:\" scheme/alias is omitted, the file must already exist on disk. # # if the file does not yet exist, the URL must incorporate file:, sq:, sqlite3:, # or any other recognized sqlite3 driver alias to force usql to create a new, # empty database at the specified path: $ usql sq://path/to/dbname.sqlite3 $ usql sqlite3://path/to/dbname.sqlite3 $ usql file:/path/to/dbname.sqlite3 # connect to a adodb ole resource (windows only) $ usql adodb://Microsoft.Jet.OLEDB.4.0/myfile.mdb $ usql \"adodb://Microsoft.ACE.OLEDB.12.0/?Extended+Properties=\\\"Text;HDR=NO;FMT=Delimited\\\"\" # connect with ODBC driver (requires building with odbc tag) $ cat /etc/odbcinst.ini [DB2] Description=DB2 driver Driver=/opt/db2/clidriver/lib/libdb2.so FileUsage = 1 DontDLClose = 1 [PostgreSQL ANSI] Description=PostgreSQL ODBC driver (ANSI version) Driver=psqlodbca.so Setup=libodbcpsqlS.so Debug=0 CommLog=1 UsageCount=1 # connect to db2, postgres databases using ODBC $ usql odbc+DB2://user:pass@localhost/dbname $ usql odbc+PostgreSQL+ANSI://user:pass@localhost/dbname?TraceFile=/path/to/trace.log Executing Queries and Commands The interactive intrepreter reads queries and meta (\\ ) commands, sending the query to the connected database: $ usql sqlite://example.sqlite3 Connected with driver sqlite3 (SQLite3 3.17.0) Type \"help\" for help. sq:example.sqlite3=> create table test (test_id int, name string); CREATE TABLE sq:example.sqlite3=> insert into test (test_id, name) values (1, 'hello'); INSERT 1 sq:example.sqlite3=> select * from test; test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> select * from test sq:example.sqlite3-> \\p select * from test sq:example.sqlite3-> \\g test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> \\c postgres://booktest@localhost error: pq: 28P01: password authentication failed for user \"booktest\" Enter password: Connected with driver postgres (PostgreSQL 9.6.6) pg:booktest@localhost=> select * from authors; author_id | name +-----------+----------------+ 1 | Unknown Master 2 | blah 3 | aoeu (3 rows) pg:booktest@localhost=> Commands may accept one or more parameter, and can be quoted using either ' or \". Command parameters may also be backtick'd. Backslash Commands Currently available commands: $ usql Type \"help\" for help. (not connected)=> \\? General \\q quit usql \\copyright show usql usage and distribution terms \\drivers display information about available database drivers Query Execute \\g [(OPTIONS)] [FILE] or ; execute query (and send results to file or |pipe) \\crosstabview [(OPTIONS)] [COLUMNS] execute query and display results in crosstab \\G [(OPTIONS)] [FILE] as \\g, but forces vertical output mode \\gexec execute query and execute each value of the result \\gset [PREFIX] execute query and store results in usql variables \\gx [(OPTIONS)] [FILE] as \\g, but forces expanded output mode \\watch [(OPTIONS)] [DURATION] execute query every specified interval Query Buffer \\e [FILE] [LINE] edit the query buffer (or file) with external editor \\p show the contents of the query buffer \\raw show the raw (non-interpolated) contents of the query buffer \\r reset (clear) the query buffer \\w FILE write query buffer to file Help \\? [commands] show help on backslash commands \\? options show help on usql command-line options \\? variables show help on special variables Input/Output \\echo [-n] [STRING] write string to standard output (-n for no newline) \\qecho [-n] [STRING] write string to \\o output stream (-n for no newline) \\warn [-n] [STRING] write string to standard error (-n for no newline) \\o [FILE] send all query results to file or |pipe \\i FILE execute commands from file \\ir FILE as \\i, but relative to location of current script Informational \\d[S+] [NAME] list tables, views, and sequences or describe table, view, sequence, or index \\da[S+] [PATTERN] list aggregates \\df[S+] [PATTERN] list functions \\di[S+] [PATTERN] list indexes \\dm[S+] [PATTERN] list materialized views \\dn[S+] [PATTERN] list schemas \\ds[S+] [PATTERN] list sequences \\dt[S+] [PATTERN] list tables \\dv[S+] [PATTERN] list views \\l[+] list databases \\ss[+] [TABLE|QUERY] [k] show stats for a table or a query Formatting \\pset [NAME [VALUE]] set table output option \\a toggle between unaligned and aligned output mode \\C [STRING] set table title, or unset if none \\f [STRING] show or set field separator for unaligned query output \\H toggle HTML output mode \\T [STRING] set HTML <table> tag attributes, or unset if none \\t [on|off] show only rows \\x [on|off|auto] toggle expanded output Transaction \\begin begin a transaction \\commit commit current transaction \\rollback rollback (abort) current transaction Connection \\c URL connect to database with url \\c DRIVER PARAMS... connect to database with SQL driver and parameters \\Z close database connection \\password [USERNAME] change the password for a user \\conninfo display information about the current database connection Operating System \\cd [DIR] change the current working directory \\setenv NAME [VALUE] set or unset environment variable \\! [COMMAND] execute command in shell or start interactive shell \\timing [on|off] toggle timing of commands Variables \\prompt [-TYPE] <VAR> [PROMPT] prompt user to set variable \\set [NAME [VALUE]] set internal variable, or list all if no parameters \\unset NAME unset (delete) internal variable Features and Compatibility The usql project's goal is to support all standard psql commands and features. Pull Requests are always appreciated! Variables and Interpolation usql supports client-side interpolation of variables that can be \\set and \\unset: $ usql (not connected)=> \\set (not connected)=> \\set FOO bar (not connected)=> \\set FOO = 'bar' (not connected)=> \\unset FOO (not connected)=> \\set (not connected)=> A \\set variable, NAME, will be directly interpolated (by string substitution) into the query when prefixed with : and optionally surrounded by quotation marks (' or \"): pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from authors where name = :'FOO'; author_id | name +-----------+------+ 7 | bar (1 rows) The three forms, :NAME, :'NAME', and :\"NAME\", are used to interpolate a variable in parts of a query that may require quoting, such as for a column name, or when doing concatenation in a query: pg:booktest@localhost=> \\set TBLNAME authors pg:booktest@localhost=> \\set COLNAME name pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\p select * from authors where \"name\" = 'bar' pg:booktest@localhost-> \\raw select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\g author_id | name +-----------+------+ 7 | bar (1 rows) pg:booktest@localhost=> Note: variables contained within other strings will NOT be interpolated: pg:booktest@localhost=> select ':FOO'; ?column? +----------+ :FOO (1 rows) pg:booktest@localhost=> \\p select ':FOO'; pg:booktest@localhost=> Backtick'd parameters Meta (\\ ) commands support backticks on parameters: (not connected)=> \\echo Welcome `echo $USER` -- 'currently:' \"(\" `date` \")\" Welcome ken -- currently: ( Wed Jun 13 12:10:27 WIB 2018 ) (not connected)=> Backtick'd parameters will be passed to the user's SHELL, exactly as written, and can be combined with \\set: pg:booktest@localhost=> \\set MYVAR `date` pg:booktest@localhost=> \\set MYVAR = 'Wed Jun 13 12:17:11 WIB 2018' pg:booktest@localhost=> \\echo :MYVAR Wed Jun 13 12:17:11 WIB 2018 pg:booktest@localhost=> Passwords usql supports reading passwords for databases from a .usqlpass file contained in the user's HOME directory at startup: $ cat $HOME/.usqlpass # format is: # protocol:host:port:dbname:user:pass postgres:*:*:*:booktest:booktest $ usql pg:// Connected with driver postgres (PostgreSQL 9.6.9) Type \"help\" for help. pg:booktest@=> Note: the .usqlpass file cannot be readable by other users. Please set the permissions accordingly: Runtime Configuration (RC) File usql supports executing a .usqlrc contained in the user's HOME directory: $ cat $HOME/.usqlrc \\echo WELCOME TO THE JUNGLE `date` \\set SYNTAX_HL_STYLE paraiso-dark $ usql WELCOME TO THE JUNGLE Thu Jun 14 02:36:53 WIB 2018 Type \"help\" for help. (not connected)=> \\set SYNTAX_HL_STYLE = 'paraiso-dark' (not connected)=> The .usqlrc file is read by usql at startup in the same way as a file passed on the command-line with -f / --file. It is commonly used to set startup environment variables and settings. You can temporarily disable the RC-file by passing -X or --no-rc on the command-line: Host Connection Information By default, usql displays connection information when connecting to a database. This might cause problems with some databases or connections. This can be disabled by setting the system environment variable USQL_SHOW_HOST_INFORMATION to false: $ export USQL_SHOW_HOST_INFORMATION=false $ usql pg://booktest@localhost Type \"help\" for help. pg:booktest@=> SHOW_HOST_INFORMATION is a standard usql variable, and can be \\set or \\unset. Additionally, it can be passed via the command-line using -v or --set: $ usql --set SHOW_HOST_INFORMATION=false pg:// Type \"help\" for help. pg:booktest@=> \\set SHOW_HOST_INFORMATION true pg:booktest@=> \\connect pg:// Connected with driver postgres (PostgreSQL 9.6.9) pg:booktest@=> Syntax Highlighting Interactive queries will be syntax highlighted by default, using Chroma. There are a number of variables that control syntax highlighting: Variable Default Values Description SYNTAX_HL true true or false enables syntax highlighting SYNTAX_HL_FORMAT dependent on terminal support formatter name Chroma formatter name SYNTAX_HL_OVERRIDE_BG true true or false enables overriding the background color of the chroma styles SYNTAX_HL_STYLE monokai style name Chroma style name Time Formatting Some databases support time/date columns that support formatting. By default, usql formats time/date columns as RFC3339Nano, and can be set using \\pset time <FORMAT>: $ usql pg:// Connected with driver postgres (PostgreSQL 13.2 (Debian 13.2-1.pgdg100+1)) Type \"help\" for help. pg:postgres@=> \\pset time RFC3339Nano pg:postgres@=> select now(); now ----------------------------- 2021-05-01T22:21:44.710385Z (1 row) pg:postgres@=> \\pset time Kitchen Time display is \"Kitchen\" (\"3:04PM\"). pg:postgres@=> select now(); now --------- 10:22PM (1 row) pg:postgres@=> Any Go supported time format or the standard Go const name (for example, Kitchen, in the above). Constants Constant Name Value ANSIC Mon Jan _2 15:04:05 2006 UnixDate Mon Jan _2 15:04:05 MST 2006 RubyDate Mon Jan 02 15:04:05 -0700 2006 RFC822 02 Jan 06 15:04 MST RFC822Z 02 Jan 06 15:04 -0700 RFC850 Monday, 02-Jan-06 15:04:05 MST RFC1123 Mon, 02 Jan 2006 15:04:05 MST RFC1123Z Mon, 02 Jan 2006 15:04:05 -0700 RFC3339 2006-01-02T15:04:05Z07:00 RFC3339Nano 2006-01-02T15:04:05.999999999Z07:00 Kitchen 3:04PM Stamp Jan _2 15:04:05 StampMilli Jan _2 15:04:05.000 StampMicro Jan _2 15:04:05.000000 StampNano Jan _2 15:04:05.000000000 Copy usql implements the \\copy command that reads data from a database connection and writes it into another one. It requires 4 parameters: source connection string destination connection string source query destination table name, optionally with columns Connection strings support same syntax as in \\connect. Source query needs to be quoted. Source query must select same number of columns and in same order as they're defined in the destination table, unless they're specified for the destination, as table_name(column1, column2, ...). Quote the whole expression, if it contains spaces. \\copy does not attempt to perform any data type conversion. Use CAST in the source query to ensure data types compatible with destination table. Some drivers may have limited data type support, and they might not work at all when combined with other limited drivers. Unlike psql, \\copy in usql cannot read data directly from files. Drivers like csvq can help with this, since they support reading CSV and JSON files. $ cat books.csv book_id,author_id,isbn,title,year,available,tags 3,1,3,one,2018,\"2018-06-01 00:00:00\",{} 4,2,4,two,2019,\"2019-06-01 00:00:00\",{} $ usql -c \"\\copy csvq://. sqlite3://test.db 'select * from books' 'books'\" Copied 2 rows Note that it might be a better idea to use tools dedicated to the destination database to load data in a robust way. \\copy reads data from plain SELECT queries. Most drivers that have \\copy enabled use INSERT statements, except for PostgreSQL ones, which use COPY TO. Because data needs to be downloaded from one database and uploaded into another, don't expect same performance as in psql. For loading large amount of data efficiently, use tools native to the destination database. You can use \\copy with variables. Better yet, put those \\set commands in your runtime configuration file at $HOME/.usqlrc and passwords at $HOME/.usqlpass. $ usql Type \"help\" for help. (not connected)=> \\set pglocal postgres://postgres@localhost:49153?sslmode=disable (not connected)=> \\set oralocal godror://system@localhost:1521/orasid (not connected)=> \\copy :pglocal :oralocal 'select staff_id, first_name from staff' 'staff(staff_id, first_name)' Contributing usql is currently a WIP, and is aiming towards a 1.0 release soon. Well-written PRs are always welcome -- and there is a clear backlog of issues marked help wanted on the GitHub issue tracker! Please pick up an issue today, and submit a PR tomorrow! For more technical details, see CONTRIBUTING.md. Related Projects dburl - Go package providing a standard, URL-style mechanism for parsing and opening database connection URLs xo - Go command-line tool to generate Go code from a database schema "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/xo/usql",
        "comments.comment_id": [21509885, 21509956],
        "comments.comment_author": ["the_duke", "lwb"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-11-11T23:05:33Z",
          "2019-11-11T23:14:50Z"
        ],
        "comments.comment_text": [
          "I have been looking for a faster alternative to pgcli [1] ( with  features like auto-complete for table names and columns) and was getting excited.<p>Sadly this is \"just\" a plain old CLI.<p>Also: had to install with `go get -tags \"no_sqlite3\" -u github.com/xo/usql` since the sqlite3 package did not build.<p>[1] <a href=\"https://www.pgcli.com/\" rel=\"nofollow\">https://www.pgcli.com/</a>",
          "I was really hoping for \\dt but looks like they haven't implemented it yet. The number one most common thing I look up when I'm using a SQL variant is how to show the tables. I suppose there would be complications with NoSQL but you could just show available collections or whatever else it maps to in that case."
        ],
        "id": "f74a941f-ee70-43cd-928b-185680f7f820",
        "url_text": "Installing | Building | Using | Database Support | Features and Compatibility | Releases | Contributing usql is a universal command-line interface for PostgreSQL, MySQL, Oracle Database, SQLite3, Microsoft SQL Server, and many other databases including NoSQL and non-relational databases! usql provides a simple way to work with SQL and NoSQL databases via a command-line inspired by PostgreSQL's psql. usql supports most of the core psql features, such as variables, backticks, and commands and has additional features that psql does not, such as syntax highlighting, context-based completion, and multiple database support. Database administrators and developers that would prefer to work with a tool like psql with non-PostgreSQL databases, will find usql intuitive, easy-to-use, and a great replacement for the command-line clients/tools for other databases. Installing usql can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the usql or usql.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) macOS Notes The recommended installation method on macOS is via brew (see below). If the following or similar error is encountered when attempting to run usql: $ usql dyld: Library not loaded: /usr/local/opt/icu4c/lib/libicuuc.68.dylib Referenced from: /Users/user/.local/bin/usql Reason: image not found Abort trap: 6 Then the ICU lib needs to be installed. This can be accomplished using brew: Installing via Homebrew (macOS and Linux) usql is available in the xo/xo tap, and can be installed in the usual way with the brew command: # install usql with \"most\" drivers $ brew install xo/xo/usql Additional support for ODBC databases can be installed by passing --with-odbc option during install: # install usql with odbc support $ brew install --with-odbc usql Installing via Scoop (Windows) usql can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install usql Installing via Go usql can be installed in the usual Go fashion: # install usql from master branch with basic database support # includes PostgreSQL, Oracle Database, MySQL, MS SQL, and SQLite3 drivers $ go install github.com/xo/usql@master Building When building usql with Go, only drivers for PostgreSQL, MySQL, SQLite3 and Microsoft SQL Server will be enabled by default. Other databases can be enabled by specifying the build tag for their database driver. Additionally, the most and all build tags include most, and all SQL drivers, respectively: # install all drivers $ go install -tags all github.com/xo/usql@master # install with most drivers (excludes unsupported drivers) $ go install -tags most github.com/xo/usql@master # install with base drivers and additional support for Oracle Database and ODBC $ go install -tags 'godror odbc' github.com/xo/usql@master For every build tag <driver>, there is also a no_<driver> build tag disabling the driver: # install all drivers excluding avatica and couchbase $ go install -tags 'all no_avatica no_couchbase' github.com/xo/usql@master Release Builds Release builds are built with the most build tag. Additional SQLite3 build tags are also specified for releases. Embedding An effort has been made to keep usql's packages modular, and reusable by other developers wishing to leverage the usql code base. As such, it is possible to embed or create a SQL command-line interface (e.g, for use by some other project as an \"official\" client) using the core usql source tree. Please refer to main.go to see how usql puts together its packages. usql's code is also well-documented -- please refer to the Go reference for an overview of the various packages and APIs. Database Support usql works with all Go standard library compatible SQL drivers supported by github.com/xo/dburl. The list of drivers that usql was built with can be displayed using the \\drivers command: $ cd $GOPATH/src/github.com/xo/usql $ export GO111MODULE=on # build excluding the base drivers, and including cassandra and moderncsqlite $ go build -tags 'no_postgres no_oracle no_sqlserver no_sqlite3 cassandra moderncsqlite' # show built driver support $ ./usql -c '\\drivers' Available Drivers: cql [ca, scy, scylla, datastax, cassandra] memsql (mysql) [me] moderncsqlite [mq, sq, file, sqlite, sqlite3, modernsqlite] mysql [my, maria, aurora, mariadb, percona] tidb (mysql) [ti] vitess (mysql) [vt] The above shows that usql was built with only the mysql, cassandra (ie, cql), and moderncsqlite drivers. The output above reflects information about the drivers available to usql, specifically the internal driver name, its primary URL scheme, the driver's available scheme aliases (shown in [...]), and the real/underlying driver (shown in (...)) for wire compatible drivers. Supported Database Schemes and Aliases The following are the Go SQL drivers that usql supports, the associated database, scheme / build tag, and scheme aliases: Database Scheme / Tag Scheme Aliases Driver Package / Notes Microsoft SQL Server sqlserver ms, mssql github.com/denisenkom/go-mssqldb MySQL mysql my, maria, aurora, mariadb, percona github.com/go-sql-driver/mysql Oracle Database oracle or, ora, oci, oci8, odpi, odpi-c github.com/sijms/go-ora/v2 PostgreSQL postgres pg, pgsql, postgresql github.com/lib/pq SQLite3 sqlite3 sq, file, sqlite github.com/mattn/go-sqlite3 Alibaba MaxCompute maxcompute mc sqlflow.org/gomaxcompute Apache Avatica avatica av, phoenix github.com/apache/calcite-avatica-go/v5 Apache H2 h2 github.com/jmrobles/h2go Apache Ignite ignite ig, gridgain github.com/amsokol/ignite-go-client/sql AWS Athena athena s3, aws github.com/uber/athenadriver/go Cassandra cassandra ca, scy, scylla, datastax, cql github.com/MichaelS11/go-cql-driver ClickHouse clickhouse ch github.com/ClickHouse/clickhouse-go Couchbase couchbase n1, n1ql github.com/couchbase/go_n1ql CSVQ csvq cs, csv, tsv, json github.com/mithrandie/csvq-driver Cznic QL ql cznic, cznicql modernc.org/ql Exasol exasol ex, exa github.com/exasol/exasol-driver-go Firebird firebird fb, firebirdsql github.com/nakagami/firebirdsql Genji genji gj github.com/genjidb/genji/driver Google BigQuery bigquery bq gorm.io/driver/bigquery/driver Google Spanner spanner sp github.com/cloudspannerecosystem/go-sql-spanner Microsoft ADODB adodb ad, ado github.com/mattn/go-adodb ModernC SQLite3 moderncsqlite mq, modernsqlite modernc.org/sqlite MySQL MyMySQL mymysql zm, mymy github.com/ziutek/mymysql/godrv Netezza netezza nz, nzgo github.com/IBM/nzgo PostgreSQL PGX pgx px github.com/jackc/pgx/v4/stdlib Presto presto pr, prs, prestos, prestodb, prestodbs github.com/prestodb/presto-go-client/presto SAP ASE sapase ax, ase, tds github.com/thda/tds SAP HANA saphana sa, sap, hana, hdb github.com/SAP/go-hdb/driver Trino trino tr, trs, trinos github.com/trinodb/trino-go-client/trino Vertica vertica ve github.com/vertica/vertica-sql-go VoltDB voltdb vo, vdb, volt github.com/VoltDB/voltdb-client-go/voltdbclient Apache Hive hive hi sqlflow.org/gohive Apache Impala impala im github.com/bippio/go-impala Azure CosmosDB cosmos cm github.com/btnguyen2k/gocosmos GO DRiver for ORacle godror gr github.com/godror/godror ODBC odbc od github.com/alexbrainman/odbc Snowflake snowflake sf github.com/snowflakedb/gosnowflake Amazon Redshift postgres rs, redshift github.com/lib/pq CockroachDB postgres cr, cdb, crdb, cockroach, cockroachdb github.com/lib/pq OLE ODBC adodb oo, ole, oleodbc github.com/mattn/go-adodb SingleStore MemSQL mysql me, memsql github.com/go-sql-driver/mysql TiDB mysql ti, tidb github.com/go-sql-driver/mysql Vitess Database mysql vt, vitess github.com/go-sql-driver/mysql NO DRIVERS no_base no base drivers (useful for development) MOST DRIVERS most all stable drivers ALL DRIVERS all all drivers NO <TAG> no_<tag> exclude driver with <tag> Requires CGO Wire compatible (see respective driver) Any of the protocol schemes/aliases shown above can be used in conjunction when connecting to a database via the command-line or with the \\connect command: # connect to a vitess database: $ usql vt://user:pass@host:3306/mydatabase $ usql (not connected)=> \\c vitess://user:pass@host:3306/mydatabase See the section below on connecting to databases for further details building DSNs/URLs for use with usql. Using After installing, usql can be used similarly to the following: # connect to a postgres database $ usql postgres://booktest@localhost/booktest # connect to an oracle database $ usql oracle://user:pass@host/oracle.sid # connect to a postgres database and run the commands contained in script.sql $ usql pg://localhost/ -f script.sql Command-line Options Supported command-line options: $ usql --help usql, the universal command-line interface for SQL databases Usage: usql [OPTIONS]... [DSN] Arguments: DSN database url Options: -c, --command=COMMAND ... run only single command (SQL or internal) and exit -f, --file=FILE ... execute commands from file and exit -w, --no-password never prompt for password -X, --no-rc do not read start up file -o, --out=OUT output file -W, --password force password prompt (should happen automatically) -1, --single-transaction execute as a single transaction (if non-interactive) -v, --set=, --variable=NAME=VALUE ... set variable NAME to VALUE -P, --pset=VAR[=ARG] ... set printing option VAR to ARG (see \\pset command) -F, --field-separator=FIELD-SEPARATOR ... field separator for unaligned output (default, \"|\") -R, --record-separator=RECORD-SEPARATOR ... record separator for unaligned output (default, \\n) -T, --table-attr=TABLE-ATTR ... set HTML table tag attributes (e.g., width, border) -A, --no-align unaligned table output mode -H, --html HTML table output mode -t, --tuples-only print rows only -x, --expanded turn on expanded table output -z, --field-separator-zero set field separator for unaligned output to zero byte -0, --record-separator-zero set record separator for unaligned output to zero byte -J, --json JSON output mode -C, --csv CSV output mode -G, --vertical vertical output mode -V, --version display version and exit Connecting to Databases usql opens a database connection by parsing a URL and passing the resulting connection string to a database driver. Database connection strings (aka \"data source name\" or DSNs) have the same parsing rules as URLs, and can be passed to usql via command-line, or to the \\connect or \\c commands. Connection strings look like the following: driver+transport://user:pass@host/dbname?opt1=a&opt2=b driver:/path/to/file /path/to/file Where the above are: Component Description driver driver scheme name or scheme alias transport tcp, udp, unix or driver name (for ODBC and ADODB) user username pass password host hostname dbname database name, instance, or service name/ID ?opt1=a&... additional database driver options (see respective SQL driver for available options) /path/to/file a path on disk Some databases, such as Microsoft SQL Server, or Oracle Database support a path component (ie, /dbname) in the form of /instance/dbname, where /instance is the optional service identifier (aka \"SID\") or database instance Driver Aliases usql supports the same driver names and aliases from the dburl package. Most databases have at least one or more alias - please refer to the dburl documentation for all supported aliases. Short Aliases All database drivers have a two character short form that is usually the first two letters of the database driver. For example, pg for postgres, my for mysql, ms for sqlserver (formerly known as mssql), or for oracle, or sq for sqlite3. Passing Driver Options Driver options are specified as standard URL query options in the form of ?opt1=a&obt2=b. Please refer to the relevant database driver's documentation for available options. Paths on Disk If a URL does not have a driver: scheme, usql will check if it is a path on disk. If the path exists, usql will attempt to use an appropriate database driver to open the path. If the specified path is a Unix Domain Socket, usql will attempt to open it using the MySQL driver. If the path is a directory, usql will attempt to open it using the PostgreSQL driver. If the path is a regular file, usql will attempt to open the file using the SQLite3 driver. Driver Defaults As with URLs, most components in the URL are optional and many components can be left out. usql will attempt connecting using defaults where possible: # connect to postgres using the local $USER and the unix domain socket in /var/run/postgresql $ usql pg:// Please see documentation for the database driver you are connecting with for more information. Connection Examples The following are example connection strings and additional ways to connect to databases using usql: # connect to a postgres database $ usql pg://user:pass@host/dbname $ usql pgsql://user:pass@host/dbname $ usql postgres://user:pass@host:port/dbname $ usql pg:// $ usql /var/run/postgresql $ usql pg://user:pass@host/dbname?sslmode=disable # Connect without SSL # connect to a mysql database $ usql my://user:pass@host/dbname $ usql mysql://user:pass@host:port/dbname $ usql my:// $ usql /var/run/mysqld/mysqld.sock # connect to a sqlserver database $ usql sqlserver://user:pass@host/instancename/dbname $ usql ms://user:pass@host/dbname $ usql ms://user:pass@host/instancename/dbname $ usql mssql://user:pass@host:port/dbname $ usql ms:// # connect to a sqlserver database using Windows domain authentication $ runas /user:ACME\\wiley /netonly \"usql mssql://host/dbname/\" # connect to a oracle database $ usql or://user:pass@host/sid $ usql oracle://user:pass@host:port/sid $ usql or:// # connect to a cassandra database $ usql ca://user:pass@host/keyspace $ usql cassandra://host/keyspace $ usql cql://host/ $ usql ca:// # connect to a sqlite database that exists on disk $ usql dbname.sqlite3 # NOTE: when connecting to a SQLite database, if the \"<driver>://\" or # \"<driver>:\" scheme/alias is omitted, the file must already exist on disk. # # if the file does not yet exist, the URL must incorporate file:, sq:, sqlite3:, # or any other recognized sqlite3 driver alias to force usql to create a new, # empty database at the specified path: $ usql sq://path/to/dbname.sqlite3 $ usql sqlite3://path/to/dbname.sqlite3 $ usql file:/path/to/dbname.sqlite3 # connect to a adodb ole resource (windows only) $ usql adodb://Microsoft.Jet.OLEDB.4.0/myfile.mdb $ usql \"adodb://Microsoft.ACE.OLEDB.12.0/?Extended+Properties=\\\"Text;HDR=NO;FMT=Delimited\\\"\" # connect with ODBC driver (requires building with odbc tag) $ cat /etc/odbcinst.ini [DB2] Description=DB2 driver Driver=/opt/db2/clidriver/lib/libdb2.so FileUsage = 1 DontDLClose = 1 [PostgreSQL ANSI] Description=PostgreSQL ODBC driver (ANSI version) Driver=psqlodbca.so Setup=libodbcpsqlS.so Debug=0 CommLog=1 UsageCount=1 # connect to db2, postgres databases using ODBC $ usql odbc+DB2://user:pass@localhost/dbname $ usql odbc+PostgreSQL+ANSI://user:pass@localhost/dbname?TraceFile=/path/to/trace.log Executing Queries and Commands The interactive intrepreter reads queries and meta (\\ ) commands, sending the query to the connected database: $ usql sqlite://example.sqlite3 Connected with driver sqlite3 (SQLite3 3.17.0) Type \"help\" for help. sq:example.sqlite3=> create table test (test_id int, name string); CREATE TABLE sq:example.sqlite3=> insert into test (test_id, name) values (1, 'hello'); INSERT 1 sq:example.sqlite3=> select * from test; test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> select * from test sq:example.sqlite3-> \\p select * from test sq:example.sqlite3-> \\g test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> \\c postgres://booktest@localhost error: pq: 28P01: password authentication failed for user \"booktest\" Enter password: Connected with driver postgres (PostgreSQL 9.6.6) pg:booktest@localhost=> select * from authors; author_id | name +-----------+----------------+ 1 | Unknown Master 2 | blah 3 | aoeu (3 rows) pg:booktest@localhost=> Commands may accept one or more parameter, and can be quoted using either ' or \". Command parameters may also be backtick'd. Backslash Commands Currently available commands: $ usql Type \"help\" for help. (not connected)=> \\? General \\q quit usql \\copyright show usql usage and distribution terms \\drivers display information about available database drivers Query Execute \\g [(OPTIONS)] [FILE] or ; execute query (and send results to file or |pipe) \\crosstabview [(OPTIONS)] [COLUMNS] execute query and display results in crosstab \\G [(OPTIONS)] [FILE] as \\g, but forces vertical output mode \\gexec execute query and execute each value of the result \\gset [PREFIX] execute query and store results in usql variables \\gx [(OPTIONS)] [FILE] as \\g, but forces expanded output mode \\watch [(OPTIONS)] [DURATION] execute query every specified interval Query Buffer \\e [FILE] [LINE] edit the query buffer (or file) with external editor \\p show the contents of the query buffer \\raw show the raw (non-interpolated) contents of the query buffer \\r reset (clear) the query buffer \\w FILE write query buffer to file Help \\? [commands] show help on backslash commands \\? options show help on usql command-line options \\? variables show help on special variables Input/Output \\echo [-n] [STRING] write string to standard output (-n for no newline) \\qecho [-n] [STRING] write string to \\o output stream (-n for no newline) \\warn [-n] [STRING] write string to standard error (-n for no newline) \\o [FILE] send all query results to file or |pipe \\i FILE execute commands from file \\ir FILE as \\i, but relative to location of current script Informational \\d[S+] [NAME] list tables, views, and sequences or describe table, view, sequence, or index \\da[S+] [PATTERN] list aggregates \\df[S+] [PATTERN] list functions \\di[S+] [PATTERN] list indexes \\dm[S+] [PATTERN] list materialized views \\dn[S+] [PATTERN] list schemas \\ds[S+] [PATTERN] list sequences \\dt[S+] [PATTERN] list tables \\dv[S+] [PATTERN] list views \\l[+] list databases \\ss[+] [TABLE|QUERY] [k] show stats for a table or a query Formatting \\pset [NAME [VALUE]] set table output option \\a toggle between unaligned and aligned output mode \\C [STRING] set table title, or unset if none \\f [STRING] show or set field separator for unaligned query output \\H toggle HTML output mode \\T [STRING] set HTML <table> tag attributes, or unset if none \\t [on|off] show only rows \\x [on|off|auto] toggle expanded output Transaction \\begin begin a transaction \\commit commit current transaction \\rollback rollback (abort) current transaction Connection \\c URL connect to database with url \\c DRIVER PARAMS... connect to database with SQL driver and parameters \\Z close database connection \\password [USERNAME] change the password for a user \\conninfo display information about the current database connection Operating System \\cd [DIR] change the current working directory \\setenv NAME [VALUE] set or unset environment variable \\! [COMMAND] execute command in shell or start interactive shell \\timing [on|off] toggle timing of commands Variables \\prompt [-TYPE] <VAR> [PROMPT] prompt user to set variable \\set [NAME [VALUE]] set internal variable, or list all if no parameters \\unset NAME unset (delete) internal variable Features and Compatibility The usql project's goal is to support all standard psql commands and features. Pull Requests are always appreciated! Variables and Interpolation usql supports client-side interpolation of variables that can be \\set and \\unset: $ usql (not connected)=> \\set (not connected)=> \\set FOO bar (not connected)=> \\set FOO = 'bar' (not connected)=> \\unset FOO (not connected)=> \\set (not connected)=> A \\set variable, NAME, will be directly interpolated (by string substitution) into the query when prefixed with : and optionally surrounded by quotation marks (' or \"): pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from authors where name = :'FOO'; author_id | name +-----------+------+ 7 | bar (1 rows) The three forms, :NAME, :'NAME', and :\"NAME\", are used to interpolate a variable in parts of a query that may require quoting, such as for a column name, or when doing concatenation in a query: pg:booktest@localhost=> \\set TBLNAME authors pg:booktest@localhost=> \\set COLNAME name pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\p select * from authors where \"name\" = 'bar' pg:booktest@localhost-> \\raw select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\g author_id | name +-----------+------+ 7 | bar (1 rows) pg:booktest@localhost=> Note: variables contained within other strings will NOT be interpolated: pg:booktest@localhost=> select ':FOO'; ?column? +----------+ :FOO (1 rows) pg:booktest@localhost=> \\p select ':FOO'; pg:booktest@localhost=> Backtick'd parameters Meta (\\ ) commands support backticks on parameters: (not connected)=> \\echo Welcome `echo $USER` -- 'currently:' \"(\" `date` \")\" Welcome ken -- currently: ( Wed Jun 13 12:10:27 WIB 2018 ) (not connected)=> Backtick'd parameters will be passed to the user's SHELL, exactly as written, and can be combined with \\set: pg:booktest@localhost=> \\set MYVAR `date` pg:booktest@localhost=> \\set MYVAR = 'Wed Jun 13 12:17:11 WIB 2018' pg:booktest@localhost=> \\echo :MYVAR Wed Jun 13 12:17:11 WIB 2018 pg:booktest@localhost=> Passwords usql supports reading passwords for databases from a .usqlpass file contained in the user's HOME directory at startup: $ cat $HOME/.usqlpass # format is: # protocol:host:port:dbname:user:pass postgres:*:*:*:booktest:booktest $ usql pg:// Connected with driver postgres (PostgreSQL 9.6.9) Type \"help\" for help. pg:booktest@=> Note: the .usqlpass file cannot be readable by other users. Please set the permissions accordingly: Runtime Configuration (RC) File usql supports executing a .usqlrc contained in the user's HOME directory: $ cat $HOME/.usqlrc \\echo WELCOME TO THE JUNGLE `date` \\set SYNTAX_HL_STYLE paraiso-dark $ usql WELCOME TO THE JUNGLE Thu Jun 14 02:36:53 WIB 2018 Type \"help\" for help. (not connected)=> \\set SYNTAX_HL_STYLE = 'paraiso-dark' (not connected)=> The .usqlrc file is read by usql at startup in the same way as a file passed on the command-line with -f / --file. It is commonly used to set startup environment variables and settings. You can temporarily disable the RC-file by passing -X or --no-rc on the command-line: Host Connection Information By default, usql displays connection information when connecting to a database. This might cause problems with some databases or connections. This can be disabled by setting the system environment variable USQL_SHOW_HOST_INFORMATION to false: $ export USQL_SHOW_HOST_INFORMATION=false $ usql pg://booktest@localhost Type \"help\" for help. pg:booktest@=> SHOW_HOST_INFORMATION is a standard usql variable, and can be \\set or \\unset. Additionally, it can be passed via the command-line using -v or --set: $ usql --set SHOW_HOST_INFORMATION=false pg:// Type \"help\" for help. pg:booktest@=> \\set SHOW_HOST_INFORMATION true pg:booktest@=> \\connect pg:// Connected with driver postgres (PostgreSQL 9.6.9) pg:booktest@=> Syntax Highlighting Interactive queries will be syntax highlighted by default, using Chroma. There are a number of variables that control syntax highlighting: Variable Default Values Description SYNTAX_HL true true or false enables syntax highlighting SYNTAX_HL_FORMAT dependent on terminal support formatter name Chroma formatter name SYNTAX_HL_OVERRIDE_BG true true or false enables overriding the background color of the chroma styles SYNTAX_HL_STYLE monokai style name Chroma style name Time Formatting Some databases support time/date columns that support formatting. By default, usql formats time/date columns as RFC3339Nano, and can be set using \\pset time <FORMAT>: $ usql pg:// Connected with driver postgres (PostgreSQL 13.2 (Debian 13.2-1.pgdg100+1)) Type \"help\" for help. pg:postgres@=> \\pset time RFC3339Nano pg:postgres@=> select now(); now ----------------------------- 2021-05-01T22:21:44.710385Z (1 row) pg:postgres@=> \\pset time Kitchen Time display is \"Kitchen\" (\"3:04PM\"). pg:postgres@=> select now(); now --------- 10:22PM (1 row) pg:postgres@=> Any Go supported time format or the standard Go const name (for example, Kitchen, in the above). Constants Constant Name Value ANSIC Mon Jan _2 15:04:05 2006 UnixDate Mon Jan _2 15:04:05 MST 2006 RubyDate Mon Jan 02 15:04:05 -0700 2006 RFC822 02 Jan 06 15:04 MST RFC822Z 02 Jan 06 15:04 -0700 RFC850 Monday, 02-Jan-06 15:04:05 MST RFC1123 Mon, 02 Jan 2006 15:04:05 MST RFC1123Z Mon, 02 Jan 2006 15:04:05 -0700 RFC3339 2006-01-02T15:04:05Z07:00 RFC3339Nano 2006-01-02T15:04:05.999999999Z07:00 Kitchen 3:04PM Stamp Jan _2 15:04:05 StampMilli Jan _2 15:04:05.000 StampMicro Jan _2 15:04:05.000000 StampNano Jan _2 15:04:05.000000000 Copy usql implements the \\copy command that reads data from a database connection and writes it into another one. It requires 4 parameters: source connection string destination connection string source query destination table name, optionally with columns Connection strings support same syntax as in \\connect. Source query needs to be quoted. Source query must select same number of columns and in same order as they're defined in the destination table, unless they're specified for the destination, as table_name(column1, column2, ...). Quote the whole expression, if it contains spaces. \\copy does not attempt to perform any data type conversion. Use CAST in the source query to ensure data types compatible with destination table. Some drivers may have limited data type support, and they might not work at all when combined with other limited drivers. Unlike psql, \\copy in usql cannot read data directly from files. Drivers like csvq can help with this, since they support reading CSV and JSON files. $ cat books.csv book_id,author_id,isbn,title,year,available,tags 3,1,3,one,2018,\"2018-06-01 00:00:00\",{} 4,2,4,two,2019,\"2019-06-01 00:00:00\",{} $ usql -c \"\\copy csvq://. sqlite3://test.db 'select * from books' 'books'\" Copied 2 rows Note that it might be a better idea to use tools dedicated to the destination database to load data in a robust way. \\copy reads data from plain SELECT queries. Most drivers that have \\copy enabled use INSERT statements, except for PostgreSQL ones, which use COPY TO. Because data needs to be downloaded from one database and uploaded into another, don't expect same performance as in psql. For loading large amount of data efficiently, use tools native to the destination database. You can use \\copy with variables. Better yet, put those \\set commands in your runtime configuration file at $HOME/.usqlrc and passwords at $HOME/.usqlpass. $ usql Type \"help\" for help. (not connected)=> \\set pglocal postgres://postgres@localhost:49153?sslmode=disable (not connected)=> \\set oralocal godror://system@localhost:1521/orasid (not connected)=> \\copy :pglocal :oralocal 'select staff_id, first_name from staff' 'staff(staff_id, first_name)' Contributing usql is currently a WIP, and is aiming towards a 1.0 release soon. Well-written PRs are always welcome -- and there is a clear backlog of issues marked help wanted on the GitHub issue tracker! Please pick up an issue today, and submit a PR tomorrow! For more technical details, see CONTRIBUTING.md. Related Projects dburl - Go package providing a standard, URL-style mechanism for parsing and opening database connection URLs xo - Go command-line tool to generate Go code from a database schema ",
        "_version_": 1718536541682270208
      },
      {
        "story_id": 21551868,
        "story_author": "UkiahSmith",
        "story_descendants": 79,
        "story_score": 147,
        "story_time": "2019-11-16T09:54:30Z",
        "story_title": "The Configuration Complexity Curse",
        "search": [
          "The Configuration Complexity Curse",
          "https://blog.cedriccharly.com/post/20191109-the-configuration-complexity-curse/",
          "Read the discussion on Hacker News, Reddit, and Lobste.rsDont be a YAML EngineerImagine that you are a new software engineer entering the industry. You thought you were ready after studying your theory and the weekend side projects. Now, you get hit with a wave of new tools and concepts out of nowhere. Microservices? REST? Cloud Computing? RPC (Whats an IDL)? Docker (Whats a container)? Kubernetes? Continuous Integration? Continuous Deployment? Even for veterans, like a frog in slowly boiling water, you look up one day and realize things have become complicated.How did this happen? Each of these things looks useful in isolation, but now you have to figure out how to use it best for yourself as well as with these other tools in concert. First a tool must be configured, usually in YAML (so much YAML). Now this tool needs to be integrated to work with everything else. If you are lucky, you may have an internal platform team to package and abstract the complexity for you. Otherwise, you are going to end up with a rube goldbergian system that wraps all these tools so you can coordinate configuration between everything just to maintain sanity.I think there is light at the end of the tunnel here, but even leading edge projects like Kubernetes suffer from this industry wide pathology we experience building software platforms.Kubernetes: Its Turtles All The Way DownYou start by hand writing YAML files and pushing them to a cluster with kubectl. Before resting on your laurels as a newly minted distributed systems engineer, you realize that you need a way to pass dynamic values to your YAML file. Enter the tool explosion; Nod along if you have ever done or considered using one of these techniques:Text Templating: Many are tempted to reach for text templating libraries like Jinja, etc. Most have experience using it for templating HTML, why not use it here? At this point you can get by with just simple scripts to template values then push to the cluster, or just go with something off the shelf. Trying to write a configuration file (basically a data structure described in text) with a text templating library is the road to tragedy. As anyone who tried to write a helm chart template knows, templates quickly become hairy and incredibly fragile. Because you are working with the template at the textual level, a template writer lacks the tools to build abstractions around the data itself and small things like indentation can break your template. I think I need to repeat this: Do not use text templating for data configuration.Data Layering: This is when you build an overlay inheritance mechanism on top of YAML, keeping the raw YAML but wrapping it in a tool that can merge these separate documents according to its own rules. kustomize would be the most well known tool now it that is it integrated into kubectl. This seems to work well enough and could be feasible for simple use cases. In comparison to data configuration languages though, this strategy breaks down when configurations grow in complexity and scale. Template writers lack abstraction and type validation that matters when things get complicated, as the semantics are locked into an opaque tool and not exposed as language features.Data Configuration Language (DCL): This is the most advanced form of data configuration we have today, if you absolutely need to manage configuration for a system now I would recommend one of these tools. If the previous techniques are what I call YAML Engineering, DCLs would be actual languages designed for defining data and working with data at a semantic level. Examples of DCLs are jsonnet (used in kubecfg/ksonnet), dhall (used in dhall-kubernetes), and starlark (used in isopod). Google internally uses a tool called borgcfg (which inspired kubecfg) which evaluates configurations in a jsonnet-like internal language to configure Borg (which inspired Kubernetes) deployments. As systems scale in complexity and size, DCLs have abstractions like types, functions, comprehensions, and conditionals to manage and create reusable configurations.So, if you want to anything more advanced than straight YAML you will need to adopt one of these approaches and integrate it with your development and release process. That will help as long as you are working directly with Kubernetes, but what about configuring your underlying infrastructure? If you are on a cloud platform like AWS or Azure, even a managed Kubernetes deployment requires a tool like CloudFormation or Terraform, which has or needs its own distinct abstractions to manage resources at that level of the stack. You cannot reuse the tooling invested in configuring Kubernetes and must yet again integrate another system for configuring the cloud provider at a lower level. The sound you hear is a turtle being stacked on another turtle.Complex Systems == Complex ConfigurationThis example with Kubernetes shows how difficult it is to manage configuration, but Kubernetes is just one of many declarative systems that show this problem. Any declarative systems and tools that grow sufficiently in size and complexity need to be abstracted but we lack common tools to do so. You may not think a Terraform configuration needs to be abstracted, but when dealing with many teams that need to deploy infrastructure in a consistent way, having reusable abstractions is incredibly valuable. Same for Continuous Integration. Maintaining a YAML file may be fine for a single project, but when your organization has hundreds of services, generating a consistent set of automation and checks is worth the effort.The main argument of this essay hits upon the cause of this issue:As systems grow and multiply in order to run at scale, the configuration also grows in size and complexity. The industry lacks the right tools to manage this growing configuration complexity.Innovation in platforms, like the cloud and Kubernetes, makes things that used to be too difficult and complex for a small team now just a simple API call away. But the loop needs to be closed on how we can build libraries that any organization can use to have a base of best practices for any platform. Even current DCLs, which I consider closer to the model we should use, lack key pieces to manage configuration at scale.CUE: Configure, Unify, ExecuteWe need a way to manage configuration that grows in size and needs to be used in many different systems. I think a tool to solve this would need two key properties:The tool is a language that has the primitives needed to create and maintain reusable, large scale configuration.The tool can orchestrate and push configuration to many different systems without having to create a new, custom tool just for that combination.There are many languages and configuration languages that attempt to solve this problem. CUE is a (new) data configuration language that uses a novel approach to solving the issue with a vision to tackle both what is needed to do configuration at large scale with a way to finally avoid the all too common tool wrapping that we see today. The creator of cuelang (this guy) worked on borgcfg at Google, where the learnings of managing configuration across a large company drives much of what makes cuelang a solution for our configuration problems today.Why CUE?Configuration Needs A Different Programming ModelIn the Kubernetes example before, I left out the option of using a a general purpose programming language. CUE is not even the first configuration language out there. Why should we pick a completely new tool instead of reusing something that already exists?CUE is based on a different model of logic programming that makes it well suited for configuration. The languages foundations make tasks like validation, templating, querying, and code generation first class features. CUE is designed around graph unification where sets of types, and values can be modeled as directed graphs and then unified to the most specific representation of all graphs. In CUE, the graphs are the config structures and values of the same key will simplify to the most specific value. The graph unification model is used successfully in computational linguistics, where a grammar definition of a human language can be thought of as 100K line configuration.This idea is much more intuitive to see with an example. Lets look at an example CUE definition:// Schema municipality: { name: string pop: int capital: bool } // Schema & Data largeCapital: municipality largeCapital: { name: string pop: >5M capital: true } // Data moscow: largeCapital moscow: { name: \"Moscow\" pop: 11.92M capital: true } We can see three separate structs defined here with varying mixtures of types and values defined for each. The combination of types and values in a single struct is significant here. In CUE, types are values. This means that types can be assigned to fields and can be immediately used to constrain values in the configuration. You can also see that fields become more constrained towards concrete values with each struct. largeCapital is a municipiality with a new constraint on the population size. moscow is a largeCapital with concrete values (the most specific type) for all fields. Between structs largeCapital and moscow, largeCapital subsumes moscow and moscow is an instance of largeCapital.If you want to go deeper I recommend reading The Logic of CUE to understand the theoretical foundation and what makes CUE different from other configuration languages. You can also watch this video from the creator if you prefer that. I think trying to go deeper into the theory would be me poorly rewording the original article. I would rather go into how the foundation of CUE enables features that are useful for configuration.Type Checking And Boilerplate RemovalGoing back to what we need from a new configuration tool, types and abstractions are the largest factors in managing large scale configuration. With types, we express constraints on data and declare intent across potentially many users. Types protect users from errors when defining configuration and serves as automatic documentation. Taking an example from the website:Spec :: { kind: string name: { first: !=\"\" // must be specified and non-empty middle?: !=\"\" // optional, but must be non-empty when specified last: !=\"\" } // The minimum must be strictly smaller than the maximum and vice versa. minimum?: int & <maximum maximum?: int & >minimum } // A spec is of type Spec spec: Spec spec: { knid: \"Homo Sapiens\" // error, misspelled field name: first: \"Jane\" name: last: \"Doe\" } In CUE, we can see what fields are needed for a Spec type as well as the constraints on each. CUEs type system is expressive, where fields can be marked simply by their type, to specifying optionality and constraints from other fields as well. This mixing of types and values is underrated. In constrast with JSON, one needs to generate a separate specification format (like OpenAPI) with significant effort to document and validate the JSON being consumed. With CUE, configuration can be checked and validated during evaluation without extra effort or tooling.Most of the current configuration tooling, both general purpose and data languages, focuses on removing boilerplate/duplicate code to reduce verbosity. Many choose to use an override model like inheritance (defining base types and modifying) due to developer familarity with the paradigm. This is a key factor to why I think DCLs have not seen widespread use in the industry. Although it would seem to be an obvious model, inheritance has problems in both small and large scale configuration. For small projects, defining abstractions early can be a large upfront ask with small payoff. After all, to benefit from deduplication there needs to be duplicate configuration in the first place (For the sake of clarity abstraction too early can be counterproductive). With CUE, defining the config grants automatic validation and documentation right out the gate; Paired with default values you can immediately cut down the effort to reuse common data.For large scale projects, inheritance creates deep layers of abstractions; It becomes difficult to tell where values are coming from in a config, much more so if the language has multiple mechanisms for abstraction (inheritance, mixins). At a practical level, deep hierarchies are common in configuration, and subtrees near the leaves can be challenging to modify. CUE takes a different approach with graph unification and disallows overrides. This improves readability as deep layering is prevented from the beginning; One does not need to trace through multiple files to see where a value came from. Although CUE does not have inheritance, it does have the concept of a value being an instance of another (the value lattice). This model is less flexible, but in return we get great clarity. To take a simple example, imagine we have an Animal class and we want to define both a Dog and Cat class. With inheritance, we could define Animal as the base type then selectively override and add fields to represent each respective subclass. Although workable, if we wanted to go further and represent each breed of Cat and Dog we could quickly run into issues as each layer of the hierarchy can choose to modify data as it chooses. In CUEs approach, instead of trying to override data at each layer, we choose to model Cat and Dog as instances of Animal. For both types, we take Animals definition but instead add constraints to bound what defines each animal. Even though we have the same hierarchy of types, at each layer the data can only become more specific for that subtype. Thanks to that for breeds we only need to further constrain what makes a Dog a Corgi and at each level we can see how the data was constrained to allow the final value.Scripting: Inversion of ControlConfiguration is never created for its own sake. The purpose of all configuration is to be fed to another system to do useful work with it. Especially today, we need to manage configuration between a dizzying amount of tools and services: Software as a Service (SaaS) offerings, Cloud vendors, Continous Integration, build tools, package managersthe list goes on. In the beginning, piping config files to various tools such as kubectl may be enough to get the job done. Eventually though, these tasks end up as part of a greater workflow that needs to be automated. Marcel describes a problem he has seen many times:At first using a general config lang is cute, piping results to kubectl and what have you. But this gets cumbersome, so one writes scripts. Then, as scripts are often inconvenient to distribute and use, so one turns this into a command line tool. These tools are typically domain specific. So sooner or later, this tool needs to be part of a larger control flow, and the whole process starts again. Ive been guilty of writing a bunch of such tools.Marcel van Lohuizen, CUE SlackThe resultant tools tend to consist of undifferentied glue code that is duplicated at each level of control. Even worse, the solution is hyper-specific to the combination of tools that needs to be integrated. The problem is almost asking for a more general, flexible approach.CUEs solution is an open, declarative scripting layer built on top of the configuration language. While configuration evaluation can remain hermetic (important for enabling CUE semantics), the scriping layer can freely combine injected data as well as functions to run various tasks in a dataflow pipeline. Data injection opens up many possibilities that can close the gap that other configuration languages miss. We can inject command line flags, data from the enviroment (environment variables, data fetched over the network), and computed data from the defined config that we need for complete automation pipelines.Today, we have incredible tool churn because we cannot easily use the same configuration between multiple systems (compatibility and accessibility). The script layer instead proposes to invert control. Instead of pulling configuration data into inaccessible scripts and tools, why not push the code closer to the data? The flow of data drives the need to endlessly wrap tools to work with multiple systems. Instead, we can define the automation where data and dataflow are first class citizens.Its hard to visualize what this means without an example. The scripting layer is still being worked on, but here is an example from running cue help cmd:package foo import \"tool/exec\" city: \"Amsterdam\" // Say hello! command hello: { var file: \"out.txt\" | string // save transcript to this file task ask: cli.Ask & { prompt: \"What is your name?\" response: string } // starts after ask task echo: exec.Run & { cmd: [\"echo\", \"Hello\", task.ask.response + \"!\"] stdout: string // capture stdout } // starts after echo task write: file.Append & { filename: var.file contents: task.echo.stdout } // also starts after echo task print: cli.Print & { contents: task.echo.stdout } } For CUE, files ending in _tool.cue are evaluated as script files with access to injected data as well as the command and task templates. Each command is given a name which can be called as the entrypoint; the hello command would be executed with cue cmd hello. This command asks for a name then writes a greeting to both stdout and a file. This workflow is implemented as a pipeline of tasks with structs used to define arguments. Since CUE can see which tasks input use another tasks output, we automatically get parallel pipelines through dataflow analysis. Tasks can cover all kinds of operations, one can see the currently available ones hereIntegration And PackagesCUE is a superset of JSON. By design, CUE aims to be useful when working with other formats; CUE supports automation to import and export to/from other data formats. Importing data and schemas into CUE (like JSON, YAML, Protocol Buffers, OpenAPI definitions) allows for managing configurations in an expressive and scalable way. Exporting to other formats enables compatibility when pushing evaluated CUE with other systems. After all, a configuration is not written for its own sake, but to be used somewhere else.The true implications of CUE code generation are realized in combination with the package system. CUE borrows Golangs package system, and plans on implementing minimal version selection with URI imports. With this, we can create reusable abstractions for configuration that have previously existed only in the realm of general programming.Take the Kubernetes example. Kubernetes is a declarative infrastructure state converger. Like a cloud provider, Kubernetes has a massive API surface spread over multiple resources. On top of that, Kubernetes allows for defining custom resources from external sources. To manually write a DCL library to cover all of the API surface would be a massive undertaking and would quickly become out of date. With CUE, we can generate CUE code from the OpenAPI specification. With the full resource API covered, we can layer our own opinionated libraries on top. Maybe you are a platform engineer at a company that wants to ensure everyone defines deployments with the required annotations, labels, and uses the company container repository. The community can also create libraries that encourage best practices and the package system makes it simple to distribute and integrate in any workflow. If the underlying resources were to change, it is straightforward to autogenerate the CUE code and check if the libraries are still compatible.The Automation DreamPutting all these ideas together, we end up with a vision for future configuration. We can have libraries for not only configuring applications but also libraries of automation that run end-to-end workflows for pushing that data to systems. CUEs design enables configuration at scale: organizations can define constraints and templates at a high level, and users layer specific values to make concrete definitions. Organizations find it easier to enforce best practices and policy, and front line engineers get templates and automation out of the box to become productive that much quicker. With distributed packaging, we are not limited to what only our teams can create but can leverage best practices and knowledge from the wider industry.CUE And YouAs of this writing, CUE is a pretty new (alpha) language. There is still much work to be done to reach the complete vision, especially in the integrations with other languages, platforms, and data formats. Fortunately, any improvements to the ecosystem can be easily shared for everyone to use.If I were to summarize the main reason to choose CUE, it is because it chooses to build from a theoretical foundation that makes it a true contender for configuration needs. So many of the useful features CUE provides fall out from its properties (commutative, associative, and idempotent). In an industry where the way we work is becoming more complicated, CUE has come along to make things much more manageable.If any of these people apply to you:A developer who wants to use Kubernetes/Cloud in an effective wayAn application developer who wants to create easy to share automation for a projectA platform developer who wants to create useful templates for every team to useAn architect who wants a simple way to enforce consistent policy across an organizationTired of writing so much YAMLyou should look into CUE.Edit 2019-12-08: Improve tone consistency; Rewrite CUE code to be idiomatic "
        ],
        "story_type": "Normal",
        "url_raw": "https://blog.cedriccharly.com/post/20191109-the-configuration-complexity-curse/",
        "comments.comment_id": [21552106, 21553314],
        "comments.comment_author": ["fake-name", "DenisM"],
        "comments.comment_descendants": [2, 1],
        "comments.comment_time": [
          "2019-11-16T11:27:30Z",
          "2019-11-16T16:11:28Z"
        ],
        "comments.comment_text": [
          "If you're thinking about implementing this or something like this, <i>please stop</i>.<p>Unless you're writing in a compiled language, use the same language your application is written in for your configuration. If you have a python application, have a python file for the configuration. Same for ruby or whatnot.<p>You don't need to use the entire language, but at least use the language's lexer/parser (cf. json/javascript). That way, all existing tooling for the language will work for the config files (ask me about how saltstack happily breaks their API because you're not \"supposed\" to use it, despite the fact that they have public docs for it). Additionally, people won't need to figure out all the stupid corner cases in your weird language that has no uses outside of a few projects.<p>Additionally, by making your configuration language an actual language, you also simplify a lot of the system design, because the configuration can act directly against your API. This means using your tool from other tools becomes much more straightforward, because the only interface you actually need is the API.<p>The <i>existence</i> of \"configuration language\" is, itself, a mistake.",
          "It occurred to me that the configuration hell is a consequence of Microservice-heavy approach: we have reduced complexity of cross-component interactions by compartmentalizing each component behind a hard boundary, and now we’re paying the price for this free lunch by trying to put those things back together and keeping them that way.<p>Turns out the complexity didn’t go anywhere, it was just biding it’s time, waiting for the right moment to strike back.<p>Damn you, entropy!"
        ],
        "id": "3cb44be5-442c-49c4-aedd-1c60adfbe371",
        "url_text": "Read the discussion on Hacker News, Reddit, and Lobste.rsDont be a YAML EngineerImagine that you are a new software engineer entering the industry. You thought you were ready after studying your theory and the weekend side projects. Now, you get hit with a wave of new tools and concepts out of nowhere. Microservices? REST? Cloud Computing? RPC (Whats an IDL)? Docker (Whats a container)? Kubernetes? Continuous Integration? Continuous Deployment? Even for veterans, like a frog in slowly boiling water, you look up one day and realize things have become complicated.How did this happen? Each of these things looks useful in isolation, but now you have to figure out how to use it best for yourself as well as with these other tools in concert. First a tool must be configured, usually in YAML (so much YAML). Now this tool needs to be integrated to work with everything else. If you are lucky, you may have an internal platform team to package and abstract the complexity for you. Otherwise, you are going to end up with a rube goldbergian system that wraps all these tools so you can coordinate configuration between everything just to maintain sanity.I think there is light at the end of the tunnel here, but even leading edge projects like Kubernetes suffer from this industry wide pathology we experience building software platforms.Kubernetes: Its Turtles All The Way DownYou start by hand writing YAML files and pushing them to a cluster with kubectl. Before resting on your laurels as a newly minted distributed systems engineer, you realize that you need a way to pass dynamic values to your YAML file. Enter the tool explosion; Nod along if you have ever done or considered using one of these techniques:Text Templating: Many are tempted to reach for text templating libraries like Jinja, etc. Most have experience using it for templating HTML, why not use it here? At this point you can get by with just simple scripts to template values then push to the cluster, or just go with something off the shelf. Trying to write a configuration file (basically a data structure described in text) with a text templating library is the road to tragedy. As anyone who tried to write a helm chart template knows, templates quickly become hairy and incredibly fragile. Because you are working with the template at the textual level, a template writer lacks the tools to build abstractions around the data itself and small things like indentation can break your template. I think I need to repeat this: Do not use text templating for data configuration.Data Layering: This is when you build an overlay inheritance mechanism on top of YAML, keeping the raw YAML but wrapping it in a tool that can merge these separate documents according to its own rules. kustomize would be the most well known tool now it that is it integrated into kubectl. This seems to work well enough and could be feasible for simple use cases. In comparison to data configuration languages though, this strategy breaks down when configurations grow in complexity and scale. Template writers lack abstraction and type validation that matters when things get complicated, as the semantics are locked into an opaque tool and not exposed as language features.Data Configuration Language (DCL): This is the most advanced form of data configuration we have today, if you absolutely need to manage configuration for a system now I would recommend one of these tools. If the previous techniques are what I call YAML Engineering, DCLs would be actual languages designed for defining data and working with data at a semantic level. Examples of DCLs are jsonnet (used in kubecfg/ksonnet), dhall (used in dhall-kubernetes), and starlark (used in isopod). Google internally uses a tool called borgcfg (which inspired kubecfg) which evaluates configurations in a jsonnet-like internal language to configure Borg (which inspired Kubernetes) deployments. As systems scale in complexity and size, DCLs have abstractions like types, functions, comprehensions, and conditionals to manage and create reusable configurations.So, if you want to anything more advanced than straight YAML you will need to adopt one of these approaches and integrate it with your development and release process. That will help as long as you are working directly with Kubernetes, but what about configuring your underlying infrastructure? If you are on a cloud platform like AWS or Azure, even a managed Kubernetes deployment requires a tool like CloudFormation or Terraform, which has or needs its own distinct abstractions to manage resources at that level of the stack. You cannot reuse the tooling invested in configuring Kubernetes and must yet again integrate another system for configuring the cloud provider at a lower level. The sound you hear is a turtle being stacked on another turtle.Complex Systems == Complex ConfigurationThis example with Kubernetes shows how difficult it is to manage configuration, but Kubernetes is just one of many declarative systems that show this problem. Any declarative systems and tools that grow sufficiently in size and complexity need to be abstracted but we lack common tools to do so. You may not think a Terraform configuration needs to be abstracted, but when dealing with many teams that need to deploy infrastructure in a consistent way, having reusable abstractions is incredibly valuable. Same for Continuous Integration. Maintaining a YAML file may be fine for a single project, but when your organization has hundreds of services, generating a consistent set of automation and checks is worth the effort.The main argument of this essay hits upon the cause of this issue:As systems grow and multiply in order to run at scale, the configuration also grows in size and complexity. The industry lacks the right tools to manage this growing configuration complexity.Innovation in platforms, like the cloud and Kubernetes, makes things that used to be too difficult and complex for a small team now just a simple API call away. But the loop needs to be closed on how we can build libraries that any organization can use to have a base of best practices for any platform. Even current DCLs, which I consider closer to the model we should use, lack key pieces to manage configuration at scale.CUE: Configure, Unify, ExecuteWe need a way to manage configuration that grows in size and needs to be used in many different systems. I think a tool to solve this would need two key properties:The tool is a language that has the primitives needed to create and maintain reusable, large scale configuration.The tool can orchestrate and push configuration to many different systems without having to create a new, custom tool just for that combination.There are many languages and configuration languages that attempt to solve this problem. CUE is a (new) data configuration language that uses a novel approach to solving the issue with a vision to tackle both what is needed to do configuration at large scale with a way to finally avoid the all too common tool wrapping that we see today. The creator of cuelang (this guy) worked on borgcfg at Google, where the learnings of managing configuration across a large company drives much of what makes cuelang a solution for our configuration problems today.Why CUE?Configuration Needs A Different Programming ModelIn the Kubernetes example before, I left out the option of using a a general purpose programming language. CUE is not even the first configuration language out there. Why should we pick a completely new tool instead of reusing something that already exists?CUE is based on a different model of logic programming that makes it well suited for configuration. The languages foundations make tasks like validation, templating, querying, and code generation first class features. CUE is designed around graph unification where sets of types, and values can be modeled as directed graphs and then unified to the most specific representation of all graphs. In CUE, the graphs are the config structures and values of the same key will simplify to the most specific value. The graph unification model is used successfully in computational linguistics, where a grammar definition of a human language can be thought of as 100K line configuration.This idea is much more intuitive to see with an example. Lets look at an example CUE definition:// Schema municipality: { name: string pop: int capital: bool } // Schema & Data largeCapital: municipality largeCapital: { name: string pop: >5M capital: true } // Data moscow: largeCapital moscow: { name: \"Moscow\" pop: 11.92M capital: true } We can see three separate structs defined here with varying mixtures of types and values defined for each. The combination of types and values in a single struct is significant here. In CUE, types are values. This means that types can be assigned to fields and can be immediately used to constrain values in the configuration. You can also see that fields become more constrained towards concrete values with each struct. largeCapital is a municipiality with a new constraint on the population size. moscow is a largeCapital with concrete values (the most specific type) for all fields. Between structs largeCapital and moscow, largeCapital subsumes moscow and moscow is an instance of largeCapital.If you want to go deeper I recommend reading The Logic of CUE to understand the theoretical foundation and what makes CUE different from other configuration languages. You can also watch this video from the creator if you prefer that. I think trying to go deeper into the theory would be me poorly rewording the original article. I would rather go into how the foundation of CUE enables features that are useful for configuration.Type Checking And Boilerplate RemovalGoing back to what we need from a new configuration tool, types and abstractions are the largest factors in managing large scale configuration. With types, we express constraints on data and declare intent across potentially many users. Types protect users from errors when defining configuration and serves as automatic documentation. Taking an example from the website:Spec :: { kind: string name: { first: !=\"\" // must be specified and non-empty middle?: !=\"\" // optional, but must be non-empty when specified last: !=\"\" } // The minimum must be strictly smaller than the maximum and vice versa. minimum?: int & <maximum maximum?: int & >minimum } // A spec is of type Spec spec: Spec spec: { knid: \"Homo Sapiens\" // error, misspelled field name: first: \"Jane\" name: last: \"Doe\" } In CUE, we can see what fields are needed for a Spec type as well as the constraints on each. CUEs type system is expressive, where fields can be marked simply by their type, to specifying optionality and constraints from other fields as well. This mixing of types and values is underrated. In constrast with JSON, one needs to generate a separate specification format (like OpenAPI) with significant effort to document and validate the JSON being consumed. With CUE, configuration can be checked and validated during evaluation without extra effort or tooling.Most of the current configuration tooling, both general purpose and data languages, focuses on removing boilerplate/duplicate code to reduce verbosity. Many choose to use an override model like inheritance (defining base types and modifying) due to developer familarity with the paradigm. This is a key factor to why I think DCLs have not seen widespread use in the industry. Although it would seem to be an obvious model, inheritance has problems in both small and large scale configuration. For small projects, defining abstractions early can be a large upfront ask with small payoff. After all, to benefit from deduplication there needs to be duplicate configuration in the first place (For the sake of clarity abstraction too early can be counterproductive). With CUE, defining the config grants automatic validation and documentation right out the gate; Paired with default values you can immediately cut down the effort to reuse common data.For large scale projects, inheritance creates deep layers of abstractions; It becomes difficult to tell where values are coming from in a config, much more so if the language has multiple mechanisms for abstraction (inheritance, mixins). At a practical level, deep hierarchies are common in configuration, and subtrees near the leaves can be challenging to modify. CUE takes a different approach with graph unification and disallows overrides. This improves readability as deep layering is prevented from the beginning; One does not need to trace through multiple files to see where a value came from. Although CUE does not have inheritance, it does have the concept of a value being an instance of another (the value lattice). This model is less flexible, but in return we get great clarity. To take a simple example, imagine we have an Animal class and we want to define both a Dog and Cat class. With inheritance, we could define Animal as the base type then selectively override and add fields to represent each respective subclass. Although workable, if we wanted to go further and represent each breed of Cat and Dog we could quickly run into issues as each layer of the hierarchy can choose to modify data as it chooses. In CUEs approach, instead of trying to override data at each layer, we choose to model Cat and Dog as instances of Animal. For both types, we take Animals definition but instead add constraints to bound what defines each animal. Even though we have the same hierarchy of types, at each layer the data can only become more specific for that subtype. Thanks to that for breeds we only need to further constrain what makes a Dog a Corgi and at each level we can see how the data was constrained to allow the final value.Scripting: Inversion of ControlConfiguration is never created for its own sake. The purpose of all configuration is to be fed to another system to do useful work with it. Especially today, we need to manage configuration between a dizzying amount of tools and services: Software as a Service (SaaS) offerings, Cloud vendors, Continous Integration, build tools, package managersthe list goes on. In the beginning, piping config files to various tools such as kubectl may be enough to get the job done. Eventually though, these tasks end up as part of a greater workflow that needs to be automated. Marcel describes a problem he has seen many times:At first using a general config lang is cute, piping results to kubectl and what have you. But this gets cumbersome, so one writes scripts. Then, as scripts are often inconvenient to distribute and use, so one turns this into a command line tool. These tools are typically domain specific. So sooner or later, this tool needs to be part of a larger control flow, and the whole process starts again. Ive been guilty of writing a bunch of such tools.Marcel van Lohuizen, CUE SlackThe resultant tools tend to consist of undifferentied glue code that is duplicated at each level of control. Even worse, the solution is hyper-specific to the combination of tools that needs to be integrated. The problem is almost asking for a more general, flexible approach.CUEs solution is an open, declarative scripting layer built on top of the configuration language. While configuration evaluation can remain hermetic (important for enabling CUE semantics), the scriping layer can freely combine injected data as well as functions to run various tasks in a dataflow pipeline. Data injection opens up many possibilities that can close the gap that other configuration languages miss. We can inject command line flags, data from the enviroment (environment variables, data fetched over the network), and computed data from the defined config that we need for complete automation pipelines.Today, we have incredible tool churn because we cannot easily use the same configuration between multiple systems (compatibility and accessibility). The script layer instead proposes to invert control. Instead of pulling configuration data into inaccessible scripts and tools, why not push the code closer to the data? The flow of data drives the need to endlessly wrap tools to work with multiple systems. Instead, we can define the automation where data and dataflow are first class citizens.Its hard to visualize what this means without an example. The scripting layer is still being worked on, but here is an example from running cue help cmd:package foo import \"tool/exec\" city: \"Amsterdam\" // Say hello! command hello: { var file: \"out.txt\" | string // save transcript to this file task ask: cli.Ask & { prompt: \"What is your name?\" response: string } // starts after ask task echo: exec.Run & { cmd: [\"echo\", \"Hello\", task.ask.response + \"!\"] stdout: string // capture stdout } // starts after echo task write: file.Append & { filename: var.file contents: task.echo.stdout } // also starts after echo task print: cli.Print & { contents: task.echo.stdout } } For CUE, files ending in _tool.cue are evaluated as script files with access to injected data as well as the command and task templates. Each command is given a name which can be called as the entrypoint; the hello command would be executed with cue cmd hello. This command asks for a name then writes a greeting to both stdout and a file. This workflow is implemented as a pipeline of tasks with structs used to define arguments. Since CUE can see which tasks input use another tasks output, we automatically get parallel pipelines through dataflow analysis. Tasks can cover all kinds of operations, one can see the currently available ones hereIntegration And PackagesCUE is a superset of JSON. By design, CUE aims to be useful when working with other formats; CUE supports automation to import and export to/from other data formats. Importing data and schemas into CUE (like JSON, YAML, Protocol Buffers, OpenAPI definitions) allows for managing configurations in an expressive and scalable way. Exporting to other formats enables compatibility when pushing evaluated CUE with other systems. After all, a configuration is not written for its own sake, but to be used somewhere else.The true implications of CUE code generation are realized in combination with the package system. CUE borrows Golangs package system, and plans on implementing minimal version selection with URI imports. With this, we can create reusable abstractions for configuration that have previously existed only in the realm of general programming.Take the Kubernetes example. Kubernetes is a declarative infrastructure state converger. Like a cloud provider, Kubernetes has a massive API surface spread over multiple resources. On top of that, Kubernetes allows for defining custom resources from external sources. To manually write a DCL library to cover all of the API surface would be a massive undertaking and would quickly become out of date. With CUE, we can generate CUE code from the OpenAPI specification. With the full resource API covered, we can layer our own opinionated libraries on top. Maybe you are a platform engineer at a company that wants to ensure everyone defines deployments with the required annotations, labels, and uses the company container repository. The community can also create libraries that encourage best practices and the package system makes it simple to distribute and integrate in any workflow. If the underlying resources were to change, it is straightforward to autogenerate the CUE code and check if the libraries are still compatible.The Automation DreamPutting all these ideas together, we end up with a vision for future configuration. We can have libraries for not only configuring applications but also libraries of automation that run end-to-end workflows for pushing that data to systems. CUEs design enables configuration at scale: organizations can define constraints and templates at a high level, and users layer specific values to make concrete definitions. Organizations find it easier to enforce best practices and policy, and front line engineers get templates and automation out of the box to become productive that much quicker. With distributed packaging, we are not limited to what only our teams can create but can leverage best practices and knowledge from the wider industry.CUE And YouAs of this writing, CUE is a pretty new (alpha) language. There is still much work to be done to reach the complete vision, especially in the integrations with other languages, platforms, and data formats. Fortunately, any improvements to the ecosystem can be easily shared for everyone to use.If I were to summarize the main reason to choose CUE, it is because it chooses to build from a theoretical foundation that makes it a true contender for configuration needs. So many of the useful features CUE provides fall out from its properties (commutative, associative, and idempotent). In an industry where the way we work is becoming more complicated, CUE has come along to make things much more manageable.If any of these people apply to you:A developer who wants to use Kubernetes/Cloud in an effective wayAn application developer who wants to create easy to share automation for a projectA platform developer who wants to create useful templates for every team to useAn architect who wants a simple way to enforce consistent policy across an organizationTired of writing so much YAMLyou should look into CUE.Edit 2019-12-08: Improve tone consistency; Rewrite CUE code to be idiomatic ",
        "_version_": 1718536543092604929
      },
      {
        "story_id": 21413785,
        "story_author": "mckinney",
        "story_descendants": 5,
        "story_score": 17,
        "story_time": "2019-10-31T21:16:43Z",
        "story_title": "“Just-in-time code generation” the coolest language feature you never heard of",
        "search": [
          "“Just-in-time code generation” the coolest language feature you never heard of",
          "https://github.com/manifold-systems/manifold/tree/master/manifold-core-parent/manifold#the-big-picture",
          "The core framework plugs directly into the Java compiler via the Javac plugin API as a universal type adapter to allow for a direct and seamless supply of types and features otherwise inaccessible to Java's type system. As such the Manifold core framework provides a foundation and plugin SPI to dynamically resolve type names and produce corresponding Java sources, and to more generally augment Java's type system. Such a plugin is called a type manifold and implements the ITypeManifold SPI. Table of Contents The Big Picture The API Anatomy of a Type Manifold Configuring Manifold Explicit Resource Compilation Modes Programming Language Manifolds Embedding with Fragments (experimental) IDE Support Projects Sample Projects Setup Platforms Javadoc License Versioning Author The Big Picture You can think of a type manifold as a just-in-time code generator. Essentially the Manifold framework plugs in and overrides the compiler's type resolver so that, via the ITypeManifold SPI, a type manifold can claim ownership of type names as the compiler encounters them and dynamically provide source code corresponding with the types. As a consequence this core functionality serves as a productive alternative to conventional code generation techniques, a long overdue advancement for static typing. To begin with, because the framework plugs directly into the compiler, a code generator as a type manifold is no longer a separate build step. What's more a type manifold generates code on-demand as the compiler asks for types. Not only does this significantly reduce the complexity of code generators, it also enables them to function incrementally. This means resources on which the generated sources are based can be edited and only the types corresponding with the changes will be regenerated and recompiled. Thus, contrary to conventional code generators, type manifolds: require zero build steps produce zero on-disk source code are by definition always in sync with resources are inherently incremental resulting in optimal build times are dead simple to use - just add a dependency to your project Moreover, type manifolds can cooperate and contribute source to types in different ways. Most often a type manifold registers as a primary contributor to supply the main body of the type. The JSON type manifold, for example, is a primary contributor because it supplies the full type definition corresponding with a JSON Schema file or sample file. Alternatively, a type manifold can be a partial or supplementary contributor. For instance, the Extension type manifold is a supplementary contributor because it augments an existing type with additional methods, interfaces, and other features. Thus both the JSON and Extension type manifolds can contribute to the same type, where the JSON manifold supplies the main body and the Extension type manifold contributes custom methods and other features provided by extension classes. As a consequence any number of type manifolds can operate in concert to form an efficient and powerful type building pipeline, thus unlike conventional code generators, type manifolds: can easily have dependencies on one another can easily contribute to one another's types are customizable with extensions Finally, the Manifold core framework can be used from IDEs and other tooling to provide consistent, unified access to the types and features produced from all type manifolds. For instance, the Manifold plugin for IntelliJ IDEA provides comprehensive support for the Manifold framework. All types and features produced from type manifolds and other Manifold SPIs are fully supported. You can edit resources such as JSON and GraphQL files and immediately use the changes in Java without a compilation step. Features like code completion, resource/code navigation, deterministic usage searching, refactoring/renaming, incremental compilation, hotswap debugging, etc. work seamlessly with all type manifolds past, present, and future. This represents a tremendous leap in productivity compared with the conventional code generation world where the burden is on the code generator author or third party to invest in one-off IDE tooling projects, which typically results in poor or no IDE representation. Thus another big advantage type manifolds possess over conventional code generators is: a unified framework... ...which enables comprehensive IDE support and more To summarize, the Manifold framework provides a clear advantage over conventional code generation techniques. Type manifolds do not entail build steps, are always in sync, operate incrementally, and are simple to add to any project. They also cooperate naturally to form a powerful type building pipeline, which via the core framework is uniformly accessible to IDEs such as IntelliJ IDEA and Android Studio. Putting it all together, the synergy resulting from these improvements has the potential to significantly increase Java developer productivity and to open minds to new possibilities. The API The framework consists of the several SPIs: ITypeManifold This SPI is the basis for implementing a type manifold. See existing type manifold projects such as manifold-graphql. ICompilerComponent Implement this low-level SPI to supplement Java with new or enhanced behavior e.g., manifold-strings and manifold-exceptions. IPreprocessor Implement this SPI to provide a preprocessor to filter source before it enters Java's parser e.g., manifold-preprocessor. Anatomy of a Type Manifold Any data resource is a potential type manifold. These include file schemas, query languages, database definitions, data services, templates, spreadsheets, programming languages, and more. So while the Manifold team provides several type manifolds out of the box the domain of possible type manifolds is virtually unlimited. Importantly, their is nothing special about the ones we provide -- you can build type manifolds using the same public API with which ours are built. The API is comprehensive and aims to fulfill the 80/20 rule -- the common use-cases are straightforward to implement, but the API is flexible enough to achieve almost any kind of type manifold. For instance, since most type manifolds are resource file based the API foundation classes handle most of the tedium with respect to file management, caching, and modeling. Also, since the primary responsibility for a type manifold is to dynamically produce Java source, Manifold provides a simple API for building and rendering Java classes. But the API is flexible so you can use other tooling as you prefer. Most resource file based type manifolds consist of three basic classes: JavaTypeManifold subclass A class to produce Java source Model subclass The Image manifold is relatively simple and nicely illustrates this structure: JavaTypeManifold Subclass public class ImageTypeManifold extends JavaTypeManifold<Model> { private static final Set<String> FILE_EXTENSIONS = new HashSet<>(Arrays.asList(\"jpg\", \"png\", \"bmp\", \"wbmp\", \"gif\")); @Override public void init(IModule module) { init(module, Model::new); } @Override public boolean handlesFileExtension(String fileExtension) { return FILE_EXTENSIONS.contains(fileExtension.toLowerCase()); } @Override protected String aliasFqn(String fqn, IFile file) { return fqn + '_' + file.getExtension(); } @Override protected boolean isInnerType(String topLevel, String relativeInner) { return false; } @Override protected String produce(String topLevelFqn, String existing, Model model, DiagnosticListener<JavaFileObject> errorHandler) { SrcClass srcClass = new ImageCodeGen(model._url, topLevelFqn).make(); StringBuilder sb = srcClass.render(new StringBuilder(), 0); return sb.toString(); } } Like most type manifolds the image manifold is file extension based, specifically it handles the domain of files having image extensions: jpg, png, etc. As you'll see, the JavaTypeManifold base class is built to handle this use-case. First, ImageTypeManifold overrides the init() method to supply the base class with its Model. We'll cover that shortly. Next, it overrides handlesFileExtension() to tell the base class which file extensions it handles. Next, since the image manifold produces classes with a slightly different name than the base file name, it overrides aliasFqn() to provide an alias for the qualified name of the form <package>.<image-name>_<ext>. The name must match the class name the image manifold produces. There are no inner classes produced by this manifold, therefore it overrides isInnerType() returning false; the base class must ask the subclass to resolve inner types. Finally, the image manifold overrides contribute(), this is where you contribute Java source for a specified class name. Source Production Class Most often, you'll want to create a separate class to handle the production of Java source. The image manifold does that with ImageCodeGen: public class ImageCodeGen { private final String _fqn; private final String _url; ImageCodeGen(String url, String topLevelFqn) { _url = url; _fqn = topLevelFqn; } public SrcClass make() { String simpleName = ManClassUtil.getShortClassName(_fqn); return new SrcClass(_fqn, SrcClass.Kind.Class).imports(URL.class, SourcePosition.class) .superClass(new SrcType(ImageIcon.class)) .addField(new SrcField(\"INSTANCE\", simpleName).modifiers(Modifier.STATIC)) .addConstructor(new SrcConstructor() .addParam(new SrcParameter(\"url\") .type(URL.class)) .modifiers(Modifier.PRIVATE) .body(new SrcStatementBlock() .addStatement(new SrcRawStatement() .rawText(\"super(url);\")) .addStatement(new SrcRawStatement() .rawText(\"INSTANCE = this;\")))) .addMethod(new SrcMethod().modifiers(Modifier.STATIC) .name(\"get\") .returns(simpleName) .body(new SrcStatementBlock() .addStatement( new SrcRawStatement() .rawText(\"try {\") .rawText(\" return INSTANCE != null ? INSTANCE : new \" + simpleName + \"(new URL(\"\\\\\" + ManEscapeUtil.escapeForJavaStringLiteral(_url) + \"\\\\\"));\") .rawText(\"} catch(Exception e) {\") .rawText(\" throw new RuntimeException(e);\") .rawText(\"}\")))); } } Here the image manifold utilizes SrcClass to build a Java source model of image classes. SrcClass is a source code production utility in the Manifold API. It's simple and handles basic code generation use-cases. Feel free to use other Java source code generation tooling if SrcClass does not suit your use-case, because ultimately your only job here is to produce a String consisting of Java source for your class. Model Subclass The third and final class the image manifold provides is the Model class: class Model extends AbstractSingleFileModel { String _url; Model(String fqn, Set<IFile> files) { super(fqn, files); assignUrl(); } private void assignUrl() { try { _url = getFile().toURI().toURL().toString(); } catch (MalformedURLException e) { throw new RuntimeException(e); } } public String getUrl() { return _url; } @Override public void updateFile(IFile file) { super.updateFile(file); assignUrl(); } } This class models the image data necessary for ImageCodeGen to produce source as a AbstractSingleFileModel subclass. In this case the model data is simply the URL for the image. Additionally, Model overrides updateFile() to keep the URL up to date in environments where it can change, such as in an IDE. Registration In order to use a type manifold in your project, it must be registered as a service. Normally, as a type manifold provider to save users of your manifold from this step, you self-register your manifold in your META-INF directly like so: src -- main ---- resources ------ META-INF -------- services ---------- manifold.api.type.ITypeManifold Following standard Java ServiceLoader protocol you create a text file called manifold.api.type.ITypeManifold in the service directory under your META-INF directory. The file should contain the fully qualified name of your type manifold class (the one that implements ITypeManifold) followed by a new blank line: As you can see building a type manifold can be relatively simple. The image manifold illustrates the basic structure of most file-based manifolds. Of course there's much more to the API. Examine the source code for other manifolds such as the GraphQL manifold (manifold-graphql) and the JavaScript manifold (manifold-js). These serve as decent reference implementations for wrapping parsers and binding to existing languages. Note with Java 9+ with named modules you register a service provider in your module-info.java file using the provides keyword: provides manifold.api.type.ITypeManifold with com.abc.MyTypeManifold Configuring Manifold There are three ways you can configure Manifold in your build: static, dynamic, and mixed. Each configuration type is determined by the following criteria: Use of compileOnly (or provided) scoping on all Manifold dependencies your project uses that are specific to compilation Use of the dynamic argument to the Manifold javac plugin e.g., -Xplugin:Manifold vs. -Xplugin:Manifold dynamic static dynamic mixed compileOnly scope dynamic plugin arg Note, static is the recommended configuration for most projects. Using it results in smaller, faster, more versatile projects. static: Compiles all resource types statically at compile-time. Distributes only Manifold runtime dependencies. Advantages: Performance. Since resource types are compiled ahead of time as .class files, load time for a resource type is the same a Java type. Zero startup time. The dynamic compilation subsystem and other services exclusive to compilation are not present at runtime, therefore Manifold has ZERO impact on startup time. Footprint. Since only runtime modules are distributed in this configuration, the resulting runtime footprint is more than 10x smaller than dynamic and mixed configurations, which translates to faster downloads. Android. Applications using Manifold statically fully support Android. Kotlin. Applications using Manifold statically fully support Kotlin and other JVM languages. Limitations: Dynamic compilation (compilation at runtime) is not supported. Usage of dynamic features such as Dark Java can't be used. Considering Dark Java is the only remaining exclusively dynamic feature of Manifold, this is a rather minor limitation. dynamic: Compiles all resource types dynamically at runtime. No resource types are compiled to .class files at compile-time. Both compile-time and runtime binaries are included in your distribution. Advantages: Dynamic. Purely dynamic features such as Dark Java enable dynamic runtime compilation of resources. Limitations: Slower startup and initialization. Manifold dynamic services contribute to a slower startup time. Additionally, initialization times for resource types are slower due to dynamic compilation on initial load. Larger footprint. Dynamic services used at compile-time are also distributed with the runtime, the additional files significantly impact the overall footprint (more than 10x). Limited use. Only Java SE environments are supported. Other JVM environments such as Android and Kotlin are not supported using this configuration. mixed: Compiles project resource types statically such as resource files, and compiles dynamic resource types dynamically such as Dark Java. Both compile-time and runtime binaries are included in your distribution. Advantages: Dynamic. Purely dynamic features such as Dark Java enable dynamic runtime compilation of resources. Performance. Since resource types are compiled ahead of time as .class files, load time for a resource type is the same a Java type. Limitations: Slower startup and initialization. Manifold dynamic services contribute to a slower startup time. Additionally, initialization times for resource types are slower due to dynamic compilation on initial load. Larger footprint. Dynamic services used at compile-time are also distributed with the runtime, the additional files significantly impact the overall footprint (more than 10x). Limited use. Only Java SE environments are supported. Other JVM environments such as Android and Kotlin are not supported using this configuration. Configurations by supported features static dynamic mixed Performance Small footprint Zero startup time Java SE Android Kotlin** Dynamic Java **All Manifold resource types such as GraphQL and JSON are fully supported with Kotlin, however Manifold Java Extension features such as @Jailbreak, unit expressions, and the preprocessor are specific to the Java compiler. Configuring Dependencies Manifold components are structured to support both static and dynamic use. For instance, the Manifold core component consists of two modules: manifold and manifold-rt, where manifold contains all the compile-time functionality and manifold-rt contains code exclusive to runtime APIs and internal runtime implementation. As such, to use the Manifold core modules statically with your project you add a compile-only dependency on manifold and a default dependency on manifold-rt. All of Manifold modules are designed in this way. As a result, components like Manifold core that provide both compile-time and runtime functionality consist of two modules manifold-xxx and manifold-xxx-rt, whereby a compile-only scope is used on manifold-xxx and a default scope is used for manifold-xxx-rt. A component that is exclusive to compile-time or that does not provide any features exclusive to compile-time does not define a separate \"rt\" module. For instance, the manifold-preprocessor is exclusive to compile-time use, therefore you always add it with a compile-only scope. Conversely, the manifold-science library does not define any features exclusive to compile-time, so you always use it with default scoping so it is packaged with your executable artifact. Static Configuration (default) Add the -Xplugin:Manifold javac argument If using Kotlin or other alternative JVM language, put Manifold resources in a separate Java compiled module and add -Amanifold.source.<file-ext>=<type-name-regex> javac arguments to explicitly compile the resources. See Explicit Resource Compilation below. Use compile-only (Gradle) or provided (Maven) scoping for Manifold dependencies exclusive to compile-time features Use default scoping for \"rt\" dependencies and any dependencies that are needed at runtime See Setup for examples of using Manifold statically. Note, static is the recommended configuration for most projects. Using it results in smaller, faster, more versatile projects. Dynamic Configuration Add the -Xplugin:Manifold dynamic javac argument Use default scoping for Manifold dependencies, there is no need to add dependencies to \"rt\" modules. See Setup for examples of using Manifold dynamically. Mixed Configuration Add the -Xplugin:Manifold javac argument Use default scoping for Manifold dependencies, there is no need to add dependencies to \"rt\" modules. You can statically compile as much or as little as preferred. See Setup for examples using Manifold both statically and dynamically. Note, the only difference between dynamic and mixed configuration is the dynamic Manifold plugin argument, which prevents Manifold from compiling resource types to disk. Runtime Dependencies If a project uses any manifold runtime dependencies (\"rt\" dependencies) by default manifold inserts a static block into all your classes to automatically initialize some runtime services: static { IBootstrap.dasBoot(); } The dasBoot() call invokes all registered IBootstrap services. The number and nature of the implementations depends on what your application does with manifold. For instance, if you are using manifold dynamically, this call initializes the dynamic compilation services among other tasks. Most projects, however, use manifold statically which means dasBoot() typically does only the following: Disables the Java 9 warning \"An illegal reflective access operation has occurred\", because that message has a history of unnecessarily alarming users. This is a noop when running on Java 8 / Android. Dynamically open the java.base module to the manifold module for common reflection access, which is a noop running on Java 8 / Android Note dasBoot() performs these tasks one time, subsequent calls simply return i.e., there is no performance penalty. If you know your code will never run on Java 9+ and/or you don't mind the Java 9+ warning message, you can eliminate the dasBoot() static initializer via the no-bootstrap plugin argument: Gradle options.compilerArgs += ['-Xplugin:Manifold no-bootstrap'] Maven <compilerArgs> <arg>-Xplugin:Manifold no-bootstrap</arg> </compilerArgs> If you need finer grained control over which classes have the static block, you can use the @NoBootstrap annotation to filter specific classes. Note, compile-only dependencies such as manifold-preprocessor, manifold-exceptions, and manifold-strings don't involve any runtime dependencies, thus if your project's exposure to manifold is limited to these dependencies, the static block is never inserted in any of your project's classes. Explicit Resource Compilation By default, Manifold compiles resource types to disk as the Java compiler encounters them in your code. As a consequence, a resource that is never used in your code as a type is not compiled. For example, if you have hundreds of JSON resource files, but your project only uses a handful type-safely with Manifold, only the handful is compiled to disk as .class files. This scheme is unsuitable, however, for modules intended for use as a dependency or library where the set of resources that needs to be compiled may include more than the ones used inside the module. For instance, an API that defines a query model in terms of GraphQL files may not use any of the resource files directly, but a module using the API would. Although Manifold can still work in situations like this (see Modes above), it does so by compiling types dynamically at runtime, which entails a performance bump the first time each type loads. If need be, you can avoid the runtime compilation cost using explicit static compilation with javac command line options. Similar to javac's source file list, Manifold provides -Akey=value javac command line options to explicitly compile resources either by type name using regular expressions or by file name using file system paths. Constraining by type name is the simplest and more flexible of the two, especially in terms of build systems such as Maven and Gradle. See the Sample Kotlin App for an example of using explicit resource compilation. By Type Name Use the -Amanifold.source.<file-ext>=<type-name-regex> javac command line option to specify Manifold types that should statically compile, whether or not they are referenced elsewhere in Java source. An easy way to tell the Java compiler to compile all the files corresponding with all the type manifolds enabled in your project: javac -Amanifold.source.*=.* ... The * wildcard selects all type manifolds and the .* regular expression selects all types for each type manifold, therefore all types expressible through Manifold are statically compiled to disk with javac. You can limit compilation to types relating to a specific file extension. For instance, if you are using the JSON manifold and you want all your JSON files to be statically compiled: javac -Amanifold.source.json=.* ... Define several arguments and use any regular expression to refine the set of types to compile: javac -Amanifold.source.json=.* -Amanifold.source.graphql=^com\\.example\\..*Queries$ ... This tells the compiler to compile all JSON files and to compile GraphQL types in package com.example ending with Queries. If need be, you can use regular expressions to invert or \"black list\" inclusions. javac -Amanifold.source.graphql=^(?!(com\\.example\\..*Queries)).*$ ... Here all GraphQL types compile except those in package com.example ending with Queries. Using the class: prefix you can constrain compilation by the class name of a type manifold. This is useful for type manifolds not based on files or file extensions. javac -Amanifold.source.class:com.example.MySpecialTypeManifold=.* ... Note, as a reminder, the javac command line arguments are additive with respect to types compiled to disk. As a general rule Manifold types referenced in Java source are always compiled, regardless of command line argument constraints. Adding Source Paths If you use the -Amanifold.source.<ext>=<regex argument, but your resources reside a directory other than Java source directories or resource directories, you can specify additional source paths that are exclusive to Manifold type compilation using the -Amanifold.source=<paths> argument. javac -Amanifold.source=/myproject/src/main/stuff ... This example adds /myproject/src/main/stuff as an additional Manifold source path. Your -Amanifold.source.<ext>=<regex arguments apply to this directory. By File Name Using path-based javac -Aother.source.files argument you can enumerate resource files that should compile statically regardless of whether or not the files are referenced in your code. javac -Aother.source.files=/myproject/src/main/resources/com/example/Queryies.gql /myproject/src/main/resources/com/example/Mutations.gql ... Use other.source.list to specify a file that contains a list of resource files that should compile statically regardless of whether or not the files are referenced in your code. The file contains a single resource path per line. javac -Aother.source.list=/myproject/target/otherfiles.txt ... Build Tooling If you define your project with Maven, you can explicitly compile resources with javac arguments like this: <!-- Configure Manifold as a Javac plugin --> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <source>8</source> <target>8</target> <encoding>UTF-8</encoding> <compilerArgs> <arg>-Xplugin:Manifold</arg> <arg>-Amanifold.source.json=.*</arg> <arg>-Amanifold.source.graphql=^com\\.example\\..*Queries$</arg> </compilerArgs> </configuration> </plugin> Similarly, with Gradle you add the arguments like this: compileJava { options.compilerArgs += ['-Xplugin:Manifold', '-Amanifold.source.json=.*', '-Amanifold.source.graphql=^com\\\\.example\\\\..*Queries$'] } Exposing Resource Types Another benefit from statically compiling resources relates to resource exposure. If your resources are statically compiled, they are available for use as .class files, not only from your own code, but also from potential consumers of your code. This is an important distinction to make because if you don't statically compile resources that are intended for use outside your project, say as part of an API you provide, those resources are not discoverable from another module using Manifold unless you explicitly expose them from your JAR-based artifact. You can do that using the Contains-Sources manifest entry. <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-jar-plugin</artifactId> <configuration> <archive> <manifestEntries> <!--expose GraphQL files if they are NOT statically compiled in your project--> <Contains-Sources>graphql</Contains-Sources> <!--JPMS module name--> <Automatic-Module-Name>mymodule</Automatic-Module-Name> </manifestEntries> </archive> </configuration> </plugin> Although Manifold could use the entire class path as the domain of potential resources types, doing so may impact performance. That's why you must opt-in your module's resources for external use. It bears repeating, if you statically compile all the resources intended for use outside your project, you do not need to opt-in your JAR for processing -- the resources are already available as .class files. Modes You can use Manifold in one of two modes which you control as an optional argument to the Manifold plugin for javac: static: -Xplugin:Manifold (default) compiles resource types statically at compile-time dynamic: -Xplugin:Manifold dynamic compiles resource types dynamically at runtime (alternatively -Xplugin:\"Manifold dynamic\", some tools may require quotes) Most projects benefit most using the default (static) mode. Dynamic mode in most cases should be reserved for specific type manifolds that are better suited to dynamic compilation. Note if you're not sure which mode to use, try the default static mode -- it's usually the right choice. General information considering the static v. dynamic mode: Both modes operate lazily: a type is not compiled unless it is used. For example, if you are using the JSON manifold, only the JSON files you reference in your code will be processed and compiled. This means Manifold will not try to compile resources your project does not expect to use directly as types. Even if you use static mode, you can still reference type manifold classes dynamically e.g., reflectively. In such a case Manifold will dynamically compile the referenced class as if you were operating in dynamic mode. In general, your code will work regardless of the mode you're using, hence the general recommendation to stay with static mode where you get the best of both worlds. Dynamic mode requires tools.jar at runtime for Java 8. Static mode is generally faster at runtime since it pre-compiles all the type manifold resources along with Java sources when you build your project Static mode automatically supports incremental compilation and hotswap debugging of modified resources in IntelliJ Note, you can use javac command line arguments to statically compile a set of specified types whether you use them directly in your code e.g., if type-safe resources are part of an API. See Explicit Resource Compilation. Programming Language Manifolds todo: use the Javascript manifold as a simple impl, and the Gosu manifold as a more complex one as reference material for this section Embedding with Fragments (experimental) You can now embed resource content such as JSON, GraphQL, XML, YAML, CSV, etc. directly in a Java source file as a type-safe resource fragment. A fragment has the same format and grammar as a resource file and, if used with the Manifold IDE plugin, can be authored with rich editor features like code highlighting, parser feedback, code completion, etc. This means you can directly embed resources closer to where you use them in your code. For instance, you can type-safely write a query in the query language your application uses directly in the Java method that uses the query. You can embed a fragment as a declaration or a value. Type Declaration You can type-safely embed resources directly in your Java code as declarations. Here's a simple example using Javascript: public class MyJavaClass { void foo() { /*[>Barker.js<] function callBark(aBarker) { aBarker.bark(); } */ Barker.callBark(new Dog()); Barker.callBark(new Tree()); class Dog { public String bark() { return \"ruff\"; } } class Tree { public String bark() { return \"rough\"; } } } } Notice the Javascript is embedded in a multiline comment. This is how you embed any kind of resource fragment as a type declaration. Here the Javascript type is declared as Barker with a .js extension indicating the resource type. Note a fragment must use [> and <] at the beginning of the comment to delimit the type name and extension. A fragment covers the remainder of the comment and must follow the format of the declared extension. You can embed any Manifold enabled resource as a fragment type declaration. Here's another example using JSON: void foo() { /*[>Planet.json<] { \"name\": \"Earth\", \"system\": { \"name\": \"Sol\", \"mass\": 1.0014 } } */ // Work with the content type-safely Planet planet = Planet.fromSource(); String name = planet.getName(); Planet.system sys = planet.getSystem(); // Make a REST call Planet.request(endpoint).postOne(planet); // Use the JSON bindings Map<String, Object> jsonBindings = planet.getBindings(); // Make a new Planet Planet mars = Planet.builder() .withName(\"Mars\") .withSystem(sys) .build(); // Transform to another format String yaml = planet.writer().toYaml(); } Scoping A fragment can be embedded anywhere in your code. The type declared in the fragment is scoped to the package of the enclosing class. Thus in the example Barker is accessible anywhere in the enclosing foo method as well as foo's declaring class and other classes in its package. Note, even though the declared type is package scoped, for the sake of readability it is best to define the fragment nearest to its intended use. In a future release this level of scoping may be enforced. Rich Editing If used with the Manifold IntelliJ IDEA plugin or the Android Studio plugin, you can edit fragments as if they were in a separate file with all the editor features you'd expect like highlighting, parser feedback, code completion, etc. This is especially useful with GraphQL, SQL, and similar resources where editing a fragment in place provides a more fluid development experience. Value Fragments Sometimes it's more convenient to use a fragment as a value as opposed to a type declaration. For example, you can create a GraphQL query as a fragment value and assign it to a variable: var moviesByGenre = \"[>.graphql<] query MoviesByGenre($genre: genre) { movies(genre: $genre) { title } }\"; var query = moviesByGenre.builder().withGenre(Action).build(); var actionMovies = query.request(ENDPOINT).post(); Here a GraphQL query is embedded directly in a String literal as a fragment value. The resulting type is based on the fragment type in use. In this case the GraphQL type manifold provides a special type with the single purpose of exposing a query builder method matching the one the MoviesByGenre query defines. Note not all manifold resources can be used as fragment values. The fragment value concept is not always a good fit. For instance, the Properties manifold does not implement fragment values because a properties type is used statically. Note fragments as values are more useful with multiline String literals via the new Text Blocks feature in Java 15: Note to Type Manifold service providers To support fragments as values you must annotate your toplevel types with @FragmentValue. This annotation defines two parameters: methodName and type, where methodName specifies the name of a static method to call on the top-level type that represents the type's value, and where type is the qualified name of the value type, which must be contravariant with the methodName return type. See the GraphQL type manifold implementation for a reference. IDE Support Manifold is fully supported in IntelliJ IDEA and Android Studio. Install Get the Manifold plugin directly from within the IDE via: Settings Plugins Marketplace search: Manifold Projects The Manifold framework consists of the core project and a collection of sub-projects implementing SPIs provided by the core. Each project represents a separate dependency you can easily add to your project: Manifold : Core Manifold : Java Extensions Manifold : GraphQL Manifold : JSON Manifold : XML Manifold : YAML Manifold : CSV Manifold : Property files Manifold : Image files Manifold : Dark Java Manifold : JavaScript Manifold : Properties Manifold : Java Templates Manifold : String Interpolation Manifold : (Un)checked Exceptions Manifold : Preprocessor Manifold : Science Manifold : Collections Manifold : I/0 Manifold : Text Use the sample projects for nice working examples of how to configure and use Manifold in your project. Manifold : Sample App Manifold : Sample GraphQL App Manifold : Sample REST API App Manifold : Sample Web App Manifold : Sample Kotlin App Manifold : Gradle Example Project Setup Building this project The manifold project is defined with Maven. To build it install Maven and run the following command. Using this project The manifold core dependency works with all build tooling, including Maven and Gradle. It also works with Java versions 8 - 17. This project consists of two modules: manifold manifold-rt For optimal performance and to work with Android and other JVM languages it is recommended to: Add a dependency on manifold-rt (Gradle: \"implementation\", Maven: \"compile\") Add manifold to the annotationProcessor path (Gradle: \"annotationProcessor\", Maven: \"annotationProcessorPaths\") See Gradle and Maven examples below. Binaries If you are not using Maven or Gradle, you can download the latest binaries here. Gradle Note, if you are targeting Android, please see the Android docs. Note, if you are using Kotlin, please see the Kotlin docs. Here is a sample build.gradle script. Change targetCompatibility and sourceCompatibility to your desired Java version (8 - 17), the script takes care of the rest. plugins { id 'java' } group 'com.example' version '1.0-SNAPSHOT' targetCompatibility = 11 sourceCompatibility = 11 repositories { jcenter() maven { url 'https://oss.sonatype.org/content/repositories/snapshots/' } } configurations { // give tests access to annotationProcessor dependencies testImplementation.extendsFrom annotationProcessor } dependencies { implementation 'systems.manifold:manifold-rt:2021.1.30' testImplementation 'junit:junit:4.12' // Add manifold to -processorpath for javac annotationProcessor group: 'systems.manifold', name: 'manifold', version: '2021.1.30' } if (JavaVersion.current() != JavaVersion.VERSION_1_8 && sourceSets.main.allJava.files.any {it.name == \"module-info.java\"}) { tasks.withType(JavaCompile) { // if you DO define a module-info.java file: options.compilerArgs += ['-Xplugin:Manifold', '--module-path', it.classpath.asPath] } } else { tasks.withType(JavaCompile) { // If you DO NOT define a module-info.java file: options.compilerArgs += ['-Xplugin:Manifold'] } } Use with accompanying settings.gradle file: rootProject.name = 'MyProject' Maven <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>com.example</groupId> <artifactId>my-manifold-app</artifactId> <version>0.1-SNAPSHOT</version> <name>My Manifold App</name> <properties> <!-- set latest manifold version here --> <manifold.version>2021.1.30</manifold.version> </properties> <dependencies> <dependency> <groupId>systems.manifold</groupId> <artifactId>manifold-rt</artifactId> <version>${manifold.version}</version> </dependency> </dependencies> <!--Add the -Xplugin:Manifold argument for the javac compiler--> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <source>11</source> <target>11</target> <encoding>UTF-8</encoding> <compilerArgs> <!-- Configure manifold plugin--> <arg>-Xplugin:Manifold</arg> </compilerArgs> <!-- Add the processor path for the plugin --> <annotationProcessorPaths> <path> <groupId>systems.manifold</groupId> <artifactId>manifold</artifactId> <version>${manifold.version}</version> </path> </annotationProcessorPaths> </configuration> </plugin> </plugins> </build> </project> Platforms Manifold supports: Java SE (8 - 17) Android Kotlin (limited) Comprehensive IDE support is also available for IntelliJ IDEA and Android Studio. Javadoc manifold: manifold-rt: License Open source Manifold is free and licensed under the Apache 2.0 license. Versioning For the versions available, see the tags on this repository. Author Scott McKinney "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/manifold-systems/manifold/tree/master/manifold-core-parent/manifold#the-big-picture",
        "comments.comment_id": [21415489, 21416541],
        "comments.comment_author": ["twunde", "staticjak"],
        "comments.comment_descendants": [1, 2],
        "comments.comment_time": [
          "2019-11-01T01:22:14Z",
          "2019-11-01T05:48:30Z"
        ],
        "comments.comment_text": [
          "This seems to be one of the ideas that just comes up again and again in the Java community. While it has its uses, it comes with a free downsides. The biggest is that debugging becomes very difficult, especially as the code becomes complex. Imagine building it a framework with a subtle is bug and your newest junior engineer struggling with it.",
          "This reminds me of AspectJ in that its a language extension with many similar features (extension methods etc). As useful as those features were, the syntactic sugar you get is confusing to new contributors to a project.<p>Here's a scenario thats played out. In the extension methods example on the Manifold README, it's highlighted that you can add your own methods to Java types. If any dev saw this in the project and they were unaware of manifold, it wouldn't be immediately clear what enables the addition of methods to the Java String type. They would figure it out eventually by either investigating or by debugging. These features blur the lines between programming language and framework.<p>Where I get weary about adding a dependency is when I see documentation with ~100 line boilerplate examples peppered with terms I've never heard before. Just because you use the word simple when describing something complex does not make it so."
        ],
        "id": "78b4a18c-da97-45ff-98db-0317ec6cca34",
        "url_text": "The core framework plugs directly into the Java compiler via the Javac plugin API as a universal type adapter to allow for a direct and seamless supply of types and features otherwise inaccessible to Java's type system. As such the Manifold core framework provides a foundation and plugin SPI to dynamically resolve type names and produce corresponding Java sources, and to more generally augment Java's type system. Such a plugin is called a type manifold and implements the ITypeManifold SPI. Table of Contents The Big Picture The API Anatomy of a Type Manifold Configuring Manifold Explicit Resource Compilation Modes Programming Language Manifolds Embedding with Fragments (experimental) IDE Support Projects Sample Projects Setup Platforms Javadoc License Versioning Author The Big Picture You can think of a type manifold as a just-in-time code generator. Essentially the Manifold framework plugs in and overrides the compiler's type resolver so that, via the ITypeManifold SPI, a type manifold can claim ownership of type names as the compiler encounters them and dynamically provide source code corresponding with the types. As a consequence this core functionality serves as a productive alternative to conventional code generation techniques, a long overdue advancement for static typing. To begin with, because the framework plugs directly into the compiler, a code generator as a type manifold is no longer a separate build step. What's more a type manifold generates code on-demand as the compiler asks for types. Not only does this significantly reduce the complexity of code generators, it also enables them to function incrementally. This means resources on which the generated sources are based can be edited and only the types corresponding with the changes will be regenerated and recompiled. Thus, contrary to conventional code generators, type manifolds: require zero build steps produce zero on-disk source code are by definition always in sync with resources are inherently incremental resulting in optimal build times are dead simple to use - just add a dependency to your project Moreover, type manifolds can cooperate and contribute source to types in different ways. Most often a type manifold registers as a primary contributor to supply the main body of the type. The JSON type manifold, for example, is a primary contributor because it supplies the full type definition corresponding with a JSON Schema file or sample file. Alternatively, a type manifold can be a partial or supplementary contributor. For instance, the Extension type manifold is a supplementary contributor because it augments an existing type with additional methods, interfaces, and other features. Thus both the JSON and Extension type manifolds can contribute to the same type, where the JSON manifold supplies the main body and the Extension type manifold contributes custom methods and other features provided by extension classes. As a consequence any number of type manifolds can operate in concert to form an efficient and powerful type building pipeline, thus unlike conventional code generators, type manifolds: can easily have dependencies on one another can easily contribute to one another's types are customizable with extensions Finally, the Manifold core framework can be used from IDEs and other tooling to provide consistent, unified access to the types and features produced from all type manifolds. For instance, the Manifold plugin for IntelliJ IDEA provides comprehensive support for the Manifold framework. All types and features produced from type manifolds and other Manifold SPIs are fully supported. You can edit resources such as JSON and GraphQL files and immediately use the changes in Java without a compilation step. Features like code completion, resource/code navigation, deterministic usage searching, refactoring/renaming, incremental compilation, hotswap debugging, etc. work seamlessly with all type manifolds past, present, and future. This represents a tremendous leap in productivity compared with the conventional code generation world where the burden is on the code generator author or third party to invest in one-off IDE tooling projects, which typically results in poor or no IDE representation. Thus another big advantage type manifolds possess over conventional code generators is: a unified framework... ...which enables comprehensive IDE support and more To summarize, the Manifold framework provides a clear advantage over conventional code generation techniques. Type manifolds do not entail build steps, are always in sync, operate incrementally, and are simple to add to any project. They also cooperate naturally to form a powerful type building pipeline, which via the core framework is uniformly accessible to IDEs such as IntelliJ IDEA and Android Studio. Putting it all together, the synergy resulting from these improvements has the potential to significantly increase Java developer productivity and to open minds to new possibilities. The API The framework consists of the several SPIs: ITypeManifold This SPI is the basis for implementing a type manifold. See existing type manifold projects such as manifold-graphql. ICompilerComponent Implement this low-level SPI to supplement Java with new or enhanced behavior e.g., manifold-strings and manifold-exceptions. IPreprocessor Implement this SPI to provide a preprocessor to filter source before it enters Java's parser e.g., manifold-preprocessor. Anatomy of a Type Manifold Any data resource is a potential type manifold. These include file schemas, query languages, database definitions, data services, templates, spreadsheets, programming languages, and more. So while the Manifold team provides several type manifolds out of the box the domain of possible type manifolds is virtually unlimited. Importantly, their is nothing special about the ones we provide -- you can build type manifolds using the same public API with which ours are built. The API is comprehensive and aims to fulfill the 80/20 rule -- the common use-cases are straightforward to implement, but the API is flexible enough to achieve almost any kind of type manifold. For instance, since most type manifolds are resource file based the API foundation classes handle most of the tedium with respect to file management, caching, and modeling. Also, since the primary responsibility for a type manifold is to dynamically produce Java source, Manifold provides a simple API for building and rendering Java classes. But the API is flexible so you can use other tooling as you prefer. Most resource file based type manifolds consist of three basic classes: JavaTypeManifold subclass A class to produce Java source Model subclass The Image manifold is relatively simple and nicely illustrates this structure: JavaTypeManifold Subclass public class ImageTypeManifold extends JavaTypeManifold<Model> { private static final Set<String> FILE_EXTENSIONS = new HashSet<>(Arrays.asList(\"jpg\", \"png\", \"bmp\", \"wbmp\", \"gif\")); @Override public void init(IModule module) { init(module, Model::new); } @Override public boolean handlesFileExtension(String fileExtension) { return FILE_EXTENSIONS.contains(fileExtension.toLowerCase()); } @Override protected String aliasFqn(String fqn, IFile file) { return fqn + '_' + file.getExtension(); } @Override protected boolean isInnerType(String topLevel, String relativeInner) { return false; } @Override protected String produce(String topLevelFqn, String existing, Model model, DiagnosticListener<JavaFileObject> errorHandler) { SrcClass srcClass = new ImageCodeGen(model._url, topLevelFqn).make(); StringBuilder sb = srcClass.render(new StringBuilder(), 0); return sb.toString(); } } Like most type manifolds the image manifold is file extension based, specifically it handles the domain of files having image extensions: jpg, png, etc. As you'll see, the JavaTypeManifold base class is built to handle this use-case. First, ImageTypeManifold overrides the init() method to supply the base class with its Model. We'll cover that shortly. Next, it overrides handlesFileExtension() to tell the base class which file extensions it handles. Next, since the image manifold produces classes with a slightly different name than the base file name, it overrides aliasFqn() to provide an alias for the qualified name of the form <package>.<image-name>_<ext>. The name must match the class name the image manifold produces. There are no inner classes produced by this manifold, therefore it overrides isInnerType() returning false; the base class must ask the subclass to resolve inner types. Finally, the image manifold overrides contribute(), this is where you contribute Java source for a specified class name. Source Production Class Most often, you'll want to create a separate class to handle the production of Java source. The image manifold does that with ImageCodeGen: public class ImageCodeGen { private final String _fqn; private final String _url; ImageCodeGen(String url, String topLevelFqn) { _url = url; _fqn = topLevelFqn; } public SrcClass make() { String simpleName = ManClassUtil.getShortClassName(_fqn); return new SrcClass(_fqn, SrcClass.Kind.Class).imports(URL.class, SourcePosition.class) .superClass(new SrcType(ImageIcon.class)) .addField(new SrcField(\"INSTANCE\", simpleName).modifiers(Modifier.STATIC)) .addConstructor(new SrcConstructor() .addParam(new SrcParameter(\"url\") .type(URL.class)) .modifiers(Modifier.PRIVATE) .body(new SrcStatementBlock() .addStatement(new SrcRawStatement() .rawText(\"super(url);\")) .addStatement(new SrcRawStatement() .rawText(\"INSTANCE = this;\")))) .addMethod(new SrcMethod().modifiers(Modifier.STATIC) .name(\"get\") .returns(simpleName) .body(new SrcStatementBlock() .addStatement( new SrcRawStatement() .rawText(\"try {\") .rawText(\" return INSTANCE != null ? INSTANCE : new \" + simpleName + \"(new URL(\"\\\\\" + ManEscapeUtil.escapeForJavaStringLiteral(_url) + \"\\\\\"));\") .rawText(\"} catch(Exception e) {\") .rawText(\" throw new RuntimeException(e);\") .rawText(\"}\")))); } } Here the image manifold utilizes SrcClass to build a Java source model of image classes. SrcClass is a source code production utility in the Manifold API. It's simple and handles basic code generation use-cases. Feel free to use other Java source code generation tooling if SrcClass does not suit your use-case, because ultimately your only job here is to produce a String consisting of Java source for your class. Model Subclass The third and final class the image manifold provides is the Model class: class Model extends AbstractSingleFileModel { String _url; Model(String fqn, Set<IFile> files) { super(fqn, files); assignUrl(); } private void assignUrl() { try { _url = getFile().toURI().toURL().toString(); } catch (MalformedURLException e) { throw new RuntimeException(e); } } public String getUrl() { return _url; } @Override public void updateFile(IFile file) { super.updateFile(file); assignUrl(); } } This class models the image data necessary for ImageCodeGen to produce source as a AbstractSingleFileModel subclass. In this case the model data is simply the URL for the image. Additionally, Model overrides updateFile() to keep the URL up to date in environments where it can change, such as in an IDE. Registration In order to use a type manifold in your project, it must be registered as a service. Normally, as a type manifold provider to save users of your manifold from this step, you self-register your manifold in your META-INF directly like so: src -- main ---- resources ------ META-INF -------- services ---------- manifold.api.type.ITypeManifold Following standard Java ServiceLoader protocol you create a text file called manifold.api.type.ITypeManifold in the service directory under your META-INF directory. The file should contain the fully qualified name of your type manifold class (the one that implements ITypeManifold) followed by a new blank line: As you can see building a type manifold can be relatively simple. The image manifold illustrates the basic structure of most file-based manifolds. Of course there's much more to the API. Examine the source code for other manifolds such as the GraphQL manifold (manifold-graphql) and the JavaScript manifold (manifold-js). These serve as decent reference implementations for wrapping parsers and binding to existing languages. Note with Java 9+ with named modules you register a service provider in your module-info.java file using the provides keyword: provides manifold.api.type.ITypeManifold with com.abc.MyTypeManifold Configuring Manifold There are three ways you can configure Manifold in your build: static, dynamic, and mixed. Each configuration type is determined by the following criteria: Use of compileOnly (or provided) scoping on all Manifold dependencies your project uses that are specific to compilation Use of the dynamic argument to the Manifold javac plugin e.g., -Xplugin:Manifold vs. -Xplugin:Manifold dynamic static dynamic mixed compileOnly scope dynamic plugin arg Note, static is the recommended configuration for most projects. Using it results in smaller, faster, more versatile projects. static: Compiles all resource types statically at compile-time. Distributes only Manifold runtime dependencies. Advantages: Performance. Since resource types are compiled ahead of time as .class files, load time for a resource type is the same a Java type. Zero startup time. The dynamic compilation subsystem and other services exclusive to compilation are not present at runtime, therefore Manifold has ZERO impact on startup time. Footprint. Since only runtime modules are distributed in this configuration, the resulting runtime footprint is more than 10x smaller than dynamic and mixed configurations, which translates to faster downloads. Android. Applications using Manifold statically fully support Android. Kotlin. Applications using Manifold statically fully support Kotlin and other JVM languages. Limitations: Dynamic compilation (compilation at runtime) is not supported. Usage of dynamic features such as Dark Java can't be used. Considering Dark Java is the only remaining exclusively dynamic feature of Manifold, this is a rather minor limitation. dynamic: Compiles all resource types dynamically at runtime. No resource types are compiled to .class files at compile-time. Both compile-time and runtime binaries are included in your distribution. Advantages: Dynamic. Purely dynamic features such as Dark Java enable dynamic runtime compilation of resources. Limitations: Slower startup and initialization. Manifold dynamic services contribute to a slower startup time. Additionally, initialization times for resource types are slower due to dynamic compilation on initial load. Larger footprint. Dynamic services used at compile-time are also distributed with the runtime, the additional files significantly impact the overall footprint (more than 10x). Limited use. Only Java SE environments are supported. Other JVM environments such as Android and Kotlin are not supported using this configuration. mixed: Compiles project resource types statically such as resource files, and compiles dynamic resource types dynamically such as Dark Java. Both compile-time and runtime binaries are included in your distribution. Advantages: Dynamic. Purely dynamic features such as Dark Java enable dynamic runtime compilation of resources. Performance. Since resource types are compiled ahead of time as .class files, load time for a resource type is the same a Java type. Limitations: Slower startup and initialization. Manifold dynamic services contribute to a slower startup time. Additionally, initialization times for resource types are slower due to dynamic compilation on initial load. Larger footprint. Dynamic services used at compile-time are also distributed with the runtime, the additional files significantly impact the overall footprint (more than 10x). Limited use. Only Java SE environments are supported. Other JVM environments such as Android and Kotlin are not supported using this configuration. Configurations by supported features static dynamic mixed Performance Small footprint Zero startup time Java SE Android Kotlin** Dynamic Java **All Manifold resource types such as GraphQL and JSON are fully supported with Kotlin, however Manifold Java Extension features such as @Jailbreak, unit expressions, and the preprocessor are specific to the Java compiler. Configuring Dependencies Manifold components are structured to support both static and dynamic use. For instance, the Manifold core component consists of two modules: manifold and manifold-rt, where manifold contains all the compile-time functionality and manifold-rt contains code exclusive to runtime APIs and internal runtime implementation. As such, to use the Manifold core modules statically with your project you add a compile-only dependency on manifold and a default dependency on manifold-rt. All of Manifold modules are designed in this way. As a result, components like Manifold core that provide both compile-time and runtime functionality consist of two modules manifold-xxx and manifold-xxx-rt, whereby a compile-only scope is used on manifold-xxx and a default scope is used for manifold-xxx-rt. A component that is exclusive to compile-time or that does not provide any features exclusive to compile-time does not define a separate \"rt\" module. For instance, the manifold-preprocessor is exclusive to compile-time use, therefore you always add it with a compile-only scope. Conversely, the manifold-science library does not define any features exclusive to compile-time, so you always use it with default scoping so it is packaged with your executable artifact. Static Configuration (default) Add the -Xplugin:Manifold javac argument If using Kotlin or other alternative JVM language, put Manifold resources in a separate Java compiled module and add -Amanifold.source.<file-ext>=<type-name-regex> javac arguments to explicitly compile the resources. See Explicit Resource Compilation below. Use compile-only (Gradle) or provided (Maven) scoping for Manifold dependencies exclusive to compile-time features Use default scoping for \"rt\" dependencies and any dependencies that are needed at runtime See Setup for examples of using Manifold statically. Note, static is the recommended configuration for most projects. Using it results in smaller, faster, more versatile projects. Dynamic Configuration Add the -Xplugin:Manifold dynamic javac argument Use default scoping for Manifold dependencies, there is no need to add dependencies to \"rt\" modules. See Setup for examples of using Manifold dynamically. Mixed Configuration Add the -Xplugin:Manifold javac argument Use default scoping for Manifold dependencies, there is no need to add dependencies to \"rt\" modules. You can statically compile as much or as little as preferred. See Setup for examples using Manifold both statically and dynamically. Note, the only difference between dynamic and mixed configuration is the dynamic Manifold plugin argument, which prevents Manifold from compiling resource types to disk. Runtime Dependencies If a project uses any manifold runtime dependencies (\"rt\" dependencies) by default manifold inserts a static block into all your classes to automatically initialize some runtime services: static { IBootstrap.dasBoot(); } The dasBoot() call invokes all registered IBootstrap services. The number and nature of the implementations depends on what your application does with manifold. For instance, if you are using manifold dynamically, this call initializes the dynamic compilation services among other tasks. Most projects, however, use manifold statically which means dasBoot() typically does only the following: Disables the Java 9 warning \"An illegal reflective access operation has occurred\", because that message has a history of unnecessarily alarming users. This is a noop when running on Java 8 / Android. Dynamically open the java.base module to the manifold module for common reflection access, which is a noop running on Java 8 / Android Note dasBoot() performs these tasks one time, subsequent calls simply return i.e., there is no performance penalty. If you know your code will never run on Java 9+ and/or you don't mind the Java 9+ warning message, you can eliminate the dasBoot() static initializer via the no-bootstrap plugin argument: Gradle options.compilerArgs += ['-Xplugin:Manifold no-bootstrap'] Maven <compilerArgs> <arg>-Xplugin:Manifold no-bootstrap</arg> </compilerArgs> If you need finer grained control over which classes have the static block, you can use the @NoBootstrap annotation to filter specific classes. Note, compile-only dependencies such as manifold-preprocessor, manifold-exceptions, and manifold-strings don't involve any runtime dependencies, thus if your project's exposure to manifold is limited to these dependencies, the static block is never inserted in any of your project's classes. Explicit Resource Compilation By default, Manifold compiles resource types to disk as the Java compiler encounters them in your code. As a consequence, a resource that is never used in your code as a type is not compiled. For example, if you have hundreds of JSON resource files, but your project only uses a handful type-safely with Manifold, only the handful is compiled to disk as .class files. This scheme is unsuitable, however, for modules intended for use as a dependency or library where the set of resources that needs to be compiled may include more than the ones used inside the module. For instance, an API that defines a query model in terms of GraphQL files may not use any of the resource files directly, but a module using the API would. Although Manifold can still work in situations like this (see Modes above), it does so by compiling types dynamically at runtime, which entails a performance bump the first time each type loads. If need be, you can avoid the runtime compilation cost using explicit static compilation with javac command line options. Similar to javac's source file list, Manifold provides -Akey=value javac command line options to explicitly compile resources either by type name using regular expressions or by file name using file system paths. Constraining by type name is the simplest and more flexible of the two, especially in terms of build systems such as Maven and Gradle. See the Sample Kotlin App for an example of using explicit resource compilation. By Type Name Use the -Amanifold.source.<file-ext>=<type-name-regex> javac command line option to specify Manifold types that should statically compile, whether or not they are referenced elsewhere in Java source. An easy way to tell the Java compiler to compile all the files corresponding with all the type manifolds enabled in your project: javac -Amanifold.source.*=.* ... The * wildcard selects all type manifolds and the .* regular expression selects all types for each type manifold, therefore all types expressible through Manifold are statically compiled to disk with javac. You can limit compilation to types relating to a specific file extension. For instance, if you are using the JSON manifold and you want all your JSON files to be statically compiled: javac -Amanifold.source.json=.* ... Define several arguments and use any regular expression to refine the set of types to compile: javac -Amanifold.source.json=.* -Amanifold.source.graphql=^com\\.example\\..*Queries$ ... This tells the compiler to compile all JSON files and to compile GraphQL types in package com.example ending with Queries. If need be, you can use regular expressions to invert or \"black list\" inclusions. javac -Amanifold.source.graphql=^(?!(com\\.example\\..*Queries)).*$ ... Here all GraphQL types compile except those in package com.example ending with Queries. Using the class: prefix you can constrain compilation by the class name of a type manifold. This is useful for type manifolds not based on files or file extensions. javac -Amanifold.source.class:com.example.MySpecialTypeManifold=.* ... Note, as a reminder, the javac command line arguments are additive with respect to types compiled to disk. As a general rule Manifold types referenced in Java source are always compiled, regardless of command line argument constraints. Adding Source Paths If you use the -Amanifold.source.<ext>=<regex argument, but your resources reside a directory other than Java source directories or resource directories, you can specify additional source paths that are exclusive to Manifold type compilation using the -Amanifold.source=<paths> argument. javac -Amanifold.source=/myproject/src/main/stuff ... This example adds /myproject/src/main/stuff as an additional Manifold source path. Your -Amanifold.source.<ext>=<regex arguments apply to this directory. By File Name Using path-based javac -Aother.source.files argument you can enumerate resource files that should compile statically regardless of whether or not the files are referenced in your code. javac -Aother.source.files=/myproject/src/main/resources/com/example/Queryies.gql /myproject/src/main/resources/com/example/Mutations.gql ... Use other.source.list to specify a file that contains a list of resource files that should compile statically regardless of whether or not the files are referenced in your code. The file contains a single resource path per line. javac -Aother.source.list=/myproject/target/otherfiles.txt ... Build Tooling If you define your project with Maven, you can explicitly compile resources with javac arguments like this: <!-- Configure Manifold as a Javac plugin --> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <source>8</source> <target>8</target> <encoding>UTF-8</encoding> <compilerArgs> <arg>-Xplugin:Manifold</arg> <arg>-Amanifold.source.json=.*</arg> <arg>-Amanifold.source.graphql=^com\\.example\\..*Queries$</arg> </compilerArgs> </configuration> </plugin> Similarly, with Gradle you add the arguments like this: compileJava { options.compilerArgs += ['-Xplugin:Manifold', '-Amanifold.source.json=.*', '-Amanifold.source.graphql=^com\\\\.example\\\\..*Queries$'] } Exposing Resource Types Another benefit from statically compiling resources relates to resource exposure. If your resources are statically compiled, they are available for use as .class files, not only from your own code, but also from potential consumers of your code. This is an important distinction to make because if you don't statically compile resources that are intended for use outside your project, say as part of an API you provide, those resources are not discoverable from another module using Manifold unless you explicitly expose them from your JAR-based artifact. You can do that using the Contains-Sources manifest entry. <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-jar-plugin</artifactId> <configuration> <archive> <manifestEntries> <!--expose GraphQL files if they are NOT statically compiled in your project--> <Contains-Sources>graphql</Contains-Sources> <!--JPMS module name--> <Automatic-Module-Name>mymodule</Automatic-Module-Name> </manifestEntries> </archive> </configuration> </plugin> Although Manifold could use the entire class path as the domain of potential resources types, doing so may impact performance. That's why you must opt-in your module's resources for external use. It bears repeating, if you statically compile all the resources intended for use outside your project, you do not need to opt-in your JAR for processing -- the resources are already available as .class files. Modes You can use Manifold in one of two modes which you control as an optional argument to the Manifold plugin for javac: static: -Xplugin:Manifold (default) compiles resource types statically at compile-time dynamic: -Xplugin:Manifold dynamic compiles resource types dynamically at runtime (alternatively -Xplugin:\"Manifold dynamic\", some tools may require quotes) Most projects benefit most using the default (static) mode. Dynamic mode in most cases should be reserved for specific type manifolds that are better suited to dynamic compilation. Note if you're not sure which mode to use, try the default static mode -- it's usually the right choice. General information considering the static v. dynamic mode: Both modes operate lazily: a type is not compiled unless it is used. For example, if you are using the JSON manifold, only the JSON files you reference in your code will be processed and compiled. This means Manifold will not try to compile resources your project does not expect to use directly as types. Even if you use static mode, you can still reference type manifold classes dynamically e.g., reflectively. In such a case Manifold will dynamically compile the referenced class as if you were operating in dynamic mode. In general, your code will work regardless of the mode you're using, hence the general recommendation to stay with static mode where you get the best of both worlds. Dynamic mode requires tools.jar at runtime for Java 8. Static mode is generally faster at runtime since it pre-compiles all the type manifold resources along with Java sources when you build your project Static mode automatically supports incremental compilation and hotswap debugging of modified resources in IntelliJ Note, you can use javac command line arguments to statically compile a set of specified types whether you use them directly in your code e.g., if type-safe resources are part of an API. See Explicit Resource Compilation. Programming Language Manifolds todo: use the Javascript manifold as a simple impl, and the Gosu manifold as a more complex one as reference material for this section Embedding with Fragments (experimental) You can now embed resource content such as JSON, GraphQL, XML, YAML, CSV, etc. directly in a Java source file as a type-safe resource fragment. A fragment has the same format and grammar as a resource file and, if used with the Manifold IDE plugin, can be authored with rich editor features like code highlighting, parser feedback, code completion, etc. This means you can directly embed resources closer to where you use them in your code. For instance, you can type-safely write a query in the query language your application uses directly in the Java method that uses the query. You can embed a fragment as a declaration or a value. Type Declaration You can type-safely embed resources directly in your Java code as declarations. Here's a simple example using Javascript: public class MyJavaClass { void foo() { /*[>Barker.js<] function callBark(aBarker) { aBarker.bark(); } */ Barker.callBark(new Dog()); Barker.callBark(new Tree()); class Dog { public String bark() { return \"ruff\"; } } class Tree { public String bark() { return \"rough\"; } } } } Notice the Javascript is embedded in a multiline comment. This is how you embed any kind of resource fragment as a type declaration. Here the Javascript type is declared as Barker with a .js extension indicating the resource type. Note a fragment must use [> and <] at the beginning of the comment to delimit the type name and extension. A fragment covers the remainder of the comment and must follow the format of the declared extension. You can embed any Manifold enabled resource as a fragment type declaration. Here's another example using JSON: void foo() { /*[>Planet.json<] { \"name\": \"Earth\", \"system\": { \"name\": \"Sol\", \"mass\": 1.0014 } } */ // Work with the content type-safely Planet planet = Planet.fromSource(); String name = planet.getName(); Planet.system sys = planet.getSystem(); // Make a REST call Planet.request(endpoint).postOne(planet); // Use the JSON bindings Map<String, Object> jsonBindings = planet.getBindings(); // Make a new Planet Planet mars = Planet.builder() .withName(\"Mars\") .withSystem(sys) .build(); // Transform to another format String yaml = planet.writer().toYaml(); } Scoping A fragment can be embedded anywhere in your code. The type declared in the fragment is scoped to the package of the enclosing class. Thus in the example Barker is accessible anywhere in the enclosing foo method as well as foo's declaring class and other classes in its package. Note, even though the declared type is package scoped, for the sake of readability it is best to define the fragment nearest to its intended use. In a future release this level of scoping may be enforced. Rich Editing If used with the Manifold IntelliJ IDEA plugin or the Android Studio plugin, you can edit fragments as if they were in a separate file with all the editor features you'd expect like highlighting, parser feedback, code completion, etc. This is especially useful with GraphQL, SQL, and similar resources where editing a fragment in place provides a more fluid development experience. Value Fragments Sometimes it's more convenient to use a fragment as a value as opposed to a type declaration. For example, you can create a GraphQL query as a fragment value and assign it to a variable: var moviesByGenre = \"[>.graphql<] query MoviesByGenre($genre: genre) { movies(genre: $genre) { title } }\"; var query = moviesByGenre.builder().withGenre(Action).build(); var actionMovies = query.request(ENDPOINT).post(); Here a GraphQL query is embedded directly in a String literal as a fragment value. The resulting type is based on the fragment type in use. In this case the GraphQL type manifold provides a special type with the single purpose of exposing a query builder method matching the one the MoviesByGenre query defines. Note not all manifold resources can be used as fragment values. The fragment value concept is not always a good fit. For instance, the Properties manifold does not implement fragment values because a properties type is used statically. Note fragments as values are more useful with multiline String literals via the new Text Blocks feature in Java 15: Note to Type Manifold service providers To support fragments as values you must annotate your toplevel types with @FragmentValue. This annotation defines two parameters: methodName and type, where methodName specifies the name of a static method to call on the top-level type that represents the type's value, and where type is the qualified name of the value type, which must be contravariant with the methodName return type. See the GraphQL type manifold implementation for a reference. IDE Support Manifold is fully supported in IntelliJ IDEA and Android Studio. Install Get the Manifold plugin directly from within the IDE via: Settings Plugins Marketplace search: Manifold Projects The Manifold framework consists of the core project and a collection of sub-projects implementing SPIs provided by the core. Each project represents a separate dependency you can easily add to your project: Manifold : Core Manifold : Java Extensions Manifold : GraphQL Manifold : JSON Manifold : XML Manifold : YAML Manifold : CSV Manifold : Property files Manifold : Image files Manifold : Dark Java Manifold : JavaScript Manifold : Properties Manifold : Java Templates Manifold : String Interpolation Manifold : (Un)checked Exceptions Manifold : Preprocessor Manifold : Science Manifold : Collections Manifold : I/0 Manifold : Text Use the sample projects for nice working examples of how to configure and use Manifold in your project. Manifold : Sample App Manifold : Sample GraphQL App Manifold : Sample REST API App Manifold : Sample Web App Manifold : Sample Kotlin App Manifold : Gradle Example Project Setup Building this project The manifold project is defined with Maven. To build it install Maven and run the following command. Using this project The manifold core dependency works with all build tooling, including Maven and Gradle. It also works with Java versions 8 - 17. This project consists of two modules: manifold manifold-rt For optimal performance and to work with Android and other JVM languages it is recommended to: Add a dependency on manifold-rt (Gradle: \"implementation\", Maven: \"compile\") Add manifold to the annotationProcessor path (Gradle: \"annotationProcessor\", Maven: \"annotationProcessorPaths\") See Gradle and Maven examples below. Binaries If you are not using Maven or Gradle, you can download the latest binaries here. Gradle Note, if you are targeting Android, please see the Android docs. Note, if you are using Kotlin, please see the Kotlin docs. Here is a sample build.gradle script. Change targetCompatibility and sourceCompatibility to your desired Java version (8 - 17), the script takes care of the rest. plugins { id 'java' } group 'com.example' version '1.0-SNAPSHOT' targetCompatibility = 11 sourceCompatibility = 11 repositories { jcenter() maven { url 'https://oss.sonatype.org/content/repositories/snapshots/' } } configurations { // give tests access to annotationProcessor dependencies testImplementation.extendsFrom annotationProcessor } dependencies { implementation 'systems.manifold:manifold-rt:2021.1.30' testImplementation 'junit:junit:4.12' // Add manifold to -processorpath for javac annotationProcessor group: 'systems.manifold', name: 'manifold', version: '2021.1.30' } if (JavaVersion.current() != JavaVersion.VERSION_1_8 && sourceSets.main.allJava.files.any {it.name == \"module-info.java\"}) { tasks.withType(JavaCompile) { // if you DO define a module-info.java file: options.compilerArgs += ['-Xplugin:Manifold', '--module-path', it.classpath.asPath] } } else { tasks.withType(JavaCompile) { // If you DO NOT define a module-info.java file: options.compilerArgs += ['-Xplugin:Manifold'] } } Use with accompanying settings.gradle file: rootProject.name = 'MyProject' Maven <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>com.example</groupId> <artifactId>my-manifold-app</artifactId> <version>0.1-SNAPSHOT</version> <name>My Manifold App</name> <properties> <!-- set latest manifold version here --> <manifold.version>2021.1.30</manifold.version> </properties> <dependencies> <dependency> <groupId>systems.manifold</groupId> <artifactId>manifold-rt</artifactId> <version>${manifold.version}</version> </dependency> </dependencies> <!--Add the -Xplugin:Manifold argument for the javac compiler--> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <source>11</source> <target>11</target> <encoding>UTF-8</encoding> <compilerArgs> <!-- Configure manifold plugin--> <arg>-Xplugin:Manifold</arg> </compilerArgs> <!-- Add the processor path for the plugin --> <annotationProcessorPaths> <path> <groupId>systems.manifold</groupId> <artifactId>manifold</artifactId> <version>${manifold.version}</version> </path> </annotationProcessorPaths> </configuration> </plugin> </plugins> </build> </project> Platforms Manifold supports: Java SE (8 - 17) Android Kotlin (limited) Comprehensive IDE support is also available for IntelliJ IDEA and Android Studio. Javadoc manifold: manifold-rt: License Open source Manifold is free and licensed under the Apache 2.0 license. Versioning For the versions available, see the tags on this repository. Author Scott McKinney ",
        "_version_": 1718536537974505472
      },
      {
        "story_id": 20449610,
        "story_author": "cube2222",
        "story_descendants": 70,
        "story_score": 310,
        "story_time": "2019-07-16T13:04:35Z",
        "story_title": "Show HN: OctoSQL – Query and join multiple databases and files, written in Go",
        "search": [
          "Show HN: OctoSQL – Query and join multiple databases and files, written in Go",
          "https://github.com/cube2222/octosql",
          "OctoSQL is a query tool that allows you to join, analyse and transform data from multiple databases, streaming sources and file formats using SQL. OctoSQL is currently being rewritten on the redesign branch. Problems OctoSQL Solves You need to join / analyze data from multiple datasources. Think of enriching an Excel file by joining it with a PostgreSQL database. You need stream aggregates over time, with live output updates. Think of a live-updated leaderboard with cat images based on a \"like\" event stream. You need aggregate streams per time window, with live output updates. Think of a unique user count per hour, per country live summary. Table of Contents What is OctoSQL? Installation Quickstart Temporal SQL Features Watermarks Triggers Retractions Example Durability Configuration JSON CSV Excel Parquet PostgreSQL MySQL Redis Kafka Documentation Architecture Datasource Pushdown Operations Roadmap What is OctoSQL? OctoSQL is a SQL query engine which allows you to write standard SQL queries on data stored in multiple SQL databases, NoSQL databases, streaming sources and files in various formats trying to push down as much of the work as possible to the source databases, not transferring unnecessary data. OctoSQL does that by creating an internal representation of your query and later translating parts of it into the query languages or APIs of the source databases. Whenever a datasource doesn't support a given operation, OctoSQL will execute it in memory, so you don't have to worry about the specifics of the underlying datasources. OctoSQL also includes temporal SQL extensions, to operate ergonomically on streams and respect their event-time (not the current system-time when the records are being processed). With OctoSQL you don't need O(n) client tools or a large data analysis system deployment. Everything's contained in a single binary. Why the name? OctoSQL stems from Octopus SQL. Octopus, because octopi have many arms, so they can grasp and manipulate multiple objects, like OctoSQL is able to handle multiple datasources simultaneously. Installation Either download the binary for your operating system (Linux, OS X and Windows are supported) from the Releases page, or install using the go command line tool: GO111MODULE=on go get -u github.com/cube2222/octosql/cmd/octosql Quickstart Let's say we have a csv file with cats, and a redis database with people (potential cat owners). Now we want to get a list of cities with the number of distinct cat names in them and the cumulative number of cat lives (as each cat has up to 9 lives left). First, create a configuration file (Configuration Syntax) For example: dataSources: - name: cats type: csv config: path: \"~/Documents/cats.csv\" - name: people type: redis config: address: \"localhost:6379\" password: \"\" databaseIndex: 0 databaseKeyName: \"id\" Then, set the OCTOSQL_CONFIG environment variable to point to the configuration file. export OCTOSQL_CONFIG=~/octosql.yaml You can also use the --config command line argument. Finally, query to your hearts desire: octosql \"SELECT p.city, FIRST(c.name), COUNT(DISTINCT c.name) cats, SUM(c.livesleft) catlives FROM cats c JOIN people p ON c.ownerid = p.id GROUP BY p.city ORDER BY catlives DESC LIMIT 9\" Example output: +---------+--------------+------+----------+ | p.city | c.name_first | cats | catlives | +---------+--------------+------+----------+ | Warren | Zoey | 68 | 570 | | Gadsden | Snickers | 52 | 388 | | Staples | Harley | 54 | 383 | | Buxton | Lucky | 45 | 373 | | Bethany | Princess | 46 | 366 | | Noxen | Sheba | 49 | 361 | | Yorklyn | Scooter | 45 | 359 | | Tuttle | Toby | 57 | 356 | | Ada | Jasmine | 49 | 351 | +---------+--------------+------+----------+ You can choose between live-table batch-table live-csv batch-csv stream-json output formats. (The live-* types will update the terminal view repeatedly every second, the batch-* ones will write the output once before exiting, the stream-* ones will print records whenever they are available) Temporal SQL Features OctoSQL features temporal SQL extensions inspired by the paper One SQL to Rule Them All. Introduction Often when you're working with streams of events, you'd like to use the time dimension somehow: Calculate average values for a day sliced by hours. Get unique user counts per day. and others All those examples have one thing in common: The time value of an event is crucial for correctness. A naive system could just use the current clock time whenever it receives an event. The correctness of this approach however, degrades quickly in the face of network problems, delivery delays, clock skew. This can be solved by using a value from the event as its time value. A new problem arises though: how do I know that I've received all events up to time X and can publish results for a given hour. You never know if there isn't somewhere a delayed event which should be factored in. This is where watermarks come into play. Watermarks Watermarks are a heuristic which try to approximate the \"current time\" when processing events. Said differently: When I receive a watermark for 12:00 I can be sure enough I've received all events of interest up to 12:00. To achieve this, they are generated at streaming sources and propagate downstream through the whole processing pipeline. The generation of watermarks usually relies on heuristics which provide satisfactory results for our given use case. OctoSQL currently contains the following watermark generators: Maximum difference watermark generator (with an offset argument) With an offset of 10 seconds, this generator says: When I've received an event for 12:00:00, then I'm sure I won't receive any event older than 11:59:50. Percentile watermark generator (with a percentile argument) With a percentile of 99.5, it will look at a specified number of recent events, and generate a watermark so that 99.5% of those events are after the watermark (not yet triggered), and the remaining 0.5% are before it. This way we set the watermark so that only a fraction of the recently seen events is potentially ignored as being late. Watermark generators are specified using table valued functions and are documented in the wiki. Triggers Another matter is triggering of keys in aggregations. Sometimes you'd like to only see the value for a given key (hour) when you know it's done, but othertimes you'd like to see partial results (how's the unique user count going this hour). That's where you can use triggers. Triggers allow you to specify when a given aggregate (or join window for that matter) is emitted or updated. OctoSQL contains multiple triggers: Watermark Trigger This is the most straightforward trigger. It emits a value whenever the watermark for a given key (or the end of the stream) is reached. So basically the \"show me when it's done\". Counting Trigger (with a count argument) This trigger will emit a value for a key every time it receives count records with this key. The count is reset whenever the key is triggered. Delay Trigger (with a delay argument) This trigger will emit a value for a key whenever the key has been inactive for the delay period. You can use multiple triggers simultaneously. (Show me the current sum every 10 received events, but also the final value after having received the watermark.) Retractions A key can be triggered multiple times with partial results. How do we know a given record is a retriggering of some key, and not a new unrelated record? OctoSQL solves this problem using a dataflow-like architecture. This means whenever a new value is sent for a key, a retraction is send for the old value. In practice this means every update is accompanied by the old record with an undo flag set. This can be visible when using a stream-* output format with partial results. Example Now we can see how it all fits together. In this example we have an events file, which contains records about points being scored in a game by multiple teams. WITH with_watermark AS (SELECT * FROM max_diff_watermark(source=>TABLE(events), offset=>INTERVAL 5 SECONDS, time_field=>DESCRIPTOR(time)) e), with_tumble AS (SELECT * FROM tumble(source=>TABLE(with_watermark), time_field=>DESCRIPTOR(e.time), window_length=> INTERVAL 1 MINUTE, offset => INTERVAL 0 SECONDS) e), counts_per_team AS (SELECT e.window_end, e.team, COUNT(*) as goals FROM with_tumble e GROUP BY e.window_end, e.team TRIGGER COUNTING 100, ON WATERMARK) SELECT * FROM counts_per_team cpt ORDER BY cpt.window_end DESC, cpt.goals ASC, cpt.team DESC We use common table expressions to break the query up into multiple stages. First we create the with_watermark intermediate table/stream. Here we use the table valued function max_diff_watermark to add watermarks to the events table - with an offset of 5 seconds based on the time record field. Then we use this intermediate table to create the with_tumble table, where we use the tumble table valued function to add a window_start and window_end field to each record, based on the record's time field. This assigns the records to 1 minute long windows. Next we create the counts_per_team table, which groups the records by their window end and team. Finally, we order those results by window end, goal count and team. Durability OctoSQL in its current design is based on on-disk transactional storage. All state is saved this way. All interactions with datasources are designed so that no records get duplicated in the face of errors or application restarts. You can also kill the OctoSQL process and start it again with the same query and storage-directory (command line argument), it will start where it left off. By default, OctoSQL will create a temporary directory for the state and delete it after termination. Configuration The configuration file has the following form dataSources: - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... ... physical: physical_plan_option: <value> Available OctoSQL-wide configuration options are: physical groupByParallelism: The parallelism of group by's and distinct queries. Will default to the CPU core count of your machine. streamJoinParallelism: The parallelism of streaming joins. Will default to the CPU core count of your machine. execution lookupJoinPrefetchCount: The count of simultaneously processed records in a lookup join. Supported Datasources JSON JSON file in one of the following forms: one record per line, no commas JSON list of records options: path - path to file containing the data, required arrayFormat - if the JSON list of records format should be used, optional: defaults to false batchSize - number of records extracted from json file in one storage transaction, optional: defaults to 1000 CSV CSV file separated using commas. The file may or may not have column names as it's first row. options: path - path to file containing the data, required headerRow - whether the first row of the CSV file contains column names or not, optional: defaults to true separator - columns separator, optional: defaults to \",\" batchSize - number of records extracted from csv file in one storage transaction, optional: defaults to 1000 Excel A single table in an Excel spreadsheet. The table may or may not have column names as it's first row. The table can be in any sheet, and start at any point, but it cannot contain spaces between columns nor spaces between rows. options: path - path to file, required headerRow - does the first row contain column names, optional: defaults to true sheet - name of the sheet in which data is stored, optional: defaults to \"Sheet1\" rootCell - name of cell (i.e \"A3\", \"BA14\") which is the leftmost cell of the first, optional: defaults to \"A1\" timeColumns - a list of columns to parse as datetime values with second precision row, optional: defaults to [] batchSize - number of records extracted from excel file in one storage transaction, optional: defaults to 1000 Parquet A single Parquet file. Nested repeated elements are not supported. Otherwise repeated xor nested elements are supported. Currently unsupported logical types, they will get parsed as the underlying primitive type: - ENUM - TIME with NANOS precision - TIMESTAMP with NANOS precision (both UTC and non-UTC) - INTERVAL - MAP options path - path to file, required batchSize - number of records extracted from parquet file in one storage transaction, optional: defaults to 1000 PostgreSQL Single PostgreSQL database table. options: address - address including port number, optional: defaults to localhost:5432 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from PostgreSQL database in one storage transaction, optional: defaults to 1000 MySQL Single MySQL database table. options: address - address including port number, optional: defaults to localhost:3306 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from MySQL database in one storage transaction, optional: defaults to 1000 Redis Redis database with the given index. Currently only hashes are supported. options: address - address including port number, optional: defaults to localhost:6379 password - optional: defaults to \"\" databaseIndex - index number of Redis database, optional: defaults to 0 databaseKeyName - column name of Redis key in OctoSQL records, optional: defaults to \"key\" batchSize - number of records extracted from Redis database in one storage transaction, optional: defaults to 1000 Kafka Multi-partition kafka topic. optional brokers - list of broker addresses (separately hosts and ports) used to connect to the kafka cluster, optional: defaults to [\"localhost:9092\"] topic - name of topic to read messages from, required partitions - topic partition count, optional: defaults to 1 startOffset - offset from which the first batch of messages will be read, optional: defaults to -1 batchSize - number of records extracted from Kafka in one storage transaction, optional: defaults to 1 json - should the messages be decoded as JSON, optional: defaults to false Documentation Documentation for the available functions: https://github.com/cube2222/octosql/wiki/Function-Documentation Documentation for the available aggregates: https://github.com/cube2222/octosql/wiki/Aggregate-Documentation Documentation for the available triggers: https://github.com/cube2222/octosql/wiki/Trigger-Documentation Documentation for the available table valued functions: https://github.com/cube2222/octosql/wiki/Table-Valued-Functions-Documentation The SQL dialect documentation: TODO ;) in short though: Available SQL constructs: Select, Where, Order By, Group By, Offset, Limit, Left Join, Right Join, Inner Join, Distinct, Union, Union All, Subqueries, Operators, Table Valued Functions, Trigger, Common Table Expressions. Available SQL types: Int, Float, String, Bool, Time, Duration, Tuple (array), Object (e.g. JSON) Describe You can describe the current plan in graphviz format using the -describe flag, like this: octosql \"...\" --describe | dot -Tpng > output.png Architecture An OctoSQL invocation gets processed in multiple phases. SQL AST First, the SQL query gets parsed into an abstract syntax tree. This phase only rules out syntax errors. Logical Plan The SQL AST gets converted into a logical query plan. This plan is still mostly a syntactic validation. It's the most naive possible translation of the SQL query. However, this plan already has more of a map-filter-reduce form. If you wanted to add a new query language to OctoSQL, the only problem you'd have to solve is translating it to this logical plan. Physical Plan The logical plan gets converted into a physical plan. This conversion finds any semantic errors in the query. If this phase is reached, then the input is correct and OctoSQL will be able execute it. This phase already understands the specifics of the underlying datasources. So it's here where the optimizer will iteratively transform the plan, pushing computation nodes down to the datasources, and deduplicating unnecessary parts. The optimizer uses a pattern matching approach, where it has rules for matching parts of the physical plan tree and how those patterns can be restructured into a more efficient version. The rules are meant to be as simple as possible and make the smallest possible changes. For example, pushing filters under maps, if they don't use any mapped variables. This way, the optimizer just keeps on iterating on the whole tree, until it can't change anything anymore. (each iteration tries to apply each rule in each possible place in the tree) This ensures that the plan reaches a local performance minimum, and the rules should be structured so that this local minimum is equal - or close to - the global minimum. (i.e. one optimization, shouldn't make another - much more useful one - impossible) Here is an example diagram of an optimized physical plan: Execution Plan The physical plan gets materialized into an execution plan. This phase has to be able to connect to the actual datasources. It may initialize connections, open files, etc. Stream Starting the execution plan creates a stream, which underneath may hold more streams, or parts of the execution plan to create streams in the future. This stream works in a pull based model. Datasource Pushdown Operations Datasource Equality In > < <= >= MySQL supported supported supported PostgreSQL supported supported supported Redis supported supported scan Kafka scan scan scan Parquet scan scan scan JSON scan scan scan CSV scan scan scan Where scan means that the whole table needs to be scanned for each access. Telemetry OctoSQL sends application telemetry on each run to help us gauge user interest and feature use. This way we know somebody uses our software, feel our work is actually useful and can prioritize features based on actual usefulness. You can turn it off (though please don't) by setting the OCTOSQL_TELEMETRY environment variable to 0. Telemetry is also fully printed in the output log of OctoSQL, if you want to see what precisely is being sent. Roadmap Additional Datasources. SQL Constructs: JSON Query HAVING, ALL, ANY Push down functions, aggregates to databases that support them. An in-memory index to save values of subqueries and save on rescanning tables which don't support a given operation, so as not to recalculate them each time. Runtime statistics Server mode Querying a json or csv table from standard input. Integration test suite Tuple splitter, returning the row for each tuple element, with the given element instead of the tuple. "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/cube2222/octosql",
        "comments.comment_id": [20449703, 20450152],
        "comments.comment_author": ["cube2222", "MoOmer"],
        "comments.comment_descendants": [12, 3],
        "comments.comment_time": [
          "2019-07-16T13:16:54Z",
          "2019-07-16T14:11:16Z"
        ],
        "comments.comment_text": [
          "Hey, one of the authors here.<p>The motivation behind this project is that I always wanted a simple commandline tool allowing me to join data from different places, without needing to set up stuff like presto or spark. On another hand, I never encountered any tool which allows me to easily query csv and json data using SQL (which at least in my opinion is fairly ergonomic to use).<p>This started as an university project, but we're now continuing it as an open source one, as it's been a great success so far.<p>Anyways, feedback greatly requested and appreciated!",
          "Really cool, thanks for sharing. You all might want to look at Apache Calcite (<a href=\"http://calcite.apache.org/\" rel=\"nofollow\">http://calcite.apache.org/</a>) as well for inspiration, which has similar functionality as a subset of its features!"
        ],
        "id": "add295d6-969f-4f3d-80dd-d203281227ec",
        "url_text": "OctoSQL is a query tool that allows you to join, analyse and transform data from multiple databases, streaming sources and file formats using SQL. OctoSQL is currently being rewritten on the redesign branch. Problems OctoSQL Solves You need to join / analyze data from multiple datasources. Think of enriching an Excel file by joining it with a PostgreSQL database. You need stream aggregates over time, with live output updates. Think of a live-updated leaderboard with cat images based on a \"like\" event stream. You need aggregate streams per time window, with live output updates. Think of a unique user count per hour, per country live summary. Table of Contents What is OctoSQL? Installation Quickstart Temporal SQL Features Watermarks Triggers Retractions Example Durability Configuration JSON CSV Excel Parquet PostgreSQL MySQL Redis Kafka Documentation Architecture Datasource Pushdown Operations Roadmap What is OctoSQL? OctoSQL is a SQL query engine which allows you to write standard SQL queries on data stored in multiple SQL databases, NoSQL databases, streaming sources and files in various formats trying to push down as much of the work as possible to the source databases, not transferring unnecessary data. OctoSQL does that by creating an internal representation of your query and later translating parts of it into the query languages or APIs of the source databases. Whenever a datasource doesn't support a given operation, OctoSQL will execute it in memory, so you don't have to worry about the specifics of the underlying datasources. OctoSQL also includes temporal SQL extensions, to operate ergonomically on streams and respect their event-time (not the current system-time when the records are being processed). With OctoSQL you don't need O(n) client tools or a large data analysis system deployment. Everything's contained in a single binary. Why the name? OctoSQL stems from Octopus SQL. Octopus, because octopi have many arms, so they can grasp and manipulate multiple objects, like OctoSQL is able to handle multiple datasources simultaneously. Installation Either download the binary for your operating system (Linux, OS X and Windows are supported) from the Releases page, or install using the go command line tool: GO111MODULE=on go get -u github.com/cube2222/octosql/cmd/octosql Quickstart Let's say we have a csv file with cats, and a redis database with people (potential cat owners). Now we want to get a list of cities with the number of distinct cat names in them and the cumulative number of cat lives (as each cat has up to 9 lives left). First, create a configuration file (Configuration Syntax) For example: dataSources: - name: cats type: csv config: path: \"~/Documents/cats.csv\" - name: people type: redis config: address: \"localhost:6379\" password: \"\" databaseIndex: 0 databaseKeyName: \"id\" Then, set the OCTOSQL_CONFIG environment variable to point to the configuration file. export OCTOSQL_CONFIG=~/octosql.yaml You can also use the --config command line argument. Finally, query to your hearts desire: octosql \"SELECT p.city, FIRST(c.name), COUNT(DISTINCT c.name) cats, SUM(c.livesleft) catlives FROM cats c JOIN people p ON c.ownerid = p.id GROUP BY p.city ORDER BY catlives DESC LIMIT 9\" Example output: +---------+--------------+------+----------+ | p.city | c.name_first | cats | catlives | +---------+--------------+------+----------+ | Warren | Zoey | 68 | 570 | | Gadsden | Snickers | 52 | 388 | | Staples | Harley | 54 | 383 | | Buxton | Lucky | 45 | 373 | | Bethany | Princess | 46 | 366 | | Noxen | Sheba | 49 | 361 | | Yorklyn | Scooter | 45 | 359 | | Tuttle | Toby | 57 | 356 | | Ada | Jasmine | 49 | 351 | +---------+--------------+------+----------+ You can choose between live-table batch-table live-csv batch-csv stream-json output formats. (The live-* types will update the terminal view repeatedly every second, the batch-* ones will write the output once before exiting, the stream-* ones will print records whenever they are available) Temporal SQL Features OctoSQL features temporal SQL extensions inspired by the paper One SQL to Rule Them All. Introduction Often when you're working with streams of events, you'd like to use the time dimension somehow: Calculate average values for a day sliced by hours. Get unique user counts per day. and others All those examples have one thing in common: The time value of an event is crucial for correctness. A naive system could just use the current clock time whenever it receives an event. The correctness of this approach however, degrades quickly in the face of network problems, delivery delays, clock skew. This can be solved by using a value from the event as its time value. A new problem arises though: how do I know that I've received all events up to time X and can publish results for a given hour. You never know if there isn't somewhere a delayed event which should be factored in. This is where watermarks come into play. Watermarks Watermarks are a heuristic which try to approximate the \"current time\" when processing events. Said differently: When I receive a watermark for 12:00 I can be sure enough I've received all events of interest up to 12:00. To achieve this, they are generated at streaming sources and propagate downstream through the whole processing pipeline. The generation of watermarks usually relies on heuristics which provide satisfactory results for our given use case. OctoSQL currently contains the following watermark generators: Maximum difference watermark generator (with an offset argument) With an offset of 10 seconds, this generator says: When I've received an event for 12:00:00, then I'm sure I won't receive any event older than 11:59:50. Percentile watermark generator (with a percentile argument) With a percentile of 99.5, it will look at a specified number of recent events, and generate a watermark so that 99.5% of those events are after the watermark (not yet triggered), and the remaining 0.5% are before it. This way we set the watermark so that only a fraction of the recently seen events is potentially ignored as being late. Watermark generators are specified using table valued functions and are documented in the wiki. Triggers Another matter is triggering of keys in aggregations. Sometimes you'd like to only see the value for a given key (hour) when you know it's done, but othertimes you'd like to see partial results (how's the unique user count going this hour). That's where you can use triggers. Triggers allow you to specify when a given aggregate (or join window for that matter) is emitted or updated. OctoSQL contains multiple triggers: Watermark Trigger This is the most straightforward trigger. It emits a value whenever the watermark for a given key (or the end of the stream) is reached. So basically the \"show me when it's done\". Counting Trigger (with a count argument) This trigger will emit a value for a key every time it receives count records with this key. The count is reset whenever the key is triggered. Delay Trigger (with a delay argument) This trigger will emit a value for a key whenever the key has been inactive for the delay period. You can use multiple triggers simultaneously. (Show me the current sum every 10 received events, but also the final value after having received the watermark.) Retractions A key can be triggered multiple times with partial results. How do we know a given record is a retriggering of some key, and not a new unrelated record? OctoSQL solves this problem using a dataflow-like architecture. This means whenever a new value is sent for a key, a retraction is send for the old value. In practice this means every update is accompanied by the old record with an undo flag set. This can be visible when using a stream-* output format with partial results. Example Now we can see how it all fits together. In this example we have an events file, which contains records about points being scored in a game by multiple teams. WITH with_watermark AS (SELECT * FROM max_diff_watermark(source=>TABLE(events), offset=>INTERVAL 5 SECONDS, time_field=>DESCRIPTOR(time)) e), with_tumble AS (SELECT * FROM tumble(source=>TABLE(with_watermark), time_field=>DESCRIPTOR(e.time), window_length=> INTERVAL 1 MINUTE, offset => INTERVAL 0 SECONDS) e), counts_per_team AS (SELECT e.window_end, e.team, COUNT(*) as goals FROM with_tumble e GROUP BY e.window_end, e.team TRIGGER COUNTING 100, ON WATERMARK) SELECT * FROM counts_per_team cpt ORDER BY cpt.window_end DESC, cpt.goals ASC, cpt.team DESC We use common table expressions to break the query up into multiple stages. First we create the with_watermark intermediate table/stream. Here we use the table valued function max_diff_watermark to add watermarks to the events table - with an offset of 5 seconds based on the time record field. Then we use this intermediate table to create the with_tumble table, where we use the tumble table valued function to add a window_start and window_end field to each record, based on the record's time field. This assigns the records to 1 minute long windows. Next we create the counts_per_team table, which groups the records by their window end and team. Finally, we order those results by window end, goal count and team. Durability OctoSQL in its current design is based on on-disk transactional storage. All state is saved this way. All interactions with datasources are designed so that no records get duplicated in the face of errors or application restarts. You can also kill the OctoSQL process and start it again with the same query and storage-directory (command line argument), it will start where it left off. By default, OctoSQL will create a temporary directory for the state and delete it after termination. Configuration The configuration file has the following form dataSources: - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... ... physical: physical_plan_option: <value> Available OctoSQL-wide configuration options are: physical groupByParallelism: The parallelism of group by's and distinct queries. Will default to the CPU core count of your machine. streamJoinParallelism: The parallelism of streaming joins. Will default to the CPU core count of your machine. execution lookupJoinPrefetchCount: The count of simultaneously processed records in a lookup join. Supported Datasources JSON JSON file in one of the following forms: one record per line, no commas JSON list of records options: path - path to file containing the data, required arrayFormat - if the JSON list of records format should be used, optional: defaults to false batchSize - number of records extracted from json file in one storage transaction, optional: defaults to 1000 CSV CSV file separated using commas. The file may or may not have column names as it's first row. options: path - path to file containing the data, required headerRow - whether the first row of the CSV file contains column names or not, optional: defaults to true separator - columns separator, optional: defaults to \",\" batchSize - number of records extracted from csv file in one storage transaction, optional: defaults to 1000 Excel A single table in an Excel spreadsheet. The table may or may not have column names as it's first row. The table can be in any sheet, and start at any point, but it cannot contain spaces between columns nor spaces between rows. options: path - path to file, required headerRow - does the first row contain column names, optional: defaults to true sheet - name of the sheet in which data is stored, optional: defaults to \"Sheet1\" rootCell - name of cell (i.e \"A3\", \"BA14\") which is the leftmost cell of the first, optional: defaults to \"A1\" timeColumns - a list of columns to parse as datetime values with second precision row, optional: defaults to [] batchSize - number of records extracted from excel file in one storage transaction, optional: defaults to 1000 Parquet A single Parquet file. Nested repeated elements are not supported. Otherwise repeated xor nested elements are supported. Currently unsupported logical types, they will get parsed as the underlying primitive type: - ENUM - TIME with NANOS precision - TIMESTAMP with NANOS precision (both UTC and non-UTC) - INTERVAL - MAP options path - path to file, required batchSize - number of records extracted from parquet file in one storage transaction, optional: defaults to 1000 PostgreSQL Single PostgreSQL database table. options: address - address including port number, optional: defaults to localhost:5432 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from PostgreSQL database in one storage transaction, optional: defaults to 1000 MySQL Single MySQL database table. options: address - address including port number, optional: defaults to localhost:3306 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from MySQL database in one storage transaction, optional: defaults to 1000 Redis Redis database with the given index. Currently only hashes are supported. options: address - address including port number, optional: defaults to localhost:6379 password - optional: defaults to \"\" databaseIndex - index number of Redis database, optional: defaults to 0 databaseKeyName - column name of Redis key in OctoSQL records, optional: defaults to \"key\" batchSize - number of records extracted from Redis database in one storage transaction, optional: defaults to 1000 Kafka Multi-partition kafka topic. optional brokers - list of broker addresses (separately hosts and ports) used to connect to the kafka cluster, optional: defaults to [\"localhost:9092\"] topic - name of topic to read messages from, required partitions - topic partition count, optional: defaults to 1 startOffset - offset from which the first batch of messages will be read, optional: defaults to -1 batchSize - number of records extracted from Kafka in one storage transaction, optional: defaults to 1 json - should the messages be decoded as JSON, optional: defaults to false Documentation Documentation for the available functions: https://github.com/cube2222/octosql/wiki/Function-Documentation Documentation for the available aggregates: https://github.com/cube2222/octosql/wiki/Aggregate-Documentation Documentation for the available triggers: https://github.com/cube2222/octosql/wiki/Trigger-Documentation Documentation for the available table valued functions: https://github.com/cube2222/octosql/wiki/Table-Valued-Functions-Documentation The SQL dialect documentation: TODO ;) in short though: Available SQL constructs: Select, Where, Order By, Group By, Offset, Limit, Left Join, Right Join, Inner Join, Distinct, Union, Union All, Subqueries, Operators, Table Valued Functions, Trigger, Common Table Expressions. Available SQL types: Int, Float, String, Bool, Time, Duration, Tuple (array), Object (e.g. JSON) Describe You can describe the current plan in graphviz format using the -describe flag, like this: octosql \"...\" --describe | dot -Tpng > output.png Architecture An OctoSQL invocation gets processed in multiple phases. SQL AST First, the SQL query gets parsed into an abstract syntax tree. This phase only rules out syntax errors. Logical Plan The SQL AST gets converted into a logical query plan. This plan is still mostly a syntactic validation. It's the most naive possible translation of the SQL query. However, this plan already has more of a map-filter-reduce form. If you wanted to add a new query language to OctoSQL, the only problem you'd have to solve is translating it to this logical plan. Physical Plan The logical plan gets converted into a physical plan. This conversion finds any semantic errors in the query. If this phase is reached, then the input is correct and OctoSQL will be able execute it. This phase already understands the specifics of the underlying datasources. So it's here where the optimizer will iteratively transform the plan, pushing computation nodes down to the datasources, and deduplicating unnecessary parts. The optimizer uses a pattern matching approach, where it has rules for matching parts of the physical plan tree and how those patterns can be restructured into a more efficient version. The rules are meant to be as simple as possible and make the smallest possible changes. For example, pushing filters under maps, if they don't use any mapped variables. This way, the optimizer just keeps on iterating on the whole tree, until it can't change anything anymore. (each iteration tries to apply each rule in each possible place in the tree) This ensures that the plan reaches a local performance minimum, and the rules should be structured so that this local minimum is equal - or close to - the global minimum. (i.e. one optimization, shouldn't make another - much more useful one - impossible) Here is an example diagram of an optimized physical plan: Execution Plan The physical plan gets materialized into an execution plan. This phase has to be able to connect to the actual datasources. It may initialize connections, open files, etc. Stream Starting the execution plan creates a stream, which underneath may hold more streams, or parts of the execution plan to create streams in the future. This stream works in a pull based model. Datasource Pushdown Operations Datasource Equality In > < <= >= MySQL supported supported supported PostgreSQL supported supported supported Redis supported supported scan Kafka scan scan scan Parquet scan scan scan JSON scan scan scan CSV scan scan scan Where scan means that the whole table needs to be scanned for each access. Telemetry OctoSQL sends application telemetry on each run to help us gauge user interest and feature use. This way we know somebody uses our software, feel our work is actually useful and can prioritize features based on actual usefulness. You can turn it off (though please don't) by setting the OCTOSQL_TELEMETRY environment variable to 0. Telemetry is also fully printed in the output log of OctoSQL, if you want to see what precisely is being sent. Roadmap Additional Datasources. SQL Constructs: JSON Query HAVING, ALL, ANY Push down functions, aggregates to databases that support them. An in-memory index to save values of subqueries and save on rescanning tables which don't support a given operation, so as not to recalculate them each time. Runtime statistics Server mode Querying a json or csv table from standard input. Integration test suite Tuple splitter, returning the row for each tuple element, with the given element instead of the tuple. ",
        "_version_": 1718536501434777601
      },
      {
        "story_id": 19074170,
        "story_author": "nosarthur",
        "story_descendants": 36,
        "story_score": 79,
        "story_time": "2019-02-04T05:22:20Z",
        "story_title": "Show HN: Gita – a CLI tool to manage multiple Git repos",
        "search": [
          "Show HN: Gita – a CLI tool to manage multiple Git repos",
          "https://github.com/nosarthur/gita",
          "_______________________________ ( ____ \\__ __|__ __( ___ ) | ( \\/ ) ( ) ( | ( ) | | | | | | | | (___) | | | ____ | | | | | ___ | | | \\_ ) | | | | | ( ) | | (___) |__) (___ | | | ) ( | (_______)_______/ )_( |/ \\| v0.15 Gita: a command-line tool to manage multiple git repos This tool does two things display the status of multiple git repos such as branch, modification, commit message side by side (batch) delegate git commands/aliases from any working directory If several repos are related, it helps to see their status together. I also hate to change directories to execute git commands. In this screenshot, the gita ll command displays the status of all repos. The gita remote dotfiles command translates to git remote -v for the dotfiles repo, even though we are not in the repo. The gita fetch command fetches from all repos and two of them have updates. To see the pre-defined commands, run gita -h or take a look at cmds.json. To add your own commands, see the customization section. To run arbitrary git command, see the superman mode section. To run arbitrary shell command, see the shell mode section. The branch color distinguishes 5 situations between local and remote branches: color meaning white local has no remote green local is the same as remote red local has diverged from remote purple local is ahead of remote (good for push) yellow local is behind remote (good for merge) The choice of purple for ahead and yellow for behind is motivated by blueshift and redshift, using green as baseline. You can change the color scheme using the gita color command. See the customization section. The additional status symbols denote symbol meaning + staged changes * unstaged changes _ untracked files/folders The bookkeeping sub-commands are gita add <repo-path(s)> [-g <groupname>]: add repo(s) to gita, optionally into an existing group gita add -a <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively and automatically generate hierarchical groups. See the customization section for more details. gita add -b <bare-repo-path(s)>: add bare repo(s) to gita. See the customization section for more details on setting custom worktree. gita add -r <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively gita clone <config-file>: clone repos in config-file (generated by gita freeze) to current directory. gita clone -p <config-file>: clone repos in config-file to prescribed paths. gita context: context sub-command gita context: show current context gita context <group-name>: set context to group-name, all operations then only apply to repos in this group gita context auto: set context automatically according to the current working directory gita context none: remove context gita color: color sub-command gita color [ll]: Show available colors and the current coloring scheme gita color reset: Reset to the default coloring scheme gita color set <situation> <color>: Use the specified color for the local-remote situation gita flags: flags sub-command gita flags set <repo-name> <flags>: add custom flags to repo gita flags [ll]: display repos with custom flags gita freeze: print information of all repos such as URL, name, and path. Use with gita clone. gita group: group sub-command gita group add <repo-name(s)> -n <group-name>: add repo(s) to a new or existing group gita group [ll]: display existing groups with repos gita group ls: display existing group names gita group rename <group-name> <new-name>: change group name gita group rm <group-name(s)>: delete group(s) gita group rmrepo <repo-name(s)> -n <group-name>: remove repo(s) from existing group gita info: info sub-command gita info [ll]: display the used and unused information items gita info add <info-item>: enable information item gita info rm <info-item>: disable information item gita ll: display the status of all repos gita ll <group-name>: display the status of repos in a group gita ll -g: display the repo summaries by groups gita ls: display the names of all repos gita ls <repo-name>: display the absolute path of one repo gita rename <repo-name> <new-name>: rename a repo gita rm <repo-name(s)>: remove repo(s) from gita (won't remove files on disk) gita -v: display gita version The git delegating sub-commands are of two formats gita <sub-command> [repo-name(s) or group-name(s)]: optional repo or group input, and no input means all repos. gita <sub-command> <repo-name(s) or groups-name(s)>: required repo name(s) or group name(s) input They translate to git <sub-command> for the corresponding repos. By default, only fetch and pull take optional input. In other words, gita fetch and gita pull apply to all repos. To see the pre-defined sub-commands, run gita -h or take a look at cmds.json. To add your own sub-commands or override the default behaviors, see the customization section. To run arbitrary git command, see the superman mode section. If more than one repos are specified, the git command runs asynchronously, with the exception of log, difftool and mergetool, which require non-trivial user input. Repo configuration is saved in $XDG_CONFIG_HOME/gita/repos.csv (most likely ~/.config/gita/repos.csv). Installation To install the latest version, run If you prefer development mode, download the source code and run pip3 install -e <gita-source-folder> In either case, calling gita in terminal may not work, then put the following line in the .bashrc file. alias gita=\"python3 -m gita\" Windows users may need to enable the ANSI escape sequence in terminal for the branch color to work. See this stackoverflow post for details. Auto-completion Download .gita-completion.bash or .gita-completion.zsh and source it in shell. Superman mode The superman mode delegates any git command or alias. Usage: gita super [repo-name(s) or group-name(s)] <any-git-command-with-or-without-options> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita super checkout master puts all repos on the master branch gita super frontend-repo backend-repo commit -am 'implement a new feature' executes git commit -am 'implement a new feature' for frontend-repo and backend-repo Shell mode The shell mode delegates any shell command. Usage: gita shell [repo-name(s) or group-name(s)] <any-shell-command> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita shell ll lists contents for all repos gita shell repo1 repo2 mkdir docs create a new directory docs in repo1 and repo2 gita shell \"git describe --abbrev=0 --tags | xargs git checkout\": check out the latest tag for all repos Customization define repo group and context When the project contains several independent but related repos, we can define a group and execute gita command on this group. For example, gita group add repo1 repo2 -n my-group gita ll my-group gita pull my-group To save more typing, one can set a group as context, then any gita command is scoped to the group gita context my-group gita ll gita pull The most useful context maybe auto. In this mode, the context is automatically determined from the current working directory (CWD): the context is the group whose member repo's path contains CWD. To set it, run To remove the context, run It is also possible to recursively add repos within a directory and generate hierarchical groups automatically. For example, running on the following folder structure src project1 repo1 repo2 repo3 project2 repo4 repo5 repo6 gives rise to 3 groups: src:repo1,repo2,repo3,repo4,repo5,repo6 src-project1:repo1,repo2 src-project2:repo4,repo5 add user-defined sub-command using json file Custom delegating sub-commands can be defined in $XDG_CONFIG_HOME/gita/cmds.json (most likely ~/.config/gita/cmds.json) And they shadow the default ones if name collisions exist. Default delegating sub-commands are defined in cmds.json. For example, gita stat <repo-name(s)> is registered as \"stat\":{ \"cmd\": \"git diff --stat\", \"help\": \"show edit statistics\" } which executes git diff --stat for the specified repo(s). To disable asynchronous execution, set disable_async to be true. See the difftool example: \"difftool\":{ \"cmd\": \"git difftool\", \"disable_async\": true, \"help\": \"show differences using a tool\" } If you want a custom command to behave like gita fetch, i.e., to apply to all repos when no repo is specified, set allow_all to be true. For example, the following snippet creates a new command gita comaster [repo-name(s)] with optional repo name input. \"comaster\":{ \"cmd\": \"checkout master\", \"allow_all\": true, \"help\": \"checkout the master branch\" } Any command that runs in the superman mode mode or the shell mode can be defined in this json format. For example, the following command runs in shell mode and fetches only the current branch from upstream. \"fetchcrt\":{ \"cmd\": \"git rev-parse --abbrev-ref HEAD | xargs git fetch --prune upstream\", \"allow_all\": true, \"shell\": true, \"help\": \"fetch current branch only\" } customize the local/remote relationship coloring displayed by the gita ll command You can see the default color scheme and the available colors via gita color. To change the color coding, use gita color set <situation> <color>. The configuration is saved in $XDG_CONFIG_HOME/gita/color.csv. customize information displayed by the gita ll command You can customize the information displayed by gita ll. The used and unused information items are shown with gita info, and the configuration is saved in $XDG_CONFIG_HOME/gita/info.csv. For example, the default setting corresponds to branch,commit_msg,commit_time customize git command flags One can set custom flags to run git commands. For example, with gita flags set my-repo --git-dir=`gita ls dotfiles` --work-tree=$HOME any git command/alias triggered from gita on dotfiles will use these flags. Note that the flags are applied immediately after git. For example, gita st dotfiles translates to git --git-dir=$HOME/somefolder --work-tree=$HOME status running from the dotfiles directory. This feature was originally added to deal with bare repo dotfiles. Requirements Gita requires Python 3.6 or higher, due to the use of f-string and asyncio module. Under the hood, gita uses subprocess to run git commands/aliases. Thus the installed git version may matter. I have git 1.8.3.1, 2.17.2, and 2.20.1 on my machines, and their results agree. Tips effect shell command enter <repo> directory cd `gita ls <repo>` delete repos in <group> gita group ll <group> | xargs gita rm Contributing To contribute, you can report/fix bugs request/implement features star/recommend this project Read this article if you have never contribute code to open source project before. Chat room is available on To run tests locally, simply pytest in the source code folder. Note that context should be set as none. More implementation details are in design.md. A step-by-step guide to reproduce this project is here. You can also sponsor me on GitHub. Any amount is appreciated! Other multi-repo tools I haven't tried them but I heard good things about them. myrepos repo "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/nosarthur/gita",
        "comments.comment_id": [19075272, 19075476],
        "comments.comment_author": ["fyhn", "stinos"],
        "comments.comment_descendants": [1, 3],
        "comments.comment_time": [
          "2019-02-04T10:43:55Z",
          "2019-02-04T11:34:37Z"
        ],
        "comments.comment_text": [
          "One thing this project does really well is to start the readme with a screenshot. I open the link, scroll down to the readme, and I immediately see what sort of user interface/experience I will get. Some commenters have noted that Gita is similar to other multi-repo tools, but both Repo and wstool are more effort to evaluate, because their readmes don't have pictures.",
          "Nice, but is there a way to just run any command? I.e. just `gita <optional repo names/paths> <pass entire command line git -C repodir>`. This has advantages in that you don't have to go round via a command file, don't have to keep it synced across machines, don't have to remember what you put in the file etc, and can just use the git syntax which already took long enough to learn by heart :P<p>I've used multiple multiple repository tools and in the end all I happen to use is one (usually versioned) file to store a list of repositories and then a command which just loops over all repos and applies anything to it. If I need custom commands I use git aliases so that works both for normal git and whatever tool used."
        ],
        "id": "a45fcc10-e3c0-4036-b37c-11fc2bcf80f5",
        "url_text": "_______________________________ ( ____ \\__ __|__ __( ___ ) | ( \\/ ) ( ) ( | ( ) | | | | | | | | (___) | | | ____ | | | | | ___ | | | \\_ ) | | | | | ( ) | | (___) |__) (___ | | | ) ( | (_______)_______/ )_( |/ \\| v0.15 Gita: a command-line tool to manage multiple git repos This tool does two things display the status of multiple git repos such as branch, modification, commit message side by side (batch) delegate git commands/aliases from any working directory If several repos are related, it helps to see their status together. I also hate to change directories to execute git commands. In this screenshot, the gita ll command displays the status of all repos. The gita remote dotfiles command translates to git remote -v for the dotfiles repo, even though we are not in the repo. The gita fetch command fetches from all repos and two of them have updates. To see the pre-defined commands, run gita -h or take a look at cmds.json. To add your own commands, see the customization section. To run arbitrary git command, see the superman mode section. To run arbitrary shell command, see the shell mode section. The branch color distinguishes 5 situations between local and remote branches: color meaning white local has no remote green local is the same as remote red local has diverged from remote purple local is ahead of remote (good for push) yellow local is behind remote (good for merge) The choice of purple for ahead and yellow for behind is motivated by blueshift and redshift, using green as baseline. You can change the color scheme using the gita color command. See the customization section. The additional status symbols denote symbol meaning + staged changes * unstaged changes _ untracked files/folders The bookkeeping sub-commands are gita add <repo-path(s)> [-g <groupname>]: add repo(s) to gita, optionally into an existing group gita add -a <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively and automatically generate hierarchical groups. See the customization section for more details. gita add -b <bare-repo-path(s)>: add bare repo(s) to gita. See the customization section for more details on setting custom worktree. gita add -r <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively gita clone <config-file>: clone repos in config-file (generated by gita freeze) to current directory. gita clone -p <config-file>: clone repos in config-file to prescribed paths. gita context: context sub-command gita context: show current context gita context <group-name>: set context to group-name, all operations then only apply to repos in this group gita context auto: set context automatically according to the current working directory gita context none: remove context gita color: color sub-command gita color [ll]: Show available colors and the current coloring scheme gita color reset: Reset to the default coloring scheme gita color set <situation> <color>: Use the specified color for the local-remote situation gita flags: flags sub-command gita flags set <repo-name> <flags>: add custom flags to repo gita flags [ll]: display repos with custom flags gita freeze: print information of all repos such as URL, name, and path. Use with gita clone. gita group: group sub-command gita group add <repo-name(s)> -n <group-name>: add repo(s) to a new or existing group gita group [ll]: display existing groups with repos gita group ls: display existing group names gita group rename <group-name> <new-name>: change group name gita group rm <group-name(s)>: delete group(s) gita group rmrepo <repo-name(s)> -n <group-name>: remove repo(s) from existing group gita info: info sub-command gita info [ll]: display the used and unused information items gita info add <info-item>: enable information item gita info rm <info-item>: disable information item gita ll: display the status of all repos gita ll <group-name>: display the status of repos in a group gita ll -g: display the repo summaries by groups gita ls: display the names of all repos gita ls <repo-name>: display the absolute path of one repo gita rename <repo-name> <new-name>: rename a repo gita rm <repo-name(s)>: remove repo(s) from gita (won't remove files on disk) gita -v: display gita version The git delegating sub-commands are of two formats gita <sub-command> [repo-name(s) or group-name(s)]: optional repo or group input, and no input means all repos. gita <sub-command> <repo-name(s) or groups-name(s)>: required repo name(s) or group name(s) input They translate to git <sub-command> for the corresponding repos. By default, only fetch and pull take optional input. In other words, gita fetch and gita pull apply to all repos. To see the pre-defined sub-commands, run gita -h or take a look at cmds.json. To add your own sub-commands or override the default behaviors, see the customization section. To run arbitrary git command, see the superman mode section. If more than one repos are specified, the git command runs asynchronously, with the exception of log, difftool and mergetool, which require non-trivial user input. Repo configuration is saved in $XDG_CONFIG_HOME/gita/repos.csv (most likely ~/.config/gita/repos.csv). Installation To install the latest version, run If you prefer development mode, download the source code and run pip3 install -e <gita-source-folder> In either case, calling gita in terminal may not work, then put the following line in the .bashrc file. alias gita=\"python3 -m gita\" Windows users may need to enable the ANSI escape sequence in terminal for the branch color to work. See this stackoverflow post for details. Auto-completion Download .gita-completion.bash or .gita-completion.zsh and source it in shell. Superman mode The superman mode delegates any git command or alias. Usage: gita super [repo-name(s) or group-name(s)] <any-git-command-with-or-without-options> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita super checkout master puts all repos on the master branch gita super frontend-repo backend-repo commit -am 'implement a new feature' executes git commit -am 'implement a new feature' for frontend-repo and backend-repo Shell mode The shell mode delegates any shell command. Usage: gita shell [repo-name(s) or group-name(s)] <any-shell-command> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita shell ll lists contents for all repos gita shell repo1 repo2 mkdir docs create a new directory docs in repo1 and repo2 gita shell \"git describe --abbrev=0 --tags | xargs git checkout\": check out the latest tag for all repos Customization define repo group and context When the project contains several independent but related repos, we can define a group and execute gita command on this group. For example, gita group add repo1 repo2 -n my-group gita ll my-group gita pull my-group To save more typing, one can set a group as context, then any gita command is scoped to the group gita context my-group gita ll gita pull The most useful context maybe auto. In this mode, the context is automatically determined from the current working directory (CWD): the context is the group whose member repo's path contains CWD. To set it, run To remove the context, run It is also possible to recursively add repos within a directory and generate hierarchical groups automatically. For example, running on the following folder structure src project1 repo1 repo2 repo3 project2 repo4 repo5 repo6 gives rise to 3 groups: src:repo1,repo2,repo3,repo4,repo5,repo6 src-project1:repo1,repo2 src-project2:repo4,repo5 add user-defined sub-command using json file Custom delegating sub-commands can be defined in $XDG_CONFIG_HOME/gita/cmds.json (most likely ~/.config/gita/cmds.json) And they shadow the default ones if name collisions exist. Default delegating sub-commands are defined in cmds.json. For example, gita stat <repo-name(s)> is registered as \"stat\":{ \"cmd\": \"git diff --stat\", \"help\": \"show edit statistics\" } which executes git diff --stat for the specified repo(s). To disable asynchronous execution, set disable_async to be true. See the difftool example: \"difftool\":{ \"cmd\": \"git difftool\", \"disable_async\": true, \"help\": \"show differences using a tool\" } If you want a custom command to behave like gita fetch, i.e., to apply to all repos when no repo is specified, set allow_all to be true. For example, the following snippet creates a new command gita comaster [repo-name(s)] with optional repo name input. \"comaster\":{ \"cmd\": \"checkout master\", \"allow_all\": true, \"help\": \"checkout the master branch\" } Any command that runs in the superman mode mode or the shell mode can be defined in this json format. For example, the following command runs in shell mode and fetches only the current branch from upstream. \"fetchcrt\":{ \"cmd\": \"git rev-parse --abbrev-ref HEAD | xargs git fetch --prune upstream\", \"allow_all\": true, \"shell\": true, \"help\": \"fetch current branch only\" } customize the local/remote relationship coloring displayed by the gita ll command You can see the default color scheme and the available colors via gita color. To change the color coding, use gita color set <situation> <color>. The configuration is saved in $XDG_CONFIG_HOME/gita/color.csv. customize information displayed by the gita ll command You can customize the information displayed by gita ll. The used and unused information items are shown with gita info, and the configuration is saved in $XDG_CONFIG_HOME/gita/info.csv. For example, the default setting corresponds to branch,commit_msg,commit_time customize git command flags One can set custom flags to run git commands. For example, with gita flags set my-repo --git-dir=`gita ls dotfiles` --work-tree=$HOME any git command/alias triggered from gita on dotfiles will use these flags. Note that the flags are applied immediately after git. For example, gita st dotfiles translates to git --git-dir=$HOME/somefolder --work-tree=$HOME status running from the dotfiles directory. This feature was originally added to deal with bare repo dotfiles. Requirements Gita requires Python 3.6 or higher, due to the use of f-string and asyncio module. Under the hood, gita uses subprocess to run git commands/aliases. Thus the installed git version may matter. I have git 1.8.3.1, 2.17.2, and 2.20.1 on my machines, and their results agree. Tips effect shell command enter <repo> directory cd `gita ls <repo>` delete repos in <group> gita group ll <group> | xargs gita rm Contributing To contribute, you can report/fix bugs request/implement features star/recommend this project Read this article if you have never contribute code to open source project before. Chat room is available on To run tests locally, simply pytest in the source code folder. Note that context should be set as none. More implementation details are in design.md. A step-by-step guide to reproduce this project is here. You can also sponsor me on GitHub. Any amount is appreciated! Other multi-repo tools I haven't tried them but I heard good things about them. myrepos repo ",
        "_version_": 1718536443557576705
      },
      {
        "story_id": 20314255,
        "story_author": "suyashkumar",
        "story_descendants": 36,
        "story_score": 142,
        "story_time": "2019-06-29T19:20:47Z",
        "story_title": "Show HN: DICOM Medical Image Parser in Go",
        "search": [
          "Show HN: DICOM Medical Image Parser in Go",
          "https://github.com/suyashkumar/dicom",
          "dicom High Performance Golang DICOM Medical Image Parser v1.0 just released! This is a library and command-line tool to read, write, and generally work with DICOM medical image files in native Go. The goal is to build a full-featured, high-performance, and readable DICOM parser for the Go community. After a fair bit of work, I've just released v1.0 of this library which is essentially rewritten from the ground up to be more canonical go, better tested, has new features, many bugfixes, and more (though there is always more to come on the roadmap). Some notable features: Parse multi-frame DICOM imagery (both encapsulated and native pixel data) Channel-based streaming of Frames to a client as they are parsed out of the dicom Cleaner Go Element and Dataset representations (in the absense of Go generics) Better support for icon image sets in addition to primary image sets Write and encode Datasets back to DICOM files Enhanced testing and benchmarking support Modern, canonical Go. Usage To use this in your golang project, import github.com/suyashkumar/dicom. This repository supports Go modules, and regularly tags releases using semantic versioning. Typical usage is straightforward: dataset, _ := dicom.ParseFile(\"testdata/1.dcm\", nil) // See also: dicom.Parse which has a generic io.Reader API. // Dataset will nicely print the DICOM dataset data out of the box. fmt.Println(dataset) // Dataset is also JSON serializable out of the box. j, _ := json.Marshal(dataset) fmt.Println(j) More details about the package (and additional examples and APIs) can be found in the godoc. CLI Tool A CLI tool that uses this package to parse imagery and metadata out of DICOMs is provided in the cmd/dicomutil package. This tool can take in a DICOM, and dump out all the elements to STDOUT, in addition to writing out any imagery to the current working directory either as PNGs or JPEG (note, it does not perform any automatic color rescaling by default). Installation You can download the prebuilt binaries from the releases tab, or use the following to download the binary at the command line using my getbin tool: wget -qO- \"https://getbin.io/suyashkumar/dicom\" | tar xvz (This attempts to infer your OS and 301 redirects wget to the latest github release asset for your system. Downloads come from GitHub releases). Usage dicomutil -path myfile.dcm Note: for some DICOMs (with native pixel data) no automatic intensity scaling is applied yet (this is coming). You can apply this in your image viewer if needed (in Preview on mac, go to Tools->Adjust Color). Build manually To build manually, ensure you have make and go installed. Clone (or go get) this repo into your $GOPATH and then simply run: Which will build the dicomutil binary and include it in a build/ folder in your current working directory. You can also built it using Go directly: go build -o dicomutil ./cmd/dicomutil History Here's a little more history on this repository for those who are interested! v0 The v0 suyashkumar/dicom started off as a hard fork of go-dicom which was not being maintained actively anymore (with the original author being supportive of my fork--thank you!). I worked on adding several new capabilities, bug fixes, and general maintainability refactors (like multiframe support, streaming parsing, updated APIs, low-level parsing bug fixes, and more). That represents the v0 history of the repository. v1 For v1 I rewrote and redesigned the core library essentially from scratch, and added several new features and bug fixes that only live in v1. The architecture and APIs are completely different, as is some of the underlying parser logic (to be more efficient and correct). Most of the core rewrite work happend at the s/1.0-rewrite branch. Acknowledgements Segmed for their help with validation and other contributions to the library. Original go-dicom Grailbio go-dicom -- commits from their fork were applied to ours GradientHealth for supporting work I did on this while there gradienthealth/dicom Innolitics DICOM browser DICOM Specification "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/suyashkumar/dicom",
        "url_text": "dicom High Performance Golang DICOM Medical Image Parser v1.0 just released! This is a library and command-line tool to read, write, and generally work with DICOM medical image files in native Go. The goal is to build a full-featured, high-performance, and readable DICOM parser for the Go community. After a fair bit of work, I've just released v1.0 of this library which is essentially rewritten from the ground up to be more canonical go, better tested, has new features, many bugfixes, and more (though there is always more to come on the roadmap). Some notable features: Parse multi-frame DICOM imagery (both encapsulated and native pixel data) Channel-based streaming of Frames to a client as they are parsed out of the dicom Cleaner Go Element and Dataset representations (in the absense of Go generics) Better support for icon image sets in addition to primary image sets Write and encode Datasets back to DICOM files Enhanced testing and benchmarking support Modern, canonical Go. Usage To use this in your golang project, import github.com/suyashkumar/dicom. This repository supports Go modules, and regularly tags releases using semantic versioning. Typical usage is straightforward: dataset, _ := dicom.ParseFile(\"testdata/1.dcm\", nil) // See also: dicom.Parse which has a generic io.Reader API. // Dataset will nicely print the DICOM dataset data out of the box. fmt.Println(dataset) // Dataset is also JSON serializable out of the box. j, _ := json.Marshal(dataset) fmt.Println(j) More details about the package (and additional examples and APIs) can be found in the godoc. CLI Tool A CLI tool that uses this package to parse imagery and metadata out of DICOMs is provided in the cmd/dicomutil package. This tool can take in a DICOM, and dump out all the elements to STDOUT, in addition to writing out any imagery to the current working directory either as PNGs or JPEG (note, it does not perform any automatic color rescaling by default). Installation You can download the prebuilt binaries from the releases tab, or use the following to download the binary at the command line using my getbin tool: wget -qO- \"https://getbin.io/suyashkumar/dicom\" | tar xvz (This attempts to infer your OS and 301 redirects wget to the latest github release asset for your system. Downloads come from GitHub releases). Usage dicomutil -path myfile.dcm Note: for some DICOMs (with native pixel data) no automatic intensity scaling is applied yet (this is coming). You can apply this in your image viewer if needed (in Preview on mac, go to Tools->Adjust Color). Build manually To build manually, ensure you have make and go installed. Clone (or go get) this repo into your $GOPATH and then simply run: Which will build the dicomutil binary and include it in a build/ folder in your current working directory. You can also built it using Go directly: go build -o dicomutil ./cmd/dicomutil History Here's a little more history on this repository for those who are interested! v0 The v0 suyashkumar/dicom started off as a hard fork of go-dicom which was not being maintained actively anymore (with the original author being supportive of my fork--thank you!). I worked on adding several new capabilities, bug fixes, and general maintainability refactors (like multiframe support, streaming parsing, updated APIs, low-level parsing bug fixes, and more). That represents the v0 history of the repository. v1 For v1 I rewrote and redesigned the core library essentially from scratch, and added several new features and bug fixes that only live in v1. The architecture and APIs are completely different, as is some of the underlying parser logic (to be more efficient and correct). Most of the core rewrite work happend at the s/1.0-rewrite branch. Acknowledgements Segmed for their help with validation and other contributions to the library. Original go-dicom Grailbio go-dicom -- commits from their fork were applied to ours GradientHealth for supporting work I did on this while there gradienthealth/dicom Innolitics DICOM browser DICOM Specification ",
        "comments.comment_id": [20314821, 20315010],
        "comments.comment_author": ["carbocation", "jacquesm"],
        "comments.comment_descendants": [1, 2],
        "comments.comment_time": [
          "2019-06-29T21:15:41Z",
          "2019-06-29T21:49:55Z"
        ],
        "comments.comment_text": [
          "I use this and it's great!<p>One thing that would be helpful (and perhaps it's already included and I just haven't seen it?) is a set  of functions to make the parsed image appropriate for display.<p>Right now, the raw pixel values are given. To turn this into an image, you have to determine the word size (e.g., 16 bits), then interpret the results in the context of the window and center values from the metadata. For my current batch of images, for example, unless I do postprocessing, the images just look black.",
          "Note that an 'image' for DICOM software can mean many things, from a collection of slices through a volume from an MRI scan to x-rays and ultrasound along with a whole pile of annotation and other meta data. DICOM is fairly complex and a good reference implementation of a parser in any language is a fantastic gift <i>if</i> it will be maintained long term.<p>Is there anything known about the reason for a fork rather than contributing back to the original?"
        ],
        "id": "32269f1b-94aa-45f9-8216-ac122ef1011f",
        "_version_": 1718536496507518976
      },
      {
        "story_id": 20950146,
        "story_author": "1_player",
        "story_descendants": 10,
        "story_score": 60,
        "story_time": "2019-09-12T11:14:30Z",
        "story_title": "Varlink – A plain-text, type-safe, discoverable, self-documenting interface",
        "search": [
          "Varlink – A plain-text, type-safe, discoverable, self-documenting interface",
          "https://varlink.org",
          "Varlink is an interface description format and protocol that aims to make services accessible to both humans and machines in the simplest feasible way. A varlink interface combines the classic UNIX command line options, STDIN/OUT/ERROR text formats, man pages, service metadata and provides the equivalent over a single file descriptor, a.k.a. FD3. Varlink is plain-text, type-safe, discoverable, self-documenting, remotable, testable, easy to debug. Varlink is accessible from any programming environment. See the Ideals page for more. And everybody likes Screenshots. Interface A varlink interface has a reverse-domain name and specifies which methods the interface implements. Each method has named and typed input and output parameters. Complex types can be aliased with the type keyword to allow reusing them and to make method signatures easier to read. The interface also specifies the errors that may be returned from its method calls. Everything can be documented by adding a comment immediately before it. The documentation is provided to clients as structured data on a well-known service interface. See the Interface Definition about the varlink syntax and how to parse an interface file. # Interface to jump a spacecraft to another point in space. # The FTL Drive is the propulsion system to achieve # faster-than-light travel through space. A ship making a # properly calculated jump can arrive safely in planetary # orbit, or alongside other ships or spaceborne objects. interface org.example.ftl # The current state of the FTL drive and the amount of # fuel available to jump. type DriveCondition ( state: (idle, spooling, busy), tylium_level: int ) # Speed, trajectory and jump duration is calculated prior # to activating the FTL drive. type DriveConfiguration ( speed: int, trajectory: int, duration: int ) # The galactic coordinates use the Sun as the origin. # Galactic longitude is measured with primary direction # from the Sun to the center of the galaxy in the galactic # plane, while the galactic latitude measures the angle # of the object above the galactic plane. type Coordinate ( longitude: float, latitude: float, distance: int ) # Monitor the drive. The method will reply with an update # whenever the drive's state changes method Monitor() -> (condition: DriveCondition) # Calculate the drive's jump parameters from the current # position to the target position in the galaxy method CalculateConfiguration( current: Coordinate, target: Coordinate ) -> (configuration: DriveConfiguration) # Jump to the calculated point in space method Jump(configuration: DriveConfiguration) -> () # There is not enough tylium to jump with the given # parameters error NotEnoughEnergy () # The supplied parameters are outside the supported range error ParameterOutOfRange (field: string) Protocol All messages are encoded as JSON objects and terminated with a single NUL byte. A service responds to requests in the same order that they are receivedmessages are never multiplexed. However, multiple requests can be queued on a connection to enable pipelining. This simplifies and minimizes the amount of state clients need to track. The common case is a simple method call with a single reply. To support monitoring calls, subscriptions, chunked data, streaming, calls may carry instructions for the server to not reply, or to reply multiple times to a single method call. See the Method Call page for a detailed description. In common programming languages, varlink clients do not require complex modules or libraries, already existing JSON and socket communication facilities are used to integrate natively into the programming languages object model. Requests specify the fully-qualified method that should be called, along with its input parameters: { \"method\": \"org.example.ftl.CalculateConfiguration\", \"parameters\": { \"current\": { \"longitude\": \"27.13\", \"latitude\": \"-12.4\", \"distance\": \"48732498234\" }, \"target\": { \"longitude\": \"-48.7\", \"latitude\": \"12.9\", \"distance\": \"354667658787\" } } } A service replies with an object that contains the output parameters: { \"parameters\": { \"configuration\": { \"speed\": \"32434234\", \"trajectory\": \"686787\", \"duration\": \"13256445\" } } } Errors contain the fully-qualified error and optional parameters as specified and documented in the varlink interface file: { \"error\": \"org.example.ftl.ParameterOutOfRange\", \"parameters\": { \"field\": \"current.distance\" } } Service Every varlink service offers the org.varlink.service interface, which describes all interfaces the service provides and provides information about the service implementation itself. See the Service page for details. Address Varlink services are expressed in URI notation. All properties after a ; character should be ignored to allow future extensions. Type Example Comment TCP tcp:127.0.0.1:12345 hostname/IP address and port UNIX socket unix:/run/org.example.ftl UNIX abstract namespace socket unix:@org.example.ftl device node device:/dev/org.kernel.example See the transport screenshot for examples. Activation A listen socket a.k.a FD3 might be passed to a varlink service at startup. Environment Example Comment LISTEN_FDS LISTEN_FDS=2 Number of file descriptors passed to the service. It always starts at 3. If more than one file descriptor is passed, LISTEN_FDNAMES identifies the varlink file descriptor. LISTEN_PID LISTEN_PID=4711 The process id of the started service. This should match the PID of the started service, otherwise everything should be ignored. LISTEN_FDNAMES LISTEN_FDNAMES=other:varlink A colon separated list of names of the passed file descriptors. The varlink file descriptor should be named varlink. The service activator should pass the command line option varlink=ADDRESS to the service, to make the listening address and possible parameters known to the service. It also shows the listening address in ps, which helps to debug a running service. See an example implementation of a service activator. Resolver Public varlink interfaces are registered system-wide by their well-known address, by default /run/org.varlink.resolver. The resolver translates a given varlink interface to the service address which provides this interface. Multiple services can implement and offer the same interface, but only one of the services is registered with the resolver. The set of registered interfaces becomes the globally visible system interface, its actual configuration is usually defined by the operating system and not managed by the services themselves. See the org.varlink.resolver interface for details. Bridge The varlink command line tool supports a bridge mode to bridge a single connection to the resolver and its registered services. It intercepts the calls to the org.varlink.service interface and replies with the information the resolver supplies. If the bridge is used over SSH, all the interfaces of the locally running services appear to the remote ssh client as if they were implemented by the bridge. See a screenshot as an example. "
        ],
        "story_type": "Normal",
        "url_raw": "https://varlink.org",
        "comments.comment_id": [20971936, 20972864],
        "comments.comment_author": ["ar-nelson", "tln"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-09-14T16:42:21Z",
          "2019-09-14T18:45:58Z"
        ],
        "comments.comment_text": [
          "So... it looks like this is an RPC protocol, similar to JSON-RPC, but the word \"RPC\" doesn't appear anywhere on the homepage or the FAQ. And it has an interface description language, similar to Protobuf.<p>It seems like it might be a nice middle ground between Protobuf (strongly-typed, specified) and JSON-RPC (human-readable, doesn't need a special library). But requiring clients to use a new RPC format is still a tough sell. What are the main advantages of Varlink over more mainstream RPC formats?",
          "This article gives some nice background.<p><a href=\"https://lwn.net/Articles/742675/\" rel=\"nofollow\">https://lwn.net/Articles/742675/</a><p>It would be interesting for the shell to open a varlink connection to commands to allow progress updates. I could also see shells command completion interface being greatly simplified by using varlink."
        ],
        "id": "7e27effa-4375-4d93-9e5f-311f3e78bfb8",
        "url_text": "Varlink is an interface description format and protocol that aims to make services accessible to both humans and machines in the simplest feasible way. A varlink interface combines the classic UNIX command line options, STDIN/OUT/ERROR text formats, man pages, service metadata and provides the equivalent over a single file descriptor, a.k.a. FD3. Varlink is plain-text, type-safe, discoverable, self-documenting, remotable, testable, easy to debug. Varlink is accessible from any programming environment. See the Ideals page for more. And everybody likes Screenshots. Interface A varlink interface has a reverse-domain name and specifies which methods the interface implements. Each method has named and typed input and output parameters. Complex types can be aliased with the type keyword to allow reusing them and to make method signatures easier to read. The interface also specifies the errors that may be returned from its method calls. Everything can be documented by adding a comment immediately before it. The documentation is provided to clients as structured data on a well-known service interface. See the Interface Definition about the varlink syntax and how to parse an interface file. # Interface to jump a spacecraft to another point in space. # The FTL Drive is the propulsion system to achieve # faster-than-light travel through space. A ship making a # properly calculated jump can arrive safely in planetary # orbit, or alongside other ships or spaceborne objects. interface org.example.ftl # The current state of the FTL drive and the amount of # fuel available to jump. type DriveCondition ( state: (idle, spooling, busy), tylium_level: int ) # Speed, trajectory and jump duration is calculated prior # to activating the FTL drive. type DriveConfiguration ( speed: int, trajectory: int, duration: int ) # The galactic coordinates use the Sun as the origin. # Galactic longitude is measured with primary direction # from the Sun to the center of the galaxy in the galactic # plane, while the galactic latitude measures the angle # of the object above the galactic plane. type Coordinate ( longitude: float, latitude: float, distance: int ) # Monitor the drive. The method will reply with an update # whenever the drive's state changes method Monitor() -> (condition: DriveCondition) # Calculate the drive's jump parameters from the current # position to the target position in the galaxy method CalculateConfiguration( current: Coordinate, target: Coordinate ) -> (configuration: DriveConfiguration) # Jump to the calculated point in space method Jump(configuration: DriveConfiguration) -> () # There is not enough tylium to jump with the given # parameters error NotEnoughEnergy () # The supplied parameters are outside the supported range error ParameterOutOfRange (field: string) Protocol All messages are encoded as JSON objects and terminated with a single NUL byte. A service responds to requests in the same order that they are receivedmessages are never multiplexed. However, multiple requests can be queued on a connection to enable pipelining. This simplifies and minimizes the amount of state clients need to track. The common case is a simple method call with a single reply. To support monitoring calls, subscriptions, chunked data, streaming, calls may carry instructions for the server to not reply, or to reply multiple times to a single method call. See the Method Call page for a detailed description. In common programming languages, varlink clients do not require complex modules or libraries, already existing JSON and socket communication facilities are used to integrate natively into the programming languages object model. Requests specify the fully-qualified method that should be called, along with its input parameters: { \"method\": \"org.example.ftl.CalculateConfiguration\", \"parameters\": { \"current\": { \"longitude\": \"27.13\", \"latitude\": \"-12.4\", \"distance\": \"48732498234\" }, \"target\": { \"longitude\": \"-48.7\", \"latitude\": \"12.9\", \"distance\": \"354667658787\" } } } A service replies with an object that contains the output parameters: { \"parameters\": { \"configuration\": { \"speed\": \"32434234\", \"trajectory\": \"686787\", \"duration\": \"13256445\" } } } Errors contain the fully-qualified error and optional parameters as specified and documented in the varlink interface file: { \"error\": \"org.example.ftl.ParameterOutOfRange\", \"parameters\": { \"field\": \"current.distance\" } } Service Every varlink service offers the org.varlink.service interface, which describes all interfaces the service provides and provides information about the service implementation itself. See the Service page for details. Address Varlink services are expressed in URI notation. All properties after a ; character should be ignored to allow future extensions. Type Example Comment TCP tcp:127.0.0.1:12345 hostname/IP address and port UNIX socket unix:/run/org.example.ftl UNIX abstract namespace socket unix:@org.example.ftl device node device:/dev/org.kernel.example See the transport screenshot for examples. Activation A listen socket a.k.a FD3 might be passed to a varlink service at startup. Environment Example Comment LISTEN_FDS LISTEN_FDS=2 Number of file descriptors passed to the service. It always starts at 3. If more than one file descriptor is passed, LISTEN_FDNAMES identifies the varlink file descriptor. LISTEN_PID LISTEN_PID=4711 The process id of the started service. This should match the PID of the started service, otherwise everything should be ignored. LISTEN_FDNAMES LISTEN_FDNAMES=other:varlink A colon separated list of names of the passed file descriptors. The varlink file descriptor should be named varlink. The service activator should pass the command line option varlink=ADDRESS to the service, to make the listening address and possible parameters known to the service. It also shows the listening address in ps, which helps to debug a running service. See an example implementation of a service activator. Resolver Public varlink interfaces are registered system-wide by their well-known address, by default /run/org.varlink.resolver. The resolver translates a given varlink interface to the service address which provides this interface. Multiple services can implement and offer the same interface, but only one of the services is registered with the resolver. The set of registered interfaces becomes the globally visible system interface, its actual configuration is usually defined by the operating system and not managed by the services themselves. See the org.varlink.resolver interface for details. Bridge The varlink command line tool supports a bridge mode to bridge a single connection to the resolver and its registered services. It intercepts the calls to the org.varlink.service interface and replies with the information the resolver supplies. If the bridge is used over SSH, all the interfaces of the locally running services appear to the remote ssh client as if they were implemented by the bridge. See a screenshot as an example. ",
        "_version_": 1718536520940388352
      },
      {
        "story_id": 19023196,
        "story_author": "l2g",
        "story_descendants": 22,
        "story_score": 163,
        "story_time": "2019-01-29T02:30:20Z",
        "story_title": "Show HN: Apprise – A lightweight all-in-one notification solution",
        "search": [
          "Show HN: Apprise – A lightweight all-in-one notification solution",
          "https://github.com/caronc/apprise",
          "apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/caronc/apprise",
        "comments.comment_id": [19026288, 19027306],
        "comments.comment_author": ["harryf", "joshstrange"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-01-29T14:00:21Z",
          "2019-01-29T15:54:22Z"
        ],
        "comments.comment_text": [
          "Cool stuff! But what about AWS SNS (<a href=\"https://aws.amazon.com/sns/\" rel=\"nofollow\">https://aws.amazon.com/sns/</a>) or Firebase Cloud Messaging (<a href=\"https://firebase.google.com/docs/cloud-messaging/\" rel=\"nofollow\">https://firebase.google.com/docs/cloud-messaging/</a>)?",
          "This is pretty cool, I drop the following shell script on all my servers:<p><pre><code>    #!/bin/bash\n    \n    if [[ -z $1 && -z $2 ]]; then\n        echo \"No Message passed\"\n    else\n        if [[ -z $2 ]]; then\n            curl -s --form-string \"token=MYAPPTOKEN\" --form-string \"user=MYUSERTOKEN\" --form-string \"message=$1\" https://api.pushover.net/1/messages.json\n        else\n            curl -s --form-string \"token=MYAPPTOKEN\" --form-string \"user=MYUSERTOKEN\" --form-string \"title=$1\" --form-string \"message=$2\"  https://api.pushover.net/1/messages.json\n        fi\n    fi\n</code></pre>\nIt's SUPER basic and probably shitty but for me it's perfect. I can add \" && push 'Command is Done'\" to the end of any command and get a notification on my watch/phone/(and desktop? But I don't have pushover on my desktop installed). Great for things you threw into a screen/tmux session and want to know when they finished."
        ],
        "id": "487d07fe-92c6-4cdd-903c-f47c3fc180ff",
        "url_text": "apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations ",
        "_version_": 1718536441755074561
      },
      {
        "story_id": 20587541,
        "story_author": "l2g",
        "story_descendants": 6,
        "story_score": 35,
        "story_time": "2019-08-01T19:44:25Z",
        "story_title": "Show HN: Apprise – A lightweight all-in-one notification solution (update)",
        "search": [
          "Show HN: Apprise – A lightweight all-in-one notification solution (update)",
          "https://github.com/caronc/apprise/#showhn-one-last-time",
          "apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/caronc/apprise/#showhn-one-last-time",
        "comments.comment_id": [20587555, 20597895],
        "comments.comment_author": ["l2g", "MaxwellsDaemon"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-08-01T19:45:59Z",
          "2019-08-03T00:50:08Z"
        ],
        "comments.comment_text": [
          "6 months ago [I posted about Apprise here](<a href=\"https://news.ycombinator.com/item?id=19023196\" rel=\"nofollow\">https://news.ycombinator.com/item?id=19023196</a>) and got a lot of amazing and encouraging feedback!  I since took just about everyone's comments and ideas at the time and implemented most of them.<p>Apprise now supports over 40+ different notification services, including configuration files that can be read from disk and the cloud! The library remains incredible light weight and easy to use.<p>I just wanted to share an almost completed solution and hope to hit you all up for more of your thoughts and advice!",
          "Worked about 10 years for a company named Appriss - <a href=\"https://appriss.com\" rel=\"nofollow\">https://appriss.com</a> - no real comment there other than to share the founding CEO used to say the board was presented with a bunch of names \"<i>and picked the worst one</i>\".<p>No real comment other than that, but nostalgia alone will make me play with this, so thanks for sharing."
        ],
        "id": "bca3125f-19ed-4b32-90e3-077dd42cccb8",
        "url_text": "apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations ",
        "_version_": 1718536506695483392
      },
      {
        "story_id": 21196709,
        "story_author": "vyuh",
        "story_descendants": 40,
        "story_score": 122,
        "story_time": "2019-10-08T20:17:59Z",
        "story_title": "Jtc – CLI tool to extract, manipulate and transform source JSON",
        "search": [
          "Jtc – CLI tool to extract, manipulate and transform source JSON",
          "https://github.com/ldn-softdev/jtc",
          "jtc - cli tool to extract, manipulate and transform source JSON jtc stand for: JSON transformational chains (used to be JSON test console). jtc offers a powerful way to select one or multiple elements from a source JSON and apply various actions on the selected elements at once (wrap selected elements into a new JSON, filter in/out, sort elements, update elements, insert new elements, remove, copy, move, compare, transform, swap around and many other operations). Enhancement requests and/or questions are more than welcome: ldn.softdev@gmail.com Content: Short description Compilation and installation options Linux and MacOS precompiled binaries Installing via MacPorts Installation on Linux distributions Manual installation Release Notes Quick-start guide list all URLs dump all bookmark names dump all URL's names dump all the URLs and corresponding names Debugging and validating JSON Complete User Guide C++ class and interface usage primer jtc vs jq utility ideology learning curve handling irregular JSONs solutions input invariance programming model JSON numerical fidelity performance compare jtc based solutions with jq's Short description - jtc is a simple yet very powerful and efficient cli utility tool to process and manipulate JSON data jtc offers following features (a short list of main features): simple user interface allowing applying a bulk of changes in a single or chained sets of commands featured walk-path interface lets extracting any combination of data from sourced JSON trees extracted data is representable either as found, or could be encapsulated in JSON array/object or transformed using templates support Regular Expressions when searching source JSON fast and efficient processing of very large JSON files (various built-in search caches) insert/update operations optionally may undergo shell cli evaluation support in-place modifications of the input/source JSON file features namespaces, facilitating interpolation of preserved JSON values in templates supports buffered and streamed modes of input read sports concurrent input JSON reading/parsing (on multi-core CPU) written entirely in C++14, no dependencies (STL only, idiomatic C++, no memory leaks) extensively debuggable conforms JSON specification (json.org) The walk-path feature is easy to understand - it's only made of 2 kinds of lexemes traversing JSON tree, which could be mixed up in any order: subscripts - enclosed into [, ], subscripts let traversing JSON tree downwards (towards the leaves) and upwards (towards the root) search lexemes - encased as <..> or >..< (for a recursive and non-recursive searches respectively); search lexemes facilitate various match criteria defined by an optional suffix and/or quantifier There's also a 3rd kind of lexemes - directives: they typically facilitate other functions like working with namespaces, controlling walk-path execution, etc; directives are syntactically similar to the search lexemes All lexemes can be iterable: iterable subscripts let iterating over children of currently addressed JSON iterables nodes (arrays/objects), while iterable search lexemes let iterating over all (recursive) matches for a given search criteria A walk-path may have an arbitrary number of lexemes -the tool accepts a virtually unlimited number of walk paths. See below more detailed explanation with examples Compilation and installation options For compiling, c++14 (or later) is required. To compile under different platforms: MacOS/BSD: c++ -o jtc -Wall -std=c++14 -Ofast jtc.cpp Linux: non-relocatable (dynamically linked) image: c++ -o jtc -Wall -std=gnu++14 -Ofast -pthread -lpthread jtc.cpp relocatable (statically linked) image: c++ -o jtc -Wall -std=gnu++14 -Ofast -static -Wl,--whole-archive -lrt -pthread -lpthread -Wl,--no-whole-archive jtc.cpp Debian: c++ -o jtc -Wall -std=c++14 -pthread -lpthread -Ofast jtc.cpp (ensure c++ poits to clang++-6.0 or above) Following debug related flags could be passed to jtc when compiling: -DNDEBUG: compile w/o debugs, however it's unadvisable - there's no performance gain from doing so -DNDBG_PARSER: disable debugs coming from parsing JSON (handy when deep debugging huge JSONs and want to skip parsing debugs) -DBG_FLOW: all debuggable function/method calls will disply an entry and exit points -DBG_mTS, -DBG_uTS: display absolute time-stamps in the debug: with millisecond accuracy and with microsecond accuracy respectively -DBG_dTS: used with either of 2 previous flags: makes time-stamp to display delta (since last debug message) instead of absolute stamp -DBG_CC: every call to a copy-constructor in Jnode class will reveal itself (handy for optimization debugging) Linux and MacOS precompiled binaries are available for download Choose the latest precompiled binary: latest macOS if you don't want to go through macOS security hurdle, then remove the quarantine attribute from the file after binary download, e.g. (assuming you opened terminal in the folder where downloaded binary is): bash $ mv ./jtc-macos-64.latest ./jtc bash $ chmod 754 ./jtc bash $ xattr -r -d com.apple.quarantine ./jtc latest linux 64 bit latest linux 32 bit Rename the downloaded file and give proper permissions. E.g., for the latest macOS: mv jtc-macos-64.latest jtc chmod 754 jtc Packaged installations: Installing via MacPorts On MacOS, you can install jtc via the MacPorts package manager: $ sudo port selfupdate $ sudo port install jtc Installation on Linux distributions jtc is packaged in the following Linux distributions and can be installed via the package manager. Fedora: jtc is present in Fedora 31 and later: openSUSE: jtc can be installed on openSUSE Tumbleweed via zypper: or on Leap 15.0 and later by adding the utilities repository and installing jtc via zypper. Manual installation: download jtc-master.zip, unzip it, descend into unzipped folder, compile using an appropriate command, move compiled file into an install location. here're the example steps for MacOS: say, jtc-master.zip has been downloaded to a folder and the terminal app is open in that folder: unzip jtc-master.zip cd jtc-master c++ -o jtc -Wall -std=c++17 -Ofast jtc.cpp sudo mv ./jtc /usr/local/bin/ Release Notes See the latest Release Notes Quick-start guide: run the command jtc -g to read the mini USER-GUIDE, with walk path syntax, usage notes, short examples read the examples just below see stackoverflow-json for lots of worked examples based on Stack Overflow questions refer to the complete User Guide for further examples and guidelines. Consider a following JSON (a mockup of a bookmark container), stored in a file Bookmarks: { \"Bookmarks\": [ { \"children\": [ { \"children\": [ { \"name\": \"The New York Times\", \"stamp\": \"2017-10-03, 12:05:19\", \"url\": \"https://www.nytimes.com/\" }, { \"name\": \"HuffPost UK\", \"stamp\": \"2017-11-23, 12:05:19\", \"url\": \"https://www.huffingtonpost.co.uk/\" } ], \"name\": \"News\", \"stamp\": \"2017-10-02, 12:05:19\" }, { \"children\": [ { \"name\": \"Digital Photography Review\", \"stamp\": \"2017-02-27, 12:05:19\", \"url\": \"https://www.dpreview.com/\" } ], \"name\": \"Photography\", \"stamp\": \"2017-02-27, 12:05:19\" } ], \"name\": \"Personal\", \"stamp\": \"2017-01-22, 12:05:19\" }, { \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ], \"name\": \"Work\", \"stamp\": \"2018-03-06, 12:07:29\" } ] } 1. let's start with a simple thing - list all URLs: bash $ jtc -w'<url>l:' Bookmarks \"https://www.nytimes.com/\" \"https://www.huffingtonpost.co.uk/\" \"https://www.dpreview.com/\" \"https://stackoverflow.com/\" \"https://en.cppreference.com/\" Let's take a look at the walk-path <url>l:: search lexemes are enclosed in angular brackets <, > - that style provides a recursive search throughout JSON suffix l instructs to search among labels only quantifier : instructs to find all occurrences, such quantifiers makes a path iterable 2. dump all bookmark names from the Work folder: bash $ jtc -w'<Work>[-1][children][:][name]' Bookmarks \"Stack Overflow\" \"C++ reference\" Here the walk-path <Work>[-1][children][:][name] is made of following lexemes: a. <Work>: find within a JSON tree the first occurrence where the JSON string value is matching \"Work\" exactly b. [-1]: step up one tier in the JSON tree structure (i.e., address an immediate parent of the found JSON element) c. [children]: select/address a node whose label is \"children\" (it'll be a JSON array, at the same tier with Work) d. [:]: select each node in the array e. [name]: select/address a node with the label \"name\" in order to understand better how the walk-path works, let's run that series of cli in a slow-motion, gradually adding lexemes to the path one by one, perhaps with the option -l to see also the labels (if any) of the selected elements: bash $ jtc -w'<Work>' -l Bookmarks \"name\": \"Work\" bash $ jtc -w'<Work>[-1]' -l Bookmarks { \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ], \"name\": \"Work\", \"stamp\": \"2018-03-06, 12:07:29\" } bash $ jtc -w'<Work>[-1][children]' -l Bookmarks \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ] bash $ jtc -w'<Work>[-1][children][:]' -l Bookmarks { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" } { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } bash $ jtc -w'<Work>[-1][children][:][name]' -l Bookmarks \"name\": \"Stack Overflow\" \"name\": \"C++ reference\" B.t.w., a better (a bit faster and more efficient) walk-path achieving the same query would be this: jtc -w'<Work>[-1][children]<name>l:' Bookmarks 3. dump all URL's names: bash $ jtc -w'<url>l:[-1][name]' Bookmarks \"The New York Times\" \"HuffPost UK\" \"Digital Photography Review\" \"Stack Overflow\" \"C++ reference\" this walk-path <url>l:[-1][name]: finds recursively (encasement <, >) each (:) JSON element with a label (l) matching url then for an each found JSON element, select its parent ([-1]) then, select a JSON (sub)element with the label \"name\" 4. dump all the URLs and their corresponding names, preferably wrap found pairs in JSON array: bash $ jtc -w'<url>l:' -w'<url>l:[-1][name]' -jl Bookmarks [ { \"name\": \"The New York Times\", \"url\": \"https://www.nytimes.com/\" }, { \"name\": \"HuffPost UK\", \"url\": \"https://www.huffingtonpost.co.uk/\" }, { \"name\": \"Digital Photography Review\", \"url\": \"https://www.dpreview.com/\" }, { \"name\": \"Stack Overflow\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"url\": \"https://en.cppreference.com/\" } ] yes, multiple walks (-w) are allowed option -j will wrap the walked outputs into a JSON array, but not just, option -l used together with -j will ensure relevant walks are grouped together (try without -l) if multiple walks (-w) are present, by default, walked results will be printed interleaved (if it can be interleaved) 5. Debugging and validating JSON jtc is extensively debuggable: the more times option -d is passed the more debugs will be produced. Enabling too many debugs might be overwhelming, though one specific case many would find extremely useful - when validating a failing JSON: bash $ <addressbook-sample.json jtc jtc json exception: expected_json_value If JSON is big, it's desirable to locate the parsing failure point. Passing just one -d let easily spotting the parsing failure point and its locus: bash $ <addressbook-sample.json jtc -d .display_opts(), option set[0]: -d (internally imposed: ) .init_inputs(), reading json from <stdin> .exception_locus_(), ... }| ],| \"children\": [,],| \"spouse\": null| },| {| ... .exception_spot_(), -------------------------------------------->| (offset: 967) jtc json parsing exception (<stdin>:967): expected_json_value bash $ Complete User Guide there's a lot more under the hood of jtc: various viewing options, directives allowing controlling walks, preserving parts of whole JSONs in namespaces, walking with various criteria, etc interpolating namespaces and walk results in templates and lexemes amending input JSONs via purge/swap/update/insert/move/merge operations comparing JSONs (or their parts) or their schemas various processing modes (streamed, buffered, concurrent parsing, chaining operations, etc) and more ... Refer to a complete User Guide for further examples and guidelines. C++ class and interface usage primer Refer to a Class usage primer document. jtc vs jq: jtc was inspired by the complexity of jq interface (and its DSL), aiming to provide users a tool which would let attaining the desired JSON queries in an easier, more feasible and succinct way utility ideology: jq is a stateful processor with own DSL, variables, operations, control flow logic, IO system, etc, etc jtc is a unix utility confining its functionality to operation types with its data model only (as per unix ideology). jtc performs one major operation at a time (like insertion, update, swap, etc), however multiple operations could be chained using / delimiter jq is non-idiomatic in a unix way, e.g.: one can write a program in jq language that even has nothing to do with JSON. Most of the requests (if not all) to manipulate JSONs are ad hoc type of tasks, and learning jq's DSL for ad hoc type of tasks is an overkill (that purpose is best facilitated with GPL, e.g.: Python). The number of asks on the stackoverflow to facilitate even simple queries for jq is huge - that's the proof in itself that for many people feasibility of attaining their asks with jq is a way too low, hence they default to posting their questions on the forum. jtc on the other hand is a utility (not a language), which employs a novel but powerful concept, which \"embeds\" the ask right into the walk-path. That facilitates a much higher feasibility of attaining a desired result: building a walk-path a lexeme by lexeme, one at a time, provides an immediate visual feedback and let coming up with the desired result rather quickly. learning curve: jq: before you could come up with a query to handle even a relatively simple ask, you need to become an expert in jq language, which will take some time. Coming up with the complex queries requires what it seems having a PhD in jq, or spending lots of time on stackoverflow and similar forums jtc employs only a simple (but powerful) concept of the walk-path (which is made only of 2 types of search lexemes, each type though has several variants) which is quite easy to grasp. handling irregular JSONs: jq: handling irregular JSONs for jq is not a challenge, building a query is! The more irregularities you need to handle the more challenging the query (jq program) becomes jtc was conceived with the idea of being capable of handling complex irregular JSONs with a simplified interface - that all is fitted into the concept of the walk-path, while daisy-chaining multiple operations is possible to satisfy almost every ask. solutions input invariance - most of jtc solutions would be input invariant (hardly the same could be stated for jq). Not that it's impossible to come up with invariant solutions in jq, it's just a lot more harder, while jtc with its walk-path model prompts for invariant solutions. I.e., the invariant solution will keep working even once the JSON outer format changes (the invariant solution only would stop working once the relationship between walked JSON elements changes). E.g.: consider a following query, extract format [ \"name\", \"surname\" ] from 2 types of JSON: bash $ case1='{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29}' bash $ case2='[{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29},{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}]' a natural, idiomatic jtc solution would be: bash $ <<<$case1 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] bash $ <<<$case2 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] [ \"Alice\", \"Price\" ] While one of the most probable jq solution would be: bash $ <<<$case1 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [\"Patrick\",\"Lynch\"] bash $ <<<$case2 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [\"Patrick\",\"Lynch\"] [\"Alice\",\"Price\"] The both solutions work correctly, however, any change in the outer encapsulation will break jq's solution , while jtc will keep working even if JSON is reshaped into an irregular structure, e.g.: #jtc: bash $ case3='{\"root\":[{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29}, {\"closed circle\":[{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}, {\"Name\":\"Rebecca\", \"Surname\":\"Hernandez\", \"gender\":\"female\", \"age\":28}]}]}' bash $ bash $ <<<$case3 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] [ \"Alice\", \"Price\" ] [ \"Rebecca\", \"Hernandez\" ] #jq: bash $ <<<$case3 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [null,null] The same property makes jtc solutions resistant to cases of incomplete data, e.g.: if we drop \"Name\" entry from one of the entries in case 2, jtc solution still works correctly: #jtc: bash $ case2='[{\"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29},{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}]' bash $ bash $ <<<$case2 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Alice\", \"Price\" ] #jq: bash $ <<<$case2 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [null,\"Lynch\"] [\"Alice\",\"Price\"] - i.e., jtc will not assume that user would require some default substitution in case of incomplete data (but if such handling is required then the walk-path can be easily enhanced) programming model jq is written in C language, which drags all intrinsic problems the language has dated its creation (here's what I mean) jtc is written in the idiomatic C++14 using STL only. jtc does not have a single naked memory allocation operator (those few new operators required for legacy interface are implemented as guards), nor it has a single naked pointer acting as a resource holder/owner, thus jtc is guaranteed to be free of memory/resources leaks (at least one class of the problems is off the table) - STL guaranty. Also, jtc is written in a very portable way, it should not cause problems compiling it under any unix like system. JSON numerical fidelity: jq is not compliant with JSON numerical definition. What jq does, it simply converts a symbolic numerical representation to an internal binary and keeps it that way. That approach: is not compliant with JSON definition of the numerical values it has problems retaining required precision might change original representation of numericals leads to incorrect processing of some JSON streams jtc validates all JSON numericals per JSON standard and keep numbers internally in their original literal format, so it's free of all the above caveats, compare: Handling jtc jq 1.6 Invalid Json: [00] <<<'[00]' jtc <<<'[00]' jq -c . Parsing result jtc json parsing exception (<stdin>:3): missed_prior_enumeration [0] Precision test: <<<'[0.99999999999999999]' jtc -r <<<'[0.99999999999999999]' jq -c . Parsing result [ 0.99999999999999999 ] [1] Retaining original format: <<<'[0.00001]' jtc -r <<<'[0.00001]' jq -c . Parsing result [ 0.00001 ] [1e-05] Stream of atomic JSONs: <<<'{}[]\"bar\"\"foo\"00123truefalsenull' jtc -Jr <<<'{}[]\"bar\"\"foo\"00123truefalsenull' jq -sc Parsing result [ {}, [], \"bar\", \"foo\", 0, 0, 123, true, false, null ] parse error: Invalid numeric literal at line 2, column 0 performance: jq is a single-threaded process jtc engages a concurrent (multi-threaded) reading/parsing when multiple files given (the advantage could be observed on multi-core CPU, though it become noticeable only with relatively big JSONs or with relatively big number of files processed) Comparison of single-threaded performance: Here's a 4+ million node JSON file standard.json: bash $ time jtc -zz standard.json 4329975 user 6.085 sec The table below compares jtc and jq performance for similar operations (using TIMEFORMAT=\"user %U sec\"): jtc 1.76 jq 1.6 parsing JSON: parsing JSON: bash $ time jtc -t2 standard.json | md5 bash $ time jq -M . standard.json | md5 d3b56762fd3a22d664fdd2f46f029599 d3b56762fd3a22d664fdd2f46f029599 user 9.110 sec user 18.853 sec removing by key from JSON: removing by key from JSON: bash $ time jtc -t2 -pw'<attributes>l:' standard.json | md5 bash $ time jq -M 'del(..|.attributes?)' standard.json | md5 0624aec46294399bcb9544ae36a33cd5 0624aec46294399bcb9544ae36a33cd5 user 10.027 sec user 27.439 sec updating JSON recursively by label: updating JSON recursively by label: bash $ time jtc -t2 -w'<attributes>l:[-1]' -i'{\"reserved\": null}' standard.json | md5 bash $ time jq -M 'walk(if type == \"object\" and has(\"attributes\") then . + { \"reserved\" : null } else . end)' standard.json | md5 6c86462ae6b71e10e3ea114e86659ab5 6c86462ae6b71e10e3ea114e86659ab5 user 12.715 sec user 29.450 sec Comparison of jtc to jtc (single-threaded to multi-threaded parsing performance): bash $ unset TIMEFORMAT bash $ bash $ # concurrent (multi-threaded) parsing: bash $ time jtc -J / -zz standard.json standard.json standard.json standard.json standard.json 21649876 real 0m10.995s # <- compare these figures user 0m34.083s sys 0m3.288s bash $ bash $ # sequential (single-threaded) parsing: bash $ time jtc -aJ / -zz standard.json standard.json standard.json standard.json standard.json 21649876 real 0m31.717s # <- compare these figures user 0m30.125s sys 0m1.555s bash $ Machine spec used for testing: Model Name: MacBook Pro Model Identifier: MacBookPro15,1 Processor Name: Intel Core i7 Processor Speed: 2,6 GHz Number of Processors: 1 Total Number of Cores: 6 L2 Cache (per Core): 256 KB L3 Cache: 12 MB Hyper-Threading Technology: Enabled Memory: 16 GB 2400 MHz DDR4 compare jtc based solutions with jq's: Here are published some answers for JSON queries using jtc, you may compare those with jq's, as well as study the feasibility of the solutions, test relevant performance, etc Refer to a complete User Guide for further examples and guidelines. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/ldn-softdev/jtc",
        "comments.comment_id": [21197221, 21197356],
        "comments.comment_author": ["meddlepal", "vyuh"],
        "comments.comment_descendants": [3, 2],
        "comments.comment_time": [
          "2019-10-08T21:01:03Z",
          "2019-10-08T21:15:43Z"
        ],
        "comments.comment_text": [
          "> jtc is written in idiomatic C++ (the most powerful programming language to date)<p>Citation needed. The `jq` vs `jtc` section is interesting, but author seems a little full of himself with some of the explanations.",
          "The author has also written related tools. One to convert XML to JSON and back (<a href=\"https://github.com/ldn-softdev/jtm\" rel=\"nofollow\">https://github.com/ldn-softdev/jtm</a>) and another to convert JSON to SQLite tables (<a href=\"https://github.com/ldn-softdev/jsl\" rel=\"nofollow\">https://github.com/ldn-softdev/jsl</a>). Combining these with the hxnormalize tool ( <a href=\"https://www.w3.org/Tools/HTML-XML-utils/man1/hxnormalize.html\" rel=\"nofollow\">https://www.w3.org/Tools/HTML-XML-utils/man1/hxnormalize.htm...</a>), one can do very sophisticated manipulation on HTML web pages.<p>HTML -> XML (via hxnormalize) -> JSON (via jtm) -> process using jtc (or even jq)"
        ],
        "id": "8437cb4b-556b-41f3-aaf9-33dfc1de794c",
        "url_text": "jtc - cli tool to extract, manipulate and transform source JSON jtc stand for: JSON transformational chains (used to be JSON test console). jtc offers a powerful way to select one or multiple elements from a source JSON and apply various actions on the selected elements at once (wrap selected elements into a new JSON, filter in/out, sort elements, update elements, insert new elements, remove, copy, move, compare, transform, swap around and many other operations). Enhancement requests and/or questions are more than welcome: ldn.softdev@gmail.com Content: Short description Compilation and installation options Linux and MacOS precompiled binaries Installing via MacPorts Installation on Linux distributions Manual installation Release Notes Quick-start guide list all URLs dump all bookmark names dump all URL's names dump all the URLs and corresponding names Debugging and validating JSON Complete User Guide C++ class and interface usage primer jtc vs jq utility ideology learning curve handling irregular JSONs solutions input invariance programming model JSON numerical fidelity performance compare jtc based solutions with jq's Short description - jtc is a simple yet very powerful and efficient cli utility tool to process and manipulate JSON data jtc offers following features (a short list of main features): simple user interface allowing applying a bulk of changes in a single or chained sets of commands featured walk-path interface lets extracting any combination of data from sourced JSON trees extracted data is representable either as found, or could be encapsulated in JSON array/object or transformed using templates support Regular Expressions when searching source JSON fast and efficient processing of very large JSON files (various built-in search caches) insert/update operations optionally may undergo shell cli evaluation support in-place modifications of the input/source JSON file features namespaces, facilitating interpolation of preserved JSON values in templates supports buffered and streamed modes of input read sports concurrent input JSON reading/parsing (on multi-core CPU) written entirely in C++14, no dependencies (STL only, idiomatic C++, no memory leaks) extensively debuggable conforms JSON specification (json.org) The walk-path feature is easy to understand - it's only made of 2 kinds of lexemes traversing JSON tree, which could be mixed up in any order: subscripts - enclosed into [, ], subscripts let traversing JSON tree downwards (towards the leaves) and upwards (towards the root) search lexemes - encased as <..> or >..< (for a recursive and non-recursive searches respectively); search lexemes facilitate various match criteria defined by an optional suffix and/or quantifier There's also a 3rd kind of lexemes - directives: they typically facilitate other functions like working with namespaces, controlling walk-path execution, etc; directives are syntactically similar to the search lexemes All lexemes can be iterable: iterable subscripts let iterating over children of currently addressed JSON iterables nodes (arrays/objects), while iterable search lexemes let iterating over all (recursive) matches for a given search criteria A walk-path may have an arbitrary number of lexemes -the tool accepts a virtually unlimited number of walk paths. See below more detailed explanation with examples Compilation and installation options For compiling, c++14 (or later) is required. To compile under different platforms: MacOS/BSD: c++ -o jtc -Wall -std=c++14 -Ofast jtc.cpp Linux: non-relocatable (dynamically linked) image: c++ -o jtc -Wall -std=gnu++14 -Ofast -pthread -lpthread jtc.cpp relocatable (statically linked) image: c++ -o jtc -Wall -std=gnu++14 -Ofast -static -Wl,--whole-archive -lrt -pthread -lpthread -Wl,--no-whole-archive jtc.cpp Debian: c++ -o jtc -Wall -std=c++14 -pthread -lpthread -Ofast jtc.cpp (ensure c++ poits to clang++-6.0 or above) Following debug related flags could be passed to jtc when compiling: -DNDEBUG: compile w/o debugs, however it's unadvisable - there's no performance gain from doing so -DNDBG_PARSER: disable debugs coming from parsing JSON (handy when deep debugging huge JSONs and want to skip parsing debugs) -DBG_FLOW: all debuggable function/method calls will disply an entry and exit points -DBG_mTS, -DBG_uTS: display absolute time-stamps in the debug: with millisecond accuracy and with microsecond accuracy respectively -DBG_dTS: used with either of 2 previous flags: makes time-stamp to display delta (since last debug message) instead of absolute stamp -DBG_CC: every call to a copy-constructor in Jnode class will reveal itself (handy for optimization debugging) Linux and MacOS precompiled binaries are available for download Choose the latest precompiled binary: latest macOS if you don't want to go through macOS security hurdle, then remove the quarantine attribute from the file after binary download, e.g. (assuming you opened terminal in the folder where downloaded binary is): bash $ mv ./jtc-macos-64.latest ./jtc bash $ chmod 754 ./jtc bash $ xattr -r -d com.apple.quarantine ./jtc latest linux 64 bit latest linux 32 bit Rename the downloaded file and give proper permissions. E.g., for the latest macOS: mv jtc-macos-64.latest jtc chmod 754 jtc Packaged installations: Installing via MacPorts On MacOS, you can install jtc via the MacPorts package manager: $ sudo port selfupdate $ sudo port install jtc Installation on Linux distributions jtc is packaged in the following Linux distributions and can be installed via the package manager. Fedora: jtc is present in Fedora 31 and later: openSUSE: jtc can be installed on openSUSE Tumbleweed via zypper: or on Leap 15.0 and later by adding the utilities repository and installing jtc via zypper. Manual installation: download jtc-master.zip, unzip it, descend into unzipped folder, compile using an appropriate command, move compiled file into an install location. here're the example steps for MacOS: say, jtc-master.zip has been downloaded to a folder and the terminal app is open in that folder: unzip jtc-master.zip cd jtc-master c++ -o jtc -Wall -std=c++17 -Ofast jtc.cpp sudo mv ./jtc /usr/local/bin/ Release Notes See the latest Release Notes Quick-start guide: run the command jtc -g to read the mini USER-GUIDE, with walk path syntax, usage notes, short examples read the examples just below see stackoverflow-json for lots of worked examples based on Stack Overflow questions refer to the complete User Guide for further examples and guidelines. Consider a following JSON (a mockup of a bookmark container), stored in a file Bookmarks: { \"Bookmarks\": [ { \"children\": [ { \"children\": [ { \"name\": \"The New York Times\", \"stamp\": \"2017-10-03, 12:05:19\", \"url\": \"https://www.nytimes.com/\" }, { \"name\": \"HuffPost UK\", \"stamp\": \"2017-11-23, 12:05:19\", \"url\": \"https://www.huffingtonpost.co.uk/\" } ], \"name\": \"News\", \"stamp\": \"2017-10-02, 12:05:19\" }, { \"children\": [ { \"name\": \"Digital Photography Review\", \"stamp\": \"2017-02-27, 12:05:19\", \"url\": \"https://www.dpreview.com/\" } ], \"name\": \"Photography\", \"stamp\": \"2017-02-27, 12:05:19\" } ], \"name\": \"Personal\", \"stamp\": \"2017-01-22, 12:05:19\" }, { \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ], \"name\": \"Work\", \"stamp\": \"2018-03-06, 12:07:29\" } ] } 1. let's start with a simple thing - list all URLs: bash $ jtc -w'<url>l:' Bookmarks \"https://www.nytimes.com/\" \"https://www.huffingtonpost.co.uk/\" \"https://www.dpreview.com/\" \"https://stackoverflow.com/\" \"https://en.cppreference.com/\" Let's take a look at the walk-path <url>l:: search lexemes are enclosed in angular brackets <, > - that style provides a recursive search throughout JSON suffix l instructs to search among labels only quantifier : instructs to find all occurrences, such quantifiers makes a path iterable 2. dump all bookmark names from the Work folder: bash $ jtc -w'<Work>[-1][children][:][name]' Bookmarks \"Stack Overflow\" \"C++ reference\" Here the walk-path <Work>[-1][children][:][name] is made of following lexemes: a. <Work>: find within a JSON tree the first occurrence where the JSON string value is matching \"Work\" exactly b. [-1]: step up one tier in the JSON tree structure (i.e., address an immediate parent of the found JSON element) c. [children]: select/address a node whose label is \"children\" (it'll be a JSON array, at the same tier with Work) d. [:]: select each node in the array e. [name]: select/address a node with the label \"name\" in order to understand better how the walk-path works, let's run that series of cli in a slow-motion, gradually adding lexemes to the path one by one, perhaps with the option -l to see also the labels (if any) of the selected elements: bash $ jtc -w'<Work>' -l Bookmarks \"name\": \"Work\" bash $ jtc -w'<Work>[-1]' -l Bookmarks { \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ], \"name\": \"Work\", \"stamp\": \"2018-03-06, 12:07:29\" } bash $ jtc -w'<Work>[-1][children]' -l Bookmarks \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ] bash $ jtc -w'<Work>[-1][children][:]' -l Bookmarks { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" } { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } bash $ jtc -w'<Work>[-1][children][:][name]' -l Bookmarks \"name\": \"Stack Overflow\" \"name\": \"C++ reference\" B.t.w., a better (a bit faster and more efficient) walk-path achieving the same query would be this: jtc -w'<Work>[-1][children]<name>l:' Bookmarks 3. dump all URL's names: bash $ jtc -w'<url>l:[-1][name]' Bookmarks \"The New York Times\" \"HuffPost UK\" \"Digital Photography Review\" \"Stack Overflow\" \"C++ reference\" this walk-path <url>l:[-1][name]: finds recursively (encasement <, >) each (:) JSON element with a label (l) matching url then for an each found JSON element, select its parent ([-1]) then, select a JSON (sub)element with the label \"name\" 4. dump all the URLs and their corresponding names, preferably wrap found pairs in JSON array: bash $ jtc -w'<url>l:' -w'<url>l:[-1][name]' -jl Bookmarks [ { \"name\": \"The New York Times\", \"url\": \"https://www.nytimes.com/\" }, { \"name\": \"HuffPost UK\", \"url\": \"https://www.huffingtonpost.co.uk/\" }, { \"name\": \"Digital Photography Review\", \"url\": \"https://www.dpreview.com/\" }, { \"name\": \"Stack Overflow\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"url\": \"https://en.cppreference.com/\" } ] yes, multiple walks (-w) are allowed option -j will wrap the walked outputs into a JSON array, but not just, option -l used together with -j will ensure relevant walks are grouped together (try without -l) if multiple walks (-w) are present, by default, walked results will be printed interleaved (if it can be interleaved) 5. Debugging and validating JSON jtc is extensively debuggable: the more times option -d is passed the more debugs will be produced. Enabling too many debugs might be overwhelming, though one specific case many would find extremely useful - when validating a failing JSON: bash $ <addressbook-sample.json jtc jtc json exception: expected_json_value If JSON is big, it's desirable to locate the parsing failure point. Passing just one -d let easily spotting the parsing failure point and its locus: bash $ <addressbook-sample.json jtc -d .display_opts(), option set[0]: -d (internally imposed: ) .init_inputs(), reading json from <stdin> .exception_locus_(), ... }| ],| \"children\": [,],| \"spouse\": null| },| {| ... .exception_spot_(), -------------------------------------------->| (offset: 967) jtc json parsing exception (<stdin>:967): expected_json_value bash $ Complete User Guide there's a lot more under the hood of jtc: various viewing options, directives allowing controlling walks, preserving parts of whole JSONs in namespaces, walking with various criteria, etc interpolating namespaces and walk results in templates and lexemes amending input JSONs via purge/swap/update/insert/move/merge operations comparing JSONs (or their parts) or their schemas various processing modes (streamed, buffered, concurrent parsing, chaining operations, etc) and more ... Refer to a complete User Guide for further examples and guidelines. C++ class and interface usage primer Refer to a Class usage primer document. jtc vs jq: jtc was inspired by the complexity of jq interface (and its DSL), aiming to provide users a tool which would let attaining the desired JSON queries in an easier, more feasible and succinct way utility ideology: jq is a stateful processor with own DSL, variables, operations, control flow logic, IO system, etc, etc jtc is a unix utility confining its functionality to operation types with its data model only (as per unix ideology). jtc performs one major operation at a time (like insertion, update, swap, etc), however multiple operations could be chained using / delimiter jq is non-idiomatic in a unix way, e.g.: one can write a program in jq language that even has nothing to do with JSON. Most of the requests (if not all) to manipulate JSONs are ad hoc type of tasks, and learning jq's DSL for ad hoc type of tasks is an overkill (that purpose is best facilitated with GPL, e.g.: Python). The number of asks on the stackoverflow to facilitate even simple queries for jq is huge - that's the proof in itself that for many people feasibility of attaining their asks with jq is a way too low, hence they default to posting their questions on the forum. jtc on the other hand is a utility (not a language), which employs a novel but powerful concept, which \"embeds\" the ask right into the walk-path. That facilitates a much higher feasibility of attaining a desired result: building a walk-path a lexeme by lexeme, one at a time, provides an immediate visual feedback and let coming up with the desired result rather quickly. learning curve: jq: before you could come up with a query to handle even a relatively simple ask, you need to become an expert in jq language, which will take some time. Coming up with the complex queries requires what it seems having a PhD in jq, or spending lots of time on stackoverflow and similar forums jtc employs only a simple (but powerful) concept of the walk-path (which is made only of 2 types of search lexemes, each type though has several variants) which is quite easy to grasp. handling irregular JSONs: jq: handling irregular JSONs for jq is not a challenge, building a query is! The more irregularities you need to handle the more challenging the query (jq program) becomes jtc was conceived with the idea of being capable of handling complex irregular JSONs with a simplified interface - that all is fitted into the concept of the walk-path, while daisy-chaining multiple operations is possible to satisfy almost every ask. solutions input invariance - most of jtc solutions would be input invariant (hardly the same could be stated for jq). Not that it's impossible to come up with invariant solutions in jq, it's just a lot more harder, while jtc with its walk-path model prompts for invariant solutions. I.e., the invariant solution will keep working even once the JSON outer format changes (the invariant solution only would stop working once the relationship between walked JSON elements changes). E.g.: consider a following query, extract format [ \"name\", \"surname\" ] from 2 types of JSON: bash $ case1='{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29}' bash $ case2='[{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29},{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}]' a natural, idiomatic jtc solution would be: bash $ <<<$case1 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] bash $ <<<$case2 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] [ \"Alice\", \"Price\" ] While one of the most probable jq solution would be: bash $ <<<$case1 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [\"Patrick\",\"Lynch\"] bash $ <<<$case2 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [\"Patrick\",\"Lynch\"] [\"Alice\",\"Price\"] The both solutions work correctly, however, any change in the outer encapsulation will break jq's solution , while jtc will keep working even if JSON is reshaped into an irregular structure, e.g.: #jtc: bash $ case3='{\"root\":[{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29}, {\"closed circle\":[{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}, {\"Name\":\"Rebecca\", \"Surname\":\"Hernandez\", \"gender\":\"female\", \"age\":28}]}]}' bash $ bash $ <<<$case3 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] [ \"Alice\", \"Price\" ] [ \"Rebecca\", \"Hernandez\" ] #jq: bash $ <<<$case3 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [null,null] The same property makes jtc solutions resistant to cases of incomplete data, e.g.: if we drop \"Name\" entry from one of the entries in case 2, jtc solution still works correctly: #jtc: bash $ case2='[{\"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29},{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}]' bash $ bash $ <<<$case2 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Alice\", \"Price\" ] #jq: bash $ <<<$case2 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [null,\"Lynch\"] [\"Alice\",\"Price\"] - i.e., jtc will not assume that user would require some default substitution in case of incomplete data (but if such handling is required then the walk-path can be easily enhanced) programming model jq is written in C language, which drags all intrinsic problems the language has dated its creation (here's what I mean) jtc is written in the idiomatic C++14 using STL only. jtc does not have a single naked memory allocation operator (those few new operators required for legacy interface are implemented as guards), nor it has a single naked pointer acting as a resource holder/owner, thus jtc is guaranteed to be free of memory/resources leaks (at least one class of the problems is off the table) - STL guaranty. Also, jtc is written in a very portable way, it should not cause problems compiling it under any unix like system. JSON numerical fidelity: jq is not compliant with JSON numerical definition. What jq does, it simply converts a symbolic numerical representation to an internal binary and keeps it that way. That approach: is not compliant with JSON definition of the numerical values it has problems retaining required precision might change original representation of numericals leads to incorrect processing of some JSON streams jtc validates all JSON numericals per JSON standard and keep numbers internally in their original literal format, so it's free of all the above caveats, compare: Handling jtc jq 1.6 Invalid Json: [00] <<<'[00]' jtc <<<'[00]' jq -c . Parsing result jtc json parsing exception (<stdin>:3): missed_prior_enumeration [0] Precision test: <<<'[0.99999999999999999]' jtc -r <<<'[0.99999999999999999]' jq -c . Parsing result [ 0.99999999999999999 ] [1] Retaining original format: <<<'[0.00001]' jtc -r <<<'[0.00001]' jq -c . Parsing result [ 0.00001 ] [1e-05] Stream of atomic JSONs: <<<'{}[]\"bar\"\"foo\"00123truefalsenull' jtc -Jr <<<'{}[]\"bar\"\"foo\"00123truefalsenull' jq -sc Parsing result [ {}, [], \"bar\", \"foo\", 0, 0, 123, true, false, null ] parse error: Invalid numeric literal at line 2, column 0 performance: jq is a single-threaded process jtc engages a concurrent (multi-threaded) reading/parsing when multiple files given (the advantage could be observed on multi-core CPU, though it become noticeable only with relatively big JSONs or with relatively big number of files processed) Comparison of single-threaded performance: Here's a 4+ million node JSON file standard.json: bash $ time jtc -zz standard.json 4329975 user 6.085 sec The table below compares jtc and jq performance for similar operations (using TIMEFORMAT=\"user %U sec\"): jtc 1.76 jq 1.6 parsing JSON: parsing JSON: bash $ time jtc -t2 standard.json | md5 bash $ time jq -M . standard.json | md5 d3b56762fd3a22d664fdd2f46f029599 d3b56762fd3a22d664fdd2f46f029599 user 9.110 sec user 18.853 sec removing by key from JSON: removing by key from JSON: bash $ time jtc -t2 -pw'<attributes>l:' standard.json | md5 bash $ time jq -M 'del(..|.attributes?)' standard.json | md5 0624aec46294399bcb9544ae36a33cd5 0624aec46294399bcb9544ae36a33cd5 user 10.027 sec user 27.439 sec updating JSON recursively by label: updating JSON recursively by label: bash $ time jtc -t2 -w'<attributes>l:[-1]' -i'{\"reserved\": null}' standard.json | md5 bash $ time jq -M 'walk(if type == \"object\" and has(\"attributes\") then . + { \"reserved\" : null } else . end)' standard.json | md5 6c86462ae6b71e10e3ea114e86659ab5 6c86462ae6b71e10e3ea114e86659ab5 user 12.715 sec user 29.450 sec Comparison of jtc to jtc (single-threaded to multi-threaded parsing performance): bash $ unset TIMEFORMAT bash $ bash $ # concurrent (multi-threaded) parsing: bash $ time jtc -J / -zz standard.json standard.json standard.json standard.json standard.json 21649876 real 0m10.995s # <- compare these figures user 0m34.083s sys 0m3.288s bash $ bash $ # sequential (single-threaded) parsing: bash $ time jtc -aJ / -zz standard.json standard.json standard.json standard.json standard.json 21649876 real 0m31.717s # <- compare these figures user 0m30.125s sys 0m1.555s bash $ Machine spec used for testing: Model Name: MacBook Pro Model Identifier: MacBookPro15,1 Processor Name: Intel Core i7 Processor Speed: 2,6 GHz Number of Processors: 1 Total Number of Cores: 6 L2 Cache (per Core): 256 KB L3 Cache: 12 MB Hyper-Threading Technology: Enabled Memory: 16 GB 2400 MHz DDR4 compare jtc based solutions with jq's: Here are published some answers for JSON queries using jtc, you may compare those with jq's, as well as study the feasibility of the solutions, test relevant performance, etc Refer to a complete User Guide for further examples and guidelines. ",
        "_version_": 1718536530763448320
      },
      {
        "story_id": 18906834,
        "story_author": "byxorna",
        "story_descendants": 17,
        "story_score": 72,
        "story_time": "2019-01-14T21:46:15Z",
        "story_title": "Tumblr Opensources Kubernetes Utilities",
        "search": [
          "Tumblr Opensources Kubernetes Utilities",
          "https://engineering.tumblr.com/post/182013497734/open-sourcing-our-kubernetes-tools",
          "At Tumblr, we are avid fans of Kubernetes. We have been using Kubernetes for all manner of workloads, like critical-path web requests handling for tumblr.com, background task executions like sending queued posts and push notifications, and scheduled jobs for spam detection and content moderation. Throughout our journey to move our 11 year old (almost 12! ) platform to a container-native architecture, we have made innumerable changes to how our applications are designed and run. Inspired by a lot of existing Kubernetes APIs and best practices, were excited to share with the community some of the tools weve developed at Tumblr as our infrastructure has evolved to work with Kubernetes.To help us integrate Kubernetes into our workflows, we have built a handful of tools of which we are open-sourcing three today! Each tool is a small, focused utility, designed to solve specific integration needs Tumblr had while migrating our workflows to Kubernetes. The tools were built to handle our needs internally, but we believe they are useful to the wider Kubernetes community.github.com/tumblr/k8s-sidecar-injectorgithub.com/tumblr/k8s-config-projectorgithub.com/tumblr/k8s-secret-projectork8s-sidecar-injectorAny company that has containerized an application as large and complex as Tumblr knows that it requires a tremendous amount of effort. Applications dont become container-native overnight, and sidecars can be useful to help emulate older deployments with colocated services on physical hosts or VMs. To reduce the amount of fragile copy-paste code by developers adding in sidecars to their Deployments and CronJobs, we created a service to dynamically inject sidecars, volumes, and environment data into pods as they are launched.The k8s-sidecar-injector listens to the Kubernetes API for Pod launches that contain annotations requesting a specific sidecar to be injected. For example, the annotation injector.tumblr.com/request=sidecar-prod-v1 will add any environment variables, volumes, and containers defined in the sidecar-prod-v1 configuration. We use this to add sidecars like logging and metrics daemons, cluster-wide environment variables like DATACENTER and HTTP_PROXY settings, and volumes for shared configuration data. By centralizing configuration of sidecars, we were able to reduce complexity in CronJobs and Deployments by hundreds of lines, eliminated copy-paste errors, and made rolling out updates to shared components in our sidecars effortless.An example sidecar ConfigMap is below, which adds a logging container, a volume from a logger-config ConfigMap, and some environment variables into the Pod.--- apiVersion: v1 kind: ConfigMap metadata: name: example-sidecars namespace: kube-system labels app: k8s-sidecar-injector data: logger-v1: | name: logger-v1 containers: - name: logger image: some/logger:2.2.3 imagePullPolicy: IfNotPresent ports: - containerPort: 8888 volumeMounts: - name: logger-conf mountPath: /etc/logger volumes: - name: logger-conf configMap: name: logger-config env: - name: DATACENTER value: dc01 - name: HTTP_PROXY value: http://my-proxy.org:8080/ - name: HTTPS_PROXY value: http://my-proxy.org:8080/This configuration will add the logger container into each pod with the annotation injector.tumblr.com/request: logger-v1, with a ConfigMap projected as a volume in /etc/logger. Additionally, every container in the Pod will get the DATACENTER=dc01 and HTTP_PROXY environment variables added, if they were not already set. This has allowed us to drastically reduce our boilerplate configuration when containerizing legacy applications that require a complex sidecar configuration.k8s-config-projectorInternally, we have many types of configuration data that is needed by a variety of applications. We store canonical settings data like feature flags, lists of hosts/IPs+ports, and application settings in git. This allows automated generation/manipulation of these settings by bots, cron jobs, Collins, and humans alike. Applications want to know about some subset of this configuration data, and they want to be informed when this data changes as quickly as possible. Kubernetes provides the ConfigMap resource, which enables users to provide their service with configuration data and update the data in running pods without requiring a redeployment. We wanted to use this to configure our services and jobs in a Kubernetes-native manner, but needed a way to bridge the gap between our canonical configuration store (git repo of config files) to ConfigMaps. Thus, was k8s-config-projector born.The Config Projector (github.com/tumblr/k8s-config-projector)[github.com/tumblr/k8s-config-projector] is a command line tool, meant to be run by CI processes. It combines a git repo hosting configuration data (feature flags, lists of hostnames+ports, application settings) with a set of projection manifest files that describe how to group/extract settings from the config repo and transmute them into ConfigMaps. The config projector allows developers to encode a set of configuration data the application needs to run into a projection manifest. As the configuration data changes in the git repository, CI will run the projector, projecting and deploying new ConfigMaps containing this updated data, without needing the application to be redeployed. Projection datasources can handle both structured and unstructured configuration files (YAML, JSON, and raw text/binary).An example projection manifest is below, describing how a fictitious notification application could request some configuration data that may dynamically change (memcached hosts, log level, launch flags, etc):--- name: notifications-us-east-1-production namespace: notification-production data: # extract some fields from JSON - source: generated/us-east-1/production/config.json output_file: config.json field_extraction: - memcached_hosts: $.memcached.notifications.production.hosts - settings: $.applications.notification.production.settings - datacenter: $.datacenter - environment: $.environment # extract a scalar value from a YAML - source: apps/us-east-1/production/notification.yaml output_file: launch_flags extract: $.launch_flags After processing by the config projector, the following ConfigMap is generated, which can then be posted to a Kubernetes cluster with kubectl create -f <generatedfile>.kind: ConfigMap apiVersion: v1 metadata name: notifications-us-east-1-production namespace: notification-production labels: tumblr.com/config-version: \"1539778254\" tumblr.com/managed-configmap: \"true\" data: config.json: | { \"memcached_hosts\": [\"2.3.4.5:11211\",\"4.5.6.7:11211\",\"6.7.8.9:11211\"], \"settings\": { \"debug\": false, \"buffer\": \"2000\", \"flavor\": \"out of control\", \"log_level\": \"INFO\", }, \"datacenter\": \"us-east-1\", \"environment\": \"production\" } launch_flags: \"-Xmx5g -Dsun.net.inetaddr.ttl=10\" With this tool, we have enabled our applications running in kubernetes to receive dynamic configuration updates without requiring container rebuilds or deployments. More examples can be found here.k8s-secret-projectorSimilar to our configuration repository, we store secure credentials in access controlled vaults, divided by production levels. We wanted to enable developers to request access to subsets of credentials for a given application without needing to grant the user access to the secrets themselves. Additionally, we wanted to make certificate and password rotation transparent to all applications, enabling us to rotate credentials in an application-agnostic manner, without needing to redeploy applications. Lastly, we wanted to introduce a mechanism where application developers would explicitly describe which credentials their services need, and enable a framework to audit and grant permissions for a service to consume a secret.The k8s-secret-projector operates similarly to the k8s-config-projector, albeit with a few differences. The secret projector combines a repository of projection manifests with a set of credential repositories. A Continuous Integration (CI) tool like Jenkins will run the k8s-secret-projector against any changes in the projection manifests repository to generate new Kubernetes Secret YAML files. Then, Continuous Deployment can deploy the generated and validated Secret files to any number of Kubernetes clusters.Take this file in the production credentials repository, named aws/credentials.json:{ \"us-east-1\": { \"region\": \"us-east-1\", \"aws\": { \"key\": \"somethignSekri7T!\", }, \"s3\": { \"key\": \"passW0rD!\", }, \"redshift\": { \"key\": \"ello0liv3r!\", \"database\": \"mydatabase\" } }, \"us-west-2\": { \"region\": \"us-west-2\", \"aws\": { \"key\": \"anotherPasswr09d!\", }, \"s3\": { \"key\": \"sueprSekur#\", } } } We need to create an amazon.yaml configuration file containing the s3.key and aws.key for us-east-1, as well as a text file containing our region. The projection manifest below will extract only the fields we need, and output them in the format desired.name: aws-credentials namespace: myteam repo: production data: # create an amazon.yaml config with the secrets we care about - name: amazon.yaml source: format: yaml json: aws/credentials.json jsonpaths: s3: $.us-east-1.s3.key aws: $.us-east-1.aws.key region: $.us-east-1.region # create a item containing just the name of the region we are in - name: region source: json: aws/credentials.json jsonpath: $.us-east-1.region Projecting this manifest with the above credentials results in the following Kubernetes Secret YAML file:apiVersion: v1 kind: Secret metadata: labels: tumblr.com/managed-secret: \"true\" tumblr.com/secret-version: master-741-7459d1abcc120 name: aws-credentials namespace: myteam data: region: dXMtZWFzdC0x # region decoded for clarity: us-east-1 amazon.yaml: LS0tCnMzOiAicGFzc1cwckQhIgphd3M6ICJzb21ldGhpZ25TZWtyaTdUISIKcmVnaW9uOiB1cy1lYXN0LTEK # amazon.yaml decoded for clarity: # --- # s3: \"passW0rD!\" # aws: \"somethignSekri7T!\" # region: us-east-1 In addition to being able to extract fields from structured YAML and JSON sources, we gave it the ability to encrypt generated Secrets before they touch disk. This allows Secrets to be deployed in shared Kubernetes environments, where users are colocated with other users, and do not feel comfortable with their Secret resources being unencrypted in etcd. Please note, this requires decryption by your applications before use. More details on how the encryption modules work can be found here.For more examples of how to use this, check out examples here!Whats NextWe are excited to share these tools with the Kubernetes open source community, and we hope they can help your organization adopt container-native thinking when managing application lifecycle like they helped Tumblr. Feature enhancements and bug fixes are welcome! And, shameless plug: if you are interested in Kubernetes, containerization technology, open source, and scaling a massive website with industry leading technologies and practices? Come join us!.- @pipefail "
        ],
        "story_type": "Normal",
        "url_raw": "https://engineering.tumblr.com/post/182013497734/open-sourcing-our-kubernetes-tools",
        "comments.comment_id": [18907172, 18907340],
        "comments.comment_author": ["throwaway_129", "p1necone"],
        "comments.comment_descendants": [1, 2],
        "comments.comment_time": [
          "2019-01-14T22:32:25Z",
          "2019-01-14T22:53:24Z"
        ],
        "comments.comment_text": [
          "Not a cryptographer, but a few things irked me about the secret projector encryption features:<p>1. Calls it AES-CBC, but it's using AES-GCM <a href=\"https://github.com/tumblr/k8s-secret-projector/blob/master/pkg/encryption/cbc/block.go#L88\" rel=\"nofollow\">https://github.com/tumblr/k8s-secret-projector/blob/master/p...</a><p>2. Encryption key generation (edited: from a password) uses a hash function rather than using a KDF. Short hashes are padded (takes first N bytes required to hit max length, appends to end) or trimmed to the appropriate length. <a href=\"https://github.com/tumblr/k8s-secret-projector/blob/master/pkg/encryption/cbc/key.go#L43\" rel=\"nofollow\">https://github.com/tumblr/k8s-secret-projector/blob/master/p...</a>",
          "My tinfoil hat is telling me that the engineers are pushing to open source as much as possible before the company inevitably goes under due to the recent banning of NSFW content."
        ],
        "id": "d35aa9ad-aa3c-43ec-b97f-a3f6c26c8b7c",
        "url_text": "At Tumblr, we are avid fans of Kubernetes. We have been using Kubernetes for all manner of workloads, like critical-path web requests handling for tumblr.com, background task executions like sending queued posts and push notifications, and scheduled jobs for spam detection and content moderation. Throughout our journey to move our 11 year old (almost 12! ) platform to a container-native architecture, we have made innumerable changes to how our applications are designed and run. Inspired by a lot of existing Kubernetes APIs and best practices, were excited to share with the community some of the tools weve developed at Tumblr as our infrastructure has evolved to work with Kubernetes.To help us integrate Kubernetes into our workflows, we have built a handful of tools of which we are open-sourcing three today! Each tool is a small, focused utility, designed to solve specific integration needs Tumblr had while migrating our workflows to Kubernetes. The tools were built to handle our needs internally, but we believe they are useful to the wider Kubernetes community.github.com/tumblr/k8s-sidecar-injectorgithub.com/tumblr/k8s-config-projectorgithub.com/tumblr/k8s-secret-projectork8s-sidecar-injectorAny company that has containerized an application as large and complex as Tumblr knows that it requires a tremendous amount of effort. Applications dont become container-native overnight, and sidecars can be useful to help emulate older deployments with colocated services on physical hosts or VMs. To reduce the amount of fragile copy-paste code by developers adding in sidecars to their Deployments and CronJobs, we created a service to dynamically inject sidecars, volumes, and environment data into pods as they are launched.The k8s-sidecar-injector listens to the Kubernetes API for Pod launches that contain annotations requesting a specific sidecar to be injected. For example, the annotation injector.tumblr.com/request=sidecar-prod-v1 will add any environment variables, volumes, and containers defined in the sidecar-prod-v1 configuration. We use this to add sidecars like logging and metrics daemons, cluster-wide environment variables like DATACENTER and HTTP_PROXY settings, and volumes for shared configuration data. By centralizing configuration of sidecars, we were able to reduce complexity in CronJobs and Deployments by hundreds of lines, eliminated copy-paste errors, and made rolling out updates to shared components in our sidecars effortless.An example sidecar ConfigMap is below, which adds a logging container, a volume from a logger-config ConfigMap, and some environment variables into the Pod.--- apiVersion: v1 kind: ConfigMap metadata: name: example-sidecars namespace: kube-system labels app: k8s-sidecar-injector data: logger-v1: | name: logger-v1 containers: - name: logger image: some/logger:2.2.3 imagePullPolicy: IfNotPresent ports: - containerPort: 8888 volumeMounts: - name: logger-conf mountPath: /etc/logger volumes: - name: logger-conf configMap: name: logger-config env: - name: DATACENTER value: dc01 - name: HTTP_PROXY value: http://my-proxy.org:8080/ - name: HTTPS_PROXY value: http://my-proxy.org:8080/This configuration will add the logger container into each pod with the annotation injector.tumblr.com/request: logger-v1, with a ConfigMap projected as a volume in /etc/logger. Additionally, every container in the Pod will get the DATACENTER=dc01 and HTTP_PROXY environment variables added, if they were not already set. This has allowed us to drastically reduce our boilerplate configuration when containerizing legacy applications that require a complex sidecar configuration.k8s-config-projectorInternally, we have many types of configuration data that is needed by a variety of applications. We store canonical settings data like feature flags, lists of hosts/IPs+ports, and application settings in git. This allows automated generation/manipulation of these settings by bots, cron jobs, Collins, and humans alike. Applications want to know about some subset of this configuration data, and they want to be informed when this data changes as quickly as possible. Kubernetes provides the ConfigMap resource, which enables users to provide their service with configuration data and update the data in running pods without requiring a redeployment. We wanted to use this to configure our services and jobs in a Kubernetes-native manner, but needed a way to bridge the gap between our canonical configuration store (git repo of config files) to ConfigMaps. Thus, was k8s-config-projector born.The Config Projector (github.com/tumblr/k8s-config-projector)[github.com/tumblr/k8s-config-projector] is a command line tool, meant to be run by CI processes. It combines a git repo hosting configuration data (feature flags, lists of hostnames+ports, application settings) with a set of projection manifest files that describe how to group/extract settings from the config repo and transmute them into ConfigMaps. The config projector allows developers to encode a set of configuration data the application needs to run into a projection manifest. As the configuration data changes in the git repository, CI will run the projector, projecting and deploying new ConfigMaps containing this updated data, without needing the application to be redeployed. Projection datasources can handle both structured and unstructured configuration files (YAML, JSON, and raw text/binary).An example projection manifest is below, describing how a fictitious notification application could request some configuration data that may dynamically change (memcached hosts, log level, launch flags, etc):--- name: notifications-us-east-1-production namespace: notification-production data: # extract some fields from JSON - source: generated/us-east-1/production/config.json output_file: config.json field_extraction: - memcached_hosts: $.memcached.notifications.production.hosts - settings: $.applications.notification.production.settings - datacenter: $.datacenter - environment: $.environment # extract a scalar value from a YAML - source: apps/us-east-1/production/notification.yaml output_file: launch_flags extract: $.launch_flags After processing by the config projector, the following ConfigMap is generated, which can then be posted to a Kubernetes cluster with kubectl create -f <generatedfile>.kind: ConfigMap apiVersion: v1 metadata name: notifications-us-east-1-production namespace: notification-production labels: tumblr.com/config-version: \"1539778254\" tumblr.com/managed-configmap: \"true\" data: config.json: | { \"memcached_hosts\": [\"2.3.4.5:11211\",\"4.5.6.7:11211\",\"6.7.8.9:11211\"], \"settings\": { \"debug\": false, \"buffer\": \"2000\", \"flavor\": \"out of control\", \"log_level\": \"INFO\", }, \"datacenter\": \"us-east-1\", \"environment\": \"production\" } launch_flags: \"-Xmx5g -Dsun.net.inetaddr.ttl=10\" With this tool, we have enabled our applications running in kubernetes to receive dynamic configuration updates without requiring container rebuilds or deployments. More examples can be found here.k8s-secret-projectorSimilar to our configuration repository, we store secure credentials in access controlled vaults, divided by production levels. We wanted to enable developers to request access to subsets of credentials for a given application without needing to grant the user access to the secrets themselves. Additionally, we wanted to make certificate and password rotation transparent to all applications, enabling us to rotate credentials in an application-agnostic manner, without needing to redeploy applications. Lastly, we wanted to introduce a mechanism where application developers would explicitly describe which credentials their services need, and enable a framework to audit and grant permissions for a service to consume a secret.The k8s-secret-projector operates similarly to the k8s-config-projector, albeit with a few differences. The secret projector combines a repository of projection manifests with a set of credential repositories. A Continuous Integration (CI) tool like Jenkins will run the k8s-secret-projector against any changes in the projection manifests repository to generate new Kubernetes Secret YAML files. Then, Continuous Deployment can deploy the generated and validated Secret files to any number of Kubernetes clusters.Take this file in the production credentials repository, named aws/credentials.json:{ \"us-east-1\": { \"region\": \"us-east-1\", \"aws\": { \"key\": \"somethignSekri7T!\", }, \"s3\": { \"key\": \"passW0rD!\", }, \"redshift\": { \"key\": \"ello0liv3r!\", \"database\": \"mydatabase\" } }, \"us-west-2\": { \"region\": \"us-west-2\", \"aws\": { \"key\": \"anotherPasswr09d!\", }, \"s3\": { \"key\": \"sueprSekur#\", } } } We need to create an amazon.yaml configuration file containing the s3.key and aws.key for us-east-1, as well as a text file containing our region. The projection manifest below will extract only the fields we need, and output them in the format desired.name: aws-credentials namespace: myteam repo: production data: # create an amazon.yaml config with the secrets we care about - name: amazon.yaml source: format: yaml json: aws/credentials.json jsonpaths: s3: $.us-east-1.s3.key aws: $.us-east-1.aws.key region: $.us-east-1.region # create a item containing just the name of the region we are in - name: region source: json: aws/credentials.json jsonpath: $.us-east-1.region Projecting this manifest with the above credentials results in the following Kubernetes Secret YAML file:apiVersion: v1 kind: Secret metadata: labels: tumblr.com/managed-secret: \"true\" tumblr.com/secret-version: master-741-7459d1abcc120 name: aws-credentials namespace: myteam data: region: dXMtZWFzdC0x # region decoded for clarity: us-east-1 amazon.yaml: LS0tCnMzOiAicGFzc1cwckQhIgphd3M6ICJzb21ldGhpZ25TZWtyaTdUISIKcmVnaW9uOiB1cy1lYXN0LTEK # amazon.yaml decoded for clarity: # --- # s3: \"passW0rD!\" # aws: \"somethignSekri7T!\" # region: us-east-1 In addition to being able to extract fields from structured YAML and JSON sources, we gave it the ability to encrypt generated Secrets before they touch disk. This allows Secrets to be deployed in shared Kubernetes environments, where users are colocated with other users, and do not feel comfortable with their Secret resources being unencrypted in etcd. Please note, this requires decryption by your applications before use. More details on how the encryption modules work can be found here.For more examples of how to use this, check out examples here!Whats NextWe are excited to share these tools with the Kubernetes open source community, and we hope they can help your organization adopt container-native thinking when managing application lifecycle like they helped Tumblr. Feature enhancements and bug fixes are welcome! And, shameless plug: if you are interested in Kubernetes, containerization technology, open source, and scaling a massive website with industry leading technologies and practices? Come join us!.- @pipefail ",
        "_version_": 1718536437540847616
      },
      {
        "story_id": 19271393,
        "story_author": "ingve",
        "story_descendants": 85,
        "story_score": 189,
        "story_time": "2019-02-28T14:12:22Z",
        "story_title": "Gomacro: Interactive Go interpreter and debugger with generics and macros",
        "search": [
          "Gomacro: Interactive Go interpreter and debugger with generics and macros",
          "https://github.com/cosmos72/gomacro#gomacro---interactive-go-interpreter-and-debugger-with-generics-and-macros",
          "gomacro is an almost complete Go interpreter, implemented in pure Go. It offers both an interactive REPL and a scripting mode, and does not require a Go toolchain at runtime (except in one very specific case: import of a 3rd party package at runtime). It has two dependencies beyond the Go standard library: github.com/peterh/liner and golang.org/x/tools/go/packages Gomacro can be used as: a standalone executable with interactive Go REPL, line editing and code completion: just run gomacro from your command line, then type Go code. Example: $ gomacro [greeting message...] gomacro> import \"fmt\" gomacro> fmt.Println(\"hello, world!\") hello, world! 14 // int <nil> // error gomacro> press TAB to autocomplete a word, and press it again to cycle on possible completions. Line editing follows mostly Emacs: Ctrl+A or Home jumps to start of line, Ctrl+E or End jumps to end of line, Ald+D deletes word starting at cursor... For the full list of key bindings, see https://github.com/peterh/liner a tool to experiment with Go generics: see Generics a Go source code debugger: see Debugger an interactive tool to make science more productive and more fun. If you use compiled Go with scientific libraries (physics, bioinformatics, statistics...) you can import the same libraries from gomacro REPL (immediate on Linux and Mac OS X, requires restarting on other platforms, see Importing packages below), call them interactively, inspect the results, feed them to other functions/libraries, all in a single session. The imported libraries will be compiled, not interpreted, so they will be as fast as in compiled Go. For a graphical user interface on top of gomacro, see Gophernotes. It is a Go kernel for Jupyter notebooks and nteract, and uses gomacro for Go code evaluation. a library that adds Eval() and scripting capabilities to your Go programs in few lines of code: package main import ( \"fmt\" \"reflect\" \"github.com/cosmos72/gomacro/fast\" ) func RunGomacro(toeval string) reflect.Value { interp := fast.New() vals, _ := interp.Eval(toeval) // for simplicity, only use the first returned value return vals[0].ReflectValue() } func main() { fmt.Println(RunGomacro(\"1+1\")) } Also, github issue #13 explains how to have your application's functions, variable, constants and types available in the interpreter. Note: gomacro license is MPL 2.0, which imposes some restrictions on programs that use gomacro. See MPL 2.0 FAQ for common questions regarding the license terms and conditions. a way to execute Go source code on-the-fly without a Go compiler: you can either run gomacro FILENAME.go (works on every supported platform) or you can insert a line #!/usr/bin/env gomacro at the beginning of a Go source file, then mark the file as executable with chmod +x FILENAME.go and finally execute it with ./FILENAME.go (works only on Unix-like systems: Linux, *BSD, Mac OS X ...) a Go code generation tool: gomacro was started as an experiment to add Lisp-like macros to Go, and they are extremely useful (in the author's opinion) to simplify code generation. Macros are normal Go functions, they are special only in one aspect: they are executed before compiling code, and their input and output is code (abstract syntax trees, in the form of go/ast.Node) Don't confuse them with C preprocessor macros: in Lisp, Scheme and now in Go, macros are regular functions written in the same programming language as the rest of the source code. They can perform arbitrary computations and call any other function or library: they can even read and write files, open network connections, etc... as a normal Go function can do. See doc/code_generation.pdf for an introduction to the topic. Installation Prerequites Go 1.13+ Supported platforms Gomacro is pure Go, and in theory it should work on any platform supported by the Go compiler. The following combinations are tested and known to work: Linux: amd64, 386, arm64, arm, mips, ppc64le Mac OS X: amd64, 386 (386 binaries running on amd64 system) Windows: amd64, 386 FreeBSD: amd64, 386 Android: arm64, arm (tested with Termux and the Go compiler distributed with it) How to install The command go get -u github.com/cosmos72/gomacro downloads, compiles and installs gomacro and its dependencies Current Status Almost complete. The main limitations and missing features are: importing 3rd party libraries at runtime currently only works on Linux and Mac OS X. On other systems as Windows, Android and *BSD it is cumbersome and requires recompiling - see Importing packages. conversions from/to unsafe.Pointer are not supported some corner cases using interpreted interfaces, as interface -> interface type assertions and type switches, are not implemented yet. some corner cases using recursive types may not work correctly. goto can only jump backward, not forward out-of-order code is under testing - some corner cases, as for example out-of-order declarations used in keys of composite literals, are not supported. Clearly, at REPL code is still executed as soon as possible, so it makes a difference mostly if you separate multiple declarations with ; on a single line. Example: var a = b; var b = 42 Support for \"batch mode\" is in progress - it reads as much source code as possible before executing it, and it's useful mostly to execute whole files or directories. The documentation also contains the full list of features and limitations Extensions Compared to compiled Go, gomacro supports several extensions: generics (experimental) - see Generics an integrated debugger, see Debugger configurable special commands. Type :help at REPL to list them, and see cmd.go:37 for the documentation and API to define new ones. untyped constants can be manipulated directly at REPL. Examples: gomacro> 1<<100 {int 1267650600228229401496703205376} // untyped.Lit gomacro> const c = 1<<100; c * c / 100000000000 {int 16069380442589902755419620923411626025222029937827} // untyped.Lit This provides a handy arbitrary-precision calculator. Note: operations on large untyped integer constants are always exact, while operations on large untyped float constants are implemented with go/constant.Value, and are exact as long as both numerator and denominator are <= 5e1232. Beyond that, go/constant.Value switches from *big.Rat to *big.Float with precision = 512, which can accumulate rounding errors. If you need exact results, convert the untyped float constant to *big.Rat (see next item) before exceeding 5e1232. untyped constants can be converted implicitly to *big.Int, *big.Rat and *big.Float. Examples: import \"math/big\" var i *big.Int = 1<<1000 // exact - would overflow int var r *big.Rat = 1.000000000000000000001 // exact - different from 1.0 var s *big.Rat = 5e1232 // exact - would overflow float64 var t *big.Rat = 1e1234 // approximate, exceeds 5e1232 var f *big.Float = 1e646456992 // largest untyped float constant that is different from +Inf Note: every time such a conversion is evaluated, it creates a new value - no risk to modify the constant. Be aware that converting a huge value to string, as typing f at REPL would do, can be very slow. zero value constructors: for any type T, the expression T() returns the zero value of the type macros, quoting and quasiquoting: see doc/code_generation.pdf and slightly relaxed checks: unused variables and unused return values never cause errors Examples Some short, notable examples - to run them on non-Linux platforms, see Importing packages first. plot mathematical functions install libraries: go get gonum.org/v1/plot gonum.org/v1/plot/plotter gonum.org/v1/plot/vg start the interpreter: gomacro at interpreter prompt, paste the whole Go code listed at https://github.com/gonum/plot/wiki/Example-plots#functions (the source code starts after the picture under the section \"Functions\", and ends just before the section \"Histograms\") still at interpreter prompt, enter main() If all goes well, it will create a file named \"functions.png\" in current directory containing the plotted functions. simple mandelbrot web server install libraries: go get github.com/sverrirab/mandelbrot-go chdir to mandelbrot-go source folder: cd; cd go/src/github.com/sverrirab/mandelbrot-go start interpreter with arguments: gomacro -i mbrot.go at interpreter prompt, enter init(); main() visit http://localhost:8090/ Be patient, rendering and zooming mandelbrot set with an interpreter is a little slow. Further examples are listed by Gophernotes Importing packages Gomacro supports the standard Go syntax import, including package renaming. Examples: import \"fmt\" import ( \"io\" \"net/http\" r \"reflect\" ) Third party packages - i.e. packages not in Go standard library - can also be imported with the same syntax, as long as the package is already installed. To install a package, follow its installation procedure: quite often it is the command go get PACKAGE-PATH The next steps depend on the system you are running gomacro on: Linux and Mac OS X If you are running gomacro on Linux or Mac OS X, import will then just work: it will automatically download, compile and import a package. Example: $ gomacro [greeting message...] gomacro> import \"gonum.org/v1/plot\" // debug: looking for package \"gonum.org/v1/plot\" ... // debug: compiling \"/home/max/go/src/gomacro.imports/gonum.org/v1/plot/plot.go\" ... go: finding module for package gonum.org/v1/plot/vg/draw go: finding module for package gonum.org/v1/plot go: found gonum.org/v1/plot in gonum.org/v1/plot v0.0.0-20200226011204-b25252b0d522 gomacro> plot.New() &{...} // *plot.Plot <nil> // error Note: internally, gomacro will compile and load a Go plugin containing the package's exported declarations. Go plugins are currently supported only on Linux and Mac OS X. WARNING On Mac OS X, never execute strip gomacro: it breaks plugin support, and loading third party packages stops working. Other systems On all other systems as Windows, Android and *BSD you can still use import, but there are more steps: you need to manually download the package, and you also need to recompile gomacro after the import (it will tell you). Example: $ go get gonum.org/v1/plot $ gomacro [greeting message...] gomacro> import \"gonum.org/v1/plot\" // warning: created file \"/home/max/go/src/github.com/cosmos72/gomacro/imports/thirdparty/gonum_org_v1_plot.go\", recompile gomacro to use it Now quit gomacro, recompile and reinstall it: gomacro> :quit $ go install github.com/cosmos72/gomacro Finally restart it. Your import is now linked inside gomacro and will work: $ gomacro [greeting message...] gomacro> import \"gonum.org/v1/plot\" gomacro> plot.New() &{...} // *plot.Plot <nil> // error Note: if you need several packages, you can first import all of them, then quit and recompile gomacro only once. Generics gomacro contains two alternative, experimental versions of Go generics: the first version is modeled after C++ templates, and is appropriately named \"C++ style\" See doc/generics-c++.md for how to enable and use them. the second version is named \"contracts are interfaces\" - or more briefly \"CTI\". It is modeled after several published proposals for Go generics, most notably Ian Lance Taylor's Type Parameters in Go It has some additions inspired from Haskell generics and original contributions from the author - in particular to create a simpler alternative to Go 2 contracts For their design document and reasoning behind some of the design choices, see doc/generics-cti.md The second version of generics \"CTI\" is enabled by default in gomacro. They are in beta status, and at the moment only generic types and functions are supported. Syntax and examples: // declare a generic type with two type arguments T and U type Pair#[T,U] struct { First T Second U } // instantiate the generic type using explicit types for T and U, // and create a variable of such type. var pair Pair#[complex64, struct{}] // equivalent: pair := Pair#[complex64, struct{}] {} // a more complex example, showing higher-order functions func Transform#[T,U](slice []T, trans func(T) U) []U { ret := make([]U, len(slice)) for i := range slice { ret[i] = trans(slice[i]) } return ret } Transform#[string,int] // returns func([]string, func(string) int) []int // returns []int{3, 2, 1} i.e. the len() of each string in input slice: Transform#[string,int]([]string{\"abc\",\"xy\",\"z\"}, func(s string) int { return len(s) }) Contracts specify the available methods of a generic type. For simplicity, they do not introduce a new syntax or new language concepts: contracts are just (generic) interfaces. With a tiny addition, actually: the ability to optionally indicate the receiver type. For example, the contract specifying that values of type T can be compared with each other to determine if the first is less, equal or greater than the second is: type Comparable#[T] interface { // returns -1 if a is less than b // returns 0 if a is equal to b // returns 1 if a is greater than b func (a T) Cmp(b T) int } A type T implements Comparable#[T] if it has a method func (T) Cmp(T) int. This interface is carefully chosen to match the existing methods of *math/big.Float, *math/big.Int and *math/big.Rat. In other words, *math/big.Float, *math/big.Int and *math/big.Rat already implement it. What about basic types as int8, int16, int32, uint... float*, complex* ... ? Gomacro extends them, automatically adding many methods equivalent to the ones declared on *math/big.Int to perform arithmetic and comparison, including Cmp which is internally defined as (no need to define it yourself): func (a int) Cmp(b int) int { if a < b { return -1 } else if a > b { return 1 } else { return 0 } } Thus the generic functions Min and Max can be written as func Min#[T: Comparable] (a, b T) T { if a.Cmp(b) < 0 { // also <= would work return a } return b } func Max#[T: Comparable] (a, b T) T { if a.Cmp(b) > 0 { // also >= would work return a } return b } Where the syntax #[T: Comparable] or equivalently #[T: Comparable#[T]] indicates that T must satisfy the contract (implement the interface) Comparable#[T] Such functions Min and Max will then work automatically for every type T that satisfies the contract (implements the interface) Comparable#[T]: all basic integers and floats, plus *math/big.Float, *math/big.Int and *math/big.Rat, plus every user-defined type T that has a method func (T) Cmp(T) int If you do not specify the contract(s) that a type must satisfy, generic functions cannot access the fields and methods of a such type, which is then treated as a \"black box\", similarly to interface{}. Two values of type T can be added if T has an appropriate method. But which name and signature should we choose to add values? Copying again from math/big, the method we choose is func (T) Add(T,T) T If receiver is a pointer, it will be set to the result - in any case, the result will also be returned. Similarly to Comparable, the contract Addable is then type Addable#[T] interface { // Add two values a, b and return the result. // If recv is a pointer, it must be non-nil // and it will be set to the result func (recv T) Add(a, b T) T } With such a contract, a generic function Sum is quite straightforward: func Sum#[T: Addable] (args ...T) T { // to create the zero value of T, // one can write 'var sum T' or equivalently 'sum := T()' // Unluckily, that's not enough for math/big numbers, which require // the receiver of method calls to be created with a function `New()` // Once math/big numbers have such method, the following // will be fully general - currently it works only on basic types. sum := T().New() for _, elem := range args { // use the method T.Add(T, T) // // as an optimization, relevant at least for math/big numbers, // also use sum as the receiver where result of Add will be stored // if the method Add has pointer receiver. // // To cover the case where method Add has instead value receiver, // also assign the returned value to sum sum = sum.Add(sum, elem) } return sum } Sum#[int] // returns func(...int) int Sum#[int] (1,2,3) // returns int(6) Sum#[complex64] // returns func(...complex64) complex64 Sum#[complex64] (1.1+2.2i, 3.3) // returns complex64(4.4+2.2i) Sum#[string] // returns func(...string) string Sum#[string](\"abc.\",\"def.\",\"xy\",\"z\") // returns \"abc.def.xyz\" Partial and full specialization of generics is not supported in CTI generics, both for simplicity and to avoid accidentally providing Turing completeness at compile-time. Instantiation of generic types and functions is on-demand. Current limitations: type inference on generic arguments #[...] is not yet implemented, thus generic arguments #[...] must be explicit. generic methods are not yet implemented. types are not checked to actually satisfy contracts. Debugger Since version 2.6, gomacro also has an integrated debugger. There are three ways to enter it: hit CTRL+C while interpreted code is running. type :debug STATEMENT-OR-FUNCTION-CALL at the prompt. add a statement (an expression is not enough) \"break\" or _ = \"break\" to your code, then execute it normally. In all cases, execution will be suspended and you will get a debug> prompt, which accepts the following commands: step, next, finish, continue, env [NAME], inspect EXPR, list, print EXPR-OR-STATEMENT Also, commands can be abbreviated. print fully supports expressions or statements with side effects, including function calls and modifying local variables. env without arguments prints all global and local variables. an empty command (i.e. just pressing enter) repeats the last command. Only interpreted statements can be debugged: expressions and compiled code will be executed, but you cannot step into them. The debugger is quite new, and may have some minor glitches. Why it was created First of all, to experiment with Go :) Second, to simplify Go code generation tools (keep reading for the gory details) Problem: \"go generate\" and many other Go tools automatically create Go source code from some kind of description - usually an interface specifications as WSDL, XSD, JSON... Such specification may be written in Go, for example when creating JSON marshallers/unmarshallers from Go structs, or in some other language, for example when creating Go structs from JSON sample data. In both cases, a variety of external programs are needed to generate Go source code: such programs need to be installed separately from the code being generated and compiled. Also, Go is currently lacking generics (read: C++-like templates) because of the rationale \"we do not yet know how to do them right, and once you do them wrong everybody is stuck with them\" The purpose of Lisp-like macros is to execute arbitrary code while compiling, in particular to generate source code. This makes them very well suited (although arguably a bit low level) for both purposes: code generation and C++-like templates, which are a special case of code generation - for a demonstration of how to implement C++-like templates on top of Lisp-like macros, see for example the project https://github.com/cosmos72/cl-parametric-types from the same author. Building a Go interpreter that supports Lisp-like macros, allows to embed all these code-generation activities into regular Go source code, without the need for external programs (except for the interpreter itself). As a free bonus, we get support for Eval() LEGAL Gomacro is distributed under the terms of Mozilla Public License 2.0 or any later version. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/cosmos72/gomacro#gomacro---interactive-go-interpreter-and-debugger-with-generics-and-macros",
        "comments.comment_id": [19272091, 19276246],
        "comments.comment_author": ["kouteiheika", "apatheticonion"],
        "comments.comment_descendants": [6, 1],
        "comments.comment_time": [
          "2019-02-28T15:43:28Z",
          "2019-02-28T23:04:08Z"
        ],
        "comments.comment_text": [
          "> Also, Go is currently lacking generics (read: C++-like templates) because of the rationale \"we do not yet know how to do them right, and once you do them wrong everybody is stuck with them\"<p>To be honest C++-like templates are probably <i>the</i> worst way to do generics. C++ \"did them wrong\" by picking C++-like templates and now everybody in the C++ community is paying the price being stuck with horrible unreadable error messages, abysmal compile times, an explosion of complexity (SFINAE, CRTP, metaprogramming using templates, template specialization, etc.) and a myriad of other problems.<p>I sincerely hope Go doesn't adopt C++-like templates. Fortunately C++-like templates are not the only game in town, and there are other, a lot more saner and principled flavours of parametric polymorphism out there.",
          "C++ style templates...?\nWhat's wrong with ripping TypeScript's generic implementation?\nIt's simple, clean and direct<p><pre><code>    function firstOf<T>(anArray: T[]): T {\n         return anArray[0]\n    }\n\n    interface thing<T, Y> {\n        key: T\n        anotherOne: Y\n    }\n\n    const things: thing<string, string> = [\n        { key: 'hello', anotherOne: 'world'},\n        { key: 'value', anotherOne: 'thing'}\n    ]\n\n    const result = firstOf<thing>(things)</code></pre>"
        ],
        "id": "32b25bc5-127a-4aee-8a16-5317dfca1cb3",
        "url_text": "gomacro is an almost complete Go interpreter, implemented in pure Go. It offers both an interactive REPL and a scripting mode, and does not require a Go toolchain at runtime (except in one very specific case: import of a 3rd party package at runtime). It has two dependencies beyond the Go standard library: github.com/peterh/liner and golang.org/x/tools/go/packages Gomacro can be used as: a standalone executable with interactive Go REPL, line editing and code completion: just run gomacro from your command line, then type Go code. Example: $ gomacro [greeting message...] gomacro> import \"fmt\" gomacro> fmt.Println(\"hello, world!\") hello, world! 14 // int <nil> // error gomacro> press TAB to autocomplete a word, and press it again to cycle on possible completions. Line editing follows mostly Emacs: Ctrl+A or Home jumps to start of line, Ctrl+E or End jumps to end of line, Ald+D deletes word starting at cursor... For the full list of key bindings, see https://github.com/peterh/liner a tool to experiment with Go generics: see Generics a Go source code debugger: see Debugger an interactive tool to make science more productive and more fun. If you use compiled Go with scientific libraries (physics, bioinformatics, statistics...) you can import the same libraries from gomacro REPL (immediate on Linux and Mac OS X, requires restarting on other platforms, see Importing packages below), call them interactively, inspect the results, feed them to other functions/libraries, all in a single session. The imported libraries will be compiled, not interpreted, so they will be as fast as in compiled Go. For a graphical user interface on top of gomacro, see Gophernotes. It is a Go kernel for Jupyter notebooks and nteract, and uses gomacro for Go code evaluation. a library that adds Eval() and scripting capabilities to your Go programs in few lines of code: package main import ( \"fmt\" \"reflect\" \"github.com/cosmos72/gomacro/fast\" ) func RunGomacro(toeval string) reflect.Value { interp := fast.New() vals, _ := interp.Eval(toeval) // for simplicity, only use the first returned value return vals[0].ReflectValue() } func main() { fmt.Println(RunGomacro(\"1+1\")) } Also, github issue #13 explains how to have your application's functions, variable, constants and types available in the interpreter. Note: gomacro license is MPL 2.0, which imposes some restrictions on programs that use gomacro. See MPL 2.0 FAQ for common questions regarding the license terms and conditions. a way to execute Go source code on-the-fly without a Go compiler: you can either run gomacro FILENAME.go (works on every supported platform) or you can insert a line #!/usr/bin/env gomacro at the beginning of a Go source file, then mark the file as executable with chmod +x FILENAME.go and finally execute it with ./FILENAME.go (works only on Unix-like systems: Linux, *BSD, Mac OS X ...) a Go code generation tool: gomacro was started as an experiment to add Lisp-like macros to Go, and they are extremely useful (in the author's opinion) to simplify code generation. Macros are normal Go functions, they are special only in one aspect: they are executed before compiling code, and their input and output is code (abstract syntax trees, in the form of go/ast.Node) Don't confuse them with C preprocessor macros: in Lisp, Scheme and now in Go, macros are regular functions written in the same programming language as the rest of the source code. They can perform arbitrary computations and call any other function or library: they can even read and write files, open network connections, etc... as a normal Go function can do. See doc/code_generation.pdf for an introduction to the topic. Installation Prerequites Go 1.13+ Supported platforms Gomacro is pure Go, and in theory it should work on any platform supported by the Go compiler. The following combinations are tested and known to work: Linux: amd64, 386, arm64, arm, mips, ppc64le Mac OS X: amd64, 386 (386 binaries running on amd64 system) Windows: amd64, 386 FreeBSD: amd64, 386 Android: arm64, arm (tested with Termux and the Go compiler distributed with it) How to install The command go get -u github.com/cosmos72/gomacro downloads, compiles and installs gomacro and its dependencies Current Status Almost complete. The main limitations and missing features are: importing 3rd party libraries at runtime currently only works on Linux and Mac OS X. On other systems as Windows, Android and *BSD it is cumbersome and requires recompiling - see Importing packages. conversions from/to unsafe.Pointer are not supported some corner cases using interpreted interfaces, as interface -> interface type assertions and type switches, are not implemented yet. some corner cases using recursive types may not work correctly. goto can only jump backward, not forward out-of-order code is under testing - some corner cases, as for example out-of-order declarations used in keys of composite literals, are not supported. Clearly, at REPL code is still executed as soon as possible, so it makes a difference mostly if you separate multiple declarations with ; on a single line. Example: var a = b; var b = 42 Support for \"batch mode\" is in progress - it reads as much source code as possible before executing it, and it's useful mostly to execute whole files or directories. The documentation also contains the full list of features and limitations Extensions Compared to compiled Go, gomacro supports several extensions: generics (experimental) - see Generics an integrated debugger, see Debugger configurable special commands. Type :help at REPL to list them, and see cmd.go:37 for the documentation and API to define new ones. untyped constants can be manipulated directly at REPL. Examples: gomacro> 1<<100 {int 1267650600228229401496703205376} // untyped.Lit gomacro> const c = 1<<100; c * c / 100000000000 {int 16069380442589902755419620923411626025222029937827} // untyped.Lit This provides a handy arbitrary-precision calculator. Note: operations on large untyped integer constants are always exact, while operations on large untyped float constants are implemented with go/constant.Value, and are exact as long as both numerator and denominator are <= 5e1232. Beyond that, go/constant.Value switches from *big.Rat to *big.Float with precision = 512, which can accumulate rounding errors. If you need exact results, convert the untyped float constant to *big.Rat (see next item) before exceeding 5e1232. untyped constants can be converted implicitly to *big.Int, *big.Rat and *big.Float. Examples: import \"math/big\" var i *big.Int = 1<<1000 // exact - would overflow int var r *big.Rat = 1.000000000000000000001 // exact - different from 1.0 var s *big.Rat = 5e1232 // exact - would overflow float64 var t *big.Rat = 1e1234 // approximate, exceeds 5e1232 var f *big.Float = 1e646456992 // largest untyped float constant that is different from +Inf Note: every time such a conversion is evaluated, it creates a new value - no risk to modify the constant. Be aware that converting a huge value to string, as typing f at REPL would do, can be very slow. zero value constructors: for any type T, the expression T() returns the zero value of the type macros, quoting and quasiquoting: see doc/code_generation.pdf and slightly relaxed checks: unused variables and unused return values never cause errors Examples Some short, notable examples - to run them on non-Linux platforms, see Importing packages first. plot mathematical functions install libraries: go get gonum.org/v1/plot gonum.org/v1/plot/plotter gonum.org/v1/plot/vg start the interpreter: gomacro at interpreter prompt, paste the whole Go code listed at https://github.com/gonum/plot/wiki/Example-plots#functions (the source code starts after the picture under the section \"Functions\", and ends just before the section \"Histograms\") still at interpreter prompt, enter main() If all goes well, it will create a file named \"functions.png\" in current directory containing the plotted functions. simple mandelbrot web server install libraries: go get github.com/sverrirab/mandelbrot-go chdir to mandelbrot-go source folder: cd; cd go/src/github.com/sverrirab/mandelbrot-go start interpreter with arguments: gomacro -i mbrot.go at interpreter prompt, enter init(); main() visit http://localhost:8090/ Be patient, rendering and zooming mandelbrot set with an interpreter is a little slow. Further examples are listed by Gophernotes Importing packages Gomacro supports the standard Go syntax import, including package renaming. Examples: import \"fmt\" import ( \"io\" \"net/http\" r \"reflect\" ) Third party packages - i.e. packages not in Go standard library - can also be imported with the same syntax, as long as the package is already installed. To install a package, follow its installation procedure: quite often it is the command go get PACKAGE-PATH The next steps depend on the system you are running gomacro on: Linux and Mac OS X If you are running gomacro on Linux or Mac OS X, import will then just work: it will automatically download, compile and import a package. Example: $ gomacro [greeting message...] gomacro> import \"gonum.org/v1/plot\" // debug: looking for package \"gonum.org/v1/plot\" ... // debug: compiling \"/home/max/go/src/gomacro.imports/gonum.org/v1/plot/plot.go\" ... go: finding module for package gonum.org/v1/plot/vg/draw go: finding module for package gonum.org/v1/plot go: found gonum.org/v1/plot in gonum.org/v1/plot v0.0.0-20200226011204-b25252b0d522 gomacro> plot.New() &{...} // *plot.Plot <nil> // error Note: internally, gomacro will compile and load a Go plugin containing the package's exported declarations. Go plugins are currently supported only on Linux and Mac OS X. WARNING On Mac OS X, never execute strip gomacro: it breaks plugin support, and loading third party packages stops working. Other systems On all other systems as Windows, Android and *BSD you can still use import, but there are more steps: you need to manually download the package, and you also need to recompile gomacro after the import (it will tell you). Example: $ go get gonum.org/v1/plot $ gomacro [greeting message...] gomacro> import \"gonum.org/v1/plot\" // warning: created file \"/home/max/go/src/github.com/cosmos72/gomacro/imports/thirdparty/gonum_org_v1_plot.go\", recompile gomacro to use it Now quit gomacro, recompile and reinstall it: gomacro> :quit $ go install github.com/cosmos72/gomacro Finally restart it. Your import is now linked inside gomacro and will work: $ gomacro [greeting message...] gomacro> import \"gonum.org/v1/plot\" gomacro> plot.New() &{...} // *plot.Plot <nil> // error Note: if you need several packages, you can first import all of them, then quit and recompile gomacro only once. Generics gomacro contains two alternative, experimental versions of Go generics: the first version is modeled after C++ templates, and is appropriately named \"C++ style\" See doc/generics-c++.md for how to enable and use them. the second version is named \"contracts are interfaces\" - or more briefly \"CTI\". It is modeled after several published proposals for Go generics, most notably Ian Lance Taylor's Type Parameters in Go It has some additions inspired from Haskell generics and original contributions from the author - in particular to create a simpler alternative to Go 2 contracts For their design document and reasoning behind some of the design choices, see doc/generics-cti.md The second version of generics \"CTI\" is enabled by default in gomacro. They are in beta status, and at the moment only generic types and functions are supported. Syntax and examples: // declare a generic type with two type arguments T and U type Pair#[T,U] struct { First T Second U } // instantiate the generic type using explicit types for T and U, // and create a variable of such type. var pair Pair#[complex64, struct{}] // equivalent: pair := Pair#[complex64, struct{}] {} // a more complex example, showing higher-order functions func Transform#[T,U](slice []T, trans func(T) U) []U { ret := make([]U, len(slice)) for i := range slice { ret[i] = trans(slice[i]) } return ret } Transform#[string,int] // returns func([]string, func(string) int) []int // returns []int{3, 2, 1} i.e. the len() of each string in input slice: Transform#[string,int]([]string{\"abc\",\"xy\",\"z\"}, func(s string) int { return len(s) }) Contracts specify the available methods of a generic type. For simplicity, they do not introduce a new syntax or new language concepts: contracts are just (generic) interfaces. With a tiny addition, actually: the ability to optionally indicate the receiver type. For example, the contract specifying that values of type T can be compared with each other to determine if the first is less, equal or greater than the second is: type Comparable#[T] interface { // returns -1 if a is less than b // returns 0 if a is equal to b // returns 1 if a is greater than b func (a T) Cmp(b T) int } A type T implements Comparable#[T] if it has a method func (T) Cmp(T) int. This interface is carefully chosen to match the existing methods of *math/big.Float, *math/big.Int and *math/big.Rat. In other words, *math/big.Float, *math/big.Int and *math/big.Rat already implement it. What about basic types as int8, int16, int32, uint... float*, complex* ... ? Gomacro extends them, automatically adding many methods equivalent to the ones declared on *math/big.Int to perform arithmetic and comparison, including Cmp which is internally defined as (no need to define it yourself): func (a int) Cmp(b int) int { if a < b { return -1 } else if a > b { return 1 } else { return 0 } } Thus the generic functions Min and Max can be written as func Min#[T: Comparable] (a, b T) T { if a.Cmp(b) < 0 { // also <= would work return a } return b } func Max#[T: Comparable] (a, b T) T { if a.Cmp(b) > 0 { // also >= would work return a } return b } Where the syntax #[T: Comparable] or equivalently #[T: Comparable#[T]] indicates that T must satisfy the contract (implement the interface) Comparable#[T] Such functions Min and Max will then work automatically for every type T that satisfies the contract (implements the interface) Comparable#[T]: all basic integers and floats, plus *math/big.Float, *math/big.Int and *math/big.Rat, plus every user-defined type T that has a method func (T) Cmp(T) int If you do not specify the contract(s) that a type must satisfy, generic functions cannot access the fields and methods of a such type, which is then treated as a \"black box\", similarly to interface{}. Two values of type T can be added if T has an appropriate method. But which name and signature should we choose to add values? Copying again from math/big, the method we choose is func (T) Add(T,T) T If receiver is a pointer, it will be set to the result - in any case, the result will also be returned. Similarly to Comparable, the contract Addable is then type Addable#[T] interface { // Add two values a, b and return the result. // If recv is a pointer, it must be non-nil // and it will be set to the result func (recv T) Add(a, b T) T } With such a contract, a generic function Sum is quite straightforward: func Sum#[T: Addable] (args ...T) T { // to create the zero value of T, // one can write 'var sum T' or equivalently 'sum := T()' // Unluckily, that's not enough for math/big numbers, which require // the receiver of method calls to be created with a function `New()` // Once math/big numbers have such method, the following // will be fully general - currently it works only on basic types. sum := T().New() for _, elem := range args { // use the method T.Add(T, T) // // as an optimization, relevant at least for math/big numbers, // also use sum as the receiver where result of Add will be stored // if the method Add has pointer receiver. // // To cover the case where method Add has instead value receiver, // also assign the returned value to sum sum = sum.Add(sum, elem) } return sum } Sum#[int] // returns func(...int) int Sum#[int] (1,2,3) // returns int(6) Sum#[complex64] // returns func(...complex64) complex64 Sum#[complex64] (1.1+2.2i, 3.3) // returns complex64(4.4+2.2i) Sum#[string] // returns func(...string) string Sum#[string](\"abc.\",\"def.\",\"xy\",\"z\") // returns \"abc.def.xyz\" Partial and full specialization of generics is not supported in CTI generics, both for simplicity and to avoid accidentally providing Turing completeness at compile-time. Instantiation of generic types and functions is on-demand. Current limitations: type inference on generic arguments #[...] is not yet implemented, thus generic arguments #[...] must be explicit. generic methods are not yet implemented. types are not checked to actually satisfy contracts. Debugger Since version 2.6, gomacro also has an integrated debugger. There are three ways to enter it: hit CTRL+C while interpreted code is running. type :debug STATEMENT-OR-FUNCTION-CALL at the prompt. add a statement (an expression is not enough) \"break\" or _ = \"break\" to your code, then execute it normally. In all cases, execution will be suspended and you will get a debug> prompt, which accepts the following commands: step, next, finish, continue, env [NAME], inspect EXPR, list, print EXPR-OR-STATEMENT Also, commands can be abbreviated. print fully supports expressions or statements with side effects, including function calls and modifying local variables. env without arguments prints all global and local variables. an empty command (i.e. just pressing enter) repeats the last command. Only interpreted statements can be debugged: expressions and compiled code will be executed, but you cannot step into them. The debugger is quite new, and may have some minor glitches. Why it was created First of all, to experiment with Go :) Second, to simplify Go code generation tools (keep reading for the gory details) Problem: \"go generate\" and many other Go tools automatically create Go source code from some kind of description - usually an interface specifications as WSDL, XSD, JSON... Such specification may be written in Go, for example when creating JSON marshallers/unmarshallers from Go structs, or in some other language, for example when creating Go structs from JSON sample data. In both cases, a variety of external programs are needed to generate Go source code: such programs need to be installed separately from the code being generated and compiled. Also, Go is currently lacking generics (read: C++-like templates) because of the rationale \"we do not yet know how to do them right, and once you do them wrong everybody is stuck with them\" The purpose of Lisp-like macros is to execute arbitrary code while compiling, in particular to generate source code. This makes them very well suited (although arguably a bit low level) for both purposes: code generation and C++-like templates, which are a special case of code generation - for a demonstration of how to implement C++-like templates on top of Lisp-like macros, see for example the project https://github.com/cosmos72/cl-parametric-types from the same author. Building a Go interpreter that supports Lisp-like macros, allows to embed all these code-generation activities into regular Go source code, without the need for external programs (except for the interpreter itself). As a free bonus, we get support for Eval() LEGAL Gomacro is distributed under the terms of Mozilla Public License 2.0 or any later version. ",
        "_version_": 1718536453902827520
      }
    ]
  }
}
