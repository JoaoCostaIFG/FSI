{
  "responseHeader": {
    "status": 0,
    "QTime": 44
  },
  "response": {
    "numFound": 222,
    "start": 0,
    "numFoundExact": true,
    "docs": [
      {
        "story_id": 20559240,
        "story_author": "mooreds",
        "story_descendants": 7,
        "story_score": 23,
        "story_time": "2019-07-29T20:45:37Z",
        "story_title": "Learn a little jq, Awk and sed",
        "search": [
          "Learn a little jq, Awk and sed",
          "https://letterstoanewdeveloper.com/2019/07/29/learn-a-little-jq-awk-and-sed/",
          "Dear new developer, You are probably going to be dealing with text files sometime during your development career. These could be plain text, csv, or json. They may have data you want to get out, or log files you want to examine. You may be transforming from one format to another. Now, if this is a regular occurrence, you may want to build a script or a program around this problem (or use a third party service which aggregates everything together). But sometimes these files are one offs. Or you use them once in a blue moon. And it can take a little while to write a script, look at the libraries, and put it all together. Another alternative is to learn some of the unix tools available on the command line. Here are three that I consider table stakes. awk This is a multi purpose line processing utility. I often want to grab lines of a log file and figure out what is going on. Heres a few lines of a log file: 54.147.20.92 - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" \"Slackbot 1.0 (+https://api.slack.com/robots)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" If I want to see only the ip addresses (assuming these are all in a file called logs.txt), Id run something like: $ awk '{print $1}' logs.txt 54.147.20.92 185.24.234.106 185.24.234.106 Theres lots more, but you can see that youd be able to slice and dice delimited data pretty easily. Heres a great article which dives in further. sed This is another line utility. You can use it for all kinds of things, but I primarily use it to do search and replace on a file. Suppose you had the same log file, but you wanted to anonymize the the ip address and the user agent. Perhaps youre going to ship them off for long term storage or something. You can easily remove this with a couple of sed commands. $ sed 's/^[^ ]*//' logs.txt |sed 's/\"[^\"]*\"$//' - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" Yes, it looks like line noise, but this is the power of regular expressions. Theyre in every language (though with slight variations) and worth learning. sed gives you the power of regular expressions at the command line for processing files. I dont have a great sed tutorial Ive found, but googling shows a number. jq If you work on the command line with modern software at all, you have encountered json. Its used for configuration files and data transmission. Sometimes you get an array of json and you just want to pick out certain attributes of it. Tools like sed and awk fail at this, because they are used to newlines separating records, not curly braces and commas. Sure, you could use regular expressions to parse simple json, and there are times when Ive done this. But a far better tool is jq. Im not as savvy with this as with the others, but have used it whenever Im dealing with an API that delivers json (which is most modern ones). I can pull the API down with curl (another great tool) and parse it out with jq. I can put these all in a script and have the exploration be repeatable. I did this a few months ago when I was doing some exploration of an elastic search system. I crafted the queries with curl and then used jq to parse out the results so that I could make some sense of this. Yes, I could have done this with a real programming language, but it would have taken longer. I could also have used a gui tool like postman, but then it would not have been replicable. sed and awk should be on every system you run across; jq is non standard, but easy to install. Its worth spending some time getting to know these tools. So next time you are processing a text file and need to extract just a bit of it, reach for sed and awk. Next time you get a hairy json file and you are peering at it, look at jq. I think youll be happy with the result. Sincerely, Dan Published July 29, 2019October 17, 2020 "
        ],
        "story_type": "Normal",
        "url_raw": "https://letterstoanewdeveloper.com/2019/07/29/learn-a-little-jq-awk-and-sed/",
        "comments.comment_id": [20562095, 20562592],
        "comments.comment_author": ["envolt", "magoon"],
        "comments.comment_descendants": [0, 3],
        "comments.comment_time": [
          "2019-07-30T04:14:52Z",
          "2019-07-30T06:42:37Z"
        ],
        "comments.comment_text": [
          "When I first saw someone using zsh (omz), I was awe-struck.<p>Same thing happens to the person sitting next to me when I pipe an output to jq.",
          "however, I find jq not so friendly piping its output to other programs."
        ],
        "id": "3068ae11-af74-41a3-8ab6-00a85aae5ab8",
        "url_text": "Dear new developer, You are probably going to be dealing with text files sometime during your development career. These could be plain text, csv, or json. They may have data you want to get out, or log files you want to examine. You may be transforming from one format to another. Now, if this is a regular occurrence, you may want to build a script or a program around this problem (or use a third party service which aggregates everything together). But sometimes these files are one offs. Or you use them once in a blue moon. And it can take a little while to write a script, look at the libraries, and put it all together. Another alternative is to learn some of the unix tools available on the command line. Here are three that I consider table stakes. awk This is a multi purpose line processing utility. I often want to grab lines of a log file and figure out what is going on. Heres a few lines of a log file: 54.147.20.92 - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" \"Slackbot 1.0 (+https://api.slack.com/robots)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" If I want to see only the ip addresses (assuming these are all in a file called logs.txt), Id run something like: $ awk '{print $1}' logs.txt 54.147.20.92 185.24.234.106 185.24.234.106 Theres lots more, but you can see that youd be able to slice and dice delimited data pretty easily. Heres a great article which dives in further. sed This is another line utility. You can use it for all kinds of things, but I primarily use it to do search and replace on a file. Suppose you had the same log file, but you wanted to anonymize the the ip address and the user agent. Perhaps youre going to ship them off for long term storage or something. You can easily remove this with a couple of sed commands. $ sed 's/^[^ ]*//' logs.txt |sed 's/\"[^\"]*\"$//' - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" Yes, it looks like line noise, but this is the power of regular expressions. Theyre in every language (though with slight variations) and worth learning. sed gives you the power of regular expressions at the command line for processing files. I dont have a great sed tutorial Ive found, but googling shows a number. jq If you work on the command line with modern software at all, you have encountered json. Its used for configuration files and data transmission. Sometimes you get an array of json and you just want to pick out certain attributes of it. Tools like sed and awk fail at this, because they are used to newlines separating records, not curly braces and commas. Sure, you could use regular expressions to parse simple json, and there are times when Ive done this. But a far better tool is jq. Im not as savvy with this as with the others, but have used it whenever Im dealing with an API that delivers json (which is most modern ones). I can pull the API down with curl (another great tool) and parse it out with jq. I can put these all in a script and have the exploration be repeatable. I did this a few months ago when I was doing some exploration of an elastic search system. I crafted the queries with curl and then used jq to parse out the results so that I could make some sense of this. Yes, I could have done this with a real programming language, but it would have taken longer. I could also have used a gui tool like postman, but then it would not have been replicable. sed and awk should be on every system you run across; jq is non standard, but easy to install. Its worth spending some time getting to know these tools. So next time you are processing a text file and need to extract just a bit of it, reach for sed and awk. Next time you get a hairy json file and you are peering at it, look at jq. I think youll be happy with the result. Sincerely, Dan Published July 29, 2019October 17, 2020 ",
        "_version_": 1718536505738133504
      },
      {
        "story_id": 21363121,
        "story_author": "signa11",
        "story_descendants": 102,
        "story_score": 636,
        "story_time": "2019-10-26T11:52:57Z",
        "story_title": "An Illustrated Guide to Useful Command Line Tools",
        "search": [
          "An Illustrated Guide to Useful Command Line Tools",
          "https://www.wezm.net/technical/2019/10/useful-command-line-tools/",
          "Published on Sat, 26 October 2019 Inspired by a similar post by Ben Boyter this a list of useful command line tools that I use. Its not a list of every tool I use. These are tools that are new or typically not part of a standard POSIX command line environment. This post is a living document and will be updated over time. It should be obvious that I have a strong preference for fast tools without a large runtime dependency like Python or node.js. Most of these tools are portable to *BSD, Linux, macOS. Many also work on Windows. For OSes that ship up to date software many are available via the system package repository. Last updated: 31 Oct 2019 About my CLI environment: I use the zsh shell, Pragmata Pro font, and base16 default dark color scheme. My prompt is generated by promptline. Table of Contents Alacritty Terminal emulator alt Find alternate files bat cat with syntax highlighting bb System monitor chars Unicode character search dot Dot files manager dust Disk usage analyser eva Calculator exa Replacement for ls fd Replacement for find hexyl Hex viewer hyperfine Benchmarking tool jq awk/XPath for JSON mdcat Render Markdown in the terminal pass Password manager Podman Docker alternative Restic Encrypted backup tool ripgrep Fast, intelligent grep shotgun Take screenshots skim Fuzzy finder slop Graphical region selection Syncthing Decentralised file synchronisation tig TUI for git titlecase Convert text to title case Universal Ctags Maintained ctags fork watchexec Run commands in response to file system changes z Jump to directories zola Static site compiler Changelog The changelog for this page Alacritty Alacritty is fast terminal emulator. Whilst not strictly a command line tool, it does host everything I do in the command line. It is the terminal emulator in use in all the screenshots on this page. Homepage alt alt is a tool for finding the alternate to a file. E.g. the header for an implementation or the test for an implementation. I use it paired with Neovim to easily toggle between tests and implementation. $ alt app/models/page.rb spec/models/page_spec.rb Homepage bat bat is an alternative to the common (mis)use of cat to print a file to the terminal. It supports syntax highlighting and git integration. Homepage bb bb is system monitor like top. It shows overall CPU and memory usage as well as detailed information per process. Homepage chars chars shows information about Unicode characters matching a search term. Homepage dot dot is a dotfiles manager. It maintains a set of symlinks according to a mappings file. I use it to manage my dotfiles. Homepage dust dust is an alternative du -sh. It calculates the size of a directory tree, printing a summary of the largest items. Homepage exa exa is a replacement for ls with sensible defaults and added features like a tree view, git integration, and optional icons. I have ls aliased to exa in my shell. Homepage eva eva is a command line calculator similar to bc, with syntax highlighting and persistent history. Homepage fd fd is an alternative to find and has a more user friendly command line interface and respects ignore files, like .gitignore. The combination of its speed and ignore file support make it excellent for searching for files in git repositories. Homepage hexyl hexyl is a hex viewer that uses Unicode characters and colour to make the output more readable. Homepage hyperfine hyperfine command line benchmarking tool. It allows you to benchmark commands with warmup and statistical analysis. Homepage jq jq is kind of like awk for JSON. It lets you transform and extract information from JSON documents. Homepage mdcat mdcat renders Markdown files in the terminal. In supported terminals (not Alacritty) links are clickable (without the url being visible like in a web browser) and images are rendered. Homepage pass pass is a password manager that uses GPG to store the passwords. I use it with the passff Firefox extension and Pass for iOS on my phone. Homepage Podman podman is an alternative to Docker that does not require a daemon. Containers are run as the user running Podman so files written into the host dont end up owned by root. The CLI is largely compatible with the docker CLI. Homepage Restic restic is a backup tool that performs client side encryption, de-duplication and supports a variety of local and remote storage backends. Homepage ripgrep ripgrep (rg) recursively searches file trees for content in files matching a regular expression. Its extremely fast, and respects ignore files and binary files by default. Homepage shotgun shotgun is a tool for taking screenshots on X.org based environments. All the screenshots in this post were taken with it. It pairs well with slop. $ shotgun $(slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\") eva.png Homepage skim skim is a fuzzy finder. It can be used to fuzzy match input fed to it. I use it with Neovim and zsh for fuzzy matching file names. Homepage slop slop (Select Operation) presents a UI to select a region of the screen or a window and prints the region to stdout. Works well with shotgun. $ slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\" -i 8389044 -g 1464x1008+291+818 Homepage Syncthing Syncthing is a decentralised file synchronisation tool. Like Dropbox but self hosted and without the need for a central third-party file store. Homepage tig tig is a ncurses TUI for git. Its great for reviewing and staging changes, viewing history and diffs. Homepage titlecase titlecase is a little tool I wrote to format text using a title case format described by John Gruber. It correctly handles punctuation, and words like iPhone. I use it to obtain consistent titles on all my blog posts. $ echo 'an illustrated guide to useful command line tools' | titlecase An Illustrated Guide to Useful Command Line Tools I typically use it from within Neovim where selected text is piped through it in-place. This is done by creating a visual selection and then typing: :!titlecase. Homepage Universal Ctags Universal Ctags is a fork of exuberant ctags that is actively maintained. ctags is used to generate a tags file that vim and other tools can use to navigate to the definition of symbols in files. $ ctags --recurse src Homepage watchexec watchexec is a file and directory watcher that can run commands in response to file-system changes. Handy for auto running tests or restarting a development web server when source files change. # run command on file change $ watchexec -w content cobalt build # kill and restart server on file change $ watchexec -w src -s SIGINT -r 'cargo run' Homepage z z tracks your most used directories and allows you to jump to them with a partial name. Homepage zola zola is a full-featured very fast static site compiler. Homepage Changelog 31 Oct 2019 Add bb, and brief descriptions to the table of contents 28 Oct 2019 Add hyperfine Comments Comments on Lobsters Comments on Hacker News Previous Post: What I Learnt Building a Lobsters TUI in Rust "
        ],
        "story_type": "Normal",
        "url_raw": "https://www.wezm.net/technical/2019/10/useful-command-line-tools/",
        "comments.comment_id": [21363660, 21364159],
        "comments.comment_author": ["aquova", "atonalfreerider"],
        "comments.comment_descendants": [4, 2],
        "comments.comment_time": [
          "2019-10-26T13:52:33Z",
          "2019-10-26T15:18:17Z"
        ],
        "comments.comment_text": [
          "I quite like this list, there are a number of utilities here that I already use on a daily basis. There are also a few utilities that I like that weren't on this list, or some alternatives to what was shown. Some off the top of my head:<p>- nnn[0] (C) - A terminal file manager, similar to ranger. It allows you to navigate directories, manipulate files, analyze disk usage, and fuzzy open files.<p>- ncdu[1] (C) - ncurses disk analyzer. Similar to du -sh, but allows for directory navigation as well.<p>- z.lua[2] (Lua) - It's an alternative to the z utility mentioned in the article. I haven't done the benchmarks, but they claim to be faster than z. I'm mostly just including it because it's what I use.<p>[0] <a href=\"https://github.com/jarun/nnn\" rel=\"nofollow\">https://github.com/jarun/nnn</a>\n[1] <a href=\"https://dev.yorhel.nl/ncdu\" rel=\"nofollow\">https://dev.yorhel.nl/ncdu</a>\n[2] <a href=\"https://github.com/skywind3000/z.lua\" rel=\"nofollow\">https://github.com/skywind3000/z.lua</a>",
          "An observation: I have always taken the meaning of the word \"illustrated\" to specifically refer to non-lexical graphics. Searching the dictionary definition of the word, I find a looser definition that applies to \"examples\" intended to aid an explanation.<p>This is a similar cognitive dissonance to when I first learned that \"Visual Basic\" and \"Visual Studio\" meant that the syntax of the displayed code was highlighted, not graphically represented in a non-lexical way."
        ],
        "id": "8f7092d0-5220-4f32-a924-a8ab72710660",
        "url_text": "Published on Sat, 26 October 2019 Inspired by a similar post by Ben Boyter this a list of useful command line tools that I use. Its not a list of every tool I use. These are tools that are new or typically not part of a standard POSIX command line environment. This post is a living document and will be updated over time. It should be obvious that I have a strong preference for fast tools without a large runtime dependency like Python or node.js. Most of these tools are portable to *BSD, Linux, macOS. Many also work on Windows. For OSes that ship up to date software many are available via the system package repository. Last updated: 31 Oct 2019 About my CLI environment: I use the zsh shell, Pragmata Pro font, and base16 default dark color scheme. My prompt is generated by promptline. Table of Contents Alacritty Terminal emulator alt Find alternate files bat cat with syntax highlighting bb System monitor chars Unicode character search dot Dot files manager dust Disk usage analyser eva Calculator exa Replacement for ls fd Replacement for find hexyl Hex viewer hyperfine Benchmarking tool jq awk/XPath for JSON mdcat Render Markdown in the terminal pass Password manager Podman Docker alternative Restic Encrypted backup tool ripgrep Fast, intelligent grep shotgun Take screenshots skim Fuzzy finder slop Graphical region selection Syncthing Decentralised file synchronisation tig TUI for git titlecase Convert text to title case Universal Ctags Maintained ctags fork watchexec Run commands in response to file system changes z Jump to directories zola Static site compiler Changelog The changelog for this page Alacritty Alacritty is fast terminal emulator. Whilst not strictly a command line tool, it does host everything I do in the command line. It is the terminal emulator in use in all the screenshots on this page. Homepage alt alt is a tool for finding the alternate to a file. E.g. the header for an implementation or the test for an implementation. I use it paired with Neovim to easily toggle between tests and implementation. $ alt app/models/page.rb spec/models/page_spec.rb Homepage bat bat is an alternative to the common (mis)use of cat to print a file to the terminal. It supports syntax highlighting and git integration. Homepage bb bb is system monitor like top. It shows overall CPU and memory usage as well as detailed information per process. Homepage chars chars shows information about Unicode characters matching a search term. Homepage dot dot is a dotfiles manager. It maintains a set of symlinks according to a mappings file. I use it to manage my dotfiles. Homepage dust dust is an alternative du -sh. It calculates the size of a directory tree, printing a summary of the largest items. Homepage exa exa is a replacement for ls with sensible defaults and added features like a tree view, git integration, and optional icons. I have ls aliased to exa in my shell. Homepage eva eva is a command line calculator similar to bc, with syntax highlighting and persistent history. Homepage fd fd is an alternative to find and has a more user friendly command line interface and respects ignore files, like .gitignore. The combination of its speed and ignore file support make it excellent for searching for files in git repositories. Homepage hexyl hexyl is a hex viewer that uses Unicode characters and colour to make the output more readable. Homepage hyperfine hyperfine command line benchmarking tool. It allows you to benchmark commands with warmup and statistical analysis. Homepage jq jq is kind of like awk for JSON. It lets you transform and extract information from JSON documents. Homepage mdcat mdcat renders Markdown files in the terminal. In supported terminals (not Alacritty) links are clickable (without the url being visible like in a web browser) and images are rendered. Homepage pass pass is a password manager that uses GPG to store the passwords. I use it with the passff Firefox extension and Pass for iOS on my phone. Homepage Podman podman is an alternative to Docker that does not require a daemon. Containers are run as the user running Podman so files written into the host dont end up owned by root. The CLI is largely compatible with the docker CLI. Homepage Restic restic is a backup tool that performs client side encryption, de-duplication and supports a variety of local and remote storage backends. Homepage ripgrep ripgrep (rg) recursively searches file trees for content in files matching a regular expression. Its extremely fast, and respects ignore files and binary files by default. Homepage shotgun shotgun is a tool for taking screenshots on X.org based environments. All the screenshots in this post were taken with it. It pairs well with slop. $ shotgun $(slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\") eva.png Homepage skim skim is a fuzzy finder. It can be used to fuzzy match input fed to it. I use it with Neovim and zsh for fuzzy matching file names. Homepage slop slop (Select Operation) presents a UI to select a region of the screen or a window and prints the region to stdout. Works well with shotgun. $ slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\" -i 8389044 -g 1464x1008+291+818 Homepage Syncthing Syncthing is a decentralised file synchronisation tool. Like Dropbox but self hosted and without the need for a central third-party file store. Homepage tig tig is a ncurses TUI for git. Its great for reviewing and staging changes, viewing history and diffs. Homepage titlecase titlecase is a little tool I wrote to format text using a title case format described by John Gruber. It correctly handles punctuation, and words like iPhone. I use it to obtain consistent titles on all my blog posts. $ echo 'an illustrated guide to useful command line tools' | titlecase An Illustrated Guide to Useful Command Line Tools I typically use it from within Neovim where selected text is piped through it in-place. This is done by creating a visual selection and then typing: :!titlecase. Homepage Universal Ctags Universal Ctags is a fork of exuberant ctags that is actively maintained. ctags is used to generate a tags file that vim and other tools can use to navigate to the definition of symbols in files. $ ctags --recurse src Homepage watchexec watchexec is a file and directory watcher that can run commands in response to file-system changes. Handy for auto running tests or restarting a development web server when source files change. # run command on file change $ watchexec -w content cobalt build # kill and restart server on file change $ watchexec -w src -s SIGINT -r 'cargo run' Homepage z z tracks your most used directories and allows you to jump to them with a partial name. Homepage zola zola is a full-featured very fast static site compiler. Homepage Changelog 31 Oct 2019 Add bb, and brief descriptions to the table of contents 28 Oct 2019 Add hyperfine Comments Comments on Lobsters Comments on Hacker News Previous Post: What I Learnt Building a Lobsters TUI in Rust ",
        "_version_": 1718536536273715200
      },
      {
        "story_id": 21572308,
        "story_author": "dragondax",
        "story_descendants": 47,
        "story_score": 98,
        "story_time": "2019-11-19T12:23:01Z",
        "story_title": "Run an Internet Speed Test from the Command Line",
        "search": [
          "Run an Internet Speed Test from the Command Line",
          "https://www.putorius.net/speed-test-command-line.html",
          "We have all used tools like speedtest.net to test upload and download speeds. Whether it was to test the WiFi in that coffee shop (I use my own tether, never unknown hot spots), preparing for a LAN party (do people still do that?), or just a step in troubleshooting, we have all been there. For one reason or another you simply think you are being cheated of bandwidth, so you want independent verification of your speeds. This typically means opening a browser and going to a website to test your connection. But what if you want to run a speed test on a remote server? In this article we will discuss running an internet speed test from the Linux command line, and skipping the browser. There is something about the raw efficiency of the command line that I am really attracted to. As I discussed in the article 5 Command Line Tool to Break Your Dependence on the GUI, I try my best to stay away from the browser. It usually creates an unnecessary distraction. The internet is designed to grab your attention like a laser pointer does to a cat. So lets get started, and figure out one more way to stay away from the GUI. Different Speed Test Packages There are a few different tools you can use to run a speed test from the command line. To make things even more confusing the two most popular share the same exact name, but both use the speedtest.net service. Unofficial Speedtest-CLI Python Script The first one is an independently written Python script that is simple to install and use. It is available in the default repositories for some popular Linux distributions. Pros: Easy to installWide AvailabilityFull list of serversCan specify upload test, download test, or both Cons: Minimal output format optionsNo verbose output option Jump to Installing speedtest-cli Python Script or How to Use speedtest-cli Python Script. Official Ookla Speedtest CLI The second tool is built by Ookla, the people who bring you the speedtest.net website and service. Installing it requires you to add a repo for your package manager. But the maintainers offer simple instructions for installation. Pros: Official release from OoklaMore robust formatting optionsOutput easier to read, better layoutVerbose output availableHas repo making it easy to get updates Cons: Use limited to nearby serversCannot specify download or upload only Jump to Official Ookla Speedtest CLI The Speedtest-cli Python Script This is an easy way to get started running a speed test on the Linux command line. Installing the Speedtest-cli Python Script Simply use your package manager to install the package. Install on Fedora using DNF sudo dnf install speedtest-cli Ubuntu or Debian using APT sudo apt-get install speedtest-cli CentOS/Red Hat 7 / 8 Unfortunately, CentOS does not offer the rpm in their repos. It can still be easily installed. Change to /usr/bin directory to make command available to all users: cd /usr/bin Install dependencies: sudo yum install -y python wget Fetch script from github: sudo wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli Make script executable: sudo chmod +x speedtest-cli Or just copy and paste the whole thing below as a single line: cd /usr/bin; sudo yum install -y python wget && wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli && sudo chmod +x speedtest-cli How to Use the Python Script to Run a Speed Test The most basic usage is to simply run the command. It will automatically select the best server based on ping responses. Speedtest-cli Python Script Options There are several options available to change the default behavior. Here we will outline the most popular options. List Available Speed Test Servers You can use the list option to find a list of available servers to run your test against. At the time of writing this list is pretty extensive with 8829 possible servers. NOTE: The servers are sorted by distance, closest first. [[emailprotected] ~]$ speedtest-cli --list Retrieving speedtest.net configuration 4847) Hotwire Fision (Philadelphia, PA, United States) [10.92 km] 10979) School District of Philadelphia (Philadelphia, PA, United States) [10.92 km] ...OUTPUT TRUNCATED... Specify Specific Server to Test Against Once you have found the server you want to test against, you can use the server <SERVER ID> to select it. The server ID is the first column in the output of the list option above. [[emailprotected] ~]$ speedtest-cli --server 4847 Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by School District of Philadelphia (Philadelphia, PA) [10.92 km]: 25.033 ms Testing download speed.. Download: 384.07 Mbit/s Testing upload speed Upload: 417.93 Mbit/s Only Test Upload or Download Speeds The option is actually designed to exclude a test. But since there are only two options it is effectively the same as selecting only one. To run only the download test, you exclude the upload, and vice versa. [[emailprotected] ~]$ speedtest-cli --no-upload Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by KamaTera INC (New Jersey, NJ) [62.36 km]: 19.785 ms Testing download speed.. Download: 600.89 Mbit/s Skipping upload test Format Output in JSON or CSV You can specify the output format in JSON or CSV. You also have the opton to use CSV with a custom delimiter. This is handy if you are going to use the output in some other script or application. [[emailprotected] ~]$ speedtest-cli --json {\"download\": 597726146.0529929, \"upload\": 562476134.8046777, \"ping\": 17.004, \"server\": {\"url\": \"http://speedtest.us-ny2.kamatera.com:8080/speedtest/upload.php\", \"lat\": \"40.0583\", \"lon\": \"-74.4057\", \"name\": \"New Jersey, NJ\", \"country\": \"United States\", \"cc\": \"US\", \"sponsor\": \"KamaTera INC\", \"id\": \"11612\" ...OUTPUT TRUNCATED... Using CSV with a custom delimiter. The default delimiter is a comma, which is implied by the name CSV. Here we use the csv-delimiter option to change the delimiter to a pipe character. [[emailprotected] ~]$ speedtest-cli --csv --csv-delimiter \"|\" 11612|KamaTera INC|New Jersey, NJ|2019-11-17T14:51:53.636981Z|62.35865439150934|8.546|588013638.8767571|512001168.48230773||x.x.x.x The Official Ookla Speedtest CLI The official Speedtest CLI (Command Line Interface) from Ookla is a little more robust. It has all of the options of the python script and more. There are also several output formats not available with the unofficial python script. Ooklas speedtest is also a little easier on the eyes. It spreads the information out which makes it easier to read and displays a neat little progress bar. A URL you can use to share the results is also displayed by default. Installing the Official Speedtest CLI Install Speedtest CLI on Ubuntu / Debian: The Speedtest CLI from Ookla is supported on Ubuntu (xenial & bionic) and Debian (jessie, stretch, buster). $ sudo apt-get install gnupg1 apt-transport-https dirmngr $ export INSTALL_KEY=379CE192D401AB61 $ export DEB_DISTRO=$(lsb_release -sc) $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys $INSTALL_KEY $ echo \"deb https://ookla.bintray.com/debian ${DEB_DISTRO} main\" | sudo tee /etc/apt/sources.list.d/speedtest.list $ sudo apt-get update $ sudo apt-get install speedtest Install Speedtest CLI on Fedora / Redhat / CentOS: Fedora has moved on to DNF for package management, but is still compatible with YUM. These instructions were tested on Fedora 31, CentOS 7 and Red Hat 8. $ sudo yum install wget $ wget https://bintray.com/ookla/rhel/rpm -O bintray-ookla-rhel.repo $ sudo mv bintray-ookla-rhel.repo /etc/yum.repos.d/ $ sudo yum install speedtest How to Use the Official Speedtest CLI Once installed you can simply call the utility by typing speedtest at the command line. This will give you all the default information that you would see on the web version of speedtest.net. Official Speedtest CLI Options The options available in the official release are more robust. Here we will outline the popular options and how to use them. List Available Speed Test Servers Using the -L (servers) option will give you a list of servers available to run a test against. This option will only show you servers that are nearby. What exactly determines nearby is undefined. But for me it looks like they are staying in the tri-state area (PA, NJ, DE). [[emailprotected] ~]$ speedtest -L Closest servers: ID Name Location Country 4847 Hotwire Fision Philadelphia, PA United States 10979 School District of Philadelphia Philadelphia, PA United States 9840 Comcast New Castle, DE United States 11612 KamaTera INC New Jersey, NJ United States ...OUTPUT TRUNCATED... Optionally, you can use the -o (host) option and specify the FQDN of the server instead of the ID. But oddly, I dont see a way to get the FQDN of the servers on the list. I am guessing this option is available for using a custom server. I havent found a way to list all servers. If you are looking to test against a server on the other side of the country, you will have to find it another way. Select Specific Server to Run Speed Test Against You can use the -s (server-id) option to select a server to use from the list. You must supply the server ID with this option. The server ID is the number in the first column of the list output above. [[emailprotected] ~]$ speedtest -s 4847 Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) Change Unit Used for Speed Output The -u (unit) option can display the speed output in many different formats. Decimal prefix, bits per second: bps, kbps, Mbps, Gbps Decimal prefix, bytes per second: B/s, kB/s, MB/s, GB/s Binary prefix, bits per second: kibps, Mibps, Gibps Binary prefix, bytes per second: kiB/s, MiB/s, GiB/s [[emailprotected] ~]$ speedtest -u MiB/s Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) ISP: Verizon Fios Latency: 10.67 ms ( 0.95 ms jitter) Download: 63.45 MiB/s (data used: 700.2 MiB) ...OUTPUT TRUNCATED... Output Formatting Options The Ookla Speedtest CLI offers decent options for output formats. Human Readable DefaultCSV Comma Separated ValueTSV Tab Separated ValueJSON JavaScript Object NotationJSONL JSON LinesJSON-PRETTY JSON Pretty Printed Here is an example using json-pretty. [[emailprotected] ~]$ speedtest -f json-pretty { \"type\": \"result\", \"timestamp\": \"2019-11-17T16:42:06Z\", \"ping\": { \"jitter\": 0.29899999999999999, \"latency\": 17.474 }, \"download\": { \"bandwidth\": 92184614, \"bytes\": 491967724, \"elapsed\": 5303 }, \"upload\": { \"bandwidth\": 45010100, \"bytes\": 313859035, \"elapsed\": 6714 }, ...OUTPUT TRUNCATED... Conclusion Running a speed test from the command line may not be something that is needed on a daily basis for most people. However, it may prove useful in some troubleshooting situations. In this article we cover how to run a speed test from the command line using two similar tools. The unofficial python script and the official Ookla Speedtest CLI. We discussed installing, using and setting options for each one. This should be enough to get you started. For more information on these tools, visit their respective home pages found in the resources section below. Resources and Links: Ookla Speedtest CLISpeedtest-cli (Python Version) on GitHub "
        ],
        "story_type": "Normal",
        "url_raw": "https://www.putorius.net/speed-test-command-line.html",
        "comments.comment_id": [21582731, 21583805],
        "comments.comment_author": ["dspillett", "virtuallynathan"],
        "comments.comment_descendants": [5, 4],
        "comments.comment_time": [
          "2019-11-20T10:49:21Z",
          "2019-11-20T13:46:04Z"
        ],
        "comments.comment_text": [
          "Note that some ISPs prioritise traffic to known speedtest targets, turning off traffic shaping rules that might otherwise slow bulk transfers. When this happens it means you are testing the likely maximum throughput of your connection not necessarily the throughput you will see more generally.<p>This is why Netflix started fast.com - because it draws data from the same distribution points as their video streaming apps it means you can't prioritise the speedtest without also doing so for the video traffic or (more likely) you can't de-prioritise the video traffic without also getting bad scores in that particular speedtest. From Netflix's point of view it is an answer to people contacting support with \"my speedtest results are fine, the problem must be your servers\" when they are experiencing video lag/drops and other such problems and the issue is due to ISP traffic shaping or the ISP simply not having enough backhaul bandwidth.<p>A more reliable test might be taking part in a busy public torrent: that way you are testing against arbitrary locations so your ISP can't be setting different shaping rules for them. Just remember to throttle upstream when testing downstream and vice-versa or saturation in the other direction will slow control packets that will in turn give you lower results for the one you are testing. This may fall into another trap though: unless you limit the number of active streams it may be an unrealistic test as more generally most processes use a small number of streams (or just a single one), and if you limit the number of streams too much you might get a lower result because each swarm member you connect to may be fairly saturated and sharing its bandwidth amongst many connections.",
          "speedtest-cli is <i>garbage</i> if you have >100Mbps Speeds. The dev refuses to acknowledge this: <a href=\"https://github.com/sivel/speedtest-cli/issues/226\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/226</a><p>Also not just me:\n<a href=\"https://github.com/sivel/speedtest-cli/issues/649\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/649</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/648\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/648</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/641\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/641</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/616\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/616</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/601\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/601</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/588\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/588</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/546\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/546</a>"
        ],
        "id": "83a187cd-64f4-47ea-86f3-7626e5aa293c",
        "url_text": "We have all used tools like speedtest.net to test upload and download speeds. Whether it was to test the WiFi in that coffee shop (I use my own tether, never unknown hot spots), preparing for a LAN party (do people still do that?), or just a step in troubleshooting, we have all been there. For one reason or another you simply think you are being cheated of bandwidth, so you want independent verification of your speeds. This typically means opening a browser and going to a website to test your connection. But what if you want to run a speed test on a remote server? In this article we will discuss running an internet speed test from the Linux command line, and skipping the browser. There is something about the raw efficiency of the command line that I am really attracted to. As I discussed in the article 5 Command Line Tool to Break Your Dependence on the GUI, I try my best to stay away from the browser. It usually creates an unnecessary distraction. The internet is designed to grab your attention like a laser pointer does to a cat. So lets get started, and figure out one more way to stay away from the GUI. Different Speed Test Packages There are a few different tools you can use to run a speed test from the command line. To make things even more confusing the two most popular share the same exact name, but both use the speedtest.net service. Unofficial Speedtest-CLI Python Script The first one is an independently written Python script that is simple to install and use. It is available in the default repositories for some popular Linux distributions. Pros: Easy to installWide AvailabilityFull list of serversCan specify upload test, download test, or both Cons: Minimal output format optionsNo verbose output option Jump to Installing speedtest-cli Python Script or How to Use speedtest-cli Python Script. Official Ookla Speedtest CLI The second tool is built by Ookla, the people who bring you the speedtest.net website and service. Installing it requires you to add a repo for your package manager. But the maintainers offer simple instructions for installation. Pros: Official release from OoklaMore robust formatting optionsOutput easier to read, better layoutVerbose output availableHas repo making it easy to get updates Cons: Use limited to nearby serversCannot specify download or upload only Jump to Official Ookla Speedtest CLI The Speedtest-cli Python Script This is an easy way to get started running a speed test on the Linux command line. Installing the Speedtest-cli Python Script Simply use your package manager to install the package. Install on Fedora using DNF sudo dnf install speedtest-cli Ubuntu or Debian using APT sudo apt-get install speedtest-cli CentOS/Red Hat 7 / 8 Unfortunately, CentOS does not offer the rpm in their repos. It can still be easily installed. Change to /usr/bin directory to make command available to all users: cd /usr/bin Install dependencies: sudo yum install -y python wget Fetch script from github: sudo wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli Make script executable: sudo chmod +x speedtest-cli Or just copy and paste the whole thing below as a single line: cd /usr/bin; sudo yum install -y python wget && wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli && sudo chmod +x speedtest-cli How to Use the Python Script to Run a Speed Test The most basic usage is to simply run the command. It will automatically select the best server based on ping responses. Speedtest-cli Python Script Options There are several options available to change the default behavior. Here we will outline the most popular options. List Available Speed Test Servers You can use the list option to find a list of available servers to run your test against. At the time of writing this list is pretty extensive with 8829 possible servers. NOTE: The servers are sorted by distance, closest first. [[emailprotected] ~]$ speedtest-cli --list Retrieving speedtest.net configuration 4847) Hotwire Fision (Philadelphia, PA, United States) [10.92 km] 10979) School District of Philadelphia (Philadelphia, PA, United States) [10.92 km] ...OUTPUT TRUNCATED... Specify Specific Server to Test Against Once you have found the server you want to test against, you can use the server <SERVER ID> to select it. The server ID is the first column in the output of the list option above. [[emailprotected] ~]$ speedtest-cli --server 4847 Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by School District of Philadelphia (Philadelphia, PA) [10.92 km]: 25.033 ms Testing download speed.. Download: 384.07 Mbit/s Testing upload speed Upload: 417.93 Mbit/s Only Test Upload or Download Speeds The option is actually designed to exclude a test. But since there are only two options it is effectively the same as selecting only one. To run only the download test, you exclude the upload, and vice versa. [[emailprotected] ~]$ speedtest-cli --no-upload Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by KamaTera INC (New Jersey, NJ) [62.36 km]: 19.785 ms Testing download speed.. Download: 600.89 Mbit/s Skipping upload test Format Output in JSON or CSV You can specify the output format in JSON or CSV. You also have the opton to use CSV with a custom delimiter. This is handy if you are going to use the output in some other script or application. [[emailprotected] ~]$ speedtest-cli --json {\"download\": 597726146.0529929, \"upload\": 562476134.8046777, \"ping\": 17.004, \"server\": {\"url\": \"http://speedtest.us-ny2.kamatera.com:8080/speedtest/upload.php\", \"lat\": \"40.0583\", \"lon\": \"-74.4057\", \"name\": \"New Jersey, NJ\", \"country\": \"United States\", \"cc\": \"US\", \"sponsor\": \"KamaTera INC\", \"id\": \"11612\" ...OUTPUT TRUNCATED... Using CSV with a custom delimiter. The default delimiter is a comma, which is implied by the name CSV. Here we use the csv-delimiter option to change the delimiter to a pipe character. [[emailprotected] ~]$ speedtest-cli --csv --csv-delimiter \"|\" 11612|KamaTera INC|New Jersey, NJ|2019-11-17T14:51:53.636981Z|62.35865439150934|8.546|588013638.8767571|512001168.48230773||x.x.x.x The Official Ookla Speedtest CLI The official Speedtest CLI (Command Line Interface) from Ookla is a little more robust. It has all of the options of the python script and more. There are also several output formats not available with the unofficial python script. Ooklas speedtest is also a little easier on the eyes. It spreads the information out which makes it easier to read and displays a neat little progress bar. A URL you can use to share the results is also displayed by default. Installing the Official Speedtest CLI Install Speedtest CLI on Ubuntu / Debian: The Speedtest CLI from Ookla is supported on Ubuntu (xenial & bionic) and Debian (jessie, stretch, buster). $ sudo apt-get install gnupg1 apt-transport-https dirmngr $ export INSTALL_KEY=379CE192D401AB61 $ export DEB_DISTRO=$(lsb_release -sc) $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys $INSTALL_KEY $ echo \"deb https://ookla.bintray.com/debian ${DEB_DISTRO} main\" | sudo tee /etc/apt/sources.list.d/speedtest.list $ sudo apt-get update $ sudo apt-get install speedtest Install Speedtest CLI on Fedora / Redhat / CentOS: Fedora has moved on to DNF for package management, but is still compatible with YUM. These instructions were tested on Fedora 31, CentOS 7 and Red Hat 8. $ sudo yum install wget $ wget https://bintray.com/ookla/rhel/rpm -O bintray-ookla-rhel.repo $ sudo mv bintray-ookla-rhel.repo /etc/yum.repos.d/ $ sudo yum install speedtest How to Use the Official Speedtest CLI Once installed you can simply call the utility by typing speedtest at the command line. This will give you all the default information that you would see on the web version of speedtest.net. Official Speedtest CLI Options The options available in the official release are more robust. Here we will outline the popular options and how to use them. List Available Speed Test Servers Using the -L (servers) option will give you a list of servers available to run a test against. This option will only show you servers that are nearby. What exactly determines nearby is undefined. But for me it looks like they are staying in the tri-state area (PA, NJ, DE). [[emailprotected] ~]$ speedtest -L Closest servers: ID Name Location Country 4847 Hotwire Fision Philadelphia, PA United States 10979 School District of Philadelphia Philadelphia, PA United States 9840 Comcast New Castle, DE United States 11612 KamaTera INC New Jersey, NJ United States ...OUTPUT TRUNCATED... Optionally, you can use the -o (host) option and specify the FQDN of the server instead of the ID. But oddly, I dont see a way to get the FQDN of the servers on the list. I am guessing this option is available for using a custom server. I havent found a way to list all servers. If you are looking to test against a server on the other side of the country, you will have to find it another way. Select Specific Server to Run Speed Test Against You can use the -s (server-id) option to select a server to use from the list. You must supply the server ID with this option. The server ID is the number in the first column of the list output above. [[emailprotected] ~]$ speedtest -s 4847 Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) Change Unit Used for Speed Output The -u (unit) option can display the speed output in many different formats. Decimal prefix, bits per second: bps, kbps, Mbps, Gbps Decimal prefix, bytes per second: B/s, kB/s, MB/s, GB/s Binary prefix, bits per second: kibps, Mibps, Gibps Binary prefix, bytes per second: kiB/s, MiB/s, GiB/s [[emailprotected] ~]$ speedtest -u MiB/s Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) ISP: Verizon Fios Latency: 10.67 ms ( 0.95 ms jitter) Download: 63.45 MiB/s (data used: 700.2 MiB) ...OUTPUT TRUNCATED... Output Formatting Options The Ookla Speedtest CLI offers decent options for output formats. Human Readable DefaultCSV Comma Separated ValueTSV Tab Separated ValueJSON JavaScript Object NotationJSONL JSON LinesJSON-PRETTY JSON Pretty Printed Here is an example using json-pretty. [[emailprotected] ~]$ speedtest -f json-pretty { \"type\": \"result\", \"timestamp\": \"2019-11-17T16:42:06Z\", \"ping\": { \"jitter\": 0.29899999999999999, \"latency\": 17.474 }, \"download\": { \"bandwidth\": 92184614, \"bytes\": 491967724, \"elapsed\": 5303 }, \"upload\": { \"bandwidth\": 45010100, \"bytes\": 313859035, \"elapsed\": 6714 }, ...OUTPUT TRUNCATED... Conclusion Running a speed test from the command line may not be something that is needed on a daily basis for most people. However, it may prove useful in some troubleshooting situations. In this article we cover how to run a speed test from the command line using two similar tools. The unofficial python script and the official Ookla Speedtest CLI. We discussed installing, using and setting options for each one. This should be enough to get you started. For more information on these tools, visit their respective home pages found in the resources section below. Resources and Links: Ookla Speedtest CLISpeedtest-cli (Python Version) on GitHub ",
        "_version_": 1718536544026886144
      },
      {
        "story_id": 20811829,
        "story_author": "weinzierl",
        "story_descendants": 59,
        "story_score": 273,
        "story_time": "2019-08-27T16:54:20Z",
        "story_title": "Curl exercises",
        "search": [
          "Curl exercises",
          "https://jvns.ca/blog/2019/08/27/curl-exercises/",
          "Recently Ive been interested in how people learn things. I was reading Kathy Sierras great book Badass: Making Users Awesome. It talks about the idea of deliberate practice. The idea is that you find a small micro-skill that can be learned in maybe 3 sessions of 45 minutes, and focus on learning that micro-skill. So, as an exercise, I was trying to think of a computer skill that I thought could be learned in 3 45-minute sessions. I thought that making HTTP requests with curl might be a skill like that, so here are some curl exercises as an experiment! whats curl? curl is a command line tool for making HTTP requests. I like it because its an easy way to test that servers or APIs are doing what I think, but its a little confusing at first! Heres a drawing explaining curls most important command line arguments (which is page 6 of my Bite Size Networking zine). You can click to make it bigger. fluency is valuable With any command line tool, I think having fluency is really helpful. Its really nice to be able to just type in the thing you need. For example recently I was testing out the Gumroad API and I was able to just type in: curl https://api.gumroad.com/v2/sales \\ -d \"access_token=<SECRET>\" \\ -X GET -d \"before=2016-09-03\" and get things working from the command line. 21 curl exercises These exercises are about understanding how to make different kinds of HTTP requests with curl. Theyre a little repetitive on purpose. They exercise basically everything I do with curl. To keep it simple, were going to make a lot of our requests to the same website: https://httpbin.org. httpbin is a service that accepts HTTP requests and then tells you what request you made. Request https://httpbin.org Request https://httpbin.org/anything. httpbin.org/anything will look at the request you made, parse it, and echo back to you what you requested. curls default is to make a GET request. Make a POST request to https://httpbin.org/anything Make a GET request to https://httpbin.org/anything, but this time add some query parameters (set value=panda). Request googles robots.txt file (www.google.com/robots.txt) Make a GET request to https://httpbin.org/anything and set the header User-Agent: elephant. Make a DELETE request to https://httpbin.org/anything Request https://httpbin.org/anything and also get the response headers Make a POST request to https://httpbin.org/anything with the JSON body {\"value\": \"panda\"} Make the same POST request as the previous exercise, but set the Content-Type header to application/json (because POST requests need to have a content type that matches their body). Look at the json field in the response to see the difference from the previous one. Make a GET request to https://httpbin.org/anything and set the header Accept-Encoding: gzip (what happens? why?) Put a bunch of a JSON in a file and then make a POST request to https://httpbin.org/anything with the JSON in that file as the body Make a request to https://httpbin.org/image and set the header Accept: image/png. Save the output to a PNG file and open the file in an image viewer. Try the same thing with different Accept: headers. Make a PUT request to https://httpbin.org/anything Request https://httpbin.org/image/jpeg, save it to a file, and open that file in your image editor. Request https://www.twitter.com. Youll get an empty response. Get curl to show you the response headers too, and try to figure out why the response was empty. Make any request to https://httpbin.org/anything and just set some nonsense headers (like panda: elephant) Request https://httpbin.org/status/404 and https://httpbin.org/status/200. Request them again and get curl to show the response headers. Request https://httpbin.org/anything and set a username and password (with -u username:password) Download the Twitter homepage (https://twitter.com) in Spanish by setting the Accept-Language: es-ES header. Make a request to the Stripe API with curl. (see https://stripe.com/docs/development for how, they give you a test API key). Try making exactly the same request to https://httpbin.org/anything. "
        ],
        "story_type": "Normal",
        "url_raw": "https://jvns.ca/blog/2019/08/27/curl-exercises/",
        "url_text": "Recently Ive been interested in how people learn things. I was reading Kathy Sierras great book Badass: Making Users Awesome. It talks about the idea of deliberate practice. The idea is that you find a small micro-skill that can be learned in maybe 3 sessions of 45 minutes, and focus on learning that micro-skill. So, as an exercise, I was trying to think of a computer skill that I thought could be learned in 3 45-minute sessions. I thought that making HTTP requests with curl might be a skill like that, so here are some curl exercises as an experiment! whats curl? curl is a command line tool for making HTTP requests. I like it because its an easy way to test that servers or APIs are doing what I think, but its a little confusing at first! Heres a drawing explaining curls most important command line arguments (which is page 6 of my Bite Size Networking zine). You can click to make it bigger. fluency is valuable With any command line tool, I think having fluency is really helpful. Its really nice to be able to just type in the thing you need. For example recently I was testing out the Gumroad API and I was able to just type in: curl https://api.gumroad.com/v2/sales \\ -d \"access_token=<SECRET>\" \\ -X GET -d \"before=2016-09-03\" and get things working from the command line. 21 curl exercises These exercises are about understanding how to make different kinds of HTTP requests with curl. Theyre a little repetitive on purpose. They exercise basically everything I do with curl. To keep it simple, were going to make a lot of our requests to the same website: https://httpbin.org. httpbin is a service that accepts HTTP requests and then tells you what request you made. Request https://httpbin.org Request https://httpbin.org/anything. httpbin.org/anything will look at the request you made, parse it, and echo back to you what you requested. curls default is to make a GET request. Make a POST request to https://httpbin.org/anything Make a GET request to https://httpbin.org/anything, but this time add some query parameters (set value=panda). Request googles robots.txt file (www.google.com/robots.txt) Make a GET request to https://httpbin.org/anything and set the header User-Agent: elephant. Make a DELETE request to https://httpbin.org/anything Request https://httpbin.org/anything and also get the response headers Make a POST request to https://httpbin.org/anything with the JSON body {\"value\": \"panda\"} Make the same POST request as the previous exercise, but set the Content-Type header to application/json (because POST requests need to have a content type that matches their body). Look at the json field in the response to see the difference from the previous one. Make a GET request to https://httpbin.org/anything and set the header Accept-Encoding: gzip (what happens? why?) Put a bunch of a JSON in a file and then make a POST request to https://httpbin.org/anything with the JSON in that file as the body Make a request to https://httpbin.org/image and set the header Accept: image/png. Save the output to a PNG file and open the file in an image viewer. Try the same thing with different Accept: headers. Make a PUT request to https://httpbin.org/anything Request https://httpbin.org/image/jpeg, save it to a file, and open that file in your image editor. Request https://www.twitter.com. Youll get an empty response. Get curl to show you the response headers too, and try to figure out why the response was empty. Make any request to https://httpbin.org/anything and just set some nonsense headers (like panda: elephant) Request https://httpbin.org/status/404 and https://httpbin.org/status/200. Request them again and get curl to show the response headers. Request https://httpbin.org/anything and set a username and password (with -u username:password) Download the Twitter homepage (https://twitter.com) in Spanish by setting the Accept-Language: es-ES header. Make a request to the Stripe API with curl. (see https://stripe.com/docs/development for how, they give you a test API key). Try making exactly the same request to https://httpbin.org/anything. ",
        "comments.comment_id": [20813303, 20813591],
        "comments.comment_author": ["AndrejKolar", "UI_at_80x24"],
        "comments.comment_descendants": [1, 2],
        "comments.comment_time": [
          "2019-08-27T19:03:29Z",
          "2019-08-27T19:33:10Z"
        ],
        "comments.comment_text": [
          "I really like using HTTPie (<a href=\"https://httpie.org\" rel=\"nofollow\">https://httpie.org</a>). Much friendlier syntax then curl, formats the output...<p>Also works great with fx (<a href=\"https://github.com/antonmedv/fx\" rel=\"nofollow\">https://github.com/antonmedv/fx</a>). Just pipe the output from HTTPie and you get easy json processing.",
          "This is more SysAdmin related, but one power-curl function I use atleast 30 times a day is this alias I have in my .bash_aliases<p>This will output the HTTP status code for a given URL.<p><pre><code>     alias hstat=\"curl -o /dev/null --silent --head --write-out '%{http_code}\\n'\" $1  \n</code></pre>\nExample:<p><pre><code>  $ hstat google.com\n  301\n</code></pre>\nI also use curl as an 'uptime monitor' by adding onto that code section.  (a file with a list of URLs, and \"if http status code !=200\" then email me.)<p>There are variations on this all over the place but I really depend on it and I like it."
        ],
        "id": "c5511fe6-8738-4287-8464-3db62119691f",
        "_version_": 1718536516130570240
      },
      {
        "story_id": 19954195,
        "story_author": "rumcajz",
        "story_descendants": 39,
        "story_score": 126,
        "story_time": "2019-05-19T16:52:59Z",
        "story_title": "Show HN: UXY – adding structure to Unix tools",
        "search": [
          "Show HN: UXY – adding structure to Unix tools",
          "https://github.com/sustrik/uxy",
          "Treating everything as a string is the way through which the great power and versatility of UNIX tools is achieved. However, sometimes the constant parsing of strings gets a bit cumbersome. UXY is a tool to manipulate UXY format, which is basically a two-dimensional table that's both human- and machine-readable. The format is deliberately designed to be as similar to the output of standard tools, such as ls or ps, as possible. UXY tool also wraps some common UNIX tools and exports their output in UXY format. Along with converters from/to other common data formats (e.g. JSON) it is meant to allow for quick and painless access to the data. Examples $ uxy ls TYPE PERMISSIONS LINKS OWNER GROUP SIZE TIME NAME - rw-r--r-- 1 martin martin 3204 2019-05-25T15:44:46.371308721+02:00 README.md - rwxr-xr-x 1 martin martin 25535 2019-05-25T16:29:28.518397541+02:00 uxy $ uxy ls | uxy fmt \"NAME SIZE\" NAME SIZE README.md 7451 uxy 11518 $ uxy ls | uxy fmt \"NAME SIZE\" | uxy align NAME SIZE README.md 7451 uxy 11518 $ uxy top | uxy fmt \"PID CPU COMMAND\" | uxy to-json [ { \"PID\": \"4704\", \"CPU\": \"12.5\", \"COMMAND\": \"top\" }, { \"PID\": \"2903\", \"CPU\": \"6.2\", \"COMMAND\": \"Web Content\" }, { \"PID\": \"1\", \"CPU\": \"0.0\", \"COMMAND\": \"systemd\" } ] $ uxy ls | uxy grep test NAME TYPE PERMISSIONS LINKS OWNER GROUP SIZE TIME NAME - rw-r--r-- 1 martin martin 45 2019-05-25T16:09:58.755551983+02:00 test.csv - rw-r--r-- 1 martin martin 84 2019-05-25T16:09:58.755552856+02:00 test.txt - rw-r--r-- 1 martin martin 75 2019-05-25T16:09:58.755559998+02:00 test.uxy $ uxy ps | uxy to-json | jq '.[].CMD' \"bash\" \"uxy\" \"uxy\" \"jq\" \"ps\" $ cat test.csv NAME,TIME Quasimodo,14:30 Moby Dick,14:22 $ cat test.csv | uxy from-csv | uxy align NAME TIME Quasimodo 14:30 \"Moby Dick\" 14:22 TOOLS UXY tools All UXY tools take input from stdin and write the result to stdout. The tools follow the Postel's principle: \"Be liberal in what you accept, conservative in what you output.\" They accept any UXY input, but they try to align the fields in the output to make it more convenient to read. uxy align uxy from-csv uxy from-json uxy from-yaml uxy grep uxy import uxy fmt uxy to-csv uxy to-json uxy to-yaml uxy trim Wrapped UNIX tools Any argument that could be passed to the original tool can also be passed to the UXY-wrapped version of the tool. The exception are the arguments that modify how the output looks like. UXY manages those arguments itself. The only control you have over the output is to either print the default (short) set of result fields (mostly defined as \"the most useful info that fits on page\") or long set of result fields (\"all the information UXY was able to extract\"): $ uxy -l ps When running with -l option it often happens that the output exceeds the terminal width, gets wrapped and unreadable. In such cases you can either filter out just the fields you are intersed in using fmt subcommand or convert the result to YAML (uxy to-yaml) which happens to render each field on a separate line: $ uxy -l ifconfig | uxy fmt \"NAME INET-ADDR\" NAME INET-ADDR enp0s31f6 \"\" lo 127.0.0.1 wlp3s0 192.168.1.7 $ uxy -l ifconfig | uxy to-yaml - ETHER-ADDR: e4:42:a6:f4:1d:02 FLAGS: UP,BROADCAST,RUNNING,MULTICAST INET-ADDR: 192.168.1.7 INET-NETMASK: 255.255.255.0 INET6-ADDR: fe80::fd53:a17f:12ce:38a8 INET6-PREFIXLEN: '64' INET6-SCOPEID: 0x20 ... uxy du uxy ifconfig uxy ls uxy lsof uxy netstat uxy ps uxy top uxy w TESTING To test, run ./test script. "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/sustrik/uxy",
        "url_text": "Treating everything as a string is the way through which the great power and versatility of UNIX tools is achieved. However, sometimes the constant parsing of strings gets a bit cumbersome. UXY is a tool to manipulate UXY format, which is basically a two-dimensional table that's both human- and machine-readable. The format is deliberately designed to be as similar to the output of standard tools, such as ls or ps, as possible. UXY tool also wraps some common UNIX tools and exports their output in UXY format. Along with converters from/to other common data formats (e.g. JSON) it is meant to allow for quick and painless access to the data. Examples $ uxy ls TYPE PERMISSIONS LINKS OWNER GROUP SIZE TIME NAME - rw-r--r-- 1 martin martin 3204 2019-05-25T15:44:46.371308721+02:00 README.md - rwxr-xr-x 1 martin martin 25535 2019-05-25T16:29:28.518397541+02:00 uxy $ uxy ls | uxy fmt \"NAME SIZE\" NAME SIZE README.md 7451 uxy 11518 $ uxy ls | uxy fmt \"NAME SIZE\" | uxy align NAME SIZE README.md 7451 uxy 11518 $ uxy top | uxy fmt \"PID CPU COMMAND\" | uxy to-json [ { \"PID\": \"4704\", \"CPU\": \"12.5\", \"COMMAND\": \"top\" }, { \"PID\": \"2903\", \"CPU\": \"6.2\", \"COMMAND\": \"Web Content\" }, { \"PID\": \"1\", \"CPU\": \"0.0\", \"COMMAND\": \"systemd\" } ] $ uxy ls | uxy grep test NAME TYPE PERMISSIONS LINKS OWNER GROUP SIZE TIME NAME - rw-r--r-- 1 martin martin 45 2019-05-25T16:09:58.755551983+02:00 test.csv - rw-r--r-- 1 martin martin 84 2019-05-25T16:09:58.755552856+02:00 test.txt - rw-r--r-- 1 martin martin 75 2019-05-25T16:09:58.755559998+02:00 test.uxy $ uxy ps | uxy to-json | jq '.[].CMD' \"bash\" \"uxy\" \"uxy\" \"jq\" \"ps\" $ cat test.csv NAME,TIME Quasimodo,14:30 Moby Dick,14:22 $ cat test.csv | uxy from-csv | uxy align NAME TIME Quasimodo 14:30 \"Moby Dick\" 14:22 TOOLS UXY tools All UXY tools take input from stdin and write the result to stdout. The tools follow the Postel's principle: \"Be liberal in what you accept, conservative in what you output.\" They accept any UXY input, but they try to align the fields in the output to make it more convenient to read. uxy align uxy from-csv uxy from-json uxy from-yaml uxy grep uxy import uxy fmt uxy to-csv uxy to-json uxy to-yaml uxy trim Wrapped UNIX tools Any argument that could be passed to the original tool can also be passed to the UXY-wrapped version of the tool. The exception are the arguments that modify how the output looks like. UXY manages those arguments itself. The only control you have over the output is to either print the default (short) set of result fields (mostly defined as \"the most useful info that fits on page\") or long set of result fields (\"all the information UXY was able to extract\"): $ uxy -l ps When running with -l option it often happens that the output exceeds the terminal width, gets wrapped and unreadable. In such cases you can either filter out just the fields you are intersed in using fmt subcommand or convert the result to YAML (uxy to-yaml) which happens to render each field on a separate line: $ uxy -l ifconfig | uxy fmt \"NAME INET-ADDR\" NAME INET-ADDR enp0s31f6 \"\" lo 127.0.0.1 wlp3s0 192.168.1.7 $ uxy -l ifconfig | uxy to-yaml - ETHER-ADDR: e4:42:a6:f4:1d:02 FLAGS: UP,BROADCAST,RUNNING,MULTICAST INET-ADDR: 192.168.1.7 INET-NETMASK: 255.255.255.0 INET6-ADDR: fe80::fd53:a17f:12ce:38a8 INET6-PREFIXLEN: '64' INET6-SCOPEID: 0x20 ... uxy du uxy ifconfig uxy ls uxy lsof uxy netstat uxy ps uxy top uxy w TESTING To test, run ./test script. ",
        "comments.comment_id": [19957013, 19957470],
        "comments.comment_author": ["dima55", "bayareanative"],
        "comments.comment_descendants": [4, 1],
        "comments.comment_time": [
          "2019-05-20T02:12:03Z",
          "2019-05-20T04:14:54Z"
        ],
        "comments.comment_text": [
          "This is becoming a really crowded space. Some other similar tools that make slightly different design choices and that have variable envisioned use cases:<p>- <a href=\"https://github.com/dkogan/vnlog\" rel=\"nofollow\">https://github.com/dkogan/vnlog</a><p>- <a href=\"https://csvkit.readthedocs.io/\" rel=\"nofollow\">https://csvkit.readthedocs.io/</a><p>- <a href=\"https://github.com/johnkerl/miller\" rel=\"nofollow\">https://github.com/johnkerl/miller</a><p>- <a href=\"https://github.com/BurntSushi/xsv\" rel=\"nofollow\">https://github.com/BurntSushi/xsv</a><p>- <a href=\"https://github.com/eBay/tsv-utils-dlang\" rel=\"nofollow\">https://github.com/eBay/tsv-utils-dlang</a><p>- <a href=\"https://stedolan.github.io/jq/\" rel=\"nofollow\">https://stedolan.github.io/jq/</a><p>- <a href=\"http://harelba.github.io/q/\" rel=\"nofollow\">http://harelba.github.io/q/</a><p>- <a href=\"https://github.com/BatchLabs/charlatan\" rel=\"nofollow\">https://github.com/BatchLabs/charlatan</a><p>- <a href=\"https://github.com/dinedal/textql\" rel=\"nofollow\">https://github.com/dinedal/textql</a><p>- <a href=\"https://github.com/dbohdan/sqawk\" rel=\"nofollow\">https://github.com/dbohdan/sqawk</a><p>(disclaimer: vnlog is my tool)",
          "A related problem is the constant churn of logging.. taking structured data, destructuring it with a string serialization and then parsing it again.<p>This resource-wasting antipattern pops up over and over again.<p>Also, logs are message-oriented entries and serializing them as discrete, lengthy files is insane.<p>Structured data should stay structured, say a time-series / log-structured database. Destructuring should be a rare event."
        ],
        "id": "60ab6a08-5f98-487b-a8a3-43e4749f229c",
        "_version_": 1718536484085039104
      },
      {
        "story_id": 21858952,
        "story_author": "pcr910303",
        "story_descendants": 38,
        "story_score": 117,
        "story_time": "2019-12-22T19:07:21Z",
        "story_title": "JSON on the Command Line with Jq",
        "search": [
          "JSON on the Command Line with Jq",
          "https://shapeshed.com/jq-json/",
          "HomePostsContactLast updated Saturday, Nov 16, 2019A series of how to examples on using jq, a command-line JSON processorEstimated reading time: 4 minutesTable of contentsHow to pretty print JSONHow to use pipes with jqHow to find a key and valueHow to find items in an arrayHow to combine filtersHow to transform JSONHow to transform data values within JSONHow to remove keys from JSONHow to map valuesHow to wrangle JSON how you wantFurther readingjq is a fantastic command-line JSON processor. It plays nice with UNIX pipes and offers extensive functionality for interrogating, manipulating and working with JSON file.How to pretty print JSONjq can do a lot but probably the highest frequency use for most users is to pretty print JSON either from a file or after a network call. Suppose that we have a file names.json containing the following json.[{\"id\": 1, \"name\": \"Arthur\", \"age\": \"21\"},{\"id\": 2, \"name\": \"Richard\", \"age\": \"32\"}] jq can pretty print this file using the . filter. This takes the entire input and sends it to standard output. Unless told not to jq will pretty print making JSON readable.jq '.' names.json [ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] How to use pipes with jqBecause jq is UNIX friendly it is possible to pipe data in and out of it. This can be useful for using jq as a filter or interacting with other tools. In the following pipeline cat pipes the file into jq and this is piped onto less. This can be very useful for viewing large JSON files.cat names.json | jq '.' | less How to find a key and valueTo find a key and value jq can filter based on keys and return the value. Suppose we have the following simple JSON document saved as dog.json.{ \"name\": \"Buster\", \"breed\": \"Golden Retriever\", \"age\": \"4\", \"owner\": { \"name\": \"Sally\" }, \"likes\": [ \"bones\", \"balls\", \"dog biscuits\" ] } jq can retrieve values from this document by passing key names.jq '.name' \"Buster\" Multiple keys can by passed separated by commas.jq '.breed,.age' \"Golden Retriever\" \"4\" To search for nested objects chain values using the dot operator just as you would in JavaScript.jq '.owner.name' \"Sally\" How to find items in an arrayTo search for items in arrays use bracket syntax with the index starting at 0.jq '.likes[0]' \"bones\" Multiple elements of an array may also be returned.echo '[\"a\",\"b\",\"c\",\"d\",\"e\"]' | jq '.[2:4]' [ \"c\", \"d\" ] How to combine filtersjq can combine filters to search within a selection. For the following JSON document suppose that the names need to be filtered.[ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] This can be achieved with a pipe with the jq filter.jq '.[] | .name' names.json \"Arthur\" \"Richard\" How to transform JSONjq can be used for more than just reading values from a JSON object. It can also transform JSON into new data structures. Returning to the dog.json example earlier a new array can be created containing the name and likes as follows.jq '[.name, .likes[]]' dog.json [ \"Buster\", \"Bones\", \"balls\", \"dog biscuits\" ] This can be very useful in data transform pipelines to shift JSON data from one structure to another.How to transform data values within JSONjq can also operate on data within JSON objects. Suppose the following JSON file exists and is saved as inventory.json.{ \"eggs\": 2, \"cheese\": 1, \"milk\": 1 } jq can be used to perform basic arithmetic on number values.jq '.eggs + 1' inventory.json 3 How to remove keys from JSONjq can remove keys from JSON objects. This outputs the JSON object without the deleted key. Suppose the following JSON is saved as cheeses.json.{ \"maroilles\": \"stinky\", \"goat\": \"mild\" } jq can remove keys as follows leaving just wonderful stinky cheese.jq 'del(.goat)' cheeses.json { \"maroilles\": \"stinky\" } How to map valuesjq can map values and perform an operation on each one. In the following example each item in an array is mapped and has two subtracted.echo '[12,14,15]' | jq 'map(.-2)' [ 10, 12, 13 ] How to wrangle JSON how you wantjq has many more advanced features to help manipulating and wrangling JSON however you want to. For more run man jq.Further readingjq project pagejq manualjq is sed for JSONbash that JSON (jq)Parsing JSON with jqHave an update or suggestion for this article? You can edit it here and send me a pull request.TagsRecent Posts "
        ],
        "story_type": "Normal",
        "url_raw": "https://shapeshed.com/jq-json/",
        "comments.comment_id": [21859335, 21860107],
        "comments.comment_author": ["dang", "aasasd"],
        "comments.comment_descendants": [0, 5],
        "comments.comment_time": [
          "2019-12-22T20:11:17Z",
          "2019-12-22T22:34:41Z"
        ],
        "comments.comment_text": [
          "Thread from 2016: <a href=\"https://news.ycombinator.com/item?id=13090604\" rel=\"nofollow\">https://news.ycombinator.com/item?id=13090604</a><p>2015: <a href=\"https://news.ycombinator.com/item?id=9446980\" rel=\"nofollow\">https://news.ycombinator.com/item?id=9446980</a><p>2014: <a href=\"https://news.ycombinator.com/item?id=7895076\" rel=\"nofollow\">https://news.ycombinator.com/item?id=7895076</a><p>2013: <a href=\"https://news.ycombinator.com/item?id=5734683\" rel=\"nofollow\">https://news.ycombinator.com/item?id=5734683</a><p>2012: <a href=\"https://news.ycombinator.com/item?id=4985250\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4985250</a><p><a href=\"https://news.ycombinator.com/item?id=4679933\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4679933</a><p>(for the curious)",
          "Every time I try to do something involved with jq (i.e. more than a couple commands deep) and inevitably end up wandering through the manual, I get a persistent feeling that jq could be cleaned up quite a bit. That it either has structure throughout that I fail to grasp, or that it's really rather disorganized and convoluted in places. This is exacerbated by me never being able to find anything in the manual without skimming half of it each single time.<p>E.g.:<p>Filtering and transforming objects by reaching a few levels deeper in them seems to be way easier to slap together in Python's list comprehensions or some functional notation. Stuff like [{ * * x, qwe: x.qwe*2} for x in a.b if x.y.z == 1].<p>Add nested lists to the above, and I can spend another fifteen minutes writing a one-liner (i.e. `a` is a list of objects with lists in some field).<p>I once had to deal with a conditional structure where a field might be one object or a list of them. Hoo boy.<p>Every time I want to check the number of entries my filters print out, I need to wrap them in an array. Repeat this a couple dozen times during writing a complex query. Weirdly, this is where strict structure gets in the way―and while I understand the reasoning, I feel that something could be done for such basic need. (Like, aggregate filters should be able to aggregate instead of acting on individual items? Maybe it's already in the language, but I'm not in the mood to go over the manual again.)<p>Variables seem to be bolted on as an afterthought, or at least the syntax doesn't exactly accommodate them. Meanwhile, they're necessary to implement some filters omitted in the language. Compare that to Lisp's simple `(let)`. IIRC the manual also says that jq has some semblance of functions, but I'm afraid to think about them with this syntax.<p>I like the idea a lot, but execution not so much. Frankly I'll probably end up throwing together a script in Lumo or something, that will accept Lisp expressions and feed JSON structure to them. (I'd use Fennel, but JSON has actual null while Lua... doesn't.)<p>Btw, I have pretty much the same sentiment about Git. Git structures, great. Git tools, oy vey. Maybe I need Lisp for Git, or at least Python for Git."
        ],
        "id": "8f13f462-28e3-469c-b994-29bf671a1fb6",
        "url_text": "HomePostsContactLast updated Saturday, Nov 16, 2019A series of how to examples on using jq, a command-line JSON processorEstimated reading time: 4 minutesTable of contentsHow to pretty print JSONHow to use pipes with jqHow to find a key and valueHow to find items in an arrayHow to combine filtersHow to transform JSONHow to transform data values within JSONHow to remove keys from JSONHow to map valuesHow to wrangle JSON how you wantFurther readingjq is a fantastic command-line JSON processor. It plays nice with UNIX pipes and offers extensive functionality for interrogating, manipulating and working with JSON file.How to pretty print JSONjq can do a lot but probably the highest frequency use for most users is to pretty print JSON either from a file or after a network call. Suppose that we have a file names.json containing the following json.[{\"id\": 1, \"name\": \"Arthur\", \"age\": \"21\"},{\"id\": 2, \"name\": \"Richard\", \"age\": \"32\"}] jq can pretty print this file using the . filter. This takes the entire input and sends it to standard output. Unless told not to jq will pretty print making JSON readable.jq '.' names.json [ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] How to use pipes with jqBecause jq is UNIX friendly it is possible to pipe data in and out of it. This can be useful for using jq as a filter or interacting with other tools. In the following pipeline cat pipes the file into jq and this is piped onto less. This can be very useful for viewing large JSON files.cat names.json | jq '.' | less How to find a key and valueTo find a key and value jq can filter based on keys and return the value. Suppose we have the following simple JSON document saved as dog.json.{ \"name\": \"Buster\", \"breed\": \"Golden Retriever\", \"age\": \"4\", \"owner\": { \"name\": \"Sally\" }, \"likes\": [ \"bones\", \"balls\", \"dog biscuits\" ] } jq can retrieve values from this document by passing key names.jq '.name' \"Buster\" Multiple keys can by passed separated by commas.jq '.breed,.age' \"Golden Retriever\" \"4\" To search for nested objects chain values using the dot operator just as you would in JavaScript.jq '.owner.name' \"Sally\" How to find items in an arrayTo search for items in arrays use bracket syntax with the index starting at 0.jq '.likes[0]' \"bones\" Multiple elements of an array may also be returned.echo '[\"a\",\"b\",\"c\",\"d\",\"e\"]' | jq '.[2:4]' [ \"c\", \"d\" ] How to combine filtersjq can combine filters to search within a selection. For the following JSON document suppose that the names need to be filtered.[ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] This can be achieved with a pipe with the jq filter.jq '.[] | .name' names.json \"Arthur\" \"Richard\" How to transform JSONjq can be used for more than just reading values from a JSON object. It can also transform JSON into new data structures. Returning to the dog.json example earlier a new array can be created containing the name and likes as follows.jq '[.name, .likes[]]' dog.json [ \"Buster\", \"Bones\", \"balls\", \"dog biscuits\" ] This can be very useful in data transform pipelines to shift JSON data from one structure to another.How to transform data values within JSONjq can also operate on data within JSON objects. Suppose the following JSON file exists and is saved as inventory.json.{ \"eggs\": 2, \"cheese\": 1, \"milk\": 1 } jq can be used to perform basic arithmetic on number values.jq '.eggs + 1' inventory.json 3 How to remove keys from JSONjq can remove keys from JSON objects. This outputs the JSON object without the deleted key. Suppose the following JSON is saved as cheeses.json.{ \"maroilles\": \"stinky\", \"goat\": \"mild\" } jq can remove keys as follows leaving just wonderful stinky cheese.jq 'del(.goat)' cheeses.json { \"maroilles\": \"stinky\" } How to map valuesjq can map values and perform an operation on each one. In the following example each item in an array is mapped and has two subtracted.echo '[12,14,15]' | jq 'map(.-2)' [ 10, 12, 13 ] How to wrangle JSON how you wantjq has many more advanced features to help manipulating and wrangling JSON however you want to. For more run man jq.Further readingjq project pagejq manualjq is sed for JSONbash that JSON (jq)Parsing JSON with jqHave an update or suggestion for this article? You can edit it here and send me a pull request.TagsRecent Posts ",
        "_version_": 1718536554487480320
      },
      {
        "story_id": 19427332,
        "story_author": "an-tao",
        "story_descendants": 105,
        "story_score": 189,
        "story_time": "2019-03-19T01:47:40Z",
        "story_title": "Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
        "search": [
          "Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
          "https://github.com/an-tao/drogon",
          "English | | Overview Drogon is a C++14/17-based HTTP application framework. Drogon can be used to easily build various types of web application server programs using C++. Drogon is the name of a dragon in the American TV series \"Game of Thrones\" that I really like. Drogon is a cross-platform framework, It supports Linux, macOS, FreeBSD, OpenBSD, HaikuOS, and Windows. Its main features are as follows: Use a non-blocking I/O network lib based on epoll (kqueue under macOS/FreeBSD) to provide high-concurrency, high-performance network IO, please visit the TFB Tests Results for more details; Provide a completely asynchronous programming mode; Support Http1.0/1.1 (server side and client side); Based on template, a simple reflection mechanism is implemented to completely decouple the main program framework, controllers and views. Support cookies and built-in sessions; Support back-end rendering, the controller generates the data to the view to generate the Html page. Views are described by CSP template files, C++ codes are embedded into Html pages through CSP tags. And the drogon command-line tool automatically generates the C++ code files for compilation; Support view page dynamic loading (dynamic compilation and loading at runtime); Provide a convenient and flexible routing solution from the path to the controller handler; Support filter chains to facilitate the execution of unified logic (such as login verification, Http Method constraint verification, etc.) before handling HTTP requests; Support https (based on OpenSSL); Support WebSocket (server side and client side); Support JSON format request and response, very friendly to the Restful API application development; Support file download and upload; Support gzip, brotli compression transmission; Support pipelining; Provide a lightweight command line tool, drogon_ctl, to simplify the creation of various classes in Drogon and the generation of view code; Support non-blocking I/O based asynchronously reading and writing database (PostgreSQL and MySQL(MariaDB) database); Support asynchronously reading and writing sqlite3 database based on thread pool; Support Redis with asynchronous reading and writing; Support ARM Architecture; Provide a convenient lightweight ORM implementation that supports for regular object-to-database bidirectional mapping; Support plugins which can be installed by the configuration file at load time; Support AOP with build-in joinpoints. Support C++ coroutines A very simple example Unlike most C++ frameworks, the main program of the drogon application can be kept clean and simple. Drogon uses a few tricks to decouple controllers from the main program. The routing settings of controllers can be done through macros or configuration file. Below is the main program of a typical drogon application: #include <drogon/drogon.h> using namespace drogon; int main() { app().setLogPath(\"./\") .setLogLevel(trantor::Logger::kWarn) .addListener(\"0.0.0.0\", 80) .setThreadNum(16) .enableRunAsDaemon() .run(); } It can be further simplified by using configuration file as follows: #include <drogon/drogon.h> using namespace drogon; int main() { app().loadConfigFile(\"./config.json\").run(); } Drogon provides some interfaces for adding controller logic directly in the main() function, for example, user can register a handler like this in Drogon: app().registerHandler(\"/test?username={name}\", [](const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback, const std::string &name) { Json::Value json; json[\"result\"]=\"ok\"; json[\"message\"]=std::string(\"hello,\")+name; auto resp=HttpResponse::newHttpJsonResponse(json); callback(resp); }, {Get,\"LoginFilter\"}); While such interfaces look intuitive, they are not suitable for complex business logic scenarios. Assuming there are tens or even hundreds of handlers that need to be registered in the framework, isn't it a better practice to implement them separately in their respective classes? So unless your logic is very simple, we don't recommend using above interfaces. Instead, we can create an HttpSimpleController as follows: /// The TestCtrl.h file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class TestCtrl:public drogon::HttpSimpleController<TestCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN PATH_ADD(\"/test\",Get); PATH_LIST_END }; /// The TestCtrl.cc file #include \"TestCtrl.h\" void TestCtrl::asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) { //write your application logic here auto resp = HttpResponse::newHttpResponse(); resp->setBody(\"<p>Hello, world!</p>\"); resp->setExpiredTime(0); callback(resp); } Most of the above programs can be automatically generated by the command line tool drogon_ctl provided by drogon (The command is drogon_ctl create controller TestCtrl). All the user needs to do is add their own business logic. In the example, the controller returns a Hello, world! string when the client accesses the http://ip/test URL. For JSON format response, we create the controller as follows: /// The header file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class JsonCtrl : public drogon::HttpSimpleController<JsonCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN //list path definitions here; PATH_ADD(\"/json\", Get); PATH_LIST_END }; /// The source file #include \"JsonCtrl.h\" void JsonCtrl::asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) { Json::Value ret; ret[\"message\"] = \"Hello, World!\"; auto resp = HttpResponse::newHttpJsonResponse(ret); callback(resp); } Let's go a step further and create a demo RESTful API with the HttpController class, as shown below (Omit the source file): /// The header file #pragma once #include <drogon/HttpController.h> using namespace drogon; namespace api { namespace v1 { class User : public drogon::HttpController<User> { public: METHOD_LIST_BEGIN //use METHOD_ADD to add your custom processing function here; METHOD_ADD(User::getInfo, \"/{id}\", Get); //path is /api/v1/User/{arg1} METHOD_ADD(User::getDetailInfo, \"/{id}/detailinfo\", Get); //path is /api/v1/User/{arg1}/detailinfo METHOD_ADD(User::newUser, \"/{name}\", Post); //path is /api/v1/User/{arg1} METHOD_LIST_END //your declaration of processing function maybe like this: void getInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void getDetailInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void newUser(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, std::string &&userName); public: User() { LOG_DEBUG << \"User constructor!\"; } }; } // namespace v1 } // namespace api As you can see, users can use the HttpController to map paths and parameters at the same time. This is a very convenient way to create a RESTful API application. In addition, you can also find that all handler interfaces are in asynchronous mode, where the response is returned by a callback object. This design is for performance reasons because in asynchronous mode the drogon application can handle a large number of concurrent requests with a small number of threads. After compiling all of the above source files, we get a very simple web application. This is a good start. For more information, please visit the wiki or DocsForge Contributions Every contribution is welcome. Please refer to the contribution guidelines for more information. "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/an-tao/drogon",
        "comments.comment_id": [19427737, 19427925],
        "comments.comment_author": ["shakna", "KirinDave"],
        "comments.comment_descendants": [1, 8],
        "comments.comment_time": [
          "2019-03-19T03:06:56Z",
          "2019-03-19T03:45:52Z"
        ],
        "comments.comment_text": [
          "2 small things.<p>a) Show me the code to start with, don't send me digging for it. A SimpleController example as the very first thing would help give a feel for the project, and makes me more likely to consider the project.<p>b) If there's an easier way (like the drogon_ctl utility at the start of Quickstart [0]), show that first, and the more detailed way second.<p>Other than that, it looks great. I've used libmicrohttpd a few times, so a bit less of an overhead always looks great.<p>[0] <a href=\"https://github.com/an-tao/drogon/wiki/quick-start\" rel=\"nofollow\">https://github.com/an-tao/drogon/wiki/quick-start</a>",
          "This is cool, and I like it. Very Haskell like, which is a compliment in my book.<p>But one thing that surprises me is that folks are essentially sleeping on HTTP/2. HTTP/2 is just a hell of a lot better in most every dimension. It's better for handshake latency, it's better for bandwidth in most cases, it's better for eliminating excess SSL overhead and also, it's kinda easier to write client libraries for, because it's so much simpler (although the parallel and concurrent nature of connections will challenge a lot of programmers).<p>It's not bad to see a new contender in this space, but it's surprising that it isn't http/2 first. Is there a good reason for this? It's busted through 90% support on caniuse, so it's hard to make an argument that adoption holds it back."
        ],
        "id": "a112525e-9084-46ed-9e92-8fe23217ccb1",
        "url_text": "English | | Overview Drogon is a C++14/17-based HTTP application framework. Drogon can be used to easily build various types of web application server programs using C++. Drogon is the name of a dragon in the American TV series \"Game of Thrones\" that I really like. Drogon is a cross-platform framework, It supports Linux, macOS, FreeBSD, OpenBSD, HaikuOS, and Windows. Its main features are as follows: Use a non-blocking I/O network lib based on epoll (kqueue under macOS/FreeBSD) to provide high-concurrency, high-performance network IO, please visit the TFB Tests Results for more details; Provide a completely asynchronous programming mode; Support Http1.0/1.1 (server side and client side); Based on template, a simple reflection mechanism is implemented to completely decouple the main program framework, controllers and views. Support cookies and built-in sessions; Support back-end rendering, the controller generates the data to the view to generate the Html page. Views are described by CSP template files, C++ codes are embedded into Html pages through CSP tags. And the drogon command-line tool automatically generates the C++ code files for compilation; Support view page dynamic loading (dynamic compilation and loading at runtime); Provide a convenient and flexible routing solution from the path to the controller handler; Support filter chains to facilitate the execution of unified logic (such as login verification, Http Method constraint verification, etc.) before handling HTTP requests; Support https (based on OpenSSL); Support WebSocket (server side and client side); Support JSON format request and response, very friendly to the Restful API application development; Support file download and upload; Support gzip, brotli compression transmission; Support pipelining; Provide a lightweight command line tool, drogon_ctl, to simplify the creation of various classes in Drogon and the generation of view code; Support non-blocking I/O based asynchronously reading and writing database (PostgreSQL and MySQL(MariaDB) database); Support asynchronously reading and writing sqlite3 database based on thread pool; Support Redis with asynchronous reading and writing; Support ARM Architecture; Provide a convenient lightweight ORM implementation that supports for regular object-to-database bidirectional mapping; Support plugins which can be installed by the configuration file at load time; Support AOP with build-in joinpoints. Support C++ coroutines A very simple example Unlike most C++ frameworks, the main program of the drogon application can be kept clean and simple. Drogon uses a few tricks to decouple controllers from the main program. The routing settings of controllers can be done through macros or configuration file. Below is the main program of a typical drogon application: #include <drogon/drogon.h> using namespace drogon; int main() { app().setLogPath(\"./\") .setLogLevel(trantor::Logger::kWarn) .addListener(\"0.0.0.0\", 80) .setThreadNum(16) .enableRunAsDaemon() .run(); } It can be further simplified by using configuration file as follows: #include <drogon/drogon.h> using namespace drogon; int main() { app().loadConfigFile(\"./config.json\").run(); } Drogon provides some interfaces for adding controller logic directly in the main() function, for example, user can register a handler like this in Drogon: app().registerHandler(\"/test?username={name}\", [](const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback, const std::string &name) { Json::Value json; json[\"result\"]=\"ok\"; json[\"message\"]=std::string(\"hello,\")+name; auto resp=HttpResponse::newHttpJsonResponse(json); callback(resp); }, {Get,\"LoginFilter\"}); While such interfaces look intuitive, they are not suitable for complex business logic scenarios. Assuming there are tens or even hundreds of handlers that need to be registered in the framework, isn't it a better practice to implement them separately in their respective classes? So unless your logic is very simple, we don't recommend using above interfaces. Instead, we can create an HttpSimpleController as follows: /// The TestCtrl.h file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class TestCtrl:public drogon::HttpSimpleController<TestCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN PATH_ADD(\"/test\",Get); PATH_LIST_END }; /// The TestCtrl.cc file #include \"TestCtrl.h\" void TestCtrl::asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) { //write your application logic here auto resp = HttpResponse::newHttpResponse(); resp->setBody(\"<p>Hello, world!</p>\"); resp->setExpiredTime(0); callback(resp); } Most of the above programs can be automatically generated by the command line tool drogon_ctl provided by drogon (The command is drogon_ctl create controller TestCtrl). All the user needs to do is add their own business logic. In the example, the controller returns a Hello, world! string when the client accesses the http://ip/test URL. For JSON format response, we create the controller as follows: /// The header file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class JsonCtrl : public drogon::HttpSimpleController<JsonCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN //list path definitions here; PATH_ADD(\"/json\", Get); PATH_LIST_END }; /// The source file #include \"JsonCtrl.h\" void JsonCtrl::asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) { Json::Value ret; ret[\"message\"] = \"Hello, World!\"; auto resp = HttpResponse::newHttpJsonResponse(ret); callback(resp); } Let's go a step further and create a demo RESTful API with the HttpController class, as shown below (Omit the source file): /// The header file #pragma once #include <drogon/HttpController.h> using namespace drogon; namespace api { namespace v1 { class User : public drogon::HttpController<User> { public: METHOD_LIST_BEGIN //use METHOD_ADD to add your custom processing function here; METHOD_ADD(User::getInfo, \"/{id}\", Get); //path is /api/v1/User/{arg1} METHOD_ADD(User::getDetailInfo, \"/{id}/detailinfo\", Get); //path is /api/v1/User/{arg1}/detailinfo METHOD_ADD(User::newUser, \"/{name}\", Post); //path is /api/v1/User/{arg1} METHOD_LIST_END //your declaration of processing function maybe like this: void getInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void getDetailInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void newUser(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, std::string &&userName); public: User() { LOG_DEBUG << \"User constructor!\"; } }; } // namespace v1 } // namespace api As you can see, users can use the HttpController to map paths and parameters at the same time. This is a very convenient way to create a RESTful API application. In addition, you can also find that all handler interfaces are in asynchronous mode, where the response is returned by a callback object. This design is for performance reasons because in asynchronous mode the drogon application can handle a large number of concurrent requests with a small number of threads. After compiling all of the above source files, we get a very simple web application. This is a good start. For more information, please visit the wiki or DocsForge Contributions Every contribution is welcome. Please refer to the contribution guidelines for more information. ",
        "_version_": 1718536461178896384
      },
      {
        "story_id": 21240641,
        "story_author": "sharkdp",
        "story_descendants": 25,
        "story_score": 96,
        "story_time": "2019-10-13T14:50:12Z",
        "story_title": "Show HN: Hyperfine – a command-line benchmarking tool",
        "search": [
          "Show HN: Hyperfine – a command-line benchmarking tool",
          "https://github.com/sharkdp/hyperfine",
          " A command-line benchmarking tool. Demo: Benchmarking fd and find: Features Statistical analysis across multiple runs. Support for arbitrary shell commands. Constant feedback about the benchmark progress and current estimates. Warmup runs can be executed before the actual benchmark. Cache-clearing commands can be set up before each timing run. Statistical outlier detection to detect interference from other programs and caching effects. Export results to various formats: CSV, JSON, Markdown, AsciiDoc. Parameterized benchmarks (e.g. vary the number of threads). Cross-platform Usage Basic benchmark To run a benchmark, you can simply call hyperfine <command>.... The argument(s) can be any shell command. For example: Hyperfine will automatically determine the number of runs to perform for each command. By default, it will perform at least 10 benchmarking runs. To change this, you can use the -m/--min-runs option: hyperfine --min-runs 5 'sleep 0.2' 'sleep 3.2' Warmup runs and preparation commands If the program execution time is limited by disk I/O, the benchmarking results can be heavily influenced by disk caches and whether they are cold or warm. If you want to run the benchmark on a warm cache, you can use the -w/--warmup option to perform a certain number of program executions before the actual benchmark: hyperfine --warmup 3 'grep -R TODO *' Conversely, if you want to run the benchmark for a cold cache, you can use the -p/--prepare option to run a special command before each timing run. For example, to clear harddisk caches on Linux, you can run sync; echo 3 | sudo tee /proc/sys/vm/drop_caches To use this specific command with Hyperfine, call sudo -v to temporarily gain sudo permissions and then call: hyperfine --prepare 'sync; echo 3 | sudo tee /proc/sys/vm/drop_caches' 'grep -R TODO *' Parameterized benchmarks If you want to run a benchmark where only a single parameter is varied (say, the number of threads), you can use the -P/--parameter-scan option and call: hyperfine --prepare 'make clean' --parameter-scan num_threads 1 12 'make -j {num_threads}' This also works with decimal numbers. The -D/--parameter-step-size option can be used to control the step size: hyperfine --parameter-scan delay 0.3 0.7 -D 0.2 'sleep {delay}' This runs sleep 0.3, sleep 0.5 and sleep 0.7. Shell functions and aliases If you are using bash, you can export shell functions to directly benchmark them with hyperfine: $ my_function() { sleep 1; } $ export -f my_function $ hyperfine my_function If you are using a different shell, or if you want to benchmark shell aliases, you may try to put them in a separate file: echo 'my_function() { sleep 1 }' > /tmp/my_function.sh echo 'alias my_alias=\"sleep 1\"' > /tmp/my_alias.sh hyperfine 'source /tmp/my_function.sh; eval my_function' hyperfine 'source /tmp/my_alias.sh; eval my_alias' Export results Hyperfine has multiple options for exporting benchmark results: CSV, JSON, Markdown (see --help text for details). To export results to Markdown, for example, you can use the --export-markdown option that will create tables like this: Command Mean [s] Min [s] Max [s] Relative find . -iregex '.*[0-9]\\.jpg$' 2.275 0.046 2.243 2.397 9.79 0.22 find . -iname '*[0-9].jpg' 1.427 0.026 1.405 1.468 6.14 0.13 fd -HI '.*[0-9]\\.jpg$' 0.232 0.002 0.230 0.236 1.00 The JSON output is useful if you want to analyze the benchmark results in more detail. See the scripts/ folder for some examples. Installation On Ubuntu Download the appropriate .deb package from the Release page and install it via dpkg: wget https://github.com/sharkdp/hyperfine/releases/download/v1.12.0/hyperfine_1.12.0_amd64.deb sudo dpkg -i hyperfine_1.12.0_amd64.deb On Fedora On Fedora, hyperfine can be installed from the official repositories: On Alpine Linux On Alpine Linux, hyperfine can be installed from the official repositories: On Arch Linux On Arch Linux, hyperfine can be installed from the official repositories: On Funtoo Linux On Funtoo Linux, hyperfine can be installed from core-kit: emerge app-benchmarks/hyperfine On NixOS On NixOS, hyperfine can be installed from the official repositories: On Void Linux Hyperfine can be installed via xbps xbps-install -S hyperfine On macOS Hyperfine can be installed via Homebrew: Or you can install using MacPorts: sudo port selfupdate sudo port install hyperfine On FreeBSD Hyperfine can be installed via pkg: On OpenBSD With conda Hyperfine can be installed via conda from the conda-forge channel: conda install -c conda-forge hyperfine With cargo (Linux, macOS, Windows) Hyperfine can be installed via cargo: Make sure that you use Rust 1.46 or higher. From binaries (Linux, macOS, Windows) Download the corresponding archive from the Release page. Alternative tools Hyperfine is inspired by bench. Integration with other tools Chronologer is a tool that uses hyperfine to visualize changes in benchmark timings across your Git history. Make sure to check out the scripts folder in this repository for a set of tools to work with hyperfine benchmark results. Origin of the name The name hyperfine was chosen in reference to the hyperfine levels of caesium 133 which play a crucial role in the definition of our base unit of time the second. License hyperfine is dual-licensed under the terms of the MIT License and the Apache License 2.0. See the LICENSE-APACHE and LICENSE-MIT files for details. "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/sharkdp/hyperfine",
        "comments.comment_id": [21240684, 21251645],
        "comments.comment_author": ["sharkdp", "kqr"],
        "comments.comment_descendants": [1, 5],
        "comments.comment_time": [
          "2019-10-13T14:57:05Z",
          "2019-10-14T19:09:09Z"
        ],
        "comments.comment_text": [
          "I have submitted \"hyperfine\" 1.5 years ago when it just came out. Since then, the program has gained functionality (statistical outlier detection, result export, parametrized benchmarks) and maturity.<p>Old discussion: <a href=\"https://news.ycombinator.com/item?id=16193225\" rel=\"nofollow\">https://news.ycombinator.com/item?id=16193225</a><p>Looking forward to your feedback!",
          "Most -- nearly all -- benchmarking tools like this work from a normality assumption, i.e. assume that results follow the normal distribution, or is close to it. Some do this on blind faith, others argue from the CLT that \"with infinite samples, the mean is normally distributed, so surely it must be also with finite number of samples, at least a little?\"<p>In fact, performance numbers (latencies) often follow a heavy-tailed distribution. For these, you need a literal shitload of samples to get even a slightly normal mean. For these, the sample mean, the sample variance, the sample centiles -- they all severely underestimate the true values.<p>What's worse is when these tools start to remove \"outliers\". With a heavy-tailed distribution, the majority of samples don't contribute very much at all to the expectation. The strongest signal is found in the extreme values. The strongest signal is found in the stuff that is thrown out. The junk that's left is the noise, the stuff that doesn't tell you very much about what you're dealing with.<p>I stand firm in my belief that unless you can prove how CLT applies to your input distributions, you should not assume normality.<p>And if you don't know what you are doing, stop reporting means. Stop reporting centiles. Report the maximum value. That's a really boring thing to hear, but it is nearly always statistically  and analytically meaningful, so it is a good default."
        ],
        "id": "ff1d53d8-aadf-4db4-8227-4e1893326ee8",
        "url_text": " A command-line benchmarking tool. Demo: Benchmarking fd and find: Features Statistical analysis across multiple runs. Support for arbitrary shell commands. Constant feedback about the benchmark progress and current estimates. Warmup runs can be executed before the actual benchmark. Cache-clearing commands can be set up before each timing run. Statistical outlier detection to detect interference from other programs and caching effects. Export results to various formats: CSV, JSON, Markdown, AsciiDoc. Parameterized benchmarks (e.g. vary the number of threads). Cross-platform Usage Basic benchmark To run a benchmark, you can simply call hyperfine <command>.... The argument(s) can be any shell command. For example: Hyperfine will automatically determine the number of runs to perform for each command. By default, it will perform at least 10 benchmarking runs. To change this, you can use the -m/--min-runs option: hyperfine --min-runs 5 'sleep 0.2' 'sleep 3.2' Warmup runs and preparation commands If the program execution time is limited by disk I/O, the benchmarking results can be heavily influenced by disk caches and whether they are cold or warm. If you want to run the benchmark on a warm cache, you can use the -w/--warmup option to perform a certain number of program executions before the actual benchmark: hyperfine --warmup 3 'grep -R TODO *' Conversely, if you want to run the benchmark for a cold cache, you can use the -p/--prepare option to run a special command before each timing run. For example, to clear harddisk caches on Linux, you can run sync; echo 3 | sudo tee /proc/sys/vm/drop_caches To use this specific command with Hyperfine, call sudo -v to temporarily gain sudo permissions and then call: hyperfine --prepare 'sync; echo 3 | sudo tee /proc/sys/vm/drop_caches' 'grep -R TODO *' Parameterized benchmarks If you want to run a benchmark where only a single parameter is varied (say, the number of threads), you can use the -P/--parameter-scan option and call: hyperfine --prepare 'make clean' --parameter-scan num_threads 1 12 'make -j {num_threads}' This also works with decimal numbers. The -D/--parameter-step-size option can be used to control the step size: hyperfine --parameter-scan delay 0.3 0.7 -D 0.2 'sleep {delay}' This runs sleep 0.3, sleep 0.5 and sleep 0.7. Shell functions and aliases If you are using bash, you can export shell functions to directly benchmark them with hyperfine: $ my_function() { sleep 1; } $ export -f my_function $ hyperfine my_function If you are using a different shell, or if you want to benchmark shell aliases, you may try to put them in a separate file: echo 'my_function() { sleep 1 }' > /tmp/my_function.sh echo 'alias my_alias=\"sleep 1\"' > /tmp/my_alias.sh hyperfine 'source /tmp/my_function.sh; eval my_function' hyperfine 'source /tmp/my_alias.sh; eval my_alias' Export results Hyperfine has multiple options for exporting benchmark results: CSV, JSON, Markdown (see --help text for details). To export results to Markdown, for example, you can use the --export-markdown option that will create tables like this: Command Mean [s] Min [s] Max [s] Relative find . -iregex '.*[0-9]\\.jpg$' 2.275 0.046 2.243 2.397 9.79 0.22 find . -iname '*[0-9].jpg' 1.427 0.026 1.405 1.468 6.14 0.13 fd -HI '.*[0-9]\\.jpg$' 0.232 0.002 0.230 0.236 1.00 The JSON output is useful if you want to analyze the benchmark results in more detail. See the scripts/ folder for some examples. Installation On Ubuntu Download the appropriate .deb package from the Release page and install it via dpkg: wget https://github.com/sharkdp/hyperfine/releases/download/v1.12.0/hyperfine_1.12.0_amd64.deb sudo dpkg -i hyperfine_1.12.0_amd64.deb On Fedora On Fedora, hyperfine can be installed from the official repositories: On Alpine Linux On Alpine Linux, hyperfine can be installed from the official repositories: On Arch Linux On Arch Linux, hyperfine can be installed from the official repositories: On Funtoo Linux On Funtoo Linux, hyperfine can be installed from core-kit: emerge app-benchmarks/hyperfine On NixOS On NixOS, hyperfine can be installed from the official repositories: On Void Linux Hyperfine can be installed via xbps xbps-install -S hyperfine On macOS Hyperfine can be installed via Homebrew: Or you can install using MacPorts: sudo port selfupdate sudo port install hyperfine On FreeBSD Hyperfine can be installed via pkg: On OpenBSD With conda Hyperfine can be installed via conda from the conda-forge channel: conda install -c conda-forge hyperfine With cargo (Linux, macOS, Windows) Hyperfine can be installed via cargo: Make sure that you use Rust 1.46 or higher. From binaries (Linux, macOS, Windows) Download the corresponding archive from the Release page. Alternative tools Hyperfine is inspired by bench. Integration with other tools Chronologer is a tool that uses hyperfine to visualize changes in benchmark timings across your Git history. Make sure to check out the scripts folder in this repository for a set of tools to work with hyperfine benchmark results. Origin of the name The name hyperfine was chosen in reference to the hyperfine levels of caesium 133 which play a crucial role in the definition of our base unit of time the second. License hyperfine is dual-licensed under the terms of the MIT License and the Apache License 2.0. See the LICENSE-APACHE and LICENSE-MIT files for details. ",
        "_version_": 1718536532512473088
      },
      {
        "story_id": 20062064,
        "story_author": "lelf",
        "story_descendants": 25,
        "story_score": 193,
        "story_time": "2019-05-31T15:57:18Z",
        "story_title": "Semantic: Parsing, analyzing, and comparing source code across many languages",
        "search": [
          "Semantic: Parsing, analyzing, and comparing source code across many languages",
          "https://github.com/github/semantic",
          "semantic is a Haskell library and command line tool for parsing, analyzing, and comparing source code. In a hurry? Check out our documentation of example uses for the semantic command line tool. Table of Contents Usage Language support Development Technology and architecture Licensing Usage Run semantic --help for complete list of up-to-date options. Parse Usage: semantic parse [--sexpression | (--json-symbols|--symbols) | --proto-symbols | --show | --quiet] [FILES...] Generate parse trees for path(s) Available options: --sexpression Output s-expression parse trees (default) --json-symbols,--symbols Output JSON symbol list --proto-symbols Output protobufs symbol list --show Output using the Show instance (debug only, format subject to change without notice) --quiet Don't produce output, but show timing stats -h,--help Show this help text Language support Language Parse AST Symbols Stack graphs Ruby JavaScript TypeScript Python Go PHP Java JSON JSX TSX CodeQL Haskell Used for code navigation on github.com. Supported Partial support Under development - N/A Development semantic requires at least GHC 8.10.1 and Cabal 3.0. We strongly recommend using ghcup to sandbox GHC versions, as GHC packages installed through your OS's package manager may not install statically-linked versions of the GHC boot libraries. semantic currently builds only on Unix systems; users of other operating systems may wish to use the Docker images. We use cabal's Nix-style local builds for development. To get started quickly: git clone git@github.com:github/semantic.git cd semantic script/bootstrap cabal v2-build all cabal v2-run semantic:test cabal v2-run semantic:semantic -- --help You can also use the Bazel build system for development. To learn more about Bazel and why it might give you a better development experience, check the build documentation. git clone git@github.com:github/semantic.git cd semantic script/bootstrap-bazel bazel build //... stack as a build tool is not officially supported; there is unofficial stack.yaml support available, though we cannot make guarantees as to its stability. Technology and architecture Architecturally, semantic: Generates per-language Haskell syntax types based on tree-sitter grammar definitions. Reads blobs from a filesystem or provided via a protocol buffer request. Returns blobs or performs analysis. Renders output in one of many supported formats. Throughout its lifestyle, semantic has leveraged a number of interesting algorithms and techniques, including: Myers' algorithm (SES) as described in the paper An O(ND) Difference Algorithm and Its Variations RWS as described in the paper RWS-Diff: Flexible and Efficient Change Detection in Hierarchical Data. Open unions and data types la carte. An implementation of Abstracting Definitional Interpreters extended to work with an la carte representation of syntax terms. Contributions Contributions are welcome! Please see our contribution guidelines and our code of conduct for details on how to participate in our community. Licensing Semantic is licensed under the MIT license. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/github/semantic",
        "url_text": "semantic is a Haskell library and command line tool for parsing, analyzing, and comparing source code. In a hurry? Check out our documentation of example uses for the semantic command line tool. Table of Contents Usage Language support Development Technology and architecture Licensing Usage Run semantic --help for complete list of up-to-date options. Parse Usage: semantic parse [--sexpression | (--json-symbols|--symbols) | --proto-symbols | --show | --quiet] [FILES...] Generate parse trees for path(s) Available options: --sexpression Output s-expression parse trees (default) --json-symbols,--symbols Output JSON symbol list --proto-symbols Output protobufs symbol list --show Output using the Show instance (debug only, format subject to change without notice) --quiet Don't produce output, but show timing stats -h,--help Show this help text Language support Language Parse AST Symbols Stack graphs Ruby JavaScript TypeScript Python Go PHP Java JSON JSX TSX CodeQL Haskell Used for code navigation on github.com. Supported Partial support Under development - N/A Development semantic requires at least GHC 8.10.1 and Cabal 3.0. We strongly recommend using ghcup to sandbox GHC versions, as GHC packages installed through your OS's package manager may not install statically-linked versions of the GHC boot libraries. semantic currently builds only on Unix systems; users of other operating systems may wish to use the Docker images. We use cabal's Nix-style local builds for development. To get started quickly: git clone git@github.com:github/semantic.git cd semantic script/bootstrap cabal v2-build all cabal v2-run semantic:test cabal v2-run semantic:semantic -- --help You can also use the Bazel build system for development. To learn more about Bazel and why it might give you a better development experience, check the build documentation. git clone git@github.com:github/semantic.git cd semantic script/bootstrap-bazel bazel build //... stack as a build tool is not officially supported; there is unofficial stack.yaml support available, though we cannot make guarantees as to its stability. Technology and architecture Architecturally, semantic: Generates per-language Haskell syntax types based on tree-sitter grammar definitions. Reads blobs from a filesystem or provided via a protocol buffer request. Returns blobs or performs analysis. Renders output in one of many supported formats. Throughout its lifestyle, semantic has leveraged a number of interesting algorithms and techniques, including: Myers' algorithm (SES) as described in the paper An O(ND) Difference Algorithm and Its Variations RWS as described in the paper RWS-Diff: Flexible and Efficient Change Detection in Hierarchical Data. Open unions and data types la carte. An implementation of Abstracting Definitional Interpreters extended to work with an la carte representation of syntax terms. Contributions Contributions are welcome! Please see our contribution guidelines and our code of conduct for details on how to participate in our community. Licensing Semantic is licensed under the MIT license. ",
        "comments.comment_id": [20062368, 20062741],
        "comments.comment_author": ["anentropic", "stcredzero"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-05-31T16:27:42Z",
          "2019-05-31T17:01:44Z"
        ],
        "comments.comment_text": [
          "Looks very interesting - would benefit from showing some examples and/or use cases",
          "The late 20th century, early 00's version:<p><a href=\"http://www.program-transformation.org/Transform/CodeCrawler\" rel=\"nofollow\">http://www.program-transformation.org/Transform/CodeCrawler</a><p>(And MOOSE)"
        ],
        "id": "649642de-3037-44fa-a1f9-419d010a2e4d",
        "_version_": 1718536487511785472
      },
      {
        "story_id": 21196177,
        "story_author": "vinnyglennon",
        "story_descendants": 1,
        "story_score": 13,
        "story_time": "2019-10-08T19:29:58Z",
        "story_title": "I wrote a command-line Ruby program to manage EC2 instances for me",
        "search": [
          "I wrote a command-line Ruby program to manage EC2 instances for me",
          "https://www.codewithjason.com/wrote-command-line-ruby-program-manage-ec2-instances/",
          "Why I did this Heroku is great, but not in 100% of cases When I want to quickly deploy a Rails application, my go-to choice is Heroku. Im a big fan of the idea that I can just run heroku create and have a production application online in just a matter of seconds. Unfortunately, Heroku isnt always a desirable option. If Im just messing around, I dont usually want to pay for Heroku features, but I also dont always want my dynos to fall asleep after 30 minutes like on the free tier. (Im aware that there are ways around this but I dont necessarily want to deal with the hassle of all that.) Also, sometimes I want finer control than what Heroku provides. I want to be closer to the metal with the ability to directly manage my EC2 instances, RDS instances, and other AWS services. Sometimes I desire this for cost reasons. Sometimes I just want to learn what I think is the valuable developer skill of knowing how to manage AWS infrastructure. Unfortunately, using AWS by itself isnt very easy. Setting up Rails on bare EC2 is a time-consuming and brain-consuming hassle Getting a Rails app standing up on AWS is pretty hard and time-consuming. Im actually not even going to get into Rails-related stuff in this post because even the small task of getting an EC2 instance up and runningwithout no additional software installed on that instanceis a lot harder than I think it should be, and theres a lot to discuss and improve just inside that step. Just to briefly illustrate what a pain in the ass it is to get an EC2 instance launched and to SSH into it, here are the steps. The steps that follow are the command-line steps. I find the AWS GUI console steps roughly equally painful. 1. Use the AWS CLI create-key-pair command to create a key pair. This step is necessary for later when I want to SSH into my instance. 2. Think of a name for the key pair and save it somewhere. Thinking of a name might seem like a trivially small hurdle, but every tiny bit of mental friction adds up. I dont want to have to think of a name, and I dont want to have to think about where to put the file (even if that means just remembering that I want to put the key in ~/.ssh, which is the most likely case. 3. Use the run-instances command, using an AMI ID (AMI == Amazon Machine Image) and passing in my key name. Now I have to go look up the run-instances (because I sure as hell dont remember it) and, look up my AMI ID, and remember what my key name is. (If you dont know what an AMI ID is, thats what determines whether the instance will be Ubuntu, Amazon Linux, Windows, etc.) 4. Use the describe-instances command to find out the public DNS name of the instance I just launched. This means I either have to search the JSON response of describe-instances for the PublicDnsName entry or apply a filter. Just like with every AWS CLI command, Id have to go look up the exact syntax for this. 5. Run the ssh command, passing in my instances DNS and the path to my key. This step is probably the easiest, although it took me a long time to commit the exact ssh -i syntax to memory. For the record, the command is ssh -i ~/.ssh/my_key.pem ubuntu@mypublicdns.com. Its a small pain in the ass to have to look up the public DNS for my instance again and remember whether my EC2 user is going to be ubuntu or ec2-user (it depends on what AMI I used). My goals for my AWS command-line tool All this fuckery was a big hassle so I decided to write my own command-line tool to manage EC2 instances. I call the tool Exosuit. You can actually try it out yourself by following these instructions. There were four specific capabilities I wanted Exosuit to have. Launch an instance By running bin/exo launch, it should launch an EC2 instance for me. It should assume I want Ubuntu. It should let me know when the instance is ready, and what its instance ID and public DNS are. SSH into an instance I should be able to run bin/exo ssh, get prompted for which instance I want to SSH into, and then get SSHd into that instance. List all running instances I should be able to run bin/exo instances to see all my running instances. It should show the instance ID and public DNS for each. Terminate instances I should be able to run bin/exo terminate which will show me all my instance IDs and allow me to select one or more of them for termination. How I did it Side note: when I first wrote this, I forgot that the AWS SDK for Ruby existed, so I reinvented some wheels. Whoops. After I wrote this I refactored the project to use AWS SDK instead of shell out to AWS CLI. For brevity Ill focus on the bin/exo launch command. Using the AW CLI run-instances command The AWS CLI command for launching an instance looks like this: aws ec2 run-instances \\ --count 1 \\ --image-id ami-05c1fa8df71875112 \\ --instance-type t2.micro \\ --key-name normal-quiet-carrot \\ --profile personal Hopefully most of these flags are self-explanatory. You might wonder where the key name of normal-quiet-carrot came from. When the bin/exo launch command is run, Exosuit asks Is there a file defined at .exosuit/config.yml that contains a key pair name and path? If not, create that file, create a new key pair with a random phrase for a name, and save the name and path to that file. Heres what my .exosuit/config.yml looks like: --- aws_profile_name: personal key_pair: name: normal-quiet-carrot path: \"~/.ssh/normal-quiet-carrot.pem\" The aws_profile_name is something that I imagine most users arent likely to need. I personally happen to have multiple AWS accounts, so its necessary for me to send a --profile flag when using AWS CLI commands so AWS knows which account of mine to use. If a profile isnt specified in .exosuit/config.yml, Exosuit will just leave the --profile flag off and everything will still work fine. Abstracting the run-instances command Once I had coded Exosuit to construct a few different AWS CLI commands (e.g. run-instances, terminate-instances), I noticed that things were getting a little repetitive. Most troubling, I had to always remember to include the --profile flag (just as I would if I were typing all this on the command line manually), and I didnt always remember to do so. In those cases my command would get sent to the wrong account. Thats bad. So I created an abstraction called AWSCommand. Heres what a usage of it looks like: command = AWSCommand.new( :run_instances, count: 1, image_id: IMAGE_ID, instance_type: INSTANCE_TYPE, key_name: key_pair.name ) JSON.parse(command.run) You can probably see the resemblance it bears to the bare run-instances usage. Note the conspicuous absence of the profile flag, which is now automatically included every single time. Listening for launch success One of my least favorite things about manually launching EC2 instances is having to check periodically to see when theyve started running. So I wanted Exosuit to tell me when my EC2 instance was running. I achieved this by writing a loop that hits AWS once per second, checking the state of my new instance each time. module Exosuit def self.launch_instance response = Instance.launch(self.key_pair) instance_id = response['Instances'][0]['InstanceId'] print \"Launching instance #{instance_id}...\" while true sleep(1) print '.' instance = Instance.find(instance_id) if instance && instance.running? puts break end end puts 'Instance is now running' puts \"Public DNS: #{instance.public_dns_name}\" end end You might wonder what Instance.find and instance.running? do. The Instance.find method will run the aws ec2 describe-instances command, parse the JSON response, then grab the relevant JSON data for whatever instance_id I passed to it. The return value is an instance of the Instance class. When an instance of Instance is instantiated, an instance variable gets set (pardon all the instances) with all the JSON data for that instance that was returned by the AWS CLI. The instance.running? method simply looks at that JSON data (which has since been converted to a Ruby hash) and checks to see what the value of ['State']['Name'] is. Heres an abbreviated version of the Instance class for reference. module Exosuit class Instance def initialize(info) @info = info end def state @info['State']['Name'] end def running? state == 'running' end end end (By the way, all the Exosuit code is available on GitHub if youd like to take a look.) Success notification As you can see from the code a couple snippets above, Exosuit lets me know once my instances has entered a running state. At this point I can run bin/exo ssh, bin/exo instances or bin/exo terminate to mess with my instance(s) as I please. Demo video Heres a small sample of Exosuit in action: Try it out yourself If youd like to try out Exosuit, just visit the Getting Started with Exosuit guide. If you think this idea is cool and useful, please let me know by opening a GitHub issue for a feature youd like to see, or tweeting at me, or simply starring the project on GitHub so I can gage interest. I hope you enjoyed this explanation and I look forward to sharing the next steps I take with this project. "
        ],
        "story_type": "Normal",
        "url_raw": "https://www.codewithjason.com/wrote-command-line-ruby-program-manage-ec2-instances/",
        "comments.comment_id": [21196575],
        "comments.comment_author": ["rpmisms"],
        "comments.comment_descendants": [0],
        "comments.comment_time": ["2019-10-08T20:04:40Z"],
        "comments.comment_text": [
          "Dude, thank you so much. This looks like exactly what I need for a project I'm on."
        ],
        "id": "29303460-7239-41eb-b2d0-b298a0118d61",
        "url_text": "Why I did this Heroku is great, but not in 100% of cases When I want to quickly deploy a Rails application, my go-to choice is Heroku. Im a big fan of the idea that I can just run heroku create and have a production application online in just a matter of seconds. Unfortunately, Heroku isnt always a desirable option. If Im just messing around, I dont usually want to pay for Heroku features, but I also dont always want my dynos to fall asleep after 30 minutes like on the free tier. (Im aware that there are ways around this but I dont necessarily want to deal with the hassle of all that.) Also, sometimes I want finer control than what Heroku provides. I want to be closer to the metal with the ability to directly manage my EC2 instances, RDS instances, and other AWS services. Sometimes I desire this for cost reasons. Sometimes I just want to learn what I think is the valuable developer skill of knowing how to manage AWS infrastructure. Unfortunately, using AWS by itself isnt very easy. Setting up Rails on bare EC2 is a time-consuming and brain-consuming hassle Getting a Rails app standing up on AWS is pretty hard and time-consuming. Im actually not even going to get into Rails-related stuff in this post because even the small task of getting an EC2 instance up and runningwithout no additional software installed on that instanceis a lot harder than I think it should be, and theres a lot to discuss and improve just inside that step. Just to briefly illustrate what a pain in the ass it is to get an EC2 instance launched and to SSH into it, here are the steps. The steps that follow are the command-line steps. I find the AWS GUI console steps roughly equally painful. 1. Use the AWS CLI create-key-pair command to create a key pair. This step is necessary for later when I want to SSH into my instance. 2. Think of a name for the key pair and save it somewhere. Thinking of a name might seem like a trivially small hurdle, but every tiny bit of mental friction adds up. I dont want to have to think of a name, and I dont want to have to think about where to put the file (even if that means just remembering that I want to put the key in ~/.ssh, which is the most likely case. 3. Use the run-instances command, using an AMI ID (AMI == Amazon Machine Image) and passing in my key name. Now I have to go look up the run-instances (because I sure as hell dont remember it) and, look up my AMI ID, and remember what my key name is. (If you dont know what an AMI ID is, thats what determines whether the instance will be Ubuntu, Amazon Linux, Windows, etc.) 4. Use the describe-instances command to find out the public DNS name of the instance I just launched. This means I either have to search the JSON response of describe-instances for the PublicDnsName entry or apply a filter. Just like with every AWS CLI command, Id have to go look up the exact syntax for this. 5. Run the ssh command, passing in my instances DNS and the path to my key. This step is probably the easiest, although it took me a long time to commit the exact ssh -i syntax to memory. For the record, the command is ssh -i ~/.ssh/my_key.pem ubuntu@mypublicdns.com. Its a small pain in the ass to have to look up the public DNS for my instance again and remember whether my EC2 user is going to be ubuntu or ec2-user (it depends on what AMI I used). My goals for my AWS command-line tool All this fuckery was a big hassle so I decided to write my own command-line tool to manage EC2 instances. I call the tool Exosuit. You can actually try it out yourself by following these instructions. There were four specific capabilities I wanted Exosuit to have. Launch an instance By running bin/exo launch, it should launch an EC2 instance for me. It should assume I want Ubuntu. It should let me know when the instance is ready, and what its instance ID and public DNS are. SSH into an instance I should be able to run bin/exo ssh, get prompted for which instance I want to SSH into, and then get SSHd into that instance. List all running instances I should be able to run bin/exo instances to see all my running instances. It should show the instance ID and public DNS for each. Terminate instances I should be able to run bin/exo terminate which will show me all my instance IDs and allow me to select one or more of them for termination. How I did it Side note: when I first wrote this, I forgot that the AWS SDK for Ruby existed, so I reinvented some wheels. Whoops. After I wrote this I refactored the project to use AWS SDK instead of shell out to AWS CLI. For brevity Ill focus on the bin/exo launch command. Using the AW CLI run-instances command The AWS CLI command for launching an instance looks like this: aws ec2 run-instances \\ --count 1 \\ --image-id ami-05c1fa8df71875112 \\ --instance-type t2.micro \\ --key-name normal-quiet-carrot \\ --profile personal Hopefully most of these flags are self-explanatory. You might wonder where the key name of normal-quiet-carrot came from. When the bin/exo launch command is run, Exosuit asks Is there a file defined at .exosuit/config.yml that contains a key pair name and path? If not, create that file, create a new key pair with a random phrase for a name, and save the name and path to that file. Heres what my .exosuit/config.yml looks like: --- aws_profile_name: personal key_pair: name: normal-quiet-carrot path: \"~/.ssh/normal-quiet-carrot.pem\" The aws_profile_name is something that I imagine most users arent likely to need. I personally happen to have multiple AWS accounts, so its necessary for me to send a --profile flag when using AWS CLI commands so AWS knows which account of mine to use. If a profile isnt specified in .exosuit/config.yml, Exosuit will just leave the --profile flag off and everything will still work fine. Abstracting the run-instances command Once I had coded Exosuit to construct a few different AWS CLI commands (e.g. run-instances, terminate-instances), I noticed that things were getting a little repetitive. Most troubling, I had to always remember to include the --profile flag (just as I would if I were typing all this on the command line manually), and I didnt always remember to do so. In those cases my command would get sent to the wrong account. Thats bad. So I created an abstraction called AWSCommand. Heres what a usage of it looks like: command = AWSCommand.new( :run_instances, count: 1, image_id: IMAGE_ID, instance_type: INSTANCE_TYPE, key_name: key_pair.name ) JSON.parse(command.run) You can probably see the resemblance it bears to the bare run-instances usage. Note the conspicuous absence of the profile flag, which is now automatically included every single time. Listening for launch success One of my least favorite things about manually launching EC2 instances is having to check periodically to see when theyve started running. So I wanted Exosuit to tell me when my EC2 instance was running. I achieved this by writing a loop that hits AWS once per second, checking the state of my new instance each time. module Exosuit def self.launch_instance response = Instance.launch(self.key_pair) instance_id = response['Instances'][0]['InstanceId'] print \"Launching instance #{instance_id}...\" while true sleep(1) print '.' instance = Instance.find(instance_id) if instance && instance.running? puts break end end puts 'Instance is now running' puts \"Public DNS: #{instance.public_dns_name}\" end end You might wonder what Instance.find and instance.running? do. The Instance.find method will run the aws ec2 describe-instances command, parse the JSON response, then grab the relevant JSON data for whatever instance_id I passed to it. The return value is an instance of the Instance class. When an instance of Instance is instantiated, an instance variable gets set (pardon all the instances) with all the JSON data for that instance that was returned by the AWS CLI. The instance.running? method simply looks at that JSON data (which has since been converted to a Ruby hash) and checks to see what the value of ['State']['Name'] is. Heres an abbreviated version of the Instance class for reference. module Exosuit class Instance def initialize(info) @info = info end def state @info['State']['Name'] end def running? state == 'running' end end end (By the way, all the Exosuit code is available on GitHub if youd like to take a look.) Success notification As you can see from the code a couple snippets above, Exosuit lets me know once my instances has entered a running state. At this point I can run bin/exo ssh, bin/exo instances or bin/exo terminate to mess with my instance(s) as I please. Demo video Heres a small sample of Exosuit in action: Try it out yourself If youd like to try out Exosuit, just visit the Getting Started with Exosuit guide. If you think this idea is cool and useful, please let me know by opening a GitHub issue for a feature youd like to see, or tweeting at me, or simply starring the project on GitHub so I can gage interest. I hope you enjoyed this explanation and I look forward to sharing the next steps I take with this project. ",
        "_version_": 1718536530743525376
      }
    ]
  }
}
