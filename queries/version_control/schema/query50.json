{
  "responseHeader": {
    "status": 0,
    "QTime": 8
  },
  "response": {
    "numFound": 10049,
    "start": 0,
    "numFoundExact": true,
    "docs": [
      {
        "story_id": 20745393,
        "story_author": "ingve",
        "story_descendants": 355,
        "story_score": 357,
        "story_time": "2019-08-20T10:39:18Z",
        "story_title": "Sunsetting Mercurial Support in Bitbucket",
        "search": [
          "Sunsetting Mercurial Support in Bitbucket",
          "https://bitbucket.org/blog/sunsetting-mercurial-support-in-bitbucket",
          "[Update Aug 26, 2020] All hg repos have now been disabled and cannot be accessed.[Update July 1, 2020] Today, mercurial repositories, snippets, and wikis will turn to read-only mode. After July 8th, 2020 they will no longer be accessible. The version control software market has evolved a lot since Bitbucket began in 2008. When we launched, centralized version control was the norm and we only supported Mercurial repos. But Git adoption has grown over the years to become the default system, helping teams of all sizes work faster as they become more distributed.As we surpass 10 million registered users on the platform, we're at a point in our growth where we are conducting a deeper evaluation of the market and how we can best support our users going forward. After much consideration, we've decided to remove Mercurial support from Bitbucket Cloud and its API. Mercurial features and repositories will be officially deprecated on July 1, 2020.Read on to learn more about this decision, the important timelines, and get migration resources and support. The timeline and how this may affect your team Here are the key dates as we sunset Mercurial functionality: February 1, 2020: users will no longer be able to create new Mercurial repositories [Extended] July 1, 2020: users will not be able to use Mercurial features. All hg repos, wikis, and snippets will be in read-only mode. Heres why were focusing on Git This wasnt an easy decision, and Mercurial will always have a special place in Bitbuckets history. DevOps adoption has skyrocketed over the last decade and our customers are adopting this new way of working at an exponential rate. In this time, Bitbucket has steadily grown from being just a version control management tool to being a place to manage the entire software development lifecycle. And there's always more work to be done. This year we will concentrate on building deeper integrations to enhance automation and collaboration. Our improvements will make it even easier and safer to plan, code, test, and deploy all from within Bitbucket. Building quality features requires intense focus, and supporting two version control systems means splitting focus doubling shipping time and technical overhead. With Git being the more popularly used tool, Mercurial runs the risk of overlooked issues as we scale. According to a Stack Overflow Developer Survey, almost 90% of developers use Git, while Mercurial is the least popular version control system with only about 3% developer adoption. In fact, Mercurial usage on Bitbucket is steadily declining, and the percentage of new Bitbucket users choosing Mercurial has fallen to less than 1%. This deprecation will enable us to focus on building the best possible experience for our users. How to migrate and export We recommend that teams migrate their existing Mercurial repos to Git. There are various Git conversion tools in the market, including hg-fast-export and hg-git mercurial plugin. We are happy to support your migration, and you can find a discussion about available options in our dedicated Community thread. If you prefer to continue using the Mercurial system, there are a number of free and paid Mercurial hosting services. We realize that there is no one-size-fits-all solution. That's why we've created the following resources to best equip you with the knowledge and tools for a seamless transition: A Community thread to discuss conversion tools, migration, tips, and offer troubleshooting help A Git tutorial that covers anywhere from the basics of creating pull requests to rebasing and Git hooks We want to thank all the loyal users who have grown with us over the years. We look forward to this new focus on our roadmap and to introducing exciting new features. "
        ],
        "story_type": "Normal",
        "url_raw": "https://bitbucket.org/blog/sunsetting-mercurial-support-in-bitbucket",
        "url_text": "[Update Aug 26, 2020] All hg repos have now been disabled and cannot be accessed.[Update July 1, 2020] Today, mercurial repositories, snippets, and wikis will turn to read-only mode. After July 8th, 2020 they will no longer be accessible. The version control software market has evolved a lot since Bitbucket began in 2008. When we launched, centralized version control was the norm and we only supported Mercurial repos. But Git adoption has grown over the years to become the default system, helping teams of all sizes work faster as they become more distributed.As we surpass 10 million registered users on the platform, we're at a point in our growth where we are conducting a deeper evaluation of the market and how we can best support our users going forward. After much consideration, we've decided to remove Mercurial support from Bitbucket Cloud and its API. Mercurial features and repositories will be officially deprecated on July 1, 2020.Read on to learn more about this decision, the important timelines, and get migration resources and support. The timeline and how this may affect your team Here are the key dates as we sunset Mercurial functionality: February 1, 2020: users will no longer be able to create new Mercurial repositories [Extended] July 1, 2020: users will not be able to use Mercurial features. All hg repos, wikis, and snippets will be in read-only mode. Heres why were focusing on Git This wasnt an easy decision, and Mercurial will always have a special place in Bitbuckets history. DevOps adoption has skyrocketed over the last decade and our customers are adopting this new way of working at an exponential rate. In this time, Bitbucket has steadily grown from being just a version control management tool to being a place to manage the entire software development lifecycle. And there's always more work to be done. This year we will concentrate on building deeper integrations to enhance automation and collaboration. Our improvements will make it even easier and safer to plan, code, test, and deploy all from within Bitbucket. Building quality features requires intense focus, and supporting two version control systems means splitting focus doubling shipping time and technical overhead. With Git being the more popularly used tool, Mercurial runs the risk of overlooked issues as we scale. According to a Stack Overflow Developer Survey, almost 90% of developers use Git, while Mercurial is the least popular version control system with only about 3% developer adoption. In fact, Mercurial usage on Bitbucket is steadily declining, and the percentage of new Bitbucket users choosing Mercurial has fallen to less than 1%. This deprecation will enable us to focus on building the best possible experience for our users. How to migrate and export We recommend that teams migrate their existing Mercurial repos to Git. There are various Git conversion tools in the market, including hg-fast-export and hg-git mercurial plugin. We are happy to support your migration, and you can find a discussion about available options in our dedicated Community thread. If you prefer to continue using the Mercurial system, there are a number of free and paid Mercurial hosting services. We realize that there is no one-size-fits-all solution. That's why we've created the following resources to best equip you with the knowledge and tools for a seamless transition: A Community thread to discuss conversion tools, migration, tips, and offer troubleshooting help A Git tutorial that covers anywhere from the basics of creating pull requests to rebasing and Git hooks We want to thank all the loyal users who have grown with us over the years. We look forward to this new focus on our roadmap and to introducing exciting new features. ",
        "comments.comment_id": [20745989, 20746077],
        "comments.comment_author": ["garganzol", "dragonsh"],
        "comments.comment_descendants": [4, 19],
        "comments.comment_time": [
          "2019-08-20T12:13:11Z",
          "2019-08-20T12:22:53Z"
        ],
        "comments.comment_text": [
          "It's funny to see how the whole world concentrates on this Git thing, while there is a treasure trove called Mercurial.<p>Mercurial was made for humans. It is seriously convenient and productive. Something I cannot say about Git, which more reminds me of an adhoc job.<p>I use both Git and Mercurial on daily basis. But my preference goes to Mercurial: it is just more sane in a big way. It is clearly a piece of art and love.",
          "It's very sad to see bitbucket dropping mercurial support. Now only Facebook and volunteers are keeping mercurial alive. \nSometimes technically better architecture and user interface lose to a non user friendly hard solutions due to inertia of mass adoption.<p>So a lesson in Software development is similar to betamax and VHS, so marketing is still a winner over technically superior architecture and ease of use. GitHub successfully marketed git, so git and GitHub are synonymous for most developers. Now majority of open source projects are reliant on a single proprietary solution Github by Microsoft, for managing code and project. Can understand the difficulty of bitbucket, when Python language itself moved out of mercurial due to the same inertia.<p>Hopefully gitlab can come out with mercurial support to migrate projects using it from bitbucket.<p>For people who believe in self hosted solution can install Kallithea (<a href=\"https://kallithea-scm.org\" rel=\"nofollow\">https://kallithea-scm.org</a>) or Rhodecode open source edition. Kallithea is used by Unity engine to manage their source code internally with mercurial."
        ],
        "id": "57cda62c-a990-4246-96fd-2efab14a3720",
        "_version_": 1718536513273200640
      },
      {
        "story_id": 21055373,
        "story_author": "gk1",
        "story_descendants": 5,
        "story_score": 26,
        "story_time": "2019-09-24T00:47:23Z",
        "story_title": "Building a Modern CI/CD Pipeline in the Serverless Era with GitOps",
        "search": [
          "Building a Modern CI/CD Pipeline in the Serverless Era with GitOps",
          "https://aws.amazon.com/blogs/aws/building-a-modern-ci-cd-pipeline-in-the-serverless-era-with-gitops/",
          "Guest post by AWS Community Hero Shimon Tolts, CTO and co-founder at Datree.io. He specializes in developer tools and infrastructure, running a company that is 100% serverless. In recent years, there was a major transition in the way you build and ship software. This was mainly around microservices, splitting code into small components, using infrastructure as code, and using Git as the single source of truth that glues it all together. In this post, I discuss the transition and the different steps of modern software development to showcase the possible solutions for the serverless world. In addition, I list useful tools that were designed for this era. What is serverless? Before I dive into the wonderful world of serverless development and tooling, heres what I mean by serverless. The AWS website talks about four main benefits: No server management. Flexible scaling. Pay for value. Automated high availability. To me, serverless is any infrastructure that you dont have to manage and scale yourself. At my company Datree.io, we run 95% of our workload on AWS Fargate and 5% on AWS Lambda. We are a serverless company; we have zero Amazon EC2 instances in our AWS account. For more information, see the following: Datree.io case study Migrating to AWS ECS Fargate in production CON320: Operational Excellence w/ Containerized Workloads Using AWS Fargate (re:Invent 2018) What is GitOps? Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. According to Luis Faceira, a CI/CD consultant, GitOps is a way of working. You might look at it as an approach in which everything starts and ends with Git. Here are some key concepts: Git as the SINGLE source of truth of a system Git as the SINGLE place where we operate (create, change and destroy) ALL environments ALL changes are observable/verifiable. How you built software before the cloud Back in the waterfall pre-cloud era, you used to have separate teams for development, testing, security, operations, monitoring, and so on. Nowadays, in most organizations, there is a transition to full developer autonomy and developers owning the entire production path. The developer is the King or Queen :) Those teams (Ops/Security/IT/etc) used to be gatekeepers to validate and control every developer change. Now they have become more of a satellite unit that drives policy and sets best practices and standards. They are no longer the production bottleneck, so they provide organization-wide platforms and enablement solutions. Everything is codified With the transition into full developer ownership of the entire pipeline, developers automated everything. We have more code than ever, and processes that used to be manual are now described in code. This is a good transition, in my opinion. Here are some of the benefits: Automation: By storing all things as code, everything can be automated, reused, and re-created in moments. Immutable: If anything goes wrong, create it again from the stored configuration. Versioning: Changes can be applied and reverted, and are tracked to a single user who made the change. GitOps: Git has become the single source of truth The second major transition is that now everything is in one place! Git is the place where all of the code is stored and where all operations are initiated. Whether its testing, building, packaging, or releasing, nowadays everything is triggered through pull requests. This is amplified by the codification of everything. Useful tools in the serverless era There are many useful tools in the market, here is a list of ones that were designed for serverless. Code Always store your code in a source control system. In recent years, more and more functions are codified, such as, BI, ops, security, and AI. For new developers, it is not always obvious that they should use source control for some functionality. GitHub AWS CodeCommit GitLab BitBucket Build and test The most common mistake I see is manually configuring build jobs in the GUI. This might be good for a small POC but it is not scalable. You should have your job codified and inside your Git repository. Here are some tools to help with building and testing: AWS CodeBuild CodeFresh GitHub Actions Jenkins-x CircleCI TravisCI Security and governance When working in a serverless way, you end up having many Git repos. The number of code packages can be overwhelming. The demand for unified code standards remains as it was but now it is much harder to enforce it on top of your R&D org. Here are some tools that might help you with the challenge: Snyk Datree PureSec Aqua Protego Bundle and release Building a serverless application is connecting microservices into one unit. For example, you might be using Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. Instead of configuring each one separately, you should use a bundler to hold the configuration in one place. That allows for easy versioning and replication of the app for several environments. Here are a couple of bundlers: Serverless Framework AWS Serverless Application Model (AWS SAM) Package When working with many different serverless components, you should create small packages of tools to be able to import across different Lambda functions. You can use a language-specific store like npm or RubyGems, or use a more holistic solution. Here are several package artifact stores that allow hosting for multiple programming languages: GitHub Package Registry Jfrog Artifactory Sonatype Nexus Monitor This part is especially tricky when working with serverless applications, as everything is split into small pieces. Its important to use monitoring tools that support this mode of work. Here are some tools that can handle serverless: Rookout Amazon CloudWatch Epsagon Lumigo NewRelic DataDog Summary The serverless era brings many transitions along with it like a codification of the entire pipeline and Git being the single source of truth. This doesnt mean that the same problems that we use to have like security, logging and more disappeared, you should continue addressing them and leveraging tools that enable you to focus on your business. "
        ],
        "story_type": "Normal",
        "url_raw": "https://aws.amazon.com/blogs/aws/building-a-modern-ci-cd-pipeline-in-the-serverless-era-with-gitops/",
        "comments.comment_id": [21057480, 21063347],
        "comments.comment_author": ["tracer4201", "acdha"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-09-24T07:36:59Z",
          "2019-09-24T18:46:43Z"
        ],
        "comments.comment_text": [
          "Lol at the image where before microservices, it was a single monolithic application.<p>FaaS has its use cases but this “serverless for every solution” or 100% serverless marketing is annoying and NOT customer centric (Amazon said they were earths most customer centric company).",
          "This post was disappointing when it first ran: I was expecting some content after the basic intro but then it’s just a couple of saved Google searches with no discussion or analysis. It would have been a lot more interesting if they’d discussed anything about the trade offs of the different services or what they liked about a particular combination."
        ],
        "id": "fc3cb876-2ae6-4d1b-8f25-c0c2268d6bb3",
        "url_text": "Guest post by AWS Community Hero Shimon Tolts, CTO and co-founder at Datree.io. He specializes in developer tools and infrastructure, running a company that is 100% serverless. In recent years, there was a major transition in the way you build and ship software. This was mainly around microservices, splitting code into small components, using infrastructure as code, and using Git as the single source of truth that glues it all together. In this post, I discuss the transition and the different steps of modern software development to showcase the possible solutions for the serverless world. In addition, I list useful tools that were designed for this era. What is serverless? Before I dive into the wonderful world of serverless development and tooling, heres what I mean by serverless. The AWS website talks about four main benefits: No server management. Flexible scaling. Pay for value. Automated high availability. To me, serverless is any infrastructure that you dont have to manage and scale yourself. At my company Datree.io, we run 95% of our workload on AWS Fargate and 5% on AWS Lambda. We are a serverless company; we have zero Amazon EC2 instances in our AWS account. For more information, see the following: Datree.io case study Migrating to AWS ECS Fargate in production CON320: Operational Excellence w/ Containerized Workloads Using AWS Fargate (re:Invent 2018) What is GitOps? Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. According to Luis Faceira, a CI/CD consultant, GitOps is a way of working. You might look at it as an approach in which everything starts and ends with Git. Here are some key concepts: Git as the SINGLE source of truth of a system Git as the SINGLE place where we operate (create, change and destroy) ALL environments ALL changes are observable/verifiable. How you built software before the cloud Back in the waterfall pre-cloud era, you used to have separate teams for development, testing, security, operations, monitoring, and so on. Nowadays, in most organizations, there is a transition to full developer autonomy and developers owning the entire production path. The developer is the King or Queen :) Those teams (Ops/Security/IT/etc) used to be gatekeepers to validate and control every developer change. Now they have become more of a satellite unit that drives policy and sets best practices and standards. They are no longer the production bottleneck, so they provide organization-wide platforms and enablement solutions. Everything is codified With the transition into full developer ownership of the entire pipeline, developers automated everything. We have more code than ever, and processes that used to be manual are now described in code. This is a good transition, in my opinion. Here are some of the benefits: Automation: By storing all things as code, everything can be automated, reused, and re-created in moments. Immutable: If anything goes wrong, create it again from the stored configuration. Versioning: Changes can be applied and reverted, and are tracked to a single user who made the change. GitOps: Git has become the single source of truth The second major transition is that now everything is in one place! Git is the place where all of the code is stored and where all operations are initiated. Whether its testing, building, packaging, or releasing, nowadays everything is triggered through pull requests. This is amplified by the codification of everything. Useful tools in the serverless era There are many useful tools in the market, here is a list of ones that were designed for serverless. Code Always store your code in a source control system. In recent years, more and more functions are codified, such as, BI, ops, security, and AI. For new developers, it is not always obvious that they should use source control for some functionality. GitHub AWS CodeCommit GitLab BitBucket Build and test The most common mistake I see is manually configuring build jobs in the GUI. This might be good for a small POC but it is not scalable. You should have your job codified and inside your Git repository. Here are some tools to help with building and testing: AWS CodeBuild CodeFresh GitHub Actions Jenkins-x CircleCI TravisCI Security and governance When working in a serverless way, you end up having many Git repos. The number of code packages can be overwhelming. The demand for unified code standards remains as it was but now it is much harder to enforce it on top of your R&D org. Here are some tools that might help you with the challenge: Snyk Datree PureSec Aqua Protego Bundle and release Building a serverless application is connecting microservices into one unit. For example, you might be using Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. Instead of configuring each one separately, you should use a bundler to hold the configuration in one place. That allows for easy versioning and replication of the app for several environments. Here are a couple of bundlers: Serverless Framework AWS Serverless Application Model (AWS SAM) Package When working with many different serverless components, you should create small packages of tools to be able to import across different Lambda functions. You can use a language-specific store like npm or RubyGems, or use a more holistic solution. Here are several package artifact stores that allow hosting for multiple programming languages: GitHub Package Registry Jfrog Artifactory Sonatype Nexus Monitor This part is especially tricky when working with serverless applications, as everything is split into small pieces. Its important to use monitoring tools that support this mode of work. Here are some tools that can handle serverless: Rookout Amazon CloudWatch Epsagon Lumigo NewRelic DataDog Summary The serverless era brings many transitions along with it like a codification of the entire pipeline and Git being the single source of truth. This doesnt mean that the same problems that we use to have like security, logging and more disappeared, you should continue addressing them and leveraging tools that enable you to focus on your business. ",
        "_version_": 1718536524950142976
      },
      {
        "story_id": 19083580,
        "story_author": "briansack35",
        "story_descendants": 3,
        "story_score": 27,
        "story_time": "2019-02-05T07:32:49Z",
        "story_title": "What is GitOps and why you should know about it",
        "search": [
          "What is GitOps and why you should know about it",
          "https://venturebeat.com/2019/02/02/what-is-gitops-and-why-you-should-know-about-it/",
          "February 2, 2019 12:10 PM Developing technology is far more expansive than just writing code. Technology teams need to consider a number of different aspects of the companys technology stack, including its compute infrastructure, storage, development pipeline, security, and more. Companies often purchase various third-party products to complete their technology stack so they dont have to build it all on their own. Each of these tools comes with some form of management console or dashboard that enables companies to tailor that specific tool to the their own needs as well as integrate it within their product. The onboarding, configuration, and ongoing management will likely occur directly from within that console. In fact, managing these third-party products and dashboards actually becomes a significant portion of a technology teams workload and today often falls under the responsibility of the DevOps team. Above: Dashboard overdose: IT organisations are bombarded by dashboards across their entire tech stack To increase automation and efficiency, some companies now are bypassing dashboards and instead preferring to manage these products directly from within their code base. These products are then deployed and configured not as another management console, but rather by developers, as code, in the companys Git (GitHub, GitLab, or Bitbucket). This trend has been coined GitOps or Git Centric, as increasingly products are being codified and deployed within Git environments. In a somewhat unusual twist of events, Git has become the tool that software companies use to manage their entire technology stack. This means they are deploying their infrastructure, security, and the automation of their development pipeline all from within their Git. Technology blog Dzone put it nicely: Increasingly, companies are using Git as their single source of truth for their code, configurations and infrastructure. All their business logic lives in Git, and automated processes can turn their Git repositories into built and deployed software Weve entered the world of GitOps. A great example of a Git Centric tool is Terraform by Hashicorp. Terraform allows developers to define their infrastructure as code within their Git environment and to consistently and automatically spin up servers with a consistent set of configurations in a scalable manner. It also helps teams of developers understand the underlying server configurations by simply looking at the code rather than having to enter their cloud providers dashboard. The GitOps market is growing exceptionally fast, with companies announcing the codification of their products on a daily basis. At the same time, it is giving developers a perhaps unconsidered and threatening gateway into their production environments. So well also see tools likeDatree.iothat provide automatic policy compliance checks for every code commit. (Disclosure: Datree.io is one of my firms investments). Other tools are emerging to allow GitOps manage and automate more than application deployments. For example, Gitpitch enables teams to build, edit, change, and publish slide presentations. In other words, developers can build entire slide decks and presentations from within their Git provider. They can then manage changes to these presentations and collaborate around them just as they would over their application code base. And theres Waffle.io, a project management tool built within Git for engineering teams. By operating from within Git, Waffle can automatically determine what features and tasks developers have been completed and what is still in progress. It can then automatically communicate status updates to the rest of the team so that everyone is on the same page. I believe Git is likely to become the de-facto tool for automating many company operations. Obviously, the first use cases are focused on the deployment and configuration of applications. But it can also be used effectively for other business processes. The image at the top of this story shows the beginnings of a GitOps landscape. Since we are still in the early days of this market, I may have missed a number of relevant companies. If so, please reach out to me at brian@tlv.partners and let me know. Brian Sack is on the investment team at TLV Partners, an early stage VC based in Tel Aviv, Israel.VentureBeat VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative technology and transact. Our site delivers essential information on data technologies and strategies to guide you as you lead your organizations. We invite you to become a member of our community, to access: up-to-date information on the subjects of interest to you our newsletters gated thought-leader content and discounted access to our prized events, such as Transform 2021: Learn More networking features, and more Become a member "
        ],
        "story_type": "Normal",
        "url_raw": "https://venturebeat.com/2019/02/02/what-is-gitops-and-why-you-should-know-about-it/",
        "comments.comment_id": [19087094, 19087577],
        "comments.comment_author": ["mameshini", "h668"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-02-05T16:56:14Z",
          "2019-02-05T17:47:34Z"
        ],
        "comments.comment_text": [
          "Great article, explains why Git can be used as the primary tool for interface between developers and infrastructure.",
          "Git is the bridge between development and production."
        ],
        "id": "17891c11-9899-4c05-b1f7-ee8b74a09dde",
        "url_text": "February 2, 2019 12:10 PM Developing technology is far more expansive than just writing code. Technology teams need to consider a number of different aspects of the companys technology stack, including its compute infrastructure, storage, development pipeline, security, and more. Companies often purchase various third-party products to complete their technology stack so they dont have to build it all on their own. Each of these tools comes with some form of management console or dashboard that enables companies to tailor that specific tool to the their own needs as well as integrate it within their product. The onboarding, configuration, and ongoing management will likely occur directly from within that console. In fact, managing these third-party products and dashboards actually becomes a significant portion of a technology teams workload and today often falls under the responsibility of the DevOps team. Above: Dashboard overdose: IT organisations are bombarded by dashboards across their entire tech stack To increase automation and efficiency, some companies now are bypassing dashboards and instead preferring to manage these products directly from within their code base. These products are then deployed and configured not as another management console, but rather by developers, as code, in the companys Git (GitHub, GitLab, or Bitbucket). This trend has been coined GitOps or Git Centric, as increasingly products are being codified and deployed within Git environments. In a somewhat unusual twist of events, Git has become the tool that software companies use to manage their entire technology stack. This means they are deploying their infrastructure, security, and the automation of their development pipeline all from within their Git. Technology blog Dzone put it nicely: Increasingly, companies are using Git as their single source of truth for their code, configurations and infrastructure. All their business logic lives in Git, and automated processes can turn their Git repositories into built and deployed software Weve entered the world of GitOps. A great example of a Git Centric tool is Terraform by Hashicorp. Terraform allows developers to define their infrastructure as code within their Git environment and to consistently and automatically spin up servers with a consistent set of configurations in a scalable manner. It also helps teams of developers understand the underlying server configurations by simply looking at the code rather than having to enter their cloud providers dashboard. The GitOps market is growing exceptionally fast, with companies announcing the codification of their products on a daily basis. At the same time, it is giving developers a perhaps unconsidered and threatening gateway into their production environments. So well also see tools likeDatree.iothat provide automatic policy compliance checks for every code commit. (Disclosure: Datree.io is one of my firms investments). Other tools are emerging to allow GitOps manage and automate more than application deployments. For example, Gitpitch enables teams to build, edit, change, and publish slide presentations. In other words, developers can build entire slide decks and presentations from within their Git provider. They can then manage changes to these presentations and collaborate around them just as they would over their application code base. And theres Waffle.io, a project management tool built within Git for engineering teams. By operating from within Git, Waffle can automatically determine what features and tasks developers have been completed and what is still in progress. It can then automatically communicate status updates to the rest of the team so that everyone is on the same page. I believe Git is likely to become the de-facto tool for automating many company operations. Obviously, the first use cases are focused on the deployment and configuration of applications. But it can also be used effectively for other business processes. The image at the top of this story shows the beginnings of a GitOps landscape. Since we are still in the early days of this market, I may have missed a number of relevant companies. If so, please reach out to me at brian@tlv.partners and let me know. Brian Sack is on the investment team at TLV Partners, an early stage VC based in Tel Aviv, Israel.VentureBeat VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative technology and transact. Our site delivers essential information on data technologies and strategies to guide you as you lead your organizations. We invite you to become a member of our community, to access: up-to-date information on the subjects of interest to you our newsletters gated thought-leader content and discounted access to our prized events, such as Transform 2021: Learn More networking features, and more Become a member ",
        "_version_": 1718536443855372290
      },
      {
        "story_id": 19100986,
        "story_author": "Boulth",
        "story_descendants": 79,
        "story_score": 289,
        "story_time": "2019-02-06T22:55:16Z",
        "story_title": "Sr.ht becomes Sourcehut",
        "search": [
          "Sr.ht becomes Sourcehut",
          "https://sourcehut.org/",
          "Welcome to sourcehut! This suite of open source tools is the software development platform you've been waiting for. We've taken the wisdom of the most successful open-source communities and turned it into a platform of efficient engineering tools. Absolutely no tracking or advertising All features work without JavaScript Many features work without an account The fastest & lightest software forge 100% free and open source software Sourcehut is currently available as a public alpha. What should I expect? \"Small internet\" protocols? The Plan 9 renaissance? Esoteric programming languages for music creation, and novel smartphone operating systems? These projects and more are waiting to be found on the sourcehut project index. Browse projects Hosted git repositories Public, private, and \"unlisted\" repositories Fine grained access control, including access for users without accounts First-class Mercurial support also available We've completely migrated our repo hosting, both git and Mercurial, to SourceHut. The speed, functionality, integrations, and minimal-yet-friendly UI makes it easy to use and work with. Peter Sanchez, Netlandish Inc. Powerful continuous integration Runs fully virtualised builds on various Linux distros and BSDs Submit ad-hoc jobs without pushing to your repository Post-build triggers for email, webhooks, etc Log in with SSH after build failures to investigate further This CI experience is leagues ahead of all others. Resubmitting builds and SSH'ing in is saving me multiple hours. Andrew Kelley, author of the Zig programming language Mailing lists & code review tools Patch review tools on the web Threaded, searchable mail archives Tools for working with third party mailing lists Powered by git send-email SourceHut mailing lists are the best thing since the invention of reviewing patches. Martijn Braam, postmarketOS developer Focused ticket tracking Actionable tasks only no discussions, questions, or duplicates Private bug reports and bug trackers for security issues Participation via email, with or without an account I think it is really convenient that you can send a plaintext email with your bug report, whether or not you have an account. Cadence Ember, author of Bibliogram Sophisticated account management & security PGP encrypted and signed emails from each service Two-factor authentication with TOTP Detailed audit logs of account activity Fine-grained third-party OAuth access controls I really appreciate the option to get encrypted mail with a PGP key that I provide why don't more companies have this?! Cadence Ember Markdown- and git-driven wikis Use git to version control and manage your wiki Use any organizational hierarchy you like, a flat wiki is not imposed Hosts the detailed sourcehut manual And more! Integrations with third-party services via dispatch.sr.ht Ad-hoc source code hosting via paste.sr.ht Static web hosting via srht.site "
        ],
        "story_type": "Normal",
        "url_raw": "https://sourcehut.org/",
        "url_text": "Welcome to sourcehut! This suite of open source tools is the software development platform you've been waiting for. We've taken the wisdom of the most successful open-source communities and turned it into a platform of efficient engineering tools. Absolutely no tracking or advertising All features work without JavaScript Many features work without an account The fastest & lightest software forge 100% free and open source software Sourcehut is currently available as a public alpha. What should I expect? \"Small internet\" protocols? The Plan 9 renaissance? Esoteric programming languages for music creation, and novel smartphone operating systems? These projects and more are waiting to be found on the sourcehut project index. Browse projects Hosted git repositories Public, private, and \"unlisted\" repositories Fine grained access control, including access for users without accounts First-class Mercurial support also available We've completely migrated our repo hosting, both git and Mercurial, to SourceHut. The speed, functionality, integrations, and minimal-yet-friendly UI makes it easy to use and work with. Peter Sanchez, Netlandish Inc. Powerful continuous integration Runs fully virtualised builds on various Linux distros and BSDs Submit ad-hoc jobs without pushing to your repository Post-build triggers for email, webhooks, etc Log in with SSH after build failures to investigate further This CI experience is leagues ahead of all others. Resubmitting builds and SSH'ing in is saving me multiple hours. Andrew Kelley, author of the Zig programming language Mailing lists & code review tools Patch review tools on the web Threaded, searchable mail archives Tools for working with third party mailing lists Powered by git send-email SourceHut mailing lists are the best thing since the invention of reviewing patches. Martijn Braam, postmarketOS developer Focused ticket tracking Actionable tasks only no discussions, questions, or duplicates Private bug reports and bug trackers for security issues Participation via email, with or without an account I think it is really convenient that you can send a plaintext email with your bug report, whether or not you have an account. Cadence Ember, author of Bibliogram Sophisticated account management & security PGP encrypted and signed emails from each service Two-factor authentication with TOTP Detailed audit logs of account activity Fine-grained third-party OAuth access controls I really appreciate the option to get encrypted mail with a PGP key that I provide why don't more companies have this?! Cadence Ember Markdown- and git-driven wikis Use git to version control and manage your wiki Use any organizational hierarchy you like, a flat wiki is not imposed Hosts the detailed sourcehut manual And more! Integrations with third-party services via dispatch.sr.ht Ad-hoc source code hosting via paste.sr.ht Static web hosting via srht.site ",
        "comments.comment_id": [19101220, 19101723],
        "comments.comment_author": ["niftich", "jordigh"],
        "comments.comment_descendants": [3, 6],
        "comments.comment_time": [
          "2019-02-06T23:23:36Z",
          "2019-02-07T00:23:06Z"
        ],
        "comments.comment_text": [
          "As of posting, the info on this site is very lean, but going forward sourcehut seems to be the name for the software suite of the service 'sr.ht'. The linkage is implied by the quote on the site that \"<i>sr.ht is a hosted instance of sourcehut provided for your convenience</i>\".<p>The impetus behind the branding clarification seems to be this HN thread [1]. For more history, see the the debut announcement [2] and its corresponding HN thread [3].<p>[1] <a href=\"https://news.ycombinator.com/item?id=18929709#18930413\" rel=\"nofollow\">https://news.ycombinator.com/item?id=18929709#18930413</a> [2] <a href=\"https://drewdevault.com/2018/11/15/sr.ht-general-availability.html\" rel=\"nofollow\">https://drewdevault.com/2018/11/15/sr.ht-general-availabilit...</a> [3] <a href=\"https://news.ycombinator.com/item?id=18458908\" rel=\"nofollow\">https://news.ycombinator.com/item?id=18458908</a>",
          "Mercurial support, fuck yeah:<p><a href=\"https://hg.sr.ht/\" rel=\"nofollow\">https://hg.sr.ht/</a><p>This is the killer feature for me.<p>Why Mercurial?<p>Here’s a list of Mercurial features that I think are really cool:<p>Revsets – a domain-specific language for querying your commits<p>Templates – a domain-specific language for altering the output of almost every command. Putting together these two you can do things like this: <a href=\"http://jordi.inversethought.com/blog/customising-mercurial-like-a-pro/\" rel=\"nofollow\">http://jordi.inversethought.com/blog/customising-mercurial-l...</a><p>Evolution – a distributed and safe way to share rewrites (think automatically recovering from upstream rebase without any git reset --hard and no git push --force).<p>Absorb – automatically amends an entire stack of WIP commits at once by picking the right diffs from your working directory based on which commits’ contexts they fit best.<p>Curses interface for hunk-picking – a unified interface for splitting diffs for any purpose: whether to revert working-directory changes, write a new commit, uncommit parts of a commit, or amend a commit with more diffs. Just add --interactive to any of those commands and start picking hunks!<p>A fairly rich built-in web interface – hg serve and point your browser to <a href=\"http://localhost:8000\" rel=\"nofollow\">http://localhost:8000</a> and you’re good to go.<p>Lots of support for monorepos – indeed, this is the main reason that Google, Facebook, and Mozilla are all pouring work into hg and are using it.<p>A consistent UI – this one is more subjective but often-touted feature of Mercurial. If a command accepts a revision/hash/tag/bookmark; it always uses the -r/--rev flag for it (and you can also always use a revset for any command that accepts a revision). If a command allows hunk selection, it always uses the -i/--interactive flag for it. The help for every command fits in about a screenful.<p>Give it a try! Mercurial is neat!"
        ],
        "id": "7d3b9c2a-01b2-41cc-8b4f-126d54a2479b",
        "_version_": 1718536444446769152
      },
      {
        "story_id": 20936679,
        "story_author": "marlynm",
        "story_descendants": 1,
        "story_score": 11,
        "story_time": "2019-09-11T04:57:48Z",
        "story_title": "Show HN: GitHub action to setup PHP with required extensions and Composer",
        "search": [
          "Show HN: GitHub action to setup PHP with required extensions and Composer",
          "https://github.com/shivammathur/setup-php",
          "Setup PHP in GitHub Actions Setup PHP with required extensions, php.ini configuration, code-coverage support and various tools like composer in GitHub Actions. This action gives you a cross platform interface to set up the PHP environment you need to test your application. Refer to Usage section and examples to see how to use this. Contents OS/Platform Support GitHub-Hosted Runners Self-Hosted Runners PHP Support PHP Extension Support Tools Support Coverage Support Xdebug PCOV Disable Coverage Usage Inputs Outputs Flags Basic Setup Matrix Setup Nightly Build Setup Thread Safe Setup Force Update Setup Verbose Setup Multi-Arch Setup Self Hosted Setup Local Testing Setup JIT Configuration Cache Extensions Cache Composer Dependencies Composer GitHub OAuth Inline PHP Scripts Problem Matchers Examples Versioning License Contributions Support This Project Dependencies Further Reading OS/Platform Support Both GitHub-hosted and self-hosted runners are suppported by setup-php on the following OS/Platforms. GitHub-Hosted Runners Virtual environment YAML workflow label Pre-installed PHP Ubuntu 18.04 ubuntu-18.04 PHP 7.1 to PHP 8.0 Ubuntu 20.04 ubuntu-latest or ubuntu-20.04 PHP 7.4 to PHP 8.0 Windows Server 2019 windows-latest or windows-2019 PHP 8.0 Windows Server 2022 windows-2022 PHP 8.0 macOS Catalina 10.15 macos-latest or macos-10.15 PHP 8.0 macOS Big Sur 11.x macos-11 PHP 8.0 Self-Hosted Runners Host OS/Virtual environment YAML workflow label Ubuntu 18.04 self-hosted or Linux Ubuntu 20.04 self-hosted or Linux Ubuntu 21.04 self-hosted or Linux Debian 9 self-hosted or Linux Debian 10 self-hosted or Linux Debian 11 self-hosted or Linux Windows 7 and newer self-hosted or Windows Windows Server 2012 R2 and newer self-hosted or Windows macOS Catalina 10.15 self-hosted or macOS macOS Big Sur 11.x x86_64/arm64 self-hosted or macOS Refer to the self-hosted setup to use the action on self-hosted runners. Operating systems based on the above Ubuntu and Debian versions are also supported on a best effort basis. If the requested PHP version is pre-installed, setup-php switches to it, otherwise it installs the PHP version. PHP Support On all supported OS/Platforms the following PHP versions are supported as per the runner. PHP 5.3 to PHP 8.2 on GitHub-hosted runners. PHP 5.6 to PHP 8.2 on self-hosted runners. PHP Version Stability Release Support Runner Support 5.3 Stable End of life GitHub-hosted 5.4 Stable End of life GitHub-hosted 5.5 Stable End of life GitHub-hosted 5.6 Stable End of life GitHub-hosted, self-hosted 7.0 Stable End of life GitHub-hosted, self-hosted 7.1 Stable End of life GitHub-hosted, self-hosted 7.2 Stable End of life GitHub-hosted, self-hosted 7.3 Stable Security fixes only GitHub-hosted, self-hosted 7.4 Stable Active GitHub-hosted, self-hosted 8.0 Stable Active GitHub-hosted, self-hosted 8.1 Nightly In development GitHub-hosted, self-hosted 8.2 Nightly In development GitHub-hosted, self-hosted Notes: Specifying 8.1 and 8.2 in php-version input installs a nightly build of PHP 8.1.0-dev and PHP 8.2.0-dev respectively. See nightly build setup for more information. To use JIT on PHP 8.0 and above, refer to the JIT configuration section. PHP Extension Support PHP extensions can be set up using the extensions input. It accepts a string in csv-format. On Ubuntu, extensions which are available as a package, available on PECL or a git repository can be set up. - name: Setup PHP with PECL extension uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: imagick, swoole On Windows, extensions available on PECL which have the DLL binary can be set up. On macOS, extensions available on PECL or a git repository can be set up. On Ubuntu and macOS to compile and install an extension from a git repository follow this guide. Extensions installed along with PHP if specified are enabled. Specific versions of extensions available on PECL can be set up by suffixing the extension's name with the version. This is useful for installing old versions of extensions which support end of life PHP versions. - name: Setup PHP with specific version of PECL extension uses: shivammathur/setup-php@v2 with: php-version: '5.4' extensions: swoole-1.9.3 Pre-release versions extensions available on PECL can be set up by suffixing the extension's name with its state i.e alpha, beta, devel or snapshot. - name: Setup PHP with pre-release PECL extension uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: xdebug-beta Shared extensions can be disabled by prefixing them with a :. All extensions depending on the specified extension will also be disabled. - name: Setup PHP and disable opcache uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: :opcache All shared extensions can be disabled by specifying none. When none is specified along with other extensions, it is hoisted to the start of the input. So, all the shared extensions will be disabled first, then rest of the extensions in the input will be processed. Note: This disables all core and third-party shared extensions and thus, can break some tools which need them. So, make sure you add the required extensions after none in the extensions input. - name: Setup PHP without any shared extensions except mbstring uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: none, mbstring Extension intl can be set up with specific ICU version for PHP 5.6 and above in Ubuntu workflows by suffixing intl with the ICU version. ICU 50.2 and newer versions are supported. Refer to ICU builds for the specific versions supported. - name: Setup PHP with intl uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: intl-69.1 Extensions loaded by default after setup-php runs can be found on the wiki. These extensions have custom support: cubrid, pdo_cubrid and gearman on Ubuntu. geos on Ubuntu and macOS. blackfire, couchbase, ioncube, oci8, pdo_firebird, pdo_oci, pecl_http, phalcon3 and phalcon4 on all supported OS. By default, extensions which cannot be added or disabled gracefully leave an error message in the logs, the action is not interrupted. To change this behaviour you can set fail-fast flag to true. - name: Setup PHP with fail-fast uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: oci8 env: fail-fast: true Tools Support These tools can be set up globally using the tools input. It accepts a string in csv-format. behat, blackfire, blackfire-player, codeception, composer, composer-normalize, composer-prefetcher, composer-require-checker, composer-unused, cs2pr, deployer, flex, grpc_php_plugin, infection, parallel-lint, pecl, phan, phing, phinx, phive, php-config, php-cs-fixer, phpcbf, phpcpd, phpcs, phpdoc or phpDocumentor, phpize, phplint, phpmd, phpspec, phpstan, phpunit, phpunit-bridge, phpunit-polyfills, prestissimo, protoc, psalm, symfony or symfony-cli, vapor or vapor-cli, wp or wp-cli - name: Setup PHP with tools uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: php-cs-fixer, phpunit In addition to above tools any composer tool or package can also be set up globally by specifying it as vendor/package matching the listing on Packagist. This format accepts the same version constraints as composer. - name: Setup PHP with tools uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: vimeo/psalm To set up a particular version of a tool, specify it in the form tool:version. Version can be in the following format: Semver. For example tool:1.2.3 or tool:1.2.3-beta1. Major version. For example tool:1 or tool:1.x. Major and minor version. For example tool:1.2 or tool:1.2.x. When you specify just the major version or the version in major.minor format, the latest patch version matching the input will be setup. Except for major versions of composer, For other tools when you specify only the major version or the version in major.minor format for any tool you can get rate limited by GitHub's API. To avoid this, it is recommended to provide a GitHub OAuth token. You can do that by setting COMPOSER_TOKEN environment variable. - name: Setup PHP with tools uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: php-cs-fixer:3, phpunit:8.5 env: COMPOSER_TOKEN: ${{ secrets.GITHUB_TOKEN }} The latest stable version of composer is set up by default. You can set up the required composer version by specifying the major version v1 or v2, or the version in major.minor or semver format, Additionally for composer snapshot and preview can also be specified to set up the respective releases. - name: Setup PHP with composer v2 uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: composer:v2 If you do not use composer in your workflow, you can specify tools: none to skip it. - name: Setup PHP without composer uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: none Scripts phpize and php-config are set up with the same version as of the input PHP version. The latest version of blackfire cli is set up when blackfire is specified in tools input. Please refer to the official documentation for using blackfire with GitHub Actions. Tools prestissimo and composer-prefetcher will be skipped unless composer:v1 is also specified in tools input. It is recommended to drop prestissimo and use composer v2. By default, tools which cannot be set up gracefully leave an error message in the logs, the action is not interrupted. To change this behaviour you can set fail-fast flag to true. - name: Setup PHP with fail-fast uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: deployer env: fail-fast: true Notes Input tools is useful to set up tools which you only use in GitHub Actions, thus keeping your composer.json tidy. If you do not want to use all your dev-dependencies in GitHub Actions workflow, you can run composer with --no-dev and install required tools using tools input to speed up your workflow. If you have a tool in your composer.json, do not set up it with tools input as the two instances of the tool might conflict. Coverage Support Xdebug Specify coverage: xdebug to use Xdebug and disable PCOV. Runs on all PHP versions supported. - name: Setup PHP with Xdebug uses: shivammathur/setup-php@v2 with: php-version: '8.0' coverage: xdebug The latest version of Xdebug compatible with the PHP version is set up by default. If you need Xdebug 2.x on PHP 7.2, 7.3 or 7.4, you can specify coverage: xdebug2. - name: Setup PHP with Xdebug 2.x uses: shivammathur/setup-php@v2 with: php-version: '7.4' coverage: xdebug2 PCOV Specify coverage: pcov to use PCOV and disable Xdebug. Runs on PHP 7.1 and newer PHP versions. If your source code directory is other than src, lib or, app, specify pcov.directory using the ini-values input. - name: Setup PHP with PCOV uses: shivammathur/setup-php@v2 with: php-version: '8.0' ini-values: pcov.directory=api #optional, see above for usage. coverage: pcov PHPUnit 8.x and above supports PCOV out of the box. If you are using PHPUnit 5.x, 6.x or 7.x, you need to set up pcov/clobber before executing your tests. - name: Setup PCOV run: | composer require pcov/clobber vendor/bin/pcov clobber Disable Coverage Specify coverage: none to disable both Xdebug and PCOV. Disable coverage for these reasons: You are not generating coverage reports while testing. It will disable Xdebug, which will have a positive impact on PHP performance. You are using phpdbg for running your tests. You are profiling your code using blackfire. You are using PHP in JIT mode. Please refer to JIT configuration section for more details. - name: Setup PHP with no coverage driver uses: shivammathur/setup-php@v2 with: php-version: '8.0' coverage: none Usage Inputs Specify using with keyword php-version (required) Specify the PHP version you want to set up. Accepts a string. For example '8.0'. Accepts latest to set up the latest stable PHP version. Accepts nightly to set up a nightly build from the master branch of PHP. Accepts the format d.x, where d is the major version. For example 5.x, 7.x and 8.x. See PHP support for supported PHP versions. extensions (optional) Specify the extensions you want to add or disable. Accepts a string in csv-format. For example mbstring, :opcache. Accepts none to disable all shared extensions. Shared extensions prefixed with : are disabled. See PHP extension support for more info. ini-values (optional) Specify the values you want to add to php.ini. Accepts a string in csv-format. For example post_max_size=256M, max_execution_time=180. Accepts ini values with commas if wrapped in quotes. For example xdebug.mode=\"develop,coverage\". coverage (optional) Specify the code-coverage driver you want to set up. Accepts xdebug, pcov or none. See coverage support for more info. tools (optional) Specify the tools you want to set up. Accepts a string in csv-format. For example: phpunit, phpcs See tools Support for tools supported. Outputs php-version To use outputs, give the setup-php step an id, you can use the same to get the outputs in a later step. Provides the PHP version in semver format. - name: Setup PHP id: setup-php uses: shivammathur/setup-php@v2 with: php-version: '8.0' - name: Print PHP version run: echo ${{ steps.setup-php.outputs.php-version }} Flags Specify using env keyword fail-fast (optional) Specify to mark the workflow as failed if an extension or tool fails to set up. This changes the default mode from graceful warnings to fail-fast. By default, it is set to false. Accepts true and false. phpts (optional) Specify to set up thread-safe version of PHP on Windows. Accepts ts and nts. By default, it is set to nts. See thread safe setup for more info. update (optional) Specify to update PHP on the runner to the latest patch version. Accepts true and false. By default, it is set to false. See force update setup for more info. See below for more info. Basic Setup Setup a particular PHP version. steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: mbstring, intl ini-values: post_max_size=256M, max_execution_time=180 coverage: xdebug tools: php-cs-fixer, phpunit Matrix Setup Setup multiple PHP versions on multiple operating systems. jobs: run: runs-on: ${{ matrix.operating-system }} strategy: matrix: operating-system: ['ubuntu-latest', 'windows-latest', 'macos-latest'] php-versions: ['7.3', '7.4', '8.0'] phpunit-versions: ['latest'] include: - operating-system: 'ubuntu-latest' php-versions: '7.2' phpunit-versions: '8.5.21' steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: ${{ matrix.php-versions }} extensions: mbstring, intl ini-values: post_max_size=256M, max_execution_time=180 coverage: xdebug tools: php-cs-fixer, phpunit:${{ matrix.phpunit-versions }} Nightly Build Setup Setup a nightly build of PHP 8.1 or PHP 8.2. This version is currently in development. Some user space extensions might not support this version currently. steps: - name: Setup nightly PHP uses: shivammathur/setup-php@v2 with: php-version: '8.1' extensions: mbstring ini-values: post_max_size=256M, max_execution_time=180 coverage: xdebug tools: php-cs-fixer, phpunit Thread Safe Setup Setup TS or NTS PHP on Windows. NTS versions are set up by default. On Ubuntu and macOS only NTS versions are supported. On Windows both TS and NTS versions are supported. jobs: run: runs-on: windows-latest name: Setup PHP TS on Windows steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' env: phpts: ts # specify ts or nts Force Update Setup Update to the latest patch of PHP versions. Pre-installed PHP versions on the GitHub Actions images are not updated to their latest patch release by default. You can specify the update environment variable to true for updating to the latest release. - name: Setup PHP with latest versions uses: shivammathur/setup-php@v2 with: php-version: '8.0' env: update: true # specify true or false Verbose Setup Debug your workflow To debug any issues, you can use the verbose tag instead of v2. - name: Setup PHP with logs uses: shivammathur/setup-php@verbose with: php-version: '8.0' Multi-Arch Setup Setup PHP on multiple architecture on Ubuntu GitHub Runners. PHP 5.6 to PHP 8.0 are supported by setup-php on multiple architecture on Ubuntu. For this, you can use shivammathur/node images as containers. These have compatible Nodejs installed for JavaScript based GitHub Actions. Currently, for ARM based setup, you will need self-hosted runners. jobs: run: runs-on: ubuntu-latest container: shivammathur/node:latest-${{ matrix.arch }} strategy: matrix: arch: [\"amd64\", \"i386\"] steps: - name: Install PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' Self Hosted Setup Setup PHP on a self-hosted runner. To set up a containerised self-hosted runner, refer to the following guides as per your base operating system. Linux Windows To set up the runner directly on the host OS or in a virtual machine, follow this requirements guide before setting up the self-hosted runner. If your workflow uses service containers, then set up the runner on a Linux host or in a Linux virtual machine. GitHub Actions does not support nested virtualization on Linux, so services will not work in a dockerized container. Specify the environment variable runner with the value self-hosted. Without this your workflow will fail. jobs: run: runs-on: self-hosted strategy: matrix: php-versions: ['5.6', '7.0', '7.1', '7.2', '7.3', '7.4', '8.0'] name: PHP ${{ matrix.php-versions }} steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: ${{ matrix.php-versions }} env: runner: self-hosted # Specify the runner. Notes Do not set up multiple self-hosted runners on a single server instance as parallel workflow will conflict with each other. Do not set up self-hosted runners on the side on your development environment or your production server. Avoid using the same labels for your self-hosted runners which are used by GitHub-hosted runners. Local Testing Setup Test your Ubuntu workflow locally using nektos/act. jobs: run: runs-on: ubuntu-latest steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' Run the workflow locally with act using shivammathur/node docker images. Choose the image tag which matches the runs-on property in your workflow. For example, if you are using ubuntu-20.04 in your workflow, run act -P ubuntu-20.04=shivammathur/node:20.04. # For runs-on: ubuntu-latest act -P ubuntu-latest=shivammathur/node:latest # For runs-on: ubuntu-20.04 act -P ubuntu-20.04=shivammathur/node:2004 # For runs-on: ubuntu-18.04 act -P ubuntu-18.04=shivammathur/node:1804 JIT Configuration Enable Just-in-time(JIT) on PHP 8.0 and above. To enable JIT, enable opcache in cli mode by setting opcache.enable_cli=1. JIT conflicts with Xdebug, PCOV, and other extensions which override zend_execute_ex function, so set coverage: none and disable any such extension if added. By default, opcache.jit=1235 and opcache.jit_buffer_size=256M are set which can be changed using ini-values input. For detailed information about JIT related directives refer to the official PHP documentation. For example to enable JIT in tracing mode with buffer size of 64 MB. - name: Setup PHP with JIT in tracing mode uses: shivammathur/setup-php@v2 with: php-version: '8.0' coverage: none ini-values: opcache.enable_cli=1, opcache.jit=tracing, opcache.jit_buffer_size=64M Cache Extensions You can cache PHP extensions using shivammathur/cache-extensions and action/cache GitHub Actions. Extensions which take very long to set up when cached are available in the next workflow run and are enabled directly. This reduces the workflow execution time. Refer to shivammathur/cache-extensions for details. Cache Composer Dependencies If your project uses composer, you can persist the composer's internal cache directory. Dependencies cached are loaded directly instead of downloading them while installation. The files cached are available across check-runs and will reduce the workflow execution time. - name: Get composer cache directory id: composer-cache run: echo \"::set-output name=dir::$(composer config cache-files-dir)\" - name: Cache dependencies uses: actions/cache@v2 with: path: ${{ steps.composer-cache.outputs.dir }} key: ${{ runner.os }}-composer-${{ hashFiles('**/composer.lock') }} restore-keys: ${{ runner.os }}-composer- - name: Install dependencies run: composer install --prefer-dist Notes Please do not cache vendor directory using action/cache as that will have side effects. If you do not commit composer.lock, you can use the hash of composer.json as the key for your cache. key: ${{ runner.os }}-composer-${{ hashFiles('**/composer.json') }} If you support a range of composer dependencies and use prefer-lowest and prefer-stable options, you can store them in your matrix and add them to the keys. key: ${{ runner.os }}-composer-${{ hashFiles('**/composer.lock') }}-${{ matrix.prefer }}- restore-keys: ${{ runner.os }}-composer-${{ matrix.prefer }}- Composer GitHub OAuth If you have a number of workflows which set up multiple tools or have many composer dependencies, you might hit the GitHub's rate limit for composer. Also, if you specify only the major version or the version in major.minor format, you can hit the rate limit. To avoid this you can specify an OAuth token by setting COMPOSER_TOKEN environment variable. You can use GITHUB_TOKEN secret for this purpose. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' env: COMPOSER_TOKEN: ${{ secrets.GITHUB_TOKEN }} Inline PHP Scripts If you have to run multiple lines of PHP code in your workflow, you can do that easily without saving it to a file. Put the code in the run property of a step and specify the shell as php {0}. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' - name: Run PHP code shell: php {0} run: | <?php $welcome = \"Hello, world\"; echo $welcome; Problem Matchers Problem matchers are json configurations which identify errors and warnings in your logs and surface them prominently in the GitHub Actions UI by highlighting them and creating code annotations. PHP Setup problem matchers for your PHP output by adding this step after the setup-php step. - name: Setup problem matchers for PHP run: echo \"::add-matcher::${{ runner.tool_cache }}/php.json\" PHPUnit Setup problem matchers for your PHPUnit output by adding this step after the setup-php step. - name: Setup problem matchers for PHPUnit run: echo \"::add-matcher::${{ runner.tool_cache }}/phpunit.json\" PHPStan PHPStan supports error reporting in GitHub Actions, so it does not require problem matchers. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: phpstan - name: Run PHPStan run: phpstan analyse src Psalm Psalm supports error reporting in GitHub Actions with an output format github. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: psalm - name: Run Psalm run: psalm --output-format=github Tools with checkstyle support For tools that support checkstyle reporting like phpstan, psalm, php-cs-fixer and phpcs you can use cs2pr to annotate your code. For examples refer to cs2pr documentation. Here is an example with phpcs. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: cs2pr, phpcs - name: Run phpcs run: phpcs -q --report=checkstyle src | cs2pr Examples Examples of using setup-php with various PHP Frameworks and Packages. Framework/Package Runs on Workflow Blackfire macOS, ubuntu and windows blackfire.yml Blackfire Player macOS, ubuntu and windows blackfire-player.yml CakePHP with MySQL and Redis ubuntu cakephp-mysql.yml CakePHP with PostgreSQL and Redis ubuntu cakephp-postgres.yml CakePHP without services macOS, ubuntu and windows cakephp.yml CodeIgniter macOS, ubuntu and windows codeigniter.yml Laravel with MySQL and Redis ubuntu laravel-mysql.yml Laravel with PostgreSQL and Redis ubuntu laravel-postgres.yml Laravel without services macOS, ubuntu and windows laravel.yml Lumen with MySQL and Redis ubuntu lumen-mysql.yml Lumen with PostgreSQL and Redis ubuntu lumen-postgres.yml Lumen without services macOS, ubuntu and windows lumen.yml Phalcon with MySQL ubuntu phalcon-mysql.yml Phalcon with PostgreSQL ubuntu phalcon-postgres.yml Roots/bedrock ubuntu bedrock.yml Roots/sage ubuntu sage.yml Slim Framework macOS, ubuntu and windows slim-framework.yml Symfony with MySQL ubuntu symfony-mysql.yml Symfony with PostgreSQL ubuntu symfony-postgres.yml Symfony without services macOS, ubuntu and windows symfony.yml Yii2 Starter Kit with MySQL ubuntu yii2-mysql.yml Yii2 Starter Kit with PostgreSQL ubuntu yii2-postgres.yml Zend Framework macOS, ubuntu and windows zend-framework.yml Versioning Use the v2 tag as setup-php version. It is a rolling tag and is synced with the latest minor and patch releases. With v2 you automatically get the bug fixes, security patches, new features and support for latest PHP releases. For debugging any issues verbose tag can be used temporarily. It outputs all the logs and is also synced with the latest releases. Semantic release versions can also be used. It is recommended to use dependabot with semantic versioning to keep the actions in your workflows up to date. Commit SHA can also be used, but are not recommended. They have to be updated with every release manually, without which you will not get any bug fixes, security patches or new features. It is highly discouraged to use the master branch as version, it might break your workflow after major releases as they have breaking changes. If you are using the v1 tag or a 1.x.y version, you should switch to v2 as v1 only gets critical bug fixes. Maintenance support for v1 will be dropped with the last PHP 8.0 release. License The scripts and documentation in this project are under the MIT License. This project has multiple dependencies. Their licenses can be found in their respective repositories. The logo for setup-php is a derivative work of php.net logo and is licensed under the CC BY-SA 4.0 License. Contributions Contributions are welcome! See Contributor's Guide before you start. If you face any issues or want to suggest a feature/improvement, start a discussion here. Contributors of setup-php and other related projects Support This Project Please star the project and share it. If you blog, please share your experience of using this action. Please sponsor setup-php using GitHub sponsors. Please reach out if you have any questions about sponsoring setup-php. Corporate Sponsors Individual Sponsors Dependencies Node.js dependencies aaronparker/VcRedist mlocati/powershell-phpmanager ppa:ondrej/php shivammathur/cache-extensions shivammathur/composer-cache shivammathur/homebrew-extensions shivammathur/homebrew-php shivammathur/icu-intl shivammathur/php-builder shivammathur/php-builder-windows shivammathur/php-ubuntu shivammathur/php5-darwin shivammathur/php5-ubuntu Further Reading About GitHub Actions GitHub Actions Syntax Other Awesome Actions "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/shivammathur/setup-php",
        "id": "c5c60444-6119-44f7-85e9-6d0d4a913675",
        "url_text": "Setup PHP in GitHub Actions Setup PHP with required extensions, php.ini configuration, code-coverage support and various tools like composer in GitHub Actions. This action gives you a cross platform interface to set up the PHP environment you need to test your application. Refer to Usage section and examples to see how to use this. Contents OS/Platform Support GitHub-Hosted Runners Self-Hosted Runners PHP Support PHP Extension Support Tools Support Coverage Support Xdebug PCOV Disable Coverage Usage Inputs Outputs Flags Basic Setup Matrix Setup Nightly Build Setup Thread Safe Setup Force Update Setup Verbose Setup Multi-Arch Setup Self Hosted Setup Local Testing Setup JIT Configuration Cache Extensions Cache Composer Dependencies Composer GitHub OAuth Inline PHP Scripts Problem Matchers Examples Versioning License Contributions Support This Project Dependencies Further Reading OS/Platform Support Both GitHub-hosted and self-hosted runners are suppported by setup-php on the following OS/Platforms. GitHub-Hosted Runners Virtual environment YAML workflow label Pre-installed PHP Ubuntu 18.04 ubuntu-18.04 PHP 7.1 to PHP 8.0 Ubuntu 20.04 ubuntu-latest or ubuntu-20.04 PHP 7.4 to PHP 8.0 Windows Server 2019 windows-latest or windows-2019 PHP 8.0 Windows Server 2022 windows-2022 PHP 8.0 macOS Catalina 10.15 macos-latest or macos-10.15 PHP 8.0 macOS Big Sur 11.x macos-11 PHP 8.0 Self-Hosted Runners Host OS/Virtual environment YAML workflow label Ubuntu 18.04 self-hosted or Linux Ubuntu 20.04 self-hosted or Linux Ubuntu 21.04 self-hosted or Linux Debian 9 self-hosted or Linux Debian 10 self-hosted or Linux Debian 11 self-hosted or Linux Windows 7 and newer self-hosted or Windows Windows Server 2012 R2 and newer self-hosted or Windows macOS Catalina 10.15 self-hosted or macOS macOS Big Sur 11.x x86_64/arm64 self-hosted or macOS Refer to the self-hosted setup to use the action on self-hosted runners. Operating systems based on the above Ubuntu and Debian versions are also supported on a best effort basis. If the requested PHP version is pre-installed, setup-php switches to it, otherwise it installs the PHP version. PHP Support On all supported OS/Platforms the following PHP versions are supported as per the runner. PHP 5.3 to PHP 8.2 on GitHub-hosted runners. PHP 5.6 to PHP 8.2 on self-hosted runners. PHP Version Stability Release Support Runner Support 5.3 Stable End of life GitHub-hosted 5.4 Stable End of life GitHub-hosted 5.5 Stable End of life GitHub-hosted 5.6 Stable End of life GitHub-hosted, self-hosted 7.0 Stable End of life GitHub-hosted, self-hosted 7.1 Stable End of life GitHub-hosted, self-hosted 7.2 Stable End of life GitHub-hosted, self-hosted 7.3 Stable Security fixes only GitHub-hosted, self-hosted 7.4 Stable Active GitHub-hosted, self-hosted 8.0 Stable Active GitHub-hosted, self-hosted 8.1 Nightly In development GitHub-hosted, self-hosted 8.2 Nightly In development GitHub-hosted, self-hosted Notes: Specifying 8.1 and 8.2 in php-version input installs a nightly build of PHP 8.1.0-dev and PHP 8.2.0-dev respectively. See nightly build setup for more information. To use JIT on PHP 8.0 and above, refer to the JIT configuration section. PHP Extension Support PHP extensions can be set up using the extensions input. It accepts a string in csv-format. On Ubuntu, extensions which are available as a package, available on PECL or a git repository can be set up. - name: Setup PHP with PECL extension uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: imagick, swoole On Windows, extensions available on PECL which have the DLL binary can be set up. On macOS, extensions available on PECL or a git repository can be set up. On Ubuntu and macOS to compile and install an extension from a git repository follow this guide. Extensions installed along with PHP if specified are enabled. Specific versions of extensions available on PECL can be set up by suffixing the extension's name with the version. This is useful for installing old versions of extensions which support end of life PHP versions. - name: Setup PHP with specific version of PECL extension uses: shivammathur/setup-php@v2 with: php-version: '5.4' extensions: swoole-1.9.3 Pre-release versions extensions available on PECL can be set up by suffixing the extension's name with its state i.e alpha, beta, devel or snapshot. - name: Setup PHP with pre-release PECL extension uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: xdebug-beta Shared extensions can be disabled by prefixing them with a :. All extensions depending on the specified extension will also be disabled. - name: Setup PHP and disable opcache uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: :opcache All shared extensions can be disabled by specifying none. When none is specified along with other extensions, it is hoisted to the start of the input. So, all the shared extensions will be disabled first, then rest of the extensions in the input will be processed. Note: This disables all core and third-party shared extensions and thus, can break some tools which need them. So, make sure you add the required extensions after none in the extensions input. - name: Setup PHP without any shared extensions except mbstring uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: none, mbstring Extension intl can be set up with specific ICU version for PHP 5.6 and above in Ubuntu workflows by suffixing intl with the ICU version. ICU 50.2 and newer versions are supported. Refer to ICU builds for the specific versions supported. - name: Setup PHP with intl uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: intl-69.1 Extensions loaded by default after setup-php runs can be found on the wiki. These extensions have custom support: cubrid, pdo_cubrid and gearman on Ubuntu. geos on Ubuntu and macOS. blackfire, couchbase, ioncube, oci8, pdo_firebird, pdo_oci, pecl_http, phalcon3 and phalcon4 on all supported OS. By default, extensions which cannot be added or disabled gracefully leave an error message in the logs, the action is not interrupted. To change this behaviour you can set fail-fast flag to true. - name: Setup PHP with fail-fast uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: oci8 env: fail-fast: true Tools Support These tools can be set up globally using the tools input. It accepts a string in csv-format. behat, blackfire, blackfire-player, codeception, composer, composer-normalize, composer-prefetcher, composer-require-checker, composer-unused, cs2pr, deployer, flex, grpc_php_plugin, infection, parallel-lint, pecl, phan, phing, phinx, phive, php-config, php-cs-fixer, phpcbf, phpcpd, phpcs, phpdoc or phpDocumentor, phpize, phplint, phpmd, phpspec, phpstan, phpunit, phpunit-bridge, phpunit-polyfills, prestissimo, protoc, psalm, symfony or symfony-cli, vapor or vapor-cli, wp or wp-cli - name: Setup PHP with tools uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: php-cs-fixer, phpunit In addition to above tools any composer tool or package can also be set up globally by specifying it as vendor/package matching the listing on Packagist. This format accepts the same version constraints as composer. - name: Setup PHP with tools uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: vimeo/psalm To set up a particular version of a tool, specify it in the form tool:version. Version can be in the following format: Semver. For example tool:1.2.3 or tool:1.2.3-beta1. Major version. For example tool:1 or tool:1.x. Major and minor version. For example tool:1.2 or tool:1.2.x. When you specify just the major version or the version in major.minor format, the latest patch version matching the input will be setup. Except for major versions of composer, For other tools when you specify only the major version or the version in major.minor format for any tool you can get rate limited by GitHub's API. To avoid this, it is recommended to provide a GitHub OAuth token. You can do that by setting COMPOSER_TOKEN environment variable. - name: Setup PHP with tools uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: php-cs-fixer:3, phpunit:8.5 env: COMPOSER_TOKEN: ${{ secrets.GITHUB_TOKEN }} The latest stable version of composer is set up by default. You can set up the required composer version by specifying the major version v1 or v2, or the version in major.minor or semver format, Additionally for composer snapshot and preview can also be specified to set up the respective releases. - name: Setup PHP with composer v2 uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: composer:v2 If you do not use composer in your workflow, you can specify tools: none to skip it. - name: Setup PHP without composer uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: none Scripts phpize and php-config are set up with the same version as of the input PHP version. The latest version of blackfire cli is set up when blackfire is specified in tools input. Please refer to the official documentation for using blackfire with GitHub Actions. Tools prestissimo and composer-prefetcher will be skipped unless composer:v1 is also specified in tools input. It is recommended to drop prestissimo and use composer v2. By default, tools which cannot be set up gracefully leave an error message in the logs, the action is not interrupted. To change this behaviour you can set fail-fast flag to true. - name: Setup PHP with fail-fast uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: deployer env: fail-fast: true Notes Input tools is useful to set up tools which you only use in GitHub Actions, thus keeping your composer.json tidy. If you do not want to use all your dev-dependencies in GitHub Actions workflow, you can run composer with --no-dev and install required tools using tools input to speed up your workflow. If you have a tool in your composer.json, do not set up it with tools input as the two instances of the tool might conflict. Coverage Support Xdebug Specify coverage: xdebug to use Xdebug and disable PCOV. Runs on all PHP versions supported. - name: Setup PHP with Xdebug uses: shivammathur/setup-php@v2 with: php-version: '8.0' coverage: xdebug The latest version of Xdebug compatible with the PHP version is set up by default. If you need Xdebug 2.x on PHP 7.2, 7.3 or 7.4, you can specify coverage: xdebug2. - name: Setup PHP with Xdebug 2.x uses: shivammathur/setup-php@v2 with: php-version: '7.4' coverage: xdebug2 PCOV Specify coverage: pcov to use PCOV and disable Xdebug. Runs on PHP 7.1 and newer PHP versions. If your source code directory is other than src, lib or, app, specify pcov.directory using the ini-values input. - name: Setup PHP with PCOV uses: shivammathur/setup-php@v2 with: php-version: '8.0' ini-values: pcov.directory=api #optional, see above for usage. coverage: pcov PHPUnit 8.x and above supports PCOV out of the box. If you are using PHPUnit 5.x, 6.x or 7.x, you need to set up pcov/clobber before executing your tests. - name: Setup PCOV run: | composer require pcov/clobber vendor/bin/pcov clobber Disable Coverage Specify coverage: none to disable both Xdebug and PCOV. Disable coverage for these reasons: You are not generating coverage reports while testing. It will disable Xdebug, which will have a positive impact on PHP performance. You are using phpdbg for running your tests. You are profiling your code using blackfire. You are using PHP in JIT mode. Please refer to JIT configuration section for more details. - name: Setup PHP with no coverage driver uses: shivammathur/setup-php@v2 with: php-version: '8.0' coverage: none Usage Inputs Specify using with keyword php-version (required) Specify the PHP version you want to set up. Accepts a string. For example '8.0'. Accepts latest to set up the latest stable PHP version. Accepts nightly to set up a nightly build from the master branch of PHP. Accepts the format d.x, where d is the major version. For example 5.x, 7.x and 8.x. See PHP support for supported PHP versions. extensions (optional) Specify the extensions you want to add or disable. Accepts a string in csv-format. For example mbstring, :opcache. Accepts none to disable all shared extensions. Shared extensions prefixed with : are disabled. See PHP extension support for more info. ini-values (optional) Specify the values you want to add to php.ini. Accepts a string in csv-format. For example post_max_size=256M, max_execution_time=180. Accepts ini values with commas if wrapped in quotes. For example xdebug.mode=\"develop,coverage\". coverage (optional) Specify the code-coverage driver you want to set up. Accepts xdebug, pcov or none. See coverage support for more info. tools (optional) Specify the tools you want to set up. Accepts a string in csv-format. For example: phpunit, phpcs See tools Support for tools supported. Outputs php-version To use outputs, give the setup-php step an id, you can use the same to get the outputs in a later step. Provides the PHP version in semver format. - name: Setup PHP id: setup-php uses: shivammathur/setup-php@v2 with: php-version: '8.0' - name: Print PHP version run: echo ${{ steps.setup-php.outputs.php-version }} Flags Specify using env keyword fail-fast (optional) Specify to mark the workflow as failed if an extension or tool fails to set up. This changes the default mode from graceful warnings to fail-fast. By default, it is set to false. Accepts true and false. phpts (optional) Specify to set up thread-safe version of PHP on Windows. Accepts ts and nts. By default, it is set to nts. See thread safe setup for more info. update (optional) Specify to update PHP on the runner to the latest patch version. Accepts true and false. By default, it is set to false. See force update setup for more info. See below for more info. Basic Setup Setup a particular PHP version. steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' extensions: mbstring, intl ini-values: post_max_size=256M, max_execution_time=180 coverage: xdebug tools: php-cs-fixer, phpunit Matrix Setup Setup multiple PHP versions on multiple operating systems. jobs: run: runs-on: ${{ matrix.operating-system }} strategy: matrix: operating-system: ['ubuntu-latest', 'windows-latest', 'macos-latest'] php-versions: ['7.3', '7.4', '8.0'] phpunit-versions: ['latest'] include: - operating-system: 'ubuntu-latest' php-versions: '7.2' phpunit-versions: '8.5.21' steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: ${{ matrix.php-versions }} extensions: mbstring, intl ini-values: post_max_size=256M, max_execution_time=180 coverage: xdebug tools: php-cs-fixer, phpunit:${{ matrix.phpunit-versions }} Nightly Build Setup Setup a nightly build of PHP 8.1 or PHP 8.2. This version is currently in development. Some user space extensions might not support this version currently. steps: - name: Setup nightly PHP uses: shivammathur/setup-php@v2 with: php-version: '8.1' extensions: mbstring ini-values: post_max_size=256M, max_execution_time=180 coverage: xdebug tools: php-cs-fixer, phpunit Thread Safe Setup Setup TS or NTS PHP on Windows. NTS versions are set up by default. On Ubuntu and macOS only NTS versions are supported. On Windows both TS and NTS versions are supported. jobs: run: runs-on: windows-latest name: Setup PHP TS on Windows steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' env: phpts: ts # specify ts or nts Force Update Setup Update to the latest patch of PHP versions. Pre-installed PHP versions on the GitHub Actions images are not updated to their latest patch release by default. You can specify the update environment variable to true for updating to the latest release. - name: Setup PHP with latest versions uses: shivammathur/setup-php@v2 with: php-version: '8.0' env: update: true # specify true or false Verbose Setup Debug your workflow To debug any issues, you can use the verbose tag instead of v2. - name: Setup PHP with logs uses: shivammathur/setup-php@verbose with: php-version: '8.0' Multi-Arch Setup Setup PHP on multiple architecture on Ubuntu GitHub Runners. PHP 5.6 to PHP 8.0 are supported by setup-php on multiple architecture on Ubuntu. For this, you can use shivammathur/node images as containers. These have compatible Nodejs installed for JavaScript based GitHub Actions. Currently, for ARM based setup, you will need self-hosted runners. jobs: run: runs-on: ubuntu-latest container: shivammathur/node:latest-${{ matrix.arch }} strategy: matrix: arch: [\"amd64\", \"i386\"] steps: - name: Install PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' Self Hosted Setup Setup PHP on a self-hosted runner. To set up a containerised self-hosted runner, refer to the following guides as per your base operating system. Linux Windows To set up the runner directly on the host OS or in a virtual machine, follow this requirements guide before setting up the self-hosted runner. If your workflow uses service containers, then set up the runner on a Linux host or in a Linux virtual machine. GitHub Actions does not support nested virtualization on Linux, so services will not work in a dockerized container. Specify the environment variable runner with the value self-hosted. Without this your workflow will fail. jobs: run: runs-on: self-hosted strategy: matrix: php-versions: ['5.6', '7.0', '7.1', '7.2', '7.3', '7.4', '8.0'] name: PHP ${{ matrix.php-versions }} steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: ${{ matrix.php-versions }} env: runner: self-hosted # Specify the runner. Notes Do not set up multiple self-hosted runners on a single server instance as parallel workflow will conflict with each other. Do not set up self-hosted runners on the side on your development environment or your production server. Avoid using the same labels for your self-hosted runners which are used by GitHub-hosted runners. Local Testing Setup Test your Ubuntu workflow locally using nektos/act. jobs: run: runs-on: ubuntu-latest steps: - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' Run the workflow locally with act using shivammathur/node docker images. Choose the image tag which matches the runs-on property in your workflow. For example, if you are using ubuntu-20.04 in your workflow, run act -P ubuntu-20.04=shivammathur/node:20.04. # For runs-on: ubuntu-latest act -P ubuntu-latest=shivammathur/node:latest # For runs-on: ubuntu-20.04 act -P ubuntu-20.04=shivammathur/node:2004 # For runs-on: ubuntu-18.04 act -P ubuntu-18.04=shivammathur/node:1804 JIT Configuration Enable Just-in-time(JIT) on PHP 8.0 and above. To enable JIT, enable opcache in cli mode by setting opcache.enable_cli=1. JIT conflicts with Xdebug, PCOV, and other extensions which override zend_execute_ex function, so set coverage: none and disable any such extension if added. By default, opcache.jit=1235 and opcache.jit_buffer_size=256M are set which can be changed using ini-values input. For detailed information about JIT related directives refer to the official PHP documentation. For example to enable JIT in tracing mode with buffer size of 64 MB. - name: Setup PHP with JIT in tracing mode uses: shivammathur/setup-php@v2 with: php-version: '8.0' coverage: none ini-values: opcache.enable_cli=1, opcache.jit=tracing, opcache.jit_buffer_size=64M Cache Extensions You can cache PHP extensions using shivammathur/cache-extensions and action/cache GitHub Actions. Extensions which take very long to set up when cached are available in the next workflow run and are enabled directly. This reduces the workflow execution time. Refer to shivammathur/cache-extensions for details. Cache Composer Dependencies If your project uses composer, you can persist the composer's internal cache directory. Dependencies cached are loaded directly instead of downloading them while installation. The files cached are available across check-runs and will reduce the workflow execution time. - name: Get composer cache directory id: composer-cache run: echo \"::set-output name=dir::$(composer config cache-files-dir)\" - name: Cache dependencies uses: actions/cache@v2 with: path: ${{ steps.composer-cache.outputs.dir }} key: ${{ runner.os }}-composer-${{ hashFiles('**/composer.lock') }} restore-keys: ${{ runner.os }}-composer- - name: Install dependencies run: composer install --prefer-dist Notes Please do not cache vendor directory using action/cache as that will have side effects. If you do not commit composer.lock, you can use the hash of composer.json as the key for your cache. key: ${{ runner.os }}-composer-${{ hashFiles('**/composer.json') }} If you support a range of composer dependencies and use prefer-lowest and prefer-stable options, you can store them in your matrix and add them to the keys. key: ${{ runner.os }}-composer-${{ hashFiles('**/composer.lock') }}-${{ matrix.prefer }}- restore-keys: ${{ runner.os }}-composer-${{ matrix.prefer }}- Composer GitHub OAuth If you have a number of workflows which set up multiple tools or have many composer dependencies, you might hit the GitHub's rate limit for composer. Also, if you specify only the major version or the version in major.minor format, you can hit the rate limit. To avoid this you can specify an OAuth token by setting COMPOSER_TOKEN environment variable. You can use GITHUB_TOKEN secret for this purpose. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' env: COMPOSER_TOKEN: ${{ secrets.GITHUB_TOKEN }} Inline PHP Scripts If you have to run multiple lines of PHP code in your workflow, you can do that easily without saving it to a file. Put the code in the run property of a step and specify the shell as php {0}. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' - name: Run PHP code shell: php {0} run: | <?php $welcome = \"Hello, world\"; echo $welcome; Problem Matchers Problem matchers are json configurations which identify errors and warnings in your logs and surface them prominently in the GitHub Actions UI by highlighting them and creating code annotations. PHP Setup problem matchers for your PHP output by adding this step after the setup-php step. - name: Setup problem matchers for PHP run: echo \"::add-matcher::${{ runner.tool_cache }}/php.json\" PHPUnit Setup problem matchers for your PHPUnit output by adding this step after the setup-php step. - name: Setup problem matchers for PHPUnit run: echo \"::add-matcher::${{ runner.tool_cache }}/phpunit.json\" PHPStan PHPStan supports error reporting in GitHub Actions, so it does not require problem matchers. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: phpstan - name: Run PHPStan run: phpstan analyse src Psalm Psalm supports error reporting in GitHub Actions with an output format github. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: psalm - name: Run Psalm run: psalm --output-format=github Tools with checkstyle support For tools that support checkstyle reporting like phpstan, psalm, php-cs-fixer and phpcs you can use cs2pr to annotate your code. For examples refer to cs2pr documentation. Here is an example with phpcs. - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: '8.0' tools: cs2pr, phpcs - name: Run phpcs run: phpcs -q --report=checkstyle src | cs2pr Examples Examples of using setup-php with various PHP Frameworks and Packages. Framework/Package Runs on Workflow Blackfire macOS, ubuntu and windows blackfire.yml Blackfire Player macOS, ubuntu and windows blackfire-player.yml CakePHP with MySQL and Redis ubuntu cakephp-mysql.yml CakePHP with PostgreSQL and Redis ubuntu cakephp-postgres.yml CakePHP without services macOS, ubuntu and windows cakephp.yml CodeIgniter macOS, ubuntu and windows codeigniter.yml Laravel with MySQL and Redis ubuntu laravel-mysql.yml Laravel with PostgreSQL and Redis ubuntu laravel-postgres.yml Laravel without services macOS, ubuntu and windows laravel.yml Lumen with MySQL and Redis ubuntu lumen-mysql.yml Lumen with PostgreSQL and Redis ubuntu lumen-postgres.yml Lumen without services macOS, ubuntu and windows lumen.yml Phalcon with MySQL ubuntu phalcon-mysql.yml Phalcon with PostgreSQL ubuntu phalcon-postgres.yml Roots/bedrock ubuntu bedrock.yml Roots/sage ubuntu sage.yml Slim Framework macOS, ubuntu and windows slim-framework.yml Symfony with MySQL ubuntu symfony-mysql.yml Symfony with PostgreSQL ubuntu symfony-postgres.yml Symfony without services macOS, ubuntu and windows symfony.yml Yii2 Starter Kit with MySQL ubuntu yii2-mysql.yml Yii2 Starter Kit with PostgreSQL ubuntu yii2-postgres.yml Zend Framework macOS, ubuntu and windows zend-framework.yml Versioning Use the v2 tag as setup-php version. It is a rolling tag and is synced with the latest minor and patch releases. With v2 you automatically get the bug fixes, security patches, new features and support for latest PHP releases. For debugging any issues verbose tag can be used temporarily. It outputs all the logs and is also synced with the latest releases. Semantic release versions can also be used. It is recommended to use dependabot with semantic versioning to keep the actions in your workflows up to date. Commit SHA can also be used, but are not recommended. They have to be updated with every release manually, without which you will not get any bug fixes, security patches or new features. It is highly discouraged to use the master branch as version, it might break your workflow after major releases as they have breaking changes. If you are using the v1 tag or a 1.x.y version, you should switch to v2 as v1 only gets critical bug fixes. Maintenance support for v1 will be dropped with the last PHP 8.0 release. License The scripts and documentation in this project are under the MIT License. This project has multiple dependencies. Their licenses can be found in their respective repositories. The logo for setup-php is a derivative work of php.net logo and is licensed under the CC BY-SA 4.0 License. Contributions Contributions are welcome! See Contributor's Guide before you start. If you face any issues or want to suggest a feature/improvement, start a discussion here. Contributors of setup-php and other related projects Support This Project Please star the project and share it. If you blog, please share your experience of using this action. Please sponsor setup-php using GitHub sponsors. Please reach out if you have any questions about sponsoring setup-php. Corporate Sponsors Individual Sponsors Dependencies Node.js dependencies aaronparker/VcRedist mlocati/powershell-phpmanager ppa:ondrej/php shivammathur/cache-extensions shivammathur/composer-cache shivammathur/homebrew-extensions shivammathur/homebrew-php shivammathur/icu-intl shivammathur/php-builder shivammathur/php-builder-windows shivammathur/php-ubuntu shivammathur/php5-darwin shivammathur/php5-ubuntu Further Reading About GitHub Actions GitHub Actions Syntax Other Awesome Actions ",
        "_version_": 1718536520623718400
      },
      {
        "story_id": 21158487,
        "story_author": "danicgross",
        "story_descendants": 65,
        "story_score": 470,
        "story_time": "2019-10-04T15:15:12Z",
        "story_title": "Streamlit: Turn a Python script into an interactive data analysis tool",
        "search": [
          "Streamlit: Turn a Python script into an interactive data analysis tool",
          "https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace",
          "Introducing Streamlit, an app framework built for ML engineersCoding a semantic search engine with real-time neural-net inference in 300 lines of Python.In my experience, every nontrivial machine learning project is eventually stitched together with bug-ridden and unmaintainable internal tools. These tools often a patchwork of Jupyter Notebooks and Flask apps are difficult to deploy, require reasoning about client-server architecture, and dont integrate well with machine learning constructs like Tensorflow GPU sessions.I saw this first at Carnegie Mellon, then at Berkeley, Google X, and finally while building autonomous robots at Zoox. These tools were often born as little Jupyter notebooks: the sensor calibration tool, the simulation comparison app, the LIDAR alignment app, the scenario replay tool, and so on.As a tool grew in importance, project managers stepped in. Processes sprouted. Requirements flowered. These solo projects gestated into scripts, and matured into gangly maintenance nightmares.The machine learning engineers ad-hoc app building flow.When a tool became crucial, we called in the tools team. They wrote fluent Vue and React. They blinged their laptops with stickers about declarative frameworks. They had a design process:The tools teams clean-slate app building flow.Which was awesome. But these tools all needed new features, like weekly. And the tools team was supporting ten other projects. They would say, well update your tool again in two months.So we were back to building our own tools, deploying Flask apps, writing HTML, CSS, and JavaScript, and trying to version control everything from notebooks to stylesheets. So my old Google X friend, Thiago Teixeira, and I began thinking about the following question: What if we could make building tools as easy as writing Python scripts?We wanted machine learning engineers to be able to create beautiful apps without needing a tools team. These internal tools should arise as a natural byproduct of the ML workflow. Writing such tools should feel like training a neural net or performing an ad-hoc analysis in Jupyter! At the same time, we wanted to preserve all of the flexibility of a powerful app framework. We wanted to create beautiful, performant tools that engineers could show off. Basically, we wanted this:The Streamlit app building flow.With an amazing beta community including engineers from Uber, Twitter, Stitch Fix, and Dropbox, we worked for a year to create Streamlit, a completely free and open source app framework for ML engineers. With each prototype, the core principles of Streamlit became simpler and purer. They are:#1: Embrace Python scripting. Streamlit apps are really just scripts that run from top to bottom. Theres no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. For example, this is how you write to the screen:import streamlit as stst.write('Hello, world!')Nice to meet you.#2: Treat widgets as variables. There are no callbacks in Streamlit! Every interaction simply reruns the script from top to bottom. This approach leads to really clean code:import streamlit as stx = st.slider('x')st.write(x, 'squared is', x * x)An interactive Streamlit app in three lines of code.#3: Reuse data and computation. What if you download lots of data or perform complex computation? The key is to safely reuse information across runs. Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default, data store that lets Streamlit apps safely and effortlessly reuse information. For example, this code downloads data only once from the Udacity Self-driving car project, yielding a simple, fast app:Using st.cache to persist data across Streamlit runs. To run this code, please follow these instructions.The output of running the st.cache example above.In short, Streamlit works like this:The entire script is run from scratch for each user interaction.Streamlit assigns each variable an up-to-date value given widget states.Caching allows Streamlit to skip redundant data fetches and computation.Or in pictures:User events trigger Streamlit to rerun the script from scratch. Only the cache persists across runs.If this sounds intriguing, you can try it right now! Just run:$ pip install --upgrade streamlit $ streamlit hello You can now view your Streamlit app in your browser. Local URL: http://localhost:8501 Network URL: http://10.0.1.29:8501This will automatically pop open a web browser pointing to your local Streamlit app. If not, just click the link.To see more examples like this fractal animation, run streamlit hello from the command line.Ok. Are you back from playing with fractals? Those can be mesmerizing.The simplicity of these ideas does not prevent you from creating incredibly rich and useful apps with Streamlit. During my time at Zoox and Google X, I watched as self-driving car projects ballooned into gigabytes of visual data, which needed to be searched and understood, including running models on images to compare performance. Every self-driving car project Ive seen eventually has had entire teams working on this tooling.Building such a tool in Streamlit is easy. This Streamlit demo lets you perform semantic search across the entire Udacity self-driving car photo dataset, visualize human-annotated ground truth labels, and run a complete neural net (YOLO) in real time from within the app [1].This 300-line Streamlit demo combines semantic visual search with interactive neural net inference.The whole app is a completely self-contained, 300-line Python script, most of which is machine learning code. In fact, there are only 23 Streamlit calls in the whole app. You can run it yourself right now!$ pip install --upgrade streamlit opencv-python$ streamlit runhttps://raw.githubusercontent.com/streamlit/demo-self-driving/master/app.pyAs we worked with machine learning teams on their own projects, we came to realize that these simple ideas yield a number of important benefits:Streamlit apps are pure Python files. So you can use your favorite editor and debugger with Streamlit.My favorite layout for writing Streamlit apps has VSCode on the left and Chrome on the right.Pure Python scripts work seamlessly with Git and other source control software, including commits, pull requests, issues, and comments. Because Streamlits underlying language is pure Python, you get all the benefits of these amazing collaboration tools for free .Because Streamlit apps are just Python scripts, you can easily version control them with Git.Streamlit provides an immediate-mode live coding environment. Just click Always rerun when Streamlit detects a source file change.Click Always rerun to enable live coding.Caching simplifies setting up computation pipelines. Amazingly, chaining cached functions automatically creates efficient computation pipelines! Consider this code adapted from our Udacity demo:A simple computation pipeline in Streamlit. To run this code, please follow these instructions.Basically, the pipeline is load_metadata create_summary. Every time the script is run Streamlit only recomputes whatever subset of the pipeline is required to get the right answer. Cool!To make apps performant, Streamlit only recomputes whatever is necessary to update the UI.Streamlit is built for GPUs. Streamlit allows direct access to machine-level primitives like TensorFlow and PyTorch and complements these libraries. For example in this demo, Streamlits cache stores the entire NVIDIA celebrity face GAN [2]. This approach enables nearly instantaneous inference as the user updates sliders.This Streamlit app demonstrates NVIDIA celebrity face GAN [2] model using Shaobo Guans TL-GAN [3].Streamlit is a free and open-source library rather than a proprietary web app. You can serve Streamlit apps on-prem without contacting us. You can even run Streamlit locally on a laptop without an Internet connection! Furthermore, existing projects can adopt Streamlit incrementally.Several ways incrementally adopt Streamlit. (Icons courtesy of fullvector / Freepik.)This just scratches the surface of what you can do with Streamlit. One of the most exciting aspects of Streamlit is how these primitives can be easily composed into complex apps that look like scripts. Theres a lot more we could say about how our architecture works and the features we have planned, but well save that for future posts.Block diagram of Streamlits components. More coming soon!Were excited to finally share Streamlit with the community today and see what you all build with it. We hope that youll find it easy and delightful to turn your Python scripts into beautiful ML apps.Thanks to Amanda Kelly, Thiago Teixeira, TC Ricks, Seth Weidman, Regan Carey, Beverly Treuille, Genevive Wachtell, and Barney Pell for their helpful input on this article.References:[1] J. Redmon and A. Farhadi, YOLOv3: An Incremental Improvement (2018), arXiv.[2] T. Karras, T. Aila, S. Laine, and J. Lehtinen, Progressive Growing of GANs for Improved Quality, Stability, and Variation (2018), ICLR.[3] S. Guan, Controlled image synthesis and editing using a novel TL-GAN model (2018), Insight Data Science Blog. "
        ],
        "story_type": "Normal",
        "url_raw": "https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace",
        "comments.comment_id": [21169495, 21170226],
        "comments.comment_author": ["_gwlb", "random42"],
        "comments.comment_descendants": [2, 0],
        "comments.comment_time": [
          "2019-10-06T02:43:38Z",
          "2019-10-06T06:33:03Z"
        ],
        "comments.comment_text": [
          "This looks really slick, can't wait to try it out!<p>If anyone is curious about other tools in the same space, our data scientists use Dash[1] and plotly to build interactive exploration and visualization apps. We set up a Git repo that deploys their apps internally with every merge to master, so they're actually building and updating tools that our operations, marketing, etc teams use every day.<p>[1] <a href=\"https://plot.ly/dash/\" rel=\"nofollow\">https://plot.ly/dash/</a>",
          "Interesting project, but why does an open source developer tool needs browser telemetry?<p>You should ask for telemetry permissions _before_ the process starts up (as you do for email address), and keep the default as \"No\", instead of start to send the data transparently unless non user friendly steps are taken by the user."
        ],
        "id": "1da01bb2-1ce3-4329-a97b-b34951d6a828",
        "url_text": "Introducing Streamlit, an app framework built for ML engineersCoding a semantic search engine with real-time neural-net inference in 300 lines of Python.In my experience, every nontrivial machine learning project is eventually stitched together with bug-ridden and unmaintainable internal tools. These tools often a patchwork of Jupyter Notebooks and Flask apps are difficult to deploy, require reasoning about client-server architecture, and dont integrate well with machine learning constructs like Tensorflow GPU sessions.I saw this first at Carnegie Mellon, then at Berkeley, Google X, and finally while building autonomous robots at Zoox. These tools were often born as little Jupyter notebooks: the sensor calibration tool, the simulation comparison app, the LIDAR alignment app, the scenario replay tool, and so on.As a tool grew in importance, project managers stepped in. Processes sprouted. Requirements flowered. These solo projects gestated into scripts, and matured into gangly maintenance nightmares.The machine learning engineers ad-hoc app building flow.When a tool became crucial, we called in the tools team. They wrote fluent Vue and React. They blinged their laptops with stickers about declarative frameworks. They had a design process:The tools teams clean-slate app building flow.Which was awesome. But these tools all needed new features, like weekly. And the tools team was supporting ten other projects. They would say, well update your tool again in two months.So we were back to building our own tools, deploying Flask apps, writing HTML, CSS, and JavaScript, and trying to version control everything from notebooks to stylesheets. So my old Google X friend, Thiago Teixeira, and I began thinking about the following question: What if we could make building tools as easy as writing Python scripts?We wanted machine learning engineers to be able to create beautiful apps without needing a tools team. These internal tools should arise as a natural byproduct of the ML workflow. Writing such tools should feel like training a neural net or performing an ad-hoc analysis in Jupyter! At the same time, we wanted to preserve all of the flexibility of a powerful app framework. We wanted to create beautiful, performant tools that engineers could show off. Basically, we wanted this:The Streamlit app building flow.With an amazing beta community including engineers from Uber, Twitter, Stitch Fix, and Dropbox, we worked for a year to create Streamlit, a completely free and open source app framework for ML engineers. With each prototype, the core principles of Streamlit became simpler and purer. They are:#1: Embrace Python scripting. Streamlit apps are really just scripts that run from top to bottom. Theres no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. For example, this is how you write to the screen:import streamlit as stst.write('Hello, world!')Nice to meet you.#2: Treat widgets as variables. There are no callbacks in Streamlit! Every interaction simply reruns the script from top to bottom. This approach leads to really clean code:import streamlit as stx = st.slider('x')st.write(x, 'squared is', x * x)An interactive Streamlit app in three lines of code.#3: Reuse data and computation. What if you download lots of data or perform complex computation? The key is to safely reuse information across runs. Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default, data store that lets Streamlit apps safely and effortlessly reuse information. For example, this code downloads data only once from the Udacity Self-driving car project, yielding a simple, fast app:Using st.cache to persist data across Streamlit runs. To run this code, please follow these instructions.The output of running the st.cache example above.In short, Streamlit works like this:The entire script is run from scratch for each user interaction.Streamlit assigns each variable an up-to-date value given widget states.Caching allows Streamlit to skip redundant data fetches and computation.Or in pictures:User events trigger Streamlit to rerun the script from scratch. Only the cache persists across runs.If this sounds intriguing, you can try it right now! Just run:$ pip install --upgrade streamlit $ streamlit hello You can now view your Streamlit app in your browser. Local URL: http://localhost:8501 Network URL: http://10.0.1.29:8501This will automatically pop open a web browser pointing to your local Streamlit app. If not, just click the link.To see more examples like this fractal animation, run streamlit hello from the command line.Ok. Are you back from playing with fractals? Those can be mesmerizing.The simplicity of these ideas does not prevent you from creating incredibly rich and useful apps with Streamlit. During my time at Zoox and Google X, I watched as self-driving car projects ballooned into gigabytes of visual data, which needed to be searched and understood, including running models on images to compare performance. Every self-driving car project Ive seen eventually has had entire teams working on this tooling.Building such a tool in Streamlit is easy. This Streamlit demo lets you perform semantic search across the entire Udacity self-driving car photo dataset, visualize human-annotated ground truth labels, and run a complete neural net (YOLO) in real time from within the app [1].This 300-line Streamlit demo combines semantic visual search with interactive neural net inference.The whole app is a completely self-contained, 300-line Python script, most of which is machine learning code. In fact, there are only 23 Streamlit calls in the whole app. You can run it yourself right now!$ pip install --upgrade streamlit opencv-python$ streamlit runhttps://raw.githubusercontent.com/streamlit/demo-self-driving/master/app.pyAs we worked with machine learning teams on their own projects, we came to realize that these simple ideas yield a number of important benefits:Streamlit apps are pure Python files. So you can use your favorite editor and debugger with Streamlit.My favorite layout for writing Streamlit apps has VSCode on the left and Chrome on the right.Pure Python scripts work seamlessly with Git and other source control software, including commits, pull requests, issues, and comments. Because Streamlits underlying language is pure Python, you get all the benefits of these amazing collaboration tools for free .Because Streamlit apps are just Python scripts, you can easily version control them with Git.Streamlit provides an immediate-mode live coding environment. Just click Always rerun when Streamlit detects a source file change.Click Always rerun to enable live coding.Caching simplifies setting up computation pipelines. Amazingly, chaining cached functions automatically creates efficient computation pipelines! Consider this code adapted from our Udacity demo:A simple computation pipeline in Streamlit. To run this code, please follow these instructions.Basically, the pipeline is load_metadata create_summary. Every time the script is run Streamlit only recomputes whatever subset of the pipeline is required to get the right answer. Cool!To make apps performant, Streamlit only recomputes whatever is necessary to update the UI.Streamlit is built for GPUs. Streamlit allows direct access to machine-level primitives like TensorFlow and PyTorch and complements these libraries. For example in this demo, Streamlits cache stores the entire NVIDIA celebrity face GAN [2]. This approach enables nearly instantaneous inference as the user updates sliders.This Streamlit app demonstrates NVIDIA celebrity face GAN [2] model using Shaobo Guans TL-GAN [3].Streamlit is a free and open-source library rather than a proprietary web app. You can serve Streamlit apps on-prem without contacting us. You can even run Streamlit locally on a laptop without an Internet connection! Furthermore, existing projects can adopt Streamlit incrementally.Several ways incrementally adopt Streamlit. (Icons courtesy of fullvector / Freepik.)This just scratches the surface of what you can do with Streamlit. One of the most exciting aspects of Streamlit is how these primitives can be easily composed into complex apps that look like scripts. Theres a lot more we could say about how our architecture works and the features we have planned, but well save that for future posts.Block diagram of Streamlits components. More coming soon!Were excited to finally share Streamlit with the community today and see what you all build with it. We hope that youll find it easy and delightful to turn your Python scripts into beautiful ML apps.Thanks to Amanda Kelly, Thiago Teixeira, TC Ricks, Seth Weidman, Regan Carey, Beverly Treuille, Genevive Wachtell, and Barney Pell for their helpful input on this article.References:[1] J. Redmon and A. Farhadi, YOLOv3: An Incremental Improvement (2018), arXiv.[2] T. Karras, T. Aila, S. Laine, and J. Lehtinen, Progressive Growing of GANs for Improved Quality, Stability, and Variation (2018), ICLR.[3] S. Guan, Controlled image synthesis and editing using a novel TL-GAN model (2018), Insight Data Science Blog. ",
        "_version_": 1718536529514594304
      },
      {
        "story_id": 21196107,
        "story_author": "joebaf",
        "story_descendants": 145,
        "story_score": 194,
        "story_time": "2019-10-08T19:25:11Z",
        "story_title": "C++ Ecosystem: Compilers, IDEs, Tools, Testing",
        "search": [
          "C++ Ecosystem: Compilers, IDEs, Tools, Testing",
          "https://www.bfilipek.com/2019/10/cppecosystem.html",
          "Table of Contents Introduction Compilers GCC Microsoft Visual C++ Clang Intel C++ Compiler Build Tools & Package Managers Make Cmake Ninja Microsoft Build Engine (MSBuild) Conan, Vcpkg, Buckaroo Integrated Development Environments Sublime Text, Atom, And Visual Studio Code Vi/Vim & Emacs Clion Qt Creator C++Builder Visual Studio Xcode KDevelop Eclipse CDT IDE Cevelop Android Studio Oracle Studio Extra: Compiler Explorer & Online Tools Debugging & Testing GDB LLDB Debugging Tools For Windows Mozillas RR CATCH/CATCH2 BOOST.TEST GOOGLE TEST CUTE DocTest Mull Sanitizers Valgrind HeapTrack Dr. Memory Deleaker Summary & More Your Turn To write a professional C++ application, you not only need a basic text editor and a compiler. You require some more tooling. In this blog post, youll see a broad list of tools that make C++ programming possible: compilers, IDEs, debuggers and other. Last Update: 14th October 2019. Note: This is a blog post based on the White Paper created by Embarcadero, see the full paper here: C++ Ecosystem White Paper. Introduction The C++ computer programming language has become one of the most widely used modern programming languages. Software built with C++ is known for its performance and efficiency. C++ has been used to build numerous vastly popular core libraries, applications such as Microsoft Office, game engines such as Unreal, software tools like Adobe Photoshop, compilers like Clang, databases like MySQL, and even operating systems such as Windows across a wide variety of platforms as it continues to evolve and grow. Modern C++ is generally defined as C++ code that utilizes language features in C++11, C++14, and C++17 based compilers. These are language standards named after the year they were defined (2011, 2014 and 2017 respectively) and include a number of significant additions and enhancements to the original core language for powerful, highly performant, and bug-free code. Modern C++ has high-level features that support object-oriented programming, functional programming, generic programming, and low-level memory manipulation features. Big names in the computer industry such as Microsoft, Intel, the Free Software Foundation, and others have their modern C++ compilers. Companies such as Microsoft, The QT Company, JetBrains, and Embarcadero provide integrated development environments for writing code in modern C++. Popular libraries are available for C++ across a wide range of computer disciplines including Artificial Intelligence, Machine Learning, Robotics, Math, Scientific Computing, Audio Processing, and Image Processing. In this blog post, we are going to cover a number of these compilers, build tools, IDEs, libraries, frameworks, coding assistants, and much more that can support and enhance your development with modern C++. Lets get started! Compilers There are a number of popular compilers that support modern C++ including GCC/g++, MSVC (Microsoft Visual C++), Clang and Intel Compiler. Each compiler has varying support for each of the major operating systems with the open-source GCC/g++ originating in the late 1980s, Microsofts Visual C++ in the early 1990s, and Clang in the late 2000s. All four compilers support modern C++ up to at least C++17, but the source code licenses for each of them vary greatly. GCC GCC is a general-use compiler developed and maintained and regularly updated by the GCC Steering committee as part of the GNU Project. GCC describes a large growing family of compilers targeting many hardware platforms and several languages. While it mainly targets Unix-like platforms, Windows support is provided through the Cygwin or MinGW runtime libraries. GCC compiles modern C++ code up to C++17 with experimental support for some C++20 features. It also compiles with a variety of language extensions that build upon C++ standards. It is free and open-source (GPL3) with the GCC Runtime Library Exception. GCC has support from build tools such as CMake and Ninja and many IDEs such as CLion, Qt Creator, and Visual Studio Code. https://gcc.gnu.org/ https://gcc.gnu.org/projects/cxx-status.html Microsoft Visual C++ Microsoft Visual C++ (MSVC) is Microsofts compiler for their custom implementation of the C++ standard, known as Visual C++. It is regularly updated, and like GCC and Clang, supports modern C++ standards up to C++17 with experimental support for some C++20 features. MSVC is the primary method for building C++ applications in Microsofts own Visual Studio. It generally targets a number of architectures on Windows, Android, iOS, and Linux. Support for build tools and IDEs are limited but growing. CMake extensions are available in Visual Studio 2019. MSVC can be used with Visual Studio Code, with limited support from CLion and Qt Creator with additional extensions. MSVC is proprietary and available under a commercial license, but theres also a Community edition. https://en.wikipedia.org/wiki/Microsoft_Visual_C%2B%2B https://devblogs.microsoft.com/visualstudio/ https://visualstudio.microsoft.com/vs/community/ Clang Clang describes a large family of compilers for the C family of languages maintained and regularly developed as part of the LLVM project. Although it targets many popular architectures, it generally targets fewer platforms than GCC. The LLVM project defines Clang through key design principles - strict adherence to C++ standards (although support for GCC extensions is offered), modular design, and minimal modification to the source codes structure during compilation, to name a few. Like GCC, Clang compiles modern C++ code with support for the C++17 standard with experimental C++20 support. It is available under an open-source (Apache License Version 2.0) license. Clang also has widespread support from build tools such as CMake and Ninja and IDEs such as CLion, Qt Creator, Xcode, and others. https://clang.llvm.org/ https://clang.llvm.org/cxx_status.html Intel C++ Compiler Intel C++ Compiler can generate highly optimized code for various Intel CPUs (including Xeon, Core, and Atom processors). The compiler can seamlessly integrate with popular IDE like Visual Studio, GCC toolchain and others. It can leverage advanced instruction set (even AVX512) and generate parallel code (for example, thanks to OpenMP 5.0 support). Intel doesnt ship the compiler with the Standard Library implementation, so it uses the library you provide on your platform. The compiler is available as a part of Intel Parallel Studio XE or Intel System Studio. https://software.intel.com/en-us/c-compilers https://software.intel.com/en-us/articles/c17-features-supported-by-intel-c-compiler On top of compilers, you need an infrastructure that helps to build a whole application: build tools, pipelines and package managers. Make Make is a well-known build system widely used, especially in Unix and Unix-like operating systems. Make is typically used to build executable programs and libraries from source code. But the tool applies to any process that involves executing arbitrary commands to transform a source file to a target result. Make is not tight to any particular programming language. It automatically determines which source files has been changed and then performs the minimal build process to get the final output. It also helps with the installation of the results in the system https://www.gnu.org/software/make/ Cmake CMake is a cross-platform tool for managing your build process. Building, especially large apps and with dependent libraries, can be a very complex process, especially when you support multiple compilers; CMake abstracts this. You can define complex build processes in one common language and convert them to native build directives for any number of supported compilers, IDEs, and build tools, including Ninja (below.) There are versions of CMake available for Windows, macOS, and Linux. https://cmake.org/ Note: Heres a good answer about the differences between Make and Cmake: Difference between using Makefile and CMake to compile the code - Stack Overflow. Ninja The Ninja build system is used for the actual process of building apps and is similar to Make. It focuses on running as fast as possible by parallelizing builds. It is commonly used paired with CMake, which supports creating build files for the Ninja build system. The feature set of Ninja is intentionally kept minimal because the focus is on speed. https://ninja-build.org/ Microsoft Build Engine (MSBuild) MSBuild is a command-line based built platform available from Microsoft under an open-source (MIT) license. It can be used to automate the process of compiling and deploying projects. It is available standalone, packaged with Visual Studio, or from Github. The structure and function of MSBuild files is very similar to Make. MSBuild has an XML based file format and mainly has support for Windows but also macOS and Linux. IDEs such as CLion and C++Builder can integrate with MSBuild as well. https://docs.microsoft.com/en-us/visualstudio/msbuild/msbuild Conan, Vcpkg, Buckaroo Package managers such as Conan, vcpkg, Buckaroo and NIX have been gaining popularity in the C++ community. A package manager is a tool to install libraries or components. Conan is a decentralized open-source (MIT) package manager that supports multiple platforms and all build systems (such as CMake and MSBuild). Conan supports binaries with a goal of automating dependency management to save time in development and continuous integration. Microsofts vcpkg is open source under an MIT license and supports Windows, macOS, and Linux. Out of the box, it makes installed libraries available in Visual Studio, but it also supports CMake build recipes. It can build libs for every toolchain that can be fitted into CMake. Buckaroo is a lesser-known open-source package manager that can pull dependencies from GitHub, BitBucket, GitLab, and others. Buckaroo offers integrations for a number of IDEs including CLion, Visual Studio Code, XCode, and others. Here are the links for the mentioned package managers: https://conan.io/ https://github.com/microsoft/vcpkg https://buckaroo.pm/ Integrated Development Environments A host of editors and integrated development environments (IDEs) can be used for developing with modern C++. Text editors are typically lightweight, but are less featureful than a full IDE and so are used only for the process of writing code, not debugging or testing it. Full development requires other tools, and an IDE contains those and integrates into a cohesive, integrated development environment. Any number of text editors like Sublime Text, Atom, Visual Studio Code, vi/vim, and Emacs can be used for writing C++ code. However, some IDEs are specifically designed with modern C++ in mind like CLion, Qt Creator, and C++Builder, while IDEs like Xcode and Visual Studio also support other languages. You can also compare various IDE for C++ in this handy table on Wikipedia: Comparison of integrated development environments - C++ - Wikipedia Sublime Text, Atom, And Visual Studio Code The list below summarises a set of advanced source code editors that thanks to various plugins and extensions allow creating applications in almost all programming languages. Sublime Text is a commercial text editor with extended support for modern C++ available via plugins. Atom is an open-source (MIT license) text editor that supports modern C++ via packages with integrations available for debugging and compiling. Visual Studio Code is a popular open-source (MIT license) source-code editor from Microsoft. A wide variety of extensions are available that bring features such as debugging and code completion for modern C++ to Visual Studio Code. Sublime Text, Atom, and Visual Studio Code are all available for Windows, macOS, and Linux. Here are the links for the above tools: https://www.sublimetext.com/ https://atom.io/ https://code.visualstudio.com/ Vi/Vim & Emacs Vi/Vim and Emacs are free command-line based text editors that are mainly used on Linux but are also available for macOS and Windows. Modern C++ support can be added to Vi/Vim through the use of scripts while modern C++ support can be added to Emacs through the use of modules. https://www.vim.org/ https://www.gnu.org/software/emacs/ Clion CLion is a commercial IDE from JetBrains that supports modern C++. It can be used with build tools like CMake and Gradle, integrates with the GDB and LLDB debuggers, can be used with version control systems like Git, test libraries like Boost.Test, and various documentation tools. It has features such as code generation, refactoring, on the fly code analysis, symbol navigation, and more. https://www.jetbrains.com/clion/ Qt Creator Qt Creator is a (non)commercial IDE from The Qt Company which supports Windows, macOS, and Linux. Qt Creator has features such as a UI designer, syntax highlighting, auto-completion, and integration with a number of different modern C++ compilers like GCC and Clang. Qt Creator tightly integrates with the Qt library for rapidly building cross-platform applications. Additionally, it integrates with standard version control systems like Git, debuggers like GDB and LLDB, build systems like CMake, and can deploy cross-platform to iOS and Android devices. https://www.qt.io/ C++Builder C++Builder is a commercial IDE from Embarcadero Technologies which runs on Windows and supports modern C++. It features the award-winning Visual Component Library (VCL) for Windows development and FireMonkey (FMX) for cross-platform development for Windows, iOS and Android. The C++Builder compiler features an enhanced version of Clang, an integrated debugger, visual UI designer, database library, comprehensive RTL, and standard features like syntax highlighting, code completion, and refactoring. C++Builder has integrations for CMake, can be used with Ninja, and also MSBuild. https://www.embarcadero.com/products/cbuilder https://www.embarcadero.com/products/cbuilder/starter Visual Studio Visual C++ is a commercial Visual Studio IDE from Microsoft. Visual Studio integrates building, debugging, and testing within the IDE. It provides the Microsoft Foundation Class (MFC) library which gives access to the Win32 APIs. Visual Studio features a visual UI designer for certain platforms, comes with MSBuild, supports CMake, and provides standard features such as code completion, refactoring, and syntax highlighting. Additionally, Visual Studio supports a number of other programming languages, and the C++ side of it is focused on Windows, with other platforms slowly being added. https://visualstudio.microsoft.com/ Xcode Xcode is a multi-language IDE from Apple available only on macOS that supports modern C++. Xcode is proprietary but available for free from Apple. Xcode has an integrated debugger, supports version control systems like Git, features a Clang compiler, and utilizes libc++ as its standard library. It supports standard features such as syntax highlighting, code completion, and finally, Xcode supports external build systems like CMake and utilizes the LLDB debugger. https://developer.apple.com/xcode/ KDevelop KDevelop (its 0.1 version was released in 1998) is a cross-platform IDE for C, C++, Python, QML/JavaScript and PHP. This IDE is part of the KDE project, and is based on KDE Frameworks and Qt. The C/C++ backend uses Clang and LLVM. It has UI integration with several version control systems: Git, SVN, Bazaar and more, build process based on CMake, QMake or custom makefiles. Among many interesting features, its essential to mention advanced syntax colouring and Context-sensitive, semantic code completion. https://www.kdevelop.org/ https://www.kdevelop.org/features Eclipse CDT IDE The Eclipse C/C++ Development Toolkit (CDT) is a combination of the Eclipse IDE with a C++ toolchain (usually GNU - GCC). This IDE supports project creation and build management for various toolchains, like the standard make build. CDT IDE offers source navigation, various source knowledge tools, such as type hierarchy, call graph, include browser, macro definition browser, code editor with syntax highlighting, folding and hyperlink navigation, source code refactoring and code generation, visual debugging tools, including memory, registers, and disassembly viewers. https://www.eclipse.org/cdt/ Cevelop Cevelop is a powerful IDE based Eclipse CDT. Its main strength lies in the powerful refactoring and static analysis support for code modernization. In addition, it comes with unit testing and TDD support for the CUTE unit testing framework. Whats more, you can easily visualize your template instantiation/function overload resolution and optimize includes. https://www.cevelop.com/ Android Studio Android Studio is the official IDE for Googles Android operating system, built on JetBrains IntelliJ IDEA software and designed specifically for Android development. It is available for download on Windows, macOS and Linux based operating systems. It is a replacement for the Eclipse Android Development Tools (ADT) as the primary IDE for native Android application development. Android Studio focuses mainly on Kotlin but you can also write applications in C++. Oracle Studio Oracle Developer Studio is Oracle Corporations flagship software development product for the Solaris and Linux operating systems. It includes optimizing C, C++, and Fortran compilers, libraries, and performance analysis and debugging tools, for Solaris on SPARC and x86 platforms, and Linux on x86/x64 platforms, including multi-core systems. You can download Developer Studio at no charge but if you want the full support and patch updates, then you need a paid support contract. The C++ Compiler supports C++14. https://www.oracle.com/technetwork/server-storage/developerstudio/overview/index.html https://www.oracle.com/technetwork/server-storage/solarisstudio/features/compilers-2332272.html If you want to check some shorter code samples and you dont want to install the whole compiler/.IDE suite then we have lots of online tools that can make those tests super simple. Just open a web browser and put the code Compiler Explorer is a web-based tool that allows you to select from a wide variety of C++ compilers and different versions of the same compiler to test out your code. This allows developers to compare the generated code for specific C++ constructs among compilers, and test for correct behaviour. Clang, GCC, and MSVC are all there but also lesser-known compilers such as DJGPP, ELLCC, Intel C++, and others. https://godbolt.org/ Extra: Heres a list of handy online compilers that you can use: like Coliru, Wandbox, CppInsighs and more: https://arnemertz.github.io/online-compilers/ Debugging & Testing GDB GDB is a portable command-line based debugging platform that supports modern C++ and is available under an open-source license (GPL). A number of editors and IDEs like Visual Studio, Qt Creator, and CLion support integration with GDB. It can also be used to debug applications remotely where GDB is running on one device, and the application being debugged is running on another device. It supports a number of platforms including Windows, macOS, and Linux. https://www.gnu.org/software/gdb/ LLDB LLDB is an open-source debugging interface that supports modern C++ and integrates with the Clang compiler. It has a number of optional performance-enhancing features such as JIT but also supports debugging memory, multiple threads, and machine code analysis. It is built in C++. LLDB is the default debugger for Xcode and can be used with Visual Studio Code, CLion, and Qt Creator. It supports a number of platforms including Windows, macOS, and Linux. https://lldb.llvm.org/ Debugging Tools For Windows On Windows, you can use several debuggers, ranging from Visual Studio Debugger (integrated and one of the most user-friendly), WinDBG, CDB and several more. WinDbg is a multipurpose debugger for the Microsoft Windows Platform. It can be used to debug user-mode applications, device drivers, and the operating system itself in kernel mode. It has a graphical user interface (GUI) and is more powerful than Visual Studio Debugger. You can debug memory dumps obtained even from kernel drivers. One of the recent exciting features in Debugging on Windows is called Time Travel Debugging (Available in WinDBG Preview and also in Visual Studio Ultimate). It allows you to record the execution of the process and then replay the steps backwards or forwards. This flexibility enables us to easily tracks back the state that caused a bug. https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/ https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/time-travel-debugging-overview Mozillas RR RR is an advanced debugger that aims to replace GDB on Linux. It offers the full state recordings of the application so that you can replay the action backwards and forwards (similarly to Time Travel Debugging). The debugger is used to work with large applications like Chrome, OpenOffice or even Firefox code bases. https://rr-project.org/ CATCH/CATCH2 Catch2 is a cross-platform open-source (BSL-1.0) testing framework for modern C++. It is very lightweight because only a header file needs to be included. Unit tests can be tagged and run in groups. It supports both test-driven development and behaviour-driven development. Catch2 also easily integrates with CLion. https://github.com/catchorg/Catch2 BOOST.TEST Boost.Test is a feature-rich open-source (BSL-1.0) testing framework that utilizes modern C++ standards. It can be used to quickly detect errors, failures, and time outs through customizable logging and real-time monitoring. Tests can be grouped into suites, and the framework supports both small scale testing and large scale testing. https://github.com/boostorg/test GOOGLE TEST Google Test is Googles C++ testing and mocking framework, which is available under an open-source (BSD) license. Google test can be used on a broad range of platforms, including Linux, macOS, Windows, and others. It contains a unit testing framework, assertions, death tests, detects failures, handles parameterized tests, and creates XML test reports. https://github.com/google/googletest CUTE CUTE is a unit testing framework integrated into Cevelop, but it can also be used standalone. It spans C++ versions from c++98 to c++2a and is header-only. While not as popular as Google Test it is less macro-ridden and uses macros only, where no appropriate C++ feature is available. In addition, it features a mode that easily allows it to run on embedded platforms, by sidestepping some of the I/O formatting features. https://cute-test.com/ DocTest DocTest is a single-header unit testing framework. Available for C++11 up to C++20 and is easy to configure and works on probably all platforms. It offers regular TDD testing macros (also with subcases) as well as BDD-style test cases. http://bit.ly/doctest-docs https://github.com/onqtam/doctest Mull Mull is an LLVM-based tool for Mutation Testing with a strong focus on C and C++ languages. In general, it creates many variations of the input source code (using LLVM bytecode) and then checks it against the test cases. Thanks to this advanced testing technique, you can make your code more secure. https://github.com/mull-project/mull PDF: https://lowlevelbits.org/pdfs/Mull_Mutation_2018.pdf Sanitizers AddressSanitizer - https://clang.llvm.org/docs/AddressSanitizer.html (supported in Clang, GCC and XCode) UndefinedBehaviorSanitizer - https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html LeakSanitizer - https://clang.llvm.org/docs/LeakSanitizer.html Application Verifier for Windows - https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/application-verifier Sanitizers are relatively new tools that add extra instrumentation to your application (for example they replace new/malloc/delete calls) and can detect various runtime errors: leaks, use after delete, double free and many others. To improve your build pipeline, many guides advice to add sanitizers steps when doing tests. Most sanitizers come from the LLVM/Clang platform, but now they also work with GCC. Unfortunately not yet with Visual Studio (but you can try Application Verifier). Valgrind Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. When you run a program through Valgrind its run on a virtual machine that emulates your host environment. Having that abstraction the tools can leverage various information about the source code and its execution. http://valgrind.org/ http://valgrind.org/info/about.html http://valgrind.org/docs/manual/quick-start.html HeapTrack HeapTrack is a FOSS project and a heap memory profiler for Linux. It traces all memory allocations and annotates these events with stack traces. The tool has two forms the command line version that grabs the data, and then the UI part that you can use to read and analyze the results. This tool is comparable to Valgrinds massif; its easier to use and should be faster to load and analyze for large projects. https://github.com/KDE/heaptrack Dr. Memory Dr. Memory is an LGPL licenced tool that allows you to monitor and intensify memory -related errors for binaries on Windows, Linux, Mac, Android. Its based on the DynamoRIO dynamic instrumentation tool platform. With the tool, you can find errors like double frees, memory leaks, handle leaks (on Windows), GDI issues, access to uninitialized memory or even errors in multithreading memory scenarios. http://drmemory.org/ https://github.com/DynamoRIO/drmemory Deleaker The primary role of Deleaker is to find leaks in your native applications. It supports Visual Studio (since 2008 till the latest 2019 version), Delphi/C++ Builder, Qt Creator, CLion (soon!). Can be used as an extension in Visual Studio or as a standalone application. Deleaker tracks leaks in C/C++ applications (Native and CLR), plus .NET code. Memory (new/delete, malloc), GDI objects, User32 objects, Handles, File views, Fibres, Critical Sections, and even more. It gathers full call stack, ability to take snapshots, compare them, view source files related to allocation. https://www.deleaker.com/ https://www.deleaker.com/docs/deleaker/tutorial.html Summary & More I hope that with the above list, you get a useful overview of the tools that are essential for C++ development. If you want to read more about other ecosystem elements: libraries, frameworks, and other tools, then please see the full report from Embarcadero: C++ Ecosystem White Paper (Its a nice looking pdf, with more than 20 pages of content!) You might check this Resource for a super long list of tools, libs, frameworks that enhance C++ development: https://github.com/fffaraz/awesome-cpp Your Turn What are your favourite tools that you use when writing C++ apps? "
        ],
        "story_type": "Normal",
        "url_raw": "https://www.bfilipek.com/2019/10/cppecosystem.html",
        "comments.comment_id": [21199733, 21201412],
        "comments.comment_author": ["cmrdporcupine", "Iv"],
        "comments.comment_descendants": [9, 0],
        "comments.comment_time": [
          "2019-10-09T03:27:23Z",
          "2019-10-09T09:10:25Z"
        ],
        "comments.comment_text": [
          "CLion is an amazing tool -- I've purchased licenses for my personal self in the past, but my employer pays for it these days.<p>My problem is they've done a terrible job of making it scale up to large code bases. I work on the chromium tree -- CLion is completely useless on it.  I have a dual 24-core xeon with 128GB of RAM and SSD and I've given it a wackload of memory, and it becomes completely inoperable, freezing all over the place.<p>Awful because I have such muscle memory for the JetBrains tools, and such a fondness for them.<p>I've gone back to using Emacs, but now with Eclim. I just couldn't get into VSCode.",
          "I had to use Qt as the UI lib for a project, it made me discover that QtCreator was actually not a Qt-only tool but a very good lightweight and generic C++ IDE.<p>That's my choice now. I need something that can navigate easily in a code base, I don't really like learning all the oddities around emacs and vim (even though I am a bit competent at vim) and I don't see what is so bad in using a mouse.<p>At first I thought annoying to have to manually edit the .includes and .config to add the includes and the macro I needed in our complex, hard-to-parse CMake based project, but now I really enjoy the freedom it gives."
        ],
        "id": "e1460a46-e683-4942-9aed-7e1292252c1e",
        "url_text": "Table of Contents Introduction Compilers GCC Microsoft Visual C++ Clang Intel C++ Compiler Build Tools & Package Managers Make Cmake Ninja Microsoft Build Engine (MSBuild) Conan, Vcpkg, Buckaroo Integrated Development Environments Sublime Text, Atom, And Visual Studio Code Vi/Vim & Emacs Clion Qt Creator C++Builder Visual Studio Xcode KDevelop Eclipse CDT IDE Cevelop Android Studio Oracle Studio Extra: Compiler Explorer & Online Tools Debugging & Testing GDB LLDB Debugging Tools For Windows Mozillas RR CATCH/CATCH2 BOOST.TEST GOOGLE TEST CUTE DocTest Mull Sanitizers Valgrind HeapTrack Dr. Memory Deleaker Summary & More Your Turn To write a professional C++ application, you not only need a basic text editor and a compiler. You require some more tooling. In this blog post, youll see a broad list of tools that make C++ programming possible: compilers, IDEs, debuggers and other. Last Update: 14th October 2019. Note: This is a blog post based on the White Paper created by Embarcadero, see the full paper here: C++ Ecosystem White Paper. Introduction The C++ computer programming language has become one of the most widely used modern programming languages. Software built with C++ is known for its performance and efficiency. C++ has been used to build numerous vastly popular core libraries, applications such as Microsoft Office, game engines such as Unreal, software tools like Adobe Photoshop, compilers like Clang, databases like MySQL, and even operating systems such as Windows across a wide variety of platforms as it continues to evolve and grow. Modern C++ is generally defined as C++ code that utilizes language features in C++11, C++14, and C++17 based compilers. These are language standards named after the year they were defined (2011, 2014 and 2017 respectively) and include a number of significant additions and enhancements to the original core language for powerful, highly performant, and bug-free code. Modern C++ has high-level features that support object-oriented programming, functional programming, generic programming, and low-level memory manipulation features. Big names in the computer industry such as Microsoft, Intel, the Free Software Foundation, and others have their modern C++ compilers. Companies such as Microsoft, The QT Company, JetBrains, and Embarcadero provide integrated development environments for writing code in modern C++. Popular libraries are available for C++ across a wide range of computer disciplines including Artificial Intelligence, Machine Learning, Robotics, Math, Scientific Computing, Audio Processing, and Image Processing. In this blog post, we are going to cover a number of these compilers, build tools, IDEs, libraries, frameworks, coding assistants, and much more that can support and enhance your development with modern C++. Lets get started! Compilers There are a number of popular compilers that support modern C++ including GCC/g++, MSVC (Microsoft Visual C++), Clang and Intel Compiler. Each compiler has varying support for each of the major operating systems with the open-source GCC/g++ originating in the late 1980s, Microsofts Visual C++ in the early 1990s, and Clang in the late 2000s. All four compilers support modern C++ up to at least C++17, but the source code licenses for each of them vary greatly. GCC GCC is a general-use compiler developed and maintained and regularly updated by the GCC Steering committee as part of the GNU Project. GCC describes a large growing family of compilers targeting many hardware platforms and several languages. While it mainly targets Unix-like platforms, Windows support is provided through the Cygwin or MinGW runtime libraries. GCC compiles modern C++ code up to C++17 with experimental support for some C++20 features. It also compiles with a variety of language extensions that build upon C++ standards. It is free and open-source (GPL3) with the GCC Runtime Library Exception. GCC has support from build tools such as CMake and Ninja and many IDEs such as CLion, Qt Creator, and Visual Studio Code. https://gcc.gnu.org/ https://gcc.gnu.org/projects/cxx-status.html Microsoft Visual C++ Microsoft Visual C++ (MSVC) is Microsofts compiler for their custom implementation of the C++ standard, known as Visual C++. It is regularly updated, and like GCC and Clang, supports modern C++ standards up to C++17 with experimental support for some C++20 features. MSVC is the primary method for building C++ applications in Microsofts own Visual Studio. It generally targets a number of architectures on Windows, Android, iOS, and Linux. Support for build tools and IDEs are limited but growing. CMake extensions are available in Visual Studio 2019. MSVC can be used with Visual Studio Code, with limited support from CLion and Qt Creator with additional extensions. MSVC is proprietary and available under a commercial license, but theres also a Community edition. https://en.wikipedia.org/wiki/Microsoft_Visual_C%2B%2B https://devblogs.microsoft.com/visualstudio/ https://visualstudio.microsoft.com/vs/community/ Clang Clang describes a large family of compilers for the C family of languages maintained and regularly developed as part of the LLVM project. Although it targets many popular architectures, it generally targets fewer platforms than GCC. The LLVM project defines Clang through key design principles - strict adherence to C++ standards (although support for GCC extensions is offered), modular design, and minimal modification to the source codes structure during compilation, to name a few. Like GCC, Clang compiles modern C++ code with support for the C++17 standard with experimental C++20 support. It is available under an open-source (Apache License Version 2.0) license. Clang also has widespread support from build tools such as CMake and Ninja and IDEs such as CLion, Qt Creator, Xcode, and others. https://clang.llvm.org/ https://clang.llvm.org/cxx_status.html Intel C++ Compiler Intel C++ Compiler can generate highly optimized code for various Intel CPUs (including Xeon, Core, and Atom processors). The compiler can seamlessly integrate with popular IDE like Visual Studio, GCC toolchain and others. It can leverage advanced instruction set (even AVX512) and generate parallel code (for example, thanks to OpenMP 5.0 support). Intel doesnt ship the compiler with the Standard Library implementation, so it uses the library you provide on your platform. The compiler is available as a part of Intel Parallel Studio XE or Intel System Studio. https://software.intel.com/en-us/c-compilers https://software.intel.com/en-us/articles/c17-features-supported-by-intel-c-compiler On top of compilers, you need an infrastructure that helps to build a whole application: build tools, pipelines and package managers. Make Make is a well-known build system widely used, especially in Unix and Unix-like operating systems. Make is typically used to build executable programs and libraries from source code. But the tool applies to any process that involves executing arbitrary commands to transform a source file to a target result. Make is not tight to any particular programming language. It automatically determines which source files has been changed and then performs the minimal build process to get the final output. It also helps with the installation of the results in the system https://www.gnu.org/software/make/ Cmake CMake is a cross-platform tool for managing your build process. Building, especially large apps and with dependent libraries, can be a very complex process, especially when you support multiple compilers; CMake abstracts this. You can define complex build processes in one common language and convert them to native build directives for any number of supported compilers, IDEs, and build tools, including Ninja (below.) There are versions of CMake available for Windows, macOS, and Linux. https://cmake.org/ Note: Heres a good answer about the differences between Make and Cmake: Difference between using Makefile and CMake to compile the code - Stack Overflow. Ninja The Ninja build system is used for the actual process of building apps and is similar to Make. It focuses on running as fast as possible by parallelizing builds. It is commonly used paired with CMake, which supports creating build files for the Ninja build system. The feature set of Ninja is intentionally kept minimal because the focus is on speed. https://ninja-build.org/ Microsoft Build Engine (MSBuild) MSBuild is a command-line based built platform available from Microsoft under an open-source (MIT) license. It can be used to automate the process of compiling and deploying projects. It is available standalone, packaged with Visual Studio, or from Github. The structure and function of MSBuild files is very similar to Make. MSBuild has an XML based file format and mainly has support for Windows but also macOS and Linux. IDEs such as CLion and C++Builder can integrate with MSBuild as well. https://docs.microsoft.com/en-us/visualstudio/msbuild/msbuild Conan, Vcpkg, Buckaroo Package managers such as Conan, vcpkg, Buckaroo and NIX have been gaining popularity in the C++ community. A package manager is a tool to install libraries or components. Conan is a decentralized open-source (MIT) package manager that supports multiple platforms and all build systems (such as CMake and MSBuild). Conan supports binaries with a goal of automating dependency management to save time in development and continuous integration. Microsofts vcpkg is open source under an MIT license and supports Windows, macOS, and Linux. Out of the box, it makes installed libraries available in Visual Studio, but it also supports CMake build recipes. It can build libs for every toolchain that can be fitted into CMake. Buckaroo is a lesser-known open-source package manager that can pull dependencies from GitHub, BitBucket, GitLab, and others. Buckaroo offers integrations for a number of IDEs including CLion, Visual Studio Code, XCode, and others. Here are the links for the mentioned package managers: https://conan.io/ https://github.com/microsoft/vcpkg https://buckaroo.pm/ Integrated Development Environments A host of editors and integrated development environments (IDEs) can be used for developing with modern C++. Text editors are typically lightweight, but are less featureful than a full IDE and so are used only for the process of writing code, not debugging or testing it. Full development requires other tools, and an IDE contains those and integrates into a cohesive, integrated development environment. Any number of text editors like Sublime Text, Atom, Visual Studio Code, vi/vim, and Emacs can be used for writing C++ code. However, some IDEs are specifically designed with modern C++ in mind like CLion, Qt Creator, and C++Builder, while IDEs like Xcode and Visual Studio also support other languages. You can also compare various IDE for C++ in this handy table on Wikipedia: Comparison of integrated development environments - C++ - Wikipedia Sublime Text, Atom, And Visual Studio Code The list below summarises a set of advanced source code editors that thanks to various plugins and extensions allow creating applications in almost all programming languages. Sublime Text is a commercial text editor with extended support for modern C++ available via plugins. Atom is an open-source (MIT license) text editor that supports modern C++ via packages with integrations available for debugging and compiling. Visual Studio Code is a popular open-source (MIT license) source-code editor from Microsoft. A wide variety of extensions are available that bring features such as debugging and code completion for modern C++ to Visual Studio Code. Sublime Text, Atom, and Visual Studio Code are all available for Windows, macOS, and Linux. Here are the links for the above tools: https://www.sublimetext.com/ https://atom.io/ https://code.visualstudio.com/ Vi/Vim & Emacs Vi/Vim and Emacs are free command-line based text editors that are mainly used on Linux but are also available for macOS and Windows. Modern C++ support can be added to Vi/Vim through the use of scripts while modern C++ support can be added to Emacs through the use of modules. https://www.vim.org/ https://www.gnu.org/software/emacs/ Clion CLion is a commercial IDE from JetBrains that supports modern C++. It can be used with build tools like CMake and Gradle, integrates with the GDB and LLDB debuggers, can be used with version control systems like Git, test libraries like Boost.Test, and various documentation tools. It has features such as code generation, refactoring, on the fly code analysis, symbol navigation, and more. https://www.jetbrains.com/clion/ Qt Creator Qt Creator is a (non)commercial IDE from The Qt Company which supports Windows, macOS, and Linux. Qt Creator has features such as a UI designer, syntax highlighting, auto-completion, and integration with a number of different modern C++ compilers like GCC and Clang. Qt Creator tightly integrates with the Qt library for rapidly building cross-platform applications. Additionally, it integrates with standard version control systems like Git, debuggers like GDB and LLDB, build systems like CMake, and can deploy cross-platform to iOS and Android devices. https://www.qt.io/ C++Builder C++Builder is a commercial IDE from Embarcadero Technologies which runs on Windows and supports modern C++. It features the award-winning Visual Component Library (VCL) for Windows development and FireMonkey (FMX) for cross-platform development for Windows, iOS and Android. The C++Builder compiler features an enhanced version of Clang, an integrated debugger, visual UI designer, database library, comprehensive RTL, and standard features like syntax highlighting, code completion, and refactoring. C++Builder has integrations for CMake, can be used with Ninja, and also MSBuild. https://www.embarcadero.com/products/cbuilder https://www.embarcadero.com/products/cbuilder/starter Visual Studio Visual C++ is a commercial Visual Studio IDE from Microsoft. Visual Studio integrates building, debugging, and testing within the IDE. It provides the Microsoft Foundation Class (MFC) library which gives access to the Win32 APIs. Visual Studio features a visual UI designer for certain platforms, comes with MSBuild, supports CMake, and provides standard features such as code completion, refactoring, and syntax highlighting. Additionally, Visual Studio supports a number of other programming languages, and the C++ side of it is focused on Windows, with other platforms slowly being added. https://visualstudio.microsoft.com/ Xcode Xcode is a multi-language IDE from Apple available only on macOS that supports modern C++. Xcode is proprietary but available for free from Apple. Xcode has an integrated debugger, supports version control systems like Git, features a Clang compiler, and utilizes libc++ as its standard library. It supports standard features such as syntax highlighting, code completion, and finally, Xcode supports external build systems like CMake and utilizes the LLDB debugger. https://developer.apple.com/xcode/ KDevelop KDevelop (its 0.1 version was released in 1998) is a cross-platform IDE for C, C++, Python, QML/JavaScript and PHP. This IDE is part of the KDE project, and is based on KDE Frameworks and Qt. The C/C++ backend uses Clang and LLVM. It has UI integration with several version control systems: Git, SVN, Bazaar and more, build process based on CMake, QMake or custom makefiles. Among many interesting features, its essential to mention advanced syntax colouring and Context-sensitive, semantic code completion. https://www.kdevelop.org/ https://www.kdevelop.org/features Eclipse CDT IDE The Eclipse C/C++ Development Toolkit (CDT) is a combination of the Eclipse IDE with a C++ toolchain (usually GNU - GCC). This IDE supports project creation and build management for various toolchains, like the standard make build. CDT IDE offers source navigation, various source knowledge tools, such as type hierarchy, call graph, include browser, macro definition browser, code editor with syntax highlighting, folding and hyperlink navigation, source code refactoring and code generation, visual debugging tools, including memory, registers, and disassembly viewers. https://www.eclipse.org/cdt/ Cevelop Cevelop is a powerful IDE based Eclipse CDT. Its main strength lies in the powerful refactoring and static analysis support for code modernization. In addition, it comes with unit testing and TDD support for the CUTE unit testing framework. Whats more, you can easily visualize your template instantiation/function overload resolution and optimize includes. https://www.cevelop.com/ Android Studio Android Studio is the official IDE for Googles Android operating system, built on JetBrains IntelliJ IDEA software and designed specifically for Android development. It is available for download on Windows, macOS and Linux based operating systems. It is a replacement for the Eclipse Android Development Tools (ADT) as the primary IDE for native Android application development. Android Studio focuses mainly on Kotlin but you can also write applications in C++. Oracle Studio Oracle Developer Studio is Oracle Corporations flagship software development product for the Solaris and Linux operating systems. It includes optimizing C, C++, and Fortran compilers, libraries, and performance analysis and debugging tools, for Solaris on SPARC and x86 platforms, and Linux on x86/x64 platforms, including multi-core systems. You can download Developer Studio at no charge but if you want the full support and patch updates, then you need a paid support contract. The C++ Compiler supports C++14. https://www.oracle.com/technetwork/server-storage/developerstudio/overview/index.html https://www.oracle.com/technetwork/server-storage/solarisstudio/features/compilers-2332272.html If you want to check some shorter code samples and you dont want to install the whole compiler/.IDE suite then we have lots of online tools that can make those tests super simple. Just open a web browser and put the code Compiler Explorer is a web-based tool that allows you to select from a wide variety of C++ compilers and different versions of the same compiler to test out your code. This allows developers to compare the generated code for specific C++ constructs among compilers, and test for correct behaviour. Clang, GCC, and MSVC are all there but also lesser-known compilers such as DJGPP, ELLCC, Intel C++, and others. https://godbolt.org/ Extra: Heres a list of handy online compilers that you can use: like Coliru, Wandbox, CppInsighs and more: https://arnemertz.github.io/online-compilers/ Debugging & Testing GDB GDB is a portable command-line based debugging platform that supports modern C++ and is available under an open-source license (GPL). A number of editors and IDEs like Visual Studio, Qt Creator, and CLion support integration with GDB. It can also be used to debug applications remotely where GDB is running on one device, and the application being debugged is running on another device. It supports a number of platforms including Windows, macOS, and Linux. https://www.gnu.org/software/gdb/ LLDB LLDB is an open-source debugging interface that supports modern C++ and integrates with the Clang compiler. It has a number of optional performance-enhancing features such as JIT but also supports debugging memory, multiple threads, and machine code analysis. It is built in C++. LLDB is the default debugger for Xcode and can be used with Visual Studio Code, CLion, and Qt Creator. It supports a number of platforms including Windows, macOS, and Linux. https://lldb.llvm.org/ Debugging Tools For Windows On Windows, you can use several debuggers, ranging from Visual Studio Debugger (integrated and one of the most user-friendly), WinDBG, CDB and several more. WinDbg is a multipurpose debugger for the Microsoft Windows Platform. It can be used to debug user-mode applications, device drivers, and the operating system itself in kernel mode. It has a graphical user interface (GUI) and is more powerful than Visual Studio Debugger. You can debug memory dumps obtained even from kernel drivers. One of the recent exciting features in Debugging on Windows is called Time Travel Debugging (Available in WinDBG Preview and also in Visual Studio Ultimate). It allows you to record the execution of the process and then replay the steps backwards or forwards. This flexibility enables us to easily tracks back the state that caused a bug. https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/ https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/time-travel-debugging-overview Mozillas RR RR is an advanced debugger that aims to replace GDB on Linux. It offers the full state recordings of the application so that you can replay the action backwards and forwards (similarly to Time Travel Debugging). The debugger is used to work with large applications like Chrome, OpenOffice or even Firefox code bases. https://rr-project.org/ CATCH/CATCH2 Catch2 is a cross-platform open-source (BSL-1.0) testing framework for modern C++. It is very lightweight because only a header file needs to be included. Unit tests can be tagged and run in groups. It supports both test-driven development and behaviour-driven development. Catch2 also easily integrates with CLion. https://github.com/catchorg/Catch2 BOOST.TEST Boost.Test is a feature-rich open-source (BSL-1.0) testing framework that utilizes modern C++ standards. It can be used to quickly detect errors, failures, and time outs through customizable logging and real-time monitoring. Tests can be grouped into suites, and the framework supports both small scale testing and large scale testing. https://github.com/boostorg/test GOOGLE TEST Google Test is Googles C++ testing and mocking framework, which is available under an open-source (BSD) license. Google test can be used on a broad range of platforms, including Linux, macOS, Windows, and others. It contains a unit testing framework, assertions, death tests, detects failures, handles parameterized tests, and creates XML test reports. https://github.com/google/googletest CUTE CUTE is a unit testing framework integrated into Cevelop, but it can also be used standalone. It spans C++ versions from c++98 to c++2a and is header-only. While not as popular as Google Test it is less macro-ridden and uses macros only, where no appropriate C++ feature is available. In addition, it features a mode that easily allows it to run on embedded platforms, by sidestepping some of the I/O formatting features. https://cute-test.com/ DocTest DocTest is a single-header unit testing framework. Available for C++11 up to C++20 and is easy to configure and works on probably all platforms. It offers regular TDD testing macros (also with subcases) as well as BDD-style test cases. http://bit.ly/doctest-docs https://github.com/onqtam/doctest Mull Mull is an LLVM-based tool for Mutation Testing with a strong focus on C and C++ languages. In general, it creates many variations of the input source code (using LLVM bytecode) and then checks it against the test cases. Thanks to this advanced testing technique, you can make your code more secure. https://github.com/mull-project/mull PDF: https://lowlevelbits.org/pdfs/Mull_Mutation_2018.pdf Sanitizers AddressSanitizer - https://clang.llvm.org/docs/AddressSanitizer.html (supported in Clang, GCC and XCode) UndefinedBehaviorSanitizer - https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html LeakSanitizer - https://clang.llvm.org/docs/LeakSanitizer.html Application Verifier for Windows - https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/application-verifier Sanitizers are relatively new tools that add extra instrumentation to your application (for example they replace new/malloc/delete calls) and can detect various runtime errors: leaks, use after delete, double free and many others. To improve your build pipeline, many guides advice to add sanitizers steps when doing tests. Most sanitizers come from the LLVM/Clang platform, but now they also work with GCC. Unfortunately not yet with Visual Studio (but you can try Application Verifier). Valgrind Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. When you run a program through Valgrind its run on a virtual machine that emulates your host environment. Having that abstraction the tools can leverage various information about the source code and its execution. http://valgrind.org/ http://valgrind.org/info/about.html http://valgrind.org/docs/manual/quick-start.html HeapTrack HeapTrack is a FOSS project and a heap memory profiler for Linux. It traces all memory allocations and annotates these events with stack traces. The tool has two forms the command line version that grabs the data, and then the UI part that you can use to read and analyze the results. This tool is comparable to Valgrinds massif; its easier to use and should be faster to load and analyze for large projects. https://github.com/KDE/heaptrack Dr. Memory Dr. Memory is an LGPL licenced tool that allows you to monitor and intensify memory -related errors for binaries on Windows, Linux, Mac, Android. Its based on the DynamoRIO dynamic instrumentation tool platform. With the tool, you can find errors like double frees, memory leaks, handle leaks (on Windows), GDI issues, access to uninitialized memory or even errors in multithreading memory scenarios. http://drmemory.org/ https://github.com/DynamoRIO/drmemory Deleaker The primary role of Deleaker is to find leaks in your native applications. It supports Visual Studio (since 2008 till the latest 2019 version), Delphi/C++ Builder, Qt Creator, CLion (soon!). Can be used as an extension in Visual Studio or as a standalone application. Deleaker tracks leaks in C/C++ applications (Native and CLR), plus .NET code. Memory (new/delete, malloc), GDI objects, User32 objects, Handles, File views, Fibres, Critical Sections, and even more. It gathers full call stack, ability to take snapshots, compare them, view source files related to allocation. https://www.deleaker.com/ https://www.deleaker.com/docs/deleaker/tutorial.html Summary & More I hope that with the above list, you get a useful overview of the tools that are essential for C++ development. If you want to read more about other ecosystem elements: libraries, frameworks, and other tools, then please see the full report from Embarcadero: C++ Ecosystem White Paper (Its a nice looking pdf, with more than 20 pages of content!) You might check this Resource for a super long list of tools, libs, frameworks that enhance C++ development: https://github.com/fffaraz/awesome-cpp Your Turn What are your favourite tools that you use when writing C++ apps? ",
        "_version_": 1718536530733039616
      },
      {
        "story_id": 18854644,
        "story_author": "severine",
        "story_descendants": 1,
        "story_score": 25,
        "story_time": "2019-01-08T10:41:22Z",
        "story_title": "KDE is considering a migration to GitLab",
        "search": [
          "KDE is considering a migration to GitLab",
          "https://gitlab.com/gitlab-org/gitlab-ce/issues/53206",
          "Background KDE is considering a migration to GitLab: members from the KDE Board of Directors, the KDE Sysadmin team and the KDE Onboarding Initiative have been following the GNOME migration as a model and have been in touch with the GNOME Foundation and GitLab. GitLab will be initially assisting them with a Proof of Concept to facilitate making an assessment and a decision that will ultimately be consulted with the KDE community. The KDE Community is a free software community dedicated to creating an open and user-friendly computing experience, offering an advanced graphical desktop, a wide variety of applications for communication, work, education and entertainment and a platform to easily build new applications upon. We have a strong focus on finding innovative solutions to old and new problems, creating a vibrant atmosphere open for experimentation. Goals The main goals for a successful migration would be: More accessible infrastructure for contributors Code review integration with git Streamlined infrastructure and tooling Good relationship and open communication channel with upstream (GitLab in this case) Migration issues tracker KDE keeps a list of issues with priorities relevant to the migration at https://gitlab.com/gitlab-org/gitlab-ce/issues/57338 Discussion We are at the initial stages of discussion, which can be followed on the notes from our regular calls. KDE is currently looking at a self-hosted solution rather than hosting at gitlab.com. Due to their policy of only hosting Free Software on their servers the present consideration is to use gitlab-ce with the Core subscription Current KDE tooling and infrastructure Gitolite Authentication and Authorization (some repositories are push restricted) Repository management Custom hooks providing notifications via email and IRC, as well as updating statuses of tasks and bugs on Phabricator and Bugzilla Phabricator Code and asset reviews Task tracking Kanban boards Wikis Bugzilla Bug reports Automated crash reports LDAP-based SSI + management Web UI Jenkins CI: Linux, FreeBSD, Windows, Android CD: Linux (Appimage & Flatpak), Windows, MacOS and Android GitLab replacements Tool Feature GitLab feature GitLab edition Gitolite Authentication LDAP and Omniauth CE Gitolite Repository management Gitaly CE Gitolite Notification (E-Mail) E-mail notification CE Gitolite Notification (IRC). Important. TBD TBC Gitolite Set ticket status IssuesSee Closing issues and quick actions CE Phabricator Code review Discussions CE Phabricator Asset reviews (additional tool in Phabricator - Pholio) Image discussionsAlso see issue #53587 CE Phabricator Task-tracking Time tracking CE Phabricator Kanban boards Issue boards CE Bugzilla Bug reports Issues CE Bugzilla Automated crash reports (not trivial to move out of Bugzilla) Issues or an external tool CE KDE identity LDAP-based SSI (SSH key management) SSH management on GitLab or syncing keys to GitLab CE KDE identity Web UI (keep for more user-facing resources, e.g. Forum) N/A? N/A? Jenkins CI: Linux, FreeBSD, Windows, Android GitLab CI or APIJenkins plugin is EE-only CE Jenkins CD: Linux (Appimage & FlatPak), Windows, MacOS and Android GitLab CI or APIJenkins plugin is EE-only CE Test instance https://invent.kde.org/kde/ Collaborators Eike Hein (@hein), Treasurer and Vice President, Board of Directors, KDE Aleix Pol (@apol), Vice President, Board of Directors, KDE Ben Cooksley (@bcooksley), Lead Sysadmin, KDE Neofytos Kolokotronis (@tetris4), Onboarding Team Lead, KDE David Planella (@dplanella), Director of Community Relations, GitLab "
        ],
        "story_type": "Normal",
        "url_raw": "https://gitlab.com/gitlab-org/gitlab-ce/issues/53206",
        "url_text": "Background KDE is considering a migration to GitLab: members from the KDE Board of Directors, the KDE Sysadmin team and the KDE Onboarding Initiative have been following the GNOME migration as a model and have been in touch with the GNOME Foundation and GitLab. GitLab will be initially assisting them with a Proof of Concept to facilitate making an assessment and a decision that will ultimately be consulted with the KDE community. The KDE Community is a free software community dedicated to creating an open and user-friendly computing experience, offering an advanced graphical desktop, a wide variety of applications for communication, work, education and entertainment and a platform to easily build new applications upon. We have a strong focus on finding innovative solutions to old and new problems, creating a vibrant atmosphere open for experimentation. Goals The main goals for a successful migration would be: More accessible infrastructure for contributors Code review integration with git Streamlined infrastructure and tooling Good relationship and open communication channel with upstream (GitLab in this case) Migration issues tracker KDE keeps a list of issues with priorities relevant to the migration at https://gitlab.com/gitlab-org/gitlab-ce/issues/57338 Discussion We are at the initial stages of discussion, which can be followed on the notes from our regular calls. KDE is currently looking at a self-hosted solution rather than hosting at gitlab.com. Due to their policy of only hosting Free Software on their servers the present consideration is to use gitlab-ce with the Core subscription Current KDE tooling and infrastructure Gitolite Authentication and Authorization (some repositories are push restricted) Repository management Custom hooks providing notifications via email and IRC, as well as updating statuses of tasks and bugs on Phabricator and Bugzilla Phabricator Code and asset reviews Task tracking Kanban boards Wikis Bugzilla Bug reports Automated crash reports LDAP-based SSI + management Web UI Jenkins CI: Linux, FreeBSD, Windows, Android CD: Linux (Appimage & Flatpak), Windows, MacOS and Android GitLab replacements Tool Feature GitLab feature GitLab edition Gitolite Authentication LDAP and Omniauth CE Gitolite Repository management Gitaly CE Gitolite Notification (E-Mail) E-mail notification CE Gitolite Notification (IRC). Important. TBD TBC Gitolite Set ticket status IssuesSee Closing issues and quick actions CE Phabricator Code review Discussions CE Phabricator Asset reviews (additional tool in Phabricator - Pholio) Image discussionsAlso see issue #53587 CE Phabricator Task-tracking Time tracking CE Phabricator Kanban boards Issue boards CE Bugzilla Bug reports Issues CE Bugzilla Automated crash reports (not trivial to move out of Bugzilla) Issues or an external tool CE KDE identity LDAP-based SSI (SSH key management) SSH management on GitLab or syncing keys to GitLab CE KDE identity Web UI (keep for more user-facing resources, e.g. Forum) N/A? N/A? Jenkins CI: Linux, FreeBSD, Windows, Android GitLab CI or APIJenkins plugin is EE-only CE Jenkins CD: Linux (Appimage & FlatPak), Windows, MacOS and Android GitLab CI or APIJenkins plugin is EE-only CE Test instance https://invent.kde.org/kde/ Collaborators Eike Hein (@hein), Treasurer and Vice President, Board of Directors, KDE Aleix Pol (@apol), Vice President, Board of Directors, KDE Ben Cooksley (@bcooksley), Lead Sysadmin, KDE Neofytos Kolokotronis (@tetris4), Onboarding Team Lead, KDE David Planella (@dplanella), Director of Community Relations, GitLab ",
        "comments.comment_id": [18868066],
        "comments.comment_author": ["s_chaudhary"],
        "comments.comment_descendants": [0],
        "comments.comment_time": ["2019-01-09T19:41:40Z"],
        "comments.comment_text": ["Looking forward to this"],
        "id": "090fbde3-023d-483b-9961-9b1aeacbeb1e",
        "_version_": 1718536435368198144
      },
      {
        "story_id": 18896422,
        "story_author": "tosh",
        "story_descendants": 12,
        "story_score": 54,
        "story_time": "2019-01-13T12:31:44Z",
        "story_title": "Awesome Dotfiles",
        "search": [
          "Awesome Dotfiles",
          "https://github.com/webpro/awesome-dotfiles",
          "Awesome dotfiles A curated list of dotfiles resources. Inspired by the awesome list thing. Note that some articles or tools may look old or old-fashioned, but this usually means they're battle-tested and mature (like dotfiles themselves). Feel free to propose new articles, projects or tools! Articles Introductions Getting started with dotfiles (L. Kappert) Getting started with dotfiles (D. Vints) Managing your dotfiles Dotfiles Are Meant to Be Forked Dotfile discovery I do Dotfiles! Tutorials Setting up a new (OS X) development machine: Part 3 - Dotfiles and custom SSH config Setting Up a Mac Dev Machine From Zero to Hero With Dotfiles; Part 2 Using Git and GitHub to manage your dotfiles conf.d like directories for zsh/bash dotfiles Managing your dotfiles The best way to store your dotfiles: A bare Git repository Shell startup Shell startup scripts Zsh/Bash startup files loading order Using specific tools Using GNU Stow to manage your dotfiles Managing Dotfile Symlinks with GNU Stow Dotfiles and dev tools provisioned by Ansible Manage a development machine with Ansible Find dotfiles repos There are many great dotfiles repos out there, each containing their own inspiration and gems. One way to go through them is to search GitHub for \"dotfiles\". Also see: Google for \"dotfiles\" Archlinux collection Tip: search for a filename on GitHub, e.g. in:path .gitconfig. Example dotfiles repos A collection of the most popular, well-maintained, and collaborative dotfiles repositories & frameworks. Some projects contain just the dotfiles. Others go further by allowing you to easily add your own custom dotfiles, and some include scripts to manage dotfiles and plugins. Bash Title Description Focus Bash it Community bash framework. Autocompletion, themes, aliases, custom functions. Well-structured framework. Mathiass dotfiles Sensible hacker defaults for macOS Lots of goodness here, great collaborative community effort. Maximum Awesome Config files for vim and tmux Vim, tmux. Built for Mac OS X. webpro's dotfiles macOS dotfiles Bash, Homebrew, Brew Cask, Git, Node.js, Hammerspoon. rootbeersoup's dotfiles Effortless Bash, Vim and macOS configuration A curl | sh installer and a Makefile offer portable and effortless setup for either permanent or temporary configuration. Luke's voidrice Arch linux dotfile bootstrap Bloatless, often suckless software. Vim config for editing documents in markdown or latex Zsh Title Description Focus thoughtbot dotfiles Set of vim, zsh, git, and tmux configuration files Zsh, vim, tmux, git, homebrew. Uses rcm. oh-my-zsh Community-driven framework for managing your zsh configuration. Includes 200+ optional plugins (rails, git, OSX, hub, capistrano, brew, ant, php, python, etc), over 140 themes to spice up your morning, and an auto-update tool. Prezto The configuration framework for Zsh. Enriches the command line interface environment with sane defaults, aliases, functions, auto completion, and prompt themes. YADR The best vim, git, zsh plugins and the cleanest vimrc you've ever seen Homebrew, zsh, git, vim, and more. Active repository. antigen Plugin manager for zsh, inspired by oh-my-zsh and vundle. Antigen is a small set of functions that help you easily manage your shell (zsh) plugins. Antigen is to zsh, what Vundle is to vim. Dries's dotfiles Simplified approach to dotfiles for macOS Zsh, Oh My Zsh, macOS, Homebrew, Mackup sobolevn's dotfiles Dotfiles for the developer happiness Zsh, Brew, Sublime, Python, Node, Elixir Fish Title Description Focus oh-my-fish Community Fish framework. Includes many plugins and themes, with installation, auto-update, and scaffolding tools. Paul's dotfiles Abundant dotfiles with a plethora of cool custom functions Fish, macOS, Homebrew, Custom Shell functions rkalis's dotfiles Well-maintained dotfiles featuring Fish, repository management and Hammerspoon Fish, macOS, Homebrew, Repository management, Hammerspoon Ansible Title Description Focus .dots New and upgraded dotfiles, now with Ansible! Completely automated desktop setup, configuration and maintenance using Ansible sloria's dotfiles sloria's dotfiles as Ansible roles Sets up a full local development environment with a single command Tools Ansible - Radically simple configuration-management, application deployment, task-execution, and multinode orchestration engine. bashdot - Minimalist dotfile management framework written entirely in bash. chezmoi - Manage your dotfiles securely across multiple machines. comtrya - Configuration management for localhost, written in Rust, for Linux, BSD, macOS, and Windows dotbare - Manage dotfiles interactively with fzf. dotbot - Tool that bootstraps your dotfiles. dotdrop - Save your dotfiles once, deploy them everywhere. dotstow - Manage dotfiles with stow. emplace - Synchronize installed packages on multiple machines using a dotfiles repository. Fisher - A package manager for Fish fresh - Keep your dotfiles fresh. Fresh is a tool to source shell configuration (aliases, functions, etc) from others into your own configuration files. GNU Stow - Symlink farm manager which takes distinct packages of software and/or data located in separate directories on the filesystem, and makes them appear to be installed in the same place. homeshick - Git dotfile synchronizer written in Bash. homesick - Your home directory is your castle. Don't leave your dotfiles behind (article). mackup - Keep your application settings in sync (OS X/Linux). Pearl - Package manager that allows to control, sync, share dotfiles as packages automatically activated during shells or editors startup. There is a wide range of packages already available in the Official Pearl Hub (for Linux and OSX). rcm - rc file (dotfile) management. themer - Manage and generate themes across your development tools from within your dotfiles. toml-bombadil - Templatize and manage your dotfiles. yadm - Tool for managing a collection of files across multiple computers, using a shared Git repository and some additional features. macOS dockutil - Command line tool for managing dock items mas - Mac App Store command line interface zero - Radically simple personal bootstrapping tool for macOS. Miscellaneous dotfiles.github.io - Your unofficial guide to dotfiles on GitHub. OS X Defaults - Centralized place for the awesome work started by @mathiasbynens on .macos. Filesystem Hierarchy Standard - Directory structure and directory contents in Linux distributions. XDG Base Directory Specification - Summary A lesson in shortcuts - How the idea of \"hidden\" or \"dot\" files was born, by Rob Pike (originally posted on Google+) Related Lists Awesome Dev Env - Curated list of awesome tools, resources and workflow tips making an awesome development environment. Awesome Fish - Curated list of packages, prompts, and resources for the fish shell. Awesome Shell - Curated list of awesome command-line frameworks, toolkits, guides and gizmos. Awesome Sysadmin - A curated list of amazingly awesome open source sysadmin resources. Awesome Zsh Plugins - List of Zsh plugins suitable for use with oh-my-zsh, antigen & Prezto. Terminals Are Sexy - A curated list of Terminal frameworks, plugins & resources for CLI lovers. Archive/abandoned projects Bashstrap battleschool Bork Cider dev-setup dotfiles Eduardo's dotfiles ellipsis holman does dotfiles Kevin's dotfiles kody osxc vcsh (article, article) License To the extent possible under law, Lars Kappert has waived all copyright and related or neighboring rights to this work. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/webpro/awesome-dotfiles",
        "comments.comment_id": [18898081, 18898412],
        "comments.comment_author": ["anschwa", "blakesterz"],
        "comments.comment_descendants": [3, 1],
        "comments.comment_time": [
          "2019-01-13T18:59:57Z",
          "2019-01-13T20:12:20Z"
        ],
        "comments.comment_text": [
          "The best way I've found to organize dotfiles is to simply use a bare git repo.<p>> git init --bare $HOME/.dotfiles`<p>> alias dot=\"git --git-dir=$HOME/.dotfiles/ --work-tree=$HOME\"<p>> dot config --local status.showUntrackedFiles no",
          "I have a totally awesome .bash_aliases file! I've taken all the best things I've seen from all the best .bash_aliases files around the web. I bet it's like 1000 lines long.<p>I can remember about 10 of the things I have in there."
        ],
        "id": "71c3fd85-b00e-4729-b702-017cdb95ae17",
        "url_text": "Awesome dotfiles A curated list of dotfiles resources. Inspired by the awesome list thing. Note that some articles or tools may look old or old-fashioned, but this usually means they're battle-tested and mature (like dotfiles themselves). Feel free to propose new articles, projects or tools! Articles Introductions Getting started with dotfiles (L. Kappert) Getting started with dotfiles (D. Vints) Managing your dotfiles Dotfiles Are Meant to Be Forked Dotfile discovery I do Dotfiles! Tutorials Setting up a new (OS X) development machine: Part 3 - Dotfiles and custom SSH config Setting Up a Mac Dev Machine From Zero to Hero With Dotfiles; Part 2 Using Git and GitHub to manage your dotfiles conf.d like directories for zsh/bash dotfiles Managing your dotfiles The best way to store your dotfiles: A bare Git repository Shell startup Shell startup scripts Zsh/Bash startup files loading order Using specific tools Using GNU Stow to manage your dotfiles Managing Dotfile Symlinks with GNU Stow Dotfiles and dev tools provisioned by Ansible Manage a development machine with Ansible Find dotfiles repos There are many great dotfiles repos out there, each containing their own inspiration and gems. One way to go through them is to search GitHub for \"dotfiles\". Also see: Google for \"dotfiles\" Archlinux collection Tip: search for a filename on GitHub, e.g. in:path .gitconfig. Example dotfiles repos A collection of the most popular, well-maintained, and collaborative dotfiles repositories & frameworks. Some projects contain just the dotfiles. Others go further by allowing you to easily add your own custom dotfiles, and some include scripts to manage dotfiles and plugins. Bash Title Description Focus Bash it Community bash framework. Autocompletion, themes, aliases, custom functions. Well-structured framework. Mathiass dotfiles Sensible hacker defaults for macOS Lots of goodness here, great collaborative community effort. Maximum Awesome Config files for vim and tmux Vim, tmux. Built for Mac OS X. webpro's dotfiles macOS dotfiles Bash, Homebrew, Brew Cask, Git, Node.js, Hammerspoon. rootbeersoup's dotfiles Effortless Bash, Vim and macOS configuration A curl | sh installer and a Makefile offer portable and effortless setup for either permanent or temporary configuration. Luke's voidrice Arch linux dotfile bootstrap Bloatless, often suckless software. Vim config for editing documents in markdown or latex Zsh Title Description Focus thoughtbot dotfiles Set of vim, zsh, git, and tmux configuration files Zsh, vim, tmux, git, homebrew. Uses rcm. oh-my-zsh Community-driven framework for managing your zsh configuration. Includes 200+ optional plugins (rails, git, OSX, hub, capistrano, brew, ant, php, python, etc), over 140 themes to spice up your morning, and an auto-update tool. Prezto The configuration framework for Zsh. Enriches the command line interface environment with sane defaults, aliases, functions, auto completion, and prompt themes. YADR The best vim, git, zsh plugins and the cleanest vimrc you've ever seen Homebrew, zsh, git, vim, and more. Active repository. antigen Plugin manager for zsh, inspired by oh-my-zsh and vundle. Antigen is a small set of functions that help you easily manage your shell (zsh) plugins. Antigen is to zsh, what Vundle is to vim. Dries's dotfiles Simplified approach to dotfiles for macOS Zsh, Oh My Zsh, macOS, Homebrew, Mackup sobolevn's dotfiles Dotfiles for the developer happiness Zsh, Brew, Sublime, Python, Node, Elixir Fish Title Description Focus oh-my-fish Community Fish framework. Includes many plugins and themes, with installation, auto-update, and scaffolding tools. Paul's dotfiles Abundant dotfiles with a plethora of cool custom functions Fish, macOS, Homebrew, Custom Shell functions rkalis's dotfiles Well-maintained dotfiles featuring Fish, repository management and Hammerspoon Fish, macOS, Homebrew, Repository management, Hammerspoon Ansible Title Description Focus .dots New and upgraded dotfiles, now with Ansible! Completely automated desktop setup, configuration and maintenance using Ansible sloria's dotfiles sloria's dotfiles as Ansible roles Sets up a full local development environment with a single command Tools Ansible - Radically simple configuration-management, application deployment, task-execution, and multinode orchestration engine. bashdot - Minimalist dotfile management framework written entirely in bash. chezmoi - Manage your dotfiles securely across multiple machines. comtrya - Configuration management for localhost, written in Rust, for Linux, BSD, macOS, and Windows dotbare - Manage dotfiles interactively with fzf. dotbot - Tool that bootstraps your dotfiles. dotdrop - Save your dotfiles once, deploy them everywhere. dotstow - Manage dotfiles with stow. emplace - Synchronize installed packages on multiple machines using a dotfiles repository. Fisher - A package manager for Fish fresh - Keep your dotfiles fresh. Fresh is a tool to source shell configuration (aliases, functions, etc) from others into your own configuration files. GNU Stow - Symlink farm manager which takes distinct packages of software and/or data located in separate directories on the filesystem, and makes them appear to be installed in the same place. homeshick - Git dotfile synchronizer written in Bash. homesick - Your home directory is your castle. Don't leave your dotfiles behind (article). mackup - Keep your application settings in sync (OS X/Linux). Pearl - Package manager that allows to control, sync, share dotfiles as packages automatically activated during shells or editors startup. There is a wide range of packages already available in the Official Pearl Hub (for Linux and OSX). rcm - rc file (dotfile) management. themer - Manage and generate themes across your development tools from within your dotfiles. toml-bombadil - Templatize and manage your dotfiles. yadm - Tool for managing a collection of files across multiple computers, using a shared Git repository and some additional features. macOS dockutil - Command line tool for managing dock items mas - Mac App Store command line interface zero - Radically simple personal bootstrapping tool for macOS. Miscellaneous dotfiles.github.io - Your unofficial guide to dotfiles on GitHub. OS X Defaults - Centralized place for the awesome work started by @mathiasbynens on .macos. Filesystem Hierarchy Standard - Directory structure and directory contents in Linux distributions. XDG Base Directory Specification - Summary A lesson in shortcuts - How the idea of \"hidden\" or \"dot\" files was born, by Rob Pike (originally posted on Google+) Related Lists Awesome Dev Env - Curated list of awesome tools, resources and workflow tips making an awesome development environment. Awesome Fish - Curated list of packages, prompts, and resources for the fish shell. Awesome Shell - Curated list of awesome command-line frameworks, toolkits, guides and gizmos. Awesome Sysadmin - A curated list of amazingly awesome open source sysadmin resources. Awesome Zsh Plugins - List of Zsh plugins suitable for use with oh-my-zsh, antigen & Prezto. Terminals Are Sexy - A curated list of Terminal frameworks, plugins & resources for CLI lovers. Archive/abandoned projects Bashstrap battleschool Bork Cider dev-setup dotfiles Eduardo's dotfiles ellipsis holman does dotfiles Kevin's dotfiles kody osxc vcsh (article, article) License To the extent possible under law, Lars Kappert has waived all copyright and related or neighboring rights to this work. ",
        "_version_": 1718536436944207872
      },
      {
        "story_id": 19580860,
        "story_author": "yvonnick",
        "story_descendants": 69,
        "story_score": 377,
        "story_time": "2019-04-05T08:56:53Z",
        "story_title": "Design Tools for Everything",
        "search": [
          "Design Tools for Everything",
          "https://github.com/LisaDziuba/Awesome-Design-Tools",
          "Awesome Design Tools Awesome Design Plugins Awesome Design Conferences Awesome Design UI Kits Awesome Design Tools as a part of Flawless App family joins Abstract! Today, were excited to announce that Flawless App has joined Abstract, a design delivery platform that brings visibility, accountability, measurability, and predictability to design. Flawless App was our first company, and were proud of everything that weve achieved with our 5-person team. Since 2015, weve launched powerful tools for designers and developers among them are Flawless App, Reduce, Flawless Feedback. Weve also invested a lot of love and care into community-driven initiatives. Awesome Design Tools is one of them. So whats next? Our team has joined Abstract and is focused on building out the Abstract SDK, bringing developers experience forward. One unbelievable journey has finished and the new one has just begun. And if you want to know more about the future of Awesome Design Tools, please keep reading our FAQ. How to Use and Contribute Now you are in the Awesome Design Tools section, if you need plugins go to Awesome Design Plugins. To find the tool, go through the Table of Contents or search for a keyword (for example, \"animation\", \"prototyping\"). Ask Lisa on Twitter. If you found some great design tool or plugin, just send a Pull Request with respect to our Contribution Guidelines (they're very simple, please take a look). Design tools should be submitted here and plugins in Awesome Design Plugins file. We use such labels for free , open source and Mac only tools, don't forget to add them. Now I'd love to see your suggestions! Table of Content Accessibility Tools Animation Tools Augmented Reality Collaboration Tools Color Picker Tools Design Feedback Tools Design Handoff Tools Design Inspiration Design System Tools Design to Code Tools Design Version Control Development Tools Experience Monitoring Font Tools Gradient Tools Icons Tools Illustrations Information Architecture Logo Design Mockup Tools No Code Tools Pixel Art Tools Prototyping Tools Screenshot Software Sketching Tools SMM Design Tools Sound Design Stock Photos Tools Stock Videos Tools for Learning Design UI Design Tools User Flow Tools User Research Tools Visual Debugging Tools Wireframing Tools 3D Modeling Software Accessibility Tools Accessibility is the practice of creating websites and apps usable for all people, including individuals with visual, motor, auditory, speech, or cognitive disabilities. Here you will find web accessibility tools, accessibility testing tools, and accessibility apps both for developers and designers: Accessibility checking features are also available in VisBug from the Visual Debugging Tools category. Animation Tools Animations guide people through the product friendly and smoothly. Live interactive UI makes users feel delighted with instant feedback and emotional touch. These free and paid tools are designed to make animation creation easier. If you plan to make animated transitions, micro-interactions or scroll-based animations, go through these tools: Drama, Principle, Framer, Invision Studio, Flinto are also among UI & UX animation apps and software animation tools. Augmented Reality Augmented Reality is a technology that upgrades our real world, by adding a layer of digital information to it. The use of AR increases day by day with dozens of new AR apps, development kits, and AR frameworks. So in this section is collected different augmented reality tools for creating, projecting and prototyping apps focused on AR: Collaboration Tools Looking to try some tools for agile design and development collaboration? Good idea because such tools make the lives of designers and developers much easier, save time and improve productivity. Well, you know all that in Slack-driven era. Here you'll find the best collaboration tools for product teams: Airtable part spreadsheet, part database, and entirely flexible, teams use Airtable to organize their work, their way. Asana the work management platform teams use to stay focused on the goals, projects, and daily tasks that grow business. Basecamp the project management suite designed to organize employees, delegate tasks, and monitor progress right from one place. Freedcamp the most innovative way to manage projects, completely free... forever. Droplr screenshot, file sharing and screencasts to help you capture and explain your ideas. Excalidraw a whiteboard tool that lets you easily sketch diagrams with a hand-drawn feel. Filestage an agile content approval software that helps you to review videos, designs, and documents with clients and co-workers. Float a tool to plan your projects and schedule your teams time all in one place. Gallery a collaborative tool for uploading design work, getting feedback, and tracking revisions. HiveDesk an automatic time tracking for remote employees with screenshots. It makes it easier to monitor the productivity of remote employees and manage projects. Jira software development tool used by agile teams. Jitsi multi-platform open-source video conferencing you can install yourself or use on their servers. Keybase a free collaboration app with built-in end-to-end encrypted chat, file sharing, git repositories, and more Lumeer - an easy visual collaborative tool to plan, organize and track anything, anywhere, anytime. Mattermost a flexible, open-source messaging platform that meets even the most demanding privacy and security standards. Milanote an easy-to-use, collaborative tool to organize your ideas and projects into visual boards. Mixed real-time whiteboard and collaboration tools for distributed teams. Moqhub fast online proofing for creative projects. Get feedback for images and PDFs. MURAL think and collaborate visually. Anywhere, anytime. Nextcloud open source collaboration platform for files, kanban boards, chat & calls, calendar and more. Notion write, plan, collaborate, and get organized. Notion is all you need in one tool. ProofHub the one place for all your projects, teams and communications. RealtimeBoard (Miro) whiteboarding platform for cross-functional team collaboration. It was recently renamed to Miro. Slack a collaboration hub for work, no matter what work you do. Its a place where conversations happen, decisions are made, and information is always at your fingertips. Sunsama a beautifully designed, team-oriented task manager that consolidates Trello/Jira/Asana tickets into a single, calendar-oriented view. Taskade team productivity made simple, fun and designed for remote teams. Taskade is collaborative and syncs in real-time across the web, mobile, and desktop. Trello a web-based project management application that enables you to organize and prioritize your projects in a fun, flexible and rewarding way. Witeboard simple real-time whiteboard for collaboration work. Workzone a simple, powerful project management software. Wrike an online project management software that gives you full visibility and control over your tasks. Zenkit a tool to schedule meetings, track the projects progress, and brainstorm new ideas. zipBoard an online visual bug tracking and website annotation tool Zulip combines the immediacy of real-time chat with an email threading model, helping to focus on important conversations while ignoring irrelevant ones. Color Picker Tools If you are looking for an eyedropper tool, color identifier or color capture, check this section. With the color pickers mentioned here, you will be able to create new color combinations and define great ones. Almost all of these tools are free: BrandColors the biggest collection of official brand color codes around. Calcolor a web-based color palette management tool which provides a new way to interact with digital colors. Chroma free web app which allows you to identify the color. Color create color schemes with the color wheel or browse thousands of color combinations from the Color community. Made by Adobe. Color by Cloudflare a tool to preview palettes against UI elements, cycle through accessible color combos and create palettes manually, via URL import, or generatively. Color Deck an HSL driven color palette collection. It's also open-source. Color Hexa free color tool providing information about any color and generating matching color palettes for designs. Color Hex Picker - a tool for getting the name of any color from any image. You also get the HEX code and RGB value for the color. Color Hunt free and open platform for color inspiration with thousands of trendy hand-picked color palettes. ColorKit a tool for blending colors and generating a color's shades and tints. Color Leap leap through time and see the colors of history. Colorpicker a complete and open-source color manipulation tool with picking. ColorSlurp the ultimate color productivity booster for designers and developers. ColorsWall/ place to store your color palettes or easy generate. Colorwise search through the color palettes of the most voted products on Product Hunt. Colour Cafe colors inspiration that has selected a modern color palette on Instagram. Colourcode an online designer tool, which allows you to easily and intuitively combine colors. Coolors the super-fast color scheme generator. Culrs thoughtfully crafted and easy-to-use color palettes. Geenes a tool to create harmonious color themes with code and sketch file export. Image Color Picker image color picker and color scheme generator. Instant Eyedropper a minimalist eyedropper tool for windows, supports multiple color formats for both designers and developers. Just Color Picker Free portable offline color picker and color editor. Khroma AI-based tool to generate color palettes based on your preferences. Material Colors Native Google's material design color palette in the form of a mac app. Material Design Palette Generator get perfect Material Design color palettes from any hex color. Paletton a designer tool for creating color combinations that work together well. Picular a rocket fast primary color generator using Google's image search. Pigment a color palette generator with innumerable color configurations suggested by the app. Pikka color picker & color schemes generator for macOS. React Color a collection of color pickers from Sketch, Photoshop, Chrome. Its free and open-source. Sip a better way to collect, organize and share colors. Global eyedropper tool for macOS. Skala Color works with a huge variety of formats, covering everything youre likely to need for web, iOS, Android, and macOS development. Swatches iOS app that lets you collect, inspect and share the colors that inspire you in your daily life. Tint & Shade Generator display tints and shades of a given hex color in 10% increments. Viz Palette color picker for data visualizations. You can also create color palettes with Leonardo, which is mentioned in the Accessibility Tools section. Design Feedback Tools How do you provide and get feedback during the development process? This process is usually pretty messy for many product teams, which caused product delays and time waste on back-and-forth communication. So if you wish to get fast and structural feedback on UI issues or visual bugs take a look at this tool: Flawless Feedback review and annotate iOS apps then share your feedback in Jira or Trello. GoVisually online proofing, design review & approval software. Design Handoff Tools Design handoff happens when designers finish the work and need to deliver designs to developers with all specs and assets. Design handoff tools allow to automatically generate style guide, comment on the design, inspect elements. These tools minimize guesswork and improve the effectiveness of the design process. You can also make a design handoff within Relay, mentioned in the Design Version Control section. Design Inspiration The creative process can be tough. So if ideas dont come to your mind right away, try to search for inspiration in the work of other designers. These design inspiration sites feature design patterns, user flows, email markups and creative solutions of popular companies and great products: Behance an online platform to showcase & discover creative work. CodeMyUI handpicked collection of web design & UI inspiration with code snippets. Collect UI a platform for your daily inspiration collected from Daily UI & beyond. Based on Dribbble shots, 14419 handpicked designs. Creative Portfolios a curation of the most creative portfolios made by designers & developers. Design Hunt a daily collection of the best products, apps, and inspirations for all creatives. Dribbble an online community for showcasing user-made artwork. A great resource to get inspired by others work. Ficture a font in use archive that will inspire you. Inspiration UI a community that aims to provide the best design resources for those who create for the web. Hover States showcase great work from the bleeding edge of digital culture for, and with, the creative community. Httpster an inspiration resource showcasing rocking websites made by people from all over the world. H69.Design landing page colletions & Free resources for designers. Lapa Ninja landing page design inspiration from around the web. 1800+ landing page examples and updated daily. Mobile Patterns a design inspirational library featuring finest UI UX Patterns (iOS and Android). Mobbin browse mobile design patterns for popular apps. Explore common user flows such as onboarding, account sign up and log in, core functionality, and more. One Page Love a collection of beautiful unique One Page websites for website design inspiration. Owwly home for digital products crafted with passion to design. Page Flows user flow videos and screenshots to inspire you when you're stuck. Discover mobile and web design patterns for over 160 different tasks. pttrns design patterns for popular mobile apps. Really Good Emails 4,150+ handpicked email design. ReallyGoodUX screenshots, and examples of great UX from real mobile and web products. Discover the best UX examplesincluding onboarding tours and walkthroughs. Saas Landing Page iscover the best landing page examples created by top-class SaaS companies, and get ideas and inspiration for your next design project. Saas Pages a collection of the best landing pages with a focus on copywriting and design. The Design Team comics about a design team for a tech startup in Silicon Valley. Typewolf helps designers choose the perfect font combination for their next design project. UI Garage the one-stop shop for designers, developers, and marketers to find inspiration, tools and the best resources. UI Sources over 500+ interactions from the best designed and top-grossing apps on the App Store today. UI Recipes weekly 15 min lessons on UI design from the hottest apps. UX Archive browse more than 400 user flows across 120 mobile apps. Examine tasks such as booking, logging in, onboarding, purchasing, searching, and more. Waveguide a design knowledge bank with thousands of artificially enriched examples of product and brand experience (examples of Mobile App, Landing pages, eCommerce, CX/UX Patterns). Web Design Museum over 1,200 carefully selected web sites that show web design trends between the years 1994 and 2006. Design System Tools I bet you heard about Design Systems, as its a pretty trendy topic. Design systems provide consistent, robust design patterns to keep design and development in sync. They are essentially collections of rules, constraints, and principles, implemented in design and code. Here you can find tools to build, maintain and organize your design system. Cabana a Premium Design System for Sketch that helps you create amazing products faster than ever before. Catalog a living style guide for digital products, combining design documentation with real live components. Design System Manager Invision's design system manager. DSK short for Design System Kit a workbench for collaboratively creating Design Systems. EOS Design System an open-source and customizable built on top of Bootstrap following the Atomic Design concept. Eva Design System customizable design system, available for Sketch with Mobile and Web component libraries. Frontify create graphical guidelines, patterns libraries, design systems. Interplay connect design and engineering around a single source of truth. The tool is not publicly available yet (beta). Lingo create a shared asset library with your entire team. Lucid tool for creating, managing and sharing a design system. From a simple component library through to detailed descriptions of your styles. Modulz design, build, document and publish your design systemwithout writing code. Prime Design System Kit it includes device templates, charts, UI kit and even illustration kit. Specify a tool to create, scale and maintain a design system. Storybook an open-source tool for developing UI components in isolation for React, Vue, and Angular. Symbol Design System design System for Sketch-based on atomic elements. Toolabs design systems and components based design, prototype and development tool. It's not public yet but you can apply to the early access. Zeroheight style guides created by designers, extended by developers, and editable by everyone. We can also add to this in Sketch, Figma, UXPin and Framer X (Framer X Team Store). Design to Code Tools Everyone can learn development but it takes time and effort. If you need a website or an app right now and you dont want to hire a developer, pay attention to the website builders. Such design to code tools will help you to make a portfolio, simple landing or an app pretty fast and beautiful. Anima a web app with a Sketch plugin that converts Sketch to HTML. Blocs a fast, easy to use and powerful visual web design tool, that lets you create responsive websites without writing code. Bootstrap Studio a powerful web design tool for creating responsive websites using the Bootstrap framework. Draftbit visually design and build mobile apps directly from your browser. EasyLogic Studio fantastic css+svg design tool, also it is converted into code as shown. Grapedrop design your components, web projects and publish them instantly online, with an easy to use editor. Mobirise an offline drag and drop website builder software that is based on Bootstrap 3/4 and \\ AMP. PageCloud the fastest and most customizable website builder that allows anyone to create their ideal website. PaintCode vector drawing app that instantly converts drawings into Swift, Objective-C, JavaScript or Java code. Pinegrow a professional visual editor for CSS Grid, Bootstrap 4 and 3, Foundation, responsive design, HTML, and CSS. px.div a proper site build tool for developers and designers. Readymag a visually-pleasing tool for designing on the web from landing pages to multimedia long-reads, presentations and portfolios. Sparkle the easiest way to make a real website, no code, no jargon. STUDIO design from scratch, collaborate in real-time and publish websites. Supernova Studio import designs from Sketch and convert them into Android, iOS or React Native code. Tilda create a website, landing page or online store for free with the help of Tilda modules and publish it on the same day. Wix the easiest and fullest-featured website builder, that allows you to create your own highly customized site. Webflow build responsive websites in your browser, then host with us or export your code to host wherever. Design Version Control Developers actively use version control tools for a long time, probably since 2005 (Git first release). Using a version control system is no brainer for dev teams, while the design version control system appeared only recently. This market is rapidly developing and we expect to see even more in version control for designers: Development Tools This section mentions development tools and browsers. Development browsers have features that help developers and designers create and test websites and apps. Experience Monitoring Listening to users is important but seeing the real usage is even more crucial. For these, you need to install different analytic tools, experience monitoring software, and user behavior apps. Just use those analytics solutions concerning users data: Font Tools Fonts are commonly used for making the web a more beautiful place. Its an essential part of any design. In this section, youll find fonts generators & font finder tools that allow you to manage and work with fonts: BeFonts a Decent collection of free fonts. Behance Free Fonts a list of free fonts uploaded on Behance. DaFont archive of freely downloadable fonts. Browse by alphabetical listing, by style, by author or by popularity. Emotype makes it easy to find fonts based on the emotions you want to convey on your website. Fontbase font management made easy. FontFabric gorgeous interface and good collection. Fontface Ninja browser extension to discover what fonts are being used on any website. FontPair a simple tool that helps you pair Google Fonts together. Fontown a typographic tool with a nonstop growing font catalog which facilitates designer workflow. Fonts thousands of beautiful fonts by Adobe. Fonts Arena curated typography website with high-quality free fonts, done-for-you research articles, free alternatives to premium fonts, news, and more. FontGet variety of fonts all sorted neatly with tags. FontSelf an extension for Illustrator and Photoshop CC that lets you turn any lettering into OpenType fonts in minutes! FontSpark a simple tool to help designers quickly find the best fonts for their projects. Font Squirrel download free fonts with wide collections. Google Fonts making the web more beautiful, fast, and open through great typography. Google Webfonts Helper a hassle way to self-host Google Fonts. Its free and open-source. LostType the first Pay-What-You-Want type foundry. Measure measure typographic line lengths with this browser extension for Chrome. RightFont font managing app, helps preview, sync, install and organize fonts over iCloud, Dropbox or Google Drive. Sans Forgetica a downloadable font that is scientifically designed to help you remember your study notes. Size Calculator calculate the perceived size using viewing distance and physical size of the printed typography. Typeface font manager that improves your design workflow with live font previews and flexible tagging. Wakamai Fondue the tool that answers the question \"what can my font do?\". Web Font Preview preview Google Font pairings with components and site templates. Webfont create and maintain custom SVG icon fonts, made a shared library of icons. WordMarkIt displays your written word/phrase with all the fonts which are already installed on your computer. You can also handle fonts with Specify, which is mentioned in the Design System Tools section. Gradient Tools You can see gradient colors are everywhere UI, branding, illustration, typography. A gradient is created by using two or more different colors to paint one element while gradually fading between them. It might look as a memorable, fresh and unique color. To make such a gradient for your design, use these gradient color palettes. You can also create and audit gradients with Leonardo, which is mentioned in the Accessibility Tools section. Icons Tools As well as fonts, icons are used in every design. These basic elements support and guide many user actions inside the product. Without a doubt, icons are a vital element in user navigation. While making those small design elements is hard and time-consuming, you can get thousands of vector icons for personal and commercial use. Animaticons a growing set of beautiful, high-resolution, animated GIF icons that you can customize. CoreUI Icons premium designed free icon set with marks in SVG, Webfont and raster formats. Digital Nomad Icons lifestyle icon & emoji pack for your next project. 25 high-resolution flat icons. Essential Glyphs created to cover your needs in perfect-shaped icons. Adapted for small and large sizes. Feather Icons each icon is designed on a 24x24 grid with an emphasis on simplicity, consistency, and readability. Flaticon 1593000+ vector icons in SVG, PSD, PNG, EPS format or as icon font. Font Awesome the web's most popular icon set and toolkit, also it's open source. Fontello tool to build custom fonts with icons, also open source. Freepik collection of exclusive freebies and all graphic resources that you need for your projects. Iconscout get high-quality Icons, Illustrations and Stock photos at one place. Iconfinder a marketplace for vector icons (SVG). Icon sets available in IconJar format. IconJar store all your icons in one icon manager. Iconmonstr discover 4412+ free icons in 305 collections.Big and continuously growing source of simple icons. Iconset free, cross-platform and fast SVG icon organizer working on Mac and Windows. Icon Store a library of free vector icons for personal and commercial projects, designed by first-class designers. ICONSVG a tool to simplify the process of finding and generating common icons for your project. Icons8 free iOS, Android and Windows styled icons. Ikonate customize, adjust and download free vector icons. illustrio a smarter icon library. Build something great with 100% customizable icons. Ionicons beautifully crafted open source icons. Material Design Icons free material design icons made possible by open source contributons. Material Icons Library free collection of 1000+ icons for popular graphics tools. Motion free, simple animated icon editor. Norde Source mac, Windows and Linux app to color and customize icon sets to fit your brand. Noun Project icons for everything. Nucleo a Mac and Windows app to collect, customize and export all your icons. Orion Icons SVG vector icons with an advanced interactive web app. Simple Icons free SVG icons for popular brands. Simply click the icon you want, and the download should start automatically. Share Icon more than 300 000 free download icons in different formats. Streamline Emoji a free collection of 850 + vector emoji. The style is inspired by the japanese Kawaii (cute) style. Svgsus SVG icon management tool. SVG Colorizer a tool to automatically change the entire color palette of any given SVG icon pack intelligently keeping the shades, highlights & shadows. SVGRepo a site with 300.000+ SVG Vectors and Icons. Tilda Icons download free icons for landing pages. More than 700 vector icons, collected in 43 sets for business. Twemoji Twitters open-source emoji has you covered for all your project's emoji needs. Unicons 1000+ pixel-perfect vector icons for your next project. VisualPharm free SVG Icons with super-fast search and free Coke. Built for fun by Icons8. Xicons free vector icons, that update every 10 days. Zwicon handcrafted icon set for your next project. You can also handle icons with Specify, which is mentioned in the Design System Tools section. Illustrations Illustrations can be used on your landing page, blog posts, inside your app or email campaign. They make your design live and playful. While drawing good illustration is a task of a skilled graphic designer, you can grab free SVG images & illustrations from very kind people in our community: Absurd Design free surrealist illustrations for your projects. Blobmaker create vector blob illustrations in the browser, with varying colour, complexity and contrast. Blush create, mix and customize illustrations made by artists around the world. Humaaans a free library to mix-&-match illustrations of people. Illustration by Pngtree over 13142 professionally designed illustrations of different styles. IRA Design create amazing illustrations, using hand-drawn sketch components, a cool selection of 5 gradients and ai., svg. or png. formats. JoeSchmoe an illustrated avatar collection for developers and designers, perfect as placeholders or live sites. LukaszAdam free vector art illustrations and icons. They are available for personal and commercial use. ManyPixels royalty-free illustrations to power up your projects. Mega Doodles Pack big vector pack with hand-drawn doodles for presentations, social media, blog posts and so on. Open Doodles a set of free illustrations by Pablo Stanley that embraces the idea of Open Design. You can copy, edit, remix, share, or redraw these images for any purpose without restriction under copyright or database law (CC0 license.). Ouch vector illustrations to class up your project. Free for both personal and commercial use. Squircley - all you need to start creating beautiful SVG squircles. unDraw a collection of beautiful SVG images. Wannapik a collection of free illustrations, vector images, photos, and animations for any use. Information Architecture Information architecture helps designers organize and structure content inside websites, mobile apps, and other software. So users will understand product functionality and will find everything needed. These information architecture tools should allow you to create visual sitemaps and to improve your website content structure: DYNO Mapper organize website projects using visual sitemaps, content inventory, content audit, content planning, daily keyword tracking, and website accessibility testing. Octopus.do visual sitemap builder. Build your website structure in real-time and rapidly share it to collaborate with your team or clients. OmniGraffle reate beautiful diagrams and designs with this powerful and easy to use app. OptimalSort card sorting tool that allows you to understand how people categorize content in your UI. Treejack upload your proposed sitemap to see how a user would move through your site. WriteMaps create sitemaps that impress! Plan out the pages and content for your next website project in a visual, fun, and beautiful manner. Logo Design A logo is the starting point of your brand identity. It reflects the product mission, functionality and brand message. Ideally, the logo creates a strong connection between your product and the users. Logo design is an art, as well as many other design disciplines. With the right logo design tools, this art can be done right a bit faster. You can also do your logo design with Adobe Photoshop, GIMP, Inkscape, Krita and Vectr which are mentioned in UI design tools. Mockup Tools A mockup is a visual way of representing the product. While a wireframe mostly represents a products structure, a mockup shows how the product is going to look like. These mockup tools that help you create and collaborate on mockups, wireframes, diagrams, and prototypes: Artboard Studio online graphic design application mainly focused on product mockups. Cleanmock create stunning mockups using the latest device frames like iPhone & custom backgrounds. Craftwork Design free and premium high-quality digital products that make your work faster and easier. Device Shots a tool that helps you create beautiful device mockups with the screenshot of your website or mobile application, for free. Devices by Facebook images and Sketch files of popular devices. Dimmy.club device mockup generator for your website and app screenshots. Frrames Frrames mockups is perfectly crafted responsive windows mockups for your ideal presentation. Lstore Graphics free and premium mockups, UI/UX tools, scene creators for busy designers. Mediamodifier create impressive product mockups in seconds. Mockflow the quickest way to brainstorm user interface ideas. Mockup World tons of free and legal, fully layered, easily customizable photo realistic PSDs. Mockups For Free free design resources, PSD files for graphic and web designers. Mockuuups drag-and-drop tool for creating beautiful app previews or any marketing materials. Mock Video instantly create mockups by adding a device frame to your videos. Moqups helps you create and collaborate in real-time on wireframes, mockups, diagrams and prototypes. Original Mockups high-quality mockups for Photoshop that make your designs stand out. Overframe for Mac record your prototype & app with device frame overlay. PixelBuddha free and premium high-quality resources for web designers and developers. Ramotion Store carefully crafted Apple and Android mockups for Sketch and Photoshop. Rotato animated 3D mockups for your app designs. SceneLab create realistic mockups and customized brand designs online. Screely quickly frame web page designs into a minimalist browser window. ScreenSpace 3D devices videos for motion designer. Screenzy instantly transform your pictures and screenshots into beautiful mockups ready to be shared on social media. Screeener use this app to insert a bunch of images to a keynote file, using the mockup you choose. Smartmockups create stunning product mockups with just a few clicks. shotsnapp create beautiful device mockup presentation for your digital app and website design. The Mockup Club a directory of the best free design mockups for Photoshop, Sketch, Figma and InVision Studio. Threed generate custom 3D Device Mockups in your Browser. UI Store Design 200+ free handpicked design resources for Sketch, Figma, Adobe XD, InVision Studio, Photoshop, Illustrator CC. UI8 Design Freebies epic design freebies to get you started. Vector Mockups Library free collection of presentation Mockups for Sketch, Figma & Photoshop. No Code Tools With a rise of no code tools, everyone with a laptop can build and launch a project. These tools help designers and makers create websites, apps, and even games. No code tools allow to automate routine tasks and can be used without a development background. Take a look at the tools here and if you need more check Design to Code section. Bubble build and host web applications without having to write code or hire a team of engineers. Carrd simple, free, fully responsive one-page sites for pretty much anything. Coda a new type of document that blends the flexibility of documents, the power of spreadsheets, and the utility of apps into a single new canvas. Kodika.io build iOS apps with Kodika no code app builder for Mac & iPad, capable of creating powerful apps with Drag & Drop. PageXL a simple one-page website builder for quickly creating landing pages and online stores. Remove.bg a free service to remove the background of any photo. Retool gives you building blocks and you can build tools much faster. sheet2api create an API from Google Sheets or Excel Online Spreadsheets, no coding required. Sheet2Site create a website from Google Sheets without writing code. Shopify one platform with all the e-commerce and point of sale features you need to start, run, and grow your business. Thunkable a drag-and-drop tool for anyone to build native mobile apps. UserGuiding create walkthroughs, checklists, hotspots, and modals to improve user onboarding. Pixel Art Tools Pixel art is a digital art form where color is applied to individual pixels to create an image. The pixel art can be used to create everything from intricate scenes and game backgrounds to character designs or emoji. If you feel curious to try, check this pixel art software for both macOS and Windows: Prototyping Tools A prototype is a simple experimental design of a proposed solution. It should help to test ideas, design assumptions, and hypotheses in a fast and cheap way. Prototyping tools allow designers and clients to see how the product would function in the real world and collaborate on this solution. Many modern prototyping tools can use for wireframing, prototyping, and collaboration: Alva create living prototypes with code components. It's also open source. Axure RP wireframing, prototyping, collaboration & specifications generation. SAP Build a complete set of cloudbased tools to design and build your enterprise app. Creo prototyping, code exporting and built-in mobile app builder. Drama prototype, animate and design in a single Mac app. InVision prototyping, collaboration & workflow platform. Flinto a Mac app for creating interactive and animated prototypes of app designs. Framer X a tool to visually design realistic interactive prototypes. Keynote the built-in Mac app for creating presentations that can also be used for quick prototyping (see how Apple designers use it in the WWDC 2014 session \"Prototyping: Fake It Till You Make It\" to verify design concepts). Lightwell visual tool and SDK to build mobile layouts and animations that translate into native iOS elements. Marvel App the collaborative design platform. Wireframe, prototype, design online and create design specs in one place. Maze a tool for designers and developers that gives analytical results with actionable KPIs for your Invision prototypes. Origami a free tool for designing modern user interfaces. Quickly put together a prototype, run it on your iPhone or iPad, iterate on it, and export code snippets your engineers can use. Pencil prototyping tool with many built-in components that people can easily install and use to create mockups in popular desktop platforms. Principle makes it easy to design animated and interactive user interfaces. ProtoPie piece hi-fi interactions together, build sensor-aided prototypes and share your amazing creations in a matter of minutes. Proto.io a tool to create fully-interactive high-fidelity prototypes that look and work exactly as your app should. Prott an easy to use and intuitive prototyping tool, promotes team collaboration. Uizard transform wireframes into high-fidelity interactive prototypes, customize style systems, export to Sketch, export to HTML/CSS code. Useberry a usability testing tool that allows importing prototypes from InVision, AdodeXD, Sketch, Marvel and getting users behavior insights with heatmaps, video recordings, user flows, time bars and answers of follow-up questions. UXPin build prototypes that feel real, with powers of code components, logic, states, and design systems. You can also do prototyping with Figma, Adobe XD, Sketch and InVision Studio, which are mentioned in the UI Design Tools section. Screenshot Software Taking screenshots is a typical task in the design & development workflow. So these free and full-featured screenshot apps can help you capture a screen with ease. Some screen captures are macOS only, while others support both OS: Camtasia a screen recorder that comes with a full-blown built-in editor. CleanShot capture your screen in a superior way with a built-in annotation tool and Quick Access Overlay. CloudApp record videos, webcam, annotate screenshots, create GIFs. Collabshot take and collaborate on screenshots in real-time with your coworkers. Gifox delightful GIF recording and sharing app. Giphy Capture capture parts of your screen and export as gif or mp4. Greenshot take a screenshot of a selected region, window or fullscreen. Kap open source screen recorder with options to export to GIF, MP4, WebM and APNG. Lighshot taking quick captures of your screen. Monosnap create screenshots, annotate and upload them to the cloud. OBS open source software for video recording and live streaming. Quicktime a video player that you can use to record your screen. ScreenFlow video editing and screen recording software for Mac. Screenie filter and search through images, change screenshot filetypes. ScreenshotAPI.net create pixel-perfect full page website screenshots. ScreenToGif record a gif of part of the screen. Only available for Windows. ShareX screen capture, file sharing, and productivity tool. Shotty a menu bar app that helps you quickly find the screenshot you're looking for so you can drag and drop it into any application. Snagit capture images and video, create GIFs, annotate, edit, and share. Snipping Tool Windows free screenshot tool. Snappy takes quick shots and organizes them for you into collections. Teampaper Snap allows you to take screenshots of a selected area. Sketching Tools Sometimes you need just a pencil and paper to start creating your app or website. So here are you can find online sketching tools with great sketch sheet templates to speed up your creative process: You can also do some sketching with Sketch mentioned in UI design tools. SMM Design Tools Often marketing teams need well-design materials. It can be different banners, promo visuals, favicons, animations or just nice images for social platforms, like Twitter or Instagram. It this section we will keep adding tools for everyone to create marketing designs. Sound Design Sound design is an art of creating a soundscape for a site, app, movie, game or any other product. The sound has great potential for transforming the way people connect with your product. Some sound design software is very advanced and can be used mostly by sound designers, while others are good for beginners. Appsounds UI Sound packs for apps, games, and any product. AudioJungle 836,206 tracks and sounds from the community of musicians and sound engineers. Bensound download creative commons music, royalty free music for free and use it in your project. Freesound a collaborative database of Creative Commons Licensed sounds. Browse, download and share sounds. Fugue Music download free background music for your videos. Max connect objects with virtual patch cords to create interactive sounds, graphics, and custom effects. Reaper import any audio and MIDI, synthesize, sample, compose, arrange, edit, mix, and master songs or any other audio projects. Sonic Pi a live coding music synth. SoundKit a UI sound library designed for all of your interface needs. UI Sounds learn sound design for user interfaces by example. Wistia Music download some free background tracks to add energy and emotion to your videos. WOWA download royalty-free music for YouTube videos, podcasts, and apps. No copyright CC0. Music inspired by Unsplash. YouTube Audio Library browse and download free music for your project. Stock Photos Tools Need a high-quality photo for iOS app or new banner? You can always shoot it yourself or borrow from the stock photo sites. Luckily, there are hundreds of beautiful, free stock photos and royalty-free images that you can use for any project: Burst free stock photos for websites and commercial use. Duotone free duotone images to use in any project, or make custom duotone images. Death to Stock paid-for stock photo service with a mailing list for occasional free packs, and a focus on not looking like stock photography. FoodiesFeed thousands of beautiful realistic free food pictures in high resolution. FreePhotos.cc free stock photos for commercial use. Freestocks.org high quality photos all released under Creative Commons CC0. Gratisography a collection of free high-resolution pictures. Jay Mantri 7 new photos released every Thursday under the Creative Commons CC0. Kaboom Pics stock photos including abstract, city/architecture, fashion, food & landscapes. LandingStock a collection of free images for your landing page. Life of Pix free high-resolution photos, created by the LEEROY team. LoremPixel an API that serves stock photos at specified sizes and categories. Great for placeholder/user-generated content. Magdeleine free high-quality stock photos for your inspiration. Moose don't search for stock photos, create them. MMT STock high-resolution photos provided by Jeffrey Betts with Creative Commons CC0. New Old Stock a vintage photos from the public archives free of known copyright restrictions. Pexels an aggregate of many free stock photo resources. Photo Creator a free tool for creating realistic stock photos on your demand. Picography free stock photos by Dave Meier and various other photographers with Creative Commons CC0. Pixabay sharing photos, illustrations, vector graphics, and film footage under a proprietary license. Picjumbo a collection of totally free photos for your commercial & personal works. Pngtree millions of PNG images, backgrounds and vectors for free download. pxhere free image stock. Reshot a massive library of handpicked free stock photos you wont find elsewhere. ShotStash thousands of free high-resolution CC0 photos and videos. SkitterPhoto a wide variety of stock photos and are released under Creative Commons CC0. Startup Stock Photos free photos for startups, bloggers and publishers. StockSnap.io a large selection of free stock photos and high-resolution images. StyledStock free feminine stock photography for every woman entrepreneur. The Gender Spectrum Collection a stock photo library featuring images of trans and non-binary models that go beyond the clichs. UI Faces an aggregator that indexes various free avatar sources that you can use in your design mockups. Unsplash stock photos free to use. #WOCinTech Chat Photos free stock photos of women technologists of diverse backgrounds. Zoommy helps you find awesome free stock photos for your creative product or inspiration. Stock Videos If you work with video-content, you will enjoy these high-quality, hand-curated stock videos. You'll find many good and free stock video sites below, which you can use on your website, in your ad campaigns or social media: Tools for Learning Design UI Design Tools What are the best UI design tools in 2019? You can pick any of the tools below and it will allow you to do dozens of design tasks UI for site or mobile, wireframe, prototype, animation, logo. These are great and fully-functional tools for UX & UI designers: Adobe XD design, prototype, and share any user experience, from websites and mobile apps to voice interactions. Affinity Designer a vector graphics editor for macOS, iOS, and Microsoft Windows. Akira native Linux App for UI and UX Design built in Vala and Gtk. Botmock design, prototype, and test voice and text conversational apps. Supports multiple platforms. CleverBrush a browser-based online vector editor and digital publishing tool which is possible to integrate to the page as JS component. Figma a design tool based in the browser, that allows to design and prototype with real-time collaboration. GIMP a free & open-source imaging and graphic design software. Gravit a free vector design app, available for macOS, Windows, Linux, ChromeOS, and browser. Illustrator create logos, icons, drawings, typography, and illustrations for print, web, video, and mobile. Made by Adobe. Inkscape a free and open-source vector graphics editor. It can create or edit vector graphics such as illustrations, diagrams, line arts, charts, logos, and complex paintings. Krita a free painting and graphic design software considered a good alternative to Adobe Photoshop. Lunacy a free native windows app that works offline and supports .sketch files. Flexible and light weighed. Intuitive and easy to use. Speedups and empowers UI and UX designers. The must have to produce stunning designs. Photopea a free browser-based graphic design app alternative to Photoshop. Photoshop imaging and graphic design software developed by Adobe. Pixelixe a graphic design tool built for marketers, bloggers and small businesses that needs to create stunning and unique images, graphics or static webpages in minutes. Sketch a design toolkit built for Mac. Studio combines design, prototyping, and collaboration into one harmonious workflow. Made by InVision. TwitPile creates tiled images out of Twitter followers, interests and lists. Vectr a simple web and desktop cross-platform tool to create vector graphics easily and intuitively. Voiceflow prototype, design and deploy real apps for Amazon Alexa & Google Home. User Flow Tools User flow is a series of steps a user takes to achieve a meaningful goal. It's a fast way to plan customer journey paths and improve UX. So if you need to make a user flow diagram, user flow map or a sitemap, take a look at these tools: Draw.io a free online diagram software for making flowcharts, process diagrams, org charts, UML, ER, and network diagrams. Flowmapp an online tool for creating sitemaps and user flows that helps you to effectively design and plan user experience. Google Drawings create diagrams and charts, for free; all within Google Docs. Lucidchart an online tool for creating diagrams, flow charts, sitemaps, and more. MindNode a mind mapping app that makes brainstorming simple and easy. NinjaMock wireframe and user flow online tool. Link your views and create logic flow prototype. All with freehand visual style. OmniGraffle a diagramming and digital illustration application for macOS and iOS. Overflow turn your designs into playable user flow diagrams that tell a story. Sketch.systems ui and flow design with interactive state machines. SQUID create beautiful User Flows in Sketch in just minutes. WebSequenceDiagrams a simple webapp to work out object interactions arranged in time sequence. Whimsical easy to create flow charts, wireframes and sticky notes. Wireflow free, online and open source tool for creating beautiful user flow prototypes. XMind: ZEN a brainstorming and mind mapping tool that can switch between outline and tree-chart. Link topics with other charts. yEd free desktop tool for making diagrams. Usable for the wide variety of use cases. Auto-layout helps a lot when making flowcharts. User Research Tools User research helps you understand user behaviors, needs, and motivations through various qualitative and quantitative methods (interviews, observation, forms, etc). These user research tools can be useful for you: Appoint.ly a web-based scheduling tool that helps to schedule meetings quickly through the integration with calendars online. Calendly Calendly helps you schedule meetings without the back-and-forth emails. Crowd Signal collect, organize and analyze data from a variety of sources, including social media and mobile. Doodle online calendar tool for time management, and coordinating events, meetings, appointments Evolt create user personas, storyboards, business model canvas, experience maps, brainstorming boards and moodboards in a clean and modern way. Feedback Lite collect high quality customer feedback using Voice of Customer solutions designed to improve your website performance and boost customer engagement. GoToMeeting a simple, extraordinarily powerful web conferencing service. Handrail end-to-end, collaborative user research and insights platform plan research, collect and analyze data, and share your findings. JotForm create online forms, get an email for each response, collect data. Lookback remotely run, record, and take notes for your user research sessions, either with a live product or with a prototype. MineTime a free calendar app with smart scheduling and time analytics features. Reflector Reflector is a basic screen-mirroring and recording tool so you can conduct user tests remotely, using any existing wireframes or prototypes. Reframer a research tool that helps your team to capture, tag (code) and identify patterns in qualitative data across multiple research participants. Sticktail a platform for centralizing, finding and sharing user insights within your organization. Survey Monkey online survey tool to capture the voices and opinions of the people who matter most to you. Typeform use a simple drag-and-drop interface to create any kind of form, survey, or questionnaire, and even take credit card payments. Wufoo reate forms, collect data and payments and automate your workflows. YesInsights simple one question and NPS surveys to improve your business. UserBit a platform of real-time research tools for your team. Tag/code interviews and feedback, capture insights, create personas, visual sitemaps and more. User Interviews recruit participants from a community of 125,000 members based on profession, demographics, geography and more, for any kind of research. Zoom it's one of the best online meeting services. Visual Debugging Tools Wireframing Tools A wireframe is a visual mockup that outlines the basic structure of the site or an app. It contains the most essential elements and the content, helping you easily explain ideas on design. Wireframes are a low-fidelity way of showing a design. This section is presented the best wireframing tools for a variety of use cases. Three D Modeling Software 3D graphics are used in gaming, film making, architecture, engineering, and 3D printing. 3D artists & designers use specific 3D modeling software, mentioned in this section. Autodesk integrated CAD, CAM, and CAE software. Unify design, engineering, and manufacturing into a single platform. Blender free and open-source 3D Creation Software. FreeCAD a free and open-source multiplatform 3D parametric modeler. MatterControl a free, open-source, all-in-one software package that lets you design, slice, organize and manage your 3D prints. Maya make animations, environments, motion graphics, virtual reality, and character creation, all in one toolset. Onshape a modeling software specially oriented to design technical and spare parts, was the first full-cloud 3D software created. OpenSCAD a software for creating solid 3D CAD objects. Rhino a curve-based 3D modeling software that creates mathematically-precise models of 3D surfaces. SketchUp 3D design software that truly makes 3D modeling for everyone, with a simple to learn yet robust toolset. Tinkercad a free, easy-to-use app for 3D design, electronics, and coding. Vectary create beautiful 3D models with our drag and drop 3D modeling tool. 3D Slash 3D modeling tool, available on all devices and all OS, online and offline. Addendum (Reference & Inspiration) Awesome Design Tools & Plugins is curated by Lisa Dziuba & Valia Havruliyk from Flawless team. And it was hugely inspired by articles from the design community and Prototypr.io Toolbox made by our good friend Graeme Fulton. If you found some great design tools, please suggest it. Thanks for making this project awesome :) "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/LisaDziuba/Awesome-Design-Tools",
        "comments.comment_id": [19581294, 19581612],
        "comments.comment_author": ["koolba", "Crinus"],
        "comments.comment_descendants": [2, 5],
        "comments.comment_time": [
          "2019-04-05T10:45:34Z",
          "2019-04-05T11:55:18Z"
        ],
        "comments.comment_text": [
          "I read the title as an imperative statement and was disappointed that it’s just yet another GitHub “<i>Awesome list of...</i>”.<p>Was really hoping for an article about building custom tooling as I feel like a lot of my day is spent doing exactly that (ie mini tools to automate recurring semi manual tasks).",
          "These lists are a nice idea, but i do not like the execution - they are little more than a dump of someone's bookmarks. Personally i'd like to see a bit more information about each entry, including screenshots and a short explanation about the program.<p>Something like Softpedia would be great (although simpler), although preferably without garbage reviews like this: <a href=\"https://www.softpedia.com/get/Programming/Coding-languages-Compilers/DMD.shtml\" rel=\"nofollow\">https://www.softpedia.com/get/Programming/Coding-languages-C...</a> (for a bonus WTF check the screenshot).<p>Alternatively (and probably easier to do) something like tinyapps.org but with a bit more information per entry."
        ],
        "id": "3ababea7-66be-4d78-8bf1-0c573fa369b8",
        "url_text": "Awesome Design Tools Awesome Design Plugins Awesome Design Conferences Awesome Design UI Kits Awesome Design Tools as a part of Flawless App family joins Abstract! Today, were excited to announce that Flawless App has joined Abstract, a design delivery platform that brings visibility, accountability, measurability, and predictability to design. Flawless App was our first company, and were proud of everything that weve achieved with our 5-person team. Since 2015, weve launched powerful tools for designers and developers among them are Flawless App, Reduce, Flawless Feedback. Weve also invested a lot of love and care into community-driven initiatives. Awesome Design Tools is one of them. So whats next? Our team has joined Abstract and is focused on building out the Abstract SDK, bringing developers experience forward. One unbelievable journey has finished and the new one has just begun. And if you want to know more about the future of Awesome Design Tools, please keep reading our FAQ. How to Use and Contribute Now you are in the Awesome Design Tools section, if you need plugins go to Awesome Design Plugins. To find the tool, go through the Table of Contents or search for a keyword (for example, \"animation\", \"prototyping\"). Ask Lisa on Twitter. If you found some great design tool or plugin, just send a Pull Request with respect to our Contribution Guidelines (they're very simple, please take a look). Design tools should be submitted here and plugins in Awesome Design Plugins file. We use such labels for free , open source and Mac only tools, don't forget to add them. Now I'd love to see your suggestions! Table of Content Accessibility Tools Animation Tools Augmented Reality Collaboration Tools Color Picker Tools Design Feedback Tools Design Handoff Tools Design Inspiration Design System Tools Design to Code Tools Design Version Control Development Tools Experience Monitoring Font Tools Gradient Tools Icons Tools Illustrations Information Architecture Logo Design Mockup Tools No Code Tools Pixel Art Tools Prototyping Tools Screenshot Software Sketching Tools SMM Design Tools Sound Design Stock Photos Tools Stock Videos Tools for Learning Design UI Design Tools User Flow Tools User Research Tools Visual Debugging Tools Wireframing Tools 3D Modeling Software Accessibility Tools Accessibility is the practice of creating websites and apps usable for all people, including individuals with visual, motor, auditory, speech, or cognitive disabilities. Here you will find web accessibility tools, accessibility testing tools, and accessibility apps both for developers and designers: Accessibility checking features are also available in VisBug from the Visual Debugging Tools category. Animation Tools Animations guide people through the product friendly and smoothly. Live interactive UI makes users feel delighted with instant feedback and emotional touch. These free and paid tools are designed to make animation creation easier. If you plan to make animated transitions, micro-interactions or scroll-based animations, go through these tools: Drama, Principle, Framer, Invision Studio, Flinto are also among UI & UX animation apps and software animation tools. Augmented Reality Augmented Reality is a technology that upgrades our real world, by adding a layer of digital information to it. The use of AR increases day by day with dozens of new AR apps, development kits, and AR frameworks. So in this section is collected different augmented reality tools for creating, projecting and prototyping apps focused on AR: Collaboration Tools Looking to try some tools for agile design and development collaboration? Good idea because such tools make the lives of designers and developers much easier, save time and improve productivity. Well, you know all that in Slack-driven era. Here you'll find the best collaboration tools for product teams: Airtable part spreadsheet, part database, and entirely flexible, teams use Airtable to organize their work, their way. Asana the work management platform teams use to stay focused on the goals, projects, and daily tasks that grow business. Basecamp the project management suite designed to organize employees, delegate tasks, and monitor progress right from one place. Freedcamp the most innovative way to manage projects, completely free... forever. Droplr screenshot, file sharing and screencasts to help you capture and explain your ideas. Excalidraw a whiteboard tool that lets you easily sketch diagrams with a hand-drawn feel. Filestage an agile content approval software that helps you to review videos, designs, and documents with clients and co-workers. Float a tool to plan your projects and schedule your teams time all in one place. Gallery a collaborative tool for uploading design work, getting feedback, and tracking revisions. HiveDesk an automatic time tracking for remote employees with screenshots. It makes it easier to monitor the productivity of remote employees and manage projects. Jira software development tool used by agile teams. Jitsi multi-platform open-source video conferencing you can install yourself or use on their servers. Keybase a free collaboration app with built-in end-to-end encrypted chat, file sharing, git repositories, and more Lumeer - an easy visual collaborative tool to plan, organize and track anything, anywhere, anytime. Mattermost a flexible, open-source messaging platform that meets even the most demanding privacy and security standards. Milanote an easy-to-use, collaborative tool to organize your ideas and projects into visual boards. Mixed real-time whiteboard and collaboration tools for distributed teams. Moqhub fast online proofing for creative projects. Get feedback for images and PDFs. MURAL think and collaborate visually. Anywhere, anytime. Nextcloud open source collaboration platform for files, kanban boards, chat & calls, calendar and more. Notion write, plan, collaborate, and get organized. Notion is all you need in one tool. ProofHub the one place for all your projects, teams and communications. RealtimeBoard (Miro) whiteboarding platform for cross-functional team collaboration. It was recently renamed to Miro. Slack a collaboration hub for work, no matter what work you do. Its a place where conversations happen, decisions are made, and information is always at your fingertips. Sunsama a beautifully designed, team-oriented task manager that consolidates Trello/Jira/Asana tickets into a single, calendar-oriented view. Taskade team productivity made simple, fun and designed for remote teams. Taskade is collaborative and syncs in real-time across the web, mobile, and desktop. Trello a web-based project management application that enables you to organize and prioritize your projects in a fun, flexible and rewarding way. Witeboard simple real-time whiteboard for collaboration work. Workzone a simple, powerful project management software. Wrike an online project management software that gives you full visibility and control over your tasks. Zenkit a tool to schedule meetings, track the projects progress, and brainstorm new ideas. zipBoard an online visual bug tracking and website annotation tool Zulip combines the immediacy of real-time chat with an email threading model, helping to focus on important conversations while ignoring irrelevant ones. Color Picker Tools If you are looking for an eyedropper tool, color identifier or color capture, check this section. With the color pickers mentioned here, you will be able to create new color combinations and define great ones. Almost all of these tools are free: BrandColors the biggest collection of official brand color codes around. Calcolor a web-based color palette management tool which provides a new way to interact with digital colors. Chroma free web app which allows you to identify the color. Color create color schemes with the color wheel or browse thousands of color combinations from the Color community. Made by Adobe. Color by Cloudflare a tool to preview palettes against UI elements, cycle through accessible color combos and create palettes manually, via URL import, or generatively. Color Deck an HSL driven color palette collection. It's also open-source. Color Hexa free color tool providing information about any color and generating matching color palettes for designs. Color Hex Picker - a tool for getting the name of any color from any image. You also get the HEX code and RGB value for the color. Color Hunt free and open platform for color inspiration with thousands of trendy hand-picked color palettes. ColorKit a tool for blending colors and generating a color's shades and tints. Color Leap leap through time and see the colors of history. Colorpicker a complete and open-source color manipulation tool with picking. ColorSlurp the ultimate color productivity booster for designers and developers. ColorsWall/ place to store your color palettes or easy generate. Colorwise search through the color palettes of the most voted products on Product Hunt. Colour Cafe colors inspiration that has selected a modern color palette on Instagram. Colourcode an online designer tool, which allows you to easily and intuitively combine colors. Coolors the super-fast color scheme generator. Culrs thoughtfully crafted and easy-to-use color palettes. Geenes a tool to create harmonious color themes with code and sketch file export. Image Color Picker image color picker and color scheme generator. Instant Eyedropper a minimalist eyedropper tool for windows, supports multiple color formats for both designers and developers. Just Color Picker Free portable offline color picker and color editor. Khroma AI-based tool to generate color palettes based on your preferences. Material Colors Native Google's material design color palette in the form of a mac app. Material Design Palette Generator get perfect Material Design color palettes from any hex color. Paletton a designer tool for creating color combinations that work together well. Picular a rocket fast primary color generator using Google's image search. Pigment a color palette generator with innumerable color configurations suggested by the app. Pikka color picker & color schemes generator for macOS. React Color a collection of color pickers from Sketch, Photoshop, Chrome. Its free and open-source. Sip a better way to collect, organize and share colors. Global eyedropper tool for macOS. Skala Color works with a huge variety of formats, covering everything youre likely to need for web, iOS, Android, and macOS development. Swatches iOS app that lets you collect, inspect and share the colors that inspire you in your daily life. Tint & Shade Generator display tints and shades of a given hex color in 10% increments. Viz Palette color picker for data visualizations. You can also create color palettes with Leonardo, which is mentioned in the Accessibility Tools section. Design Feedback Tools How do you provide and get feedback during the development process? This process is usually pretty messy for many product teams, which caused product delays and time waste on back-and-forth communication. So if you wish to get fast and structural feedback on UI issues or visual bugs take a look at this tool: Flawless Feedback review and annotate iOS apps then share your feedback in Jira or Trello. GoVisually online proofing, design review & approval software. Design Handoff Tools Design handoff happens when designers finish the work and need to deliver designs to developers with all specs and assets. Design handoff tools allow to automatically generate style guide, comment on the design, inspect elements. These tools minimize guesswork and improve the effectiveness of the design process. You can also make a design handoff within Relay, mentioned in the Design Version Control section. Design Inspiration The creative process can be tough. So if ideas dont come to your mind right away, try to search for inspiration in the work of other designers. These design inspiration sites feature design patterns, user flows, email markups and creative solutions of popular companies and great products: Behance an online platform to showcase & discover creative work. CodeMyUI handpicked collection of web design & UI inspiration with code snippets. Collect UI a platform for your daily inspiration collected from Daily UI & beyond. Based on Dribbble shots, 14419 handpicked designs. Creative Portfolios a curation of the most creative portfolios made by designers & developers. Design Hunt a daily collection of the best products, apps, and inspirations for all creatives. Dribbble an online community for showcasing user-made artwork. A great resource to get inspired by others work. Ficture a font in use archive that will inspire you. Inspiration UI a community that aims to provide the best design resources for those who create for the web. Hover States showcase great work from the bleeding edge of digital culture for, and with, the creative community. Httpster an inspiration resource showcasing rocking websites made by people from all over the world. H69.Design landing page colletions & Free resources for designers. Lapa Ninja landing page design inspiration from around the web. 1800+ landing page examples and updated daily. Mobile Patterns a design inspirational library featuring finest UI UX Patterns (iOS and Android). Mobbin browse mobile design patterns for popular apps. Explore common user flows such as onboarding, account sign up and log in, core functionality, and more. One Page Love a collection of beautiful unique One Page websites for website design inspiration. Owwly home for digital products crafted with passion to design. Page Flows user flow videos and screenshots to inspire you when you're stuck. Discover mobile and web design patterns for over 160 different tasks. pttrns design patterns for popular mobile apps. Really Good Emails 4,150+ handpicked email design. ReallyGoodUX screenshots, and examples of great UX from real mobile and web products. Discover the best UX examplesincluding onboarding tours and walkthroughs. Saas Landing Page iscover the best landing page examples created by top-class SaaS companies, and get ideas and inspiration for your next design project. Saas Pages a collection of the best landing pages with a focus on copywriting and design. The Design Team comics about a design team for a tech startup in Silicon Valley. Typewolf helps designers choose the perfect font combination for their next design project. UI Garage the one-stop shop for designers, developers, and marketers to find inspiration, tools and the best resources. UI Sources over 500+ interactions from the best designed and top-grossing apps on the App Store today. UI Recipes weekly 15 min lessons on UI design from the hottest apps. UX Archive browse more than 400 user flows across 120 mobile apps. Examine tasks such as booking, logging in, onboarding, purchasing, searching, and more. Waveguide a design knowledge bank with thousands of artificially enriched examples of product and brand experience (examples of Mobile App, Landing pages, eCommerce, CX/UX Patterns). Web Design Museum over 1,200 carefully selected web sites that show web design trends between the years 1994 and 2006. Design System Tools I bet you heard about Design Systems, as its a pretty trendy topic. Design systems provide consistent, robust design patterns to keep design and development in sync. They are essentially collections of rules, constraints, and principles, implemented in design and code. Here you can find tools to build, maintain and organize your design system. Cabana a Premium Design System for Sketch that helps you create amazing products faster than ever before. Catalog a living style guide for digital products, combining design documentation with real live components. Design System Manager Invision's design system manager. DSK short for Design System Kit a workbench for collaboratively creating Design Systems. EOS Design System an open-source and customizable built on top of Bootstrap following the Atomic Design concept. Eva Design System customizable design system, available for Sketch with Mobile and Web component libraries. Frontify create graphical guidelines, patterns libraries, design systems. Interplay connect design and engineering around a single source of truth. The tool is not publicly available yet (beta). Lingo create a shared asset library with your entire team. Lucid tool for creating, managing and sharing a design system. From a simple component library through to detailed descriptions of your styles. Modulz design, build, document and publish your design systemwithout writing code. Prime Design System Kit it includes device templates, charts, UI kit and even illustration kit. Specify a tool to create, scale and maintain a design system. Storybook an open-source tool for developing UI components in isolation for React, Vue, and Angular. Symbol Design System design System for Sketch-based on atomic elements. Toolabs design systems and components based design, prototype and development tool. It's not public yet but you can apply to the early access. Zeroheight style guides created by designers, extended by developers, and editable by everyone. We can also add to this in Sketch, Figma, UXPin and Framer X (Framer X Team Store). Design to Code Tools Everyone can learn development but it takes time and effort. If you need a website or an app right now and you dont want to hire a developer, pay attention to the website builders. Such design to code tools will help you to make a portfolio, simple landing or an app pretty fast and beautiful. Anima a web app with a Sketch plugin that converts Sketch to HTML. Blocs a fast, easy to use and powerful visual web design tool, that lets you create responsive websites without writing code. Bootstrap Studio a powerful web design tool for creating responsive websites using the Bootstrap framework. Draftbit visually design and build mobile apps directly from your browser. EasyLogic Studio fantastic css+svg design tool, also it is converted into code as shown. Grapedrop design your components, web projects and publish them instantly online, with an easy to use editor. Mobirise an offline drag and drop website builder software that is based on Bootstrap 3/4 and \\ AMP. PageCloud the fastest and most customizable website builder that allows anyone to create their ideal website. PaintCode vector drawing app that instantly converts drawings into Swift, Objective-C, JavaScript or Java code. Pinegrow a professional visual editor for CSS Grid, Bootstrap 4 and 3, Foundation, responsive design, HTML, and CSS. px.div a proper site build tool for developers and designers. Readymag a visually-pleasing tool for designing on the web from landing pages to multimedia long-reads, presentations and portfolios. Sparkle the easiest way to make a real website, no code, no jargon. STUDIO design from scratch, collaborate in real-time and publish websites. Supernova Studio import designs from Sketch and convert them into Android, iOS or React Native code. Tilda create a website, landing page or online store for free with the help of Tilda modules and publish it on the same day. Wix the easiest and fullest-featured website builder, that allows you to create your own highly customized site. Webflow build responsive websites in your browser, then host with us or export your code to host wherever. Design Version Control Developers actively use version control tools for a long time, probably since 2005 (Git first release). Using a version control system is no brainer for dev teams, while the design version control system appeared only recently. This market is rapidly developing and we expect to see even more in version control for designers: Development Tools This section mentions development tools and browsers. Development browsers have features that help developers and designers create and test websites and apps. Experience Monitoring Listening to users is important but seeing the real usage is even more crucial. For these, you need to install different analytic tools, experience monitoring software, and user behavior apps. Just use those analytics solutions concerning users data: Font Tools Fonts are commonly used for making the web a more beautiful place. Its an essential part of any design. In this section, youll find fonts generators & font finder tools that allow you to manage and work with fonts: BeFonts a Decent collection of free fonts. Behance Free Fonts a list of free fonts uploaded on Behance. DaFont archive of freely downloadable fonts. Browse by alphabetical listing, by style, by author or by popularity. Emotype makes it easy to find fonts based on the emotions you want to convey on your website. Fontbase font management made easy. FontFabric gorgeous interface and good collection. Fontface Ninja browser extension to discover what fonts are being used on any website. FontPair a simple tool that helps you pair Google Fonts together. Fontown a typographic tool with a nonstop growing font catalog which facilitates designer workflow. Fonts thousands of beautiful fonts by Adobe. Fonts Arena curated typography website with high-quality free fonts, done-for-you research articles, free alternatives to premium fonts, news, and more. FontGet variety of fonts all sorted neatly with tags. FontSelf an extension for Illustrator and Photoshop CC that lets you turn any lettering into OpenType fonts in minutes! FontSpark a simple tool to help designers quickly find the best fonts for their projects. Font Squirrel download free fonts with wide collections. Google Fonts making the web more beautiful, fast, and open through great typography. Google Webfonts Helper a hassle way to self-host Google Fonts. Its free and open-source. LostType the first Pay-What-You-Want type foundry. Measure measure typographic line lengths with this browser extension for Chrome. RightFont font managing app, helps preview, sync, install and organize fonts over iCloud, Dropbox or Google Drive. Sans Forgetica a downloadable font that is scientifically designed to help you remember your study notes. Size Calculator calculate the perceived size using viewing distance and physical size of the printed typography. Typeface font manager that improves your design workflow with live font previews and flexible tagging. Wakamai Fondue the tool that answers the question \"what can my font do?\". Web Font Preview preview Google Font pairings with components and site templates. Webfont create and maintain custom SVG icon fonts, made a shared library of icons. WordMarkIt displays your written word/phrase with all the fonts which are already installed on your computer. You can also handle fonts with Specify, which is mentioned in the Design System Tools section. Gradient Tools You can see gradient colors are everywhere UI, branding, illustration, typography. A gradient is created by using two or more different colors to paint one element while gradually fading between them. It might look as a memorable, fresh and unique color. To make such a gradient for your design, use these gradient color palettes. You can also create and audit gradients with Leonardo, which is mentioned in the Accessibility Tools section. Icons Tools As well as fonts, icons are used in every design. These basic elements support and guide many user actions inside the product. Without a doubt, icons are a vital element in user navigation. While making those small design elements is hard and time-consuming, you can get thousands of vector icons for personal and commercial use. Animaticons a growing set of beautiful, high-resolution, animated GIF icons that you can customize. CoreUI Icons premium designed free icon set with marks in SVG, Webfont and raster formats. Digital Nomad Icons lifestyle icon & emoji pack for your next project. 25 high-resolution flat icons. Essential Glyphs created to cover your needs in perfect-shaped icons. Adapted for small and large sizes. Feather Icons each icon is designed on a 24x24 grid with an emphasis on simplicity, consistency, and readability. Flaticon 1593000+ vector icons in SVG, PSD, PNG, EPS format or as icon font. Font Awesome the web's most popular icon set and toolkit, also it's open source. Fontello tool to build custom fonts with icons, also open source. Freepik collection of exclusive freebies and all graphic resources that you need for your projects. Iconscout get high-quality Icons, Illustrations and Stock photos at one place. Iconfinder a marketplace for vector icons (SVG). Icon sets available in IconJar format. IconJar store all your icons in one icon manager. Iconmonstr discover 4412+ free icons in 305 collections.Big and continuously growing source of simple icons. Iconset free, cross-platform and fast SVG icon organizer working on Mac and Windows. Icon Store a library of free vector icons for personal and commercial projects, designed by first-class designers. ICONSVG a tool to simplify the process of finding and generating common icons for your project. Icons8 free iOS, Android and Windows styled icons. Ikonate customize, adjust and download free vector icons. illustrio a smarter icon library. Build something great with 100% customizable icons. Ionicons beautifully crafted open source icons. Material Design Icons free material design icons made possible by open source contributons. Material Icons Library free collection of 1000+ icons for popular graphics tools. Motion free, simple animated icon editor. Norde Source mac, Windows and Linux app to color and customize icon sets to fit your brand. Noun Project icons for everything. Nucleo a Mac and Windows app to collect, customize and export all your icons. Orion Icons SVG vector icons with an advanced interactive web app. Simple Icons free SVG icons for popular brands. Simply click the icon you want, and the download should start automatically. Share Icon more than 300 000 free download icons in different formats. Streamline Emoji a free collection of 850 + vector emoji. The style is inspired by the japanese Kawaii (cute) style. Svgsus SVG icon management tool. SVG Colorizer a tool to automatically change the entire color palette of any given SVG icon pack intelligently keeping the shades, highlights & shadows. SVGRepo a site with 300.000+ SVG Vectors and Icons. Tilda Icons download free icons for landing pages. More than 700 vector icons, collected in 43 sets for business. Twemoji Twitters open-source emoji has you covered for all your project's emoji needs. Unicons 1000+ pixel-perfect vector icons for your next project. VisualPharm free SVG Icons with super-fast search and free Coke. Built for fun by Icons8. Xicons free vector icons, that update every 10 days. Zwicon handcrafted icon set for your next project. You can also handle icons with Specify, which is mentioned in the Design System Tools section. Illustrations Illustrations can be used on your landing page, blog posts, inside your app or email campaign. They make your design live and playful. While drawing good illustration is a task of a skilled graphic designer, you can grab free SVG images & illustrations from very kind people in our community: Absurd Design free surrealist illustrations for your projects. Blobmaker create vector blob illustrations in the browser, with varying colour, complexity and contrast. Blush create, mix and customize illustrations made by artists around the world. Humaaans a free library to mix-&-match illustrations of people. Illustration by Pngtree over 13142 professionally designed illustrations of different styles. IRA Design create amazing illustrations, using hand-drawn sketch components, a cool selection of 5 gradients and ai., svg. or png. formats. JoeSchmoe an illustrated avatar collection for developers and designers, perfect as placeholders or live sites. LukaszAdam free vector art illustrations and icons. They are available for personal and commercial use. ManyPixels royalty-free illustrations to power up your projects. Mega Doodles Pack big vector pack with hand-drawn doodles for presentations, social media, blog posts and so on. Open Doodles a set of free illustrations by Pablo Stanley that embraces the idea of Open Design. You can copy, edit, remix, share, or redraw these images for any purpose without restriction under copyright or database law (CC0 license.). Ouch vector illustrations to class up your project. Free for both personal and commercial use. Squircley - all you need to start creating beautiful SVG squircles. unDraw a collection of beautiful SVG images. Wannapik a collection of free illustrations, vector images, photos, and animations for any use. Information Architecture Information architecture helps designers organize and structure content inside websites, mobile apps, and other software. So users will understand product functionality and will find everything needed. These information architecture tools should allow you to create visual sitemaps and to improve your website content structure: DYNO Mapper organize website projects using visual sitemaps, content inventory, content audit, content planning, daily keyword tracking, and website accessibility testing. Octopus.do visual sitemap builder. Build your website structure in real-time and rapidly share it to collaborate with your team or clients. OmniGraffle reate beautiful diagrams and designs with this powerful and easy to use app. OptimalSort card sorting tool that allows you to understand how people categorize content in your UI. Treejack upload your proposed sitemap to see how a user would move through your site. WriteMaps create sitemaps that impress! Plan out the pages and content for your next website project in a visual, fun, and beautiful manner. Logo Design A logo is the starting point of your brand identity. It reflects the product mission, functionality and brand message. Ideally, the logo creates a strong connection between your product and the users. Logo design is an art, as well as many other design disciplines. With the right logo design tools, this art can be done right a bit faster. You can also do your logo design with Adobe Photoshop, GIMP, Inkscape, Krita and Vectr which are mentioned in UI design tools. Mockup Tools A mockup is a visual way of representing the product. While a wireframe mostly represents a products structure, a mockup shows how the product is going to look like. These mockup tools that help you create and collaborate on mockups, wireframes, diagrams, and prototypes: Artboard Studio online graphic design application mainly focused on product mockups. Cleanmock create stunning mockups using the latest device frames like iPhone & custom backgrounds. Craftwork Design free and premium high-quality digital products that make your work faster and easier. Device Shots a tool that helps you create beautiful device mockups with the screenshot of your website or mobile application, for free. Devices by Facebook images and Sketch files of popular devices. Dimmy.club device mockup generator for your website and app screenshots. Frrames Frrames mockups is perfectly crafted responsive windows mockups for your ideal presentation. Lstore Graphics free and premium mockups, UI/UX tools, scene creators for busy designers. Mediamodifier create impressive product mockups in seconds. Mockflow the quickest way to brainstorm user interface ideas. Mockup World tons of free and legal, fully layered, easily customizable photo realistic PSDs. Mockups For Free free design resources, PSD files for graphic and web designers. Mockuuups drag-and-drop tool for creating beautiful app previews or any marketing materials. Mock Video instantly create mockups by adding a device frame to your videos. Moqups helps you create and collaborate in real-time on wireframes, mockups, diagrams and prototypes. Original Mockups high-quality mockups for Photoshop that make your designs stand out. Overframe for Mac record your prototype & app with device frame overlay. PixelBuddha free and premium high-quality resources for web designers and developers. Ramotion Store carefully crafted Apple and Android mockups for Sketch and Photoshop. Rotato animated 3D mockups for your app designs. SceneLab create realistic mockups and customized brand designs online. Screely quickly frame web page designs into a minimalist browser window. ScreenSpace 3D devices videos for motion designer. Screenzy instantly transform your pictures and screenshots into beautiful mockups ready to be shared on social media. Screeener use this app to insert a bunch of images to a keynote file, using the mockup you choose. Smartmockups create stunning product mockups with just a few clicks. shotsnapp create beautiful device mockup presentation for your digital app and website design. The Mockup Club a directory of the best free design mockups for Photoshop, Sketch, Figma and InVision Studio. Threed generate custom 3D Device Mockups in your Browser. UI Store Design 200+ free handpicked design resources for Sketch, Figma, Adobe XD, InVision Studio, Photoshop, Illustrator CC. UI8 Design Freebies epic design freebies to get you started. Vector Mockups Library free collection of presentation Mockups for Sketch, Figma & Photoshop. No Code Tools With a rise of no code tools, everyone with a laptop can build and launch a project. These tools help designers and makers create websites, apps, and even games. No code tools allow to automate routine tasks and can be used without a development background. Take a look at the tools here and if you need more check Design to Code section. Bubble build and host web applications without having to write code or hire a team of engineers. Carrd simple, free, fully responsive one-page sites for pretty much anything. Coda a new type of document that blends the flexibility of documents, the power of spreadsheets, and the utility of apps into a single new canvas. Kodika.io build iOS apps with Kodika no code app builder for Mac & iPad, capable of creating powerful apps with Drag & Drop. PageXL a simple one-page website builder for quickly creating landing pages and online stores. Remove.bg a free service to remove the background of any photo. Retool gives you building blocks and you can build tools much faster. sheet2api create an API from Google Sheets or Excel Online Spreadsheets, no coding required. Sheet2Site create a website from Google Sheets without writing code. Shopify one platform with all the e-commerce and point of sale features you need to start, run, and grow your business. Thunkable a drag-and-drop tool for anyone to build native mobile apps. UserGuiding create walkthroughs, checklists, hotspots, and modals to improve user onboarding. Pixel Art Tools Pixel art is a digital art form where color is applied to individual pixels to create an image. The pixel art can be used to create everything from intricate scenes and game backgrounds to character designs or emoji. If you feel curious to try, check this pixel art software for both macOS and Windows: Prototyping Tools A prototype is a simple experimental design of a proposed solution. It should help to test ideas, design assumptions, and hypotheses in a fast and cheap way. Prototyping tools allow designers and clients to see how the product would function in the real world and collaborate on this solution. Many modern prototyping tools can use for wireframing, prototyping, and collaboration: Alva create living prototypes with code components. It's also open source. Axure RP wireframing, prototyping, collaboration & specifications generation. SAP Build a complete set of cloudbased tools to design and build your enterprise app. Creo prototyping, code exporting and built-in mobile app builder. Drama prototype, animate and design in a single Mac app. InVision prototyping, collaboration & workflow platform. Flinto a Mac app for creating interactive and animated prototypes of app designs. Framer X a tool to visually design realistic interactive prototypes. Keynote the built-in Mac app for creating presentations that can also be used for quick prototyping (see how Apple designers use it in the WWDC 2014 session \"Prototyping: Fake It Till You Make It\" to verify design concepts). Lightwell visual tool and SDK to build mobile layouts and animations that translate into native iOS elements. Marvel App the collaborative design platform. Wireframe, prototype, design online and create design specs in one place. Maze a tool for designers and developers that gives analytical results with actionable KPIs for your Invision prototypes. Origami a free tool for designing modern user interfaces. Quickly put together a prototype, run it on your iPhone or iPad, iterate on it, and export code snippets your engineers can use. Pencil prototyping tool with many built-in components that people can easily install and use to create mockups in popular desktop platforms. Principle makes it easy to design animated and interactive user interfaces. ProtoPie piece hi-fi interactions together, build sensor-aided prototypes and share your amazing creations in a matter of minutes. Proto.io a tool to create fully-interactive high-fidelity prototypes that look and work exactly as your app should. Prott an easy to use and intuitive prototyping tool, promotes team collaboration. Uizard transform wireframes into high-fidelity interactive prototypes, customize style systems, export to Sketch, export to HTML/CSS code. Useberry a usability testing tool that allows importing prototypes from InVision, AdodeXD, Sketch, Marvel and getting users behavior insights with heatmaps, video recordings, user flows, time bars and answers of follow-up questions. UXPin build prototypes that feel real, with powers of code components, logic, states, and design systems. You can also do prototyping with Figma, Adobe XD, Sketch and InVision Studio, which are mentioned in the UI Design Tools section. Screenshot Software Taking screenshots is a typical task in the design & development workflow. So these free and full-featured screenshot apps can help you capture a screen with ease. Some screen captures are macOS only, while others support both OS: Camtasia a screen recorder that comes with a full-blown built-in editor. CleanShot capture your screen in a superior way with a built-in annotation tool and Quick Access Overlay. CloudApp record videos, webcam, annotate screenshots, create GIFs. Collabshot take and collaborate on screenshots in real-time with your coworkers. Gifox delightful GIF recording and sharing app. Giphy Capture capture parts of your screen and export as gif or mp4. Greenshot take a screenshot of a selected region, window or fullscreen. Kap open source screen recorder with options to export to GIF, MP4, WebM and APNG. Lighshot taking quick captures of your screen. Monosnap create screenshots, annotate and upload them to the cloud. OBS open source software for video recording and live streaming. Quicktime a video player that you can use to record your screen. ScreenFlow video editing and screen recording software for Mac. Screenie filter and search through images, change screenshot filetypes. ScreenshotAPI.net create pixel-perfect full page website screenshots. ScreenToGif record a gif of part of the screen. Only available for Windows. ShareX screen capture, file sharing, and productivity tool. Shotty a menu bar app that helps you quickly find the screenshot you're looking for so you can drag and drop it into any application. Snagit capture images and video, create GIFs, annotate, edit, and share. Snipping Tool Windows free screenshot tool. Snappy takes quick shots and organizes them for you into collections. Teampaper Snap allows you to take screenshots of a selected area. Sketching Tools Sometimes you need just a pencil and paper to start creating your app or website. So here are you can find online sketching tools with great sketch sheet templates to speed up your creative process: You can also do some sketching with Sketch mentioned in UI design tools. SMM Design Tools Often marketing teams need well-design materials. It can be different banners, promo visuals, favicons, animations or just nice images for social platforms, like Twitter or Instagram. It this section we will keep adding tools for everyone to create marketing designs. Sound Design Sound design is an art of creating a soundscape for a site, app, movie, game or any other product. The sound has great potential for transforming the way people connect with your product. Some sound design software is very advanced and can be used mostly by sound designers, while others are good for beginners. Appsounds UI Sound packs for apps, games, and any product. AudioJungle 836,206 tracks and sounds from the community of musicians and sound engineers. Bensound download creative commons music, royalty free music for free and use it in your project. Freesound a collaborative database of Creative Commons Licensed sounds. Browse, download and share sounds. Fugue Music download free background music for your videos. Max connect objects with virtual patch cords to create interactive sounds, graphics, and custom effects. Reaper import any audio and MIDI, synthesize, sample, compose, arrange, edit, mix, and master songs or any other audio projects. Sonic Pi a live coding music synth. SoundKit a UI sound library designed for all of your interface needs. UI Sounds learn sound design for user interfaces by example. Wistia Music download some free background tracks to add energy and emotion to your videos. WOWA download royalty-free music for YouTube videos, podcasts, and apps. No copyright CC0. Music inspired by Unsplash. YouTube Audio Library browse and download free music for your project. Stock Photos Tools Need a high-quality photo for iOS app or new banner? You can always shoot it yourself or borrow from the stock photo sites. Luckily, there are hundreds of beautiful, free stock photos and royalty-free images that you can use for any project: Burst free stock photos for websites and commercial use. Duotone free duotone images to use in any project, or make custom duotone images. Death to Stock paid-for stock photo service with a mailing list for occasional free packs, and a focus on not looking like stock photography. FoodiesFeed thousands of beautiful realistic free food pictures in high resolution. FreePhotos.cc free stock photos for commercial use. Freestocks.org high quality photos all released under Creative Commons CC0. Gratisography a collection of free high-resolution pictures. Jay Mantri 7 new photos released every Thursday under the Creative Commons CC0. Kaboom Pics stock photos including abstract, city/architecture, fashion, food & landscapes. LandingStock a collection of free images for your landing page. Life of Pix free high-resolution photos, created by the LEEROY team. LoremPixel an API that serves stock photos at specified sizes and categories. Great for placeholder/user-generated content. Magdeleine free high-quality stock photos for your inspiration. Moose don't search for stock photos, create them. MMT STock high-resolution photos provided by Jeffrey Betts with Creative Commons CC0. New Old Stock a vintage photos from the public archives free of known copyright restrictions. Pexels an aggregate of many free stock photo resources. Photo Creator a free tool for creating realistic stock photos on your demand. Picography free stock photos by Dave Meier and various other photographers with Creative Commons CC0. Pixabay sharing photos, illustrations, vector graphics, and film footage under a proprietary license. Picjumbo a collection of totally free photos for your commercial & personal works. Pngtree millions of PNG images, backgrounds and vectors for free download. pxhere free image stock. Reshot a massive library of handpicked free stock photos you wont find elsewhere. ShotStash thousands of free high-resolution CC0 photos and videos. SkitterPhoto a wide variety of stock photos and are released under Creative Commons CC0. Startup Stock Photos free photos for startups, bloggers and publishers. StockSnap.io a large selection of free stock photos and high-resolution images. StyledStock free feminine stock photography for every woman entrepreneur. The Gender Spectrum Collection a stock photo library featuring images of trans and non-binary models that go beyond the clichs. UI Faces an aggregator that indexes various free avatar sources that you can use in your design mockups. Unsplash stock photos free to use. #WOCinTech Chat Photos free stock photos of women technologists of diverse backgrounds. Zoommy helps you find awesome free stock photos for your creative product or inspiration. Stock Videos If you work with video-content, you will enjoy these high-quality, hand-curated stock videos. You'll find many good and free stock video sites below, which you can use on your website, in your ad campaigns or social media: Tools for Learning Design UI Design Tools What are the best UI design tools in 2019? You can pick any of the tools below and it will allow you to do dozens of design tasks UI for site or mobile, wireframe, prototype, animation, logo. These are great and fully-functional tools for UX & UI designers: Adobe XD design, prototype, and share any user experience, from websites and mobile apps to voice interactions. Affinity Designer a vector graphics editor for macOS, iOS, and Microsoft Windows. Akira native Linux App for UI and UX Design built in Vala and Gtk. Botmock design, prototype, and test voice and text conversational apps. Supports multiple platforms. CleverBrush a browser-based online vector editor and digital publishing tool which is possible to integrate to the page as JS component. Figma a design tool based in the browser, that allows to design and prototype with real-time collaboration. GIMP a free & open-source imaging and graphic design software. Gravit a free vector design app, available for macOS, Windows, Linux, ChromeOS, and browser. Illustrator create logos, icons, drawings, typography, and illustrations for print, web, video, and mobile. Made by Adobe. Inkscape a free and open-source vector graphics editor. It can create or edit vector graphics such as illustrations, diagrams, line arts, charts, logos, and complex paintings. Krita a free painting and graphic design software considered a good alternative to Adobe Photoshop. Lunacy a free native windows app that works offline and supports .sketch files. Flexible and light weighed. Intuitive and easy to use. Speedups and empowers UI and UX designers. The must have to produce stunning designs. Photopea a free browser-based graphic design app alternative to Photoshop. Photoshop imaging and graphic design software developed by Adobe. Pixelixe a graphic design tool built for marketers, bloggers and small businesses that needs to create stunning and unique images, graphics or static webpages in minutes. Sketch a design toolkit built for Mac. Studio combines design, prototyping, and collaboration into one harmonious workflow. Made by InVision. TwitPile creates tiled images out of Twitter followers, interests and lists. Vectr a simple web and desktop cross-platform tool to create vector graphics easily and intuitively. Voiceflow prototype, design and deploy real apps for Amazon Alexa & Google Home. User Flow Tools User flow is a series of steps a user takes to achieve a meaningful goal. It's a fast way to plan customer journey paths and improve UX. So if you need to make a user flow diagram, user flow map or a sitemap, take a look at these tools: Draw.io a free online diagram software for making flowcharts, process diagrams, org charts, UML, ER, and network diagrams. Flowmapp an online tool for creating sitemaps and user flows that helps you to effectively design and plan user experience. Google Drawings create diagrams and charts, for free; all within Google Docs. Lucidchart an online tool for creating diagrams, flow charts, sitemaps, and more. MindNode a mind mapping app that makes brainstorming simple and easy. NinjaMock wireframe and user flow online tool. Link your views and create logic flow prototype. All with freehand visual style. OmniGraffle a diagramming and digital illustration application for macOS and iOS. Overflow turn your designs into playable user flow diagrams that tell a story. Sketch.systems ui and flow design with interactive state machines. SQUID create beautiful User Flows in Sketch in just minutes. WebSequenceDiagrams a simple webapp to work out object interactions arranged in time sequence. Whimsical easy to create flow charts, wireframes and sticky notes. Wireflow free, online and open source tool for creating beautiful user flow prototypes. XMind: ZEN a brainstorming and mind mapping tool that can switch between outline and tree-chart. Link topics with other charts. yEd free desktop tool for making diagrams. Usable for the wide variety of use cases. Auto-layout helps a lot when making flowcharts. User Research Tools User research helps you understand user behaviors, needs, and motivations through various qualitative and quantitative methods (interviews, observation, forms, etc). These user research tools can be useful for you: Appoint.ly a web-based scheduling tool that helps to schedule meetings quickly through the integration with calendars online. Calendly Calendly helps you schedule meetings without the back-and-forth emails. Crowd Signal collect, organize and analyze data from a variety of sources, including social media and mobile. Doodle online calendar tool for time management, and coordinating events, meetings, appointments Evolt create user personas, storyboards, business model canvas, experience maps, brainstorming boards and moodboards in a clean and modern way. Feedback Lite collect high quality customer feedback using Voice of Customer solutions designed to improve your website performance and boost customer engagement. GoToMeeting a simple, extraordinarily powerful web conferencing service. Handrail end-to-end, collaborative user research and insights platform plan research, collect and analyze data, and share your findings. JotForm create online forms, get an email for each response, collect data. Lookback remotely run, record, and take notes for your user research sessions, either with a live product or with a prototype. MineTime a free calendar app with smart scheduling and time analytics features. Reflector Reflector is a basic screen-mirroring and recording tool so you can conduct user tests remotely, using any existing wireframes or prototypes. Reframer a research tool that helps your team to capture, tag (code) and identify patterns in qualitative data across multiple research participants. Sticktail a platform for centralizing, finding and sharing user insights within your organization. Survey Monkey online survey tool to capture the voices and opinions of the people who matter most to you. Typeform use a simple drag-and-drop interface to create any kind of form, survey, or questionnaire, and even take credit card payments. Wufoo reate forms, collect data and payments and automate your workflows. YesInsights simple one question and NPS surveys to improve your business. UserBit a platform of real-time research tools for your team. Tag/code interviews and feedback, capture insights, create personas, visual sitemaps and more. User Interviews recruit participants from a community of 125,000 members based on profession, demographics, geography and more, for any kind of research. Zoom it's one of the best online meeting services. Visual Debugging Tools Wireframing Tools A wireframe is a visual mockup that outlines the basic structure of the site or an app. It contains the most essential elements and the content, helping you easily explain ideas on design. Wireframes are a low-fidelity way of showing a design. This section is presented the best wireframing tools for a variety of use cases. Three D Modeling Software 3D graphics are used in gaming, film making, architecture, engineering, and 3D printing. 3D artists & designers use specific 3D modeling software, mentioned in this section. Autodesk integrated CAD, CAM, and CAE software. Unify design, engineering, and manufacturing into a single platform. Blender free and open-source 3D Creation Software. FreeCAD a free and open-source multiplatform 3D parametric modeler. MatterControl a free, open-source, all-in-one software package that lets you design, slice, organize and manage your 3D prints. Maya make animations, environments, motion graphics, virtual reality, and character creation, all in one toolset. Onshape a modeling software specially oriented to design technical and spare parts, was the first full-cloud 3D software created. OpenSCAD a software for creating solid 3D CAD objects. Rhino a curve-based 3D modeling software that creates mathematically-precise models of 3D surfaces. SketchUp 3D design software that truly makes 3D modeling for everyone, with a simple to learn yet robust toolset. Tinkercad a free, easy-to-use app for 3D design, electronics, and coding. Vectary create beautiful 3D models with our drag and drop 3D modeling tool. 3D Slash 3D modeling tool, available on all devices and all OS, online and offline. Addendum (Reference & Inspiration) Awesome Design Tools & Plugins is curated by Lisa Dziuba & Valia Havruliyk from Flawless team. And it was hugely inspired by articles from the design community and Prototypr.io Toolbox made by our good friend Graeme Fulton. If you found some great design tools, please suggest it. Thanks for making this project awesome :) ",
        "_version_": 1718536467653853184
      },
      {
        "story_id": 19914380,
        "story_author": "amatt189",
        "story_descendants": 3,
        "story_score": 6,
        "story_time": "2019-05-14T22:19:31Z",
        "story_title": "Detect credentials & secrets in code via machine learning",
        "search": [
          "Detect credentials & secrets in code via machine learning",
          "https://medium.com/@watchtowerai/introducing-radar-api-detect-credentials-secrets-in-code-via-machine-learning-fe402b818bf1",
          "In 2016, hackers gained access to Ubers private code repositories and used hard-coded credentials to exfiltrate 57 million driver records from an AWS S3 bucket. As a result of this breach, and its subsequent cover-up, Uber was fined $148 million. Could they have prevented such an incident? If this can happen to Uber, it can happen to any other company.Source code hosting providers such as GitHub and GitLab have socialized software development, making it easier to collaborate on projects & ship code quickly. But this hasnt been without consequences: its led to an increase in the accidental publication of sensitive information such as API keys, secrets, and credentials. This sensitive information can range from SSH keys to API keys, and even passwords. After analyzing millions of commits, our research team found that this problem is widespread and occurs daily as new commits are pushed up as part of the software development lifecycle. Abuse of these leaked credentials can cause salient security & compliance risks, such as catastrophic loss of sensitive customer data, harming an organization both financially and reputationally, while putting consumers at risk.When asked about the Uber leak, GitHub had the following comment:Our recommendation is to never store access tokens, passwords, or other authentication or encryption keys in the code. If the developer must include them in the code, we recommend they implement additional operational safeguards to prevent unauthorized access or misuse.Even AWS, the largest cloud provider by market share, has been financially impacted by leaked AWS keys and was motivated to create Git-secrets to help combat this problem. However, they acknowledge the difficulty of this problem, and even they cannot guarantee fool-proof detection of their own AWS keys. Many organizations have become hyper-aware of the potential risks of credential leakage, so related open source tools are gaining popularity on platforms like GitHub despite their low accuracy rates.Our team has been evaluating existing tools, and weve built a solution that leverages machine learning to dramatically improve the chances of accurately detecting credentials in your source code. Read on to learn more.We evaluated several popular tools used today to protect and secure GitHub repositories:truffleHog, 3k+ stars on GitHub Searches through git repositories for high entropy strings and secrets, digging deep into commit historyGitrob, 3k+ stars on GitHub Reconnaissance tool for GitHub organizationsGitleaks, 4k+ start on GitHub Audit git repos for secretsGit-secrets, 5k+ stars on GitHub Prevents you from committing secrets and credentials into git repositoriesIts worth noting that we focused on the most popular open source projects there are other projects like Yelps detect-secrets and Auth0s repo-supervisor that serve a similar purpose, though we didnt include them in this analysis to avoid redundancy. Well provide a brief introduction of each tool, an analysis of their pros & cons, followed by a more detailed comparison with examples. If youre already familiar with common tools and are curious about our approach, feel free to skip ahead to the section titled Our Approach: Radar below.The two main algorithms used in these tools are entropy and regular expressions:Entropy: Shannon entropy, or information entropy, is the average rate at which information is produced by a stochastic source of data. A high entropy score is the result of high variability of information, e.g. a string with only one repetitive character will have low entropy, since there is only one state, and can be converted to 1 bit of information. On the other hand, a string with a diverse set of characters that appears highly random, such as API keys, will require more bits to transmit, having much higher entropy. See here for more info.Regular Expressions (Regex): A regular expression, or regex, is a sequence of characters that defines a search pattern. Regexes are used to perform any lexical searching to match a variable name or API key pattern. See here for more info.truffleHogtruffleHog is an open-source tool written in Python that searches through git commit history for credentials/secrets that are accidentally committed.Pros:truffleHog scans the whole history of branches and commits, thus nothing will be missed if committed.truffleHog allows for both regex based and high entropy based flagging.Users can provide custom regexes to suit their needs accordingly.Cons:High entropy is a commonly used method to detect randomly generated API keys in many tools. However, because of the lack of contextual awareness, this method tends to be noisy. For example, it can be hard to distinguish between long variable names and credentials, e.g. TestScrapeLoopRunReportsTargetDownOnInvalidUTF8 which is a high-entropy string and would likely get flagged as a false positive.Regular expressions are a powerful but limited method that searches for generic patterns. Thus, they only work well on finding keys with explicitly defined & repeatable patterns, e.g. starting with some fixed characters, or very lengthy keys. Requiring these unique characteristics dramatically reduces the pool of potential tokens that can be flagged accurately.GitrobGitrob is an open-source tool written in Go that helps find potentially sensitive files. Different than the other tools listed, it has a broader range of object detection beyond API keys.Pros:Similar to truffleHog, it drills deep into the commit history of a repository, and the user can adjust how far back in the commit history to scan.The UI is very friendly for users to manipulate and analyze results.Also provides regex for searching for generic keys, e.g. /([a-f09\\-\\$\\/]{20,})/gmi.Cons:The search mechanisms are fairly simple mainly keyword searching, which yields many false positives. Users have to manually go through all detected files and tokens to check if they contain valid sensitive info or not, which can be very time-consuming.The regex for API keys does not have a cap on the length of the key, which can lead to a high number of false positives and is not comprehensive enough to capture keys with more diverse character sets.GitleaksGitleaks is an open-source tool written in Go that provides a way to find unwanted data types in code checked in to git. Imprecisely, its a Go version of truffleHog, although algorithm-wise, there are unique aspects.Pros:It combines a regex list similar to truffleHog and a high entropy method to do credential detection.It offers the user options to adjust their regex list and entropy range.Cons:Similar to truffleHog.Git-secretsGit-secrets is a tool written in bash to prevent users from committing AWS API keys.Pros:The tools main methods only occupy one file, so the code is easy to understand, use, and extend.Cons:The searching domain is limited to several regexes for AWS keys.Poses potential risks missing real AWS API keys if the users variable name does not match Git-secrets regexes, which is a common problem for all regex based searching methods. In other words, Git-secrets detection mandates that the AWS secret key must have key in the variable name, so a change of the variable name to aws_secret or secret_token would not trigger the regex, leading to a false negative.While all the tools mentioned have their respective differentiators, they all have common limitations in their search mechanisms that severely limit their accuracy. On the one hand, searching for keys using regexes might provide high signal for distinctive API key patterns such as Google ( AZia*) or Slack ( xoxp-*) keys, however, other patterns such as MongoDBs are indistinguishable from a universally unique identifier ( UUID). The regex results can also be quite noisy in that they match a lot of hashes/SHAs as well. On the other hand, Shannon entropy is a more comprehensive search method, but due to the sheer quantity of its output, it yields a high degree of false positives, making it untenable to use at scale.Since most approaches in this domain have mainly been a mixture of regexes and Shannon entropy, which each have their respective shortcomings, our team sought to develop a novel method leveraging deep learning to overcome the limitations of these methods.Here well introduce our deep learning based approach, called Radar, trained on features extracted from a broad set of API key patterns and their surrounding context in code. Utilizing contextual information is not a novel idea even regex based methods attempt this by looking backward for high-value words. However, our model approaches this problem in a more comprehensive way. Other solutions are based upon detection techniques that leverage heuristics around variable naming conventions, but this approach is rigid and brittle.For example, consider the generic API key regex in truffleHog:/[a|A][p|P][i|I][_]?[k|K][e|E][y|Y].*['|\\\"][0-9a-zA-Z]{32,45}['|\\\"]/Now, consider the following sample code:api_key = '12345678901234567890123456789012' api_token = '12345678901234567890123456789012'In this code example, when applying truffleHogs regex, the API key in the first line will be captured, while the second will not. This demonstrates that regex searching is limited to the lexical format of the code. Because variable naming conventions differ from developer to developer, this can easily lead to situations where designed regexes do not match the variable names that a regex is searching for. Abiding by naming conventions to be compliant with a regexs rules would not be too difficult, but as a prerequisite, developers would need to read the source code to understand and identify all situations that the tool would work well in.We dont believe tools should dictate and constrain how developers work instead the optimal tool should fit their existing workflow. The way that we deal with the problem of naming variation is that we dont require situations to be lexically similar they only need to be semantically similar. In other words, it shouldnt matter if the variable name is access_token or access_key since they have the same meaning.A deeper technical dive of our model will be done in a subsequent post. If youre eager to try out Radar on GitHub repos, feel free to jump ahead to Announcing the Radar API below.In this section, we evaluate the performance of our model against the tools mentioned above by scanning a sample code repository that mimics the potential occurrence of API keys in the real world. This was done to confirm our understanding of the algorithm in each tool, and to present visual examples of their advantages and limitations.We copied over ten real-world examples that we collected during the scanning process and also added nine ambiguous examples, which represent different misleading situations that can yield false positive detections.We highlighted the results found from each detector. True positives are highlighted in green. False positives are highlighted in red. We evaluated precision, recall and F1 score to quantify & compare model performance.For reference, precision is the fraction of correctly predicted positive instances among all predicted instances. Recall is the fraction of correctly predicted instances over total number of positive instances. F1 score is an overall measure of a models accuracy that combines precision and recall.truffleHogFirst, we scanned the repo with truffleHogs regex method.Precision: 3/3, Recall: 3/10.F1: 46%As mentioned, truffleHog has two regex patterns for capturing generic/application agnostic API keys, this pattern captures an alphanumeric token between 32 and 45 characters with a variable name containing apikey or secret.We next scanned the repo with truffleHogs Entropy method.Precision: 10/17, Recall: 10/10.F1: 74%truffleHogs high entropy method doesnt check variable names, it only calculates the Shannon entropy for each token and therefore, is quite noisy. When considering the low ratio of real API keys to false positives in real-world scenarios (<1:100), the number of results from this algorithm can be overwhelming to review.GitleaksCommand: gitleaks repo-path=./ verbosePrecision: 1/1, Recall: 1/10.F1: 18%Gitleaks is similar to truffleHog, in that it mainly uses regexes to detect credentials; however, their regexes are slightly non-traditional. For example, Gitleaks only captures api_key_github, which is an uncommon variable naming convention. The name github_api_key is the more common variable name people would assign to a GitHub API key. Nonetheless, almost all available tools support adding custom regexes into their regex list so this will not be a major issue if users adjust or input regexes to meet their needs.GitrobPrecision: 8/14, Recall: 8/10.F1: 66%Its hard to directly compare Gitrob to other tools because it effectively captures all strings longer than 20 characters, within the character set [a-fA-F09], and requires users to manually go through the results to check if they are valid. Gitrob provides a nice UI so that the user can easily go through each file.Watchtower RadarPrecision: 9/10, Recall: 9/10.F1: 90%Because Radar incorporates more comprehensive features as noted above, the false negative rate and false positive rate are both quite low, enhancing the user experience when verifying results.Its important to note that this test repository was created internally, and results will vary across repositories that are scanned. There are trade-offs to any of the approaches above, each with their own merits. As such, we have released Radar as an API that you can use to scan GitHub repositories for sensitive credentials. The service is free to use for the first 5 scans, so please feel free to try it here radar.watchtower.ai by logging in with GitHub. Note that the service scans both public and private repos, and does not store or track sensitive findings. Radar scans the full commit history for repos that have 1000 unique commits or fewer. For larger repos, Radar scans the current working directory (i.e. the latest version of the files in the default branch). To scan the full commit history of repos larger than 1000 commits, or to increase your scan limit, please contact us via email at support@watchtower.ai. You can also schedule a demo via our website at www.watchtower.ai.Start a ScanFor example, after logging in, you can start a new scan of public GitHub repo via the command line:curl https://radar.watchtower.ai/api/v1/scans/new \\-u API_KEY: \\-d 'public_github_url=https://github.com/watchtowerdlp/sample'This will run the scan asynchronously, and youll be notified when the results are ready to view:{ \"id\": \"0b6b08cf-f1ff-436b-a69f-7a1cb0d06e44\", \"url\": \"https://github.com/watchtowerdlp/sample\", \"duration\": \"0.822404097\", \"created_at\": \"2019-04-28 22:51:10 UTC\", \"scanned_files\": 24, \"status_code\": 200, \"results_count\": 1, \"results\": [ { \"result_id\": \"db2d2f85-8c06-41ef-85bf-2ea2dc786ca3\", \"repo_path\": \"sample\", \"file_path\": \"sample.rb\", \"branch\": \"origin/master\", \"commit_hash\": \"3a07b08d2461b6906376081d1f13b303215bf55d\", \"author_email\": \"49463194+watchtowerdlp@users.noreply.github.com\", \"context\": \"a4300836696c47b4f2d7c'\\n+\\n+auth_token = '\", \"token\": \"db\", \"token_length\": 32, \"permalink\": \"https://github.com/watchtowerdlp/sample/blob/3a07b08d2461b6906376081d1f13b303215bf55d/sample.rb#L7\", \"created_at\": \"2019-04-28 22:51:13 UTC\" } ]}Likewise, you can configure a webhook endpoint to be programmatically notified when the scan is complete and results are available for review. If you prefer, you can also view scan results in the dashboard, screenshots below. You can read the complete API docs here: radar.watchtower.ai.View scans in the dashboard:As well as their results:Its worth noting that during the course of our research we noticed that most keys that we found were stored deep in the commit history of a repo or were included in a now-deleted file. In git, deleting files with sensitive data, or deleting the tokens themselves, is actually only at the surface level, and the secrets can still be dug up in the commit history. As such, the only effective way to remove these keys is to delete the commit that introduced the key or to delete the entire commit history and start a brand new history. As such a detection tool like Radar could be applied as part of an engineers CI/CD workflow before any code gets pushed to a remote repo to prevent the need to edit the git history. This proactive approach would be instrumental in reducing the accidental exposure of secrets. Contact us if youre interested in leveraging Radar in this way.To increase your scan limit, or scan the full commit history of larger repos, please email us at support@watchtower.ai. Likewise, we would welcome your feedback on the API please dont hesitate to reach out via email with your thoughts.About WatchtowerWatchtower is a data security platform that uses machine learning to identify business-critical data, like customer PII, across SaaS and data infrastructure. Our team, based in San Francisco & Palo Alto and backed by leading Silicon Valley investors, has experience building cloud services & APIs and the software to protect the data flowing into & through those systems at some of the fastest growing platform companies in the world. Were hiring!Ive been in the security industry for a while and was looking for a strong, automated solution for data discovery, classification, and protection. I was very impressed with the accuracy of the classification on my unstructured data nothing on the market comes close to this.Shahar Ben-HadorCIO, ExabeamTo ensure I had the proper context to reduce risk at our company, I needed a solution that was cloud first and able to provide visibility into my SaaS providers & infrastructure without slowing down the business. There were plenty of proxy solutions out on the market but none that were able to provide the same visibility and frictionless experience for our team members, enabling us to combat data spray and classification issues. That was until we started leveraging Watchtower their API-driven solution accurately monitored and was able to take immediate action, which was a game changer for us.CISO Hyper-Growth Tech CompanyWatchtower saves us substantial time and is more effective than doing this manually We need this capability and Im grateful that Watchtower does such a good job. It is better than I expected and I typically have expectations that are high.CISOFortune 100 Company "
        ],
        "story_type": "Normal",
        "url_raw": "https://medium.com/@watchtowerai/introducing-radar-api-detect-credentials-secrets-in-code-via-machine-learning-fe402b818bf1",
        "comments.comment_id": [19914505, 19914686],
        "comments.comment_author": ["sjegler91", "yiang"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-05-14T22:39:27Z",
          "2019-05-14T23:02:29Z"
        ],
        "comments.comment_text": [
          "interesting use case of nlp and impressive results!",
          "How many types of credentials it can cover?"
        ],
        "id": "10341a71-cbdf-4ec7-ab55-b14774ee2583",
        "url_text": "In 2016, hackers gained access to Ubers private code repositories and used hard-coded credentials to exfiltrate 57 million driver records from an AWS S3 bucket. As a result of this breach, and its subsequent cover-up, Uber was fined $148 million. Could they have prevented such an incident? If this can happen to Uber, it can happen to any other company.Source code hosting providers such as GitHub and GitLab have socialized software development, making it easier to collaborate on projects & ship code quickly. But this hasnt been without consequences: its led to an increase in the accidental publication of sensitive information such as API keys, secrets, and credentials. This sensitive information can range from SSH keys to API keys, and even passwords. After analyzing millions of commits, our research team found that this problem is widespread and occurs daily as new commits are pushed up as part of the software development lifecycle. Abuse of these leaked credentials can cause salient security & compliance risks, such as catastrophic loss of sensitive customer data, harming an organization both financially and reputationally, while putting consumers at risk.When asked about the Uber leak, GitHub had the following comment:Our recommendation is to never store access tokens, passwords, or other authentication or encryption keys in the code. If the developer must include them in the code, we recommend they implement additional operational safeguards to prevent unauthorized access or misuse.Even AWS, the largest cloud provider by market share, has been financially impacted by leaked AWS keys and was motivated to create Git-secrets to help combat this problem. However, they acknowledge the difficulty of this problem, and even they cannot guarantee fool-proof detection of their own AWS keys. Many organizations have become hyper-aware of the potential risks of credential leakage, so related open source tools are gaining popularity on platforms like GitHub despite their low accuracy rates.Our team has been evaluating existing tools, and weve built a solution that leverages machine learning to dramatically improve the chances of accurately detecting credentials in your source code. Read on to learn more.We evaluated several popular tools used today to protect and secure GitHub repositories:truffleHog, 3k+ stars on GitHub Searches through git repositories for high entropy strings and secrets, digging deep into commit historyGitrob, 3k+ stars on GitHub Reconnaissance tool for GitHub organizationsGitleaks, 4k+ start on GitHub Audit git repos for secretsGit-secrets, 5k+ stars on GitHub Prevents you from committing secrets and credentials into git repositoriesIts worth noting that we focused on the most popular open source projects there are other projects like Yelps detect-secrets and Auth0s repo-supervisor that serve a similar purpose, though we didnt include them in this analysis to avoid redundancy. Well provide a brief introduction of each tool, an analysis of their pros & cons, followed by a more detailed comparison with examples. If youre already familiar with common tools and are curious about our approach, feel free to skip ahead to the section titled Our Approach: Radar below.The two main algorithms used in these tools are entropy and regular expressions:Entropy: Shannon entropy, or information entropy, is the average rate at which information is produced by a stochastic source of data. A high entropy score is the result of high variability of information, e.g. a string with only one repetitive character will have low entropy, since there is only one state, and can be converted to 1 bit of information. On the other hand, a string with a diverse set of characters that appears highly random, such as API keys, will require more bits to transmit, having much higher entropy. See here for more info.Regular Expressions (Regex): A regular expression, or regex, is a sequence of characters that defines a search pattern. Regexes are used to perform any lexical searching to match a variable name or API key pattern. See here for more info.truffleHogtruffleHog is an open-source tool written in Python that searches through git commit history for credentials/secrets that are accidentally committed.Pros:truffleHog scans the whole history of branches and commits, thus nothing will be missed if committed.truffleHog allows for both regex based and high entropy based flagging.Users can provide custom regexes to suit their needs accordingly.Cons:High entropy is a commonly used method to detect randomly generated API keys in many tools. However, because of the lack of contextual awareness, this method tends to be noisy. For example, it can be hard to distinguish between long variable names and credentials, e.g. TestScrapeLoopRunReportsTargetDownOnInvalidUTF8 which is a high-entropy string and would likely get flagged as a false positive.Regular expressions are a powerful but limited method that searches for generic patterns. Thus, they only work well on finding keys with explicitly defined & repeatable patterns, e.g. starting with some fixed characters, or very lengthy keys. Requiring these unique characteristics dramatically reduces the pool of potential tokens that can be flagged accurately.GitrobGitrob is an open-source tool written in Go that helps find potentially sensitive files. Different than the other tools listed, it has a broader range of object detection beyond API keys.Pros:Similar to truffleHog, it drills deep into the commit history of a repository, and the user can adjust how far back in the commit history to scan.The UI is very friendly for users to manipulate and analyze results.Also provides regex for searching for generic keys, e.g. /([a-f09\\-\\$\\/]{20,})/gmi.Cons:The search mechanisms are fairly simple mainly keyword searching, which yields many false positives. Users have to manually go through all detected files and tokens to check if they contain valid sensitive info or not, which can be very time-consuming.The regex for API keys does not have a cap on the length of the key, which can lead to a high number of false positives and is not comprehensive enough to capture keys with more diverse character sets.GitleaksGitleaks is an open-source tool written in Go that provides a way to find unwanted data types in code checked in to git. Imprecisely, its a Go version of truffleHog, although algorithm-wise, there are unique aspects.Pros:It combines a regex list similar to truffleHog and a high entropy method to do credential detection.It offers the user options to adjust their regex list and entropy range.Cons:Similar to truffleHog.Git-secretsGit-secrets is a tool written in bash to prevent users from committing AWS API keys.Pros:The tools main methods only occupy one file, so the code is easy to understand, use, and extend.Cons:The searching domain is limited to several regexes for AWS keys.Poses potential risks missing real AWS API keys if the users variable name does not match Git-secrets regexes, which is a common problem for all regex based searching methods. In other words, Git-secrets detection mandates that the AWS secret key must have key in the variable name, so a change of the variable name to aws_secret or secret_token would not trigger the regex, leading to a false negative.While all the tools mentioned have their respective differentiators, they all have common limitations in their search mechanisms that severely limit their accuracy. On the one hand, searching for keys using regexes might provide high signal for distinctive API key patterns such as Google ( AZia*) or Slack ( xoxp-*) keys, however, other patterns such as MongoDBs are indistinguishable from a universally unique identifier ( UUID). The regex results can also be quite noisy in that they match a lot of hashes/SHAs as well. On the other hand, Shannon entropy is a more comprehensive search method, but due to the sheer quantity of its output, it yields a high degree of false positives, making it untenable to use at scale.Since most approaches in this domain have mainly been a mixture of regexes and Shannon entropy, which each have their respective shortcomings, our team sought to develop a novel method leveraging deep learning to overcome the limitations of these methods.Here well introduce our deep learning based approach, called Radar, trained on features extracted from a broad set of API key patterns and their surrounding context in code. Utilizing contextual information is not a novel idea even regex based methods attempt this by looking backward for high-value words. However, our model approaches this problem in a more comprehensive way. Other solutions are based upon detection techniques that leverage heuristics around variable naming conventions, but this approach is rigid and brittle.For example, consider the generic API key regex in truffleHog:/[a|A][p|P][i|I][_]?[k|K][e|E][y|Y].*['|\\\"][0-9a-zA-Z]{32,45}['|\\\"]/Now, consider the following sample code:api_key = '12345678901234567890123456789012' api_token = '12345678901234567890123456789012'In this code example, when applying truffleHogs regex, the API key in the first line will be captured, while the second will not. This demonstrates that regex searching is limited to the lexical format of the code. Because variable naming conventions differ from developer to developer, this can easily lead to situations where designed regexes do not match the variable names that a regex is searching for. Abiding by naming conventions to be compliant with a regexs rules would not be too difficult, but as a prerequisite, developers would need to read the source code to understand and identify all situations that the tool would work well in.We dont believe tools should dictate and constrain how developers work instead the optimal tool should fit their existing workflow. The way that we deal with the problem of naming variation is that we dont require situations to be lexically similar they only need to be semantically similar. In other words, it shouldnt matter if the variable name is access_token or access_key since they have the same meaning.A deeper technical dive of our model will be done in a subsequent post. If youre eager to try out Radar on GitHub repos, feel free to jump ahead to Announcing the Radar API below.In this section, we evaluate the performance of our model against the tools mentioned above by scanning a sample code repository that mimics the potential occurrence of API keys in the real world. This was done to confirm our understanding of the algorithm in each tool, and to present visual examples of their advantages and limitations.We copied over ten real-world examples that we collected during the scanning process and also added nine ambiguous examples, which represent different misleading situations that can yield false positive detections.We highlighted the results found from each detector. True positives are highlighted in green. False positives are highlighted in red. We evaluated precision, recall and F1 score to quantify & compare model performance.For reference, precision is the fraction of correctly predicted positive instances among all predicted instances. Recall is the fraction of correctly predicted instances over total number of positive instances. F1 score is an overall measure of a models accuracy that combines precision and recall.truffleHogFirst, we scanned the repo with truffleHogs regex method.Precision: 3/3, Recall: 3/10.F1: 46%As mentioned, truffleHog has two regex patterns for capturing generic/application agnostic API keys, this pattern captures an alphanumeric token between 32 and 45 characters with a variable name containing apikey or secret.We next scanned the repo with truffleHogs Entropy method.Precision: 10/17, Recall: 10/10.F1: 74%truffleHogs high entropy method doesnt check variable names, it only calculates the Shannon entropy for each token and therefore, is quite noisy. When considering the low ratio of real API keys to false positives in real-world scenarios (<1:100), the number of results from this algorithm can be overwhelming to review.GitleaksCommand: gitleaks repo-path=./ verbosePrecision: 1/1, Recall: 1/10.F1: 18%Gitleaks is similar to truffleHog, in that it mainly uses regexes to detect credentials; however, their regexes are slightly non-traditional. For example, Gitleaks only captures api_key_github, which is an uncommon variable naming convention. The name github_api_key is the more common variable name people would assign to a GitHub API key. Nonetheless, almost all available tools support adding custom regexes into their regex list so this will not be a major issue if users adjust or input regexes to meet their needs.GitrobPrecision: 8/14, Recall: 8/10.F1: 66%Its hard to directly compare Gitrob to other tools because it effectively captures all strings longer than 20 characters, within the character set [a-fA-F09], and requires users to manually go through the results to check if they are valid. Gitrob provides a nice UI so that the user can easily go through each file.Watchtower RadarPrecision: 9/10, Recall: 9/10.F1: 90%Because Radar incorporates more comprehensive features as noted above, the false negative rate and false positive rate are both quite low, enhancing the user experience when verifying results.Its important to note that this test repository was created internally, and results will vary across repositories that are scanned. There are trade-offs to any of the approaches above, each with their own merits. As such, we have released Radar as an API that you can use to scan GitHub repositories for sensitive credentials. The service is free to use for the first 5 scans, so please feel free to try it here radar.watchtower.ai by logging in with GitHub. Note that the service scans both public and private repos, and does not store or track sensitive findings. Radar scans the full commit history for repos that have 1000 unique commits or fewer. For larger repos, Radar scans the current working directory (i.e. the latest version of the files in the default branch). To scan the full commit history of repos larger than 1000 commits, or to increase your scan limit, please contact us via email at support@watchtower.ai. You can also schedule a demo via our website at www.watchtower.ai.Start a ScanFor example, after logging in, you can start a new scan of public GitHub repo via the command line:curl https://radar.watchtower.ai/api/v1/scans/new \\-u API_KEY: \\-d 'public_github_url=https://github.com/watchtowerdlp/sample'This will run the scan asynchronously, and youll be notified when the results are ready to view:{ \"id\": \"0b6b08cf-f1ff-436b-a69f-7a1cb0d06e44\", \"url\": \"https://github.com/watchtowerdlp/sample\", \"duration\": \"0.822404097\", \"created_at\": \"2019-04-28 22:51:10 UTC\", \"scanned_files\": 24, \"status_code\": 200, \"results_count\": 1, \"results\": [ { \"result_id\": \"db2d2f85-8c06-41ef-85bf-2ea2dc786ca3\", \"repo_path\": \"sample\", \"file_path\": \"sample.rb\", \"branch\": \"origin/master\", \"commit_hash\": \"3a07b08d2461b6906376081d1f13b303215bf55d\", \"author_email\": \"49463194+watchtowerdlp@users.noreply.github.com\", \"context\": \"a4300836696c47b4f2d7c'\\n+\\n+auth_token = '\", \"token\": \"db\", \"token_length\": 32, \"permalink\": \"https://github.com/watchtowerdlp/sample/blob/3a07b08d2461b6906376081d1f13b303215bf55d/sample.rb#L7\", \"created_at\": \"2019-04-28 22:51:13 UTC\" } ]}Likewise, you can configure a webhook endpoint to be programmatically notified when the scan is complete and results are available for review. If you prefer, you can also view scan results in the dashboard, screenshots below. You can read the complete API docs here: radar.watchtower.ai.View scans in the dashboard:As well as their results:Its worth noting that during the course of our research we noticed that most keys that we found were stored deep in the commit history of a repo or were included in a now-deleted file. In git, deleting files with sensitive data, or deleting the tokens themselves, is actually only at the surface level, and the secrets can still be dug up in the commit history. As such, the only effective way to remove these keys is to delete the commit that introduced the key or to delete the entire commit history and start a brand new history. As such a detection tool like Radar could be applied as part of an engineers CI/CD workflow before any code gets pushed to a remote repo to prevent the need to edit the git history. This proactive approach would be instrumental in reducing the accidental exposure of secrets. Contact us if youre interested in leveraging Radar in this way.To increase your scan limit, or scan the full commit history of larger repos, please email us at support@watchtower.ai. Likewise, we would welcome your feedback on the API please dont hesitate to reach out via email with your thoughts.About WatchtowerWatchtower is a data security platform that uses machine learning to identify business-critical data, like customer PII, across SaaS and data infrastructure. Our team, based in San Francisco & Palo Alto and backed by leading Silicon Valley investors, has experience building cloud services & APIs and the software to protect the data flowing into & through those systems at some of the fastest growing platform companies in the world. Were hiring!Ive been in the security industry for a while and was looking for a strong, automated solution for data discovery, classification, and protection. I was very impressed with the accuracy of the classification on my unstructured data nothing on the market comes close to this.Shahar Ben-HadorCIO, ExabeamTo ensure I had the proper context to reduce risk at our company, I needed a solution that was cloud first and able to provide visibility into my SaaS providers & infrastructure without slowing down the business. There were plenty of proxy solutions out on the market but none that were able to provide the same visibility and frictionless experience for our team members, enabling us to combat data spray and classification issues. That was until we started leveraging Watchtower their API-driven solution accurately monitored and was able to take immediate action, which was a game changer for us.CISO Hyper-Growth Tech CompanyWatchtower saves us substantial time and is more effective than doing this manually We need this capability and Im grateful that Watchtower does such a good job. It is better than I expected and I typically have expectations that are high.CISOFortune 100 Company ",
        "_version_": 1718536482562506753
      },
      {
        "story_id": 19367379,
        "story_author": "kanishkdudeja",
        "story_descendants": 3,
        "story_score": 10,
        "story_time": "2019-03-12T12:15:08Z",
        "story_title": "Manifesto for Async Software Development",
        "search": [
          "Manifesto for Async Software Development",
          "http://asyncmanifesto.org/",
          "It's time for a 21st century successor to Agile And its most popular incarnation, Scrum After many years of developing software using Agile methodologies like Scrum, the time has come to value: Modern tools and flexible work environments over meetings and office hours Flexibility in prioritization over detailed planning Comprehensive documentation over tribal knowledge That is, while there is value in the latter items, there is more value in the former items. Principles of Async Software Development Modern tools Whether you're a fan of GitLab, GitHub, Bitbucket, or something else, good async collaboration tools like these all share a few important things in common: Each deeply integrates version control with issue tracking. Each offers rich prioritization and assignment features. Each wraps all that up into a slick user experience that anyone on the team can use, including nontechnical people. Meetings only as a last resort Meetings are very costly to your business. That's because creative professionals need long stretches of uninterrupted time to get meaningful work done. Thus, async communication should be your default, because it prevents context switching. Forget the planning meetings. Product owners can replace planning meetings by simply filing issues in the issue tracker, assigning priority, assigning them to people, and setting a release milestone. People will know what to work on by simply working on whatever the highest priority issue is in their queue. Skip the daily standups. Product owners can ascertain status by reading the comment threads of issues currently being worked on and posting questions as needed. Retire the backlog grooming sessions. Product owners should own the issue queue and frequently reassess priority on their own. They should loop in other people on an as-needed basis for advice. Call a meeting only when all other channels of communication aren't suitable for a specific issue. Flexible work environments Since modern tools and async communication makes 1950s-style meetings-centric office cultures obsolete, we can enjoy much more flexible work environments now. Adopt a hotelling policy at your office. Don't assign desks to anyone by default. Anyone who requests an assigned desk should get to choose whether it's a private office or in a communal space. Discourage one-size-fits-all space management. Some people work better in crowds, others work better at home. Let people decide for themselves. Document everything The better documented your workflow, the less your workers will need to interrupt each other to seek out tribal knowledge. A question answered in a FAQ or some other form of async communication is much better than one answered by a shoulder tap. "
        ],
        "story_type": "Normal",
        "url_raw": "http://asyncmanifesto.org/",
        "url_text": "It's time for a 21st century successor to Agile And its most popular incarnation, Scrum After many years of developing software using Agile methodologies like Scrum, the time has come to value: Modern tools and flexible work environments over meetings and office hours Flexibility in prioritization over detailed planning Comprehensive documentation over tribal knowledge That is, while there is value in the latter items, there is more value in the former items. Principles of Async Software Development Modern tools Whether you're a fan of GitLab, GitHub, Bitbucket, or something else, good async collaboration tools like these all share a few important things in common: Each deeply integrates version control with issue tracking. Each offers rich prioritization and assignment features. Each wraps all that up into a slick user experience that anyone on the team can use, including nontechnical people. Meetings only as a last resort Meetings are very costly to your business. That's because creative professionals need long stretches of uninterrupted time to get meaningful work done. Thus, async communication should be your default, because it prevents context switching. Forget the planning meetings. Product owners can replace planning meetings by simply filing issues in the issue tracker, assigning priority, assigning them to people, and setting a release milestone. People will know what to work on by simply working on whatever the highest priority issue is in their queue. Skip the daily standups. Product owners can ascertain status by reading the comment threads of issues currently being worked on and posting questions as needed. Retire the backlog grooming sessions. Product owners should own the issue queue and frequently reassess priority on their own. They should loop in other people on an as-needed basis for advice. Call a meeting only when all other channels of communication aren't suitable for a specific issue. Flexible work environments Since modern tools and async communication makes 1950s-style meetings-centric office cultures obsolete, we can enjoy much more flexible work environments now. Adopt a hotelling policy at your office. Don't assign desks to anyone by default. Anyone who requests an assigned desk should get to choose whether it's a private office or in a communal space. Discourage one-size-fits-all space management. Some people work better in crowds, others work better at home. Let people decide for themselves. Document everything The better documented your workflow, the less your workers will need to interrupt each other to seek out tribal knowledge. A question answered in a FAQ or some other form of async communication is much better than one answered by a shoulder tap. ",
        "comments.comment_id": [19371539, 19373363],
        "comments.comment_author": ["reallydude", "nwah1"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-03-12T18:41:55Z",
          "2019-03-12T21:43:42Z"
        ],
        "comments.comment_text": [
          "> A question answered in a FAQ or some other form of async communication is much better than one answered by a shoulder tap.<p>This doesn't scale. Eventually, you get to a point where you don't know where to look for context and context ends up getting spread around multiple areas. Complexity cannot be wrapped up in a wiki obviously enough, that you can fully grasp every possible issue without comprehending the whole wiki.",
          "This is entirely from an engineer's perspective. The reason agile and scrum are so popular is because it works for managers.<p>Managers need to herd all the cats, keep them on task, and give them continuous deadlines to report status in order to keep a low level fire burning under their employees' butts.<p>For open source software projects and hobby projects, this type of \"asynchronous\" development is perfectly fine.<p>For corporate environments, the problem is that employees are not necessarily intrinsically motivated and also need a way to communicate regularly up a chain of authority. This is the point of meetings.<p>Furthermore, managers generally succeed based on social intelligence. By regularly keeping tabs on employees using real meetings, they can assess the emotional state of the team... such as the morale or whether anyone is struggling or going through personal problems that they are reluctant to broadcast."
        ],
        "id": "bc0a22b4-4fe3-4415-a22c-0602b4d86ccf",
        "_version_": 1718536458946478080
      },
      {
        "story_id": 19652376,
        "story_author": "based2",
        "story_descendants": 61,
        "story_score": 145,
        "story_time": "2019-04-13T11:35:47Z",
        "story_title": "Infrastructure as Code, Part One",
        "search": [
          "Infrastructure as Code, Part One",
          "https://crate.io/a/infrastructure-as-code-part-one/",
          "Let's say you've developed a new feature and you want to release it. You've avoided all the typical pitfalls when it comes to making a new release and you've done everything as you were meant to. It's not a Friday, it's not 5 pm, and so on. But your organization is still doing manual releases. So that means that a systems administrator is logging on to each one of your production machines and deploying the latest version of the code. At your organization, the rules demand that you submit a rollout request in advance. And in this instance, you are granted a rollout window for Tuesday afternoon. Your application changes have already been successfully deployed in the staging environment. The tests pass, and everything else has gone smoothly. You're feeling confident. Tuesday afternoon rolls around, and it's time to make the release to the production environment. The deployment is successful on the first machine. And the second machine. But wait. Something goes wrong on the third machine. It turns out that the third production server has a different set of application dependencies installed. The versions are incompatible. You start debugging, but there's a fixed time window for the deployment, and time is running out... Eventually, time runs out. You've missed the window. The entire release is rolled back. And now you have to request another rollout. But approval from stakeholders is slow, so the next opportunity is a week away. Damn. You spend the rest of the day investigating what went wrong. Eventually, you figure it out. Somebody logged on to the third machine last week and manually updated some of the software. These changes were never propagated to the other servers, or back to the staging environment, or dev environments. Does any of this feel familiar? If so, you're not alone. Fortunately, Infrastructure as Code (IaC) can help you mitigate all of the problems described above. In part one of this IaC miniseries, I will introduce you to the basic concepts and explain some of the benefits. An Introduction As the name suggests, Infrastructure as Code uses code to provision, configure, and manage infrastructure. Using the right set of tools, it is straightforward to create a description of the infrastructure on which your application should be deployed. This description includes the specification of virtual machines, storage, software stacks, network configurations, security features, user accounts, access control limits, and so on. This description is done using code, often using a declarative language. The language you use varies depending on the tools you use, from common scripting languages to Domain Specific Languages (DSL) provided by the tools. IaC has evolved alongside the emergence of Infrastructure as a Service (IaaS) and other cloud-based services. The programmability of IaaS and the declarative nature of IaC work very well together. Instead of setting up that cloud environment by hand each time, you can just automate it with IaC. But that doesn't mean that IaC limits you to IaaS, whether public, private, or hybrid. With a little extra work, you can use infrastructure configuration tools to manage a traditional collection of physical servers. Alternatives Before I continue, it would be remiss of me not to mention the other options. There are three main ones that I can think of: Manually setting up your infrastructure using the visual console provided by your cloud provider.For example, using the Azure Portal by hand to set up your Microsoft Azure products, one by one. Clicking around the interface, creating a new VM, choosing the operating system from a drop-down menu, launching it, monitoring the status, and so on. Using the CLI tool provided by a cloud provider. Instead of using a cloud provider, you're managing your own physical machines or virtual machines. And you have written your own collection of configuration tools, management tools, deployment scripts, and so on. If you're using a cloud provider, the console is a great way to learn the ropes. But this quickly grows tiresome and error-prone if you try to manage your whole setup like this. And in addition, there's usually no built-in change visibility. You have to remember what actions you took, and document them for the rest of the team. And if you're managing your own servers by hand, developing your own system administration scripts can work okay. But it's easy for such a system to become a bit of a non-standard hodgepodge. And, well, things can quickly get out of hand... The Landscape Okay, let's take a high-level look at the IaC landscape. There are basically two categories of tools: Orchestration tools are used to provision, organize, and manage infrastructure components. Examples include Terraform, AWS CloudFormation, Azure Resource Manager. Configuration management tools are used to install, update, and manage the software running on the infrastructure components. Examples include SaltStack, Puppet, Chef, and Ansible. Additionally, when it comes to IaC, there are two primary approaches to managing infrastructure: Some tools (e.g., SaltStack) treat infrastructure components as mutable objects. That is, every time you make a change to the configuration, the necessary set of changes are made to the already-running components. Other tools (e.g., Terraform) treat infrastructure components as immutable objects. That is, when you make a change, new components with the new configuration are created to replace the old components. What Are the Benefits? Developer Mindset Developers ideally concern themselves with creating stable and sustainable software. But when infrastructure is \"something those other people take care of,\" it's easy for developers not to consider how the software they are building is going to be run. When you manage the infrastructure using code and involve the application developers in that process, it can prompt a change in mindset. How is this application going to be deployed? How is it going to be maintained once it's running? How are upgrades done? Have you thought about all the ways it might fail on a production machine? What preventative measures can you take? All of this and more becomes a natural part of the application development process itself when it is integrated with IaC. Version Control Because you are defining your infrastructure with code, it should also be versioned in a repository like the rest of your code. All of the benefits that change control offers application development are made available for infrastructure management: A single source of truth. The code itself, and the way it is laid out is a communication tool and can be used to understand the way things are built. The history of changes is recorded and accessible so you can understand what has changed and why over time. Coding standards can be developed and maintained. Pull requests and code reviews increase knowledge sharing and improve the overall quality of the code. Experimental changes can be made on branches and tested in isolation. Making changes is less daunting because if something goes wrong, you can revert back to a previously known working version. And so on, and so on... Knowledge Sharing It's a widespread phenomenon that one or two people in an organization will accumulate critical information that only they seem to possess. [At one of my previous companies, we kept a list of such individuals called the \"red bus\" list, so named because of the risk that one of them might be hit by one. Ed.] Less dramatically, such people take holidays and sometimes get sick, like anyone. So you should be able to cope with them not being around. With your infrastructure documented using code, it is hopefully relatively straightforward to understand. And what's more, this is a type of living documentation that should always be up-to-date. Better Use of Time In general, humans are bad with tedious, repetitive tasks. We lose interest, and that's when we're most likely to make mistakes. Fortunately, this is what machines are good at. When you manage your infrastructure with code, you offload all of the tedious, repetitive work to computers. And as a result, you reduce the chance of inconsistencies, mistakes, incomplete work, and other forms of human error. This also allows your people to spend their time and energies on other more important, high-value tasks Continuous Integration Continuous Integration (CI) is the practice of team members merging their code changes into a mainline branch multiple times per day. The benefit is that you are continually validating your code against your test suites, and you can adapt to changes being made to the codebase in a manageable, incremental fashion, as and when it happens, and not all in one go at the end of your project (what has been termed \"integration hell\"). IaC improves CI because changes to the infrastructure can be tested like changes to the application code. Additionally, a temporary testing environment can be provisioned for every change to application code. Continuous Delivery Continuous Delivery (CD) is the process of making regular automated releases every time code is pushed to the mainline branch. CD and IaC are a match made in heaven. With IaC, you can set up a deployment pipeline that automates the process of moving different versions of your application from one environment to the next as your testing and release schedules dictate. But more on that topic in another post. :) Wrap Up Infrastructure as Code (IaC) bridges the gap between system administrators and developers, and in doing so it: Helps developers think about the entire lifecycle of the software they are developing. Brings version control, code reviews, knowledge sharing, and other benefits to systems administration. Enables you to radically transform your CI and CD pipeline. Reduces the need for repetitive and error-prone tasks, freeing people up to work on more exciting things. The end result should be a more consistent, reliable, and well-understood infrastructure that is easier to develop and easier to maintain. In part two of this miniseries, I dive into the details and show you some of the ways we're using Terraform at Crate.io for provisioning and managing infrastructure. "
        ],
        "story_type": "Normal",
        "url_raw": "https://crate.io/a/infrastructure-as-code-part-one/",
        "comments.comment_id": [19653312, 19653843],
        "comments.comment_author": ["mverwijs", "royjacobs"],
        "comments.comment_descendants": [3, 14],
        "comments.comment_time": [
          "2019-04-13T14:51:53Z",
          "2019-04-13T16:12:39Z"
        ],
        "comments.comment_text": [
          "Nice piece. Looking forward to Part II.<p>What I am missing (often, in these type of articles as well as in actual production environments) is the fact that if you develop (infrastructure) code, you also need  to test your (infrastructure) code. Which means you need actual infrastructure to test on.<p>In my case, this means network equipment, storage equipment and actual physical servers.<p>If you're in a cloud, this means you need a seperate account on a seperate creditcard and start from there to build up the infra that Dev and Ops can start deploying their infra-as-code on.<p>And this test-infrastructure is <i>not</i> the test environments other teams run <i>their</i> tests on.<p>If that is not available, automating your infrastructure can be dangerous at best. Since you cannot properly test your code. And your code will rot.",
          "What I find unfortunate about infrastructure-as-code tooling is that a lot of the tooling isn't actually using code, but instead uses esoteric configuration languages. Indeed, the article refers to Terraform with its custom syntax.<p>Imho tools should use actual code (whether it's TypeScript or Kotlin or whatever) instead of reinventing constructs like loops and string interpolation.<p>Thankfully these tools are getting more popular, because frankly I can't stand configuring another Kubernetes or GCP resource using a huge block of copy/pasted YAML."
        ],
        "id": "e42416d1-1107-41e0-8f37-45b14d951d6e",
        "url_text": "Let's say you've developed a new feature and you want to release it. You've avoided all the typical pitfalls when it comes to making a new release and you've done everything as you were meant to. It's not a Friday, it's not 5 pm, and so on. But your organization is still doing manual releases. So that means that a systems administrator is logging on to each one of your production machines and deploying the latest version of the code. At your organization, the rules demand that you submit a rollout request in advance. And in this instance, you are granted a rollout window for Tuesday afternoon. Your application changes have already been successfully deployed in the staging environment. The tests pass, and everything else has gone smoothly. You're feeling confident. Tuesday afternoon rolls around, and it's time to make the release to the production environment. The deployment is successful on the first machine. And the second machine. But wait. Something goes wrong on the third machine. It turns out that the third production server has a different set of application dependencies installed. The versions are incompatible. You start debugging, but there's a fixed time window for the deployment, and time is running out... Eventually, time runs out. You've missed the window. The entire release is rolled back. And now you have to request another rollout. But approval from stakeholders is slow, so the next opportunity is a week away. Damn. You spend the rest of the day investigating what went wrong. Eventually, you figure it out. Somebody logged on to the third machine last week and manually updated some of the software. These changes were never propagated to the other servers, or back to the staging environment, or dev environments. Does any of this feel familiar? If so, you're not alone. Fortunately, Infrastructure as Code (IaC) can help you mitigate all of the problems described above. In part one of this IaC miniseries, I will introduce you to the basic concepts and explain some of the benefits. An Introduction As the name suggests, Infrastructure as Code uses code to provision, configure, and manage infrastructure. Using the right set of tools, it is straightforward to create a description of the infrastructure on which your application should be deployed. This description includes the specification of virtual machines, storage, software stacks, network configurations, security features, user accounts, access control limits, and so on. This description is done using code, often using a declarative language. The language you use varies depending on the tools you use, from common scripting languages to Domain Specific Languages (DSL) provided by the tools. IaC has evolved alongside the emergence of Infrastructure as a Service (IaaS) and other cloud-based services. The programmability of IaaS and the declarative nature of IaC work very well together. Instead of setting up that cloud environment by hand each time, you can just automate it with IaC. But that doesn't mean that IaC limits you to IaaS, whether public, private, or hybrid. With a little extra work, you can use infrastructure configuration tools to manage a traditional collection of physical servers. Alternatives Before I continue, it would be remiss of me not to mention the other options. There are three main ones that I can think of: Manually setting up your infrastructure using the visual console provided by your cloud provider.For example, using the Azure Portal by hand to set up your Microsoft Azure products, one by one. Clicking around the interface, creating a new VM, choosing the operating system from a drop-down menu, launching it, monitoring the status, and so on. Using the CLI tool provided by a cloud provider. Instead of using a cloud provider, you're managing your own physical machines or virtual machines. And you have written your own collection of configuration tools, management tools, deployment scripts, and so on. If you're using a cloud provider, the console is a great way to learn the ropes. But this quickly grows tiresome and error-prone if you try to manage your whole setup like this. And in addition, there's usually no built-in change visibility. You have to remember what actions you took, and document them for the rest of the team. And if you're managing your own servers by hand, developing your own system administration scripts can work okay. But it's easy for such a system to become a bit of a non-standard hodgepodge. And, well, things can quickly get out of hand... The Landscape Okay, let's take a high-level look at the IaC landscape. There are basically two categories of tools: Orchestration tools are used to provision, organize, and manage infrastructure components. Examples include Terraform, AWS CloudFormation, Azure Resource Manager. Configuration management tools are used to install, update, and manage the software running on the infrastructure components. Examples include SaltStack, Puppet, Chef, and Ansible. Additionally, when it comes to IaC, there are two primary approaches to managing infrastructure: Some tools (e.g., SaltStack) treat infrastructure components as mutable objects. That is, every time you make a change to the configuration, the necessary set of changes are made to the already-running components. Other tools (e.g., Terraform) treat infrastructure components as immutable objects. That is, when you make a change, new components with the new configuration are created to replace the old components. What Are the Benefits? Developer Mindset Developers ideally concern themselves with creating stable and sustainable software. But when infrastructure is \"something those other people take care of,\" it's easy for developers not to consider how the software they are building is going to be run. When you manage the infrastructure using code and involve the application developers in that process, it can prompt a change in mindset. How is this application going to be deployed? How is it going to be maintained once it's running? How are upgrades done? Have you thought about all the ways it might fail on a production machine? What preventative measures can you take? All of this and more becomes a natural part of the application development process itself when it is integrated with IaC. Version Control Because you are defining your infrastructure with code, it should also be versioned in a repository like the rest of your code. All of the benefits that change control offers application development are made available for infrastructure management: A single source of truth. The code itself, and the way it is laid out is a communication tool and can be used to understand the way things are built. The history of changes is recorded and accessible so you can understand what has changed and why over time. Coding standards can be developed and maintained. Pull requests and code reviews increase knowledge sharing and improve the overall quality of the code. Experimental changes can be made on branches and tested in isolation. Making changes is less daunting because if something goes wrong, you can revert back to a previously known working version. And so on, and so on... Knowledge Sharing It's a widespread phenomenon that one or two people in an organization will accumulate critical information that only they seem to possess. [At one of my previous companies, we kept a list of such individuals called the \"red bus\" list, so named because of the risk that one of them might be hit by one. Ed.] Less dramatically, such people take holidays and sometimes get sick, like anyone. So you should be able to cope with them not being around. With your infrastructure documented using code, it is hopefully relatively straightforward to understand. And what's more, this is a type of living documentation that should always be up-to-date. Better Use of Time In general, humans are bad with tedious, repetitive tasks. We lose interest, and that's when we're most likely to make mistakes. Fortunately, this is what machines are good at. When you manage your infrastructure with code, you offload all of the tedious, repetitive work to computers. And as a result, you reduce the chance of inconsistencies, mistakes, incomplete work, and other forms of human error. This also allows your people to spend their time and energies on other more important, high-value tasks Continuous Integration Continuous Integration (CI) is the practice of team members merging their code changes into a mainline branch multiple times per day. The benefit is that you are continually validating your code against your test suites, and you can adapt to changes being made to the codebase in a manageable, incremental fashion, as and when it happens, and not all in one go at the end of your project (what has been termed \"integration hell\"). IaC improves CI because changes to the infrastructure can be tested like changes to the application code. Additionally, a temporary testing environment can be provisioned for every change to application code. Continuous Delivery Continuous Delivery (CD) is the process of making regular automated releases every time code is pushed to the mainline branch. CD and IaC are a match made in heaven. With IaC, you can set up a deployment pipeline that automates the process of moving different versions of your application from one environment to the next as your testing and release schedules dictate. But more on that topic in another post. :) Wrap Up Infrastructure as Code (IaC) bridges the gap between system administrators and developers, and in doing so it: Helps developers think about the entire lifecycle of the software they are developing. Brings version control, code reviews, knowledge sharing, and other benefits to systems administration. Enables you to radically transform your CI and CD pipeline. Reduces the need for repetitive and error-prone tasks, freeing people up to work on more exciting things. The end result should be a more consistent, reliable, and well-understood infrastructure that is easier to develop and easier to maintain. In part two of this miniseries, I dive into the details and show you some of the ways we're using Terraform at Crate.io for provisioning and managing infrastructure. ",
        "_version_": 1718536471076405248
      },
      {
        "story_id": 21572278,
        "story_author": "timqian",
        "story_descendants": 9,
        "story_score": 16,
        "story_time": "2019-11-19T12:16:34Z",
        "story_title": "Show HN: I want to write resume in markdown, so I build this tool",
        "search": [
          "Show HN: I want to write resume in markdown, so I build this tool",
          "https://github.com/timqian/resumd",
          "Features Convert to html/pdf (example PDF) Web UI Customizable themes(GitHub theme, timqian.com theme, TUI theme) Easy to share and edit A pure frontend project, your data on your hand Aim To be the best markdown to resume tool Sponsors Your logo here Thanks toast-ui.react-editor Markdown to HTML: showdownjs Demo of showdownjs GitHub styled markdown css: github-markdown-css HTML to PDF: jsPDF Print HTML to PDF in browser Print.js Print page without header and footer Create react app Logo: Picas Logo Font: Ceviche One Change style sheets with JS Previous art Cv-maker: markdown based resume. GitHub; Hacker News disscussions; Online markdown resume editor: DeerResume Markdown-resuem-js: command line tool to turn markdown document into a resume in HTML and PDF. Markdown-resume: another command line tool. TODO i18n: https://codesandbox.io/s/1zxox032q Author timqian "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/timqian/resumd",
        "url_text": "Features Convert to html/pdf (example PDF) Web UI Customizable themes(GitHub theme, timqian.com theme, TUI theme) Easy to share and edit A pure frontend project, your data on your hand Aim To be the best markdown to resume tool Sponsors Your logo here Thanks toast-ui.react-editor Markdown to HTML: showdownjs Demo of showdownjs GitHub styled markdown css: github-markdown-css HTML to PDF: jsPDF Print HTML to PDF in browser Print.js Print page without header and footer Create react app Logo: Picas Logo Font: Ceviche One Change style sheets with JS Previous art Cv-maker: markdown based resume. GitHub; Hacker News disscussions; Online markdown resume editor: DeerResume Markdown-resuem-js: command line tool to turn markdown document into a resume in HTML and PDF. Markdown-resume: another command line tool. TODO i18n: https://codesandbox.io/s/1zxox032q Author timqian ",
        "comments.comment_id": [21572323, 21578956],
        "comments.comment_author": ["jjjbokma", "masukomi"],
        "comments.comment_descendants": [0, 1],
        "comments.comment_time": [
          "2019-11-19T12:27:21Z",
          "2019-11-19T22:23:33Z"
        ],
        "comments.comment_text": [
          "I did something similar using Pandoc and a dedicated template: <a href=\"https://github.com/john-bokma/resume-pandoc\" rel=\"nofollow\">https://github.com/john-bokma/resume-pandoc</a>",
          "I don't get it. It's a markdown editor, of which there are a bajillion. I don't see anything that's obviously resume specific about it besides the stuff you happened to type into it.<p>what am i missing?"
        ],
        "id": "423d6d07-5298-4cd2-8f29-8ae8bd3e2e66",
        "_version_": 1718536544025837568
      },
      {
        "story_id": 19738327,
        "story_author": "ariehkovler",
        "story_descendants": 88,
        "story_score": 75,
        "story_time": "2019-04-24T13:55:08Z",
        "story_title": "We need a new generation of source control",
        "search": [
          "We need a new generation of source control",
          "https://www.rookout.com/cant-git-no-satisfaction-why-we-need-a-new-gen-source-control/",
          "By: | January 5, 2019Liran is the Co-Founder and CTO of Rookout. Hes an Observability and Instrumentation expert with a deep understanding of Java, Python, Node, and C++. Liran has broad experience in cybersecurity and compliance from his past roles. When not coding, you can find Liran hosting his podcast, speaking at conferences, writing about his tech adventures, and trying out the local cuisine when traveling.Remember the good old days of enterprise software? When everything had to be installed on-premises? To install an application, youd have to set up a big, vertically scalable server. You would then have to execute a single process written in C/C++, Java or .NET. Well, as you know, those days are long gone.Everything has changed with the transition to the cloud and SaaS. Today, instead of comprising a single vertically scalable process, most applications comprise multiple horizontally scalable processes. This model was first pioneered by Googles borg and by Netflix on EC2. Nowadays, though, you no longer have to be a large enterprise to access microservice infrastructures. Kubernetes and serverless have made microservices viable and accessible to even small startups and lone coders.Lets Git down to businessSo where does Git fit into the picture? Git is an excellent match for single-process applications, but it starts to fail when it comes to multi-process applications. This is precisely what gave birth to the endless mono-repo vs. multi-repo flame-wars.Each side of this debate classifies the other as zealous extremists (as only developers can!), but both of them miss the crux of the matter: Git and its accompanying ecosystem are not yet fit for the task of developing modern cloud-native applications.Shots fired: multi-repos suckBefore we dive in, lets answer this: whats great about Git? Its the almighty atomic commit, the groundbreaking (at the time) branching capabilities, and the ever-useful blame. Well, these beloved features all but disappear in a multi-repo setup. Working in multiple repositories comes with significant drawbacks, which is why its not at all surprising that some of the biggest names in the tech world, including Google and Facebook, have gone down the mono-repo path at a huge investment of time and resources.Dependency management in a multi-repo setup is a nightmare. Instead of having everything in a single repository, you end up with repositories pointing to each other using two git features (git submodules and git subtree) and language-specific dependency management such as npm or Maven. The very existence of the many different methods to manage multi-repos is in itself proof that none of these tools are enough on their own. Gits source-of-truth is no longer a single folder on your computer but a mishmash of source providers and various artifactories.In developers everyday work, repository separation becomes an artificial barrier that impacts technological decisions. This creates a Conways Law effect, making early design decisions about component boundaries very hard to change. It also makes large scale refactorings a much trickier business.However, the biggest failure of the multi-repo is cultural. Instead of having all your source code readily available to all developers, they have to jump hurdles to figure out which repo they need and then clone it. These seemingly-small obstacles often become high fences: developers stop reading and updating code in components and repositories that arent directly in their responsibility.With all these engineering, operations and cultural barriers, why doesnt everyone go the mono-repo route?Take no prisoners: mono-repos suck tooOnce youve packed everything into a single repository, figuring out the connections within the repository becomes a challenge. For humans, that can chip away at the original architecture, breaking away useful abstractions and jumbling everything together.For machines, this lack of separation within the repo is even worse. When you push a code change to a repo, automated processes kick in. CI systems build and test the code, and then CD systems deploy it. Sometimes its to a test or staging environment, and sometimes directly to production.There are certain components you will need to build and deploy hundreds of times a day. At the same time, there are other more delicate and mission-critical components. These require human supervision and extra precaution. The problem with mono-repository is that it mixes all of these components into one. More surprising is the fact that todays vast Git CI ecosystem, with its impressive offerings in both the hosted and the SaaS space, doesnt even try to tackle the issue. In fact, not only will Git CI tools rebuild and redeploy your entire repo, they are often built explicitly for multi-repo projects.Another issue is large repository sizes. Git doesnt handle large repos gracefully. You can easily end up with repo sizes that dont fit in your hard-drive, or clone time that ends up in the hours. For big projects, this requires careful management and pruning of commit history. It is also essential to avoid committing dependencies, auto-generated files and other large files which may be necessary for specific scenarios.Is there still hope for multi-repos?There are new tools that seek to bring some of the benefits of mono-repos to multi-repos. These tools try to set up a configuration that would unite multiple repos under a single umbrella/abstraction layer, thus making managing multiple-repositories easier for example, TwoSigmas Git-meta, mateodelnortes meta, gitslave ,and a bunch of others.These tools bring back a bit of sanity into the complexities of managing multi-repos, reducing some of the toil and error-prone manual operations. But none of them truly give back the control and power of a single Git repo.You cant have your cake and Git it tooThe downsides of multi-repos are real. You cant deny the value of a (truly) single source of truth, (truly) atomic commits, and a (truly) single place to develop and collaborate. On the other hand, none of the downsides of mono-repos are inherent. All of them are related to the current implementation of the Git source control tool itself and its accompanying eco-system, especially CI/CD tools.Its time for a new generation of source control that wasnt purely designed for open-source projects, C and the Linux kernel. A source control designed for delivering modern applications in a polyglot cloud-native world. One that embraces code dependencies and helps the engineering team define and manage them, rather than scaring them away. A source control that treats CI, CD, and releases as first-class citizens, rather than relying on the very useful add-ons provided by GitHub and its community. "
        ],
        "story_type": "Normal",
        "url_raw": "https://www.rookout.com/cant-git-no-satisfaction-why-we-need-a-new-gen-source-control/",
        "comments.comment_id": [19739585, 19739587],
        "comments.comment_author": ["hliyan", "jrockway"],
        "comments.comment_descendants": [5, 0],
        "comments.comment_time": [
          "2019-04-24T15:48:25Z",
          "2019-04-24T15:48:36Z"
        ],
        "comments.comment_text": [
          "Are we mistaking a dependency control problem as a revision control problem?<p>In a previous life, before microservices, CI/CD etc. existed, we did just fine with 20-30 CVS repositories, each representing a separate component (a running process) in a very large distributed system.<p>The only difference was that we did not have to marshal a large number of 3rd party dependencies that were constantly undergoing version changes. We basically relied on C++, the standard template library and a tightly version controlled set of internal libraries with a single stable version shared across the entire org. The whole system would have been between 750,000 - 1,000,000 lines of code (libraries included).<p>I'm not saying that that's the right approach. But it's mind boggling for me that we can't solve this problem easily anymore.",
          "The source control system is not the piece of the equation that matters to most people.  The build system is the important part.  That's what prevents you from rebuilding the repository when you only change one Kubernetes config file, or what causes 100 docker images to be built because you changed a file in libc.<p>I think the tooling around this is fairly limited right now.  I feel that most people are hoping docker caches stuff intelligently, which it doesn't.  People should probably be using Bazel, but language support is hit-or-miss and it's very complicated.  (It's aggravated by the fact that every language now considers itself responsible for building its own code.  go \"just works\", which is great, but it's hard to translate that local caching to something that can be spread among multiple build workers.  Bazel attempts to make all that work, but it basically has to start from scratch, which is unfortunate.  It also means that you can't just start using some crazy new language unless you want to now support it in the build system.  We all hate Makefiles, but the whole \"foo.c becomes foo.o\" model was much more straightforward than what languages do today.)"
        ],
        "id": "eae1e8b5-6f95-459b-b223-5f1b74e0f824",
        "url_text": "By: | January 5, 2019Liran is the Co-Founder and CTO of Rookout. Hes an Observability and Instrumentation expert with a deep understanding of Java, Python, Node, and C++. Liran has broad experience in cybersecurity and compliance from his past roles. When not coding, you can find Liran hosting his podcast, speaking at conferences, writing about his tech adventures, and trying out the local cuisine when traveling.Remember the good old days of enterprise software? When everything had to be installed on-premises? To install an application, youd have to set up a big, vertically scalable server. You would then have to execute a single process written in C/C++, Java or .NET. Well, as you know, those days are long gone.Everything has changed with the transition to the cloud and SaaS. Today, instead of comprising a single vertically scalable process, most applications comprise multiple horizontally scalable processes. This model was first pioneered by Googles borg and by Netflix on EC2. Nowadays, though, you no longer have to be a large enterprise to access microservice infrastructures. Kubernetes and serverless have made microservices viable and accessible to even small startups and lone coders.Lets Git down to businessSo where does Git fit into the picture? Git is an excellent match for single-process applications, but it starts to fail when it comes to multi-process applications. This is precisely what gave birth to the endless mono-repo vs. multi-repo flame-wars.Each side of this debate classifies the other as zealous extremists (as only developers can!), but both of them miss the crux of the matter: Git and its accompanying ecosystem are not yet fit for the task of developing modern cloud-native applications.Shots fired: multi-repos suckBefore we dive in, lets answer this: whats great about Git? Its the almighty atomic commit, the groundbreaking (at the time) branching capabilities, and the ever-useful blame. Well, these beloved features all but disappear in a multi-repo setup. Working in multiple repositories comes with significant drawbacks, which is why its not at all surprising that some of the biggest names in the tech world, including Google and Facebook, have gone down the mono-repo path at a huge investment of time and resources.Dependency management in a multi-repo setup is a nightmare. Instead of having everything in a single repository, you end up with repositories pointing to each other using two git features (git submodules and git subtree) and language-specific dependency management such as npm or Maven. The very existence of the many different methods to manage multi-repos is in itself proof that none of these tools are enough on their own. Gits source-of-truth is no longer a single folder on your computer but a mishmash of source providers and various artifactories.In developers everyday work, repository separation becomes an artificial barrier that impacts technological decisions. This creates a Conways Law effect, making early design decisions about component boundaries very hard to change. It also makes large scale refactorings a much trickier business.However, the biggest failure of the multi-repo is cultural. Instead of having all your source code readily available to all developers, they have to jump hurdles to figure out which repo they need and then clone it. These seemingly-small obstacles often become high fences: developers stop reading and updating code in components and repositories that arent directly in their responsibility.With all these engineering, operations and cultural barriers, why doesnt everyone go the mono-repo route?Take no prisoners: mono-repos suck tooOnce youve packed everything into a single repository, figuring out the connections within the repository becomes a challenge. For humans, that can chip away at the original architecture, breaking away useful abstractions and jumbling everything together.For machines, this lack of separation within the repo is even worse. When you push a code change to a repo, automated processes kick in. CI systems build and test the code, and then CD systems deploy it. Sometimes its to a test or staging environment, and sometimes directly to production.There are certain components you will need to build and deploy hundreds of times a day. At the same time, there are other more delicate and mission-critical components. These require human supervision and extra precaution. The problem with mono-repository is that it mixes all of these components into one. More surprising is the fact that todays vast Git CI ecosystem, with its impressive offerings in both the hosted and the SaaS space, doesnt even try to tackle the issue. In fact, not only will Git CI tools rebuild and redeploy your entire repo, they are often built explicitly for multi-repo projects.Another issue is large repository sizes. Git doesnt handle large repos gracefully. You can easily end up with repo sizes that dont fit in your hard-drive, or clone time that ends up in the hours. For big projects, this requires careful management and pruning of commit history. It is also essential to avoid committing dependencies, auto-generated files and other large files which may be necessary for specific scenarios.Is there still hope for multi-repos?There are new tools that seek to bring some of the benefits of mono-repos to multi-repos. These tools try to set up a configuration that would unite multiple repos under a single umbrella/abstraction layer, thus making managing multiple-repositories easier for example, TwoSigmas Git-meta, mateodelnortes meta, gitslave ,and a bunch of others.These tools bring back a bit of sanity into the complexities of managing multi-repos, reducing some of the toil and error-prone manual operations. But none of them truly give back the control and power of a single Git repo.You cant have your cake and Git it tooThe downsides of multi-repos are real. You cant deny the value of a (truly) single source of truth, (truly) atomic commits, and a (truly) single place to develop and collaborate. On the other hand, none of the downsides of mono-repos are inherent. All of them are related to the current implementation of the Git source control tool itself and its accompanying eco-system, especially CI/CD tools.Its time for a new generation of source control that wasnt purely designed for open-source projects, C and the Linux kernel. A source control designed for delivering modern applications in a polyglot cloud-native world. One that embraces code dependencies and helps the engineering team define and manage them, rather than scaring them away. A source control that treats CI, CD, and releases as first-class citizens, rather than relying on the very useful add-ons provided by GitHub and its community. ",
        "_version_": 1718536475113422848
      },
      {
        "story_id": 21714735,
        "story_author": "gilfillan9",
        "story_descendants": 3,
        "story_score": 38,
        "story_time": "2019-12-05T18:22:22Z",
        "story_title": "Welcome to Space",
        "search": [
          "Welcome to Space",
          "https://blog.jetbrains.com/blog/2019/12/05/welcome-to-space/",
          "New Products NewsWelcome to Space! Today at KotlinConf, we announced our brand new product Space, and we have already opened the Early Access Program. What is Space Space is an integrated team environment that provides teams and organizations with the tools they need to collaborate effectively and efficiently. It has Git-based Version Control, Code Review, Automation (CI/CD) based on Kotlin Scripting, Package Repositories, Planning tools, Issue Tracker, Chats, Blogs, Meetings, and Team Directory, among other features. Space was born out of our own needs at JetBrains. As a company, weve grown from a team of 3 developers to over 1200 people, 60% of whom are technical. With this growth, weve found our current use of independent tools has often created silos, leading to miscommunication, less efficient collaboration, and loss of information. Space is about people and teams In Space, the concept of a team is a first-class citizen. When you join a team, you are automatically included in everything related to it, be it meetings, blogs, source control, calendars, vacations, etc. This eliminates the need for creating concepts such as groups and then making sure that every team member is also part of the corresponding group. Tight integration between the areas of the system provides for numerous advantages such as knowing a persons availability, which is useful if you want to have a chat or assign a code review to them. Space is a platform Space allows you to build on the platform in multiple ways. Whether you do so by using webhooks, HTTP API, or even plugins (on the self-hosted version), you can extend the existing functionality of Space and make use of the information available to you without needing to hook up many different solutions that create silos of data. Space as a service or self-hosted We will be offering Space either as a service, hosted and managed entirely by JetBrains, or as a self-hosted version (available in the near future). The pricing plan has four levels, starting at the free tier, which is ideal for small teams, and progressing up to the Enterprise tier, which meets the needs of large organizations. Space is available everywhere Built from the ground up with Kotlin multiplatform technology, Space clients are available for web, desktop, and mobile platforms, and offer full IDE integration with the IntelliJ Platform. Space Launch and roadmap Today we already provide a ton of functionality, such as: Version Control Code Reviews Blogs Chats Team Directory Package Registry Planning Issue Tracker IDE Integration We have a lot more planned for Space, including: Knowledge Base Automation CI/CD Pipelines Personal To-Do lists and notification management The Early Access Program will be accepting requests on a first-come-first-served basis, and well be gradually increasing the number of teams that can gain access. So dont wait! Sign up now to get your team Space! "
        ],
        "story_type": "Normal",
        "url_raw": "https://blog.jetbrains.com/blog/2019/12/05/welcome-to-space/",
        "url_text": "New Products NewsWelcome to Space! Today at KotlinConf, we announced our brand new product Space, and we have already opened the Early Access Program. What is Space Space is an integrated team environment that provides teams and organizations with the tools they need to collaborate effectively and efficiently. It has Git-based Version Control, Code Review, Automation (CI/CD) based on Kotlin Scripting, Package Repositories, Planning tools, Issue Tracker, Chats, Blogs, Meetings, and Team Directory, among other features. Space was born out of our own needs at JetBrains. As a company, weve grown from a team of 3 developers to over 1200 people, 60% of whom are technical. With this growth, weve found our current use of independent tools has often created silos, leading to miscommunication, less efficient collaboration, and loss of information. Space is about people and teams In Space, the concept of a team is a first-class citizen. When you join a team, you are automatically included in everything related to it, be it meetings, blogs, source control, calendars, vacations, etc. This eliminates the need for creating concepts such as groups and then making sure that every team member is also part of the corresponding group. Tight integration between the areas of the system provides for numerous advantages such as knowing a persons availability, which is useful if you want to have a chat or assign a code review to them. Space is a platform Space allows you to build on the platform in multiple ways. Whether you do so by using webhooks, HTTP API, or even plugins (on the self-hosted version), you can extend the existing functionality of Space and make use of the information available to you without needing to hook up many different solutions that create silos of data. Space as a service or self-hosted We will be offering Space either as a service, hosted and managed entirely by JetBrains, or as a self-hosted version (available in the near future). The pricing plan has four levels, starting at the free tier, which is ideal for small teams, and progressing up to the Enterprise tier, which meets the needs of large organizations. Space is available everywhere Built from the ground up with Kotlin multiplatform technology, Space clients are available for web, desktop, and mobile platforms, and offer full IDE integration with the IntelliJ Platform. Space Launch and roadmap Today we already provide a ton of functionality, such as: Version Control Code Reviews Blogs Chats Team Directory Package Registry Planning Issue Tracker IDE Integration We have a lot more planned for Space, including: Knowledge Base Automation CI/CD Pipelines Personal To-Do lists and notification management The Early Access Program will be accepting requests on a first-come-first-served basis, and well be gradually increasing the number of teams that can gain access. So dont wait! Sign up now to get your team Space! ",
        "comments.comment_id": [21714981, 21716371],
        "comments.comment_author": ["stephenr", "toyg"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-12-05T18:44:21Z",
          "2019-12-05T20:54:23Z"
        ],
        "comments.comment_text": [
          "This looks interesting, I’m curious how the “free” tier interacts with the “self hosted” part - storage limits and compute limits don’t make a lot of sense if you’re providing those resources yourself.",
          "Interesting, but it smells a lot of enterprise bloat. All-in-one solutions, in the long run, tend to feel inferior to dynamic setups.<p>It’s true that that particular market is long-suffering. If this thing can keep admin overhead to a minimum and actually make teams communicate more, it’s welcome."
        ],
        "id": "dcea7891-bd45-4156-9d80-f745eadc8831",
        "_version_": 1718536549406081024
      },
      {
        "story_id": 21071181,
        "story_author": "JA7Cal",
        "story_descendants": 15,
        "story_score": 43,
        "story_time": "2019-09-25T14:36:38Z",
        "story_title": "Automated Reports with Jupyter Notebooks (Using Jupytext and Papermill)",
        "search": [
          "Automated Reports with Jupyter Notebooks (Using Jupytext and Papermill)",
          "https://medium.com/capital-fund-management/automated-reports-with-jupyter-notebooks-using-jupytext-and-papermill-619e60c37330",
          "Jupyter notebooks are one of the best available tools for running code interactively and writing a narrative with data and plots. What is less known is that they can be conveniently versioned and run automatically.Do you have a Jupyter notebook with plots and figures that you regularly run manually? Wouldnt it be nice to use the same notebook and instead have an automated reporting system, launched from a script? What if this script could even pass some parameters to the notebook it runs?This post explains in a few steps how this can be done concretely, including within a production environment.Example notebookWe will show you how to version control, automatically run and publish a notebook that depends on a parameter. As an example, we will use a notebook that describes the world population and the gross domestic product for a given year. It is simple to use: just change the year variable in the first cell, re-run, and you get the plots for the chosen year. But this requires a manual intervention. It would be much more convenient if the update could be automated and produce a report for each possible value of the year parameter (more generally, a notebook can update its results based not only on some user-provided parameters, but also through a connection to a database, etc.).Version controlIn a professional environment, notebooks are designed by, say, a data scientist, but the task of running them in production may be handled by a different team. So in general people have to share notebooks. This is best done through a version control system.Jupyter notebooks are famous for the difficulty of their version control. Lets consider our notebook above, with a file size of 3 MB, much of it being contributed by the embedded Plotly library. The notebook is less than 80 KB if we remove the output of the second code cell. And as small as 1.75 KB when all outputs are removed. This shows how much of its contents is unrelated to pure code! If we dont pay attention, code changes in the notebook will be lost in an ocean of binary contents.To get meaningful diffs, we use Jupytext (disclaimer: Im the author of Jupytext). Jupytext can be installed with pip or conda. Once the notebook server is restarted, a Jupytext menu appears in Jupyter:We click on Pair Notebook with Markdown, save the notebook and we obtain two representations of the notebook: world_fact.ipynb (with both input and output cells) and world_fact.md (with only the input cells).Jupytexts representation of notebooks as Markdown files is compatible with all major Markdown editors and viewers, including GitHub and VS Code. The Markdown version is for example rendered by GitHub as:As you can see, the Markdown file does not include any output. Indeed, we dont want it at this stage since we only need to share the notebook code. The Markdown file also has a very clear diff history, which makes versioning notebooks simple.The world_facts.md file is automatically updated by Jupyter when you save the notebook. And the other way round also works! If you modify world_facts.md with either a text editor, or by pulling the latest contributions from the version control system, then the changes appear in Jupyter when you refresh the notebook in the browser.In our version control system, we only need to track the Markdown file (and we even explicitly ignore all .ipynb files). Obviously, the team that will execute the notebook needs to regenerate the world_fact.ipynb document. For this they use Jupytext in the command line:$ jupytext world_facts.md --to ipynb[jupytext] Reading world_facts.md[jupytext] Writing world_facts.ipynbWe are now properly versioning the notebook. The diff history is much clearer. See for instance how the addition of the gross domestic products to our report looks like:Jupyter notebooks as Scripts?As an alternative to the Markdown representation, we could have paired the notebook to a world_facts.py script using Jupytext. You should give it a try if your notebook contains more code than text. That's often a first good step towards a complete and efficient refactoring of long notebooks: once the notebook is represented as a script, you can extract any complex code and move it to a (unit-tested) library using the refactoring tools in your IDE.JupyterLab, JupyterHub, Binder, Nteract, Colab & Cloud notebooks?Do you use JupyterLab and not Jupyter Notebook? No worries: the method above also applies in this case. You will just have to use the Jupytext extension for JupyterLab instead of the Jupytext menu. And in case you were wondering, Jupytext also work in JupyterHub and Binder.If you use other notebook editors like Nteract desktop, CoCalc, Google Colab, or another cloud notebook editor, you may not be able to use Jupytext as a plugin in the editor. In this case you can simply use Jupytext in the command line. Close your notebook and inject the pairing information into world_facts.ipynb with$ jupytext --set-formats ipynb,md world_facts.ipynband then keep the two representations synchronised with$ jupytext --sync world_facts.ipynbNotebook parametersPapermill is the reference library for executing notebooks with parameters.Papermill needs to know which cell contains the notebook parameters. This is simply done by adding a parameter tag in that cell with the cell toolbar in Jupyter Notebook:In JupyterLab you can use the celltags extension.And if you prefer you can also directly edit world_facts.md and add the tag there:```python tags=[\"parameters\"]year = 2000```Automated executionWe now have all the information required to execute the notebook on a production server.Production environmentIn order to execute the notebook, we need to know in which environment it should run. As we are working with a Python notebook in this example, we list its dependencies in a requirements.txt file, as is standard for Python projects.For simplicity, we also include the notebook tools in the same environment, i.e. add jupytext and papermill to the same requirements.txt file. Strictly speaking, these tools could be installed and executed in another Python environment.The corresponding Python environment is created with either$ conda create -n run_notebook --file requirements.txt -yor$ pip install -r requirements.txt(if in a virtual environment).Please note that the requirements.txt file is just one way of specifying an execution environment. The Reproducible Execution Environment Specification by the Binder team is one of the most complete references on the subject.Continuous IntegrationIt is a good practice to test each new contribution to either the notebook or its requirements. For this you can use for example Travis CI, a continuous integration solution. You will need only these two commands:pip install -r requirements.txt to install the dependenciesjupytext world_facts.md --set-kernel - --execute to test the execution of the notebook in the current Python environment.You can find a concrete example in our .travis.yml file.We are already executing the notebook automatically, arent we? Travis will tell us if a regression is introduced in the project What progress! But were not 100% done yet, as we promised to execute the notebook with parameters.Using the right kernelJupyter notebooks are associated with a kernel (i.e. a pointer to a local Python environment), but that kernel might not be available on your production machine. In this case, we simply update the notebook kernel so as to point to the environment that we have just created:$ jupytext world_facts.ipynb --set-kernel -Note that the minus sign in --set-kernel - above represents the current Python environment. In our example this yields:[jupytext] Reading world_facts.ipynb[jupytext] Updating notebook metadata with '{\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}}' [jupytext] Writing world_facts.ipynb (destination file replaced)In case you want to use another kernel just pass the kernel name to the --set-kernel option (you can get the list of all available kernels with jupyter kernelspec list and/or declare a new kernel with python -m ipykernel install --name kernel_name --user).Executing the notebook with parametersWe are now ready to use Papermill for executing the notebook.$ papermill world_facts.ipynb world_facts_2017.ipynb -p year 2017Input Notebook: world_facts.ipynb Output Notebook: world_facts_2017.ipynb 100%|| 8/8 [00:04<00:00, 1.41it/s]Were done! The notebook has been executed and the file world_facts_2017.ipynb contains the outputs.Publishing the NotebookIts time to deliver the notebook that was just executed. Maybe you want it in your mailbox? Or maybe you prefer to get a URL where you can see the result? We cover a few ways of doing that.GitHub can display Jupyter notebooks. This is a convenient solution, as you can easily choose who can access repositories. This works well as long as you dont include any interactive JavaScript plots or widgets in the notebook (the JavaScript parts are ignored by GitHub). In the case of our notebook, the interactive plots do not appear on GitHub, so we need another approach.Another option is to use the Jupyter Notebook Viewer. The nbviewer service can render any notebook which is publicly available on GitHub. Our notebook is thus rendered correctly there. If your notebook is not public, you can choose to install nbviewer locally.Alternatively, you can convert the executed notebook to HTML, and publish it on GitHub pages, or on your own HTML server, or send it over email. Converting the notebook to HTML is easily done with$ jupyter nbconvert world_facts_2017.ipynb --to html[NbConvertApp] Converting notebook world_facts_2017.ipynb to html [NbConvertApp] Writing 3361863 bytes to world_facts_2017.htmlThe resulting HTML file includes the code cells as below:But maybe you dont want to see the input cells in the HTML? You just need to add --no-input:$ jupyter nbconvert --to html --no-input world_facts_2017.ipynb --output world_facts_2017_report.htmlAnd youll get a cleaner report:Sending the standalone HTML file as an attachment in an email is an easy exercise. Embedding the report in the body of the email is also possible (but interactive plots wont work).Finally, if you are looking for a polished report and have some knowledge of LaTeX, you can give the PDF export option of Jupyters nbconvert command a try.Using pipesAn alternative to using named files would be to use pipes. jupytext, nbconvert and papermill all support them. A one-liner substitute for the previous commands is:$ cat world_facts.md \\ | jupytext --from md --to ipynb --set-kernel - \\ | papermill -p year 2017 \\ | jupyter nbconvert --stdin --output world_facts_2017_report.htmlConclusionYou should now be able to set up a full pipeline for generating reports in production, based on Jupyter notebooks. We have seen how to:version control a notebook with Jupytextshare a notebook and its dependencies between various userstest a notebook with continuous integrationexecute a notebook with parameters using Papermilland finally, how to publish the notebook (on GitHub or nbviewer), or render it as a static HTML page.The technology used in this example is fully based on the Jupyter Project, which is the de facto standard for Data Science. The tools used here are all open source and work well with any continuous integration framework.You have everything you need to schedule and deliver fine-tuned, code-free reports!EpilogueThe tools used here are written in Python. But they are language agnostic. Thanks to the Jupyter framework, they actually apply to any of the 40+ programming language for which a Jupyter kernel exists.Now, imagine that you have authored a document containing a few Bash command lines, just like this blog post. Install Jupytext and the bash kernel, and the blog post becomes this interactive Jupyter notebook!Going further, shouldnt we make sure that every instruction in our post actually works? We do that via our continuous integration spoiler alert: thats as simple as jupytext --execute README.md!AcknowledgmentsMarc would like to thank Eric Lebigot and Florent Zara for their contributions to this article, and to CFM for supporting this work through their Open-Source Program.About the authorThis article was written by Marc Wouts. Marc joined the research team of CFM in 2012 and has worked on a range of research projects, from optimal trading to portfolio construction.Marc has always been interested in finding efficient workflows for doing collaborative research involving data and code. In 2015 he authored an internal tool for publishing Jupyter and R Markdown notebooks on Atlassians Confluence wiki, providing a first solution for collaborating on notebooks. In 2018, he authored Jupytext, an open-source program that facilitates the version control of Jupyter notebooks. Marc is also interested in data visualisation, and coordinates a working group on this subject at CFM.Marc obtained a PhD in Probability Theory from the Paris Diderot University in 2007.DisclaimerAll views included in this document constitute judgments of its author(s) and do not necessarily reflect the views of Capital Fund Management or any of its affiliates. The information provided in this document is general information only, does not constitute investment or other advice, and is subject to change without notice. "
        ],
        "story_type": "Normal",
        "url_raw": "https://medium.com/capital-fund-management/automated-reports-with-jupyter-notebooks-using-jupytext-and-papermill-619e60c37330",
        "comments.comment_id": [21071306, 21072228],
        "comments.comment_author": ["KyleOS", "brummm"],
        "comments.comment_descendants": [0, 1],
        "comments.comment_time": [
          "2019-09-25T14:49:46Z",
          "2019-09-25T16:21:21Z"
        ],
        "comments.comment_text": [
          "Awesome article - I'm wondering, for \"Publishing the Notebook\" part of the workflow, have you ever seen Kyso (<a href=\"https://kyso.io\" rel=\"nofollow\">https://kyso.io</a>) - disclaimer, I'm a founder. We started Kyso to make it easier to communicate insights gained from analysis to non-technical people by converting data science tools (e.g. Jupyter Notebooks) into conversational tools in the form of blog posts. You can make public posts or have an internal \"data blog\" for your team, where you push your work to Github and it is reflected on Kyso. Would love to hear your thoughts on how it could fit into existing workflows.",
          "I don't think Jupyter notebooks should be used for automated jobs. They're great for exploratory stuff but once things are getting fleshed out and cleaned up, one should move to proper python files that can be unit tested and versioned without having to go to crazy lengths..."
        ],
        "id": "a61d7d77-3f65-46d3-aff5-6285ed790c6d",
        "url_text": "Jupyter notebooks are one of the best available tools for running code interactively and writing a narrative with data and plots. What is less known is that they can be conveniently versioned and run automatically.Do you have a Jupyter notebook with plots and figures that you regularly run manually? Wouldnt it be nice to use the same notebook and instead have an automated reporting system, launched from a script? What if this script could even pass some parameters to the notebook it runs?This post explains in a few steps how this can be done concretely, including within a production environment.Example notebookWe will show you how to version control, automatically run and publish a notebook that depends on a parameter. As an example, we will use a notebook that describes the world population and the gross domestic product for a given year. It is simple to use: just change the year variable in the first cell, re-run, and you get the plots for the chosen year. But this requires a manual intervention. It would be much more convenient if the update could be automated and produce a report for each possible value of the year parameter (more generally, a notebook can update its results based not only on some user-provided parameters, but also through a connection to a database, etc.).Version controlIn a professional environment, notebooks are designed by, say, a data scientist, but the task of running them in production may be handled by a different team. So in general people have to share notebooks. This is best done through a version control system.Jupyter notebooks are famous for the difficulty of their version control. Lets consider our notebook above, with a file size of 3 MB, much of it being contributed by the embedded Plotly library. The notebook is less than 80 KB if we remove the output of the second code cell. And as small as 1.75 KB when all outputs are removed. This shows how much of its contents is unrelated to pure code! If we dont pay attention, code changes in the notebook will be lost in an ocean of binary contents.To get meaningful diffs, we use Jupytext (disclaimer: Im the author of Jupytext). Jupytext can be installed with pip or conda. Once the notebook server is restarted, a Jupytext menu appears in Jupyter:We click on Pair Notebook with Markdown, save the notebook and we obtain two representations of the notebook: world_fact.ipynb (with both input and output cells) and world_fact.md (with only the input cells).Jupytexts representation of notebooks as Markdown files is compatible with all major Markdown editors and viewers, including GitHub and VS Code. The Markdown version is for example rendered by GitHub as:As you can see, the Markdown file does not include any output. Indeed, we dont want it at this stage since we only need to share the notebook code. The Markdown file also has a very clear diff history, which makes versioning notebooks simple.The world_facts.md file is automatically updated by Jupyter when you save the notebook. And the other way round also works! If you modify world_facts.md with either a text editor, or by pulling the latest contributions from the version control system, then the changes appear in Jupyter when you refresh the notebook in the browser.In our version control system, we only need to track the Markdown file (and we even explicitly ignore all .ipynb files). Obviously, the team that will execute the notebook needs to regenerate the world_fact.ipynb document. For this they use Jupytext in the command line:$ jupytext world_facts.md --to ipynb[jupytext] Reading world_facts.md[jupytext] Writing world_facts.ipynbWe are now properly versioning the notebook. The diff history is much clearer. See for instance how the addition of the gross domestic products to our report looks like:Jupyter notebooks as Scripts?As an alternative to the Markdown representation, we could have paired the notebook to a world_facts.py script using Jupytext. You should give it a try if your notebook contains more code than text. That's often a first good step towards a complete and efficient refactoring of long notebooks: once the notebook is represented as a script, you can extract any complex code and move it to a (unit-tested) library using the refactoring tools in your IDE.JupyterLab, JupyterHub, Binder, Nteract, Colab & Cloud notebooks?Do you use JupyterLab and not Jupyter Notebook? No worries: the method above also applies in this case. You will just have to use the Jupytext extension for JupyterLab instead of the Jupytext menu. And in case you were wondering, Jupytext also work in JupyterHub and Binder.If you use other notebook editors like Nteract desktop, CoCalc, Google Colab, or another cloud notebook editor, you may not be able to use Jupytext as a plugin in the editor. In this case you can simply use Jupytext in the command line. Close your notebook and inject the pairing information into world_facts.ipynb with$ jupytext --set-formats ipynb,md world_facts.ipynband then keep the two representations synchronised with$ jupytext --sync world_facts.ipynbNotebook parametersPapermill is the reference library for executing notebooks with parameters.Papermill needs to know which cell contains the notebook parameters. This is simply done by adding a parameter tag in that cell with the cell toolbar in Jupyter Notebook:In JupyterLab you can use the celltags extension.And if you prefer you can also directly edit world_facts.md and add the tag there:```python tags=[\"parameters\"]year = 2000```Automated executionWe now have all the information required to execute the notebook on a production server.Production environmentIn order to execute the notebook, we need to know in which environment it should run. As we are working with a Python notebook in this example, we list its dependencies in a requirements.txt file, as is standard for Python projects.For simplicity, we also include the notebook tools in the same environment, i.e. add jupytext and papermill to the same requirements.txt file. Strictly speaking, these tools could be installed and executed in another Python environment.The corresponding Python environment is created with either$ conda create -n run_notebook --file requirements.txt -yor$ pip install -r requirements.txt(if in a virtual environment).Please note that the requirements.txt file is just one way of specifying an execution environment. The Reproducible Execution Environment Specification by the Binder team is one of the most complete references on the subject.Continuous IntegrationIt is a good practice to test each new contribution to either the notebook or its requirements. For this you can use for example Travis CI, a continuous integration solution. You will need only these two commands:pip install -r requirements.txt to install the dependenciesjupytext world_facts.md --set-kernel - --execute to test the execution of the notebook in the current Python environment.You can find a concrete example in our .travis.yml file.We are already executing the notebook automatically, arent we? Travis will tell us if a regression is introduced in the project What progress! But were not 100% done yet, as we promised to execute the notebook with parameters.Using the right kernelJupyter notebooks are associated with a kernel (i.e. a pointer to a local Python environment), but that kernel might not be available on your production machine. In this case, we simply update the notebook kernel so as to point to the environment that we have just created:$ jupytext world_facts.ipynb --set-kernel -Note that the minus sign in --set-kernel - above represents the current Python environment. In our example this yields:[jupytext] Reading world_facts.ipynb[jupytext] Updating notebook metadata with '{\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}}' [jupytext] Writing world_facts.ipynb (destination file replaced)In case you want to use another kernel just pass the kernel name to the --set-kernel option (you can get the list of all available kernels with jupyter kernelspec list and/or declare a new kernel with python -m ipykernel install --name kernel_name --user).Executing the notebook with parametersWe are now ready to use Papermill for executing the notebook.$ papermill world_facts.ipynb world_facts_2017.ipynb -p year 2017Input Notebook: world_facts.ipynb Output Notebook: world_facts_2017.ipynb 100%|| 8/8 [00:04<00:00, 1.41it/s]Were done! The notebook has been executed and the file world_facts_2017.ipynb contains the outputs.Publishing the NotebookIts time to deliver the notebook that was just executed. Maybe you want it in your mailbox? Or maybe you prefer to get a URL where you can see the result? We cover a few ways of doing that.GitHub can display Jupyter notebooks. This is a convenient solution, as you can easily choose who can access repositories. This works well as long as you dont include any interactive JavaScript plots or widgets in the notebook (the JavaScript parts are ignored by GitHub). In the case of our notebook, the interactive plots do not appear on GitHub, so we need another approach.Another option is to use the Jupyter Notebook Viewer. The nbviewer service can render any notebook which is publicly available on GitHub. Our notebook is thus rendered correctly there. If your notebook is not public, you can choose to install nbviewer locally.Alternatively, you can convert the executed notebook to HTML, and publish it on GitHub pages, or on your own HTML server, or send it over email. Converting the notebook to HTML is easily done with$ jupyter nbconvert world_facts_2017.ipynb --to html[NbConvertApp] Converting notebook world_facts_2017.ipynb to html [NbConvertApp] Writing 3361863 bytes to world_facts_2017.htmlThe resulting HTML file includes the code cells as below:But maybe you dont want to see the input cells in the HTML? You just need to add --no-input:$ jupyter nbconvert --to html --no-input world_facts_2017.ipynb --output world_facts_2017_report.htmlAnd youll get a cleaner report:Sending the standalone HTML file as an attachment in an email is an easy exercise. Embedding the report in the body of the email is also possible (but interactive plots wont work).Finally, if you are looking for a polished report and have some knowledge of LaTeX, you can give the PDF export option of Jupyters nbconvert command a try.Using pipesAn alternative to using named files would be to use pipes. jupytext, nbconvert and papermill all support them. A one-liner substitute for the previous commands is:$ cat world_facts.md \\ | jupytext --from md --to ipynb --set-kernel - \\ | papermill -p year 2017 \\ | jupyter nbconvert --stdin --output world_facts_2017_report.htmlConclusionYou should now be able to set up a full pipeline for generating reports in production, based on Jupyter notebooks. We have seen how to:version control a notebook with Jupytextshare a notebook and its dependencies between various userstest a notebook with continuous integrationexecute a notebook with parameters using Papermilland finally, how to publish the notebook (on GitHub or nbviewer), or render it as a static HTML page.The technology used in this example is fully based on the Jupyter Project, which is the de facto standard for Data Science. The tools used here are all open source and work well with any continuous integration framework.You have everything you need to schedule and deliver fine-tuned, code-free reports!EpilogueThe tools used here are written in Python. But they are language agnostic. Thanks to the Jupyter framework, they actually apply to any of the 40+ programming language for which a Jupyter kernel exists.Now, imagine that you have authored a document containing a few Bash command lines, just like this blog post. Install Jupytext and the bash kernel, and the blog post becomes this interactive Jupyter notebook!Going further, shouldnt we make sure that every instruction in our post actually works? We do that via our continuous integration spoiler alert: thats as simple as jupytext --execute README.md!AcknowledgmentsMarc would like to thank Eric Lebigot and Florent Zara for their contributions to this article, and to CFM for supporting this work through their Open-Source Program.About the authorThis article was written by Marc Wouts. Marc joined the research team of CFM in 2012 and has worked on a range of research projects, from optimal trading to portfolio construction.Marc has always been interested in finding efficient workflows for doing collaborative research involving data and code. In 2015 he authored an internal tool for publishing Jupyter and R Markdown notebooks on Atlassians Confluence wiki, providing a first solution for collaborating on notebooks. In 2018, he authored Jupytext, an open-source program that facilitates the version control of Jupyter notebooks. Marc is also interested in data visualisation, and coordinates a working group on this subject at CFM.Marc obtained a PhD in Probability Theory from the Paris Diderot University in 2007.DisclaimerAll views included in this document constitute judgments of its author(s) and do not necessarily reflect the views of Capital Fund Management or any of its affiliates. The information provided in this document is general information only, does not constitute investment or other advice, and is subject to change without notice. ",
        "_version_": 1718536525518471169
      },
      {
        "story_id": 19991764,
        "story_author": "owenwil",
        "story_descendants": 30,
        "story_score": 75,
        "story_time": "2019-05-23T13:51:24Z",
        "story_title": "GitHub's new features show it’s finally listening to developers",
        "search": [
          "GitHub's new features show it’s finally listening to developers",
          "https://char.gd/blog/2019/github-is-building-the-developer-platform-we-always-needed",
          "When GitHub was acquired by Microsoft in 2018, the company was in crisis. Internally, its culture was toxicand the company was struggling to ship meaningful features or improvements at all. In 2016, I wrote about how frustrated developers, exasperated by the company's ignorance of their pleas to build better tools, took to the platform to write an open letter, a final attempt at getting someone to care. The letter, \"Dear GitHub,\" was a brutal look at how little the company had done to foster the giant, open community, and was opaque about what it paid attention to:Those of us who run some of the most popular projects on GitHub feel completely ignored by you. Weve gone through the only support channel that you have given us either to receive an empty response or even no response at all. We have no visibility into what has happened with our requests, or whether GitHub is working on them. Since our own work is usually done in the open and everyone has input into the process, it seems strange for us to be in the dark about one of our most important project dependencies.At this inflection point, things looked badand it felt like GitHub was going to disappear. There was a time when I wondered what would happen if the world's largest open source community just went out business...but Microsoft has changed that narrative in a short period of time.GitHub, it seems, is thriving again. It just showed the fruits of that labor, and what it looks like when a company is participating in the discussion in the open, listening to the developers that know it best.At an event called GitHub Satellite, the company unveiled the biggest set of new features in memory, all designed to address glaring problems the platform has faced for years. They're designed to help make GitHub a better place to work, and contribute to the open source community as a whole. It started with the small stuff that mattershelping surface who's actually building the packages we use every day with new contributor and dependency tools, that show the people giving back to open source in a bigger way Then, it moved on to the big stuff: how GitHub is building a new way forward. Starting today, there are ways to actually financially support the people building things that we use, and support the work they've often been doing invisibly for free, or for a tiny amount of donations.Open source was always in a dangerous place: many of the things critical technology rely on come from work made available for free on GitHub, and the world's largest companies have built their fortunes atop of these projects without giving back a dollar. Many of these contributors building those tools burn out, or simply can't deal with the load, and sometimes it goes wrong when they step back, ultimating giving the reins to someone else who may not have the same intention. Long term, it wasn't sustainablenor was it a way to build the positive, diverse ecosystem of contributors that open source desperately needs. The tool, called GitHub Sponsor, allow contributors to support a specific person or project directlyand the company doesn't take any sort of cut, commission or fee. Yes, GitHub is swallowing the cost of those transactions entirely to give the creator as much of the money as possible. But, what's even wilder is that it's matching every dollar contributed in the first year, an outrageously bold commitment that's only possible with the backing of a company like Microsoft.With a way to directly support the people behind open source work, built directly into the world's most popular developer platform, we finally have the crucial missing piece to create a healthier way forward for open source. Most importantly, this helps to change the open source narrative: you shouldn't feel like you need to work for free, especially if companies are actually making money off of your work. Sponsorship right there on the page with the installation instructions helps pave the way for companies to actually fund the work they rely on, and that matters. There were a slew of other announcements to push the platform forward, too, from enterprise features for \"internal\" packages, hosted enterprise accounts, better permissions, automated security fixes for vulnerable repositories and much, much more.GitHub finally realized it has an opportunityand a responsibilityto define the way we build the internet's infrastructure. It was always best placed to build a more healthy community, but never stepped up to the platenow it has, and I can't wait to put my money behind the projects that have made my own work possible.But, the most important thing? GitHub showed it's listening, and it's acting. Alongside Microsoft's continued genuine efforts to build the best open developer experience regardless of the tools you use, it's putting its money where its mouth is, and it's good for all of us.GitHub's back. And it's building something for all of us again. Read these next "
        ],
        "story_type": "Normal",
        "url_raw": "https://char.gd/blog/2019/github-is-building-the-developer-platform-we-always-needed",
        "comments.comment_id": [19992416, 19992793],
        "comments.comment_author": ["Someone1234", "CameronNemo"],
        "comments.comment_descendants": [0, 2],
        "comments.comment_time": [
          "2019-05-23T15:05:30Z",
          "2019-05-23T15:46:53Z"
        ],
        "comments.comment_text": [
          "The main thread seems better:<p><a href=\"https://news.ycombinator.com/item?id=19989684\" rel=\"nofollow\">https://news.ycombinator.com/item?id=19989684</a><p>This lacks additional content or context. It is simply the author promoting some 2016 letter/article criticising GitHub, and then very loosely tying it to recent changes. It is self-promoting blogspam.",
          ">the world's largest open source community<p>Is github even considered an open source community? Open source projects work on github, sure, (so do closed source projects) but github is not the community. It is a tool."
        ],
        "id": "4063d627-1778-4d48-8b89-fc12614c05c8",
        "url_text": "When GitHub was acquired by Microsoft in 2018, the company was in crisis. Internally, its culture was toxicand the company was struggling to ship meaningful features or improvements at all. In 2016, I wrote about how frustrated developers, exasperated by the company's ignorance of their pleas to build better tools, took to the platform to write an open letter, a final attempt at getting someone to care. The letter, \"Dear GitHub,\" was a brutal look at how little the company had done to foster the giant, open community, and was opaque about what it paid attention to:Those of us who run some of the most popular projects on GitHub feel completely ignored by you. Weve gone through the only support channel that you have given us either to receive an empty response or even no response at all. We have no visibility into what has happened with our requests, or whether GitHub is working on them. Since our own work is usually done in the open and everyone has input into the process, it seems strange for us to be in the dark about one of our most important project dependencies.At this inflection point, things looked badand it felt like GitHub was going to disappear. There was a time when I wondered what would happen if the world's largest open source community just went out business...but Microsoft has changed that narrative in a short period of time.GitHub, it seems, is thriving again. It just showed the fruits of that labor, and what it looks like when a company is participating in the discussion in the open, listening to the developers that know it best.At an event called GitHub Satellite, the company unveiled the biggest set of new features in memory, all designed to address glaring problems the platform has faced for years. They're designed to help make GitHub a better place to work, and contribute to the open source community as a whole. It started with the small stuff that mattershelping surface who's actually building the packages we use every day with new contributor and dependency tools, that show the people giving back to open source in a bigger way Then, it moved on to the big stuff: how GitHub is building a new way forward. Starting today, there are ways to actually financially support the people building things that we use, and support the work they've often been doing invisibly for free, or for a tiny amount of donations.Open source was always in a dangerous place: many of the things critical technology rely on come from work made available for free on GitHub, and the world's largest companies have built their fortunes atop of these projects without giving back a dollar. Many of these contributors building those tools burn out, or simply can't deal with the load, and sometimes it goes wrong when they step back, ultimating giving the reins to someone else who may not have the same intention. Long term, it wasn't sustainablenor was it a way to build the positive, diverse ecosystem of contributors that open source desperately needs. The tool, called GitHub Sponsor, allow contributors to support a specific person or project directlyand the company doesn't take any sort of cut, commission or fee. Yes, GitHub is swallowing the cost of those transactions entirely to give the creator as much of the money as possible. But, what's even wilder is that it's matching every dollar contributed in the first year, an outrageously bold commitment that's only possible with the backing of a company like Microsoft.With a way to directly support the people behind open source work, built directly into the world's most popular developer platform, we finally have the crucial missing piece to create a healthier way forward for open source. Most importantly, this helps to change the open source narrative: you shouldn't feel like you need to work for free, especially if companies are actually making money off of your work. Sponsorship right there on the page with the installation instructions helps pave the way for companies to actually fund the work they rely on, and that matters. There were a slew of other announcements to push the platform forward, too, from enterprise features for \"internal\" packages, hosted enterprise accounts, better permissions, automated security fixes for vulnerable repositories and much, much more.GitHub finally realized it has an opportunityand a responsibilityto define the way we build the internet's infrastructure. It was always best placed to build a more healthy community, but never stepped up to the platenow it has, and I can't wait to put my money behind the projects that have made my own work possible.But, the most important thing? GitHub showed it's listening, and it's acting. Alongside Microsoft's continued genuine efforts to build the best open developer experience regardless of the tools you use, it's putting its money where its mouth is, and it's good for all of us.GitHub's back. And it's building something for all of us again. Read these next ",
        "_version_": 1718536485212258305
      },
      {
        "story_id": 19032734,
        "story_author": "clusmore",
        "story_descendants": 24,
        "story_score": 63,
        "story_time": "2019-01-30T04:41:10Z",
        "story_title": "Show HN: Serve Static GitHub Pages Locally",
        "search": [
          "Show HN: Serve Static GitHub Pages Locally",
          "https://github.com/CurtisLusmore/ghp",
          "A simple web server for serving static GitHub Pages locally, to test before deploying. This can be useful compared to browsing local HTML files from your browser when you use absolute paths in links, such as /about, /js/app.js, /css/style.css, etc., which won't resolve correctly in the context of your filesystem. It is also handy compared to something like python -m http.server which doesn't support dropping the file extension, e.g. /about rather than /about.html. When requesting any path ($path), ghp will do the following (all file operations are relative to the root commandline flag): Check whether $path points to a file, if so serve that file Check whether $path points to a directory, if so serve $path/index.html Check whether $path.html points to a file, if so serve that file Check whether 404.html is a file, if so serve that file as a 404 Serve a 404 If any of the above results in serving a Markdown file (extension .md), render the contents as HTML by using the GitHub Markdown API. Getting It From source (requires installing Go): $ git clone https://github.com/CurtisLusmore/ghp $ cd ghp $ go build ghp.go With Go Get (requires installing Go): $ go get github.com/CurtisLusmore/ghp Pre-compiled binaries: Check the latest Releases Usage $ ghp -help Usage of ghp: -port int The port to serve over (default 8080) -root string The root directory to serve files from (your GitHub Pages repo) (default \".\") $ ghp -root MyGitHubPages Notes This tool currently does not support building Jekyll-based GitHub Pages. If you use Jekyll-based GitHub Pages, please see Setting up your GitHub Pages site locally with Jekyll. As this tool exposes your filesystem to your network, you should be careful using this on untrusted networks. This tool will send the contents of Markdown files (extension .md) to https://api.github.com/markdown. Make sure not to run this in a directory with sensitive content in Markdown files - this is mostly intended for use on files you are likely to push to GitHub Pages anyway. Todo Confirm response headers match live GitHub Pages "
        ],
        "story_type": "ShowHN",
        "url_raw": "https://github.com/CurtisLusmore/ghp",
        "url_text": "A simple web server for serving static GitHub Pages locally, to test before deploying. This can be useful compared to browsing local HTML files from your browser when you use absolute paths in links, such as /about, /js/app.js, /css/style.css, etc., which won't resolve correctly in the context of your filesystem. It is also handy compared to something like python -m http.server which doesn't support dropping the file extension, e.g. /about rather than /about.html. When requesting any path ($path), ghp will do the following (all file operations are relative to the root commandline flag): Check whether $path points to a file, if so serve that file Check whether $path points to a directory, if so serve $path/index.html Check whether $path.html points to a file, if so serve that file Check whether 404.html is a file, if so serve that file as a 404 Serve a 404 If any of the above results in serving a Markdown file (extension .md), render the contents as HTML by using the GitHub Markdown API. Getting It From source (requires installing Go): $ git clone https://github.com/CurtisLusmore/ghp $ cd ghp $ go build ghp.go With Go Get (requires installing Go): $ go get github.com/CurtisLusmore/ghp Pre-compiled binaries: Check the latest Releases Usage $ ghp -help Usage of ghp: -port int The port to serve over (default 8080) -root string The root directory to serve files from (your GitHub Pages repo) (default \".\") $ ghp -root MyGitHubPages Notes This tool currently does not support building Jekyll-based GitHub Pages. If you use Jekyll-based GitHub Pages, please see Setting up your GitHub Pages site locally with Jekyll. As this tool exposes your filesystem to your network, you should be careful using this on untrusted networks. This tool will send the contents of Markdown files (extension .md) to https://api.github.com/markdown. Make sure not to run this in a directory with sensitive content in Markdown files - this is mostly intended for use on files you are likely to push to GitHub Pages anyway. Todo Confirm response headers match live GitHub Pages ",
        "comments.comment_id": [19032744, 19033338],
        "comments.comment_author": ["clusmore", "guessmyname"],
        "comments.comment_descendants": [2, 1],
        "comments.comment_time": [
          "2019-01-30T04:42:53Z",
          "2019-01-30T07:29:33Z"
        ],
        "comments.comment_text": [
          "Hey HN,<p>I made this for myself, because my blog is served as static HTML via GitHub Pages and it's hard to test locally because all of my absolute paths break (links, imgs, scripts, css, etc.). I tried using the Python module http.server, but it doesn't support dropping the file extension (e.g. /about instead of /about.html). Thought somebody else doing the same might find this useful.",
          "Congratulations on your first Go (Golang) project.<p>The source code doesn’t do anything related with GitHub pages though.<p>It simply starts a file server with no more features than adding “.html” at the end of the URL.<p>You may want to change the default port from :80 to :8080 so people don’t have to set “-port” every time.<p>Also, in the README, you say that one of the implemented features is:<p>> <i>4. Check whether 404.html is a file, if so serve that file as a 404</i><p>But your code says otherwise [1] since there is no 404.html file in the repository.<p>[1] <a href=\"https://github.com/CurtisLusmore/ghp/blob/6b6e2bd/ghp.go#L25-L29\" rel=\"nofollow\">https://github.com/CurtisLusmore/ghp/blob/6b6e2bd/ghp.go#L25...</a>"
        ],
        "id": "559e1d0c-0947-4d98-ac05-d7134eae722b",
        "_version_": 1718536442101104640
      },
      {
        "story_id": 19311118,
        "story_author": "darrinm",
        "story_descendants": 34,
        "story_score": 60,
        "story_time": "2019-03-05T15:29:56Z",
        "story_title": "Coder (Visual Studio Code in browser) goes open source",
        "search": [
          "Coder (Visual Studio Code in browser) goes open source",
          "https://coder.com",
          "From the Developers ofcode-serverThe developer workspace platformCentralize the creation and management of cloud developer workspacesWorks with Azure, GCP, AWS, OpenShift, and anywhere you run KubernetesDevelop in VS Code, Jupyter, RStudio, IntelliJ, PyCharm, and any JetBrains IDEThe problems with development running on local machinesOnboarding delays from environment setupConfiguration drift as the project evolvesSource code on insecure endpointsLimited compute powerMove development to your cloudCoder handles the orchestration of new conformant and consistent workspaces using source-controlled Dockerfiles and workspace templates. Empower developers and data scientists to spin up self-serve workspaces that just work.Keep your workflowCoder works with the tools that you work with. No need to change your preferred editor, CI tooling, or version control systemwith support for Docker in Docker, too.Run VS Code, Jupyter Notebook, RStudio, IntelliJ, PyCharm, & other IDEs and editorsRun VS Code locally via SSH connection to your Coder workspace (local support for JetBrains IDEs coming soon)Version control with GitHub, GitLab, & BitbucketPersonalize your workspaces to fit your flowSpeed up builds and testsUse the power and scale of the cloud to offload the burden of slow builds and tests from your local machine.Deploy workspaces with the CPUs and memory you needAccommodate bursty workloads by utilizing the clusters idle CPUs and memoryHarness the parallelism of GPUs for deep learning and other intensive workloadsDevelop with any deviceAccess your workspace from anywhere with the same snappy development experience you expect from a local IDE.Work from home, the office, or wherever you areCode using any device, even an iPadOnboard to new projects from wherever, wheneverOnboarding to a new project can take days away from your productivity and hinders collaboration. Get on the same page and stay on the same page faster.Start a new project with all the required tools and dependencies in minutesCollaborate across teams with easeWork with your team from anywhere in the world without latencySecure your data and source codeSource code and data can remain secured on the cluster or in authorized repositories not sitting on a workstation or laptop.Get the security benefits of VDI with a better developer experienceReduce the risk of source code leaksIdeal for zero-trust networks and air-gapped environmentsCasestudyKazoo reduces onboarding time with Coder\"New hires have a shorter onboarding experience because theyre just spinning up a Coder workspace instead of installing locally and having to worry about whether all the dependencies are up to date.\"Joe MainwaringDirector of Infrastructure, KazooRead case studyFAQIs Coder priced per developer or environment?Is Coder SaaS?What can I expect my infrastructure costs to be?How can I procure Coder for my government agency?What does the price of Coder include?What type of support is available?Can I add users at any time?Get started with Coder todayOur commitment to open sourceLearn more about our projects and our commitment to the open-source community.Code-server: the heart of CoderCode-server is the primary open source project we maintain. It allows developers to use a browser to access remote dev environments running VS Code. Coder builds upon the success of code-server and adds features designed for enterprise teams including support for additional IDEs and advanced security features.CookiesWe use cookies to make your experience better. "
        ],
        "story_type": "Normal",
        "url_raw": "https://coder.com",
        "url_text": "From the Developers ofcode-serverThe developer workspace platformCentralize the creation and management of cloud developer workspacesWorks with Azure, GCP, AWS, OpenShift, and anywhere you run KubernetesDevelop in VS Code, Jupyter, RStudio, IntelliJ, PyCharm, and any JetBrains IDEThe problems with development running on local machinesOnboarding delays from environment setupConfiguration drift as the project evolvesSource code on insecure endpointsLimited compute powerMove development to your cloudCoder handles the orchestration of new conformant and consistent workspaces using source-controlled Dockerfiles and workspace templates. Empower developers and data scientists to spin up self-serve workspaces that just work.Keep your workflowCoder works with the tools that you work with. No need to change your preferred editor, CI tooling, or version control systemwith support for Docker in Docker, too.Run VS Code, Jupyter Notebook, RStudio, IntelliJ, PyCharm, & other IDEs and editorsRun VS Code locally via SSH connection to your Coder workspace (local support for JetBrains IDEs coming soon)Version control with GitHub, GitLab, & BitbucketPersonalize your workspaces to fit your flowSpeed up builds and testsUse the power and scale of the cloud to offload the burden of slow builds and tests from your local machine.Deploy workspaces with the CPUs and memory you needAccommodate bursty workloads by utilizing the clusters idle CPUs and memoryHarness the parallelism of GPUs for deep learning and other intensive workloadsDevelop with any deviceAccess your workspace from anywhere with the same snappy development experience you expect from a local IDE.Work from home, the office, or wherever you areCode using any device, even an iPadOnboard to new projects from wherever, wheneverOnboarding to a new project can take days away from your productivity and hinders collaboration. Get on the same page and stay on the same page faster.Start a new project with all the required tools and dependencies in minutesCollaborate across teams with easeWork with your team from anywhere in the world without latencySecure your data and source codeSource code and data can remain secured on the cluster or in authorized repositories not sitting on a workstation or laptop.Get the security benefits of VDI with a better developer experienceReduce the risk of source code leaksIdeal for zero-trust networks and air-gapped environmentsCasestudyKazoo reduces onboarding time with Coder\"New hires have a shorter onboarding experience because theyre just spinning up a Coder workspace instead of installing locally and having to worry about whether all the dependencies are up to date.\"Joe MainwaringDirector of Infrastructure, KazooRead case studyFAQIs Coder priced per developer or environment?Is Coder SaaS?What can I expect my infrastructure costs to be?How can I procure Coder for my government agency?What does the price of Coder include?What type of support is available?Can I add users at any time?Get started with Coder todayOur commitment to open sourceLearn more about our projects and our commitment to the open-source community.Code-server: the heart of CoderCode-server is the primary open source project we maintain. It allows developers to use a browser to access remote dev environments running VS Code. Coder builds upon the success of code-server and adds features designed for enterprise teams including support for additional IDEs and advanced security features.CookiesWe use cookies to make your experience better. ",
        "comments.comment_id": [19313749, 19315009],
        "comments.comment_author": ["Hortinstein", "wodenokoto"],
        "comments.comment_descendants": [3, 2],
        "comments.comment_time": [
          "2019-03-05T20:07:17Z",
          "2019-03-05T22:53:48Z"
        ],
        "comments.comment_text": [
          "Has anyone tried this and compared it to <a href=\"https://www.theia-ide.org/\" rel=\"nofollow\">https://www.theia-ide.org/</a>?<p>First thing off the bat I notice is that Coder looks harder to deploy or try out, Theia was super easy, on the landing page they had a docker one liner:<p>docker run -it -p 3000:3000 -v \"$(pwd):/home/project:cached\" theiaide/theia:next",
          "I used to work at a place where we all used the enterprise server version of RStudio, which also runs in a browser.<p>There was a lot of good thing about that setup. Nobody could walk home with code, and no code was lost on somebody laptop.<p>Execution happened on a server, much more powerful than any dev machine."
        ],
        "id": "2dc1a49d-1e91-411c-bdbc-b3e5c4a277e5",
        "_version_": 1718536456779071488
      },
      {
        "story_id": 21403906,
        "story_author": "guessmyname",
        "story_descendants": 32,
        "story_score": 59,
        "story_time": "2019-10-30T23:02:02Z",
        "story_title": "GitHub Student Developer Pack",
        "search": [
          "GitHub Student Developer Pack",
          "https://github.blog/2019-10-30-get-over-100k-worth-of-tools-with-the-github-student-developer-pack/",
          "The GitHub Student Developer Pack now offers over $100k worth of tools and training to every student developer, anywhere that GitHub is available. If youre new to the Pack, it provides verified students with GitHub Pro at no charge while they are in school, plus free access to the best developer tools and trainingthanks to our partnersso they can learn by doing. As the Pack continues to grow and shape the next generation of developers, we continue to listen. Were here to better understand how youre using these tools and whats missing that you hope to see included. Whether youre developing your portfolio, building a new desktop app, or creating an interactive mapthe goal of the Pack is to provide you with the tools you need to be successful. This year, the value of the Pack tripled during the Fall semester by adding nineteen new partners in October plus a dozen in September to the twenty-one who joined our long-time partners for Back-to-School. Lets highlight the partners who joined us since our last post in August. A few words from our new partners We ask all of our new partners what motivated them to join the Pack. Heres a sample of what theyve told us: When I was a student, I actually used the GitHub Student Developer Pack myself! It allowed me to test and learn how to use tools that I wouldnt have been able to otherwise. The Pack was a great help! -Floran Pagliai, Weglot The way that developers learn best is by getting their hands dirty, trying different things, and experimenting with a variety of tools. The Pack allows developers to do that and get exposure creating awesome projects outside the confines of the classroom. -Naeem ul Haq, Educative Not many high school and even college students would ordinarily be able to afford [our tool.] Letting students try it will give us valuable feedback about what up-and-coming developers are looking for. -Levie Rufenacht, Wisej The Pack opens new doors to students that were not accessible before. It basically gives them whats needed to build a project from A to Z, lets them cultivate their curiosity, an essential quality for a developer, and free their creativity. Learning should know no boundaries. -Julien Lehuraux, USE Together Partner details The following are our new partners and the tools and training they are providing, for free, to students: October Appfigures, app store analytics, optimization, and intelligence Astra Security, security suite for your website, including firewall, malware scanner, and a managed bug bounty platform BoltFlare, reliable, secure, and scalable managed WordPress hosting BrowserStack, test your web apps, providing instant access to 2,000+ browsers and real iOS and Android devices Codecov, implement code coverage easier to develop healthier code Educative, level up on trending coding skills at your own pace with interactive, text-based courses EverSQL, boost your database performance by automatically optimizing your SQL queries HazeOver, get focused while working on projects or studying Iconscout, design resources marketplace with high quality icons, illustrations, and stock images Interview Cake, makes coding interviews a piece of cake with practice questions, data structures and algorithms reference pages, cheat sheets, and more Kaltura, build interactive video experiences and advanced media applications MNX.io, managed Cloud hosting for developers NetLicensing, cost-effective and integrated Licensing-as-a-Service (LaaS) solution for your software on any platform from Desktop to IoT and SaaS Scrapinghub, battle-tested cloud platform for running web crawlers where you can manage and automate your web spiders at scale Testmail, unlimited email addresses and mailboxes for automating email tests with powerful APIs Typeform, interactive forms, surveys, and quizzes to engage and grow your audience USE-Together, provides a remote pair programming and team collaboration tool Weglot, make any website multilingual and manage your translations through a single platform Wisej, build powerful web applications in Visual Studio with C# or VB.NET September Blackfire, Code performance measurement tool where you can find and fix bottlenecks Canva, create professional looking graphics and designs, featuring thousands of templates and an easy to use editor Covalence, provides an exclusive developer community and allows you to learn Full Stack web-development with no long-term commitments Crowdin, cloud-based solution that streamlines localization management Education Host, web hosting platform to host assignment and project work GoRails, tutorials for web developers learning Ruby, Rails, Javascript, Turbolinks, Stimulus.js, Vue.js, and more Honeybadger, exception, uptime, and cron monitoring Mailgun, APIs that enable you to send, receive, and track email One Month, learn HTML, CSS, JavaScript and Python in just 30 days Repl.it, an online IDE that lets you instantly code in over fifty languages so you can start learning, building, collaborating, and hosting all in one place Storyscript, top-level, cloud native programming language that helps you orchestrate data flow seamlessly between microservices Vaadin, open source Java framework for building Progressive Web Applications What can I do with the Pack? Looking for ideas for how to use all the new tools? Take a look at these projects or check out GitHub Educations Twitter and Facebook for suggestions. Make a request Have a tool youd like to see included in the Pack? Let the developer know (and tag us) on social media using the hashtag #GitHubPack. Companies interested in including an offer in the Pack can apply to be a partner. Not yet a member of the Pack? Its available for all verified students ages 13+, anywhere in the world where GitHub is available. Join today using your school-issued email, student ID, or other proof of current academic enrollment. Explore the latest offers in the GitHub Student Developer Pack "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.blog/2019-10-30-get-over-100k-worth-of-tools-with-the-github-student-developer-pack/",
        "comments.comment_id": [21404840, 21405132],
        "comments.comment_author": ["zachlatta", "sdan"],
        "comments.comment_descendants": [0, 1],
        "comments.comment_time": [
          "2019-10-31T01:28:13Z",
          "2019-10-31T02:24:53Z"
        ],
        "comments.comment_text": [
          "For those who are part of Hack Club (<a href=\"https://hackclub.com\" rel=\"nofollow\">https://hackclub.com</a>), we partnered with GitHub to provide speedy access to the Student Developer Pack (48 hrs vs. 2-3 weeks). Redeem at <a href=\"https://hack.af/pack\" rel=\"nofollow\">https://hack.af/pack</a>.<p>More on the partnership at <a href=\"https://medium.com/hackclub/github-hack-club-grants-for-your-club-custom-merch-more-f64d6da0d782\" rel=\"nofollow\">https://medium.com/hackclub/github-hack-club-grants-for-your...</a>",
          "Just a heads up:<p>Do NOT use AWS Educate (regardless if you get it from some hackathon or Github).<p>The amount of pain and suffering you have to go through just to get permission to spin up a EC2 instance is unbearable. Compounded with a ton of restrictions and 60-minute sessions, it'll make you unlike AWS real-quick.<p>I would instead suggest to use the $300 GCP credit (since there's no restrictions for that) if you need cloud credits."
        ],
        "id": "88c0e826-6d33-4064-997e-532bfc1e7728",
        "url_text": "The GitHub Student Developer Pack now offers over $100k worth of tools and training to every student developer, anywhere that GitHub is available. If youre new to the Pack, it provides verified students with GitHub Pro at no charge while they are in school, plus free access to the best developer tools and trainingthanks to our partnersso they can learn by doing. As the Pack continues to grow and shape the next generation of developers, we continue to listen. Were here to better understand how youre using these tools and whats missing that you hope to see included. Whether youre developing your portfolio, building a new desktop app, or creating an interactive mapthe goal of the Pack is to provide you with the tools you need to be successful. This year, the value of the Pack tripled during the Fall semester by adding nineteen new partners in October plus a dozen in September to the twenty-one who joined our long-time partners for Back-to-School. Lets highlight the partners who joined us since our last post in August. A few words from our new partners We ask all of our new partners what motivated them to join the Pack. Heres a sample of what theyve told us: When I was a student, I actually used the GitHub Student Developer Pack myself! It allowed me to test and learn how to use tools that I wouldnt have been able to otherwise. The Pack was a great help! -Floran Pagliai, Weglot The way that developers learn best is by getting their hands dirty, trying different things, and experimenting with a variety of tools. The Pack allows developers to do that and get exposure creating awesome projects outside the confines of the classroom. -Naeem ul Haq, Educative Not many high school and even college students would ordinarily be able to afford [our tool.] Letting students try it will give us valuable feedback about what up-and-coming developers are looking for. -Levie Rufenacht, Wisej The Pack opens new doors to students that were not accessible before. It basically gives them whats needed to build a project from A to Z, lets them cultivate their curiosity, an essential quality for a developer, and free their creativity. Learning should know no boundaries. -Julien Lehuraux, USE Together Partner details The following are our new partners and the tools and training they are providing, for free, to students: October Appfigures, app store analytics, optimization, and intelligence Astra Security, security suite for your website, including firewall, malware scanner, and a managed bug bounty platform BoltFlare, reliable, secure, and scalable managed WordPress hosting BrowserStack, test your web apps, providing instant access to 2,000+ browsers and real iOS and Android devices Codecov, implement code coverage easier to develop healthier code Educative, level up on trending coding skills at your own pace with interactive, text-based courses EverSQL, boost your database performance by automatically optimizing your SQL queries HazeOver, get focused while working on projects or studying Iconscout, design resources marketplace with high quality icons, illustrations, and stock images Interview Cake, makes coding interviews a piece of cake with practice questions, data structures and algorithms reference pages, cheat sheets, and more Kaltura, build interactive video experiences and advanced media applications MNX.io, managed Cloud hosting for developers NetLicensing, cost-effective and integrated Licensing-as-a-Service (LaaS) solution for your software on any platform from Desktop to IoT and SaaS Scrapinghub, battle-tested cloud platform for running web crawlers where you can manage and automate your web spiders at scale Testmail, unlimited email addresses and mailboxes for automating email tests with powerful APIs Typeform, interactive forms, surveys, and quizzes to engage and grow your audience USE-Together, provides a remote pair programming and team collaboration tool Weglot, make any website multilingual and manage your translations through a single platform Wisej, build powerful web applications in Visual Studio with C# or VB.NET September Blackfire, Code performance measurement tool where you can find and fix bottlenecks Canva, create professional looking graphics and designs, featuring thousands of templates and an easy to use editor Covalence, provides an exclusive developer community and allows you to learn Full Stack web-development with no long-term commitments Crowdin, cloud-based solution that streamlines localization management Education Host, web hosting platform to host assignment and project work GoRails, tutorials for web developers learning Ruby, Rails, Javascript, Turbolinks, Stimulus.js, Vue.js, and more Honeybadger, exception, uptime, and cron monitoring Mailgun, APIs that enable you to send, receive, and track email One Month, learn HTML, CSS, JavaScript and Python in just 30 days Repl.it, an online IDE that lets you instantly code in over fifty languages so you can start learning, building, collaborating, and hosting all in one place Storyscript, top-level, cloud native programming language that helps you orchestrate data flow seamlessly between microservices Vaadin, open source Java framework for building Progressive Web Applications What can I do with the Pack? Looking for ideas for how to use all the new tools? Take a look at these projects or check out GitHub Educations Twitter and Facebook for suggestions. Make a request Have a tool youd like to see included in the Pack? Let the developer know (and tag us) on social media using the hashtag #GitHubPack. Companies interested in including an offer in the Pack can apply to be a partner. Not yet a member of the Pack? Its available for all verified students ages 13+, anywhere in the world where GitHub is available. Join today using your school-issued email, student ID, or other proof of current academic enrollment. Explore the latest offers in the GitHub Student Developer Pack ",
        "_version_": 1718536537614843904
      },
      {
        "story_id": 21112632,
        "story_author": "severine",
        "story_descendants": 135,
        "story_score": 442,
        "story_time": "2019-09-30T07:38:00Z",
        "story_title": "KDE is adopting GitLab",
        "search": [
          "KDE is adopting GitLab",
          "https://about.gitlab.com/press/releases/2019-09-17-gitlab-adopted-by-KDE.html",
          "You are here: Press and LogosPress releasesGitLab Adopted by KDE to Foster Open Source Contributions KDE Open Source Community has access to GitLab DevOps platform increasing members software-building options MILAN, ITALY Akademy September 11, 2019 Today GitLab, the DevOps platform delivered as a single application, announced that KDE, an international technology community that creates free and open source software for desktop and portable computing, is adopting GitLab for use by its developers to further enhance infrastructure accessibility and encourage contributions. KDE is a free and open source software community dedicated to creating a user-friendly computing experience. It offers an advanced graphical desktop, a wide variety of applications for communication, work, education and entertainment, and a platform for easily building new applications. Adding access to GitLab will provide the KDE community with additional options for accessible infrastructure for contributors, code review integration with git, streamlined infrastructure and tooling, and an open communication channel with the upstream GitLab community. With the adoption of GitLab, the KDE community, one of the largest Free Software communities with more than 2.600 contributors, will have access to an even wider range of development and code review features with GitLabs DevOps platform to complement current tools used by the KDE community. The KDE community will also be able to integrate GitLabs single application for the DevOps lifecycle to their development workflow, from planning to development and deployment to monitoring. Using GitLab, KDE contributors will have access to Concurrent DevOps, and the ability to manage and secure across stages. GitLab also provides increased visibility and comprehensive governance and accelerates software lifecycles. Were thrilled that the KDE community has chosen to adopt GitLab to offer its developers with additional tooling and features for building cutting-edge applications, said David Planella, Director of Community Relations, GitLab. KDE places a strong emphasis on finding innovative solutions to old and new problems in an atmosphere of open experimentation. This thinking aligns with GitLabs goal of helping teams better collaborate on software development, and we look forward to supporting KDE as they continue to build great software for millions of users across the globe. Lydia Pintscher, President of KDE e.V., said: For an open community like KDE, having friendly, easy-to-use infrastructure is crucial. We have spent the last two years significantly reducing the barriers of entry all across KDE. Moving to GitLab is a major step in that process. Note to editors: During GitLab Commit, GitLabs inaugural user events in Brooklyn on September 17 and London on October 9, KDE will participate in a panel on the benefits of using GitLab with KDE projects. About KDE KDE is an international community that creates Free Software for desktop and portable computing. Among KDE's products are a modern desktop system for Linux and UNIX platforms, and comprehensive office productivity and groupware suites. KDE offers hundreds of software titles in many categories including web applications, multimedia, entertainment, educational, graphics and software development. KDE software is translated into more than 65 languages and is built with ease of use and modern accessibility principles in mind. KDE's full-featured applications run natively on Linux, BSD, Solaris, Windows and macOS. About GitLab GitLab is the DevOps platform built from the ground up as a single application for all stages of the DevOps lifecycle enabling Product, Development, QA, Security, and Operations teams to work concurrently on the same project. GitLab provides a single data store, one user interface, and one permission model across the DevOps lifecycle. This allows teams to significantly reduce cycle times through more efficient collaboration and enhanced focus. Built on Open Source, GitLab works alongside its growing community, which is composed of thousands of developers and millions of users, to continuously deliver new DevOps innovations. GitLab has an estimated 30 million+ users (both Paid and Free) from startups to global enterprises, including Ticketmaster, Jaguar Land Rover, NASDAQ, Dish Network, and Comcast trust GitLab to deliver great software faster. All-remote since 2014, GitLab has more than 1,300 team members in 65 countries. Media Contact Natasha Woods GitLab nwoods@gitlab.com (415) 312-5289 KDE Press Room press@kde.org Git is a trademark of Software Freedom Conservancy and our use of 'GitLab' is under license View page source Edit in Web IDE please contribute. GitLab B.V. "
        ],
        "story_type": "Normal",
        "url_raw": "https://about.gitlab.com/press/releases/2019-09-17-gitlab-adopted-by-KDE.html",
        "comments.comment_id": [21112936, 21113256],
        "comments.comment_author": ["TekMol", "EspadaV9"],
        "comments.comment_descendants": [17, 3],
        "comments.comment_time": [
          "2019-09-30T08:40:31Z",
          "2019-09-30T09:37:59Z"
        ],
        "comments.comment_text": [
          "So far, I am happy only using pure git.<p>If you guys that use GitLab or GitHub or alike would have to switch to pure git - what is the one thing you would miss most?",
          "Couldn't see a link in the post but there seems to be an active GitLab instance here[1]. I've only done a few patches with their previous Phabricator flow and it was a very different experience from the usual GitHub/GitLab workflow. I'm really hoping that switching to something more people have experience with increases the number of contributors.<p>[1]: <a href=\"https://invent.kde.org/public/\" rel=\"nofollow\">https://invent.kde.org/public/</a>"
        ],
        "id": "5e2839bb-3dfd-44d3-aa8a-25848b05ce74",
        "url_text": "You are here: Press and LogosPress releasesGitLab Adopted by KDE to Foster Open Source Contributions KDE Open Source Community has access to GitLab DevOps platform increasing members software-building options MILAN, ITALY Akademy September 11, 2019 Today GitLab, the DevOps platform delivered as a single application, announced that KDE, an international technology community that creates free and open source software for desktop and portable computing, is adopting GitLab for use by its developers to further enhance infrastructure accessibility and encourage contributions. KDE is a free and open source software community dedicated to creating a user-friendly computing experience. It offers an advanced graphical desktop, a wide variety of applications for communication, work, education and entertainment, and a platform for easily building new applications. Adding access to GitLab will provide the KDE community with additional options for accessible infrastructure for contributors, code review integration with git, streamlined infrastructure and tooling, and an open communication channel with the upstream GitLab community. With the adoption of GitLab, the KDE community, one of the largest Free Software communities with more than 2.600 contributors, will have access to an even wider range of development and code review features with GitLabs DevOps platform to complement current tools used by the KDE community. The KDE community will also be able to integrate GitLabs single application for the DevOps lifecycle to their development workflow, from planning to development and deployment to monitoring. Using GitLab, KDE contributors will have access to Concurrent DevOps, and the ability to manage and secure across stages. GitLab also provides increased visibility and comprehensive governance and accelerates software lifecycles. Were thrilled that the KDE community has chosen to adopt GitLab to offer its developers with additional tooling and features for building cutting-edge applications, said David Planella, Director of Community Relations, GitLab. KDE places a strong emphasis on finding innovative solutions to old and new problems in an atmosphere of open experimentation. This thinking aligns with GitLabs goal of helping teams better collaborate on software development, and we look forward to supporting KDE as they continue to build great software for millions of users across the globe. Lydia Pintscher, President of KDE e.V., said: For an open community like KDE, having friendly, easy-to-use infrastructure is crucial. We have spent the last two years significantly reducing the barriers of entry all across KDE. Moving to GitLab is a major step in that process. Note to editors: During GitLab Commit, GitLabs inaugural user events in Brooklyn on September 17 and London on October 9, KDE will participate in a panel on the benefits of using GitLab with KDE projects. About KDE KDE is an international community that creates Free Software for desktop and portable computing. Among KDE's products are a modern desktop system for Linux and UNIX platforms, and comprehensive office productivity and groupware suites. KDE offers hundreds of software titles in many categories including web applications, multimedia, entertainment, educational, graphics and software development. KDE software is translated into more than 65 languages and is built with ease of use and modern accessibility principles in mind. KDE's full-featured applications run natively on Linux, BSD, Solaris, Windows and macOS. About GitLab GitLab is the DevOps platform built from the ground up as a single application for all stages of the DevOps lifecycle enabling Product, Development, QA, Security, and Operations teams to work concurrently on the same project. GitLab provides a single data store, one user interface, and one permission model across the DevOps lifecycle. This allows teams to significantly reduce cycle times through more efficient collaboration and enhanced focus. Built on Open Source, GitLab works alongside its growing community, which is composed of thousands of developers and millions of users, to continuously deliver new DevOps innovations. GitLab has an estimated 30 million+ users (both Paid and Free) from startups to global enterprises, including Ticketmaster, Jaguar Land Rover, NASDAQ, Dish Network, and Comcast trust GitLab to deliver great software faster. All-remote since 2014, GitLab has more than 1,300 team members in 65 countries. Media Contact Natasha Woods GitLab nwoods@gitlab.com (415) 312-5289 KDE Press Room press@kde.org Git is a trademark of Software Freedom Conservancy and our use of 'GitLab' is under license View page source Edit in Web IDE please contribute. GitLab B.V. ",
        "_version_": 1718536528031907840
      },
      {
        "story_id": 21681303,
        "story_author": "jph00",
        "story_descendants": 5,
        "story_score": 28,
        "story_time": "2019-12-02T10:31:58Z",
        "story_title": "Nbdev: Use Notebooks for Everything",
        "search": [
          "Nbdev: Use Notebooks for Everything",
          "https://www.fast.ai/2019/12/02/nbdev/",
          "Written: 02 Dec 2019 by Jeremy Howard I really do think [nbdev] is a huge step forward for programming environments: Chris Lattner, inventor of Swift, LLVM, and Swift Playgrounds. My fast.ai colleague Sylvain Gugger and I have been working on a labor of love for the last couple of years. It is a Python programming environment called nbdev, which allows you to create complete python packages, including tests and a rich documentation system, all in Jupyter Notebooks. Weve already written a large programming library (fastai v2) using nbdev, as well as a range of smaller projects. Nbdev is a system for something that we call exploratory programming. Exploratory programming is based on the observation that most of us spend most of our time as coders exploring and experimenting. We experiment with a new API that we havent used before, to understand exactly how it behaves. We explore the behavior of an algorithm that we are developing, to see how it works with various kinds of data. We try to debug our code, by exploring different combinations of inputs. And so forth nbdev: exploratory programming Software development tools Interactive programming environments So whats missing in Jupyter Notebook? Dynamic Python What now nbdev: exploratory programming We believe that the very process of exploration is valuable in itself, and that this process should be saved so that other programmers (including yourself in six months time) can see what happened and learn by example. Think of it as something like a scientists journal. You can use it to show the things that you tried, what worked and what didnt, and what you did to develop your understanding of the system within which you are working. During this exploration, you will realize that some parts of this understanding are critical for the system to work. Therefore, the exploration should include the addition of tests and assertions to ensure this behavior. This kind of exploring is easiest when you develop on the prompt (or REPL), or using a notebook-oriented development system like Jupyter Notebooks. But these systems are not as strong for the programming part. This is why people use such systems mainly for early exploring, and then switch to an IDE or text editor later in a project. They switch to get features like good doc lookup, good syntax highlighting, integration with unit tests, and (critically!) the ability to produce final, distributable source code files, as opposed to notebooks or REPL histories. The point of nbdev is to bring the key benefits of IDE/editor development into the notebook system, so you can work in notebooks without compromise for the entire lifecycle. To support this kind of exploration, nbdev is built on top of Jupyter Notebook (which also means we get much better support for Pythons dynamic features than in a normal editor or IDE), and adds the following critically important tools for software development: Python modules are automatically created for you, following best practices such as automatically defining __all__ (more details) with your exported functions, classes, and variables Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks Automatically create searchable, hyperlinked documentation from your code; any word you surround in backticks will by hyperlinked to the appropriate documentation, a sidebar will be created for you in your documentation site with links to each of your modules, and more Pip installers (uploaded to pypi for you) Testing (defined directly in your notebooks, and run in parallel) Continuous integration Version control conflict handling Heres a snippet from our actual source code for nbdev, which is itself written in nbdev (of course!) Exploring the notebook file format in the nbdev source code As you can see, when you build software this way, everyone in your project team gets to benefit from the work you do in building an understanding of the problem domain, such as file formats, performance characteristics, API edge cases, and so forth. Since development occurs in a notebook, you can also add charts, text, links, images, videos, etc, that will be included automatically in the documentation of your library. The cells where your code is defined will be hidden and replaced by standardized documentation of your function, showing its name, arguments, docstring, and link to the source code on GitHub. For more information about features, installation, and how to use nbdev, see its documentation (which is, naturally, automatically generated from its source code). Ill be posting a step by step tutorial in the coming days. In the rest of this post, Ill describe more of the history and context behind the why: why did we build it, and why did we design it the way we did. First, lets talk about a little history (And if youre not interested in the history, you can skip ahead to Whats missing in Jupyter Notebook.) Most software development tools are not built from the foundations of thinking about exploratory programming. When I began coding, around 30 years ago, waterfall software development was used nearly exclusively. It seemed to me at the time that this approach, where an entire software system would be defined in minute detail upfront, and then coded as closely to the specification as possible, did not fit at all well with how I actually got work done. In the 1990s, however, things started to change. Agile development became popular. People started to understand the reality that most software development is an iterative process, and developed ways of working which respected this fact. However, we did not see major changes to the software development tools that we used, that matched the major changes to our ways of working. There were some pieces of tooling which got added to our arsenal, particularly around being able to do test driven development more easily. However this tooling tended to appear as minor extensions to existing editors and development environments, rather than truly rethinking what a development environment could look like. In recent years weve also begun to see increasing interest in exploratory testing as an important part of the agile toolbox. We absolutely agree! But we also think this doesnt go nearly far enough; we think in nearly every part of the software development process that exploration should be a central part of the story. The legendary Donald Knuth was way ahead of his time. He wanted to see things done very differently. In 1983 he developed a methodology called literate programming. He describes it as a methodology that combines a programming language with a documentation language, thereby making programs more robust, more portable, more easily maintained, and arguably more fun to write than programs that are written only in a high-level language. The main idea is to treat a program as a piece of literature, addressed to human beings rather than to a computer. For a long time I was fascinated by this idea, but unfortunately it never really went anywhere. The tooling available for working this way resulted in software development taking much longer, and very few people decided that this compromise was worth it. Nearly 30 years later another brilliant and revolutionary thinker, Bret Victor, expressed his deep discontent for the current generation of development tools, and described how to design a programming system for understanding programs. As he said in his groundbreaking speech Inventing on Principle: Our current conception of what a computer program is a list of textual definitions that you hand to a compiler thats derived straight from Fortran and ALGOL in the late 50s. Those languages were designed for punchcards. He laid out, and illustrated with fully worked examples, a range of new principles for designing programming systems. Whilst nobody has as yet fully implemented all of his ideas, there have been some significant attempts to implement some parts of them. Perhaps the most well-known and complete implementation, including inline display of intermediate results, is Chris Lattners Swift and Xcode Playgrounds. Demonstration of Playgounds in Xcode Whilst this is a big step forward, it is still very constrained by the basic limitations of sitting within a development environment which was not originally built with such explorations in mind. For instance, the exploration process is not captured by this at all, tests cannot be directly integrated into it, and the full rich vision of literate programming cannot be implemented. Interactive programming environments There has been another very different direction in software development, which is interactive programming (and the related live programming). It started many decades ago with the LISP and Forth REPLs, which allowed developers to interactively add and remove code in a running application. Smalltalk took things even further, providing a fully interactive visual workspace. In all these cases, the languages themselves were well suited to this kind of interactive work, for instance with LISPs macro system and code as data foundations. Live programming in Smalltalk (1980) Although this approach isnt how most regular software development is done today, it is the most popular approach in many areas of scientific, statistical, and other data-driven programming. (JavaScript front-end programming is however increasingly borrowing ideas from those approaches, such as hot reloading and in-browser live editing.) Matlab, for instance, started out as an entirely interactive tool back in the 1970s, and today is still widely used in engineering, biology, and various other areas (it also provides regular software development features nowadays). Similar approaches were used by SPLUS, and its open source cousin R, which is today extremely popular in the statistic and data visualization communities (amongst others). I got particularly excited when I first used Mathematica about 25 years ago. Mathematica looked to me like the closest thing Id seen to something that could support literate programming, without compromising on productivity. To do this, it used a notebook interface, which behaved a lot like a traditional REPL, but also allowed other types of information to be included, including charts, images, formatted text, outlining sections, and so forth. In fact, not only did it not compromise on productivity, but I found it actually allowed me to build things that previously were beyond me, because I could try algorithms out and immediately get feedback in a very visual way. In the end though, Mathematica didnt really help me build anything useful, because I couldnt distribute my code or applications to colleagues (unless they spent thousands of dollars for a Mathematica license to use it), and I couldnt easily create web applications for people to access from the browser. In addition, I found my Mathematica code would often end up much slower and more memory hungry than code I wrote in other languages. So you can imagine my excitement when Jupyter Notebook appeared on the scene. This used the same basic notebook interface as Mathematica (although, at first, with a small subset of the functionality) but was open source, and allowed me to write in languages that were widely supported and freely available. Ive been using Jupyter not just for exploring algorithms, APIs, and new research ideas, but also as a teaching tool at fast.ai. Many students have found that the ability to experiment with inputs and view intermediate results and outputs, as well as try out their own modifications, helped them to more fully and deeply understand the topics being discussed. We are also writing a book entirely using Jupyter Notebooks, which has been an absolute pleasure, allowing us to combine prose, code examples, hierarchical structured headings, and so forth, whilst ensuring that our sample outputs (including charts, tables, and images) always correctly match up to the code examples. In short: we have really enjoyed using Jupyter Notebook, we find that we do great work using it, and our students love it. But it just seemed like such a shame that we werent actually using it to build our software! So whats missing in Jupyter Notebook? Whilst Jupyter Notebook is great at the explorable part part of explorable programming, its not so great at the programming part. For instance, it doesnt really provide a way to do things like: Create modular reusable code, which can be run outside of Jupyter Creating hyperlinked searchable documentation Test code (including automatically through continuous integration) Navigate code Handle version control Because of this, people generally have to switch between a mix of poorly integrated tools, with significant friction as they move from tool to tool, to get the advantages of each: Development Pros Cons IDE/Editor Produces final distributable module Integration with doc lookup Integration with syntax highlighting and type-checking Non-interactive, so hard to explore Incomplete support of dynamic languages Documentation is text-only No facility for documenting a session of interaction, or explaining through example REPL/shell Good for small interactive explorations Bad for everything else, including producing distributable modules Traditional notebooks Mixes code, rich text, and images Explaining thru examples by recording a session of interaction Accurate code navigation and auto-completion for dynamic languages Same cons as REPL programming We decided that the best way to handle these things was to leverage great tools that already exist, where possible, and build our own where needed. For instance, for handling pull requests and viewing diffs, theres already a great tool: ReviewNB. When you look at graphical diffs in ReviewNB, you suddenly realize how much has been missing all this time in plain text diffs. For instance, what if a commit made your image generation blurry? Or made your charts appear without labels? You really know whats going on when you have that visual diff. Visual diff in ReviewNB, showing change to tabular output Many merge conflicts are avoided with nbdev, because it installs git hooks for you which strip out much of the metadata that causes those conflicts in the first place. If you get a merge conflict when you pull from git, just run nbdev_fix_merge. With this command, nbdev will simply use your cell outputs where there are conflicts in outputs, and if there are conflicts in cell inputs, then both cells are included in the final notebook, along with conflict markers so you can easily find them and fix them directly in Jupyter. An example of a cell-based nbdev merge conflict Modular reusable code is created by nbdev by simply creating standard Python modules. nbdev looks for special comments in code cells, such as #export, which indicates that a cell should be exported to a python module. Each notebook is associated with a particular python module by using a special comment at the start of the notebook. A documentation site (using Jekyll, so supported directly by GitHub Pages) is automatically built from the notebooks and special comments. We wrote our own documentation system, since existing approaches such as Sphinx didnt provided all the features that we needed. For code navigation, there are already wonderful features built into most editors and IDEs, such as vim, Emacs, and vscode. And as a bonus, GitHub even supports code navigation directly in its web interface now (in beta, in selected projects, such as fastai)! So weve ensured that the code that nbdev exports can be navigated and edited directly in any of these systems - and that any edits can be automatically synchronized back with the notebooks. For testing, weve written our own simple library and command line tool. Tests are written directly in notebooks, as part of the exploration and development (and documentation) process, and the command line tool runs tests in all notebooks in parallel. The natural statefulness of notebooks turns out to be a really great way to develop both unit tests and integration tests. Rather than having special syntax to learn to create test suites, you just use the regular collection and looping constructs in python. So theres much fewer new concepts to learn. These tests can also be run in your normal continuous integration tools, and they provide clear information about the source of any test errors that come up. The default nbdev template includes integration with GitHub Actions for continuous integration and other features (PRs for other platforms are welcome). Dynamic Python One of the challenges in fully supporting Python in a regular editor or IDE is that Python has particularly strong dynamic features. For instance, you can add methods to a class at any time, you can change the way that classes are created and how they work by using the metaclass system, and you can change how functions and methods behave by using decorators. Microsoft developed the Language Server Protocol, which can be used by development environments to get information about the current file and project necessary for auto-completions, code navigation, and so forth. However, with a truly dynamic language like python, such information will always just be guesses, since actually providing the correct information would require running the python code itself (which it cant really do, for all kinds of reasons - for instance the code may be in a state while youre writing it that actually deletes all your files!) On the other hand, a notebook contains an actual running Python interpreter instance that youre fully in control of. So Jupyter can provide auto-completions, parameter lists, and context-sensitive documentation based on the actual state of your code. For instance, when using Pandas we get tab completion of all the column names of our DataFrames. Weve found that this feature of Jupyter Notebook makes exploratory programming significantly more productive. We havent needed to change anything to make it work well in nbdev; its just part of the great features of Jupyter that we get for free by building on that platform. What now In conjunction with developing nbdev, weve been writing fastai v2 from scratch entirely in nbdev. fastai v2 provides a rich, well-structured API for building deep learning models. It will be released in the first half of 2020. Its already feature complete, and early adopters are already building cool projects with the pre-release version. Weve also written other projects in fastai v2, some of which will be released in the coming weeks. Weve found that were 2x-3x more productive using nbdev than using traditional programming tools. For me, this is a big surprise, since I have coded nearly every day for over 30 years, and in that time have tried dozens of tools, libraries, and systems for building programs. I didnt expect there could still be room for such a big job in productivity. Its made me feel excited about the future, because I suspect there could still be a lot of room to develop other ideas for developer productivity, and because Im looking forward to seeing what people build with nbdev. If you decide to give it a go, please let us know how you get along! And of course feel free to ask any questions. The best place for these discussions is this forum thread that weve created for nbdev. PRs are of course welcome in the nbdev GitHub repo. Thank you for taking an interest in our project! Acknowledgements: Thanks to Alexis Gallagher and Viacheslav Kovalevskyi for their helpful feedback on drafts of this article. Thanks to Andrew Shaw for helping to build prototypes of show_doc, and to Stas Bekman for much of the git hooks functionality. Thanks to Hamel Husain for helpful ideas for using GitHub Actions. "
        ],
        "story_type": "Normal",
        "url_raw": "https://www.fast.ai/2019/12/02/nbdev/",
        "comments.comment_id": [21681405, 21685964],
        "comments.comment_author": ["jph00", "thesorrow"],
        "comments.comment_descendants": [0, 1],
        "comments.comment_time": [
          "2019-12-02T10:54:41Z",
          "2019-12-02T19:35:14Z"
        ],
        "comments.comment_text": [
          "Hi folks - Sylvain Gugger and I created nbdev. Let me know if you have any questions about it.<p>Since nbdev is written in nbdev itself, you can also use its repo as an example project to see how it works. Another good example project written with nbdev is fastprogress: <a href=\"https://github.com/fastai/fastprogress\" rel=\"nofollow\">https://github.com/fastai/fastprogress</a>",
          "Looks great ! I don't know If you use Apache Airflow in ML or ETL pipelines but I wonder if one could use nbdev to generate Airflow Dags/Operator. There's already some work done to execute Notebooks[0] but would be cool to convert them to Python code!<p>[0] <a href=\"https://airflow.apache.org/docs/stable/howto/operator/papermill.html\" rel=\"nofollow\">https://airflow.apache.org/docs/stable/howto/operator/paperm...</a>"
        ],
        "id": "8c4befe5-8a78-4ced-806c-f5a2a322ae44",
        "url_text": "Written: 02 Dec 2019 by Jeremy Howard I really do think [nbdev] is a huge step forward for programming environments: Chris Lattner, inventor of Swift, LLVM, and Swift Playgrounds. My fast.ai colleague Sylvain Gugger and I have been working on a labor of love for the last couple of years. It is a Python programming environment called nbdev, which allows you to create complete python packages, including tests and a rich documentation system, all in Jupyter Notebooks. Weve already written a large programming library (fastai v2) using nbdev, as well as a range of smaller projects. Nbdev is a system for something that we call exploratory programming. Exploratory programming is based on the observation that most of us spend most of our time as coders exploring and experimenting. We experiment with a new API that we havent used before, to understand exactly how it behaves. We explore the behavior of an algorithm that we are developing, to see how it works with various kinds of data. We try to debug our code, by exploring different combinations of inputs. And so forth nbdev: exploratory programming Software development tools Interactive programming environments So whats missing in Jupyter Notebook? Dynamic Python What now nbdev: exploratory programming We believe that the very process of exploration is valuable in itself, and that this process should be saved so that other programmers (including yourself in six months time) can see what happened and learn by example. Think of it as something like a scientists journal. You can use it to show the things that you tried, what worked and what didnt, and what you did to develop your understanding of the system within which you are working. During this exploration, you will realize that some parts of this understanding are critical for the system to work. Therefore, the exploration should include the addition of tests and assertions to ensure this behavior. This kind of exploring is easiest when you develop on the prompt (or REPL), or using a notebook-oriented development system like Jupyter Notebooks. But these systems are not as strong for the programming part. This is why people use such systems mainly for early exploring, and then switch to an IDE or text editor later in a project. They switch to get features like good doc lookup, good syntax highlighting, integration with unit tests, and (critically!) the ability to produce final, distributable source code files, as opposed to notebooks or REPL histories. The point of nbdev is to bring the key benefits of IDE/editor development into the notebook system, so you can work in notebooks without compromise for the entire lifecycle. To support this kind of exploration, nbdev is built on top of Jupyter Notebook (which also means we get much better support for Pythons dynamic features than in a normal editor or IDE), and adds the following critically important tools for software development: Python modules are automatically created for you, following best practices such as automatically defining __all__ (more details) with your exported functions, classes, and variables Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks Automatically create searchable, hyperlinked documentation from your code; any word you surround in backticks will by hyperlinked to the appropriate documentation, a sidebar will be created for you in your documentation site with links to each of your modules, and more Pip installers (uploaded to pypi for you) Testing (defined directly in your notebooks, and run in parallel) Continuous integration Version control conflict handling Heres a snippet from our actual source code for nbdev, which is itself written in nbdev (of course!) Exploring the notebook file format in the nbdev source code As you can see, when you build software this way, everyone in your project team gets to benefit from the work you do in building an understanding of the problem domain, such as file formats, performance characteristics, API edge cases, and so forth. Since development occurs in a notebook, you can also add charts, text, links, images, videos, etc, that will be included automatically in the documentation of your library. The cells where your code is defined will be hidden and replaced by standardized documentation of your function, showing its name, arguments, docstring, and link to the source code on GitHub. For more information about features, installation, and how to use nbdev, see its documentation (which is, naturally, automatically generated from its source code). Ill be posting a step by step tutorial in the coming days. In the rest of this post, Ill describe more of the history and context behind the why: why did we build it, and why did we design it the way we did. First, lets talk about a little history (And if youre not interested in the history, you can skip ahead to Whats missing in Jupyter Notebook.) Most software development tools are not built from the foundations of thinking about exploratory programming. When I began coding, around 30 years ago, waterfall software development was used nearly exclusively. It seemed to me at the time that this approach, where an entire software system would be defined in minute detail upfront, and then coded as closely to the specification as possible, did not fit at all well with how I actually got work done. In the 1990s, however, things started to change. Agile development became popular. People started to understand the reality that most software development is an iterative process, and developed ways of working which respected this fact. However, we did not see major changes to the software development tools that we used, that matched the major changes to our ways of working. There were some pieces of tooling which got added to our arsenal, particularly around being able to do test driven development more easily. However this tooling tended to appear as minor extensions to existing editors and development environments, rather than truly rethinking what a development environment could look like. In recent years weve also begun to see increasing interest in exploratory testing as an important part of the agile toolbox. We absolutely agree! But we also think this doesnt go nearly far enough; we think in nearly every part of the software development process that exploration should be a central part of the story. The legendary Donald Knuth was way ahead of his time. He wanted to see things done very differently. In 1983 he developed a methodology called literate programming. He describes it as a methodology that combines a programming language with a documentation language, thereby making programs more robust, more portable, more easily maintained, and arguably more fun to write than programs that are written only in a high-level language. The main idea is to treat a program as a piece of literature, addressed to human beings rather than to a computer. For a long time I was fascinated by this idea, but unfortunately it never really went anywhere. The tooling available for working this way resulted in software development taking much longer, and very few people decided that this compromise was worth it. Nearly 30 years later another brilliant and revolutionary thinker, Bret Victor, expressed his deep discontent for the current generation of development tools, and described how to design a programming system for understanding programs. As he said in his groundbreaking speech Inventing on Principle: Our current conception of what a computer program is a list of textual definitions that you hand to a compiler thats derived straight from Fortran and ALGOL in the late 50s. Those languages were designed for punchcards. He laid out, and illustrated with fully worked examples, a range of new principles for designing programming systems. Whilst nobody has as yet fully implemented all of his ideas, there have been some significant attempts to implement some parts of them. Perhaps the most well-known and complete implementation, including inline display of intermediate results, is Chris Lattners Swift and Xcode Playgrounds. Demonstration of Playgounds in Xcode Whilst this is a big step forward, it is still very constrained by the basic limitations of sitting within a development environment which was not originally built with such explorations in mind. For instance, the exploration process is not captured by this at all, tests cannot be directly integrated into it, and the full rich vision of literate programming cannot be implemented. Interactive programming environments There has been another very different direction in software development, which is interactive programming (and the related live programming). It started many decades ago with the LISP and Forth REPLs, which allowed developers to interactively add and remove code in a running application. Smalltalk took things even further, providing a fully interactive visual workspace. In all these cases, the languages themselves were well suited to this kind of interactive work, for instance with LISPs macro system and code as data foundations. Live programming in Smalltalk (1980) Although this approach isnt how most regular software development is done today, it is the most popular approach in many areas of scientific, statistical, and other data-driven programming. (JavaScript front-end programming is however increasingly borrowing ideas from those approaches, such as hot reloading and in-browser live editing.) Matlab, for instance, started out as an entirely interactive tool back in the 1970s, and today is still widely used in engineering, biology, and various other areas (it also provides regular software development features nowadays). Similar approaches were used by SPLUS, and its open source cousin R, which is today extremely popular in the statistic and data visualization communities (amongst others). I got particularly excited when I first used Mathematica about 25 years ago. Mathematica looked to me like the closest thing Id seen to something that could support literate programming, without compromising on productivity. To do this, it used a notebook interface, which behaved a lot like a traditional REPL, but also allowed other types of information to be included, including charts, images, formatted text, outlining sections, and so forth. In fact, not only did it not compromise on productivity, but I found it actually allowed me to build things that previously were beyond me, because I could try algorithms out and immediately get feedback in a very visual way. In the end though, Mathematica didnt really help me build anything useful, because I couldnt distribute my code or applications to colleagues (unless they spent thousands of dollars for a Mathematica license to use it), and I couldnt easily create web applications for people to access from the browser. In addition, I found my Mathematica code would often end up much slower and more memory hungry than code I wrote in other languages. So you can imagine my excitement when Jupyter Notebook appeared on the scene. This used the same basic notebook interface as Mathematica (although, at first, with a small subset of the functionality) but was open source, and allowed me to write in languages that were widely supported and freely available. Ive been using Jupyter not just for exploring algorithms, APIs, and new research ideas, but also as a teaching tool at fast.ai. Many students have found that the ability to experiment with inputs and view intermediate results and outputs, as well as try out their own modifications, helped them to more fully and deeply understand the topics being discussed. We are also writing a book entirely using Jupyter Notebooks, which has been an absolute pleasure, allowing us to combine prose, code examples, hierarchical structured headings, and so forth, whilst ensuring that our sample outputs (including charts, tables, and images) always correctly match up to the code examples. In short: we have really enjoyed using Jupyter Notebook, we find that we do great work using it, and our students love it. But it just seemed like such a shame that we werent actually using it to build our software! So whats missing in Jupyter Notebook? Whilst Jupyter Notebook is great at the explorable part part of explorable programming, its not so great at the programming part. For instance, it doesnt really provide a way to do things like: Create modular reusable code, which can be run outside of Jupyter Creating hyperlinked searchable documentation Test code (including automatically through continuous integration) Navigate code Handle version control Because of this, people generally have to switch between a mix of poorly integrated tools, with significant friction as they move from tool to tool, to get the advantages of each: Development Pros Cons IDE/Editor Produces final distributable module Integration with doc lookup Integration with syntax highlighting and type-checking Non-interactive, so hard to explore Incomplete support of dynamic languages Documentation is text-only No facility for documenting a session of interaction, or explaining through example REPL/shell Good for small interactive explorations Bad for everything else, including producing distributable modules Traditional notebooks Mixes code, rich text, and images Explaining thru examples by recording a session of interaction Accurate code navigation and auto-completion for dynamic languages Same cons as REPL programming We decided that the best way to handle these things was to leverage great tools that already exist, where possible, and build our own where needed. For instance, for handling pull requests and viewing diffs, theres already a great tool: ReviewNB. When you look at graphical diffs in ReviewNB, you suddenly realize how much has been missing all this time in plain text diffs. For instance, what if a commit made your image generation blurry? Or made your charts appear without labels? You really know whats going on when you have that visual diff. Visual diff in ReviewNB, showing change to tabular output Many merge conflicts are avoided with nbdev, because it installs git hooks for you which strip out much of the metadata that causes those conflicts in the first place. If you get a merge conflict when you pull from git, just run nbdev_fix_merge. With this command, nbdev will simply use your cell outputs where there are conflicts in outputs, and if there are conflicts in cell inputs, then both cells are included in the final notebook, along with conflict markers so you can easily find them and fix them directly in Jupyter. An example of a cell-based nbdev merge conflict Modular reusable code is created by nbdev by simply creating standard Python modules. nbdev looks for special comments in code cells, such as #export, which indicates that a cell should be exported to a python module. Each notebook is associated with a particular python module by using a special comment at the start of the notebook. A documentation site (using Jekyll, so supported directly by GitHub Pages) is automatically built from the notebooks and special comments. We wrote our own documentation system, since existing approaches such as Sphinx didnt provided all the features that we needed. For code navigation, there are already wonderful features built into most editors and IDEs, such as vim, Emacs, and vscode. And as a bonus, GitHub even supports code navigation directly in its web interface now (in beta, in selected projects, such as fastai)! So weve ensured that the code that nbdev exports can be navigated and edited directly in any of these systems - and that any edits can be automatically synchronized back with the notebooks. For testing, weve written our own simple library and command line tool. Tests are written directly in notebooks, as part of the exploration and development (and documentation) process, and the command line tool runs tests in all notebooks in parallel. The natural statefulness of notebooks turns out to be a really great way to develop both unit tests and integration tests. Rather than having special syntax to learn to create test suites, you just use the regular collection and looping constructs in python. So theres much fewer new concepts to learn. These tests can also be run in your normal continuous integration tools, and they provide clear information about the source of any test errors that come up. The default nbdev template includes integration with GitHub Actions for continuous integration and other features (PRs for other platforms are welcome). Dynamic Python One of the challenges in fully supporting Python in a regular editor or IDE is that Python has particularly strong dynamic features. For instance, you can add methods to a class at any time, you can change the way that classes are created and how they work by using the metaclass system, and you can change how functions and methods behave by using decorators. Microsoft developed the Language Server Protocol, which can be used by development environments to get information about the current file and project necessary for auto-completions, code navigation, and so forth. However, with a truly dynamic language like python, such information will always just be guesses, since actually providing the correct information would require running the python code itself (which it cant really do, for all kinds of reasons - for instance the code may be in a state while youre writing it that actually deletes all your files!) On the other hand, a notebook contains an actual running Python interpreter instance that youre fully in control of. So Jupyter can provide auto-completions, parameter lists, and context-sensitive documentation based on the actual state of your code. For instance, when using Pandas we get tab completion of all the column names of our DataFrames. Weve found that this feature of Jupyter Notebook makes exploratory programming significantly more productive. We havent needed to change anything to make it work well in nbdev; its just part of the great features of Jupyter that we get for free by building on that platform. What now In conjunction with developing nbdev, weve been writing fastai v2 from scratch entirely in nbdev. fastai v2 provides a rich, well-structured API for building deep learning models. It will be released in the first half of 2020. Its already feature complete, and early adopters are already building cool projects with the pre-release version. Weve also written other projects in fastai v2, some of which will be released in the coming weeks. Weve found that were 2x-3x more productive using nbdev than using traditional programming tools. For me, this is a big surprise, since I have coded nearly every day for over 30 years, and in that time have tried dozens of tools, libraries, and systems for building programs. I didnt expect there could still be room for such a big job in productivity. Its made me feel excited about the future, because I suspect there could still be a lot of room to develop other ideas for developer productivity, and because Im looking forward to seeing what people build with nbdev. If you decide to give it a go, please let us know how you get along! And of course feel free to ask any questions. The best place for these discussions is this forum thread that weve created for nbdev. PRs are of course welcome in the nbdev GitHub repo. Thank you for taking an interest in our project! Acknowledgements: Thanks to Alexis Gallagher and Viacheslav Kovalevskyi for their helpful feedback on drafts of this article. Thanks to Andrew Shaw for helping to build prototypes of show_doc, and to Stas Bekman for much of the git hooks functionality. Thanks to Hamel Husain for helpful ideas for using GitHub Actions. ",
        "_version_": 1718536548162469888
      },
      {
        "story_id": 19274083,
        "story_author": "stanzheng",
        "story_descendants": 24,
        "story_score": 34,
        "story_time": "2019-02-28T19:06:27Z",
        "story_title": "What Is Dark?",
        "search": [
          "What Is Dark?",
          "https://medium.com/darklang/the-design-of-dark-59f5d38e52d2",
          "Dark is a holistic programming language, structured editor, and infrastructure, for building backend web services. Its aimed at frontend, backend, and mobile engineers.Our goal is to make coding 100x easier, which we believe will allow a billion people to build software. This is a huge challenge, that requires a significant rethink of how we write software.This post is to talk about how we think about the world. Were introducing our core philosophy, high level direction, and our goals. Were going to go into more specifics in future posts (see bottom; or get notified by following Dark or via the mailing list).Accidental complexityThe defining principle behind Dark is that we want to remove all accidental complexity from coding. Building applications today is incredibly complex, and great deal of that complexity is accidental.The phrase accidental complexity (or incidental complexity) comes from an essay called No Silver Bullet. In it, Fred Brooks divided complexity into essential complexity, which is core of the business problem youre solving, and accidental complexity, which is everything else you have to do to make software.He pointed out that in order to improve productivity 10x, we have to remove 90% of the things that we do. At the time, he felt that software development wasnt 90% accidental complexity, and so no 10x improvement was possible. Hence the name No Silver Bullet.When I look around software today, 90% seems like a low estimate. The amount of accidental complexity is out of control. Look at Its the Future, and How it feels to learn JavaScript in 2016. Why do we need to know how to use all these tools to write software?Fundamentally, writing software is just receiving data, manipulating it, storing it, and sending it somewhere easy concepts that we learn in our first programming tutorials. Why is it that to build an application today we need to learn Kubernetes, Docker, Git, load balancers, dozens of AWS services, SQL, NoSQL, Kafka, Unix, GraphQL, gRPC, npm, Heroku, DNS, memcached, JWT, Nginx, and the rest of the endless list of tools and technologies that each provide one part of an application?Complexity is out of control, and it hasnt shown any signs of slowing down. Its already at the point where frontend developers feel they cant write backends. They have so much of their own complexity to deal with that they cant find space for the backend complexity. How did we let this happen?A holistic languageThe core problem here is that the tools we have been building as an industry are incremental. This is natural: you build a new tool to scratch an itch, to solve known problems, and you do it in a way that can be adopted next to users existing tools. Do one thing and do it well is the core of the Unix philosophy that weve been following since the 70s.Heres the evolution of common tools in our industry:RCS CVS Subversion GitServers Shared unix boxes Virtual-private servers Heroku Lambda or KubernetesNightly build script BuildBot Jenkins CircleCIEd Vi Vim Atom/Sublime/VSCode/etcEach new tool improves on the previous tool, but fills the same space without removing concepts and complexity. In fact, they often increase complexity: Git is a better tool than Subversion, but Subversion was a lot simpler.Decreasing complexityInstead of building a better version of existing stuff, the way to decrease complexity is to entirely remove things.If we build a tool that does two things, it removes not just the interface, but also reduces the surface area of both problems. If we bring four or eight tools together, we can eliminate huge swathes of complexity, interfacing and overlap between the tools.Dark is holistic: it combines an editor with a language and infrastructure. It is not a general-purpose language, or a general-purpose editor, or general-purpose infrastructure. Each piece is designed specifically to work with the other pieces, and this tight integration allows us cut out a great deal of that complexity:Infrastructure complexityDeployment complexityAPI complexityCode-as-text complexityInfrastructure complexityInfrastructure complexity is everything involved in working with the machines on which we run our code, as opposed to just working with data and customers. This includes Docker and Kubernetes, EC2 and the entire AWS/GCP/Azure ecosystem, dealing with queues, networking, firewalls, load balancers, service discovery, scaling, monitoring, security, DBs, sharding, and optimization.In Dark we run the infrastructure for you. Because we deeply understand your application code, traffic, and data we can make good decisions about the architecture you need. As such, we can cut out all of the infrastructure choices: you dont need to handle machines, orchestration, storage, databases, or queues.In addition, we should be able to make great scaling decisions automatically we think of Dark as an infrastructure compiler, compiling the ideal distributed system for your application. We expect to be able to abstract and simplify in ways that Lambda, Kubernetes or Heroku cant.Take databases for example. Today, an application and its database need to be carefully managed together, despite being completely different in terms of operational concerns, languages, formats, and operator skillsets. Combining a database with its application removes many questions that neither component can answer by itself, such as what indexes are needed, how to shard, the best way to write queries, and how to manage long-running data migrations.Similarly, interfacing with a database from an application is complex: I need to think about whether the types line up, how my ORM might fail, how it will scale, what indexes it needs. Why cant I just store data, query data, and access data when I need it?The tight integration of language, editor and infrastructure also allows us to provide observability into your live production systems right from the Dark editor, which completely changes how people code and build systems. It also removes the need for separate monitoring, tracing, instrumentation, logging, and error handling.Deployment complexityDeployment complexity is the problem of syncing finished code from one machine to another. This is one of those things that should be trivial, but doing it safely and quickly causes so much trouble that I built CircleCI to help solve it.Companies dedicate massive resources to reducing the end-to-end time of shipping a change to customers. But the act of shipping code involves necessary steps which take time. Packaging code (Docker container, tarball, webpack, AMI, git checkout, jars), testing it (CircleCI, code coverage, browser testing/Selenium), syncing it (git push to Heroku, Docker registries, artifact hosting, S3, CDNs), enabling the new code (Kubernetes, reverse proxies, Capistrano, swapping symlinks), and rolling it out (feature flags, blue-green deploys, DB migrations, API versioning), involves not just massive complexity and tooling, but also unavoidable wall-clock time.In Dark, we redesign the deployment process to not require these steps. Instead, deployment is trivial. As soon as you write code in the Dark editor, its immediately deployed to production, ready to be enabled with a feature flag. There is no long build process, no tarballs, no containers. Its a simple diff from your editor to our infrastructure.Dark is designed to make this process safe. Routes, APIs, and queues are all statically typed, and every function and type is versioned and immutable. Deployment safety features, such as feature flags, unit tests, and powerful database migrations, are built into the tooling, language, and editor. Even though a deploy takes only ~50ms, deployments are significantly less risky in Dark than they are in modern toolchains.API complexityAPI complexity comes from how much harder it is to use an API vs calling a function. Using APIs should be about as easy as calling a function, but we have to deal with authentication, rate limiting, error handling, retries. The way to handle these is different in every API, from auth type to protocol (HTTP+JSON / REST / Protobuf / gRPC / GraphQL / SOAP) to calling conventions, and there are millions of APIs in the world to deal with (and growing).In Dark, making an API call is as simple as a function call. Our tools for building connectors to 3rd party services handle rate limiting, authentication, error handling, and retries, with great default behaviour. Visibility around costs and failures are built into the editor, as is support for secret keys and Personally Identifying Information.Code-as-text complexityCode-as-text complexity is a bit of a catch all. It refers to all the miscellaneous problems we have around how we write code and the tooling around it.An obvious example is syntax errors. Syntax errors exist because we write code as text, and then ask the compiler to read it, which sometimes it cannot. To never ever have a syntax error again, we must have an editor that doesnt allow them to be added one that speaks the language, not just plain text.This concept can be extended to many parts of the tool chain. We rename functions with our editors search-and-replace functions, which dont understand the difference between a function and a variable; we merge branches with Git, which doesnt understand the syntax or semantics of our code. We write code in Python or Node or Rust, but also have to consider an entire operating system running under it. We use dozens of text-based tools we barely understand, such as git, bash, grep, sed because none of our tools actually understand our code.The Dark language is built hand-in-hand with a structured editor and infrastructure. As such, the autocomplete is aware of your whole program, and syntax errors arent possible. Refactoring tools are built in, as are collaboration and access controls. Version control is first-class, and is tightly integrated with the rest of the language, including feature flags, and function and type versioning.Who are we building Dark for?Long term, were building Dark for everybody experienced coders, new coders, and non-coders but we need to start smaller than that. Right now, were building Dark for existing developers who want to build backends, targeting two major areas.The first is backends for client apps, such as mobile apps, or single page web apps written in React, Vue, Angular, etc. If youre a frontend/mobile developer who is dissatisfied with how hard it is to make a backend, youll probably like Dark.The second is building new services in existing microservice-based architectures. Building a new service means signing up for a future of yaml files, deployment pipelines, 2am pages, zero-day security alerts, and eternal maintenance. If youre looking at new languages and tools (such as Lambda) to reduce this burden, then Dark should help. Thats what Dark is built for, allowing you to build backend services with little-to-no scaling and maintenance overhead.Naturally, developers making brand new web startups or side projects should also have a great time with Dark. On the other hand, if youre building stuff like embedded systems, theorem provers, or machine learning models, Dark wont support this sort of application for many years.Dark has a mission to bring coding to a billion people. Our belief is that we need to start with existing developers and expand out: if we make coding for existing developers 100x easier, then we significantly lower the barrier to entry for everyone whos trying to learn to code, or people in coding-adjacent jobs, like designers, PMs, analysts, and information workers in general.Learn more about DarkThis has been an introduction to how we think about the world with Dark. We plan to release many more details and plans over the next few months as we get closer to a public release. Heres are some posts were working on:The problem with textHow did the world get so complex? (Rails monoliths vs microservices)The downsides of Dark open source, self hosting, and potential lock-inThings like and unlike Dark (comparison to similar concepts like Eve, Glitch, Smalltalk, or Bret Victors work)How Dark deploys in 50msDont invent anything new and Innovate at the boundariesImplicit vs Explicit in Dark (dynamic vs static typing)Real problems with functional languagesWhy Dark isnt Low Code or No CodeDesigning a string type for a Unicode worldA retrospective on our experiments with a graph-based languageWhy make our own language?If theres one youd like us to prioritize, or any questions about Dark, say hi on Twitter. You should also join our beta, and follow us on Twitter or Medium or via RSS, or follow me or Ellen on Twitter.Thanks to Corey Montella, Dan Bentley, Evan Conrad, Geoffrey Litt, James Tamplin, Jeff Dickey, Jennifer Wei, Justin Poirier, Mike Plotz, Paul Stefan Ort, Ramin Keene, Russell Smith, Steve Krouse and Vaibhav Sagar for reviewing drafts of this post. "
        ],
        "story_type": "Normal",
        "url_raw": "https://medium.com/darklang/the-design-of-dark-59f5d38e52d2",
        "comments.comment_id": [19274825, 19275116],
        "comments.comment_author": ["Alex3917", "shalabhc"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-02-28T20:27:53Z",
          "2019-02-28T20:58:25Z"
        ],
        "comments.comment_text": [
          "I think this is fundamentally the correct solution to the problem, so I'm excited to see how this develops.<p>What's the business model? E.g. open tooling, pay for hosting/monitoring/optimization?",
          "Looks promising! Since this subsumes the database - what kind of transnational data consistency is provided? How do upgrades to your data model work?"
        ],
        "id": "0b23de7a-4477-41be-a500-d19169c2671d",
        "url_text": "Dark is a holistic programming language, structured editor, and infrastructure, for building backend web services. Its aimed at frontend, backend, and mobile engineers.Our goal is to make coding 100x easier, which we believe will allow a billion people to build software. This is a huge challenge, that requires a significant rethink of how we write software.This post is to talk about how we think about the world. Were introducing our core philosophy, high level direction, and our goals. Were going to go into more specifics in future posts (see bottom; or get notified by following Dark or via the mailing list).Accidental complexityThe defining principle behind Dark is that we want to remove all accidental complexity from coding. Building applications today is incredibly complex, and great deal of that complexity is accidental.The phrase accidental complexity (or incidental complexity) comes from an essay called No Silver Bullet. In it, Fred Brooks divided complexity into essential complexity, which is core of the business problem youre solving, and accidental complexity, which is everything else you have to do to make software.He pointed out that in order to improve productivity 10x, we have to remove 90% of the things that we do. At the time, he felt that software development wasnt 90% accidental complexity, and so no 10x improvement was possible. Hence the name No Silver Bullet.When I look around software today, 90% seems like a low estimate. The amount of accidental complexity is out of control. Look at Its the Future, and How it feels to learn JavaScript in 2016. Why do we need to know how to use all these tools to write software?Fundamentally, writing software is just receiving data, manipulating it, storing it, and sending it somewhere easy concepts that we learn in our first programming tutorials. Why is it that to build an application today we need to learn Kubernetes, Docker, Git, load balancers, dozens of AWS services, SQL, NoSQL, Kafka, Unix, GraphQL, gRPC, npm, Heroku, DNS, memcached, JWT, Nginx, and the rest of the endless list of tools and technologies that each provide one part of an application?Complexity is out of control, and it hasnt shown any signs of slowing down. Its already at the point where frontend developers feel they cant write backends. They have so much of their own complexity to deal with that they cant find space for the backend complexity. How did we let this happen?A holistic languageThe core problem here is that the tools we have been building as an industry are incremental. This is natural: you build a new tool to scratch an itch, to solve known problems, and you do it in a way that can be adopted next to users existing tools. Do one thing and do it well is the core of the Unix philosophy that weve been following since the 70s.Heres the evolution of common tools in our industry:RCS CVS Subversion GitServers Shared unix boxes Virtual-private servers Heroku Lambda or KubernetesNightly build script BuildBot Jenkins CircleCIEd Vi Vim Atom/Sublime/VSCode/etcEach new tool improves on the previous tool, but fills the same space without removing concepts and complexity. In fact, they often increase complexity: Git is a better tool than Subversion, but Subversion was a lot simpler.Decreasing complexityInstead of building a better version of existing stuff, the way to decrease complexity is to entirely remove things.If we build a tool that does two things, it removes not just the interface, but also reduces the surface area of both problems. If we bring four or eight tools together, we can eliminate huge swathes of complexity, interfacing and overlap between the tools.Dark is holistic: it combines an editor with a language and infrastructure. It is not a general-purpose language, or a general-purpose editor, or general-purpose infrastructure. Each piece is designed specifically to work with the other pieces, and this tight integration allows us cut out a great deal of that complexity:Infrastructure complexityDeployment complexityAPI complexityCode-as-text complexityInfrastructure complexityInfrastructure complexity is everything involved in working with the machines on which we run our code, as opposed to just working with data and customers. This includes Docker and Kubernetes, EC2 and the entire AWS/GCP/Azure ecosystem, dealing with queues, networking, firewalls, load balancers, service discovery, scaling, monitoring, security, DBs, sharding, and optimization.In Dark we run the infrastructure for you. Because we deeply understand your application code, traffic, and data we can make good decisions about the architecture you need. As such, we can cut out all of the infrastructure choices: you dont need to handle machines, orchestration, storage, databases, or queues.In addition, we should be able to make great scaling decisions automatically we think of Dark as an infrastructure compiler, compiling the ideal distributed system for your application. We expect to be able to abstract and simplify in ways that Lambda, Kubernetes or Heroku cant.Take databases for example. Today, an application and its database need to be carefully managed together, despite being completely different in terms of operational concerns, languages, formats, and operator skillsets. Combining a database with its application removes many questions that neither component can answer by itself, such as what indexes are needed, how to shard, the best way to write queries, and how to manage long-running data migrations.Similarly, interfacing with a database from an application is complex: I need to think about whether the types line up, how my ORM might fail, how it will scale, what indexes it needs. Why cant I just store data, query data, and access data when I need it?The tight integration of language, editor and infrastructure also allows us to provide observability into your live production systems right from the Dark editor, which completely changes how people code and build systems. It also removes the need for separate monitoring, tracing, instrumentation, logging, and error handling.Deployment complexityDeployment complexity is the problem of syncing finished code from one machine to another. This is one of those things that should be trivial, but doing it safely and quickly causes so much trouble that I built CircleCI to help solve it.Companies dedicate massive resources to reducing the end-to-end time of shipping a change to customers. But the act of shipping code involves necessary steps which take time. Packaging code (Docker container, tarball, webpack, AMI, git checkout, jars), testing it (CircleCI, code coverage, browser testing/Selenium), syncing it (git push to Heroku, Docker registries, artifact hosting, S3, CDNs), enabling the new code (Kubernetes, reverse proxies, Capistrano, swapping symlinks), and rolling it out (feature flags, blue-green deploys, DB migrations, API versioning), involves not just massive complexity and tooling, but also unavoidable wall-clock time.In Dark, we redesign the deployment process to not require these steps. Instead, deployment is trivial. As soon as you write code in the Dark editor, its immediately deployed to production, ready to be enabled with a feature flag. There is no long build process, no tarballs, no containers. Its a simple diff from your editor to our infrastructure.Dark is designed to make this process safe. Routes, APIs, and queues are all statically typed, and every function and type is versioned and immutable. Deployment safety features, such as feature flags, unit tests, and powerful database migrations, are built into the tooling, language, and editor. Even though a deploy takes only ~50ms, deployments are significantly less risky in Dark than they are in modern toolchains.API complexityAPI complexity comes from how much harder it is to use an API vs calling a function. Using APIs should be about as easy as calling a function, but we have to deal with authentication, rate limiting, error handling, retries. The way to handle these is different in every API, from auth type to protocol (HTTP+JSON / REST / Protobuf / gRPC / GraphQL / SOAP) to calling conventions, and there are millions of APIs in the world to deal with (and growing).In Dark, making an API call is as simple as a function call. Our tools for building connectors to 3rd party services handle rate limiting, authentication, error handling, and retries, with great default behaviour. Visibility around costs and failures are built into the editor, as is support for secret keys and Personally Identifying Information.Code-as-text complexityCode-as-text complexity is a bit of a catch all. It refers to all the miscellaneous problems we have around how we write code and the tooling around it.An obvious example is syntax errors. Syntax errors exist because we write code as text, and then ask the compiler to read it, which sometimes it cannot. To never ever have a syntax error again, we must have an editor that doesnt allow them to be added one that speaks the language, not just plain text.This concept can be extended to many parts of the tool chain. We rename functions with our editors search-and-replace functions, which dont understand the difference between a function and a variable; we merge branches with Git, which doesnt understand the syntax or semantics of our code. We write code in Python or Node or Rust, but also have to consider an entire operating system running under it. We use dozens of text-based tools we barely understand, such as git, bash, grep, sed because none of our tools actually understand our code.The Dark language is built hand-in-hand with a structured editor and infrastructure. As such, the autocomplete is aware of your whole program, and syntax errors arent possible. Refactoring tools are built in, as are collaboration and access controls. Version control is first-class, and is tightly integrated with the rest of the language, including feature flags, and function and type versioning.Who are we building Dark for?Long term, were building Dark for everybody experienced coders, new coders, and non-coders but we need to start smaller than that. Right now, were building Dark for existing developers who want to build backends, targeting two major areas.The first is backends for client apps, such as mobile apps, or single page web apps written in React, Vue, Angular, etc. If youre a frontend/mobile developer who is dissatisfied with how hard it is to make a backend, youll probably like Dark.The second is building new services in existing microservice-based architectures. Building a new service means signing up for a future of yaml files, deployment pipelines, 2am pages, zero-day security alerts, and eternal maintenance. If youre looking at new languages and tools (such as Lambda) to reduce this burden, then Dark should help. Thats what Dark is built for, allowing you to build backend services with little-to-no scaling and maintenance overhead.Naturally, developers making brand new web startups or side projects should also have a great time with Dark. On the other hand, if youre building stuff like embedded systems, theorem provers, or machine learning models, Dark wont support this sort of application for many years.Dark has a mission to bring coding to a billion people. Our belief is that we need to start with existing developers and expand out: if we make coding for existing developers 100x easier, then we significantly lower the barrier to entry for everyone whos trying to learn to code, or people in coding-adjacent jobs, like designers, PMs, analysts, and information workers in general.Learn more about DarkThis has been an introduction to how we think about the world with Dark. We plan to release many more details and plans over the next few months as we get closer to a public release. Heres are some posts were working on:The problem with textHow did the world get so complex? (Rails monoliths vs microservices)The downsides of Dark open source, self hosting, and potential lock-inThings like and unlike Dark (comparison to similar concepts like Eve, Glitch, Smalltalk, or Bret Victors work)How Dark deploys in 50msDont invent anything new and Innovate at the boundariesImplicit vs Explicit in Dark (dynamic vs static typing)Real problems with functional languagesWhy Dark isnt Low Code or No CodeDesigning a string type for a Unicode worldA retrospective on our experiments with a graph-based languageWhy make our own language?If theres one youd like us to prioritize, or any questions about Dark, say hi on Twitter. You should also join our beta, and follow us on Twitter or Medium or via RSS, or follow me or Ellen on Twitter.Thanks to Corey Montella, Dan Bentley, Evan Conrad, Geoffrey Litt, James Tamplin, Jeff Dickey, Jennifer Wei, Justin Poirier, Mike Plotz, Paul Stefan Ort, Ramin Keene, Russell Smith, Steve Krouse and Vaibhav Sagar for reviewing drafts of this post. ",
        "_version_": 1718536454024462336
      },
      {
        "story_id": 20531039,
        "story_author": "pizza",
        "story_descendants": 380,
        "story_score": 396,
        "story_time": "2019-07-25T23:51:30Z",
        "story_title": "“My GitHub account has been restricted due to US sanctions as I live in Crimea”",
        "search": [
          "“My GitHub account has been restricted due to US sanctions as I live in Crimea”",
          "https://github.com/tkashkin/GameHub/issues/289",
          "Discoverability is also a very important factor. I don't think many people will find GameHub on a self-hosted server somewhere and I don't think many of them will report issues there either. I also think that this is a very important aspect. Reporting a bug with your pre-existing account that you use for many other things is very different from subscribing somewhere obscure (and anything less famous than Github can be considered obscure when one has to register; hence everything), filling in your e-mail address, waiting for the confirmation e-mail, and then reporting a bug where you expect fewer people will comment. On the one hand, I really hope the Github restrictions are manageable, but on the other hand working with suboptimal tools and the risk of having even more drastic restrictions on your project in the future really is not motivating. And given the limitations you already noticed, it doesn't look particularly encouraging with Github. I suppose Gitlab and Bitbucket are the best alternatives in terms of number of users, but would be surprised if the user base was even remotely comparable to Github's. There is also https://teknik.io/, which I'm pretty sure is not hosted in the US, but I don't remember where are the servers and could not find the information now. Alternatively, are there any tools out there to automatically mirror a project on two different platforms, and maybe synchronize issues too? This would allow moving the project on a platform where you have no restrictions, but have a tool automating synchronization on Github to keep it running here too and keep the user base. You would still get the Github coverage, the project would look like it is maintained here, and it would, but in case the restrictions get worse in the future, everything would be on your other repository too, including issues. This is probably stupid, excuse my ignorance. Or something like this, if it's not too much extra hassle: https://moox.io/blog/keep-in-sync-git-repos-on-github-gitlab-bitbucket/ "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/tkashkin/GameHub/issues/289",
        "url_text": "Discoverability is also a very important factor. I don't think many people will find GameHub on a self-hosted server somewhere and I don't think many of them will report issues there either. I also think that this is a very important aspect. Reporting a bug with your pre-existing account that you use for many other things is very different from subscribing somewhere obscure (and anything less famous than Github can be considered obscure when one has to register; hence everything), filling in your e-mail address, waiting for the confirmation e-mail, and then reporting a bug where you expect fewer people will comment. On the one hand, I really hope the Github restrictions are manageable, but on the other hand working with suboptimal tools and the risk of having even more drastic restrictions on your project in the future really is not motivating. And given the limitations you already noticed, it doesn't look particularly encouraging with Github. I suppose Gitlab and Bitbucket are the best alternatives in terms of number of users, but would be surprised if the user base was even remotely comparable to Github's. There is also https://teknik.io/, which I'm pretty sure is not hosted in the US, but I don't remember where are the servers and could not find the information now. Alternatively, are there any tools out there to automatically mirror a project on two different platforms, and maybe synchronize issues too? This would allow moving the project on a platform where you have no restrictions, but have a tool automating synchronization on Github to keep it running here too and keep the user base. You would still get the Github coverage, the project would look like it is maintained here, and it would, but in case the restrictions get worse in the future, everything would be on your other repository too, including issues. This is probably stupid, excuse my ignorance. Or something like this, if it's not too much extra hassle: https://moox.io/blog/keep-in-sync-git-repos-on-github-gitlab-bitbucket/ ",
        "comments.comment_id": [20531466, 20531526],
        "comments.comment_author": ["kragen", "dngray"],
        "comments.comment_descendants": [24, 4],
        "comments.comment_time": [
          "2019-07-26T01:14:06Z",
          "2019-07-26T01:23:56Z"
        ],
        "comments.comment_text": [
          "During the Cold War, the primary thing that distinguished the US side from the USSR side was that, while speech by USSR people and people sympathetic to the USSR (like, say, CPUSA) was published in the US, publishing speech sympathetic to the US in the USSR would get you arrested.<p>It seems like the US has become what the USSR was: companies who publish speech from people in Iran or Crimea are now violating the law by doing so.  Moreover, in cases like this one, already-published material is being destroyed, in a way that wasn't practical in the USSR, due to technological limitations (though they did try — famously Beria was airbrushed out of official photographs).  The US doesn't yet have massive prison camp systems full of political dissidents — but then, for the first couple of decades, neither did the USSR.<p>This is a crucially important reason to move away from centralized and proprietary systems and onto decentralized, peer-to-peer, secure, free-software systems.<p>Learn from history.<p>(Edit: previously I gave Noam Chomsky as an example of someone sympathetic to the USSR, but that is at best debatable.  The Communist Party USA, now in its 101st year, is a much clearer example, one that published continuously throughout the Cold War despite US prosecutions of its leadership.)",
          "I'm from a US-friendly nation but I get terribly sad when I hear things like this.<p>Contribution to the free software world should transcend where someone is from whether it be China, Russia, Iran or whatever  new \"enemy\" our leaders decide.<p>As much as I dislike Facebook for privacy related reasons, maybe it's time that more tech companies setup a Tor Hidden Service and not ask where people are from.<p>Could this be used to protect them against having to implement sanctions against various countries if they don't actually know where their users are from?<p>It would also make blocking those websites difficult by repressive regimes too. They could implement the ability to upload a private PGP key so email notifications can be encrypted end-to-end, Facebook also allows for this.<p>The impending bifurcation of the Internet is something that I hate to think of, but it seems something a lot of governments are hell bent on, particularly with their insistence on \"backdoors in encryption\". One country's back door will be another country's vulnerability.<p><i>Now I think I will go and and donate to Tor Project to make myself feel better</i>."
        ],
        "id": "20e19cff-8e77-4041-af32-964ac07f7475",
        "_version_": 1718536504732549120
      },
      {
        "story_id": 18808909,
        "story_author": "louis-paul",
        "story_descendants": 391,
        "story_score": 332,
        "story_time": "2019-01-02T18:57:37Z",
        "story_title": "Monorepos: Please don’t",
        "search": [
          "Monorepos: Please don’t",
          "https://medium.com/@mattklein123/monorepos-please-dont-e9a279be011b",
          "Here we are at the beginning of 2019 and Im engaged in yet another discussion on the merits (or lack thereof) of keeping all of an organizations code in a monorepo. For those of you not familiar with this concept, the idea behind a monorepo is to store all code in a single version control system (VCS) repository. The alternative, of course, is to store code split into many different VCS repositories, usually on a service/application/library basis. For the purpose of this post I will call the multiple repository solution a polyrepo.Some of techs biggest names use a monorepo, including Google, Facebook, Twitter, and others. Surely if these companies all use a monorepo, the benefits must be tremendous, and we should all do the same, right? Wrong! As the title says: please, do not use a monorepo! Why? Because, at scale, a monorepo must solve every problem that a polyrepo must solve, with the downside of encouraging tight coupling, and the additional herculean effort of tackling VCS scalability. Thus, in the medium to long term, a monorepo provides zero organizational benefits, while inevitably leaving some of an organizations best engineers with a wicked case of PTSD (manifested via drooling and incoherent mumbling about git performance internals).A quick note: what do I mean by at scale? There is no definitive answer to this question, but because I know I will be asked, lets say for the sake of discussion that at scale means over 100 developers writing code full time.Theoretical monorepo benefits and why they cannot be achieved without polyrepo-style tooling (or are a lie)Theoretical benefit 1: Easier collaboration and code sharingMonorepo proponents will argue that when all code is present within a single repository, the likelihood of code duplication is small, and its more likely that different teams will collaborate together on shared infrastructure.Here is the ugly truth about even a medium size monorepo (and this will be a recurring theme throughout this section): it quickly becomes unreasonable for a single developer to have the entire repository on their machine, or to search through it using tools like grep. Thus, any monorepo that hopes to scale must provide two things:Some type of virtual file system (VFS) that allows a portion of the code to be present locally. This might be accomplished via a proprietary VCS like Perforce which natively operates this way, via Googles G3 internal tooling, or Microsofts GVFS.Sophisticated source code indexing/searching/discovery capabilities as a service. Since no individual developer is going to have all code in a searchable state, its critical that there exists some capability to perform a search across the entire codebase.Given that a developer will only access small portions of the codebase at a time, is there any real difference between checking out a portion of the tree via a VFS or checking out multiple repositories? There is no difference.In terms of source code indexing/searching/discovery capabilities, a tool can trivially iterate over many repositories and collate the results. In fact, this is how GitHubs own searching capabilities work as well as newer and more sophisticated indexing and collaboration tools such as Sourcegraph.Thus, in terms of collaboration and code sharing, at scale, developers are exposed to subsections of code through higher layer tooling. Whether the code is in a monorepo or polyrepo is irrelevant; the problem being solved is the same, and the efficacy of collaboration and code sharing has everything to do with engineering culture and nothing to do with code storage.Theoretical benefit 2: Single build / no dependency managementThe next thing that monorepo proponents will typically say is that by having all code in a single repository, there is no need for dependency management because all source code is built at the same time. This is a lie! At scale, there is simply no way to rebuild the entirety of a codebase and run all automated tests when each change is submitted (or, more importantly and more often, in CI when a change is proposed). To deal with this problem, all of the large monorepos have developed sophisticated build systems (see Bazel/Blaze from Google and Buck from Facebook as examples) that are designed in such a way as to internally track dependencies and build a directed acyclic graph (DAG) of the source code. This DAG allows for efficient build and test caching such that only code that changes, or code that depends on it, needs to be built and tested.Furthermore, because code that is built must actually be deployed, and not all software is deployed at the same time, it is essential that build artifacts are carefully tracked so that previously deployed software can be redeployed to new hosts as needed. This reality means that even in a monorepo world, multiple versions of code exist at the same time in the wild, and must be carefully tracked and reconciled.Monorepo proponents will argue that even with the large amount of build/dependency tracking required, there is still substantial benefit because a single commit/SHA describes the entire state of the world. I would argue this benefit is dubious; given the DAG that already exists, its a trivial leap to include individual repository SHAs as part of the DAG, and in fact, Bazel can seamlessly work across repositories or within a single repository, abstracting the underlying layout from the developer. Furthermore, automated refactor tooling can trivially be built that automatically bumps dependent library versions across many repositories, thus blurring the difference between a monorepo and polyrepo in this area (more on this below).The end result is that the realities of build/deploy management at scale are largely identical whether using a monorepo or polyrepo. The tools dont care, and neither should the developers writing code.Theoretical benefit 3: Code refactors are easy / atomic commitsThe final benefit that monorepo proponents typically tout is the fact that when all code is in a single repository, it makes code refactors much easier, due to ease of searching and the idea that a single atomic commit can span the entire codebase. This is a fallacy for multiple reasons:As described above, at scale, a developer will not be able to easily edit or search the entirety of the codebase on their local machine. Thus, the idea that one can clone all of the code and simply do a grep/replace is not trivial in practice.If we assume that via a sophisticated VFS a developer can clone and edit the entire codebase, the next question is how often does that actually happen? Im not talking about fixing a bug in an implementation of a shared library, as this type of fix is identically carried out whether using a monorepo or polyrepo (assuming similar build/deploy tooling as described in the previous section). Im talking about a library API change that has follow-on build breakage effects for other code. In very large code bases, it is likely impossible to make a change to a fundamental API and get it code reviewed by every affected team before merge conflicts force the process to start over again. Developers are faced with two realistic choices. First, they can give up, and work around the API issue (this happens more often than we would like to admit). Second, they can deprecate the existing API, implement a new API, and then go through the laborious process of individual deprecation changes throughout the codebase. Either way, this is exactly the same process undertaken in a polyrepo.In a service oriented world, applications are now composed of many loosely coupled services that interact with each other using some type of well specified API. Larger organizations inevitably migrate to an IDL such as Thrift or Protobuf that allow for type-safe APIs and backwards compatible changes. As described in the previous section on build/deploy management, code is not deployed at the same time. It might be deployed over a period of hours, days, or months. Thus, modern developers must think about backwards compatibility in the wild. This is a simple reality of modern application development that many developers would like to ignore but cannot. Thus, when it comes to services, versus library APIs, developers must use one of the two options described above (give up on changing an API or go through a deprecation cycle), and this is no different whether using a monorepo or polyrepo.In terms of actually making refactor changes across large codebases, many organizations end up developing automated refactor tooling such as fastmod, recently released by Facebook. As elsewhere, a tool such as this can trivially operate within a single repository or across multiple repositories. Lyft has a tool internally called refactorator which does just this. It works like fastmod but automates making changes across our polyrepo, including opening PRs, tracking review status, etc.Unique monorepo downsidesIn the previous section I laid out all of the theoretical benefits that a monorepo provides, and explained why in order to realize them, extraordinarily complex tooling must be developed that is no different to what is required for a polyrepo. In this section, Im going to cover two unique downsides to monorepos.Downside 1: Tight coupling and OSSOrganizationally, a monorepo encourages tight coupling and development of brittle software. It gives developers the feeling they can easily fix abstraction mistakes, when they actually cannot in the real world due to the realities of staggered build/deploy and the human/organizational/cultural factors inherent in asking developers to make changes across the entire codebase.Polyrepo code layout offers clear team/project/abstraction/ownership boundaries and encourages developers to think carefully about contracts. This is a subtle yet hugely important benefit: it imbues an organizations developers with a more scalable and long-term way of thinking. Furthermore, the use of a polyrepo does not mean that developers cannot reach across repository boundaries. Whether this happens or not is a function of the engineering culture in place versus whether the organization uses a monorepo or polyrepo.Tight coupling also has substantial implications with regard to open source. If an organization wishes to create or easily consume OSS, using a polyrepo is required. The contortions that large monorepo organizations undertake (reverse import/export, private/public issue tracking, shim layers to abstract standard library differences, etc.) are not conducive to productive OSS collaboration and community building, and also create substantial overhead for engineers within the organization.Downside 2: VCS scalabilityScaling a single VCS to hundreds of developers, hundreds of millions lines of code, and a rapid rate of submissions is a monumental task. Twitters monorepo roll-out about 5 years ago (based on git) was one of the biggest software engineering boondoggles I have ever witnessed in my career. Running simple commands such as git status would take minutes. If an individual clone got too far behind, it took hours to catch up (for a time there was even a practice of shipping hard drives to remote employees with a recent clone to start out with). I bring this up not specifically to make fun of Twitter engineering, but to illustrate how hard this problem is. Im told that 5 years later, the performance of Twitters monorepo is still not what the developer tooling team there would like, and not for lack of trying.Of course, the past 5 years has also seen development in this area. Microsofts git VFS which is used internally to develop Windows, has tackled creating a real VFS for git, as I described above, as a requirement for monorepo scalability (and with Microsofts acquisition of GitHub it seems likely this level of git scalability will find its way into GitHubs enterprise offerings). And, of course, Google and Facebook continue to invest tremendous resources into their internal systems to keep them running, although none of this work is publicly available.However, why bother solving the VCS scalability problem at all when, as described in the previous section, tooling will also need to be built that is identical to what is required for a polyrepo? There is no good reason.ConclusionAs is often the case in software engineering, we tend to look at techs most successful companies for guidance on best practices, without understanding the monumental engineering that has gone into making those companies successful at scale. Monorepos, in my opinion, are an egregious example of this. Google, Facebook, and Twitter have invested extensively in their code storage systems, only to wind up with a solution that is no different from what is required when using a polyrepo, yet leads to tight coupling and requires a substantial investment in VCS scalability.The frank reality is that, at scale, how well an organization does with code sharing, collaboration, tight coupling, etc. is a direct result of engineering culture and leadership, and has nothing to do with whether a monorepo or a polyrepo is used. The two solutions end up looking identical to the developer. In the face of this, why use a monorepo in the first place? Please dont! "
        ],
        "story_type": "Normal",
        "url_raw": "https://medium.com/@mattklein123/monorepos-please-dont-e9a279be011b",
        "comments.comment_id": [18810604, 18811368],
        "comments.comment_author": ["mrgriffin", "curtis"],
        "comments.comment_descendants": [6, 10],
        "comments.comment_time": [
          "2019-01-02T21:53:11Z",
          "2019-01-02T23:28:14Z"
        ],
        "comments.comment_text": [
          "My problem with polyrepos is that often organizations end up splitting things too finely, and now I'm unable to make a single commit to introduce a feature because my changes have to live across several repositories. Which makes code review more annoying because you have to tab back and forth to see all the context. It's doubly frustrating when I'm (or my team is) the only people working on those repositories, because now it doesn't feel like it gained any advantages. I know the author addresses this, but I can't imagine projects are typically at the scale they're describing. Certainly it's not my experience.<p>Also I definitely miss the ability to make changes to fundamental (internal) libraries used by every project. It's too much hassle to track down all the uses of a particular function, so I end up putting that change elsewhere, which means someone else will do it a little different in their corner of the world, which utterly confuses the first person who's unlucky enough to work in both code bases (at the same time, or after moving teams).",
          "My advice is that if components need to release together, then they ought to be in the same repo.  I'd probably go further and say that if you just think components might need to release together then they should go in the same repo, because you can in fact pretty easily manage projects with different release schedules from the same repo if you really need to.<p>On the other hand if you've got a whole bunch of components in different repos which need to release together it suddenly becomes a real pain.<p>If you've got components that will never need to release together, then of course you can stick them in different repositories.  But if you do this and you want to share common code between the repositories then you will need to manage that code with some sort of robust versioning system, and robust versioning systems are hard.  Only do something like that when the value is high enough to justify the overhead.  If you're in a startup, chances are very good that the value is <i>not</i> high enough.<p>As a final observation, you can split big repositories into smaller ones quite easily (in Git anyway) but sticking small repositories together into a bigger one is a lot harder.  So start out with a monorepo and only split smaller repositories out when it's clear that it really makes sense."
        ],
        "id": "1b7458e6-581a-42ea-a754-33e800e903df",
        "url_text": "Here we are at the beginning of 2019 and Im engaged in yet another discussion on the merits (or lack thereof) of keeping all of an organizations code in a monorepo. For those of you not familiar with this concept, the idea behind a monorepo is to store all code in a single version control system (VCS) repository. The alternative, of course, is to store code split into many different VCS repositories, usually on a service/application/library basis. For the purpose of this post I will call the multiple repository solution a polyrepo.Some of techs biggest names use a monorepo, including Google, Facebook, Twitter, and others. Surely if these companies all use a monorepo, the benefits must be tremendous, and we should all do the same, right? Wrong! As the title says: please, do not use a monorepo! Why? Because, at scale, a monorepo must solve every problem that a polyrepo must solve, with the downside of encouraging tight coupling, and the additional herculean effort of tackling VCS scalability. Thus, in the medium to long term, a monorepo provides zero organizational benefits, while inevitably leaving some of an organizations best engineers with a wicked case of PTSD (manifested via drooling and incoherent mumbling about git performance internals).A quick note: what do I mean by at scale? There is no definitive answer to this question, but because I know I will be asked, lets say for the sake of discussion that at scale means over 100 developers writing code full time.Theoretical monorepo benefits and why they cannot be achieved without polyrepo-style tooling (or are a lie)Theoretical benefit 1: Easier collaboration and code sharingMonorepo proponents will argue that when all code is present within a single repository, the likelihood of code duplication is small, and its more likely that different teams will collaborate together on shared infrastructure.Here is the ugly truth about even a medium size monorepo (and this will be a recurring theme throughout this section): it quickly becomes unreasonable for a single developer to have the entire repository on their machine, or to search through it using tools like grep. Thus, any monorepo that hopes to scale must provide two things:Some type of virtual file system (VFS) that allows a portion of the code to be present locally. This might be accomplished via a proprietary VCS like Perforce which natively operates this way, via Googles G3 internal tooling, or Microsofts GVFS.Sophisticated source code indexing/searching/discovery capabilities as a service. Since no individual developer is going to have all code in a searchable state, its critical that there exists some capability to perform a search across the entire codebase.Given that a developer will only access small portions of the codebase at a time, is there any real difference between checking out a portion of the tree via a VFS or checking out multiple repositories? There is no difference.In terms of source code indexing/searching/discovery capabilities, a tool can trivially iterate over many repositories and collate the results. In fact, this is how GitHubs own searching capabilities work as well as newer and more sophisticated indexing and collaboration tools such as Sourcegraph.Thus, in terms of collaboration and code sharing, at scale, developers are exposed to subsections of code through higher layer tooling. Whether the code is in a monorepo or polyrepo is irrelevant; the problem being solved is the same, and the efficacy of collaboration and code sharing has everything to do with engineering culture and nothing to do with code storage.Theoretical benefit 2: Single build / no dependency managementThe next thing that monorepo proponents will typically say is that by having all code in a single repository, there is no need for dependency management because all source code is built at the same time. This is a lie! At scale, there is simply no way to rebuild the entirety of a codebase and run all automated tests when each change is submitted (or, more importantly and more often, in CI when a change is proposed). To deal with this problem, all of the large monorepos have developed sophisticated build systems (see Bazel/Blaze from Google and Buck from Facebook as examples) that are designed in such a way as to internally track dependencies and build a directed acyclic graph (DAG) of the source code. This DAG allows for efficient build and test caching such that only code that changes, or code that depends on it, needs to be built and tested.Furthermore, because code that is built must actually be deployed, and not all software is deployed at the same time, it is essential that build artifacts are carefully tracked so that previously deployed software can be redeployed to new hosts as needed. This reality means that even in a monorepo world, multiple versions of code exist at the same time in the wild, and must be carefully tracked and reconciled.Monorepo proponents will argue that even with the large amount of build/dependency tracking required, there is still substantial benefit because a single commit/SHA describes the entire state of the world. I would argue this benefit is dubious; given the DAG that already exists, its a trivial leap to include individual repository SHAs as part of the DAG, and in fact, Bazel can seamlessly work across repositories or within a single repository, abstracting the underlying layout from the developer. Furthermore, automated refactor tooling can trivially be built that automatically bumps dependent library versions across many repositories, thus blurring the difference between a monorepo and polyrepo in this area (more on this below).The end result is that the realities of build/deploy management at scale are largely identical whether using a monorepo or polyrepo. The tools dont care, and neither should the developers writing code.Theoretical benefit 3: Code refactors are easy / atomic commitsThe final benefit that monorepo proponents typically tout is the fact that when all code is in a single repository, it makes code refactors much easier, due to ease of searching and the idea that a single atomic commit can span the entire codebase. This is a fallacy for multiple reasons:As described above, at scale, a developer will not be able to easily edit or search the entirety of the codebase on their local machine. Thus, the idea that one can clone all of the code and simply do a grep/replace is not trivial in practice.If we assume that via a sophisticated VFS a developer can clone and edit the entire codebase, the next question is how often does that actually happen? Im not talking about fixing a bug in an implementation of a shared library, as this type of fix is identically carried out whether using a monorepo or polyrepo (assuming similar build/deploy tooling as described in the previous section). Im talking about a library API change that has follow-on build breakage effects for other code. In very large code bases, it is likely impossible to make a change to a fundamental API and get it code reviewed by every affected team before merge conflicts force the process to start over again. Developers are faced with two realistic choices. First, they can give up, and work around the API issue (this happens more often than we would like to admit). Second, they can deprecate the existing API, implement a new API, and then go through the laborious process of individual deprecation changes throughout the codebase. Either way, this is exactly the same process undertaken in a polyrepo.In a service oriented world, applications are now composed of many loosely coupled services that interact with each other using some type of well specified API. Larger organizations inevitably migrate to an IDL such as Thrift or Protobuf that allow for type-safe APIs and backwards compatible changes. As described in the previous section on build/deploy management, code is not deployed at the same time. It might be deployed over a period of hours, days, or months. Thus, modern developers must think about backwards compatibility in the wild. This is a simple reality of modern application development that many developers would like to ignore but cannot. Thus, when it comes to services, versus library APIs, developers must use one of the two options described above (give up on changing an API or go through a deprecation cycle), and this is no different whether using a monorepo or polyrepo.In terms of actually making refactor changes across large codebases, many organizations end up developing automated refactor tooling such as fastmod, recently released by Facebook. As elsewhere, a tool such as this can trivially operate within a single repository or across multiple repositories. Lyft has a tool internally called refactorator which does just this. It works like fastmod but automates making changes across our polyrepo, including opening PRs, tracking review status, etc.Unique monorepo downsidesIn the previous section I laid out all of the theoretical benefits that a monorepo provides, and explained why in order to realize them, extraordinarily complex tooling must be developed that is no different to what is required for a polyrepo. In this section, Im going to cover two unique downsides to monorepos.Downside 1: Tight coupling and OSSOrganizationally, a monorepo encourages tight coupling and development of brittle software. It gives developers the feeling they can easily fix abstraction mistakes, when they actually cannot in the real world due to the realities of staggered build/deploy and the human/organizational/cultural factors inherent in asking developers to make changes across the entire codebase.Polyrepo code layout offers clear team/project/abstraction/ownership boundaries and encourages developers to think carefully about contracts. This is a subtle yet hugely important benefit: it imbues an organizations developers with a more scalable and long-term way of thinking. Furthermore, the use of a polyrepo does not mean that developers cannot reach across repository boundaries. Whether this happens or not is a function of the engineering culture in place versus whether the organization uses a monorepo or polyrepo.Tight coupling also has substantial implications with regard to open source. If an organization wishes to create or easily consume OSS, using a polyrepo is required. The contortions that large monorepo organizations undertake (reverse import/export, private/public issue tracking, shim layers to abstract standard library differences, etc.) are not conducive to productive OSS collaboration and community building, and also create substantial overhead for engineers within the organization.Downside 2: VCS scalabilityScaling a single VCS to hundreds of developers, hundreds of millions lines of code, and a rapid rate of submissions is a monumental task. Twitters monorepo roll-out about 5 years ago (based on git) was one of the biggest software engineering boondoggles I have ever witnessed in my career. Running simple commands such as git status would take minutes. If an individual clone got too far behind, it took hours to catch up (for a time there was even a practice of shipping hard drives to remote employees with a recent clone to start out with). I bring this up not specifically to make fun of Twitter engineering, but to illustrate how hard this problem is. Im told that 5 years later, the performance of Twitters monorepo is still not what the developer tooling team there would like, and not for lack of trying.Of course, the past 5 years has also seen development in this area. Microsofts git VFS which is used internally to develop Windows, has tackled creating a real VFS for git, as I described above, as a requirement for monorepo scalability (and with Microsofts acquisition of GitHub it seems likely this level of git scalability will find its way into GitHubs enterprise offerings). And, of course, Google and Facebook continue to invest tremendous resources into their internal systems to keep them running, although none of this work is publicly available.However, why bother solving the VCS scalability problem at all when, as described in the previous section, tooling will also need to be built that is identical to what is required for a polyrepo? There is no good reason.ConclusionAs is often the case in software engineering, we tend to look at techs most successful companies for guidance on best practices, without understanding the monumental engineering that has gone into making those companies successful at scale. Monorepos, in my opinion, are an egregious example of this. Google, Facebook, and Twitter have invested extensively in their code storage systems, only to wind up with a solution that is no different from what is required when using a polyrepo, yet leads to tight coupling and requires a substantial investment in VCS scalability.The frank reality is that, at scale, how well an organization does with code sharing, collaboration, tight coupling, etc. is a direct result of engineering culture and leadership, and has nothing to do with whether a monorepo or a polyrepo is used. The two solutions end up looking identical to the developer. In the face of this, why use a monorepo in the first place? Please dont! ",
        "_version_": 1718536433174577152
      },
      {
        "story_id": 19276542,
        "story_author": "mericsson",
        "story_descendants": 38,
        "story_score": 111,
        "story_time": "2019-02-28T23:38:28Z",
        "story_title": "Bitbucket Pipes",
        "search": [
          "Bitbucket Pipes",
          "https://bitbucket.org/blog/meet-bitbucket-pipes-30-ways-to-automate-your-ci-cd-pipeline",
          "The democratizing nature of DevOps has seen the responsibility of building and managing CI/CD pipelines transition from specialized release engineers to developers. But automating a robust, dependable CI/CD pipelineistedious work. Developers need to connect to multiple tools to deliver software, andwriting pipeline integrations for these servicesis a manual, error-prone process. Theres research involved to ensure dependencies are accounted for, as well as debugging and maintainingintegrationswhen updates are made. Its no wonder many teams put automating CI/CD firmly in the too hard basket. But those days are over. In 2016 we launched Bitbucket Pipelines: a CI/CD tool in the cloud that's part of your repository and makes it easy for developers to configure pipelines with code.Andtoday we are launching Bitbucket Pipes to make it easier to build powerful, automated CI/CD workflows in a plug and play fashion without the hassle ofmanaging integrations.Weve worked with industry leaders including Microsoft, AWS, Slack, Google Cloud and more to build supported pipes that help automate your CI/CD pipeline, and made it simple to create your own to help abstract any duplicated configuration across your repositories. Announcing Bitbucket Pipes Whether youre creating a simple deploy pipeline to a hosting service like AWS, utilizing a multi-cloud deployment strategy, or automating a sophisticated pipeline that involves security scanning, monitoring, and artifact management, Bitbucket Pipes makes it easy to build and automate a CI/CD pipeline that meets your exact needs. Simply select the appropriate pipes you need and enter in the variables required by the pipe to run. Not only do supported pipes make it trivial to set up your external services across pipelines and repositories, theyre also updated and maintained by the author meaning you never have to worry about updating or re-configuring them yourself. The end result is an easy way to build, update, modify, and maintain CI/CD pipelines no matter how sophisticated they are. In the example below you can see how easy configuring your pipeline becomes by simply copying and pasting pipes on the right, versus manually typing and configuring the same pipeline on the left. New users can easily browse and select pipes to get started, while more experienced users can not only reuse pipes across their repositories, but discover new and interesting ways to automate their pipelines. An open approach to automation Theres no one-size-fits-all approach to software development ? developers should work with whatever tools best suit their needs. Whether its hosting, monitoring, incident management and everything in-between, weve partnered with some of the best in the industry to bring the tools you already use right into your CI/CD pipeline. These partners are just scratching the surface for Bitbucket Pipes and we have more supported pipes to come. And we want the community involved too ? tell us which services youd like to see or even build your own. Its easy to build pipes that meet your exact needs and we cant wait to see how your team automates your CI/CD workflow. Get started with Bitbucket Pipes Get started with our pre-configured pipes or create your own today. For those new to Bitbucket, sign up, create your first repository and enable Bitbucket Pipelines. For existing Bitbucket Pipelines users, you can find the new Pipes view in the online .yml editor. Read our docs to find out more. "
        ],
        "story_type": "Normal",
        "url_raw": "https://bitbucket.org/blog/meet-bitbucket-pipes-30-ways-to-automate-your-ci-cd-pipeline",
        "url_text": "The democratizing nature of DevOps has seen the responsibility of building and managing CI/CD pipelines transition from specialized release engineers to developers. But automating a robust, dependable CI/CD pipelineistedious work. Developers need to connect to multiple tools to deliver software, andwriting pipeline integrations for these servicesis a manual, error-prone process. Theres research involved to ensure dependencies are accounted for, as well as debugging and maintainingintegrationswhen updates are made. Its no wonder many teams put automating CI/CD firmly in the too hard basket. But those days are over. In 2016 we launched Bitbucket Pipelines: a CI/CD tool in the cloud that's part of your repository and makes it easy for developers to configure pipelines with code.Andtoday we are launching Bitbucket Pipes to make it easier to build powerful, automated CI/CD workflows in a plug and play fashion without the hassle ofmanaging integrations.Weve worked with industry leaders including Microsoft, AWS, Slack, Google Cloud and more to build supported pipes that help automate your CI/CD pipeline, and made it simple to create your own to help abstract any duplicated configuration across your repositories. Announcing Bitbucket Pipes Whether youre creating a simple deploy pipeline to a hosting service like AWS, utilizing a multi-cloud deployment strategy, or automating a sophisticated pipeline that involves security scanning, monitoring, and artifact management, Bitbucket Pipes makes it easy to build and automate a CI/CD pipeline that meets your exact needs. Simply select the appropriate pipes you need and enter in the variables required by the pipe to run. Not only do supported pipes make it trivial to set up your external services across pipelines and repositories, theyre also updated and maintained by the author meaning you never have to worry about updating or re-configuring them yourself. The end result is an easy way to build, update, modify, and maintain CI/CD pipelines no matter how sophisticated they are. In the example below you can see how easy configuring your pipeline becomes by simply copying and pasting pipes on the right, versus manually typing and configuring the same pipeline on the left. New users can easily browse and select pipes to get started, while more experienced users can not only reuse pipes across their repositories, but discover new and interesting ways to automate their pipelines. An open approach to automation Theres no one-size-fits-all approach to software development ? developers should work with whatever tools best suit their needs. Whether its hosting, monitoring, incident management and everything in-between, weve partnered with some of the best in the industry to bring the tools you already use right into your CI/CD pipeline. These partners are just scratching the surface for Bitbucket Pipes and we have more supported pipes to come. And we want the community involved too ? tell us which services youd like to see or even build your own. Its easy to build pipes that meet your exact needs and we cant wait to see how your team automates your CI/CD workflow. Get started with Bitbucket Pipes Get started with our pre-configured pipes or create your own today. For those new to Bitbucket, sign up, create your first repository and enable Bitbucket Pipelines. For existing Bitbucket Pipelines users, you can find the new Pipes view in the online .yml editor. Read our docs to find out more. ",
        "comments.comment_id": [19277482, 19277802],
        "comments.comment_author": ["imcotton", "holmb"],
        "comments.comment_descendants": [3, 0],
        "comments.comment_time": [
          "2019-03-01T02:30:02Z",
          "2019-03-01T03:56:39Z"
        ],
        "comments.comment_text": [
          "Free tier(Build minutes: 50 mins/mo)<p>Thanks but no thank you, meanwhile GitLab offers 2000 mins/mo",
          "Seems somewhat similar to resources[0] in Concourse in that they use a container image that has a defined entry point.<p>Concourse refers to this as a \"get step\"[1] or a \"put step\", which calls a pre-defined script inside the container with a custom set of parameters. The \"put step\" is used when you expect side effects, while a \"get step\" is used to check status on some resource and trigger jobs.<p>In general it makes the CI/CD system easily composable and clean. Concourse manages this very well and while I haven't used Bitbucket Pipes I suspect it to be a good experience as well.<p>[0] <a href=\"https://concourse-ci.org/resources.html\" rel=\"nofollow\">https://concourse-ci.org/resources.html</a>\n[1] <a href=\"https://concourse-ci.org/implementing-resources.html\" rel=\"nofollow\">https://concourse-ci.org/implementing-resources.html</a>"
        ],
        "id": "f8bbc086-9ba1-4b4b-be10-63d0c38d52f2",
        "_version_": 1718536454160777216
      },
      {
        "story_id": 21439378,
        "story_author": "chmaynard",
        "story_descendants": 1,
        "story_score": 24,
        "story_time": "2019-11-04T06:11:27Z",
        "story_title": "Highlights from Git 2.24",
        "search": [
          "Highlights from Git 2.24",
          "https://github.blog/2019-11-03-highlights-from-git-2-24/",
          "The open source Git project just released Git 2.24 with features and bug fixes from over 78 contributors, 21 of them new. Heres our look at some of the most exciting features and changes introduced since Git 2.23. Feature macros Since the very early days, Git has shipped with a configuration subsystem that lets you configure different global or repository-specific settings. For example, the first time you wrote a commit on a new machine, you might have been reminded to set your user.name and user.email settings[1] if you havent already. It turns out, git config is used for many things ranging anywhere from the identity you commit with and what line endings to use all the way to configuring aliases to other Git commands and what algorithm is chosen to produce diffs. Usually, configuring some behavior requires only a single configuration change, like enabling or disabling any of the aforementioned values. But what about when it doesnt? What do you do when you dont know which configuration values to change? For example, lets say you want to live on the bleeding-edge of the latest from upstream Git, but dont have a chance to discover all the new configurable options. In Git 2.24, you can now opt into feature macrosone Git configuration that implies many others. These are hand-selected by the developers of Git, and they let you opt into a certain feature or adopt a handful of settings based on the characteristics of your repository. For example, lets pretend that you have a particularly large repository, and youre noticing some slow-downs. With enough searching, you might find that setting index.version to 4 could help, but discovering this can seem like a stretch. Instead, you can enable feature.manyFiles with: $ git config feature.manyFiles true Now youre opted into the features that will make your experience with Git the smoothest it can be. Setting this signals to Git that youre willing to adopt whichever settings Git developers feel can make your experience smoothest (right now, this means that the index.version and core.untrackedCache to enable path-prefix compression and the untracked cache, respectively). Not only that, but you can feel even better knowing that any new features in a release that might help your use case will be included in the macro. [source] Commit graphs by default You may remember commit graphs, a feature that we have discussed in some of our previous highlights. Since its introduction in Git 2.19, this feature has received a steady stream of attention. When enabled, and kept reasonably up to date, commit graphs can represent an order of magnitude improvement in the performance of loading commits. Now in Git 2.24, commit graphs are enabled by default, meaning that your repository will see an improvement the next time you run git gc. Previously, this feature was an opt-in behind an experimental core.commitGraph configuration (as well as a handful of others), but after extensive testing[2], its ready for prime time. Besides being the new standard, here are a few other changes to commit graphs: All commit-graph sub-commands (e.g., git commit-graph write, git commit-graph verify, and so on) now have support for the -[no-]progress. Now the progress meters for these commands behave in the usual way: writing only to a terminal by default, and respecting -[no-]progress to override that. A new configuration value to automatically update the commit-graph file while fetching has been introduced, which takes advantage of commit graph chains to write a portion of history onto a commit-graph chain for later compaction. This means that every time you get new commits from a remote, they are guaranteed to be in a commit-graph immediately, and you dont have to wait around for the next auto-gc. To try this out today, set the fetch.writeCommitGraph configuration variable to true. Lots of bug fixes to improve the performance and reliability of the commit-graph commands, especially when faced with corrupt repositories. The commit-graph commands now also support Gits latest tracing mechanism, trace2. [source, source, source, source, source, source, source] Adopting the Contributor Covenant Since the last release, the Git project has discussed at length adopting a code of conduct to solidify welcoming and inclusive behavior on the mailing list where Git development takes place. Because communication between Git developers happens over email, it can be intimidating or unwelcoming to new contributors who may not be familiar with the values of the people contributing to Git. The Git community has long relied on a policy of be nice, as much as possible (to quote this thread). This approach is in the right spirit, but it may not be readily apparent to new contributors unfamiliar with the existing culture. Likewise, it can make an individual feel uncomfortable engaging with a project that has not solidified its values. By adopting a code of conduct, the Git project is making it clear which behaviors it encourages and which it wont tolerate. New contributors are able to see explicitly what the projects values are, and they can put their trust in Gits choice of using the well-trusted and widely-adopted Contributor Covenant. This code of conduct is enforced by the projects leadership, who will handle any case in which an individual does not adhere to the guidelines. New contributors can be assured that the Git community is behind this adoption with the introduction of the Code of Conduct, Acked-by 16 prominent members of the Git community. [source] Alternative history rewriting tools If youve ever wanted to perform a complicated operation over the history of your repositorylike expunging a file from a repositorys history or extracting the history pertaining to just one directoryyou may have visited the documentation for git filter-branch. git filter-branch is a long-standing and powerful tool for rewriting history[3]. With git filter-branch, you can do all of those aforementioned operations and much more. However, this flexibility comes at a hefty cost: git filter-branch is notoriously complicated to use (not to mention slow), and can often lead its users towards unintended changes, including repository corruption and data loss. In other words, git filter-branch is starting to show its age. Now, as of Git 2.24, the Git project instead recommends a new, independent tool, git filter-repo. git filter-repo serves to avoid many of the pitfalls that users experienced with git filter-branch. Instead of reprocessing every commit in order, git filter-repo operates on an efficient, stream representation of history to run much faster. The tool is extremely powerful, and all of its capabilities are documented thoroughly. Here are a few highlights about how you can use git filter-repo: git filter-repo --analyzeprovides a human-readable selection of metrics profiling the size of your repository. This includes how many objects of each kind there are, which files and directories are largest, which extensions take up the most space, and so on. And this isnt your only option in this space. For additional metrics on the shape of your repository, check out another tool, git sizer. You can also filter the history of your repository to contain only certain paths, with --path-{glob,regex} and similar options. [source] Likewise, you can run a find and replace operation over history, as well as strip blobs that are larger than a fixed threshold. [source] When rewriting history, any rewritten commits (along with their ancestors) will get a new SHA-1 to identify them. By default, git filter-repo updates all other references to these SHA-1s, like other commit messages that reference them. By a similar token, git filter-repo also has options to rewrite the names of contributors using .mailmap. [source, source] Finally, git filter-repo is extensible. It provides a flexible interface for specifying callbacks in Python (e.g., calling a function when git filter-repoencounters a blob/tree/commit, new filetype, etc.), as well as defining new sub-commands entirely. View a portfolio of demo extensions, or define your own to support a complex history rewrite. [source] git filter-branch will, however, remain included in the usual distributions of Git for some time. git filter-repo is another alternative for performing complex modifications to your repositorys history, and its now the official recommendation from upstream. [source] Tidbits You might be aware that many Git commands take one or more optional reference names as arguments. For example, git log without arguments will display a log of everything thats reachable from the currently checked-out branch, but git log my-feature ^master will show you only whats on my-feature and not on master. But what if your branch is called --super-dangerous-option, you probably dont want to invoke git log since itll interpret the argument as an option, not a branch name. You could try and disambiguate by invoking git log 'refs/heads/--super-dangerous-option', but if youre scripting, you may not know under what namespace the argument youre getting belongs. Git 2.24 has a new way to prevent this sort of option injection attack using --end-of-options. When Git sees this as an argument to any command, it knows to treat the remaining arguments as such, and wont interpret them as more options. So, instead of the string previously mentioned, you could write the following to get the history of your (admittedly, pretty oddly named) branch: $ git log --end-of-options --super-dangerous-option Not using the standard -- was an intentional choice here, since this is already a widely-used mechanism in Git to separate reference names from files. In this example, you could have also written git log --end-of-options --super-dangerous-option ^master -- path/to/file to get only the history over that range which modified that specific file. [source] In a previous post, we talked about git rebases new --rebase-merges option, which allows users to perform rebases while preserving the structure of history. But when Git encounters a merge point how does it unify the two histories? By default, it uses a strategy known internally as recursive, which is most likely the merge strategy youre currently using. You might not know that you can tell Git which merge strategy to use, and picking one over the other may result in a different resolution[4]. Now, git rebase --rebase-merges supports the --strategy and --strategy-option options of git rebase, so you can rebase history while both preserving its structural integrity and specifying your own merge resolution strategy. [source] Git supports a number of hooks, which are specially-named executable files that Git will run at various points during your workflow. For example, a pre-push hook is invoked after running git push but before the push actually occurs and so on. A new hook has been added to allow callers to interact with Git after a merge has been carried out, but before the resulting commit is written. To intercept this point, callers can place an executable file of their choice in .git/hooks/pre-merge-commit. [source] Git has learned a handful of new tricks since the last release to handle partial clones. For those who arent up to date on what partial cloning in Git looks like, heres a quick primer. When cloning a repository, users can specify that they would only like some of its objects by using a filter. When doing so, the remote from which the user clones is designated as a promisor, meaning that it promises to send the remaining objects later on if the user requests them down the road. Up until 2.24, Git only supported a single promisor remote, but this latest release now supports more than one promisor remote. This is especially interesting since it means that users can configure a handful of geographically close remotes, and not all of those remotes have to have all of the objects. Theres more work planned in this area, so stay tuned for more updates on this feature in the future. [source] Last but not least, Gits command-line completion engine has learned how to complete configuration variables on per-command configurations. Git has a hierarchy of places where gitconfig files can be found: your repository (via .git/config), your home directory, and so on. But, Git also supports the top-level -c flag to specify configuration variables for each command. Heres an example: $ git -c core.autocrlf=false add path/to/my/file This invocation will disable Gits auto-CRLF conversion just for the duration of the git add (and any other internal commands that Git may run as a part of git add). However, if you forgot the name of the variable that you were trying to set Gits command-line completion engine learned how to provide a completion list of configuration variable names in Git 2.24. So, if you ever forget where you are in the middle of a -c ..., all you need to do is press tab. [source] [1]When you create a commit, these are the configuration values that Git is looking at to generate the signature; Git-parlance for the name/email-pair that is shown wherever an identity is expected. [2]Including at GitHub, where weve used the commit-graph file behind the scenes since August to achieve speed-ups of over 50 percent on operations which traverse history. [3]In fact, this tool appeared in git/git with 6f6826c52b which was first released in v1.5.3 over 12 years ago. [4]For example, the patience merge strategy is widely regarded as one that will move up chunks of your diff (these are usually known as hunks) that look textually related (for example, closing function braces), but in fact produce awkward-looking diffs. Learn more Thats just a sample of changes from the latest version. Check out the release notes for 2.24 or any previous versions in the Git repository. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.blog/2019-11-03-highlights-from-git-2-24/",
        "comments.comment_id": [21439402],
        "comments.comment_author": ["chmaynard"],
        "comments.comment_descendants": [0],
        "comments.comment_time": ["2019-11-04T06:20:38Z"],
        "comments.comment_text": [
          "Release notes: <a href=\"https://github.com/git/git/blob/v2.24.0/Documentation/RelNotes/2.24.0.txt\" rel=\"nofollow\">https://github.com/git/git/blob/v2.24.0/Documentation/RelNot...</a>"
        ],
        "id": "73a1f66a-35f5-410b-8307-6ad1a01594ee",
        "url_text": "The open source Git project just released Git 2.24 with features and bug fixes from over 78 contributors, 21 of them new. Heres our look at some of the most exciting features and changes introduced since Git 2.23. Feature macros Since the very early days, Git has shipped with a configuration subsystem that lets you configure different global or repository-specific settings. For example, the first time you wrote a commit on a new machine, you might have been reminded to set your user.name and user.email settings[1] if you havent already. It turns out, git config is used for many things ranging anywhere from the identity you commit with and what line endings to use all the way to configuring aliases to other Git commands and what algorithm is chosen to produce diffs. Usually, configuring some behavior requires only a single configuration change, like enabling or disabling any of the aforementioned values. But what about when it doesnt? What do you do when you dont know which configuration values to change? For example, lets say you want to live on the bleeding-edge of the latest from upstream Git, but dont have a chance to discover all the new configurable options. In Git 2.24, you can now opt into feature macrosone Git configuration that implies many others. These are hand-selected by the developers of Git, and they let you opt into a certain feature or adopt a handful of settings based on the characteristics of your repository. For example, lets pretend that you have a particularly large repository, and youre noticing some slow-downs. With enough searching, you might find that setting index.version to 4 could help, but discovering this can seem like a stretch. Instead, you can enable feature.manyFiles with: $ git config feature.manyFiles true Now youre opted into the features that will make your experience with Git the smoothest it can be. Setting this signals to Git that youre willing to adopt whichever settings Git developers feel can make your experience smoothest (right now, this means that the index.version and core.untrackedCache to enable path-prefix compression and the untracked cache, respectively). Not only that, but you can feel even better knowing that any new features in a release that might help your use case will be included in the macro. [source] Commit graphs by default You may remember commit graphs, a feature that we have discussed in some of our previous highlights. Since its introduction in Git 2.19, this feature has received a steady stream of attention. When enabled, and kept reasonably up to date, commit graphs can represent an order of magnitude improvement in the performance of loading commits. Now in Git 2.24, commit graphs are enabled by default, meaning that your repository will see an improvement the next time you run git gc. Previously, this feature was an opt-in behind an experimental core.commitGraph configuration (as well as a handful of others), but after extensive testing[2], its ready for prime time. Besides being the new standard, here are a few other changes to commit graphs: All commit-graph sub-commands (e.g., git commit-graph write, git commit-graph verify, and so on) now have support for the -[no-]progress. Now the progress meters for these commands behave in the usual way: writing only to a terminal by default, and respecting -[no-]progress to override that. A new configuration value to automatically update the commit-graph file while fetching has been introduced, which takes advantage of commit graph chains to write a portion of history onto a commit-graph chain for later compaction. This means that every time you get new commits from a remote, they are guaranteed to be in a commit-graph immediately, and you dont have to wait around for the next auto-gc. To try this out today, set the fetch.writeCommitGraph configuration variable to true. Lots of bug fixes to improve the performance and reliability of the commit-graph commands, especially when faced with corrupt repositories. The commit-graph commands now also support Gits latest tracing mechanism, trace2. [source, source, source, source, source, source, source] Adopting the Contributor Covenant Since the last release, the Git project has discussed at length adopting a code of conduct to solidify welcoming and inclusive behavior on the mailing list where Git development takes place. Because communication between Git developers happens over email, it can be intimidating or unwelcoming to new contributors who may not be familiar with the values of the people contributing to Git. The Git community has long relied on a policy of be nice, as much as possible (to quote this thread). This approach is in the right spirit, but it may not be readily apparent to new contributors unfamiliar with the existing culture. Likewise, it can make an individual feel uncomfortable engaging with a project that has not solidified its values. By adopting a code of conduct, the Git project is making it clear which behaviors it encourages and which it wont tolerate. New contributors are able to see explicitly what the projects values are, and they can put their trust in Gits choice of using the well-trusted and widely-adopted Contributor Covenant. This code of conduct is enforced by the projects leadership, who will handle any case in which an individual does not adhere to the guidelines. New contributors can be assured that the Git community is behind this adoption with the introduction of the Code of Conduct, Acked-by 16 prominent members of the Git community. [source] Alternative history rewriting tools If youve ever wanted to perform a complicated operation over the history of your repositorylike expunging a file from a repositorys history or extracting the history pertaining to just one directoryyou may have visited the documentation for git filter-branch. git filter-branch is a long-standing and powerful tool for rewriting history[3]. With git filter-branch, you can do all of those aforementioned operations and much more. However, this flexibility comes at a hefty cost: git filter-branch is notoriously complicated to use (not to mention slow), and can often lead its users towards unintended changes, including repository corruption and data loss. In other words, git filter-branch is starting to show its age. Now, as of Git 2.24, the Git project instead recommends a new, independent tool, git filter-repo. git filter-repo serves to avoid many of the pitfalls that users experienced with git filter-branch. Instead of reprocessing every commit in order, git filter-repo operates on an efficient, stream representation of history to run much faster. The tool is extremely powerful, and all of its capabilities are documented thoroughly. Here are a few highlights about how you can use git filter-repo: git filter-repo --analyzeprovides a human-readable selection of metrics profiling the size of your repository. This includes how many objects of each kind there are, which files and directories are largest, which extensions take up the most space, and so on. And this isnt your only option in this space. For additional metrics on the shape of your repository, check out another tool, git sizer. You can also filter the history of your repository to contain only certain paths, with --path-{glob,regex} and similar options. [source] Likewise, you can run a find and replace operation over history, as well as strip blobs that are larger than a fixed threshold. [source] When rewriting history, any rewritten commits (along with their ancestors) will get a new SHA-1 to identify them. By default, git filter-repo updates all other references to these SHA-1s, like other commit messages that reference them. By a similar token, git filter-repo also has options to rewrite the names of contributors using .mailmap. [source, source] Finally, git filter-repo is extensible. It provides a flexible interface for specifying callbacks in Python (e.g., calling a function when git filter-repoencounters a blob/tree/commit, new filetype, etc.), as well as defining new sub-commands entirely. View a portfolio of demo extensions, or define your own to support a complex history rewrite. [source] git filter-branch will, however, remain included in the usual distributions of Git for some time. git filter-repo is another alternative for performing complex modifications to your repositorys history, and its now the official recommendation from upstream. [source] Tidbits You might be aware that many Git commands take one or more optional reference names as arguments. For example, git log without arguments will display a log of everything thats reachable from the currently checked-out branch, but git log my-feature ^master will show you only whats on my-feature and not on master. But what if your branch is called --super-dangerous-option, you probably dont want to invoke git log since itll interpret the argument as an option, not a branch name. You could try and disambiguate by invoking git log 'refs/heads/--super-dangerous-option', but if youre scripting, you may not know under what namespace the argument youre getting belongs. Git 2.24 has a new way to prevent this sort of option injection attack using --end-of-options. When Git sees this as an argument to any command, it knows to treat the remaining arguments as such, and wont interpret them as more options. So, instead of the string previously mentioned, you could write the following to get the history of your (admittedly, pretty oddly named) branch: $ git log --end-of-options --super-dangerous-option Not using the standard -- was an intentional choice here, since this is already a widely-used mechanism in Git to separate reference names from files. In this example, you could have also written git log --end-of-options --super-dangerous-option ^master -- path/to/file to get only the history over that range which modified that specific file. [source] In a previous post, we talked about git rebases new --rebase-merges option, which allows users to perform rebases while preserving the structure of history. But when Git encounters a merge point how does it unify the two histories? By default, it uses a strategy known internally as recursive, which is most likely the merge strategy youre currently using. You might not know that you can tell Git which merge strategy to use, and picking one over the other may result in a different resolution[4]. Now, git rebase --rebase-merges supports the --strategy and --strategy-option options of git rebase, so you can rebase history while both preserving its structural integrity and specifying your own merge resolution strategy. [source] Git supports a number of hooks, which are specially-named executable files that Git will run at various points during your workflow. For example, a pre-push hook is invoked after running git push but before the push actually occurs and so on. A new hook has been added to allow callers to interact with Git after a merge has been carried out, but before the resulting commit is written. To intercept this point, callers can place an executable file of their choice in .git/hooks/pre-merge-commit. [source] Git has learned a handful of new tricks since the last release to handle partial clones. For those who arent up to date on what partial cloning in Git looks like, heres a quick primer. When cloning a repository, users can specify that they would only like some of its objects by using a filter. When doing so, the remote from which the user clones is designated as a promisor, meaning that it promises to send the remaining objects later on if the user requests them down the road. Up until 2.24, Git only supported a single promisor remote, but this latest release now supports more than one promisor remote. This is especially interesting since it means that users can configure a handful of geographically close remotes, and not all of those remotes have to have all of the objects. Theres more work planned in this area, so stay tuned for more updates on this feature in the future. [source] Last but not least, Gits command-line completion engine has learned how to complete configuration variables on per-command configurations. Git has a hierarchy of places where gitconfig files can be found: your repository (via .git/config), your home directory, and so on. But, Git also supports the top-level -c flag to specify configuration variables for each command. Heres an example: $ git -c core.autocrlf=false add path/to/my/file This invocation will disable Gits auto-CRLF conversion just for the duration of the git add (and any other internal commands that Git may run as a part of git add). However, if you forgot the name of the variable that you were trying to set Gits command-line completion engine learned how to provide a completion list of configuration variable names in Git 2.24. So, if you ever forget where you are in the middle of a -c ..., all you need to do is press tab. [source] [1]When you create a commit, these are the configuration values that Git is looking at to generate the signature; Git-parlance for the name/email-pair that is shown wherever an identity is expected. [2]Including at GitHub, where weve used the commit-graph file behind the scenes since August to achieve speed-ups of over 50 percent on operations which traverse history. [3]In fact, this tool appeared in git/git with 6f6826c52b which was first released in v1.5.3 over 12 years ago. [4]For example, the patience merge strategy is widely regarded as one that will move up chunks of your diff (these are usually known as hunks) that look textually related (for example, closing function braces), but in fact produce awkward-looking diffs. Learn more Thats just a sample of changes from the latest version. Check out the release notes for 2.24 or any previous versions in the Git repository. ",
        "_version_": 1718536538897252352
      },
      {
        "story_id": 20574797,
        "story_author": "ingve",
        "story_descendants": 49,
        "story_score": 103,
        "story_time": "2019-07-31T14:46:21Z",
        "story_title": "Lefthook: Knock your team’s code back into shape",
        "search": [
          "Lefthook: Knock your team’s code back into shape",
          "https://evilmartians.com/chronicles/lefthook-knock-your-teams-code-back-into-shape",
          "Meet Lefthook, thefastest polyglot Git hooks manager out there, andmake sure not asingle line ofunruly code makes it into production. See how easy it istoinstall Lefthook, recently adopted by Discourse, Logux, andOpenstax, for most common frontend andbackend environments andensure all developers onyour team can rely onasingle flexible tool. Andit also has emojis Days, when asingle piece ofsoftware that millions rely onwas created by asingle developer inanivory tower, are long gone. Even Git, universally believed tobethebrainchild ofLinus Torvalds alone, was created with thehelp ofcontributors andisnow being maintained by ateam ofdozens. No matter ifyou work onanopen source project with thewhole world being your oyster, oryou are blooming inawalled garden ofproprietary commercial softwareyou still work inateam. Andeven with awell-organized system ofpull requests andcode reviews maintaining thecode quality across thelarge codebase with dozens ofcontributors isnot aneasy task. Hook me up Hooksways tofire off custom scripts when certain important actions (commit, push, etc.) occurare baked right into Git, so ifyou are comfortable with Bash andtheinternals oftheworld most popular version control systemyou dont need any external tools per se: just edit ./.git/hooks/pre-commit andput insome well-formed script that will, for instance, lint your files before you commit. However, when you work onaproject, you are most interested inwriting projects codenot thecode that checks it. Intheworld ofmodern web development tooling iseverything, andmyriads oftools exist for asingle reason: reducing overhead andcomplexity. Git hooks are not theexception: inJavaScript community, theweapon ofchoice isHusky with Webpack, Babel, andcreate-react-app relying onthis Node-based tool; Rails-centric backend world, however, ismostly ruled by Overcommit that comes as aRuby gem. Find detailed comparisons ofLefthook with other tools intheprojects wiki. Both tools are excellent intheir regard, but inamixed team offrontend andbackend developers, as Evil Martians are, you will often end up having two separate setups for Ruby andJavaScript with frontenders andbackenders linting their commits each intheir way. With Lefthook, you dont need tothink twiceits asingle Go binary that has wrappers both for JavaScript andfor Ruby. It can also beused as astandalone tool for any other environment. For most common use cases, Lefthook requires zero setup. Go language makes Lefthook lightning-fast andprovides support for concurrently executed scripts out ofthebox. Thefact that theexecutable isasingle machine code binary also removes theneed for minding external dependencies (Husky + lint-staged add roughly fifteen hundred dependencies toyour node_modules). It also removes theheadache ofreinstalling dependencies after each update ofyour development environment (try running aglobally installed Ruby gem with another version ofRuby). With Lefthook mentioned either inpackage.json orGemfile, andalefthook.yml configured intheprojects root (see examples below) thetool will beinstalled andused against your code automatically onthenext git pull, yarn install/ bundle install andgit add/git commitwith zero overhead for new contributors. An extensive README describes all possible usage scenarios. Thestraightforward syntax for configuration does not hide actual commands being run by Lefthookmaking sure that nothing funny ishappening below thebelt. Discourse with apunch Discoursean incredibly popular open source platform for forum-style discussionshas recently transitioned from Overcommit toLefthook andnever looked back. With almost 700 contributors authoring 34K commits andcounting, running linters onall new contributions isapriority. With Overcommit though, team members had toremind newcomers toinstall required tools continually Now with @arkweid/lefthook being adev dependency intheprojects package.json, nosetup isnecessary for new contributors. Lefthook allowed tohalf theamount oftime that pre-commit scripts take onlocalhost. The PR that changed theGit hook manager required, inessence, changing .overcommit.yml tolefthook.yml. Ifyou compare themyou will see that Lefthooks configuration ismuch more explicit while theOvercommits one relies mostly onthemagic ofplugins. Discourses CI output before andafter Lefthook Besides changing theway theoutput looksLefthook offers anice summary ofeverything it doesLefthook allowed tohalf theamount oftime that pre-commit scripts take onlocalhost, andincrease theCI run by 20% (on CI environments with better support for parallel execution thegain can beconsiderably more). A pretty bonus Round one Everything Lefthook needs tofunctionis alefthook binary installed somewhere inyour system (either globally orlocally), andalefthook.yml file inaproject root. The binary can either beinstalled globally (with Homebrew for macOS, snap for Ubuntu, AUR for Arch, orgo get anywhere), orlisted as adevelopment dependency either inRubys Gemfile, orNode.js package.json. If you are configuring Lefthook for thefirst time inyour projectyou need tochoose between these options, depending onyour preferences. Themain upside ofputting lefthook inyour Gemfile or@arkweid/lefthook inyour package.json isthat you dont need toworry that your contributors may not have lefthook installed system-wideafter thenext bundle install oryarn install thebinary will beinplace. After you have lefthook inyour systemrun lefthook install intheprojects root togenerate lefthook.yml andvisit theprojects repo for examples onhow touse thesyntax, here isaparticularly full one. Here ishow thecode describing actions oneach pre-commit (right after you type git commit -m \"new feature\" but right before it gets committed) might look like: pre-commit: commands: stylelint: tags: frontend style glob: \"*.{js}\" run: yarn stylelint {staged_files} rubocop: tags: backend style glob: \"*.{rb}\" exclude: \"application.rb|routes.rb\" run: bundle exec rubocop {all_files} scripts: \"good_job.js\": runner: node Then commit lefthook.yml toyour repository andpush it toyour favorite remotenow everyone onyour team will see it inaction every time they commit code. Blow by blow If you want tocheck Lefthook out onademo project quicklywe recommend cloning theevil_chat repositorya project we build inour celebrated Modern Frontend inRails series ofposts (Pt. 1, Pt. 2, Pt. 3). In this project, we use pre-commit hooks configured with Lefthook for formatting JavaScript andCSS files with Prettier, andlinting them with ESlint andstylelint. Heres how toquickly see Lefthook inaction. First, clone therepo andrun package managers: $ git clone git@github.com:demiazz/evil_chat.git $ bundle && yarn Now, go andbreak some CSS orJS inany .pcss or.js files. $ git add . && git commit -m \"Now I am become death, destroyer of worlds\" Wait for it! If all goes well (meaning you succeeded inbreaking thecode), this iswhat you are going tosee: A bad commit The failing scripts are shown with aboxing glove emoji ontheoutputgiving you areal left hook tobring back your attention! For now, our linting covers only thefrontend part oftheapp. What about adding some good old Rubocop tothemix? Edit your lefthook.yml toinclude thefollowing lines: # lefthook.yml pre-commit: parallel: true # tell Lefthook to utilise all cores commands: js: glob: \"*.js\" run: yarn prettier --write {staged_files} && yarn eslint {staged_files} && git add {staged_files} css: glob: \"*.{css,pcss}\" run: yarn prettier --write {staged_files} && yarn stylelint --fix {staged_files} && git add {staged_files} # Add these lines rubocop: glob: \"*.{rb}\" run: rubocop {staged_files} --parallel Note thehandy {staged_files} shortcut that allows you totarget only thefiles that are staged for acurrent commit. Now go back tofix your JS andCSS andcommit astyle offense inany oftheRuby files (yes, you have our permission). Feel free tothrow insome comments here andthere so that git picks up changes for different filetypes. Rubocop comes into play Now CSS andJS are fine, but Ruby needs another look, hence theleft hook! Roll with thepunches Here isthesummary ofthefeatures that make Lefthook stand out ofcompetition, andbring flexibility toyour workflow, see thecomplete list here. Speed Lefthook squeezes out every bit ofparallelism from your machine (or aCI server), you only need totoggle one setting: parallel: true. Heres theconfig file that describes alint series ofcommands that you can run with lefthook run lint from your command line. These are thesame commands that Discourse used torun onTravis. Lefthook gives you anability torun custom tasks like that. As analternative, you can set thesame commands toberun onpre-commit, pre-push, post-checkout, post-merge, orany other available Git hooks. # lefthook.yml lint: # parallel: true commands: rubocop: run: bundle exec rubocop --parallel prettier: run: yarn prettier --list-different \"app/assets/stylesheets/**/*.scss\" \"app/assets/javascripts/**/*.es6\" \"test/javascripts/**/*.es6\" eslint-assets: run: yarn eslint --ext .es6 app/assets/javascripts eslint-test: run: yarn eslint --ext .es6 test/javascripts eslint-plugins-assets: run: yarn eslint --ext .es6 plugins/**/assets/javascripts eslint-plugins-test: run: yarn eslint --ext .es6 plugins/**/test/javascripts eslint-assets-tests: run: yarn eslint app/assets/javascripts test/javascripts With parallel: true commented out, onmy system, this task takes over 30 seconds. With theparallel feature turned on, it takes 15.5 secondstwice as fast! Flexibility Direct control If you want torun your hook directly, without waiting for aGit action: $ lefthook run pre-commit Flexible lists offiles You can use built-in shortcuts {staged_files} and{all_files}, ordefine your own lists according tospecific selection. pre-commit: commands: frontend-linter: run: yarn eslint {staged_files} backend-linter: run: bundle exec rubocop {all_files} frontend-style: files: git diff --name-only HEAD @{push} run: yarn stylelint {files} Glob/Regex filters If you want tofilter alist offiles onthefly with aglob oraRegex. pre-commit: commands: backend-linter: glob: \"*.{rb}\" # glob filter exclude: \"application.rb|routes.rb\" # regexp filter run: bundle exec rubocop {all_files} Run your own scripts If one-liners are not enough, you can tell Lefthook toexecute custom scripts. commit-msg: scripts: \"good_job\": runner: bash Tags andlocal config for even more flexibility You can group your tasks by tags andthen exclude them when you run hooks locally (e.g., you are abackend developer andnot interested inrunning tasks onfrontend code). With Lefthook, you can create alefthook-local.yml file inyour project root (dont forget toadd it toyour .gitignore): all thesettings described here would override theones from themain lefthook.yml. Now you can assign tags todifferent series ofcommands # lefthook.yml pre-push: commands: stylelint: tags: frontend-style # a tag files: git diff --name-only master glob: \"*.{js}\" run: yarn stylelint {files} rubocop: tags: backend-style # a tag files: git diff --name-only master glob: \"*.{rb}\" run: bundle exec rubocop {files} andexclude them from being run locally: # lefthook-local.yml pre-push: exclude_tags: - frontend-style K.O. Do you use Docker for local development? Perhaps you do, perhaps not yet, but there isabig chance that someone else onyour team does otherwise. Some people embrace containerized development fully, while others prefer torely onwell-groomed local environments. Your main lefthook.yml may contain thefollowing: post-push: scripts: \"good_job.js\": runner: bash However, you want torun thesame task inaDocker container, but you dont want tomess up thesetup for everyone else. By using alefthook-local.yml file (the one you dont check ininto version control), you can alter thecommand just slightly, andjust for your local setup, by using a{cmd} shortcut, just like that: # lefthook-local.yml pre-commit: scripts: \"good_job.js\": runner: docker exec -it --rm <container_id_or_name> {cmd} {cmd} will bereplaced by acommand from themain config. The resulting command will look like this: docker exec -it --rm <container_id_or_name> node good_job.js 8 9 10 Knockout! We are confident that Lefthook iscurrently thefastest andmost flexible Git hook manager inour galaxy, so we encourage everyone tofollow theexample ofDiscourse andeither add Lefthook toyour projects orcreate apull request proposing thechange inyour favorite open source repositories. The polyglot nature ofLefthook allows it tobeused inpure frontend, pure backend, ormixed full-stack teams, andwith all common development setups onall major operating systems, including Windows. Find thebest way toinstall it, depending onyour stack, andgive it ago! See how we use Lefthook intandem with Crystalball inour commercial projects by checking out this post onDev.to. If you see our work onLefthook as yet another perfect example ofreinventing thewheel, still give it atryyou will soon realize that Lefthook ismore ofajet pack, then yet another good old wheel for git hooks management. And never, never throw inthetowel onautomating your Git andGitHub workflow! "
        ],
        "story_type": "Normal",
        "url_raw": "https://evilmartians.com/chronicles/lefthook-knock-your-teams-code-back-into-shape",
        "comments.comment_id": [20576448, 20579361],
        "comments.comment_author": ["davidjnelson", "vemv"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-07-31T17:34:50Z",
          "2019-07-31T22:42:48Z"
        ],
        "comments.comment_text": [
          "Love these kinds of tools.  What is different between husky / git-staged and this?<p>It’s faster and smaller?<p>Quite happy with husky and running tslint/tests with husky on precommit currently.  Curious what the biggest benefits are from left hook and if it’s a pain to switch to if they are amazing benefits.<p>The biggest annoyance with current setup is on prem ci with Jenkins and GitHub Enterprise and complex docker bash scripts embedded in Jenkins.  It’s a pain to get lint/test results to show up in the GitHub pull request ui.  Doesn’t seem like this tool would help make that use case faster to set up.",
          "Generally I'm of the opinion that git hooks are not the right place to place linters or formatters.<p>For a JVM language, it means that you'll spin up a JVM for each step (and you can well have 15 formatters + linters in your stack).<p>So, the formatter better lives in a dev-only part of your codebase, so it's always loaded, interacting with your codebase using _runtime insights_ otherwise impossible to gather, and it's easily hackable.<p>I've authored this kind of solution in the past. It works great, and a proper CI pipeline (i.e. including linters) kills the other half of the problem."
        ],
        "id": "96f2b5b3-b5f6-4c35-ae03-0c6b3c516179",
        "url_text": "Meet Lefthook, thefastest polyglot Git hooks manager out there, andmake sure not asingle line ofunruly code makes it into production. See how easy it istoinstall Lefthook, recently adopted by Discourse, Logux, andOpenstax, for most common frontend andbackend environments andensure all developers onyour team can rely onasingle flexible tool. Andit also has emojis Days, when asingle piece ofsoftware that millions rely onwas created by asingle developer inanivory tower, are long gone. Even Git, universally believed tobethebrainchild ofLinus Torvalds alone, was created with thehelp ofcontributors andisnow being maintained by ateam ofdozens. No matter ifyou work onanopen source project with thewhole world being your oyster, oryou are blooming inawalled garden ofproprietary commercial softwareyou still work inateam. Andeven with awell-organized system ofpull requests andcode reviews maintaining thecode quality across thelarge codebase with dozens ofcontributors isnot aneasy task. Hook me up Hooksways tofire off custom scripts when certain important actions (commit, push, etc.) occurare baked right into Git, so ifyou are comfortable with Bash andtheinternals oftheworld most popular version control systemyou dont need any external tools per se: just edit ./.git/hooks/pre-commit andput insome well-formed script that will, for instance, lint your files before you commit. However, when you work onaproject, you are most interested inwriting projects codenot thecode that checks it. Intheworld ofmodern web development tooling iseverything, andmyriads oftools exist for asingle reason: reducing overhead andcomplexity. Git hooks are not theexception: inJavaScript community, theweapon ofchoice isHusky with Webpack, Babel, andcreate-react-app relying onthis Node-based tool; Rails-centric backend world, however, ismostly ruled by Overcommit that comes as aRuby gem. Find detailed comparisons ofLefthook with other tools intheprojects wiki. Both tools are excellent intheir regard, but inamixed team offrontend andbackend developers, as Evil Martians are, you will often end up having two separate setups for Ruby andJavaScript with frontenders andbackenders linting their commits each intheir way. With Lefthook, you dont need tothink twiceits asingle Go binary that has wrappers both for JavaScript andfor Ruby. It can also beused as astandalone tool for any other environment. For most common use cases, Lefthook requires zero setup. Go language makes Lefthook lightning-fast andprovides support for concurrently executed scripts out ofthebox. Thefact that theexecutable isasingle machine code binary also removes theneed for minding external dependencies (Husky + lint-staged add roughly fifteen hundred dependencies toyour node_modules). It also removes theheadache ofreinstalling dependencies after each update ofyour development environment (try running aglobally installed Ruby gem with another version ofRuby). With Lefthook mentioned either inpackage.json orGemfile, andalefthook.yml configured intheprojects root (see examples below) thetool will beinstalled andused against your code automatically onthenext git pull, yarn install/ bundle install andgit add/git commitwith zero overhead for new contributors. An extensive README describes all possible usage scenarios. Thestraightforward syntax for configuration does not hide actual commands being run by Lefthookmaking sure that nothing funny ishappening below thebelt. Discourse with apunch Discoursean incredibly popular open source platform for forum-style discussionshas recently transitioned from Overcommit toLefthook andnever looked back. With almost 700 contributors authoring 34K commits andcounting, running linters onall new contributions isapriority. With Overcommit though, team members had toremind newcomers toinstall required tools continually Now with @arkweid/lefthook being adev dependency intheprojects package.json, nosetup isnecessary for new contributors. Lefthook allowed tohalf theamount oftime that pre-commit scripts take onlocalhost. The PR that changed theGit hook manager required, inessence, changing .overcommit.yml tolefthook.yml. Ifyou compare themyou will see that Lefthooks configuration ismuch more explicit while theOvercommits one relies mostly onthemagic ofplugins. Discourses CI output before andafter Lefthook Besides changing theway theoutput looksLefthook offers anice summary ofeverything it doesLefthook allowed tohalf theamount oftime that pre-commit scripts take onlocalhost, andincrease theCI run by 20% (on CI environments with better support for parallel execution thegain can beconsiderably more). A pretty bonus Round one Everything Lefthook needs tofunctionis alefthook binary installed somewhere inyour system (either globally orlocally), andalefthook.yml file inaproject root. The binary can either beinstalled globally (with Homebrew for macOS, snap for Ubuntu, AUR for Arch, orgo get anywhere), orlisted as adevelopment dependency either inRubys Gemfile, orNode.js package.json. If you are configuring Lefthook for thefirst time inyour projectyou need tochoose between these options, depending onyour preferences. Themain upside ofputting lefthook inyour Gemfile or@arkweid/lefthook inyour package.json isthat you dont need toworry that your contributors may not have lefthook installed system-wideafter thenext bundle install oryarn install thebinary will beinplace. After you have lefthook inyour systemrun lefthook install intheprojects root togenerate lefthook.yml andvisit theprojects repo for examples onhow touse thesyntax, here isaparticularly full one. Here ishow thecode describing actions oneach pre-commit (right after you type git commit -m \"new feature\" but right before it gets committed) might look like: pre-commit: commands: stylelint: tags: frontend style glob: \"*.{js}\" run: yarn stylelint {staged_files} rubocop: tags: backend style glob: \"*.{rb}\" exclude: \"application.rb|routes.rb\" run: bundle exec rubocop {all_files} scripts: \"good_job.js\": runner: node Then commit lefthook.yml toyour repository andpush it toyour favorite remotenow everyone onyour team will see it inaction every time they commit code. Blow by blow If you want tocheck Lefthook out onademo project quicklywe recommend cloning theevil_chat repositorya project we build inour celebrated Modern Frontend inRails series ofposts (Pt. 1, Pt. 2, Pt. 3). In this project, we use pre-commit hooks configured with Lefthook for formatting JavaScript andCSS files with Prettier, andlinting them with ESlint andstylelint. Heres how toquickly see Lefthook inaction. First, clone therepo andrun package managers: $ git clone git@github.com:demiazz/evil_chat.git $ bundle && yarn Now, go andbreak some CSS orJS inany .pcss or.js files. $ git add . && git commit -m \"Now I am become death, destroyer of worlds\" Wait for it! If all goes well (meaning you succeeded inbreaking thecode), this iswhat you are going tosee: A bad commit The failing scripts are shown with aboxing glove emoji ontheoutputgiving you areal left hook tobring back your attention! For now, our linting covers only thefrontend part oftheapp. What about adding some good old Rubocop tothemix? Edit your lefthook.yml toinclude thefollowing lines: # lefthook.yml pre-commit: parallel: true # tell Lefthook to utilise all cores commands: js: glob: \"*.js\" run: yarn prettier --write {staged_files} && yarn eslint {staged_files} && git add {staged_files} css: glob: \"*.{css,pcss}\" run: yarn prettier --write {staged_files} && yarn stylelint --fix {staged_files} && git add {staged_files} # Add these lines rubocop: glob: \"*.{rb}\" run: rubocop {staged_files} --parallel Note thehandy {staged_files} shortcut that allows you totarget only thefiles that are staged for acurrent commit. Now go back tofix your JS andCSS andcommit astyle offense inany oftheRuby files (yes, you have our permission). Feel free tothrow insome comments here andthere so that git picks up changes for different filetypes. Rubocop comes into play Now CSS andJS are fine, but Ruby needs another look, hence theleft hook! Roll with thepunches Here isthesummary ofthefeatures that make Lefthook stand out ofcompetition, andbring flexibility toyour workflow, see thecomplete list here. Speed Lefthook squeezes out every bit ofparallelism from your machine (or aCI server), you only need totoggle one setting: parallel: true. Heres theconfig file that describes alint series ofcommands that you can run with lefthook run lint from your command line. These are thesame commands that Discourse used torun onTravis. Lefthook gives you anability torun custom tasks like that. As analternative, you can set thesame commands toberun onpre-commit, pre-push, post-checkout, post-merge, orany other available Git hooks. # lefthook.yml lint: # parallel: true commands: rubocop: run: bundle exec rubocop --parallel prettier: run: yarn prettier --list-different \"app/assets/stylesheets/**/*.scss\" \"app/assets/javascripts/**/*.es6\" \"test/javascripts/**/*.es6\" eslint-assets: run: yarn eslint --ext .es6 app/assets/javascripts eslint-test: run: yarn eslint --ext .es6 test/javascripts eslint-plugins-assets: run: yarn eslint --ext .es6 plugins/**/assets/javascripts eslint-plugins-test: run: yarn eslint --ext .es6 plugins/**/test/javascripts eslint-assets-tests: run: yarn eslint app/assets/javascripts test/javascripts With parallel: true commented out, onmy system, this task takes over 30 seconds. With theparallel feature turned on, it takes 15.5 secondstwice as fast! Flexibility Direct control If you want torun your hook directly, without waiting for aGit action: $ lefthook run pre-commit Flexible lists offiles You can use built-in shortcuts {staged_files} and{all_files}, ordefine your own lists according tospecific selection. pre-commit: commands: frontend-linter: run: yarn eslint {staged_files} backend-linter: run: bundle exec rubocop {all_files} frontend-style: files: git diff --name-only HEAD @{push} run: yarn stylelint {files} Glob/Regex filters If you want tofilter alist offiles onthefly with aglob oraRegex. pre-commit: commands: backend-linter: glob: \"*.{rb}\" # glob filter exclude: \"application.rb|routes.rb\" # regexp filter run: bundle exec rubocop {all_files} Run your own scripts If one-liners are not enough, you can tell Lefthook toexecute custom scripts. commit-msg: scripts: \"good_job\": runner: bash Tags andlocal config for even more flexibility You can group your tasks by tags andthen exclude them when you run hooks locally (e.g., you are abackend developer andnot interested inrunning tasks onfrontend code). With Lefthook, you can create alefthook-local.yml file inyour project root (dont forget toadd it toyour .gitignore): all thesettings described here would override theones from themain lefthook.yml. Now you can assign tags todifferent series ofcommands # lefthook.yml pre-push: commands: stylelint: tags: frontend-style # a tag files: git diff --name-only master glob: \"*.{js}\" run: yarn stylelint {files} rubocop: tags: backend-style # a tag files: git diff --name-only master glob: \"*.{rb}\" run: bundle exec rubocop {files} andexclude them from being run locally: # lefthook-local.yml pre-push: exclude_tags: - frontend-style K.O. Do you use Docker for local development? Perhaps you do, perhaps not yet, but there isabig chance that someone else onyour team does otherwise. Some people embrace containerized development fully, while others prefer torely onwell-groomed local environments. Your main lefthook.yml may contain thefollowing: post-push: scripts: \"good_job.js\": runner: bash However, you want torun thesame task inaDocker container, but you dont want tomess up thesetup for everyone else. By using alefthook-local.yml file (the one you dont check ininto version control), you can alter thecommand just slightly, andjust for your local setup, by using a{cmd} shortcut, just like that: # lefthook-local.yml pre-commit: scripts: \"good_job.js\": runner: docker exec -it --rm <container_id_or_name> {cmd} {cmd} will bereplaced by acommand from themain config. The resulting command will look like this: docker exec -it --rm <container_id_or_name> node good_job.js 8 9 10 Knockout! We are confident that Lefthook iscurrently thefastest andmost flexible Git hook manager inour galaxy, so we encourage everyone tofollow theexample ofDiscourse andeither add Lefthook toyour projects orcreate apull request proposing thechange inyour favorite open source repositories. The polyglot nature ofLefthook allows it tobeused inpure frontend, pure backend, ormixed full-stack teams, andwith all common development setups onall major operating systems, including Windows. Find thebest way toinstall it, depending onyour stack, andgive it ago! See how we use Lefthook intandem with Crystalball inour commercial projects by checking out this post onDev.to. If you see our work onLefthook as yet another perfect example ofreinventing thewheel, still give it atryyou will soon realize that Lefthook ismore ofajet pack, then yet another good old wheel for git hooks management. And never, never throw inthetowel onautomating your Git andGitHub workflow! ",
        "_version_": 1718536506197409792
      },
      {
        "story_id": 20636546,
        "story_author": "vikrum",
        "story_descendants": 19,
        "story_score": 127,
        "story_time": "2019-08-07T16:58:59Z",
        "story_title": "Launch HN: Gold Fig (YC S19) – Version Control for Settings Pages",
        "search": [
          "Launch HN: Gold Fig (YC S19) – Version Control for Settings Pages",
          "Ahoy HN! We’re Greg and Vikrum, co-founders of Gold Fig.<p>Gold Fig is a tool that automatically creates a shared log of configuration changes to the SaaS tools you use. Modern applications are built atop a menagerie of these tools. Services like Stripe, SendGrid, Zapier, Segment, Twilio, Sentry, Travis, GSuite, domain registrars, CDNs, or even internal dashboards can directly affect your production and corporate environments, yet their configuration is not tracked with anything near the same fidelity as source code.<p>Mistakes occur when people make config changes without proper context. Depending on which service was impacted this can result in outage, loss of revenue, or reputational harm. It’s usually the thing that broke in a subtle way for some extended period of time that bites the worst. Moreover, when an incident does occur, the respondent often also has limited context about changes made, leading to longer resolution times and possibly even further misconfiguration as previous configuration was lost. As an example, we've personally experienced this pain when managing the CDNs fronting core services. We had to synchronize changes across our Fastly configurations, DNS records, and origin servers, with no single source of truth to guide us. Any mistake could result in downtime.<p>Some teams attempt to address this lack of context by putting one person in charge of doing all of the configuration for a service provider, leading to development bottlenecks. Other teams attempt to manually track these updates in a text file, email threads, or in their team chat. With Gold Fig, we want teams to be able to confidently share the management of their SaaS tools. Team members should have access to the full context behind all of the configurations they manage, and should be able to easily keep themselves up to date as they evolve. Gold Fig lives alongside automation tools like Terraform and Cloudformation, allowing you to plug the gaps that those tools can’t cover.<p>Our initial product is a browser extension that automatically launches on settings pages of SaaS tools. When you make a change on these sites, the extension gives you the opportunity to also provide a commit message, similar to how you would with a code check-in. Now you have a record of some button clicks that impacted your environment. The change has a permanent URL so you can look it up later if you forgot what you did, use it to help you move settings from staging to prod, or have others review the changes that were made. Now that Gold Fig has captured what changed, when, and why, you’ll never get stuck in a situation where only one person knows exactly how something was configured.  A byproduct of Gold Fig is that you now have a foothold into being able to undo these types of changes. We envision Gold Fig being part of all devops team’s way of surfacing and tracking changes. In the future we’ll be able to empower teams to do pull-request like approvals, show context before a change is about to occur, and provide more awareness to those responding production incidents.<p>Our extension is able to capture payloads while being generally agnostic of the site itself. We’ll capture changes even if we haven’t seen the site before or if something has changed from the previous time we encountered it. For common sites like AWS or GCP we capture additional rich context like the product being impacted, the region/zone, and project name. We also aim to work on sites we don’t have access to like internal dashboards teams have built to manage customers, environments, or settings.<p>We’d love to hear your experience with settings pages and SaaS configs. We’ll be here listening to your feedback, answering your questions, and happy to field any feature requests for Gold Fig you may have. You can give it a whirl here:  <a href=\"https://app.goldfiglabs.com/\" rel=\"nofollow\">https://app.goldfiglabs.com/</a> Thank you!<p>Greg & Vikrum - hello@goldfiglabs.com"
        ],
        "story_text": "Ahoy HN! We’re Greg and Vikrum, co-founders of Gold Fig.<p>Gold Fig is a tool that automatically creates a shared log of configuration changes to the SaaS tools you use. Modern applications are built atop a menagerie of these tools. Services like Stripe, SendGrid, Zapier, Segment, Twilio, Sentry, Travis, GSuite, domain registrars, CDNs, or even internal dashboards can directly affect your production and corporate environments, yet their configuration is not tracked with anything near the same fidelity as source code.<p>Mistakes occur when people make config changes without proper context. Depending on which service was impacted this can result in outage, loss of revenue, or reputational harm. It’s usually the thing that broke in a subtle way for some extended period of time that bites the worst. Moreover, when an incident does occur, the respondent often also has limited context about changes made, leading to longer resolution times and possibly even further misconfiguration as previous configuration was lost. As an example, we've personally experienced this pain when managing the CDNs fronting core services. We had to synchronize changes across our Fastly configurations, DNS records, and origin servers, with no single source of truth to guide us. Any mistake could result in downtime.<p>Some teams attempt to address this lack of context by putting one person in charge of doing all of the configuration for a service provider, leading to development bottlenecks. Other teams attempt to manually track these updates in a text file, email threads, or in their team chat. With Gold Fig, we want teams to be able to confidently share the management of their SaaS tools. Team members should have access to the full context behind all of the configurations they manage, and should be able to easily keep themselves up to date as they evolve. Gold Fig lives alongside automation tools like Terraform and Cloudformation, allowing you to plug the gaps that those tools can’t cover.<p>Our initial product is a browser extension that automatically launches on settings pages of SaaS tools. When you make a change on these sites, the extension gives you the opportunity to also provide a commit message, similar to how you would with a code check-in. Now you have a record of some button clicks that impacted your environment. The change has a permanent URL so you can look it up later if you forgot what you did, use it to help you move settings from staging to prod, or have others review the changes that were made. Now that Gold Fig has captured what changed, when, and why, you’ll never get stuck in a situation where only one person knows exactly how something was configured.  A byproduct of Gold Fig is that you now have a foothold into being able to undo these types of changes. We envision Gold Fig being part of all devops team’s way of surfacing and tracking changes. In the future we’ll be able to empower teams to do pull-request like approvals, show context before a change is about to occur, and provide more awareness to those responding production incidents.<p>Our extension is able to capture payloads while being generally agnostic of the site itself. We’ll capture changes even if we haven’t seen the site before or if something has changed from the previous time we encountered it. For common sites like AWS or GCP we capture additional rich context like the product being impacted, the region/zone, and project name. We also aim to work on sites we don’t have access to like internal dashboards teams have built to manage customers, environments, or settings.<p>We’d love to hear your experience with settings pages and SaaS configs. We’ll be here listening to your feedback, answering your questions, and happy to field any feature requests for Gold Fig you may have. You can give it a whirl here:  <a href=\"https://app.goldfiglabs.com/\" rel=\"nofollow\">https://app.goldfiglabs.com/</a> Thank you!<p>Greg & Vikrum - hello@goldfiglabs.com",
        "story_type": "LanchHN",
        "comments.comment_id": [20637062, 20637521],
        "comments.comment_author": ["_pius", "mayop100"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-08-07T17:50:40Z",
          "2019-08-07T18:35:56Z"
        ],
        "comments.comment_text": [
          "<i>... A browser extension that automatically launches on settings pages of SaaS tools. When you make a change on these sites, the extension gives you the opportunity to also provide a commit message, similar to how you would with a code check-in. Now you have a record of some button clicks that impacted your environment. The change has a permanent URL so you can look it up later if you forgot what you did, use it to help you move settings from staging to prod, or have others review the changes that were made. ... Our extension is able to capture payloads while being generally agnostic of the site itself. We’ll capture changes even if we haven’t seen the site before or if something has changed from the previous time we encountered it.</i><p>This sounds amazing, would have definitely made my life easier on many projects ... looking forward to checking it out!<p>Oh, and one thing to add if you haven’t already is the ability to whitelist which domains the extension is active on for privacy purposes.",
          "The Gold Fig founders know what they're talking about here. Vikrum ran devops for Firebase from the first server in 2011 up to millions of users after the Google acquisition. Greg was the TL for the Firebase Realtime Database, our flagship product that powers a huge number of apps. They're experts, not only because of what they learned building Firebase, but also because they got insight into how our big customers ran their services and teams.<p>Vikrum actually built a browser plugin at Firebase that was simple but incredibly useful. It color-coded cloud settings pages for different environments (prod, staging, dev) to prevent someone from accidentally changing the config in prod when they didn't mean to. We required the whole Firebase team to use it, and it honestly saved us from multiple downtime incidents.<p>If you're managing a service, I really suggest you try Gold Fig out. You can get the benefit of everything that Greg and Vikrum have learned (painfully from years of real experience) about how to run services and be safe with your config!<p>(I'm one of the Firebase founders)"
        ],
        "id": "c5c857ee-012d-4260-9042-5c68a288318e",
        "_version_": 1718536508332310528
      },
      {
        "story_id": 20995200,
        "story_author": "andygcook",
        "story_descendants": 349,
        "story_score": 848,
        "story_time": "2019-09-17T14:05:24Z",
        "story_title": "Gitlab More Than Doubles Valuation to $2.75B Ahead of Planned 2020 IPO",
        "search": [
          "Gitlab More Than Doubles Valuation to $2.75B Ahead of Planned 2020 IPO",
          "https://www.forbes.com/sites/alexkonrad/2019/09/17/gitlab-doubles-valuation-to-nearly-3-billion/",
          "Edit StoryEditors' Pick|Sep 17, 2019,09:00am EDT|This article is more than 2 years old. Sid Sijbrandij is having no difficulty finding investor suitors ahead of a planned 2020 IPO. GitLab Some tech companies shy away from putting a date on going publicor even making a decision about whether they plan to list their businesses at all. At GitLab, Sid Sijbrandij already has a date set: November 18, 2020. Theres symbolic meaning behind it: Two days before is the birthdate of the twin children of the CFO of GitLab, a leader in the expanding area of tech known as DevOps. But thats a Monday. Better to go out midweek, Sijbrandij thoughtand on his grandfathers 100th birthday. You talk about how you are going to achieve things, and internally we can say, this is where we are going, he says. A tech unicorn setting such specifics is unusual. But GitLab isnt feeling any pressure. Investors are eager to pour funds into the company, which says its annualized revenue is growing at a rate of 143% year-to-year, with net retention of customer spending at 153%. The latest: a $268 million Series E funding round that values GitLab at $2.75 billion. The round more than doubles GitLabs valuation from its previous funding round, when it was valued at $1.1 billion, a year ago. This time, GitLab is backed by a consortium of investors who are eager to see its founders IPO plans come to fruition soon. Iconiq Capital, the lead investor in its previous raise, co-led the round with Goldman Sachs, itself a major GitLab customer. Y Combinators growth fund also joined, as well as a group intended to help guide the company from private to public that includes Coatue, Franklin Templeton, Tiger Global Management and Two Sigma Ventures. Appearing at No. 32 on the 2019 Forbes Cloud 100 list, GitLab is unusual not just for its blunt IPO plans but also for its mostly no-office expansion (its official address in San Francisco is a UPS store). As a leading startup in DevOps, GitLab works behind the scenes, helping customers like Delta, Nvidia and Ticketmaster ship their own software faster. With big businesses often using ten or more applications to build and release their own software, Sijbrandij opted for a platform approach that put specialists in product, development, security and operations all working together in one app. At Goldman Sachs, for example, more than 7,500 people use GitLab on a daily basis, GitLab says. With no formal office, GitLab meetings happen virtually over tools like Zoom. GitLab To keep up with it all, GitLabs employee ranks have swelled to more than 800 in 55 countries. The funding fuels plans to hire hundreds more. And as it hires, GitLab plans to beef up its monitoring, planning and security tools in upcoming months. The company also must provide support for a growing base of more than 100,000 organizations using its tools. Though GitLab is reaching public-company scale, Sijbrandij still operates the business in his own unusual way. On a call ahead of the funding, he warns that were not alone in addition to a public relations professional, there are two other managers on the call, one from marketing and one from product. Theyre part of a shadow program in which employees spend two weeks sitting in on their CEOs meetings, feedback sessions and media or analyst calls. Sijbrandij instituted the program six months ago while looking for a chief of staff. The program, he says, can teach employees the ins and outs of the business at higher velocity. Investors who back GitLab are signing up for such experiments and Sijbrandijs unique brand of leadership. So far, theres plenty of demandeven for GitLab publicizing its IPO target date. Since we were clear about our ambition, that gave them extra faith that we will succeed, Sijbrandij says. Follow me onTwitterorLinkedIn.Send me a securetip.I'm a senior editor at Forbes covering venture capital, cloud and enterprise software out of New York. I edit the Midas List, Midas List Europe, Cloud 100 list and 30PrintReprints & Permissions "
        ],
        "story_type": "Normal",
        "url_raw": "https://www.forbes.com/sites/alexkonrad/2019/09/17/gitlab-doubles-valuation-to-nearly-3-billion/",
        "url_text": "Edit StoryEditors' Pick|Sep 17, 2019,09:00am EDT|This article is more than 2 years old. Sid Sijbrandij is having no difficulty finding investor suitors ahead of a planned 2020 IPO. GitLab Some tech companies shy away from putting a date on going publicor even making a decision about whether they plan to list their businesses at all. At GitLab, Sid Sijbrandij already has a date set: November 18, 2020. Theres symbolic meaning behind it: Two days before is the birthdate of the twin children of the CFO of GitLab, a leader in the expanding area of tech known as DevOps. But thats a Monday. Better to go out midweek, Sijbrandij thoughtand on his grandfathers 100th birthday. You talk about how you are going to achieve things, and internally we can say, this is where we are going, he says. A tech unicorn setting such specifics is unusual. But GitLab isnt feeling any pressure. Investors are eager to pour funds into the company, which says its annualized revenue is growing at a rate of 143% year-to-year, with net retention of customer spending at 153%. The latest: a $268 million Series E funding round that values GitLab at $2.75 billion. The round more than doubles GitLabs valuation from its previous funding round, when it was valued at $1.1 billion, a year ago. This time, GitLab is backed by a consortium of investors who are eager to see its founders IPO plans come to fruition soon. Iconiq Capital, the lead investor in its previous raise, co-led the round with Goldman Sachs, itself a major GitLab customer. Y Combinators growth fund also joined, as well as a group intended to help guide the company from private to public that includes Coatue, Franklin Templeton, Tiger Global Management and Two Sigma Ventures. Appearing at No. 32 on the 2019 Forbes Cloud 100 list, GitLab is unusual not just for its blunt IPO plans but also for its mostly no-office expansion (its official address in San Francisco is a UPS store). As a leading startup in DevOps, GitLab works behind the scenes, helping customers like Delta, Nvidia and Ticketmaster ship their own software faster. With big businesses often using ten or more applications to build and release their own software, Sijbrandij opted for a platform approach that put specialists in product, development, security and operations all working together in one app. At Goldman Sachs, for example, more than 7,500 people use GitLab on a daily basis, GitLab says. With no formal office, GitLab meetings happen virtually over tools like Zoom. GitLab To keep up with it all, GitLabs employee ranks have swelled to more than 800 in 55 countries. The funding fuels plans to hire hundreds more. And as it hires, GitLab plans to beef up its monitoring, planning and security tools in upcoming months. The company also must provide support for a growing base of more than 100,000 organizations using its tools. Though GitLab is reaching public-company scale, Sijbrandij still operates the business in his own unusual way. On a call ahead of the funding, he warns that were not alone in addition to a public relations professional, there are two other managers on the call, one from marketing and one from product. Theyre part of a shadow program in which employees spend two weeks sitting in on their CEOs meetings, feedback sessions and media or analyst calls. Sijbrandij instituted the program six months ago while looking for a chief of staff. The program, he says, can teach employees the ins and outs of the business at higher velocity. Investors who back GitLab are signing up for such experiments and Sijbrandijs unique brand of leadership. So far, theres plenty of demandeven for GitLab publicizing its IPO target date. Since we were clear about our ambition, that gave them extra faith that we will succeed, Sijbrandij says. Follow me onTwitterorLinkedIn.Send me a securetip.I'm a senior editor at Forbes covering venture capital, cloud and enterprise software out of New York. I edit the Midas List, Midas List Europe, Cloud 100 list and 30PrintReprints & Permissions ",
        "comments.comment_id": [20995527, 20995864],
        "comments.comment_author": ["ko3us", "petercooper"],
        "comments.comment_descendants": [5, 12],
        "comments.comment_time": [
          "2019-09-17T14:29:26Z",
          "2019-09-17T14:54:24Z"
        ],
        "comments.comment_text": [
          "We’ve been using Gitlab for 4 years now.<p>What got us initially was the free private repos before github had that.<p>We are now a paying customer.<p>Their integrated CICD is amazing. It works perfectly for all our needs and integrates really easily with AWS and GCP.<p>Also their customer service is really damn good. If I ever have an issue, it’s dealt with so fast and with so much detail. Honestly one of the best customer service I’ve experienced.<p>Their product is feature rich, priced right and is easy.<p>I’m amazed at how the operate. Kudos to the team",
          "Does anyone know why GitLab hasn't taken off so much amongst open source projects?<p>I have no horse in the race (indeed, I'd love for there to be more variety in this space) but one of my jobs is to link to open source repos and I've just checked.. and the last one I linked to was in December 2018. In the niches I cover, almost no-one seems to actually using GitLab for their open source repos.<p>Lest you think it's just me, compare <a href=\"https://news.ycombinator.com/from?site=github.com\" rel=\"nofollow\">https://news.ycombinator.com/from?site=github.com</a> to <a href=\"https://news.ycombinator.com/from?site=gitlab.com\" rel=\"nofollow\">https://news.ycombinator.com/from?site=gitlab.com</a> .. the first is packed with projects posted here on a daily basis. The latter? 13 project links in about 50 days."
        ],
        "id": "7da46dd6-1f2a-4299-ad86-22d84785c705",
        "_version_": 1718536522976722945
      },
      {
        "story_id": 20618427,
        "story_author": "jgraham",
        "story_descendants": 8,
        "story_score": 52,
        "story_time": "2019-08-05T20:16:49Z",
        "story_title": "Git-Revise",
        "search": [
          "Git-Revise",
          "https://mystor.github.io/git-revise.html",
          "NIKA:\\git-revise\\> list (Aug. 6, 2019): Added the \"What git-revise is not\" section. At Mozilla I often end up building my changes in a patch stack, and used git rebase -i1 to make changes to commits in response to review comments etc. Unfortunately, with a repository as large as mozilla-central2, git rebase -i has some downsides: It's slow! Rebase operates directly on the worktree, so it performs a full checkout of each commit in the stack, and frequently refreshes worktree state. On large repositories (especially on NTFS) that can take a long time. It triggers rebuilds! Because rebase touches the file tree, some build systems (like gecko's recursive-make backend) rebuild unnecessarially. It's stateful! If the rebase fails, the repository is in a weird mid-rebase state, and in edge cases I've accidentally dropped commits due to other processes racing on the repository lock. It's clunky! Common tasks (like splitting & rewording commits) require multiple steps and are unintuitive. Naturally, I did the only reasonable thing: Build a brand-new tool. source: xkcd git-revise is a history editing tool designed for the patch-stack workflow. It's fast, non-destructive, and aims to provide a familiar, powerful, and easy to use re-imagining of the patch stack workflow. It's fast I would never claim to be a benchmarking expert 3, but git-revise performs substantially better than rebase for small history editing tasks 4. In a test applying a single-line change to a mozilla-central commit 20 patches up the stack I saw a 15x speed improvement. $ time bash -c 'git commit --fixup=$TARGET; EDITOR=true git rebase -i --autosquash $TARGET~' <snip> real 0m10.733s $ time git revise $TARGET <snip> real 0m0.685s git-revise accomplishes this using an in-memory rebase algorithm operating directly on git's trees, meaning it never has to touch your index or working directory, avoiding expensive disk I/O! It's handy git-revise isn't just a faster git rebase -i, it provides helpful commands, flags, and tools which make common changes faster, and easier: Fixup Fast $ git add . $ git revise HEAD~~ Running git revise $COMMIT directly collects changes staged in the index, and directly applies them to the specified commit. Conflicts are resolved interactively, and a warning will be shown if the final state of the tree is different from what you started with! With an extra -e, you can update the commit message at the same time, and -a will stage your changes, so you don't have to! 5 Split Commits $ git revise -c $COMMIT Select changes to be included in part [1]: diff --git b/file.txt a/file.txt <snip> Apply this hunk to index [y,n,q,a,d,e,?]? Sometimes, a commit needs to be split in two, perhaps because a change ended up in the wrong commit. The --cut flag (and cut interactive command) provides a fast way to split a commit in-place. Running git revise --cut $COMMIT will start a git add -p-style hunk selector, allowing you to pick changes for part 1, and the rest will end up in part 2. No more tinkering around with edit during a rebase to split off that comment you accidentally added to the wrong commit! Interactive Mode git-revise has a git rebase -i-style interactive mode, but with some quality-of-life improvements, on top of being fast: Implicit Base Commit If a base commit isn't provided, --interactive will implicitly locate a safe base commit to start from, walking up from HEAD, and stopping at published & merge commits. Often git revise -i is all you need! The index Todo Staged changes in the index automatically appear in interactive mode, and can be moved around and treated like any other commit in range. No need to turn it into a commit with a dummy name before you pop open interactive mode & squash it into another commit! Bulk Commit Rewording Ever wanted to update a bunch of commit messages at once? Perhaps they're all missing the bug number? Well, git revise -ie has you covered. It'll open a special Interactive Mode where each command is prefixed with a ++, and the full commit message is present after it. Changes made to these commit messages will be applied before executing the TODOs, meaning you can edit them in bulk. I use this constantly to add bug numbers, elaborate on commit details, and add reviewer information to commit messages. ++ pick f5a02a16731a Bug ??? - My commit summary, r=? The full commit message body follows! ++ pick fef1aeddd6fb Bug ??? - Another commit, r=? Another commit's body! Autosquash Support $ git revise --autosquash If you're used to git rebase -i --autosquash, revise works with you. Running git revise --autosquash will automatically reorder and apply fixup commits created with git commit --fixup=$COMMIT and similar tools, and thanks to the implicit base commit, you don't even need to specify it. You can even pass the -i flag if you want to edit the generated todo list before running it. It's non-destructive git-revise doesn't touch either your working directory, or your index. This means that if it's killed while running, your repository won't be changed, and you can't end up in a mid-rebase state while using it. Problems like conflicts are resolved interactively, while the command is running, without changing the actual files you've been working on. And, as no files are touched, git-revise won't trigger any unnecessary rebuilds! What git-revise is not (Section Added: Aug. 6, 2019) git-revise does not aim to be a complete replacement for git rebase -i. It has a specific use-case in mind, namely incremental changes to a patch stack, and excludes features which rebase supports. In my personal workflow, I still reach for git rebase [-i] when I need to rebase my local commits due to new upstream changes, and I imagine there are people with advanced workflows who cannot use git revise. Working directory changes: git-revise does not modify your working directory or index while it's running. This is part of what allows it to be so fast. However, it also means that certain rebase features, such as the edit interactive command, are not possible. This also is why git revise -i does not support removing commits from within a patch series: doing so would require changing the state of your working directory due to the now-missing commit. If you want to drop a commit you can instead move it to the end of the list and mark it as index. The commit will disappear from history, but your index and working directory won't be changed. A quick git reset --hard HEAD will update your index and working directory. These restrictions may change in the future. Features like this have been requested, and it might be useful to allow opting-in to dropping commits on the floor or pausing mid-revise. Merging through renames & copies: git-revise uses a custom merge backend, which doesn't attempt to handle file renames or copies. For changes which need to be merged or rebased through file renames and copies, git rebase is a better option. Complex history rewriting: git rebase supports rebasing complex commits, such as merges. In contrast, git-revise does not currently aim to support these more advanced features of git rebase. Interested? Awesome! git-revise is a MIT-licensed pure-Python 3.6+ package, and can be installed with pip: $ python3 -m pip install --user git-revise You can also check out the source on GitHub, and read the manpage online, or by running man git revise in your terminal. I'll leave you with some handy links to resources to learn more about git-revise, how it works, and how you can contribute! Repository: https://github.com/mystor/git-revise Bug Tracker: https://github.com/mystor/git-revise/issues Manpage: https://git-revise.readthedocs.io/en/latest/man.html Installing: https://git-revise.readthedocs.io/en/latest/install.html Contributing: https://git-revise.readthedocs.io/en/latest/contributing.html "
        ],
        "story_type": "Normal",
        "url_raw": "https://mystor.github.io/git-revise.html",
        "comments.comment_id": [20620441, 20620524],
        "comments.comment_author": ["WorldMaker", "eridius"],
        "comments.comment_descendants": [2, 1],
        "comments.comment_time": [
          "2019-08-05T23:37:35Z",
          "2019-08-05T23:47:19Z"
        ],
        "comments.comment_text": [
          "Reminds me some of Raymond Chen's \"Stupid Git Tricks\" series  [0-6] of blog posts where he used a lot of git commit-tree and similar low level tools to avoid worktree changes and minimize GC churn (partly because of working in the humongous Windows git, which of course seems to have similar issues to the Mozilla ones mentioned here such as auto-rebuild tools).<p>It makes a bunch of sense to build nicer porcelain tools for such low level git magic when it becomes semi-routine.<p>(I couldn't find a good permalink for the entire series as a whole, so linked are all the individual posts.)<p>[0] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190506-00/?p=102478\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190506-00/?p=10...</a><p>[1] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190507-00/?p=102480\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190507-00/?p=10...</a><p>[2] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190508-00/?p=102482\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190508-00/?p=10...</a><p>[3] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190509-00/?p=102485\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190509-00/?p=10...</a><p>[4] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190510-00/?p=102488\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190510-00/?p=10...</a><p>[5] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190513-00/?p=102490\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190513-00/?p=10...</a><p>[6] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190515-00/?p=102495\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190515-00/?p=10...</a>",
          "This sounds exactly like my dream tool, the very thing I've been meaning to write myself and haven't gotten around to it. Thank you for writing this!<p>Suggestion: I'd love to see a `revise.autoSquash` git config flag (like `rebase.autoSquash`) to always autosquash in interactive mode. Maybe you already support it, but if so, the manpage doesn't list it."
        ],
        "id": "1b90a2a8-e821-4155-9787-15511011277f",
        "url_text": "NIKA:\\git-revise\\> list (Aug. 6, 2019): Added the \"What git-revise is not\" section. At Mozilla I often end up building my changes in a patch stack, and used git rebase -i1 to make changes to commits in response to review comments etc. Unfortunately, with a repository as large as mozilla-central2, git rebase -i has some downsides: It's slow! Rebase operates directly on the worktree, so it performs a full checkout of each commit in the stack, and frequently refreshes worktree state. On large repositories (especially on NTFS) that can take a long time. It triggers rebuilds! Because rebase touches the file tree, some build systems (like gecko's recursive-make backend) rebuild unnecessarially. It's stateful! If the rebase fails, the repository is in a weird mid-rebase state, and in edge cases I've accidentally dropped commits due to other processes racing on the repository lock. It's clunky! Common tasks (like splitting & rewording commits) require multiple steps and are unintuitive. Naturally, I did the only reasonable thing: Build a brand-new tool. source: xkcd git-revise is a history editing tool designed for the patch-stack workflow. It's fast, non-destructive, and aims to provide a familiar, powerful, and easy to use re-imagining of the patch stack workflow. It's fast I would never claim to be a benchmarking expert 3, but git-revise performs substantially better than rebase for small history editing tasks 4. In a test applying a single-line change to a mozilla-central commit 20 patches up the stack I saw a 15x speed improvement. $ time bash -c 'git commit --fixup=$TARGET; EDITOR=true git rebase -i --autosquash $TARGET~' <snip> real 0m10.733s $ time git revise $TARGET <snip> real 0m0.685s git-revise accomplishes this using an in-memory rebase algorithm operating directly on git's trees, meaning it never has to touch your index or working directory, avoiding expensive disk I/O! It's handy git-revise isn't just a faster git rebase -i, it provides helpful commands, flags, and tools which make common changes faster, and easier: Fixup Fast $ git add . $ git revise HEAD~~ Running git revise $COMMIT directly collects changes staged in the index, and directly applies them to the specified commit. Conflicts are resolved interactively, and a warning will be shown if the final state of the tree is different from what you started with! With an extra -e, you can update the commit message at the same time, and -a will stage your changes, so you don't have to! 5 Split Commits $ git revise -c $COMMIT Select changes to be included in part [1]: diff --git b/file.txt a/file.txt <snip> Apply this hunk to index [y,n,q,a,d,e,?]? Sometimes, a commit needs to be split in two, perhaps because a change ended up in the wrong commit. The --cut flag (and cut interactive command) provides a fast way to split a commit in-place. Running git revise --cut $COMMIT will start a git add -p-style hunk selector, allowing you to pick changes for part 1, and the rest will end up in part 2. No more tinkering around with edit during a rebase to split off that comment you accidentally added to the wrong commit! Interactive Mode git-revise has a git rebase -i-style interactive mode, but with some quality-of-life improvements, on top of being fast: Implicit Base Commit If a base commit isn't provided, --interactive will implicitly locate a safe base commit to start from, walking up from HEAD, and stopping at published & merge commits. Often git revise -i is all you need! The index Todo Staged changes in the index automatically appear in interactive mode, and can be moved around and treated like any other commit in range. No need to turn it into a commit with a dummy name before you pop open interactive mode & squash it into another commit! Bulk Commit Rewording Ever wanted to update a bunch of commit messages at once? Perhaps they're all missing the bug number? Well, git revise -ie has you covered. It'll open a special Interactive Mode where each command is prefixed with a ++, and the full commit message is present after it. Changes made to these commit messages will be applied before executing the TODOs, meaning you can edit them in bulk. I use this constantly to add bug numbers, elaborate on commit details, and add reviewer information to commit messages. ++ pick f5a02a16731a Bug ??? - My commit summary, r=? The full commit message body follows! ++ pick fef1aeddd6fb Bug ??? - Another commit, r=? Another commit's body! Autosquash Support $ git revise --autosquash If you're used to git rebase -i --autosquash, revise works with you. Running git revise --autosquash will automatically reorder and apply fixup commits created with git commit --fixup=$COMMIT and similar tools, and thanks to the implicit base commit, you don't even need to specify it. You can even pass the -i flag if you want to edit the generated todo list before running it. It's non-destructive git-revise doesn't touch either your working directory, or your index. This means that if it's killed while running, your repository won't be changed, and you can't end up in a mid-rebase state while using it. Problems like conflicts are resolved interactively, while the command is running, without changing the actual files you've been working on. And, as no files are touched, git-revise won't trigger any unnecessary rebuilds! What git-revise is not (Section Added: Aug. 6, 2019) git-revise does not aim to be a complete replacement for git rebase -i. It has a specific use-case in mind, namely incremental changes to a patch stack, and excludes features which rebase supports. In my personal workflow, I still reach for git rebase [-i] when I need to rebase my local commits due to new upstream changes, and I imagine there are people with advanced workflows who cannot use git revise. Working directory changes: git-revise does not modify your working directory or index while it's running. This is part of what allows it to be so fast. However, it also means that certain rebase features, such as the edit interactive command, are not possible. This also is why git revise -i does not support removing commits from within a patch series: doing so would require changing the state of your working directory due to the now-missing commit. If you want to drop a commit you can instead move it to the end of the list and mark it as index. The commit will disappear from history, but your index and working directory won't be changed. A quick git reset --hard HEAD will update your index and working directory. These restrictions may change in the future. Features like this have been requested, and it might be useful to allow opting-in to dropping commits on the floor or pausing mid-revise. Merging through renames & copies: git-revise uses a custom merge backend, which doesn't attempt to handle file renames or copies. For changes which need to be merged or rebased through file renames and copies, git rebase is a better option. Complex history rewriting: git rebase supports rebasing complex commits, such as merges. In contrast, git-revise does not currently aim to support these more advanced features of git rebase. Interested? Awesome! git-revise is a MIT-licensed pure-Python 3.6+ package, and can be installed with pip: $ python3 -m pip install --user git-revise You can also check out the source on GitHub, and read the manpage online, or by running man git revise in your terminal. I'll leave you with some handy links to resources to learn more about git-revise, how it works, and how you can contribute! Repository: https://github.com/mystor/git-revise Bug Tracker: https://github.com/mystor/git-revise/issues Manpage: https://git-revise.readthedocs.io/en/latest/man.html Installing: https://git-revise.readthedocs.io/en/latest/install.html Contributing: https://git-revise.readthedocs.io/en/latest/contributing.html ",
        "_version_": 1718536507755593728
      },
      {
        "story_id": 21305843,
        "story_author": "MilnerRoute",
        "story_descendants": 5,
        "story_score": 11,
        "story_time": "2019-10-20T18:05:45Z",
        "story_title": "Microsoft vs. IBM: A major shift in Java support",
        "search": [
          "Microsoft vs. IBM: A major shift in Java support",
          "https://www.theserverside.com/opinion/Microsoft-vs-IBM-A-major-shift-in-Java-support",
          "Once an afterthought in the Java community, Microsoft has seemingly overtaken IBM as the preeminent advocate among developers at the Oracle Code One conference. SAN FRANCISCO -- There once was a time when IBM was arguably the most dominant force in the enterprise Java community. And yet at Oracle Code One 2019, signs all pointed to how Microsoft wants in, while IBM wants out -- a major shift in the Microsoft vs. IBM discussion. It was always IBM, after all, that invested heavily in Java development, while Microsoft didn't bother. As IBM pushes itself away from the Java table, Microsoft appears ready to take a seat. Microsoft vs. IBM: A reversal of roles IBM invented the Eclipse IDE. IBM pushed Fortune 500 clients onto WebSphere, which drove widespread adoption of server-side Java. And when Oracle bought Sun Microsystems a decade ago, it was IBM that put in a serious bid for Sun's technologies. If Oracle hadn't sweetened its offer, IBM likely would have become the steward of the Java language. But IBM's interest waned over the years, and the company has severely neglected its WebSphere user base, providing inadequate updates to the server, portal and web content management (WCM) tools. Even today, the WCM editor doesn't support multiple browser tabs. The web-based server administration UI blows up when you click the back button, and the portal configuration tool is severely outdated. Nobody in the industry was surprised when IBM sold its on-premises WebSphere offerings to HCL Industries earlier this year. From a user perspective, it feels like IBM gave up on WebSphere and sever-side Java a long time ago. Instead, Big Blue is focused on AI, the cloud and its bewildering assortment of Watson-branded tools. IBM certainly didn't present as strong a front at Oracle Code One 2019 as it did when the conference was called JavaOne, and that contrasts starkly with Microsoft. Microsoft vs. IBM: One lays inroads for Java developers, the other retracts Microsoft's .NET platform has always been a direct competitor to Java EE, and any tool that flew under Bill Gates' flag was ripe for criticism from the Java community -- no matter how technically sound it might be. Despite a position behind the 8-ball in terms of Java mindshare, Microsoft has done everything it can over the past 18 months to endear itself to the developer community. Microsoft became an AdoptOpenJDK sponsor in June 2018. More recently, Microsoft acquired jClarity, which meant respected Java Champions such as Martijn Verburg and Ben Evans were brought into the Microsoft fold. And, despite owning Team Foundation Server, a popular and capable version-control system in its own right, Microsoft spent $7.5 billion on GitHub, a DVCS tool that hosts a variety of Apache and other open source Java projects. Microsoft servers have never been the primary deployment target of Java EE apps. But a cloud-native Java application that runs within Docker can be hosted on Microsoft Azure with ease. Containerization has opened up the playing field, and Microsoft can salivate over the formerly inaccessible revenue potential the enterprise Java space represents. Oracle Code One 2019 may well be remembered as the turning point in the Microsoft vs. IBM discussion. This is the year that saw Microsoft start to make serious inroads to the Java community and transform itself from a punching bag to a respected advocate. This notion really hit home when I saw a session at Code One with Kirk Pepperdine -- a Java Champion who has always been fiercely independent -- and realized that he's now a principal engineer at Microsoft. It's equally as elucidating to see Reza Rahman -- the former Java EE evangelist for Oracle -- represent Microsoft in his \"Birds of a Feather\" sessions. It's pretty clear. Microsoft has made a serious play at the enterprise Java space, and IBM has drifted off in other directions. Dig Deeper on JSRs and APIs Is Apache Tomcat the right Java application server for you? By: TimCulverhouse Tomcat vs WebSphere: How these application servers compare By: CameronMcKenzie Azure tools for cloud-based development Hot skills: IBMs WAS CE offers free route to develop and deploy applications By: NickLangley "
        ],
        "story_type": "Normal",
        "url_raw": "https://www.theserverside.com/opinion/Microsoft-vs-IBM-A-major-shift-in-Java-support",
        "comments.comment_id": [21306161, 21306258],
        "comments.comment_author": ["mshockwave", "__initbrian__"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-10-20T18:45:51Z",
          "2019-10-20T18:57:47Z"
        ],
        "comments.comment_text": [
          "I don't think donating huge money to Github can be an evidence that M$ are moving toward Java",
          "Buying developers via github seemed focused toward JS. I went to [0] to prove it but turns out Java is the second most popular language on GitHub projects<p>[0] <a href=\"https://octoverse.github.com/projects\" rel=\"nofollow\">https://octoverse.github.com/projects</a>"
        ],
        "id": "c838d582-eed8-44f3-bf2b-be9958e73710",
        "url_text": "Once an afterthought in the Java community, Microsoft has seemingly overtaken IBM as the preeminent advocate among developers at the Oracle Code One conference. SAN FRANCISCO -- There once was a time when IBM was arguably the most dominant force in the enterprise Java community. And yet at Oracle Code One 2019, signs all pointed to how Microsoft wants in, while IBM wants out -- a major shift in the Microsoft vs. IBM discussion. It was always IBM, after all, that invested heavily in Java development, while Microsoft didn't bother. As IBM pushes itself away from the Java table, Microsoft appears ready to take a seat. Microsoft vs. IBM: A reversal of roles IBM invented the Eclipse IDE. IBM pushed Fortune 500 clients onto WebSphere, which drove widespread adoption of server-side Java. And when Oracle bought Sun Microsystems a decade ago, it was IBM that put in a serious bid for Sun's technologies. If Oracle hadn't sweetened its offer, IBM likely would have become the steward of the Java language. But IBM's interest waned over the years, and the company has severely neglected its WebSphere user base, providing inadequate updates to the server, portal and web content management (WCM) tools. Even today, the WCM editor doesn't support multiple browser tabs. The web-based server administration UI blows up when you click the back button, and the portal configuration tool is severely outdated. Nobody in the industry was surprised when IBM sold its on-premises WebSphere offerings to HCL Industries earlier this year. From a user perspective, it feels like IBM gave up on WebSphere and sever-side Java a long time ago. Instead, Big Blue is focused on AI, the cloud and its bewildering assortment of Watson-branded tools. IBM certainly didn't present as strong a front at Oracle Code One 2019 as it did when the conference was called JavaOne, and that contrasts starkly with Microsoft. Microsoft vs. IBM: One lays inroads for Java developers, the other retracts Microsoft's .NET platform has always been a direct competitor to Java EE, and any tool that flew under Bill Gates' flag was ripe for criticism from the Java community -- no matter how technically sound it might be. Despite a position behind the 8-ball in terms of Java mindshare, Microsoft has done everything it can over the past 18 months to endear itself to the developer community. Microsoft became an AdoptOpenJDK sponsor in June 2018. More recently, Microsoft acquired jClarity, which meant respected Java Champions such as Martijn Verburg and Ben Evans were brought into the Microsoft fold. And, despite owning Team Foundation Server, a popular and capable version-control system in its own right, Microsoft spent $7.5 billion on GitHub, a DVCS tool that hosts a variety of Apache and other open source Java projects. Microsoft servers have never been the primary deployment target of Java EE apps. But a cloud-native Java application that runs within Docker can be hosted on Microsoft Azure with ease. Containerization has opened up the playing field, and Microsoft can salivate over the formerly inaccessible revenue potential the enterprise Java space represents. Oracle Code One 2019 may well be remembered as the turning point in the Microsoft vs. IBM discussion. This is the year that saw Microsoft start to make serious inroads to the Java community and transform itself from a punching bag to a respected advocate. This notion really hit home when I saw a session at Code One with Kirk Pepperdine -- a Java Champion who has always been fiercely independent -- and realized that he's now a principal engineer at Microsoft. It's equally as elucidating to see Reza Rahman -- the former Java EE evangelist for Oracle -- represent Microsoft in his \"Birds of a Feather\" sessions. It's pretty clear. Microsoft has made a serious play at the enterprise Java space, and IBM has drifted off in other directions. Dig Deeper on JSRs and APIs Is Apache Tomcat the right Java application server for you? By: TimCulverhouse Tomcat vs WebSphere: How these application servers compare By: CameronMcKenzie Azure tools for cloud-based development Hot skills: IBMs WAS CE offers free route to develop and deploy applications By: NickLangley ",
        "_version_": 1718536534627450880
      },
      {
        "story_id": 20090370,
        "story_author": "guessmyname",
        "story_descendants": 33,
        "story_score": 169,
        "story_time": "2019-06-04T00:03:28Z",
        "story_title": "GitHub Package Registry Will Support Swift Packages",
        "search": [
          "GitHub Package Registry Will Support Swift Packages",
          "https://github.blog/2019-06-03-github-package-registry-will-support-swift-packages/",
          "On May 10, we announced the limited beta of GitHub Package Registry, a package management service that makes it easy to publish public or private packages next to your source code. It currently supports familiar package management tools: JavaScript (npm), Java (Maven), Ruby (RubyGems), .NET (NuGet), and Docker images, with more to come. Today were excited to announce that well be adding support for Swift packages to GitHub Package Registry. Swift packages make it easy to share your libraries and source code across your projects and with the Swift community. Available on GitHub, Swift Package Manager is a single cross-platform tool for building, running, testing, and packaging your Swift code. Package configurations are written in Swift, making it easy to configure targets, declare products, and manage package dependencies. Together, the Swift Package Manager and GitHub Package Registry will make it even easier for you to publish and manage your Swift packages. Its essential for mobile developers to have the best tools in order to be more productive. With the growth of the Swift ecosystem, were thrilled to work together with the team at Apple to help create new workflows for Swift developers. Since its launch, weve been amazed to see your excitement to get started with GitHub Package Registry. During this beta period, were committed to learning from communities and ecosystems alike about how it meets your needs and what we can do to make it even better. If you havent done so already, you can sign up for the limited beta now. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.blog/2019-06-03-github-package-registry-will-support-swift-packages/",
        "url_text": "On May 10, we announced the limited beta of GitHub Package Registry, a package management service that makes it easy to publish public or private packages next to your source code. It currently supports familiar package management tools: JavaScript (npm), Java (Maven), Ruby (RubyGems), .NET (NuGet), and Docker images, with more to come. Today were excited to announce that well be adding support for Swift packages to GitHub Package Registry. Swift packages make it easy to share your libraries and source code across your projects and with the Swift community. Available on GitHub, Swift Package Manager is a single cross-platform tool for building, running, testing, and packaging your Swift code. Package configurations are written in Swift, making it easy to configure targets, declare products, and manage package dependencies. Together, the Swift Package Manager and GitHub Package Registry will make it even easier for you to publish and manage your Swift packages. Its essential for mobile developers to have the best tools in order to be more productive. With the growth of the Swift ecosystem, were thrilled to work together with the team at Apple to help create new workflows for Swift developers. Since its launch, weve been amazed to see your excitement to get started with GitHub Package Registry. During this beta period, were committed to learning from communities and ecosystems alike about how it meets your needs and what we can do to make it even better. If you havent done so already, you can sign up for the limited beta now. ",
        "comments.comment_id": [20090667, 20093651],
        "comments.comment_author": ["OberstKrueger", "jopsen"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-06-04T00:47:58Z",
          "2019-06-04T10:34:52Z"
        ],
        "comments.comment_text": [
          "This will go well with Xcode 11 supporting Swift Package Manager natively.<p><a href=\"https://developer.apple.com/xcode/\" rel=\"nofollow\">https://developer.apple.com/xcode/</a>",
          "How immutable is this registry?<p>What happens if the owner tries to remove their package, or rename it? or modify it?\nWhat if the owner deletes his/her account / organization, or renames user / organization?<p>The nice thing about many package managers is that they don't allow mutation, removal or renaming of packages.\nSo when you have a lock-file checked into git you can be sure you can fetch all the dependencies again."
        ],
        "id": "3c4c9821-29f6-4415-a12d-752bbbb0485a",
        "_version_": 1718536488691433472
      },
      {
        "story_id": 20242820,
        "story_author": "keufran",
        "story_descendants": 58,
        "story_score": 74,
        "story_time": "2019-06-21T14:34:59Z",
        "story_title": "Ask HN: How to handle code reviews with a visually impaired coworker?",
        "search": [
          "Ask HN: How to handle code reviews with a visually impaired coworker?",
          "Hello World,<p>My dev team has switched to a workflow using merge requests and code reviews (mostly by commenting the merge request). Our tool is Gitlab.<p>I needed to embed a visually impaired co-worker in my team but I face some difficulties:<p>- Gitlab accessibility seems to be really bad and my co-worker is unable to use the interface  to create Merge Request (he uses accessibility tools, of course). I dont'even speak about reading and writing comments in merge request.<p>- I don't know how to handle the code review process with him. We could do physical Code Review sessions, but it's difficult because I've a very chaotic schedule and so it's difficult to find a common timeslot. Furthermore, it's very difficult for my co-worker to handle all the remarks in one session for any Merge Request with a significant amount of code.<p>- I need to keep in place the existing tooling for the rest of the team<p>Does anybody knows of tools interfacing with Gitlab (or the git repository) or methodologies that could help us ?"
        ],
        "story_text": "Hello World,<p>My dev team has switched to a workflow using merge requests and code reviews (mostly by commenting the merge request). Our tool is Gitlab.<p>I needed to embed a visually impaired co-worker in my team but I face some difficulties:<p>- Gitlab accessibility seems to be really bad and my co-worker is unable to use the interface  to create Merge Request (he uses accessibility tools, of course). I dont'even speak about reading and writing comments in merge request.<p>- I don't know how to handle the code review process with him. We could do physical Code Review sessions, but it's difficult because I've a very chaotic schedule and so it's difficult to find a common timeslot. Furthermore, it's very difficult for my co-worker to handle all the remarks in one session for any Merge Request with a significant amount of code.<p>- I need to keep in place the existing tooling for the rest of the team<p>Does anybody knows of tools interfacing with Gitlab (or the git repository) or methodologies that could help us ?",
        "story_type": "AskHN",
        "comments.comment_id": [20243514, 20244110],
        "comments.comment_author": ["jareds", "sn"],
        "comments.comment_descendants": [3, 2],
        "comments.comment_time": [
          "2019-06-21T15:45:30Z",
          "2019-06-21T16:41:11Z"
        ],
        "comments.comment_text": [
          "I'm a totally blind developer and I find the easiest way to do code reviews is to use git format-patch on the branch containing the code. I read the patch files in a text editor. Perhaps comments in the pull request referencing a commit and line would allow the developer to get the required context from the patch files?",
          "I work with a blind developer. We create personal feature branches, file a ticket for merge requests in our ticketing system, code review them directly in the ticket or via email, and then rebase the feature branch onto master. We don't, but you could plausibly do reviews directly in git via editing the file with any comments.<p>I've been wanting to try gerrit however: <a href=\"https://gerrit-review.googlesource.com/Documentation/dev-design.html#_accessibility_considerations\" rel=\"nofollow\">https://gerrit-review.googlesource.com/Documentation/dev-des...</a><p>Please report back on whatever you end up doing.<p>One caution I have is that long term you should consider moving away from the gitlab tools if they are unable to fix accessibility concerns, so that everyone is on equal ground.<p>I also don't know how flexible he is on which browser or operating system he uses; he may want to try some others to see if they work any better."
        ],
        "id": "284e2ed2-0715-4997-b215-ea4d84f73646",
        "_version_": 1718536494182825984
      },
      {
        "story_id": 20558322,
        "story_author": "homarp",
        "story_descendants": 5,
        "story_score": 49,
        "story_time": "2019-07-29T19:18:18Z",
        "story_title": "Revec: Program Rejuvenation Through Revectorization",
        "search": [
          "Revec: Program Rejuvenation Through Revectorization",
          "https://github.com/revec/llvm-revec",
          "Revec: Program Rejuvenation through Revectorization This is a fork of the LLVM repository that implements Revec, an IR-level pass that retargets vectorized code between processor generations. In general, revectorization is our proposed task of recompiling non-portable vectorized code to leverage higher bitwidth instruction sets on newer architectures. For details of the pass, please see Revec: Program Rejuvenation through Revectorization. Citation: (* denotes equal contribution) Charith Mendis *, Ajay Jain *, Paras Jain, and Saman Amarasinghe. 2019. Revec: Program Rejuvenation through Revectorization. In Proceedings of the 28th International Conference on Compiler Construction (CC 19), February 1617, 2019, Washington, DC, USA. ACM, New York, NY, USA, 13 pages. Compiling LLVM and Clang with Revec Install Clang-6.0, CMake, and Ninja to build our modified version of LLVM/Clang: sudo apt-get update sudo apt-get install llvm-6.0 clang-6.0 cmake ninja-build Clone LLVM and Clang with the Revectorizer pass: cd $ROOTDIR git clone git@github.com:revec/llvm-revec.git llvm cd $ROOTDIR/llvm/tools git clone git@github.com:revec/clang-revec.git clang cd $ROOTDIR/llvm/tools/clang git clone git@github.com:llvm-mirror/clang-tools-extra.git extra cd $ROOTDIR/llvm/projects git clone git@github.com:llvm-mirror/libcxx.git git clone git@github.com:llvm-mirror/libcxxabi.git git clone git@github.com:llvm-mirror/compiler-rt.git Build Clang: mkdir $ROOTDIR/build cd $ROOTDIR/build CXX=clang++-6.0 CC=clang-6.0 cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=RelWithDebInfo -DLLVM_USE_LINKER=gold -DLLVM_TARGETS_TO_BUILD=\"X86\" ninja clang "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/revec/llvm-revec",
        "url_text": "Revec: Program Rejuvenation through Revectorization This is a fork of the LLVM repository that implements Revec, an IR-level pass that retargets vectorized code between processor generations. In general, revectorization is our proposed task of recompiling non-portable vectorized code to leverage higher bitwidth instruction sets on newer architectures. For details of the pass, please see Revec: Program Rejuvenation through Revectorization. Citation: (* denotes equal contribution) Charith Mendis *, Ajay Jain *, Paras Jain, and Saman Amarasinghe. 2019. Revec: Program Rejuvenation through Revectorization. In Proceedings of the 28th International Conference on Compiler Construction (CC 19), February 1617, 2019, Washington, DC, USA. ACM, New York, NY, USA, 13 pages. Compiling LLVM and Clang with Revec Install Clang-6.0, CMake, and Ninja to build our modified version of LLVM/Clang: sudo apt-get update sudo apt-get install llvm-6.0 clang-6.0 cmake ninja-build Clone LLVM and Clang with the Revectorizer pass: cd $ROOTDIR git clone git@github.com:revec/llvm-revec.git llvm cd $ROOTDIR/llvm/tools git clone git@github.com:revec/clang-revec.git clang cd $ROOTDIR/llvm/tools/clang git clone git@github.com:llvm-mirror/clang-tools-extra.git extra cd $ROOTDIR/llvm/projects git clone git@github.com:llvm-mirror/libcxx.git git clone git@github.com:llvm-mirror/libcxxabi.git git clone git@github.com:llvm-mirror/compiler-rt.git Build Clang: mkdir $ROOTDIR/build cd $ROOTDIR/build CXX=clang++-6.0 CC=clang-6.0 cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=RelWithDebInfo -DLLVM_USE_LINKER=gold -DLLVM_TARGETS_TO_BUILD=\"X86\" ninja clang ",
        "comments.comment_id": [20574660, 20574822],
        "comments.comment_author": ["wyldfire", "CalChris"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-07-31T14:27:59Z",
          "2019-07-31T14:49:11Z"
        ],
        "comments.comment_text": [
          "> Revec automatically finds equivalences between vector intrinsics across different instruction generations by enumerating all combinations. Equivalences are established through randomized and corner case testing<p>> We adopt a test case fuzzing approach for generating equivalences, with 18,000 randomly generated inputs and combinatorially generated corner-case inputs, inspired by test case generation in [6].  ... helped Revec to significantly reduce erroneous equivalences, and is consistent with the methodology in [6].<p>It sounds like an interesting approach.  But how hard would it have been to ask an expert to encode this information instead of empirically?  And what about instruction side effects, proc/coproc flags/modes etc?  Does equivalence consider those?<p>The llvm-revec fork has the pass, but does it also have the code to reproduce the automated enumeration (and testing)?",
          "This paper was mentioned in the <i>RISC Is Fundamentally Unscalable</i> post but with respect to VLIW.<p><pre><code>  The problem is that this is really fucking hard to\n  compile, and that’s what Intel screwed up. Intel\n  assumed that compilers in 2001 could extract the\n  instruction-level parallelism necessary to make\n  VLIW work, but in reality we’ve only very recently\n  figured out how to reliably do that.\n</code></pre>\nThe ReVec article/repo doesn't mention VLIW as an application but I can see parallels in the problem. If you compile for an 8-wide vector processor and you get a 16-wide machine, you need to re-vectorize your code to take advantage of the newly available width. That's similar if you change widen a VLIW. Similar, not same. It seems an obvious <i>possible</i> application but the authors didn't mention it."
        ],
        "id": "278a6982-b6be-40ca-acd2-ffabaf7de784",
        "_version_": 1718536505718210560
      },
      {
        "story_id": 20330324,
        "story_author": "myroon5",
        "story_descendants": 39,
        "story_score": 225,
        "story_time": "2019-07-01T22:24:01Z",
        "story_title": "Scaling from 2k to 25k engineers on GitHub at Microsoft",
        "search": [
          "Scaling from 2k to 25k engineers on GitHub at Microsoft",
          "https://jeffwilcox.blog/2019/06/scaling-25k/",
          "At Microsoft today we have almost 25,000 engineers participating in our official GitHub organizations for open source, a great number of them contributing to open source communities throughout GitHub. Its been quite a ride: thats 10X the engineers we were working with when I posted in 2015 about our experience scaling from 20 to 2,000 engineers in the Azure open source org. As a member of Microsofts Open Source Programs Office (OSPO) team, its been exciting to be a part of this growth, and I wanted to take some time to write down some of the investments we have made so that others could get a peek inside. Using data gathered through an open source project Ill mention in this post, I was able to query our GitHub entity data to create this chart of Microsofts public repos created since 2011: Some of our employees have always been on GitHub, contributing to projects, sharing their ideas and insights. Many of our teams are new to social coding and working in the open, and look to OSPO to help provide documentation and guidance. We have engineers who are busy contributing to projects all over GitHub, donating their time and code or entire projects, many I am sure enjoy working with open source in an official capacity, others after hours, or just hacking away. Looking at the contributors to virtual-kubelet, a project thats part of the Cloud Native Computing Foundation, I see familiar names of people Ive worked with. The Visual Studio Code team has been on fire, moving at a fast pace for years. Theres an entire community of awesome people from around the world on GitHub every day opening issues, performing code reviews, and contributing code to VSCode. These teams are finding new ways to communicate, to build consensus and governance models, and to invite maintainers into the fold. In this post, I will cover: Core principles the Open Source Programs Office uses to guide the open source and GitHub experience Technical investments we have made to scale GitHub Program investments Key learnings Looking to the future Resources including the open source projects mentioned in this post Im going to be focusing more on the tactical approach we took to enabling GitHub at scale and not any specific projects experience - though I hope to track down and share the experiences that projects like the Terminal, TypeScript, Accessibility Insights, and the Windows Calculator have had. We work hard to build the right experiences so that engineers have everything they need, without the OSPO team getting in their way. It should be no surprise that open source is a big part of what has helped us to scale, and we continue to give back and contribute where we can: weve adopted CLA Assistant, an open source project started by SAP, and now contribute to the project (we threw away our home-built, aging Contributor License Agreement (CLAs) bot!) were using an open source attribution engine built by Amazon our self-service GitHub management tooling is open source were collaborating on Clearly Defined, an OSI project, to crawl all the open source projects we can find to discover license, copyright, and other data, curating and sharing the resulting data in the open our team has invested in moving to more common open services and systems such as containers and Postgres and MongoDB to make it easier to collaborate with other companies and their preferred tech stacks Looking forward, we have a lot more work to do to focus on developing our capabilities - evolving our maturity models around what healthy and awesome projects look like, helping graduate work into the community and to foundations, and continuing rapid iteration, experiments, and learning from all of this. Im so excited to see where we are at in another few years, and encouraged by the collaboration happening on open source projects across the industry, the developing communities around specific Microsoft technologies, and all the random contributions that Microsoft engineers are making to open source all over GitHub as their teams take dependencies on and get involved in the associated communities. Principles weve adopted To encourage the right behaviors and teach Microsoft employees how to use GitHub and participate in communities, weve identified a number of tenants or principles that we try and use in everything that OSPO does specific to our tooling and approach. For example, we encourage teams to work in the open on GitHub, getting to learn the tools and lingo. Eliminate + Simplify Wed rather remove a complex process to reduce the work our engineers need to do if it is not providing the right return or value. If we need to ask teams questions about the project they want to release as open source, we focus on what problem were trying to solve, and we have been able to eliminate questions that we used to ask, after thinking through the outcomes with stakeholders and advisors. Looking back five years, we used to have a number of manual registration systems where engineers would let us know what open source they use. These were often free-form text fields, and many teams would only take a best-faith effort to share that data. Today weve been able to eliminate many of the registration scenarios by detecting the use of open source in many scenarios across the company, just asking follow-up questions or going through reviews for certain projects when necessary or needing more information. Eliminating process is not always possible, but if we continually ask questions about the workflows and guidance, and ask the teams using these systems to provide feedback and suggestions, hopefully well test the edges and eliminate where possible. Self-service At scale, we cannot be a roadblock, and we trust and encourage our engineers to learn through experience. We want to make sure users learn about GitHub teams, and how to request to join them. There is no possible way that we would have been able to get 25,000 people collaborating on GitHub so quickly without building a self-service experience whereby our engineers could join our GitHub orgs at their pace as they have a need to learn and participate. Traditionally, a GitHub invitation needs to be sent by an org owner to a specific username or e-mail address, and that just would not work well at our scale, without being in the way of our engineers. Thanks to the GitHub API and our GitHub management portal, things are smooth. Whenever possible, we want to provide documents, guidance, and other reusable resources, instead of having to rely on special knowledge or process that has manual steps. Delegation Our office provides guidance and resources to help advise Microsoft businesses in their approach to contributing to, using, and releasing open source, but we leave the business decisions with the particular business. Decisions such as whether to release open source go through a question/answer experience, and the outcome is either automatic approval, or a business approval workflow that kicks off, allowing a teams business and legal representatives make the final decisions and choices. We also have begun deputizing open source champs: people who can help spread the good word and provide opinions and advice to their teams about how to think about open source. Transparency Open source centers around collaboration, but one of the challenges today with GitHub orgs full of many repos is identifying what teams have access to accept pull requests or administer settings. While GitHub shows a lot of this data if you start with the Teams view in an org and drill into a specific team, theres no view for a given repos teams, unless youre an admin for that repo. For all of our repos, releases and reviews, Our portal exposes the given teams that control each repo All the release requests and data are stored in work items available to any employee Our portal for GitHub management shows all GitHub Teams, including secret or hidden teams We enable cross-org search in our portal, to more easily locate similarly named teams and repos across orgs, reducing confusion and internal support costs Thanks to this, out of the 1000s of repos, its relatively painless to find the right contacts for a project inside the company. Authentic GitHub Engineers should learn how the GitHub interface works, how pull requests and reviews, issues, forks, organization teams, and collaborators function. Whenever possible, we hope that our users go directly to GitHub to manage their teams, configure team maintainers, welcome new community members into their projects as maintainers or contributors. While we do have separate internal interfaces and tools, these ideally augment the native experience. Weve built a browser extension that our users can install to help light up GitHub to make it easier to find employees, resources, or internal tools. Its important to us that engineers learn the GitHub fork and pull request model for contributing, as we strive to use a similar experience for inner source work. Technical investments Our engineering team has made many technical investments to help scale the company to be able to participate in open source better. Whenever possible we want to use open source to make open source better and share those learnings and contributions for other companies and individuals to use. Well invest in our own tooling when we must, but look to the marketplace and work that others are doing on the horizon: we are excited to see GitHubs Enterprise Cloud product evolve and drive new features and capabilities into the product to make enterprise-scale open source easier. Adopting CLA Assistant Today we host an instance of an open source project for Contributor License Agreement (CLA) management, integrated with GitHub, called CLA Assistant. This allows us to make sure that people contributing to our projects have signed the appropriate CLA before we accept a contribution. Once the CLA for an entity is signed once by a community member, they can then contribute to all other repos, so its really a relatively low burden that helps keep our lawyer friends happy. CLA Assistant is an open source project that was started by SAP and is licensed as Apache 2.0. Weve contributed to the project functionality to help with scale issues and rate limiting, automatic signing to handle when employees join and leave the company, and are excited to be able to support a new class of corporate CLAs thanks to contributions made by others to that project. In 2017 we migrated to this new open solution from the in-house CLA bot that we abandoned. I really like how the VSCode team messaged this to their community with a GitHub issue (#34239). The system is powered by GitHub org-level webhooks, so it is always on and teams do not need to worry about whether their repos are protected with the CLA and ready for contribution. Data and Insights GitHub provides useful information including traffic stats, contributor info and other breakdowns at the repository level. Across Microsofts open source projects, however, we have a need to be able to slice and dice data looking for trends over time, analyzing investments at scale, and so realized early on that we needed to import all of the available data we can from our GitHub open source releases into our own big data systems such as Azure Data Lake and Azure Data Explorer. Newer GitHub Enterprise Cloud features look to help provide organization insights, and were super excited to give those a try to augment the other data and insight methods we are using today. Here are some of the projects weve used, created, or collaborated on. GHTorrent Our team is one of the sponsors of the GHTorrent Project, an effort to create a scalable, queryable, offline mirror of data offered through the GitHub REST API. The repo is at github.com/gousiosg/github-mirror. This data, similar to GHArchive, helps learn about the broad collaboration happening on GitHub. We donate cloud compute resources to the project run by Georgios Gousios who kicked off the project with ideas and collaboration with Diomidis Spinellis. At Microsoft, we ingest the data into Azure Data Lake to be able to run interesting queries and learn more about trends and happenings beyond our campus. GHCrawler GHCrawler is a robust GitHub API crawler that walks a queue of GitHub entities transitively, retrieving and storing their contents. The initial project launched in 2016 and evolved significantly at the TODO Group tools hackathon in June 2017 while working with other companies to abstract the data stores to support other technologies and stacks. The crawler taught us a lot about the GitHub API and how to be friendly citizens by focusing on the caching of entities, the use of e-tags, and being careful to not repeatedly fetch the same resource. Different from GHTorrent, the crawler is able to traverse Microsofts own open source organizations using tokens that can peer into our current GitHub team memberships, retrieve info about maintainers, configuration, and private repos, and so is very useful in answering operational questions about the growth of GitHub at Microsoft. We ingest the data from GHCrawler into both Azure Data Lake Azure Data Explorer and use that data in Power BI dashboards and reports, live data display on internal sites, and other resources. The chart of new repos at the top of this post was created by querying this crawler data as stored in Azure Data Explorer. Heres a query that returns repos created in the official Microsoft organization in the last 30 days (that are public): Since the data comes from GitHub but is stored internally, teams can make use of the data for their own business needs and interests, without having to worry about GitHub API rate limiting, the specifics around collecting that data, and just being able to focus on using the data effectively to solve business problems. A favorite resource for many teams looking to build new communities is the data around pull requests and issues, and also collected traffic API data. Since GitHub only provides a 2-week window of consolidated traffic info, storing the data in our big data tools helps us look at trends more easily. ClearlyDefined ClearlyDefined is an Open Source Initiative (OSI) project that has a mission to help FOSS projects thrive by being clearly defined. The lack of clarity around licenses reduces engagement that ends up meaning fewer users, fewer contributors, and a smaller community. Communities choose a license for their project with the terms that they like. Defining and knowing the license for an open source project is essential to a successful community partnership, and so ClearlyDefined helps clarify that by identifying key data such as license set, attribution parties, code location, and when the data is missing, curation can help. Microsoft contributes to the effort and is making use of the data to help provide license clarity in the tooling used to help teams understand their use of open source. As of June 2019, ClearlyDefined has over 6.3 million definitions. These definitions are created by running tools such as licensee, scancode-toolkit, and fossology tools across oodles of open source. repolinter Another TODO Group project OSPO collaborates on is repolinter. Given a source repository, you can run repolinter to learn basic information that can help inform whether a project has all that is necessary to incubate and build a healthy community, such as license files README files Code of Conduct, governance or contribution information No binaries Licenses detectable by licensee Source headers have license information We hope to be able to share this data in a more visible way to help teams see where they can make simple improvements, or even by automatically opening pull requests if they are missing the essentials such as a mention of the Code of Conduct. oss-attribution-builder Weve collaborated with Amazon and use their amzn/oss-attribution-builder project to help build open source license notice files to help teams meet their legal obligations. GitHub management portal Microsofts GitHub management portal that employees to use, detailed in my 2015 post on the portal, handles: self-service org joining for employees cross-organization search of people, repos, and teams support jobs to maintain data generating digests and reports caching GitHub REST API responses processing web hook events Here you can see an experience where you can search, sort and filter repos across all of the many GitHub orgs in one place, improving discoverability. The portal itself is open source and has been rebuilt and refactored over the past few years to try and be more useful and to allow other companies to use it: Node service and site, now implemented in TypeScript Backing stores supported include Postgres and others backed by interfaces An out-of-the-box run local in memory experience is now available The repo is on GitHub at microsoft/opensource-portal. New repo wizard While we have a very liberal and friendly policy to make it easy for an engineering team to decide to share some code such as a sample, we do want to make sure that theres a process in place that includes a business decision and legal team approval if theres a major release being considered. Microsoft has chosen to turn off the Allow Members to Create Repositories option on our open source GitHub organizations. Instead, we have our own new repository wizard within our GitHub management tooling. This site collects information about the purpose of the repo, the common GitHub Team permissions that will help a group of engineers get off and running, and then populates the repo with the standard Microsoft open source content such as Code of Conduct info in the README, an appropriate LICENSE file for the project, and the original purpose and use of the repo, for data reporting purposes internally. To make this experience easier to find, we work to educate people through documentation and tools such as the previously mentioned Open Source Assistant web browser extension. The extension is able to light up the green new repo button on our orgs and link directly to the wizard, providing a better user experience than the you have insufficient permission to create a new repository message that our users would receive on GitHub otherwise. The outcome of the repo wizard will be the creation of a public or private repo on GitHub. We ask, by policy, that teams only use our open source GitHub orgs for work they are going to ship within the next 30 days. Release reviews The outcome of the new repo wizard is either auto-approval to make a project public, or the kick-off of a business and legal review process, to help comply with policy, and inform leaders about important open source decisions that are being made. We have an internal service called the Usage Service that creates a work item in our work item tracking system (Azure Boards), and assigns that work item to the appropriate legal and business reviewer. The work item is then passed along, with fields or more info being filled out, discussions are had, and eventually the review will be final approved and ready for teams to ship their project to the world and go build that open community. Here is a screenshot, redacted of the details and names, showing the work item for the release approval of the Windows Calculator: After the final approval, an automated e-mail is sent to the original requester with any guidance from the various reviewers involved in the process, to help them understand any notifications, legal obligations, or other work that needs to be done to support their release. Open Source Assistant Browser Extension Internally we ship a browser extension that lights up GitHub with Microsoft-specific information and guidance. Since a GitHub username may not be related to a persons recognized corporate identity, the extension lights up the more common corporate alias inline throughout GitHub - pull requests, issues, people profiles. By highlighting other Microsoft employees on GitHub, people who use the extension are able to dedicate additional focus on a good collaboration experience with the community, while easily being able to better identify their coworkers on the site. The extension helps people to continue to use the native GitHub experience, while augmenting bits and pieces where weve made decisions around our GitHub org settings, such as disabling New Repo creation direct on GitHub. The extension also is also to provide links to policy around open source, link to the new repository wizard. Since we disable native repo creation on GitHub in order to ask engineers to complete our wizard to learn more about their intent with their open source release, we often would get support questions about how to create repos, since GitHub would display the message that the user could not create repos. Now, this is what they see: In another sceenshot, you can see a Join button that lets an employee self-service join the GitHub organization through our GitHub management portal: Finally, since we want teams to use our official GitHub org that is configured automatically with the CLA system, with compliance tools, audit logs, and the ability to manage the lifecycle of when users join and leave, we do not allow teams to create new GitHub orgs for open source. By updating GitHubs new organization page, we have a chance to let people know about the policy and other resources before they get too far down the road of trying to setup yet-another-org. The extension works Firefox, Chrome, and the new Edge based on Chromium. Microsoft employees can learn more and install the extension at aka.ms/1esassistant. While this extenson is not open source today, if others think it could help their organization, I can see what we can do. Regular automation jobs Weve implemented jobs that do everything from keeping data and caches up-to-date to helping make sure that we have confidence in the membership of our GitHub organizations. These jobs are all open source, implemented in TypeScript, as a part of the opensource-portal repo. Removing former employees Connected to HR data sources, we can react when an employee leaves the company. In a big company, people are always changing roles, trying new things, and sometimes they leave for other opportunities. We unlink people when they leave: removing the association between their GitHub account and their corporate identity, and removing their GitHub organization membership(s). We welcome former employees to continue to contribute and participate to projects where they should continue to be maintainers, but for clarity, do not allow them to remain as org members. We also send an e-mail to their former manager, so they have confidence that the right things have happened with permissions and system access. Enforced system teams To help enable operations, support, meet security obligations, and light up bots such as our CLA system, we have a set of GitHub teams that are automatically and permanently added to every repository in our GitHub orgs, when the job is configured. By monitoring the GitHub event bus, the job adds what we call system teams to a repo the instant a new repo is created. And, if a repo admin tries to remove or change the permissions that such a system team has, the job automatically restores its permission. An occassional cronjob also runs to validate and reenforce these teams as needed. Preventing big permission grants As the number of members in a GitHub organization grows high, youll also start to see very large teams. We have teams such as All Employee Members that are designed to give easy read access to private repos that are being finished up and polished before going open source. An unintended side effect we found: some people wanted to just give all members administrative access over their repo, to make the act of onboarding get much easier. Trouble is, with admin comes great power, and any of the thousands of members of the org could then delete the repo, change whatever they want, override branch protections, etc. This also would automatically subscribe thousands of people to spammy notification messages, depending on their GitHub notification preferences. As a result, we have a large permissions job that monitors for grants of write or admin access to an org-defined large number of people. Once such a grant is made, and the team is too large, the job will automatically downgrade the permissions to try and prevent mistakes, and then send an e-mail address to the person who made the permission choice, hoping to educate them about the risks and help them understand what changes were made. GitHub username changes GitHub allows users to change their username. This is excellent: many people start with a cute screen name or other indicator, then realize theyre professionally developing software with their peers in the industry, and often want to change that username. Not all users know that they can rename their account, so we do get people who delete an account, just to create a new one. However, the majority of GitHub apps and APIs work off of GitHub usernames instead of the integer-based user ID. To help stay ahead of this, we have a cronjob that looks up GitHub users by ID and then refreshes our internal data store of those usernames, in the hope that we can improve the API success rate and reduce support costs when renames happen. For critical operations, additional logic has been added to GitHub apps to anticipate the potential of a user rename happening, and being able to smartly attempt to fallback to looking up the user first by ID, or alerting and sending the issue to operational support. Weekly and daily digests We have a cronjob that sends updates about new changes and interesting happenings on GitHub repos. These are typically only send to repo admins, to not spam too many people. The digests are personalized to the receipient, and cover scenarios such as: notifying the creator of a repo, and their manager, of the new repo notifying when the # of admins becomes rather high, so that team maintainers can cleanup permissions abandoned repos that havent been touched in years when a team is down to a single maintainer, suggesting they appoint another maintainer or two when a repository has been flipped from private to public Program investments More important than the technical tools and hacks we put in place, the programs that we run in the Open Source Programs Office help drive process improvements, awareness of features and capabilities, roll out new functionality, and to emphasis the work that is happening in the open in new ways. Heres a set of the OSPO investments we have been making or put in place to help drive improvement in the open source maturity level of the company. Docs We have a central documentation repository that is easily accessible to employees where they can read through guides on how to go about releasing open source, or contributing to it. The hope is that teams new to many of the tools and approaches in open source can refer to the material, share it with others, and help us to scale this to others. Heres a rough look at the outline of some of that material. In time I hope we can prepare a version of these docs to share with the world - General Information - What exactly is Open Source? - When and why Open Source - Can I use it? - Misconceptions - Bad Practices - Benefits - Use - Overview - Security and vulnerability management - Copyleft obligations - Code signing - Attribution guidelines - Business review process - Required notice generation - Automated registration of use - Contribute - Contribution overview - Forking repositories - Business review, if required - Release - Release overview - Checklist - Copyright headers - Foster your community - Build and publish your project - Code signing - Business review, if required - Data and Insights - GitHub insights - Registrations and request insights - GitHub information - GitHub overview - Accounts and linking - Repositories - Teams - Organizations - Transferring and migrating repos - Service accounts - Two-factor authentication - Apps, services and integrations - Large files, LFS Monthly newsletter The team prepares a monthly newsletter to all interested parties in the company, including all linked GitHub users, with a distribution north of 25,000 members. As a low-frequency resource, the hope is that the newsletter helps provide useful information without inundating teams. Typical newsletters will include 3-5 topics and updates of note, some data or graphs and charts of adoption and community health, and the date for the next open source meetup. Open Source Meetup A monthly gathering on the Microsoft campus (or online), the meetup is a quick event that starts off with networking, a quick lunch, and then 3-4 speakers who share their recent open source experiences. This helps connect people, have interesting conversations, and build community. Open Source Champs A cross-company group of people who get it, these are some of the best open source minds at Microsoft, who have connections, experience, data, and can help drive best practices and advise on situations. The champs primarily get involved in a specific discussion list in the company today, but hopefully in time they will also be able to help share information with their organizations, and build a two-way conversation channel with more stakeholders and businesses across the company. The champs will be a key part of helping us continue to scale and share and get subject matter experts connected with the right teams as they open up. Internal support OSPO offers an internal support experience, with internal mail, a Teams channel, and other places, to help support people as needed. The most common issues tend to be helping people find the documentation for what they are looking to do, pointing them at the GitHub account linking experience or new repo wizard, or helping answer policy clarifications and questions. Not all questions can be answered by OSPO - we do at times need people to reach out to their legal contact for their organization to support their request. It is also super important to us that we update internal docs and tools when we learn of gaps or improvements that can be made, so that the next group of people with the same need will not need an escalation or support incident to help answer their question. We look to our principles around delegation, self-service, and elimination to help reduce support cost, solving problems before they grow too large. A clever hack or targeted fix often helps a lot. Executive council and briefings Appointed executives from across Microsofts businesses make up the majority of the OSS Exec Council that meets at least quarterly to discuss progress on evolving the culture, being available to advise how their groups are contributing to open source, and otherwise helping provide a conduit with leadership, the open source champs, and our open source office. In many ways, this group is the board of directors of our OSPO team and we look forward to helping to tackle whatever is next as a company in this space. Having this in place helps us take the temperature of leadership of where to spend our effort and how to think about complicated issues and how to make things friction-free for our engineering teams who want to contribute to and use open source everywhere. Were also available to brief business leaders and others on their approach, share the learnings others have had, and strive to remove roadblocks with all teams. Learnings Weve learned a lot, and continue to learn as we scale the open source adoption across the company. Many of the specifics learnings Ive had relate to how we operate at scale, how much we can set teams free with tools and access, and how to help others learn from the experiences. Just a few short stories Our GitHub Bedlam moment GitHub is super real: we finally had our own Bedlam incident earlier this year we had a GitHub team inside one of our largest GitHub orgs called All Members, to make it easy to quickly give access to private repos ahead of the launch of new work at events. The idea is that as long as you give all members read access to the repo, they can then fork and submit pull requests, making it easier than trying to figure out what teams may grant them the rights they need. It also encourages the more open contribution model we love. GitHub Team Discussions shipped in late 2017 and essentially were not very frequently used by our open source teams for quite a while. Finally, one quiet day in January, it finally happened someone posted a message to the all employees in this org team discussion, immediately going out to a lot of people and then someone posted about how to update your notification settings and then it spiraled out from there. Along the way we learned a few things: People learned about GitHub discussions We may have found a few minor bugs in the notification settings on GitHub that were corrected We realized there are downsides to massive teams We had some fun, too Old time Microsoft people smiled a lot. Bedlam was a classic Microsoft learning lesson. Its fun that we can still have similar experiences with the modern developer toolkit, too Two-factor authentication A surprising amount of our internal support traffic comes from employees who unfortunately lose access to their GitHub accounts. Since we require our users to use GitHubs two-factor authentication, and this is a separate two-factor system than the Azure Active Directory multi-factor auth that our employees also use, its easy for people to get confused, or to lose access to the GitHub side of things. Thankfully, GitHub does a great job of helping remind people to save their two-factor authentication codes and to encourage other technology such as the use of U2F devices. Everytime a new iPhone model comes out, our support volume spikes a lot of people use two-factor authentication apps on their phone, but then forget to store their backup codes or anticipate that wiping their phone and upgrading to a new one may not always restore what they think it may. I strongly recommend that everyone have a YubiKey to help make things easier when using GitHub on a daily basis. GitHub API: user renames Most GitHub REST v3 API calls operate on GitHub username as opposed to user IDs. GitHub allows users to change their username, which means that if you are not also validating what their current username is, you could find that a few API calls are not working for a particular set of users who have changed their usernames. There is a GitHub API that can take a GitHub user ID and provide you the current user information response, so you can hit that occassionally, such as in a daily job which also respects e-tags and caches responses, to make sure you have the accurate username before calling operational APIs. At scale, we tend to see about 25 user renames per week at Microsoft right now. GitHub API: conditional requests The GitHub APIs conditional request guidance is good, but many libraries or users of their APIs do not seem to use them by default. As long as you keep a cached version of the entities you request, you can use the e-tag and an If-None-Match header to help reduce your load on the rate limits for GitHub. A request with the existing e-tag, to check if the entity has changed, will not count against your rolling API limit if it has not changed. Its great seeing more and more libraries such as octokit/rest.js supporting this concept in various ways, or having extensibility for it. Be nice to GitHubs API. Team vs Individual repo permissions Whenever possible we strongly encourage that the repositories that Microsoft governs use team permissions instead of granting individual permissions. GitHub Teams support multiple team maintainers, helping projects evolve over time, and making it easier to keep access and admin permission assigned as needed. Weve had to do a lot of user education on this topic: if someone opens a support ticket needing access permissions, we will never grant them individual permissions (Collaborator on GitHub) to a repo, but instead will ask them to help us find or create a new Team on GitHub for that permission assignment to go to. ** Team permissions. Teams with maintainers. Avoid individual permissions.** This helps keep things sane. Very sane. Transparency around permissions As previously mentioned, our GitHub management portal shows all employees all the permissions for teams, including secret teams. Since we are focused on enabling open source on GitHub, this helps answer questions people have about who has access to administer or change settings on a repo and reduces support volume. On GitHub you can natively see, given a team, which repos and permissions it has, if you are a member of the org. However, you cannot see easily from a given repo who the teams are. Transparent data in the portal reduces support costs around the 404 these arent the droids you are looking for experience on GitHub: since GitHub will return a 404 for a repo that an org member does not have access to, we used to get a decent amount of support traffic from people asking if the URL they were sent by a team member was real or not. Paying GitHub Before we spent a few billion dollars on GitHub, we had paid GitHub organization accounts at various pricing tiers for our open source GitHub orgs: we needed private repo access for people getting ready to release their open source. Early on we would have a bunch of people running around with corporate cards and expensing GitHub. Over the years we were able to consolidate this spend and bring sanity as the GitHub use scaled, and also to help use that as a forcing function: by centrally funding the official GitHub organizations for open source in the OSPO team, we were able to make it easy for teams to get going without having to worry about billing or setting up new organizations and systems. GitHub offered a really useful annual invoice payment method, so instead of having to worry about using a corporate card or other payment method each month, we would just submit a single payment for a set of GitHub orgs, then process that for payment. GitHub sales was super helpful and friendly. The only minor issue weve had with this is the few times that we had to make changes to the LFS data packs for an org during the year we would solve this by approving the purchase order for GitHub to include some additional buffer, so that GitHub sales could just invoice us for the additional data packs as needed. Before we used the invoice payment method, we did use corporate credit cards as needed and approved by our finance team; it was nice that we could associate a Billing Manager or managers to the orgs to help manage that without having to worry about permission to be org owners or control the resource itself. Kudos to the friendly GitHub sales and their supporting staff - they were always willing to jump on a Zoom video conference call and work through the details, such as when we were signing the Corporate Terms of Service agreement for some of our orgs. GitHubs annual invoice payment option is really straightforward and can help you reduce random one-off corporate card expenses. Org proliferation Many Microsoft teams love to ship the org chart, but when it comes to reinforcing Microsofts open source releases, we prefer all repos to go in our few main organizations. This also helps make it easier for engineers to get going: they do not need to identify both an org and a team to work on a repo, they just need to know the repo name or team details. By policy and reinforced in our tooling and other systems, we have asked people to just use the official Microsoft GitHub org since early 2017. This has been super important by providing a really straightforward way for people to get access and not worry about cross-org permissions or finding things. We also know that product names and teams change so often that these things just would not scale. While the GitHub rename features provide some flexibility to orgs and repos, we would prefer not to change too many things at once. Newer GitHub features such as an enterprise account announced in May sound really useful to help address this in the future: if you can combine compliance and billing together, perhaps it will be easier to have additional organizations where it makes sense. Build a strategy around how many GitHub organizations your organization will have for open source. The answer might be 1. Human challenges Were continually learning from people! People are great. Products vs Projects Microsoft is super crisp on the requirements to ship products and services: the Microsoft Security Development Lifecycle (SDL) helps team to be mindful of security and how to build and ship software. As a company, our products also have requirements around code signing, packaging, localization, globalization, servicing, accessibility, etc. Shipping an open source code project (a repo) is a little less involved, but teams still need to do the right thing. Our policies around releasing source code are more about business purpose and approval, making sure governance information and LICENSE files and READMEs are in place, and that teams are ready to support, build and evolve a friendly open community. Weve had a few learning situations where teams thought that the full set of requirements to release a product - such as localizing in many languages - were required to release an open source repo. For now, we emphasize to teams that projects are not products, but often, they compliment one another, and sometimes, they literally are the same thing. Products and services are very different from open source code. Thats OK. Tell your friends. Forking guidance Forking is a big deal in the open source world. There are times where a fork is the right evolution or move for a community to evolve, but weve found that some teams view forks as the way to contribute upstream. For example: if someone wants to contribute to CLA Assistant, one approach would be to fork the repo to their individual GitHub account, and then to submit a pull request to that upstream project. Another approach would be to fork the project to the official Microsoft organization, prepare changes, and then submit it as a pull request. While the second example makes it clear that this is very much a Microsoft contribution, it creates confusion, because Microsoft just wants to participate in the upstream project, and not fork it in any hard way. We strongly encourage teams to simply fork and submit pull requests, the GitHub way, from their individual accounts, not from the official organization account. We want teams to always contribute to the upstream when possible and only fork as a last resort. Forking can be a big deal. Upstream is the right place to contribute. Support volume (e-mail) Since we set the e-mail address opensource at microsoft as the e-mail address associated with the official Microsoft organization on GitHub, we get a lot of e-mail traffic from people looking for help with issues and products. Within GitHub, if youre browsing a repo and click Contact GitHub in the footer of the web page, it essentially asks whether you are looking to report a GitHub issue or an issue with the repo. This helps reduce issue/support traffic to GitHub for open source repos. GitHub then offers the e-mail address associated with the SUPPORT files for the repo, or falls back to the org-wide e-mail address. So we get a lot of e-mail. Weve learned to improve spam filters and use templates and work to address issues, but we do get a lot of mail. Expect that support will need to happen. People want more open source from us! On the entertaining side, many passionate users of long-time Microsoft software regularly write in to ask us to open source their favorite projects Flight Simulator Microsoft Money Age of Empires our operating system Its great that people really want to see this, but I think a lot of folks do not always understand that commercial software often has many dependencies or licensed components that may not be open source, or the code requires a very specialized build environment, etc. I do love seeing the historical releases to share code, such as the original winfile.exe or old-school MS-DOS 1.25 and 2.0! Hopefully teams will find the time to share when it makes sense. Its also a reality that historical software releases are not a place that will be easy to build an active collaborative community around unless a clean mechanism exists to release and build modern bits. Open source all the things Exciting new GitHub features As outlined in Build like an open source community with GitHub Enterprise, Mario Rodriguez from GitHub highlights how the features that help make GitHub great for open source can also be super useful for any organization. Weve started moving to GitHub Enterprise Cloud for more of our open source organizations and Im excited to see what our engineering teams will do with these new capabilities. A few capabilities in particular that were starting to use include: Org insights Available in beta now for Enterprise Cloud accounts, you can take a look at the high-level trends around new issues, closed issues, and other key specs and data points for all of your repositories across the org. Another nice view is the where members are getting work done look at where time is being spent on GitHub by everyone contributing to the org. Maintainer roles Especially interesting to teams like .NET which have a large amount of activity and maintainers, new roles beyond the classic read/write/admin on GitHub means that they can appoint Triage users (can manage issues and PRs, but not push code directly to the repo) or Maintain users (additional settings on top of issues and PR management). Transferring issues Moving issues between repos used to be done by third-party, unofficial, or random tooling, and now that GitHub directly supports issue movement, were pretty happy to see this additional option open for communities who have issues spanning multiple repos. Audit logs GitHub has an audit log API beta for Enterprise Cloud customers now. Using GraphQL, this means that we will be able to keep an audit log copy in our systems for review, analysis, and without the manual or cumbersome approach required today to either ingest and store webhook information in an append-only data store, and/or parse the JSON and CSV versions of the audit log for a GitHub organization. Looking forward We have a lot left to do and our journey will continue to get more interesting as Microsoft continues its open source adventure. Some of the things the Open Source Programs Office will be thinking through will include As a company we will continue to work to evolve our take on a maturity model for open source organizations, and I cant wait to see what is next. Sharing guidance and guides Similar to the Open Source Guides at //opensource.guide created by GitHub, wed love to share, to help others in the industry learn from our progress. Playbooks around how to think about open source, making business decisions related to open source, all are fun topics we would love to share our learnings and take on. We may also be able to share our docs, or a version of them, with the world. Major kudos to Google who already shares their open source guidance and policies on their site. Identifying contribution opportunities We are starting to look at ways to draw attention to contribution opportunities, such as highlighting up-for-grabs issues across our releases, and also recognizing when our employees contribute to open projects outside control of the company, too. By updating the opensource.microsoft.com site, we hope to be able to tell good stories and share useful information about what our teams are up to. As a short-term experiment, we are listing up for grabs issues right on the homepage of our open source site, to learn about whether people find that interesting. Open Source Resources Here are open source GitHub repos mentioned in this post. Check them out! cla-assistant/cla-assistant amzn/oss-attribution-builder clearlydefined/crawler todogroup/repolinter clearlydefined/curated-data microsoft/opensource-portal microsoft/ghcrawler fossology/fossology nexB/scancode-toolkit licensee/licensee octokit/rest.js Hope you found this interesting, let me know what you think on Twitter. If you were expecting a short or concise post, you may have clicked on the wrong thing Jeff Wilcox Principal Software Engineer Microsoft Open Source Programs Office "
        ],
        "story_type": "Normal",
        "url_raw": "https://jeffwilcox.blog/2019/06/scaling-25k/",
        "comments.comment_id": [20331878, 20332519],
        "comments.comment_author": ["cshg", "jillesvangurp"],
        "comments.comment_descendants": [5, 1],
        "comments.comment_time": [
          "2019-07-02T03:22:11Z",
          "2019-07-02T06:00:32Z"
        ],
        "comments.comment_text": [
          "The title makes it sound like the number of employees at Github scaled to 25k. The actual meaning though is that the number of Github contributers / participants at Microsoft increased to 25k.<p>Quote:\n\"At Microsoft today we have almost 25,000 engineers participating in our official GitHub organizations for open source, a great number of them contributing to open source communities throughout GitHub.\"<p>Quite misleading IMO.",
          "Nice to see that MS is very serious about open source. This is a huge change from back when Steve Balmer was still in charge and open source was a dirty word.<p>Employing this many people to work on open source costs billions. I'm sure not all of those people are doing this full time. But still, this represents an enormous investment. That just goes to show how incredibly valuable OSS is these days. That's similar to the net worth of some open core companies that have been debated on HN recently (Mongo, Elastic, Redis, etc.).<p>MS is getting plenty of return on investment. For example a lot of their development tooling is getting significant external contributions. Co-developing software with externals makes economic sense when the primary function of that software is to help people find their way to your for profit services and software. This reduces their cost, makes the software more valuable, and grows the user base of the software. More users means more opportunity for these people to find their way to for profit MS stuff. Even as a recruiting tool this is super valuable since no doubt many external contributors are on the radar for hiring.<p>Just having a lot of developers give up their mac books and instead choosing to run windows again increases the chance that they might end up in Azure, which as we learned recently mostly runs Linux stuff these days. MS is relearning that staying on friendly terms with developers is important for them. This is something that e.g. Google or Apple may want to consider. They do lots of OSS as well of course but they seem a lot more focused on internals lately and some of what they do seems a bit hostile even. Especially Google is starting to act a lot like MS used to act.<p>Maybe buying Gitlab would not be a bad idea for them ..."
        ],
        "id": "ebfea52d-f06b-4468-8163-c994eb6ee073",
        "url_text": "At Microsoft today we have almost 25,000 engineers participating in our official GitHub organizations for open source, a great number of them contributing to open source communities throughout GitHub. Its been quite a ride: thats 10X the engineers we were working with when I posted in 2015 about our experience scaling from 20 to 2,000 engineers in the Azure open source org. As a member of Microsofts Open Source Programs Office (OSPO) team, its been exciting to be a part of this growth, and I wanted to take some time to write down some of the investments we have made so that others could get a peek inside. Using data gathered through an open source project Ill mention in this post, I was able to query our GitHub entity data to create this chart of Microsofts public repos created since 2011: Some of our employees have always been on GitHub, contributing to projects, sharing their ideas and insights. Many of our teams are new to social coding and working in the open, and look to OSPO to help provide documentation and guidance. We have engineers who are busy contributing to projects all over GitHub, donating their time and code or entire projects, many I am sure enjoy working with open source in an official capacity, others after hours, or just hacking away. Looking at the contributors to virtual-kubelet, a project thats part of the Cloud Native Computing Foundation, I see familiar names of people Ive worked with. The Visual Studio Code team has been on fire, moving at a fast pace for years. Theres an entire community of awesome people from around the world on GitHub every day opening issues, performing code reviews, and contributing code to VSCode. These teams are finding new ways to communicate, to build consensus and governance models, and to invite maintainers into the fold. In this post, I will cover: Core principles the Open Source Programs Office uses to guide the open source and GitHub experience Technical investments we have made to scale GitHub Program investments Key learnings Looking to the future Resources including the open source projects mentioned in this post Im going to be focusing more on the tactical approach we took to enabling GitHub at scale and not any specific projects experience - though I hope to track down and share the experiences that projects like the Terminal, TypeScript, Accessibility Insights, and the Windows Calculator have had. We work hard to build the right experiences so that engineers have everything they need, without the OSPO team getting in their way. It should be no surprise that open source is a big part of what has helped us to scale, and we continue to give back and contribute where we can: weve adopted CLA Assistant, an open source project started by SAP, and now contribute to the project (we threw away our home-built, aging Contributor License Agreement (CLAs) bot!) were using an open source attribution engine built by Amazon our self-service GitHub management tooling is open source were collaborating on Clearly Defined, an OSI project, to crawl all the open source projects we can find to discover license, copyright, and other data, curating and sharing the resulting data in the open our team has invested in moving to more common open services and systems such as containers and Postgres and MongoDB to make it easier to collaborate with other companies and their preferred tech stacks Looking forward, we have a lot more work to do to focus on developing our capabilities - evolving our maturity models around what healthy and awesome projects look like, helping graduate work into the community and to foundations, and continuing rapid iteration, experiments, and learning from all of this. Im so excited to see where we are at in another few years, and encouraged by the collaboration happening on open source projects across the industry, the developing communities around specific Microsoft technologies, and all the random contributions that Microsoft engineers are making to open source all over GitHub as their teams take dependencies on and get involved in the associated communities. Principles weve adopted To encourage the right behaviors and teach Microsoft employees how to use GitHub and participate in communities, weve identified a number of tenants or principles that we try and use in everything that OSPO does specific to our tooling and approach. For example, we encourage teams to work in the open on GitHub, getting to learn the tools and lingo. Eliminate + Simplify Wed rather remove a complex process to reduce the work our engineers need to do if it is not providing the right return or value. If we need to ask teams questions about the project they want to release as open source, we focus on what problem were trying to solve, and we have been able to eliminate questions that we used to ask, after thinking through the outcomes with stakeholders and advisors. Looking back five years, we used to have a number of manual registration systems where engineers would let us know what open source they use. These were often free-form text fields, and many teams would only take a best-faith effort to share that data. Today weve been able to eliminate many of the registration scenarios by detecting the use of open source in many scenarios across the company, just asking follow-up questions or going through reviews for certain projects when necessary or needing more information. Eliminating process is not always possible, but if we continually ask questions about the workflows and guidance, and ask the teams using these systems to provide feedback and suggestions, hopefully well test the edges and eliminate where possible. Self-service At scale, we cannot be a roadblock, and we trust and encourage our engineers to learn through experience. We want to make sure users learn about GitHub teams, and how to request to join them. There is no possible way that we would have been able to get 25,000 people collaborating on GitHub so quickly without building a self-service experience whereby our engineers could join our GitHub orgs at their pace as they have a need to learn and participate. Traditionally, a GitHub invitation needs to be sent by an org owner to a specific username or e-mail address, and that just would not work well at our scale, without being in the way of our engineers. Thanks to the GitHub API and our GitHub management portal, things are smooth. Whenever possible, we want to provide documents, guidance, and other reusable resources, instead of having to rely on special knowledge or process that has manual steps. Delegation Our office provides guidance and resources to help advise Microsoft businesses in their approach to contributing to, using, and releasing open source, but we leave the business decisions with the particular business. Decisions such as whether to release open source go through a question/answer experience, and the outcome is either automatic approval, or a business approval workflow that kicks off, allowing a teams business and legal representatives make the final decisions and choices. We also have begun deputizing open source champs: people who can help spread the good word and provide opinions and advice to their teams about how to think about open source. Transparency Open source centers around collaboration, but one of the challenges today with GitHub orgs full of many repos is identifying what teams have access to accept pull requests or administer settings. While GitHub shows a lot of this data if you start with the Teams view in an org and drill into a specific team, theres no view for a given repos teams, unless youre an admin for that repo. For all of our repos, releases and reviews, Our portal exposes the given teams that control each repo All the release requests and data are stored in work items available to any employee Our portal for GitHub management shows all GitHub Teams, including secret or hidden teams We enable cross-org search in our portal, to more easily locate similarly named teams and repos across orgs, reducing confusion and internal support costs Thanks to this, out of the 1000s of repos, its relatively painless to find the right contacts for a project inside the company. Authentic GitHub Engineers should learn how the GitHub interface works, how pull requests and reviews, issues, forks, organization teams, and collaborators function. Whenever possible, we hope that our users go directly to GitHub to manage their teams, configure team maintainers, welcome new community members into their projects as maintainers or contributors. While we do have separate internal interfaces and tools, these ideally augment the native experience. Weve built a browser extension that our users can install to help light up GitHub to make it easier to find employees, resources, or internal tools. Its important to us that engineers learn the GitHub fork and pull request model for contributing, as we strive to use a similar experience for inner source work. Technical investments Our engineering team has made many technical investments to help scale the company to be able to participate in open source better. Whenever possible we want to use open source to make open source better and share those learnings and contributions for other companies and individuals to use. Well invest in our own tooling when we must, but look to the marketplace and work that others are doing on the horizon: we are excited to see GitHubs Enterprise Cloud product evolve and drive new features and capabilities into the product to make enterprise-scale open source easier. Adopting CLA Assistant Today we host an instance of an open source project for Contributor License Agreement (CLA) management, integrated with GitHub, called CLA Assistant. This allows us to make sure that people contributing to our projects have signed the appropriate CLA before we accept a contribution. Once the CLA for an entity is signed once by a community member, they can then contribute to all other repos, so its really a relatively low burden that helps keep our lawyer friends happy. CLA Assistant is an open source project that was started by SAP and is licensed as Apache 2.0. Weve contributed to the project functionality to help with scale issues and rate limiting, automatic signing to handle when employees join and leave the company, and are excited to be able to support a new class of corporate CLAs thanks to contributions made by others to that project. In 2017 we migrated to this new open solution from the in-house CLA bot that we abandoned. I really like how the VSCode team messaged this to their community with a GitHub issue (#34239). The system is powered by GitHub org-level webhooks, so it is always on and teams do not need to worry about whether their repos are protected with the CLA and ready for contribution. Data and Insights GitHub provides useful information including traffic stats, contributor info and other breakdowns at the repository level. Across Microsofts open source projects, however, we have a need to be able to slice and dice data looking for trends over time, analyzing investments at scale, and so realized early on that we needed to import all of the available data we can from our GitHub open source releases into our own big data systems such as Azure Data Lake and Azure Data Explorer. Newer GitHub Enterprise Cloud features look to help provide organization insights, and were super excited to give those a try to augment the other data and insight methods we are using today. Here are some of the projects weve used, created, or collaborated on. GHTorrent Our team is one of the sponsors of the GHTorrent Project, an effort to create a scalable, queryable, offline mirror of data offered through the GitHub REST API. The repo is at github.com/gousiosg/github-mirror. This data, similar to GHArchive, helps learn about the broad collaboration happening on GitHub. We donate cloud compute resources to the project run by Georgios Gousios who kicked off the project with ideas and collaboration with Diomidis Spinellis. At Microsoft, we ingest the data into Azure Data Lake to be able to run interesting queries and learn more about trends and happenings beyond our campus. GHCrawler GHCrawler is a robust GitHub API crawler that walks a queue of GitHub entities transitively, retrieving and storing their contents. The initial project launched in 2016 and evolved significantly at the TODO Group tools hackathon in June 2017 while working with other companies to abstract the data stores to support other technologies and stacks. The crawler taught us a lot about the GitHub API and how to be friendly citizens by focusing on the caching of entities, the use of e-tags, and being careful to not repeatedly fetch the same resource. Different from GHTorrent, the crawler is able to traverse Microsofts own open source organizations using tokens that can peer into our current GitHub team memberships, retrieve info about maintainers, configuration, and private repos, and so is very useful in answering operational questions about the growth of GitHub at Microsoft. We ingest the data from GHCrawler into both Azure Data Lake Azure Data Explorer and use that data in Power BI dashboards and reports, live data display on internal sites, and other resources. The chart of new repos at the top of this post was created by querying this crawler data as stored in Azure Data Explorer. Heres a query that returns repos created in the official Microsoft organization in the last 30 days (that are public): Since the data comes from GitHub but is stored internally, teams can make use of the data for their own business needs and interests, without having to worry about GitHub API rate limiting, the specifics around collecting that data, and just being able to focus on using the data effectively to solve business problems. A favorite resource for many teams looking to build new communities is the data around pull requests and issues, and also collected traffic API data. Since GitHub only provides a 2-week window of consolidated traffic info, storing the data in our big data tools helps us look at trends more easily. ClearlyDefined ClearlyDefined is an Open Source Initiative (OSI) project that has a mission to help FOSS projects thrive by being clearly defined. The lack of clarity around licenses reduces engagement that ends up meaning fewer users, fewer contributors, and a smaller community. Communities choose a license for their project with the terms that they like. Defining and knowing the license for an open source project is essential to a successful community partnership, and so ClearlyDefined helps clarify that by identifying key data such as license set, attribution parties, code location, and when the data is missing, curation can help. Microsoft contributes to the effort and is making use of the data to help provide license clarity in the tooling used to help teams understand their use of open source. As of June 2019, ClearlyDefined has over 6.3 million definitions. These definitions are created by running tools such as licensee, scancode-toolkit, and fossology tools across oodles of open source. repolinter Another TODO Group project OSPO collaborates on is repolinter. Given a source repository, you can run repolinter to learn basic information that can help inform whether a project has all that is necessary to incubate and build a healthy community, such as license files README files Code of Conduct, governance or contribution information No binaries Licenses detectable by licensee Source headers have license information We hope to be able to share this data in a more visible way to help teams see where they can make simple improvements, or even by automatically opening pull requests if they are missing the essentials such as a mention of the Code of Conduct. oss-attribution-builder Weve collaborated with Amazon and use their amzn/oss-attribution-builder project to help build open source license notice files to help teams meet their legal obligations. GitHub management portal Microsofts GitHub management portal that employees to use, detailed in my 2015 post on the portal, handles: self-service org joining for employees cross-organization search of people, repos, and teams support jobs to maintain data generating digests and reports caching GitHub REST API responses processing web hook events Here you can see an experience where you can search, sort and filter repos across all of the many GitHub orgs in one place, improving discoverability. The portal itself is open source and has been rebuilt and refactored over the past few years to try and be more useful and to allow other companies to use it: Node service and site, now implemented in TypeScript Backing stores supported include Postgres and others backed by interfaces An out-of-the-box run local in memory experience is now available The repo is on GitHub at microsoft/opensource-portal. New repo wizard While we have a very liberal and friendly policy to make it easy for an engineering team to decide to share some code such as a sample, we do want to make sure that theres a process in place that includes a business decision and legal team approval if theres a major release being considered. Microsoft has chosen to turn off the Allow Members to Create Repositories option on our open source GitHub organizations. Instead, we have our own new repository wizard within our GitHub management tooling. This site collects information about the purpose of the repo, the common GitHub Team permissions that will help a group of engineers get off and running, and then populates the repo with the standard Microsoft open source content such as Code of Conduct info in the README, an appropriate LICENSE file for the project, and the original purpose and use of the repo, for data reporting purposes internally. To make this experience easier to find, we work to educate people through documentation and tools such as the previously mentioned Open Source Assistant web browser extension. The extension is able to light up the green new repo button on our orgs and link directly to the wizard, providing a better user experience than the you have insufficient permission to create a new repository message that our users would receive on GitHub otherwise. The outcome of the repo wizard will be the creation of a public or private repo on GitHub. We ask, by policy, that teams only use our open source GitHub orgs for work they are going to ship within the next 30 days. Release reviews The outcome of the new repo wizard is either auto-approval to make a project public, or the kick-off of a business and legal review process, to help comply with policy, and inform leaders about important open source decisions that are being made. We have an internal service called the Usage Service that creates a work item in our work item tracking system (Azure Boards), and assigns that work item to the appropriate legal and business reviewer. The work item is then passed along, with fields or more info being filled out, discussions are had, and eventually the review will be final approved and ready for teams to ship their project to the world and go build that open community. Here is a screenshot, redacted of the details and names, showing the work item for the release approval of the Windows Calculator: After the final approval, an automated e-mail is sent to the original requester with any guidance from the various reviewers involved in the process, to help them understand any notifications, legal obligations, or other work that needs to be done to support their release. Open Source Assistant Browser Extension Internally we ship a browser extension that lights up GitHub with Microsoft-specific information and guidance. Since a GitHub username may not be related to a persons recognized corporate identity, the extension lights up the more common corporate alias inline throughout GitHub - pull requests, issues, people profiles. By highlighting other Microsoft employees on GitHub, people who use the extension are able to dedicate additional focus on a good collaboration experience with the community, while easily being able to better identify their coworkers on the site. The extension helps people to continue to use the native GitHub experience, while augmenting bits and pieces where weve made decisions around our GitHub org settings, such as disabling New Repo creation direct on GitHub. The extension also is also to provide links to policy around open source, link to the new repository wizard. Since we disable native repo creation on GitHub in order to ask engineers to complete our wizard to learn more about their intent with their open source release, we often would get support questions about how to create repos, since GitHub would display the message that the user could not create repos. Now, this is what they see: In another sceenshot, you can see a Join button that lets an employee self-service join the GitHub organization through our GitHub management portal: Finally, since we want teams to use our official GitHub org that is configured automatically with the CLA system, with compliance tools, audit logs, and the ability to manage the lifecycle of when users join and leave, we do not allow teams to create new GitHub orgs for open source. By updating GitHubs new organization page, we have a chance to let people know about the policy and other resources before they get too far down the road of trying to setup yet-another-org. The extension works Firefox, Chrome, and the new Edge based on Chromium. Microsoft employees can learn more and install the extension at aka.ms/1esassistant. While this extenson is not open source today, if others think it could help their organization, I can see what we can do. Regular automation jobs Weve implemented jobs that do everything from keeping data and caches up-to-date to helping make sure that we have confidence in the membership of our GitHub organizations. These jobs are all open source, implemented in TypeScript, as a part of the opensource-portal repo. Removing former employees Connected to HR data sources, we can react when an employee leaves the company. In a big company, people are always changing roles, trying new things, and sometimes they leave for other opportunities. We unlink people when they leave: removing the association between their GitHub account and their corporate identity, and removing their GitHub organization membership(s). We welcome former employees to continue to contribute and participate to projects where they should continue to be maintainers, but for clarity, do not allow them to remain as org members. We also send an e-mail to their former manager, so they have confidence that the right things have happened with permissions and system access. Enforced system teams To help enable operations, support, meet security obligations, and light up bots such as our CLA system, we have a set of GitHub teams that are automatically and permanently added to every repository in our GitHub orgs, when the job is configured. By monitoring the GitHub event bus, the job adds what we call system teams to a repo the instant a new repo is created. And, if a repo admin tries to remove or change the permissions that such a system team has, the job automatically restores its permission. An occassional cronjob also runs to validate and reenforce these teams as needed. Preventing big permission grants As the number of members in a GitHub organization grows high, youll also start to see very large teams. We have teams such as All Employee Members that are designed to give easy read access to private repos that are being finished up and polished before going open source. An unintended side effect we found: some people wanted to just give all members administrative access over their repo, to make the act of onboarding get much easier. Trouble is, with admin comes great power, and any of the thousands of members of the org could then delete the repo, change whatever they want, override branch protections, etc. This also would automatically subscribe thousands of people to spammy notification messages, depending on their GitHub notification preferences. As a result, we have a large permissions job that monitors for grants of write or admin access to an org-defined large number of people. Once such a grant is made, and the team is too large, the job will automatically downgrade the permissions to try and prevent mistakes, and then send an e-mail address to the person who made the permission choice, hoping to educate them about the risks and help them understand what changes were made. GitHub username changes GitHub allows users to change their username. This is excellent: many people start with a cute screen name or other indicator, then realize theyre professionally developing software with their peers in the industry, and often want to change that username. Not all users know that they can rename their account, so we do get people who delete an account, just to create a new one. However, the majority of GitHub apps and APIs work off of GitHub usernames instead of the integer-based user ID. To help stay ahead of this, we have a cronjob that looks up GitHub users by ID and then refreshes our internal data store of those usernames, in the hope that we can improve the API success rate and reduce support costs when renames happen. For critical operations, additional logic has been added to GitHub apps to anticipate the potential of a user rename happening, and being able to smartly attempt to fallback to looking up the user first by ID, or alerting and sending the issue to operational support. Weekly and daily digests We have a cronjob that sends updates about new changes and interesting happenings on GitHub repos. These are typically only send to repo admins, to not spam too many people. The digests are personalized to the receipient, and cover scenarios such as: notifying the creator of a repo, and their manager, of the new repo notifying when the # of admins becomes rather high, so that team maintainers can cleanup permissions abandoned repos that havent been touched in years when a team is down to a single maintainer, suggesting they appoint another maintainer or two when a repository has been flipped from private to public Program investments More important than the technical tools and hacks we put in place, the programs that we run in the Open Source Programs Office help drive process improvements, awareness of features and capabilities, roll out new functionality, and to emphasis the work that is happening in the open in new ways. Heres a set of the OSPO investments we have been making or put in place to help drive improvement in the open source maturity level of the company. Docs We have a central documentation repository that is easily accessible to employees where they can read through guides on how to go about releasing open source, or contributing to it. The hope is that teams new to many of the tools and approaches in open source can refer to the material, share it with others, and help us to scale this to others. Heres a rough look at the outline of some of that material. In time I hope we can prepare a version of these docs to share with the world - General Information - What exactly is Open Source? - When and why Open Source - Can I use it? - Misconceptions - Bad Practices - Benefits - Use - Overview - Security and vulnerability management - Copyleft obligations - Code signing - Attribution guidelines - Business review process - Required notice generation - Automated registration of use - Contribute - Contribution overview - Forking repositories - Business review, if required - Release - Release overview - Checklist - Copyright headers - Foster your community - Build and publish your project - Code signing - Business review, if required - Data and Insights - GitHub insights - Registrations and request insights - GitHub information - GitHub overview - Accounts and linking - Repositories - Teams - Organizations - Transferring and migrating repos - Service accounts - Two-factor authentication - Apps, services and integrations - Large files, LFS Monthly newsletter The team prepares a monthly newsletter to all interested parties in the company, including all linked GitHub users, with a distribution north of 25,000 members. As a low-frequency resource, the hope is that the newsletter helps provide useful information without inundating teams. Typical newsletters will include 3-5 topics and updates of note, some data or graphs and charts of adoption and community health, and the date for the next open source meetup. Open Source Meetup A monthly gathering on the Microsoft campus (or online), the meetup is a quick event that starts off with networking, a quick lunch, and then 3-4 speakers who share their recent open source experiences. This helps connect people, have interesting conversations, and build community. Open Source Champs A cross-company group of people who get it, these are some of the best open source minds at Microsoft, who have connections, experience, data, and can help drive best practices and advise on situations. The champs primarily get involved in a specific discussion list in the company today, but hopefully in time they will also be able to help share information with their organizations, and build a two-way conversation channel with more stakeholders and businesses across the company. The champs will be a key part of helping us continue to scale and share and get subject matter experts connected with the right teams as they open up. Internal support OSPO offers an internal support experience, with internal mail, a Teams channel, and other places, to help support people as needed. The most common issues tend to be helping people find the documentation for what they are looking to do, pointing them at the GitHub account linking experience or new repo wizard, or helping answer policy clarifications and questions. Not all questions can be answered by OSPO - we do at times need people to reach out to their legal contact for their organization to support their request. It is also super important to us that we update internal docs and tools when we learn of gaps or improvements that can be made, so that the next group of people with the same need will not need an escalation or support incident to help answer their question. We look to our principles around delegation, self-service, and elimination to help reduce support cost, solving problems before they grow too large. A clever hack or targeted fix often helps a lot. Executive council and briefings Appointed executives from across Microsofts businesses make up the majority of the OSS Exec Council that meets at least quarterly to discuss progress on evolving the culture, being available to advise how their groups are contributing to open source, and otherwise helping provide a conduit with leadership, the open source champs, and our open source office. In many ways, this group is the board of directors of our OSPO team and we look forward to helping to tackle whatever is next as a company in this space. Having this in place helps us take the temperature of leadership of where to spend our effort and how to think about complicated issues and how to make things friction-free for our engineering teams who want to contribute to and use open source everywhere. Were also available to brief business leaders and others on their approach, share the learnings others have had, and strive to remove roadblocks with all teams. Learnings Weve learned a lot, and continue to learn as we scale the open source adoption across the company. Many of the specifics learnings Ive had relate to how we operate at scale, how much we can set teams free with tools and access, and how to help others learn from the experiences. Just a few short stories Our GitHub Bedlam moment GitHub is super real: we finally had our own Bedlam incident earlier this year we had a GitHub team inside one of our largest GitHub orgs called All Members, to make it easy to quickly give access to private repos ahead of the launch of new work at events. The idea is that as long as you give all members read access to the repo, they can then fork and submit pull requests, making it easier than trying to figure out what teams may grant them the rights they need. It also encourages the more open contribution model we love. GitHub Team Discussions shipped in late 2017 and essentially were not very frequently used by our open source teams for quite a while. Finally, one quiet day in January, it finally happened someone posted a message to the all employees in this org team discussion, immediately going out to a lot of people and then someone posted about how to update your notification settings and then it spiraled out from there. Along the way we learned a few things: People learned about GitHub discussions We may have found a few minor bugs in the notification settings on GitHub that were corrected We realized there are downsides to massive teams We had some fun, too Old time Microsoft people smiled a lot. Bedlam was a classic Microsoft learning lesson. Its fun that we can still have similar experiences with the modern developer toolkit, too Two-factor authentication A surprising amount of our internal support traffic comes from employees who unfortunately lose access to their GitHub accounts. Since we require our users to use GitHubs two-factor authentication, and this is a separate two-factor system than the Azure Active Directory multi-factor auth that our employees also use, its easy for people to get confused, or to lose access to the GitHub side of things. Thankfully, GitHub does a great job of helping remind people to save their two-factor authentication codes and to encourage other technology such as the use of U2F devices. Everytime a new iPhone model comes out, our support volume spikes a lot of people use two-factor authentication apps on their phone, but then forget to store their backup codes or anticipate that wiping their phone and upgrading to a new one may not always restore what they think it may. I strongly recommend that everyone have a YubiKey to help make things easier when using GitHub on a daily basis. GitHub API: user renames Most GitHub REST v3 API calls operate on GitHub username as opposed to user IDs. GitHub allows users to change their username, which means that if you are not also validating what their current username is, you could find that a few API calls are not working for a particular set of users who have changed their usernames. There is a GitHub API that can take a GitHub user ID and provide you the current user information response, so you can hit that occassionally, such as in a daily job which also respects e-tags and caches responses, to make sure you have the accurate username before calling operational APIs. At scale, we tend to see about 25 user renames per week at Microsoft right now. GitHub API: conditional requests The GitHub APIs conditional request guidance is good, but many libraries or users of their APIs do not seem to use them by default. As long as you keep a cached version of the entities you request, you can use the e-tag and an If-None-Match header to help reduce your load on the rate limits for GitHub. A request with the existing e-tag, to check if the entity has changed, will not count against your rolling API limit if it has not changed. Its great seeing more and more libraries such as octokit/rest.js supporting this concept in various ways, or having extensibility for it. Be nice to GitHubs API. Team vs Individual repo permissions Whenever possible we strongly encourage that the repositories that Microsoft governs use team permissions instead of granting individual permissions. GitHub Teams support multiple team maintainers, helping projects evolve over time, and making it easier to keep access and admin permission assigned as needed. Weve had to do a lot of user education on this topic: if someone opens a support ticket needing access permissions, we will never grant them individual permissions (Collaborator on GitHub) to a repo, but instead will ask them to help us find or create a new Team on GitHub for that permission assignment to go to. ** Team permissions. Teams with maintainers. Avoid individual permissions.** This helps keep things sane. Very sane. Transparency around permissions As previously mentioned, our GitHub management portal shows all employees all the permissions for teams, including secret teams. Since we are focused on enabling open source on GitHub, this helps answer questions people have about who has access to administer or change settings on a repo and reduces support volume. On GitHub you can natively see, given a team, which repos and permissions it has, if you are a member of the org. However, you cannot see easily from a given repo who the teams are. Transparent data in the portal reduces support costs around the 404 these arent the droids you are looking for experience on GitHub: since GitHub will return a 404 for a repo that an org member does not have access to, we used to get a decent amount of support traffic from people asking if the URL they were sent by a team member was real or not. Paying GitHub Before we spent a few billion dollars on GitHub, we had paid GitHub organization accounts at various pricing tiers for our open source GitHub orgs: we needed private repo access for people getting ready to release their open source. Early on we would have a bunch of people running around with corporate cards and expensing GitHub. Over the years we were able to consolidate this spend and bring sanity as the GitHub use scaled, and also to help use that as a forcing function: by centrally funding the official GitHub organizations for open source in the OSPO team, we were able to make it easy for teams to get going without having to worry about billing or setting up new organizations and systems. GitHub offered a really useful annual invoice payment method, so instead of having to worry about using a corporate card or other payment method each month, we would just submit a single payment for a set of GitHub orgs, then process that for payment. GitHub sales was super helpful and friendly. The only minor issue weve had with this is the few times that we had to make changes to the LFS data packs for an org during the year we would solve this by approving the purchase order for GitHub to include some additional buffer, so that GitHub sales could just invoice us for the additional data packs as needed. Before we used the invoice payment method, we did use corporate credit cards as needed and approved by our finance team; it was nice that we could associate a Billing Manager or managers to the orgs to help manage that without having to worry about permission to be org owners or control the resource itself. Kudos to the friendly GitHub sales and their supporting staff - they were always willing to jump on a Zoom video conference call and work through the details, such as when we were signing the Corporate Terms of Service agreement for some of our orgs. GitHubs annual invoice payment option is really straightforward and can help you reduce random one-off corporate card expenses. Org proliferation Many Microsoft teams love to ship the org chart, but when it comes to reinforcing Microsofts open source releases, we prefer all repos to go in our few main organizations. This also helps make it easier for engineers to get going: they do not need to identify both an org and a team to work on a repo, they just need to know the repo name or team details. By policy and reinforced in our tooling and other systems, we have asked people to just use the official Microsoft GitHub org since early 2017. This has been super important by providing a really straightforward way for people to get access and not worry about cross-org permissions or finding things. We also know that product names and teams change so often that these things just would not scale. While the GitHub rename features provide some flexibility to orgs and repos, we would prefer not to change too many things at once. Newer GitHub features such as an enterprise account announced in May sound really useful to help address this in the future: if you can combine compliance and billing together, perhaps it will be easier to have additional organizations where it makes sense. Build a strategy around how many GitHub organizations your organization will have for open source. The answer might be 1. Human challenges Were continually learning from people! People are great. Products vs Projects Microsoft is super crisp on the requirements to ship products and services: the Microsoft Security Development Lifecycle (SDL) helps team to be mindful of security and how to build and ship software. As a company, our products also have requirements around code signing, packaging, localization, globalization, servicing, accessibility, etc. Shipping an open source code project (a repo) is a little less involved, but teams still need to do the right thing. Our policies around releasing source code are more about business purpose and approval, making sure governance information and LICENSE files and READMEs are in place, and that teams are ready to support, build and evolve a friendly open community. Weve had a few learning situations where teams thought that the full set of requirements to release a product - such as localizing in many languages - were required to release an open source repo. For now, we emphasize to teams that projects are not products, but often, they compliment one another, and sometimes, they literally are the same thing. Products and services are very different from open source code. Thats OK. Tell your friends. Forking guidance Forking is a big deal in the open source world. There are times where a fork is the right evolution or move for a community to evolve, but weve found that some teams view forks as the way to contribute upstream. For example: if someone wants to contribute to CLA Assistant, one approach would be to fork the repo to their individual GitHub account, and then to submit a pull request to that upstream project. Another approach would be to fork the project to the official Microsoft organization, prepare changes, and then submit it as a pull request. While the second example makes it clear that this is very much a Microsoft contribution, it creates confusion, because Microsoft just wants to participate in the upstream project, and not fork it in any hard way. We strongly encourage teams to simply fork and submit pull requests, the GitHub way, from their individual accounts, not from the official organization account. We want teams to always contribute to the upstream when possible and only fork as a last resort. Forking can be a big deal. Upstream is the right place to contribute. Support volume (e-mail) Since we set the e-mail address opensource at microsoft as the e-mail address associated with the official Microsoft organization on GitHub, we get a lot of e-mail traffic from people looking for help with issues and products. Within GitHub, if youre browsing a repo and click Contact GitHub in the footer of the web page, it essentially asks whether you are looking to report a GitHub issue or an issue with the repo. This helps reduce issue/support traffic to GitHub for open source repos. GitHub then offers the e-mail address associated with the SUPPORT files for the repo, or falls back to the org-wide e-mail address. So we get a lot of e-mail. Weve learned to improve spam filters and use templates and work to address issues, but we do get a lot of mail. Expect that support will need to happen. People want more open source from us! On the entertaining side, many passionate users of long-time Microsoft software regularly write in to ask us to open source their favorite projects Flight Simulator Microsoft Money Age of Empires our operating system Its great that people really want to see this, but I think a lot of folks do not always understand that commercial software often has many dependencies or licensed components that may not be open source, or the code requires a very specialized build environment, etc. I do love seeing the historical releases to share code, such as the original winfile.exe or old-school MS-DOS 1.25 and 2.0! Hopefully teams will find the time to share when it makes sense. Its also a reality that historical software releases are not a place that will be easy to build an active collaborative community around unless a clean mechanism exists to release and build modern bits. Open source all the things Exciting new GitHub features As outlined in Build like an open source community with GitHub Enterprise, Mario Rodriguez from GitHub highlights how the features that help make GitHub great for open source can also be super useful for any organization. Weve started moving to GitHub Enterprise Cloud for more of our open source organizations and Im excited to see what our engineering teams will do with these new capabilities. A few capabilities in particular that were starting to use include: Org insights Available in beta now for Enterprise Cloud accounts, you can take a look at the high-level trends around new issues, closed issues, and other key specs and data points for all of your repositories across the org. Another nice view is the where members are getting work done look at where time is being spent on GitHub by everyone contributing to the org. Maintainer roles Especially interesting to teams like .NET which have a large amount of activity and maintainers, new roles beyond the classic read/write/admin on GitHub means that they can appoint Triage users (can manage issues and PRs, but not push code directly to the repo) or Maintain users (additional settings on top of issues and PR management). Transferring issues Moving issues between repos used to be done by third-party, unofficial, or random tooling, and now that GitHub directly supports issue movement, were pretty happy to see this additional option open for communities who have issues spanning multiple repos. Audit logs GitHub has an audit log API beta for Enterprise Cloud customers now. Using GraphQL, this means that we will be able to keep an audit log copy in our systems for review, analysis, and without the manual or cumbersome approach required today to either ingest and store webhook information in an append-only data store, and/or parse the JSON and CSV versions of the audit log for a GitHub organization. Looking forward We have a lot left to do and our journey will continue to get more interesting as Microsoft continues its open source adventure. Some of the things the Open Source Programs Office will be thinking through will include As a company we will continue to work to evolve our take on a maturity model for open source organizations, and I cant wait to see what is next. Sharing guidance and guides Similar to the Open Source Guides at //opensource.guide created by GitHub, wed love to share, to help others in the industry learn from our progress. Playbooks around how to think about open source, making business decisions related to open source, all are fun topics we would love to share our learnings and take on. We may also be able to share our docs, or a version of them, with the world. Major kudos to Google who already shares their open source guidance and policies on their site. Identifying contribution opportunities We are starting to look at ways to draw attention to contribution opportunities, such as highlighting up-for-grabs issues across our releases, and also recognizing when our employees contribute to open projects outside control of the company, too. By updating the opensource.microsoft.com site, we hope to be able to tell good stories and share useful information about what our teams are up to. As a short-term experiment, we are listing up for grabs issues right on the homepage of our open source site, to learn about whether people find that interesting. Open Source Resources Here are open source GitHub repos mentioned in this post. Check them out! cla-assistant/cla-assistant amzn/oss-attribution-builder clearlydefined/crawler todogroup/repolinter clearlydefined/curated-data microsoft/opensource-portal microsoft/ghcrawler fossology/fossology nexB/scancode-toolkit licensee/licensee octokit/rest.js Hope you found this interesting, let me know what you think on Twitter. If you were expecting a short or concise post, you may have clicked on the wrong thing Jeff Wilcox Principal Software Engineer Microsoft Open Source Programs Office ",
        "_version_": 1718536496931143680
      },
      {
        "story_id": 19822065,
        "story_author": "nik1aa5",
        "story_descendants": 10,
        "story_score": 58,
        "story_time": "2019-05-03T20:40:25Z",
        "story_title": "Code Review from the Command Line (2018)",
        "search": [
          "Code Review from the Command Line (2018)",
          "https://blog.jez.io/cli-code-review/",
          "I do the bulk of my code reviews from the command line, especially when reviewing larger changes. Ive built up a number of tools and config settings that help me dig into the nuances of the code Im reviewing, so that I can understand it better than if I were just browsing online. In particular, Ill walk through how I check out the code in the first place, get a feel for what changed, visualize the relationships between the files that changed, bring up the code diffs in Vim, leverage the unique power of the editor and the terminal. But first, lets talk briefly about the point of code review in the first place. Code review philosophy When I ask that other people review my code, its an opportunity for me to teach them about the change Ive just made. When I review someone elses code, its to learn something from them. Some other benefits of code review include: Team awareness (to keep a pulse on what else is going on within your team). Finding alternative solutions (maybe theres a small change that lets us kill two birds with one stone). If this is different from how you think about code review, check out this talk. Code review is a powerful tool for learning and growing a team. With that out of the way, lets dive into the tools I use to maximize benefit I get from code review. Checking out the code The first step to reviewing code in the terminal is to check out the code in the first place. One option is to simply to git pull and then git checkout <branch>. But if you happen to be using GitHub, we can get this down to just one command: hub pr checkout <pr-number> It works using hub, which is a tool that exposes various features of GitHub from the command line. If the pull request is from someone elses fork, hub is even smart enough to add their fork as a remote and fetch it. At first glance With the branch checked out locally, usually my next step is to get a feel for what changed. For this, Ive written a git alias that shows: which files changed how many lines changed in each file (additions and deletions) how many lines changed overall Heres the definition of git stat from my ~/.gitconfig: [alias] # list files which have changed since REVIEW_BASE # (REVIEW_BASE defaults to 'master' in my zshrc) files = !git diff --name-only $(git merge-base HEAD \\\"$REVIEW_BASE\\\") # Same as above, but with a diff stat instead of just names # (better for interactive use) stat = !git diff --stat $(git merge-base HEAD \\\"$REVIEW_BASE\\\") Under the hood, it just works using git diff, git merge-base, and a personal environment variable REVIEW_BASE. REVIEW_BASE lets us choose which branch to review relative to. Most of the time, REVIEW_BASE is master, but this isnt always the case! Some repos branch off of gh-pages. Sometimes I like to review the most recent commit as if it were its own branch. To review the code relative so some other base, set REVIEW_BASE before running git stat: # Review between 'gh-pages' and the current branch REVIEW_BASE=gh-pages git stat # Review changes made by the last commit of this branch: REVIEW_BASE=HEAD^ git stat I have export REVIEW_BASE=master in my ~/.bashrc, because most projects branch off of master. Nothing too crazy yetGitHub can already do everything weve seen so far. Lets start to up the ante. Visualizing file change frequency Ive written a short script that shows me a visualization of how frequently the files involved in this branch change over time: This command identifies two main things: Files with lots of changes. Files that have changed a lot in the past are likely to change in the future. I review these files with an eye towards what the next change will bring. Is this change robust enough to still be useful in the future? Will we throw this out soon after merging it? Files with few changes. Files that arent changed frequently are more likely to be brittle. Alternatively, its often the case that infrequently changed files stay unchanged because the change is better made elsewhere. Does this change challenge an implicit assumption so that some other part of the code was relying on? Is there a better place for this change? Those two commands (git stat and git heatmap) are how I kick off my code review: getting a birds-eye view of the change and some historical context for what Im dealing with. Next, I drill down into the relationships between the files that changed. Visualizing relationships between files At work I review JavaScript files, so Ive built out this next bit of tooling specifically for JavaScript.The techniques here apply to any language that you can statically analyze. In particular, I have a rough prototype of everything JavaScript-specific you see here that works with Standard ML instead. If you can find me the dependency information for your favorite language, Id be happy to help you turn it into a visualization. It helps to understand which files import others, so I have a command that computes the dependency graph of the files changed on this branch: This is where we start to see some distinct advantages over what GitHub provides. As you see above, the git depgraph alias calculates the dependency graph for files changed by this branch. Why is this useful? Maybe we want to start reviewing from Provider.js, since it doesnt depend on any other files that have changed. Maybe we want to work the other way: start with Elements.js so we know the motivation for why Provider.js had to changed in the first place. In either case, we can see the structure of the change. Three files depend on Elements.js, so its serving the needs of many modules. Element.js only has one dependency, etc. Each branchs dependency graph shows different information; it can be surprising what turns up. I have the git depgraph alias defined like this: [alias] depgraph = !git madge image --webpack-config webpack.config.js --basedir . --style solarized-dark src Some notes about this definition: It depends on the git-madge command, which you can download and install here. Its using this projects webpack.config.js file, so Ive made this alias local to the repo, rather than available globally. It dumps the image to stdout. Above, we used iTerm2s imgcat program to pipe stdin and dump a raster image to the terminal. If you dont use iTerm2 or dont want to install imgcat, you can pipe it to Preview using openThe open command is macOS-specific. On Linux, you might want to look at the display command from ImageMagick. (open -f -a Preview) or just redirect the PNG to a file. The git depgraph alias is a game changer. It makes it easier to get spun up in new code bases, helps make sense of large changes, and just looks plain cool. But at the end of the day, we came here to review some code, so lets take a look at how we can actually view the diffs of the files that changed. Reviewing the diffs To review the diffs, the simplest option is to just run git diff master..HEAD. This has a bunch of downsides: No syntax highlighting (everything is either green or red). No surrounding context (for example, GitHub lets you click to expand lines above or below a diff hunk). The diff is unified, instead of split into two columns. No way to exclude a specific file (the 300 line diff to your yarn.lock file is sometimes nice to hide). My solution to all of these problems is to view the diffs in Vim, with the help of two Vim plugins and two git aliases. Before we get to that, heres a screenshot: Looks pretty similar to GitHubs interface, with the added bonus that its using my favorite colorscheme! The Vim plugins featured are: tpope/vim-fugitive for showing the side-by-side diff (:Gdiff). airblade/vim-gitgutter for showing the +/- signs. jez/vim-colors-solarized for tweaking the diff highlight colors.Ive patched the default Solarized colors for Vim so that lines retain their syntax highlighting in the diff mode, while the backgrounds are highlighted. You can see how this works in this commit: https://github.com/jez/vim-colors-solarized/commit/bca72cc And to orchestrate the whole thing, Ive set up these two aliases: [alias] # NOTE: These aliases depend on the `git files` alias from # a few sections ago! # Open all files changed since REVIEW_BASE in Vim tabs # Then, run fugitive's :Gdiff in each tab, and finally # tell vim-gitgutter to show +/- for changes since REVIEW_BASE review = !vim -p $(git files) +\\\"tabdo Gdiff $REVIEW_BASE\\\" +\\\"let g:gitgutter_diff_base = '$REVIEW_BASE'\\\" # Same as the above, except specify names of files as arguments, # instead of opening all files: # git reviewone foo.js bar.js reviewone = !vim -p +\\\"tabdo Gdiff $REVIEW_BASE\\\" +\\\"let g:gitgutter_diff_base = '$REVIEW_BASE'\\\" Heres how they work: git review opens each file changed by this branch as a tab in Vim. Then :Gdiff from vim-fugitive shows the diff in each tab. git reviewone is like git review, but you specify which files to open (in case you only want to diff a few). Like with the git stat alias, these aliases respect the REVIEW_BASE environment variable Ive set up in my ~/.bashrc. (Scroll back up for a refresher.) For example, to review all files relative to master: REVIEW_BASE=master git review At this point, you might think that all weve done is re-create the GitHub code review experience in Vim. But actually what weve done is so much more powerful. Interactive Code Review When reviewing on GitHub, the code is completely staticyou cant change it. Also, because the code is coming from GitHubs servers, its laggy when you click around to view related files. By switching our code review to the terminal, we can now edit files, jump to other files, and run arbitrary commands at no cost. It might not be obvious how huge of a win this is, so lets see some examples. Take this screenshot of the requireElement function. It moved from above the findElement function to below it (probably because the former calls the latter): But is the location of the requireElement function the only thing thats changed? By editing the file to move the function back to its original location, vim-fugitive will automatically recompute the diff. And in fact, we can see that the type of the argument has changed too, from string to ElementType: If we had been viewing this on GitHub, we might have taken for granted that the function didnt change. But since were in our editor, we can interactively play around with our code and discover things we might have missed otherwise. The advantages of interactive code review go well beyond this example: In a Flow project, we can ask for the type of a variable. In a test file, we can change the test and see if it still passes or if it now fails. We can grep the project for all uses of a function (including files not changed by this branch). We can open up related files for cross-referencing. We can run the code in a debugger and see how it behaves. By having the full power of our editor, we can literally retrace the steps that the author went through to create the pull request. If our goal is to understand and learn from code review, theres no better way than walking in the authors shoes. Recap To recap, heres a list of the tools I use to review code at the command line: hub pr checkout git stat to list files that have changed git heatmap to show how frequently these files change git depgraph to show a graph of which files depend on which git review to open diffs of all the files in Vim git reviewone to open diffs for a specific handful of files If youre having trouble incorporating any of these into your workflow, feel free to reach out and let me know! Im happy to help. "
        ],
        "story_type": "Normal",
        "url_raw": "https://blog.jez.io/cli-code-review/",
        "comments.comment_id": [19825360, 19825978],
        "comments.comment_author": ["gfiorav", "badfrog"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-05-04T09:19:29Z",
          "2019-05-04T12:19:42Z"
        ],
        "comments.comment_text": [
          "I’ve gotten used to Vimium [0] (a chrome extension that emulates Vim commands in the browser) and I rarely use the mouse or arrow keys now. For me, that has been the biggest leap in productivity for my CRs.<p>I agree that it’s nice to review within the context of the terminal, but I still think the UI (at least for Github) is easygoing and productive (specially when you ditch the mouse).<p>[0] - <a href=\"https://chrome.google.com/webstore/detail/vimium/dbepggeogbaibhgnhhndojpepiihcmeb\" rel=\"nofollow\">https://chrome.google.com/webstore/detail/vimium/dbepggeogba...</a>",
          "Reminds me of Jane Street's code review in emacs: <a href=\"https://blog.janestreet.com/putting-the-i-back-in-ide-towards-a-github-explorer/\" rel=\"nofollow\">https://blog.janestreet.com/putting-the-i-back-in-ide-toward...</a>"
        ],
        "id": "ad51db81-7ba6-4542-9188-109cd6b32a5c",
        "url_text": "I do the bulk of my code reviews from the command line, especially when reviewing larger changes. Ive built up a number of tools and config settings that help me dig into the nuances of the code Im reviewing, so that I can understand it better than if I were just browsing online. In particular, Ill walk through how I check out the code in the first place, get a feel for what changed, visualize the relationships between the files that changed, bring up the code diffs in Vim, leverage the unique power of the editor and the terminal. But first, lets talk briefly about the point of code review in the first place. Code review philosophy When I ask that other people review my code, its an opportunity for me to teach them about the change Ive just made. When I review someone elses code, its to learn something from them. Some other benefits of code review include: Team awareness (to keep a pulse on what else is going on within your team). Finding alternative solutions (maybe theres a small change that lets us kill two birds with one stone). If this is different from how you think about code review, check out this talk. Code review is a powerful tool for learning and growing a team. With that out of the way, lets dive into the tools I use to maximize benefit I get from code review. Checking out the code The first step to reviewing code in the terminal is to check out the code in the first place. One option is to simply to git pull and then git checkout <branch>. But if you happen to be using GitHub, we can get this down to just one command: hub pr checkout <pr-number> It works using hub, which is a tool that exposes various features of GitHub from the command line. If the pull request is from someone elses fork, hub is even smart enough to add their fork as a remote and fetch it. At first glance With the branch checked out locally, usually my next step is to get a feel for what changed. For this, Ive written a git alias that shows: which files changed how many lines changed in each file (additions and deletions) how many lines changed overall Heres the definition of git stat from my ~/.gitconfig: [alias] # list files which have changed since REVIEW_BASE # (REVIEW_BASE defaults to 'master' in my zshrc) files = !git diff --name-only $(git merge-base HEAD \\\"$REVIEW_BASE\\\") # Same as above, but with a diff stat instead of just names # (better for interactive use) stat = !git diff --stat $(git merge-base HEAD \\\"$REVIEW_BASE\\\") Under the hood, it just works using git diff, git merge-base, and a personal environment variable REVIEW_BASE. REVIEW_BASE lets us choose which branch to review relative to. Most of the time, REVIEW_BASE is master, but this isnt always the case! Some repos branch off of gh-pages. Sometimes I like to review the most recent commit as if it were its own branch. To review the code relative so some other base, set REVIEW_BASE before running git stat: # Review between 'gh-pages' and the current branch REVIEW_BASE=gh-pages git stat # Review changes made by the last commit of this branch: REVIEW_BASE=HEAD^ git stat I have export REVIEW_BASE=master in my ~/.bashrc, because most projects branch off of master. Nothing too crazy yetGitHub can already do everything weve seen so far. Lets start to up the ante. Visualizing file change frequency Ive written a short script that shows me a visualization of how frequently the files involved in this branch change over time: This command identifies two main things: Files with lots of changes. Files that have changed a lot in the past are likely to change in the future. I review these files with an eye towards what the next change will bring. Is this change robust enough to still be useful in the future? Will we throw this out soon after merging it? Files with few changes. Files that arent changed frequently are more likely to be brittle. Alternatively, its often the case that infrequently changed files stay unchanged because the change is better made elsewhere. Does this change challenge an implicit assumption so that some other part of the code was relying on? Is there a better place for this change? Those two commands (git stat and git heatmap) are how I kick off my code review: getting a birds-eye view of the change and some historical context for what Im dealing with. Next, I drill down into the relationships between the files that changed. Visualizing relationships between files At work I review JavaScript files, so Ive built out this next bit of tooling specifically for JavaScript.The techniques here apply to any language that you can statically analyze. In particular, I have a rough prototype of everything JavaScript-specific you see here that works with Standard ML instead. If you can find me the dependency information for your favorite language, Id be happy to help you turn it into a visualization. It helps to understand which files import others, so I have a command that computes the dependency graph of the files changed on this branch: This is where we start to see some distinct advantages over what GitHub provides. As you see above, the git depgraph alias calculates the dependency graph for files changed by this branch. Why is this useful? Maybe we want to start reviewing from Provider.js, since it doesnt depend on any other files that have changed. Maybe we want to work the other way: start with Elements.js so we know the motivation for why Provider.js had to changed in the first place. In either case, we can see the structure of the change. Three files depend on Elements.js, so its serving the needs of many modules. Element.js only has one dependency, etc. Each branchs dependency graph shows different information; it can be surprising what turns up. I have the git depgraph alias defined like this: [alias] depgraph = !git madge image --webpack-config webpack.config.js --basedir . --style solarized-dark src Some notes about this definition: It depends on the git-madge command, which you can download and install here. Its using this projects webpack.config.js file, so Ive made this alias local to the repo, rather than available globally. It dumps the image to stdout. Above, we used iTerm2s imgcat program to pipe stdin and dump a raster image to the terminal. If you dont use iTerm2 or dont want to install imgcat, you can pipe it to Preview using openThe open command is macOS-specific. On Linux, you might want to look at the display command from ImageMagick. (open -f -a Preview) or just redirect the PNG to a file. The git depgraph alias is a game changer. It makes it easier to get spun up in new code bases, helps make sense of large changes, and just looks plain cool. But at the end of the day, we came here to review some code, so lets take a look at how we can actually view the diffs of the files that changed. Reviewing the diffs To review the diffs, the simplest option is to just run git diff master..HEAD. This has a bunch of downsides: No syntax highlighting (everything is either green or red). No surrounding context (for example, GitHub lets you click to expand lines above or below a diff hunk). The diff is unified, instead of split into two columns. No way to exclude a specific file (the 300 line diff to your yarn.lock file is sometimes nice to hide). My solution to all of these problems is to view the diffs in Vim, with the help of two Vim plugins and two git aliases. Before we get to that, heres a screenshot: Looks pretty similar to GitHubs interface, with the added bonus that its using my favorite colorscheme! The Vim plugins featured are: tpope/vim-fugitive for showing the side-by-side diff (:Gdiff). airblade/vim-gitgutter for showing the +/- signs. jez/vim-colors-solarized for tweaking the diff highlight colors.Ive patched the default Solarized colors for Vim so that lines retain their syntax highlighting in the diff mode, while the backgrounds are highlighted. You can see how this works in this commit: https://github.com/jez/vim-colors-solarized/commit/bca72cc And to orchestrate the whole thing, Ive set up these two aliases: [alias] # NOTE: These aliases depend on the `git files` alias from # a few sections ago! # Open all files changed since REVIEW_BASE in Vim tabs # Then, run fugitive's :Gdiff in each tab, and finally # tell vim-gitgutter to show +/- for changes since REVIEW_BASE review = !vim -p $(git files) +\\\"tabdo Gdiff $REVIEW_BASE\\\" +\\\"let g:gitgutter_diff_base = '$REVIEW_BASE'\\\" # Same as the above, except specify names of files as arguments, # instead of opening all files: # git reviewone foo.js bar.js reviewone = !vim -p +\\\"tabdo Gdiff $REVIEW_BASE\\\" +\\\"let g:gitgutter_diff_base = '$REVIEW_BASE'\\\" Heres how they work: git review opens each file changed by this branch as a tab in Vim. Then :Gdiff from vim-fugitive shows the diff in each tab. git reviewone is like git review, but you specify which files to open (in case you only want to diff a few). Like with the git stat alias, these aliases respect the REVIEW_BASE environment variable Ive set up in my ~/.bashrc. (Scroll back up for a refresher.) For example, to review all files relative to master: REVIEW_BASE=master git review At this point, you might think that all weve done is re-create the GitHub code review experience in Vim. But actually what weve done is so much more powerful. Interactive Code Review When reviewing on GitHub, the code is completely staticyou cant change it. Also, because the code is coming from GitHubs servers, its laggy when you click around to view related files. By switching our code review to the terminal, we can now edit files, jump to other files, and run arbitrary commands at no cost. It might not be obvious how huge of a win this is, so lets see some examples. Take this screenshot of the requireElement function. It moved from above the findElement function to below it (probably because the former calls the latter): But is the location of the requireElement function the only thing thats changed? By editing the file to move the function back to its original location, vim-fugitive will automatically recompute the diff. And in fact, we can see that the type of the argument has changed too, from string to ElementType: If we had been viewing this on GitHub, we might have taken for granted that the function didnt change. But since were in our editor, we can interactively play around with our code and discover things we might have missed otherwise. The advantages of interactive code review go well beyond this example: In a Flow project, we can ask for the type of a variable. In a test file, we can change the test and see if it still passes or if it now fails. We can grep the project for all uses of a function (including files not changed by this branch). We can open up related files for cross-referencing. We can run the code in a debugger and see how it behaves. By having the full power of our editor, we can literally retrace the steps that the author went through to create the pull request. If our goal is to understand and learn from code review, theres no better way than walking in the authors shoes. Recap To recap, heres a list of the tools I use to review code at the command line: hub pr checkout git stat to list files that have changed git heatmap to show how frequently these files change git depgraph to show a graph of which files depend on which git review to open diffs of all the files in Vim git reviewone to open diffs for a specific handful of files If youre having trouble incorporating any of these into your workflow, feel free to reach out and let me know! Im happy to help. ",
        "_version_": 1718536479570919426
      },
      {
        "story_id": 19541870,
        "story_author": "testcross",
        "story_descendants": 4,
        "story_score": 82,
        "story_time": "2019-04-01T10:10:22Z",
        "story_title": "Git Implemented in OCaml",
        "search": [
          "Git Implemented in OCaml",
          "https://github.com/mirage/ocaml-git",
          "Support for on-disk and in-memory Git stores. Can read and write all the Git objects: blobs, trees, commits and tags. It can also handle pack files, pack indexes and index files (where the staging area lives - only for git-unix package). All the objects share a consistent API, and convenience functions are provided to manipulate the different objects. For instance, it is possible to make a pack file position independent (as the Zlib compression might change the relative offsets between the packed objects), to generate pack indexes from pack files, or to expand the filesystem of a given commit. The library comes with some command-line tools called ogit-* as a Proof-of-concept of the core library which shares a similar interface with git, but where all operations are mapped to the API exposed by ocaml-git (and hence using only OCaml code). However, these tools are not meant to be used. They are just examples of how to use ocaml-git. ocaml-git wants to be a low-level library for irmin. By this fact, high-level commands such as a (patience) diff, git status, etc. are not implemented. As a MirageOS project, ocaml-git is system agnostic. However, it provides a git-unix package which uses UNIX syscall and is able to introspect a usual Git repository in a filesystem. However, ocaml-git handles only Git objects and does not populate your filesystem as git does. For example, Git_unix.Sync.fetch does not give you files fetched from the repository but only synchronizes .git with that repository. The API documentation is available online. Build, Install Instructions and Packages To build and install the project, simply run: $ opam install git $ opam install git-unix $ opam install git-mirage Linking-trick ocaml-git uses 2 libraries with the linking-trick: digestif checkseum These libraries provide a C implementation and an OCaml implementation (mostly to be compatible with js_of_ocaml). However, utop or any a build-system such as ocamlbuild are not able to choose between these implementations. So, you must explicitely choose one. These libraries use virtual-library available with dune. If your build-system is dune, you should not have any problem about that where dune is able to take the default implementation of these libraries. What is supported The loose object files can be read and written; blobs (files) trees (directories) commits (revision history) tags (annotated tags) references (branch names) The PACK files (collections of compressed loose objects using a binary-diff representation) and PACK indexes (indexes of pack files) can be read and written). The binary diff hunks are exposed using a high-level position-independent representation so that they can be manipulated more easily. Pack file can be created and is compressed. The INDEX file (used as for managing the staging area) are fully supported, which means that git diff and git status will work as expected on a repository created by the library. This feature is only available for git-unix when it needs to introspect a file-system. Cloning and fetching (using various options) are fully supported for the Git protocol, the smart-HTTP protocol and git+ssh. A subset of the protocol capabilities are implemented (mainly thin-pack, ofs-delta, side-band-64k and allow-reachable-sha1-in-want). Pushing is still experimental and needs more testing. An abstraction for Git Store Is available. Various store implementations are available: An in-memory implementation; A unix filesystem implementation; What is not supported No server-side operations are currently supported. No GC. Updates, merge and rebase are not supported. Use irmin instead. Performance Performance is comparable to the Git tool. Example This utop example must run into the ocaml-git repository when the given path is .. # ;; load necessary modules # #require \"checkseum.c\" ;; # #require \"digestif.c\" ;; # #require \"git-unix\" ;; # ;; we are going to use this project's local repository # module Store = Git_unix.Store ;; module Store = Git_unix.Store # ;; this module is useful for finding git objects in a git store # module Search = Git.Search.Make (Digestif.SHA1) (Store) ;; module Search : sig type hash = Store.hash type store = Store.t type pred = [ `Commit of hash | `Tag of string * hash | `Tree of string * hash | `Tree_root of hash ] val pred : store -> ?full:bool -> hash -> pred list Lwt.t type path = [ `Commit of path | `Path of string list | `Tag of string * path ] val mem : store -> hash -> path -> bool Lwt.t val find : store -> hash -> path -> hash option Lwt.t end # ;; we want to read the contents of a blob under name [filename] # let read filename = let open Lwt_result.Syntax in (* get store located in current root's .git folder *) let* store = Store.v (Fpath.v (Sys.getcwd ())) in (* find obj-id pointed at by master branch (reference) *) let* commit_id = Store.Ref.resolve store Git.Reference.master in let open Lwt.Syntax in (* find obj-id of of [filename] as a git blob *) let* blob_id = Search.find store commit_id (`Commit (`Path [ filename ])) in match blob_id with | None -> Lwt.return (Error (`Not_found commit_id)) | Some hash -> (* read contents of the blob *) Store.read store hash ;; val read : string -> (Store.Value.t, Store.error) Lwt_result.t = <fun> # let pp = let ok ppf = function | Git.Value.Blob b -> Fmt.string ppf (Git.Blob.to_string b) | _ -> Fmt.string ppf \"#git-object\" in Fmt.result ~ok ~error:Store.pp_error;; val pp : ('_weak1 Git.Value.t, Store.error) result Fmt.t = <fun> # Lwt_main.run Lwt.Infix.(read \"README.md\" >|= pp Fmt.stdout) ;; ocaml-git -- Git format and protocol in pure OCaml Support for on-disk and in-memory Git stores. Can read and write all the Git objects: the usual blobs, trees, commits and tags but also the pack files, pack indexes and the index file (where the staging area lives). [...] () License MIT, see LICENSE.md file for its text. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/mirage/ocaml-git",
        "comments.comment_id": [19545513, 19545515],
        "comments.comment_author": ["AceJohnny2", "avsm"],
        "comments.comment_descendants": [1, 0],
        "comments.comment_time": [
          "2019-04-01T17:06:14Z",
          "2019-04-01T17:06:49Z"
        ],
        "comments.comment_text": [
          "> <i>Updates, merge and rebase are not supported. Use irmin instead.</i><p>So essentially, OGit <i>cannot</i> be used as a CLI replacement for Git?",
          "Making this go fast and suitable for use in the Irmin (irmin.io) branching db has been a fun and multi-year effort.<p>See <a href=\"https://discuss.ocaml.org/t/ann-ocaml-git-2-0/2740\" rel=\"nofollow\">https://discuss.ocaml.org/t/ann-ocaml-git-2-0/2740</a> the discussion on how ocaml-git 2.0 came to be, and some of the libraries and abstractions that had to be developed for it.  A lot of those are now being applied to other protocols in MirageOS, such as the new e-mail stack (who would have thought MIME parsing would be so difficult? <a href=\"https://github.com/mirage/mrmime\" rel=\"nofollow\">https://github.com/mirage/mrmime</a>)"
        ],
        "id": "6c5bed79-589a-4b07-a66f-a6976bb1eedd",
        "url_text": "Support for on-disk and in-memory Git stores. Can read and write all the Git objects: blobs, trees, commits and tags. It can also handle pack files, pack indexes and index files (where the staging area lives - only for git-unix package). All the objects share a consistent API, and convenience functions are provided to manipulate the different objects. For instance, it is possible to make a pack file position independent (as the Zlib compression might change the relative offsets between the packed objects), to generate pack indexes from pack files, or to expand the filesystem of a given commit. The library comes with some command-line tools called ogit-* as a Proof-of-concept of the core library which shares a similar interface with git, but where all operations are mapped to the API exposed by ocaml-git (and hence using only OCaml code). However, these tools are not meant to be used. They are just examples of how to use ocaml-git. ocaml-git wants to be a low-level library for irmin. By this fact, high-level commands such as a (patience) diff, git status, etc. are not implemented. As a MirageOS project, ocaml-git is system agnostic. However, it provides a git-unix package which uses UNIX syscall and is able to introspect a usual Git repository in a filesystem. However, ocaml-git handles only Git objects and does not populate your filesystem as git does. For example, Git_unix.Sync.fetch does not give you files fetched from the repository but only synchronizes .git with that repository. The API documentation is available online. Build, Install Instructions and Packages To build and install the project, simply run: $ opam install git $ opam install git-unix $ opam install git-mirage Linking-trick ocaml-git uses 2 libraries with the linking-trick: digestif checkseum These libraries provide a C implementation and an OCaml implementation (mostly to be compatible with js_of_ocaml). However, utop or any a build-system such as ocamlbuild are not able to choose between these implementations. So, you must explicitely choose one. These libraries use virtual-library available with dune. If your build-system is dune, you should not have any problem about that where dune is able to take the default implementation of these libraries. What is supported The loose object files can be read and written; blobs (files) trees (directories) commits (revision history) tags (annotated tags) references (branch names) The PACK files (collections of compressed loose objects using a binary-diff representation) and PACK indexes (indexes of pack files) can be read and written). The binary diff hunks are exposed using a high-level position-independent representation so that they can be manipulated more easily. Pack file can be created and is compressed. The INDEX file (used as for managing the staging area) are fully supported, which means that git diff and git status will work as expected on a repository created by the library. This feature is only available for git-unix when it needs to introspect a file-system. Cloning and fetching (using various options) are fully supported for the Git protocol, the smart-HTTP protocol and git+ssh. A subset of the protocol capabilities are implemented (mainly thin-pack, ofs-delta, side-band-64k and allow-reachable-sha1-in-want). Pushing is still experimental and needs more testing. An abstraction for Git Store Is available. Various store implementations are available: An in-memory implementation; A unix filesystem implementation; What is not supported No server-side operations are currently supported. No GC. Updates, merge and rebase are not supported. Use irmin instead. Performance Performance is comparable to the Git tool. Example This utop example must run into the ocaml-git repository when the given path is .. # ;; load necessary modules # #require \"checkseum.c\" ;; # #require \"digestif.c\" ;; # #require \"git-unix\" ;; # ;; we are going to use this project's local repository # module Store = Git_unix.Store ;; module Store = Git_unix.Store # ;; this module is useful for finding git objects in a git store # module Search = Git.Search.Make (Digestif.SHA1) (Store) ;; module Search : sig type hash = Store.hash type store = Store.t type pred = [ `Commit of hash | `Tag of string * hash | `Tree of string * hash | `Tree_root of hash ] val pred : store -> ?full:bool -> hash -> pred list Lwt.t type path = [ `Commit of path | `Path of string list | `Tag of string * path ] val mem : store -> hash -> path -> bool Lwt.t val find : store -> hash -> path -> hash option Lwt.t end # ;; we want to read the contents of a blob under name [filename] # let read filename = let open Lwt_result.Syntax in (* get store located in current root's .git folder *) let* store = Store.v (Fpath.v (Sys.getcwd ())) in (* find obj-id pointed at by master branch (reference) *) let* commit_id = Store.Ref.resolve store Git.Reference.master in let open Lwt.Syntax in (* find obj-id of of [filename] as a git blob *) let* blob_id = Search.find store commit_id (`Commit (`Path [ filename ])) in match blob_id with | None -> Lwt.return (Error (`Not_found commit_id)) | Some hash -> (* read contents of the blob *) Store.read store hash ;; val read : string -> (Store.Value.t, Store.error) Lwt_result.t = <fun> # let pp = let ok ppf = function | Git.Value.Blob b -> Fmt.string ppf (Git.Blob.to_string b) | _ -> Fmt.string ppf \"#git-object\" in Fmt.result ~ok ~error:Store.pp_error;; val pp : ('_weak1 Git.Value.t, Store.error) result Fmt.t = <fun> # Lwt_main.run Lwt.Infix.(read \"README.md\" >|= pp Fmt.stdout) ;; ocaml-git -- Git format and protocol in pure OCaml Support for on-disk and in-memory Git stores. Can read and write all the Git objects: the usual blobs, trees, commits and tags but also the pack files, pack indexes and the index file (where the staging area lives). [...] () License MIT, see LICENSE.md file for its text. ",
        "_version_": 1718536466291752960
      },
      {
        "story_id": 18799404,
        "story_author": "lwhsiao",
        "story_descendants": 47,
        "story_score": 122,
        "story_time": "2019-01-01T07:56:44Z",
        "story_title": "Rust 2019: Tame Complexity through Community Involvement Tools",
        "search": [
          "Rust 2019: Tame Complexity through Community Involvement Tools",
          "https://internals.rust-lang.org/t/rust-2019-address-the-big-problem/9109",
          "December 27, 2018, 7:10pm #1 Tame Complexity through Community Involvement Tools Edit: Id just like to say that I think 2018 has been a great year for rust. Rereading this, I realize may have come off as a little negative. End edit. Rust has a big problem. Our feedback loops have gotten big because we have gotten big (as a community). Weve started to council together at the end of the year. I dont think its bad, but part of it is reaching consensus on the current state of Rust. I think its good actually, but we need more things like it - Im not suggesting we call for even more blog posts, but that we start tailoring activities to address the big problem. The big problem is growth (and increased community size) - its a good problem to have, but it does cause issues. Rust has a lot of gaps caused by its explosive growth, more prominent ones have already been covered: A gap between what we have approved (language wise) and what has been implemented (the fallow year) A gap between input and processing capacity team/person wise (organizational debt) A gap between what we have and what we have documented (Stevek Klabniks 2019 post) These are all artifacts of growth. Its gotten harder to hold everything about Rust in your head. The tools the Rust community has been using (mostly github provided ones) were good at small sizes, but dont scale well. Theyre leaky abstractions. Pros and Cons analysis with consensus represented by a linear discussion, state scattered across the web, etc. Harder to follow, harder to moderate, harder to contribute to. Its harder to: track things you care about (I see this as a large contributing factor in the impl Trait return position & website controversies) see the overall status of rust see where things need help Proposal We should build something to fix that. We should build Rust specific infrastructure to manage the complexity that comes community sprawl (the sprawl is not bad). Organization specific infra is a thing (Im looking at you task-cluster that replaced jenkins) What will it look like? Im not sure, but I suspect each team will need something different - form following domain. I have a couple of ideas after reading some other Rust2019 posts of what said central, easy to find tooling should include: RFC Stuff We should probably get an RFC tracker. A real tracker, not github issues (Can you imagine if https://caniuse.com/ used Github issues/prs for its UI? ). Lets track by time and state, category and syntax impact (and more?!). There is so much metadata about RFCs that would be useful if it was understandable by computers. Lay people could get a better idea of whats going on, and contributors could see whats happening at a glance. In addition to the overview/tracking tool, we need a better tool for contributors of RFCs to use. Discussion is the big one. Can we build a tool for structured discussions - pros/cons, links to other RFCs, view changes overtime and hidden votes (because while things arent a popularity, its probably still a good idea to let people express themselves, even if only so the moderators / leaders can see). There are also proposals for limiting throughput, a more distinct staged process, needing a team champion, etc. We could integrate that too. Docs I heard the docs need love. So itd be great to have a list of things that need doing. Sections to proofread, sections that need feedback (e.g. Im trying to accomplish x, but its not working / coming off right), things to write (cookbook), brainstorming structure/approach ideas, etc. Itd also be nice to a have one big index to rule them all, about the various docs, whats next on the doc teams plate (were hoping to improve x, other big initiatives, etc), etc. Misc The embededded working group could probably use a page listing all the platforms, and what their status is, along with a list of drivers. It actually probably exists. Somewhere. There are probably other things for other teams that would be great to put into this one, mythical centralized location. An improved team index would be nice (e.g. direct links to the discord / other place of preferred chat, links to the team specific areas, etc.) If we were to open infrastructure support to contributors (I have a perfectly good machine laying around Id be willing to run a CI runner on 24/7), wed probably want a public dashboard of some sort listing current capacity and a way to volunteer your computer. Barring that, wed probably want a graph of average wait time on compiler PRs (which is measured in days!). There is surely a lot more than this to what wed want (I think) - Im not really familiar enough to say though. Heck half these things are from what Ive read in the 2018 posts, if not more. In Closing The rust community has been great. In addition to the great language, the people are great, and trying hard things - transparent consensus (well, taking in put into account)-based decision making. Its been harder to do that at scale (it either exacts a big toll or breaks down). I think we should fix that. This is a call to trade generic tools for hand-crafted tools. To trade in a little team personality (each team does stuff differently), for less friction (and more accessible to new people). Custom tooling is a big effort. But I think its the best way a growing community long term without giving up the great things that characterize Rusts governance and growth. kennytm December 27, 2018, 8:03pm #2 Those are nifty graphs. Ill take those I think that in terms of gathering support for expanding infrastructure, a better graph would be of the average wait time of things that are S-waiting-on-bors (and perhaps S-waiting-on-crater). That would really drive home the infrastructure woes. The other graphs are useful as well though because they highlight the people component (review/author/team/bikeshed). I wouldnt want to lose any of the current graphs. anp December 27, 2018, 9:08pm #4 As someone whos built a few pieces of custom tooling for Rust including rfcbot and a now-defunct dashboard with many of the metrics youre describing, IMO the bottleneck on these ideas is availability and prioritization of effort/time/labor/work/etc. This kind of work doesnt typically carry lots of prestige (one of the main motivators for contributions), often requires lots of information gathering if youre not building tooling for your own processes, and is often hard to prioritize against other direct impact work. To build things like those described, we either need to find people already in the community with the right desires/skills/experience/etc, attract people to the community who would like to contribute to projects like these, or fund their design and construction via someones full-time work. The first option seems to be hard because existing contributors are often furiously working away on the topics which originally captured their attention. I can see how some existing proposals to do less might free individuals up for some meta-work, most proposals Ive seen have other proposals for priorities that those contributors might adopt to pay down organizational debt within each of the teams. There have been a few attempts in the past to rally contributions around these kinds of custom infra applications (the bors2 conversations in the community team a little while ago springs to mind) and I at least am not aware of any of them taking off. A new effort in that direction would, I hope, address typical pitfalls for coordinating that kind of work. Aside: which team would lead the design of an RFC tracker? Core? Infra? A new one? Questions about the governance of tools like the ones proposed have already come up for rfcbot and figuring out some answer for their home seems like a good idea. The second option to get people working on these is an interesting one but still reduces to a pretty similar problem: who among the existing community will lead the design and implementation of the new tools? What tools will they have to succeed? In terms of full-time funding, Im not sure who would fund this work Mozilla? They seem a bit capped out on the Rust initiatives theyre funding (although I dont have any special info there). Some large tech companies have expressed interest in funding work on Rust, but AFAICT theyre mostly focused on specific things they need to help it succeed in their environments. Perhaps theres another angle to play there? EDIT: I worry that I enumerated a bunch of obstacles without expressing enough enthusiasm this is a great set of ideas and some combination of them should definitely happen! Weve talked about trying to make rfcbot into various forms of this for a little while (@nikomatsakis and I talked about a more general-purpose RFC tracker and workflow organizer at rustconf2016?) and have been mostly blocked on my time and a slow trickle of contributions. Arguably rfcbot could have been the wrong choice for where to build this (the code isnt exactly polished lol), arguably my lack of time has meant that mentoring new contributors hasnt happened when it could have, etc. Not saying that any of this is impossible but that the lack of these tools today seems to me more due to the underlying question of Who? than whether theyd be desirable. I think well end up with a couple of different groups of contributors in this: Disgruntled people who have a hard time following things, who want to see it better People who finish whatever theyve been enthusiastically doing (either completing or burning out/turning over the reigns) People who love stats & transparency maybe. Id think at the very least wed want a working group for this, probably with people from each team at least for coordination. Maybe a proper team? It seems like itd be the in the purview of both Core and Infra, and yet maybe a big enough undertaking to be separate. I didnt even think about funding, but thats a really good point - it will take a lot of bandwidth. I do think that the easiest thing to secure funding for would be for more infrastructure (if we have nice graphs and projections showing potential impact of contributions on turn-around time). Its funny that this is meant to be a way to help solve coordination issues, and the biggest threat to its success is probably coordination issues. I think the following will be pivotal to getting things off the ground and successful: getting a small core estabilished - sort of like how tower-web and warp were worked on for a bit before being released. Its important to get a core established to have something to fit the pieces of the full implementation into. And we dont want a bunch of bike-shedding to prevent the groundwork from happening. Having a coordinator. Once we get far enough, I imagine that there will be lots of little things that could be worked on in parallel. Maybe not though. Emphasis on a couple of highly visible things for the core - PR wait-time and RFC tracking perhaps? Something thats not too hard, and yet is would be liked by a lot of people. I think average turn-around time and RFC tracking brings a lot of simple to see transparency and benefit Official backing - namely a place to live (both domain and server wise), and guidance in terms of extracting / integrating with the current crop of rust stuff. Data sources and points of interaction. A large part of me thinks wed do well to make it modular so other things could reuse it as necessary. Sort of a code level modular Drupal / CMS thing? (disclaimer: Ive never touched PHP/Drupal). In that linked thread you can see that the RFCBot is being used outside of Rust. Anyway, I think these are good points that need to be brought up and figured out - thanks for sharing them. If I had to choose between hearing about obstacles and hearing only enthusiasm, Id choose to hear the obstacles any day . Hopefully putting an emphasis and discussion such tooling will help get the who question answered. Is forge linked to from the main site? Writing from mobile. llogiq January 1, 2019, 9:15am #7 Regarding RFCs, Id like to see a collaborative mind-mapping tool where people could upvote branches to make them appear stronger, and hide (or, for mod team members, even remove) them. This could be a great help to map out the design space. Regarding docs, I held the docs workshop at the RustFest 2019 and I think doing more of such workshops off- and online would be a good start to improve matters. I really like the idea of rust specific infrastructure, mostly because i was thinking about it too Id volunteer time for that and i actually view it as high impact work, at least in the long run. I also prefer starting with a very small scope and slowly build features out from there, feeling out what is really needed and accepted by the rust community while were going. But without fulltime work, this is a really big project. Itd be a big project for a fulltime team zoul January 1, 2019, 3:40pm #9 Exactly. Is it completely nave to think about pooling resources with other languages such as Swift? After all, the problems being solved are very similar, both in the domain and in the social dynamics. I just dont know if the required coordination wouldnt outweight the advantages. Soni January 1, 2019, 3:50pm #10 I think git is a suitable tool for tracking PR discussion. @Soni Sorry to be pedantic, but, do you mean: Using github PRs directly, one per RFC? Using git repos, one per RFC (suggested in earlier proposals), with multiple PRs to manage various stages? Using git repos, storing the discussion directly in them? I dont think Github PRs scale well. The linear discussion model breaks down easily when the PR gets lots of traffic, and its harder to track things programatically. You can adapt the track-ability with stuff like the RFC bot being invoked that maintains its own machine readable records (well, in a db thats searchable somewhere better than the GitHub issue search. Thats still a little brittle in my opinion because of the opportunity for operator error. I think the staged rfc proposal (already linked in my initial post) outlines some good ideas in this regard. The discussion nut is little harder to crack. I do like @llogiqs suggestion of mind-mapping. I think we still need a discussion area, but a nice mind-map would probably go a long ways to calming the discussion stream, especially if we can take comments after the fact and pin them to parts of the mind-map (e.g. for people who come in, dont read the full thing but want to make a point known - and I think that input is still valuable because sometimes its people who feel like they are at the margin, and want to make some lesser point known). There is another proposal for rate-limiting RFCs which is an approach I dont like as much for tackling this, but it makes some good points (Table of Contents) and would probably be workable, but sub-par in my opinion. Im not opposed to keeping Git as a storage mechanism, as it does have great history tools. The biggest draw back of course for custom infra would be the cost. And second would be learning a custom UI. Soni January 1, 2019, 5:05pm #12 None of the above. Instead, all of these: One git repo for ALL RFCs. One branch per RFC. Forks of branches to add comments. Comments can be merged back upstream and git has tools to browse a specific uh, thingy of commits. (e.g. merging commits A and B from different branches requires a merge commit C. ignoring the merge commit, git lets you pick one of those branches to view the commit sequence within it.) This means you just branch a branch to comment on an RFC, and at the end it all gets merged back into the RFCs history. More importantly, its possible to attach RFC changes to specific discussions, something we cant do today. (except through linking to a comment, but that has issues like being hard to follow.) Git does everything we want, including the ability to branch off discussions and create arbitrary new discussion bases on the same repo. Itd be kinda like letting internals (this forum) users create arbitrary categories and subcategories, I think. But it directly integrates with the RFC system and history. Aloso January 2, 2019, 7:29am #13 I do believe we need better tools to discuss RFCs and make decisions. Right now, RFCs often get so many comments that GitHub collapses several hundred of them. However, RFCs are discussed on other forums, too. This means that nobody has time to read all the discussions on all forums. Im proposing a tool to file RFCs, discuss them, and reach a consensus. This tool could also be used for everything else that requires a decision by the community. In this tool, an RFC consists of three parts: The wiki, the feelings section, and the discussion. The wiki describes the RFC in detail and is editable by anyone. In the feelings section, users can express their feelings (e.g. this is confusing or this adds unnecessary complexity or nobody needs this or this would help me a lot or I prefer the syntax with the question mark). Feelings consist of one sentence and can be voted on (totally agree / somewhat agree / somewhat disagree / totally disagree). The feelings are then sorted by how many users agree with them. They basically act as a survey that points out the most frequent concerns. The comments section should be a tree (reddit-like) to allow structuring long discussions. Id like to know what you think about this, and what else you think is important. Id volunteer to create this tool (but hopefully with some input and help). Soni January 2, 2019, 12:33pm #14 The comments section should be a DAG (git-like). @Soni, while the comments being attached to changes is cool, I dont see how it addresses the following issues (which I think are the most pressing issues): Comments being disorganized (the same point made repeatedly) Comments being widely-scattered (in multiple locations) It does raise the bar for adding comments, which maybe reduces both issues but I dont think curtailing feedback is a good way to manage community involvement. I do think it works great for when you have a small number of contributors for any given area of a repository, but the issue that were suffering from (but it is a good issue to have!) is the large number of participants in individual RFCs. I find myself firmly in the camp that a centralized solution is the best for for managing RFCs - perhaps you could explain how the RFC process would benefit from the decentralized DAG format youre suggesting? @Aloso, I think those are great ideas. Itd probably be cool to be able to branch discussions off of the feelings points. We probably want some way to let mods modify the discussion (with open history - the point of the modifications wouldnt be to censor, but to organize). My specific inspiration for this is that in Zulip (chat program), all messages in room need a topic, and if you forget, another person change change the topic for said message so it ends up in the right thread. Aloso January 2, 2019, 4:40pm #16 Of course we could do something similar. I like the idea of adding tags to comments. Then someone can view all comments with a specific tag. With a tree-like discussion, we could also encourage people to start one top-level discussion for each topic, so its structured automatically. True, true. I was mostly thinking for people who come and make a comment without reading all the threads. Of course if we do have threaded discussion, thats a lot less likely. I would like to link post with my draft ideas regarding service for improving RFC process: Of course it will be hard to build such service from ground-up, so I do not propose to start building it right now, but think about it as a general direction in which I think we probably should move. Soni January 2, 2019, 4:47pm #19 It literally attaches parents and children to comments, and you can have multiple branches going on at once. How does that not solve the disorganization problem? (each branch is a separate topic, sort of. you can even merge topics or address many of them at once!) As for comments being widely-scattered, GitHub can actually display these quite well, if with a few filtering issues. (click the number beside fork on a repo. the UI is mostly already there, so they have very little work to do on their end to make the experience better for us.) @Soni How are the parent-child comments stored - as empty commits and its the message? In a text file? In a different text format (json, yaml, maybe folders?) to handle nesting? Anything flat and youll probably have tons of merge conflicts. I consider the current GitHub UI for managing forks, as it relates to using it to explore and get a good overview of branched changes in this scenario, pretty atrocious, not matter how little work it would take for Github to cater to our needs (which I think would be a lot, and even if it was a little, we should base our workflow on a potential change another organization hasnt yet made). Your approach comes off (to me) this is a good idea, and so we should make it work. Id like to approach the issue from more of a what do we want, and whats the best way to implement it?. If we can make a way to make it fit into Git after the fact, after weve gathered all the requirements (quotes because its more like aspirations than requirements, and I think not everything will make the cut), then Im all for it. I do worry that by committing (heh, I love puns) to Git early that well leave features off the table that could be useful. I think that the very least, in order to use Git wed have to roll our own UI as a frontend for it, so the format stays standardized. "
        ],
        "story_type": "Normal",
        "url_raw": "https://internals.rust-lang.org/t/rust-2019-address-the-big-problem/9109",
        "comments.comment_id": [18801614, 18802456],
        "comments.comment_author": ["scaleout1", "Lerc"],
        "comments.comment_descendants": [7, 3],
        "comments.comment_time": [
          "2019-01-01T18:56:58Z",
          "2019-01-01T21:22:14Z"
        ],
        "comments.comment_text": [
          "Recently started introducing Rust in my team and here are my notes on some of the issues we have run into so far. We are primarily a Scala/JVM shop with a little bit of Python.<p>- `Error Handling` is a bit of a dumpster fire right now. Rust has something called a `Result` enum which is similar to `Either[Error,T]` monad in Scala however every single Rust crate I have used so far its creating its own `Error` enum which makes it really hard to compose `Result`. Ideally I would like to chain `Results` as `e1.and_then(e2).and_then(e3)` but its not possible due to incompatible error enums. Ended up using `<a href=\"https://docs.rs/crate/custom_error/1.3.0`\" rel=\"nofollow\">https://docs.rs/crate/custom_error/1.3.0`</a> to align the types.<p>- A lot of basic things are still influx and community wide standards have not been established. For example: I needed to externalize some environment specific settings in a Config but couldnt figure out where to put non-code assets in a cargo project and then how to reliable read them. In JVM world `src/main/resources` acts like a standard place for stuff like this but that patterns has not been established yet.<p>- Distributing code inside the company is hard because there is no integration with Artifactory or similar tools. We are directly referring git sha in Cargo right now and waiting for better solutions.<p>- Rust comes with a default unit test framework but it pretty bare bones. I havent seen examples of test fixture, setup/teardown support and loading test configs etc.<p>- I really like Rust compiler because of really good error messages it produces but its really slow and you start to notice it as you add more code/external crates<p>-IDE support is good but not great. I am using IntelliJ with Rust plugin as we use IntelliJ for Scala/JVM and it is nowhere as good as even Scala plugin which is pretty mediocre in itself.<p>Overall I am pretty happy with the language (except for Error issue) and most of my gripes are around the ecosystem and tooling around the language. Hopefully these will be resolved as language gains more momentum",
          "What I would like to see is something where people could submit a small working program in one language and a Rust expert would make a program that does the same thing in idiomatic Rust.   This doesn't need to be anything too complex.  A wiki would be fine, the work would be in having volunteers to make the demos.<p>If you don't know the idioms you end up doing a sort of transliteration of the source language where the concepts may work against the nature of Rust.  A developer can spend a long time barking up the wrong tree with few indicators as to which is the right tree.<p>As an example of what I mean.   Here is a program that I wrote in JavaScript specifically because I could not find a way to write it in Rust that felt like I was doing it the right way.<p><a href=\"https://codepen.io/Lerc/pen/yQxmob?editors=0010\" rel=\"nofollow\">https://codepen.io/Lerc/pen/yQxmob?editors=0010</a><p>I could come up with Rust versions that were basically emulating pointers with array indexes(and a mess when something is deleted), or map versions that incurred lots of run-time lookup.   The crux of the problem, I think is either how to do a mutable graph of mutable nodes, or how to do map on a immutable graph of immutable nodes to a new immutable graph of immutable nodes in the next state along.<p>I'm sure others have this issue of  \"I know how to do it in X, doing it that way in Rust is a mess, what's the right way?\""
        ],
        "id": "1bcf4c3b-7d40-4b61-94cc-5c31ac8a0447",
        "url_text": "December 27, 2018, 7:10pm #1 Tame Complexity through Community Involvement Tools Edit: Id just like to say that I think 2018 has been a great year for rust. Rereading this, I realize may have come off as a little negative. End edit. Rust has a big problem. Our feedback loops have gotten big because we have gotten big (as a community). Weve started to council together at the end of the year. I dont think its bad, but part of it is reaching consensus on the current state of Rust. I think its good actually, but we need more things like it - Im not suggesting we call for even more blog posts, but that we start tailoring activities to address the big problem. The big problem is growth (and increased community size) - its a good problem to have, but it does cause issues. Rust has a lot of gaps caused by its explosive growth, more prominent ones have already been covered: A gap between what we have approved (language wise) and what has been implemented (the fallow year) A gap between input and processing capacity team/person wise (organizational debt) A gap between what we have and what we have documented (Stevek Klabniks 2019 post) These are all artifacts of growth. Its gotten harder to hold everything about Rust in your head. The tools the Rust community has been using (mostly github provided ones) were good at small sizes, but dont scale well. Theyre leaky abstractions. Pros and Cons analysis with consensus represented by a linear discussion, state scattered across the web, etc. Harder to follow, harder to moderate, harder to contribute to. Its harder to: track things you care about (I see this as a large contributing factor in the impl Trait return position & website controversies) see the overall status of rust see where things need help Proposal We should build something to fix that. We should build Rust specific infrastructure to manage the complexity that comes community sprawl (the sprawl is not bad). Organization specific infra is a thing (Im looking at you task-cluster that replaced jenkins) What will it look like? Im not sure, but I suspect each team will need something different - form following domain. I have a couple of ideas after reading some other Rust2019 posts of what said central, easy to find tooling should include: RFC Stuff We should probably get an RFC tracker. A real tracker, not github issues (Can you imagine if https://caniuse.com/ used Github issues/prs for its UI? ). Lets track by time and state, category and syntax impact (and more?!). There is so much metadata about RFCs that would be useful if it was understandable by computers. Lay people could get a better idea of whats going on, and contributors could see whats happening at a glance. In addition to the overview/tracking tool, we need a better tool for contributors of RFCs to use. Discussion is the big one. Can we build a tool for structured discussions - pros/cons, links to other RFCs, view changes overtime and hidden votes (because while things arent a popularity, its probably still a good idea to let people express themselves, even if only so the moderators / leaders can see). There are also proposals for limiting throughput, a more distinct staged process, needing a team champion, etc. We could integrate that too. Docs I heard the docs need love. So itd be great to have a list of things that need doing. Sections to proofread, sections that need feedback (e.g. Im trying to accomplish x, but its not working / coming off right), things to write (cookbook), brainstorming structure/approach ideas, etc. Itd also be nice to a have one big index to rule them all, about the various docs, whats next on the doc teams plate (were hoping to improve x, other big initiatives, etc), etc. Misc The embededded working group could probably use a page listing all the platforms, and what their status is, along with a list of drivers. It actually probably exists. Somewhere. There are probably other things for other teams that would be great to put into this one, mythical centralized location. An improved team index would be nice (e.g. direct links to the discord / other place of preferred chat, links to the team specific areas, etc.) If we were to open infrastructure support to contributors (I have a perfectly good machine laying around Id be willing to run a CI runner on 24/7), wed probably want a public dashboard of some sort listing current capacity and a way to volunteer your computer. Barring that, wed probably want a graph of average wait time on compiler PRs (which is measured in days!). There is surely a lot more than this to what wed want (I think) - Im not really familiar enough to say though. Heck half these things are from what Ive read in the 2018 posts, if not more. In Closing The rust community has been great. In addition to the great language, the people are great, and trying hard things - transparent consensus (well, taking in put into account)-based decision making. Its been harder to do that at scale (it either exacts a big toll or breaks down). I think we should fix that. This is a call to trade generic tools for hand-crafted tools. To trade in a little team personality (each team does stuff differently), for less friction (and more accessible to new people). Custom tooling is a big effort. But I think its the best way a growing community long term without giving up the great things that characterize Rusts governance and growth. kennytm December 27, 2018, 8:03pm #2 Those are nifty graphs. Ill take those I think that in terms of gathering support for expanding infrastructure, a better graph would be of the average wait time of things that are S-waiting-on-bors (and perhaps S-waiting-on-crater). That would really drive home the infrastructure woes. The other graphs are useful as well though because they highlight the people component (review/author/team/bikeshed). I wouldnt want to lose any of the current graphs. anp December 27, 2018, 9:08pm #4 As someone whos built a few pieces of custom tooling for Rust including rfcbot and a now-defunct dashboard with many of the metrics youre describing, IMO the bottleneck on these ideas is availability and prioritization of effort/time/labor/work/etc. This kind of work doesnt typically carry lots of prestige (one of the main motivators for contributions), often requires lots of information gathering if youre not building tooling for your own processes, and is often hard to prioritize against other direct impact work. To build things like those described, we either need to find people already in the community with the right desires/skills/experience/etc, attract people to the community who would like to contribute to projects like these, or fund their design and construction via someones full-time work. The first option seems to be hard because existing contributors are often furiously working away on the topics which originally captured their attention. I can see how some existing proposals to do less might free individuals up for some meta-work, most proposals Ive seen have other proposals for priorities that those contributors might adopt to pay down organizational debt within each of the teams. There have been a few attempts in the past to rally contributions around these kinds of custom infra applications (the bors2 conversations in the community team a little while ago springs to mind) and I at least am not aware of any of them taking off. A new effort in that direction would, I hope, address typical pitfalls for coordinating that kind of work. Aside: which team would lead the design of an RFC tracker? Core? Infra? A new one? Questions about the governance of tools like the ones proposed have already come up for rfcbot and figuring out some answer for their home seems like a good idea. The second option to get people working on these is an interesting one but still reduces to a pretty similar problem: who among the existing community will lead the design and implementation of the new tools? What tools will they have to succeed? In terms of full-time funding, Im not sure who would fund this work Mozilla? They seem a bit capped out on the Rust initiatives theyre funding (although I dont have any special info there). Some large tech companies have expressed interest in funding work on Rust, but AFAICT theyre mostly focused on specific things they need to help it succeed in their environments. Perhaps theres another angle to play there? EDIT: I worry that I enumerated a bunch of obstacles without expressing enough enthusiasm this is a great set of ideas and some combination of them should definitely happen! Weve talked about trying to make rfcbot into various forms of this for a little while (@nikomatsakis and I talked about a more general-purpose RFC tracker and workflow organizer at rustconf2016?) and have been mostly blocked on my time and a slow trickle of contributions. Arguably rfcbot could have been the wrong choice for where to build this (the code isnt exactly polished lol), arguably my lack of time has meant that mentoring new contributors hasnt happened when it could have, etc. Not saying that any of this is impossible but that the lack of these tools today seems to me more due to the underlying question of Who? than whether theyd be desirable. I think well end up with a couple of different groups of contributors in this: Disgruntled people who have a hard time following things, who want to see it better People who finish whatever theyve been enthusiastically doing (either completing or burning out/turning over the reigns) People who love stats & transparency maybe. Id think at the very least wed want a working group for this, probably with people from each team at least for coordination. Maybe a proper team? It seems like itd be the in the purview of both Core and Infra, and yet maybe a big enough undertaking to be separate. I didnt even think about funding, but thats a really good point - it will take a lot of bandwidth. I do think that the easiest thing to secure funding for would be for more infrastructure (if we have nice graphs and projections showing potential impact of contributions on turn-around time). Its funny that this is meant to be a way to help solve coordination issues, and the biggest threat to its success is probably coordination issues. I think the following will be pivotal to getting things off the ground and successful: getting a small core estabilished - sort of like how tower-web and warp were worked on for a bit before being released. Its important to get a core established to have something to fit the pieces of the full implementation into. And we dont want a bunch of bike-shedding to prevent the groundwork from happening. Having a coordinator. Once we get far enough, I imagine that there will be lots of little things that could be worked on in parallel. Maybe not though. Emphasis on a couple of highly visible things for the core - PR wait-time and RFC tracking perhaps? Something thats not too hard, and yet is would be liked by a lot of people. I think average turn-around time and RFC tracking brings a lot of simple to see transparency and benefit Official backing - namely a place to live (both domain and server wise), and guidance in terms of extracting / integrating with the current crop of rust stuff. Data sources and points of interaction. A large part of me thinks wed do well to make it modular so other things could reuse it as necessary. Sort of a code level modular Drupal / CMS thing? (disclaimer: Ive never touched PHP/Drupal). In that linked thread you can see that the RFCBot is being used outside of Rust. Anyway, I think these are good points that need to be brought up and figured out - thanks for sharing them. If I had to choose between hearing about obstacles and hearing only enthusiasm, Id choose to hear the obstacles any day . Hopefully putting an emphasis and discussion such tooling will help get the who question answered. Is forge linked to from the main site? Writing from mobile. llogiq January 1, 2019, 9:15am #7 Regarding RFCs, Id like to see a collaborative mind-mapping tool where people could upvote branches to make them appear stronger, and hide (or, for mod team members, even remove) them. This could be a great help to map out the design space. Regarding docs, I held the docs workshop at the RustFest 2019 and I think doing more of such workshops off- and online would be a good start to improve matters. I really like the idea of rust specific infrastructure, mostly because i was thinking about it too Id volunteer time for that and i actually view it as high impact work, at least in the long run. I also prefer starting with a very small scope and slowly build features out from there, feeling out what is really needed and accepted by the rust community while were going. But without fulltime work, this is a really big project. Itd be a big project for a fulltime team zoul January 1, 2019, 3:40pm #9 Exactly. Is it completely nave to think about pooling resources with other languages such as Swift? After all, the problems being solved are very similar, both in the domain and in the social dynamics. I just dont know if the required coordination wouldnt outweight the advantages. Soni January 1, 2019, 3:50pm #10 I think git is a suitable tool for tracking PR discussion. @Soni Sorry to be pedantic, but, do you mean: Using github PRs directly, one per RFC? Using git repos, one per RFC (suggested in earlier proposals), with multiple PRs to manage various stages? Using git repos, storing the discussion directly in them? I dont think Github PRs scale well. The linear discussion model breaks down easily when the PR gets lots of traffic, and its harder to track things programatically. You can adapt the track-ability with stuff like the RFC bot being invoked that maintains its own machine readable records (well, in a db thats searchable somewhere better than the GitHub issue search. Thats still a little brittle in my opinion because of the opportunity for operator error. I think the staged rfc proposal (already linked in my initial post) outlines some good ideas in this regard. The discussion nut is little harder to crack. I do like @llogiqs suggestion of mind-mapping. I think we still need a discussion area, but a nice mind-map would probably go a long ways to calming the discussion stream, especially if we can take comments after the fact and pin them to parts of the mind-map (e.g. for people who come in, dont read the full thing but want to make a point known - and I think that input is still valuable because sometimes its people who feel like they are at the margin, and want to make some lesser point known). There is another proposal for rate-limiting RFCs which is an approach I dont like as much for tackling this, but it makes some good points (Table of Contents) and would probably be workable, but sub-par in my opinion. Im not opposed to keeping Git as a storage mechanism, as it does have great history tools. The biggest draw back of course for custom infra would be the cost. And second would be learning a custom UI. Soni January 1, 2019, 5:05pm #12 None of the above. Instead, all of these: One git repo for ALL RFCs. One branch per RFC. Forks of branches to add comments. Comments can be merged back upstream and git has tools to browse a specific uh, thingy of commits. (e.g. merging commits A and B from different branches requires a merge commit C. ignoring the merge commit, git lets you pick one of those branches to view the commit sequence within it.) This means you just branch a branch to comment on an RFC, and at the end it all gets merged back into the RFCs history. More importantly, its possible to attach RFC changes to specific discussions, something we cant do today. (except through linking to a comment, but that has issues like being hard to follow.) Git does everything we want, including the ability to branch off discussions and create arbitrary new discussion bases on the same repo. Itd be kinda like letting internals (this forum) users create arbitrary categories and subcategories, I think. But it directly integrates with the RFC system and history. Aloso January 2, 2019, 7:29am #13 I do believe we need better tools to discuss RFCs and make decisions. Right now, RFCs often get so many comments that GitHub collapses several hundred of them. However, RFCs are discussed on other forums, too. This means that nobody has time to read all the discussions on all forums. Im proposing a tool to file RFCs, discuss them, and reach a consensus. This tool could also be used for everything else that requires a decision by the community. In this tool, an RFC consists of three parts: The wiki, the feelings section, and the discussion. The wiki describes the RFC in detail and is editable by anyone. In the feelings section, users can express their feelings (e.g. this is confusing or this adds unnecessary complexity or nobody needs this or this would help me a lot or I prefer the syntax with the question mark). Feelings consist of one sentence and can be voted on (totally agree / somewhat agree / somewhat disagree / totally disagree). The feelings are then sorted by how many users agree with them. They basically act as a survey that points out the most frequent concerns. The comments section should be a tree (reddit-like) to allow structuring long discussions. Id like to know what you think about this, and what else you think is important. Id volunteer to create this tool (but hopefully with some input and help). Soni January 2, 2019, 12:33pm #14 The comments section should be a DAG (git-like). @Soni, while the comments being attached to changes is cool, I dont see how it addresses the following issues (which I think are the most pressing issues): Comments being disorganized (the same point made repeatedly) Comments being widely-scattered (in multiple locations) It does raise the bar for adding comments, which maybe reduces both issues but I dont think curtailing feedback is a good way to manage community involvement. I do think it works great for when you have a small number of contributors for any given area of a repository, but the issue that were suffering from (but it is a good issue to have!) is the large number of participants in individual RFCs. I find myself firmly in the camp that a centralized solution is the best for for managing RFCs - perhaps you could explain how the RFC process would benefit from the decentralized DAG format youre suggesting? @Aloso, I think those are great ideas. Itd probably be cool to be able to branch discussions off of the feelings points. We probably want some way to let mods modify the discussion (with open history - the point of the modifications wouldnt be to censor, but to organize). My specific inspiration for this is that in Zulip (chat program), all messages in room need a topic, and if you forget, another person change change the topic for said message so it ends up in the right thread. Aloso January 2, 2019, 4:40pm #16 Of course we could do something similar. I like the idea of adding tags to comments. Then someone can view all comments with a specific tag. With a tree-like discussion, we could also encourage people to start one top-level discussion for each topic, so its structured automatically. True, true. I was mostly thinking for people who come and make a comment without reading all the threads. Of course if we do have threaded discussion, thats a lot less likely. I would like to link post with my draft ideas regarding service for improving RFC process: Of course it will be hard to build such service from ground-up, so I do not propose to start building it right now, but think about it as a general direction in which I think we probably should move. Soni January 2, 2019, 4:47pm #19 It literally attaches parents and children to comments, and you can have multiple branches going on at once. How does that not solve the disorganization problem? (each branch is a separate topic, sort of. you can even merge topics or address many of them at once!) As for comments being widely-scattered, GitHub can actually display these quite well, if with a few filtering issues. (click the number beside fork on a repo. the UI is mostly already there, so they have very little work to do on their end to make the experience better for us.) @Soni How are the parent-child comments stored - as empty commits and its the message? In a text file? In a different text format (json, yaml, maybe folders?) to handle nesting? Anything flat and youll probably have tons of merge conflicts. I consider the current GitHub UI for managing forks, as it relates to using it to explore and get a good overview of branched changes in this scenario, pretty atrocious, not matter how little work it would take for Github to cater to our needs (which I think would be a lot, and even if it was a little, we should base our workflow on a potential change another organization hasnt yet made). Your approach comes off (to me) this is a good idea, and so we should make it work. Id like to approach the issue from more of a what do we want, and whats the best way to implement it?. If we can make a way to make it fit into Git after the fact, after weve gathered all the requirements (quotes because its more like aspirations than requirements, and I think not everything will make the cut), then Im all for it. I do worry that by committing (heh, I love puns) to Git early that well leave features off the table that could be useful. I think that the very least, in order to use Git wed have to roll our own UI as a frontend for it, so the format stays standardized. ",
        "_version_": 1718536432266510336
      },
      {
        "story_id": 19654603,
        "story_author": "runningmike",
        "story_descendants": 6,
        "story_score": 7,
        "story_time": "2019-04-13T17:58:44Z",
        "story_title": "The holy grail for solving IT problems?",
        "search": [
          "The holy grail for solving IT problems?",
          "https://nocomplexity.com/nocode-solutions/",
          "Within the field of business IT problems it is hard to resit the temptation to use a tool that promise to solve all your complex IT problems. Despite the progress and technology innovation in the last 25 years some IT related problems are still very hard to solve. To name of few typical business IT problems: Too high development and operational cost.Inflexible IT solutions when business needs change.Vendor lock-ins.Security, privacy and safety risks.Availability issues.Disasters for business operations due to the fact that minimum quality standards are not met. E.g. quality aspects like data integrity, configuration management or restoring systems still cause real disasters for business continuity. The track record for successful large IT projects is still very miserable, also in 2019. So every sensible company is still fearful for starting a new big IT project. Since no one is immune for marketing you should be suspicious solving your real complex problems using a new holy grail IT solution. Good marketing has an emphasis that there is something real new on the table. A current trend is using low-code or no-code solutions to solve all complex IT problems that will occur during and after large IT projects are finished. Since developing IT solutions involves high cost and takes often more time than estimate the new holy grail solutions claim to solve this productivity and cost problem. But this is not all: the new low-code solutions also claim to solve all typical IT life cycle management problems. I love new solutions that solve real problems. New IT solutions can bring real advantages for business at large. But is the holy grail promised by the new low-code tools really true and proven? The new holy grail solutions can be categorized as the new era of MDA solution tools. Model driven architecture (MDA) is a software design approach for the development of software systems. Model driven architecture provides models written in well-defined design language. The models consist of multiple components including model, transformation and meta-models. Tools that use a MDA approach claim to generate working software out of a model. So no more coding, just model your problem. So the promise is not to create software manual anymore, but the solution will generate software based on the model directly. So bye bye high skilled traditional software engineers? MDA is not new. From 1980 several names have been given to the MDA idea of generating software based on a model, e.g.: Low code toolingNo code toolingModel Driven Engineering (MDE)Model Driven Design (MDD)4GL tools The promises made by MDA software solutions, no-code solutions or low-code software vendors are high. So you should be aware of key aspects when evaluating if a MDA software solution solves your problem. Key factors to evaluate MDA like tools are: MDA is a concept with strong underlying believes of a problem situation it tries to solve. This means that every MDA solution tool is biased and does not work for ever problem situation. So always determine first your exact problem situation and the problems or complexity problems you want to solve before evaluating solutions. An software tool alone will never solve your organizational problems that make software development costly in your organization. The productivity gain promised by MDA tools is not always met. When trying to solve complex problems with a MDA tool-based approach, the productivity gain is at best doubtful. Most productivity problems are not related to the software design, delivery tools or process. The hard and complex problems that impact performance in large business IT projects are related with having no general agreement and perception of the goals, requirements and principles for the IT system that must be created to solve some fuzzy problem definition.MDA is has a strong focus on initial productivity gain. But a continuous fast changing business context with changing requirements requires an approach and toolset that is suited for giving long term productivity benefits.Personal that is able to analyse, model and create and update a model in the required model language for the chosen low-code tool is not widely available. Most business problems are not modelled and solved using the BPMN standard (Business Process Model and Notation). Software engineers are good at creating software and some are also great in developing software architectures. Creating BPM models or other type of models to address business problems are no core competence of software engineers. And remember: Adjusting parameters using check-boxes or drawing lines on a canvas to outline a process is just engineering. Most real business people will never use or have the time or knowledge for making changing within business administration interfaces. Configuration using no-code tools is most of the time still programming, but with painful limitations.MDA tools and products developed have difficulties to keep up with the continuously changing IT technology world. Browser technology and mobile technology are constantly changing. However this is not always an evolutionary path.General DSLs (Domain Specific Language) used within MDA tools rarely bring the productivity gain. DSLs like BPMN, UML, SYSML are known to be complex to model a problem fast and correct for software generation.Custom defined DSLs used by several MDA tools are not open so you are locked in by the tool and the vendor.Full model testing, so business process testing using the generated software is often lacking. In order to prevent surprises automated correctness testing of the model is a severe challenge for MDA tools.Nonfunctional requirements like performance, security and stability are hard to incorporate within a model or generated code.Discipline in relation with your business culture is crucial for long term success with MDA tools. Simple changes on data or model parameters require the tool process to be followed exactly. Every step in the way the tool vendor has implemented it. So you need to follow the rules of the tools to implement changes correctly.MDA tools are not strong on versioning and dealing with multiple parallel development tracks and teams simultaneously. Most tools are in fact based on a big design upfront paradigm, like an overall data model. Versioning on models, meta data and data of all created and generated software assets is poorly supported, if supported at all. Common practices seen in mature agile software development tools using micro services paradigms and advanced distributed version control systems are often lacking in the new low-code MDA family of tools. In large companies it is not uncommon to encounter models with hundreds (or even thousands) of entities / classes. That kind of models not only doesnt help with developing software faster and more cost-efficiently but even has an adverse effect. Simple solutions for complex business IT problems rarely exist. Within 2019 creating complex software always means that minimum quality levels for e.g. security and privacy are mandatory. This is not simple at all. Creating a simple solution means you have to do a lot of difficult and inconvenient work. E.g. solid and proven business and IT architectures and designs are needed. For some problem areas you should create software to generate software to solve the problem needed. E.g. creating software to test a new part of an airplane control system. There is definitely a category of business problems that is well suited for using new latest MDA category of no-code solutions. Innovation within the field of business IT and the way business IT software is created is always needed. The latest family of MDA no-code and low-code tools do trigger innovation processes. E.g. a new tool will force you to look at your problem context from a different perspective. But never fall in love with an IT solution if you do not fully understand the problem and the problem context you want to solve. The productivity loss of solving business IT problems is seldom directly related to your current software engineering tools or process. Software is binary. Only a problem that is well stated and constant can be solved efficient using traditional software development tools. Else use or try new machine learning techniques. Business problems where organizational and human factors have large impact on the problem situation are by nature harder to solve. The new family of MDA tools will not change this. "
        ],
        "story_type": "Normal",
        "url_raw": "https://nocomplexity.com/nocode-solutions/",
        "comments.comment_id": [19655539, 19657548],
        "comments.comment_author": ["mpoteat", "ekovarski"],
        "comments.comment_descendants": [3, 0],
        "comments.comment_time": [
          "2019-04-13T20:30:39Z",
          "2019-04-14T04:02:00Z"
        ],
        "comments.comment_text": [
          "Am I having a stroke? This article has a lot of words but doesn't seem to have any actual meaning. A parody about business buzzwords?",
          "The holy grail is no complexity.<p>It's definitely light on details and the Object Management Group was and is def not the holy grail for solving IT problems."
        ],
        "id": "6894cf4f-5f81-4406-bc04-5555fd5e278e",
        "url_text": "Within the field of business IT problems it is hard to resit the temptation to use a tool that promise to solve all your complex IT problems. Despite the progress and technology innovation in the last 25 years some IT related problems are still very hard to solve. To name of few typical business IT problems: Too high development and operational cost.Inflexible IT solutions when business needs change.Vendor lock-ins.Security, privacy and safety risks.Availability issues.Disasters for business operations due to the fact that minimum quality standards are not met. E.g. quality aspects like data integrity, configuration management or restoring systems still cause real disasters for business continuity. The track record for successful large IT projects is still very miserable, also in 2019. So every sensible company is still fearful for starting a new big IT project. Since no one is immune for marketing you should be suspicious solving your real complex problems using a new holy grail IT solution. Good marketing has an emphasis that there is something real new on the table. A current trend is using low-code or no-code solutions to solve all complex IT problems that will occur during and after large IT projects are finished. Since developing IT solutions involves high cost and takes often more time than estimate the new holy grail solutions claim to solve this productivity and cost problem. But this is not all: the new low-code solutions also claim to solve all typical IT life cycle management problems. I love new solutions that solve real problems. New IT solutions can bring real advantages for business at large. But is the holy grail promised by the new low-code tools really true and proven? The new holy grail solutions can be categorized as the new era of MDA solution tools. Model driven architecture (MDA) is a software design approach for the development of software systems. Model driven architecture provides models written in well-defined design language. The models consist of multiple components including model, transformation and meta-models. Tools that use a MDA approach claim to generate working software out of a model. So no more coding, just model your problem. So the promise is not to create software manual anymore, but the solution will generate software based on the model directly. So bye bye high skilled traditional software engineers? MDA is not new. From 1980 several names have been given to the MDA idea of generating software based on a model, e.g.: Low code toolingNo code toolingModel Driven Engineering (MDE)Model Driven Design (MDD)4GL tools The promises made by MDA software solutions, no-code solutions or low-code software vendors are high. So you should be aware of key aspects when evaluating if a MDA software solution solves your problem. Key factors to evaluate MDA like tools are: MDA is a concept with strong underlying believes of a problem situation it tries to solve. This means that every MDA solution tool is biased and does not work for ever problem situation. So always determine first your exact problem situation and the problems or complexity problems you want to solve before evaluating solutions. An software tool alone will never solve your organizational problems that make software development costly in your organization. The productivity gain promised by MDA tools is not always met. When trying to solve complex problems with a MDA tool-based approach, the productivity gain is at best doubtful. Most productivity problems are not related to the software design, delivery tools or process. The hard and complex problems that impact performance in large business IT projects are related with having no general agreement and perception of the goals, requirements and principles for the IT system that must be created to solve some fuzzy problem definition.MDA is has a strong focus on initial productivity gain. But a continuous fast changing business context with changing requirements requires an approach and toolset that is suited for giving long term productivity benefits.Personal that is able to analyse, model and create and update a model in the required model language for the chosen low-code tool is not widely available. Most business problems are not modelled and solved using the BPMN standard (Business Process Model and Notation). Software engineers are good at creating software and some are also great in developing software architectures. Creating BPM models or other type of models to address business problems are no core competence of software engineers. And remember: Adjusting parameters using check-boxes or drawing lines on a canvas to outline a process is just engineering. Most real business people will never use or have the time or knowledge for making changing within business administration interfaces. Configuration using no-code tools is most of the time still programming, but with painful limitations.MDA tools and products developed have difficulties to keep up with the continuously changing IT technology world. Browser technology and mobile technology are constantly changing. However this is not always an evolutionary path.General DSLs (Domain Specific Language) used within MDA tools rarely bring the productivity gain. DSLs like BPMN, UML, SYSML are known to be complex to model a problem fast and correct for software generation.Custom defined DSLs used by several MDA tools are not open so you are locked in by the tool and the vendor.Full model testing, so business process testing using the generated software is often lacking. In order to prevent surprises automated correctness testing of the model is a severe challenge for MDA tools.Nonfunctional requirements like performance, security and stability are hard to incorporate within a model or generated code.Discipline in relation with your business culture is crucial for long term success with MDA tools. Simple changes on data or model parameters require the tool process to be followed exactly. Every step in the way the tool vendor has implemented it. So you need to follow the rules of the tools to implement changes correctly.MDA tools are not strong on versioning and dealing with multiple parallel development tracks and teams simultaneously. Most tools are in fact based on a big design upfront paradigm, like an overall data model. Versioning on models, meta data and data of all created and generated software assets is poorly supported, if supported at all. Common practices seen in mature agile software development tools using micro services paradigms and advanced distributed version control systems are often lacking in the new low-code MDA family of tools. In large companies it is not uncommon to encounter models with hundreds (or even thousands) of entities / classes. That kind of models not only doesnt help with developing software faster and more cost-efficiently but even has an adverse effect. Simple solutions for complex business IT problems rarely exist. Within 2019 creating complex software always means that minimum quality levels for e.g. security and privacy are mandatory. This is not simple at all. Creating a simple solution means you have to do a lot of difficult and inconvenient work. E.g. solid and proven business and IT architectures and designs are needed. For some problem areas you should create software to generate software to solve the problem needed. E.g. creating software to test a new part of an airplane control system. There is definitely a category of business problems that is well suited for using new latest MDA category of no-code solutions. Innovation within the field of business IT and the way business IT software is created is always needed. The latest family of MDA no-code and low-code tools do trigger innovation processes. E.g. a new tool will force you to look at your problem context from a different perspective. But never fall in love with an IT solution if you do not fully understand the problem and the problem context you want to solve. The productivity loss of solving business IT problems is seldom directly related to your current software engineering tools or process. Software is binary. Only a problem that is well stated and constant can be solved efficient using traditional software development tools. Else use or try new machine learning techniques. Business problems where organizational and human factors have large impact on the problem situation are by nature harder to solve. The new family of MDA tools will not change this. ",
        "_version_": 1718536471131979777
      },
      {
        "story_id": 18852810,
        "story_author": "whoisnnamdi",
        "story_descendants": 47,
        "story_score": 98,
        "story_time": "2019-01-08T03:15:07Z",
        "story_title": "GitLab’s CEO reflects on GitHub’s move to offer free private repos",
        "search": [
          "GitLab’s CEO reflects on GitHub’s move to offer free private repos",
          "https://about.gitlab.com/2019/01/07/github-offering-free-private-repos-for-up-to-three-collaborators/",
          "GitHub just launched free private repos with up to three collaborators. I like to think that increased competition from us (GitLab) contributed to this change as one Hacker News commenter stated, \"Thank you GitLab for providing market competition forcing GitHub to consider this!\" Some history When we originally announced GitLab.com I made the main point that it would have private repos for free. I think it is great for beginner users that private repos on GitHub are now free. If you're starting to program and aren't ready to share your code with the world yet, you don't have to have a paid account to keep it private. At the time, I was very disappointed to learn that Bitbucket.org already offered the same. GitLab took off despite that and GitLab.com recently surpassed 10 million projects and in the top dev tools ranking of Axosoft GitLab climbed the ranks 4 spots and overtook GitHub for the first year. Looking ahead At GitLab we think that repositories will become a commodity. I think Microsoft will try to generate more revenue with people using Azure more instead of paying for repos. We're focusing on making a single application for the entire DevOps lifecycle that can replace a lot of other tools. Or, as Stavros Korokithakis phrased it: \"My move to GitLab was basically 'Come for the free repos, stay for the rest of the amazing features.' I will not be moving off it, and my new repos will keep being on GitLab.\" We think the long-term trend is multi-cloud and we'll keep shipping with our 2,200 other contributors to make this a reality. In the meantime, here are some more details of our current GitLab free private repository offering vs GitHub's: Free functionality GitLab GitHub Private repositories Yes Yes Number of collaborators Unlimited 3 Wiki Yes No (public or paid only) Pages Yes No (public or paid only) Capacity 10GB 1GB Indicates who is paying No Yes Free CI 2,000 min. Maybe a free tier for Actions on Azure Entire DevOps lifecycle Yes No Location of the repo Anywhere Not in groups/orgs API concurrent rate limit 36000 5000 Sign up for a free trial .@github now offers free private repos for up to three collaborators here are our thoughts Sid Sijbrandij Click to tweet Sign up for GitLabs twice-monthly newsletter "
        ],
        "story_type": "Normal",
        "url_raw": "https://about.gitlab.com/2019/01/07/github-offering-free-private-repos-for-up-to-three-collaborators/",
        "url_text": "GitHub just launched free private repos with up to three collaborators. I like to think that increased competition from us (GitLab) contributed to this change as one Hacker News commenter stated, \"Thank you GitLab for providing market competition forcing GitHub to consider this!\" Some history When we originally announced GitLab.com I made the main point that it would have private repos for free. I think it is great for beginner users that private repos on GitHub are now free. If you're starting to program and aren't ready to share your code with the world yet, you don't have to have a paid account to keep it private. At the time, I was very disappointed to learn that Bitbucket.org already offered the same. GitLab took off despite that and GitLab.com recently surpassed 10 million projects and in the top dev tools ranking of Axosoft GitLab climbed the ranks 4 spots and overtook GitHub for the first year. Looking ahead At GitLab we think that repositories will become a commodity. I think Microsoft will try to generate more revenue with people using Azure more instead of paying for repos. We're focusing on making a single application for the entire DevOps lifecycle that can replace a lot of other tools. Or, as Stavros Korokithakis phrased it: \"My move to GitLab was basically 'Come for the free repos, stay for the rest of the amazing features.' I will not be moving off it, and my new repos will keep being on GitLab.\" We think the long-term trend is multi-cloud and we'll keep shipping with our 2,200 other contributors to make this a reality. In the meantime, here are some more details of our current GitLab free private repository offering vs GitHub's: Free functionality GitLab GitHub Private repositories Yes Yes Number of collaborators Unlimited 3 Wiki Yes No (public or paid only) Pages Yes No (public or paid only) Capacity 10GB 1GB Indicates who is paying No Yes Free CI 2,000 min. Maybe a free tier for Actions on Azure Entire DevOps lifecycle Yes No Location of the repo Anywhere Not in groups/orgs API concurrent rate limit 36000 5000 Sign up for a free trial .@github now offers free private repos for up to three collaborators here are our thoughts Sid Sijbrandij Click to tweet Sign up for GitLabs twice-monthly newsletter ",
        "comments.comment_id": [18853309, 18854257],
        "comments.comment_author": ["ccostes", "gandutraveler"],
        "comments.comment_descendants": [4, 5],
        "comments.comment_time": [
          "2019-01-08T05:03:28Z",
          "2019-01-08T08:58:31Z"
        ],
        "comments.comment_text": [
          "> At the time, I was very disappointed to learn that BitBucket.org already offered the same. GitLab took off despite that and GitLab.com recently surpassed 10 million projects.<p>Is it just me, or does this sentence not make sense? I don't know why this is disappointing to him, or why he would just be learning this now.",
          "I'm starting to believe that Gitlab actually has marketing team dedicated to HN. Everytime there is a Gitlab HN post you would find random unrelated comments on why they prefer Gitlab over Github."
        ],
        "id": "15f38d12-e894-46e5-96c5-3d08f4ee2cc7",
        "_version_": 1718536435246563328
      },
      {
        "story_id": 19669153,
        "story_author": "themlaiguy",
        "story_descendants": 5,
        "story_score": 7,
        "story_time": "2019-04-15T21:37:08Z",
        "story_title": "Why is version control in Jupyter notebooks so hard?",
        "search": [
          "Why is version control in Jupyter notebooks so hard?",
          "Are there any tools that help with version control on notebooks?"
        ],
        "story_text": "Are there any tools that help with version control on notebooks?",
        "story_type": "AskHN",
        "comments.comment_id": [19670482, 19674241],
        "comments.comment_author": ["snilzzor", "amirathi"],
        "comments.comment_descendants": [0, 0],
        "comments.comment_time": [
          "2019-04-16T01:47:18Z",
          "2019-04-16T15:04:38Z"
        ],
        "comments.comment_text": [
          "I've been clearing my output using nbconvert before putting the notebook into version control. I have a precommit hook and a check in CI. This works for my use case but I can understand needing to preserve output.<p>jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace my_notebook_name.ipynb",
          "You bet. I built ReviewNB[1] specifically for Jupyter Notebook code reviews.<p>There's also,<p>- nbstripout[2] for stripping outputs automatically before every commit<p>- nbdime[3] for diff'ing notebooks locally<p>- jupytext[4] for converting notebooks to markdown and vice-a-versa<p>[1] <a href=\"https://www.reviewnb.com/\" rel=\"nofollow\">https://www.reviewnb.com/</a><p>[2] <a href=\"https://github.com/kynan/nbstripout\" rel=\"nofollow\">https://github.com/kynan/nbstripout</a><p>[3] <a href=\"https://github.com/jupyter/nbdime\" rel=\"nofollow\">https://github.com/jupyter/nbdime</a><p>[4] <a href=\"https://github.com/mwouts/jupytext\" rel=\"nofollow\">https://github.com/mwouts/jupytext</a>"
        ],
        "id": "4390753e-f62f-46cb-8dc7-6be5dcf59f4e",
        "_version_": 1718536472010686464
      },
      {
        "story_id": 20140826,
        "story_author": "TopHatCroat",
        "story_descendants": 43,
        "story_score": 116,
        "story_time": "2019-06-09T18:20:01Z",
        "story_title": "Versioning Your Home Directory",
        "search": [
          "Versioning Your Home Directory",
          "https://martinovic.blog/post/home_git/",
          "Git is an incredibly useful tool for a programmer, you use it to version the work you do and distribute it easily on other computers. But over the years, Ive found that a lot of my workflow depends on various configurations and helper scripts I have in my path. So why not version those as well, it does allow you to get started in seconds in a very familiar environment on every new computer or even a server if you spend a lot of your time in SSH sessions. Bonus points for making it a public repository so others can take a look at your stuff and possibly get inspiration for improvements in their workflow, just make sure you dont commit any private keys. The setup Feel free to check out my home repository over here. As you can see, it contains my .zshrc, .vimrc a bin directory with various scripts, dotfiles and some other stuff Ive curated while doing what I do best, fidgeting around a computer. You can start your own version by simply initializing a Git repository in your home directory. Now your directory will be full of stuff you probably dont want to commit, like your downloads, pictures, private keys, shell history or what have you. So its important to set up the ~/.gitignore file immediately to stop you from accidentally committing stuff you didnt want to. Most importantly, you should ignore everything by default in your ~/.gitignore: # Blacklist all in this folder /* # Except for these !.gitignore !.gitconfig !.notes/ !bin/ !Development/Sh This will instruct Git to keep its hands off all files except for the .gitignore, .gitconfig and the files in the .notes, bin and Development/Sh directories. So now, when you want to add any other file or directory you will require a force flag like so And thats basically all you need to do to set this up, its now up to you to add and commit all you want and push it to the Git hosting service of your choice. The cloning So now its time to clone this into your new home directory, but if you try cloning it using git clone command it will complain that it cant do that in an non empty directory. To get around this, you need to initialize it first then manually add the remote and finally force a checkout like so: cd ~ # Create an empty git repository in your home folder git init # Add the remote git remote add origin https://github.com/TopHatCroat/home # Get the stuff from up there git fetch # Be careful! This will overwrite any local files existing on remote! git reset --hard origin/master Running the last command will basically checkout all the files from the origin master branch and overwrite any files with the same path, so be careful if you already did some setup on the new machine. And thats it, youve saved yourself the bother of copying over the config files. The only thing that remains is to install the tools that use the configs youve just brought over. If you are feeling adventurous you could make a script that will do that for you like I did in here, but that is up to you. EDIT: As a response to this blog post others have pointed out another viable solution, or by using GNU stow, or homesick or rcm. Still, the solution presented here is the simplest to setup and understand and it does not depend on any other tool besides Git. All in all, there are a bunch of ways to do this and a lot more resources are available here. "
        ],
        "story_type": "Normal",
        "url_raw": "https://martinovic.blog/post/home_git/",
        "url_text": "Git is an incredibly useful tool for a programmer, you use it to version the work you do and distribute it easily on other computers. But over the years, Ive found that a lot of my workflow depends on various configurations and helper scripts I have in my path. So why not version those as well, it does allow you to get started in seconds in a very familiar environment on every new computer or even a server if you spend a lot of your time in SSH sessions. Bonus points for making it a public repository so others can take a look at your stuff and possibly get inspiration for improvements in their workflow, just make sure you dont commit any private keys. The setup Feel free to check out my home repository over here. As you can see, it contains my .zshrc, .vimrc a bin directory with various scripts, dotfiles and some other stuff Ive curated while doing what I do best, fidgeting around a computer. You can start your own version by simply initializing a Git repository in your home directory. Now your directory will be full of stuff you probably dont want to commit, like your downloads, pictures, private keys, shell history or what have you. So its important to set up the ~/.gitignore file immediately to stop you from accidentally committing stuff you didnt want to. Most importantly, you should ignore everything by default in your ~/.gitignore: # Blacklist all in this folder /* # Except for these !.gitignore !.gitconfig !.notes/ !bin/ !Development/Sh This will instruct Git to keep its hands off all files except for the .gitignore, .gitconfig and the files in the .notes, bin and Development/Sh directories. So now, when you want to add any other file or directory you will require a force flag like so And thats basically all you need to do to set this up, its now up to you to add and commit all you want and push it to the Git hosting service of your choice. The cloning So now its time to clone this into your new home directory, but if you try cloning it using git clone command it will complain that it cant do that in an non empty directory. To get around this, you need to initialize it first then manually add the remote and finally force a checkout like so: cd ~ # Create an empty git repository in your home folder git init # Add the remote git remote add origin https://github.com/TopHatCroat/home # Get the stuff from up there git fetch # Be careful! This will overwrite any local files existing on remote! git reset --hard origin/master Running the last command will basically checkout all the files from the origin master branch and overwrite any files with the same path, so be careful if you already did some setup on the new machine. And thats it, youve saved yourself the bother of copying over the config files. The only thing that remains is to install the tools that use the configs youve just brought over. If you are feeling adventurous you could make a script that will do that for you like I did in here, but that is up to you. EDIT: As a response to this blog post others have pointed out another viable solution, or by using GNU stow, or homesick or rcm. Still, the solution presented here is the simplest to setup and understand and it does not depend on any other tool besides Git. All in all, there are a bunch of ways to do this and a lot more resources are available here. ",
        "comments.comment_id": [20142395, 20142745],
        "comments.comment_author": ["drexlspivey", "neilv"],
        "comments.comment_descendants": [3, 0],
        "comments.comment_time": [
          "2019-06-09T22:46:02Z",
          "2019-06-10T00:06:37Z"
        ],
        "comments.comment_text": [
          "For dotfiles I use the workflow from this comment <a href=\"https://news.ycombinator.com/item?id=11071754\" rel=\"nofollow\">https://news.ycombinator.com/item?id=11071754</a> with a keybase encrypted repo<p><quote>\nI use:<p><pre><code>    git init --bare $HOME/.myconf\n    alias config='/usr/bin/git --git-dir=$HOME/.myconf/ --work-tree=$HOME'\n    config config status.showUntrackedFiles no\n</code></pre>\nwhere my ~/.myconf directory is a git bare repository. Then any file within the home folder can be versioned with normal commands like:<p><pre><code>    config status\n    config add .vimrc\n    config commit -m \"Add vimrc\"\n    config add .config/redshift.conf\n    config commit -m \"Add redshift config\"\n    config push\n</code></pre>\nAnd so one…<p>No extra tooling, no symlinks, files are tracked on a version control system, you can use different branches for different computers, you can replicate you configuration easily on new installation. \n</quote>",
          "I used to do this (using CVS, at the time), for keeping the configs and projects on my desktop and laptop in sync, as well as for having backups.  Quick small notes:<p>* I split everything into topical modules, which were immediate subdirectories of the home directory, partly so the little laptop didn't have to install everything.<p>* There was a \"config\" directory that included most of the typical home directory dotfiles, as well as things like fonts.  It had a small shell script in it, for keeping symlinks from the home dir up to date (e.g., \"~/.bashrc\" was a symlink to \"config/bashrc\".<p>* Not everything was in CVS.  Photos, for example, were backed up separately.<p>* It was good to have my server Internet-accessible, since a few times I'd walk for an hour to work from my laptop, and realize I didn't have the latest version of my code.<p>* Nowadays I would probably use Git, only for the sake of standardizing, but the simple model of CVS was nice for this purpose.  (I also knew many other fancier SCM systems, but intentionally went with a simple one.)  Also, the repository was just in RCS format (files with reverse deltas of diffs), so, on rare occasions that I made an oops (like accidentally committed a huge file I didn't intend to), I could just SSH in and fix it manually.<p>* I had various servers for this over time, starting with my desktop, and then a PC in the closet, then a colo server, and then (my favorite) a small home RAID server I made from a fanless Atom board and Debian (which was effectively silent, running years 24/7 before I finally took it out of service, still working; one drive had to be replaced during that time, but was RAID mirrored)."
        ],
        "id": "a27aa05a-dc53-4293-96f2-078a8bd235ba",
        "_version_": 1718536490370203648
      },
      {
        "story_id": 20646674,
        "story_author": "tosh",
        "story_descendants": 68,
        "story_score": 107,
        "story_time": "2019-08-08T17:40:55Z",
        "story_title": "The market figured out Gitlab’s secret",
        "search": [
          "The market figured out Gitlab’s secret",
          "https://about.gitlab.com/2019/08/08/built-in-ci-cd-version-control-secret/",
          "Theres a movement in the DevOps industry and the world right now: to do more in a simple way that inspires us to innovate. GitLab started this trend in the DevOps space by simplifying the delivery of code by combining GitLab CI and GitLab version control. We didn't originally buy into the idea that this was the right way to do things, but it became our secret capability that weve doubled down on. Lets combine applications The story starts with Kamil Trzciski, now a distinguished engineer at GitLab. Soon after Kamil came to work for GitLab full time, he began talking with me and my co-founder, Dmitriy Zaporozhets, suggesting that we bring our two projects together GitLab Version Control and GitLab CI, making it into one application. Dmitriy didnt think it was a good idea. GitLab version control and CI were already perfectly integrated with single sign-on and APIs that fit like a glove. He thought that combining them would make GitLab a monolith of an application, that it would be disastrous for our code quality, and an unfortunate user experience. After time though, Dmitriy started to think it was the right idea as it would deliver a seamless experience for developers to deliver code quickly. After Dmitriy was convinced, they came to me. I also didnt think it was a good idea. At the time I believed we needed to have tools that are composable and that could integrate with other tools, in line with the Unix philosophy. Kamil convinced me to think about the efficiencies of having a single application. Well, if you dont believe that its better for a user, at least believe its more efficient for us, because we only have to release one application instead of two. Efficiency is in our values. - Kamil Trzcinski, distinguished engineer at GitLab Realizing the future of DevOps is a single application That made sense to me and I no longer stood in their way. The two projects merged and the results were beyond my expectations. The efficiencies that were so appealing to us, also made it appealing to our customers. We realized we stumbled on a big secret because nobody believed that the two combined together would be a better way of continuously delivering code to market. We doubled down on this philosophy and we started doing continuous delivery. From that day on, I saw the value of having a single application. For example, a new feature we are implementing is auto-remediation. When a vulnerability comes out, say a heart bleed, GitLab will automatically detect where in your codebase that vulnerability exists, update the dependency, and deliver it to your production environment. This level of automation would be hard to implement without being in a single application. By combining the projects we unified teams helping them realize the original intent of DevOps and that is magical to see. The market validates our secret And while we bet on this philosophy the industry is now seeing it as well. In September of 2015 we combined GitLab CI and GitLab version control to create a single application. By March of 2017, Bitbucket also realized the advantages of this architecture and released Pipelines as a built-in part of Bitbucket. In 2018, GitHub announced Actions with CI-like functionality built into a single application offering. In the last six months, JFrog acquired Shippable and Idera acquired Travis CI, showing a consolidation of the DevOps market and a focus on CI. The market is validating what we continually hear from our users and customers: that a simple, single DevOps application meets their needs better. We hope you will continue to join us in our effort to bring teams together to innovate. Everyone can contribute here at GitLab and as always, we value your feedback, thoughts, and contributions. Want to hear me talk through the origin story? Listen to the Software Engineering Daily podcast where I talk about combining GitLab CI and GitLab Version Control. The industry has caught onto @GitLabs secret. Learn more about why GitLab combined GitLab CI and GitLab version control Sid Sijbrandij Click to tweet Sign up for GitLabs twice-monthly newsletter "
        ],
        "story_type": "Normal",
        "url_raw": "https://about.gitlab.com/2019/08/08/built-in-ci-cd-version-control-secret/",
        "comments.comment_id": [20647790, 20648739],
        "comments.comment_author": ["whalesalad", "asadkn"],
        "comments.comment_descendants": [5, 1],
        "comments.comment_time": [
          "2019-08-08T19:32:16Z",
          "2019-08-08T21:04:04Z"
        ],
        "comments.comment_text": [
          "Every time github releases a feature this is the response. These posts are so pathetic. I’m surprised that they continue to play this angle. It’s a very polarizing way to address the community that will definitely continue to stir up us vs them mentality between GitHub and Gitlab users.",
          "For all the bashing GitLab gets, personally, I want GitLab to survive and keep competing at some level with GitHub.<p>Should GitHub dominate the market and gobble up competition, we all know how it goes for its parent company."
        ],
        "id": "d99228d2-cded-4c57-8c9b-f9ebae68cf1f",
        "url_text": "Theres a movement in the DevOps industry and the world right now: to do more in a simple way that inspires us to innovate. GitLab started this trend in the DevOps space by simplifying the delivery of code by combining GitLab CI and GitLab version control. We didn't originally buy into the idea that this was the right way to do things, but it became our secret capability that weve doubled down on. Lets combine applications The story starts with Kamil Trzciski, now a distinguished engineer at GitLab. Soon after Kamil came to work for GitLab full time, he began talking with me and my co-founder, Dmitriy Zaporozhets, suggesting that we bring our two projects together GitLab Version Control and GitLab CI, making it into one application. Dmitriy didnt think it was a good idea. GitLab version control and CI were already perfectly integrated with single sign-on and APIs that fit like a glove. He thought that combining them would make GitLab a monolith of an application, that it would be disastrous for our code quality, and an unfortunate user experience. After time though, Dmitriy started to think it was the right idea as it would deliver a seamless experience for developers to deliver code quickly. After Dmitriy was convinced, they came to me. I also didnt think it was a good idea. At the time I believed we needed to have tools that are composable and that could integrate with other tools, in line with the Unix philosophy. Kamil convinced me to think about the efficiencies of having a single application. Well, if you dont believe that its better for a user, at least believe its more efficient for us, because we only have to release one application instead of two. Efficiency is in our values. - Kamil Trzcinski, distinguished engineer at GitLab Realizing the future of DevOps is a single application That made sense to me and I no longer stood in their way. The two projects merged and the results were beyond my expectations. The efficiencies that were so appealing to us, also made it appealing to our customers. We realized we stumbled on a big secret because nobody believed that the two combined together would be a better way of continuously delivering code to market. We doubled down on this philosophy and we started doing continuous delivery. From that day on, I saw the value of having a single application. For example, a new feature we are implementing is auto-remediation. When a vulnerability comes out, say a heart bleed, GitLab will automatically detect where in your codebase that vulnerability exists, update the dependency, and deliver it to your production environment. This level of automation would be hard to implement without being in a single application. By combining the projects we unified teams helping them realize the original intent of DevOps and that is magical to see. The market validates our secret And while we bet on this philosophy the industry is now seeing it as well. In September of 2015 we combined GitLab CI and GitLab version control to create a single application. By March of 2017, Bitbucket also realized the advantages of this architecture and released Pipelines as a built-in part of Bitbucket. In 2018, GitHub announced Actions with CI-like functionality built into a single application offering. In the last six months, JFrog acquired Shippable and Idera acquired Travis CI, showing a consolidation of the DevOps market and a focus on CI. The market is validating what we continually hear from our users and customers: that a simple, single DevOps application meets their needs better. We hope you will continue to join us in our effort to bring teams together to innovate. Everyone can contribute here at GitLab and as always, we value your feedback, thoughts, and contributions. Want to hear me talk through the origin story? Listen to the Software Engineering Daily podcast where I talk about combining GitLab CI and GitLab Version Control. The industry has caught onto @GitLabs secret. Learn more about why GitLab combined GitLab CI and GitLab version control Sid Sijbrandij Click to tweet Sign up for GitLabs twice-monthly newsletter ",
        "_version_": 1718536508665757696
      },
      {
        "story_id": 21437037,
        "story_author": "protontypes",
        "story_descendants": 54,
        "story_score": 150,
        "story_time": "2019-11-03T22:07:25Z",
        "story_title": "Robotics Development Environment with ROS in C++ and Python",
        "search": [
          "Robotics Development Environment with ROS in C++ and Python",
          "https://github.com/Ly0n/awesome-robotic-tooling",
          "Awesome Robotic Tooling Robotic resources and tools for professional development in C++ or Python with a touch of ROS, autonomous driving and aerospace. To stop reinventing the wheel you need to know about the wheel. This list is an attempt to show the variety of open and free tools in software and hardware development, which are useful in professional robotic development. Since the development processes are of crucial importance for the approval of such systems, the interaction of development processes and tools plays a central role. Your contribution is necessary to keep this list alive, increase the quality and to expand it. You can read more about it's origin and how you can participate in the contribution guide and related blog post. Contents Communication and Coordination Documentation and Presentation Requirements and Safety Architecture and Design Frameworks and Stacks Development Environment Code and Run Template Build and Deploy Unit and Integration Test Lint and Format Debugging and Tracing Version Control Simulation Electronics and Mechanics Sensor Processing Calibration and Transformation Perception Pipeline Machine Learning Parallel Processing Image Processing Radar Processing Lidar and Point Cloud Processing Localization and State Estimation Simultaneous Localization and Mapping Lidar Visual Vector Map Prediction Behavior and Decision Planning and Control User Interaction Graphical User Interface Acoustic User Interface Command Line Interface Data Visualization and Mission Control Annotation Point Cloud RViz Operation System Monitoring Database and Record Network Distributed File System Server Infrastructure and High Performance Computing Embedded Operation System Real-Time Kernel Network and Middleware Ethernet and Wireless Networking Controller Area Network Sensor and Acuator Interfaces Security Datasets Communication and Coordination Agile Development - Manifesto for Agile Software Development. Gitflow - Makes parallel development very easy, by isolating new development from finished work. DeepL - An online translator that outperforms Google, Microsoft and Facebook. Taiga - Agile Projectmanagment Tool. Kanboard - Minimalistic Kanban Board. kanban - Free, open source, self-hosted, Kanban board for GitLab issues. Gitlab - Simple Selfhosted Gitlab Server with Docker. Gogs - Build a simple, stable and extensible self-hosted Git service that can be setup in the most painless way. Wekan - Meteor based Kanban Board. JIRA API - Python Library for REST API of Jira. Taiga API - Python Library for REST API of Taiga. Chronos-Timetracker - Desktop client for JIRA. Track time, upload worklogs without a hassle. Grge - Grge is a daemon and command line utility augmenting GitLab. gitlab-triage - Gitlab's issues and merge requests triage, automated. Helpy - A modern, open source helpdesk customer support application. ONLYOFFICE - A free open source collaborative system developed to manage documents, projects, customer relationship and email correspondence, all in one place. discourse - A platform for community discussion. Free, open, simple. Gerrit - A code review and project management tool for Git based projects. jitsi-meet - Secure, Simple and Scalable Video Conferences that you use as a standalone app or embed in your web application. mattermost - An open source, private cloud, Slack-alternative. openproject - The leading open source project management software. leantime - Leantime is a lean project management system for innovators. gitter - Gitter is a chat and networking platform that helps to manage, grow and connect communities through messaging, content and discovery. Documentation and Presentation Typora - A Minimalist Markdown Editor. Markor - A Simple Markdown Editor for your Android Device. Pandoc - Universal markup converter. Yaspeller - Command line tool for spell checking. ReadtheDocs - Build your local ReadtheDocs Server. Doxygen - Doxygen is the de facto standard tool for generating documentation from annotated C++ sources. Sphinx - A tool that makes it easy to create intelligent and beautiful documentation for Python projects. Word-to-Markdown - A ruby gem to liberate content from Microsoft Word document. paperless - Index and archive all of your scanned paper documents. carbon - Share beautiful images of your source code. undraw - Free Professional business SVGs easy to customize. asciinema - Lets you easily record terminal sessions and replay them in a terminal as well as in a web browser. inkscape - Inkscape is a professional vector graphics editor for Linux, Windows and macOS. Reveal-Hugo - A Hugo theme for Reveal.js that makes authoring and customization a breeze. With it, you can turn any properly-formatted Hugo content into a HTML presentation. Hugo-Webslides - This is a Hugo template to create WebSlides presentation using markdown. jupyter2slides - Cloud Native Presentation Slides with Jupyter Notebook + Reveal.js. patat - Terminal-based presentations using Pandoc. github-changelog-generator - Automatically generate change log from your tags, issues, labels and pull requests on GitHub. GitLab-Release-Note-Generator - A Gitlab release note generator that generates release note on latest tag. OCRmyPDF - Adds an OCR text layer to scanned PDF files, allowing them to be searched. papermill - A tool for parameterizing, executing, and analyzing Jupyter Notebooks. docsy - An example documentation site using the Docsy Hugo theme. actions-hugo - Deploy website based on Hugo to GitHub Pages. overleaf - An open-source online real-time collaborative LaTeX editor. landslide - Generate HTML5 slideshows from markdown, ReST, or textile. libreoffice-impress-templates - Freely-licensed LibreOffice Impress templates. opensourcedesign - Community and Resources for Free Design and Logo Creation. olive - A free non-linear video editor aiming to provide a fully-featured alternative to high-end professional video editing software. buku - Browser-independent bookmark manager. swiftlatex - A WYSIWYG Browser-based LaTeX Editor. ReLaXed - Allows complex PDF layouts to be defined with CSS and JavaScript, while writing the content in a friendly, minimal syntax close to Markdown or LaTeX. foam - Foam is a personal knowledge management and sharing system inspired by Roam Research, built on Visual Studio Code and GitHub. CodiMD - Open Source Online Real-time collaborate on team documentation in markdown. jupyter-book - Build interactive, publication-quality documents from Jupyter Notebooks. InvoiceNet - Deep neural network to extract intelligent information from invoice documents. tesseract - Open Source OCR Engine. Requirements and Safety awesome-safety-critical - List of resources about programming practices for writing safety-critical software. open-autonomous-safety - OAS is a fully open-source library of Voyage's safety processes and testing procedures, designed to supplement existing safety programs at self-driving car startups across the world. CarND-Functional-Safety-Project - Create functional safety documents in this Udacity project. Automated Valet Parking Safety Documents - Created to support the safe testing of the Automated Valet Parking function using the StreetDrone test vehicle in a car park. safe_numerics - Replacements to standard numeric types which throw exceptions on errors. Air Vehicle C++ development coding standards - Provide direction and guidance to C++ programmers that will enable them to employ good programming style and proven programming practices leading to safe, reliable, testable, and maintainable code. AUTOSAR Coding Standard - Guidelines for the use of the C++14 language in critical and safety-related system. The W-Model and Lean Scaled Agility for Engineering - Ford applied an agile V-Model method from Vector that can be used in safety related project management. doorstop - Requirements management using version control. capella - Comprehensive, extensible and field-proven MBSE tool and method to successfully design systems architecture. robmosys - RobMoSys envisions an integrated approach built on top of the current code-centric robotic platforms, by applying model-driven methods and tools. Papyrus for Robotics - A graphical editing tool for robotic applications that complies with the RobMoSys approach. fossology - A toolkit you can run license, copyright and export control scans from the command line. ScenarioArchitect - The Scenario Architect is a basic python tool to generate, import and export short scene snapshots. Architecture and Design Guidelines - How to architect ROS-based systems. yEd - A powerful desktop application that can be used to quickly and effectively generate high-quality diagrams. yed_py - Generates graphML that can be opened in yEd. Plantuml - Web application to generate UML diagrams on-the-fly in your live documentation. rqt_graph - Provides a GUI plugin for visualizing the ROS computation graph. rqt_launchtree - An RQT plugin for hierarchical launchfile configuration introspection. cpp-dependencies - Tool to check C++ #include dependencies (dependency graphs created in .dot format). pydeps - Python Module Dependency graphs. aztarna - A footprinting tool for robots. draw.io - A free online diagram software for making flowcharts, process diagrams, org charts, UML, ER and network diagrams. vscode-drawio - This extension integrates Draw.io into VS Code. Frameworks and Stacks ROS - (Robot Operating System) provides libraries and tools to help software developers create robot applications. awesome-ros2 - A curated list of awesome Robot Operating System Version 2.0 (ROS 2) resources and libraries. Autoware.Auto - Autoware.Auto applies best-in-class software engineering for autonomous driving. Autoware.ai - Autoware.AI is the world's first \"All-in-One\" open-source software for autonomous driving technology. OpenPilot - Open Source Adaptive Cruise Control (ACC) and Lane Keeping Assist System (LKAS). Apollo - High performance, flexible architecture which accelerates the development, testing, and deployment of Autonomous Vehicles. PythonRobotics - This is a Python code collection of robotics algorithms, especially for autonomous navigation. Stanford Self Driving Car Code - Stanford Code From Cars That Entered DARPA Grand Challenges. astrobee - Astrobee is a free-flying robot designed to operate as a payload inside the International Space Station (ISS). CARMAPlatform - Enables cooperative automated driving plug-in. Automotive Grade Linux - Automotive Grade Linux is a collaborative open source project that is bringing together automakers, suppliers and technology companies to accelerate the development and adoption of a fully open software stack for the connected car. PX4 - An open source flight control software for drones and other unmanned vehicles. KubOS - An open-source software stack for satellites. mod_vehicle_dynamics_control - TUM Roborace Team Software Stack - Path tracking control, velocity control, curvature control and state estimation. Aslan - Open source self-driving software for low speed environments. open-source-rover - A build-it-yourself, 6-wheel rover based on the rovers on Mars from JPL. pybotics - An open-source and peer-reviewed Python toolbox for robot kinematics and calibration. makani - Contains the working Makani flight simulator, controller (autopilot), visualizer, and command center flight monitoring tools. mir_robot - This is a community project to use the MiR Robots with ROS. Development Environment Code and Run Vim-ros - Vim plugin for ROS development. Visual Studio Code - Code editor for edit-build-debug cycle. atom - Hackable text editor for the 21st century. Teletype - Share your workspace with team members and collaborate on code in real time in Atom. Sublime - A sophisticated text editor for code, markup and prose. ade-cli - The ADE Development Environment (ADE) uses docker and Gitlab to manage environments of per project development tools and optional volume images. recipe-wizard - A Dockerfile generator for running OpenGL (GLX) applications with nvidia-docker2, CUDA, ROS, and Gazebo on a remote headless server system. Jupyter ROS - Jupyter widget helpers for ROS, the Robot Operating System. ros_rqt_plugin - The ROS Qt Creator Plug-in for Python. xeus-cling - Jupyter kernel for the C++ programming language. ROS IDEs - This page collects experience and advice on using integrated development environments (IDEs) with ROS. TabNine - The all-language autocompleter. kite - Use machine learning to give you useful code completions for Python. jedi - Autocompletion and static analysis library for python. roslibpy - Python ROS Bridge library allows to use Python and IronPython to interact with ROS, the open-source robotic middleware. pybind11 - Seamless operability between C++11 and Python. Sourcetrail - Free and open-source cross-platform source explorer. rebound - Command-line tool that instantly fetches Stack Overflow results when an exception is thrown. mybinder - Open notebooks in an executable environment, making your code immediately reproducible by anyone, anywhere. ROSOnWindows - An experimental release of ROS1 for Windows. live-share - Real-time collaborative development from the comfort of your favorite tools. cocalc - Collaborative Calculation in the Cloud. EasyClangComplete - Robust C/C++ code completion for Sublime Text 3. vscode-ros - Visual Studio Code extension for Robot Operating System (ROS) development. awesome-hpp - A curated list of awesome header-only C++ libraries. Template ROS - Template for ROS node standardization in C++. Launch - Templates on how to create launch files for larger projects. Bash - A bash scripting template incorporating best practices & several useful functions. URDF - Examples on how to create Unified Robot Description Format (URDF) for different kinds of robots. Python - Style guide to be followed in writing Python code for ROS. Docker - The Dockerfile in the minimal-ade project shows a minimal example of how to create a custom base image. VS Code ROS2 Workspace Template - Template for using VSCode as an IDE for ROS2 development. Build and Deploy qemu-user-static - Enable an execution of different multi-architecture containers by QEMU and binfmt_misc. Cross compile ROS 2 on QNX - Introduces how to cross compile ROS 2 on QNX. bloom - A release automation tool which makes releasing catkin packages easier. superflore - An extended platform release manager for Robot Operating System. catkin_tools - Command line tools for working with catkin. industrial_ci - Easy continuous integration repository for ROS repositories. ros_gitlab_ci - Contains helper scripts and instructions on how to use Continuous Integration (CI) for ROS projects hosted on a GitLab instance. gitlab-runner - Runs tests and sends the results to GitLab. colcon-core - Command line tool to improve the workflow of building, testing and using multiple software packages. gitlab-release - Simple python3 script to upload files (from ci) to the current projects release (tag). clang - This is a compiler front-end for the C family of languages (C, C++, Objective-C, and Objective-C++) which is built as part of the LLVM compiler infrastructure project. catkin_virtualenv - Bundle python requirements in a catkin package via virtualenv. pyenv - Simple Python version management. aptly - Debian repository management tool. cross_compile - Assets used for ROS2 cross-compilation. docker_images - Official Docker images maintained by OSRF on ROS(2) and Gazebo. robot_upstart - Presents a suite of scripts to assist with launching background ROS processes on Ubuntu Linux PCs. robot_systemd - Units for managing startup and shutdown of roscore and roslaunch. ryo-iso - A modern ISO builder that streamlines the process of deploying a complete robot operating system from a yaml config file. network_autoconfig - Automatic configuration of ROS networking for most use cases without impacting usage that require manual configuration. rosbuild - The ROS build farm. cros - A single thread pure C implementation of the ROS framework. Unit and Integration Test setup-ros - This action sets up a ROS and ROS 2 environment for use in GitHub actions. UnitTesting - This page lays out the rationale, best practices, and policies for writing and running unit tests and integration tests for ROS. googletest - Google's C++ test framework. pytest - The pytest framework makes it easy to write small tests, yet scales to support complex functional testing. doctest - The fastest feature-rich C++11/14/17/20 single-header testing framework for unit tests and TDD. osrf_testing_tools_cpp - Contains testing tools for C++, and is used in OSRF projects. code_coverage - ROS package to run coverage testing. action-ros-ci - GitHub Action to build and test ROS 2 packages using colcon. Lint and Format action-ros-lint - GitHub action to run linters on ROS 2 packages. cppcheck - Static analysis of C/C++ code. hadolint - Dockerfile linter, validate inline bash, written in Haskell. shellcheck - A static analysis tool for shell scripts. catkin_lint - Checks package configurations for the catkin build system of ROS. pylint - Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. black - The uncompromising Python code formatter. pydocstyle - A static analysis tool for checking compliance with Python docstring conventions. haros - Static analysis of ROS application code. pydantic - Data parsing and validation using Python type hints. Debugging and Tracing heaptrack - Traces all memory allocations and annotates these events with stack traces. ros2_tracing - Tracing tools for ROS 2. Linuxperf - Various Linux performance material. lptrace - It lets you see in real-time what functions a Python program is running. pyre-check - Performant type-checking for python. FlameGraph - Visualize profiled code. gpuvis - GPU Trace Visualizer. sanitizer - AddressSanitizer, ThreadSanitizer, MemorySanitizer. cppinsights - C++ Insights - See your source code with the eyes of a compiler. inspect - The inspect module provides functions for learning about live objects, including modules, classes, instances, functions, and methods. Roslaunch Nodes in Valgrind or GDB - When debugging roscpp nodes that you are launching with roslaunch, you may wish to launch the node in a debugging program like gdb or valgrind instead. pyperformance - Python Performance Benchmark Suite. qira - QIRA is a competitor to strace and gdb. gdb-frontend - GDBFrontend is an easy, flexible and extensionable gui debugger. lttng - An open source software toolkit which you can use to simultaneously trace the Linux kernel, user applications, and user libraries. ros2-performance - Allows to easily create arbitrary ROS2 systems and then measures their performance. bcc - Tools for BPF-based Linux IO analysis, networking, monitoring, and more. tracy - A real time, nanosecond resolution, remote telemetry frame profiler for games and other applications. bpftrace - High-level tracing language for Linux eBPF. pudb - Full-screen console debugger for Python. backward-cpp - A beautiful stack trace pretty printer for C++. gdb-dashboard - GDB dashboard is a standalone .gdbinit file written using the Python API that enables a modular interface showing relevant information about the program being debugged. hotspot - The Linux perf GUI for performance analysis. memory_profiler - A python module for monitoring memory consumption of a process as well as line-by-line analysis of memory consumption for python programs. ros1_fuzzer - This fuzzer aims to help developers and researchers to find bugs and vulnerabilities in ROS nodes by performing fuzz tests over topics that the target nodes process. vscode-debug-visualizer - An extension for VS Code that visualizes data during debugging. action-tmate - Debug your GitHub Actions via SSH by using tmate to get access to the runner system itself. libstatistics_collector - ROS 2 library providing classes to collect measurements and calculate statistics across them. system_metrics_collector - Lightweight, real-time system metrics collector for ROS2 systems. Version Control git-fuzzy - A CLI interface to git that relies heavily on fzf. meld - Meld is a visual diff and merge tool that helps you compare files, directories, and version controlled projects. tig - Text-mode interface for git. gitg - A graphical user interface for git. git-cola - The highly caffeinated Git GUI. python-gitlab - A Python package providing access to the GitLab server API. bfg-repo-cleaner - Removes large or troublesome blobs like git-filter-branch does, but faster. nbdime - Tools for diffing and merging of Jupyter notebooks. semantic-release - Fully automated version management and package publishing. go-semrel-gitab - Automate version management for Gitlab. Git-repo - Git-Repo helps manage many Git repositories, does the uploads to revision control systems, and automates parts of the development workflow. dive - A tool for exploring each layer in a docker image. dvc - Management and versioning of datasets and machine learning models. learnGitBranching - A git repository visualizer, sandbox, and a series of educational tutorials and challenges. gitfs - You can mount a remote repository's branch locally, and any subsequent changes made to the files will be automatically committed to the remote. git-secret - Encrypts files with permitted users' public keys, allowing users you trust to access encrypted data using pgp and their secret keys. git-sweep - A command-line tool that helps you clean up Git branches that have been merged into master. lazygit - A simple terminal UI for git commands, written in Go with the gocui library. glab - An open-source GitLab command line tool. Simulation Drake - Drake aims to simulate even very complex dynamics of robots. Webots - Webots is an open source robot simulator compatible (among others) with ROS and ROS2. lgsv - LG Electronics America R&D Center has developed an HDRP Unity-based multi-robot simulator for autonomous vehicle developers. carla - Open-source simulator for autonomous driving research. awesome-CARLA - A curated list of awesome CARLA tutorials, blogs, and related projects. ros-bridge - ROS bridge for CARLA Simulator. scenario_runner - Traffic scenario definition and execution engine. deepdive - End-to-end simulation for self-driving cars. uuv_simulator - Gazebo/ROS packages for underwater robotics simulation. AirSim - Open source simulator for autonomous vehicles built on Unreal Engine. self-driving-car-sim - A self-driving car simulator built with Unity. ROSIntegration - Unreal Engine Plugin to enable ROS Support. gym-gazebo - An OpenAI gym extension for using Gazebo known as gym-gazebo. highway-env - A collection of environments for autonomous driving and tactical decision-making tasks. VREP Interface - ROS Bridge for the VREP simulator. car_demo - This is a simulation of a Prius in gazebo 9 with sensor data being published using ROS kinetic. sumo - Eclipse SUMO is an open source, highly portable, microscopic and continuous road traffic simulation package designed to handle large road networks. open-simulation-interface - A generic interface for the environmental perception of automated driving functions in virtual scenarios. ESIM - An Open Event Camera Simulator. Menge - Crowd Simulation Framework. pedsim_ros - Pedestrian simulator powered by the social force model for Gazebo. opencrg - Open file formats and open source tools for the detailed description, creation and evaluation of road surfaces. esmini - A basic OpenSCENARIO player. OpenSceneGraph - An open source high performance 3D graphics toolkit, used by application developers in fields such as visual simulation, games, virtual reality, scientific visualization and modelling. morse - An academic robotic simulator, based on the Blender Game Engine and the Bullet Physics engine. ROSIntegrationVision - Support for ROS-enabled RGBD data acquisition in Unreal Engine Projects. fetch_gazebo - Contains the Gazebo simulation for Fetch Robotics Fetch and Freight Research Edition Robots. rotors_simulator - Provides some multirotor models. flow - A computational framework for deep RL and control experiments for traffic microsimulation. gnss-ins-sim - GNSS + inertial navigation, sensor fusion simulator. Motion trajectory generator, sensor models, and navigation. Ignition Robotics - Test control strategies in safety, and take advantage of simulation in continuous integration tests. simulation assets for the SubT - This collection contains simulation assets for the SubT Challenge Virtual Competition in Gazebo. gazebo_ros_motors - Contains currently two motor plugins for Gazebo, one with an ideal speed controller and one without a controller that models a DC motor. map2gazebo - ROS package for creating Gazebo environments from 2D maps. sim_vehicle_dynamics - Vehicle Dynamics Simulation Software of TUM Roborace Team. gym-carla - An OpenAI gym wrapper for CARLA simulator. simbody - High-performance C++ multibody dynamics/physics library for simulating articulated biomechanical and mechanical systems like vehicles, robots, and the human skeleton. gazebo_models - This repository holds the Gazebo model database. pylot - Autonomous driving platform running on the CARLA simulator. flightmare - Flightmare is composed of two main components: a configurable rendering engine built on Unity and a flexible physics engine for dynamics simulation. champ - ROS Packages for CHAMP Quadruped Controller. rex-gym - OpenAI Gym environments for an open-source quadruped robot (SpotMicro). Trick - Developed at the NASA Johnson Space Center, is a powerful simulation development framework that enables users to build applications for all phases of space vehicle development. usv_sim_lsa - Unmanned Surface Vehicle simulation on Gazebo with water current and winds. 42 - Simulation for spacecraft attitude control system analysis and design. Complete_Street_Rule - A scenario oriented design tool intended to enable users to quickly create procedurally generated multimodal streets in ArcGIS CityEngine. AutoCore simulation - Provides test environment for Autoware and still during early development, contents below may changed during updates. fields-ignition - Generate random crop fields for Ignition Gazebo. Electronics and Mechanics HRIM - An information model for robot hardware. URDF - Repository for Unified Robot Description Format (URDF) parsing code. phobos - An add-on for Blender allowing to create URDF, SDF and SMURF robot models in a WYSIWYG environment. urdf-viz - Visualize URDF/XACRO file, URDF Viewer works on Windows/macOS/Linux. solidworks_urdf_exporter - SolidWorks to URDF Exporter. FreeCAD - Your own 3D parametric modeler. kicad - A Cross Platform and Open Source Electronics Design Automation Suite. PcbDraw - Convert your KiCAD board into a nice looking 2D drawing suitable for pinout diagrams. kicad-3rd-party-tools - Tools made by others to augment the KiCad PCB EDA suite. PandaPower - An easy to use open source tool for power system modeling, analysis and optimization with a high degree of automation. LibrePCB - A powerful, innovative and intuitive EDA tool for everyone. openscad - A software for creating solid 3D CAD models. ngspice - A open source spice simulator for electric and electronic circuits. GNSS-SDR - GNSS-SDR provides interfaces for a wide range of radio frequency front-ends and raw sample file formats, generates processing outputs in standard formats. riscv - The Free and Open RISC Instruction Set Architecture. urdfpy - A simple and easy-to-use library for loading, manipulating, saving, and visualizing URDF files. FMPy - Simulate Functional Mockup Units (FMUs) in Python. FMIKit-Simulink - Import and export Functional Mock-up Units with Simulink. oemof-solph - A modular open source framework to model energy supply systems. NASA-3D-Resources - Here you'll find a growing collection of 3D models, textures, and images from inside NASA. SUAVE - An Aircraft Design Toolbox. opem - The Open-Source PEMFC Simulation Tool (OPEM) is a modeling tool for evaluating the performance of proton exchange membrane fuel cells. pvlib-python - A community supported tool that provides a set of functions and classes for simulating the performance of photovoltaic energy systems. WireViz - A tool for easily documenting cables, wiring harnesses and connector pinouts. Horizon - EDA is an Electronic Design Automation package supporting an integrated end-to-end workflow for printed circuit board design including parts management and schematic entry. tigl - The TiGL Geometry Library can be used for the computation and processing of aircraft geometries stored inside CPACS files. foxBMS - A free, open and flexible development environment to design battery management systems. cadCAD - A Python package that assists in the processes of designing, testing and validating complex systems through simulation, with support for Monte Carlo methods, A/B testing and parameter sweeping. OpenMDAO - An open-source framework for efficient multidisciplinary optimization. ODrive - The aim is to make it possible to use inexpensive brushless motors in high performance robotics projects. OpenTirePython - An open-source mathematical tire modelling library. Sensor Processing Calibration and Transformation tf2 - Transform library, which lets the user keep track of multiple coordinate frames over time. lidar_align - A simple method for finding the extrinsic calibration between a 3D lidar and a 6-dof pose sensor. kalibr - The Kalibr visual-inertial calibration toolbox. Calibnet - Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks. lidar_camera_calibration - ROS package to find a rigid-body transformation between a LiDAR and a camera. ILCC - Reflectance Intensity Assisted Automatic and Accurate Extrinsic Calibration of 3D LiDAR. easy_handeye - Simple, straighforward ROS library for hand-eye calibration. imu_utils - A ROS package tool to analyze the IMU performance. kalibr_allan - IMU Allan standard deviation charts for use with Kalibr and inertial kalman filters. pyquaternion - A full-featured Python module for representing and using quaternions. robot_calibration - This package offers calibration of a number of parameters of a robot, such as: 3D Camera intrinsics, extrinsics Joint angle offsets and robot frame offsets. multi_sensor_calibration - Contains a calibration tool to calibrate a sensor setup consisting of lidars, radars and cameras. LiDARTag - A Real-Time Fiducial Tag using Point Clouds Lidar Data. multicam_calibration - Extrinsic and intrinsic calbration of cameras. ikpy - An Inverse Kinematics library aiming performance and modularity. livox_camera_lidar_calibration - Calibrate the extrinsic parameters between Livox LiDAR and camera. lidar_camera_calibration - Camera LiDAR Calibration using ROS, OpenCV, and PCL. Perception Pipeline SARosPerceptionKitti - ROS package for the Perception (Sensor Processing, Detection, Tracking and Evaluation) of the KITTI Vision Benchmark Suite. multiple-object-tracking-lidar - C++ implementation to Detect, track and classify multiple objects using LIDAR scans or point cloud. cadrl_ros - ROS package for dynamic obstacle avoidance for ground robots trained with deep RL. AugmentedAutoencoder - RGB-based pipeline for object detection and 6D pose estimation. jsk_recognition - A stack for the perception packages which are used in JSK lab. GibsonEnv - Gibson Environments: Real-World Perception for Embodied Agents. morefusion - Multi-object Reasoning for 6D Pose Estimation from Volumetric Fusion. Machine Learning DLIB - A toolkit for making real world machine learning and data analysis applications in C++. fastai - The fastai library simplifies training fast and accurate neural nets using modern best practices. tpot - A Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. deap - Distributed Evolutionary Algorithms in Python. gym - A toolkit for developing and comparing reinforcement learning algorithms. tensorflow_ros_cpp - A ROS package that allows to do Tensorflow inference in C++ without the need to compile TF yourself. Tensorflow Federated - TensorFlow Federated (TFF) is an open-source framework for machine learning and other computations on decentralized data. finn - Fast, Scalable Quantized Neural Network Inference on FPGAs. neuropod - Neuropod is a library that provides a uniform interface to run deep learning models from multiple frameworks in C++ and Python. leela-zero - This is a fairly faithful reimplementation of the system described in the Alpha Go Zero paper \"Mastering the Game of Go without Human Knowledge\". Trax - A library for deep learning that focuses on sequence models and reinforcement learning. mlflow - A platform to streamline machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models. Netron - Visualizer for neural network, deep learning and machine learning models. MNN - A blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba. Tensorforce - An open-source deep reinforcement learning framework, with an emphasis on modularized flexible library design and straightforward usability for applications in research and practice. Dopamine - A research framework for fast prototyping of reinforcement learning algorithms. catalyst - Was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. ray - A fast and simple framework for building and running distributed applications. tf-agents - A reliable, scalable and easy to use TensorFlow library for Contextual Bandits and Reinforcement Learning. ReAgent - An open source end-to-end platform for applied reinforcement learning (RL) developed and used at Facebook. Awesome-Mobile-Machine-Learning - A curated list of awesome mobile machine learning resources for iOS, Android, and edge devices. cnn-explainer - Learning Convolutional Neural Networks with Interactive Visualization. modelzoo - A collection of machine-learned models for use in autonomous driving applications. Parallel Processing dask - Parallel computing with task scheduling for Python. cupy - NumPy-like API accelerated with CUDA. Thrust - A C++ parallel programming library which resembles the C++ Standard Library. ArrayFire - A general purpose GPU library. OpenMP - An application programming interface that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran. VexCL - VexCL is a C++ vector expression template library for OpenCL/CUDA/OpenMP. PYNQ - An open-source project from Xilinx that makes it easy to design embedded systems with Zynq All Programmable Systems on Chips. numba - NumPy aware dynamic Python compiler using LLVM. TensorRT - A C++ library for high performance inference on NVIDIA GPUs and deep learning accelerators. libcudacxx - Provides a heterogeneous implementation of the C++ Standard Library that can be used in and between CPU and GPU code. Image Processing CV-pretrained-model - A collection of computer vision pre-trained models. image_pipeline - Fills the gap between getting raw images from a camera driver and higher-level vision processing. gstreamer - A pipeline-based multimedia framework that links together a wide variety of media processing systems to complete complex workflows. ros2_openvino_toolkit - Provides a ROS-adaptered runtime framework of neural network which quickly deploys applications and solutions for vision inference. vision_visp - Wraps the ViSP moving edge tracker provided by the ViSP visual servoing library into a ROS package. apriltag_ros - A ROS wrapper of the AprilTag 3 visual fiducial detector. deep_object_pose - Deep Object Pose Estimation. DetectAndTrack - Detect-and-Track: Efficient Pose. SfMLearner - An unsupervised learning framework for depth and ego-motion estimation. imgaug - Image augmentation for machine learning experiments. vision_opencv - Packages for interfacing ROS with OpenCV, a library of programming functions for real time computer vision. darknet_ros - YOLO ROS: Real-Time Object Detection for ROS. ros_ncnn - YOLACT / YOLO ( among other things ) on NCNN inference engine for ROS. tf-pose-estimation - Deep Pose Estimation implemented using Tensorflow with Custom Architectures for fast inference. find-object - Simple Qt interface to try OpenCV implementations of SIFT, SURF, FAST, BRIEF and other feature detectors and descriptors. yolact - A simple, fully convolutional model for real-time instance segmentation. Kimera-Semantics - Real-Time 3D Semantic Reconstruction from 2D data. detectron2 - A next-generation research platform for object detection and segmentation. OpenVX - Enables performance and power-optimized computer vision processing, especially important in embedded and real-time use cases. 3d-vehicle-tracking - Official implementation of Joint Monocular 3D Vehicle Detection and Tracking. pysot - The goal of PySOT is to provide a high-quality, high-performance codebase for visual tracking research. semantic_slam - Real time semantic slam in ROS with a hand held RGB-D camera. kitti_scan_unfolding - We propose KITTI scan unfolding in our paper Scan-based Semantic Segmentation of LiDAR Point Clouds: An Experimental Study. packnet-sfm - Official PyTorch implementation of self-supervised monocular depth estimation methods invented by the ML Team at Toyota Research Institute (TRI). AB3DMOT - This work proposes a simple yet accurate real-time baseline 3D multi-object tracking system. monoloco - Official implementation of \"MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation\" in PyTorch. Poly-YOLO - Builds on the original ideas of YOLOv3 and removes two of its weaknesses: a large amount of rewritten labels and inefficient distribution of anchors. satellite-image-deep-learning - Resources for deep learning with satellite & aerial imagery. robosat - Semantic segmentation on aerial and satellite imagery. big_transfer - Model for General Visual Representation Learning created by Google Research. LEDNet - A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation. TorchSeg - This project aims at providing a fast, modular reference implementation for semantic segmentation models using PyTorch. simpledet - A Simple and Versatile Framework for Object Detection and Instance Recognition. meshroom - Meshroom is a free, open-source 3D Reconstruction Software based on the AliceVision Photogrammetric Computer Vision framework. EasyOCR - Ready-to-use Optical character recognition (OCR) with 40+ languages supported including Chinese, Japanese, Korean and Thai. pytracking - A general python framework for visual object tracking and video object segmentation, based on PyTorch. ros_deep_learning - Deep learning inference nodes for ROS with support for NVIDIA Jetson TX1/TX2/Xavier and TensorRT. hyperpose - HyperPose: A Flexible Library for Real-time Human Pose Estimation. fawkes - Privacy preserving tool against facial recognition systems. anonymizer - An anonymizer to obfuscate faces and license plates. opendatacam - Only saves surveyed meta-data, in particular the path an object moved or number of counted objects at a certain point. Cam2BEV - TensorFlow Implementation for Computing a Semantically Segmented Bird's Eye View (BEV) Image Given the Images of Multiple Vehicle-Mounted Cameras. satpy - Python package for earth-observing satellite data processing. flownet2-pytorch - Pytorch implementation of FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks. Simd - C++ image processing and machine learning library with using of SIMD: SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, AVX, AVX2, AVX-512, VMX(Altivec) and VSX(Power7), NEON for ARM. Radar Processing pyroSAR - Framework for large-scale SAR satellite data processing. CameraRadarFusionNet - TUM Roborace Team Software Stack - Path tracking control, velocity control, curvature control and state estimation. Lidar and Point Cloud Processing cilantro - A lean C++ library for working with point cloud data. open3d - Open3D: A Modern Library for 3D Data Processing. SqueezeSeg - Implementation of SqueezeSeg, convolutional neural networks for LiDAR point clout segmentation. point_cloud_io - ROS nodes to read and write point clouds from and to files (e.g. ply, vtk). python-pcl - Python bindings to the pointcloud library. libpointmatcher - An \"Iterative Closest Point\" library for 2-D/3-D mapping in Robotics. depth_clustering - Fast and robust clustering of point clouds generated with a Velodyne sensor. lidar-bonnetal - Semantic and Instance Segmentation of LiDAR point clouds for autonomous driving. CSF - LiDAR point cloud ground filtering / segmentation (bare earth extraction) method based on cloth simulation. robot_body_filter - A highly configurable LaserScan/PointCloud2 filter that allows to dynamically remove the 3D body of the robot from the measurements. grid_map - Universal grid map library for mobile robotic mapping. elevation_mapping - Robot-centric elevation mapping for rough terrain navigation. rangenet_lib - Contains simple usage explanations of how the RangeNet++ inference works with the TensorRT and C++ interface. pointcloud_to_laserscan - Converts a 3D Point Cloud into a 2D laser scan. octomap - An Efficient Probabilistic 3D Mapping Framework Based on Octrees. pptk - Point Processing Toolkit from HEREMaps. gpu-voxels - GPU-Voxels is a CUDA based library which allows high resolution volumetric collision detection between animated 3D models and live pointclouds from 3D sensors of all kinds. spatio_temporal_voxel_layer - A new voxel layer leveraging modern 3D graphics tools to modernize navigation environmental representations. LAStools - Award-winning software for efficient LiDAR processing. PCDet - A general PyTorch-based codebase for 3D object detection from point cloud. PDAL - A C++ BSD library for translating and manipulating point cloud data. PotreeConverter - Builds a potree octree from las, laz, binary ply, xyz or ptx files. fast_gicp - A collection of GICP-based fast point cloud registration algorithms. ndt_omp - Multi-threaded and SSE friendly NDT algorithm. laser_line_extraction - A ROS packages that extracts line segments from LaserScan messages. Go-ICP - Implementation of the Go-ICP algorithm for globally optimal 3D pointset registration. PointCNN - A simple and general framework for feature learning from point clouds. segmenters_lib - The LiDAR segmenters library, for segmentation-based detection. MotionNet - Joint Perception and Motion Prediction for Autonomous Driving Based on Bird's Eye View Maps. PolarSeg - An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation. traversability_mapping - Takes in point cloud from a Velodyne VLP-16 Lidar and outputs a traversability map for autonomous navigation in real-time. lidar_super_resolution - Simulation-based Lidar Super-resolution for Ground Vehicles. Cupoch - A library that implements rapid 3D data processing and robotics computation using CUDA. linefit_ground_segmentation - Implementation of the ground segmentation algorithm. Draco - A library for compressing and decompressing 3D geometric meshes and point clouds. Votenet - Deep Hough Voting for 3D Object Detection in Point Clouds. lidar_undistortion - Provides lidar motion undistortion based on an external 6DoF pose estimation input. superpoint_graph - Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs. RandLA-Net - Efficient Semantic Segmentation of Large-Scale Point Clouds. Det3D - A first 3D Object Detection toolbox which provides off the box implementations of many 3D object detection algorithms such as PointPillars, SECOND, PIXOR. OverlapNet - A modified Siamese Network that predicts the overlap and relative yaw angle of a pair of range images generated by 3D LiDAR scans. mp2p_icp - A repertory of multi primitive-to-primitive (MP2P) ICP algorithms in C++. OpenPCDet - A Toolbox for LiDAR-based 3D Object Detection. torch-points3d - Pytorch framework for doing deep learning on point clouds. PolyFit - Polygonal Surface Reconstruction from Point Clouds. mmdetection3d - Next-generation platform for general 3D object detection. gpd - Takes a point cloud as input and produces pose estimates of viable grasps as output. SalsaNext - Uncertainty-aware Semantic Segmentation of LiDAR Point Clouds for Autonomous Driving. Super-Fast-Accurate-3D-Object-Detection - Super Fast and Accurate 3D Object Detection based on 3D LiDAR Point Clouds (The PyTorch implementation). Localization and State Estimation evo - Python package for the evaluation of odometry and SLAM. robot_localization - A package of nonlinear state estimation nodes. fuse - General architecture for performing sensor fusion live on a robot. GeographicLib - A C++ library for geographic projections. ntripbrowser - A Python API for browsing NTRIP (Networked Transport of RTCM via Internet Protocol). imu_tools - IMU-related filters and visualizers. RTKLIB - A version of RTKLIB optimized for single and dual frequency low cost GPS receivers, especially u-blox receivers. gLAB - Performs precise modeling of GNSS observables (pseudorange and carrier phase) at the centimetre level, allowing standalone GPS positioning, PPP, SBAS and DGNSS. ai-imu-dr - Contains the code of our novel accurate method for dead reckoning of wheeled vehicles based only on an IMU. Kalman-and-Bayesian-Filters-in-Python - Kalman Filter book using Jupyter Notebook. mcl_3dl - A ROS node to perform a probabilistic 3-D/6-DOF localization system for mobile robots with 3-D LIDAR(s). se2lam - On-SE(2) Localization and Mapping for Ground Vehicles by Fusing Odometry and Vision. mmWave-localization-learning - ML-based positioning method from mmWave transmissions - with high accuracy and energy efficiency. dynamic_robot_localization - A ROS package that offers 3 DoF and 6 DoF localization using PCL and allows dynamic map update using OctoMap. eagleye - An open-source software for vehicle localization utilizing GNSS and IMU. python-sgp4 - Python version of the SGP4 satellite position library. PROJ - Cartographic Projections and Coordinate Transformations Library. rpg_trajectory_evaluation - Implements common used trajectory evaluation methods for visual(-inertial) odometry. pymap3d - Pure-Python (Numpy optional) 3D coordinate conversions for geospace ecef enu eci. Simultaneous Localization and Mapping Lidar loam_velodyne - Laser Odometry and Mapping (Loam) is a realtime method for state estimation and mapping using a 3D lidar. lio-mapping - Implementation of Tightly Coupled 3D Lidar Inertial Odometry and Mapping (LIO-mapping). A-LOAM - Advanced implementation of LOAM. Fast LOAM - Fast and Optimized Lidar Odometry And Mapping. LIO_SAM - Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping. cartographer_ros - Provides ROS integration for Cartographer. loam_livox - A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR. StaticMapping - Use LiDAR to map the static world. semantic_suma - Semantic Mapping using Surfel Mapping and Semantic Segmentation. slam_toolbox - Slam Toolbox for lifelong mapping and localization in potentially massive maps with ROS . maplab - An open visual-inertial mapping framework. hdl_graph_slam - An open source ROS package for real-time 6DOF SLAM using a 3D LIDAR. interactive_slam - In contrast to existing automatic SLAM packages, we with minimal human effort. LeGO-LOAM - Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain. pyslam - Contains a monocular Visual Odometry (VO) pipeline in Python. Kitware SLAM - LiDAR-only visual SLAM developped by Kitware, as well as ROS and ParaView wrappings for easier use. horizon_highway_slam - A robust, low drift, and real time highway SLAM package suitable for Livox Horizon lidar. mola - A Modular System for Localization and Mapping. DH3D - Deep Hierarchical 3D Descriptors for Robust Large-Scale 6DOF Relocalization. LaMa - LaMa is a C++11 software library for robotic localization and mapping. LIO-SAM - Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping. Scan Context - Global LiDAR descriptor for place recognition and long-term localization. Visual orb_slam_2_ros - A ROS implementation of ORB_SLAM2. orbslam-map-saving-extension - In this extensions the map of ORB-features be saved to the disk as a reference for future runs along the same track. dso - Direct Sparse Odometry. viso2 - A ROS wrapper for libviso2, a library for visual odometry. xivo - X Inertial-aided Visual Odometry. rovio - Robust Visual Inertial Odometry Framework. LSD-SLAM - Large-Scale Direct Monocular SLAM is a real-time monocular SLAM. CubeSLAM and ORB SLAM - Monocular 3D Object Detection and SLAM Package of CubeSLAM and ORB SLAM. VINS-Fusion - A Robust and Versatile Multi-Sensor Visual-Inertial State Estimator. openvslam - OpenVSLAM: A Versatile Visual SLAM Framework. basalt - Visual-Inertial Mapping with Non-Linear Factor Recovery. Kimera - A C++ library for real-time metric-semantic simultaneous localization and mapping, which uses camera images and inertial data to build a semantically annotated 3D mesh of the environment. tagslam - A ROS-based package for Simultaneous Localization and Mapping using AprilTag fiducial markers. LARVIO - A lightweight, accurate and robust monocular visual inertial odometry based on Multi-State Constraint Kalman Filter. fiducials - Simultaneous localization and mapping using fiducial markers. open_vins - An open source platform for visual-inertial navigation research. ORB_SLAM3 - ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM. Atlas - End-to-End 3D Scene Reconstruction from Posed Images. vilib - This library focuses on the front-end of VIO pipelines with CUDA. hloc - A modular toolbox for state-of-the-art 6-DoF visual localization. It implements Hierarchical Localization, leveraging image retrieval and feature matching, and is fast, accurate, and scalable. ESVO - A novel pipeline for real-time visual odometry using a stereo event-based camera. Vector Map OpenDRIVE - An open file format for the logical description of road networks. MapsModelsImporter - A Blender add-on to import models from google maps. Lanelet2 - Map handling framework for automated driving. barefoot - Online and Offline map matching that can be used stand-alone and in the cloud. iD - The easy-to-use OpenStreetMap editor in JavaScript. RapiD - An enhanced version of iD for mapping with AI created by Facebook. segmap - A map representation based on 3D segments. Mapbox - A JavaScript library for interactive, customizable vector maps on the web. osrm-backend - Open Source Routing Machine - C++ backend. assuremapingtools - Desktop based tool for viewing, editing and saving road network maps for autonomous vehicle platforms such as Autoware. geopandas - A project to add support for geographic data to pandas objects. MapToolbox - Plugins to make Autoware vector maps in Unity. imagery-index - An index of aerial and satellite imagery useful for mapping. mapillary_tools - A library for processing and uploading images to Mapillary. mapnik - Combines pixel-perfect image output with lightning-fast cartographic algorithms, and exposes interfaces in C++, Python, and Node. gdal - GDAL is an open source X/MIT licensed translator library for raster and vector geospatial data formats. grass - GRASS GIS - free and open source Geographic Information System (GIS). 3d-tiles - Specification for streaming massive heterogeneous 3D geospatial datasets. osmnx - Python for street networks. Retrieve, model, analyze, and visualize street networks and other spatial data from OpenStreetMap. Prediction Awesome-Interaction-aware-Trajectory-Prediction - A selection of state-of-the-art research materials on trajectory prediction. sgan - Socially Acceptable Trajectories with Generative Adversarial Networks. Behavior and Decision Groot - Graphical Editor to create BehaviorTrees. Compliant with BehaviorTree.CPP. BehaviorTree.CPP - Behavior Trees Library in C++. RAFCON - Uses hierarchical state machines, featuring concurrent state execution, to represent robot programs. ROSPlan - Generic framework for task planning in a ROS system. ad-rss-lib - Library implementing the Responsibility Sensitive Safety model (RSS) for Autonomous Vehicles. FlexBE - Graphical editor for hierarchical state machines, based on ROS's smach. sts_bt_library - This library provides the functionality to set up your own behavior tree logic by using the defined tree structures like Fallback, Sequence or Parallel Nodes. SMACC - An Event-Driven, Asynchronous, Behavioral State Machine Library for real-time ROS (Robotic Operating System) applications written in C++ . py_trees_ros - Behaviours, trees and utilities that extend py_trees for use with ROS. Planning and Control pacmod - Designed to allow the user to control a vehicle with the PACMod drive-by-wire system. mpcc - Model Predictive Contouring Controller for Autonomous Racing. rrt - C++ RRT (Rapidly-exploring Random Tree) implementation. HypridAStarTrailer - A path planning algorithm based on Hybrid A* for trailer truck. path_planner - Hybrid A* Path Planner for the KTH Research Concept Vehicle. open_street_map - ROS packages for working with Open Street Map geographic information. Open Source Car Control - An assemblage of software and hardware designs that enable computer control of modern cars in order to facilitate the development of autonomous vehicle technology. fastrack - A ROS implementation of Fast and Safe Tracking (FaSTrack). commonroad - Composable benchmarks for motion planning on roads. traffic-editor - A graphical editor for robot traffic flows. steering_functions - Contains a C++ library that implements steering functions for car-like robots with limited turning radius. moveit - Easy-to-use robotics manipulation platform for developing applications, evaluating designs, and building integrated products. flexible-collision-library - A library for performing three types of proximity queries on a pair of geometric models composed of triangles. aikido - Artificial Intelligence for Kinematics, Dynamics, and Optimization. casADi - A symbolic framework for numeric optimization implementing automatic differentiation in forward and reverse modes on sparse matrix-valued computational graphs. ACADO Toolkit - A software environment and algorithm collection for automatic control and dynamic optimization. control-toolbox - An efficient C++ library for control, estimation, optimization and motion planning in robotics. CrowdNav - Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning. ompl - Consists of many state-of-the-art sampling-based motion planning algorithms. openrave - Open Robotics Automation Virtual Environment: An environment for testing, developing, and deploying robotics motion planning algorithms. teb_local_planner - An optimal trajectory planner considering distinctive topologies for mobile robots based on Timed-Elastic-Bands. pinocchio - A fast and flexible implementation of Rigid Body Dynamics algorithms and their analytical derivatives. rmf_core - The rmf_core packages provide the centralized functions of the Robotics Middleware Framework (RMF). OpEn - A solver for Fast & Accurate Embedded Optimization for next-generation Robotics and Autonomous Systems. autogenu-jupyter - This project provides the continuation/GMRES method (C/GMRES method) based solvers for nonlinear model predictive control (NMPC) and an automatic code generator for NMPC. global_racetrajectory_optimization - This repository contains multiple approaches for generating global racetrajectories. toppra - A library for computing the time-optimal path parametrization for robots subject to kinematic and dynamic constraints. tinyspline - TinySpline is a small, yet powerful library for interpolating, transforming, and querying arbitrary NURBS, B-Splines, and Bzier curves. dual quaternions ros - ROS python package for dual quaternion SLERP. mb planner - Aerial vehicle planner for tight spaces. Used in DARPA SubT Challenge. ilqr - Iterative Linear Quadratic Regulator with auto-differentiatiable dynamics models. EGO-Planner - A lightweight gradient-based local planner without ESDF construction, which significantly reduces computation time compared to some state-of-the-art methods. pykep - A scientific library providing basic tools for research in interplanetary trajectory design. am_traj - Alternating Minimization Based Trajectory Generation for Quadrotor Aggressive Flight. GraphBasedLocalTrajectoryPlanner - Was used on a real race vehicle during the Roborace Season Alpha and achieved speeds above 200km/h. User Interaction Graphical User Interface imgui - Designed to enable fast iterations and to empower programmers to create content creation tools and visualization / debug tools. qtpy - Provides an uniform layer to support PyQt5, PySide2, PyQt4 and PySide with a single codebase. mir - Mir is set of libraries for building Wayland based shells. rqt - A Qt-based framework for GUI development for ROS. It consists of three parts/metapackages. cage - This is Cage, a Wayland kiosk. A kiosk runs a single, maximized application. chilipie - Easy-to-use Raspberry Pi image for booting directly into full-screen Chrome. pencil - A tool for making diagrams and GUI prototyping that everyone can use. dynamic_reconfigure - The focus of dynamic_reconfigure is on providing a standard way to expose a subset of a node's parameters to external reconfiguration. ddynamic_reconfigure - Allows modifying parameters of a ROS node using the dynamic_reconfigure framework without having to write cfg files. elements - A lightweight, fine-grained, resolution independent, modular GUI library. NanoGUI - A minimalistic cross-platform widget library for OpenGL 3.x or higher. Acoustic User Interface pyo - A Python module written in C containing classes for a wide variety of audio signal processing types. rhasspy - Rhasspy (pronounced RAH-SPEE) is an offline, multilingual voice assistant toolkit inspired by Jasper that works well with Home Assistant, Hass.io, and Node-RED. mycroft-core - Mycroft is a hackable open source voice assistant. DDSP - A library of differentiable versions of common DSP functions (such as synthesizers, waveshapers, and filters). NoiseTorch - Creates a virtual microphone that suppresses noise, in any application. DeepSpeech - An open source Speech-To-Text engine, using a model trained by machine learning techniques based on Baidu's Deep Speech research paper. waveglow - A Flow-based Generative Network for Speech Synthesis. Command Line Interface the-art-of-command-line - Master the command line, in one page. dotfiles of cornerman - Powerful zsh and vim dotfiles. dotbot - A tool that bootstraps your dotfiles. prompt-hjem - A beautiful zsh prompt. ag - A code-searching tool similar to ack, but faster. fzf - A command-line fuzzy finder. pkgtop - Interactive package manager and resource monitor designed for the GNU/Linux. asciimatics - A cross platform package to do curses-like operations, plus higher level APIs and widgets to create text UIs and ASCII art animations. gocui - Minimalist Go package aimed at creating Console User Interfaces. TerminalImageViewer - Small C++ program to display images in a (modern) terminal using RGB ANSI codes and unicode block graphics characters. rosshow - Visualize ROS topics inside a terminal with Unicode/ASCII art. python-prompt-toolkit - Library for building powerful interactive command line applications in Python. guake - Drop-down terminal for GNOME. wemux - Multi-User Tmux Made Easy. tmuxp - A session manager built on libtmux. mapscii - World map renderer for your console. terminator - The goal of this project is to produce a useful tool for arranging terminals. bat - A cat(1) clone with wings. fx - Command-line tool and terminal JSON viewer. tmate - Instant terminal sharing. Data Visualization and Mission Control xdot - Interactive viewer for graphs written in Graphviz's dot language. guacamole - Clientless remote desktop gateway. It supports standard protocols like VNC, RDP, and SSH. ros3djs - 3D Visualization Library for use with the ROS JavaScript Libraries. webviz - Web-based visualization libraries like rviz. plotly.py - An open-source, interactive graphing library for Python. PlotJuggler - The timeseries visualization tool that you deserve. bokeh - Interactive Data Visualization in the browser, from Python. voila - From Jupyter notebooks to standalone web applications and dashboards. Pangolin - Pangolin is a lightweight portable rapid development library for managing OpenGL display / interaction and abstracting video input. rqt_bag - Provides a GUI plugin for displaying and replaying ROS bag files. kepler.gl - Kepler.gl is a powerful open source geospatial analysis tool for large-scale data sets. qgis_ros - Access bagged and live topic data in a highly featured GIS environment. openmct - A web based mission control framework. web_video_server - HTTP Streaming of ROS Image Topics in Multiple Formats. RVizWeb - Provides a convenient way of building and launching a web application with features similar to RViz. marvros - MAVLink to ROS gateway with proxy for Ground Control Station. octave - Provides a convenient command line interface for solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with Matlab. streetscape.gl - Streetscape.gl is a toolkit for visualizing autonomous and robotics data in the XVIZ protocol. urdf-loaders - URDF Loaders for Unity and THREE.js with example ATHLETE URDF File. obs-studio - Free and open source software for live streaming and screen recording. Annotation labelbox - The fastest way to annotate data to build and ship artificial intelligence applications. PixelAnnotationTool - Annotate quickly images. LabelImg - A graphical image annotation tool and label object bounding boxes in images. cvat - Powerful and efficient Computer Vision Annotation Tool (CVAT). point_labeler - Tool for labeling of a single point clouds or a stream of point clouds. label-studio - Label Studio is a multi-type data labeling and annotation tool with standardized output format. napari - A fast, interactive, multi-dimensional image viewer for python. semantic-segmentation-editor - A web based labeling tool for creating AI training data sets (2D and 3D). 3d-bat - 3D Bounding Box Annotation Tool for Point cloud and Image Labeling. labelme - Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation). universal-data-tool - Collaborate & label any type of data, images, text, or documents, in an easy web interface or desktop app. BMW-Labeltool-Lite - Provides you with a easy to use labeling tool for State-of-the-art Deep Learning training purposes. Point Cloud CloudCompare - CloudCompare is a 3D point cloud (and triangular mesh) processing software. Potree - WebGL point cloud viewer for large datasets. point_cloud_viewer - Makes viewing massive point clouds easy and convenient. LidarView - Performs real-time visualization and easy processing of live captured 3D LiDAR data from Lidar sensors. VeloView - Performs real-time visualization of live captured 3D LiDAR data from Velodyne's HDL sensors. entwine - A data organization library for massive point clouds, designed to conquer datasets of trillions of points as well as desktop-scale point clouds. polyscope - A C++ & Python viewer for 3D data like meshes and point clouds. Pcx - Point cloud importer & renderer for Unity. ImmersivePoints - A web-application for virtual reality devices to explore 3D data in the most natural way possible. RViz mapviz - Modular ROS visualization tool for 2D data. rviz_cinematographer - Easy to use tools to create and edit trajectories for the rviz camera. rviz_satellite - Display internet satellite imagery in RViz. rviz_visual_tools - C++ API wrapper for displaying shapes and meshes in Rviz. xpp - Visualization of motion-plans for legged robots. rviz stereo - 3D stereo rendering displays a different view to each eye so that the scene appears to have depth. jsk_visualization - Jsk visualization ros packages for rviz and rqt. moveit_visual_tools - Helper functions for displaying and debugging MoveIt! data in Rviz via published markers. Operation System Monitoring rosmon - ROS node launcher & monitoring daemon. multimaster_fkie - GUI-based management environment that is very useful to manage ROS-launch configurations and control running nodes. collectd - A small daemon which collects system information periodically and provides mechanisms to store and monitor the values in a variety of ways. lnav - An enhanced log file viewer that takes advantage of any semantic information that can be gleaned from the files being viewed, such as timestamps and log levels. htop - An interactive text-mode process viewer for Unix systems. It aims to be a better 'top'. atop - System and process monitor for Linux with logging and replay function. psutil - Cross-platform lib for process and system monitoring in Python. gputil - A Python module for getting the GPU status from NVIDA GPUs using nvidia-smi programmically in Python. gpustat - A simple command-line utility for querying and monitoring GPU status. nvtop - NVIDIA GPUs htop like monitoring tool. spdlog - Very fast, header-only/compiled, C++ logging library. ctop - Top-like interface for container metrics. ntop - Web-based Traffic and Security Network Traffic Monitoring. jupyterlab-nvdashboard - A JupyterLab extension for displaying dashboards of GPU usage. Database and Record ncdu - Ncdu is a disk usage analyzer with an ncurses interface. borg - Deduplicating archiver with compression and authenticated encryption. bag-database - A server that catalogs bag files and provides a web-based UI for accessing them. marv-robotics - MARV Robotics is a powerful and extensible data management platform. kitti2bag - Convert KITTI dataset to ROS bag file the easy way. pykitti - Python tools for working with KITTI data. rosbag_editor - Create a rosbag from a given one, using a simple GUI. nextcloud - Nextcloud is a suite of client-server software for creating and using file hosting services. ros_type_introspection - Deserialize ROS messages that are unknown at compilation time. syncthing - A continuous file synchronization program. rqt_bag_exporter - Qt GUI to export ROS bag topics to files (CSV and/or video). xviz - A protocol for real-time transfer and visualization of autonomy data. kitti_to_rosbag - A Dataset tools for working with the KITTI dataset raw data and converting it to a ROS bag. Also allows a library for direct access to poses, velodyne scans, and images. ros_numpy - Tools for converting ROS messages to and from numpy arrays. kitti_ros - A ROS-based player to replay KiTTI dataset. DuckDB - An embeddable SQL OLAP Database Management System. Network Distributed File System sshfs - File system based on the SSH File Transfer Protocol. moosefs - A scalable distributed storage system. ceph - A distributed object, block, and file storage platform. nfs - A distributed file system protocol originally developed by Sun Microsystems. ansible-role-nfs - Installs NFS utilities on RedHat/CentOS or Debian/Ubuntu. Server Infrastructure and High Performance Computing mass - Self-service, remote installation of Windows, CentOS, ESXi and Ubuntu on real servers turns your data centre into a bare metal cloud. polyaxon - A platform for reproducing and managing the whole life cycle of machine learning and deep learning applications. localstack - A fully functional local AWS cloud stack. Develop and test your cloud & Serverless apps offline. nvidia-docker - Build and run Docker containers leveraging NVIDIA GPUs. kubeflow - Machine Learning Toolkit for Kubernetes. log-pilot - Collect logs for docker containers. traefik - The Cloud Native Edge Router. graylog2-server - Free and open source log management. ansible - Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy. pyinfra - It can be used for ad-hoc command execution, service deployment, configuration management and more. docker-py - A Python library for the Docker Engine API. noVNC - VNC client using HTML5. Slurm - Slurm: A Highly Scalable Workload Manager. jupyterhub - Multi-user server for Jupyter notebooks. Portainer - Making Docker management easy. enroot - A simple, yet powerful tool to turn traditional container/OS images into unprivileged sandboxes. docker-firefox - Run a Docker Container with Firefox and noVNC for remote access to headless servers. luigi - A Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in. triton-inference-server - NVIDIA Triton Inference Server provides a cloud inferencing solution optimized for NVIDIA GPUs. cudf - Provides a pandas-like API that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming. Embedded Operation System vxworks7-ros2-build - Build system to automate the build of VxWorks 7 and ROS2. Yocto - Produce tools and processes that enable the creation of Linux distributions for embedded software that are independent of the underlying architecture of the embedded hardware. Automotive Graded Linux - A collaborative open source project that is bringing together automakers, suppliers and technology companies to build a Linux-based, open software platform for automotive applications that can serve as the de facto industry standard. bitbake - A generic task execution engine that allows shell and Python tasks to be run efficiently and in parallel while working within complex inter-task dependency constraints. Jailhouse - Jailhouse is a partitioning Hypervisor based on Linux. Xen - An open-source (GPL) type-1 or baremetal hypervisor. QEMU - A generic and open source machine emulator and virtualizer. qemu-xilinx - A fork of Quick EMUlator (QEMU) with improved support and modelling for the Xilinx platforms. rosserial - A ROS client library for small, embedded devices, such as Arduino. meta-ros - OpenEmbedded Layer for ROS Applications. meta-balena - Run Docker containers on embedded devices. micro-ros - The major changes compared to \"regular\" ROS 2 is that micro-ROS uses a Real-Time Operating System (RTOS) instead of Linux, and DDS for eXtremely Resource Constrained Environments. nvidia-container-runtime - NVIDIA Container Runtime is a GPU aware container runtime, compatible with the Open Containers Initiative (OCI) specification used by Docker, CRI-O, and other popular container technologie. fusesoc - Package manager and build abstraction tool for FPGA/ASIC development. jetson_easy - Automatically script to setup and configure your NVIDIA Jetson. docker-jetpack-sdk - Allows for usage of the NVIDIA JetPack SDK within a docker container for download, flashing, and install. Pressed - Provides a way to set answers to questions asked during the installation process of debian, without having to manually enter the answers while the installation is running. jetson_stats - A package to monitoring and control your NVIDIA Jetson (Xavier NX, Nano, AGX Xavier, TX1, TX2) Works with all NVIDIA Jetson ecosystem. ros_jetson_stats - The ROS jetson-stats wrapper. The status of your NVIDIA jetson in diagnostic messages. OpenCR - Open-source Control Module for ROS. acrn-hypervisor - Defines a device hypervisor reference stack and an architecture for running multiple software subsystems, managed securely, on a consolidated system by means of a virtual machine manager. jetson-containers - Machine Learning Containers for Jetson and JetPack 4.4. Real-Time Kernel ELISA - Project is to make it easier for companies to build and certify Linux-based safety-critical applications systems whose failure could result in loss of human life, significant property damage or environmental damage. PREEMPT_RT kernel patch - Aim of the PREEMPT_RT kernel patch is to minimize the amount of kernel code that is non-preemptible. Network and Middleware performance_test - Tool to test the performance of pub/sub based communication frameworks. realtime_support - Minimal real-time testing utility for measuring jitter and latency. ros1_bridge - ROS 2 package that provides bidirectional communication between ROS 1 and ROS 2. Fast-RTPS - A Protocol, which provides publisher-subscriber communications over unreliable transports such as UDP, as defined and maintained by the Object Management Group (OMG) consortium. protobuf - Google's data interchange format. opensplice - Vortex OpenSplice Community Edition. cyclonedds - Eclipse Cyclone DDS is a very performant and robust open-source DDS implementation. iceoryx - An IPC middleware for POSIX-based systems. rosbridge_suite - Provides a JSON interface to ROS, allowing any client to send JSON to publish or subscribe to ROS topics, call ROS services, and more. ros2arduino - This library helps the Arduino board communicate with the ROS2 using XRCE-DDS. eCAL - The enhanced communication abstraction layer (eCAL) is a middleware that enables scalable, high performance interprocess communication on a single computer node or between different nodes in a computer network. AUTOSAR-Adaptive - The implementation of AUTOSAR Adaptive Platform based on the R19-11. ocpp - The Open Charge Point Protocol (OCPP) is a network protocol for communication between electric vehicle chargers and a central backoffice system. Ethernet and Wireless Networking SOES - SOES is an EtherCAT slave stack written in C. netplan - Simply create a YAML description of the required network interfaces and what each should be configured to do. airalab - AIRA is reference Robonomics network client for ROS-enabled cyber-physical systems. rdbox - RDBOX is a IT infrastructure for ROS robots. ros_ethercat - This is a reimplementation of the main loop of pr2_ethercat without dependencies on PR2 software. wavemon - An ncurses-based monitoring application for wireless network devices. wireless - Making info about wireless networks available to ROS. ptpd - PTP daemon (PTPd) is an implementation the Precision Time Protocol (PTP) version 2 as defined by 'IEEE Std 1588-2008'. PTP provides precise time coordination of Ethernet LAN connected computers. iperf - A TCP, UDP, and SCTP network bandwidth measurement tool. tcpreplay - Pcap editing and replay tools. nethogs - It groups bandwidth by process. pyshark - Python wrapper for tshark, allowing python packet parsing using wireshark dissectors. pingtop - Ping multiple servers and show results in a top-like terminal UI. termshark - A terminal UI for tshark, inspired by Wireshark. udpreplay - Replay UDP packets from a pcap file. openwifi - Linux mac80211 compatible full-stack IEEE802.11/Wi-Fi design based on Software Defined Radio. Controller Area Network AndrOBD - Android OBD diagnostics with any ELM327 adapter. ddt4all - DDT4All is a tool to create your own ECU parameters screens and connect to a CAN network with a cheap ELM327 interface. cabana - CAN visualizer and DBC maker. opendbc - The project to democratize access to the decoder ring of your car. libuavcan - An open lightweight protocol designed for reliable communication in aerospace and robotic applications over robust vehicular networks such as CAN bus. python-can - The can package provides controller area network support for Python developers. CANopenNode - The internationally standardized (EN 50325-4) (CiA301) CAN-based higher-layer protocol for embedded control system. python-udsoncan - Python implementation of UDS (ISO-14229) standard. uds-c - Unified Diagnostics Service (UDS) and OBD-II (On Board Diagnostics for Vehicles) C Library. cantools - CAN BUS tools in Python 3. CANdevStudio - CANdevStudio aims to be cost-effective replacement for CAN simulation software. It can work with variety of CAN hardware interfaces. can-utils - Linux-CAN / SocketCAN user space applications. ros_canopen - CANopen driver framework for ROS. decanstructor - The definitive ROS CAN analysis tool. kvaser_interface - This package was developed as a standardized way to access Kvaser CAN devices from ROS. canmatrix - Converting CAN Database Formats .arxml .dbc .dbf .kcd. autosar - A set of python modules for working with AUTOSAR XML files. canopen - A Python implementation of the CANopen standard. The aim of the project is to support the most common parts of the CiA 301 standard in a Pythonic interface. SavvyCAN - A Qt5 based cross platform tool which can be used to load, save, and capture canbus frames. Open-Vehicle-Monitoring-System-3 - The system provides live monitoring of vehicle metrics like state of charge, temperatures, tyre pressures and diagnostic fault conditions. Sensor and Acuator Interfaces Tesla-API - Provides functionality to monitor and control the Model S (and future Tesla vehicles) remotely. flirpy - A Python library to interact with FLIR thermal imaging cameras and images. nerian_stereo - ROS node for Nerian's SceneScan and SP1 stereo vision sensors. pymmw - This is a toolbox composed of Python scripts to interact with TI's evaluation module (BoosterPack) for the IWR1443 mmWave sensing device. ti_mmwave_rospkg - TI mmWave radar ROS driver (with sensor fusion and hybrid). pacmod3 - This ROS node is designed to allow the user to control a vehicle with the PACMod drive-by-wire system, board revision 3. ros2_intel_realsense - These are packages for using Intel RealSense cameras (D400 series) with ROS2. sick_scan - This stack provides a ROS2 driver for the SICK TiM series of laser scanners. ouster_example - Sample code for connecting to and configuring the OS1, reading and visualizing data, and interfacing with ROS. ros2_ouster_drivers - These are an implementation of ROS2 drivers for the Ouster OS-1 3D lidars. livox_ros_driver - A new ROS package, specially used to connect LiDAR products produced by Livox. velodyne - A collection of ROS packages supporting Velodyne high definition 3D LIDARs. ublox - Provides support for u-blox GPS receivers. crazyflie_ros - ROS Driver for Bitcraze Crazyflie. pointgrey_camera_driver - ROS driver for Pt. Grey cameras, based on the official FlyCapture2 SDK. novatel_gps_driver - ROS driver for NovAtel GPS / GNSS receivers. pylon-ros-camera - The official pylon ROS driver for Basler GigE Vision and USB3 Vision cameras. ethz_piksi_ros - Contains (python) ROS drivers, tools, launch files, and wikis about how to use Piksi Real Time Kinematic (RTK) GPS device in ROS. sick_safetyscanners - A ROS Driver which reads the raw data from the SICK Safety Scanners and publishes the data as a laser_scan msg. bosch_imu_driver - A driver for the sensor IMU Bosch BNO055. It was implemented only the UART communication interface (correct sensor mode should be selected). oxford_gps_eth - Ethernet interface to OxTS GPS receivers using the NCOM packet structure. ifm3d - Library and Utilities for working with ifm pmd-based 3D ToF Cameras. cepton_sdk_redist - Provides ROS support for Cepton LiDAR. jetson_csi_cam - A ROS package making it simple to use CSI cameras on the Nvidia Jetson TK1, TX1, or TX2 with ROS. ros_astra_camera - A ROS driver for Orbbec 3D cameras. spot_ros - ROS Driver for Spot. Security owasp-threat-dragon-desktop - Threat Dragon is a free, open-source, cross-platform threat modeling application including system diagramming and a rule engine to auto-generate threats/mitigations. launch_ros_sandbox - Can define launch files running nodes in restrained environments, such as Docker containers or separate user accounts with limited privileges. wolfssl - A small, fast, portable implementation of TLS/SSL for embedded devices to the cloud. CANalyzat0r - Security analysis toolkit for proprietary car protocols. RSF - Robot Security Framework (RSF) is a standardized methodology to perform security assessments in robotics. How-to-Secure-A-Linux-Server - An evolving how-to guide for securing a Linux server. lynis - Security auditing tool for Linux, macOS, and UNIX-based systems. Assists with compliance testing (HIPAA/ISO27001/PCI DSS) and system hardening. OpenVPN - An open source VPN daemon. openfortivpn - A client for PPP+SSL VPN tunnel services and compatible with Fortinet VPNs. WireGuard - WireGuard is a novel VPN that runs inside the Linux Kernel and utilizes state-of-the-art cryptography. ssh-auditor - Scans for weak ssh passwords on your network. vulscan - Advanced vulnerability scanning with Nmap NSE. nmap-vulners - NSE script based on Vulners.com API. brutespray - Automatically attempts default creds on found services. fail2ban - Daemon to ban hosts that cause multiple authentication errors. DependencyCheck - A software composition analysis utility that detects publicly disclosed vulnerabilities in application dependencies. Firejail - A SUID sandbox program that reduces the risk of security breaches by restricting the running environment of untrusted applications using Linux namespaces, seccomp-bpf and Linux capabilities. RVD - Robot Vulnerability Database. Community-contributed archive of robot vulnerabilities and weaknesses. ros2_dds_security - Adding security enhancements by defining a Service Plugin Interface (SPI) architecture, a set of builtin implementations of the SPIs, and the security model enforced by the SPIs. Security-Enhanced Linux - A Linux kernel security module that provides a mechanism for supporting access control security policies, including mandatory access controls (MAC). OpenTitan - Will make the silicon Root of Trust design and implementation more transparent, trustworthy, and secure for enterprises, platform providers, and chip manufacturers. OpenTitan is administered by lowRISC CIC as a collaborative project to produce high quality, open IP for instantiation as a full-featured product. bandit - A tool designed to find common security issues in Python code. hardening - A quick way to make a Ubuntu server a bit more secure. Passbolt - Passbolt is a free and open source password manager that allows team members to store and share credentials securely. gopass - A password manager for the command line written in Go. pass - The standard unix password manager. Vault - A tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, certificates, and more. legion - An open source, easy-to-use, super-extensible and semi-automated network penetration testing framework that aids in discovery, reconnaissance and exploitation of information systems. openscap - The oscap program is a command line tool that allows users to load, scan, validate, edit, and export SCAP documents. Datasets KITTI-360 - This large-scale dataset contains 320k images and 100k laser scans in a driving distance of 73.7km. waymo_ros - This is a ROS package to connect Waymo open dataset to ROS. waymo-open-dataset - The Waymo Open Dataset is comprised of high-resolution sensor data collected by Waymo self-driving cars in a wide variety of conditions. Ford Autonomous Vehicle Dataset - Ford presents a challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times. awesome-robotics-datasets - A collection of useful datasets for robotics and computer vision. nuscenes-devkit - The devkit of the nuScenes dataset. dataset-api - This is a repo of toolkit for ApolloScape Dataset, CVPR 2019 Workshop on Autonomous Driving Challenge and ECCV 2018 challenge. utbm_robocar_dataset - EU Long-term Dataset with Multiple Sensors for Autonomous Driving. DBNet - A Large-Scale Dataset for Driving Behavior Learning. argoverse-api - Official GitHub repository for Argoverse dataset. DDAD - A new autonomous driving benchmark from TRI (Toyota Research Institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions. pandaset-devkit - Public large-scale dataset for autonomous driving provided by Hesai & Scale. a2d2_to_ros - Utilities for converting A2D2 data sets to ROS bags. awesome-satellite-imagery-datasets - List of satellite image training datasets with annotations for computer vision and deep learning. sentinelsat - Search and download Copernicus Sentinel satellite images. adas-dataset-form - Thermal Dataset for Algorithm Training. h3d - The H3D is a large scale full-surround 3D multi-object detection and tracking dataset from Honda. Mapillary Vistas Dataset - A diverse street-level imagery dataset with pixelaccurate and instancespecific human annotations for understanding street scenes around the world. TensorFlow Datasets - TensorFlow Datasets provides many public datasets as tf.data.Datasets. racetrack-database - Contains center lines (x- and y-coordinates), track widths and race lines for over 20 race tracks (mainly F1 and DTM) all over the world. BlenderProc - A procedural Blender pipeline for photorealistic training image generation. Atlatec Sample Map Data - 3D map for autonomous driving and simulation created from nothing but two cameras and GPS in downtown San Francisco. Lyft Level 5 Dataset - Level 5 is developing a self-driving system for the Lyft network. We're collecting and processing data from our autonomous fleet and sharing it with you. holicity - A City-Scale Data Platform for Learning Holistic 3D Structures. UTD19 - Largest multi-city traffic dataset publically available. ASTYX HIRES2019 DATASET - Automotive Radar Dataset for Deep Learning Based 3D Object Detection. Objectron - A collection of short, object-centric video clips, which are accompanied by AR session metadata that includes camera poses, sparse point-clouds and characterization of the planar surfaces in the surrounding environment. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.com/Ly0n/awesome-robotic-tooling",
        "comments.comment_id": [21437583, 21438738],
        "comments.comment_author": ["heyflyguy", "a_t48"],
        "comments.comment_descendants": [3, 4],
        "comments.comment_time": [
          "2019-11-03T23:40:59Z",
          "2019-11-04T03:16:05Z"
        ],
        "comments.comment_text": [
          "If you want to really enjoy the ingenuity of people that otherwise might be considered \"simple\", check out the ROS for Agriculture group and slack channels.  People converting 70s and 80s era farm equipment to be automated, and it's pure gold.",
          "ROS is really great for getting started with robotics, but from my experience (and from others who I've talked to) tends to buckle under its own weight in larger projects. This isn't to say you shouldn't use it, just be careful with it. It has some really questionable design decisions and implementation details that can bite you later."
        ],
        "id": "4d8a9bc8-f339-416a-a0ce-0046a5ba83d7",
        "url_text": "Awesome Robotic Tooling Robotic resources and tools for professional development in C++ or Python with a touch of ROS, autonomous driving and aerospace. To stop reinventing the wheel you need to know about the wheel. This list is an attempt to show the variety of open and free tools in software and hardware development, which are useful in professional robotic development. Since the development processes are of crucial importance for the approval of such systems, the interaction of development processes and tools plays a central role. Your contribution is necessary to keep this list alive, increase the quality and to expand it. You can read more about it's origin and how you can participate in the contribution guide and related blog post. Contents Communication and Coordination Documentation and Presentation Requirements and Safety Architecture and Design Frameworks and Stacks Development Environment Code and Run Template Build and Deploy Unit and Integration Test Lint and Format Debugging and Tracing Version Control Simulation Electronics and Mechanics Sensor Processing Calibration and Transformation Perception Pipeline Machine Learning Parallel Processing Image Processing Radar Processing Lidar and Point Cloud Processing Localization and State Estimation Simultaneous Localization and Mapping Lidar Visual Vector Map Prediction Behavior and Decision Planning and Control User Interaction Graphical User Interface Acoustic User Interface Command Line Interface Data Visualization and Mission Control Annotation Point Cloud RViz Operation System Monitoring Database and Record Network Distributed File System Server Infrastructure and High Performance Computing Embedded Operation System Real-Time Kernel Network and Middleware Ethernet and Wireless Networking Controller Area Network Sensor and Acuator Interfaces Security Datasets Communication and Coordination Agile Development - Manifesto for Agile Software Development. Gitflow - Makes parallel development very easy, by isolating new development from finished work. DeepL - An online translator that outperforms Google, Microsoft and Facebook. Taiga - Agile Projectmanagment Tool. Kanboard - Minimalistic Kanban Board. kanban - Free, open source, self-hosted, Kanban board for GitLab issues. Gitlab - Simple Selfhosted Gitlab Server with Docker. Gogs - Build a simple, stable and extensible self-hosted Git service that can be setup in the most painless way. Wekan - Meteor based Kanban Board. JIRA API - Python Library for REST API of Jira. Taiga API - Python Library for REST API of Taiga. Chronos-Timetracker - Desktop client for JIRA. Track time, upload worklogs without a hassle. Grge - Grge is a daemon and command line utility augmenting GitLab. gitlab-triage - Gitlab's issues and merge requests triage, automated. Helpy - A modern, open source helpdesk customer support application. ONLYOFFICE - A free open source collaborative system developed to manage documents, projects, customer relationship and email correspondence, all in one place. discourse - A platform for community discussion. Free, open, simple. Gerrit - A code review and project management tool for Git based projects. jitsi-meet - Secure, Simple and Scalable Video Conferences that you use as a standalone app or embed in your web application. mattermost - An open source, private cloud, Slack-alternative. openproject - The leading open source project management software. leantime - Leantime is a lean project management system for innovators. gitter - Gitter is a chat and networking platform that helps to manage, grow and connect communities through messaging, content and discovery. Documentation and Presentation Typora - A Minimalist Markdown Editor. Markor - A Simple Markdown Editor for your Android Device. Pandoc - Universal markup converter. Yaspeller - Command line tool for spell checking. ReadtheDocs - Build your local ReadtheDocs Server. Doxygen - Doxygen is the de facto standard tool for generating documentation from annotated C++ sources. Sphinx - A tool that makes it easy to create intelligent and beautiful documentation for Python projects. Word-to-Markdown - A ruby gem to liberate content from Microsoft Word document. paperless - Index and archive all of your scanned paper documents. carbon - Share beautiful images of your source code. undraw - Free Professional business SVGs easy to customize. asciinema - Lets you easily record terminal sessions and replay them in a terminal as well as in a web browser. inkscape - Inkscape is a professional vector graphics editor for Linux, Windows and macOS. Reveal-Hugo - A Hugo theme for Reveal.js that makes authoring and customization a breeze. With it, you can turn any properly-formatted Hugo content into a HTML presentation. Hugo-Webslides - This is a Hugo template to create WebSlides presentation using markdown. jupyter2slides - Cloud Native Presentation Slides with Jupyter Notebook + Reveal.js. patat - Terminal-based presentations using Pandoc. github-changelog-generator - Automatically generate change log from your tags, issues, labels and pull requests on GitHub. GitLab-Release-Note-Generator - A Gitlab release note generator that generates release note on latest tag. OCRmyPDF - Adds an OCR text layer to scanned PDF files, allowing them to be searched. papermill - A tool for parameterizing, executing, and analyzing Jupyter Notebooks. docsy - An example documentation site using the Docsy Hugo theme. actions-hugo - Deploy website based on Hugo to GitHub Pages. overleaf - An open-source online real-time collaborative LaTeX editor. landslide - Generate HTML5 slideshows from markdown, ReST, or textile. libreoffice-impress-templates - Freely-licensed LibreOffice Impress templates. opensourcedesign - Community and Resources for Free Design and Logo Creation. olive - A free non-linear video editor aiming to provide a fully-featured alternative to high-end professional video editing software. buku - Browser-independent bookmark manager. swiftlatex - A WYSIWYG Browser-based LaTeX Editor. ReLaXed - Allows complex PDF layouts to be defined with CSS and JavaScript, while writing the content in a friendly, minimal syntax close to Markdown or LaTeX. foam - Foam is a personal knowledge management and sharing system inspired by Roam Research, built on Visual Studio Code and GitHub. CodiMD - Open Source Online Real-time collaborate on team documentation in markdown. jupyter-book - Build interactive, publication-quality documents from Jupyter Notebooks. InvoiceNet - Deep neural network to extract intelligent information from invoice documents. tesseract - Open Source OCR Engine. Requirements and Safety awesome-safety-critical - List of resources about programming practices for writing safety-critical software. open-autonomous-safety - OAS is a fully open-source library of Voyage's safety processes and testing procedures, designed to supplement existing safety programs at self-driving car startups across the world. CarND-Functional-Safety-Project - Create functional safety documents in this Udacity project. Automated Valet Parking Safety Documents - Created to support the safe testing of the Automated Valet Parking function using the StreetDrone test vehicle in a car park. safe_numerics - Replacements to standard numeric types which throw exceptions on errors. Air Vehicle C++ development coding standards - Provide direction and guidance to C++ programmers that will enable them to employ good programming style and proven programming practices leading to safe, reliable, testable, and maintainable code. AUTOSAR Coding Standard - Guidelines for the use of the C++14 language in critical and safety-related system. The W-Model and Lean Scaled Agility for Engineering - Ford applied an agile V-Model method from Vector that can be used in safety related project management. doorstop - Requirements management using version control. capella - Comprehensive, extensible and field-proven MBSE tool and method to successfully design systems architecture. robmosys - RobMoSys envisions an integrated approach built on top of the current code-centric robotic platforms, by applying model-driven methods and tools. Papyrus for Robotics - A graphical editing tool for robotic applications that complies with the RobMoSys approach. fossology - A toolkit you can run license, copyright and export control scans from the command line. ScenarioArchitect - The Scenario Architect is a basic python tool to generate, import and export short scene snapshots. Architecture and Design Guidelines - How to architect ROS-based systems. yEd - A powerful desktop application that can be used to quickly and effectively generate high-quality diagrams. yed_py - Generates graphML that can be opened in yEd. Plantuml - Web application to generate UML diagrams on-the-fly in your live documentation. rqt_graph - Provides a GUI plugin for visualizing the ROS computation graph. rqt_launchtree - An RQT plugin for hierarchical launchfile configuration introspection. cpp-dependencies - Tool to check C++ #include dependencies (dependency graphs created in .dot format). pydeps - Python Module Dependency graphs. aztarna - A footprinting tool for robots. draw.io - A free online diagram software for making flowcharts, process diagrams, org charts, UML, ER and network diagrams. vscode-drawio - This extension integrates Draw.io into VS Code. Frameworks and Stacks ROS - (Robot Operating System) provides libraries and tools to help software developers create robot applications. awesome-ros2 - A curated list of awesome Robot Operating System Version 2.0 (ROS 2) resources and libraries. Autoware.Auto - Autoware.Auto applies best-in-class software engineering for autonomous driving. Autoware.ai - Autoware.AI is the world's first \"All-in-One\" open-source software for autonomous driving technology. OpenPilot - Open Source Adaptive Cruise Control (ACC) and Lane Keeping Assist System (LKAS). Apollo - High performance, flexible architecture which accelerates the development, testing, and deployment of Autonomous Vehicles. PythonRobotics - This is a Python code collection of robotics algorithms, especially for autonomous navigation. Stanford Self Driving Car Code - Stanford Code From Cars That Entered DARPA Grand Challenges. astrobee - Astrobee is a free-flying robot designed to operate as a payload inside the International Space Station (ISS). CARMAPlatform - Enables cooperative automated driving plug-in. Automotive Grade Linux - Automotive Grade Linux is a collaborative open source project that is bringing together automakers, suppliers and technology companies to accelerate the development and adoption of a fully open software stack for the connected car. PX4 - An open source flight control software for drones and other unmanned vehicles. KubOS - An open-source software stack for satellites. mod_vehicle_dynamics_control - TUM Roborace Team Software Stack - Path tracking control, velocity control, curvature control and state estimation. Aslan - Open source self-driving software for low speed environments. open-source-rover - A build-it-yourself, 6-wheel rover based on the rovers on Mars from JPL. pybotics - An open-source and peer-reviewed Python toolbox for robot kinematics and calibration. makani - Contains the working Makani flight simulator, controller (autopilot), visualizer, and command center flight monitoring tools. mir_robot - This is a community project to use the MiR Robots with ROS. Development Environment Code and Run Vim-ros - Vim plugin for ROS development. Visual Studio Code - Code editor for edit-build-debug cycle. atom - Hackable text editor for the 21st century. Teletype - Share your workspace with team members and collaborate on code in real time in Atom. Sublime - A sophisticated text editor for code, markup and prose. ade-cli - The ADE Development Environment (ADE) uses docker and Gitlab to manage environments of per project development tools and optional volume images. recipe-wizard - A Dockerfile generator for running OpenGL (GLX) applications with nvidia-docker2, CUDA, ROS, and Gazebo on a remote headless server system. Jupyter ROS - Jupyter widget helpers for ROS, the Robot Operating System. ros_rqt_plugin - The ROS Qt Creator Plug-in for Python. xeus-cling - Jupyter kernel for the C++ programming language. ROS IDEs - This page collects experience and advice on using integrated development environments (IDEs) with ROS. TabNine - The all-language autocompleter. kite - Use machine learning to give you useful code completions for Python. jedi - Autocompletion and static analysis library for python. roslibpy - Python ROS Bridge library allows to use Python and IronPython to interact with ROS, the open-source robotic middleware. pybind11 - Seamless operability between C++11 and Python. Sourcetrail - Free and open-source cross-platform source explorer. rebound - Command-line tool that instantly fetches Stack Overflow results when an exception is thrown. mybinder - Open notebooks in an executable environment, making your code immediately reproducible by anyone, anywhere. ROSOnWindows - An experimental release of ROS1 for Windows. live-share - Real-time collaborative development from the comfort of your favorite tools. cocalc - Collaborative Calculation in the Cloud. EasyClangComplete - Robust C/C++ code completion for Sublime Text 3. vscode-ros - Visual Studio Code extension for Robot Operating System (ROS) development. awesome-hpp - A curated list of awesome header-only C++ libraries. Template ROS - Template for ROS node standardization in C++. Launch - Templates on how to create launch files for larger projects. Bash - A bash scripting template incorporating best practices & several useful functions. URDF - Examples on how to create Unified Robot Description Format (URDF) for different kinds of robots. Python - Style guide to be followed in writing Python code for ROS. Docker - The Dockerfile in the minimal-ade project shows a minimal example of how to create a custom base image. VS Code ROS2 Workspace Template - Template for using VSCode as an IDE for ROS2 development. Build and Deploy qemu-user-static - Enable an execution of different multi-architecture containers by QEMU and binfmt_misc. Cross compile ROS 2 on QNX - Introduces how to cross compile ROS 2 on QNX. bloom - A release automation tool which makes releasing catkin packages easier. superflore - An extended platform release manager for Robot Operating System. catkin_tools - Command line tools for working with catkin. industrial_ci - Easy continuous integration repository for ROS repositories. ros_gitlab_ci - Contains helper scripts and instructions on how to use Continuous Integration (CI) for ROS projects hosted on a GitLab instance. gitlab-runner - Runs tests and sends the results to GitLab. colcon-core - Command line tool to improve the workflow of building, testing and using multiple software packages. gitlab-release - Simple python3 script to upload files (from ci) to the current projects release (tag). clang - This is a compiler front-end for the C family of languages (C, C++, Objective-C, and Objective-C++) which is built as part of the LLVM compiler infrastructure project. catkin_virtualenv - Bundle python requirements in a catkin package via virtualenv. pyenv - Simple Python version management. aptly - Debian repository management tool. cross_compile - Assets used for ROS2 cross-compilation. docker_images - Official Docker images maintained by OSRF on ROS(2) and Gazebo. robot_upstart - Presents a suite of scripts to assist with launching background ROS processes on Ubuntu Linux PCs. robot_systemd - Units for managing startup and shutdown of roscore and roslaunch. ryo-iso - A modern ISO builder that streamlines the process of deploying a complete robot operating system from a yaml config file. network_autoconfig - Automatic configuration of ROS networking for most use cases without impacting usage that require manual configuration. rosbuild - The ROS build farm. cros - A single thread pure C implementation of the ROS framework. Unit and Integration Test setup-ros - This action sets up a ROS and ROS 2 environment for use in GitHub actions. UnitTesting - This page lays out the rationale, best practices, and policies for writing and running unit tests and integration tests for ROS. googletest - Google's C++ test framework. pytest - The pytest framework makes it easy to write small tests, yet scales to support complex functional testing. doctest - The fastest feature-rich C++11/14/17/20 single-header testing framework for unit tests and TDD. osrf_testing_tools_cpp - Contains testing tools for C++, and is used in OSRF projects. code_coverage - ROS package to run coverage testing. action-ros-ci - GitHub Action to build and test ROS 2 packages using colcon. Lint and Format action-ros-lint - GitHub action to run linters on ROS 2 packages. cppcheck - Static analysis of C/C++ code. hadolint - Dockerfile linter, validate inline bash, written in Haskell. shellcheck - A static analysis tool for shell scripts. catkin_lint - Checks package configurations for the catkin build system of ROS. pylint - Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. black - The uncompromising Python code formatter. pydocstyle - A static analysis tool for checking compliance with Python docstring conventions. haros - Static analysis of ROS application code. pydantic - Data parsing and validation using Python type hints. Debugging and Tracing heaptrack - Traces all memory allocations and annotates these events with stack traces. ros2_tracing - Tracing tools for ROS 2. Linuxperf - Various Linux performance material. lptrace - It lets you see in real-time what functions a Python program is running. pyre-check - Performant type-checking for python. FlameGraph - Visualize profiled code. gpuvis - GPU Trace Visualizer. sanitizer - AddressSanitizer, ThreadSanitizer, MemorySanitizer. cppinsights - C++ Insights - See your source code with the eyes of a compiler. inspect - The inspect module provides functions for learning about live objects, including modules, classes, instances, functions, and methods. Roslaunch Nodes in Valgrind or GDB - When debugging roscpp nodes that you are launching with roslaunch, you may wish to launch the node in a debugging program like gdb or valgrind instead. pyperformance - Python Performance Benchmark Suite. qira - QIRA is a competitor to strace and gdb. gdb-frontend - GDBFrontend is an easy, flexible and extensionable gui debugger. lttng - An open source software toolkit which you can use to simultaneously trace the Linux kernel, user applications, and user libraries. ros2-performance - Allows to easily create arbitrary ROS2 systems and then measures their performance. bcc - Tools for BPF-based Linux IO analysis, networking, monitoring, and more. tracy - A real time, nanosecond resolution, remote telemetry frame profiler for games and other applications. bpftrace - High-level tracing language for Linux eBPF. pudb - Full-screen console debugger for Python. backward-cpp - A beautiful stack trace pretty printer for C++. gdb-dashboard - GDB dashboard is a standalone .gdbinit file written using the Python API that enables a modular interface showing relevant information about the program being debugged. hotspot - The Linux perf GUI for performance analysis. memory_profiler - A python module for monitoring memory consumption of a process as well as line-by-line analysis of memory consumption for python programs. ros1_fuzzer - This fuzzer aims to help developers and researchers to find bugs and vulnerabilities in ROS nodes by performing fuzz tests over topics that the target nodes process. vscode-debug-visualizer - An extension for VS Code that visualizes data during debugging. action-tmate - Debug your GitHub Actions via SSH by using tmate to get access to the runner system itself. libstatistics_collector - ROS 2 library providing classes to collect measurements and calculate statistics across them. system_metrics_collector - Lightweight, real-time system metrics collector for ROS2 systems. Version Control git-fuzzy - A CLI interface to git that relies heavily on fzf. meld - Meld is a visual diff and merge tool that helps you compare files, directories, and version controlled projects. tig - Text-mode interface for git. gitg - A graphical user interface for git. git-cola - The highly caffeinated Git GUI. python-gitlab - A Python package providing access to the GitLab server API. bfg-repo-cleaner - Removes large or troublesome blobs like git-filter-branch does, but faster. nbdime - Tools for diffing and merging of Jupyter notebooks. semantic-release - Fully automated version management and package publishing. go-semrel-gitab - Automate version management for Gitlab. Git-repo - Git-Repo helps manage many Git repositories, does the uploads to revision control systems, and automates parts of the development workflow. dive - A tool for exploring each layer in a docker image. dvc - Management and versioning of datasets and machine learning models. learnGitBranching - A git repository visualizer, sandbox, and a series of educational tutorials and challenges. gitfs - You can mount a remote repository's branch locally, and any subsequent changes made to the files will be automatically committed to the remote. git-secret - Encrypts files with permitted users' public keys, allowing users you trust to access encrypted data using pgp and their secret keys. git-sweep - A command-line tool that helps you clean up Git branches that have been merged into master. lazygit - A simple terminal UI for git commands, written in Go with the gocui library. glab - An open-source GitLab command line tool. Simulation Drake - Drake aims to simulate even very complex dynamics of robots. Webots - Webots is an open source robot simulator compatible (among others) with ROS and ROS2. lgsv - LG Electronics America R&D Center has developed an HDRP Unity-based multi-robot simulator for autonomous vehicle developers. carla - Open-source simulator for autonomous driving research. awesome-CARLA - A curated list of awesome CARLA tutorials, blogs, and related projects. ros-bridge - ROS bridge for CARLA Simulator. scenario_runner - Traffic scenario definition and execution engine. deepdive - End-to-end simulation for self-driving cars. uuv_simulator - Gazebo/ROS packages for underwater robotics simulation. AirSim - Open source simulator for autonomous vehicles built on Unreal Engine. self-driving-car-sim - A self-driving car simulator built with Unity. ROSIntegration - Unreal Engine Plugin to enable ROS Support. gym-gazebo - An OpenAI gym extension for using Gazebo known as gym-gazebo. highway-env - A collection of environments for autonomous driving and tactical decision-making tasks. VREP Interface - ROS Bridge for the VREP simulator. car_demo - This is a simulation of a Prius in gazebo 9 with sensor data being published using ROS kinetic. sumo - Eclipse SUMO is an open source, highly portable, microscopic and continuous road traffic simulation package designed to handle large road networks. open-simulation-interface - A generic interface for the environmental perception of automated driving functions in virtual scenarios. ESIM - An Open Event Camera Simulator. Menge - Crowd Simulation Framework. pedsim_ros - Pedestrian simulator powered by the social force model for Gazebo. opencrg - Open file formats and open source tools for the detailed description, creation and evaluation of road surfaces. esmini - A basic OpenSCENARIO player. OpenSceneGraph - An open source high performance 3D graphics toolkit, used by application developers in fields such as visual simulation, games, virtual reality, scientific visualization and modelling. morse - An academic robotic simulator, based on the Blender Game Engine and the Bullet Physics engine. ROSIntegrationVision - Support for ROS-enabled RGBD data acquisition in Unreal Engine Projects. fetch_gazebo - Contains the Gazebo simulation for Fetch Robotics Fetch and Freight Research Edition Robots. rotors_simulator - Provides some multirotor models. flow - A computational framework for deep RL and control experiments for traffic microsimulation. gnss-ins-sim - GNSS + inertial navigation, sensor fusion simulator. Motion trajectory generator, sensor models, and navigation. Ignition Robotics - Test control strategies in safety, and take advantage of simulation in continuous integration tests. simulation assets for the SubT - This collection contains simulation assets for the SubT Challenge Virtual Competition in Gazebo. gazebo_ros_motors - Contains currently two motor plugins for Gazebo, one with an ideal speed controller and one without a controller that models a DC motor. map2gazebo - ROS package for creating Gazebo environments from 2D maps. sim_vehicle_dynamics - Vehicle Dynamics Simulation Software of TUM Roborace Team. gym-carla - An OpenAI gym wrapper for CARLA simulator. simbody - High-performance C++ multibody dynamics/physics library for simulating articulated biomechanical and mechanical systems like vehicles, robots, and the human skeleton. gazebo_models - This repository holds the Gazebo model database. pylot - Autonomous driving platform running on the CARLA simulator. flightmare - Flightmare is composed of two main components: a configurable rendering engine built on Unity and a flexible physics engine for dynamics simulation. champ - ROS Packages for CHAMP Quadruped Controller. rex-gym - OpenAI Gym environments for an open-source quadruped robot (SpotMicro). Trick - Developed at the NASA Johnson Space Center, is a powerful simulation development framework that enables users to build applications for all phases of space vehicle development. usv_sim_lsa - Unmanned Surface Vehicle simulation on Gazebo with water current and winds. 42 - Simulation for spacecraft attitude control system analysis and design. Complete_Street_Rule - A scenario oriented design tool intended to enable users to quickly create procedurally generated multimodal streets in ArcGIS CityEngine. AutoCore simulation - Provides test environment for Autoware and still during early development, contents below may changed during updates. fields-ignition - Generate random crop fields for Ignition Gazebo. Electronics and Mechanics HRIM - An information model for robot hardware. URDF - Repository for Unified Robot Description Format (URDF) parsing code. phobos - An add-on for Blender allowing to create URDF, SDF and SMURF robot models in a WYSIWYG environment. urdf-viz - Visualize URDF/XACRO file, URDF Viewer works on Windows/macOS/Linux. solidworks_urdf_exporter - SolidWorks to URDF Exporter. FreeCAD - Your own 3D parametric modeler. kicad - A Cross Platform and Open Source Electronics Design Automation Suite. PcbDraw - Convert your KiCAD board into a nice looking 2D drawing suitable for pinout diagrams. kicad-3rd-party-tools - Tools made by others to augment the KiCad PCB EDA suite. PandaPower - An easy to use open source tool for power system modeling, analysis and optimization with a high degree of automation. LibrePCB - A powerful, innovative and intuitive EDA tool for everyone. openscad - A software for creating solid 3D CAD models. ngspice - A open source spice simulator for electric and electronic circuits. GNSS-SDR - GNSS-SDR provides interfaces for a wide range of radio frequency front-ends and raw sample file formats, generates processing outputs in standard formats. riscv - The Free and Open RISC Instruction Set Architecture. urdfpy - A simple and easy-to-use library for loading, manipulating, saving, and visualizing URDF files. FMPy - Simulate Functional Mockup Units (FMUs) in Python. FMIKit-Simulink - Import and export Functional Mock-up Units with Simulink. oemof-solph - A modular open source framework to model energy supply systems. NASA-3D-Resources - Here you'll find a growing collection of 3D models, textures, and images from inside NASA. SUAVE - An Aircraft Design Toolbox. opem - The Open-Source PEMFC Simulation Tool (OPEM) is a modeling tool for evaluating the performance of proton exchange membrane fuel cells. pvlib-python - A community supported tool that provides a set of functions and classes for simulating the performance of photovoltaic energy systems. WireViz - A tool for easily documenting cables, wiring harnesses and connector pinouts. Horizon - EDA is an Electronic Design Automation package supporting an integrated end-to-end workflow for printed circuit board design including parts management and schematic entry. tigl - The TiGL Geometry Library can be used for the computation and processing of aircraft geometries stored inside CPACS files. foxBMS - A free, open and flexible development environment to design battery management systems. cadCAD - A Python package that assists in the processes of designing, testing and validating complex systems through simulation, with support for Monte Carlo methods, A/B testing and parameter sweeping. OpenMDAO - An open-source framework for efficient multidisciplinary optimization. ODrive - The aim is to make it possible to use inexpensive brushless motors in high performance robotics projects. OpenTirePython - An open-source mathematical tire modelling library. Sensor Processing Calibration and Transformation tf2 - Transform library, which lets the user keep track of multiple coordinate frames over time. lidar_align - A simple method for finding the extrinsic calibration between a 3D lidar and a 6-dof pose sensor. kalibr - The Kalibr visual-inertial calibration toolbox. Calibnet - Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks. lidar_camera_calibration - ROS package to find a rigid-body transformation between a LiDAR and a camera. ILCC - Reflectance Intensity Assisted Automatic and Accurate Extrinsic Calibration of 3D LiDAR. easy_handeye - Simple, straighforward ROS library for hand-eye calibration. imu_utils - A ROS package tool to analyze the IMU performance. kalibr_allan - IMU Allan standard deviation charts for use with Kalibr and inertial kalman filters. pyquaternion - A full-featured Python module for representing and using quaternions. robot_calibration - This package offers calibration of a number of parameters of a robot, such as: 3D Camera intrinsics, extrinsics Joint angle offsets and robot frame offsets. multi_sensor_calibration - Contains a calibration tool to calibrate a sensor setup consisting of lidars, radars and cameras. LiDARTag - A Real-Time Fiducial Tag using Point Clouds Lidar Data. multicam_calibration - Extrinsic and intrinsic calbration of cameras. ikpy - An Inverse Kinematics library aiming performance and modularity. livox_camera_lidar_calibration - Calibrate the extrinsic parameters between Livox LiDAR and camera. lidar_camera_calibration - Camera LiDAR Calibration using ROS, OpenCV, and PCL. Perception Pipeline SARosPerceptionKitti - ROS package for the Perception (Sensor Processing, Detection, Tracking and Evaluation) of the KITTI Vision Benchmark Suite. multiple-object-tracking-lidar - C++ implementation to Detect, track and classify multiple objects using LIDAR scans or point cloud. cadrl_ros - ROS package for dynamic obstacle avoidance for ground robots trained with deep RL. AugmentedAutoencoder - RGB-based pipeline for object detection and 6D pose estimation. jsk_recognition - A stack for the perception packages which are used in JSK lab. GibsonEnv - Gibson Environments: Real-World Perception for Embodied Agents. morefusion - Multi-object Reasoning for 6D Pose Estimation from Volumetric Fusion. Machine Learning DLIB - A toolkit for making real world machine learning and data analysis applications in C++. fastai - The fastai library simplifies training fast and accurate neural nets using modern best practices. tpot - A Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. deap - Distributed Evolutionary Algorithms in Python. gym - A toolkit for developing and comparing reinforcement learning algorithms. tensorflow_ros_cpp - A ROS package that allows to do Tensorflow inference in C++ without the need to compile TF yourself. Tensorflow Federated - TensorFlow Federated (TFF) is an open-source framework for machine learning and other computations on decentralized data. finn - Fast, Scalable Quantized Neural Network Inference on FPGAs. neuropod - Neuropod is a library that provides a uniform interface to run deep learning models from multiple frameworks in C++ and Python. leela-zero - This is a fairly faithful reimplementation of the system described in the Alpha Go Zero paper \"Mastering the Game of Go without Human Knowledge\". Trax - A library for deep learning that focuses on sequence models and reinforcement learning. mlflow - A platform to streamline machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models. Netron - Visualizer for neural network, deep learning and machine learning models. MNN - A blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba. Tensorforce - An open-source deep reinforcement learning framework, with an emphasis on modularized flexible library design and straightforward usability for applications in research and practice. Dopamine - A research framework for fast prototyping of reinforcement learning algorithms. catalyst - Was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. ray - A fast and simple framework for building and running distributed applications. tf-agents - A reliable, scalable and easy to use TensorFlow library for Contextual Bandits and Reinforcement Learning. ReAgent - An open source end-to-end platform for applied reinforcement learning (RL) developed and used at Facebook. Awesome-Mobile-Machine-Learning - A curated list of awesome mobile machine learning resources for iOS, Android, and edge devices. cnn-explainer - Learning Convolutional Neural Networks with Interactive Visualization. modelzoo - A collection of machine-learned models for use in autonomous driving applications. Parallel Processing dask - Parallel computing with task scheduling for Python. cupy - NumPy-like API accelerated with CUDA. Thrust - A C++ parallel programming library which resembles the C++ Standard Library. ArrayFire - A general purpose GPU library. OpenMP - An application programming interface that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran. VexCL - VexCL is a C++ vector expression template library for OpenCL/CUDA/OpenMP. PYNQ - An open-source project from Xilinx that makes it easy to design embedded systems with Zynq All Programmable Systems on Chips. numba - NumPy aware dynamic Python compiler using LLVM. TensorRT - A C++ library for high performance inference on NVIDIA GPUs and deep learning accelerators. libcudacxx - Provides a heterogeneous implementation of the C++ Standard Library that can be used in and between CPU and GPU code. Image Processing CV-pretrained-model - A collection of computer vision pre-trained models. image_pipeline - Fills the gap between getting raw images from a camera driver and higher-level vision processing. gstreamer - A pipeline-based multimedia framework that links together a wide variety of media processing systems to complete complex workflows. ros2_openvino_toolkit - Provides a ROS-adaptered runtime framework of neural network which quickly deploys applications and solutions for vision inference. vision_visp - Wraps the ViSP moving edge tracker provided by the ViSP visual servoing library into a ROS package. apriltag_ros - A ROS wrapper of the AprilTag 3 visual fiducial detector. deep_object_pose - Deep Object Pose Estimation. DetectAndTrack - Detect-and-Track: Efficient Pose. SfMLearner - An unsupervised learning framework for depth and ego-motion estimation. imgaug - Image augmentation for machine learning experiments. vision_opencv - Packages for interfacing ROS with OpenCV, a library of programming functions for real time computer vision. darknet_ros - YOLO ROS: Real-Time Object Detection for ROS. ros_ncnn - YOLACT / YOLO ( among other things ) on NCNN inference engine for ROS. tf-pose-estimation - Deep Pose Estimation implemented using Tensorflow with Custom Architectures for fast inference. find-object - Simple Qt interface to try OpenCV implementations of SIFT, SURF, FAST, BRIEF and other feature detectors and descriptors. yolact - A simple, fully convolutional model for real-time instance segmentation. Kimera-Semantics - Real-Time 3D Semantic Reconstruction from 2D data. detectron2 - A next-generation research platform for object detection and segmentation. OpenVX - Enables performance and power-optimized computer vision processing, especially important in embedded and real-time use cases. 3d-vehicle-tracking - Official implementation of Joint Monocular 3D Vehicle Detection and Tracking. pysot - The goal of PySOT is to provide a high-quality, high-performance codebase for visual tracking research. semantic_slam - Real time semantic slam in ROS with a hand held RGB-D camera. kitti_scan_unfolding - We propose KITTI scan unfolding in our paper Scan-based Semantic Segmentation of LiDAR Point Clouds: An Experimental Study. packnet-sfm - Official PyTorch implementation of self-supervised monocular depth estimation methods invented by the ML Team at Toyota Research Institute (TRI). AB3DMOT - This work proposes a simple yet accurate real-time baseline 3D multi-object tracking system. monoloco - Official implementation of \"MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation\" in PyTorch. Poly-YOLO - Builds on the original ideas of YOLOv3 and removes two of its weaknesses: a large amount of rewritten labels and inefficient distribution of anchors. satellite-image-deep-learning - Resources for deep learning with satellite & aerial imagery. robosat - Semantic segmentation on aerial and satellite imagery. big_transfer - Model for General Visual Representation Learning created by Google Research. LEDNet - A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation. TorchSeg - This project aims at providing a fast, modular reference implementation for semantic segmentation models using PyTorch. simpledet - A Simple and Versatile Framework for Object Detection and Instance Recognition. meshroom - Meshroom is a free, open-source 3D Reconstruction Software based on the AliceVision Photogrammetric Computer Vision framework. EasyOCR - Ready-to-use Optical character recognition (OCR) with 40+ languages supported including Chinese, Japanese, Korean and Thai. pytracking - A general python framework for visual object tracking and video object segmentation, based on PyTorch. ros_deep_learning - Deep learning inference nodes for ROS with support for NVIDIA Jetson TX1/TX2/Xavier and TensorRT. hyperpose - HyperPose: A Flexible Library for Real-time Human Pose Estimation. fawkes - Privacy preserving tool against facial recognition systems. anonymizer - An anonymizer to obfuscate faces and license plates. opendatacam - Only saves surveyed meta-data, in particular the path an object moved or number of counted objects at a certain point. Cam2BEV - TensorFlow Implementation for Computing a Semantically Segmented Bird's Eye View (BEV) Image Given the Images of Multiple Vehicle-Mounted Cameras. satpy - Python package for earth-observing satellite data processing. flownet2-pytorch - Pytorch implementation of FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks. Simd - C++ image processing and machine learning library with using of SIMD: SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, AVX, AVX2, AVX-512, VMX(Altivec) and VSX(Power7), NEON for ARM. Radar Processing pyroSAR - Framework for large-scale SAR satellite data processing. CameraRadarFusionNet - TUM Roborace Team Software Stack - Path tracking control, velocity control, curvature control and state estimation. Lidar and Point Cloud Processing cilantro - A lean C++ library for working with point cloud data. open3d - Open3D: A Modern Library for 3D Data Processing. SqueezeSeg - Implementation of SqueezeSeg, convolutional neural networks for LiDAR point clout segmentation. point_cloud_io - ROS nodes to read and write point clouds from and to files (e.g. ply, vtk). python-pcl - Python bindings to the pointcloud library. libpointmatcher - An \"Iterative Closest Point\" library for 2-D/3-D mapping in Robotics. depth_clustering - Fast and robust clustering of point clouds generated with a Velodyne sensor. lidar-bonnetal - Semantic and Instance Segmentation of LiDAR point clouds for autonomous driving. CSF - LiDAR point cloud ground filtering / segmentation (bare earth extraction) method based on cloth simulation. robot_body_filter - A highly configurable LaserScan/PointCloud2 filter that allows to dynamically remove the 3D body of the robot from the measurements. grid_map - Universal grid map library for mobile robotic mapping. elevation_mapping - Robot-centric elevation mapping for rough terrain navigation. rangenet_lib - Contains simple usage explanations of how the RangeNet++ inference works with the TensorRT and C++ interface. pointcloud_to_laserscan - Converts a 3D Point Cloud into a 2D laser scan. octomap - An Efficient Probabilistic 3D Mapping Framework Based on Octrees. pptk - Point Processing Toolkit from HEREMaps. gpu-voxels - GPU-Voxels is a CUDA based library which allows high resolution volumetric collision detection between animated 3D models and live pointclouds from 3D sensors of all kinds. spatio_temporal_voxel_layer - A new voxel layer leveraging modern 3D graphics tools to modernize navigation environmental representations. LAStools - Award-winning software for efficient LiDAR processing. PCDet - A general PyTorch-based codebase for 3D object detection from point cloud. PDAL - A C++ BSD library for translating and manipulating point cloud data. PotreeConverter - Builds a potree octree from las, laz, binary ply, xyz or ptx files. fast_gicp - A collection of GICP-based fast point cloud registration algorithms. ndt_omp - Multi-threaded and SSE friendly NDT algorithm. laser_line_extraction - A ROS packages that extracts line segments from LaserScan messages. Go-ICP - Implementation of the Go-ICP algorithm for globally optimal 3D pointset registration. PointCNN - A simple and general framework for feature learning from point clouds. segmenters_lib - The LiDAR segmenters library, for segmentation-based detection. MotionNet - Joint Perception and Motion Prediction for Autonomous Driving Based on Bird's Eye View Maps. PolarSeg - An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation. traversability_mapping - Takes in point cloud from a Velodyne VLP-16 Lidar and outputs a traversability map for autonomous navigation in real-time. lidar_super_resolution - Simulation-based Lidar Super-resolution for Ground Vehicles. Cupoch - A library that implements rapid 3D data processing and robotics computation using CUDA. linefit_ground_segmentation - Implementation of the ground segmentation algorithm. Draco - A library for compressing and decompressing 3D geometric meshes and point clouds. Votenet - Deep Hough Voting for 3D Object Detection in Point Clouds. lidar_undistortion - Provides lidar motion undistortion based on an external 6DoF pose estimation input. superpoint_graph - Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs. RandLA-Net - Efficient Semantic Segmentation of Large-Scale Point Clouds. Det3D - A first 3D Object Detection toolbox which provides off the box implementations of many 3D object detection algorithms such as PointPillars, SECOND, PIXOR. OverlapNet - A modified Siamese Network that predicts the overlap and relative yaw angle of a pair of range images generated by 3D LiDAR scans. mp2p_icp - A repertory of multi primitive-to-primitive (MP2P) ICP algorithms in C++. OpenPCDet - A Toolbox for LiDAR-based 3D Object Detection. torch-points3d - Pytorch framework for doing deep learning on point clouds. PolyFit - Polygonal Surface Reconstruction from Point Clouds. mmdetection3d - Next-generation platform for general 3D object detection. gpd - Takes a point cloud as input and produces pose estimates of viable grasps as output. SalsaNext - Uncertainty-aware Semantic Segmentation of LiDAR Point Clouds for Autonomous Driving. Super-Fast-Accurate-3D-Object-Detection - Super Fast and Accurate 3D Object Detection based on 3D LiDAR Point Clouds (The PyTorch implementation). Localization and State Estimation evo - Python package for the evaluation of odometry and SLAM. robot_localization - A package of nonlinear state estimation nodes. fuse - General architecture for performing sensor fusion live on a robot. GeographicLib - A C++ library for geographic projections. ntripbrowser - A Python API for browsing NTRIP (Networked Transport of RTCM via Internet Protocol). imu_tools - IMU-related filters and visualizers. RTKLIB - A version of RTKLIB optimized for single and dual frequency low cost GPS receivers, especially u-blox receivers. gLAB - Performs precise modeling of GNSS observables (pseudorange and carrier phase) at the centimetre level, allowing standalone GPS positioning, PPP, SBAS and DGNSS. ai-imu-dr - Contains the code of our novel accurate method for dead reckoning of wheeled vehicles based only on an IMU. Kalman-and-Bayesian-Filters-in-Python - Kalman Filter book using Jupyter Notebook. mcl_3dl - A ROS node to perform a probabilistic 3-D/6-DOF localization system for mobile robots with 3-D LIDAR(s). se2lam - On-SE(2) Localization and Mapping for Ground Vehicles by Fusing Odometry and Vision. mmWave-localization-learning - ML-based positioning method from mmWave transmissions - with high accuracy and energy efficiency. dynamic_robot_localization - A ROS package that offers 3 DoF and 6 DoF localization using PCL and allows dynamic map update using OctoMap. eagleye - An open-source software for vehicle localization utilizing GNSS and IMU. python-sgp4 - Python version of the SGP4 satellite position library. PROJ - Cartographic Projections and Coordinate Transformations Library. rpg_trajectory_evaluation - Implements common used trajectory evaluation methods for visual(-inertial) odometry. pymap3d - Pure-Python (Numpy optional) 3D coordinate conversions for geospace ecef enu eci. Simultaneous Localization and Mapping Lidar loam_velodyne - Laser Odometry and Mapping (Loam) is a realtime method for state estimation and mapping using a 3D lidar. lio-mapping - Implementation of Tightly Coupled 3D Lidar Inertial Odometry and Mapping (LIO-mapping). A-LOAM - Advanced implementation of LOAM. Fast LOAM - Fast and Optimized Lidar Odometry And Mapping. LIO_SAM - Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping. cartographer_ros - Provides ROS integration for Cartographer. loam_livox - A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR. StaticMapping - Use LiDAR to map the static world. semantic_suma - Semantic Mapping using Surfel Mapping and Semantic Segmentation. slam_toolbox - Slam Toolbox for lifelong mapping and localization in potentially massive maps with ROS . maplab - An open visual-inertial mapping framework. hdl_graph_slam - An open source ROS package for real-time 6DOF SLAM using a 3D LIDAR. interactive_slam - In contrast to existing automatic SLAM packages, we with minimal human effort. LeGO-LOAM - Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain. pyslam - Contains a monocular Visual Odometry (VO) pipeline in Python. Kitware SLAM - LiDAR-only visual SLAM developped by Kitware, as well as ROS and ParaView wrappings for easier use. horizon_highway_slam - A robust, low drift, and real time highway SLAM package suitable for Livox Horizon lidar. mola - A Modular System for Localization and Mapping. DH3D - Deep Hierarchical 3D Descriptors for Robust Large-Scale 6DOF Relocalization. LaMa - LaMa is a C++11 software library for robotic localization and mapping. LIO-SAM - Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping. Scan Context - Global LiDAR descriptor for place recognition and long-term localization. Visual orb_slam_2_ros - A ROS implementation of ORB_SLAM2. orbslam-map-saving-extension - In this extensions the map of ORB-features be saved to the disk as a reference for future runs along the same track. dso - Direct Sparse Odometry. viso2 - A ROS wrapper for libviso2, a library for visual odometry. xivo - X Inertial-aided Visual Odometry. rovio - Robust Visual Inertial Odometry Framework. LSD-SLAM - Large-Scale Direct Monocular SLAM is a real-time monocular SLAM. CubeSLAM and ORB SLAM - Monocular 3D Object Detection and SLAM Package of CubeSLAM and ORB SLAM. VINS-Fusion - A Robust and Versatile Multi-Sensor Visual-Inertial State Estimator. openvslam - OpenVSLAM: A Versatile Visual SLAM Framework. basalt - Visual-Inertial Mapping with Non-Linear Factor Recovery. Kimera - A C++ library for real-time metric-semantic simultaneous localization and mapping, which uses camera images and inertial data to build a semantically annotated 3D mesh of the environment. tagslam - A ROS-based package for Simultaneous Localization and Mapping using AprilTag fiducial markers. LARVIO - A lightweight, accurate and robust monocular visual inertial odometry based on Multi-State Constraint Kalman Filter. fiducials - Simultaneous localization and mapping using fiducial markers. open_vins - An open source platform for visual-inertial navigation research. ORB_SLAM3 - ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM. Atlas - End-to-End 3D Scene Reconstruction from Posed Images. vilib - This library focuses on the front-end of VIO pipelines with CUDA. hloc - A modular toolbox for state-of-the-art 6-DoF visual localization. It implements Hierarchical Localization, leveraging image retrieval and feature matching, and is fast, accurate, and scalable. ESVO - A novel pipeline for real-time visual odometry using a stereo event-based camera. Vector Map OpenDRIVE - An open file format for the logical description of road networks. MapsModelsImporter - A Blender add-on to import models from google maps. Lanelet2 - Map handling framework for automated driving. barefoot - Online and Offline map matching that can be used stand-alone and in the cloud. iD - The easy-to-use OpenStreetMap editor in JavaScript. RapiD - An enhanced version of iD for mapping with AI created by Facebook. segmap - A map representation based on 3D segments. Mapbox - A JavaScript library for interactive, customizable vector maps on the web. osrm-backend - Open Source Routing Machine - C++ backend. assuremapingtools - Desktop based tool for viewing, editing and saving road network maps for autonomous vehicle platforms such as Autoware. geopandas - A project to add support for geographic data to pandas objects. MapToolbox - Plugins to make Autoware vector maps in Unity. imagery-index - An index of aerial and satellite imagery useful for mapping. mapillary_tools - A library for processing and uploading images to Mapillary. mapnik - Combines pixel-perfect image output with lightning-fast cartographic algorithms, and exposes interfaces in C++, Python, and Node. gdal - GDAL is an open source X/MIT licensed translator library for raster and vector geospatial data formats. grass - GRASS GIS - free and open source Geographic Information System (GIS). 3d-tiles - Specification for streaming massive heterogeneous 3D geospatial datasets. osmnx - Python for street networks. Retrieve, model, analyze, and visualize street networks and other spatial data from OpenStreetMap. Prediction Awesome-Interaction-aware-Trajectory-Prediction - A selection of state-of-the-art research materials on trajectory prediction. sgan - Socially Acceptable Trajectories with Generative Adversarial Networks. Behavior and Decision Groot - Graphical Editor to create BehaviorTrees. Compliant with BehaviorTree.CPP. BehaviorTree.CPP - Behavior Trees Library in C++. RAFCON - Uses hierarchical state machines, featuring concurrent state execution, to represent robot programs. ROSPlan - Generic framework for task planning in a ROS system. ad-rss-lib - Library implementing the Responsibility Sensitive Safety model (RSS) for Autonomous Vehicles. FlexBE - Graphical editor for hierarchical state machines, based on ROS's smach. sts_bt_library - This library provides the functionality to set up your own behavior tree logic by using the defined tree structures like Fallback, Sequence or Parallel Nodes. SMACC - An Event-Driven, Asynchronous, Behavioral State Machine Library for real-time ROS (Robotic Operating System) applications written in C++ . py_trees_ros - Behaviours, trees and utilities that extend py_trees for use with ROS. Planning and Control pacmod - Designed to allow the user to control a vehicle with the PACMod drive-by-wire system. mpcc - Model Predictive Contouring Controller for Autonomous Racing. rrt - C++ RRT (Rapidly-exploring Random Tree) implementation. HypridAStarTrailer - A path planning algorithm based on Hybrid A* for trailer truck. path_planner - Hybrid A* Path Planner for the KTH Research Concept Vehicle. open_street_map - ROS packages for working with Open Street Map geographic information. Open Source Car Control - An assemblage of software and hardware designs that enable computer control of modern cars in order to facilitate the development of autonomous vehicle technology. fastrack - A ROS implementation of Fast and Safe Tracking (FaSTrack). commonroad - Composable benchmarks for motion planning on roads. traffic-editor - A graphical editor for robot traffic flows. steering_functions - Contains a C++ library that implements steering functions for car-like robots with limited turning radius. moveit - Easy-to-use robotics manipulation platform for developing applications, evaluating designs, and building integrated products. flexible-collision-library - A library for performing three types of proximity queries on a pair of geometric models composed of triangles. aikido - Artificial Intelligence for Kinematics, Dynamics, and Optimization. casADi - A symbolic framework for numeric optimization implementing automatic differentiation in forward and reverse modes on sparse matrix-valued computational graphs. ACADO Toolkit - A software environment and algorithm collection for automatic control and dynamic optimization. control-toolbox - An efficient C++ library for control, estimation, optimization and motion planning in robotics. CrowdNav - Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning. ompl - Consists of many state-of-the-art sampling-based motion planning algorithms. openrave - Open Robotics Automation Virtual Environment: An environment for testing, developing, and deploying robotics motion planning algorithms. teb_local_planner - An optimal trajectory planner considering distinctive topologies for mobile robots based on Timed-Elastic-Bands. pinocchio - A fast and flexible implementation of Rigid Body Dynamics algorithms and their analytical derivatives. rmf_core - The rmf_core packages provide the centralized functions of the Robotics Middleware Framework (RMF). OpEn - A solver for Fast & Accurate Embedded Optimization for next-generation Robotics and Autonomous Systems. autogenu-jupyter - This project provides the continuation/GMRES method (C/GMRES method) based solvers for nonlinear model predictive control (NMPC) and an automatic code generator for NMPC. global_racetrajectory_optimization - This repository contains multiple approaches for generating global racetrajectories. toppra - A library for computing the time-optimal path parametrization for robots subject to kinematic and dynamic constraints. tinyspline - TinySpline is a small, yet powerful library for interpolating, transforming, and querying arbitrary NURBS, B-Splines, and Bzier curves. dual quaternions ros - ROS python package for dual quaternion SLERP. mb planner - Aerial vehicle planner for tight spaces. Used in DARPA SubT Challenge. ilqr - Iterative Linear Quadratic Regulator with auto-differentiatiable dynamics models. EGO-Planner - A lightweight gradient-based local planner without ESDF construction, which significantly reduces computation time compared to some state-of-the-art methods. pykep - A scientific library providing basic tools for research in interplanetary trajectory design. am_traj - Alternating Minimization Based Trajectory Generation for Quadrotor Aggressive Flight. GraphBasedLocalTrajectoryPlanner - Was used on a real race vehicle during the Roborace Season Alpha and achieved speeds above 200km/h. User Interaction Graphical User Interface imgui - Designed to enable fast iterations and to empower programmers to create content creation tools and visualization / debug tools. qtpy - Provides an uniform layer to support PyQt5, PySide2, PyQt4 and PySide with a single codebase. mir - Mir is set of libraries for building Wayland based shells. rqt - A Qt-based framework for GUI development for ROS. It consists of three parts/metapackages. cage - This is Cage, a Wayland kiosk. A kiosk runs a single, maximized application. chilipie - Easy-to-use Raspberry Pi image for booting directly into full-screen Chrome. pencil - A tool for making diagrams and GUI prototyping that everyone can use. dynamic_reconfigure - The focus of dynamic_reconfigure is on providing a standard way to expose a subset of a node's parameters to external reconfiguration. ddynamic_reconfigure - Allows modifying parameters of a ROS node using the dynamic_reconfigure framework without having to write cfg files. elements - A lightweight, fine-grained, resolution independent, modular GUI library. NanoGUI - A minimalistic cross-platform widget library for OpenGL 3.x or higher. Acoustic User Interface pyo - A Python module written in C containing classes for a wide variety of audio signal processing types. rhasspy - Rhasspy (pronounced RAH-SPEE) is an offline, multilingual voice assistant toolkit inspired by Jasper that works well with Home Assistant, Hass.io, and Node-RED. mycroft-core - Mycroft is a hackable open source voice assistant. DDSP - A library of differentiable versions of common DSP functions (such as synthesizers, waveshapers, and filters). NoiseTorch - Creates a virtual microphone that suppresses noise, in any application. DeepSpeech - An open source Speech-To-Text engine, using a model trained by machine learning techniques based on Baidu's Deep Speech research paper. waveglow - A Flow-based Generative Network for Speech Synthesis. Command Line Interface the-art-of-command-line - Master the command line, in one page. dotfiles of cornerman - Powerful zsh and vim dotfiles. dotbot - A tool that bootstraps your dotfiles. prompt-hjem - A beautiful zsh prompt. ag - A code-searching tool similar to ack, but faster. fzf - A command-line fuzzy finder. pkgtop - Interactive package manager and resource monitor designed for the GNU/Linux. asciimatics - A cross platform package to do curses-like operations, plus higher level APIs and widgets to create text UIs and ASCII art animations. gocui - Minimalist Go package aimed at creating Console User Interfaces. TerminalImageViewer - Small C++ program to display images in a (modern) terminal using RGB ANSI codes and unicode block graphics characters. rosshow - Visualize ROS topics inside a terminal with Unicode/ASCII art. python-prompt-toolkit - Library for building powerful interactive command line applications in Python. guake - Drop-down terminal for GNOME. wemux - Multi-User Tmux Made Easy. tmuxp - A session manager built on libtmux. mapscii - World map renderer for your console. terminator - The goal of this project is to produce a useful tool for arranging terminals. bat - A cat(1) clone with wings. fx - Command-line tool and terminal JSON viewer. tmate - Instant terminal sharing. Data Visualization and Mission Control xdot - Interactive viewer for graphs written in Graphviz's dot language. guacamole - Clientless remote desktop gateway. It supports standard protocols like VNC, RDP, and SSH. ros3djs - 3D Visualization Library for use with the ROS JavaScript Libraries. webviz - Web-based visualization libraries like rviz. plotly.py - An open-source, interactive graphing library for Python. PlotJuggler - The timeseries visualization tool that you deserve. bokeh - Interactive Data Visualization in the browser, from Python. voila - From Jupyter notebooks to standalone web applications and dashboards. Pangolin - Pangolin is a lightweight portable rapid development library for managing OpenGL display / interaction and abstracting video input. rqt_bag - Provides a GUI plugin for displaying and replaying ROS bag files. kepler.gl - Kepler.gl is a powerful open source geospatial analysis tool for large-scale data sets. qgis_ros - Access bagged and live topic data in a highly featured GIS environment. openmct - A web based mission control framework. web_video_server - HTTP Streaming of ROS Image Topics in Multiple Formats. RVizWeb - Provides a convenient way of building and launching a web application with features similar to RViz. marvros - MAVLink to ROS gateway with proxy for Ground Control Station. octave - Provides a convenient command line interface for solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with Matlab. streetscape.gl - Streetscape.gl is a toolkit for visualizing autonomous and robotics data in the XVIZ protocol. urdf-loaders - URDF Loaders for Unity and THREE.js with example ATHLETE URDF File. obs-studio - Free and open source software for live streaming and screen recording. Annotation labelbox - The fastest way to annotate data to build and ship artificial intelligence applications. PixelAnnotationTool - Annotate quickly images. LabelImg - A graphical image annotation tool and label object bounding boxes in images. cvat - Powerful and efficient Computer Vision Annotation Tool (CVAT). point_labeler - Tool for labeling of a single point clouds or a stream of point clouds. label-studio - Label Studio is a multi-type data labeling and annotation tool with standardized output format. napari - A fast, interactive, multi-dimensional image viewer for python. semantic-segmentation-editor - A web based labeling tool for creating AI training data sets (2D and 3D). 3d-bat - 3D Bounding Box Annotation Tool for Point cloud and Image Labeling. labelme - Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation). universal-data-tool - Collaborate & label any type of data, images, text, or documents, in an easy web interface or desktop app. BMW-Labeltool-Lite - Provides you with a easy to use labeling tool for State-of-the-art Deep Learning training purposes. Point Cloud CloudCompare - CloudCompare is a 3D point cloud (and triangular mesh) processing software. Potree - WebGL point cloud viewer for large datasets. point_cloud_viewer - Makes viewing massive point clouds easy and convenient. LidarView - Performs real-time visualization and easy processing of live captured 3D LiDAR data from Lidar sensors. VeloView - Performs real-time visualization of live captured 3D LiDAR data from Velodyne's HDL sensors. entwine - A data organization library for massive point clouds, designed to conquer datasets of trillions of points as well as desktop-scale point clouds. polyscope - A C++ & Python viewer for 3D data like meshes and point clouds. Pcx - Point cloud importer & renderer for Unity. ImmersivePoints - A web-application for virtual reality devices to explore 3D data in the most natural way possible. RViz mapviz - Modular ROS visualization tool for 2D data. rviz_cinematographer - Easy to use tools to create and edit trajectories for the rviz camera. rviz_satellite - Display internet satellite imagery in RViz. rviz_visual_tools - C++ API wrapper for displaying shapes and meshes in Rviz. xpp - Visualization of motion-plans for legged robots. rviz stereo - 3D stereo rendering displays a different view to each eye so that the scene appears to have depth. jsk_visualization - Jsk visualization ros packages for rviz and rqt. moveit_visual_tools - Helper functions for displaying and debugging MoveIt! data in Rviz via published markers. Operation System Monitoring rosmon - ROS node launcher & monitoring daemon. multimaster_fkie - GUI-based management environment that is very useful to manage ROS-launch configurations and control running nodes. collectd - A small daemon which collects system information periodically and provides mechanisms to store and monitor the values in a variety of ways. lnav - An enhanced log file viewer that takes advantage of any semantic information that can be gleaned from the files being viewed, such as timestamps and log levels. htop - An interactive text-mode process viewer for Unix systems. It aims to be a better 'top'. atop - System and process monitor for Linux with logging and replay function. psutil - Cross-platform lib for process and system monitoring in Python. gputil - A Python module for getting the GPU status from NVIDA GPUs using nvidia-smi programmically in Python. gpustat - A simple command-line utility for querying and monitoring GPU status. nvtop - NVIDIA GPUs htop like monitoring tool. spdlog - Very fast, header-only/compiled, C++ logging library. ctop - Top-like interface for container metrics. ntop - Web-based Traffic and Security Network Traffic Monitoring. jupyterlab-nvdashboard - A JupyterLab extension for displaying dashboards of GPU usage. Database and Record ncdu - Ncdu is a disk usage analyzer with an ncurses interface. borg - Deduplicating archiver with compression and authenticated encryption. bag-database - A server that catalogs bag files and provides a web-based UI for accessing them. marv-robotics - MARV Robotics is a powerful and extensible data management platform. kitti2bag - Convert KITTI dataset to ROS bag file the easy way. pykitti - Python tools for working with KITTI data. rosbag_editor - Create a rosbag from a given one, using a simple GUI. nextcloud - Nextcloud is a suite of client-server software for creating and using file hosting services. ros_type_introspection - Deserialize ROS messages that are unknown at compilation time. syncthing - A continuous file synchronization program. rqt_bag_exporter - Qt GUI to export ROS bag topics to files (CSV and/or video). xviz - A protocol for real-time transfer and visualization of autonomy data. kitti_to_rosbag - A Dataset tools for working with the KITTI dataset raw data and converting it to a ROS bag. Also allows a library for direct access to poses, velodyne scans, and images. ros_numpy - Tools for converting ROS messages to and from numpy arrays. kitti_ros - A ROS-based player to replay KiTTI dataset. DuckDB - An embeddable SQL OLAP Database Management System. Network Distributed File System sshfs - File system based on the SSH File Transfer Protocol. moosefs - A scalable distributed storage system. ceph - A distributed object, block, and file storage platform. nfs - A distributed file system protocol originally developed by Sun Microsystems. ansible-role-nfs - Installs NFS utilities on RedHat/CentOS or Debian/Ubuntu. Server Infrastructure and High Performance Computing mass - Self-service, remote installation of Windows, CentOS, ESXi and Ubuntu on real servers turns your data centre into a bare metal cloud. polyaxon - A platform for reproducing and managing the whole life cycle of machine learning and deep learning applications. localstack - A fully functional local AWS cloud stack. Develop and test your cloud & Serverless apps offline. nvidia-docker - Build and run Docker containers leveraging NVIDIA GPUs. kubeflow - Machine Learning Toolkit for Kubernetes. log-pilot - Collect logs for docker containers. traefik - The Cloud Native Edge Router. graylog2-server - Free and open source log management. ansible - Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy. pyinfra - It can be used for ad-hoc command execution, service deployment, configuration management and more. docker-py - A Python library for the Docker Engine API. noVNC - VNC client using HTML5. Slurm - Slurm: A Highly Scalable Workload Manager. jupyterhub - Multi-user server for Jupyter notebooks. Portainer - Making Docker management easy. enroot - A simple, yet powerful tool to turn traditional container/OS images into unprivileged sandboxes. docker-firefox - Run a Docker Container with Firefox and noVNC for remote access to headless servers. luigi - A Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in. triton-inference-server - NVIDIA Triton Inference Server provides a cloud inferencing solution optimized for NVIDIA GPUs. cudf - Provides a pandas-like API that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming. Embedded Operation System vxworks7-ros2-build - Build system to automate the build of VxWorks 7 and ROS2. Yocto - Produce tools and processes that enable the creation of Linux distributions for embedded software that are independent of the underlying architecture of the embedded hardware. Automotive Graded Linux - A collaborative open source project that is bringing together automakers, suppliers and technology companies to build a Linux-based, open software platform for automotive applications that can serve as the de facto industry standard. bitbake - A generic task execution engine that allows shell and Python tasks to be run efficiently and in parallel while working within complex inter-task dependency constraints. Jailhouse - Jailhouse is a partitioning Hypervisor based on Linux. Xen - An open-source (GPL) type-1 or baremetal hypervisor. QEMU - A generic and open source machine emulator and virtualizer. qemu-xilinx - A fork of Quick EMUlator (QEMU) with improved support and modelling for the Xilinx platforms. rosserial - A ROS client library for small, embedded devices, such as Arduino. meta-ros - OpenEmbedded Layer for ROS Applications. meta-balena - Run Docker containers on embedded devices. micro-ros - The major changes compared to \"regular\" ROS 2 is that micro-ROS uses a Real-Time Operating System (RTOS) instead of Linux, and DDS for eXtremely Resource Constrained Environments. nvidia-container-runtime - NVIDIA Container Runtime is a GPU aware container runtime, compatible with the Open Containers Initiative (OCI) specification used by Docker, CRI-O, and other popular container technologie. fusesoc - Package manager and build abstraction tool for FPGA/ASIC development. jetson_easy - Automatically script to setup and configure your NVIDIA Jetson. docker-jetpack-sdk - Allows for usage of the NVIDIA JetPack SDK within a docker container for download, flashing, and install. Pressed - Provides a way to set answers to questions asked during the installation process of debian, without having to manually enter the answers while the installation is running. jetson_stats - A package to monitoring and control your NVIDIA Jetson (Xavier NX, Nano, AGX Xavier, TX1, TX2) Works with all NVIDIA Jetson ecosystem. ros_jetson_stats - The ROS jetson-stats wrapper. The status of your NVIDIA jetson in diagnostic messages. OpenCR - Open-source Control Module for ROS. acrn-hypervisor - Defines a device hypervisor reference stack and an architecture for running multiple software subsystems, managed securely, on a consolidated system by means of a virtual machine manager. jetson-containers - Machine Learning Containers for Jetson and JetPack 4.4. Real-Time Kernel ELISA - Project is to make it easier for companies to build and certify Linux-based safety-critical applications systems whose failure could result in loss of human life, significant property damage or environmental damage. PREEMPT_RT kernel patch - Aim of the PREEMPT_RT kernel patch is to minimize the amount of kernel code that is non-preemptible. Network and Middleware performance_test - Tool to test the performance of pub/sub based communication frameworks. realtime_support - Minimal real-time testing utility for measuring jitter and latency. ros1_bridge - ROS 2 package that provides bidirectional communication between ROS 1 and ROS 2. Fast-RTPS - A Protocol, which provides publisher-subscriber communications over unreliable transports such as UDP, as defined and maintained by the Object Management Group (OMG) consortium. protobuf - Google's data interchange format. opensplice - Vortex OpenSplice Community Edition. cyclonedds - Eclipse Cyclone DDS is a very performant and robust open-source DDS implementation. iceoryx - An IPC middleware for POSIX-based systems. rosbridge_suite - Provides a JSON interface to ROS, allowing any client to send JSON to publish or subscribe to ROS topics, call ROS services, and more. ros2arduino - This library helps the Arduino board communicate with the ROS2 using XRCE-DDS. eCAL - The enhanced communication abstraction layer (eCAL) is a middleware that enables scalable, high performance interprocess communication on a single computer node or between different nodes in a computer network. AUTOSAR-Adaptive - The implementation of AUTOSAR Adaptive Platform based on the R19-11. ocpp - The Open Charge Point Protocol (OCPP) is a network protocol for communication between electric vehicle chargers and a central backoffice system. Ethernet and Wireless Networking SOES - SOES is an EtherCAT slave stack written in C. netplan - Simply create a YAML description of the required network interfaces and what each should be configured to do. airalab - AIRA is reference Robonomics network client for ROS-enabled cyber-physical systems. rdbox - RDBOX is a IT infrastructure for ROS robots. ros_ethercat - This is a reimplementation of the main loop of pr2_ethercat without dependencies on PR2 software. wavemon - An ncurses-based monitoring application for wireless network devices. wireless - Making info about wireless networks available to ROS. ptpd - PTP daemon (PTPd) is an implementation the Precision Time Protocol (PTP) version 2 as defined by 'IEEE Std 1588-2008'. PTP provides precise time coordination of Ethernet LAN connected computers. iperf - A TCP, UDP, and SCTP network bandwidth measurement tool. tcpreplay - Pcap editing and replay tools. nethogs - It groups bandwidth by process. pyshark - Python wrapper for tshark, allowing python packet parsing using wireshark dissectors. pingtop - Ping multiple servers and show results in a top-like terminal UI. termshark - A terminal UI for tshark, inspired by Wireshark. udpreplay - Replay UDP packets from a pcap file. openwifi - Linux mac80211 compatible full-stack IEEE802.11/Wi-Fi design based on Software Defined Radio. Controller Area Network AndrOBD - Android OBD diagnostics with any ELM327 adapter. ddt4all - DDT4All is a tool to create your own ECU parameters screens and connect to a CAN network with a cheap ELM327 interface. cabana - CAN visualizer and DBC maker. opendbc - The project to democratize access to the decoder ring of your car. libuavcan - An open lightweight protocol designed for reliable communication in aerospace and robotic applications over robust vehicular networks such as CAN bus. python-can - The can package provides controller area network support for Python developers. CANopenNode - The internationally standardized (EN 50325-4) (CiA301) CAN-based higher-layer protocol for embedded control system. python-udsoncan - Python implementation of UDS (ISO-14229) standard. uds-c - Unified Diagnostics Service (UDS) and OBD-II (On Board Diagnostics for Vehicles) C Library. cantools - CAN BUS tools in Python 3. CANdevStudio - CANdevStudio aims to be cost-effective replacement for CAN simulation software. It can work with variety of CAN hardware interfaces. can-utils - Linux-CAN / SocketCAN user space applications. ros_canopen - CANopen driver framework for ROS. decanstructor - The definitive ROS CAN analysis tool. kvaser_interface - This package was developed as a standardized way to access Kvaser CAN devices from ROS. canmatrix - Converting CAN Database Formats .arxml .dbc .dbf .kcd. autosar - A set of python modules for working with AUTOSAR XML files. canopen - A Python implementation of the CANopen standard. The aim of the project is to support the most common parts of the CiA 301 standard in a Pythonic interface. SavvyCAN - A Qt5 based cross platform tool which can be used to load, save, and capture canbus frames. Open-Vehicle-Monitoring-System-3 - The system provides live monitoring of vehicle metrics like state of charge, temperatures, tyre pressures and diagnostic fault conditions. Sensor and Acuator Interfaces Tesla-API - Provides functionality to monitor and control the Model S (and future Tesla vehicles) remotely. flirpy - A Python library to interact with FLIR thermal imaging cameras and images. nerian_stereo - ROS node for Nerian's SceneScan and SP1 stereo vision sensors. pymmw - This is a toolbox composed of Python scripts to interact with TI's evaluation module (BoosterPack) for the IWR1443 mmWave sensing device. ti_mmwave_rospkg - TI mmWave radar ROS driver (with sensor fusion and hybrid). pacmod3 - This ROS node is designed to allow the user to control a vehicle with the PACMod drive-by-wire system, board revision 3. ros2_intel_realsense - These are packages for using Intel RealSense cameras (D400 series) with ROS2. sick_scan - This stack provides a ROS2 driver for the SICK TiM series of laser scanners. ouster_example - Sample code for connecting to and configuring the OS1, reading and visualizing data, and interfacing with ROS. ros2_ouster_drivers - These are an implementation of ROS2 drivers for the Ouster OS-1 3D lidars. livox_ros_driver - A new ROS package, specially used to connect LiDAR products produced by Livox. velodyne - A collection of ROS packages supporting Velodyne high definition 3D LIDARs. ublox - Provides support for u-blox GPS receivers. crazyflie_ros - ROS Driver for Bitcraze Crazyflie. pointgrey_camera_driver - ROS driver for Pt. Grey cameras, based on the official FlyCapture2 SDK. novatel_gps_driver - ROS driver for NovAtel GPS / GNSS receivers. pylon-ros-camera - The official pylon ROS driver for Basler GigE Vision and USB3 Vision cameras. ethz_piksi_ros - Contains (python) ROS drivers, tools, launch files, and wikis about how to use Piksi Real Time Kinematic (RTK) GPS device in ROS. sick_safetyscanners - A ROS Driver which reads the raw data from the SICK Safety Scanners and publishes the data as a laser_scan msg. bosch_imu_driver - A driver for the sensor IMU Bosch BNO055. It was implemented only the UART communication interface (correct sensor mode should be selected). oxford_gps_eth - Ethernet interface to OxTS GPS receivers using the NCOM packet structure. ifm3d - Library and Utilities for working with ifm pmd-based 3D ToF Cameras. cepton_sdk_redist - Provides ROS support for Cepton LiDAR. jetson_csi_cam - A ROS package making it simple to use CSI cameras on the Nvidia Jetson TK1, TX1, or TX2 with ROS. ros_astra_camera - A ROS driver for Orbbec 3D cameras. spot_ros - ROS Driver for Spot. Security owasp-threat-dragon-desktop - Threat Dragon is a free, open-source, cross-platform threat modeling application including system diagramming and a rule engine to auto-generate threats/mitigations. launch_ros_sandbox - Can define launch files running nodes in restrained environments, such as Docker containers or separate user accounts with limited privileges. wolfssl - A small, fast, portable implementation of TLS/SSL for embedded devices to the cloud. CANalyzat0r - Security analysis toolkit for proprietary car protocols. RSF - Robot Security Framework (RSF) is a standardized methodology to perform security assessments in robotics. How-to-Secure-A-Linux-Server - An evolving how-to guide for securing a Linux server. lynis - Security auditing tool for Linux, macOS, and UNIX-based systems. Assists with compliance testing (HIPAA/ISO27001/PCI DSS) and system hardening. OpenVPN - An open source VPN daemon. openfortivpn - A client for PPP+SSL VPN tunnel services and compatible with Fortinet VPNs. WireGuard - WireGuard is a novel VPN that runs inside the Linux Kernel and utilizes state-of-the-art cryptography. ssh-auditor - Scans for weak ssh passwords on your network. vulscan - Advanced vulnerability scanning with Nmap NSE. nmap-vulners - NSE script based on Vulners.com API. brutespray - Automatically attempts default creds on found services. fail2ban - Daemon to ban hosts that cause multiple authentication errors. DependencyCheck - A software composition analysis utility that detects publicly disclosed vulnerabilities in application dependencies. Firejail - A SUID sandbox program that reduces the risk of security breaches by restricting the running environment of untrusted applications using Linux namespaces, seccomp-bpf and Linux capabilities. RVD - Robot Vulnerability Database. Community-contributed archive of robot vulnerabilities and weaknesses. ros2_dds_security - Adding security enhancements by defining a Service Plugin Interface (SPI) architecture, a set of builtin implementations of the SPIs, and the security model enforced by the SPIs. Security-Enhanced Linux - A Linux kernel security module that provides a mechanism for supporting access control security policies, including mandatory access controls (MAC). OpenTitan - Will make the silicon Root of Trust design and implementation more transparent, trustworthy, and secure for enterprises, platform providers, and chip manufacturers. OpenTitan is administered by lowRISC CIC as a collaborative project to produce high quality, open IP for instantiation as a full-featured product. bandit - A tool designed to find common security issues in Python code. hardening - A quick way to make a Ubuntu server a bit more secure. Passbolt - Passbolt is a free and open source password manager that allows team members to store and share credentials securely. gopass - A password manager for the command line written in Go. pass - The standard unix password manager. Vault - A tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, certificates, and more. legion - An open source, easy-to-use, super-extensible and semi-automated network penetration testing framework that aids in discovery, reconnaissance and exploitation of information systems. openscap - The oscap program is a command line tool that allows users to load, scan, validate, edit, and export SCAP documents. Datasets KITTI-360 - This large-scale dataset contains 320k images and 100k laser scans in a driving distance of 73.7km. waymo_ros - This is a ROS package to connect Waymo open dataset to ROS. waymo-open-dataset - The Waymo Open Dataset is comprised of high-resolution sensor data collected by Waymo self-driving cars in a wide variety of conditions. Ford Autonomous Vehicle Dataset - Ford presents a challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times. awesome-robotics-datasets - A collection of useful datasets for robotics and computer vision. nuscenes-devkit - The devkit of the nuScenes dataset. dataset-api - This is a repo of toolkit for ApolloScape Dataset, CVPR 2019 Workshop on Autonomous Driving Challenge and ECCV 2018 challenge. utbm_robocar_dataset - EU Long-term Dataset with Multiple Sensors for Autonomous Driving. DBNet - A Large-Scale Dataset for Driving Behavior Learning. argoverse-api - Official GitHub repository for Argoverse dataset. DDAD - A new autonomous driving benchmark from TRI (Toyota Research Institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions. pandaset-devkit - Public large-scale dataset for autonomous driving provided by Hesai & Scale. a2d2_to_ros - Utilities for converting A2D2 data sets to ROS bags. awesome-satellite-imagery-datasets - List of satellite image training datasets with annotations for computer vision and deep learning. sentinelsat - Search and download Copernicus Sentinel satellite images. adas-dataset-form - Thermal Dataset for Algorithm Training. h3d - The H3D is a large scale full-surround 3D multi-object detection and tracking dataset from Honda. Mapillary Vistas Dataset - A diverse street-level imagery dataset with pixelaccurate and instancespecific human annotations for understanding street scenes around the world. TensorFlow Datasets - TensorFlow Datasets provides many public datasets as tf.data.Datasets. racetrack-database - Contains center lines (x- and y-coordinates), track widths and race lines for over 20 race tracks (mainly F1 and DTM) all over the world. BlenderProc - A procedural Blender pipeline for photorealistic training image generation. Atlatec Sample Map Data - 3D map for autonomous driving and simulation created from nothing but two cameras and GPS in downtown San Francisco. Lyft Level 5 Dataset - Level 5 is developing a self-driving system for the Lyft network. We're collecting and processing data from our autonomous fleet and sharing it with you. holicity - A City-Scale Data Platform for Learning Holistic 3D Structures. UTD19 - Largest multi-city traffic dataset publically available. ASTYX HIRES2019 DATASET - Automotive Radar Dataset for Deep Learning Based 3D Object Detection. Objectron - A collection of short, object-centric video clips, which are accompanied by AR session metadata that includes camera poses, sparse point-clouds and characterization of the planar surfaces in the surrounding environment. ",
        "_version_": 1718536538803929088
      },
      {
        "story_id": 20325011,
        "story_author": "sohkamyung",
        "story_descendants": 70,
        "story_score": 193,
        "story_time": "2019-07-01T13:25:31Z",
        "story_title": "How to support open-source software and stay sane",
        "search": [
          "How to support open-source software and stay sane",
          "https://www.nature.com/articles/d41586-019-02046-0",
          "On 10 April, astrophysicists announced that they had captured the first ever image of a black hole. This was exhilarating news, but none of the giddy headlines mentioned that the image would have been impossible without open-source software. The image was created using Matplotlib, a Python library for graphing data, as well as other components of the open-source Python ecosystem. Just five days later, the US National Science Foundation (NSF) rejected a grant proposal to support that ecosystem, saying that the software lacked sufficient impact.Its a familiar problem: open-source software is widely acknowledged as crucially important in science, yet it is funded non-sustainably. Support work is often handled ad hoc by overworked graduate students and postdocs, and can lead to burnout. Its sort of the difference between having insurance and having a GoFundMe when their grandma goes to the hospital, says Anne Carpenter, a computational biologist at the Broad Institute of Harvard and MIT in Cambridge, Massachusetts, whose lab developed the image-analysis tool CellProfiler. Its just not a nice way to live.Scientists writing open-source software often lack formal training in software engineering, which means that they might never have learnt best practices for code documentation and testing. But poorly maintained software can waste time and effort, and hinder reproducibility. Biologists who use computational tools routinely spend hours and hours trying to get other researchers code to run, says Adam Siepel, a computational biologist at Cold Spring Harbor Laboratory in New York, and a maintainer of PHAST, a tool used for comparative and evolutionary genomics. They try to find it and theres no website, or the link is broken, or it no longer compiles, or crashes when theyve tried to run it on their data.But there are resources that can help, and models to emulate. If your research group is planning to release open-source software, you can prepare for the support work and the questions that will arise as others begin to use it. It isnt easy, but the effort can yield citations and name recognition for the developers, and improve efficiency in the field, says Wolfgang Huber, a computational biologist at the European Molecular Biology Laboratory in Heidelberg, Germany. Plus, he adds, I think its fun.Have a planFor developers of scientific software, release day isnt the end of the labour, but often the beginning. Tim Hopper, a data scientist at Cylance in Raleigh, North Carolina, says on Twitter, Give a man a fish and you feed him for a day. Write a program to fish for him and you maintain it for a lifetime. Carpenter hired a full-time software engineer to handle maintenance for CellProfiler, which logs about 700 questions and 100 bug reports or feature requests per year, or about 15 per week. But most open-source software maintenance is done on a volunteer basis. I did this myself, like after midnight, says Siepel of his tech-support efforts on PHAST.To prepare for whats coming, it helps to have an idea of what youre getting into. Some software will just need short-term support, whereas other programs might be used for decades. Nelle Varoquaux says that, in her field of machine learning in biology, software tools quickly become obsolete because the size of the data sets is changing so rapidly. Varoquaux is a computational biologist at the University of California, Berkeley, and co-developer of scikitlearn, a machine-learning package for Python. When I started my PhD, everything I worked on fitted into RAM, and I never had a memory problem, she says. But today, memory is a huge challenge. She estimates that she will need to maintain two tools she built for analysing DNA and chromosome conformation iced and pastis for only another five years before they become obsolete.Obsolescence isnt bad, she adds: knowing when to stop supporting software is an important skill. Let a tool die when it has reached the end of its usefulness or, when a maintainer wants to quit, orphan it and search for a foster parent, Huber advises.However long your software will be used for, good software-engineering practices and documentation are essential, says Andreas Mueller, a machine-learning scientist at Columbia University in New York City. These include continuous integration systems (such as TravisCI), version control (Git) and unit testing. Continuous integration tells you, every time you change your code, if it still works or if you broke it, as long as you write the correct tests for it to run, says Mueller; version control is a system of recording changes to source code so that you can revert to any previous version if necessary; and unit testing tests each individual component of the software to ensure that it is sound. The combination, he says, will 100% save you time. Organizations such as volunteer-run Software Carpentry and the eScience Institute at the University of Washington, Seattle, host bootcamps on software development, and make tutorials available on GitHub. The Netherlands eScience Center in Amsterdam provides a guide to software-development best practices at https://guide.esciencecenter.nl.To facilitate maintenance, Varoquaux recommends focusing on code readability over peak performance. I always try to make it readable and well-documented and tested, so if something breaks I can fix it quickly, she says.And thats inevitable when it comes to software: As soon as you have users, theyre going to find bugs, Varoquaux says. Huber recommends fielding user questions through a public forum, such as Stack Overflow, where users can tag their question with the software name. Do not respond to private mails for support from users, he says. Public forums offer three advantages. First, they reach many more users than do individual e-mails. For everybody who writes an e-mail, theres probably 100 people who are too shy to ask, says Huber. Second, they tend to encourage more focused and thoughtful questions. Third, they dissuade users from the time-wasting strategy of e-mailing multiple software maintainers separately with the same question.Huber also recommends releasing your software to a repository such as the Comprehensive R Archive Network (CRAN) or Bioconductor, an umbrella archive for biological software written in R, instead of to your personal home page or GitHub. Such repositories are curated, and have submission guidelines for naming conventions and required components, much as scientific journals do. And both CRAN and Bioconductor offer testing and continuous integration on several platforms, and robust, easy-to-use installers, says Huber.A matter of fundingSoftware support requires both time and money. But funding can be hard to come by. In the United States, the National Institutes of Health (NIH) and the NSF focus on new research, and the maintenance of open-source software often doesnt fit well into their requirements. Thats really the tragedy of the funding agencies in general, says Carpenter. Theyll fund 50 different groups to make 50 different algorithms, but they wont pay for one software engineer.But some funding does exist from these and other organizations. One Twitter thread (see go.nature.com/2yekao5) documents grants from the NSFs Division of Biological Infrastructure, the NIHs National Human Genome Research Institute and the National Cancer Institute, and a joint programme from the NSF and the UK Biotechnology and Biological Sciences Research Council (now part of UK Research and Innovation). Private US foundations such as the Gordon and Betty Moore Foundation, the Alfred P. Sloan Foundation and the Chan Zuckerberg Initiative (CZI) also fund open-source software support. The CZI provides support for the Python-based image-processing software scikit-image, the ImageJ and Fiji platforms, and also funds the software engineer on Carpenters team.In the United Kingdom, the Software Sustainability Institute, based at the University of Edinburgh, provides free, short, online evaluations of software sustainability, and fellowships of 3,000 ($US3,800) for researchers based in Britain or their collaborators. The institute periodically makes slots available for people to work with their experts for up to six months to develop new software or sharpen existing software and maintenance practices. In Germany, Huber recommends the European Commissions network grants and the German ministry of sciences deNBI initiative, both of which provide funding for Bioconductor.The general problem of digital-infrastructure maintenance is gaining more attention. Varoquaux and her colleagues have received $138,000 from the Alfred P. Sloan and Ford foundations to study the visible and invisible work of maintaining open-source software, she says, including burnout in researchers who devote their time to this work part of a portfolio of 13 digital-infrastructure research projects funded to the tune of $1.3 million. In May, the CZI announced three requests for proposals to fund open-source biomedical software, the first of which opened in June. Siepel has a review article in the press in Genome Biology on the challenge of funding open-source software support.And funding is needed: writing software that is easy for others to use on a wide range of data takes much more effort than software that works only for you. The difference is at least as large as between the polished paper published in Nature and the first stack of slides for a lab meeting with the underlying results, Huber says.Still, theres real value in the exercise. Siepels team sometimes responds to user queries by pointing out that theyre applying the software to the wrong data, a subtlety that an evolutionary biologist would notice but a software engineer might not. Theres a sort of idiom: eat your own dog food, Huber says: If you use your own software for real questions, then you realize where its bad, where its lacking. Having a domain expert write the software tends to make the software more valuable. "
        ],
        "story_type": "Normal",
        "url_raw": "https://www.nature.com/articles/d41586-019-02046-0",
        "comments.comment_id": [20327021, 20330323],
        "comments.comment_author": ["boron1006", "a3_nm"],
        "comments.comment_descendants": [6, 2],
        "comments.comment_time": [
          "2019-07-01T16:33:04Z",
          "2019-07-01T22:23:51Z"
        ],
        "comments.comment_text": [
          "Great article, and fantastic to see a spotlight on an issue that I've thought a lot about.<p>The sad part is that to a lot of scientists and researchers, software/software engineers isn't something worth paying for. It's not uncommon to see \"programmer\" jobs that are looking for 3+ years of experience that offer <$15 dollars an hour in the US. Sometimes they're \"volunteer intern\" positions. Of course the people who end up filling these positions aren't usually actual developers, so the software gets built poorly, eventually gets scrapped, and the cycle continues.<p>Management also hasn't really evolved past the 90's. Non-technical scientists often want 100% of control and to make each decision, but don't want to spend any time on it. This means developers often have little to no specs to work with, but spend all of their time guessing about what the scientists want, and having to go back and fix everything after.<p>>“That’s really the tragedy of the funding agencies in general,” says Carpenter. “They’ll fund 50 different groups to make 50 different algorithms, but they won’t pay for one software engineer.”<p>This is the crux of my frustration. It's not even 50 different algorithms often. A lot of the time, 50 different research groups will be working on very similar programs, and none will be able to deliver a working version.<p>Though the article mentions that research funding does exist, clicking on one of those funding pages and looking through their examples reveals that only ~1/10 of their websites are actually still active, and they aren't old sites. Again this goes back to the whole \"scientists don't value software thing\". I've seen scientists happily sign off on spending $20,000+ on hardware components that would usually cost <$100 to make, but balk at contributing $50 yearly to support open source.<p>I got lucky that I managed to find a place where I get paid fairly, and my boss is actually technical and can manage tech projects well, but these places are few and far between.",
          "Argh, this article is going to give researchers the misleading impression that releasing code as open-source is complicated, that you need to maintain it, ask yourself difficult questions, be an expert in programming, find funding to keep it around, etc.<p>But in my field, in most cases the source code is never released at all. That's a far bigger problem that not having support to use it.<p>So fellow academics, please don't use this article as an excuse to not release your code. When in doubt, just push the thing to Gitlab as is, add a README that says \"This is research code for paper X and it is unmaintained.\", and disappear. It's not ideal, it's not the best way to do science -- but it's much better than not releasing your code.<p>Related: the CRAPL <a href=\"http://matt.might.net/articles/crapl/\" rel=\"nofollow\">http://matt.might.net/articles/crapl/</a>"
        ],
        "id": "e4ca2b5e-f45c-4e34-aa80-61510d2967b5",
        "url_text": "On 10 April, astrophysicists announced that they had captured the first ever image of a black hole. This was exhilarating news, but none of the giddy headlines mentioned that the image would have been impossible without open-source software. The image was created using Matplotlib, a Python library for graphing data, as well as other components of the open-source Python ecosystem. Just five days later, the US National Science Foundation (NSF) rejected a grant proposal to support that ecosystem, saying that the software lacked sufficient impact.Its a familiar problem: open-source software is widely acknowledged as crucially important in science, yet it is funded non-sustainably. Support work is often handled ad hoc by overworked graduate students and postdocs, and can lead to burnout. Its sort of the difference between having insurance and having a GoFundMe when their grandma goes to the hospital, says Anne Carpenter, a computational biologist at the Broad Institute of Harvard and MIT in Cambridge, Massachusetts, whose lab developed the image-analysis tool CellProfiler. Its just not a nice way to live.Scientists writing open-source software often lack formal training in software engineering, which means that they might never have learnt best practices for code documentation and testing. But poorly maintained software can waste time and effort, and hinder reproducibility. Biologists who use computational tools routinely spend hours and hours trying to get other researchers code to run, says Adam Siepel, a computational biologist at Cold Spring Harbor Laboratory in New York, and a maintainer of PHAST, a tool used for comparative and evolutionary genomics. They try to find it and theres no website, or the link is broken, or it no longer compiles, or crashes when theyve tried to run it on their data.But there are resources that can help, and models to emulate. If your research group is planning to release open-source software, you can prepare for the support work and the questions that will arise as others begin to use it. It isnt easy, but the effort can yield citations and name recognition for the developers, and improve efficiency in the field, says Wolfgang Huber, a computational biologist at the European Molecular Biology Laboratory in Heidelberg, Germany. Plus, he adds, I think its fun.Have a planFor developers of scientific software, release day isnt the end of the labour, but often the beginning. Tim Hopper, a data scientist at Cylance in Raleigh, North Carolina, says on Twitter, Give a man a fish and you feed him for a day. Write a program to fish for him and you maintain it for a lifetime. Carpenter hired a full-time software engineer to handle maintenance for CellProfiler, which logs about 700 questions and 100 bug reports or feature requests per year, or about 15 per week. But most open-source software maintenance is done on a volunteer basis. I did this myself, like after midnight, says Siepel of his tech-support efforts on PHAST.To prepare for whats coming, it helps to have an idea of what youre getting into. Some software will just need short-term support, whereas other programs might be used for decades. Nelle Varoquaux says that, in her field of machine learning in biology, software tools quickly become obsolete because the size of the data sets is changing so rapidly. Varoquaux is a computational biologist at the University of California, Berkeley, and co-developer of scikitlearn, a machine-learning package for Python. When I started my PhD, everything I worked on fitted into RAM, and I never had a memory problem, she says. But today, memory is a huge challenge. She estimates that she will need to maintain two tools she built for analysing DNA and chromosome conformation iced and pastis for only another five years before they become obsolete.Obsolescence isnt bad, she adds: knowing when to stop supporting software is an important skill. Let a tool die when it has reached the end of its usefulness or, when a maintainer wants to quit, orphan it and search for a foster parent, Huber advises.However long your software will be used for, good software-engineering practices and documentation are essential, says Andreas Mueller, a machine-learning scientist at Columbia University in New York City. These include continuous integration systems (such as TravisCI), version control (Git) and unit testing. Continuous integration tells you, every time you change your code, if it still works or if you broke it, as long as you write the correct tests for it to run, says Mueller; version control is a system of recording changes to source code so that you can revert to any previous version if necessary; and unit testing tests each individual component of the software to ensure that it is sound. The combination, he says, will 100% save you time. Organizations such as volunteer-run Software Carpentry and the eScience Institute at the University of Washington, Seattle, host bootcamps on software development, and make tutorials available on GitHub. The Netherlands eScience Center in Amsterdam provides a guide to software-development best practices at https://guide.esciencecenter.nl.To facilitate maintenance, Varoquaux recommends focusing on code readability over peak performance. I always try to make it readable and well-documented and tested, so if something breaks I can fix it quickly, she says.And thats inevitable when it comes to software: As soon as you have users, theyre going to find bugs, Varoquaux says. Huber recommends fielding user questions through a public forum, such as Stack Overflow, where users can tag their question with the software name. Do not respond to private mails for support from users, he says. Public forums offer three advantages. First, they reach many more users than do individual e-mails. For everybody who writes an e-mail, theres probably 100 people who are too shy to ask, says Huber. Second, they tend to encourage more focused and thoughtful questions. Third, they dissuade users from the time-wasting strategy of e-mailing multiple software maintainers separately with the same question.Huber also recommends releasing your software to a repository such as the Comprehensive R Archive Network (CRAN) or Bioconductor, an umbrella archive for biological software written in R, instead of to your personal home page or GitHub. Such repositories are curated, and have submission guidelines for naming conventions and required components, much as scientific journals do. And both CRAN and Bioconductor offer testing and continuous integration on several platforms, and robust, easy-to-use installers, says Huber.A matter of fundingSoftware support requires both time and money. But funding can be hard to come by. In the United States, the National Institutes of Health (NIH) and the NSF focus on new research, and the maintenance of open-source software often doesnt fit well into their requirements. Thats really the tragedy of the funding agencies in general, says Carpenter. Theyll fund 50 different groups to make 50 different algorithms, but they wont pay for one software engineer.But some funding does exist from these and other organizations. One Twitter thread (see go.nature.com/2yekao5) documents grants from the NSFs Division of Biological Infrastructure, the NIHs National Human Genome Research Institute and the National Cancer Institute, and a joint programme from the NSF and the UK Biotechnology and Biological Sciences Research Council (now part of UK Research and Innovation). Private US foundations such as the Gordon and Betty Moore Foundation, the Alfred P. Sloan Foundation and the Chan Zuckerberg Initiative (CZI) also fund open-source software support. The CZI provides support for the Python-based image-processing software scikit-image, the ImageJ and Fiji platforms, and also funds the software engineer on Carpenters team.In the United Kingdom, the Software Sustainability Institute, based at the University of Edinburgh, provides free, short, online evaluations of software sustainability, and fellowships of 3,000 ($US3,800) for researchers based in Britain or their collaborators. The institute periodically makes slots available for people to work with their experts for up to six months to develop new software or sharpen existing software and maintenance practices. In Germany, Huber recommends the European Commissions network grants and the German ministry of sciences deNBI initiative, both of which provide funding for Bioconductor.The general problem of digital-infrastructure maintenance is gaining more attention. Varoquaux and her colleagues have received $138,000 from the Alfred P. Sloan and Ford foundations to study the visible and invisible work of maintaining open-source software, she says, including burnout in researchers who devote their time to this work part of a portfolio of 13 digital-infrastructure research projects funded to the tune of $1.3 million. In May, the CZI announced three requests for proposals to fund open-source biomedical software, the first of which opened in June. Siepel has a review article in the press in Genome Biology on the challenge of funding open-source software support.And funding is needed: writing software that is easy for others to use on a wide range of data takes much more effort than software that works only for you. The difference is at least as large as between the polished paper published in Nature and the first stack of slides for a lab meeting with the underlying results, Huber says.Still, theres real value in the exercise. Siepels team sometimes responds to user queries by pointing out that theyre applying the software to the wrong data, a subtlety that an evolutionary biologist would notice but a software engineer might not. Theres a sort of idiom: eat your own dog food, Huber says: If you use your own software for real questions, then you realize where its bad, where its lacking. Having a domain expert write the software tends to make the software more valuable. ",
        "_version_": 1718536496836771840
      },
      {
        "story_id": 19805938,
        "story_author": "soheilpro",
        "story_descendants": 45,
        "story_score": 288,
        "story_time": "2019-05-02T08:48:22Z",
        "story_title": "GitHub Learning Lab",
        "search": [
          "GitHub Learning Lab",
          "https://lab.github.com/",
          "Our most popular courses Introduction to GitHub The GitHub Training Team If you are looking for a quick and fun introduction to GitHub, you've found it. This class will get you started using GitHub in less than an hour. Git GitHub Pages Branches Commits Pull Requests Learning should be fun There are no simulations or boring tutorials here, just hands-on lessons created with by the GitHub community and taught by the friendly Learning Lab bot. Real projects Learn new skills while working in your own copy of a real project. Helpful bot Our friendly bot provides instructions and feedback throughout your journey. Real workflow Everything happens in GitHub Issues and Pull Requests. Our Learning Paths First Day on GitHub The GitHub Training Team Welcome to GitHub! We're so glad you're here. We know it can look overwhelming at first, so we've put together a few of our favorite courses for people logging in for the first time What is GitHub? Introduction to GitHub Git Handbook First Week on GitHub The GitHub Training Team After you've mastered the basics, learn some of the fun things you can do on GitHub. From GitHub Pages to building projects with your friends, this path will give you plenty of new ideas. Discover GitHub Pages GitHub Pages Reviewing pull requests GitHub Actions: Hello World GitHub Actions: Continuous Integration GitHub Actions: Publish to GitHub Packages Learn GitHub with GitHub Uploading your project to GitHub The GitHub Training Team Youre an upload away from using a full suite of development tools and premier third-party apps on GitHub. This course helps you seamlessly upload your code to GitHub and introduces you to exciting next steps to elevate your project. Languages and Tools Introduction to HTML The GitHub Training Team If you are looking for a quick and fun introduction to the exciting world of programming, this course is for you. Learn fundamental HTML skills and build your first webpage in less than an hour. Introduction to Node with Express everydeveloper Node.js gives you the ability to run JavaScript files on the server-side. Express is a library for Node.js, that allows you to make requests to different \"endpoints\" and get a response back. Node Express JavaScript JSON API Intermediate NodeJS Course everydeveloper This tutorial expands on concepts in the intro to Node.js and Express.js course. You will learn how to use a database (MongoDB) to Create, Read, Update, and Delete data. node.js express.js mongoose.js JavaScript MongoDB Introduction to PHP everydeveloper PHP is a server-side programming language that can insert dynamic code into your HTML. PHP is used in popular content management systems, such as WordPress and Drupal. Notating with LilyPond gitmusical LilyPond is an open source technology for notating music in plain text files. In this course, we'll cover the fundamentals of music notation in LilyPond. GitHub Actions DevOps with GitHub CodeQL U-Boot Challenge (C/C++) The GitHub Training Team Learn to use CodeQL, a query language that helps find bugs in source code. Find 9 remote code execution vulnerabilities in the open-source project Das U-Boot, and join the growing community of security researchers using CodeQL. Enterprise on GitHub InnerSource fundamentals The GitHub Training Team Organizations of all sizes and in all industries are chatting about InnerSource concepts. This course walks you through some of the key concepts of InnerSource and helps you build up an internal toolkit for adopting InnerSource practices. Create an open source program The GitHub Training Team Learn how to work alongside the open source communities that build software you're already using, and put your business at the forefront of the world's most innovative and secure code. Open source Enterprise Licensing Templates Guidelines "
        ],
        "story_type": "Normal",
        "url_raw": "https://lab.github.com/",
        "url_text": "Our most popular courses Introduction to GitHub The GitHub Training Team If you are looking for a quick and fun introduction to GitHub, you've found it. This class will get you started using GitHub in less than an hour. Git GitHub Pages Branches Commits Pull Requests Learning should be fun There are no simulations or boring tutorials here, just hands-on lessons created with by the GitHub community and taught by the friendly Learning Lab bot. Real projects Learn new skills while working in your own copy of a real project. Helpful bot Our friendly bot provides instructions and feedback throughout your journey. Real workflow Everything happens in GitHub Issues and Pull Requests. Our Learning Paths First Day on GitHub The GitHub Training Team Welcome to GitHub! We're so glad you're here. We know it can look overwhelming at first, so we've put together a few of our favorite courses for people logging in for the first time What is GitHub? Introduction to GitHub Git Handbook First Week on GitHub The GitHub Training Team After you've mastered the basics, learn some of the fun things you can do on GitHub. From GitHub Pages to building projects with your friends, this path will give you plenty of new ideas. Discover GitHub Pages GitHub Pages Reviewing pull requests GitHub Actions: Hello World GitHub Actions: Continuous Integration GitHub Actions: Publish to GitHub Packages Learn GitHub with GitHub Uploading your project to GitHub The GitHub Training Team Youre an upload away from using a full suite of development tools and premier third-party apps on GitHub. This course helps you seamlessly upload your code to GitHub and introduces you to exciting next steps to elevate your project. Languages and Tools Introduction to HTML The GitHub Training Team If you are looking for a quick and fun introduction to the exciting world of programming, this course is for you. Learn fundamental HTML skills and build your first webpage in less than an hour. Introduction to Node with Express everydeveloper Node.js gives you the ability to run JavaScript files on the server-side. Express is a library for Node.js, that allows you to make requests to different \"endpoints\" and get a response back. Node Express JavaScript JSON API Intermediate NodeJS Course everydeveloper This tutorial expands on concepts in the intro to Node.js and Express.js course. You will learn how to use a database (MongoDB) to Create, Read, Update, and Delete data. node.js express.js mongoose.js JavaScript MongoDB Introduction to PHP everydeveloper PHP is a server-side programming language that can insert dynamic code into your HTML. PHP is used in popular content management systems, such as WordPress and Drupal. Notating with LilyPond gitmusical LilyPond is an open source technology for notating music in plain text files. In this course, we'll cover the fundamentals of music notation in LilyPond. GitHub Actions DevOps with GitHub CodeQL U-Boot Challenge (C/C++) The GitHub Training Team Learn to use CodeQL, a query language that helps find bugs in source code. Find 9 remote code execution vulnerabilities in the open-source project Das U-Boot, and join the growing community of security researchers using CodeQL. Enterprise on GitHub InnerSource fundamentals The GitHub Training Team Organizations of all sizes and in all industries are chatting about InnerSource concepts. This course walks you through some of the key concepts of InnerSource and helps you build up an internal toolkit for adopting InnerSource practices. Create an open source program The GitHub Training Team Learn how to work alongside the open source communities that build software you're already using, and put your business at the forefront of the world's most innovative and secure code. Open source Enterprise Licensing Templates Guidelines ",
        "comments.comment_id": [19806284, 19808784],
        "comments.comment_author": ["tomovo", "diminish"],
        "comments.comment_descendants": [3, 5],
        "comments.comment_time": [
          "2019-05-02T10:06:57Z",
          "2019-05-02T15:20:48Z"
        ],
        "comments.comment_text": [
          "Not trying to weaken the GitLab brand at all... /s",
          "A Very good SEO move, to stir some of the 'lab' traffic away from gitlab."
        ],
        "id": "239e5598-69e6-4b29-ad35-d44f1f6ec536",
        "_version_": 1718536478834819073
      },
      {
        "story_id": 20920555,
        "story_author": "edmorley",
        "story_descendants": 369,
        "story_score": 703,
        "story_time": "2019-09-09T17:55:26Z",
        "story_title": "Running GitHub on Rails 6.0",
        "search": [
          "Running GitHub on Rails 6.0",
          "https://github.blog/2019-09-09-running-github-on-rails-6-0/",
          "On August 26, 2019, the GitHub application was deployed to production with 100 percent of traffic on the newest Rails version: 6.0. This change came just 1.5 weeks after the final release of Rails 6.0. Rails upgrades arent always something companies announce, but looking back at GitHubs history of being on a custom fork of Rails 3.2, this upgrade is a big deal. It represents how far weve come in the last few years, and the hard work and dedication from our upgrade team made it smoother, easier, and faster than any of our previous upgrades. At GitHub, we have a lot to celebrate with the release of Rails 6.0 and the subsequent production deploy. First, we were more involved in this release than we have been in any previous release of Rails. GitHub engineers sent over 100 pull requests to Rails 6.0 to improve documentation, fix bugs, add features, and speed up performance. For many GitHub contributors, this was the first time sending changes to the Rails framework, demonstrating that upgrading Rails not only helps GitHub internally, but also improves our developer community as well. Second, we deployed Rails 6.0 to production without any negative impact to customerswe had only one Rails 6.0 exception occur during testing, and it was hit by a bot! We were able to achieve this level of stability for the upgrade because we were heavily involved with its development. As soon as we finished the Rails 5.2 upgrade last year, we started upgrading our application to Rails 6.0. Instead of waiting for the final release, wed upgrade every week by pulling in the latest changes from Rails master and run all of our tests against that new version. This allowed us to find regressions quickly and earlyoften finding regressions in Rails master just hours after they were introduced. Upgrading weekly made it easy to find where these regressions were introduced since we were bisecting Rails with only a weeks worth of commits instead of more than a year of commits. Once our build for Rails 6.0 was green, wed merge the pull request to master, and all new code that went into GitHub would need to pass in Rails 5.2 and the newest master build of Rails. Upgrading every week worked so well that well continue using this process for upgrading from 6.0 to 6.1. In addition to ensuring that Rails 6.0 was stable, we also contributed to the new features of the framework like parallel testing and multiple databases. The code for these tools is used in our production application every dayits well-tested, GitHub-approved code in a public, open source framework. By upstreaming this tooling, were able to reduce complexity in our code base and set a standard where so many companies once had to implement this functionality on their own. There are so many wins to staying upgraded that go beyond more security, faster performance, and new features. By staying current with Rails master, were influencing the future of the framework to meet our needs and giving back to the open source community in big ways. This process means that the GitHub code base evolves alongside Rails instead of in response to Rails. Investing in our application by staying up to date with the Rails framework has had a tremendous positive effect on our code base and engineering teams. Staying current allows us to invest in our community, invest in our tools for the long term, and improve the experience of working with the GitHub code base for our engineers. Keep a look out for all the great stuff well be contributing to Rails 6.1 and beyondthis is just the beginning. "
        ],
        "story_type": "Normal",
        "url_raw": "https://github.blog/2019-09-09-running-github-on-rails-6-0/",
        "url_text": "On August 26, 2019, the GitHub application was deployed to production with 100 percent of traffic on the newest Rails version: 6.0. This change came just 1.5 weeks after the final release of Rails 6.0. Rails upgrades arent always something companies announce, but looking back at GitHubs history of being on a custom fork of Rails 3.2, this upgrade is a big deal. It represents how far weve come in the last few years, and the hard work and dedication from our upgrade team made it smoother, easier, and faster than any of our previous upgrades. At GitHub, we have a lot to celebrate with the release of Rails 6.0 and the subsequent production deploy. First, we were more involved in this release than we have been in any previous release of Rails. GitHub engineers sent over 100 pull requests to Rails 6.0 to improve documentation, fix bugs, add features, and speed up performance. For many GitHub contributors, this was the first time sending changes to the Rails framework, demonstrating that upgrading Rails not only helps GitHub internally, but also improves our developer community as well. Second, we deployed Rails 6.0 to production without any negative impact to customerswe had only one Rails 6.0 exception occur during testing, and it was hit by a bot! We were able to achieve this level of stability for the upgrade because we were heavily involved with its development. As soon as we finished the Rails 5.2 upgrade last year, we started upgrading our application to Rails 6.0. Instead of waiting for the final release, wed upgrade every week by pulling in the latest changes from Rails master and run all of our tests against that new version. This allowed us to find regressions quickly and earlyoften finding regressions in Rails master just hours after they were introduced. Upgrading weekly made it easy to find where these regressions were introduced since we were bisecting Rails with only a weeks worth of commits instead of more than a year of commits. Once our build for Rails 6.0 was green, wed merge the pull request to master, and all new code that went into GitHub would need to pass in Rails 5.2 and the newest master build of Rails. Upgrading every week worked so well that well continue using this process for upgrading from 6.0 to 6.1. In addition to ensuring that Rails 6.0 was stable, we also contributed to the new features of the framework like parallel testing and multiple databases. The code for these tools is used in our production application every dayits well-tested, GitHub-approved code in a public, open source framework. By upstreaming this tooling, were able to reduce complexity in our code base and set a standard where so many companies once had to implement this functionality on their own. There are so many wins to staying upgraded that go beyond more security, faster performance, and new features. By staying current with Rails master, were influencing the future of the framework to meet our needs and giving back to the open source community in big ways. This process means that the GitHub code base evolves alongside Rails instead of in response to Rails. Investing in our application by staying up to date with the Rails framework has had a tremendous positive effect on our code base and engineering teams. Staying current allows us to invest in our community, invest in our tools for the long term, and improve the experience of working with the GitHub code base for our engineers. Keep a look out for all the great stuff well be contributing to Rails 6.1 and beyondthis is just the beginning. ",
        "comments.comment_id": [20921454, 20921848],
        "comments.comment_author": ["sellingwebsite", "juliendc"],
        "comments.comment_descendants": [18, 10],
        "comments.comment_time": [
          "2019-09-09T19:27:03Z",
          "2019-09-09T20:02:18Z"
        ],
        "comments.comment_text": [
          "My comment on one of the previous discussions:<p>I don't want to start a flame war here, but I think Rails (and gem ecosystem in general) is a better choice than Django, at least for SaaS apps.<p>These are all my personal opinions, take it with a grain of salt. Having said that, here we go:<p>* Authentication - it is a pain if you'd like to deviate from the standard Django User model (using username to login instead of an email). I don't like Devise either.<p>* Asset pipeline, even though it is not updated anymore (sprockets) and partially replaced by the webpacker, is still better in Rails<p>* Configuration spread across multiple files, by environment, instead of a single config.py file<p>* Sidekiq has a better API compared to Celery. Also, Celery's default broker is RabbitMQ, not Redis. It is really hard to find managed RabbitMQ hosting, for Redis there are plenty<p>* Mailer previews, small but quite useful utility<p>* Better security by default: Rails comes pre-configured with a bunch of security headers[0].<p>* Testing - Minitest and Capybara is just a joy to work with.<p>* I prefer ActiveRecord over Django ORM<p>* Rails isn't afraid to deprecate <i>things</i> and move forward. This isn't the case with Django, which is big on backwards compatibility. I don't like it, since it puts into a disadvantage folks who are starting new projects. Different strokes for different folks, I suppose<p>I could go on and on, but I remember struggling a lot with Django/Celery when building a SaaS app. I decided to switch to Rails and haven't looked back (Rails has its warts as well). YMMV<p>[0] <a href=\"https://guides.rubyonrails.org/security.html#default-headers\" rel=\"nofollow\">https://guides.rubyonrails.org/security.html#default-headers</a><p>EDIT: Added last point about backwards compatibility",
          "I've a bunch of small Rails apps running on Heroku and I've to say that I'm impressed by the relevance of the new features in the latest Rails releases. Action Text, Active Storage and Action Cable are solving common and painful issues in any web app.<p>I've recently built a web app with Node and the time we spent solving problems which have already been solved a thousand times is astonishing. Things like picking an ORM, having a proper database migration system, running a test suite. It's actually quite depressing when you come from Rails where everything is working coherently out of the box.<p>The fact that there is no standards in the Node ecosystem make it a bit more painful. You have to carefully choose between many different libraries to solve your problem. Some of them are in TypeScript, other still use callbacks, etc. We basically had to glue together many libraries to get something working. All those hours could have been spent building the actual product and delivering value to our customers.<p>Hope they will ship many more releases!"
        ],
        "id": "25d6ec8d-31e2-49da-a7a1-1883a195cb5c",
        "_version_": 1718536520249376768
      },
      {
        "story_id": 21661013,
        "story_author": "mana99",
        "story_descendants": 32,
        "story_score": 168,
        "story_time": "2019-11-29T00:10:46Z",
        "story_title": "Making Git and Jupyter Notebooks play nice",
        "search": [
          "Making Git and Jupyter Notebooks play nice",
          "http://timstaley.co.uk/posts/making-git-and-jupyter-notebooks-play-nice/",
          "Summary: jq rocks for speedy JSON mangling. Use it to make powerful git clean filters, e.g. when stripping out unwanted cached-data from Jupyter notebooks. You can find the documentation of git 'clean' and 'smudge' filters buried in the page on git-attributes, or see my example setup below. The trouble with notebooks For a year or so now I've been using Jupyter notebooks as a means to produce tutorials and other documentation (see e.g. the voeventdb.remote tutorial). It's a powerful medium, providing a good compromise between ease-of-editing and the capability to interleave text, code, intermediate results, plots, and even nicely-typeset LaTeX-encoded equations. I've even gone to far as to urge its adoption in recent conference talks. However, this powerful interface inherits the age-old curse of WYSIWYG editors - the document-files tend to contain more than just plain-text, and therefore are not-so-easy to handle with standard version-control tools. In the case of Jupyter, the format doesn't stray too far from comfortable plain-text territory - the ipynb format is just a custom JSON data-structure, with the occasional base-64 encoded blob for images and other binary data. Which means version-control systems such as Git can handle it quite well, but diff-comparing different versions of a complex notebook quickly becomes a chore as you scroll past long blocks of unintelligible base-64 gibberish. This is a problem when working with long-lived, multiple-revision or (especially) multiple-coauthor projects. What can we do about this? First, it's worth mentioning the initial \"I'll figure this out later\" solution which has served many users sufficiently well for a while - if you're typically only working from one machine, and you just want to keep your notebooks vaguely manageable, you can get by for a long time by manually hitting Cell -> All Output -> Clear (followed by a Save) before you commit your notebooks. This wipes the slate clean with regards to cell outputs (plots, prints, whatver), so you'll need to re-run any computation next time you run the notebook. The problems with this approach are that A. it's manual, so you'll have to painstakingly open up every notebook you recently re-ran and clear it before you commit, and B. it doesn't even fully solve the 'noise-in-the-diffs' problem, since every notebook also contains a 'metadata' section, which looks a bit like this: { \"metadata\": { \"kernelspec\": { \"display_name\": \"Python 2\", \"language\": \"python\", \"name\": \"python2\" }, \"language_info\": { \"codemirror_mode\": { \"name\": \"ipython\", \"version\": 2 }, \"file_extension\": \".py\", \"mimetype\": \"text/x-python\", \"name\": \"python\", \"nbconvert_exporter\": \"python\", \"pygments_lexer\": \"ipython2\", \"version\": \"2.7.12\" } } Note the metadata section is effectively a blank slate, and has a myriad of possible uses, but for most users it will just contain the above. This is useful for checking a previously run notebook, but is mostly unwanted information when checking-in files to a multi-user project where everyone's using a slightly different Python version - it just generates more diff-noise. Possible Pythonic solutions nbdime - an nbformat diff-GUI We clearly need some tooling, and there are some Python projects out there trying to address exactly this problem. First, it's worth mentioning nbdime, which picks up the ball from where the (now defunct) nbdiff project left off and attempts to provide content-aware diffing and merging of Jupyter notebooks - a meld (GUI) diff-tool equivalent for the nbformat, if you will. I think nbdime has the potential to be a really good beginner-friendly, general purpose notebook-handling tool and I want to see it succeed. However; it's currently somewhat of a beta, and more importantly it only fills one role in the notebook editing toolbox - viewing crufty diffs. What I really want to do is automatically clear out all the cruft and minimize the diffs in the first place. nbstripout - does what it says on the tin A little searching then leads to nbstripout, which is a one-module Python script wrapping the nbformat processing functions, and adding some automagic for setting up your git config (on which more in a moment). This effectively automates the 'clear all output' manual process described above. However, this doesn't suit me for a couple of reasons; it leaves in that problematic 'metadata' section and also it's **slowww**. Running a script manually and expecting a short delay is fine, but we're going to integrate this into our git setup. That means it will run every time we hit git diff! One of the few things I love about git is that it's typically blazing fast; so a delay of nearly a fifth of a second every time I try to interact with it gets old pretty quickly: time nbstripout 01-parsing.ipynb real 0m0.174s user 0m0.152s sys 0m0.016s (Note, this is a small notebook-file, on a fairly beefy laptop with an SSD). This not a criticism of nbstripout so much as an inherent flaw in using Python for low-latency tasks - that cold-startup overhead on the CPython interpreter is a killer. (Which in turn harks back to ancient history of mercurial vs git!) Enter jq Fortunately, we have another option (thanks to Jan Schulz for the tip-off on this). Since the nbformat is just JSON, we can make use of jq, 'a lightweight and flexible command-line JSON processor' ('sed for JSON data'). There's a modicum of set-up overhead as jq has its very own query / filter language, but the documentation is good and the hard work has been done for you already. Here's the jq invocation I'm currently using: jq --indent 1 \\ ' (.cells[] | select(has(\"outputs\")) | .outputs) = [] | (.cells[] | select(has(\"execution_count\")) | .execution_count) = null | .metadata = {\"language_info\": {\"name\":\"python\", \"pygments_lexer\": \"ipython3\"}} | .cells[].metadata = {} ' 01-parsing.ipynb Each line inside the single-quotes defines a filter - the first selects any entries from the 'cells' list, and blanks any outputs. The second resets any execution counts. The third wipes the notebook metadata, replacing it with the minimum of required information for the notebook to still run without complaints [*] and work correctly when formatted with nbsphinx. The fourth filter-line, .cells[].metadata = {} is a matter of preference and situation - in recent versions of Jupyter every cell can be marked hidden / collapsed / write-protected, etc. I'm not interested in that metadata usually but of course you may want to keep it for some projects. We now have a fully stripped-down notebook that should contain only the common information needed to execute with whatever local Python installation is available (assuming Python2/3 compatibility, correctly set-up library installs and all the rest). Note you'll need jq version 1.5 or greater, since the --indent option was only recently implemented and is necessary to conform with the nbformat. Fortunately that should only be a small binary-download away, even if you're on ancient linux or OSX. That's a bit of a handful to type, but you can set it up as an alias in your .bashrc with a bit of careful quotation-escaping: alias nbstrip_jq=\"jq --indent 1 \\ '(.cells[] | select(has(\\\"outputs\\\")) | .outputs) = [] \\ | (.cells[] | select(has(\\\"execution_count\\\")) | .execution_count) = null \\ | .metadata = {\\\"language_info\\\": {\\\"name\\\": \\\"python\\\", \\\"pygments_lexer\\\": \\\"ipython3\\\"}} \\ | .cells[].metadata = {} \\ '\" Which can then be used conveniently like so: nbstrip_jq 01-parsing.ipynb > stripped.ipynb Not only does this give us full control to wipe that pesky metadata, it's pretty damn quick, taking something like a tenth of the time of nbstripout in my (admittedly ad-hoc) testing: nbstrip_jq 01-parsing.ipynb # (JSON contents omitted) real 0m0.015s user 0m0.008s sys 0m0.004s [*]Note on handling the notebook-level metadata section: Previously I had been blanking the metadata entirely, but it turns out that the pygments_lexer entry is crucial for nbsphinx to format notebooks with the correct syntax highlighting, hence the slightly awkward entry you see here. You might want to take a more careful approach and put together a jq-filter which merely removes (or normalizes) the Python version numbers, thereby lessening the risk of inadvertently wiping useful metadata. But for the purposes of this blog post I wanted to keep things as simple as possible while actually giving a usable, working setup. Automation: Integrating with git So we're all tooled up, but the question remains - how do we get git to run this automatically for us? For this, we dive into 'gitattributes' functionality, specifically the filter section. This describes how to define 'clean' and 'smudge' (reverse of clean) filters, which are operations that transform our data as it is checked in or out of the git-repository, so that (for example) our notebook-output cells are always stripped away from the JSON-data before it's added to the git repository: In the general case you can also define a smudge-filter to take your repository contents and do something with it to make it local to your system, but we'll not be needing that here - we'll just use the cat command as a placeholder. The easiest way to explain how to configure this is with an example. Personally, I want notebook-cleaning behaviour to be the default across all my git-repositories, so I have the following entries in my global ~/.gitconfig file: [core] attributesfile = ~/.gitattributes_global [filter \"nbstrip_full\"] clean = \"jq --indent 1 \\ '(.cells[] | select(has(\\\"outputs\\\")) | .outputs) = [] \\ | (.cells[] | select(has(\\\"execution_count\\\")) | .execution_count) = null \\ | .metadata = {\\\"language_info\\\": {\\\"name\\\": \\\"python\\\", \\\"pygments_lexer\\\": \\\"ipython3\\\"}} \\ | .cells[].metadata = {} \\ '\" smudge = cat required = true And then in ~/.gitattributes_global: *.ipynb filter=nbstrip_full (Note, once you've defined your filter you can just as easily assign it to files in a repository specific .gitattributes file if you prefer a fine-grained approach.) That's it! You're all set to go version control notebooks like a champ! Well, almost. Getting started and gotchas Note that we're into git-powertool territory here, so things might be a little less polished compared to the (cough) usual intuitive git interface you're used to. To start off with, assuming a pre-existing set of notebooks, you'll want to add a 'do-nothing' commit, where you simply pull in the newly-filtered versions of your notebooks and trim out any unwanted metadata. Just git add your notebooks, noting that you may need to touch them first, so git picks up on the timestamp-modification and actually looks at the files for changes. Then, to see the patch removing all the cruft. Commit that, then go ahead, run your notebooks, leave uncleaned outputs all over the place. Unless you change the actual code-cell contents, your git diff should be blank! Great. Except. If you have executed a notebook since your last commit, git status may show that file as 'modified', despite the fact that when you git diff, the filters go into action and no differences-to-HEAD are found. So you have to 'tune out' these false-positive modified flags when reading the git-status. Another issue is that if you use a diff-GUI such as meld, then beware: unlike git diff, git difftool will not apply filters to the working directory before comparing with the repo HEAD - so your command-line and GUI diffs have suddenly diverged! The logic behind this difference in behaviour is that GUI programs give the option to edit the local working-copy directly, as discussed at length in this thread. This has clearly caught out others before. If they bother you, these false-positives and diff-divergences can easily be resolved by manually applying the jq-filters before you run your diffs. For convenience, my ~/.bashrc also defines the following command to apply the filters to all notebooks in the current working directory: function nbstrip_all_cwd { for nbfile in *.ipynb; do echo \"$( nbstrip_jq $nbfile )\" > $nbfile done unset nbfile } Addtionally, let me note that clean/smudge filters often do not play well with rebase operations. Things get very confusing if you try to rebase across commits before / after applying a clean-filter. The simplest way to work around this is to simply comment out the relevant filter-assignment line in .gitattributes_global while performing a rebase, then uncomment it when done. As a parting note, if you also choose to configure your gitattributes globally, you may want to know how to 'whitelist' notebooks in a particular repository (for example, if you're checking-in executed notebooks to a github-pages documentation branch). This is dead easy, just add a local .gitattributes file to the repository and 'unset' the filter attribute, like so: Or you could replace the *.ipynb with a path to a specific notebook, etc. Hope that helps! Comments or corrections very welcome via Twitter. "
        ],
        "story_type": "Normal",
        "url_raw": "http://timstaley.co.uk/posts/making-git-and-jupyter-notebooks-play-nice/",
        "comments.comment_id": [21661478, 21663231],
        "comments.comment_author": ["FridgeSeal", "TimSAstro"],
        "comments.comment_descendants": [1, 1],
        "comments.comment_time": [
          "2019-11-29T02:12:26Z",
          "2019-11-29T09:33:20Z"
        ],
        "comments.comment_text": [
          "Things I’ve found in checked-in notebooks:<p>* database credentials.<p>* sensitive data<p>* a whole DataBricks webpage because the person didn’t understand how to export just the notebook.<p>* collections of notebooks named only what step in the process they are, and literally nothing about what they actually do.<p>* Whole base64 encoded images and zip files<p>* packages imported by manually manipulating system environment paths<p>* multi-processing/multithreading by shelling out and calling new python instances<p>* good old “don’t run these cells”",
          "Hi, HN!<p>Author here. A friend mentioned this was on the front page so I wanted to stop by and make explicit that this advice is OUT OF DATE as far as I'm concerned. It's way too much hassle (I work in a much larger team now than I did then!) and doesn't play well with rebase etc.<p>These days I either recommend the jupytext approach (not tried it but seems sensible) or personally I just use Sphinx-gallery.<p>Advantages:<p>* Plain python files play well with IDE refactoring, Black formatter, etc etc.<p>* You now have a readymade 'tutorial' page for your docs.<p>* Files are run with every docs build, so you can configure things to alert you when they're broken.<p>Disadvantages:<p>* You end up editing a throwaway notebook file. If you forget to copy-paste your edits back to the source, and rebuild, you have lost your edits. However, this forces me to keep the 'temporary, exploratory' nature at the front of my mind and not allow the notebook code to grow too large before performing some clean-up."
        ],
        "id": "73890fd1-c83b-4b65-9d1c-fa6fad1fa5c3",
        "url_text": "Summary: jq rocks for speedy JSON mangling. Use it to make powerful git clean filters, e.g. when stripping out unwanted cached-data from Jupyter notebooks. You can find the documentation of git 'clean' and 'smudge' filters buried in the page on git-attributes, or see my example setup below. The trouble with notebooks For a year or so now I've been using Jupyter notebooks as a means to produce tutorials and other documentation (see e.g. the voeventdb.remote tutorial). It's a powerful medium, providing a good compromise between ease-of-editing and the capability to interleave text, code, intermediate results, plots, and even nicely-typeset LaTeX-encoded equations. I've even gone to far as to urge its adoption in recent conference talks. However, this powerful interface inherits the age-old curse of WYSIWYG editors - the document-files tend to contain more than just plain-text, and therefore are not-so-easy to handle with standard version-control tools. In the case of Jupyter, the format doesn't stray too far from comfortable plain-text territory - the ipynb format is just a custom JSON data-structure, with the occasional base-64 encoded blob for images and other binary data. Which means version-control systems such as Git can handle it quite well, but diff-comparing different versions of a complex notebook quickly becomes a chore as you scroll past long blocks of unintelligible base-64 gibberish. This is a problem when working with long-lived, multiple-revision or (especially) multiple-coauthor projects. What can we do about this? First, it's worth mentioning the initial \"I'll figure this out later\" solution which has served many users sufficiently well for a while - if you're typically only working from one machine, and you just want to keep your notebooks vaguely manageable, you can get by for a long time by manually hitting Cell -> All Output -> Clear (followed by a Save) before you commit your notebooks. This wipes the slate clean with regards to cell outputs (plots, prints, whatver), so you'll need to re-run any computation next time you run the notebook. The problems with this approach are that A. it's manual, so you'll have to painstakingly open up every notebook you recently re-ran and clear it before you commit, and B. it doesn't even fully solve the 'noise-in-the-diffs' problem, since every notebook also contains a 'metadata' section, which looks a bit like this: { \"metadata\": { \"kernelspec\": { \"display_name\": \"Python 2\", \"language\": \"python\", \"name\": \"python2\" }, \"language_info\": { \"codemirror_mode\": { \"name\": \"ipython\", \"version\": 2 }, \"file_extension\": \".py\", \"mimetype\": \"text/x-python\", \"name\": \"python\", \"nbconvert_exporter\": \"python\", \"pygments_lexer\": \"ipython2\", \"version\": \"2.7.12\" } } Note the metadata section is effectively a blank slate, and has a myriad of possible uses, but for most users it will just contain the above. This is useful for checking a previously run notebook, but is mostly unwanted information when checking-in files to a multi-user project where everyone's using a slightly different Python version - it just generates more diff-noise. Possible Pythonic solutions nbdime - an nbformat diff-GUI We clearly need some tooling, and there are some Python projects out there trying to address exactly this problem. First, it's worth mentioning nbdime, which picks up the ball from where the (now defunct) nbdiff project left off and attempts to provide content-aware diffing and merging of Jupyter notebooks - a meld (GUI) diff-tool equivalent for the nbformat, if you will. I think nbdime has the potential to be a really good beginner-friendly, general purpose notebook-handling tool and I want to see it succeed. However; it's currently somewhat of a beta, and more importantly it only fills one role in the notebook editing toolbox - viewing crufty diffs. What I really want to do is automatically clear out all the cruft and minimize the diffs in the first place. nbstripout - does what it says on the tin A little searching then leads to nbstripout, which is a one-module Python script wrapping the nbformat processing functions, and adding some automagic for setting up your git config (on which more in a moment). This effectively automates the 'clear all output' manual process described above. However, this doesn't suit me for a couple of reasons; it leaves in that problematic 'metadata' section and also it's **slowww**. Running a script manually and expecting a short delay is fine, but we're going to integrate this into our git setup. That means it will run every time we hit git diff! One of the few things I love about git is that it's typically blazing fast; so a delay of nearly a fifth of a second every time I try to interact with it gets old pretty quickly: time nbstripout 01-parsing.ipynb real 0m0.174s user 0m0.152s sys 0m0.016s (Note, this is a small notebook-file, on a fairly beefy laptop with an SSD). This not a criticism of nbstripout so much as an inherent flaw in using Python for low-latency tasks - that cold-startup overhead on the CPython interpreter is a killer. (Which in turn harks back to ancient history of mercurial vs git!) Enter jq Fortunately, we have another option (thanks to Jan Schulz for the tip-off on this). Since the nbformat is just JSON, we can make use of jq, 'a lightweight and flexible command-line JSON processor' ('sed for JSON data'). There's a modicum of set-up overhead as jq has its very own query / filter language, but the documentation is good and the hard work has been done for you already. Here's the jq invocation I'm currently using: jq --indent 1 \\ ' (.cells[] | select(has(\"outputs\")) | .outputs) = [] | (.cells[] | select(has(\"execution_count\")) | .execution_count) = null | .metadata = {\"language_info\": {\"name\":\"python\", \"pygments_lexer\": \"ipython3\"}} | .cells[].metadata = {} ' 01-parsing.ipynb Each line inside the single-quotes defines a filter - the first selects any entries from the 'cells' list, and blanks any outputs. The second resets any execution counts. The third wipes the notebook metadata, replacing it with the minimum of required information for the notebook to still run without complaints [*] and work correctly when formatted with nbsphinx. The fourth filter-line, .cells[].metadata = {} is a matter of preference and situation - in recent versions of Jupyter every cell can be marked hidden / collapsed / write-protected, etc. I'm not interested in that metadata usually but of course you may want to keep it for some projects. We now have a fully stripped-down notebook that should contain only the common information needed to execute with whatever local Python installation is available (assuming Python2/3 compatibility, correctly set-up library installs and all the rest). Note you'll need jq version 1.5 or greater, since the --indent option was only recently implemented and is necessary to conform with the nbformat. Fortunately that should only be a small binary-download away, even if you're on ancient linux or OSX. That's a bit of a handful to type, but you can set it up as an alias in your .bashrc with a bit of careful quotation-escaping: alias nbstrip_jq=\"jq --indent 1 \\ '(.cells[] | select(has(\\\"outputs\\\")) | .outputs) = [] \\ | (.cells[] | select(has(\\\"execution_count\\\")) | .execution_count) = null \\ | .metadata = {\\\"language_info\\\": {\\\"name\\\": \\\"python\\\", \\\"pygments_lexer\\\": \\\"ipython3\\\"}} \\ | .cells[].metadata = {} \\ '\" Which can then be used conveniently like so: nbstrip_jq 01-parsing.ipynb > stripped.ipynb Not only does this give us full control to wipe that pesky metadata, it's pretty damn quick, taking something like a tenth of the time of nbstripout in my (admittedly ad-hoc) testing: nbstrip_jq 01-parsing.ipynb # (JSON contents omitted) real 0m0.015s user 0m0.008s sys 0m0.004s [*]Note on handling the notebook-level metadata section: Previously I had been blanking the metadata entirely, but it turns out that the pygments_lexer entry is crucial for nbsphinx to format notebooks with the correct syntax highlighting, hence the slightly awkward entry you see here. You might want to take a more careful approach and put together a jq-filter which merely removes (or normalizes) the Python version numbers, thereby lessening the risk of inadvertently wiping useful metadata. But for the purposes of this blog post I wanted to keep things as simple as possible while actually giving a usable, working setup. Automation: Integrating with git So we're all tooled up, but the question remains - how do we get git to run this automatically for us? For this, we dive into 'gitattributes' functionality, specifically the filter section. This describes how to define 'clean' and 'smudge' (reverse of clean) filters, which are operations that transform our data as it is checked in or out of the git-repository, so that (for example) our notebook-output cells are always stripped away from the JSON-data before it's added to the git repository: In the general case you can also define a smudge-filter to take your repository contents and do something with it to make it local to your system, but we'll not be needing that here - we'll just use the cat command as a placeholder. The easiest way to explain how to configure this is with an example. Personally, I want notebook-cleaning behaviour to be the default across all my git-repositories, so I have the following entries in my global ~/.gitconfig file: [core] attributesfile = ~/.gitattributes_global [filter \"nbstrip_full\"] clean = \"jq --indent 1 \\ '(.cells[] | select(has(\\\"outputs\\\")) | .outputs) = [] \\ | (.cells[] | select(has(\\\"execution_count\\\")) | .execution_count) = null \\ | .metadata = {\\\"language_info\\\": {\\\"name\\\": \\\"python\\\", \\\"pygments_lexer\\\": \\\"ipython3\\\"}} \\ | .cells[].metadata = {} \\ '\" smudge = cat required = true And then in ~/.gitattributes_global: *.ipynb filter=nbstrip_full (Note, once you've defined your filter you can just as easily assign it to files in a repository specific .gitattributes file if you prefer a fine-grained approach.) That's it! You're all set to go version control notebooks like a champ! Well, almost. Getting started and gotchas Note that we're into git-powertool territory here, so things might be a little less polished compared to the (cough) usual intuitive git interface you're used to. To start off with, assuming a pre-existing set of notebooks, you'll want to add a 'do-nothing' commit, where you simply pull in the newly-filtered versions of your notebooks and trim out any unwanted metadata. Just git add your notebooks, noting that you may need to touch them first, so git picks up on the timestamp-modification and actually looks at the files for changes. Then, to see the patch removing all the cruft. Commit that, then go ahead, run your notebooks, leave uncleaned outputs all over the place. Unless you change the actual code-cell contents, your git diff should be blank! Great. Except. If you have executed a notebook since your last commit, git status may show that file as 'modified', despite the fact that when you git diff, the filters go into action and no differences-to-HEAD are found. So you have to 'tune out' these false-positive modified flags when reading the git-status. Another issue is that if you use a diff-GUI such as meld, then beware: unlike git diff, git difftool will not apply filters to the working directory before comparing with the repo HEAD - so your command-line and GUI diffs have suddenly diverged! The logic behind this difference in behaviour is that GUI programs give the option to edit the local working-copy directly, as discussed at length in this thread. This has clearly caught out others before. If they bother you, these false-positives and diff-divergences can easily be resolved by manually applying the jq-filters before you run your diffs. For convenience, my ~/.bashrc also defines the following command to apply the filters to all notebooks in the current working directory: function nbstrip_all_cwd { for nbfile in *.ipynb; do echo \"$( nbstrip_jq $nbfile )\" > $nbfile done unset nbfile } Addtionally, let me note that clean/smudge filters often do not play well with rebase operations. Things get very confusing if you try to rebase across commits before / after applying a clean-filter. The simplest way to work around this is to simply comment out the relevant filter-assignment line in .gitattributes_global while performing a rebase, then uncomment it when done. As a parting note, if you also choose to configure your gitattributes globally, you may want to know how to 'whitelist' notebooks in a particular repository (for example, if you're checking-in executed notebooks to a github-pages documentation branch). This is dead easy, just add a local .gitattributes file to the repository and 'unset' the filter attribute, like so: Or you could replace the *.ipynb with a path to a specific notebook, etc. Hope that helps! Comments or corrections very welcome via Twitter. ",
        "_version_": 1718536547062513664
      }
    ]
  }
}
