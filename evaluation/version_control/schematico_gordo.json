{
  "docs": [
    {
      "id": 20993747,
      "title": "Show HN: Statistical tool for analyzing a Git repository",
      "search": [
        "Show HN: Statistical tool for analyzing a Git repository",
        "ShowHN",
        "https://github.com/arzzen/git-quick-stats/",
        "GIT quick statistics git-quick-stats is a simple and efficient way to access various statistics in a git repository. Any git repository may contain tons of information about commits, contributors, and files. Extracting this information is not always trivial, mostly because there are a gadzillion options to a gadzillion git commands I dont think there is a single person alive who knows them all. Probably not even Linus Torvalds himself :). Table of Contents Screenshots Usage Interactive Non-interactive Command-line arguments Git log since and until Git log limit Git log options Git pathspec Git merge view strategy Color themes Installation UNIX and Linux macOS Windows Docker System requirements Dependencies FAQ Contribution Code reviews Some tips for good pull requests Formatting Tests Licensing Contributors Backers Sponsors Screenshots Usage Interactive git-quick-stats has a built-in interactive menu that can be executed as such: Or Non-interactive For those who prefer to utilize command-line options, git-quick-stats also has a non-interactive mode supporting both short and long options: git-quick-stats <optional-command-to-execute-directly> Or git quick-stats <optional-command-to-execute-directly> Command-line arguments Possible arguments in short and long form: GENERATE OPTIONS -T, --detailed-git-stats give a detailed list of git stats -R, --git-stats-by-branch see detailed list of git stats by branch -c, --changelogs see changelogs -L, --changelogs-by-author see changelogs by author -S, --my-daily-stats see your current daily stats -V, --csv-output-by-branch output daily stats by branch in CSV format -j, --json-output save git log as a JSON formatted file to a specified area LIST OPTIONS -b, --branch-tree show an ASCII graph of the git repo branch history -D, --branches-by-date show branches by date -C, --contributors see a list of everyone who contributed to the repo -a, --commits-per-author displays a list of commits per author -d, --commits-per-day displays a list of commits per day -m, --commits-by-month displays a list of commits per month -w, --commits-by-weekday displays a list of commits per weekday -o, --commits-by-hour displays a list of commits per hour -A, --commits-by-author-by-hour displays a list of commits per hour by author -z, --commits-by-timezone displays a list of commits per timezone -Z, --commits-by-author-by-timezone displays a list of commits per timezone by author SUGGEST OPTIONS -r, --suggest-reviewers show the best people to contact to review code -h, -?, --help display this help text in the terminal Git log since and until You can set the variables _GIT_SINCE and/or _GIT_UNTIL before running git-quick-stats to limit the git log. These work similar to git's built-in --since and --until log options. export _GIT_SINCE=\"2017-01-20\" export _GIT_UNTIL=\"2017-01-22\" Once set, run git quick-stats as normal. Note that this affects all stats that parse the git log history until unset. Git log limit You can set variable _GIT_LIMIT for limited output. It will affect the \"changelogs\" and \"branch tree\" options. Git log options You can set _GIT_LOG_OPTIONS for git log options: export _GIT_LOG_OPTIONS=\"--ignore-all-space --ignore-blank-lines\" Git pathspec You can exclude a directory from the stats by using pathspec export _GIT_PATHSPEC=':!directory' You can also exclude files from the stats. Note that it works with any alphanumeric, glob, or regex that git respects. export _GIT_PATHSPEC=':!package-lock.json' Git merge view strategy You can set the variable _GIT_MERGE_VIEW to enable merge commits to be part of the stats by setting _GIT_MERGE_VIEW to enable. You can also choose to only show merge commits by setting _GIT_MERGE_VIEW to exclusive. Default is to not show merge commits. These work similar to git's built-in --merges and --no-merges log options. export _GIT_MERGE_VIEW=\"enable\" export _GIT_MERGE_VIEW=\"exclusive\" Git branch You can set the variable _GIT_BRANCH to set the branch of the stats. Works with commands --git-stats-by-branch and --csv-output-by-branch. export _GIT_BRANCH=\"master\" Color themes You can change to the legacy color scheme by toggling the variable _MENU_THEME between default and legacy export _MENU_THEME=\"legacy\" Installation Debian and Ubuntu If you are on at least Debian Bullseye or Ubuntu Focal you can use apt for installation: apt install git-quick-stats UNIX and Linux git clone https://github.com/arzzen/git-quick-stats.git && cd git-quick-stats sudo make install For uninstalling, open up the cloned directory and run For update/reinstall macOS (homebrew) brew install git-quick-stats Or you can follow the UNIX and Linux instructions if you wish. Windows If you are installing with Cygwin, use these scripts: installer uninstaller If you are wishing to use this with WSL, follow the UNIX and Linux instructions. Docker You can use the Docker image provided: Build: docker build -t arzzen/git-quick-stats . Run interactive menu: docker run --rm -it -v $(pwd):/git arzzen/git-quick-stats Docker pull command: docker pull arzzen/git-quick-stats docker repository System requirements An OS with a Bash shell Tools we use: awk basename cat column echo git grep head printf seq sort tput tr uniq wc Dependencies bsdmainutils apt install bsdmainutils FAQ Q: I get some errors after run git-quick-stats in cygwin like /usr/local/bin/git-quick-stats: line 2: $'\\r': command not found A: You can run the dos2unix app in cygwin as follows: /bin/dos2unix.exe /usr/local/bin/git-quick-stats. This will convert the script from the CR-LF convention that Microsoft uses to the LF convention that UNIX, OS X, and Linux use. You should then should be able to run it as normal. Q: How they could be used in a project with many git projects and statistics would show a summary of all git projects? A: If you want to include submodule logs, you can try using the following: export _GIT_LOG_OPTIONS=\"-p --submodule=log\" (more info about git log --submodule) Contribution Want to contribute? Great! First, read this page. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Some tips for good pull requests Use our code When in doubt, try to stay true to the existing code of the project. Write a descriptive commit message. What problem are you solving and what are the consequences? Where and what did you test? Some good tips: here and here. If your PR consists of multiple commits which are successive improvements / fixes to your first commit, consider squashing them into a single commit (git rebase -i) such that your PR is a single commit on top of the current HEAD. This make reviewing the code so much easier, and our history more readable. Formatting This documentation is written using standard markdown syntax. Please submit your changes using the same syntax. Tests Licensing MIT see LICENSE for the full license text. Contributors This project exists thanks to all the people who contribute. Backers Thank you to all our backers! [Become a backer] Sponsors Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor] ",
        "Pull Panda [0] is another tool we've been using that is offered as a SaaS and is now free since it was acquired by Github this year. It tells you average PR review time, average PR diff size, who is most requested for review, review-comment ratio, etc. I can't believe it's taken Github so long to make progress on dashboards like this for engineering managers, but looking forward to the time Pull Panda is fully integrated.<p>[0] <a href=\"https://pullreminders.com\" rel=\"nofollow\">https://pullreminders.com</a>",
        "I made a tool myself [0] with slightly different tools. It answers two questions: Who are the relevant coders and what parts of the code are the hotspots?<p>Here is an example run on sqlite:<p><pre><code>    Top Committers (of 28 authors):\n    D. Richard Hipp      13359 commits during 19 years until 2019-09-17\n    Dan Kennedy          5813 commits during 17 years until 2019-09-16\n     together these authors have 80+% of the commits (19172/20987)\n\n    Files with most commits:\n    1143 commits: src/sqlite.h.in      during 19 years until 2019-09-16\n    1331 commits: src/where.c          during 19 years until 2019-09-03\n    1360 commits: src/btree.c          during 18 years until 2019-08-24\n    1650 commits: src/vdbe.c           during 19 years until 2019-09-16\n    1893 commits: src/sqliteInt.h      during 19 years until 2019-09-14\n\n    Files with most authors:\n    11 authors: src/main.c          \n    11 authors: src/sqliteInt.h     \n    12 authors: configure.ac        \n    12 authors: src/shell.c         \n    15 authors: Makefile.in         \n\n    By file extension:\n    .test: 1333 files\n       .c: 379 files\n     together these make up 80+% of the files (1712/2138)\n</code></pre>\n[0] <a href=\"https://github.com/qznc/dot/blob/master/bin/git-overview\" rel=\"nofollow\">https://github.com/qznc/dot/blob/master/bin/git-overview</a>"
      ],
      "relevant": "true"
    },
    {
      "id": 21624739,
      "title": "Legit: A CLI tool to make Git more accessible",
      "search": [
        "Legit: A CLI tool to make Git more accessible",
        "Normal",
        "https://frostming.github.io/legit/",
        "Welcome github // pypi // issue tracker Legit is a complementary command-line interface for Git, optimized for workflow simplicity. It is heavily inspired by GitHub for Mac. Feature branch workflows are dead simple. $ git switch <branch> # Switches to branch. Stashes and restores unstaged changes. $ git sync # Synchronizes current branch. Auto-merge/rebase, un/stash. $ git publish <branch> # Publishes branch to remote server. $ git unpublish <branch> # Removes branch from remote server. $ git branches [wildcard pattern] # Nice & pretty list of branches + publication status. $ git undo [--hard] # Removes the last commit from history. Installing Legit If you are using Homebrew: $ brew install legit Or install legit via pip: $ pip install legit To enable the git aliases: $ legit --install Nice and simple the way it should be. ",
        "Quoting the README:<p>> ## The Concept<p>> GitHub for Mac is not just a Git client.<p>> This comment[0] on Hacker News says it best:<p>>> They haven't re-created the git CLI tool in a GUI, they've created something\n>> different. They've created a tool that makes Git more accessible. Little\n>> things like auto-stashing when you switch branches will confuse git\n>> veterans, but it will make Git much easier to grok for newcomers because of\n>> the assumptions it makes about your Git workflow. Why not bring this\n>> innovation back to the command line?<p>It provides a few interfaces like switch <branch> which switches to specified branch by automatically stashing any changes, or sync [<branch>] which (smartly) synchronizes the given branch. Etc...<p>[0] <a href=\"https://news.ycombinator.com/item?id=2684483\" rel=\"nofollow\">https://news.ycombinator.com/item?id=2684483</a>",
        "Wouldn't it be enough to add some git aliases for this?\nLike:<p><pre><code>  [alias]\n  hist = log --pretty=format:\\\"%h %ad | %s%d [%an]\\\" --graph --decorate --date=short\n  quick-push = \"!f() { git add . && git commit -m \\\"$1\\\" && git push; }; f\"\n  grep-log = log -E -i --grep\n  grep-hist = log --pretty=format:\\\"%h %ad | %s%d [%an]\\\" --graph --decorate --date=short -E -i --grep</code></pre>"
      ],
      "relevant": "true"
    },
    {
      "id": 20242820,
      "title": "Ask HN: How to handle code reviews with a visually impaired coworker?",
      "search": [
        "Ask HN: How to handle code reviews with a visually impaired coworker?",
        "Hello World,<p>My dev team has switched to a workflow using merge requests and code reviews (mostly by commenting the merge request). Our tool is Gitlab.<p>I needed to embed a visually impaired co-worker in my team but I face some difficulties:<p>- Gitlab accessibility seems to be really bad and my co-worker is unable to use the interface  to create Merge Request (he uses accessibility tools, of course). I dont'even speak about reading and writing comments in merge request.<p>- I don't know how to handle the code review process with him. We could do physical Code Review sessions, but it's difficult because I've a very chaotic schedule and so it's difficult to find a common timeslot. Furthermore, it's very difficult for my co-worker to handle all the remarks in one session for any Merge Request with a significant amount of code.<p>- I need to keep in place the existing tooling for the rest of the team<p>Does anybody knows of tools interfacing with Gitlab (or the git repository) or methodologies that could help us ?",
        "AskHN",
        "I'm a totally blind developer and I find the easiest way to do code reviews is to use git format-patch on the branch containing the code. I read the patch files in a text editor. Perhaps comments in the pull request referencing a commit and line would allow the developer to get the required context from the patch files?",
        "I work with a blind developer. We create personal feature branches, file a ticket for merge requests in our ticketing system, code review them directly in the ticket or via email, and then rebase the feature branch onto master. We don't, but you could plausibly do reviews directly in git via editing the file with any comments.<p>I've been wanting to try gerrit however: <a href=\"https://gerrit-review.googlesource.com/Documentation/dev-design.html#_accessibility_considerations\" rel=\"nofollow\">https://gerrit-review.googlesource.com/Documentation/dev-des...</a><p>Please report back on whatever you end up doing.<p>One caution I have is that long term you should consider moving away from the gitlab tools if they are unable to fix accessibility concerns, so that everyone is on equal ground.<p>I also don't know how flexible he is on which browser or operating system he uses; he may want to try some others to see if they work any better."
      ],
      "relevant": "false"
    },
    {
      "id": 20269142,
      "title": "Ask HN: GitHub or Gitlab",
      "search": [
        "Ask HN: GitHub or Gitlab",
        "Migrating from an existing Git provider, curious what other folks have to say.<p>We're a building a SaaS app with no open source components at the moment.",
        "AskHN",
        "I have been enjoying Gitlab's built-in continuous integration service. If your app does not already use something else for continuous integration, it might be worthwhile to try out.",
        "We currently use Bitbucket (do not recommend) and we're evaluating GitLab.<p>First impression is that it's a great package. The only downside for us is that we have a lot of legacy baggage, mostly rather opinionated build tools set up the way we liked it. Getting that to work with GitLab, which itself has a very opinionated view of the CI/CD pipeline is a bit of effort. But for a greenfield project I would go with GitLab in a second!<p>GitLab is a \"batteries included\" kind of tool. They've put together a lot of excellent stuff and integrated different tools very well together. For example, we need to perform security scans on our code and artifacts and GitLab offers that with minimal effort and awesome integrations into your pull request (which they call merge requests) mechanism.<p>Some may not like their approach, especially if they already have something that doesn't quite fit. But if you're starting from scratch, you really should give them a chance. They really know what they're doing ;-)"
      ],
      "relevant": "true"
    },
    {
      "id": 18833090,
      "title": "Ask HN: What CI tool do you use in 2019?",
      "search": [
        "Ask HN: What CI tool do you use in 2019?",
        "I found lambCI which seems best but too bad it doesn't support bitbucket. Which tool do you use?<p>Travis costs several hundred dollars for 3-4 consecutive build",
        "AskHN",
        "Distelli (now puppet pipelines) let's you run as many builds as you want on your own hardware.<p><a href=\"https://puppet.com/blog/welcome-distelli-to-puppet-family\" rel=\"nofollow\">https://puppet.com/blog/welcome-distelli-to-puppet-family</a>",
        "Semaphore (<a href=\"https://semaphoreci.com\" rel=\"nofollow\">https://semaphoreci.com</a>) recently introduced pay-per-use model with autoscaling, so you don’t have to pay a large sum just to run a few parallel jobs (disclaimer: cofounder here, open for questions)."
      ],
      "relevant": "true"
    },
    {
      "id": 21836184,
      "title": "GitHub Actions is my new favorite free programming tool [video]",
      "search": [
        "GitHub Actions is my new favorite free programming tool [video]",
        "Normal",
        "https://www.bytesized.xyz/github-actions-tutorial",
        "Enjoying these posts? Subscribe for more Subscribe to be notified of new content and support Bytesized! You'll be a part of the community helping keep this site independent and ad-free. Already a member? Sign in ",
        "I would hesitate to build on anything that relies on GitHub native tooling. GitHub support is absolutely the worst. Their tools fail in weird ways at times, and without support you will be stuck. For personal/non production apps, fine. But beware of using it as a core part of your infrastructure.",
        "I have spent the last two days fighting with these new Github tools. I've been trying to do a simple Hello-World Maven app, built on Github Actions and deployed to Github Packages. It does not work.<p>Github documentation is a disaster. They leave out critical parts. They don't provide examples. Everything they write is terse, confusing, and incomplete.<p>They have short little articles on how to do things, and for each sub-task they have a link to docs somewhere else. This would be fine, except the links don't point to anything useful.<p>To give an example: they say you can use the Github API to talk to Github Packages, but the link goes to their generic GraphQL documentation. They don't point to any reference material on the actual calls to the packages service. If it exists, I can't find it.<p>If you go to the main page in your account for Github Packages, it says that all you have to do is this:<p>mvn deploy -Dregistry=<a href=\"https://maven.pkg.github.com/mycompany\" rel=\"nofollow\">https://maven.pkg.github.com/mycompany</a> -Dtoken=GH_TOKEN<p>That is just straight-out wrong. It does not work.<p>Seriously, Github, you need to fire your documentation team and hire some people who know how to write. Perhaps you should hire people who have actually used your tools to write the docs. Or just provide some freakin' working examples."
      ],
      "relevant": "true"
    },
    {
      "id": 20618427,
      "title": "Git-Revise",
      "search": [
        "Git-Revise",
        "Normal",
        "https://mystor.github.io/git-revise.html",
        "NIKA:\\git-revise\\> list (Aug. 6, 2019): Added the \"What git-revise is not\" section. At Mozilla I often end up building my changes in a patch stack, and used git rebase -i1 to make changes to commits in response to review comments etc. Unfortunately, with a repository as large as mozilla-central2, git rebase -i has some downsides: It's slow! Rebase operates directly on the worktree, so it performs a full checkout of each commit in the stack, and frequently refreshes worktree state. On large repositories (especially on NTFS) that can take a long time. It triggers rebuilds! Because rebase touches the file tree, some build systems (like gecko's recursive-make backend) rebuild unnecessarially. It's stateful! If the rebase fails, the repository is in a weird mid-rebase state, and in edge cases I've accidentally dropped commits due to other processes racing on the repository lock. It's clunky! Common tasks (like splitting & rewording commits) require multiple steps and are unintuitive. Naturally, I did the only reasonable thing: Build a brand-new tool. source: xkcd git-revise is a history editing tool designed for the patch-stack workflow. It's fast, non-destructive, and aims to provide a familiar, powerful, and easy to use re-imagining of the patch stack workflow. It's fast I would never claim to be a benchmarking expert 3, but git-revise performs substantially better than rebase for small history editing tasks 4. In a test applying a single-line change to a mozilla-central commit 20 patches up the stack I saw a 15x speed improvement. $ time bash -c 'git commit --fixup=$TARGET; EDITOR=true git rebase -i --autosquash $TARGET~' <snip> real 0m10.733s $ time git revise $TARGET <snip> real 0m0.685s git-revise accomplishes this using an in-memory rebase algorithm operating directly on git's trees, meaning it never has to touch your index or working directory, avoiding expensive disk I/O! It's handy git-revise isn't just a faster git rebase -i, it provides helpful commands, flags, and tools which make common changes faster, and easier: Fixup Fast $ git add . $ git revise HEAD~~ Running git revise $COMMIT directly collects changes staged in the index, and directly applies them to the specified commit. Conflicts are resolved interactively, and a warning will be shown if the final state of the tree is different from what you started with! With an extra -e, you can update the commit message at the same time, and -a will stage your changes, so you don't have to! 5 Split Commits $ git revise -c $COMMIT Select changes to be included in part [1]: diff --git b/file.txt a/file.txt <snip> Apply this hunk to index [y,n,q,a,d,e,?]? Sometimes, a commit needs to be split in two, perhaps because a change ended up in the wrong commit. The --cut flag (and cut interactive command) provides a fast way to split a commit in-place. Running git revise --cut $COMMIT will start a git add -p-style hunk selector, allowing you to pick changes for part 1, and the rest will end up in part 2. No more tinkering around with edit during a rebase to split off that comment you accidentally added to the wrong commit! Interactive Mode git-revise has a git rebase -i-style interactive mode, but with some quality-of-life improvements, on top of being fast: Implicit Base Commit If a base commit isn't provided, --interactive will implicitly locate a safe base commit to start from, walking up from HEAD, and stopping at published & merge commits. Often git revise -i is all you need! The index Todo Staged changes in the index automatically appear in interactive mode, and can be moved around and treated like any other commit in range. No need to turn it into a commit with a dummy name before you pop open interactive mode & squash it into another commit! Bulk Commit Rewording Ever wanted to update a bunch of commit messages at once? Perhaps they're all missing the bug number? Well, git revise -ie has you covered. It'll open a special Interactive Mode where each command is prefixed with a ++, and the full commit message is present after it. Changes made to these commit messages will be applied before executing the TODOs, meaning you can edit them in bulk. I use this constantly to add bug numbers, elaborate on commit details, and add reviewer information to commit messages. ++ pick f5a02a16731a Bug ??? - My commit summary, r=? The full commit message body follows! ++ pick fef1aeddd6fb Bug ??? - Another commit, r=? Another commit's body! Autosquash Support $ git revise --autosquash If you're used to git rebase -i --autosquash, revise works with you. Running git revise --autosquash will automatically reorder and apply fixup commits created with git commit --fixup=$COMMIT and similar tools, and thanks to the implicit base commit, you don't even need to specify it. You can even pass the -i flag if you want to edit the generated todo list before running it. It's non-destructive git-revise doesn't touch either your working directory, or your index. This means that if it's killed while running, your repository won't be changed, and you can't end up in a mid-rebase state while using it. Problems like conflicts are resolved interactively, while the command is running, without changing the actual files you've been working on. And, as no files are touched, git-revise won't trigger any unnecessary rebuilds! What git-revise is not (Section Added: Aug. 6, 2019) git-revise does not aim to be a complete replacement for git rebase -i. It has a specific use-case in mind, namely incremental changes to a patch stack, and excludes features which rebase supports. In my personal workflow, I still reach for git rebase [-i] when I need to rebase my local commits due to new upstream changes, and I imagine there are people with advanced workflows who cannot use git revise. Working directory changes: git-revise does not modify your working directory or index while it's running. This is part of what allows it to be so fast. However, it also means that certain rebase features, such as the edit interactive command, are not possible. This also is why git revise -i does not support removing commits from within a patch series: doing so would require changing the state of your working directory due to the now-missing commit. If you want to drop a commit you can instead move it to the end of the list and mark it as index. The commit will disappear from history, but your index and working directory won't be changed. A quick git reset --hard HEAD will update your index and working directory. These restrictions may change in the future. Features like this have been requested, and it might be useful to allow opting-in to dropping commits on the floor or pausing mid-revise. Merging through renames & copies: git-revise uses a custom merge backend, which doesn't attempt to handle file renames or copies. For changes which need to be merged or rebased through file renames and copies, git rebase is a better option. Complex history rewriting: git rebase supports rebasing complex commits, such as merges. In contrast, git-revise does not currently aim to support these more advanced features of git rebase. Interested? Awesome! git-revise is a MIT-licensed pure-Python 3.6+ package, and can be installed with pip: $ python3 -m pip install --user git-revise You can also check out the source on GitHub, and read the manpage online, or by running man git revise in your terminal. I'll leave you with some handy links to resources to learn more about git-revise, how it works, and how you can contribute! Repository: https://github.com/mystor/git-revise Bug Tracker: https://github.com/mystor/git-revise/issues Manpage: https://git-revise.readthedocs.io/en/latest/man.html Installing: https://git-revise.readthedocs.io/en/latest/install.html Contributing: https://git-revise.readthedocs.io/en/latest/contributing.html ",
        "Reminds me some of Raymond Chen's \"Stupid Git Tricks\" series  [0-6] of blog posts where he used a lot of git commit-tree and similar low level tools to avoid worktree changes and minimize GC churn (partly because of working in the humongous Windows git, which of course seems to have similar issues to the Mozilla ones mentioned here such as auto-rebuild tools).<p>It makes a bunch of sense to build nicer porcelain tools for such low level git magic when it becomes semi-routine.<p>(I couldn't find a good permalink for the entire series as a whole, so linked are all the individual posts.)<p>[0] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190506-00/?p=102478\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190506-00/?p=10...</a><p>[1] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190507-00/?p=102480\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190507-00/?p=10...</a><p>[2] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190508-00/?p=102482\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190508-00/?p=10...</a><p>[3] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190509-00/?p=102485\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190509-00/?p=10...</a><p>[4] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190510-00/?p=102488\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190510-00/?p=10...</a><p>[5] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190513-00/?p=102490\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190513-00/?p=10...</a><p>[6] <a href=\"https://devblogs.microsoft.com/oldnewthing/20190515-00/?p=102495\" rel=\"nofollow\">https://devblogs.microsoft.com/oldnewthing/20190515-00/?p=10...</a>",
        "This sounds exactly like my dream tool, the very thing I've been meaning to write myself and haven't gotten around to it. Thank you for writing this!<p>Suggestion: I'd love to see a `revise.autoSquash` git config flag (like `rebase.autoSquash`) to always autosquash in interactive mode. Maybe you already support it, but if so, the manpage doesn't list it."
      ],
      "relevant": "true"
    },
    {
      "id": 19074170,
      "title": "Show HN: Gita – a CLI tool to manage multiple Git repos",
      "search": [
        "Show HN: Gita – a CLI tool to manage multiple Git repos",
        "ShowHN",
        "https://github.com/nosarthur/gita",
        "_______________________________ ( ____ \\__ __|__ __( ___ ) | ( \\/ ) ( ) ( | ( ) | | | | | | | | (___) | | | ____ | | | | | ___ | | | \\_ ) | | | | | ( ) | | (___) |__) (___ | | | ) ( | (_______)_______/ )_( |/ \\| v0.15 Gita: a command-line tool to manage multiple git repos This tool does two things display the status of multiple git repos such as branch, modification, commit message side by side (batch) delegate git commands/aliases from any working directory If several repos are related, it helps to see their status together. I also hate to change directories to execute git commands. In this screenshot, the gita ll command displays the status of all repos. The gita remote dotfiles command translates to git remote -v for the dotfiles repo, even though we are not in the repo. The gita fetch command fetches from all repos and two of them have updates. To see the pre-defined commands, run gita -h or take a look at cmds.json. To add your own commands, see the customization section. To run arbitrary git command, see the superman mode section. To run arbitrary shell command, see the shell mode section. The branch color distinguishes 5 situations between local and remote branches: color meaning white local has no remote green local is the same as remote red local has diverged from remote purple local is ahead of remote (good for push) yellow local is behind remote (good for merge) The choice of purple for ahead and yellow for behind is motivated by blueshift and redshift, using green as baseline. You can change the color scheme using the gita color command. See the customization section. The additional status symbols denote symbol meaning + staged changes * unstaged changes _ untracked files/folders The bookkeeping sub-commands are gita add <repo-path(s)> [-g <groupname>]: add repo(s) to gita, optionally into an existing group gita add -a <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively and automatically generate hierarchical groups. See the customization section for more details. gita add -b <bare-repo-path(s)>: add bare repo(s) to gita. See the customization section for more details on setting custom worktree. gita add -r <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively gita clone <config-file>: clone repos in config-file (generated by gita freeze) to current directory. gita clone -p <config-file>: clone repos in config-file to prescribed paths. gita context: context sub-command gita context: show current context gita context <group-name>: set context to group-name, all operations then only apply to repos in this group gita context auto: set context automatically according to the current working directory gita context none: remove context gita color: color sub-command gita color [ll]: Show available colors and the current coloring scheme gita color reset: Reset to the default coloring scheme gita color set <situation> <color>: Use the specified color for the local-remote situation gita flags: flags sub-command gita flags set <repo-name> <flags>: add custom flags to repo gita flags [ll]: display repos with custom flags gita freeze: print information of all repos such as URL, name, and path. Use with gita clone. gita group: group sub-command gita group add <repo-name(s)> -n <group-name>: add repo(s) to a new or existing group gita group [ll]: display existing groups with repos gita group ls: display existing group names gita group rename <group-name> <new-name>: change group name gita group rm <group-name(s)>: delete group(s) gita group rmrepo <repo-name(s)> -n <group-name>: remove repo(s) from existing group gita info: info sub-command gita info [ll]: display the used and unused information items gita info add <info-item>: enable information item gita info rm <info-item>: disable information item gita ll: display the status of all repos gita ll <group-name>: display the status of repos in a group gita ll -g: display the repo summaries by groups gita ls: display the names of all repos gita ls <repo-name>: display the absolute path of one repo gita rename <repo-name> <new-name>: rename a repo gita rm <repo-name(s)>: remove repo(s) from gita (won't remove files on disk) gita -v: display gita version The git delegating sub-commands are of two formats gita <sub-command> [repo-name(s) or group-name(s)]: optional repo or group input, and no input means all repos. gita <sub-command> <repo-name(s) or groups-name(s)>: required repo name(s) or group name(s) input They translate to git <sub-command> for the corresponding repos. By default, only fetch and pull take optional input. In other words, gita fetch and gita pull apply to all repos. To see the pre-defined sub-commands, run gita -h or take a look at cmds.json. To add your own sub-commands or override the default behaviors, see the customization section. To run arbitrary git command, see the superman mode section. If more than one repos are specified, the git command runs asynchronously, with the exception of log, difftool and mergetool, which require non-trivial user input. Repo configuration is saved in $XDG_CONFIG_HOME/gita/repos.csv (most likely ~/.config/gita/repos.csv). Installation To install the latest version, run If you prefer development mode, download the source code and run pip3 install -e <gita-source-folder> In either case, calling gita in terminal may not work, then put the following line in the .bashrc file. alias gita=\"python3 -m gita\" Windows users may need to enable the ANSI escape sequence in terminal for the branch color to work. See this stackoverflow post for details. Auto-completion Download .gita-completion.bash or .gita-completion.zsh and source it in shell. Superman mode The superman mode delegates any git command or alias. Usage: gita super [repo-name(s) or group-name(s)] <any-git-command-with-or-without-options> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita super checkout master puts all repos on the master branch gita super frontend-repo backend-repo commit -am 'implement a new feature' executes git commit -am 'implement a new feature' for frontend-repo and backend-repo Shell mode The shell mode delegates any shell command. Usage: gita shell [repo-name(s) or group-name(s)] <any-shell-command> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita shell ll lists contents for all repos gita shell repo1 repo2 mkdir docs create a new directory docs in repo1 and repo2 gita shell \"git describe --abbrev=0 --tags | xargs git checkout\": check out the latest tag for all repos Customization define repo group and context When the project contains several independent but related repos, we can define a group and execute gita command on this group. For example, gita group add repo1 repo2 -n my-group gita ll my-group gita pull my-group To save more typing, one can set a group as context, then any gita command is scoped to the group gita context my-group gita ll gita pull The most useful context maybe auto. In this mode, the context is automatically determined from the current working directory (CWD): the context is the group whose member repo's path contains CWD. To set it, run To remove the context, run It is also possible to recursively add repos within a directory and generate hierarchical groups automatically. For example, running on the following folder structure src project1 repo1 repo2 repo3 project2 repo4 repo5 repo6 gives rise to 3 groups: src:repo1,repo2,repo3,repo4,repo5,repo6 src-project1:repo1,repo2 src-project2:repo4,repo5 add user-defined sub-command using json file Custom delegating sub-commands can be defined in $XDG_CONFIG_HOME/gita/cmds.json (most likely ~/.config/gita/cmds.json) And they shadow the default ones if name collisions exist. Default delegating sub-commands are defined in cmds.json. For example, gita stat <repo-name(s)> is registered as \"stat\":{ \"cmd\": \"git diff --stat\", \"help\": \"show edit statistics\" } which executes git diff --stat for the specified repo(s). To disable asynchronous execution, set disable_async to be true. See the difftool example: \"difftool\":{ \"cmd\": \"git difftool\", \"disable_async\": true, \"help\": \"show differences using a tool\" } If you want a custom command to behave like gita fetch, i.e., to apply to all repos when no repo is specified, set allow_all to be true. For example, the following snippet creates a new command gita comaster [repo-name(s)] with optional repo name input. \"comaster\":{ \"cmd\": \"checkout master\", \"allow_all\": true, \"help\": \"checkout the master branch\" } Any command that runs in the superman mode mode or the shell mode can be defined in this json format. For example, the following command runs in shell mode and fetches only the current branch from upstream. \"fetchcrt\":{ \"cmd\": \"git rev-parse --abbrev-ref HEAD | xargs git fetch --prune upstream\", \"allow_all\": true, \"shell\": true, \"help\": \"fetch current branch only\" } customize the local/remote relationship coloring displayed by the gita ll command You can see the default color scheme and the available colors via gita color. To change the color coding, use gita color set <situation> <color>. The configuration is saved in $XDG_CONFIG_HOME/gita/color.csv. customize information displayed by the gita ll command You can customize the information displayed by gita ll. The used and unused information items are shown with gita info, and the configuration is saved in $XDG_CONFIG_HOME/gita/info.csv. For example, the default setting corresponds to branch,commit_msg,commit_time customize git command flags One can set custom flags to run git commands. For example, with gita flags set my-repo --git-dir=`gita ls dotfiles` --work-tree=$HOME any git command/alias triggered from gita on dotfiles will use these flags. Note that the flags are applied immediately after git. For example, gita st dotfiles translates to git --git-dir=$HOME/somefolder --work-tree=$HOME status running from the dotfiles directory. This feature was originally added to deal with bare repo dotfiles. Requirements Gita requires Python 3.6 or higher, due to the use of f-string and asyncio module. Under the hood, gita uses subprocess to run git commands/aliases. Thus the installed git version may matter. I have git 1.8.3.1, 2.17.2, and 2.20.1 on my machines, and their results agree. Tips effect shell command enter <repo> directory cd `gita ls <repo>` delete repos in <group> gita group ll <group> | xargs gita rm Contributing To contribute, you can report/fix bugs request/implement features star/recommend this project Read this article if you have never contribute code to open source project before. Chat room is available on To run tests locally, simply pytest in the source code folder. Note that context should be set as none. More implementation details are in design.md. A step-by-step guide to reproduce this project is here. You can also sponsor me on GitHub. Any amount is appreciated! Other multi-repo tools I haven't tried them but I heard good things about them. myrepos repo ",
        "One thing this project does really well is to start the readme with a screenshot. I open the link, scroll down to the readme, and I immediately see what sort of user interface/experience I will get. Some commenters have noted that Gita is similar to other multi-repo tools, but both Repo and wstool are more effort to evaluate, because their readmes don't have pictures.",
        "Nice, but is there a way to just run any command? I.e. just `gita <optional repo names/paths> <pass entire command line git -C repodir>`. This has advantages in that you don't have to go round via a command file, don't have to keep it synced across machines, don't have to remember what you put in the file etc, and can just use the git syntax which already took long enough to learn by heart :P<p>I've used multiple multiple repository tools and in the end all I happen to use is one (usually versioned) file to store a list of repositories and then a command which just loops over all repos and applies anything to it. If I need custom commands I use git aliases so that works both for normal git and whatever tool used."
      ],
      "relevant": "true"
    },
    {
      "id": 18874707,
      "title": "Ask HN: Which Wiki or internal documentation tools do you use?",
      "search": [
        "Ask HN: Which Wiki or internal documentation tools do you use?",
        "I am curious what internal documentation tools people are using. My current use case is an engineering handbook and internal documentation that isn't related to any specific codebase.<p>Some basic requirements are:\n- searchable\n- code snippets\n- hyperlinking<p>I know of a few tools and I've even worked at companies that use git repos with markdown. I'd love to hear everyone's thoughts.",
        "AskHN",
        "Notion (<a href=\"https://notion.so\" rel=\"nofollow\">https://notion.so</a>) All in one workspace, combines, trello, airtable, evernote, and docs. Great embed support and awesome for small teams and personal use. It's a modern day commonplace book.",
        "Confluence.<p>It's not bad. It's not great either.  Which is pretty much the best way to describe all Atlassian products. It's unbearable on slow or spotty internet connections. It's riddled with bugs (just the other day my spacebar literally stopped working in their edit view). The formatting gets in your way more than markdown, but at least not as much as something like OneNote.<p>But, it's powerful. Like any Atlassian product, you can script it to do basically whatever you want. You can organize your company workspaces however you'd like. You can set up tables which can automatically pull summary lines of any other page labeled with a specific label, which is great for automatically building indexes or tables of content.<p>Overall, I haven't found anything better, and I write a lot of technical documentation for our company.<p>For comparison's sake:<p>- Github/Gitlab wikis are pretty bad. There's almost no advantage to using them over just storing markdown in the repository.<p>- Which, we did for a while, and it works fine, but in its simplicity it misses some of the key features that I do like about Confluence (like those automated index pages and page comments).<p>- We also used Dropbox Paper for a while; I <i>really</i> like it, but its primarily useful for, let's call them \"transactional\" documentation (write, get feedback, never look at again); It's not very good at being a Wiki, storing long-term long-form information. And given that Confluence can handle both pretty well, there's not a great argument for adopting a new service from an entirely different company we don't otherwise use.<p>- We have G-Suite and thus Drive/Docs. The inability to easily write technical documentation with inline/block code makes it a non-starter. No thanks. If Drive added a markdown editor like Dropbox Paper I'd probably push to switch; the search is pretty great, its a platform we already have, and you get a full, amazing document editor for those documents where it makes sense.<p>- I've never used Quip in a real work setting."
      ],
      "relevant": "true"
    },
    {
      "id": 19276542,
      "title": "Bitbucket Pipes",
      "search": [
        "Bitbucket Pipes",
        "Normal",
        "https://bitbucket.org/blog/meet-bitbucket-pipes-30-ways-to-automate-your-ci-cd-pipeline",
        "The democratizing nature of DevOps has seen the responsibility of building and managing CI/CD pipelines transition from specialized release engineers to developers. But automating a robust, dependable CI/CD pipelineistedious work. Developers need to connect to multiple tools to deliver software, andwriting pipeline integrations for these servicesis a manual, error-prone process. Theres research involved to ensure dependencies are accounted for, as well as debugging and maintainingintegrationswhen updates are made. Its no wonder many teams put automating CI/CD firmly in the too hard basket. But those days are over. In 2016 we launched Bitbucket Pipelines: a CI/CD tool in the cloud that's part of your repository and makes it easy for developers to configure pipelines with code.Andtoday we are launching Bitbucket Pipes to make it easier to build powerful, automated CI/CD workflows in a plug and play fashion without the hassle ofmanaging integrations.Weve worked with industry leaders including Microsoft, AWS, Slack, Google Cloud and more to build supported pipes that help automate your CI/CD pipeline, and made it simple to create your own to help abstract any duplicated configuration across your repositories. Announcing Bitbucket Pipes Whether youre creating a simple deploy pipeline to a hosting service like AWS, utilizing a multi-cloud deployment strategy, or automating a sophisticated pipeline that involves security scanning, monitoring, and artifact management, Bitbucket Pipes makes it easy to build and automate a CI/CD pipeline that meets your exact needs. Simply select the appropriate pipes you need and enter in the variables required by the pipe to run. Not only do supported pipes make it trivial to set up your external services across pipelines and repositories, theyre also updated and maintained by the author meaning you never have to worry about updating or re-configuring them yourself. The end result is an easy way to build, update, modify, and maintain CI/CD pipelines no matter how sophisticated they are. In the example below you can see how easy configuring your pipeline becomes by simply copying and pasting pipes on the right, versus manually typing and configuring the same pipeline on the left. New users can easily browse and select pipes to get started, while more experienced users can not only reuse pipes across their repositories, but discover new and interesting ways to automate their pipelines. An open approach to automation Theres no one-size-fits-all approach to software development ? developers should work with whatever tools best suit their needs. Whether its hosting, monitoring, incident management and everything in-between, weve partnered with some of the best in the industry to bring the tools you already use right into your CI/CD pipeline. These partners are just scratching the surface for Bitbucket Pipes and we have more supported pipes to come. And we want the community involved too ? tell us which services youd like to see or even build your own. Its easy to build pipes that meet your exact needs and we cant wait to see how your team automates your CI/CD workflow. Get started with Bitbucket Pipes Get started with our pre-configured pipes or create your own today. For those new to Bitbucket, sign up, create your first repository and enable Bitbucket Pipelines. For existing Bitbucket Pipelines users, you can find the new Pipes view in the online .yml editor. Read our docs to find out more. ",
        "Free tier(Build minutes: 50 mins/mo)<p>Thanks but no thank you, meanwhile GitLab offers 2000 mins/mo",
        "Seems somewhat similar to resources[0] in Concourse in that they use a container image that has a defined entry point.<p>Concourse refers to this as a \"get step\"[1] or a \"put step\", which calls a pre-defined script inside the container with a custom set of parameters. The \"put step\" is used when you expect side effects, while a \"get step\" is used to check status on some resource and trigger jobs.<p>In general it makes the CI/CD system easily composable and clean. Concourse manages this very well and while I haven't used Bitbucket Pipes I suspect it to be a good experience as well.<p>[0] <a href=\"https://concourse-ci.org/resources.html\" rel=\"nofollow\">https://concourse-ci.org/resources.html</a>\n[1] <a href=\"https://concourse-ci.org/implementing-resources.html\" rel=\"nofollow\">https://concourse-ci.org/implementing-resources.html</a>"
      ],
      "relevant": "true"
    },
    {
      "id": 19842382,
      "title": "Windows Console Tools",
      "search": [
        "Windows Console Tools",
        "Normal",
        "https://github.com/Microsoft/Terminal",
        "Welcome to the Windows Terminal, Console and Command-Line repo This repository contains the source code for: Windows Terminal Windows Terminal Preview The Windows console host (conhost.exe) Components shared between the two projects ColorTool Sample projects that show how to consume the Windows Console APIs Related repositories include: Windows Terminal Documentation (Repo: Contribute to the docs) Console API Documentation Cascadia Code Font Installing and running Windows Terminal Note: Windows Terminal requires Windows 10 1903 (build 18362) or later Microsoft Store [Recommended] Install the Windows Terminal from the Microsoft Store. This allows you to always be on the latest version when we release new builds with automatic upgrades. This is our preferred method. Other install methods Via GitHub For users who are unable to install Windows Terminal from the Microsoft Store, released builds can be manually downloaded from this repository's Releases page. Download the Microsoft.WindowsTerminal_<versionNumber>.msixbundle file from the Assets section. To install the app, you can simply double-click on the .msixbundle file, and the app installer should automatically run. If that fails for any reason, you can try the following command at a PowerShell prompt: # NOTE: If you are using PowerShell 7+, please run # Import-Module Appx -UseWindowsPowerShell # before using Add-AppxPackage. Add-AppxPackage Microsoft.WindowsTerminal_<versionNumber>.msixbundle Note: If you install Terminal manually: Terminal will not auto-update when new builds are released so you will need to regularly install the latest Terminal release to receive all the latest fixes and improvements! Via Windows Package Manager CLI (aka winget) winget users can download and install the latest Terminal release by installing the Microsoft.WindowsTerminal package: winget install --id=Microsoft.WindowsTerminal -e Via Chocolatey (unofficial) Chocolatey users can download and install the latest Terminal release by installing the microsoft-windows-terminal package: choco install microsoft-windows-terminal To upgrade Windows Terminal using Chocolatey, run the following: choco upgrade microsoft-windows-terminal If you have any issues when installing/upgrading the package please go to the Windows Terminal package page and follow the Chocolatey triage process Via Scoop (unofficial) Scoop users can download and install the latest Terminal release by installing the windows-terminal package: scoop bucket add extras scoop install windows-terminal To update Windows Terminal using Scoop, run the following: scoop update windows-terminal If you have any issues when installing/updating the package, please search for or report the same on the issues page of Scoop Extras bucket repository. Windows Terminal 2.0 Roadmap The plan for delivering Windows Terminal 2.0 is described here and will be updated as the project proceeds. Project Build Status Project Build Status Terminal ColorTool Terminal & Console Overview Please take a few minutes to review the overview below before diving into the code: Windows Terminal Windows Terminal is a new, modern, feature-rich, productive terminal application for command-line users. It includes many of the features most frequently requested by the Windows command-line community including support for tabs, rich text, globalization, configurability, theming & styling, and more. The Terminal will also need to meet our goals and measures to ensure it remains fast and efficient, and doesn't consume vast amounts of memory or power. The Windows Console Host The Windows Console host, conhost.exe, is Windows' original command-line user experience. It also hosts Windows' command-line infrastructure and the Windows Console API server, input engine, rendering engine, user preferences, etc. The console host code in this repository is the actual source from which the conhost.exe in Windows itself is built. Since taking ownership of the Windows command-line in 2014, the team added several new features to the Console, including background transparency, line-based selection, support for ANSI / Virtual Terminal sequences, 24-bit color, a Pseudoconsole (\"ConPTY\"), and more. However, because Windows Console's primary goal is to maintain backward compatibility, we have been unable to add many of the features the community (and the team) have been wanting for the last several years including tabs, unicode text, and emoji. These limitations led us to create the new Windows Terminal. You can read more about the evolution of the command-line in general, and the Windows command-line specifically in this accompanying series of blog posts on the Command-Line team's blog. Shared Components While overhauling Windows Console, we modernized its codebase considerably, cleanly separating logical entities into modules and classes, introduced some key extensibility points, replaced several old, home-grown collections and containers with safer, more efficient STL containers, and made the code simpler and safer by using Microsoft's Windows Implementation Libraries - WIL. This overhaul resulted in several of Console's key components being available for re-use in any terminal implementation on Windows. These components include a new DirectWrite-based text layout and rendering engine, a text buffer capable of storing both UTF-16 and UTF-8, a VT parser/emitter, and more. Creating the new Windows Terminal When we started planning the new Windows Terminal application, we explored and evaluated several approaches and technology stacks. We ultimately decided that our goals would be best met by continuing our investment in our C++ codebase, which would allow us to reuse several of the aforementioned modernized components in both the existing Console and the new Terminal. Further, we realized that this would allow us to build much of the Terminal's core itself as a reusable UI control that others can incorporate into their own applications. The result of this work is contained within this repo and delivered as the Windows Terminal application you can download from the Microsoft Store, or directly from this repo's releases. Resources For more information about Windows Terminal, you may find some of these resources useful and interesting: Command-Line Blog Command-Line Backgrounder Blog Series Windows Terminal Launch: Terminal \"Sizzle Video\" Windows Terminal Launch: Build 2019 Session Run As Radio: Show 645 - Windows Terminal with Richard Turner Azure Devops Podcast: Episode 54 - Kayla Cinnamon and Rich Turner on DevOps on the Windows Terminal Microsoft Ignite 2019 Session: The Modern Windows Command Line: Windows Terminal - BRK3321 FAQ I built and ran the new Terminal, but it looks just like the old console Cause: You're launching the incorrect solution in Visual Studio. Solution: Make sure you're building & deploying the CascadiaPackage project in Visual Studio. Note: OpenConsole.exe is just a locally-built conhost.exe, the classic Windows Console that hosts Windows' command-line infrastructure. OpenConsole is used by Windows Terminal to connect to and communicate with command-line applications (via ConPty). Documentation All project documentation is located at aka.ms/terminal-docs. If you would like to contribute to the documentation, please submit a pull request on the Windows Terminal Documentation repo. Contributing We are excited to work alongside you, our amazing community, to build and enhance Windows Terminal! BEFORE you start work on a feature/fix, please read & follow our Contributor's Guide to help avoid any wasted or duplicate effort. Communicating with the Team The easiest way to communicate with the team is via GitHub issues. Please file new issues, feature requests and suggestions, but DO search for similar open/closed pre-existing issues before creating a new issue. If you would like to ask a question that you feel doesn't warrant an issue (yet), please reach out to us via Twitter: Kayla Cinnamon, Program Manager: @cinnamon_msft Dustin Howett, Engineering Lead: @dhowett Michael Niksa, Senior Developer: @michaelniksa Mike Griese, Developer: @zadjii Carlos Zamora, Developer: @cazamor_msft Leon Liang, Developer: @leonmsft Pankaj Bhojwani, Developer Leonard Hecker, Developer: @LeonardHecker Developer Guidance Prerequisites You must be running Windows 1903 (build >= 10.0.18362.0) or later to run Windows Terminal You must enable Developer Mode in the Windows Settings app to locally install and run Windows Terminal You must have PowerShell 7 or later installed You must have the Windows 10 1903 SDK installed You must have at least VS 2019 installed You must install the following Workloads via the VS Installer. Note: Opening the solution in VS 2019 will prompt you to install missing components automatically: Desktop Development with C++ Universal Windows Platform Development The following Individual Components C++ (v142) Universal Windows Platform Tools Building the Code This repository uses git submodules for some of its dependencies. To make sure submodules are restored or updated, be sure to run the following prior to building: git submodule update --init --recursive OpenConsole.sln may be built from within Visual Studio or from the command-line using a set of convenience scripts & tools in the /tools directory: Building in PowerShell Import-Module .\\tools\\OpenConsole.psm1 Set-MsBuildDevEnvironment Invoke-OpenConsoleBuild Building in Cmd Running & Debugging To debug the Windows Terminal in VS, right click on CascadiaPackage (in the Solution Explorer) and go to properties. In the Debug menu, change \"Application process\" and \"Background task process\" to \"Native Only\". You should then be able to build & debug the Terminal project by hitting F5. You will not be able to launch the Terminal directly by running the WindowsTerminal.exe. For more details on why, see #926, #4043 Coding Guidance Please review these brief docs below about our coding practices. If you find something missing from these docs, feel free to contribute to any of our documentation files anywhere in the repository (or write some new ones!) This is a work in progress as we learn what we'll need to provide people in order to be effective contributors to our project. Coding Style Code Organization Exceptions in our legacy codebase Helpful smart pointers and macros for interfacing with Windows in WIL Code of Conduct This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. ",
        "Is there some MS event going on right now?",
        "So Microsoft has the chutzpah to create a github repository trying to dupe people into contributing enhancements to components of their for-profit spyware?"
      ],
      "relevant": "false"
    },
    {
      "id": 19989684,
      "title": "GitHub Sponsors",
      "search": [
        "GitHub Sponsors",
        "Normal",
        "https://github.com/sponsors",
        "GitHub Sponsors Invest in the software that powers your world A new way to contribute to opensource Invest in the open source projects you depend on. Contributors are working behind the scenes to make open source better for everyonegive them the help and recognition they deserve. Open source is an integral component of Stripes software supply chain, providing distribution that meets our users where they are and extending our platform to its fullest potential. Mike Fix, OSS lead & Software Engineer, Stripe Invest in your supply chain Sponsor the open source software your team has built its business on. Fund the projects that make up your software supply chain to improve its performance, reliability, and stability. Sponsor a project At New Relic developers are at the heart of everything we do, and that includes investing in the growth of a thriving open source community. Jonan Scheffler, Director of Developer Relations, New Relic You depend on open source every day A command line tool and library for transferring data with URL syntax, supporting HTTP, HTTPS, FTP, FTPS 18,600 4,000 Sponsor cURL is included in almost every modern devicesmartphones, cars, TVs, laptops, servers, gaming consoles, printers, and beyond. 10,000,000,000+ Installations of cURL worldwide 643 Community contributors 8 Maintainers TLS/SSL and crypto library 14,100 6,300 Sponsor OpenSSL is used to encrypt an estimated 66% of the web, including popular sites like Facebook, Google, and Netflix. 1,320,000,000+ Websites use OpenSSL worldwide 519 Community contributors 18 Maintainers Make open source careers possible Everyone should be able to afford to contribute to open source. Help make open source a viable, lucrative career path for people to create and contribute to our digital infrastructure. Sponsor in three easy steps Visit the projects sponsorship page. Choose a subscription tier. Pay! Explore the projects you depend on @katmeisters goal is to dedicate more time to opensource $15,000 a month When I reach this goal, Ill be able to quit my day job and work on open source full-time. 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 % towards $15,000 goal GitHub Sponsors could eventually lead to a better world where an open source maintainer generates enough income to bring on a second maintainer. David Nolen, Maintainer of Clojurescript Available in 36 regions Join the waitlist to receive updates when we expand. Anyone can sponsor, but you must reside in a supported region to receive funds. Australia Austria Belgium Bulgaria New Canada Cyprus Czech Republic Denmark Estonia Finland France Germany Greece Hong Kong SAR Ireland Italy Japan Latvia Lithuania Luxembourg Malta Mexico Netherlands New Zealand Norway Poland Portugal Romania New Singapore Slovakia Slovenia Spain Sweden Switzerland United Kingdom United States of America Frequently asked questions When can I get off the waitlist to join GitHub Sponsors? If youre in a supported country or region, you can join the program immediately. If youre in an unsupported country or region, join the waitlist for updates on availability in your location. While we cant provide an estimated timeframe, were growing all the time! Do I need to fill out a tax form to receive sponsorships? Yes, your tax information must be on file in order for GitHub to make payments to your bank account. Tax documents collected may vary based on your location. Note that we do our best to help you with the Sponsors program, but were unable to provide you with tax guidance. How do I sponsor a project? If your organization pays via credit card or PayPal, you can join the beta and start sponsoring right away. For organizations with invoiced billing, contact us. Invest in the projects youdepend on Invest in the projects you depend on ",
        "This seems like it's needed and (dareisay) overdue?<p>Integrating sponsorship subscriptions into the core experience is sure to increase payments, a la twitch subscriptions/payments (which Youtube is just now copying).<p>I imagine this will change the fundamental dynamics around OSS projects, but not sure how, nor whether it is all positive.<p>- If maintainers can see who donated, do they prioritize issues / pull requests? (I think that could be a good thing actually).<p>- Do companies use GitHub sponsorships to judge the health of dependencies? Will they create budgets to support their dependencies systematically?<p>- Does this hurt FOSS contributions, because now people start to expect to be paid rather than doing it for inherent motivations? Will this generate toxic politics among project contributors regarding who gets credit + gets paid?<p>- Will this mean that microsoft gets a bunch of PII on top-notch developers (have to enter name + address info to receive or send payments), and get much more value from that data than I can imagine?",
        "This is a nice start for allowing sending \"coffee money\" between persons. If however you want to drive Serious Money into actually funding OSS projects please remember this: While virtually no company has a donations budget, almost every company has a $$$ marketing budget.<p>Please let me give you some of that money that would otherwise be spent on blue pens with logos and endless display ads to GitHub projects. I'd be happy to drive $xxK/mo to open source projects my company depends on or that are simply being used by an audience that aligns with our own. To sell that internally, I need (as in, I would be laughed out of the room to propose it without):<p>- My sponsoring company logo on the GitHub project page<p>- UTM links and all that jazz to attribute traffic and campaigns to the specific projects that we sponsor<p>See <a href=\"https://webpack.js.org/\" rel=\"nofollow\">https://webpack.js.org/</a> for a good example of a successful sponsorship program. Literally the biggest hurdle remaining for BigCorp to sponsor something like Webpack today is selling your boss on \"Patreon\" and \"OpenCollective\". But if you just increase our GitHub budget by a few K/month, AND the marketers get attributable traffic to boot that we can point to, well that's an easy sell!"
      ],
      "relevant": "true"
    },
    {
      "id": 19240168,
      "title": "Git 2.21 Highlights",
      "search": [
        "Git 2.21 Highlights",
        "Normal",
        "https://github.blog/2019-02-24-highlights-from-git-2-21/",
        "The open source Git project just released Git 2.21 with features and bug fixes from over 60 contributors. We last caught up with you on the latest Git releases when 2.19 was released. Heres a look at some of the most interesting features and changes introduced since then. Human-readable dates with --date=human As part of its output, git log displays the date each commit was authored. Without making an alternate selection, timestamps will display in Gits default format (for example, Tue Feb 12 09:00:33 2019 -0800). Thats very precise, but a lot of those details are things that you might already know, or dont care about. For example, if the commit happened today, you already know what year it is. Likewise, if a commit happened seven years ago, you dont care which second it was authored. So what do you do? You could use --date=relative, which would give you output like 6 days ago, but often, you want to relate six days ago to a specific event, like my meeting last Wednesday. So, was six days ago Tuesday, or was it Wednesday that you were interested in? Git 2.21 introduces a new date format that tells you exactly when something occurred with just the right amount of detail: --date=human. Heres how git log looks with the new format: Thats both more accurate than --date=relative and easier to consume than the full weight of --date=default. But what about when youre scripting? Here, you might want to frequently switch between the human and machine-readable formats while putting together a pipeline. Git 2.21 has an option suitable for this setting, too: --date=auto:human. When printing output to a pager, if Git is given this option it will act as if it had been passed --date=human. When otherwise printing output to a non-pager, Git will act as if no format had been given at all. If human isnt quite your speed, you can combine auto with any other format of your choosing, like --date=auto:relative. [source] Detecting case-insensitive path collisions One commonly asked Git question is, After cloning a repository, why does git status report some of the files as modified? Quite often, the answer is that the repository contains a tree which cannot be represented on your file system. For instance, if it contains both file as well as FILE and your file system is case-insensitive, Git can only checkout one of those files. Worse, Git doesnt actually detect this case during the clone; it simply writes out each path, unaware that the file system considers them to be the same file. You only find out something has gone wrong when you see a mystery modification. The exact rules for when this occurs will vary from system to system. In addition to folding what we normally consider upper and lowercase characters in English, you may also see this from language-specific conversions, non-printing characters, or Unicode normalization. In Git 2.20, git clone now detects and reports colliding groups during the initial checkout, which should remove some of the confusion. Unfortunately, Git cant actually fix the problem for you. What the original committer put in the repository cant be checked out as-is on your file system. So if youre thinking about putting files into a multi-platform project that differ only in case, the best advice is still: dont. [source] Performance improvements and other bits Behind the scenes, a lot has changed over the last couple of Git releases, too. Were dedicating this section to overview a few of these changes. Not all of them will impact your Git usage day-to-day, but some will, and all of the changes are especially important for server administrators. Multi-pack indexes Git stores objects (e.g., representations of the files, directories, and more that make up your Git repository) in both the loose and packed formats. A loose object is a compressed encoding of an object, stored in a file. A packed object is stored in a packfile, which is a collection of objects, written in terms of deltas of one another. Because it can be costly to rewrite these packs every time a new object is added to the repository, repositories tend to accumulate many loose objects or individual packs over time. Eventually, these are reconciled during a repack operation. However, this reconciliation is not possible for larger repositories, like the Windows repository. Instead of repacking, Git can now create a multi-pack index file, which is a listing of objects residing in multiple packs, removing the need to perform expensive repacks (in many cases). [source] Delta islands An important optimization for Git servers is that the format for transmitted objects is the same as the heavily-compressed on-disk packfiles. That means that in many cases, Git can serve repositories to clients by simply copying bytes off disk without having to inflate individual objects. But sometimes this assumption breaks down. Objects on disk may be stored as deltas against one another. When two versions of a file have similar content, we might store the full contents of one version (the base), but only the differences against the base for the other version. This creates a complication when serving a fetch. If object A is stored as a delta against object B, we can only send the client our on-disk version of A if we are also sending them B (or if we know they already have B). Otherwise, we have to reconstruct the full contents of A and re-compress it. This happens rarely in many repositories where clients clone all of the objects stored by the server. But it can be quite common when multiple distinct but overlapping sets of objects are stored in the same packfile (for example, due to repository forks or unmerged pull requests). Git may store a delta between objects found only in two different forks. When someone clones one of the forks, they want only one of the objects, and we have to discard the delta. Git 2.20 solves this by introducing the concept of delta islands. Repository administrators can partition the ref namespace into distinct islands, and Git will avoid making deltas between islands. The end result is a repository which is slightly larger on disk but is still able to serve client fetches much more cheaply. [source 1, source 2] Delta reuse with bitmaps We already discussed the importance of reusing on-disk deltas when serving fetches, but how do we know when the other side has the base object they need to use the delta we send them? If were sending them the base, too, then the answer is easy. But if were not, how do we know if they have it? That answer is deceptively simple: the client will have already told us which commits it has (so that we dont bother sending them again). If they claim to have a commit which contains the base object, then we can re-use the delta. But theres one hitch: we not only need to know about the commit they mentioned, but also the entire object graph. The base may have been part of a commit hundreds or thousands of commits deep in the history of the project. Git doesnt traverse the entire object graph to check for possible bases because its too expensive to do so. For instance, walking the entire graph of a Linux kernel takes roughly 30 seconds. Fortunately, theres already a solution within Git: reachability bitmaps. Git has an optional on-disk data structure to record the sets of objects reachable from each commit. When this data is available, we can query it to quickly determine whether the client has a base object. This results in the server generating smaller packs that are produced more quickly for an overall faster fetch experience. [source] Custom alternates reference advertisement Repository alternates are a tool that server administrators have at their disposal to reduce redundant information. When two repositories are known to share objects (like a fork and its parent), the fork can list the parent as an alternate, and any objects the fork doesnt have itself, it can look for in its parent. This is helpful since we can avoid storing twice the vast number of objects shared between the fork and parent. Likewise, a repository with alternates advertises tips it has when receiving a push. In other words, before writing from your computer to a remote, that remote will tell you what the tips of its branches are, so you can determine information that is already known by the remote, and therefore use less bandwidth. When a repository has alternates, the tips advertisement is the union of all local and alternate branch tips. But what happens when computing the tips of an alternate is more expensive than a client sending redundant data? It makes the push so slow that we have disabled this feature for years at GitHub. In Git 2.20, repositories can hook into the way that they enumerate alternate tips, and make the corresponding transaction much faster. [source] Tidbits Now that weve highlighted a handful of the changes in the past two releases, we want to share a summary of a few other interesting changes. As always, you can learn more by clicking the source link, or reading the documentation or release notes. Have you ever tried to run git cherry-pick on a merge commit only to have it fail? You might have found that the fix involves passing -m1 and moved on. In fact, -m1 says to select the first parent as the mainline, and it replays the relevant commits. Prior to Git 2.21, passing this option on a non-merge commit caused an error, but now it transparently does what you meant. [source] Veteran Git users from our last post might recall that git branch -l establishes a reflog for a newly created branch, instead of listing all branches. Now, instead of doing something you almost certainly didnt mean, git branch -l will list all of your repositorys branches, keeping in line with other commands that accept -l. [source] If youve ever been stuck or forgotten what a certain command or flag does, you might have run git --help (or git -h) to learn more. In Git 2.21, this invocation now follows aliases, and shows the aliased commands helptext. [source] In repositories with large on-disk checkouts, git status can take a long time to complete. In order to indicate that its making progress, the status command now displays a progress bar. [source] Many parts of Git have historically been implemented as shell scripts, calling into tools written in C to do the heavy lifting. While this allowed rapid prototyping, the resulting tools could often be slow due to the overhead of running many separate programs. There are continuing efforts to move these scripts into C, affecting git submodule, git bisect, and git rebase. You may notice rebase in particular being much faster, due to the hard work of the Summer of Code students, Pratik Karki and Alban Gruin. The -G option tells git log to only show commits whose diffs match a particular pattern. But until Git 2.21, it was searching binary files as if they were text, which made things slower and often produced confusing results. [source] Thats all for now We went through a few of the changes that have happened over the last couple of versions, but theres a lot more to discover. Read the release notes for 2.21, or review the release notes for previous versions in the Git repository. ",
        "Because git != GitHub, the actual change log can be found here: <a href=\"https://public-inbox.org/git/xmqqtvgtkq46.fsf@gitster-ct.c.googlers.com/\" rel=\"nofollow\">https://public-inbox.org/git/xmqqtvgtkq46.fsf@gitster-ct.c.g...</a>",
        "Detecting files that differ only in case being cloned onto a case-insensitive file system is a great feature. We recently ran into an issue when using the Nix build tool, which caches inputs based on a SHA hash, where it generated different hashes between Linux (w/ a case sensitive file system) and Mac (with a case insensitive one) machines.<p>Tracking that one down was pretty tricky. I didn't even consider that something at the Git level could be done to improve the situation!"
      ],
      "relevant": "true"
    },
    {
      "id": 19540845,
      "title": "Git implemented in Rust",
      "search": [
        "Git implemented in Rust",
        "Normal",
        "https://github.com/chrisdickinson/git-rs",
        "git-rs Implementing git in rust for fun and education! This is actually my second stab at it, so big blocks will land in place from my first attempt. I'm trying again this year after reading more of \"Programming Rust\" (Blandy, Orendorff). TODO Read objects from loose store Read objects from pack store Read packfile indexes Read delta'd objects Fix interface so we don't need to run open for each read() BUG: certain OFS deltas are misapplied. Isolate the error case Fix it Load refs off of disk Parse git signatures (\"Identity\"'s) Create iterator for walking commit graph Create iterator for walking trees Materialize trees to disk (post gitindex?) Create index from packfile Rename Storage trait to Queryable Rework object loading API from <Type + Boxed reader> to \"we take a writable object\" Carry the rework out through StorageSet Create the index Wrap it in a nice API refs v2 Load refs on demand Load packed-refs .git/index support Read git index cache Write git index cache Create interface for writing new objects Add benchmarks Create packfile from list of objects (API TKTK) Network protocol receive-pack send-pack Try publishing to crates Write documentation Use crate in another project PLAN 2019-02-08 Update It's been a minute! As you might have seen, figuring out packfile indexing has forced a lot of changes on the repo. There's now a src/pack/read.rs file that holds generic read implementations for any BufRead + Seek. The signature of the Storage trait changed -- instead of returning a boxed read object, it now accepts a Write destination. Further, Storage is now Queryable (a better name!). Because we moved from returning a Box to accepting generic Write, we could no longer box Queryables. I didn't know this about Rust, so TIL! StorageSet objects had to be rethought as a result -- they could no longer contain Box'd Storage objects. Instead, we put the compiler to work -- because storage sets are known at compile time, I implemented Queryable for the unit type, (), two types (S, T), and arrays of single types Vec<T>. This means that a StorageSet may hold a single, top-level Queryable, which might contain nested heterogenous Queryable definitions. It gives me warm, fuzzy feelings You might also note that we're not actually done indexing packfiles. Here's the sitch: in order to create a packfile index, you have to run a CRC32 over the compressed bytes in the packfile. The ZlibDecoder will pull more bytes from the underlying stream than it needs, so you can't take the route of handing it a CrcReader and get good results. It's got to be a multi-pass deal. The current plan is: run one pass to get offsets, un-delta'd shas and types. Run a second pass to resolve CRCs and decompress deltas. This can be done in parallel. 2019-01-23 Update It's time to start indexing packfiles. This'll let us start talking to external servers and cloning things! However, it's kind of a pain. Packfiles (viewed as a store) aren't hugely useful until you have an index, so I had designed them as an object that takes an optional index from outside. My thinking was that if an index was not given, we would build one in-memory. That just blew up in my face, a little bit. In order to build an index from a packfile you have to iterate over all of the objects. For each object, you want to record the offset and the SHA1 id of the object at that offset. However, the object might be an offset or a reference delta. That means that in order to index a packfile, you've got to be able to read delta'd objects at offsets within the packfile (implying you already have the Packfile instance created) and outside of the packfile ( implying you have a StorageSet.) In other words: my assumptions about the program design are wrong. So, in the next day or so I'll be reversing course. It should be possible to produce a Packfile as a non-store object and iterate over it. The \"store\" form of a packfile should be the combination of a Packfile and an Index (a PackfileStore.) This means I'll be splitting the logic of src/stores/mmap_pack into \"sequential packfile reads\" and \"random access packfile reads (with an index.)\" It's fun to be wrong 2019-01-21 Update Well, that was a fun bug. Let's walk through it, shall we? This occasionally showed up when a delta would decode another delta'd object. I found a hash that would reliably fail to load. We'd fail the read because the incoming base object would not match the 2nd delta's \"base size\". Here. Removing the check to see if I got the deltas wrong would cause the thread to panic -- the delta's base size wasn't a lie. First, I switched back to my old mmap-less packfile implementation, because I recently touched that code. \"Revert the thing you touched last\" is a winning strategy in these cases: doesn't cost expensive thinking, quickly puts bounds around the bug. Alas, the old packfile implementation also had this bug. No dice. I compared the output of this git implementation to my JS implementation. I confirmed that the output of the JS implementation worked by comparing its output for the hash of concern to vanilla git. After confirming that, I logged out the offsets being read from the file and the expected sizes. I compared this to similar debugging output I added to git-rs. The offsets are the bound values sent into the packfile. For the outermost read (\"Give me the object represented by eff4c6de1bd15cb0d28581e4da17035dbf9e8596\"), the offsets come from the packfile index. For OFS_DELTA (\"offset delta\") types, the start offset is obtained by reading a varint from the packfile and subtracting it from the current start offset. The offsets and expected sizes matched! This meant that: I was reading the correct data from the packfile I was reading varints correctly The bug must be in the application of the delta to a base object From there I added logging above these state transitions, noting the particulars of the operation. I added the same logging to the JS implementation, and found that (aside from totally bailing out ahead of the 2nd delta application) the commands were the same. So it wasn't even that my delta code was wrong: it was my Read state machine. At this point, I was like: \"This is a read state machine bug. I know this.\" So, one of the things this state machine does is carefully bail out if it can't write all of the bytes for a command. (\"If there remains an extent to write, record the next state and break the loop.\") However, at this point we've already consumed the last command. There are no more instructions. So if this function were to be called again, ... We would politely (but firmly) tell the caller to buzz off (written == 0, here.). The fix turned out to be simple, as these fixes usually are. (I need to write a test for this, I know. I know. Pile your shame on me.) So what did we learn? Always test your state machines, folks. (I've said it once, and I'm saying it again.) Malleable reference implementations will save your bacon. Make sure you can trust your reference implementation. Anyway. The tree reader works now! 2019-01-19 Update It's slightly faster! mmap sped things along nicely, shaving 20ms off of our runtime. We're still reliably slower than git, though. It might be because we load the refset immediately. I kept the immutable \"file per read\" packfile store around; I think it may come in handy in the future. It would be excellent to capture this as a benchmark instead of running it ad-hoc. I integrated the tree walking iterator and got a nice surprise: There's a bug in my OFS delta code! This is interesting, because it only appears for certain blobs in certain repositories. Otherwise other OFS deltas seem to resolve cleanly. Case in point: many of the commits I load as a test of the commit walk-er are OFS-delta'd. Also of note: I've split from src/bin.rs into dedicated binaries for tree walking and commit walking. Today's theme: isolate the bug in a test case. EOD Update: It's really helpful to have a reference implementation. I've confirmed that the reference implementation can read the object that breaks this project. We are reading the same offsets, as well (phew) I've further confirmed that swapping out the packfile implementation for the older, slower packfile doesn't affect anything. I suspect this means there's either a problem in my delta code (highly possible!), my varint decoding code (very possible), or the Read implementation for Deltas. Yay, narrowed down results! 2019-01-15 Update I added an (experimental) git_rs::walk::tree iterator to take a Tree and yield a path + a blob for each item. It's probably slower than it should be: for each item it has to clone a PathBuf, because I couldn't work out the lifetimes. If you know how to fix that, please open an issue and let me know I took some time to clean up the warnings during builds. Oh! I also installed Clippy which warns about higher level antipatterns in Rust! I'm still noodling over the 2-3x slowdown between vanilla git and Our Git. I think I might create two packfile interfaces -- one \"generic\" and one \"mmap\"'d, to see if one or the other makes up the difference in performance. This also has the virtue of being unsafe code, which is something I have not yet used in Rust! 2019-01-06 Update I wrote an iterator for commits! The first cut kept a Vec of (Id, Commit) around, so we could always pick the most recent \"next\" commit out of the graph (since commits may have many parents.) But in finishing up the collections section of \"Programming Rust\" I noticed that BinaryHeap was available, which keeps entries in sorted order. You don't often get to choose the underlying storage mechanism of your collections in JS, so this hadn't occurred to me! Anyway. I swapped out the Vec for a BinaryHeap in this commit. Because this pushes the ordering into an Ord impl for a type, this opens up the possibility of using the one iterator definition for multiple different orderings. Neat! Testing against a couple long-lived repo, the results coming out of git_rs are exactly the same as git! However, it takes about twice the time: 60ms for git_rs where git takes 30ms. I think I have a lead on this, and it has to do with packfile stores: each read from a packfile opens a new File instance. I've added a TODO section to keep track of what comes next! 2019-01-02 Update I implemented ref loading. It was a bit of a pain! Translating to and from Path types took a bit of doing. I've been trying to read up on Rust idioms -- I found a couple of resources: The Rust API Guidelines doc has been very helpful. @mre's idiomatic rust repo collects many interesting links. I've also been idly checking out videos from RustConf 2018 As a result, I've implemented FromStr for Id, (hopefully) giving it a more idiomatic API -- let id: Id = str.parse()? 2018-12-27 Update Rust is feeling more natural. This chain felt natural to write. I was even able to cross-index a list with only a minimum of fighting the borrow checker. I split the objects interface into Type + boxed read with a method for reifying the data into an Object. This feels good! It lets folks check to see, for example, if they're referring to a Blob without having to load the entire Blob into memory. The closure interface for the loose interface works pretty well, but pack interfaces need to be able to ask the overarching set of stores for a reference due to REF_DELTA objects. This is a bummer, because it really quickly turns into \"fighting the borrow checker.\" Right now I think the way forward is to build a StorageSet that holds a Vec of heterogenous Box<Storage> objects, where Storage is a new trait that specifies get(&Id, &StorageSet). A sidenote re: the loose store: it feels kind of odd to have to produce a git_rs::Error instead of a std::io::Error. Room for improvement! Oh! It was pretty easy to add a binary to this lib crate. And now we can git log other repos! 2018-12-21 Update Decided to focus on moving a bit slower and making sure I have tests for primitives this time around. Moved away from my original Box<Write> trait object design for object instance reading & storage format in favor of generics. ",
        "I love to see people reimplementing existing tools on their own, because I find that to be a great way to learn more about those tools. I started on a Git implementation in Rust as well, though I haven't worked on it in a while: <a href=\"https://github.com/avik-das/gitters\" rel=\"nofollow\">https://github.com/avik-das/gitters</a>",
        "If you're interested in this, you may enjoy \"Building Git\" by James Coglan - a comprehensive book that takes you through reimplementing git in Ruby.<p><a href=\"https://shop.jcoglan.com/building-git/\" rel=\"nofollow\">https://shop.jcoglan.com/building-git/</a>"
      ],
      "relevant": "true"
    },
    {
      "id": 20499070,
      "title": "GitHub is down",
      "search": [
        "GitHub is down",
        "Normal",
        "https://www.githubstatus.com/?",
        "All Systems Operational Git Operations ? Operational API Requests ? Operational Visit www.githubstatus.com for more information Operational Pull Requests ? Operational GitHub Actions ? Operational GitHub Packages ? Operational GitHub Pages ? Operational Operational Degraded Performance Partial Outage Major Outage Maintenance Past Incidents Nov 6, 2021 No incidents reported today. Nov 5, 2021 No incidents reported. Nov 4, 2021 Resolved - This incident has been resolved. Nov 4, 18:54 UTC Investigating - We are investigating reports of degraded performance for GitHub Actions. Nov 4, 18:13 UTC Nov 3, 2021 No incidents reported. Nov 2, 2021 No incidents reported. Nov 1, 2021 No incidents reported. Oct 31, 2021 No incidents reported. Oct 30, 2021 No incidents reported. Oct 29, 2021 No incidents reported. Oct 28, 2021 No incidents reported. Oct 27, 2021 No incidents reported. Oct 26, 2021 No incidents reported. Oct 25, 2021 No incidents reported. Oct 24, 2021 No incidents reported. Oct 23, 2021 No incidents reported. ",
        "Although it's nice that Git is Git and we can all mostly still work, it still seems foolish to rely on a single point of failure like Github. I've been toying around with the idea of creating a tool that would map the Git api to work with two+ hosting services at the same time. The effect would be something like, run \"$git push\" and it pushes to Github, Bitbucket, and Gitlab. I can't imagine something like this would be too difficult and would eliminate having to twiddle your thumbs while you wait for things to come back up.",
        "I sometimes look forward to outages like this just so I can read the post-downtime-resolution blog post that almost always follow. I find reading about how companies deal with issues when shit hits the fan to be really interesting."
      ],
      "relevant": "true"
    },
    {
      "id": 19750271,
      "title": "Gitlab 11.10 Released",
      "search": [
        "Gitlab 11.10 Released",
        "Normal",
        "https://about.gitlab.com/2019/04/22/gitlab-11-10-released/",
        "Easily see pipeline health across projects GitLab continues to add features to provide visibility into the DevOps lifecycle. This release enhances the Operations Dashboard with a powerful feature that provides an overview of pipeline status. This is handy even when looking at a single project's pipeline, but is especially valuable when using multi-project pipelines - common when you have a microservices architecture and you need to run a pipeline to test and deploy code housed in multiple different project repositories. Now you can get instant visibility at a glance into the health of all of your pipelines on the Operations Dashboard, no matter where they run. Run pipelines against merged results Over time its possible for your source and target branches to diverge, which can result in the scenario where both source and target pipelines pass, but the combined output fails. Now, you can run pipelines against the merged result prior to merging. This allows you to quickly catch errors that would only surface if you had rebased often, allowing for much quicker resolution of pipeline failures and more efficient usage of GitLab Runners. Further streamline collaboration With GitLab 11.10, we provide even more features to simplify collaboration and developer workflows. In a previous release, we introduced merge request suggestions, allowing a reviewer to suggest a one-line change in a merge request comment that can be readily committed from within the comment thread interface. Our users loved it and wanted more. Now, you can suggest a multi-line change, specifying which existing lines to remove, and introducing multiple lines of additions. Thank you for contributing improvement suggestions! And so much more So many great features are available in this release, like Scoped Labels, a more thorough Container Registry cleanup, Composable Auto DevOps, and the ability to purchase additional CI Runner minutes. Read on to learn about them all! Join us for an upcoming event This month's Most Valuable Person (MVP) is Takuya Noguchi This months MVP goes to Takuya Noguchi. Takuya made many contributions to GitLab, including fixing bugs, cleaning up both backend and frontend technical debt, as well as making UI improvements. Thank you! Key improvements released in GitLab 11.10 The Operations Dashboard in GitLab is a powerful feature allowing users to have an overview of project information throughout the entire GitLab instance. You add individual projects, one by one, so its flexible to whichever specific projects are of interest. With this release, we added pipeline status information to the Operations Dashboard. This helps teams view the pipeline health of all the projects that they care about, together in a single interface. Pipelines for Merged Results When working in a feature (source) branch, its normal to have it diverge over time from the target branch if you arent rebasing frequently. This can result in a situation where both the source and target branchs pipelines are green and there are no merge conflicts, but the combined output will result in a failed pipeline due to an incompatibility between the changes. By having your merge request pipeline automatically create a new ref that contains the combined merge result of the source and target branch, then running the pipeline against that ref, we can better ensure that the combined result will be valid. Please note that if you are using merge request pipelines (in any capacity) and you use private GitLab runners that are version 11.8 or older, you will need to upgrade them to avoid running into the issue described in gitlab-ee#11122. Users of GitLabs shared Runner fleet are not impacted. Suggest changes to multiple lines Collaborating on merge requests often involves spotting problems and suggesting a solution. In GitLab 11.6, we introduced support for suggesting a change to a single line. With 11.10, changes can now be suggested to multiple lines when leaving a comment on a merge request diff, and accepted with a single click, by any user with write permissions to the source branch. This new feature avoids the copy/paste workflow of old. Scoped Labels Scoped Labels enable teams to apply mutually exclusive labels (that share the same scope) on an issue, merge request, or epic, solving the use cases of custom fields and custom workflow states. They are configured simply using a special double colon syntax in the label title. Suppose you wanted a custom field in issues to track the platform operating system that your features target. And each issue should only target one platform. You would create labels platform::iOS, platform::Android, platform::Linux, and others, as necessary. Applying any one of these labels on a given issue would automatically remove any other existing label that starts with platform::, as desired. Suppose you have the labels workflow::development, workflow::review, and workflow::deployed, representing workflow states of your particular team. If an issue already has the label workflow::development applied, and a developer wanted to advance the issue to workflow::review, they would simply apply that label, and the workflow::development label would automatically be removed, as desired. This behavior already exists when you move issues across label lists in an issue board representing your teams workflow. But now team members who may not be working in an issue board directly, would still nonetheless be able to advance workflow states consistently in issues themselves. More thorough Container Registry cleanup In normal use of the Container Registry with CI pipelines, typically you will end up pushing many iterative revisions to the same tag. Due to the way Docker Distribution is implemented, the default behavior is to preserve all revisions in the system this ends up consuming a lot of space under this usage pattern. By using the -m parameter with registry-garbage-collect, administrators now have an easy way to wipe out these historical revisions and free up valuable storage space. Purchase add-on CI Runner minutes Users on GitLab.coms paid plans (Gold, Silver, Bronze) can now purchase additional CI Runner minutes. Previously, users were limited by the minutes quota included in their plan. This improvement enables users to proactively purchase minutes in addition to their free minutes, which reduces the potential for any interruptions in service due to stopped pipelines. The current price is $8 for 1,000 minutes and there is no cap to how many add-on minutes you can buy. Your add-on minutes will only begin to be used once your monthly quota has been used and whatever add-on minutes are left at the end of the month will roll over. In a future release, we aim to extend this to Free plans as well. Composable Auto DevOps Auto DevOps enables teams to adopt modern DevOps practices with little to no effort. Starting in GitLab 11.10 each job of Auto DevOps is being made available as an independent template. Using the includes feature of GitLab CI, users can include only certain stages of Auto DevOps while continuing to use their own custom gitlab-ci.yml. This will enable teams to include just the desired jobs while taking advantage of any updates made upstream. Until now, managing group membership on GitLab.com was a manual effort. Youre now able to use SAML SSO and manage group membership with SCIM, allowing your organization to create, remove, and update users on GitLab.com. This is especially useful for enterprises who typically manage large numbers of users with centralized identity providers. Now, youre able to use a provider like Azure Active Directory as the single source of truth and feel confident that your users will be provisioned and de-provisioned automatically based on your identity provider, not by hand. Previously, SAML-based SSO for groups required that a user sign in with both their GitLab user credentials and their identity provider. Now, a user will be able to use SSO to immediately sign in with a GitLab user linked to the configured group. This removes the need to sign in twice, making SAML SSO more convenient and useful for enterprises using it on GitLab.com. Other improvements in GitLab 11.10 Child Epics roadmap In a previous release, we added Child Epics the ability to have epics of epics to help teams manage work breakdown structures. Child epics are shown in the epic page of the parent epic. With this release, you can now see a roadmap view of the child epics in the parent epic page itself. This helps teams see the timeline view of those child epics, allowing you to manage time dependencies. Filter merge requests by target branch Git workflows for releasing or deploying software often involve multiple long-running branches, either for backporting fixes to older versions (e.g. stable-11-9) or moving through a QA process to product (e.g. integration), but finding the merge requests that target these branches can be difficult among many open merge requests. The merge request list for projects and groups can now be filtered by the target branch of the merge request, making it simpler to find the merge request you are looking for. Thank you, Hiroyuki Sato, for the contribution! Sort Wiki pages by created date A projects Wiki allows teams to share documentation and other important information conveniently, side by side with source code and issues. In this release, the list of pages in a Wiki can be sorted by created date and title, allowing users to locate recently created content quickly. See Load Balancer metrics in your Grafana dashboard Ensuring your GitLab instance stays healthy is critical. Weve previously given you default dashboards to review in our bundled-in Grafana instance. Starting with this release, we now include additional dashboards to monitor your NGINX load balancers. Multiple queries per chart GitLab allows you to create charts to visualize the metrics you are collecting. Often, such as when you want to see the maximum and average value of a metric, its crucial to be able to visualize multiple values on the same chart. Starting with this release, you can now do so. Enrich Container Scanning report with more metadata This release enriches the Container Scanning report with more metadata, adding impacted component (a Clair feature) to the existing metadata: priority, identifier (with a link on mitre.org), and affected layer (ex: debian:8). Multi-module Maven projects support for Dependency Scanning This release enables multi-module Maven projects for GitLab Dependency Scanning. Previously, if a sub-module had a dependency with another sub-module sibling, it could not resolve the download from the Maven Central repository. Now a multi-module Maven project is created with two modules and a dependency between the two modules. The sibling dependency is now available in the local Maven repo, permitting the build to continue. Allow users to change the path for cloning in CI By default, the GitLab Runner clones the project to a unique subpath of the $CI_BUILDS_DIR. However, for some projects, such as Golang projects, there may be a requirement to have the code cloned to a specific directory to be built. In GitLab 11.10, weve introduced a variable called GIT_CLONE_PATH that allows users to specify the specific path that the GitLab Runner will clone to before starting the job. Enable/disable Auto DevOps at the Group level Enabling Auto DevOps for your GitLab.com project provides an easy way to get started with modern DevOps workflows, from build all the way to deploy. Starting with GitLab 11.10 weve added the ability to enable/disable Auto DevOps for all the projects that are part of any given group. Update Kubernetes deployments label selector Deploy boards provide an easy way to gain insight into your Kubernetes deployments. As part of this release, were updating the way labels are matched to deployments; deploy boards will now match by app.example.com/app and app.example.com/env or app. This will allow us to prevent conflicts when doing filtering and risk incorrect deploys associated to the project. Additionally, starting in GitLab 12.0 we will remove app label matching from Kubernetes deployment selector and will instead match only on app.example.com/app and app.example.com/env. Group Runners for group-level clusters Group-level clusters now support the installation of GitLab Runner. Group-level Kubernetes runners will appear for child projects as group runners tagged with the labels cluster and kubernetes. Add control for git clean flags for GitLab CI/CD jobs By default, GitLab Runner runs a git clean as part of the process of checking out your code while running a job in GitLab CI/CD. In GitLab 11.10, were adding the ability for users to control the flags passed to the git clean command. This will help teams who have dedicated runners or are building projects from large mono repositories to manage the checkout process prior to executing their scripts. The new variable GIT_CLEAN_FLAGS defaults to -ffdx and accepts all the possible options of the git clean command. Secure environments may require checking with an additional external authorization resource to permit project access. We added support for this additional layer of access control in 10.6, and weve heard requests from the community to move this functionality to Core. Were happy to do this for External Authorization and bring this additional level of security to Core instances, since its a feature cared about by individual contributors. Merge request popovers In this release, we introduce enriched popovers when hovering over a merge request link. While we previously only displayed the title of the merge request, you can now view the merge request status, CI pipeline status, title, and short URL. In future releases, we plan to add additional important information like assignees and milestones and bring popovers to issues too. Push and merge when pipeline succeeds Trunk-based development claims that long-running branches should be avoided in favor of small, short-lived, single-owner branches. For the smallest changes it is not uncommon to push directly to the target branch, but this runs the risk of breaking the build. In this release, GitLab supports new Git push options to open a merge request automatically, set the target branch, and enable merge when the pipeline succeeds via the command line, at the moment that you push to your branch. Improved integration with external monitoring dashboards GitLab can access multiple Prometheus servers (at the environment, project and soon group levels) but the complexity of multiple endpoints can be difficult or unsupported by common dashboarding tools. In this release teams can now interact with a single Prometheus API interface, making integration with services like Grafana much simpler. Monitor resources requested by your cluster GitLab can assist you by monitoring the Kubernetes cluster you use for your staging and production applications. Starting in this release, you can monitor the requested CPU and Memory resources by your cluster helping you spot potential application impacts before they happen. SAST for Elixir We are continuing to add support for more languages and depth in our security scans. With this release, we enable security scanning for projects created using Elixir, now broadened to projects created using the Phoenix framework. Show DAST results in the Group Security Dashboard We have added Dynamic Application Security Testing (DAST) results in the Group Security Dashboard to accompany results already present for SAST, Container Scanning, and Dependency Scanning. Add metrics report type to merge requests GitLab already provides several report types to be included directly into the merge request from Code Quality and Unit Test reports from the Verify stage to SAST and DAST from the Secure stage. While those specific report types are very valuable, there is also value in providing a primitive that can be used across many different types of use cases. In GitLab 11.10 we are shipping metrics reporting directly in the merge request that expects a simple key/value pair of metrics. This will allow users to track any changes, including custom metrics, over time and how they change with a given merge request. Use cases such as memory usage, specialized load testing, and other code health statuses can be converted to simple metrics that will then be exposed directly in the MR alongside the other built-in reports. Simple masking of protected variables in logs GitLab provides several ways to protect and limit the scope of variables in GitLab CI/CD. However, there are still ways that variables can leak into build logs, either intentionally or unintentionally. GitLab takes risk management and audit seriously and continues to add features to help with compliance efforts. In GitLab 11.10, weve added the ability to mask certain types of variables if they are found in the job trace logs, adding a level of protection against accidentally leaking the content of those variables into the logs. Also, GitLab will now automatically mask many of the built-in token variables. Simplified and improved license page To improve the user experience and make handling license keys easier, weve redesigned the license page in the admin panel and emphasized the most important elements of the page. Just-in-time Kubernetes resource creation GitLabs Kubernetes integration takes advantage of RBAC security by creating a service account and a dedicated namespace for each GitLab project. Starting with this release, to maximize the efficiency with which these resources are created, they will be created only when they are needed for deployment. When a Kubernetes deployment takes place, GitLab CI will create the resources prior to deployment. Show function invocation count for Knative functions Functions deployed with GitLab Serverless will now include the number of invocations received for the particular function. Showing the number of invocations requires Prometheus to be installed on the cluster where Knative is installed. Allow Developers to create projects in groups in Core We added a configurable option to allow the Developer role to create projects in groups back in 10.5, and were adding this option to Core. Creating projects is a key capability for productivity in GitLab, and moving this option to Core helps reduce barriers when members of an instance want to work on something new. Fix returned project_id in blob search API with Elasticsearch We fixed a bug in the blob search API with Elasticsearch that was incorrectly returning 0 for project_id. You will need to reindex Elasticsearch to get the correct project_id values after installing this version of GitLab. Omnibus improvements The following improvements have been made to Omnibus in GitLab 11.10: GitLab 11.10 includes Mattermost 5.9.0, an open source Slack-alternative whose newest release includes a new Integrations Directory, a simple way to migrate data from Hipchat, and much more. This version also includes security updates and upgrading is recommended. GitLab dashboards are now pre-loaded in the bundled Grafana, making it even easier to begin monitoring your instance of GitLab. We have added support to clean up old container images from the Docker registry. We have updated ca-certs to 2019-01-23. GitLab chart improvements The following improvement has been made to GitLab charts: Support for Elasticsearch has been added. Deprecations GitLab Geo will enforce Hashed Storage in GitLab 12.0 GitLab Geo requires Hashed Storage to mitigate race conditions on secondary nodes. This was noted in gitlab-ce#40970. In GitLab 11.5, we added this requirement to the Geo documentation in gitlab-ee#8053. With GitLab 11.6, sudo gitlab-rake gitlab:geo:check checks that Hashed Storage is enabled, and all projects are migrated. See gitlab-ee#8289. If you are using Geo, please run this check and migrate as soon as possible. In GitLab 11.8, a permanently dismissable warning is displayed on the Admin Area Geo Nodes page if the above checks are not resolved: gitlab-ee!8433. In GitLab 12.0, Geo will enforce the Hashed Storage requirement. See gitlab-ee#8690. Planned removal date: Jun. 22, 2019 Ubuntu 14.04 support GitLab 11.10 will be the last release with support for Ubuntu 14.04. Canonical has announced that standard support for Ubuntu 14.04 will end on April 2019. We recommend that users upgrade to one of the currently supported LTS versions Ubuntu 16.04 or Ubuntu 18.04. Planned removal date: May 22, 2019 Limit maximum number of pipelines created by a single push Previously, GitLab would create pipelines for the HEAD of every branch included in a push. That makes sense for developers that may be pushing more than one change at a time (say to a feature branch, and the develop branch). However, when pushing a large repository with many active branches perhaps to move, mirror, or fork it from another location - it does not make sense to create a pipeline for every branch. Starting in GitLab 11.10, we will create a maximum of 4 pipelines during a push operation. Planned removal date: May 22, 2019 Deprecate legacy code paths GitLab Runner Since GitLab 11.9, GitLab Runner has been using a new method for cloning/fetching the repository. Currently, GitLab Runner will use the old method if the new one is not supported. Please see this issue for additional details. In GitLab 11.0 we changed how the metrics server is configured for GitLab Runner. metrics_server will be removed in favor of listen_address in GitLab 12.0. Please see this issue for additional details. In 11.3, GitLab Runner started supporting multiple cache providers; this resulted in new settings for S3 specific configuration. In the documentation, there is a table of what changed, and how to migrate to the new configuration. Please see this issue for additional details. These paths will no longer be available in GitLab 12.0. As a user, you dont have to change anything apart from making sure the GitLab instance is running 11.9+ when upgrading to GitLab Runner 12.0. Planned removal date: Jun. 22, 2019 Deprecate entrypoint feature flag for GitLab Runner In 11.4 GitLab Runner introduced a feature flag FF_K8S_USE_ENTRYPOINT_OVER_COMMAND in order to fix issues like #2338 & #3536. In GitLab 12.0, we will switch to the correct behavior as if the feature flag was turned off. Please see this issue for additional details. Planned removal date: Jun. 22, 2019 Deprecate support of Linux distribution that reached EOL for GitLab Runner Some Linux distributions in which you could install GitLab Runner have reached End of Life support. In GitLab 12.0, GitLab Runner will no longer distribute packages to those Linux distributions. A full list of distributions which are no longer supported can be found in our documentation. Thanks, Javier Jardn for your contribution! Planned removal date: Jun. 22, 2019 Remove legacy GitLab Runner Helper commands As part of our effort to support a Windows Docker executor, we had to deprecate some old commands that are used for the helper image. In GitLab 12.0, GitLab Runner will start using the new commands. This only affects users who are overriding the helper image. Please see this issue for additional details. Planned removal date: Jun. 22, 2019 Remove legacy git clean mechanism from GitLab Runner With GitLab Runner 11.10 were introducing a way to configure how git clean command is being executed by Runner. Additionally, the new cleanup strategy removes the usage of git reset and moves the git clean command after the checkout step. Since this is a behavior change that may affect some users, weve prepared a FF_USE_LEGACY_GIT_CLEAN_STRATEGY feature flag. When set to true it will restore the legacy cleanup strategy. More about how to use feature flags in GitLab Runner can be found in the documentation In GitLab Runner 12.0, GitLab Runner will drop support for the legacy cleanup strategy and remove the ability to restore it with a feature flag. Please see this issue for additional details. Planned removal date: Jun. 22, 2019 System Info section in the admin panel GitLab presents information about your GitLab instance at admin/system_info, but this information can be inaccurate. Were removing this section of the admin panel in GitLab 12.0 and recommend using other monitoring capabilities. Planned removal date: Jun. 22, 2019 Support for Prometheus 1.x in Omnibus GitLab With GitLab 11.4, the bundled Prometheus 1.0 version is deprecated in Omnibus GitLab. Prometheus 2.0 is now included. However, the metrics format is incompatible with 1.0. Existing installations can upgrade to 2.0 and optionally migrate their data using an included tool. With GitLab 12.0, any installation not yet running Prometheus 2.0 will be automatically upgraded. Metric data from Prometheus 1.0 will not be migrated and will be lost. Planned removal date: Jun. 22, 2019 GitLab Subscription Plans GitLab is available in self-managed and cloud SaaS options. Self-managed: Deploy on-premises or on your favorite cloud platform. Free: For small teams, personal projects, or GitLab trials with unlimited time. Premium: For distributed teams who need advanced features, high availability, and 24/7 support. Ultimate: For enterprises that want to align strategy and execution with enhanced security and compliance. Cloud SaaS - GitLab.com: hosted, managed, and administered by GitLab with free and paid subscriptions for individuals and teams. Free: Unlimited private repositories and unlimited collaborators on a project. Premium: For teams that need more robust DevOps capabilities, compliance and faster support. Ultimate: Great with many CI/CD jobs. Every public project gets the features of Ultimate for free irrespective of their plan. Cover image licensed under Unsplash ",
        "GitLab has really come a long way in the past few years. The days of being a github-alike are long gone. Very happy to see them continue to find success.",
        "Can't help but feel that their focus on moving all operation insights into gitlab itself will not be as successful as they want it to be (as far as I read, their goal was to replace their operations and monitoring tools with gitlab itself[1]). I've worked with the ultimate edition for a year and the kubernetes integration is nowhere close to the insight you would get from google, amazon or azure in terms of insight and integration with ops-land. I wish all these hours were spent on improving the developer lifecycle instead.<p>I can understand how their huge success of \"built-in CI\" quickly leads to \"built-in XYZ\", but competing with github that lets other companies solve developer/ci problems via marketplace (and now CI via their new terraform actions) may lead to loosing market-leader [my take on their product] position for a code/test/review/merge ecosystem.<p>[1]: <a href=\"https://about.gitlab.com/2018/10/01/gitlab-product-vision/\" rel=\"nofollow\">https://about.gitlab.com/2018/10/01/gitlab-product-vision/</a>"
      ],
      "relevant": "true"
    },
    {
      "id": 20478156,
      "title": "Gitlab: 2019 Developer Report",
      "search": [
        "Gitlab: 2019 Developer Report",
        "Normal",
        "https://about.gitlab.com/developer-survey/2019/",
        "For the fourth year in a row, we asked DevOps teams to tell the truth about their practices and processes, their challenges and their careers. Download the report In the midst of a global pandemic and a new way of working, DevOps teams got serious about what matters most. Our 2021 Global DevSecOps Survey found sharp increases in automation, release cadences, continuous deployments, and security postures, as well as a growing reliance on cutting edge technologies, including artificial intelligence and machine learning. Nearly 4,300 people shared their struggles and successes, and demonstrated a commitment to DevOps maturity like weve never seen before. Download the survey results Developers A full 60% of devs are releasing code 2x faster than ever before, thanks to DevOps. And in the last year, developers definitely went big: Rather than focusing on incremental improvements, devs brought high impact technologies into their process including source code management, CI/CD, a DevOps platform, and automated testing. Whats still challenging? Testing (not enough, too late in the process, insufficient automation) and code reviews (too little, too late, take too long). And developers have a long list of process improvements from AI/ML to performance optimizations theyd like to have time to tackle. Read the entire survey Security In a dramatic sign of progress, 72% of security pros rated their organizations security efforts as either good or strong. DevOps teams are certainly running more security scans than ever before: over half run SAST scans, 44% run DAST, and around 50% scan containers and dependencies. And 70% of security team members say security has shifted left. But, old habits die hard when it comes to friction with developers. Although the percentage of security pros frustrated with developer behaviors around bugs has decreased substantially, true harmony remains a way off over three-quarters of the security team continue to think devs find too few bugs too late in the process. Download the survey Operations On the ops side, automation is happening: 56% reported their teams are either fully or mostly automated. And changes are happening too devs are taking on more traditional ops tasks, like provisioning, while the operations team is either managing cloud services or focused on infrastructure or hardware. And the future is shifting too: A slim majority of ops pros think advanced programming will be the most important skill for their future careers, an interesting direction for this role. Take a deep dive into our 2021 DevSecOps survey results and see how you compare. Download the survey Curious what nearly 4,300 people had to say? Download the Report Git is a trademark of Software Freedom Conservancy and our use of 'GitLab' is under license View page source Edit in Web IDE please contribute. GitLab B.V. ",
        "Here is the link to the report: <a href=\"https://about.gitlab.com/resources/downloads/2019-global-developer-report.pdf\" rel=\"nofollow\">https://about.gitlab.com/resources/downloads/2019-global-dev...</a>",
        "> Fully 61% of companies say GitLab is their most used tool for CI and build<p>a survey by gitlab is biased towards gitlab, how strange"
      ],
      "relevant": "true"
    },
    {
      "id": 19805938,
      "title": "GitHub Learning Lab",
      "search": [
        "GitHub Learning Lab",
        "Normal",
        "https://lab.github.com/",
        "Our most popular courses Introduction to GitHub The GitHub Training Team If you are looking for a quick and fun introduction to GitHub, you've found it. This class will get you started using GitHub in less than an hour. Git GitHub Pages Branches Commits Pull Requests Learning should be fun There are no simulations or boring tutorials here, just hands-on lessons created with by the GitHub community and taught by the friendly Learning Lab bot. Real projects Learn new skills while working in your own copy of a real project. Helpful bot Our friendly bot provides instructions and feedback throughout your journey. Real workflow Everything happens in GitHub Issues and Pull Requests. Our Learning Paths First Day on GitHub The GitHub Training Team Welcome to GitHub! We're so glad you're here. We know it can look overwhelming at first, so we've put together a few of our favorite courses for people logging in for the first time What is GitHub? Introduction to GitHub Git Handbook First Week on GitHub The GitHub Training Team After you've mastered the basics, learn some of the fun things you can do on GitHub. From GitHub Pages to building projects with your friends, this path will give you plenty of new ideas. Discover GitHub Pages GitHub Pages Reviewing pull requests GitHub Actions: Hello World GitHub Actions: Continuous Integration GitHub Actions: Publish to GitHub Packages Learn GitHub with GitHub Uploading your project to GitHub The GitHub Training Team Youre an upload away from using a full suite of development tools and premier third-party apps on GitHub. This course helps you seamlessly upload your code to GitHub and introduces you to exciting next steps to elevate your project. Languages and Tools Introduction to HTML The GitHub Training Team If you are looking for a quick and fun introduction to the exciting world of programming, this course is for you. Learn fundamental HTML skills and build your first webpage in less than an hour. Introduction to Node with Express everydeveloper Node.js gives you the ability to run JavaScript files on the server-side. Express is a library for Node.js, that allows you to make requests to different \"endpoints\" and get a response back. Node Express JavaScript JSON API Intermediate NodeJS Course everydeveloper This tutorial expands on concepts in the intro to Node.js and Express.js course. You will learn how to use a database (MongoDB) to Create, Read, Update, and Delete data. node.js express.js mongoose.js JavaScript MongoDB Introduction to PHP everydeveloper PHP is a server-side programming language that can insert dynamic code into your HTML. PHP is used in popular content management systems, such as WordPress and Drupal. Notating with LilyPond gitmusical LilyPond is an open source technology for notating music in plain text files. In this course, we'll cover the fundamentals of music notation in LilyPond. GitHub Actions DevOps with GitHub CodeQL U-Boot Challenge (C/C++) The GitHub Training Team Learn to use CodeQL, a query language that helps find bugs in source code. Find 9 remote code execution vulnerabilities in the open-source project Das U-Boot, and join the growing community of security researchers using CodeQL. Enterprise on GitHub InnerSource fundamentals The GitHub Training Team Organizations of all sizes and in all industries are chatting about InnerSource concepts. This course walks you through some of the key concepts of InnerSource and helps you build up an internal toolkit for adopting InnerSource practices. Create an open source program The GitHub Training Team Learn how to work alongside the open source communities that build software you're already using, and put your business at the forefront of the world's most innovative and secure code. Open source Enterprise Licensing Templates Guidelines ",
        "Not trying to weaken the GitLab brand at all... /s",
        "A Very good SEO move, to stir some of the 'lab' traffic away from gitlab."
      ],
      "relevant": "true"
    },
    {
      "id": 21263839,
      "title": "Show HN: Schema.org with GitHub and scripts: A tool to make data more useful",
      "search": [
        "Show HN: Schema.org with GitHub and scripts: A tool to make data more useful",
        "ShowHN",
        "http://www.ditabase.io/",
        "DitaBase is a new kind of data platform that can handle any format. Managing data across formats, contexts, and industries has never been easier. pip install dit-cli class SKU {| Parents = [ProductField] Str sku_str; sig Python Str func getAlt() {| <|return '(|<|this.sku_str|>[5:] + '_FINAL'|)'|> |} sig static JavaScript func validator(Str sku) {| if (!/^[A-Z]{2,3}-[A-Z]{0,2}\\d{3,4}$/.test(<|sku|>)) { <|throw ArgumentErr(|`\"${<|sku|>}\" failed regex.`|)|> } |} |} Keep up to date as DitaBase develops! Chat on Discord Watch on Twitch What is DitaBase? DitaBase is on a quest to solve the problem of disparate data. It's weapon is dit, a totally new style of container file. Dit can hold any data, with scripts and classes to organize it. With DitaBase, competing formats can work together. Unrelated industries can share datasets with ease. Complex questions can be answered with a single query. Someday, there will be no file extensions: every file will be a dit. ",
        "I don't think I understand the concept behind this, it feels a bit like an attempt at using github-as-a-database or turning it into one with schemas, scripts and validators?",
        "I liked your survey ;) It would be great if you could disclose (in a few days or so) how many visitors took part in it compared to how many people your site visited!<p>I find your concept a little bit messy though (while your website design is really clean, I love it!).\nI think it's because you described so many ideas without a lot of technical explanations. Your scenarios are helpful, but they are also quite superficial - I don't see exactly how your solution might be of help there. I also had difficulties extrapolating how your solution might work technically from you example document.<p>I think you need a very robust and clear implementation for it to be really useful!<p>Btw, I spent some significant time on a typed data description language meant to be an alternative to JSON, also with extensive schema support. I had a vision very similar to yours, but I failed to implement it. It just turned out to be too hard to do it so that its usage is always frictionless.<p>Just a couple of days ago, however, I started to port some of my ideas to a very slim JSON-Schema alternative. As a key difference to your idea, I don't want to include turing complete validators into the schema but rather use globally unique type names so that these validators can be implemented somewhere else in arbitrary programming languages. It also allows for automatic UI generation.<p>If you want to stay in touch and exchange ideas, you can reach out to me on twitter (hediet_dev)!"
      ],
      "relevant": "false"
    },
    {
      "id": 21041673,
      "title": "Gitlab 12.3 Released",
      "search": [
        "Gitlab 12.3 Released",
        "Normal",
        "https://about.gitlab.com/2019/09/22/gitlab-12-3-released/",
        "This month's release of GitLab 12.3 is especially exciting following an eventful week in which we hosted our first GitLab Users Conference in Brooklyn New York and announced the completion of a $268 million Series E round of fundraising; which will enable us to invest in making all of our DevOps platform offerings, including monitoring, security, and planning, best in class. Modern web applications are exposed to new risk from many places, including potentially every client that connects and sends traffic. A Web Application Firewall (WAF) provides monitoring and rules to protect applications in production. In GitLab 12.3 we are shipping our first iteration of a Web Application Firewall built into the GitLab SDLC platform. Its focus is on monitoring and reporting of security concerns related to your Kubernetes clusters. Future releases will expand the WAF capabilities to block malicious traffic, create and manage firewall rules, and inform earlier stages of development to take action to further reduce risk. Software delivery teams everywhere need the right information and insight in order to improve their productivity and efficiency. Too often, invisible bottlenecks and roadblocks force teams to wait and waste time rather than delivering new features. Beginning with 12.3, were starting to release new analytics features to help teams and leaders better understand their overall productivity and effectiveness for both Groups and Projects. Productivity Analytics will help teams and their leaders discover best practices to improve productivity. Initially focusing on the time it takes to merge MRs, GitLab will make it possible to drill into the data and learn insight that can guide future improvements. In many organizations, leaders are responsible for multiple projects and Group level analytics workspace is intended to provide productivity and performance insight and visibility across multiple projects. These two features are only the first in a series of updates that will specifically improve visibility and insight so that teams can become more efficient. Compliance with policies and procedures is a common challenge that software teams face. For many GitLab users, having development teams collaborate in a single application makes compliance easier. In this 12.3 release of GitLab, we're including several features that will continue to streamline efforts to reduce compliance risks. MR approval rules provides a way to prevent teams from merging in code that introduces unsupported licenses. Require code owner approval per branch makes it possible to protect branches and require code owner approval of changes. And much more! There are so many great features within GitLab 12.3 that we couldnt possibly highlight them all (even though we really want to). Better resource visibility with Global view for group-level cluster deployments/environments, more efficient Git fetches with Compress Git ref advertisements over HTTP, and more efficient reviews with Keyboard Shortcut for Next and Previous Unresolved Discussion Register now to join us at our first European user conference in London on Oct 9! Join us for an upcoming event This month's Most Valuable Person (MVP) is Cdric Tabin Cdrics contribution to GitLab 12.3 adds a new CI job keyword allowing interruptible builds. They worked for more than 9 months contributing this feature and working with our review teams to get it across the line. Thanks so much to Cdric for their tireless effort on this contribution! Key improvements released in GitLab 12.3 Web Application Firewall for Kubernetes Ingress GitLab now adds the modsecurity Web Application Firewall (WAF) plug-in to your cluster when you install the Ingress app in your Kubernetes cluster. The WAF is able to determine whether or not HTTP or HTTPS traffic to your app contains malicious code, such as SQL injection, cross-site scripting, or trojans. The WAF is pre-configured with a powerful set of rules, the OWASP ModSecurity Core Rules (CRS), to detect many different types of attacks out-of-the-box. The documentation describes how to view the WAF logs so you can see what type of malicious traffic your app is subjected to when it is running in production. Productivity Analytics Currently there are relatively few sources of data and analytics, which can help engineering leaders understand their team, project, and group productivity. As Peter Drucker has once said, Whats measured improves. Subscribing to this view, we are releasing our first iteration of Productivity Analytics in order to help engineering leaders uncover patterns and best practices to improve overall productivity. The focus of this release is on the time it takes to merge MRs depending on their size. Users can make use of our existing filters and drill down to a specific author or label in a group for a specific date range. As we go forward and iterate on productivity analytics, we will add additional data points in order to help identify dependencies which may be contributing to increased active development and/or wait times. Note that in this initial release of Productivity Analytics, we have not backfilled the historical data for the newly derived metrics to ensure that this background process does not adversely impact the upgrade from 12.2 to 12.3. Youre welcome to follow the issue, where we are working on that here. Global view for group-level cluster deployments/environments Provisioning a group-level cluster is a great way for operators to provide an application development platform for developers. Scaling cluster resources can be challenging and requires a global view of resource usage. The new Environments section of the cluster page provides an overview of all the projects that are making use of the Kubernetes cluster, including the deployments/environments that have been provisioned and the numbers of pods used by each environment. Leverage merge request approvals to prevent merging prohibited licenses MVC If you have strict License Compliance restrictions, you may set the License Compliance feature to disallow a merge when a blacklisted license is found in a merge request. This prevents the new introduction of licenses which you have expressly prohibited. Currently you can set the approvers for the License-Check group in project settings then require the check by following the directions outlined in the documentation. Other improvements in GitLab 12.3 Analytics Workspace Engineering and product teams can span multiple GitLab groups and projects, yet most of our analytics have traditionally been developed at the project level. This is why we created a workspace where users can aggregate insights across groups, subgroups, and projects. The Analytics workspace will make it easier for teams and leaders to analyze and manage team metrics. The workspace will be available in Core. However, in some cases, specific features will be available to Enterprise Edition customers. As we move to the Analysis workspace, we will ensure that existing project-level analytics functionality is available to Community Edition users when moved to the new workspace. In GitLab 12.3, we are releasing the first iteration of group and project level Productivity Analytics and group-level Cycle Analytics. In subsequent releases, we will enable the selection of multiple groups and subgroups; porting all analytics features for an instance. We would love your feedback and input on the strategy for Analytics/Value Stream Management Design Management notifications In GitLab 12.2 we released our initial iteration of Design Management. An important part of continuing to evolve is to ensure users are properly notified about these activities. Conversations in designs will now create To-Dos for mentioned users and send notifications based on their preferences. This helps to ensure that important feedback isnt missed and can be actioned. In a future release, well add those conversations to the main discussion tab for an easier conversation flow. API for Merge Request Approval Rules Approval Rules for Merge Requests allow you to communicate who should participate in code reviews by specifying the eligible approvers and the minimum number of approvals for each. Approval rules are shown in the merge request widget so the next reviewer can easily be spotted. In GitLab 12.3, support for Approval Rules has been added to the API for Projects and Merge Requests. Keyboard shortcut for next and previous unresolved discussion Reviewing, discussing and resolving feedback is the basis of code review in GitLab. The Jump to next unresolved discussion button makes it easy to quickly jump from discussion to discussion. In GitLab 12.3, the new n and p keyboard shortcuts to move to the next and previous unresolved discussions in Merge Requests make it even more convenient to review changes. API to require merge request approval by code owners per branch Using merge request approvals to restrict how code is pushed to protected branches is helpful for promoting code quality and implementing compliance controls. But not all merge requests target stable branches, and not all stable branches need the same controls. In GitLab 12.3, it is possible to require code owners approval for specific branches (through the API) to prevent directly pushing changes to files with code owners, or merging changes without the code owners approval. Note: This feature is only available via the API in GitLab 12.3. In GitLab 12.4 it will be available through the Protected Branch settings. Follow issue #13251 for updates. Flexible rules keyword for controlling pipeline behaviors Only/except rules in pipelines can have a lot of implicit behaviors, and as more and more are added to your pipelines, it can become very difficult to understand if a given job is going to run or not in different situations. We are introducing a new rules: syntax that will make it much easier to implement and understand complex rules. This syntax is optional and can exist in the same pipeline, but not the same jobs, as the current only/except approach. only/except: external_pull_requests for external repositories GitLab CI has an external repos capability intended to allow customers to use external repos for source control and GitLab for CI/CD. Until now, however, the CI_PIPELINE_SOURCE always showed push because it was based on the pull mirror rather than the source external repo or webhook. This prevented GitLab us from correctly supporting only/except: merge_requests-style options. In the 12.3 release weve resolved this limitation. Remove container images from CI/CD The GitLab Container Registry allows users to leverage GitLab CI/CD to build and push images/tags to their project. Changes to the Container Registry are made by a service account called CI Registry User that is invoked from .gitlab-ci.yml with the predefined environment variable CI_REGISTRY_USER. Previously, this service account could push new tags to the registry, but it lacked permissions to untag images. This prevented branch-specific images from being removed, resulting in additional storage costs and creating difficulty when navigating the registry user interface due to the large number of additional, unneeded tags. In 12.3 we have expanded the permissions of CI_REGISTRY_USER to allow for untagging of images so you can clean up branch-specific tags as part of your regular CI/CD workflow and use GitLab CI/CD to automate your cleanup scripts. This issue is part of a larger epic to lower the cost of the Container Registry by improving storage management. Validate domains when performing full DAST active scans You can now ensure DAST only performs active scans against domains which are intentionally configured for DAST scanning. This helps to ensure that DAST active scans are not unintentionally run against domains which could be serving content or being used in a production capacity. There is no change to passive DAST scans, since passive scanning does not potentially negatively impact scanned sites. SAST Spotbugs analyzer updated for Java 11 The SAST SpotBugs analyzer has been updated to allow scanning of code written with Java 11. You can enable this by setting the SAST_JAVA_VERSION environment variable in your project. Run Pipeline button for Merge Request Pipelines Pipelines for merge requests recently introduced a new way to run a pipeline in the context of a merge request, but the only way to trigger a new run of one of these pipelines was through a push. In this release weve added a button to start a new pipeline, making it much easier to retry a failed pipeline. User-defined CI variables available to docker build with Auto DevOps CI variables allow you to tailor how processes are run to build your application as part of your CI pipeline. Starting in GitLab 12.3, you can extend the availability of user-defined CI variables to the docker build step in Auto DevOps. The data is made available as a new build secret value. List one or more variables using the AUTO_DEVOPS_BUILD_IMAGE_FORWARDED_CI_VARIABLES variable and it will be made available to use in docker build. Knative for group and instance-level clusters Group and instance-level clusters now support the installation of Knative, the Kubernetes-based platform to deploy and manage serverless workloads. This will allow multiple projects to make use of GitLab Serverless features leveraging a single cluster. Line charts for metrics dashboard When it comes to visualizing metrics, oftentimes users would like to choose different types of visualization for specific metrics (e.g., line chart for CPU, area chart for disk space). To help achieve this, we added line charts as a part of our effort to enhance our dashboard offering around monitoring. Quick actions to add/remove Zoom meetings on issues Synchronous collaboration is a critical part of any fire-fight. We are streamlining the number of steps it takes to spin up a conference bridge and engage all required parties by embedding this functionality, using Zoom, directly in an issue. Once a user has started a Zoom meeting, they can attach it to an issue using a quick action and inputting the Zoom meeting URL (e.g. /zoom https://gitlab.zoom.us/s/123456). A button will appear at the top of the issue giving users direct access to the conference bridge. When the incident has resolved, the Zoom meeting can be removed using /remove_zoom. This feature is generally available on GitLab.com and feature flagged for use on self-managed instances. If you are interested in taking advantage of this feature for your self-managed instance of GitLab, operators can enable the issue_zoom_integration feature flag. In next months release of GitLab 12.4 we plan to remove the feature flag and make the Zoom Issue integration generally available for all self-managed users. Geo displays secondary lag when pushing via Git HTTP Fetching large amounts of data can take a long time for users in remote locations. Replicating repositories with Geo reduces the time it takes to clone and fetch large repositories by creating read-only secondary nodes near remote user. Because secondary nodes lag behind the primary node, GitLab now provides an estimate of replication lag whenever git push is used over HTTP. This raises the visibility of using a Geo node and allows users to detect increases in replication lag and report them to systems administrators. Because of protocol limitations, this message is not available when using git pull. Disable 2FA for selected OAuth providers Organizations using enforced two-factor authentication and an identity provider also using 2FA may find the requirement to authenticate twice frustrating. Thanks to a community contribution, youre now able to bypass 2FA for selected OAuth identity providers in GitLab. For those using providers that handle 2FA, this makes logging into GitLab a friendlier and easier experience. Thanks to dodocat for the contribution! IP address restriction supports multiple subnets Iterating on restricting group activity by IP address, GitLab 12.3 introduces the ability to specify multiple IP subnets. For geographically distributed organizations, this helps this feature shine; instead of specifying a single (and overly permissive) range, large organizations can now restrict incoming traffic to their specific needs. Design Management status and discussion counts In GitLab 12.2 we released the first iteration of Design Management, which allowed uploading designs directly to issues. They were uploaded in a separate tab within issues and the activity in each version of Designs wasnt clear to the user. Now, when designs are uploaded, status icons are added in each version to let you know if its a new design or a change to an existing design. We also added discussion counts on designs to better inform users of conversations happening. Were excited about these additions to Design Management to help improve the collaboration and conversation inside of GitLab for design and engineering teams. Compress Git ref advertisements over HTTP When fetching changes to a Git repository, the Git server advertises a list of all the branches and tags in the repository. This is called ref advertisement and can be many megabytes for large projects. In GitLab 12.3, when fetching over HTTP, the ref advertisement will be compressed for supported clients reducing the amount of data being transferred and making fetch operations faster. On an average weekday, GitLab.com serves about 850GB of ref advertisements over HTTP. After enabling ref compression, bytes transferred has decreased by approximately 70 percent. Audit logs for Git Push events (Beta) Git history can be rewritten to change commits, authors and timestamps, which makes it possible to craft clear and useful history for future developers. For audit, this is a problem. In GitLab 12.3, Git push events that add commits, rewrite history or otherwise modify the repository can be added to the audit log. Audit logs for push events are disabled by default to prevent performance degradation on GitLab instances with very high Git write traffic. In an upcoming release, Audit Logs for Git push events will be enabled by default. Follow #7865 for updates. Smarter Web IDE default commit options Previously the Web IDE defaulted to Commit to current branch when making a commit. This made it easy for those with permissions to accidentally push changes to master or other protected branches. Now when making changes in the Web IDE, the default commit options are smarter to prevent making changes to the wrong branch. Smarter commit options prevent accidental pushes to master and protected branches for users with write access. When the user has no write access, additional details are provided as to why options are unavailable. Additionally, the new commit options also support commits to a non-default branch with or without an existing Merge Request. Per-job timeouts for CI/CD pipelines Different jobs have different execution characteristics and may need different timeouts to be set on a per-job basis. Timeouts can be configured by adding the timeout: keyword to your job in .gitlab-ci.yml, along with a number to indicate the number of minutes to wait before the job fails. Thanks to Michal Siwek for the contribution. interruptible keyword to indicate if a job can be safely canceled The new interruptible keyword can be used to indicate whether or not a job should be canceled if made redundant by a newer run of the same job. The keyword defaults to false, so it should be used to specify jobs that are safe to stop. This value will only be used if the automatic cancellation of redundant pipelines feature is enabled. This helps avoid duplication of unnecessary jobs across your pipelines, reducing costs and making your pipelines more efficient. Due to a bug in the Runner, some executors do not stop in-progress jobs when cancelled. This is planned to be fixed in 12.4. Status checking for pipeline triggers Weve recently improved the way cross-project pipelines can trigger each other, but one thing still missing was having the triggering pipeline wait or confirm success of the triggered pipeline. It was possible using API polling, but in this release weve introduced the depends and wait strategies where this can be handled automatically for you. If you are triggering a pipeline you want to depend on, it will wait for the pipeline to complete and will verify it succeeds before finishing the triggering job. If you choose wait, it will wait for it to finish but proceed regardless of pass/fail status. API endpoint to list the Docker images/tags of a group The GitLab Container Registry allows users to build and push Docker images/tags to their project from the command line, CI/CD, or from an API. However, until GitLab 12.3 we did not offer any visibility into images/tags at the group level, a popular request from users. We have added two API endpoints that will give visibility into which images and tags exist at the group level. This is the first step in helping improve visibility and discoverability for the Container Registry. Next, well leverage the API to create a group-level browser as part of the Container Registry user interface. SAST scanning without Docker-in-Docker SAST scans can now optionally be done without using Docker-in-Docker. This means SAST scanning can be configured to not need elevated privileges. Edit vulnerability dismissal reasons Vulnerability dismissal reasons can now be edited and deleted. This allows you to add and update context for a vulnerability as you learn more over time. Improve Pages initial setup experience To improve the Pages user experience, we added a banner that notifies users of the possible initial setup time. We understand it can be frustrating to see the Congratulations message without the page being available. The banner helps make it more clear what to expect. Show Kubernetes cluster used for deployment The jobs detail page now displays the name of the Kubernetes cluster that was used for the given deployment. Project owners and maintainers are presented with a link on the cluster name which will link directly to the cluster details page. JupyterHub for group-level Kubernetes clusters Group-level clusters now support the installation of JupyterHub, a multi-user service for easily launching notebooks as well as creating runbooks for operators. This extends the availability of JupyterHub to both project-level and group-level clusters. Close issue via slash command in Slack Chat tools are required during fire-fights to resolve modern IT incidents. This tool needs to be closely integrated with the systems you are managing and the tools where you will actually perform remediation. As much as possible, you want to minimize context and tool switching while youre working to restore services and update your external stakeholders. In 12.3, we have added an additional slash command to the suite of commands we offer with our ChatOps product based on Slack. Issues can now be closed from Slack without needing to switch tools, just locate the issue in question, and manually close it. You can close it from where you are working. Geo natively supports Docker Registry replication Geo natively supports replicating a Docker Registry between primary and secondary Geo nodes. This allows Geo users to use a Docker Registry on a close-by secondary node. This approach is storage-agnostic and can be used for object storage, such as S3, or local storage. When using distributed object storage (e.g. S3) for a Docker Registry, the primary and secondary Geo nodes can use the same storage type. This approach does not rely on Geos native replication ability. API activity included in IP address group restriction GitLab 12.0 saw the introduction of restricting a groups activity by IP address. Weve iterated on this feature to also include API activity, where incoming requests will be rejected if they do not adhere to the groups restriction. This solves an important problem for compliance-minded enterprises with an extended approach to access control, covering both UI and API activity. System hooks for project and group member updates System hooks allow for powerful automation by triggering requests when a variety of events in GitLab take place. Thanks to a community contribution, project and group membership changes are now supported in system hooks. This is a terrific addition for those who are looking to build additional levels of oversight and automation over membership changes. Thanks to Brandon Williams for the contribution! S/MIME Email Signing Notification emails sent by GitLab can now be signed with S/MIME for improved security on the instance level. Thanks Siemens, @bufferoverflow, and @dlouzan for the contribution! Omnibus improvements SSL authentication to external Postgres servers is now supported by specifying the certificate in gitlab.rb. GitLab 12.3 includes Mattermost 5.14, an open source Slack-alternative whose newest release includes keyboard accessibility improvements, enhanced Jira integration, and more. This version also includes security updates and upgrade from earlier versions is recommended. Deprecations Users with manually configured .gitlab-ci.yml using Secure features should be aware of deprecations in GitLab 12.3 If you have manually configured your .gitlab-ci.yml configuration file to use DEP_SCAN_DISABLE_REMOTE_CHECKS or DS_DISABLE_REMOTE_CHECKS flag variables, you need to remove this as it is no longer supported. If you utilize vendored templates, your configuration will be kept up to date with variable and argument changes. As announced in GitLab Release 12.0 post Planned removal date: GitLab 12.3 gitlab-monitor tool renamed to gitlab-exporter To prevent confusion with the broader GitLab Monitor feature set, the gitlab-monitor tool has been renamed to gitlab-exporter. gitlab-exporter is a Prometheus web exporter that gives administrators insight into their GitLab instance, whereas the GitLab Monitor features enable monitoring of applications built using GitLab. If youre using Omnibus, youll have to update your gitlab.rb file. Planned removal date: September 22, 2019 Update public projects and subgroups in private groups to private visibility In GitLab 10.0, we introduced a change to clarify the visibility setting of public projects and subgroups in a private group. Since the most restrictive visibility setting applied, attempting to set a project or subgroup to public within a private group resulted in the visibility change being rejected. To prevent errors with existing projects and subgroups with public visibility, all public projects and subgroups in private groups will be updated from public to private in GitLab 12.5. If youd like a less restrictive visibility setting on a particular project or subgroup, you can simply move it out of the private group. Planned removal date: Nov. 22, 2019 GitLab Subscription Plans GitLab is available in self-managed and cloud SaaS options. Self-managed: Deploy on-premises or on your favorite cloud platform. Free: For small teams, personal projects, or GitLab trials with unlimited time. Premium: For distributed teams who need advanced features, high availability, and 24/7 support. Ultimate: For enterprises that want to align strategy and execution with enhanced security and compliance. Cloud SaaS - GitLab.com: hosted, managed, and administered by GitLab with free and paid subscriptions for individuals and teams. Free: Unlimited private repositories and unlimited collaborators on a project. Premium: For teams that need more robust DevOps capabilities, compliance and faster support. Ultimate: Great with many CI/CD jobs. Every public project gets the features of Ultimate for free irrespective of their plan. Cover image licensed under Free ",
        "Gitlab is one of the finest companies out there. Following their progress and seeing them raising $268 million is pretty exciting",
        "The container cleanup effort is welcomed, and I hope the milestone is completed in the near future. My private instance on LXD is using about 350GB, and given that I don't commit large files on Git, I suspect that about 330-340GB is all from the registry. All the latest tagged images probably make up 2GB worth of downloads."
      ],
      "relevant": "true"
    }
  ]
}
