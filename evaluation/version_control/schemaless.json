{
  "docs": [
    {
      "id": 19706396,
      "title": "Fh: File history with ed, diff, awk, sed, and sh",
      "search": [
        "Fh: File history with ed, diff, awk, sed, and sh",
        "https://github.com/xorhash/fh",
        "fh records changes to a file on a per-file basis, similar to RCS and SCCS. It is, however, considerably more primitive. Design goals: no support for multi-user environments (no locking, etc.) implemented in shell script must use ed(1) should work or easily be made to work on 7th Edition UNIX I've taken care not to use any shell scripting constructions that didn't exist in the Bourne shell, but I may have missed things; however, the shebang needs to be removed on 7th Edition UNIX. fh uses a chain of ed(1) scripts to construct any version of a file. It even allows recording of commit messages. Why? I saw the following passage in diff(1) of 7th Edition UNIX: The -e option produces a script of a, c and d commands for the editor ed, which will recreate file2 from file1. The -f option produces a similar script, not useful with ed, in the opposite order. In connection with -e, the following shell program may help maintain multiple versions of a file. Only an ancestral file ($1) and a chain of version-to-version ed scripts ($2,$3,...) made by diff need be on hand. A `latest version' appears on the standard output. (shift; cat $*; echo 1,$p) ed - $1 After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious. License ISC, see LICENSE. Installation and usage Copy the fl, fr and fu files to a directory in $PATH; make sure they are marked as executable. Copy the man pages fl.1, fr.1, fu.1 and fh.5 to a directory in $MANPATH. For usage, see the supplied man pages. man.md is available on the web. For 7th Edition UNIX, the man pages written in mdoc macrosneed to be converted to old man macros first: mkdir man mandoc -Tman fl.1 > man/fl.1 mandoc -Tman fr.1 > man/fr.1 mandoc -Tman fu.1 > man/fu.1 mandoc -Tman fh.5 > man/fh.5 ",
        "Wait, what?<p><i>\"After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious.\"</i><p>Ah, ok.<p>Also, if you want to experiment with diffs and recreating versions:<p>Patch: <a href=\"https://en.m.wikipedia.org/wiki/Patch_(Unix)\" rel=\"nofollow\">https://en.m.wikipedia.org/wiki/Patch_(Unix)</a><p>Xdelta:  <a href=\"https://github.com/jmacd/xdelta\" rel=\"nofollow\">https://github.com/jmacd/xdelta</a>",
        "It's not a bad exercise.  In a Unix class I took, we had to implement an RDBMS using only the traditional Unix software tools.  A little later, for work, I had to learn to write highly portable and fairly secure shell scripts.<p>A few reasons to do exercises like this:<p>* To understand the Unix software tools and Bourne/etc. shell scripting models.  There are good lessons, and also examples of what not to do (or the tradeoffs), which I don't think you'll find anywhere else.  (For example, the simple Unix streams model is very powerful, and also often used in very kludgey ways in practice, and the Bourne evaluation model is probably much worse and dangerous than a beginner might think.)<p>* Knowing how to do things with just shell scripts is great for some kinds of bootstrapping, commands in build tools, or other situations in which you can only assume a shell interpreter and possibly certain command-line tools.<p>* Handy for configuring your interactive shell to do what you want.<p>* It's one way to familiarize with parts of a Unix-ish system that you might not normally, especially if you're spending all your time learning your way around huge stacks of tools that obscure the lower-level mechanics.<p>* You'll appreciate Perl and other languages much more, in some ways."
      ],
      "relevant": "true"
    },
    {
      "id": 20745393,
      "title": "Sunsetting Mercurial Support in Bitbucket",
      "search": [
        "Sunsetting Mercurial Support in Bitbucket",
        "https://bitbucket.org/blog/sunsetting-mercurial-support-in-bitbucket",
        "[Update Aug 26, 2020] All hg repos have now been disabled and cannot be accessed.[Update July 1, 2020] Today, mercurial repositories, snippets, and wikis will turn to read-only mode. After July 8th, 2020 they will no longer be accessible. The version control software market has evolved a lot since Bitbucket began in 2008. When we launched, centralized version control was the norm and we only supported Mercurial repos. But Git adoption has grown over the years to become the default system, helping teams of all sizes work faster as they become more distributed.As we surpass 10 million registered users on the platform, we're at a point in our growth where we are conducting a deeper evaluation of the market and how we can best support our users going forward. After much consideration, we've decided to remove Mercurial support from Bitbucket Cloud and its API. Mercurial features and repositories will be officially deprecated on July 1, 2020.Read on to learn more about this decision, the important timelines, and get migration resources and support. The timeline and how this may affect your team Here are the key dates as we sunset Mercurial functionality: February 1, 2020: users will no longer be able to create new Mercurial repositories [Extended] July 1, 2020: users will not be able to use Mercurial features. All hg repos, wikis, and snippets will be in read-only mode. Heres why were focusing on Git This wasnt an easy decision, and Mercurial will always have a special place in Bitbuckets history. DevOps adoption has skyrocketed over the last decade and our customers are adopting this new way of working at an exponential rate. In this time, Bitbucket has steadily grown from being just a version control management tool to being a place to manage the entire software development lifecycle. And there's always more work to be done. This year we will concentrate on building deeper integrations to enhance automation and collaboration. Our improvements will make it even easier and safer to plan, code, test, and deploy all from within Bitbucket. Building quality features requires intense focus, and supporting two version control systems means splitting focus doubling shipping time and technical overhead. With Git being the more popularly used tool, Mercurial runs the risk of overlooked issues as we scale. According to a Stack Overflow Developer Survey, almost 90% of developers use Git, while Mercurial is the least popular version control system with only about 3% developer adoption. In fact, Mercurial usage on Bitbucket is steadily declining, and the percentage of new Bitbucket users choosing Mercurial has fallen to less than 1%. This deprecation will enable us to focus on building the best possible experience for our users. How to migrate and export We recommend that teams migrate their existing Mercurial repos to Git. There are various Git conversion tools in the market, including hg-fast-export and hg-git mercurial plugin. We are happy to support your migration, and you can find a discussion about available options in our dedicated Community thread. If you prefer to continue using the Mercurial system, there are a number of free and paid Mercurial hosting services. We realize that there is no one-size-fits-all solution. That's why we've created the following resources to best equip you with the knowledge and tools for a seamless transition: A Community thread to discuss conversion tools, migration, tips, and offer troubleshooting help A Git tutorial that covers anywhere from the basics of creating pull requests to rebasing and Git hooks We want to thank all the loyal users who have grown with us over the years. We look forward to this new focus on our roadmap and to introducing exciting new features. ",
        "It's funny to see how the whole world concentrates on this Git thing, while there is a treasure trove called Mercurial.<p>Mercurial was made for humans. It is seriously convenient and productive. Something I cannot say about Git, which more reminds me of an adhoc job.<p>I use both Git and Mercurial on daily basis. But my preference goes to Mercurial: it is just more sane in a big way. It is clearly a piece of art and love.",
        "It's very sad to see bitbucket dropping mercurial support. Now only Facebook and volunteers are keeping mercurial alive. \nSometimes technically better architecture and user interface lose to a non user friendly hard solutions due to inertia of mass adoption.<p>So a lesson in Software development is similar to betamax and VHS, so marketing is still a winner over technically superior architecture and ease of use. GitHub successfully marketed git, so git and GitHub are synonymous for most developers. Now majority of open source projects are reliant on a single proprietary solution Github by Microsoft, for managing code and project. Can understand the difficulty of bitbucket, when Python language itself moved out of mercurial due to the same inertia.<p>Hopefully gitlab can come out with mercurial support to migrate projects using it from bitbucket.<p>For people who believe in self hosted solution can install Kallithea (<a href=\"https://kallithea-scm.org\" rel=\"nofollow\">https://kallithea-scm.org</a>) or Rhodecode open source edition. Kallithea is used by Unity engine to manage their source code internally with mercurial."
      ],
      "relevant": "false"
    },
    {
      "id": 21071181,
      "title": "Automated Reports with Jupyter Notebooks (Using Jupytext and Papermill)",
      "search": [
        "Automated Reports with Jupyter Notebooks (Using Jupytext and Papermill)",
        "https://medium.com/capital-fund-management/automated-reports-with-jupyter-notebooks-using-jupytext-and-papermill-619e60c37330",
        "Jupyter notebooks are one of the best available tools for running code interactively and writing a narrative with data and plots. What is less known is that they can be conveniently versioned and run automatically.Do you have a Jupyter notebook with plots and figures that you regularly run manually? Wouldnt it be nice to use the same notebook and instead have an automated reporting system, launched from a script? What if this script could even pass some parameters to the notebook it runs?This post explains in a few steps how this can be done concretely, including within a production environment.Example notebookWe will show you how to version control, automatically run and publish a notebook that depends on a parameter. As an example, we will use a notebook that describes the world population and the gross domestic product for a given year. It is simple to use: just change the year variable in the first cell, re-run, and you get the plots for the chosen year. But this requires a manual intervention. It would be much more convenient if the update could be automated and produce a report for each possible value of the year parameter (more generally, a notebook can update its results based not only on some user-provided parameters, but also through a connection to a database, etc.).Version controlIn a professional environment, notebooks are designed by, say, a data scientist, but the task of running them in production may be handled by a different team. So in general people have to share notebooks. This is best done through a version control system.Jupyter notebooks are famous for the difficulty of their version control. Lets consider our notebook above, with a file size of 3 MB, much of it being contributed by the embedded Plotly library. The notebook is less than 80 KB if we remove the output of the second code cell. And as small as 1.75 KB when all outputs are removed. This shows how much of its contents is unrelated to pure code! If we dont pay attention, code changes in the notebook will be lost in an ocean of binary contents.To get meaningful diffs, we use Jupytext (disclaimer: Im the author of Jupytext). Jupytext can be installed with pip or conda. Once the notebook server is restarted, a Jupytext menu appears in Jupyter:We click on Pair Notebook with Markdown, save the notebook and we obtain two representations of the notebook: world_fact.ipynb (with both input and output cells) and world_fact.md (with only the input cells).Jupytexts representation of notebooks as Markdown files is compatible with all major Markdown editors and viewers, including GitHub and VS Code. The Markdown version is for example rendered by GitHub as:As you can see, the Markdown file does not include any output. Indeed, we dont want it at this stage since we only need to share the notebook code. The Markdown file also has a very clear diff history, which makes versioning notebooks simple.The world_facts.md file is automatically updated by Jupyter when you save the notebook. And the other way round also works! If you modify world_facts.md with either a text editor, or by pulling the latest contributions from the version control system, then the changes appear in Jupyter when you refresh the notebook in the browser.In our version control system, we only need to track the Markdown file (and we even explicitly ignore all .ipynb files). Obviously, the team that will execute the notebook needs to regenerate the world_fact.ipynb document. For this they use Jupytext in the command line:$ jupytext world_facts.md --to ipynb[jupytext] Reading world_facts.md[jupytext] Writing world_facts.ipynbWe are now properly versioning the notebook. The diff history is much clearer. See for instance how the addition of the gross domestic products to our report looks like:Jupyter notebooks as Scripts?As an alternative to the Markdown representation, we could have paired the notebook to a world_facts.py script using Jupytext. You should give it a try if your notebook contains more code than text. That's often a first good step towards a complete and efficient refactoring of long notebooks: once the notebook is represented as a script, you can extract any complex code and move it to a (unit-tested) library using the refactoring tools in your IDE.JupyterLab, JupyterHub, Binder, Nteract, Colab & Cloud notebooks?Do you use JupyterLab and not Jupyter Notebook? No worries: the method above also applies in this case. You will just have to use the Jupytext extension for JupyterLab instead of the Jupytext menu. And in case you were wondering, Jupytext also work in JupyterHub and Binder.If you use other notebook editors like Nteract desktop, CoCalc, Google Colab, or another cloud notebook editor, you may not be able to use Jupytext as a plugin in the editor. In this case you can simply use Jupytext in the command line. Close your notebook and inject the pairing information into world_facts.ipynb with$ jupytext --set-formats ipynb,md world_facts.ipynband then keep the two representations synchronised with$ jupytext --sync world_facts.ipynbNotebook parametersPapermill is the reference library for executing notebooks with parameters.Papermill needs to know which cell contains the notebook parameters. This is simply done by adding a parameter tag in that cell with the cell toolbar in Jupyter Notebook:In JupyterLab you can use the celltags extension.And if you prefer you can also directly edit world_facts.md and add the tag there:```python tags=[\"parameters\"]year = 2000```Automated executionWe now have all the information required to execute the notebook on a production server.Production environmentIn order to execute the notebook, we need to know in which environment it should run. As we are working with a Python notebook in this example, we list its dependencies in a requirements.txt file, as is standard for Python projects.For simplicity, we also include the notebook tools in the same environment, i.e. add jupytext and papermill to the same requirements.txt file. Strictly speaking, these tools could be installed and executed in another Python environment.The corresponding Python environment is created with either$ conda create -n run_notebook --file requirements.txt -yor$ pip install -r requirements.txt(if in a virtual environment).Please note that the requirements.txt file is just one way of specifying an execution environment. The Reproducible Execution Environment Specification by the Binder team is one of the most complete references on the subject.Continuous IntegrationIt is a good practice to test each new contribution to either the notebook or its requirements. For this you can use for example Travis CI, a continuous integration solution. You will need only these two commands:pip install -r requirements.txt to install the dependenciesjupytext world_facts.md --set-kernel - --execute to test the execution of the notebook in the current Python environment.You can find a concrete example in our .travis.yml file.We are already executing the notebook automatically, arent we? Travis will tell us if a regression is introduced in the project What progress! But were not 100% done yet, as we promised to execute the notebook with parameters.Using the right kernelJupyter notebooks are associated with a kernel (i.e. a pointer to a local Python environment), but that kernel might not be available on your production machine. In this case, we simply update the notebook kernel so as to point to the environment that we have just created:$ jupytext world_facts.ipynb --set-kernel -Note that the minus sign in --set-kernel - above represents the current Python environment. In our example this yields:[jupytext] Reading world_facts.ipynb[jupytext] Updating notebook metadata with '{\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}}' [jupytext] Writing world_facts.ipynb (destination file replaced)In case you want to use another kernel just pass the kernel name to the --set-kernel option (you can get the list of all available kernels with jupyter kernelspec list and/or declare a new kernel with python -m ipykernel install --name kernel_name --user).Executing the notebook with parametersWe are now ready to use Papermill for executing the notebook.$ papermill world_facts.ipynb world_facts_2017.ipynb -p year 2017Input Notebook: world_facts.ipynb Output Notebook: world_facts_2017.ipynb 100%|| 8/8 [00:04<00:00, 1.41it/s]Were done! The notebook has been executed and the file world_facts_2017.ipynb contains the outputs.Publishing the NotebookIts time to deliver the notebook that was just executed. Maybe you want it in your mailbox? Or maybe you prefer to get a URL where you can see the result? We cover a few ways of doing that.GitHub can display Jupyter notebooks. This is a convenient solution, as you can easily choose who can access repositories. This works well as long as you dont include any interactive JavaScript plots or widgets in the notebook (the JavaScript parts are ignored by GitHub). In the case of our notebook, the interactive plots do not appear on GitHub, so we need another approach.Another option is to use the Jupyter Notebook Viewer. The nbviewer service can render any notebook which is publicly available on GitHub. Our notebook is thus rendered correctly there. If your notebook is not public, you can choose to install nbviewer locally.Alternatively, you can convert the executed notebook to HTML, and publish it on GitHub pages, or on your own HTML server, or send it over email. Converting the notebook to HTML is easily done with$ jupyter nbconvert world_facts_2017.ipynb --to html[NbConvertApp] Converting notebook world_facts_2017.ipynb to html [NbConvertApp] Writing 3361863 bytes to world_facts_2017.htmlThe resulting HTML file includes the code cells as below:But maybe you dont want to see the input cells in the HTML? You just need to add --no-input:$ jupyter nbconvert --to html --no-input world_facts_2017.ipynb --output world_facts_2017_report.htmlAnd youll get a cleaner report:Sending the standalone HTML file as an attachment in an email is an easy exercise. Embedding the report in the body of the email is also possible (but interactive plots wont work).Finally, if you are looking for a polished report and have some knowledge of LaTeX, you can give the PDF export option of Jupyters nbconvert command a try.Using pipesAn alternative to using named files would be to use pipes. jupytext, nbconvert and papermill all support them. A one-liner substitute for the previous commands is:$ cat world_facts.md \\ | jupytext --from md --to ipynb --set-kernel - \\ | papermill -p year 2017 \\ | jupyter nbconvert --stdin --output world_facts_2017_report.htmlConclusionYou should now be able to set up a full pipeline for generating reports in production, based on Jupyter notebooks. We have seen how to:version control a notebook with Jupytextshare a notebook and its dependencies between various userstest a notebook with continuous integrationexecute a notebook with parameters using Papermilland finally, how to publish the notebook (on GitHub or nbviewer), or render it as a static HTML page.The technology used in this example is fully based on the Jupyter Project, which is the de facto standard for Data Science. The tools used here are all open source and work well with any continuous integration framework.You have everything you need to schedule and deliver fine-tuned, code-free reports!EpilogueThe tools used here are written in Python. But they are language agnostic. Thanks to the Jupyter framework, they actually apply to any of the 40+ programming language for which a Jupyter kernel exists.Now, imagine that you have authored a document containing a few Bash command lines, just like this blog post. Install Jupytext and the bash kernel, and the blog post becomes this interactive Jupyter notebook!Going further, shouldnt we make sure that every instruction in our post actually works? We do that via our continuous integration spoiler alert: thats as simple as jupytext --execute README.md!AcknowledgmentsMarc would like to thank Eric Lebigot and Florent Zara for their contributions to this article, and to CFM for supporting this work through their Open-Source Program.About the authorThis article was written by Marc Wouts. Marc joined the research team of CFM in 2012 and has worked on a range of research projects, from optimal trading to portfolio construction.Marc has always been interested in finding efficient workflows for doing collaborative research involving data and code. In 2015 he authored an internal tool for publishing Jupyter and R Markdown notebooks on Atlassians Confluence wiki, providing a first solution for collaborating on notebooks. In 2018, he authored Jupytext, an open-source program that facilitates the version control of Jupyter notebooks. Marc is also interested in data visualisation, and coordinates a working group on this subject at CFM.Marc obtained a PhD in Probability Theory from the Paris Diderot University in 2007.DisclaimerAll views included in this document constitute judgments of its author(s) and do not necessarily reflect the views of Capital Fund Management or any of its affiliates. The information provided in this document is general information only, does not constitute investment or other advice, and is subject to change without notice. ",
        "Awesome article - I'm wondering, for \"Publishing the Notebook\" part of the workflow, have you ever seen Kyso (<a href=\"https://kyso.io\" rel=\"nofollow\">https://kyso.io</a>) - disclaimer, I'm a founder. We started Kyso to make it easier to communicate insights gained from analysis to non-technical people by converting data science tools (e.g. Jupyter Notebooks) into conversational tools in the form of blog posts. You can make public posts or have an internal \"data blog\" for your team, where you push your work to Github and it is reflected on Kyso. Would love to hear your thoughts on how it could fit into existing workflows.",
        "I don't think Jupyter notebooks should be used for automated jobs. They're great for exploratory stuff but once things are getting fleshed out and cleaned up, one should move to proper python files that can be unit tested and versioned without having to go to crazy lengths..."
      ],
      "relevant": "true"
    },
    {
      "id": 19669153,
      "title": "Why is version control in Jupyter notebooks so hard?",
      "search": [
        "Why is version control in Jupyter notebooks so hard?",
        "Are there any tools that help with version control on notebooks?",
        "I've been clearing my output using nbconvert before putting the notebook into version control. I have a precommit hook and a check in CI. This works for my use case but I can understand needing to preserve output.<p>jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace my_notebook_name.ipynb",
        "You bet. I built ReviewNB[1] specifically for Jupyter Notebook code reviews.<p>There's also,<p>- nbstripout[2] for stripping outputs automatically before every commit<p>- nbdime[3] for diff'ing notebooks locally<p>- jupytext[4] for converting notebooks to markdown and vice-a-versa<p>[1] <a href=\"https://www.reviewnb.com/\" rel=\"nofollow\">https://www.reviewnb.com/</a><p>[2] <a href=\"https://github.com/kynan/nbstripout\" rel=\"nofollow\">https://github.com/kynan/nbstripout</a><p>[3] <a href=\"https://github.com/jupyter/nbdime\" rel=\"nofollow\">https://github.com/jupyter/nbdime</a><p>[4] <a href=\"https://github.com/mwouts/jupytext\" rel=\"nofollow\">https://github.com/mwouts/jupytext</a>"
      ],
      "relevant": "true"
    },
    {
      "id": 20646674,
      "title": "The market figured out Gitlab’s secret",
      "search": [
        "The market figured out Gitlab’s secret",
        "https://about.gitlab.com/2019/08/08/built-in-ci-cd-version-control-secret/",
        "Theres a movement in the DevOps industry and the world right now: to do more in a simple way that inspires us to innovate. GitLab started this trend in the DevOps space by simplifying the delivery of code by combining GitLab CI and GitLab version control. We didn't originally buy into the idea that this was the right way to do things, but it became our secret capability that weve doubled down on. Lets combine applications The story starts with Kamil Trzciski, now a distinguished engineer at GitLab. Soon after Kamil came to work for GitLab full time, he began talking with me and my co-founder, Dmitriy Zaporozhets, suggesting that we bring our two projects together GitLab Version Control and GitLab CI, making it into one application. Dmitriy didnt think it was a good idea. GitLab version control and CI were already perfectly integrated with single sign-on and APIs that fit like a glove. He thought that combining them would make GitLab a monolith of an application, that it would be disastrous for our code quality, and an unfortunate user experience. After time though, Dmitriy started to think it was the right idea as it would deliver a seamless experience for developers to deliver code quickly. After Dmitriy was convinced, they came to me. I also didnt think it was a good idea. At the time I believed we needed to have tools that are composable and that could integrate with other tools, in line with the Unix philosophy. Kamil convinced me to think about the efficiencies of having a single application. Well, if you dont believe that its better for a user, at least believe its more efficient for us, because we only have to release one application instead of two. Efficiency is in our values. - Kamil Trzcinski, distinguished engineer at GitLab Realizing the future of DevOps is a single application That made sense to me and I no longer stood in their way. The two projects merged and the results were beyond my expectations. The efficiencies that were so appealing to us, also made it appealing to our customers. We realized we stumbled on a big secret because nobody believed that the two combined together would be a better way of continuously delivering code to market. We doubled down on this philosophy and we started doing continuous delivery. From that day on, I saw the value of having a single application. For example, a new feature we are implementing is auto-remediation. When a vulnerability comes out, say a heart bleed, GitLab will automatically detect where in your codebase that vulnerability exists, update the dependency, and deliver it to your production environment. This level of automation would be hard to implement without being in a single application. By combining the projects we unified teams helping them realize the original intent of DevOps and that is magical to see. The market validates our secret And while we bet on this philosophy the industry is now seeing it as well. In September of 2015 we combined GitLab CI and GitLab version control to create a single application. By March of 2017, Bitbucket also realized the advantages of this architecture and released Pipelines as a built-in part of Bitbucket. In 2018, GitHub announced Actions with CI-like functionality built into a single application offering. In the last six months, JFrog acquired Shippable and Idera acquired Travis CI, showing a consolidation of the DevOps market and a focus on CI. The market is validating what we continually hear from our users and customers: that a simple, single DevOps application meets their needs better. We hope you will continue to join us in our effort to bring teams together to innovate. Everyone can contribute here at GitLab and as always, we value your feedback, thoughts, and contributions. Want to hear me talk through the origin story? Listen to the Software Engineering Daily podcast where I talk about combining GitLab CI and GitLab Version Control. The industry has caught onto @GitLabs secret. Learn more about why GitLab combined GitLab CI and GitLab version control Sid Sijbrandij Click to tweet Sign up for GitLabs twice-monthly newsletter ",
        "Every time github releases a feature this is the response. These posts are so pathetic. I’m surprised that they continue to play this angle. It’s a very polarizing way to address the community that will definitely continue to stir up us vs them mentality between GitHub and Gitlab users.",
        "For all the bashing GitLab gets, personally, I want GitLab to survive and keep competing at some level with GitHub.<p>Should GitHub dominate the market and gobble up competition, we all know how it goes for its parent company."
      ],
      "relevant": "true"
    },
    {
      "id": 19738327,
      "title": "We need a new generation of source control",
      "search": [
        "We need a new generation of source control",
        "https://www.rookout.com/cant-git-no-satisfaction-why-we-need-a-new-gen-source-control/",
        "By: | January 5, 2019Liran is the Co-Founder and CTO of Rookout. Hes an Observability and Instrumentation expert with a deep understanding of Java, Python, Node, and C++. Liran has broad experience in cybersecurity and compliance from his past roles. When not coding, you can find Liran hosting his podcast, speaking at conferences, writing about his tech adventures, and trying out the local cuisine when traveling.Remember the good old days of enterprise software? When everything had to be installed on-premises? To install an application, youd have to set up a big, vertically scalable server. You would then have to execute a single process written in C/C++, Java or .NET. Well, as you know, those days are long gone.Everything has changed with the transition to the cloud and SaaS. Today, instead of comprising a single vertically scalable process, most applications comprise multiple horizontally scalable processes. This model was first pioneered by Googles borg and by Netflix on EC2. Nowadays, though, you no longer have to be a large enterprise to access microservice infrastructures. Kubernetes and serverless have made microservices viable and accessible to even small startups and lone coders.Lets Git down to businessSo where does Git fit into the picture? Git is an excellent match for single-process applications, but it starts to fail when it comes to multi-process applications. This is precisely what gave birth to the endless mono-repo vs. multi-repo flame-wars.Each side of this debate classifies the other as zealous extremists (as only developers can!), but both of them miss the crux of the matter: Git and its accompanying ecosystem are not yet fit for the task of developing modern cloud-native applications.Shots fired: multi-repos suckBefore we dive in, lets answer this: whats great about Git? Its the almighty atomic commit, the groundbreaking (at the time) branching capabilities, and the ever-useful blame. Well, these beloved features all but disappear in a multi-repo setup. Working in multiple repositories comes with significant drawbacks, which is why its not at all surprising that some of the biggest names in the tech world, including Google and Facebook, have gone down the mono-repo path at a huge investment of time and resources.Dependency management in a multi-repo setup is a nightmare. Instead of having everything in a single repository, you end up with repositories pointing to each other using two git features (git submodules and git subtree) and language-specific dependency management such as npm or Maven. The very existence of the many different methods to manage multi-repos is in itself proof that none of these tools are enough on their own. Gits source-of-truth is no longer a single folder on your computer but a mishmash of source providers and various artifactories.In developers everyday work, repository separation becomes an artificial barrier that impacts technological decisions. This creates a Conways Law effect, making early design decisions about component boundaries very hard to change. It also makes large scale refactorings a much trickier business.However, the biggest failure of the multi-repo is cultural. Instead of having all your source code readily available to all developers, they have to jump hurdles to figure out which repo they need and then clone it. These seemingly-small obstacles often become high fences: developers stop reading and updating code in components and repositories that arent directly in their responsibility.With all these engineering, operations and cultural barriers, why doesnt everyone go the mono-repo route?Take no prisoners: mono-repos suck tooOnce youve packed everything into a single repository, figuring out the connections within the repository becomes a challenge. For humans, that can chip away at the original architecture, breaking away useful abstractions and jumbling everything together.For machines, this lack of separation within the repo is even worse. When you push a code change to a repo, automated processes kick in. CI systems build and test the code, and then CD systems deploy it. Sometimes its to a test or staging environment, and sometimes directly to production.There are certain components you will need to build and deploy hundreds of times a day. At the same time, there are other more delicate and mission-critical components. These require human supervision and extra precaution. The problem with mono-repository is that it mixes all of these components into one. More surprising is the fact that todays vast Git CI ecosystem, with its impressive offerings in both the hosted and the SaaS space, doesnt even try to tackle the issue. In fact, not only will Git CI tools rebuild and redeploy your entire repo, they are often built explicitly for multi-repo projects.Another issue is large repository sizes. Git doesnt handle large repos gracefully. You can easily end up with repo sizes that dont fit in your hard-drive, or clone time that ends up in the hours. For big projects, this requires careful management and pruning of commit history. It is also essential to avoid committing dependencies, auto-generated files and other large files which may be necessary for specific scenarios.Is there still hope for multi-repos?There are new tools that seek to bring some of the benefits of mono-repos to multi-repos. These tools try to set up a configuration that would unite multiple repos under a single umbrella/abstraction layer, thus making managing multiple-repositories easier for example, TwoSigmas Git-meta, mateodelnortes meta, gitslave ,and a bunch of others.These tools bring back a bit of sanity into the complexities of managing multi-repos, reducing some of the toil and error-prone manual operations. But none of them truly give back the control and power of a single Git repo.You cant have your cake and Git it tooThe downsides of multi-repos are real. You cant deny the value of a (truly) single source of truth, (truly) atomic commits, and a (truly) single place to develop and collaborate. On the other hand, none of the downsides of mono-repos are inherent. All of them are related to the current implementation of the Git source control tool itself and its accompanying eco-system, especially CI/CD tools.Its time for a new generation of source control that wasnt purely designed for open-source projects, C and the Linux kernel. A source control designed for delivering modern applications in a polyglot cloud-native world. One that embraces code dependencies and helps the engineering team define and manage them, rather than scaring them away. A source control that treats CI, CD, and releases as first-class citizens, rather than relying on the very useful add-ons provided by GitHub and its community. ",
        "Are we mistaking a dependency control problem as a revision control problem?<p>In a previous life, before microservices, CI/CD etc. existed, we did just fine with 20-30 CVS repositories, each representing a separate component (a running process) in a very large distributed system.<p>The only difference was that we did not have to marshal a large number of 3rd party dependencies that were constantly undergoing version changes. We basically relied on C++, the standard template library and a tightly version controlled set of internal libraries with a single stable version shared across the entire org. The whole system would have been between 750,000 - 1,000,000 lines of code (libraries included).<p>I'm not saying that that's the right approach. But it's mind boggling for me that we can't solve this problem easily anymore.",
        "The source control system is not the piece of the equation that matters to most people.  The build system is the important part.  That's what prevents you from rebuilding the repository when you only change one Kubernetes config file, or what causes 100 docker images to be built because you changed a file in libc.<p>I think the tooling around this is fairly limited right now.  I feel that most people are hoping docker caches stuff intelligently, which it doesn't.  People should probably be using Bazel, but language support is hit-or-miss and it's very complicated.  (It's aggravated by the fact that every language now considers itself responsible for building its own code.  go \"just works\", which is great, but it's hard to translate that local caching to something that can be spread among multiple build workers.  Bazel attempts to make all that work, but it basically has to start from scratch, which is unfortunate.  It also means that you can't just start using some crazy new language unless you want to now support it in the build system.  We all hate Makefiles, but the whole \"foo.c becomes foo.o\" model was much more straightforward than what languages do today.)"
      ],
      "relevant": "false"
    },
    {
      "id": 21836168,
      "title": "Kubernetes Security Tools Abound",
      "search": [
        "Kubernetes Security Tools Abound",
        "https://searchitoperations.techtarget.com/news/252475750/New-Kubernetes-security-tools-abound-as-container-deployments-grow",
        "As Kubernetes use expands in production, enterprises have a number of IT security tools to choose from for centralized, policy-based control over container clusters. Emerging Kubernetes security tools focus security operations at higher layers of the IT stack, as the container orchestration platform grows into a production staple for mainstream enterprises. Approaches to Kubernetes security vary among these tools -- one newcomer, Octarine, piggybacks on the Envoy proxy and Istio service mesh to monitor container infrastructure for threats and enforce security policy, while an existing container security startup, NeuVector, plugs policy-as-code tools into the CI/CD pipeline. Another tool from managed Kubernetes player Fairwinds combines security and IT performance and reliability monitoring with a multi-cluster dashboard view. What they all have in common is their emergence as enterprises look for centralized points of Kubernetes security control over an entire environment, in addition to tools that operate at the individual container or application workload level. \"Looking at the container workload is great, but if you don't have a sense of everything that's running within your Kubernetes ecosystem and how it's communicating, it's very easy for rogue deployments to slip in when you have hundreds of namespaces and thousands of pods,\" said Trevor Bossert, manager of DevOps at Primer AI, a data analytics firm in San Francisco which began using Octarine's Kubernetes security software six months ago. \"This will let you know when [cluster configurations] are violating policies, like if they're public by default when they're not supposed to be.\" Octarine rides along in service mesh sidecar Octarine, based in Sunnyvale, Calif., came out of stealth last month with a service mesh-based approach to Kubernetes security. The company claims that installing its software on the Envoy service mesh proxy gives users a clearer picture of the Kubernetes orchestration layer than tools that monitor the infrastructure from privileged containers on hosts. Placing security monitoring and enforcement inside Envoy lets Octarine see whether container workloads are exposed to the internet, how secrets are exposed to container workloads and monitor east-west traffic more effectively, according to Octarine CTO and co-founder Haim Helman. Envoy is often associated with the Istio service mesh, which has its own security features, but Octarine doesn't replace those features, which include the enforcement of role-based access control and mutual TLS encryption. Instead, Octarine collects security telemetry and identifies anomalies and threats with its Octarine Runtime module, and manages Kubernetes security policy-as-code with a tool it calls Guardrails. It can feed security monitoring information into Istio's control plane if a user already has it, or run its own service mesh control plane if the user doesn't have Istio in place. There are other ways to create and enforce Kubernetes policy-as-code, among them the open source Open Policy Agent (OPA) that rose in popularity among large organizations in 2019, but midsize companies with smaller teams may find Octarine's policy-as-code features easier to use. Octarine's service-mesh-based Kubernetes security tool centralizes policy management at the network level \"Not having to craft all policies from scratch, being able to [let] Octarine observe the traffic and providing the best policy, is less time-consuming and involves less duplication of work, especially for a smaller team like ours,\" said Primer AI's Bossert. Running Octarine on Envoy offloads some of the resource requirements from the container host, and managing mTLS encryption and policy-as-code together through Istio is also convenient, he said. Larger organizations such as the U.S. Air Force will also keep an eye on Octarine as it matures, as OPA has been unwieldy to use so far, but would most like to use a Kubernetes policy as code tool that isn't tied to a particular service mesh. \"You can end up with massive lock-in if you abstract teams from the infrastructure, but then couple [security policy] tightly with a mesh again,\" said Nicolas Chaillan, chief software officer for the military branch, which has deployed Istio in production but plans to evaluate other service meshes, including Linkerd. NeuVector loops in CRDs for Kubernetes security NeuVector released a Kubernetes security policy-as-code tool that moved it up the stack last month, which deploys Kubernetes Custom Resource Definitions (CRDs) that are version-controlled and tested within a CI/CD pipeline instead of a service mesh. The company, which began as a container runtime scanning tool, also added network-based data loss prevention (DLP) features and multi-cluster management in version 3.0 in March. A lot of tools cover just one aspect of security management, and just figuring out how all the pieces fit together is a hassle. In about three to five years, I think we'll see consolidation in the market and more complete [products]. Sean McCormickVice president of engineering, Element Analytics Like Octarine, NeuVector can observe normal container behavior on a Kubernetes cluster network and define appropriate application behavior instead of requiring that users create policy from scratch. But for users interested in OPA, NeuVector's tool can import OPA-based policy-as-code data into CRDs as well. \"With an engineering team of 20 people it's hard to pull in new things like service mesh,\" said NeuVector user Sean McCormick, vice president of engineering at Element Analytics, an industrial data analytics firm in San Francisco. \"Being able to export security rules is also nice, so you don't have to spend a week learning rules in a new place.\" McCormick also plans to evaluate NeuVector's DLP features, and would like to see the vendor expand further to offer a web application firewall and application code security analysis. \"There are way too many security tools,\" he said. \"A lot of tools cover just one aspect of security management, and just figuring out how all the pieces fit together is a hassle. In about three to five years, I think we'll see consolidation in the market and more complete [products].\" NeuVector's CRD workflow for Kubernetes policy-as-code. Fairwinds tackles Kubernetes security fundamentals Another container management vendor that looks to expand its influence in the Kubernetes security realm is Fairwinds, a managed Kubernetes service provider in Boston. Fairwinds, formerly ReactiveOps, originally specialized in fully managed Kubernetes clusters, but launched Kubernetes management tools customers can use on their own beginning with the Polaris Kubernetes distro and Goldilocks resource request optimization tool in July. Last month, it added Fairwinds Insights, which displays Kubernetes security monitoring data alongside performance and reliability feedback. Fairwinds Insights also presents ranked remediation recommendations that include YAML code users can copy and paste to shore up vulnerabilities. The tool will also pull in and orchestrate third-party Kubernetes security utilities such as Aqua's kube-hunter. Fairwinds Insights is not as in-depth a tool as OPA or full-blown policy-as-code, but it could help smaller shops move from Kubernetes clusters fully managed by the vendor to self-managed environments, while maintaining security best practices. Fairwinds Insights prioritizes Kubernetes security and reliability action items and includes remediation recommendations for users. For companies such as Philadelphia-based Sidecar, a marketing and advertising software firm, Fairwinds Insights will cover the most crucial Kubernetes security management requirements at a cluster-wide level while the IT team hones its container management skills. \"A tool at the network infrastructure level gets past the most immediate security concerns, such as locking down public access to clusters and configuring AWS load-balancers,\" said Dominic O'Kane, manager of cloud engineering at Sidecar, which also uses Fairwinds' managed services. \"Then we can take on more fine-grained tools that look at individual applications and containers.\" Dig Deeper on Managing Virtual Containers CNCF policy-as-code project bridges Kubernetes security gaps By: BethPariseau Sysdig deal reflects infrastructure-as-code security buzz By: BethPariseau Kubernetes security automation saves SecOps sanity By: BethPariseau What is container management and why is it important? By: EmilyMell ",
        "As a (perhaps overly cynical) outside observer it feels that \"kubernetes X abound\" for all X.  There's just such a complex ecosystem of tooling evolving here.",
        "I find it interesting that companies can convince auditors that security sidecars that add auth and encryption actually meet compliance requirements... it's a nice architecture but I'd argue it renders the environment non-compliant."
      ],
      "relevant": "false"
    },
    {
      "id": 19078281,
      "title": "MIT Hacker Tools: a lecture series on programmer tools",
      "search": [
        "MIT Hacker Tools: a lecture series on programmer tools",
        "https://hacker-tools.github.io/",
        "This class has moved to https://missing.csail.mit.edu/. You should go there to see the newest version of the material. This site is being left up for archival purposes. You can see all the lectures from the IAP 2019 course here. ",
        "Hi all! We (@anishathalye, @jjgo, and @jonhoo) have long felt that while university CS classes are great at teaching specific topics, they often leave it to students to figure out a lot of the common knowledge about how to actually use your computer. And in particular, how to use it efficiently.<p>There’s just no class in the undergrad curriculum that teaches you how to become familiar with the system you’re working with! Students are expected to know about, or figure out, the shell, editors, remote access and file management, version control, debugging and profiling utilities, and all sorts of other useful tools on their own. Often times, they won’t even know that many of these tools exist, and instead do things in roundabout ways or simply be left frustrated about their development environment.<p>To help mitigate this, we decided to run this short lecture series at MIT during the January Independent Activities Period that we called “Hacker Tools” (in reference to “hacker culture”, not hacking computers). Our hope was that through this class, and the resulting lecture materials and videos, we might be able to bootstrap students’ knowledge about the tools that are available to them, which they can then put to use throughout their time at university, and beyond.<p>We’ve shared both the lecture notes and the recordings of the lectures in the hopes that people outside of MIT may also find these resources useful in making better use of their tools. If that turns out to be true, we’re also thinking of re-doing the videos in screen-cast style with live chat and a proper microphone when we get the time. If that sounds interesting to you, and if you have ideas about other things you’d like to see us cover, please leave a comment below; we’d love to hear from you!<p>We’re sure there are also plenty of cool tools that we didn’t get to cover in this series that you all know and love. Please share them below along with a short description so we can all learn something new!<p>Anish, Jose, and Jon",
        "The equivalent UCLA course is CS35L: Software Construction Laboratory (<a href=\"https://web.cs.ucla.edu/classes/winter19/cs35L/\" rel=\"nofollow\">https://web.cs.ucla.edu/classes/winter19/cs35L/</a>). It's taught by Paul Eggert (big open source/coreutils/emacs contributor + author of diff/sort)."
      ],
      "relevant": "true"
    },
    {
      "id": 19185570,
      "title": "Arm Helium: New vector extension for the M-Profile Architecture",
      "search": [
        "Arm Helium: New vector extension for the M-Profile Architecture",
        "https://community.arm.com/processors/b/blog/posts/arm-helium-the-new-vector-extension-for-arm-m-profile-architecture",
        "Arm has announced the latest version of the Armv8-M architecture, known as Armv8.1-M, including the new M-Profile Vector Extension (MVE). The vector extensionbrings up to15times performance uplift to machine learning (ML) functions, and up to5times uplift to signal processing functionscompared to existing Armv8-M implementations. It may be viewed as the Armv8-M architectures version of the Advanced SIMD Extension (Neon) in the A-Profile. Arm Helium technologyis the M-Profile Vector Extension (MVE)for the Arm Cortex-M processor series. Armv8.1-M architecture new features A new vector instruction set extension (MVE) Additional instruction set enhancements for loops and branches (Low Overhead Branch Extension) Instructions providing half precision floating-point support Instruction improving state management of the Floating Point Unit (FPU) Enhancements to debug including: Performance Monitoring Unit (PMU) Unprivileged Debug Extension Debug support for MVE Reliability, Availability and Serviceability (RAS) extension Start early software development Arm tools aredeveloped along with the architecture. They are now ready for lead partners to start developing software and migrating libraries and other code to Helium,to enable performance increases for DSP and machine learning applications. Tools with support include: Fast Models for software execution and optimization on a virtual platform Arm Development Studio for comprehensive software development and debugging on Windows or Linux for any Arm-based projects Arm Compiler 6 for maximizing code performance Keil MDK for software development and debugging in Windows for Cortex-M and microcontrollers The Armv8.1-M simple programmers model, combined with familiar Arm tools, is a key advantage of Helium. Using a single toolchain for control and data processing, leads to lower development costs and less code maintenance. Virtual platform with Fast Models Arm Fast Modelsprovide fast, flexible programmer's view models of Arm architecture and IP, enabling software development of drivers, firmware, operating systems, and applications prior to silicon availability. Fast Models allow full control over the simulation, including profiling, debug and trace. There is a Fast Model available for lead partners, which can be used for early software development. It is based on the MPS2 Fixed Virtual Platform (FVP). The Armv8-M architecture envelope model (AEM) has been extended via the plugin interface to support Helium. This provides a suitable platform to get started writing and debugging software. Code, build and debug with Development Studio Development Studiofeaturing Keil MDK(Vision) has added Helium support for software compilation (Arm Compiler 6) and debugging. This includes disassembly and updated register views for new registers in Armv8.1-M.The toolsuite is also available for lead partners today. Performance enhancements to Cortex-M Helium, the M-Profile Vector Extension included in Armv8.1-M, brings significant enhancements to the Cortex-M processor range and will enable the use of a single CPU for both control and data processing code. The performance enhancements enable applications, such as machine learning and DSP. Arm tools have been developed in parallel with the architecture and are available now for lead partners to start developing software on both Windows and Linux. The Helium support in Arm Compiler 6, combined with leading performance and code density, make it a great choice to get a jumpstart on migrating software to Helium. Arm Fast Models combined with Arm debuggers make it possible to run code and see the Architecture Reference Manual in action. Further reading The press announcementgives a high-level overview of Armv8.1-M and Helium, plusdetails on the performanceenhancements. The 'Making Helium' blogoffers insight intothe creation of Arm's MVE. For full details on the architecture see the Architecture Reference Manual for Armv8.1-M. The Introduction to Armv8.1-M architecture white paper showcases the technical highlights of the new features and is available to download below. Download Armv8.1-M Architecture White Paper November 3, 2021 November 3, 2021 October 29, 2021 ",
        "The blog post <a href=\"https://community.arm.com/arm-research/b/articles/posts/making-helium-why-not-just-add-neon\" rel=\"nofollow\">https://community.arm.com/arm-research/b/articles/posts/maki...</a> has more useful content.",
        "Meantime, in the royalty-free side, RISCV's work on V extension continues: <a href=\"https://www.embecosm.com/2018/09/09/supporting-the-risc-v-vector-extension-in-gcc-and-llvm/\" rel=\"nofollow\">https://www.embecosm.com/2018/09/09/supporting-the-risc-v-ve...</a>"
      ],
      "relevant": "false"
    },
    {
      "id": 21534619,
      "title": "Show HN: Respresso – localization and design asset optimizer for iOS and Android",
      "search": [
        "Show HN: Respresso – localization and design asset optimizer for iOS and Android",
        "https://respresso.io/",
        "Save hours with efficient collaboration Manage your resources in multiplatform environments. Fonts Easy integration of custom fonts Localizations Modify localization texts or add a new language to your project, without developers Images Change or resize an image anytime and keep in sync on all platforms App icons No more generating thousands of icon sizes, just use one SVG for all platforms. Colors No more incorrect guideline colors. Your designers will have the ability to set the perfect colors to be used on all platforms Raw Easy access to all your common config files(JSON, XML, YAML) Customize Respresso Extend Respressos functionality or connect your work tools like Slack, Teams, Jenkins etc. Version control All resources are under version control. You can easily lock your assets version and reuse it later. Be agile and spare development time Collaborate on assets with your team members or customer from anywhere in a transparent way to boost your productivity. It automatically transforms and delivers to your project without assistance. Your assets will be ready for use almost immediately. It takes care of your digital assets (images, texts, colors, fonts, etc.) across multiple platforms and projects. Help your team focus during the development stage. Developers code, designers deliver graphics, the marketing team, translators manage your localization and this is just the beginning of stress-free development. Start using in 3 simple steps Easy as pie 1-minute setup Upload your origin resources Sync converted resources across multiple platforms Respresso is simple but powerful Respresso can easily be integrated into your build process and also works well with your Continuous Integration tools. Respresso manages your resources Convert your resources automatically to platform-specific formats such as VectorDrawable for Android, PDF for iOS etc. Synchronize with your project in build-time regardless of which platform you use What else is it good for? Versioned resources and repeatable build support for CI & CD A better team experience via team and project creation. It provides you with a way to track every change in each project and send feedback. For example you will be notified when someone changes the key of a localization you use Well-separated roles spell correction and translation without developer intervention fix any graphics files and change icons without coding skills rebrand the application by changing colors and app icons Enforce naming conventions to enhance code quality Try our image converter Simply upload your chosen SVG file and download the generated resources as VectorDrawable for Android and PDF for iOS. Get access for Free Once you create an account you can try Respresso for free, for iOS, Android and Web frontend projects, too. Do you have any question? ",
        "Respresso helps mobile developers by automatically optimizing their design assets for iOS and Android and comes with a live localization feature.<p>It's still in beta and all feedback is highly appreciated.",
        "What is the pricing?"
      ],
      "relevant": "false"
    },
    {
      "id": 19128641,
      "title": "Ubuntu 18.04.2 LTS Released",
      "search": [
        "Ubuntu 18.04.2 LTS Released",
        "https://wiki.ubuntu.com/BionicBeaver/ReleaseNotes/ChangeSummary/18.04.2",
        "Contents Installation bug fixes Upgrade bug fixes Desktop fixes Server and Cloud related fixes Kernel and Hardware support updates Unsorted changesThis is a brief summary of bugs fixed between Ubuntu 18.04.1 and 18.04.2. This summary covers only changes to packages in main and restricted, which account for all packages in the officially-supported CD images; there are further changes to various packages in universe and multiverse. Some of these fixes were by Ubuntu developers directly, while others were by upstream developers and backported to Ubuntu. For full details, see the individual package changelogs. In addition to the bugs listed below, this update includes all security updates from the Ubuntu Security Notice list affecting Ubuntu 18.04 LTS that were released up to and including February 4, 2019. The last update included was USN-3871-3 (Linux kernel vulnerabilities). Installation bug fixes Updated CD images are provided with this release, including fixes for some installation bugs. (Many installation problems are hardware-specific; for those, see \"Hardware support bugs\" below.) console-setup 1762952 keyboard-configuration.config: While sourcing config files to re-seed debconf, load missing variables as empty values of same console-setup 1762952 keyboard-configuration.{config,templates}: There is no good default for layout toggling, stop pretending there is. Console users can set one with dpkg-reconfigure or editing /etc/defaults/keyboard livecd-rootfs 1302192 generate all tar files with --xattrs. livecd-rootfs 1585233 ubuntu-cpc: Reintroduce the -root.tar.xz artifact. console-setup 1788597 keyboard-configuration.config: Fix exit/return thinko livecd-rootfs 1783129 Disentangle enabling universe in the final image a little from having PREINSTALLED=true set and enable it for a live-server build. livecd-rootfs 1776891 Disable journald rate limiting in the live-server live session. initramfs-tools 1769682 scripts/functions: write netplan config files to /run/netplan for network devices configured with configure_networking. initramfs-tools 1769682 scripts/functions: add 'critical: true' parameter; requires netplan 0.36.2. initramfs-tools 1661629 Work out the kernel modules required to support ZFS filesystems and add them as necessary. ubiquity 1749289 Implement missing reboot and shutdown methods in debconf_ui ubiquity 1777900 Add systemd-resolved to oem-config.target's Wants livecd-rootfs 1792905 Ensure /lib/modules exists in root tarballs and sqashfs. initramfs-tools 1667512 b4804dd] Only sync the filesystem containing the initramfs initramfs-tools 1791959 debian/initramfs-tools.postinst: remove orphaned old-dkms initrd files in /boot. ubiquity 1789920 Mask ubiquity.service in a system installed with oem-config/enabled=true, as this prevents getty@tty1.service from running. livecd-rootfs 1805190 Include grub efi packages in manifests for uefi images. livecd-rootfs 1799773 Disable checksum generation. installation-guide 1730322 Describe HWE kernel, point release and url for supported arm64 platforms. installation-guide 1804306 Move supported arm64 server list to a wiki page to ease maintenance debian-installer 1807023 build/pkg-lists/base: add ca-certificates-udeb to enable HTTPS without d-i/allow_unauthenticated_ssl in stock initramfs image as in Debian. ubiquity 1768230 scripts/plugininstall.py: don't hard-code a resume partition in /etc/initramfs-tools/conf.d/resume at install time. In bionic and later, initramfs-tools will autodetect an appropriate resume partition at initramfs generation time, so ubiquity's resume setting is redundant and possibly wrong. debian-installer 1809021 Add HWE variants for all architectures partman-efi 1803031 check.d/efi: Make sure we block on a missing EFI partition no matter what architecture, not just for ia64. One could attempt to install on EFI x86 and will need an ESP to be able to install GRUB. ubiquity 1772374 scripts/plugininstall.py: Make sure efivars is bind-mounted when installing the bootloader. ubiquity 1803031 Automatic update of included source packages: partman-efi 71ubuntu2.2. Upgrade bug fixes These changes fix upgrade issues, smoothing the way for future upgrades to later releases of Ubuntu. ubuntu-release-upgrader 1783589 make sure that snapd is installed before trying to use it. ubuntu-release-upgrader 1783593 update the view with information regarding the progress of snaps being installed. ubuntu-release-upgrader 1783738 when checking for connectivity to the snap store use C.UTF-8 for the language so error message matching works. ubuntu-release-upgrader 1785096 Remove debs from apt's \"Dir::Cache::archives\" folder after the upgrade has completed successfully. ubuntu-release-upgrader 1766890 Upgrade libc6 before other packages to work around trigger issues ubuntu-release-upgrader 1787668 DistUpgradeQuirks.py: if ubuntu-desktop or snapd isn't in the package cache don't try and run the quirk for installing or upgrading snaps. gnome-initial-setup 1781417 Display the ubuntu welcome wizard in Unity, don't display the \"what's new\" page though since Unity didn't change ubuntu-release-upgrader 1787649 DistUpgradeController.py: If the attempt to only upgrade libc6 ends up creating an issue with apt's problem resolver try the full upgrade. This is what would have happened before the fix for bug 1766890. update-manager 1317164 Print transaction error and let the user try again applying updates update-manager 1791931 Don't ask backend to do package operations aready done. Aptdaemon cancels the transaction when asked to remove packages already removed which results the failure being shown to the user. This is unnecessary as update-manager can just filter the package operations to be done using a fresh cache and decrease the likelyhood of hitting a race condition where packages to be removed are already removed. ubuntu-release-upgrader 1797209 do-release-upgrade: do not run the release upgrade if either not all updates are installed or a reboot is required due to a libc6 upgrade. ubuntu-release-upgrader 1796940 debian/control: change ubuntu-release-upgrader-core to depend on ca-certificates unattended-upgrades 1789637 Unlock for dpkg operations with apt_pkg.pkgsystem_unlock_inner() when it is available. Also stop running when reacquiring the lock fails. Thanks to Julian Andres Klode for original partial patch unattended-upgrades 1781586 Skip rebuilding python-apt in upgrade autopkgtests. Python-apt has a new build dependency making the rebuilding as is failing and the reference handling issue is worked around in unattended-upgrades already. unattended-upgrades 1785093 Stop trying when no adjustment could be made and adjust package candidates only to lower versions unattended-upgrades 1785093 Skip already adjusted packages from being checked for readjusting. This makes it clearer that the recursion ends and can also be a bit quicker. update-manager 1072136 Keep or delete packages after looping over all of them. This prevents the resolver from changing the packages in the loop resulting in not keeping some phased packages back from being upgraded. update-manager 1795898 Stop lazy import of InstallBackends. Lazy imports made update-manager crash when an update-manager update changed the backend API and an updated incompatible backend was loaded to the not updated running update-manager process. update-manager 1790670 Cancel transaction on exit only when Cancel button is active. Also ignore exception when cancellation fails. ubuntu-release-upgrader 1799839 DistUpgrade/DistUpgradeController.py: check all the python symlinks and versions instead of the python one. Thanks to juliank for the assistance. ubuntu-release-upgrader 1799710 do-release-upgrade: Do not block release upgrades if the installable updates are ones which are not fully phased. update-manager 1787553 Add a reminder to enable Livepatch. ubuntu-release-upgrader 1797384 Set icon_name in the dialogs so that there will be an icon in gnome-shell. update-notifier 1800862 Check if a Livepatch patch has been applied during boot or before update-notifier has started. unattended-upgrades 1778219 Trigger unattended-upgrade-shutdown actions with PrepareForShutdown() Performing upgrades in service's ExecStop did not work when the upgrades involved restarting services because systemd blocked other stop/start actions making maintainer scripts time out and be killed leaving a broken system behind. Running unattended-upgrades.service before shutdown.target as a oneshot service made it run after unmounting filesystems and scheduling services properly on shutdown is a complex problem and adding more services to the mix make it even more fragile. The solution of monitoring PrepareForShutdown() signal from DBus allows Unattended Upgrade to run _before_ the jobs related to shutdown are queued thus package upgrades can safely restart services without risking causing deadlocks or breaking part of the shutdown actions. Also ask running unattended-upgrades to stop when shutdown starts even in InstallOnShutdown mode and refactor most of unattended-upgrade-shutdown to UnattendedUpgradesShutdown class. unattended-upgrades 1778219 Increase logind's InhibitDelayMaxSec to 30s. This allows more time for unattended-upgrades to shut down gracefully or even install a few packages in InstallOnShutdown mode, but is still a big step back from the 30 minutes allowed for InstallOnShutdown previously. Users enabling InstallOnShutdown node are advised to increase InhibitDelayMaxSec even further possibly to 30 minutes. unattended-upgrades 1803749 Stop using ActionGroups, they interfere with apt.Cache.clear() causing all autoremovable packages to be handled as newly autoremovable ones and be removed by default. Dropping ActionGroup usage does not slow down the most frequent case of not having anything to upgrade and when there are packages to upgrade the gain is small compared to the actual package installation. Also collect autoremovable packages before adjusting candidates because that also changed .is_auto_removable attribute of some of them. update-manager 1805118 Do not show the livepatch reminder if update-manager is running on a distribution without software-properties-gtk. update-manager 1787553 Add a reminder to enable Livepatch. software-properties 1805436 cloudarchive: Enable support for the Stein Ubuntu Cloud Archive on 18.04. shotwell 1802895 New bugfix update shotwell 1606491 Fix \"Out of memory\" issues when scrolling through large collections shotwell 1723181 direct: Fix crash when dismissing modifications libreoffice 1799230 New upstream release libreoffice-l10n 1799230 New upstream release unattended-upgrades 1806487 Start service after systemd-logind.service to be able to take inhibition lock unattended-upgrades 1806487 Handle gracefully when logind is down update-notifier 1809505 src/update-notifier.c: Don't use G_SPAWN_DO_NOT_REAP_CHILD in order to avoid zombie processes. update-manager 1798618 UpdateManager/Core/MetaRelease.py: set prompt in MetaReleaseCore so that do-release-upgrade can provide more informative error messages. update-manager 1795024 UpdateManager/Core/MetaRelease.py: set prompt in MetaReleaseCore so that do-release-upgrade can provide more informative error messages. ubuntu-release-upgrader 1795024 data/release-upgrades: Clarify documentation regarding the behavior for different Prompt settings. ubuntu-release-upgrader 1798618 do-release-upgrade: Utilize information regarding what Prompt is set to so that a more informative error message can be displayed. ubuntu-release-upgrader 1795024 do-release-upgrade: Utilize information regarding what Prompt is set to so that a more informative error message can be displayed. ubuntu-release-upgrader 1786484 DistUpgrade/DistUpgradeCache.py: When calculating free space needed for mount points don't use a negative number as the buffer. ubuntu-release-upgrader 1807043 DistUpgrade/DistUpgradeController.py: When rewriting sources.list entries check to see if the source provides packages for the release to which the upgrade is occurring. If it doesn't the entry is disabled thereby improving upgrades with PPAs. ubuntu-release-upgrader 1807032 do-release-upgrade: add a parameter to allow third party mirrors and repositories, additionally pass along the environmental variable RELEASE_UPGRADER_ALLOW_THIRD_PARTY via pkexec and sudo. ubuntu-release-upgrader 1771387 DistUpgrade/DistUpgradeCache.py: in the event there is a failure to calculate the upgrade provide information about the log files in /var/log/dist-upgrade. ubuntu-release-upgrader 1773637 DistUpgrade/xorg_fix_proprietary.py: modify how the system is checked to see if nvidia is being used, drop fglrx check since it has been deprecated. Desktop fixes These changes mainly affect desktop installations of Ubuntu, Kubuntu, Edubuntu and other Ubuntu-based systems. xorg 1768610 Rename nux config leftovers which might change the environment even when not running an unity session vte2.91 1780501 Revert the changes to revert-pcre2.patch in the previous SRU since they introduced API incompatibilies which aren't OK in an SRU. gnome-menus 1765799 Fix app menus not updating correctly after app install or removal. This patch was accidentally dropped in previous merge. gnome-shell-extension-ubuntu-dock 1712661 Fix crash connecting/disconnecting monitors gnome-shell-extension-ubuntu-dock 1784920 Fix showApps button label position gnome-control-center 1779051 Fix crash when settings changed after panel closed gnome-software 1778160 Fix crash when have plugs with multiple slots available gnome-software 1781996 don't segfault on null snap installation date gnome-shell 1718931 New upstream release gnome-shell 1782614 New upstream release gnome-shell 1784671 Don't handle key presses directly if there are modifiers mutter 1754949 Avoid crashing when warning about wrongly set crtc mode mutter 1767956 Don't crash if drmModeGetResources returns NULL mutter 1784398 Always update monitor in wayland, avoiding crash mutter 1772831 Don't return screen resolutions that can't be applied mutter 1422253 Don't crash if a modal dialog closes while being dragged mutter 1723615 Don't try to use an invalid monitor mode to figure out scaling mutter 1783311 No-change backport to bionic gnome-software 1775226 also disable offline updates in refresh plugin ubuntu-report 1786432 Include optional DCD OEM file ubuntu-report 1784383 Collect number of disks and their sizes apport 1778497 Add a remember option to whoopsie so that users can diminish crash interactions apport 1778694 Move apport autoreport service files to apport binary package. Having them in apport-noui was creating a bug where autoreport wasn't working on desktop. As we check in the launched script for whoopsie and autoreport file, we don't autoreport by default. apport-noui still touches the file to enable autoreport on install. apport 1778694 Start apport-autoreport after installing apport-noui which is part of improving apport's automatic crash reporing feature. software-properties 1770686 SoftwarePropertiesGtk.py: Hide livepatch widgets in flavors without an online account panel in gnome-control-center glib2.0 1789472 New upstream release ubuntu-settings 1782190 revert in communitheme session default font size to 11, as it creates rendering issues in GNOME Shell firefox 1791789 Fix : Mark distribution search engines as read-only, so that they are marked as hidden in rather than removed from the search engine cache when a user \"removes\" them (they can't actually be removed from disk). This stops them from reappearing on cache rebuilds firefox 1791789 Backport upstream change to the search service to not handle locale changes on shutdown. As well as resulting in en-US search engines being added to the search engine cache for all locales, it was resulting in a cache rebuild on every restart, making the above bug worse (fixes another part of ) firefox 1791789 Set \"spellchecker.dictionary_path\" by default to point to /usr/share/hunspell so that system dictionaries are loaded again, now that Firefox no longer loads them from its own install directory. Fixes another part of firefox 1791789 Final part of the fix for : Cleanup extra Amazon.com search engine in locales that have their own Amazon search engine fonts-liberation 1769654 Backport to bionic to fix skewed font metrics fonts-liberation2 1769654 Backport to bionic to fix skewed font metrics libreoffice 1785679 New upstream release libreoffice-l10n 1785679 New upstream release evolution-data-server 1784514 New upstream release apport 1791324 Handle old reports generated pre-apport with \"remember\" option. If the option isn't there, consider as false. nautilus 1782681 New upstream release: 3.26.4 nautilus 1765776 New upstream release: 3.26.4 nautilus 1767027 Follow nautilus settings to search only in current folder. This doesn't apply to gnome-shell search provider for indexed searches. nautilus 1764779 don't crash if selecting a volume that is not mounted nautilus 1713581 don't crash when try to select a file multiple times in a single run nvidia-graphics-drivers-340 1791542 Use diversions for \"libGLESv1_CM.so\" \"libGLESv1_CM.so.1\". nvidia-graphics-drivers-340 1761593 Install the modprobe file in /lib, and remove the blacklist file manually. gnome-mines 1772191 Backport fix from 3.30 for missing high resolution app icon gnome-software 1789336 Show verified developers gnome-software 1789338 Use wide scope searching gnome-software 1756379 Delay startup of service to allow the shell to load first ubuntu-themes 1773045 Remove nonexistent gnome-builder.css ubuntu-themes 1781736 Radiance: fix typo in assets link for focused buttons ubuntu-themes 1781736 Radiance: Use scaled image for buttons border ubuntu-themes 1782038 Ambiance, Radiance: don't use padding on window buttons for chromium ubuntu-themes 1758841 Ambiance: use default foreground color for toolbar menus ubuntu-themes 1743373 Ambiance, Radiance: show proper arrow in combobox ubuntu-themes 1785699 Ambiance, Radiance: use default disabled color for actions headerbar buttons ubuntu-themes 1761684 Ambiance, Radiance: fix list selected color for gnome-boxes ubuntu-themes 1795895 Ambiance, Radiance: properly theme disabled and hovered accelerators xorg-server 1789913 prime-sync-refactor.diff: Fix crash on modesetting+amdgpu hybrid. nautilus 1795028 Fix remote filesystem check on file during search nautilus 1798426 Refreshed to add memory leak and potential crash fixes gnome-settings-daemon 1797322 backport fix from upstream to resolve suspend/resume rfk issues xdg-utils 1743216 Use perl's decode() to ensure we don't pass invalid UTF-8 to D-Bus, as doing so triggers an assertion from libdbus which makes us crash. fonts-noto-color-emoji 1788256 New upstream release evince 1790609 New upstream release apport 1760220 apport/ui.py: when using ubuntu-bug properly handle executables which start with /snap/bin. apport 1780767 test/test_ui_gtk.py: Increase the timeout so that when the autopkgtest infrastructure is busy the tests should not fail. gnome-shell 1739931 use defined color for menu separators gnome-shell 1743058 set StEntry minimun height to work properly with Ubuntu font gnome-shell 1745888 Don't emit two click events on touch under X11 gnome-shell 1725312 Handle NULL scroll bars in st-scroll-view gnome-calculator 1790876 New upstream stable release mutter 1727356 Create back buffers in early intel GPU generations mutter 1730211 Fix typing capital letters when using OSD keyboard gnome-initial-setup 1789925 rework the patch from the previous upload to change the right reference this time gnome-initial-setup 1764723 Show an error message in case livepatch setup fails. This also will make sure that gnome-intial-setup does not quit before livepatch responds back. totem 1433984 backport fix for nautilus sometime crashing when closing the video properties dialog totem 1798399 backport fixes to make the gallery plugin work again desktop-file-utils 1768271 backport debian change to fix update-desktop-error with gnome-font-viewer 3.28 gnome-desktop3 1795668 Enable bubblewrap hardening for thumnbailers ghostscript 1806517 SECURITY REGRESSION: multiple regressions libgnomekbd 1721893 backport a patch available on bugzilla to fix a segfault in the keyboard layout preview code. The most common case seems to be g-c-c trying to preview a 'default' layout which was working before some other fixes landed, that fixes the regression. gnome-software 1552792 Build with PackageKit autoremove support gnome-software 1785240 Stop cancelling snapd authorization triggers error notification gnome-software 1754864 Pull related flatpak refs gnome-software 1798228 Fix snap search result ordering gnome-software 1719797 Stop reboot notification from timing out gnome-software 1798470 Support composite CAB files gnome-desktop3 1807127 Fix thumbnailer on 32-bit systems where /lib64 is not available. This fixes a regression introduced in the previous update. gedit 1800179 'document selector: make search caseless', it makes the filter in the open document dialog work in a more logical way gnome-shell 1765304 Cherry-pick upstream commit to prevent focus stealing on password fields in firefox when ibus is used libgweather 1803487 New upstream release: 3.28.2 nautilus 1804685 'preferences-window: Fix icon views captions order' software-properties 1807373 SoftwarePropertiesGtk.py: when checking a package's depends for DKMS also pass on an AttributeError gvfs 1803158 Use O_RDWR to fix fstat when writing gvfs 1798725 common: Prevent crashes on invalid autorun file gvfs 1630905 daemon: Prevent deadlock and invalid read when closing channels gvfs 1792878 workaround libsoup limitation to prevent dav lockups gvfs 1778322 smbbrowse: Force NT1 protocol version for workgroup support gvfs 1803190 smb: Add workaround to fix removal of non-empty dir gjs 1809181 New upstream release gjs 1803271 New upstream release gparted 1779292 d/patches/lvm2_prompt_patch.patch: Apply upstream patch to fix pvresize compatibility with LVM2 >= 2.02.171, which broke LVM PV resizing due to the addition of a confirmation dialogue for the resize. cups 1804576 fix-handling-of-MaxJobTime.patch: Fix handling of MaxJobTime 0 totem 1726688 Together, these patches fix the right-click .desktop actions gnome-shell-extension-ubuntu-dock 1743976 Avoid repainting an unchanging dock. gnome-shell-extension-ubuntu-dock 1769383 theming: Ensure _trackingWindows contains valid windows gnome-shell-extension-ubuntu-dock 1769383 extension: Ensure signal disconnection firefox 1808980 Build with --enable-rust-simd (except on i386 and armhf) libdrm 1798597 Backport to bionic for 18.04.2 HWE stack update. wayland 1798597 Backport for 18.04.2 HWE stack update. wayland 1781440 control: Bump Breaks/Replaces again to match reality. mesa 1798597 Backport for 18.04.2 HWE stack update. xf86-input-wacom-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xorg-server-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-input-libinput-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-amdgpu-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-ati-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-vesa-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-vmware-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-nouveau-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-fbdev-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-qxl-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xserver-xorg-video-dummy-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. xorg-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. cairo 1560286 \"Revert \"Correctly decode Adobe CMYK JPEGs in PDF export\" From further testing and investigation it appears that many PDF viewers already have a workaround to invert Adobe CMYK JPEGs, so our generated PDFs display incorrectly with those viewers due to double-inversion. xserver-xorg-video-intel-hwe-18.04 1798597 Backport for 18.04.2 HWE stack update. These changes mainly affect installations of Ubuntu on server systems and clouds. keepalived 1744062 d/p/fix-removing-left-over-addresses-if-keepalived-abort.patch: Cherry-picked from upstream to ensure left-over VIPs and eVIPs are properly removed on restart if keepalived terminates abonormally. This fix is from the upstream 1.4.0 release. cloud-init 1784685 cherry-pick 3cee0bf8: oracle: fix detect_openstack to report True on cloud-init 1777912 New upstream snapshot. cloud-init 1770712 debian/rules: update version.version_string to contain packaged version. cloud-init 1777912 New upstream release. aodh 1776375 Support same projects in different domain maas 1773201 Stable Release Update. New upstream release, MAAS 2.4.0 freeipmi 1784926 Cherry-pick patches from upstream that add support to ipmi-locate for parsing ACPI/SPMI tables out of sysfs: rax-nova-agent 1783614 New upstream version 2.1.15 libvirt 1788226 Fix an issue where guests with plenty of hostdevs attached where detected as not shut down due to the kernel needing more time to free up resources apache2 1750356 d/p/balance-member-long-hostname-part{1,2}.patch: Provide an RFC1035 compliant version of the hostname in the proxy_worker_shared structure. A hostname that is too long is no longer a fatal error. chrony 1787366 d/p/lp-1787366-fall-back-to-urandom.patch: avoid hangs when starting the service on newer kernels by falling back to urandom. avahi 1752411 debian/avahi-daemon-check-dns.sh: On some hardware, the 'host' command gets stuck and does not timeout as it should leaving this script and boot-up hanging indefinitely. Launch host with 'timeout' to kill it after 5 seconds in these cases as a workaround. unbound 1788622 d/p/lp-1788622-fix-systemd-reload.patch: Fix hang due to all worker threads stopping on reload maas 1788641 Stable Release Update. New upstream release, MAAS 2.4.2 postfix 1753470 debian/patches/fix-postconf-segfault.diff: Fix a postconf segfault when map file cannot be read. Thanks to Viktor Dukhovni <postfix- users@dukhovni.org>. qemu 1755912 d/p/lp-1755912-qxl-fix-local-renderer-crash.patch: Fix an issue triggered by migrations with UI frontends or frequent guest resolution changes qemu 1787408 d/p/ubuntu/target-ppc-extend-eieio-for-POWER9.patch: Backport to extend eieio for POWER9 emulation. rax-nova-agent 1788716 Use DefaultDependencies=no to prevent forming service ordering cycles openjdk-lts 1788250 debian/rules: by default leave atk disabled, move accessibility bridge to recommends. openjdk-lts 1788267 debian/rules: by default leave atk disabled, move accessibility bridge to recommends. openvpn 1787208 d/openvpn@.service: Add CAP_AUDIT_WRITE to avoid issues with callout scripts breaking due to sudo/pam being unable to audit the action. Fixed in upstream issue #918, suggested to Debian in #868806 libvirt 1788603 d/p/ubuntu-aa/lp-1788603-fix-ptrace-rules-with-kernel-4.18.patch: avoid issues with newer kernels >=4.18 libvirt 1789659 d/p/ubuntu/lp-1789659-don-t-check-for-parallel-iteration-in-hash.patch: remove broken and redundant check for parallel iteration in hash functions nova-lxd 1789427 New upstream stable point release for OpenStack Queens. nginx 1782226 Stable Release Update. Do not attempt to start nginx if other daemon is binding to port 80, to prevent install failure: open-vm-tools 1791220 d/p/ubuntu/lp-1791220-Disable-hgfsServer-not-VMware.patch: avoid crashing with segfaults when force starting the service in non VMWare environments. open-vm-tools 1790145 d/p/debian/scsi-udev-rule: fix applying of the scsi timeout horizon 1790189 d/theme/ubuntu/_styles.scss: Ensure btn-danger rules are preferred when used with a, a:link, a:visited and dropdown-menu.li. nova 1761140 d/control: Drop circular dependencies. nova-compute depends on nova-compute-* packages. nova-compute-* packages shouldn't depend on nova-compute. nova-compute-* should however depend on nova-common.. cloud-initramfs-tools 1792905 copymods: Take ownership of lib/modules squid3 1792728 d/usr.sbin.squid: Update apparmor profile to grant read access to squid binary webkit2gtk 1795901 Install missing headers lxcfs 1788232 New upstream bugfix release: lxc 1788457 New upstream bugfix release: snapd-glib 1785240 Support auth cancelled error snapd-glib 1789338 Support wide scope searches snapd-glib 1789336 Support publisher information snapd-glib 1789335 Support refresh information networkd-dispatcher 1797884 Allow overriding /usr/lib scripts in /etc/networkd-dispatcher. Replaces our patch to use /usr/lib/networkd-dispatcher with the solution contributed upstream that has a search path with both /usr/lib/ and /etc locations. qemu 1790901 Update pxe netboot images for KVM s390x to qemu 3.0 level The SLOF source pieces in src:qemu are only used for s390x netboot, which are independent ROMs (no linking). All other binaries out of this are part of src:slof and independent. cloud-init 1795953 New upstream release. horizon 1778771 d/p/add-enabled-check-in-backups-panel.patch: Cherry-picked from https://review.openstack.org/#/c/605994/ to ensure Volume Backups panel is disabled if enable_backup is False. lxd 1788280 New upstream bugfix release: lxd 1788314 Temporarily disable ZFS tests on s390x due to conflict between zfsutils-linux and s390-tools netplan.io 1770082 Fix typo breaking rename on 'netplan apply'. netplan.io 1793309 Backport netplan 0.40.1 to 18.04. netplan.io 1795343 Deal gracefully with empty files on 'netplan apply' netplan.io 1786726 Don't render ipv4 dns-search unless we have an ipv4 address. netplan.io 1736965 Set permissive umask on networkd .network, .link and .netdev files netplan.io 1768560 Set permissive umask on networkd .network, .link and .netdev files netplan.io 1747455 Fix support for link-scope routes. netplan.io 1756701 Spell Gratuitous ARP correctly and make it work. netplan.io 1783940 Many typo fixes for documentation. netplan.io 1771704 Allow link-local addresses to be configured. netplan.io 1736975 Forces bridges with no addresses to be brought online. netplan.io 1770082 Write udev .rules files to /run/udev/rules.d to enforce interface renaming. netplan.io 1768823 Don't traceback for 'netplan ip leases' when iface is not managed or doesn't DHCP netplan.io 1771440 Fix duplicate \"/\" path separator in error messages netplan.io 1768798 Fix incorrect terminal reset in 'netplan try' on Ctrl-C. netplan.io 1768783 Updated doc entries: mtu, fix fwmark->mark, cleanup optional. netplan.io 1770082 Generate udev rules files to rename devices Due to a systemd issue[1], using link files to rename interfaces doesn't work as expected. Link files will not rename an interface if it was already renamed, and interfaces are renamed in initrd, so set-name will often not work as expected when rebooting. However, rules files will cause a renaming, even if the interface has been renamed in initrd. amavisd-new 1792293 d/p/100_more_amavisd_helpers_fixes: Fix Debian/Ubuntu pathing in amavisd-release amavisd-new 1770532 d/p/105_amavisd_fix_originating_dkim_signing.patch: Fix DKIM signing in 2.11.0 nova 1795424 New stable point release for OpenStack Queens. heat 1795424 New stable point release for OpenStack Queens. keystone 1795424 New stable point release for OpenStack Queens. python-openstackclient 1800490 New stable point release for OpenStack Queens. open-vm-tools 1793219 d/p/lp-1793219-fix-stats-overflow.patch: fix potential overflow of 32 bit /proc values bind9 1769440 d/p/skip-rtld-deepbind-for-dyndb.diff: fix named-pkcs11 crashing on startup. Thanks to Petr Menk <pemensik@redhat.com> neutron 1795424 New stable point release for OpenStack Queens. neutron 1790598 d/p/metadata-use-requests-for-comms-with-nova-api.patch: Cherry-picked from https://review.openstack.org/#/c/599541/ to enable cert management where IP addresses are used in subject alternate names. openldap 1783183 d/apparmor-profile: update apparmor profile to allow reading of files needed when slapd is behaving as a kerberos/gssapi client and acquiring its own ticket. samba 1795772 d/p/fix-rmdir.patch: fix the patch to not apply with offset, which previously made it change the wrong, almost identical, function. samba 1795772 d/p/fix-rmdir.patch: Fix to make smbclient report directory-not-empty errors ca-certificates-java 1770553 Backport from Cosmic. ca-certificates-java 1771815 Merge from Debian unstable. Remaining changes: ca-certificates-java 1771363 debian/postinst.in: Detect PKCS12 cacert keystore generated by previous ca-certificates-java and convert them to JKS. ca-certificates-java 1769013 Merge from debian unstable. Remaining changes: ca-certificates-java 1739631 Merge from debian unstable. Remaining changes: apache2 1782806 d/debhelper/apache2-maintscript-helper: fix typo in apache2_switch_mpm()'s a2query call. openjdk-lts 1800792 debian/patches/sec-webrev-11.0.1-b21-S8211731.patch: apply missing patch from the security update. bubblewrap 1795668 Don't install setuid on Ubuntu & derivatives since Ubuntu's kernel enables unprivileged user namespaces ubuntu-keyring 1798073 Validate that all shipped fragments are signed. netplan.io 1802322 Fix idempotency in renaming: bond members should be exempt from rename, as they may all share a single MAC for the bond device. netplan.io 1770082 Fix typo breaking rename on 'netplan apply'. netplan.io 1793309 Backport netplan 0.40.1 to 18.04. netplan.io 1795343 Deal gracefully with empty files on 'netplan apply' netplan.io 1786726 Don't render ipv4 dns-search unless we have an ipv4 address. netplan.io 1736965 Set permissive umask on networkd .network, .link and .netdev files netplan.io 1768560 Set permissive umask on networkd .network, .link and .netdev files netplan.io 1747455 Fix support for link-scope routes. netplan.io 1756701 Spell Gratuitous ARP correctly and make it work. netplan.io 1783940 Many typo fixes for documentation. netplan.io 1771704 Allow link-local addresses to be configured. netplan.io 1736975 Forces bridges with no addresses to be brought online. netplan.io 1770082 Write udev .rules files to /run/udev/rules.d to enforce interface renaming. netplan.io 1768823 Don't traceback for 'netplan ip leases' when iface is not managed or doesn't DHCP netplan.io 1771440 Fix duplicate \"/\" path separator in error messages netplan.io 1768798 Fix incorrect terminal reset in 'netplan try' on Ctrl-C. netplan.io 1768783 Updated doc entries: mtu, fix fwmark->mark, cleanup optional. rax-nova-agent 1804445 New upstream version 2.1.18 rax-nova-agent 1788716 Use DefaultDependencies=no to prevent forming service ordering cycles libvirt 1801666 d/control: explicitly Build-dep on libwiretap-dev to fix FTBFS since libwireshark 2.6.x SRU upload exim4 1786508 d/p/eximstats_unitialized_value.patch: Fix uninitialized value error in eximstats. postfix 1791139 d/postfix-{cdb,ldap,lmdb,mysql,pcre,pgsql}.postinst, d/postfix.postinst: Handle empty alias_database field in main.cf lxc 1804755 New upstream bugfix release: lxcfs 1804753 New upstream bugfix release: barbican 1806043 New stable point release for OpenStack Queens. neutron-fwaas 1806043 New stable point release for OpenStack Queens. keystone 1806043 New stable point release for OpenStack Queens. nova 1744079 d/p/disk-size-live-migration-overcommit.patch: Cherry-picked from https://review.openstack.org/#/c/602478 to ensure proper disk calculation during live migration with over-commit. nova 1806043 New stable point release for OpenStack Queens. horizon 1802226 d/openstack-dashboard.postinst: Ensure that if memcached is installed it is restarted in post-install script after collecting/compressing static assets, enabling refresh of memcached static assets after upgrade. psmisc 1806060 fix killall option parsing walinuxagent 1799498 New upstream release. ceph 1796645 New upstream point release. cinder 1806043 New stable point release for OpenStack Queens. netplan.io 1811210 No change rebuild in -security pocket to fix dependency issue samba 1801227 d/p/auth-fail-eexist.diff: smbc_opendir should not return EEXIST with invalid login credentials. Thanks to David Mulder. network-manager-applet 1807460 'libnma: unescape certificate paths in URIs', that fixes selecting files with a space in their name openvswitch 1780752 New upstream point release: sssd 1807246 d/p/fix-id-out-of-range-lookup.patch: CACHE_REQ: Do not fail the domain locator plugin if ID outside the domain range is looked up. Thanks to Jakub Hrozek <jhrozek@redhat.com>. sssd 1793882 d/t/common-tests, d/t/control, d/t/ldap-user-group-krb5-auth, d/t/ldap-user-group-ldap-auth, d/t/login.exp, d/t/util: add DEP8 tests for kerberos and LDAP deja-dup 1804744 Fix bug preventing restore on a fresh install if you don't also set your current backup location to the old backup. snapd 1811233 New upstream release, snapd 1779403 New upstream release, snapd 1814355 disable systemd environment generator on bionic to fix ceph 1813582 d/p/lp1813582.patch: Cherry pick fix for crash in rados py binding under gnocchi (LP: #1813582). libmemcached 1573594 Fix missing null termination in PROTOCOL_BINARY_CMD_SASL_LIST_MECHS response handling (LP: #1573594) haproxy 1804069 d/p/stksess-align.patch: Make sure stksess is properly aligned. (LP: #1804069) landscape-client 1788219 debian/patches/nutanix-kvm.patch: Update vm_info.py to include Nutanix hypervisor. (LP: #1788219) landscape-client 1670291 debian/patches/release-upgrade-success.patch: Enable landscape-client to survive trusty upgrade. (LP: #1670291) landscape-client 1670291 debian/patches/post-upgrade-reboot.patch: Force reboot operation in case systemd fails. (LP: #1670291) landscape-client 1765518 debian/patches/unicode-tags-script.patch: Permit environments containing unicode chars for script execution. (LP: #1765518) landscape-client 1616116 debian/patches/1616116-resync-loop.patch: Clear hash id database on package resync. (LP: #1616116) Kernel and Hardware support updates Considerable work has been done in Ubuntu 18.04.2 on improving support for many specific items of hardware. intel-microcode 1778738 Default to early instead of auto, and install all of the microcode, not just the one matching the current CPU, if MODULES=most is set in the initramfs-tools config amd64-microcode 1778738 Default to 'early' instead of 'auto' in the initramfs-tools hook when building with MODULES=most lshw 1752523 d/patches/lshw-fix-unknown-version-issue.patch: Cherry pick from upstream. open-iscsi 1785108 d/net-interface-handler: Apply changes only for the iscsi-root grub2 1786491 Verify that the current and newer kernels are signed when grub is updated, to make sure people do not accidentally shutdown without a signed kernel. grub2-signed 1786491 Rebuild against grub2 2.02-2ubuntu8.3 and check kernel is signed on amd64 EFI before installing grub. linux-meta-gcp 1780923 linux-gcp: add a meta package for the extra modules linux-oem 1781895 Bluetooth: Redpine: Bionics: L2test transfer is failed to start in Ubuntu 18.04 linux-oem 1782070 Redpine] Upgrades to improve throughput and stability grub2-signed 1785859 Rebuild against grub2 2.02-2ubuntu8.4 grub2 1785859 debian/patches/ofnet-init-structs-in-bootpath-parser.patch: initialize structs in bootpath parser. Fixes netboot issues on ppc64el. gnu-efi 1790709 New upstream version 3.0.8. shim-signed 1790724 Backport shim-signed 1.37 to Ubuntu 18.04. shim-signed 1778848 debian/shim-signed.postinst: use --auto-nvram with grub-install in case we're installing on a NVRAM-unavailable platform. shim-signed 1778848 debian/control: bump the dependency for grub2-common to make sure grub-install supports --auto-nvram. shim-signed 1778848 debian/control: switch the grub-efi-amd64-bin dependency to grub-efi-amd64-signed. ipxe 1789319 Build ROMs for QEMU with CONFIG=qemu libglvnd 1782285 rules, libgles2: Add GLESv1 support. libglvnd 1780039 Always return an error from eglMakeCurrent if EGLDisplay is invalid. bolt 1786265 New upstream version bolt 1778020 should resolve issues with devices not being authorized on initial boot e.g in lightdm open-iscsi 1755858 make iscsid socket activated to only activate it as-needed ubuntu-drivers-common 1789201 Start before oem-config.service. open-iscsi 1791108 d/net-interface-handler: replace 'domainsearch' with the correct configuration option 'search' in net-interface-handler shim-signed 1792575 debian/control: add Breaks: grub-efi-amd64-signed (<< 1.93.7), as the new version of shim exercises a bug in relocation code for chainload that was fixed in that upload of grub, affecting Windows 7, Windows 10, and some netboot scenarios where chainloading is required. shim-signed 1790724 Backport shim-signed 1.37 to Ubuntu 18.04. grub2-signed 1792575 Rebuild against grub2 2.02-2ubuntu8.6 grub2-signed 788298 Rebuild against grub2 2.02-2ubuntu8.5 grub2 1792575 debian/patches/linuxefi_fix_relocate_coff.patch: fix typo in relocate_coff() causing issues with relocation of code in chainload. grub2 1792575 debian/patches/linuxefi_truncate_overlong_reloc_section.patch: The Windows 7 bootloader has inconsistent headers; truncate to the smaller, correct size to fix chainloading Windows 7. grub2 788298 debian/patches/grub-reboot-warn.patch: Warn when \"for the next boot only\" promise cannot be kept. util-linux 1783810 Use getrandom() with GRND_NONBLOCK to avoid hangs in early boot when e.g. the partition is resized. Cherry picked from upstream. skiboot 1785026 d/opal-prd.logrotate: fix ownership of /var/log/opal-prd.log. friendly-recovery 1766872 Cleanup lintian warnings. linux 1796542 Silent data corruption in Linux kernel 4.15 linux 1789746 getxattr: always handle namespaced attributes linux 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux 1791569 some nvidia p1000 graphic cards hang during the boot linux 1783746 ipmmu is always registered linux 1794889 Bionic update: upstream stable patchset 2018-09-27 linux-kvm 1796542 Silent data corruption in Linux kernel 4.15 linux-kvm 1793841 IP_SET modules not included in kernel build, prevents container functionality linux-kvm 1789746 getxattr: always handle namespaced attributes linux-kvm 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux-kvm 1791569 some nvidia p1000 graphic cards hang during the boot linux-kvm 1783746 ipmmu is always registered linux-kvm 1794889 Bionic update: upstream stable patchset 2018-09-27 gdm3 1780076 Add utils-add-new-gdm-disable-wayland-binary.patch, cherry-picked from cosmic. linux-aws 1796542 Silent data corruption in Linux kernel 4.15 linux-aws 1794175 kyber-iosched module missing from linux-modules package linux-aws 1789746 getxattr: always handle namespaced attributes linux-aws 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux-aws 1791569 some nvidia p1000 graphic cards hang during the boot linux-aws 1783746 ipmmu is always registered linux-aws 1794889 Bionic update: upstream stable patchset 2018-09-27 linux-gcp 1796542 Silent data corruption in Linux kernel 4.15 linux-gcp 1789746 getxattr: always handle namespaced attributes linux-gcp 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux-gcp 1791569 some nvidia p1000 graphic cards hang during the boot linux-gcp 1783746 ipmmu is always registered linux-gcp 1794889 Bionic update: upstream stable patchset 2018-09-27 linux-oem 1796542 Silent data corruption in Linux kernel 4.15 grub2 1785033 debian/patches/0001-i386-linux-Add-support-for-ext_lfb_base.patch: Add support for ext_lfb_base. grub2-signed 1785033 Rebuild against grub2 2.02-2ubuntu8.7 linux-azure 1798185 Enable CONFIG_INFINIBAND_USER_MAD linux-azure 1798185 Enable CONFIG_INFINIBAND_USER_MAD linux-azure 1796542 Silent data corruption in Linux kernel 4.15 linux-azure 1789746 getxattr: always handle namespaced attributes linux-azure 1789118 Fails to boot under Xen PV: BUG: unable to handle kernel paging request at edc21fd9 linux-azure 1791569 some nvidia p1000 graphic cards hang during the boot linux-azure 1783746 ipmmu is always registered linux-azure 1794889 Bionic update: upstream stable patchset 2018-09-27 s390-tools 1777600 zdev: Adjust zdev modprobe path to be compatible with split-usr systems. s390-tools 1794308 zdev: Trigger generic_ccw devices on any kernel module loads. secureboot-db 1776996 Backport secureboot-db from cosmic to apply the August 2016 dbx updates from Microsoft. mokutil 1797011 Backport mokutil 0.3.0+1538710437.fb6250f-0ubuntu2 to 18.04. parted 1798675 debian/patches/Read-NVMe-model-names-from-sysfs.patch: Expose NVMe model names when available instead of the generic \"NVMe Device\" string. kmod 1786574 Remove i2c_i801 from d/modprobe.d/blacklist.conf. ubuntu-drivers-common 1797147 Improve pid detection, and restore the default pci power control profile in performance mode. nvidia-prime 1797147 Give udev the time to add the drm device. Fixes a race condition that causes problems when using lightdm and sddm. shim-signed 1726803 Don't fail non-interactive upgrade of nvidia module and module removals linux-oem 1800770 Thunderbolt runtime D3 and PCIe D3 Cold support linux-azure 1722226 linux-azure: fix systemd ADT test failure grub2 1784363 debian/default/grub.md5sum: add entry for 2.02-2ubuntu8.7; to force an update of /etc/default/grub back to the correct timeout value of 0 if the file has otherwise not been edited by the user. grub2 1788727 debian/grub-check-signatures: Handle the case where we have unsigned vmlinuz and signed vmlinuz.efi.signed. grub2-signed 1784363 Rebuild against grub2 2.02-2ubuntu8.9 grub2-signed 1788727 Rebuild against grub2 2.02-2ubuntu8.9 bolt 1798014 New upstream version bolt 1800715 use -Dprivileged-group=sudo, the admin group is not 'wheel' for us linux-aws 1801305 Restore request-based mode to xen-blkfront for AWS kernels fwupd 1791999 1768627 1719797 Restrict libsmbios-dev to x86 architectures linux-gcp-edge 1796647 Shared folders cannot be mounted in ubuntu/cosmic64 due to missing vbox modules open-iscsi 1802354 d/iscsi-disk.rules, d/rules: Add a udev rule so that iscsid.service will be started when iscsi disks are attached. usbmuxd 1778767 backport some fixes for missing udev events on new kernel which were leading to the service not restarting after disconnecting and reconnecting a device. Thanks Leo Soares for suggesting a first version of the backport hwdata 1755490 Change PNP vendor name for GSM to LG Electronics open-iscsi 1807978 debian/iscsi-disk.rules: Fix bug with LVM on top of iscsi devices. open-iscsi 1806777 debian/extra/initramfs.local-top: handle iSCSI iBFT DHCP to correctly run ipconfig to gather all DHCP config info, including DNS search domain, which iBFT can't provide. lxd 1804876 New upstream bugfix release e2fsprogs 1807288 debian/patches/0001-libext2fs-fix-regression-so-we-are-correctly- transla.patch: cherry-pick upstream fix so we are correctly translating acls in mkfs.ext4. Closes initramfs-tools 1802591 scripts/functions: include a new option to skip enslaved network devices. Include the new variable NETWORK_SKIP_ENSLAVED. When set to a value different than \"0\", this variable will cause any enslaved network devices to be skipped from the list of netbootable devices. This variable can be set via the configuration files under /etc/initramfs-tools/ or via any configuration file under the initrd directory /conf/conf.d/ via a hook script. rdma-core 1794825 Drop to avoid issues with the sysV to systemd wrapper starting the service instead of the socket linux-oracle 1802591 Skip enslaved devices during boot linux-firmware 1808528 Update linux-firmware in bionic for 18.10 hwe kernel util-linux 1784347 debian/patches/support_alternate_partition_sep.patch: support alternate partition separators. Common cases of no separator, using \"p\" only, and \"-part\" (the only previously supported), are covered now, which should let fdisk show partitions in a way closer to reality on most systems. Patch from Kyle Mahlkuch, with changes by Karel Zak. irqbalance 1811655 Added aarch64-log-spam preventing syslog spam on small aarch64 systems which lack a PCI bus (e.g. rpi3b, rpi3b+). flash-kernel 1764491 Add Raspberry Pi 3 Model B+ to the db. flash-kernel 1811216 Modify the Pi 3 boot.scr addresses to fit a bigger kernel, prepare separate versions for armhf and arm64. linux-meta-hwe 1798352 linux-snapdragon: missing meta packages for this flavour livecd-rootfs 1805668 More changes for raspi3 build support: livecd-rootfs 1805668 Another batch of cherry-picks for raspi3 support u-boot 1805668 Backport to bionic. grub2 1789918 debian/grub-check-signatures: check kernel signatures against keys known in firmware, in case a kernel is signed but not using a key that will pass validation, such as when using kernels coming from a PPA. grub2 1812863 debian/patches/mkconfig_leave_breadcrumbs.patch: make sure grub-mkconfig leaves a trace of what files were sourced to help generate the config we're building. grub2 1800722 debian/patches/quick-boot-lvm.patch: If we don't have writable grubenv and we're on EFI, always show the menu. Closes linux 1813663 External monitors does not work anymore 4.15.0-44 thermald 1803360 Honor ACPI _CRT for processor thermal zone There are some new fanless platforms use DPTF's virtual sensor instead of INT340X devices. Because of that, the _PSV is no longer in use, at least not directly, hence its value may set higher then _CRT. To a fanless system that means no cooling device gets activated before _CRT, so the system will be considered overheated by Linux kernel, and gets shutdown by the kernel. Upstream fixes: kmod 1802689 Add i2c_i801 back to d/modprobe.d/blacklist.conf again due to regressions. e2fsprogs 1798562 d/patches/0001-resize2fs-update-checksums-in-the-extent-tree-s-relo.patch: do the checksum update later in extent tree relocated block to denote the inode number change, otherwise the checksum update might be done in the old copy of the block. linux-oem 1813663 External monitors does not work anymore 4.15.0-44 linux-oem 1811777 Fix non-working pinctrl-intel linux-oem 1811929 Fix not working Goodix touchpad linux-hwe 1814555 Ubuntu boot failure. 4.18.0-14 boot stalls. (does not boot) linux-hwe 1813873 Userspace break as a result of missing patch backport grub2 1814575 debian/grub-check-signatures: properly account for DB showing as empty on some broken firmwares: Guard against mokutil --export --db failing, and do a better job at finding the DER certs for conversion to PEM format. grub2 1401532 debian/patches/linuxefi_disable_sb_fallback.patch: Disallow unsigned kernels if UEFI Secure Boot is enabled. If UEFI Secure Boot is enabled and kernel signature verification fails, do not boot the kernel. Patch from Linn Crosetto. grub2 1814403 debian/patches/quick-boot-lvm.patch: checking the return value of 'lsefi' when the command doesn't exist does not do what's expected, so instead check the value of $grub_platform which is simpler anyway. grub2-signed 1401532 Rebuild against grub2 2.02-2ubuntu8.11. Unsorted changes python-wadllib 1729754 Fix MIME encoding of binary parts. guile-2.0 1780996 Convert triggers to noawait (Closes: #903915) dpdk 1784816 Make DPDK LTS release available in Bionic open-vm-tools 1784638 Merge with Upstream Tag stable-10.3.0 from https://github.com/vmware/open-vm-tools/releases/tag/stable-10.3.0 Remaining changes: gpgme1.0 1762384 Make debian/libgpgme-dev.links executable, it uses dh-exec packagekit 1722185 Fix debconf interaction base-files 1134036 Install locale-check command to /usr/bin and invoke it from /etc/profile.d/01-locale-fix.sh to ensure locale related environment variables are set to valid values. cryptsetup 1651818 Apply patch from Trent Nelson to fix cryptroot-unlock for busybox compatibility. clamav 1792051 debian/clamav-daemon.config.in: fix infinite loop during dpkg-reconfigure python3.6 1792143 SRU: Update Python 3.6 to the recent subminor release. python3.6 1768644 Don't inject dpkg's compiler specs into distutils. python-pylxd 1775238 d/control: Add python(3)-requests-toolbelt to python(3)-pylxd Depends and python3-requests-unixsocket to python3-pylxd Depends. Also set min version of toolbelt globally. appstream 1792537 cache-explicit-variants.patch: fix crash when upgrading from bionic to cosmic gcc-defaults 1769657 SRU: Bump GCC 8 based versions to 8.2.0. gcc-8-cross 1769657 SRU: Build using gcc 8.2.0-1ubuntu2 gcc-8 1769657 SRU: Update the package to the current version in cosmic. gcc-7 1769657 SRU: Update the package to the current version in cosmic. gcc-7 1721355 Update the gcc-foffload-default patch. cross-toolchain-base 1769657 SRU: gcc-7-cross 1769657 SRU: Build using 7.3.0-18ubuntu0.1. binutils 1769657 SRU: binutils 1763098 Fix PR gprof/23056, memory corruption in gprof. binutils 1763096 Fix PR binutils/23054, memory corruption in as. binutils 1763094 Fix PR ld/23055, memory corruption in ld. plymouth 1767918 debian/patches/git_ensure_tty_closed_0a662723.patch: ensure tty is closed on deactivate. apparmor 1788929 disallow writes to thumbnailer dir apparmor 1794848 disallow access to the dirs of private files apturl 1338482 really* work golang-1.10 1794395 Backport to 18.04. man-db 1785414 Backport seccomp sandbox improvements from 2.8.4: packagekit 1790613 Pass --no-restart-after-upgrade to dh_installsystemd to avoid PackageKit restarting while upgrading under PackageKit packagekit 1795614 debian/patches/frontend-locking.diff: Implement frontend locking in a simple way. Will need some more work to upstream, and possibly some error checking. packagekit 1790671 debian/patches/aptcc-Fix-invalid-version-dereference-in-AptInf-prov.patch, aptcc-removing-duplicate-delete-call.patch: Fix invalid dereference and delete wrong (duplicate) \"delete\" statement in providesCodec apt 1796808 Set DPKG_FRONTEND_LOCKED when running {pre,post}-invoke scripts. Some post-invoke scripts install packages, which fails because the environment variable is not set. This sets the variable for all three kinds of scripts {pre,post-}invoke and pre-install-pkgs, but we will only allow post-invoke at a later time. apt 1787120 Support records larger than 32kb in 'apt show' (Closes: #905527) apt 1781169 Add support for dpkg frontend lock (Closes: #869546) apt 1794957 http: Stop pipeline after close only if it was not filled before apt 1794053 pkgCacheFile: Only unlock in destructor if locked before python-apt 1795407 Frontend locking and related locking improvements dpkg 1796081 Apply patch from upstream to add frontend locking: distro-info-data 1800656 Add Ubuntu 19.04 Disco Dingo. valgrind 1781128 Apply post 3.13 PPC64 related patches. clamav 1783632 SECURITY REGRESSION: clamav-daemon fails to start due to options removed in new version and manually edited configuration file. packagekit 1552792 Correct autoremove behaviour to only autoremove packages that relate to the current transaction sosreport 1803735 d/p/dont-collect-some-tracing-instance-files.patch: sosreport 1775195 New 3.6 upstream release. major enhancements to core features and existing plugins: dh-golang 1794936 d/patches/0001-Fix-index-out-of-range-when-using-gccgo.-Closes-9072.patch: backport fix for building with gccgo from debian. python-wsme 1805125 d/control: Correct dependency on python3-netaddr for python3-wsme, avoiding needless install of Python 2. python-memcache 1802487 d/p/py2-perf.patch: Cherry pick fix to use native C implementation for pickle under Python 2, resolving issues with performance degradation. tmux 1766942 d/p/tmux-pane-border-status-leak.patch: Fixed memory leak in screen_redraw_make_pane_status (upstream fix). livecd-rootfs 1805497 Include snaps in image manifests debootstrap 1773496 For (Ubuntu) releases disco+ default to MERGED_USR=yes, -k extract option. tar 1809827 backport \"Fix the --add-file option.\" upstream commit, thanks Martin Vogt llvm-toolchain-7 1798597 Backport to bionic for 18.04.2 HWE stack update. systemd 1811471 d/p/resolve-enable-EDNS0-towards-the-127.0.0.53-stub-res.patch getaddrinfo() failures when fallback to dns tcp queries, so enable edns0 in resolv.conf systemd 1804487 d/p/resolved-Increase-size-of-TCP-stub-replies.patch dns failures with edns0 disabled and truncated response BionicBeaver/ReleaseNotes/ChangeSummary/18.04.2 (last edited 2019-02-14 15:43:41 by sil2100) ",
        "I for one would very much like to see this bug in 18.04 fixed as a priority:<p><a href=\"https://bugs.launchpad.net/ubuntu/+source/openjdk-lts/+bug/1796027\" rel=\"nofollow\">https://bugs.launchpad.net/ubuntu/+source/openjdk-lts/+bug/1...</a><p>The Java 11 package installs Java 10. This was supposed to be a short term hack (because Ubuntu's long term supported version 18.04 was being released shortly before Java's long term supported version 11) but it's been a good while now - six months or so.<p>The short term support version 18.10 of Ubuntu has Java 11 so it's very non-obvious what the blocker is.<p>To me this seems like a really poor choice. The result of installing the Java 11 package but getting Java 10 <i>clearly</i> fails the principle of least astonishment. If we can live without the fix a quarter of the way to the next LTS edition of Ubuntu then we could have lived with Java 10 as the preferred (and correctly named) package in the first place.<p>Meanwhile the bug asks us not to spam with requests for updates yet there's no suggestion of where we can go to gauge what the timescales we're up against are.<p>It definitely dents my confidence in Ubuntu as a well organised distribution.",
        "This is not an official announcement and it has not been officially declared released or ISOs updated.<p>The last official comment was it was delayed till Thursday, Feb. 14:<p><a href=\"https://lists.ubuntu.com/archives/ubuntu-release/2019-February/004694.html\" rel=\"nofollow\">https://lists.ubuntu.com/archives/ubuntu-release/2019-Februa...</a><p>Those preparing the release will update release notes and fixed bug lists (like this one) before the release."
      ],
      "relevant": "false"
    },
    {
      "id": 19470064,
      "title": "Qri: A global dataset version control system built on the distributed web",
      "search": [
        "Qri: A global dataset version control system built on the distributed web",
        "https://github.com/qri-io/qri",
        "Qri CLI a dataset version control system built on the distributed web Website | Packages | Contribute | Issues | Docs | Download Welcome Question Answer \"I want to learn about Qri\" Read the official documentation \"I want to download Qri\" Download Qri or brew install qri-io/qri/qri \"I have a question\" Create an issue and use the label 'question' \"I found a bug\" Create an issue and use the label 'bug' \"I want to help build the Qri backend\" Read the Contributing guides \"I want to build Qri from source\" Build Qri from source qri is a global dataset version control system built on the distributed web Breaking that down: global so that if anyone, anywhere has published work with the same or similar datasets, you can discover it. Specific to datasets because data deserves purpose-built tools version control to keep data in sync, attributing all changes to authors On the distributed web to make all of the data published on qri simultaneously available, letting peers work on data together. If youre unfamiliar with version control, particularly the distributed kind, well you're probably viewing this document on github which is a version control system intended for code. Its underlying technology git popularized some magic sauce that has inspired a generation of programmers and popularized concepts at the heart of the distributed web. Qri is applying that family of concepts to four common data problems: Discovery Can I find data Im looking for? Trust Can I trust what Ive found? Friction Can I make this work with my other stuff? Sync How do I handle changes in data? Because qri is global and content-addressed, adding data to qri also checks the entire network to see if someone has added it before. Since qri is focused solely on datasets, it can provide meaningful search results. Every change on qri is associated with a peer, creating an audit-able trail you can use to quickly see what has changed and who has changed it. All datasets on qri are automatically described at the time of ingest using a flexible schema that makes data naturally inter-operate. Qri comes with tools to turn all datasets on the network into a JSON API with a single command. Finally, all changes in qri are tracked & synced. Building From Source To build qri you'll need the go programming language on your machine. $ git clone https://github.com/qri-io/qri $ cd qri $ make install If this is your first time building, this command will have a lot of output. That's good! Its means it's working :) It'll take a minute or two to build. After this is done, there will be a new binary qri in your ~/go/bin directory if using go modules, and $GOPATH/bin directory otherwise. You should be able to run: and see help output. Building on Windows To start, make sure that you have enabled Developer Mode. A library that we depend on needs it enabled in order to properly handle symlinks. If not done, you'll likely get the error message \"A required privilege is not held by the client\". You should not need to Run As Administrator to build or run qri. We do not recommend using administrator to run qri. Shell For your shell, we recommend using msys2. Other shells, such as cmd, Powershell, or cygwin may also be usable, but msys2 makes it easy to install our required dependencies. IPFS also recommends msys2, and qri is built on top of IPFS. Dependencies Building depends upon having git and make installed. If using msys2, you can easily install these by using the package manager \"pacman\". In a shell, type: Assuming you've also installed go using the official Windows installer linked above, you will also need to add go to your PATH by modifying your environment variable. See the next section on \"Environment variables\" for more information. Due to how msys2 treats the PATH variable, you also need to add a new environment variable MSYS2_PATH_TYPE, with the value inherit, using the same procedure. Once these steps are complete, proceed to building. Building on Rasberry PI On a Raspberry PI, you'll need to increase your swap file size in order to build. Normal desktop and server linux OSes should be fine to proceed to building. One symptom of having not enough swap space is the go install command producing an error message ending with: To increase your swapfile size, first turn off the swapfile: sudo dphys-swapfile swapoff Then edit /etc/dphys-swapfile as root and set CONF_SWAPSIZE to 1024. Finally turn on the swapfile again: sudo dphys-swapfile swapon Otherwise linux machines with reduced memory will have other ways to increase their swap file sizes. Check documentation for your particular machine. Packages Qri is comprised of many specialized packages. Below you will find a summary of each package. Package Go Docs Go Report Card Description api user accessible layer, primarily made for communication with our frontend webapp cmd our command line interface config user configuration details, includes peer's profile lib takes arguments from the cmd and api layer and forms proper requests to call to the action layer p2p the peer to peer communication layer of qri repo the repository: saving, removing, and storing datasets, profiles, and the config dataset the blueprint for a dataset, the atoms that make up qri registry the blueprint for a registry: the service that allows profiles to be unique and datasets to be searchable starlib the starlark standard library available for qri transform scripts qfs \"qri file sytem\" is Qri's file system abstraction for getting & storing data from different sources ioes package to handle in, out, and error streams: gives us better control of where we send output and errors jsonschema used to describe the structure of a dataset, so we can validate datasets and determine dataset interop Outside Libraries The following packages are not under Qri, but are important dependencies, so we display their latest versions for convenience. Package Version ipfs This documentation has been adapted from the Cycle.js documentation. ",
        "Interesting project, particularly with the choice of IPFS and DCAT -- something I'll have to look into.  There have been other efforts to handle mostly file-based scientific data with versioning in both distributed (Dat <a href=\"https://blog.datproject.org/tag/science/\" rel=\"nofollow\">https://blog.datproject.org/tag/science/</a>) and centralized ways (DataHub <a href=\"https://datahub.csail.mit.edu/www/\" rel=\"nofollow\">https://datahub.csail.mit.edu/www/</a>).  Juan Benet visited our research center to give a talk about IPFS a few years ago. Really fantastic stuff.<p>I'm the creator of DVID (<a href=\"http://dvid.io\" rel=\"nofollow\">http://dvid.io</a>), which has an entirely different approach to how we might handle distributed versioning of scientific data primarily at a larger scale (100 GB to petabytes).  Like Qri and IPFS, DVID is written in Go.  Our research group works in Connectomics.  We start with massive 3D brain image volumes and apply automated and manual segmentation to mine the neurons and synapses of all that data.  There's also a lot of associated data to manage the production of connectomes.<p>One of our requirements, though, is having low-latency reads and writes to the data.  We decided to create a Science API that shields clients from how the data is actually represented, and for now, have used an ordered key-value stores for the backend.  Pluggable \"datatypes\" provide the Science API and also translate requests into the underlying key-value pairs, which are the units for versioning.  It's worked out pretty well for us and I'm now working on overhauling the store interface and improving the movement of versions between servers.  At our scale, it's useful to be able to mail a hard drive to a collaborator to establish the base DAG data and then let them eventually do a \"pull request\" for their relatively small modifications.<p>We've published some of our data online (<a href=\"http://emdata.janelia.org\" rel=\"nofollow\">http://emdata.janelia.org</a>) and visitors can actually browse through the 3d images using a Google-developed web app, Neuroglancer.  It's running on a relatively small VM so I imagine any significant HN traffic might crush it :/  We are still figuring out the best way to handle the public-facing side.<p>I think a lot of people are coming up with their own ideas about how to version scientific data, so maybe we should establish a meeting or workshop to discuss how some of these systems might interoperate?  The RDA (<a href=\"https://rd-alliance.org/\" rel=\"nofollow\">https://rd-alliance.org/</a>) \nhas been trying to establish working groups and standards, although they weren't really looking at distributed versioning a few years ago.  We need something like a Github for scientific data where papers can reference data at a particular commit and then offer improvements through pull requests.",
        "I really love the design and style qri! It is fun!<p>Can I ask why, for a git-style system, IPFS was chosen instead of GUN or SSB?<p>Certainly, images/files/etc. are better in IPFS than GUN or SSB.<p>But, you're gonna have a nightmare doing any git-style index/patch/object/etc. operations with it - both GUN & SSB's algorithms are meant to handle this type of stuff.<p>Did you guys do any analysis?"
      ],
      "relevant": "false"
    },
    {
      "id": 19412088,
      "title": "Sviatoslav Richter: A Pianist Who Made the Earth Move (2015)",
      "search": [
        "Sviatoslav Richter: A Pianist Who Made the Earth Move (2015)",
        "https://www.npr.org/sections/deceptivecadence/2015/03/19/393778706/sviatoslav-richter-the-pianist-who-made-the-earth-move",
        "NPRs sites use cookies, similar tracking and storage technologies, and information about the device you use to access our sites (together, cookies) to enhance your viewing, listening and user experience, personalize content, personalize messages from NPRs sponsors, provide social media features, and analyze NPRs traffic. This information is shared with social media, sponsorship, analytics, and other vendors or service providers. See details. You may click on Your Choices below to learn about and use cookie management tools to limit use of cookies when you visit NPRs sites. You can adjust your cookie choices in those tools at any time. If you click Agree and Continue below, you acknowledge that your cookie choices in those tools will be respected and that you otherwise agree to the use of cookies on NPRs sites. ",
        "A colleague came to Richter after a concert and asked him: \"How could you play that passage, I practiced it for 100 times with no luck\". Richter: \"I practiced it 1000 times\".",
        "I discovered Richter when I listened to Mussorgsky's Pictures of an Exhibition on an old LP. But I really understood Richter's boldness and genius when a friend of mine lended me a CD with Schubert's last piano sonata (D960).<p>Just listen to the way the first bars, containing the unforgettable opening theme, are usually played by most of the best pianists:<p>- Brendel: <a href=\"https://m.youtube.com/watch?v=TKy0Lyl4g-s\" rel=\"nofollow\">https://m.youtube.com/watch?v=TKy0Lyl4g-s</a><p>- Uchida: <a href=\"https://m.youtube.com/watch?v=l7cc2FD06FM\" rel=\"nofollow\">https://m.youtube.com/watch?v=l7cc2FD06FM</a><p>- Kovacevich: <a href=\"https://m.youtube.com/watch?v=MAZ8PA5_gVA\" rel=\"nofollow\">https://m.youtube.com/watch?v=MAZ8PA5_gVA</a><p>And now listen to Richter's version:<p><a href=\"https://m.youtube.com/watch?v=ZbJtHzaFpBQ\" rel=\"nofollow\">https://m.youtube.com/watch?v=ZbJtHzaFpBQ</a><p>This very slow tempo is extremely difficult from the point of view of the pianist, because it requires great control of dynamics and precision. However, it allows the listener to understand all the intricacies of the harmony and of the texture. I do not usually listen Richter's version because it requires deep concentration, but I can say I never really understood this piece until I heard Richter.<p>Happy birthday, Maestro!"
      ],
      "relevant": "false"
    },
    {
      "id": 19652376,
      "title": "Infrastructure as Code, Part One",
      "search": [
        "Infrastructure as Code, Part One",
        "https://crate.io/a/infrastructure-as-code-part-one/",
        "Let's say you've developed a new feature and you want to release it. You've avoided all the typical pitfalls when it comes to making a new release and you've done everything as you were meant to. It's not a Friday, it's not 5 pm, and so on. But your organization is still doing manual releases. So that means that a systems administrator is logging on to each one of your production machines and deploying the latest version of the code. At your organization, the rules demand that you submit a rollout request in advance. And in this instance, you are granted a rollout window for Tuesday afternoon. Your application changes have already been successfully deployed in the staging environment. The tests pass, and everything else has gone smoothly. You're feeling confident. Tuesday afternoon rolls around, and it's time to make the release to the production environment. The deployment is successful on the first machine. And the second machine. But wait. Something goes wrong on the third machine. It turns out that the third production server has a different set of application dependencies installed. The versions are incompatible. You start debugging, but there's a fixed time window for the deployment, and time is running out... Eventually, time runs out. You've missed the window. The entire release is rolled back. And now you have to request another rollout. But approval from stakeholders is slow, so the next opportunity is a week away. Damn. You spend the rest of the day investigating what went wrong. Eventually, you figure it out. Somebody logged on to the third machine last week and manually updated some of the software. These changes were never propagated to the other servers, or back to the staging environment, or dev environments. Does any of this feel familiar? If so, you're not alone. Fortunately, Infrastructure as Code (IaC) can help you mitigate all of the problems described above. In part one of this IaC miniseries, I will introduce you to the basic concepts and explain some of the benefits. An Introduction As the name suggests, Infrastructure as Code uses code to provision, configure, and manage infrastructure. Using the right set of tools, it is straightforward to create a description of the infrastructure on which your application should be deployed. This description includes the specification of virtual machines, storage, software stacks, network configurations, security features, user accounts, access control limits, and so on. This description is done using code, often using a declarative language. The language you use varies depending on the tools you use, from common scripting languages to Domain Specific Languages (DSL) provided by the tools. IaC has evolved alongside the emergence of Infrastructure as a Service (IaaS) and other cloud-based services. The programmability of IaaS and the declarative nature of IaC work very well together. Instead of setting up that cloud environment by hand each time, you can just automate it with IaC. But that doesn't mean that IaC limits you to IaaS, whether public, private, or hybrid. With a little extra work, you can use infrastructure configuration tools to manage a traditional collection of physical servers. Alternatives Before I continue, it would be remiss of me not to mention the other options. There are three main ones that I can think of: Manually setting up your infrastructure using the visual console provided by your cloud provider.For example, using the Azure Portal by hand to set up your Microsoft Azure products, one by one. Clicking around the interface, creating a new VM, choosing the operating system from a drop-down menu, launching it, monitoring the status, and so on. Using the CLI tool provided by a cloud provider. Instead of using a cloud provider, you're managing your own physical machines or virtual machines. And you have written your own collection of configuration tools, management tools, deployment scripts, and so on. If you're using a cloud provider, the console is a great way to learn the ropes. But this quickly grows tiresome and error-prone if you try to manage your whole setup like this. And in addition, there's usually no built-in change visibility. You have to remember what actions you took, and document them for the rest of the team. And if you're managing your own servers by hand, developing your own system administration scripts can work okay. But it's easy for such a system to become a bit of a non-standard hodgepodge. And, well, things can quickly get out of hand... The Landscape Okay, let's take a high-level look at the IaC landscape. There are basically two categories of tools: Orchestration tools are used to provision, organize, and manage infrastructure components. Examples include Terraform, AWS CloudFormation, Azure Resource Manager. Configuration management tools are used to install, update, and manage the software running on the infrastructure components. Examples include SaltStack, Puppet, Chef, and Ansible. Additionally, when it comes to IaC, there are two primary approaches to managing infrastructure: Some tools (e.g., SaltStack) treat infrastructure components as mutable objects. That is, every time you make a change to the configuration, the necessary set of changes are made to the already-running components. Other tools (e.g., Terraform) treat infrastructure components as immutable objects. That is, when you make a change, new components with the new configuration are created to replace the old components. What Are the Benefits? Developer Mindset Developers ideally concern themselves with creating stable and sustainable software. But when infrastructure is \"something those other people take care of,\" it's easy for developers not to consider how the software they are building is going to be run. When you manage the infrastructure using code and involve the application developers in that process, it can prompt a change in mindset. How is this application going to be deployed? How is it going to be maintained once it's running? How are upgrades done? Have you thought about all the ways it might fail on a production machine? What preventative measures can you take? All of this and more becomes a natural part of the application development process itself when it is integrated with IaC. Version Control Because you are defining your infrastructure with code, it should also be versioned in a repository like the rest of your code. All of the benefits that change control offers application development are made available for infrastructure management: A single source of truth. The code itself, and the way it is laid out is a communication tool and can be used to understand the way things are built. The history of changes is recorded and accessible so you can understand what has changed and why over time. Coding standards can be developed and maintained. Pull requests and code reviews increase knowledge sharing and improve the overall quality of the code. Experimental changes can be made on branches and tested in isolation. Making changes is less daunting because if something goes wrong, you can revert back to a previously known working version. And so on, and so on... Knowledge Sharing It's a widespread phenomenon that one or two people in an organization will accumulate critical information that only they seem to possess. [At one of my previous companies, we kept a list of such individuals called the \"red bus\" list, so named because of the risk that one of them might be hit by one. Ed.] Less dramatically, such people take holidays and sometimes get sick, like anyone. So you should be able to cope with them not being around. With your infrastructure documented using code, it is hopefully relatively straightforward to understand. And what's more, this is a type of living documentation that should always be up-to-date. Better Use of Time In general, humans are bad with tedious, repetitive tasks. We lose interest, and that's when we're most likely to make mistakes. Fortunately, this is what machines are good at. When you manage your infrastructure with code, you offload all of the tedious, repetitive work to computers. And as a result, you reduce the chance of inconsistencies, mistakes, incomplete work, and other forms of human error. This also allows your people to spend their time and energies on other more important, high-value tasks Continuous Integration Continuous Integration (CI) is the practice of team members merging their code changes into a mainline branch multiple times per day. The benefit is that you are continually validating your code against your test suites, and you can adapt to changes being made to the codebase in a manageable, incremental fashion, as and when it happens, and not all in one go at the end of your project (what has been termed \"integration hell\"). IaC improves CI because changes to the infrastructure can be tested like changes to the application code. Additionally, a temporary testing environment can be provisioned for every change to application code. Continuous Delivery Continuous Delivery (CD) is the process of making regular automated releases every time code is pushed to the mainline branch. CD and IaC are a match made in heaven. With IaC, you can set up a deployment pipeline that automates the process of moving different versions of your application from one environment to the next as your testing and release schedules dictate. But more on that topic in another post. :) Wrap Up Infrastructure as Code (IaC) bridges the gap between system administrators and developers, and in doing so it: Helps developers think about the entire lifecycle of the software they are developing. Brings version control, code reviews, knowledge sharing, and other benefits to systems administration. Enables you to radically transform your CI and CD pipeline. Reduces the need for repetitive and error-prone tasks, freeing people up to work on more exciting things. The end result should be a more consistent, reliable, and well-understood infrastructure that is easier to develop and easier to maintain. In part two of this miniseries, I dive into the details and show you some of the ways we're using Terraform at Crate.io for provisioning and managing infrastructure. ",
        "Nice piece. Looking forward to Part II.<p>What I am missing (often, in these type of articles as well as in actual production environments) is the fact that if you develop (infrastructure) code, you also need  to test your (infrastructure) code. Which means you need actual infrastructure to test on.<p>In my case, this means network equipment, storage equipment and actual physical servers.<p>If you're in a cloud, this means you need a seperate account on a seperate creditcard and start from there to build up the infra that Dev and Ops can start deploying their infra-as-code on.<p>And this test-infrastructure is <i>not</i> the test environments other teams run <i>their</i> tests on.<p>If that is not available, automating your infrastructure can be dangerous at best. Since you cannot properly test your code. And your code will rot.",
        "What I find unfortunate about infrastructure-as-code tooling is that a lot of the tooling isn't actually using code, but instead uses esoteric configuration languages. Indeed, the article refers to Terraform with its custom syntax.<p>Imho tools should use actual code (whether it's TypeScript or Kotlin or whatever) instead of reinventing constructs like loops and string interpolation.<p>Thankfully these tools are getting more popular, because frankly I can't stand configuring another Kubernetes or GCP resource using a huge block of copy/pasted YAML."
      ],
      "relevant": "false"
    },
    {
      "id": 21158487,
      "title": "Streamlit: Turn a Python script into an interactive data analysis tool",
      "search": [
        "Streamlit: Turn a Python script into an interactive data analysis tool",
        "https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace",
        "Introducing Streamlit, an app framework built for ML engineersCoding a semantic search engine with real-time neural-net inference in 300 lines of Python.In my experience, every nontrivial machine learning project is eventually stitched together with bug-ridden and unmaintainable internal tools. These tools often a patchwork of Jupyter Notebooks and Flask apps are difficult to deploy, require reasoning about client-server architecture, and dont integrate well with machine learning constructs like Tensorflow GPU sessions.I saw this first at Carnegie Mellon, then at Berkeley, Google X, and finally while building autonomous robots at Zoox. These tools were often born as little Jupyter notebooks: the sensor calibration tool, the simulation comparison app, the LIDAR alignment app, the scenario replay tool, and so on.As a tool grew in importance, project managers stepped in. Processes sprouted. Requirements flowered. These solo projects gestated into scripts, and matured into gangly maintenance nightmares.The machine learning engineers ad-hoc app building flow.When a tool became crucial, we called in the tools team. They wrote fluent Vue and React. They blinged their laptops with stickers about declarative frameworks. They had a design process:The tools teams clean-slate app building flow.Which was awesome. But these tools all needed new features, like weekly. And the tools team was supporting ten other projects. They would say, well update your tool again in two months.So we were back to building our own tools, deploying Flask apps, writing HTML, CSS, and JavaScript, and trying to version control everything from notebooks to stylesheets. So my old Google X friend, Thiago Teixeira, and I began thinking about the following question: What if we could make building tools as easy as writing Python scripts?We wanted machine learning engineers to be able to create beautiful apps without needing a tools team. These internal tools should arise as a natural byproduct of the ML workflow. Writing such tools should feel like training a neural net or performing an ad-hoc analysis in Jupyter! At the same time, we wanted to preserve all of the flexibility of a powerful app framework. We wanted to create beautiful, performant tools that engineers could show off. Basically, we wanted this:The Streamlit app building flow.With an amazing beta community including engineers from Uber, Twitter, Stitch Fix, and Dropbox, we worked for a year to create Streamlit, a completely free and open source app framework for ML engineers. With each prototype, the core principles of Streamlit became simpler and purer. They are:#1: Embrace Python scripting. Streamlit apps are really just scripts that run from top to bottom. Theres no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. For example, this is how you write to the screen:import streamlit as stst.write('Hello, world!')Nice to meet you.#2: Treat widgets as variables. There are no callbacks in Streamlit! Every interaction simply reruns the script from top to bottom. This approach leads to really clean code:import streamlit as stx = st.slider('x')st.write(x, 'squared is', x * x)An interactive Streamlit app in three lines of code.#3: Reuse data and computation. What if you download lots of data or perform complex computation? The key is to safely reuse information across runs. Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default, data store that lets Streamlit apps safely and effortlessly reuse information. For example, this code downloads data only once from the Udacity Self-driving car project, yielding a simple, fast app:Using st.cache to persist data across Streamlit runs. To run this code, please follow these instructions.The output of running the st.cache example above.In short, Streamlit works like this:The entire script is run from scratch for each user interaction.Streamlit assigns each variable an up-to-date value given widget states.Caching allows Streamlit to skip redundant data fetches and computation.Or in pictures:User events trigger Streamlit to rerun the script from scratch. Only the cache persists across runs.If this sounds intriguing, you can try it right now! Just run:$ pip install --upgrade streamlit $ streamlit hello You can now view your Streamlit app in your browser. Local URL: http://localhost:8501 Network URL: http://10.0.1.29:8501This will automatically pop open a web browser pointing to your local Streamlit app. If not, just click the link.To see more examples like this fractal animation, run streamlit hello from the command line.Ok. Are you back from playing with fractals? Those can be mesmerizing.The simplicity of these ideas does not prevent you from creating incredibly rich and useful apps with Streamlit. During my time at Zoox and Google X, I watched as self-driving car projects ballooned into gigabytes of visual data, which needed to be searched and understood, including running models on images to compare performance. Every self-driving car project Ive seen eventually has had entire teams working on this tooling.Building such a tool in Streamlit is easy. This Streamlit demo lets you perform semantic search across the entire Udacity self-driving car photo dataset, visualize human-annotated ground truth labels, and run a complete neural net (YOLO) in real time from within the app [1].This 300-line Streamlit demo combines semantic visual search with interactive neural net inference.The whole app is a completely self-contained, 300-line Python script, most of which is machine learning code. In fact, there are only 23 Streamlit calls in the whole app. You can run it yourself right now!$ pip install --upgrade streamlit opencv-python$ streamlit runhttps://raw.githubusercontent.com/streamlit/demo-self-driving/master/app.pyAs we worked with machine learning teams on their own projects, we came to realize that these simple ideas yield a number of important benefits:Streamlit apps are pure Python files. So you can use your favorite editor and debugger with Streamlit.My favorite layout for writing Streamlit apps has VSCode on the left and Chrome on the right.Pure Python scripts work seamlessly with Git and other source control software, including commits, pull requests, issues, and comments. Because Streamlits underlying language is pure Python, you get all the benefits of these amazing collaboration tools for free .Because Streamlit apps are just Python scripts, you can easily version control them with Git.Streamlit provides an immediate-mode live coding environment. Just click Always rerun when Streamlit detects a source file change.Click Always rerun to enable live coding.Caching simplifies setting up computation pipelines. Amazingly, chaining cached functions automatically creates efficient computation pipelines! Consider this code adapted from our Udacity demo:A simple computation pipeline in Streamlit. To run this code, please follow these instructions.Basically, the pipeline is load_metadata create_summary. Every time the script is run Streamlit only recomputes whatever subset of the pipeline is required to get the right answer. Cool!To make apps performant, Streamlit only recomputes whatever is necessary to update the UI.Streamlit is built for GPUs. Streamlit allows direct access to machine-level primitives like TensorFlow and PyTorch and complements these libraries. For example in this demo, Streamlits cache stores the entire NVIDIA celebrity face GAN [2]. This approach enables nearly instantaneous inference as the user updates sliders.This Streamlit app demonstrates NVIDIA celebrity face GAN [2] model using Shaobo Guans TL-GAN [3].Streamlit is a free and open-source library rather than a proprietary web app. You can serve Streamlit apps on-prem without contacting us. You can even run Streamlit locally on a laptop without an Internet connection! Furthermore, existing projects can adopt Streamlit incrementally.Several ways incrementally adopt Streamlit. (Icons courtesy of fullvector / Freepik.)This just scratches the surface of what you can do with Streamlit. One of the most exciting aspects of Streamlit is how these primitives can be easily composed into complex apps that look like scripts. Theres a lot more we could say about how our architecture works and the features we have planned, but well save that for future posts.Block diagram of Streamlits components. More coming soon!Were excited to finally share Streamlit with the community today and see what you all build with it. We hope that youll find it easy and delightful to turn your Python scripts into beautiful ML apps.Thanks to Amanda Kelly, Thiago Teixeira, TC Ricks, Seth Weidman, Regan Carey, Beverly Treuille, Genevive Wachtell, and Barney Pell for their helpful input on this article.References:[1] J. Redmon and A. Farhadi, YOLOv3: An Incremental Improvement (2018), arXiv.[2] T. Karras, T. Aila, S. Laine, and J. Lehtinen, Progressive Growing of GANs for Improved Quality, Stability, and Variation (2018), ICLR.[3] S. Guan, Controlled image synthesis and editing using a novel TL-GAN model (2018), Insight Data Science Blog. ",
        "This looks really slick, can't wait to try it out!<p>If anyone is curious about other tools in the same space, our data scientists use Dash[1] and plotly to build interactive exploration and visualization apps. We set up a Git repo that deploys their apps internally with every merge to master, so they're actually building and updating tools that our operations, marketing, etc teams use every day.<p>[1] <a href=\"https://plot.ly/dash/\" rel=\"nofollow\">https://plot.ly/dash/</a>",
        "Interesting project, but why does an open source developer tool needs browser telemetry?<p>You should ask for telemetry permissions _before_ the process starts up (as you do for email address), and keep the default as \"No\", instead of start to send the data transparently unless non user friendly steps are taken by the user."
      ],
      "relevant": "false"
    },
    {
      "id": 19647692,
      "title": "IPFS Project Roadmap",
      "search": [
        "IPFS Project Roadmap",
        "https://github.com/ipfs/roadmap",
        "Table of Contents IPFS Mission Statement 2020 Priority Content Routing 2020 Working Groups 2020 Epics 2019 Priority Package Managers Future Goals Large Files Decentralized Web Encrypted Web Distributed Web Personal Web Sneaker Web Interplanetary Web - Mars 2024 Packet Switched Web Data Web Package Switched Web Self-Archiving Web Versioning Datasets Interplanetary DevOps The World's Knowledge becomes accessible through the DWeb WebOS IPFS Mission Statement The mission of IPFS is to create a resilient, upgradable, open network to preserve and grow humanitys knowledge. This looks different! Want to participate in helping define our \"Mission Statement 2.0\"? Add your thoughts here! 2020 Priority Scoping in to 2020 H1 Instead of a 2020 year-long plan, we decided to focus on a 2020 H1 plan (covering Q1 & Q2) so as to: Enable our team to truly focus on one thing, complete it, and then move on to other challenges instead of doing many things at once Better understand the components of each goal and plan our time accordingly to hit them by not trying to nail down plans too far into the future Be adaptable and prepared for surprises, re-prioritizations, or market shifts that require us to refocus energy or change our plan in the course of the year 2020 H1 Priority Selection Criteria Before selecting a 2020 H1 priority, we did an open call for Theme Proposals to surface areas the community felt were of high importance and urgency. We combined these great proposals with an analysis of the project, team, and ecosystem state - and the biggest risks to IPFS Project success. Out of that analysis, we identified there were two main aspects our 2020 H1 plan MUST address: Mitigate current IPFS pain points around network performance and end user experience that are hindering wider adoption and scale Increase velocity, alignment, and capacity for IPFS devs and contributors to ensure our time and efforts are highly leveraged (because if we can make fast, sustained, high-quality progress by leveling-up our focus and healthy habits, we can achieve our goals faster and ensure contributing to IPFS is fun and productive!) Content Routing Given the selection criteria, our main priority for the first half of 2020 - the next 6 months - is improving the performance and reliability of content routing in the IPFS network. 'Content routing' is the process of finding a node hosting the content you're looking for, such that you can fetch the desired data and quickly load your website/dapp/video/etc. As the IPFS network scaled this past year (over 30x!), it ran into new problems in our distributed routing algorithms - struggling to find content spread across many unreliable nodes. This was especially painful for IPNS, which relied on multiple of these slow/unreliable queries to find the latest version of a file. These performance gaps caused IPFS to lag and stall while searching for the needed content, hurting the end user experience and making IPFS feel broken. Searching the network to find desired content (aka, using IPFS as a decentralized CDN) is one of the most common actions for new IPFS users and is required by most ipfs-powered dapp use cases - therefore, it's the number 1 pain point we need to mitigate in order to unlock increased adoption and scalability of the network! We considered a number of other potential goals - especially all the great 2020 Theme Proposals - before selecting this priority. However, we decided it was more important to focus core working group dev time on the main blockers and pain points to enable the entire ecosystem to grow and succeed. Many of these proposals are actually very well suited for community ownership via DevGrants and collaborations - and some of them, like \"IPFS in Rust\" and \"Examples and Tutorials\", already have grants or bounties associated with them! 2020 Working Groups The IPFS project includes the collective work of serveral focused teams, called Working Groups (WGs). Each group defines its own roadmap with tasks and priorities derived from the main IPFS Project Priority. To better orient around our core focus for 2020 H1, we created a few new working groups (notably \"Content Routing\"), and spun others down (notably our \"Package Managers\" working group). For 2020 H1, we have 5 main working groups - with our \"Ecosystem\" working group divided into 3 sub-groups. Each WGs top-line focus: Content Routing: Ensure all IPFS users can find and access content they care about in a distributed network of nodes Testground: Provide robust feedback loops for content routing development, debugging, and benchmarking at scale Bifrost (IPFS Infra): Make sure our gateway and infra scale to support access to the IPFS network Ecosystem: Ensure community health and growth through collaborations, developer experience and platform availability Browsers / Connectivity: Maximize the availability and connectivity of IPFS on the web Collabs / Community: Support IPFS users and grow new opportunities through research, collaborations and community engagement Dev Ex: Support the IPFS technical community through documentation, contributor experience, API ergonomics and tooling Project: Support team functioning, prioritization, and day-to-day operations Looking for more specifics? Check out the docs on our team roles and structure! 2020 Epics We've expanded our 2020 Priority into a list of Epic Endeavours that give an overview of the primary targets IPFS has for 2020 H1. If you are pumped about these Epics and want to help, you can get involved! See the call to action (CTA) for each section below. 1. Build IPFS dev capacity and velocity In order to achieve our content routing goal for 2020 H1, we need to level up our own leverage, coordination, velocity, and planning as a project to ensure all contributors spend their time and energy effectively. This includes a few different dimensions: Integrate research via the ResNetLab into our design practice to ensure our work builds on the knowledge and experience of leading researchers in our fields Empower new contributors in the IPFS ecosystem through DevGrants and collaborations to upgrade and extend IPFS to solve new problems Invest in developer tooling, automation, and fast feedback loops to accelerate experimentation and iteration Upgrade project planning and management within and between working groups to ensure we define, estimate, track and unblock our work efficiently Focus our attention on fewer things to improve completion rate and reduce churn, saying \"not now\" or finding other champions for nice-to-have projects in order to allocate energy and attention to the most important work You can get involved with ResNetLab RFPs or by proposing/funding projects in the DevGrants repo! 2. Improve content routing performance such that 95th percentile content routing speed is <5s Improving content routing performance requires making improvements and bugfixes to the go-libp2p DHT at scale, and changing how we form, query, and resolve content in the IPFS network to be faster and more scalable. This involves a combination of research, design, implementation, and testing. Making changes to the configuration of the entire network is non-trivial - that's why we've been investing in the InterPlanetary Testground, a new set of tools for testing next generation P2P applications, to help us diagnose issues and evaluate improvements prior to rolling out upgrades to the entire public network. You can track the work in these milestones on ZenHub: Content Routing ZenHub Roadmap Testground ZenHub Roadmap If you want to help refine the detailed milestones, or take on some of the improvements required to hit this goal, see the Content Routing Work Plan to dive deeper! 3. Invest in IPFS community enablement and support Supporting community health and growth continues to be a core focus for IPFS as we scale to more users, applications, and use cases. Refining our adoption pathways, continuing to grow platform availability, and supporting our collaborators to bring IPFS to new users and use cases helps us maximize the impact and value we create in the world. Scale the number of users and applications supported by IPFS through talks, guides, and how-tos Refine our APIs to simplify end-user adoption and maximize ease of use Bring IPFS to browsers to maximize default availability and connectivity on the web Continue impoving our new IPFS Docs Site, to ensure developer & user questions are clearly answered and actionable Invest in explicit community stewardship responsibilities to ensure there are answers, tools, and fast feedback loops to support new IPFS users and contributors Great ways to start helping enable the IPFS community include: suggesting or building new tools to support IPFS users, reviewing open PRs, answering questions on http://discuss.ipfs.io and on our IRC channels on freenode/matrix, or writing your own how-tos and guides to use IPFS for your use case! 2019 Priority Our core goal for 2019 was to make large-scale improvements to the IPFS network around scalability, performance, and usability. By focusing on the Package Managers use case, we hoped to identify, prioritize, and demonstrably resolve performance/usability issues, while driving adoption via a common and compelling use case that all developers experience daily. We hoped this focus would help us hone IPFS to be production ready (in functionality and practice), help scale network usage to millions of nodes, and accelerate our project and community growth/velocity. Graded 2019 Epics The reference implementations of the IPFS Protocol (Go & JS) become Production Ready Support Software Package Managers in entering the Distributed Web Scale the IPFS Network The IPFS Community of Builders gets together for the 1st IPFS Conf IPFS testing, benchmarks, and performance optimizations Support the growing IPFS Community You can see the details in what work we took on in each milestone, and which we achieved in the archived 2019 IPFS Project Roadmap. Sorting Function D = Difficulty (or \"Delta\" or \"Distance\"), E = Ecosystem Growth, I = Importance To identify our top focus for 2019 and rank the future goals in our upgrade path, we used a sorting function to prioritize potential focus areas. Each goal was given a score from 1 (low) - 5 (high) on each axis. We sorted first in terms of low difficulty or \"delta\" (i.e. minimal additional requirements and fewer dependencies from the capabilities IPFS has now), then high ecosystem growth (growing our community and resources to help us gravity assist and accelerate our progress), and finally high importance (we want IPFS to have a strong, positive impact on the world). Future goals below are listed in priority order using this sorting function. Package Managers (D1 E5 I3) The most used code and binary Package Managers are powered by IPFS. Package Managers collect and curate sizable datasets. Top package managers distribute code libraries (eg npm, pypi, cargo, ...), binaries and program source code (eg apt, pacman, brew ...), full applications (app stores), datasets, and more. They are critical components of the programming and computing experience, and are the perfect use case for IPFS. Most package managers can benefit tremendously from the content-addressing, peer-to-peer, decentralized, and offline capabilities of IPFS. Existing Package Managers should switch over to using IPFS as a default, or at least an optional way of distributing their assets, and their own updates. New Package Managers should be built entirely on IPFS. --- Code libraries, programs, and datasets should become permanent, resilient, partition tolerant, and authenticated, and IPFS can get them there. Registries become curators and seeds, but do not have to bear the costs of the entire bandwidth. Registries could become properly decentralized. --- We have a number of challenges ahead to make this a reality, but we are already beyond half-way. We should be able to get top package managers to (a) use IPFS as an optional distribution mechanism, then (b) use that to test all kinds of edge cases in the wild and to drive performance improvements , then (c) get package managers to switch over the defaults. Future Goals Large Files (D1 E4 I3) By 2020, IPFS becomes the default way to distribute files or collections of files above 1GB HTTP is not good for distributing large files or large collections of small files. Anything above 1GB starts running into problems (resuming, duplication, centralized bandwidth limitations, etc). BitTorrent works well for single archives that won't change, or won't duplicate, but fails in a number of places. IPFS has solved most of the hard problems but hasn't yet made the experience so easy that people default to IPFS to distribute large files. IPFS should solve this problem so well, it should be so easy and delightful to use, and it should be so high performance that it becomes the default way to move anything above 1GB world-wide. This is a massive hole right now that IPFS is well-poised to fill -- we just need to solve some performance and usability problems. Decentralized Web (D2 E4 I3) IPFS supports decentralized web apps built on p2p connections with power and capabilities at the edges. In web 2.0, control of the web is centralized - its location-addressing model and client-server architecture encourage reliance and trust of centralized operators to host services, data, and intermediate connections. Walled gardens are common, and our data is locked into centralized systems that increase the risk of privacy breaches, state control, or that a single party can shut down valued services. The decentralized web is all about peer-to-peer connections and putting users in control of their tools and data. It does this by connecting users directly to each other and using verifiable tools like hash-links and encryption to ensure the power and control in the network is retained by the participants themselves. The decentralized web (as distinguished from Distributed Web) is NOT about partition tolerance, or making the web work equally well in local-area networks/mobile/offline - the focus here is on the control and ownership of services. IPFS has solved most of the hard underlying design problems for decentralized web, but hasn't yet made the experience easy enough for end-users to experience it in the applications, tools, and services they use. This requires tooling and solutions for developers to sustainably run their business without any centralized intermediary facilitating the network (though centralized providers may still exist to augment and improve the experience for services that already work decentralized by design). Designing Federation for interop with current systems is key for the Migration Path. Encrypted Web (D2 E3 I4) Apps and Data are fully end-to-end encrypted at rest. Users have reader, writer, and usage privacy. Apps and user data on IPFS are completely end-to-end encrypted, at rest, with only users having access. Users get reader and writer privacy by default. Any nodes providing services usually do so over encrypted data and never get access to the plain data. The apps themselves are distributed encrypted, decrypted and loaded in a safe sandbox in the users' control. Attackers (including ISPs) lose the ability to spy on users' data, and even which applications users are using. This works with all top use case apps -- email, chat, forums, collaboration tools, etc. Distributed Web (D2 E2 I4) Info and apps function equally well in local area networks and offline. The Web is a partitionable fabric, like the internet. The web and mobile -- the most important application platforms on the planet -- are capable of working entirely in sub-networks. The norm for networked apps is to use the available data and connections, to sync asynchronously, and to leverage local connectivity protocols. The main apps for every top use case work equally well in offline or local network settings. It means IPFS and apps on top work excellently on desktops, the browser, and mobile. Users can use webapps like email, chat, forums, social networks, collaboration tools, games, and so on without having to be connected to the global internet. Oh, and getting files from one computer to another right next to it finally becomes an easy thing to do (airdrop level of easy). Personal Web (D3 E4 I2) Personal Data and programs are under user control. The memex becomes reality. The web becomes a drastically more personal thing. Users' data and exploration is under the users' control -- similar to how a \"personal computer\" is under the user's control, and \"the cloud\" is not. Users decide which apps and other people get access to their data. Explorations can be recorded for the user in memex fashion. The user gets to keep copies of all the data they have observed through the web. A self-archiving personal record forms, which the user can always go back to, explore, and use -- whether or not those applications are still in development by their authors. Sneaker Web (D3 E2 I4) The web functions over disconnected sneaker networks, spreading information, app data, apps, and more. The web is capable of working fully distributed, and can even hop across disconnected components of the internet. Apps and their data can flow across high latency, intermittent, asynchronous links across them. People in disconnected networks get the same applications, the same quality of experience, and the same ability to distribute their contributions as anybody in the strongest connected component (\"the backbone of the internet\"). The Web is totally resistant to large scale partitions. Information can flow so easily across disconnected components that there is no use in trying to block or control information at the borders. Interplanetary Web - Mars 2024. (D3 E3 I4) Mars. Let's live the interplanetary dream!** SpaceX plans to land on mars in 2022, and send humans in 2024. By then, IPFS should be the default/best choice for SpaceX networking. The first humans on mars should use IPFS to run the top 10 networked apps. That means truly excellent and well-known IPFS apps addressing the top 10 networked use cases must exist. For that to happen, the entire system needs to be rock solid, audited, performant, powerful, easy-to-use, well known, and so on. It means IPFS must work on a range of platforms (desktop, servers, web, mobile), and to work with both special purpose local area networks, and across interplanetary distances. If we achieve this, while solving for general use and general users (not specifically for the Mars use case, then IPFS will be in tremendous standing. Packet Switched Web (D3 E2 I3) IPFS protocols use packet switching, and the network can relay all kinds of traffic easily, tolerating switch failures. The infrastructure protocols (libp2p, IPFS, etc.) and the end-user app protocols (the logic of the app) can work entirely over a packet switching layer. Protocols like BitSwap, DHT, PubSub become drastically higher performance, and unconstrained by packets sent before theirs. Web applications can form their own isolated virtual networks, allowing their users to distribute the packets. Users can form their own groups and their own virtual networks, allowing users to only operate in a subnet they trust, and ensure all of their traffic is moving between trusted switches. The big public network uses packet switching by default. Data Web (D4 E3 I3) Large Datasets are open, easy to access, easy to replicate, version controlled, secure, permanent. We constantly lose access to important information, either because it ceases to exist or simply due to virtual virtual barriers (i.e. censorship, lack of connectivity and so on). Information also often loses its way into the peers that most needed it and there aren't good ways to signal that some dataset wasn't contemplated, referenced. We want to improve this dramatically, making the data that is produced more easy to access through making it versionased, secure and easier to replicate and locate. Package Switched Web (D4 E2 I2) Data in the web can be moved around over a package switching network. Shipping TB or PB hard drives of data becomes normal. Beyond circuit switching and packet switching, the web works over package switching! It is possible to send apps, app assets, app user generated data, and so on through hard drives means. This means that the network stack and the IPLD graph sync layers are natively capable of using data in external, removable media. It is easy for a user Alice to save a lot of data to a removable drive, for Alice to mail the drive to another user Bob, and for Bob to plug in the drive to see his application finish loading what Alice wanted to show Bob. Instead of having to fumble with file exports, file systems, OS primitives, and worse -- IPFS, libp2p, and the apps just work -- there is a standard way to say \"i want this data in this drive\", and \"i want to use the data from this drive\". Once that happens, it can enable proper sneakernet web. Self-Archiving Web (D4 E4 I4) The Web becomes permanent, no more broken Links. Increase the lifespan of a Webpage from 6 months to (as good as a book). The Internet Archive(s, plural) Content Address their snapshots to maximize deduplications and hit factor. IPFS becomes the platform that enables the multiple Internet Archives to store, replicate and share responsibility over who possesses what. It becomes simple for any institution (from large organization to small local library) to become an Internet Archive node. Users can search through these Internet Archives nodes, fully compliant with data protection laws. Versioning Datasets (D4 E3 I3) IPFS becomes the default way to version datasets, and unlocks a dataset distribution and utility explosion similar to what VCS did for code. IPFS emerged from dataset versioning, package management, and distribution concerns. There are huge gaping holes in this space because large datasets are very unwieldy and defy most systems that make small files easy to version, package, and distribute. IPFS was designed with this kind of problem in mind and has the primitives in place to solve many of these problems. There are many things missing: (a) most importantly, a toolchain for version history management that works with these large graphs (most of what git does). (b) Better deduplication and representation techniques. (c) Virtual filesystem support -- to plug under existing architectures. (d) Ways to easily wrap existing data layouts (filestore) -- to plug on top existing architectures. (e) An unrelenting focus on extremely high performance. (f) primitives to retrieve and query relevant pieces of versioned datasets (IPLD Selectors and Queries). --- But luckily, all of these things can be added incrementally to enhance the tooling and win over more user segments. Interplanetary DevOps (D4 E2 I2) Versioning, packaging, distribution, and loading of Programs, Containers, OSes, VMs, defaults to IPFS. IPFS is great for versioning, deduping, packaging, distributing assets, through a variety of mediums. IPFS can revolutionize computing infrastructure systems. It has the potential to become the default way for datacenter and server infrastructure users to set up their infrastructure. This can happen at several different layers. (a) In the simplest sense, IPFS can help distribute programs to servers, by sitting within the OS, and plugging in as the downloading mechanism (replace wget, rsync, etc.). (b) IPFS can also distribute containers -- it can sit alongside docker, kubernetes, and similar systems to help version, dedup, and distribute containerized services. (c) IPFS can also distribute OSes themselves, by plugging in at the OS package manager level, and by distributing OS installation media. (d) IPFS can also version, dedup, and distribute VMs, first by sitting alongside host OSes and hypervisors moving around VM snapshots, and then by modeling VMs themselves on top of IPFS/IPLD. --- To get there, we will need to solve many of the same problems as package managers, and more. We will need the IPLD importers to model and version the media super-effectively. The World's Knowledge becomes accessible through the DWeb (D5 E2 I5) Humanity deserves equal access to the Knowledge. Platforms such as Wikipedia, Coursera, Edx, Khan Academy and others need to be available independently of Location and Connectivity. The content of this services need to exist everywhere. These replicas should be part of the whole world's dataset and not disjoint dataset. Anyone should be able to access these through the protocol, without having to deploy new services per area. WebOS (D5 E2 I3) The Web Platform and the OS'es merge. The rift between the web and the OS is finally healed. The OS and local programs and WebApps merge. They are not just indistinguishable, they are the same thing. \"Installing\" becomes pinning applications to the local computer. \"Saving\" things locally is also just pinning. The browser and the OS are no longer distinguishable. The entire OS data itself is modelled on top of IPLD, and the internal FileSystem is IPFS (or something on top, like unixfs). The OS and programs can manipulate IPLD data structures natively. The entire state of the OS itself can be represented and stored as IPLD. The OS can be frozen or snapshotted, and then resumed. A computer boots from the same OS hash, drastically reducing attack surface. ",
        "\"2019 Goal: The most used code and binary Package Managers are powered by IPFS.\"<p>That's kind of stupid-ambitious for 2019 when another 2019 goal is \"a production-ready implementation\" and IPFS has been around for 3 years already.<p>This isn't a roadmap, it's a wishlist. And I'm someone who wants to see IPFS succeed.",
        "This is not a roadmap, but rather a wishlist. There is a fundamental problem that IPFS needs to solve first. This problem is called an efficient WebRTC-based DHT. In order to change the web, IPFS needs to become usable in the browsers. Since the backbone of IPFS is DHT, there need to be an efficient UDP-based solution for \"DHT in the web\". Right now this isn't possible and reasons are not just technical, but political. The IPFS team would need to convince all major players that enabling this DHT scenario is a good idea."
      ],
      "relevant": "false"
    },
    {
      "id": 21055373,
      "title": "Building a Modern CI/CD Pipeline in the Serverless Era with GitOps",
      "search": [
        "Building a Modern CI/CD Pipeline in the Serverless Era with GitOps",
        "https://aws.amazon.com/blogs/aws/building-a-modern-ci-cd-pipeline-in-the-serverless-era-with-gitops/",
        "Guest post by AWS Community Hero Shimon Tolts, CTO and co-founder at Datree.io. He specializes in developer tools and infrastructure, running a company that is 100% serverless. In recent years, there was a major transition in the way you build and ship software. This was mainly around microservices, splitting code into small components, using infrastructure as code, and using Git as the single source of truth that glues it all together. In this post, I discuss the transition and the different steps of modern software development to showcase the possible solutions for the serverless world. In addition, I list useful tools that were designed for this era. What is serverless? Before I dive into the wonderful world of serverless development and tooling, heres what I mean by serverless. The AWS website talks about four main benefits: No server management. Flexible scaling. Pay for value. Automated high availability. To me, serverless is any infrastructure that you dont have to manage and scale yourself. At my company Datree.io, we run 95% of our workload on AWS Fargate and 5% on AWS Lambda. We are a serverless company; we have zero Amazon EC2 instances in our AWS account. For more information, see the following: Datree.io case study Migrating to AWS ECS Fargate in production CON320: Operational Excellence w/ Containerized Workloads Using AWS Fargate (re:Invent 2018) What is GitOps? Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. According to Luis Faceira, a CI/CD consultant, GitOps is a way of working. You might look at it as an approach in which everything starts and ends with Git. Here are some key concepts: Git as the SINGLE source of truth of a system Git as the SINGLE place where we operate (create, change and destroy) ALL environments ALL changes are observable/verifiable. How you built software before the cloud Back in the waterfall pre-cloud era, you used to have separate teams for development, testing, security, operations, monitoring, and so on. Nowadays, in most organizations, there is a transition to full developer autonomy and developers owning the entire production path. The developer is the King or Queen :) Those teams (Ops/Security/IT/etc) used to be gatekeepers to validate and control every developer change. Now they have become more of a satellite unit that drives policy and sets best practices and standards. They are no longer the production bottleneck, so they provide organization-wide platforms and enablement solutions. Everything is codified With the transition into full developer ownership of the entire pipeline, developers automated everything. We have more code than ever, and processes that used to be manual are now described in code. This is a good transition, in my opinion. Here are some of the benefits: Automation: By storing all things as code, everything can be automated, reused, and re-created in moments. Immutable: If anything goes wrong, create it again from the stored configuration. Versioning: Changes can be applied and reverted, and are tracked to a single user who made the change. GitOps: Git has become the single source of truth The second major transition is that now everything is in one place! Git is the place where all of the code is stored and where all operations are initiated. Whether its testing, building, packaging, or releasing, nowadays everything is triggered through pull requests. This is amplified by the codification of everything. Useful tools in the serverless era There are many useful tools in the market, here is a list of ones that were designed for serverless. Code Always store your code in a source control system. In recent years, more and more functions are codified, such as, BI, ops, security, and AI. For new developers, it is not always obvious that they should use source control for some functionality. GitHub AWS CodeCommit GitLab BitBucket Build and test The most common mistake I see is manually configuring build jobs in the GUI. This might be good for a small POC but it is not scalable. You should have your job codified and inside your Git repository. Here are some tools to help with building and testing: AWS CodeBuild CodeFresh GitHub Actions Jenkins-x CircleCI TravisCI Security and governance When working in a serverless way, you end up having many Git repos. The number of code packages can be overwhelming. The demand for unified code standards remains as it was but now it is much harder to enforce it on top of your R&D org. Here are some tools that might help you with the challenge: Snyk Datree PureSec Aqua Protego Bundle and release Building a serverless application is connecting microservices into one unit. For example, you might be using Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. Instead of configuring each one separately, you should use a bundler to hold the configuration in one place. That allows for easy versioning and replication of the app for several environments. Here are a couple of bundlers: Serverless Framework AWS Serverless Application Model (AWS SAM) Package When working with many different serverless components, you should create small packages of tools to be able to import across different Lambda functions. You can use a language-specific store like npm or RubyGems, or use a more holistic solution. Here are several package artifact stores that allow hosting for multiple programming languages: GitHub Package Registry Jfrog Artifactory Sonatype Nexus Monitor This part is especially tricky when working with serverless applications, as everything is split into small pieces. Its important to use monitoring tools that support this mode of work. Here are some tools that can handle serverless: Rookout Amazon CloudWatch Epsagon Lumigo NewRelic DataDog Summary The serverless era brings many transitions along with it like a codification of the entire pipeline and Git being the single source of truth. This doesnt mean that the same problems that we use to have like security, logging and more disappeared, you should continue addressing them and leveraging tools that enable you to focus on your business. ",
        "Lol at the image where before microservices, it was a single monolithic application.<p>FaaS has its use cases but this “serverless for every solution” or 100% serverless marketing is annoying and NOT customer centric (Amazon said they were earths most customer centric company).",
        "This post was disappointing when it first ran: I was expecting some content after the basic intro but then it’s just a couple of saved Google searches with no discussion or analysis. It would have been a lot more interesting if they’d discussed anything about the trade offs of the different services or what they liked about a particular combination."
      ],
      "relevant": "false"
    },
    {
      "id": 21196107,
      "title": "C++ Ecosystem: Compilers, IDEs, Tools, Testing",
      "search": [
        "C++ Ecosystem: Compilers, IDEs, Tools, Testing",
        "https://www.bfilipek.com/2019/10/cppecosystem.html",
        "Table of Contents Introduction Compilers GCC Microsoft Visual C++ Clang Intel C++ Compiler Build Tools & Package Managers Make Cmake Ninja Microsoft Build Engine (MSBuild) Conan, Vcpkg, Buckaroo Integrated Development Environments Sublime Text, Atom, And Visual Studio Code Vi/Vim & Emacs Clion Qt Creator C++Builder Visual Studio Xcode KDevelop Eclipse CDT IDE Cevelop Android Studio Oracle Studio Extra: Compiler Explorer & Online Tools Debugging & Testing GDB LLDB Debugging Tools For Windows Mozillas RR CATCH/CATCH2 BOOST.TEST GOOGLE TEST CUTE DocTest Mull Sanitizers Valgrind HeapTrack Dr. Memory Deleaker Summary & More Your Turn To write a professional C++ application, you not only need a basic text editor and a compiler. You require some more tooling. In this blog post, youll see a broad list of tools that make C++ programming possible: compilers, IDEs, debuggers and other. Last Update: 14th October 2019. Note: This is a blog post based on the White Paper created by Embarcadero, see the full paper here: C++ Ecosystem White Paper. Introduction The C++ computer programming language has become one of the most widely used modern programming languages. Software built with C++ is known for its performance and efficiency. C++ has been used to build numerous vastly popular core libraries, applications such as Microsoft Office, game engines such as Unreal, software tools like Adobe Photoshop, compilers like Clang, databases like MySQL, and even operating systems such as Windows across a wide variety of platforms as it continues to evolve and grow. Modern C++ is generally defined as C++ code that utilizes language features in C++11, C++14, and C++17 based compilers. These are language standards named after the year they were defined (2011, 2014 and 2017 respectively) and include a number of significant additions and enhancements to the original core language for powerful, highly performant, and bug-free code. Modern C++ has high-level features that support object-oriented programming, functional programming, generic programming, and low-level memory manipulation features. Big names in the computer industry such as Microsoft, Intel, the Free Software Foundation, and others have their modern C++ compilers. Companies such as Microsoft, The QT Company, JetBrains, and Embarcadero provide integrated development environments for writing code in modern C++. Popular libraries are available for C++ across a wide range of computer disciplines including Artificial Intelligence, Machine Learning, Robotics, Math, Scientific Computing, Audio Processing, and Image Processing. In this blog post, we are going to cover a number of these compilers, build tools, IDEs, libraries, frameworks, coding assistants, and much more that can support and enhance your development with modern C++. Lets get started! Compilers There are a number of popular compilers that support modern C++ including GCC/g++, MSVC (Microsoft Visual C++), Clang and Intel Compiler. Each compiler has varying support for each of the major operating systems with the open-source GCC/g++ originating in the late 1980s, Microsofts Visual C++ in the early 1990s, and Clang in the late 2000s. All four compilers support modern C++ up to at least C++17, but the source code licenses for each of them vary greatly. GCC GCC is a general-use compiler developed and maintained and regularly updated by the GCC Steering committee as part of the GNU Project. GCC describes a large growing family of compilers targeting many hardware platforms and several languages. While it mainly targets Unix-like platforms, Windows support is provided through the Cygwin or MinGW runtime libraries. GCC compiles modern C++ code up to C++17 with experimental support for some C++20 features. It also compiles with a variety of language extensions that build upon C++ standards. It is free and open-source (GPL3) with the GCC Runtime Library Exception. GCC has support from build tools such as CMake and Ninja and many IDEs such as CLion, Qt Creator, and Visual Studio Code. https://gcc.gnu.org/ https://gcc.gnu.org/projects/cxx-status.html Microsoft Visual C++ Microsoft Visual C++ (MSVC) is Microsofts compiler for their custom implementation of the C++ standard, known as Visual C++. It is regularly updated, and like GCC and Clang, supports modern C++ standards up to C++17 with experimental support for some C++20 features. MSVC is the primary method for building C++ applications in Microsofts own Visual Studio. It generally targets a number of architectures on Windows, Android, iOS, and Linux. Support for build tools and IDEs are limited but growing. CMake extensions are available in Visual Studio 2019. MSVC can be used with Visual Studio Code, with limited support from CLion and Qt Creator with additional extensions. MSVC is proprietary and available under a commercial license, but theres also a Community edition. https://en.wikipedia.org/wiki/Microsoft_Visual_C%2B%2B https://devblogs.microsoft.com/visualstudio/ https://visualstudio.microsoft.com/vs/community/ Clang Clang describes a large family of compilers for the C family of languages maintained and regularly developed as part of the LLVM project. Although it targets many popular architectures, it generally targets fewer platforms than GCC. The LLVM project defines Clang through key design principles - strict adherence to C++ standards (although support for GCC extensions is offered), modular design, and minimal modification to the source codes structure during compilation, to name a few. Like GCC, Clang compiles modern C++ code with support for the C++17 standard with experimental C++20 support. It is available under an open-source (Apache License Version 2.0) license. Clang also has widespread support from build tools such as CMake and Ninja and IDEs such as CLion, Qt Creator, Xcode, and others. https://clang.llvm.org/ https://clang.llvm.org/cxx_status.html Intel C++ Compiler Intel C++ Compiler can generate highly optimized code for various Intel CPUs (including Xeon, Core, and Atom processors). The compiler can seamlessly integrate with popular IDE like Visual Studio, GCC toolchain and others. It can leverage advanced instruction set (even AVX512) and generate parallel code (for example, thanks to OpenMP 5.0 support). Intel doesnt ship the compiler with the Standard Library implementation, so it uses the library you provide on your platform. The compiler is available as a part of Intel Parallel Studio XE or Intel System Studio. https://software.intel.com/en-us/c-compilers https://software.intel.com/en-us/articles/c17-features-supported-by-intel-c-compiler On top of compilers, you need an infrastructure that helps to build a whole application: build tools, pipelines and package managers. Make Make is a well-known build system widely used, especially in Unix and Unix-like operating systems. Make is typically used to build executable programs and libraries from source code. But the tool applies to any process that involves executing arbitrary commands to transform a source file to a target result. Make is not tight to any particular programming language. It automatically determines which source files has been changed and then performs the minimal build process to get the final output. It also helps with the installation of the results in the system https://www.gnu.org/software/make/ Cmake CMake is a cross-platform tool for managing your build process. Building, especially large apps and with dependent libraries, can be a very complex process, especially when you support multiple compilers; CMake abstracts this. You can define complex build processes in one common language and convert them to native build directives for any number of supported compilers, IDEs, and build tools, including Ninja (below.) There are versions of CMake available for Windows, macOS, and Linux. https://cmake.org/ Note: Heres a good answer about the differences between Make and Cmake: Difference between using Makefile and CMake to compile the code - Stack Overflow. Ninja The Ninja build system is used for the actual process of building apps and is similar to Make. It focuses on running as fast as possible by parallelizing builds. It is commonly used paired with CMake, which supports creating build files for the Ninja build system. The feature set of Ninja is intentionally kept minimal because the focus is on speed. https://ninja-build.org/ Microsoft Build Engine (MSBuild) MSBuild is a command-line based built platform available from Microsoft under an open-source (MIT) license. It can be used to automate the process of compiling and deploying projects. It is available standalone, packaged with Visual Studio, or from Github. The structure and function of MSBuild files is very similar to Make. MSBuild has an XML based file format and mainly has support for Windows but also macOS and Linux. IDEs such as CLion and C++Builder can integrate with MSBuild as well. https://docs.microsoft.com/en-us/visualstudio/msbuild/msbuild Conan, Vcpkg, Buckaroo Package managers such as Conan, vcpkg, Buckaroo and NIX have been gaining popularity in the C++ community. A package manager is a tool to install libraries or components. Conan is a decentralized open-source (MIT) package manager that supports multiple platforms and all build systems (such as CMake and MSBuild). Conan supports binaries with a goal of automating dependency management to save time in development and continuous integration. Microsofts vcpkg is open source under an MIT license and supports Windows, macOS, and Linux. Out of the box, it makes installed libraries available in Visual Studio, but it also supports CMake build recipes. It can build libs for every toolchain that can be fitted into CMake. Buckaroo is a lesser-known open-source package manager that can pull dependencies from GitHub, BitBucket, GitLab, and others. Buckaroo offers integrations for a number of IDEs including CLion, Visual Studio Code, XCode, and others. Here are the links for the mentioned package managers: https://conan.io/ https://github.com/microsoft/vcpkg https://buckaroo.pm/ Integrated Development Environments A host of editors and integrated development environments (IDEs) can be used for developing with modern C++. Text editors are typically lightweight, but are less featureful than a full IDE and so are used only for the process of writing code, not debugging or testing it. Full development requires other tools, and an IDE contains those and integrates into a cohesive, integrated development environment. Any number of text editors like Sublime Text, Atom, Visual Studio Code, vi/vim, and Emacs can be used for writing C++ code. However, some IDEs are specifically designed with modern C++ in mind like CLion, Qt Creator, and C++Builder, while IDEs like Xcode and Visual Studio also support other languages. You can also compare various IDE for C++ in this handy table on Wikipedia: Comparison of integrated development environments - C++ - Wikipedia Sublime Text, Atom, And Visual Studio Code The list below summarises a set of advanced source code editors that thanks to various plugins and extensions allow creating applications in almost all programming languages. Sublime Text is a commercial text editor with extended support for modern C++ available via plugins. Atom is an open-source (MIT license) text editor that supports modern C++ via packages with integrations available for debugging and compiling. Visual Studio Code is a popular open-source (MIT license) source-code editor from Microsoft. A wide variety of extensions are available that bring features such as debugging and code completion for modern C++ to Visual Studio Code. Sublime Text, Atom, and Visual Studio Code are all available for Windows, macOS, and Linux. Here are the links for the above tools: https://www.sublimetext.com/ https://atom.io/ https://code.visualstudio.com/ Vi/Vim & Emacs Vi/Vim and Emacs are free command-line based text editors that are mainly used on Linux but are also available for macOS and Windows. Modern C++ support can be added to Vi/Vim through the use of scripts while modern C++ support can be added to Emacs through the use of modules. https://www.vim.org/ https://www.gnu.org/software/emacs/ Clion CLion is a commercial IDE from JetBrains that supports modern C++. It can be used with build tools like CMake and Gradle, integrates with the GDB and LLDB debuggers, can be used with version control systems like Git, test libraries like Boost.Test, and various documentation tools. It has features such as code generation, refactoring, on the fly code analysis, symbol navigation, and more. https://www.jetbrains.com/clion/ Qt Creator Qt Creator is a (non)commercial IDE from The Qt Company which supports Windows, macOS, and Linux. Qt Creator has features such as a UI designer, syntax highlighting, auto-completion, and integration with a number of different modern C++ compilers like GCC and Clang. Qt Creator tightly integrates with the Qt library for rapidly building cross-platform applications. Additionally, it integrates with standard version control systems like Git, debuggers like GDB and LLDB, build systems like CMake, and can deploy cross-platform to iOS and Android devices. https://www.qt.io/ C++Builder C++Builder is a commercial IDE from Embarcadero Technologies which runs on Windows and supports modern C++. It features the award-winning Visual Component Library (VCL) for Windows development and FireMonkey (FMX) for cross-platform development for Windows, iOS and Android. The C++Builder compiler features an enhanced version of Clang, an integrated debugger, visual UI designer, database library, comprehensive RTL, and standard features like syntax highlighting, code completion, and refactoring. C++Builder has integrations for CMake, can be used with Ninja, and also MSBuild. https://www.embarcadero.com/products/cbuilder https://www.embarcadero.com/products/cbuilder/starter Visual Studio Visual C++ is a commercial Visual Studio IDE from Microsoft. Visual Studio integrates building, debugging, and testing within the IDE. It provides the Microsoft Foundation Class (MFC) library which gives access to the Win32 APIs. Visual Studio features a visual UI designer for certain platforms, comes with MSBuild, supports CMake, and provides standard features such as code completion, refactoring, and syntax highlighting. Additionally, Visual Studio supports a number of other programming languages, and the C++ side of it is focused on Windows, with other platforms slowly being added. https://visualstudio.microsoft.com/ Xcode Xcode is a multi-language IDE from Apple available only on macOS that supports modern C++. Xcode is proprietary but available for free from Apple. Xcode has an integrated debugger, supports version control systems like Git, features a Clang compiler, and utilizes libc++ as its standard library. It supports standard features such as syntax highlighting, code completion, and finally, Xcode supports external build systems like CMake and utilizes the LLDB debugger. https://developer.apple.com/xcode/ KDevelop KDevelop (its 0.1 version was released in 1998) is a cross-platform IDE for C, C++, Python, QML/JavaScript and PHP. This IDE is part of the KDE project, and is based on KDE Frameworks and Qt. The C/C++ backend uses Clang and LLVM. It has UI integration with several version control systems: Git, SVN, Bazaar and more, build process based on CMake, QMake or custom makefiles. Among many interesting features, its essential to mention advanced syntax colouring and Context-sensitive, semantic code completion. https://www.kdevelop.org/ https://www.kdevelop.org/features Eclipse CDT IDE The Eclipse C/C++ Development Toolkit (CDT) is a combination of the Eclipse IDE with a C++ toolchain (usually GNU - GCC). This IDE supports project creation and build management for various toolchains, like the standard make build. CDT IDE offers source navigation, various source knowledge tools, such as type hierarchy, call graph, include browser, macro definition browser, code editor with syntax highlighting, folding and hyperlink navigation, source code refactoring and code generation, visual debugging tools, including memory, registers, and disassembly viewers. https://www.eclipse.org/cdt/ Cevelop Cevelop is a powerful IDE based Eclipse CDT. Its main strength lies in the powerful refactoring and static analysis support for code modernization. In addition, it comes with unit testing and TDD support for the CUTE unit testing framework. Whats more, you can easily visualize your template instantiation/function overload resolution and optimize includes. https://www.cevelop.com/ Android Studio Android Studio is the official IDE for Googles Android operating system, built on JetBrains IntelliJ IDEA software and designed specifically for Android development. It is available for download on Windows, macOS and Linux based operating systems. It is a replacement for the Eclipse Android Development Tools (ADT) as the primary IDE for native Android application development. Android Studio focuses mainly on Kotlin but you can also write applications in C++. Oracle Studio Oracle Developer Studio is Oracle Corporations flagship software development product for the Solaris and Linux operating systems. It includes optimizing C, C++, and Fortran compilers, libraries, and performance analysis and debugging tools, for Solaris on SPARC and x86 platforms, and Linux on x86/x64 platforms, including multi-core systems. You can download Developer Studio at no charge but if you want the full support and patch updates, then you need a paid support contract. The C++ Compiler supports C++14. https://www.oracle.com/technetwork/server-storage/developerstudio/overview/index.html https://www.oracle.com/technetwork/server-storage/solarisstudio/features/compilers-2332272.html If you want to check some shorter code samples and you dont want to install the whole compiler/.IDE suite then we have lots of online tools that can make those tests super simple. Just open a web browser and put the code Compiler Explorer is a web-based tool that allows you to select from a wide variety of C++ compilers and different versions of the same compiler to test out your code. This allows developers to compare the generated code for specific C++ constructs among compilers, and test for correct behaviour. Clang, GCC, and MSVC are all there but also lesser-known compilers such as DJGPP, ELLCC, Intel C++, and others. https://godbolt.org/ Extra: Heres a list of handy online compilers that you can use: like Coliru, Wandbox, CppInsighs and more: https://arnemertz.github.io/online-compilers/ Debugging & Testing GDB GDB is a portable command-line based debugging platform that supports modern C++ and is available under an open-source license (GPL). A number of editors and IDEs like Visual Studio, Qt Creator, and CLion support integration with GDB. It can also be used to debug applications remotely where GDB is running on one device, and the application being debugged is running on another device. It supports a number of platforms including Windows, macOS, and Linux. https://www.gnu.org/software/gdb/ LLDB LLDB is an open-source debugging interface that supports modern C++ and integrates with the Clang compiler. It has a number of optional performance-enhancing features such as JIT but also supports debugging memory, multiple threads, and machine code analysis. It is built in C++. LLDB is the default debugger for Xcode and can be used with Visual Studio Code, CLion, and Qt Creator. It supports a number of platforms including Windows, macOS, and Linux. https://lldb.llvm.org/ Debugging Tools For Windows On Windows, you can use several debuggers, ranging from Visual Studio Debugger (integrated and one of the most user-friendly), WinDBG, CDB and several more. WinDbg is a multipurpose debugger for the Microsoft Windows Platform. It can be used to debug user-mode applications, device drivers, and the operating system itself in kernel mode. It has a graphical user interface (GUI) and is more powerful than Visual Studio Debugger. You can debug memory dumps obtained even from kernel drivers. One of the recent exciting features in Debugging on Windows is called Time Travel Debugging (Available in WinDBG Preview and also in Visual Studio Ultimate). It allows you to record the execution of the process and then replay the steps backwards or forwards. This flexibility enables us to easily tracks back the state that caused a bug. https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/ https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/time-travel-debugging-overview Mozillas RR RR is an advanced debugger that aims to replace GDB on Linux. It offers the full state recordings of the application so that you can replay the action backwards and forwards (similarly to Time Travel Debugging). The debugger is used to work with large applications like Chrome, OpenOffice or even Firefox code bases. https://rr-project.org/ CATCH/CATCH2 Catch2 is a cross-platform open-source (BSL-1.0) testing framework for modern C++. It is very lightweight because only a header file needs to be included. Unit tests can be tagged and run in groups. It supports both test-driven development and behaviour-driven development. Catch2 also easily integrates with CLion. https://github.com/catchorg/Catch2 BOOST.TEST Boost.Test is a feature-rich open-source (BSL-1.0) testing framework that utilizes modern C++ standards. It can be used to quickly detect errors, failures, and time outs through customizable logging and real-time monitoring. Tests can be grouped into suites, and the framework supports both small scale testing and large scale testing. https://github.com/boostorg/test GOOGLE TEST Google Test is Googles C++ testing and mocking framework, which is available under an open-source (BSD) license. Google test can be used on a broad range of platforms, including Linux, macOS, Windows, and others. It contains a unit testing framework, assertions, death tests, detects failures, handles parameterized tests, and creates XML test reports. https://github.com/google/googletest CUTE CUTE is a unit testing framework integrated into Cevelop, but it can also be used standalone. It spans C++ versions from c++98 to c++2a and is header-only. While not as popular as Google Test it is less macro-ridden and uses macros only, where no appropriate C++ feature is available. In addition, it features a mode that easily allows it to run on embedded platforms, by sidestepping some of the I/O formatting features. https://cute-test.com/ DocTest DocTest is a single-header unit testing framework. Available for C++11 up to C++20 and is easy to configure and works on probably all platforms. It offers regular TDD testing macros (also with subcases) as well as BDD-style test cases. http://bit.ly/doctest-docs https://github.com/onqtam/doctest Mull Mull is an LLVM-based tool for Mutation Testing with a strong focus on C and C++ languages. In general, it creates many variations of the input source code (using LLVM bytecode) and then checks it against the test cases. Thanks to this advanced testing technique, you can make your code more secure. https://github.com/mull-project/mull PDF: https://lowlevelbits.org/pdfs/Mull_Mutation_2018.pdf Sanitizers AddressSanitizer - https://clang.llvm.org/docs/AddressSanitizer.html (supported in Clang, GCC and XCode) UndefinedBehaviorSanitizer - https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html LeakSanitizer - https://clang.llvm.org/docs/LeakSanitizer.html Application Verifier for Windows - https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/application-verifier Sanitizers are relatively new tools that add extra instrumentation to your application (for example they replace new/malloc/delete calls) and can detect various runtime errors: leaks, use after delete, double free and many others. To improve your build pipeline, many guides advice to add sanitizers steps when doing tests. Most sanitizers come from the LLVM/Clang platform, but now they also work with GCC. Unfortunately not yet with Visual Studio (but you can try Application Verifier). Valgrind Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. When you run a program through Valgrind its run on a virtual machine that emulates your host environment. Having that abstraction the tools can leverage various information about the source code and its execution. http://valgrind.org/ http://valgrind.org/info/about.html http://valgrind.org/docs/manual/quick-start.html HeapTrack HeapTrack is a FOSS project and a heap memory profiler for Linux. It traces all memory allocations and annotates these events with stack traces. The tool has two forms the command line version that grabs the data, and then the UI part that you can use to read and analyze the results. This tool is comparable to Valgrinds massif; its easier to use and should be faster to load and analyze for large projects. https://github.com/KDE/heaptrack Dr. Memory Dr. Memory is an LGPL licenced tool that allows you to monitor and intensify memory -related errors for binaries on Windows, Linux, Mac, Android. Its based on the DynamoRIO dynamic instrumentation tool platform. With the tool, you can find errors like double frees, memory leaks, handle leaks (on Windows), GDI issues, access to uninitialized memory or even errors in multithreading memory scenarios. http://drmemory.org/ https://github.com/DynamoRIO/drmemory Deleaker The primary role of Deleaker is to find leaks in your native applications. It supports Visual Studio (since 2008 till the latest 2019 version), Delphi/C++ Builder, Qt Creator, CLion (soon!). Can be used as an extension in Visual Studio or as a standalone application. Deleaker tracks leaks in C/C++ applications (Native and CLR), plus .NET code. Memory (new/delete, malloc), GDI objects, User32 objects, Handles, File views, Fibres, Critical Sections, and even more. It gathers full call stack, ability to take snapshots, compare them, view source files related to allocation. https://www.deleaker.com/ https://www.deleaker.com/docs/deleaker/tutorial.html Summary & More I hope that with the above list, you get a useful overview of the tools that are essential for C++ development. If you want to read more about other ecosystem elements: libraries, frameworks, and other tools, then please see the full report from Embarcadero: C++ Ecosystem White Paper (Its a nice looking pdf, with more than 20 pages of content!) You might check this Resource for a super long list of tools, libs, frameworks that enhance C++ development: https://github.com/fffaraz/awesome-cpp Your Turn What are your favourite tools that you use when writing C++ apps? ",
        "CLion is an amazing tool -- I've purchased licenses for my personal self in the past, but my employer pays for it these days.<p>My problem is they've done a terrible job of making it scale up to large code bases. I work on the chromium tree -- CLion is completely useless on it.  I have a dual 24-core xeon with 128GB of RAM and SSD and I've given it a wackload of memory, and it becomes completely inoperable, freezing all over the place.<p>Awful because I have such muscle memory for the JetBrains tools, and such a fondness for them.<p>I've gone back to using Emacs, but now with Eclim. I just couldn't get into VSCode.",
        "I had to use Qt as the UI lib for a project, it made me discover that QtCreator was actually not a Qt-only tool but a very good lightweight and generic C++ IDE.<p>That's my choice now. I need something that can navigate easily in a code base, I don't really like learning all the oddities around emacs and vim (even though I am a bit competent at vim) and I don't see what is so bad in using a mouse.<p>At first I thought annoying to have to manually edit the .includes and .config to add the includes and the macro I needed in our complex, hard-to-parse CMake based project, but now I really enjoy the freedom it gives."
      ],
      "relevant": "false"
    },
    {
      "id": 21511411,
      "title": "Building a Developer Tools Business",
      "search": [
        "Building a Developer Tools Business",
        "https://manifold.co/blog/founders-guide-developer-tools-sell-to-engineers",
        "Proudly powered by LiteSpeed Web ServerPlease be advised that LiteSpeed Technologies Inc. is not a web hosting company and, as such, has no control over content found on this site. ",
        "I would also recommend dev tools founders (really any founder) read GitLab’s company handbook at <a href=\"https://about.gitlab.com/handbook/\" rel=\"nofollow\">https://about.gitlab.com/handbook/</a>, especially the sales and marketing sections. It’s awesome how public they are about how they operate. You’ll learn a lot.<p>We (Sourcegraph) have started our own company handbook (<a href=\"https://about.sourcegraph.com/handbook\" rel=\"nofollow\">https://about.sourcegraph.com/handbook</a>), inspired by GitLab. So far it has helped new teammates onboard more quickly and (along with other efforts to diligently document internal practices) helped us work efficiently on a growing team spanning many timezones.",
        "I find that the key thing to overcome when selling to developers is the extreme reluctance to spend money buying something that they can build themselves.  You see,<p>1.) Developers love to build things.<p>2.) Developers hate spending money.<p>3.) Developers undervalue their time.<p>If your product looks like it would have been fun to build, you'll lose the entire \"insufficiently supervised developer\" demographic.  Those guys will happily spend tens of thousands of dollars of billable hours implementing an in-house version of your thing to avoid the possibility of outgrowing your Free Tier.<p>I've seen this play out with S3stat customers (which costs $10/month, or three minutes twenty seconds of fully loaded engineer cost), where somebody will spend a week building an in-house version of the service and standing up a server to run it.  Nicely done.  You'll break even on your investment in 21 years.<p>I've had moderate success with my latest API product pointing to a \"Boss Page\" that outlines things like build-vs-buy costs, and why you really would be better off paying us for this thing rather than dedicating an in-house guy to building and maintaining it.<p>It's a tough one."
      ],
      "relevant": "false"
    },
    {
      "id": 19311118,
      "title": "Coder (Visual Studio Code in browser) goes open source",
      "search": [
        "Coder (Visual Studio Code in browser) goes open source",
        "https://coder.com",
        "From the Developers ofcode-serverThe developer workspace platformCentralize the creation and management of cloud developer workspacesWorks with Azure, GCP, AWS, OpenShift, and anywhere you run KubernetesDevelop in VS Code, Jupyter, RStudio, IntelliJ, PyCharm, and any JetBrains IDEThe problems with development running on local machinesOnboarding delays from environment setupConfiguration drift as the project evolvesSource code on insecure endpointsLimited compute powerMove development to your cloudCoder handles the orchestration of new conformant and consistent workspaces using source-controlled Dockerfiles and workspace templates. Empower developers and data scientists to spin up self-serve workspaces that just work.Keep your workflowCoder works with the tools that you work with. No need to change your preferred editor, CI tooling, or version control systemwith support for Docker in Docker, too.Run VS Code, Jupyter Notebook, RStudio, IntelliJ, PyCharm, & other IDEs and editorsRun VS Code locally via SSH connection to your Coder workspace (local support for JetBrains IDEs coming soon)Version control with GitHub, GitLab, & BitbucketPersonalize your workspaces to fit your flowSpeed up builds and testsUse the power and scale of the cloud to offload the burden of slow builds and tests from your local machine.Deploy workspaces with the CPUs and memory you needAccommodate bursty workloads by utilizing the clusters idle CPUs and memoryHarness the parallelism of GPUs for deep learning and other intensive workloadsDevelop with any deviceAccess your workspace from anywhere with the same snappy development experience you expect from a local IDE.Work from home, the office, or wherever you areCode using any device, even an iPadOnboard to new projects from wherever, wheneverOnboarding to a new project can take days away from your productivity and hinders collaboration. Get on the same page and stay on the same page faster.Start a new project with all the required tools and dependencies in minutesCollaborate across teams with easeWork with your team from anywhere in the world without latencySecure your data and source codeSource code and data can remain secured on the cluster or in authorized repositories not sitting on a workstation or laptop.Get the security benefits of VDI with a better developer experienceReduce the risk of source code leaksIdeal for zero-trust networks and air-gapped environmentsCasestudyKazoo reduces onboarding time with Coder\"New hires have a shorter onboarding experience because theyre just spinning up a Coder workspace instead of installing locally and having to worry about whether all the dependencies are up to date.\"Joe MainwaringDirector of Infrastructure, KazooRead case studyFAQIs Coder priced per developer or environment?Is Coder SaaS?What can I expect my infrastructure costs to be?How can I procure Coder for my government agency?What does the price of Coder include?What type of support is available?Can I add users at any time?Get started with Coder todayOur commitment to open sourceLearn more about our projects and our commitment to the open-source community.Code-server: the heart of CoderCode-server is the primary open source project we maintain. It allows developers to use a browser to access remote dev environments running VS Code. Coder builds upon the success of code-server and adds features designed for enterprise teams including support for additional IDEs and advanced security features.CookiesWe use cookies to make your experience better. ",
        "Has anyone tried this and compared it to <a href=\"https://www.theia-ide.org/\" rel=\"nofollow\">https://www.theia-ide.org/</a>?<p>First thing off the bat I notice is that Coder looks harder to deploy or try out, Theia was super easy, on the landing page they had a docker one liner:<p>docker run -it -p 3000:3000 -v \"$(pwd):/home/project:cached\" theiaide/theia:next",
        "I used to work at a place where we all used the enterprise server version of RStudio, which also runs in a browser.<p>There was a lot of good thing about that setup. Nobody could walk home with code, and no code was lost on somebody laptop.<p>Execution happened on a server, much more powerful than any dev machine."
      ],
      "relevant": "false"
    }
  ]
}
