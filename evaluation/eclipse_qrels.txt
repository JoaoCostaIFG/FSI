{
        "story_id":19825059,
        "story_id":20613541,
        "story_id":21840414,
        "story_id":19572994,
        "story_id":19123157,
        "story_id":21207425,
      {
        "story_id":19016997,
        "story_author":"theBashShell",
        "story_descendants":58,
        "story_score":115,
        "story_time":"2019-01-28T12:44:39Z",
        "story_title":"Apache Flink",
        "search":["Apache Flink",
          "https://github.com/apache/flink",
          "Apache Flink is an open source stream processing framework with powerful stream- and batch-processing capabilities. Learn more about Flink at https://flink.apache.org/ Features A streaming-first runtime that supports both batch processing and data streaming programs Elegant and fluent APIs in Java and Scala A runtime that supports very high throughput and low event latency at the same time Support for event time and out-of-order processing in the DataStream API, based on the Dataflow Model Flexible windowing (time, count, sessions, custom triggers) across different time semantics (event time, processing time) Fault-tolerance with exactly-once processing guarantees Natural back-pressure in streaming programs Libraries for Graph processing (batch), Machine Learning (batch), and Complex Event Processing (streaming) Built-in support for iterative programs (BSP) in the DataSet (batch) API Custom memory management for efficient and robust switching between in-memory and out-of-core data processing algorithms Compatibility layers for Apache Hadoop MapReduce Integration with YARN, HDFS, HBase, and other components of the Apache Hadoop ecosystem Streaming Example case class WordWithCount(word: String, count: Long) val text = env.socketTextStream(host, port, '\\n') val windowCounts = text.flatMap { w => w.split(\"\\\\s\") } .map { w => WordWithCount(w, 1) } .keyBy(\"word\") .window(TumblingProcessingTimeWindow.of(Time.seconds(5))) .sum(\"count\") windowCounts.print() Batch Example case class WordWithCount(word: String, count: Long) val text = env.readTextFile(path) val counts = text.flatMap { w => w.split(\"\\\\s\") } .map { w => WordWithCount(w, 1) } .groupBy(\"word\") .sum(\"count\") counts.writeAsCsv(outputPath) Building Apache Flink from Source Prerequisites for building Flink: Unix-like environment (we use Linux, Mac OS X, Cygwin, WSL) Git Maven (we recommend version 3.2.5 and require at least 3.1.1) Java 8 or 11 (Java 9 or 10 may work) git clone https://github.com/apache/flink.git cd flink mvn clean package -DskipTests # this will take up to 10 minutes Flink is now installed in build-target. NOTE: Maven 3.3.x can build Flink, but will not properly shade away certain dependencies. Maven 3.1.1 creates the libraries properly. To build unit tests with Java 8, use Java 8u51 or above to prevent failures in unit tests that use the PowerMock runner. Developing Flink The Flink committers use IntelliJ IDEA to develop the Flink codebase. We recommend IntelliJ IDEA for developing projects that involve Scala code. Minimal requirements for an IDE are: Support for Java and Scala (also mixed projects) Support for Maven with Java and Scala IntelliJ IDEA The IntelliJ IDE supports Maven out of the box and offers a plugin for Scala development. IntelliJ download: https://www.jetbrains.com/idea/ IntelliJ Scala Plugin: https://plugins.jetbrains.com/plugin/?id=1347 Check out our Setting up IntelliJ guide for details. Eclipse Scala IDE NOTE: From our experience, this setup does not work with Flink due to deficiencies of the old Eclipse version bundled with Scala IDE 3.0.3 or due to version incompatibilities with the bundled Scala version in Scala IDE 4.4.1. We recommend to use IntelliJ instead (see above) Support Dont hesitate to ask! Contact the developers and community on the mailing lists if you need any help. Open an issue if you found a bug in Flink. Documentation The documentation of Apache Flink is located on the website: https://flink.apache.org or in the docs/ directory of the source code. Fork and Contribute This is an active open-source project. We are always open to people who want to use the system or contribute to it. Contact us if you are looking for implementation tasks that fit your skills. This article describes how to contribute to Apache Flink. About Apache Flink is an open source project of The Apache Software Foundation (ASF). The Apache Flink project originated from the Stratosphere research project. "],
        "story_type":"Normal",
        "url_raw":"https://github.com/apache/flink",
        "url_text":"Apache Flink is an open source stream processing framework with powerful stream- and batch-processing capabilities. Learn more about Flink at https://flink.apache.org/ Features A streaming-first runtime that supports both batch processing and data streaming programs Elegant and fluent APIs in Java and Scala A runtime that supports very high throughput and low event latency at the same time Support for event time and out-of-order processing in the DataStream API, based on the Dataflow Model Flexible windowing (time, count, sessions, custom triggers) across different time semantics (event time, processing time) Fault-tolerance with exactly-once processing guarantees Natural back-pressure in streaming programs Libraries for Graph processing (batch), Machine Learning (batch), and Complex Event Processing (streaming) Built-in support for iterative programs (BSP) in the DataSet (batch) API Custom memory management for efficient and robust switching between in-memory and out-of-core data processing algorithms Compatibility layers for Apache Hadoop MapReduce Integration with YARN, HDFS, HBase, and other components of the Apache Hadoop ecosystem Streaming Example case class WordWithCount(word: String, count: Long) val text = env.socketTextStream(host, port, '\\n') val windowCounts = text.flatMap { w => w.split(\"\\\\s\") } .map { w => WordWithCount(w, 1) } .keyBy(\"word\") .window(TumblingProcessingTimeWindow.of(Time.seconds(5))) .sum(\"count\") windowCounts.print() Batch Example case class WordWithCount(word: String, count: Long) val text = env.readTextFile(path) val counts = text.flatMap { w => w.split(\"\\\\s\") } .map { w => WordWithCount(w, 1) } .groupBy(\"word\") .sum(\"count\") counts.writeAsCsv(outputPath) Building Apache Flink from Source Prerequisites for building Flink: Unix-like environment (we use Linux, Mac OS X, Cygwin, WSL) Git Maven (we recommend version 3.2.5 and require at least 3.1.1) Java 8 or 11 (Java 9 or 10 may work) git clone https://github.com/apache/flink.git cd flink mvn clean package -DskipTests # this will take up to 10 minutes Flink is now installed in build-target. NOTE: Maven 3.3.x can build Flink, but will not properly shade away certain dependencies. Maven 3.1.1 creates the libraries properly. To build unit tests with Java 8, use Java 8u51 or above to prevent failures in unit tests that use the PowerMock runner. Developing Flink The Flink committers use IntelliJ IDEA to develop the Flink codebase. We recommend IntelliJ IDEA for developing projects that involve Scala code. Minimal requirements for an IDE are: Support for Java and Scala (also mixed projects) Support for Maven with Java and Scala IntelliJ IDEA The IntelliJ IDE supports Maven out of the box and offers a plugin for Scala development. IntelliJ download: https://www.jetbrains.com/idea/ IntelliJ Scala Plugin: https://plugins.jetbrains.com/plugin/?id=1347 Check out our Setting up IntelliJ guide for details. Eclipse Scala IDE NOTE: From our experience, this setup does not work with Flink due to deficiencies of the old Eclipse version bundled with Scala IDE 3.0.3 or due to version incompatibilities with the bundled Scala version in Scala IDE 4.4.1. We recommend to use IntelliJ instead (see above) Support Dont hesitate to ask! Contact the developers and community on the mailing lists if you need any help. Open an issue if you found a bug in Flink. Documentation The documentation of Apache Flink is located on the website: https://flink.apache.org or in the docs/ directory of the source code. Fork and Contribute This is an active open-source project. We are always open to people who want to use the system or contribute to it. Contact us if you are looking for implementation tasks that fit your skills. This article describes how to contribute to Apache Flink. About Apache Flink is an open source project of The Apache Software Foundation (ASF). The Apache Flink project originated from the Stratosphere research project. ",
        "comments.comment_id":[19033235,
          19035612],
        "comments.comment_author":["meritt",
          "lenticular"],
        "comments.comment_descendants":[9,
          3],
        "comments.comment_time":["2019-01-30T06:56:19Z",
          "2019-01-30T15:09:33Z"],
        "comments.comment_text":["Apache Flink, Flume, Storm, Samza, Spark, Apex, and Kafka all do basically the same thing. I feel like this is a bit overboard. And this is before we talk about the non-Apache stream-processing frameworks out there.<p>* Apache Flink is an open source stream processing framework<p>* Apache Flume is a distributed, reliable, and available software for efficiently collecting, aggregating, and moving large amounts of log data.<p>* Apache Storm is a distributed stream processing computation framework<p>* Apache Samza is an open-source near-realtime, asynchronous computational framework for stream processing<p>* Apache Spark is an open-source distributed general-purpose cluster-computing framework.<p>* Apache Apex is a YARN-native platform that unifies stream and batch processing.<p>* Apache Kafka is an open-source stream-processing software platform",
          "Controversial opinion here, but all of these distributed streaming architectures are massively overused. They certainly have their place, but you probably don't need them. I see it all the time with ML work. You wind up using a cluster to overcome the memory inefficiency of Spark, when you could have just used a single machine. For example, I've done huge graph clustering models on a single machine just by being smart about memory consumption. It would have taken an enormous and expensive Spark cluster."],
        "id":"3dd1b271-d571-4303-ae74-283bd369de4f",
        "_version_":1718441008779231232},
      {
        "story_id":20759879,
        "story_author":"thekhatribharat",
        "story_descendants":12,
        "story_score":14,
        "story_time":"2019-08-21T18:17:16Z",
        "story_title":"Ask HN: Recommendations for full-blown polyglot web IDEs",
        "search":["Ask HN: Recommendations for full-blown polyglot web IDEs",
          "I'm looking for full-blown web IDE solutions, with the following features:<p>1) Polyglot: Support for multiple programming languages and preferably web frameworks (using project templates, base container images, etc.), build and test tools<p>2) Embedded Shell: CLI shell to interact with the execution environment<p>Some options I'm aware of include:<p>1) Eclipse Che on OpenShift (https://che.openshift.io)<p>2) Repl.it (https://repl.it) - This isn't a full-blown IDE though"],
        "story_text":"I'm looking for full-blown web IDE solutions, with the following features:<p>1) Polyglot: Support for multiple programming languages and preferably web frameworks (using project templates, base container images, etc.), build and test tools<p>2) Embedded Shell: CLI shell to interact with the execution environment<p>Some options I'm aware of include:<p>1) Eclipse Che on OpenShift (https://che.openshift.io)<p>2) Repl.it (https://repl.it) - This isn't a full-blown IDE though",
        "story_type":"AskHN",
        "comments.comment_id":[20760856,
          20761128],
        "comments.comment_author":["ruffrey",
          "kpsychwave"],
        "comments.comment_descendants":[1,
          0],
        "comments.comment_time":["2019-08-21T19:41:20Z",
          "2019-08-21T20:06:34Z"],
        "comments.comment_text":["JetBrains\n<a href=\"https://www.jetbrains.com\" rel=\"nofollow\">https://www.jetbrains.com</a>",
          "You can self-host the pre-AWS Cloud9 IDE on a VPS: <a href=\"https://hub.docker.com/r/sapk/cloud9/\" rel=\"nofollow\">https://hub.docker.com/r/sapk/cloud9/</a>"],
        "id":"da2568b1-036b-4e31-a6eb-d8771a30db2f",
        "_version_":1718441053600612352},
      {
        "story_id":21196107,
        "story_author":"joebaf",
        "story_descendants":145,
        "story_score":194,
        "story_time":"2019-10-08T19:25:11Z",
        "story_title":"C++ Ecosystem: Compilers, IDEs, Tools, Testing",
        "search":["C++ Ecosystem: Compilers, IDEs, Tools, Testing",
          "https://www.bfilipek.com/2019/10/cppecosystem.html",
          "Table of Contents Introduction Compilers GCC Microsoft Visual C++ Clang Intel C++ Compiler Build Tools & Package Managers Make Cmake Ninja Microsoft Build Engine (MSBuild) Conan, Vcpkg, Buckaroo Integrated Development Environments Sublime Text, Atom, And Visual Studio Code Vi/Vim & Emacs Clion Qt Creator C++Builder Visual Studio Xcode KDevelop Eclipse CDT IDE Cevelop Android Studio Oracle Studio Extra: Compiler Explorer & Online Tools Debugging & Testing GDB LLDB Debugging Tools For Windows Mozillas RR CATCH/CATCH2 BOOST.TEST GOOGLE TEST CUTE DocTest Mull Sanitizers Valgrind HeapTrack Dr. Memory Deleaker Summary & More Your Turn To write a professional C++ application, you not only need a basic text editor and a compiler. You require some more tooling. In this blog post, youll see a broad list of tools that make C++ programming possible: compilers, IDEs, debuggers and other. Last Update: 14th October 2019. Note: This is a blog post based on the White Paper created by Embarcadero, see the full paper here: C++ Ecosystem White Paper. Introduction The C++ computer programming language has become one of the most widely used modern programming languages. Software built with C++ is known for its performance and efficiency. C++ has been used to build numerous vastly popular core libraries, applications such as Microsoft Office, game engines such as Unreal, software tools like Adobe Photoshop, compilers like Clang, databases like MySQL, and even operating systems such as Windows across a wide variety of platforms as it continues to evolve and grow. Modern C++ is generally defined as C++ code that utilizes language features in C++11, C++14, and C++17 based compilers. These are language standards named after the year they were defined (2011, 2014 and 2017 respectively) and include a number of significant additions and enhancements to the original core language for powerful, highly performant, and bug-free code. Modern C++ has high-level features that support object-oriented programming, functional programming, generic programming, and low-level memory manipulation features. Big names in the computer industry such as Microsoft, Intel, the Free Software Foundation, and others have their modern C++ compilers. Companies such as Microsoft, The QT Company, JetBrains, and Embarcadero provide integrated development environments for writing code in modern C++. Popular libraries are available for C++ across a wide range of computer disciplines including Artificial Intelligence, Machine Learning, Robotics, Math, Scientific Computing, Audio Processing, and Image Processing. In this blog post, we are going to cover a number of these compilers, build tools, IDEs, libraries, frameworks, coding assistants, and much more that can support and enhance your development with modern C++. Lets get started! Compilers There are a number of popular compilers that support modern C++ including GCC/g++, MSVC (Microsoft Visual C++), Clang and Intel Compiler. Each compiler has varying support for each of the major operating systems with the open-source GCC/g++ originating in the late 1980s, Microsofts Visual C++ in the early 1990s, and Clang in the late 2000s. All four compilers support modern C++ up to at least C++17, but the source code licenses for each of them vary greatly. GCC GCC is a general-use compiler developed and maintained and regularly updated by the GCC Steering committee as part of the GNU Project. GCC describes a large growing family of compilers targeting many hardware platforms and several languages. While it mainly targets Unix-like platforms, Windows support is provided through the Cygwin or MinGW runtime libraries. GCC compiles modern C++ code up to C++17 with experimental support for some C++20 features. It also compiles with a variety of language extensions that build upon C++ standards. It is free and open-source (GPL3) with the GCC Runtime Library Exception. GCC has support from build tools such as CMake and Ninja and many IDEs such as CLion, Qt Creator, and Visual Studio Code. https://gcc.gnu.org/ https://gcc.gnu.org/projects/cxx-status.html Microsoft Visual C++ Microsoft Visual C++ (MSVC) is Microsofts compiler for their custom implementation of the C++ standard, known as Visual C++. It is regularly updated, and like GCC and Clang, supports modern C++ standards up to C++17 with experimental support for some C++20 features. MSVC is the primary method for building C++ applications in Microsofts own Visual Studio. It generally targets a number of architectures on Windows, Android, iOS, and Linux. Support for build tools and IDEs are limited but growing. CMake extensions are available in Visual Studio 2019. MSVC can be used with Visual Studio Code, with limited support from CLion and Qt Creator with additional extensions. MSVC is proprietary and available under a commercial license, but theres also a Community edition. https://en.wikipedia.org/wiki/Microsoft_Visual_C%2B%2B https://devblogs.microsoft.com/visualstudio/ https://visualstudio.microsoft.com/vs/community/ Clang Clang describes a large family of compilers for the C family of languages maintained and regularly developed as part of the LLVM project. Although it targets many popular architectures, it generally targets fewer platforms than GCC. The LLVM project defines Clang through key design principles - strict adherence to C++ standards (although support for GCC extensions is offered), modular design, and minimal modification to the source codes structure during compilation, to name a few. Like GCC, Clang compiles modern C++ code with support for the C++17 standard with experimental C++20 support. It is available under an open-source (Apache License Version 2.0) license. Clang also has widespread support from build tools such as CMake and Ninja and IDEs such as CLion, Qt Creator, Xcode, and others. https://clang.llvm.org/ https://clang.llvm.org/cxx_status.html Intel C++ Compiler Intel C++ Compiler can generate highly optimized code for various Intel CPUs (including Xeon, Core, and Atom processors). The compiler can seamlessly integrate with popular IDE like Visual Studio, GCC toolchain and others. It can leverage advanced instruction set (even AVX512) and generate parallel code (for example, thanks to OpenMP 5.0 support). Intel doesnt ship the compiler with the Standard Library implementation, so it uses the library you provide on your platform. The compiler is available as a part of Intel Parallel Studio XE or Intel System Studio. https://software.intel.com/en-us/c-compilers https://software.intel.com/en-us/articles/c17-features-supported-by-intel-c-compiler On top of compilers, you need an infrastructure that helps to build a whole application: build tools, pipelines and package managers. Make Make is a well-known build system widely used, especially in Unix and Unix-like operating systems. Make is typically used to build executable programs and libraries from source code. But the tool applies to any process that involves executing arbitrary commands to transform a source file to a target result. Make is not tight to any particular programming language. It automatically determines which source files has been changed and then performs the minimal build process to get the final output. It also helps with the installation of the results in the system https://www.gnu.org/software/make/ Cmake CMake is a cross-platform tool for managing your build process. Building, especially large apps and with dependent libraries, can be a very complex process, especially when you support multiple compilers; CMake abstracts this. You can define complex build processes in one common language and convert them to native build directives for any number of supported compilers, IDEs, and build tools, including Ninja (below.) There are versions of CMake available for Windows, macOS, and Linux. https://cmake.org/ Note: Heres a good answer about the differences between Make and Cmake: Difference between using Makefile and CMake to compile the code - Stack Overflow. Ninja The Ninja build system is used for the actual process of building apps and is similar to Make. It focuses on running as fast as possible by parallelizing builds. It is commonly used paired with CMake, which supports creating build files for the Ninja build system. The feature set of Ninja is intentionally kept minimal because the focus is on speed. https://ninja-build.org/ Microsoft Build Engine (MSBuild) MSBuild is a command-line based built platform available from Microsoft under an open-source (MIT) license. It can be used to automate the process of compiling and deploying projects. It is available standalone, packaged with Visual Studio, or from Github. The structure and function of MSBuild files is very similar to Make. MSBuild has an XML based file format and mainly has support for Windows but also macOS and Linux. IDEs such as CLion and C++Builder can integrate with MSBuild as well. https://docs.microsoft.com/en-us/visualstudio/msbuild/msbuild Conan, Vcpkg, Buckaroo Package managers such as Conan, vcpkg, Buckaroo and NIX have been gaining popularity in the C++ community. A package manager is a tool to install libraries or components. Conan is a decentralized open-source (MIT) package manager that supports multiple platforms and all build systems (such as CMake and MSBuild). Conan supports binaries with a goal of automating dependency management to save time in development and continuous integration. Microsofts vcpkg is open source under an MIT license and supports Windows, macOS, and Linux. Out of the box, it makes installed libraries available in Visual Studio, but it also supports CMake build recipes. It can build libs for every toolchain that can be fitted into CMake. Buckaroo is a lesser-known open-source package manager that can pull dependencies from GitHub, BitBucket, GitLab, and others. Buckaroo offers integrations for a number of IDEs including CLion, Visual Studio Code, XCode, and others. Here are the links for the mentioned package managers: https://conan.io/ https://github.com/microsoft/vcpkg https://buckaroo.pm/ Integrated Development Environments A host of editors and integrated development environments (IDEs) can be used for developing with modern C++. Text editors are typically lightweight, but are less featureful than a full IDE and so are used only for the process of writing code, not debugging or testing it. Full development requires other tools, and an IDE contains those and integrates into a cohesive, integrated development environment. Any number of text editors like Sublime Text, Atom, Visual Studio Code, vi/vim, and Emacs can be used for writing C++ code. However, some IDEs are specifically designed with modern C++ in mind like CLion, Qt Creator, and C++Builder, while IDEs like Xcode and Visual Studio also support other languages. You can also compare various IDE for C++ in this handy table on Wikipedia: Comparison of integrated development environments - C++ - Wikipedia Sublime Text, Atom, And Visual Studio Code The list below summarises a set of advanced source code editors that thanks to various plugins and extensions allow creating applications in almost all programming languages. Sublime Text is a commercial text editor with extended support for modern C++ available via plugins. Atom is an open-source (MIT license) text editor that supports modern C++ via packages with integrations available for debugging and compiling. Visual Studio Code is a popular open-source (MIT license) source-code editor from Microsoft. A wide variety of extensions are available that bring features such as debugging and code completion for modern C++ to Visual Studio Code. Sublime Text, Atom, and Visual Studio Code are all available for Windows, macOS, and Linux. Here are the links for the above tools: https://www.sublimetext.com/ https://atom.io/ https://code.visualstudio.com/ Vi/Vim & Emacs Vi/Vim and Emacs are free command-line based text editors that are mainly used on Linux but are also available for macOS and Windows. Modern C++ support can be added to Vi/Vim through the use of scripts while modern C++ support can be added to Emacs through the use of modules. https://www.vim.org/ https://www.gnu.org/software/emacs/ Clion CLion is a commercial IDE from JetBrains that supports modern C++. It can be used with build tools like CMake and Gradle, integrates with the GDB and LLDB debuggers, can be used with version control systems like Git, test libraries like Boost.Test, and various documentation tools. It has features such as code generation, refactoring, on the fly code analysis, symbol navigation, and more. https://www.jetbrains.com/clion/ Qt Creator Qt Creator is a (non)commercial IDE from The Qt Company which supports Windows, macOS, and Linux. Qt Creator has features such as a UI designer, syntax highlighting, auto-completion, and integration with a number of different modern C++ compilers like GCC and Clang. Qt Creator tightly integrates with the Qt library for rapidly building cross-platform applications. Additionally, it integrates with standard version control systems like Git, debuggers like GDB and LLDB, build systems like CMake, and can deploy cross-platform to iOS and Android devices. https://www.qt.io/ C++Builder C++Builder is a commercial IDE from Embarcadero Technologies which runs on Windows and supports modern C++. It features the award-winning Visual Component Library (VCL) for Windows development and FireMonkey (FMX) for cross-platform development for Windows, iOS and Android. The C++Builder compiler features an enhanced version of Clang, an integrated debugger, visual UI designer, database library, comprehensive RTL, and standard features like syntax highlighting, code completion, and refactoring. C++Builder has integrations for CMake, can be used with Ninja, and also MSBuild. https://www.embarcadero.com/products/cbuilder https://www.embarcadero.com/products/cbuilder/starter Visual Studio Visual C++ is a commercial Visual Studio IDE from Microsoft. Visual Studio integrates building, debugging, and testing within the IDE. It provides the Microsoft Foundation Class (MFC) library which gives access to the Win32 APIs. Visual Studio features a visual UI designer for certain platforms, comes with MSBuild, supports CMake, and provides standard features such as code completion, refactoring, and syntax highlighting. Additionally, Visual Studio supports a number of other programming languages, and the C++ side of it is focused on Windows, with other platforms slowly being added. https://visualstudio.microsoft.com/ Xcode Xcode is a multi-language IDE from Apple available only on macOS that supports modern C++. Xcode is proprietary but available for free from Apple. Xcode has an integrated debugger, supports version control systems like Git, features a Clang compiler, and utilizes libc++ as its standard library. It supports standard features such as syntax highlighting, code completion, and finally, Xcode supports external build systems like CMake and utilizes the LLDB debugger. https://developer.apple.com/xcode/ KDevelop KDevelop (its 0.1 version was released in 1998) is a cross-platform IDE for C, C++, Python, QML/JavaScript and PHP. This IDE is part of the KDE project, and is based on KDE Frameworks and Qt. The C/C++ backend uses Clang and LLVM. It has UI integration with several version control systems: Git, SVN, Bazaar and more, build process based on CMake, QMake or custom makefiles. Among many interesting features, its essential to mention advanced syntax colouring and Context-sensitive, semantic code completion. https://www.kdevelop.org/ https://www.kdevelop.org/features Eclipse CDT IDE The Eclipse C/C++ Development Toolkit (CDT) is a combination of the Eclipse IDE with a C++ toolchain (usually GNU - GCC). This IDE supports project creation and build management for various toolchains, like the standard make build. CDT IDE offers source navigation, various source knowledge tools, such as type hierarchy, call graph, include browser, macro definition browser, code editor with syntax highlighting, folding and hyperlink navigation, source code refactoring and code generation, visual debugging tools, including memory, registers, and disassembly viewers. https://www.eclipse.org/cdt/ Cevelop Cevelop is a powerful IDE based Eclipse CDT. Its main strength lies in the powerful refactoring and static analysis support for code modernization. In addition, it comes with unit testing and TDD support for the CUTE unit testing framework. Whats more, you can easily visualize your template instantiation/function overload resolution and optimize includes. https://www.cevelop.com/ Android Studio Android Studio is the official IDE for Googles Android operating system, built on JetBrains IntelliJ IDEA software and designed specifically for Android development. It is available for download on Windows, macOS and Linux based operating systems. It is a replacement for the Eclipse Android Development Tools (ADT) as the primary IDE for native Android application development. Android Studio focuses mainly on Kotlin but you can also write applications in C++. Oracle Studio Oracle Developer Studio is Oracle Corporations flagship software development product for the Solaris and Linux operating systems. It includes optimizing C, C++, and Fortran compilers, libraries, and performance analysis and debugging tools, for Solaris on SPARC and x86 platforms, and Linux on x86/x64 platforms, including multi-core systems. You can download Developer Studio at no charge but if you want the full support and patch updates, then you need a paid support contract. The C++ Compiler supports C++14. https://www.oracle.com/technetwork/server-storage/developerstudio/overview/index.html https://www.oracle.com/technetwork/server-storage/solarisstudio/features/compilers-2332272.html If you want to check some shorter code samples and you dont want to install the whole compiler/.IDE suite then we have lots of online tools that can make those tests super simple. Just open a web browser and put the code Compiler Explorer is a web-based tool that allows you to select from a wide variety of C++ compilers and different versions of the same compiler to test out your code. This allows developers to compare the generated code for specific C++ constructs among compilers, and test for correct behaviour. Clang, GCC, and MSVC are all there but also lesser-known compilers such as DJGPP, ELLCC, Intel C++, and others. https://godbolt.org/ Extra: Heres a list of handy online compilers that you can use: like Coliru, Wandbox, CppInsighs and more: https://arnemertz.github.io/online-compilers/ Debugging & Testing GDB GDB is a portable command-line based debugging platform that supports modern C++ and is available under an open-source license (GPL). A number of editors and IDEs like Visual Studio, Qt Creator, and CLion support integration with GDB. It can also be used to debug applications remotely where GDB is running on one device, and the application being debugged is running on another device. It supports a number of platforms including Windows, macOS, and Linux. https://www.gnu.org/software/gdb/ LLDB LLDB is an open-source debugging interface that supports modern C++ and integrates with the Clang compiler. It has a number of optional performance-enhancing features such as JIT but also supports debugging memory, multiple threads, and machine code analysis. It is built in C++. LLDB is the default debugger for Xcode and can be used with Visual Studio Code, CLion, and Qt Creator. It supports a number of platforms including Windows, macOS, and Linux. https://lldb.llvm.org/ Debugging Tools For Windows On Windows, you can use several debuggers, ranging from Visual Studio Debugger (integrated and one of the most user-friendly), WinDBG, CDB and several more. WinDbg is a multipurpose debugger for the Microsoft Windows Platform. It can be used to debug user-mode applications, device drivers, and the operating system itself in kernel mode. It has a graphical user interface (GUI) and is more powerful than Visual Studio Debugger. You can debug memory dumps obtained even from kernel drivers. One of the recent exciting features in Debugging on Windows is called Time Travel Debugging (Available in WinDBG Preview and also in Visual Studio Ultimate). It allows you to record the execution of the process and then replay the steps backwards or forwards. This flexibility enables us to easily tracks back the state that caused a bug. https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/ https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/time-travel-debugging-overview Mozillas RR RR is an advanced debugger that aims to replace GDB on Linux. It offers the full state recordings of the application so that you can replay the action backwards and forwards (similarly to Time Travel Debugging). The debugger is used to work with large applications like Chrome, OpenOffice or even Firefox code bases. https://rr-project.org/ CATCH/CATCH2 Catch2 is a cross-platform open-source (BSL-1.0) testing framework for modern C++. It is very lightweight because only a header file needs to be included. Unit tests can be tagged and run in groups. It supports both test-driven development and behaviour-driven development. Catch2 also easily integrates with CLion. https://github.com/catchorg/Catch2 BOOST.TEST Boost.Test is a feature-rich open-source (BSL-1.0) testing framework that utilizes modern C++ standards. It can be used to quickly detect errors, failures, and time outs through customizable logging and real-time monitoring. Tests can be grouped into suites, and the framework supports both small scale testing and large scale testing. https://github.com/boostorg/test GOOGLE TEST Google Test is Googles C++ testing and mocking framework, which is available under an open-source (BSD) license. Google test can be used on a broad range of platforms, including Linux, macOS, Windows, and others. It contains a unit testing framework, assertions, death tests, detects failures, handles parameterized tests, and creates XML test reports. https://github.com/google/googletest CUTE CUTE is a unit testing framework integrated into Cevelop, but it can also be used standalone. It spans C++ versions from c++98 to c++2a and is header-only. While not as popular as Google Test it is less macro-ridden and uses macros only, where no appropriate C++ feature is available. In addition, it features a mode that easily allows it to run on embedded platforms, by sidestepping some of the I/O formatting features. https://cute-test.com/ DocTest DocTest is a single-header unit testing framework. Available for C++11 up to C++20 and is easy to configure and works on probably all platforms. It offers regular TDD testing macros (also with subcases) as well as BDD-style test cases. http://bit.ly/doctest-docs https://github.com/onqtam/doctest Mull Mull is an LLVM-based tool for Mutation Testing with a strong focus on C and C++ languages. In general, it creates many variations of the input source code (using LLVM bytecode) and then checks it against the test cases. Thanks to this advanced testing technique, you can make your code more secure. https://github.com/mull-project/mull PDF: https://lowlevelbits.org/pdfs/Mull_Mutation_2018.pdf Sanitizers AddressSanitizer - https://clang.llvm.org/docs/AddressSanitizer.html (supported in Clang, GCC and XCode) UndefinedBehaviorSanitizer - https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html LeakSanitizer - https://clang.llvm.org/docs/LeakSanitizer.html Application Verifier for Windows - https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/application-verifier Sanitizers are relatively new tools that add extra instrumentation to your application (for example they replace new/malloc/delete calls) and can detect various runtime errors: leaks, use after delete, double free and many others. To improve your build pipeline, many guides advice to add sanitizers steps when doing tests. Most sanitizers come from the LLVM/Clang platform, but now they also work with GCC. Unfortunately not yet with Visual Studio (but you can try Application Verifier). Valgrind Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. When you run a program through Valgrind its run on a virtual machine that emulates your host environment. Having that abstraction the tools can leverage various information about the source code and its execution. http://valgrind.org/ http://valgrind.org/info/about.html http://valgrind.org/docs/manual/quick-start.html HeapTrack HeapTrack is a FOSS project and a heap memory profiler for Linux. It traces all memory allocations and annotates these events with stack traces. The tool has two forms the command line version that grabs the data, and then the UI part that you can use to read and analyze the results. This tool is comparable to Valgrinds massif; its easier to use and should be faster to load and analyze for large projects. https://github.com/KDE/heaptrack Dr. Memory Dr. Memory is an LGPL licenced tool that allows you to monitor and intensify memory -related errors for binaries on Windows, Linux, Mac, Android. Its based on the DynamoRIO dynamic instrumentation tool platform. With the tool, you can find errors like double frees, memory leaks, handle leaks (on Windows), GDI issues, access to uninitialized memory or even errors in multithreading memory scenarios. http://drmemory.org/ https://github.com/DynamoRIO/drmemory Deleaker The primary role of Deleaker is to find leaks in your native applications. It supports Visual Studio (since 2008 till the latest 2019 version), Delphi/C++ Builder, Qt Creator, CLion (soon!). Can be used as an extension in Visual Studio or as a standalone application. Deleaker tracks leaks in C/C++ applications (Native and CLR), plus .NET code. Memory (new/delete, malloc), GDI objects, User32 objects, Handles, File views, Fibres, Critical Sections, and even more. It gathers full call stack, ability to take snapshots, compare them, view source files related to allocation. https://www.deleaker.com/ https://www.deleaker.com/docs/deleaker/tutorial.html Summary & More I hope that with the above list, you get a useful overview of the tools that are essential for C++ development. If you want to read more about other ecosystem elements: libraries, frameworks, and other tools, then please see the full report from Embarcadero: C++ Ecosystem White Paper (Its a nice looking pdf, with more than 20 pages of content!) You might check this Resource for a super long list of tools, libs, frameworks that enhance C++ development: https://github.com/fffaraz/awesome-cpp Your Turn What are your favourite tools that you use when writing C++ apps? "],
        "story_type":"Normal",
        "url_raw":"https://www.bfilipek.com/2019/10/cppecosystem.html",
        "comments.comment_id":[21199733,
          21201412],
        "comments.comment_author":["cmrdporcupine",
          "Iv"],
        "comments.comment_descendants":[9,
          0],
        "comments.comment_time":["2019-10-09T03:27:23Z",
          "2019-10-09T09:10:25Z"],
        "comments.comment_text":["CLion is an amazing tool -- I've purchased licenses for my personal self in the past, but my employer pays for it these days.<p>My problem is they've done a terrible job of making it scale up to large code bases. I work on the chromium tree -- CLion is completely useless on it.  I have a dual 24-core xeon with 128GB of RAM and SSD and I've given it a wackload of memory, and it becomes completely inoperable, freezing all over the place.<p>Awful because I have such muscle memory for the JetBrains tools, and such a fondness for them.<p>I've gone back to using Emacs, but now with Eclim. I just couldn't get into VSCode.",
          "I had to use Qt as the UI lib for a project, it made me discover that QtCreator was actually not a Qt-only tool but a very good lightweight and generic C++ IDE.<p>That's my choice now. I need something that can navigate easily in a code base, I don't really like learning all the oddities around emacs and vim (even though I am a bit competent at vim) and I don't see what is so bad in using a mouse.<p>At first I thought annoying to have to manually edit the .includes and .config to add the includes and the macro I needed in our complex, hard-to-parse CMake based project, but now I really enjoy the freedom it gives."],
        "id":"5c6ed4f7-8344-47b5-913f-8f0c42d88093",
        "url_text":"Table of Contents Introduction Compilers GCC Microsoft Visual C++ Clang Intel C++ Compiler Build Tools & Package Managers Make Cmake Ninja Microsoft Build Engine (MSBuild) Conan, Vcpkg, Buckaroo Integrated Development Environments Sublime Text, Atom, And Visual Studio Code Vi/Vim & Emacs Clion Qt Creator C++Builder Visual Studio Xcode KDevelop Eclipse CDT IDE Cevelop Android Studio Oracle Studio Extra: Compiler Explorer & Online Tools Debugging & Testing GDB LLDB Debugging Tools For Windows Mozillas RR CATCH/CATCH2 BOOST.TEST GOOGLE TEST CUTE DocTest Mull Sanitizers Valgrind HeapTrack Dr. Memory Deleaker Summary & More Your Turn To write a professional C++ application, you not only need a basic text editor and a compiler. You require some more tooling. In this blog post, youll see a broad list of tools that make C++ programming possible: compilers, IDEs, debuggers and other. Last Update: 14th October 2019. Note: This is a blog post based on the White Paper created by Embarcadero, see the full paper here: C++ Ecosystem White Paper. Introduction The C++ computer programming language has become one of the most widely used modern programming languages. Software built with C++ is known for its performance and efficiency. C++ has been used to build numerous vastly popular core libraries, applications such as Microsoft Office, game engines such as Unreal, software tools like Adobe Photoshop, compilers like Clang, databases like MySQL, and even operating systems such as Windows across a wide variety of platforms as it continues to evolve and grow. Modern C++ is generally defined as C++ code that utilizes language features in C++11, C++14, and C++17 based compilers. These are language standards named after the year they were defined (2011, 2014 and 2017 respectively) and include a number of significant additions and enhancements to the original core language for powerful, highly performant, and bug-free code. Modern C++ has high-level features that support object-oriented programming, functional programming, generic programming, and low-level memory manipulation features. Big names in the computer industry such as Microsoft, Intel, the Free Software Foundation, and others have their modern C++ compilers. Companies such as Microsoft, The QT Company, JetBrains, and Embarcadero provide integrated development environments for writing code in modern C++. Popular libraries are available for C++ across a wide range of computer disciplines including Artificial Intelligence, Machine Learning, Robotics, Math, Scientific Computing, Audio Processing, and Image Processing. In this blog post, we are going to cover a number of these compilers, build tools, IDEs, libraries, frameworks, coding assistants, and much more that can support and enhance your development with modern C++. Lets get started! Compilers There are a number of popular compilers that support modern C++ including GCC/g++, MSVC (Microsoft Visual C++), Clang and Intel Compiler. Each compiler has varying support for each of the major operating systems with the open-source GCC/g++ originating in the late 1980s, Microsofts Visual C++ in the early 1990s, and Clang in the late 2000s. All four compilers support modern C++ up to at least C++17, but the source code licenses for each of them vary greatly. GCC GCC is a general-use compiler developed and maintained and regularly updated by the GCC Steering committee as part of the GNU Project. GCC describes a large growing family of compilers targeting many hardware platforms and several languages. While it mainly targets Unix-like platforms, Windows support is provided through the Cygwin or MinGW runtime libraries. GCC compiles modern C++ code up to C++17 with experimental support for some C++20 features. It also compiles with a variety of language extensions that build upon C++ standards. It is free and open-source (GPL3) with the GCC Runtime Library Exception. GCC has support from build tools such as CMake and Ninja and many IDEs such as CLion, Qt Creator, and Visual Studio Code. https://gcc.gnu.org/ https://gcc.gnu.org/projects/cxx-status.html Microsoft Visual C++ Microsoft Visual C++ (MSVC) is Microsofts compiler for their custom implementation of the C++ standard, known as Visual C++. It is regularly updated, and like GCC and Clang, supports modern C++ standards up to C++17 with experimental support for some C++20 features. MSVC is the primary method for building C++ applications in Microsofts own Visual Studio. It generally targets a number of architectures on Windows, Android, iOS, and Linux. Support for build tools and IDEs are limited but growing. CMake extensions are available in Visual Studio 2019. MSVC can be used with Visual Studio Code, with limited support from CLion and Qt Creator with additional extensions. MSVC is proprietary and available under a commercial license, but theres also a Community edition. https://en.wikipedia.org/wiki/Microsoft_Visual_C%2B%2B https://devblogs.microsoft.com/visualstudio/ https://visualstudio.microsoft.com/vs/community/ Clang Clang describes a large family of compilers for the C family of languages maintained and regularly developed as part of the LLVM project. Although it targets many popular architectures, it generally targets fewer platforms than GCC. The LLVM project defines Clang through key design principles - strict adherence to C++ standards (although support for GCC extensions is offered), modular design, and minimal modification to the source codes structure during compilation, to name a few. Like GCC, Clang compiles modern C++ code with support for the C++17 standard with experimental C++20 support. It is available under an open-source (Apache License Version 2.0) license. Clang also has widespread support from build tools such as CMake and Ninja and IDEs such as CLion, Qt Creator, Xcode, and others. https://clang.llvm.org/ https://clang.llvm.org/cxx_status.html Intel C++ Compiler Intel C++ Compiler can generate highly optimized code for various Intel CPUs (including Xeon, Core, and Atom processors). The compiler can seamlessly integrate with popular IDE like Visual Studio, GCC toolchain and others. It can leverage advanced instruction set (even AVX512) and generate parallel code (for example, thanks to OpenMP 5.0 support). Intel doesnt ship the compiler with the Standard Library implementation, so it uses the library you provide on your platform. The compiler is available as a part of Intel Parallel Studio XE or Intel System Studio. https://software.intel.com/en-us/c-compilers https://software.intel.com/en-us/articles/c17-features-supported-by-intel-c-compiler On top of compilers, you need an infrastructure that helps to build a whole application: build tools, pipelines and package managers. Make Make is a well-known build system widely used, especially in Unix and Unix-like operating systems. Make is typically used to build executable programs and libraries from source code. But the tool applies to any process that involves executing arbitrary commands to transform a source file to a target result. Make is not tight to any particular programming language. It automatically determines which source files has been changed and then performs the minimal build process to get the final output. It also helps with the installation of the results in the system https://www.gnu.org/software/make/ Cmake CMake is a cross-platform tool for managing your build process. Building, especially large apps and with dependent libraries, can be a very complex process, especially when you support multiple compilers; CMake abstracts this. You can define complex build processes in one common language and convert them to native build directives for any number of supported compilers, IDEs, and build tools, including Ninja (below.) There are versions of CMake available for Windows, macOS, and Linux. https://cmake.org/ Note: Heres a good answer about the differences between Make and Cmake: Difference between using Makefile and CMake to compile the code - Stack Overflow. Ninja The Ninja build system is used for the actual process of building apps and is similar to Make. It focuses on running as fast as possible by parallelizing builds. It is commonly used paired with CMake, which supports creating build files for the Ninja build system. The feature set of Ninja is intentionally kept minimal because the focus is on speed. https://ninja-build.org/ Microsoft Build Engine (MSBuild) MSBuild is a command-line based built platform available from Microsoft under an open-source (MIT) license. It can be used to automate the process of compiling and deploying projects. It is available standalone, packaged with Visual Studio, or from Github. The structure and function of MSBuild files is very similar to Make. MSBuild has an XML based file format and mainly has support for Windows but also macOS and Linux. IDEs such as CLion and C++Builder can integrate with MSBuild as well. https://docs.microsoft.com/en-us/visualstudio/msbuild/msbuild Conan, Vcpkg, Buckaroo Package managers such as Conan, vcpkg, Buckaroo and NIX have been gaining popularity in the C++ community. A package manager is a tool to install libraries or components. Conan is a decentralized open-source (MIT) package manager that supports multiple platforms and all build systems (such as CMake and MSBuild). Conan supports binaries with a goal of automating dependency management to save time in development and continuous integration. Microsofts vcpkg is open source under an MIT license and supports Windows, macOS, and Linux. Out of the box, it makes installed libraries available in Visual Studio, but it also supports CMake build recipes. It can build libs for every toolchain that can be fitted into CMake. Buckaroo is a lesser-known open-source package manager that can pull dependencies from GitHub, BitBucket, GitLab, and others. Buckaroo offers integrations for a number of IDEs including CLion, Visual Studio Code, XCode, and others. Here are the links for the mentioned package managers: https://conan.io/ https://github.com/microsoft/vcpkg https://buckaroo.pm/ Integrated Development Environments A host of editors and integrated development environments (IDEs) can be used for developing with modern C++. Text editors are typically lightweight, but are less featureful than a full IDE and so are used only for the process of writing code, not debugging or testing it. Full development requires other tools, and an IDE contains those and integrates into a cohesive, integrated development environment. Any number of text editors like Sublime Text, Atom, Visual Studio Code, vi/vim, and Emacs can be used for writing C++ code. However, some IDEs are specifically designed with modern C++ in mind like CLion, Qt Creator, and C++Builder, while IDEs like Xcode and Visual Studio also support other languages. You can also compare various IDE for C++ in this handy table on Wikipedia: Comparison of integrated development environments - C++ - Wikipedia Sublime Text, Atom, And Visual Studio Code The list below summarises a set of advanced source code editors that thanks to various plugins and extensions allow creating applications in almost all programming languages. Sublime Text is a commercial text editor with extended support for modern C++ available via plugins. Atom is an open-source (MIT license) text editor that supports modern C++ via packages with integrations available for debugging and compiling. Visual Studio Code is a popular open-source (MIT license) source-code editor from Microsoft. A wide variety of extensions are available that bring features such as debugging and code completion for modern C++ to Visual Studio Code. Sublime Text, Atom, and Visual Studio Code are all available for Windows, macOS, and Linux. Here are the links for the above tools: https://www.sublimetext.com/ https://atom.io/ https://code.visualstudio.com/ Vi/Vim & Emacs Vi/Vim and Emacs are free command-line based text editors that are mainly used on Linux but are also available for macOS and Windows. Modern C++ support can be added to Vi/Vim through the use of scripts while modern C++ support can be added to Emacs through the use of modules. https://www.vim.org/ https://www.gnu.org/software/emacs/ Clion CLion is a commercial IDE from JetBrains that supports modern C++. It can be used with build tools like CMake and Gradle, integrates with the GDB and LLDB debuggers, can be used with version control systems like Git, test libraries like Boost.Test, and various documentation tools. It has features such as code generation, refactoring, on the fly code analysis, symbol navigation, and more. https://www.jetbrains.com/clion/ Qt Creator Qt Creator is a (non)commercial IDE from The Qt Company which supports Windows, macOS, and Linux. Qt Creator has features such as a UI designer, syntax highlighting, auto-completion, and integration with a number of different modern C++ compilers like GCC and Clang. Qt Creator tightly integrates with the Qt library for rapidly building cross-platform applications. Additionally, it integrates with standard version control systems like Git, debuggers like GDB and LLDB, build systems like CMake, and can deploy cross-platform to iOS and Android devices. https://www.qt.io/ C++Builder C++Builder is a commercial IDE from Embarcadero Technologies which runs on Windows and supports modern C++. It features the award-winning Visual Component Library (VCL) for Windows development and FireMonkey (FMX) for cross-platform development for Windows, iOS and Android. The C++Builder compiler features an enhanced version of Clang, an integrated debugger, visual UI designer, database library, comprehensive RTL, and standard features like syntax highlighting, code completion, and refactoring. C++Builder has integrations for CMake, can be used with Ninja, and also MSBuild. https://www.embarcadero.com/products/cbuilder https://www.embarcadero.com/products/cbuilder/starter Visual Studio Visual C++ is a commercial Visual Studio IDE from Microsoft. Visual Studio integrates building, debugging, and testing within the IDE. It provides the Microsoft Foundation Class (MFC) library which gives access to the Win32 APIs. Visual Studio features a visual UI designer for certain platforms, comes with MSBuild, supports CMake, and provides standard features such as code completion, refactoring, and syntax highlighting. Additionally, Visual Studio supports a number of other programming languages, and the C++ side of it is focused on Windows, with other platforms slowly being added. https://visualstudio.microsoft.com/ Xcode Xcode is a multi-language IDE from Apple available only on macOS that supports modern C++. Xcode is proprietary but available for free from Apple. Xcode has an integrated debugger, supports version control systems like Git, features a Clang compiler, and utilizes libc++ as its standard library. It supports standard features such as syntax highlighting, code completion, and finally, Xcode supports external build systems like CMake and utilizes the LLDB debugger. https://developer.apple.com/xcode/ KDevelop KDevelop (its 0.1 version was released in 1998) is a cross-platform IDE for C, C++, Python, QML/JavaScript and PHP. This IDE is part of the KDE project, and is based on KDE Frameworks and Qt. The C/C++ backend uses Clang and LLVM. It has UI integration with several version control systems: Git, SVN, Bazaar and more, build process based on CMake, QMake or custom makefiles. Among many interesting features, its essential to mention advanced syntax colouring and Context-sensitive, semantic code completion. https://www.kdevelop.org/ https://www.kdevelop.org/features Eclipse CDT IDE The Eclipse C/C++ Development Toolkit (CDT) is a combination of the Eclipse IDE with a C++ toolchain (usually GNU - GCC). This IDE supports project creation and build management for various toolchains, like the standard make build. CDT IDE offers source navigation, various source knowledge tools, such as type hierarchy, call graph, include browser, macro definition browser, code editor with syntax highlighting, folding and hyperlink navigation, source code refactoring and code generation, visual debugging tools, including memory, registers, and disassembly viewers. https://www.eclipse.org/cdt/ Cevelop Cevelop is a powerful IDE based Eclipse CDT. Its main strength lies in the powerful refactoring and static analysis support for code modernization. In addition, it comes with unit testing and TDD support for the CUTE unit testing framework. Whats more, you can easily visualize your template instantiation/function overload resolution and optimize includes. https://www.cevelop.com/ Android Studio Android Studio is the official IDE for Googles Android operating system, built on JetBrains IntelliJ IDEA software and designed specifically for Android development. It is available for download on Windows, macOS and Linux based operating systems. It is a replacement for the Eclipse Android Development Tools (ADT) as the primary IDE for native Android application development. Android Studio focuses mainly on Kotlin but you can also write applications in C++. Oracle Studio Oracle Developer Studio is Oracle Corporations flagship software development product for the Solaris and Linux operating systems. It includes optimizing C, C++, and Fortran compilers, libraries, and performance analysis and debugging tools, for Solaris on SPARC and x86 platforms, and Linux on x86/x64 platforms, including multi-core systems. You can download Developer Studio at no charge but if you want the full support and patch updates, then you need a paid support contract. The C++ Compiler supports C++14. https://www.oracle.com/technetwork/server-storage/developerstudio/overview/index.html https://www.oracle.com/technetwork/server-storage/solarisstudio/features/compilers-2332272.html If you want to check some shorter code samples and you dont want to install the whole compiler/.IDE suite then we have lots of online tools that can make those tests super simple. Just open a web browser and put the code Compiler Explorer is a web-based tool that allows you to select from a wide variety of C++ compilers and different versions of the same compiler to test out your code. This allows developers to compare the generated code for specific C++ constructs among compilers, and test for correct behaviour. Clang, GCC, and MSVC are all there but also lesser-known compilers such as DJGPP, ELLCC, Intel C++, and others. https://godbolt.org/ Extra: Heres a list of handy online compilers that you can use: like Coliru, Wandbox, CppInsighs and more: https://arnemertz.github.io/online-compilers/ Debugging & Testing GDB GDB is a portable command-line based debugging platform that supports modern C++ and is available under an open-source license (GPL). A number of editors and IDEs like Visual Studio, Qt Creator, and CLion support integration with GDB. It can also be used to debug applications remotely where GDB is running on one device, and the application being debugged is running on another device. It supports a number of platforms including Windows, macOS, and Linux. https://www.gnu.org/software/gdb/ LLDB LLDB is an open-source debugging interface that supports modern C++ and integrates with the Clang compiler. It has a number of optional performance-enhancing features such as JIT but also supports debugging memory, multiple threads, and machine code analysis. It is built in C++. LLDB is the default debugger for Xcode and can be used with Visual Studio Code, CLion, and Qt Creator. It supports a number of platforms including Windows, macOS, and Linux. https://lldb.llvm.org/ Debugging Tools For Windows On Windows, you can use several debuggers, ranging from Visual Studio Debugger (integrated and one of the most user-friendly), WinDBG, CDB and several more. WinDbg is a multipurpose debugger for the Microsoft Windows Platform. It can be used to debug user-mode applications, device drivers, and the operating system itself in kernel mode. It has a graphical user interface (GUI) and is more powerful than Visual Studio Debugger. You can debug memory dumps obtained even from kernel drivers. One of the recent exciting features in Debugging on Windows is called Time Travel Debugging (Available in WinDBG Preview and also in Visual Studio Ultimate). It allows you to record the execution of the process and then replay the steps backwards or forwards. This flexibility enables us to easily tracks back the state that caused a bug. https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/ https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/time-travel-debugging-overview Mozillas RR RR is an advanced debugger that aims to replace GDB on Linux. It offers the full state recordings of the application so that you can replay the action backwards and forwards (similarly to Time Travel Debugging). The debugger is used to work with large applications like Chrome, OpenOffice or even Firefox code bases. https://rr-project.org/ CATCH/CATCH2 Catch2 is a cross-platform open-source (BSL-1.0) testing framework for modern C++. It is very lightweight because only a header file needs to be included. Unit tests can be tagged and run in groups. It supports both test-driven development and behaviour-driven development. Catch2 also easily integrates with CLion. https://github.com/catchorg/Catch2 BOOST.TEST Boost.Test is a feature-rich open-source (BSL-1.0) testing framework that utilizes modern C++ standards. It can be used to quickly detect errors, failures, and time outs through customizable logging and real-time monitoring. Tests can be grouped into suites, and the framework supports both small scale testing and large scale testing. https://github.com/boostorg/test GOOGLE TEST Google Test is Googles C++ testing and mocking framework, which is available under an open-source (BSD) license. Google test can be used on a broad range of platforms, including Linux, macOS, Windows, and others. It contains a unit testing framework, assertions, death tests, detects failures, handles parameterized tests, and creates XML test reports. https://github.com/google/googletest CUTE CUTE is a unit testing framework integrated into Cevelop, but it can also be used standalone. It spans C++ versions from c++98 to c++2a and is header-only. While not as popular as Google Test it is less macro-ridden and uses macros only, where no appropriate C++ feature is available. In addition, it features a mode that easily allows it to run on embedded platforms, by sidestepping some of the I/O formatting features. https://cute-test.com/ DocTest DocTest is a single-header unit testing framework. Available for C++11 up to C++20 and is easy to configure and works on probably all platforms. It offers regular TDD testing macros (also with subcases) as well as BDD-style test cases. http://bit.ly/doctest-docs https://github.com/onqtam/doctest Mull Mull is an LLVM-based tool for Mutation Testing with a strong focus on C and C++ languages. In general, it creates many variations of the input source code (using LLVM bytecode) and then checks it against the test cases. Thanks to this advanced testing technique, you can make your code more secure. https://github.com/mull-project/mull PDF: https://lowlevelbits.org/pdfs/Mull_Mutation_2018.pdf Sanitizers AddressSanitizer - https://clang.llvm.org/docs/AddressSanitizer.html (supported in Clang, GCC and XCode) UndefinedBehaviorSanitizer - https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html LeakSanitizer - https://clang.llvm.org/docs/LeakSanitizer.html Application Verifier for Windows - https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/application-verifier Sanitizers are relatively new tools that add extra instrumentation to your application (for example they replace new/malloc/delete calls) and can detect various runtime errors: leaks, use after delete, double free and many others. To improve your build pipeline, many guides advice to add sanitizers steps when doing tests. Most sanitizers come from the LLVM/Clang platform, but now they also work with GCC. Unfortunately not yet with Visual Studio (but you can try Application Verifier). Valgrind Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. When you run a program through Valgrind its run on a virtual machine that emulates your host environment. Having that abstraction the tools can leverage various information about the source code and its execution. http://valgrind.org/ http://valgrind.org/info/about.html http://valgrind.org/docs/manual/quick-start.html HeapTrack HeapTrack is a FOSS project and a heap memory profiler for Linux. It traces all memory allocations and annotates these events with stack traces. The tool has two forms the command line version that grabs the data, and then the UI part that you can use to read and analyze the results. This tool is comparable to Valgrinds massif; its easier to use and should be faster to load and analyze for large projects. https://github.com/KDE/heaptrack Dr. Memory Dr. Memory is an LGPL licenced tool that allows you to monitor and intensify memory -related errors for binaries on Windows, Linux, Mac, Android. Its based on the DynamoRIO dynamic instrumentation tool platform. With the tool, you can find errors like double frees, memory leaks, handle leaks (on Windows), GDI issues, access to uninitialized memory or even errors in multithreading memory scenarios. http://drmemory.org/ https://github.com/DynamoRIO/drmemory Deleaker The primary role of Deleaker is to find leaks in your native applications. It supports Visual Studio (since 2008 till the latest 2019 version), Delphi/C++ Builder, Qt Creator, CLion (soon!). Can be used as an extension in Visual Studio or as a standalone application. Deleaker tracks leaks in C/C++ applications (Native and CLR), plus .NET code. Memory (new/delete, malloc), GDI objects, User32 objects, Handles, File views, Fibres, Critical Sections, and even more. It gathers full call stack, ability to take snapshots, compare them, view source files related to allocation. https://www.deleaker.com/ https://www.deleaker.com/docs/deleaker/tutorial.html Summary & More I hope that with the above list, you get a useful overview of the tools that are essential for C++ development. If you want to read more about other ecosystem elements: libraries, frameworks, and other tools, then please see the full report from Embarcadero: C++ Ecosystem White Paper (Its a nice looking pdf, with more than 20 pages of content!) You might check this Resource for a super long list of tools, libs, frameworks that enhance C++ development: https://github.com/fffaraz/awesome-cpp Your Turn What are your favourite tools that you use when writing C++ apps? ",
        "_version_":1718441064945156096},
      {
        "story_id":19851744,
        "story_author":"howard941",
        "story_descendants":195,
        "story_score":670,
        "story_time":"2019-05-07T17:49:50Z",
        "story_title":"The Amazing $1 Microcontroller (2017)",
        "search":["The Amazing $1 Microcontroller (2017)",
          "https://jaycarlson.net/microcontrollers/",
          "Silcon Labs EFM8: Fantastic value and ease-of-use from the only 8-bit part with a totally-free cross-platform vendor ecosystemThe EFM8 was the fastest 8-bit part in my round-up, and admittedly, my favorite 8-bit architecture to develop with overall. What these parts lack in brains they make up for in brawns 14-bit ADCs, 12-bit DACs, lots of timers, and a 72 MHz core clock speed that gives you timing options not found in any other part in the round-up.Plus, this is the only 8-bit part with a totally-free, cross-platform, vendor-provided ecosystem. Let that sink in.Keil C51 is a silly compiler, but Silicon Labs does an excellent job hiding it under the hood even when running its Eclipse-based Simplicity Studio on Linux or macOS.Simplicity Configurator is the lightest-weight code generator in our round-up, using only 534 bytes of flash to house the entire DMX-512 receiver project. It was one of the easiest to use, and seemed to strike a good balance between abstraction, performance, and ease of use.Debugging speeds are snappy with a J-Link debugger, but at $35, the official Silicon Labs USB Debug Adapter is one of the cheapest first-party debugger in the round-up, and clones of the hardware are even cheaper.And call me old-fashioned, but I think the 8051 definitely has a place in 2017 especially among hobbyists and students, where its bit-addressable memory, easy-to-use peripherals, and fuse-free configuration help get students comfortable with microcontrollers quickly.Microchip megaAVR & tinyAVR 1-Series: Different strokes for different folks still with the best 8-bit toolchain availableThe megaAVR came in surprisingly flat for me: especially when compared with its lower-cost, new sibling, the tinyAVR 1-Series.Theres no comparison when it comes to price: tinyAVR has incredible value packing in a nice assortment of timers, analog peripherals (including a DAC), and a new 20 MHz internal oscillator while costing 20-40% less than the megaAVR.While the megaAVR has a perplexing debugging experience that requires two completely different interfaces and protocols to work with the part, the new one-wire UPDI interface the tinyAVR sports worked flawlessly in my testing.But thats the crux of the problem for the tinyAVR by shedding many of its megaAVR roots, Microchip ended up with a wonderful microcontroller that will be challenging to use for a large base of Atmel fans: indie developers and hobbyists who use low-cost, open-source programmers (which dont support the UPDI interface).While the tinyAVR wasnt the fastest part in the round-up (even among 8-bitters), it was the most efficient both in terms of active-mode power and clock efficiency. Amazingly, the AVR only uses about twice as many instructions as 16- and 32-bit parts when performing 16-bit math.Unfortunately, the AVR system as a whole is not without its issues. The Windows-only Atmel Studio is still buggy (especially with older megaAVR devices and AVR Dragon stuff in my tests), and there isnt an under-$50 low-cost debugger available (other than hacking apart Xplained Mini dev boards).In many ways, there seems to be a tacit demarcation Atmel creates between its hobbyist/indie developers, and the professional shops that use Atmel parts.As a professional embedded developer, I most definitely have access to Windows computers, and I have no problem blowing a few billable hours worth of pay on a $140 debugger.But even as popular as Atmel is among hobbyists, Atmel has largely stayed out of this space directly. Instead, theyve secured small-volume AVR sales by relying on the open-source community to build their own tools for themselves: turning out a slew of hardware and software used to program the megaAVR devices.While I applaud the efforts of these developers, these tools are inferior to Atmels. Their programming speeds are terrible, they dont support the new tinyAVR 1-Series devices, and they have absolutely no debug capability.Having said that, both the megaAVR and tinyAVR have the best toolchain available for 8-bit MCU development. The part supports a full, end-to-end Makefile-based GCC toolchain.If you love printf() debugging, would never touch a proprietary toolchain, and hate IDEs, megaAVR and old tinyAVR parts are definitely for you. The older ones are still available in DIP packages, and as you probably know, there are a ton of low-cost programmers available across the world. The online community is massive, and as clunky as I find Atmel START to be, I have to applaud its support for Makefile-based project generation.Consequently, the megaAVR remains the most open-source 8-bit microcontroller on the market by a long shot.But Id really like to see Microchip provide a PicKit-priced debugger with UPDI support and allow off-board debugging the way their PIC Curiosity Boards do.I also hope these open-source projects can add UPDI support to their tools, so that hobbyists and indie developers can start integrating the tinyAVR into their projects its a much better part, and if youre an AVR user with access to Atmel Studio, you really ought to buy an Xplained Mini board and take it for a spin.STM32F0: A low-cost, no-nonsense part with arguably the best Arm development ecosystem testedThe STM32F0 was the lowest-power Arm microcontroller in the round-up, and also one of the easiest to use. STM32CubeMX doesnt generate the most compact code on Arm (that honor belongs to Cypress PSoC Creator and Infineon DAVE), but it has a snappy interface, and the generated code is easy enough to manipulate for your own goals.I love the nearly-stock Eclipse-based environment that System Workbench for STM32 provides, and the ST-Link and excellent Discovery/Nucleo boards seals the deal for me.Most pros have used ST parts in their work, but for all these reasons, any hobbyist looking at moving to Arm should probably pick up a dev board from this ecosystem, too. ST has a huge market footprint, so theres tons of resources online aimed at both hobbyists and professionals.SAM D10: Killer performance & peripherals, but with runtime library hiccupsThe Microchip/Atmel SAM D10 (and the broader D11/D20/D21 ecosystem) has good value (considering their analog portfolio includes a DAC, and they have good timing options), and the SAM D10 was the most efficient part tested when running at full speed.Professionals will like the easy-to-use, well-documented header files, and hobbyists will appreciate the 1.27mm-pitch SOIC package options and GCC compilers that come with the Arm ecosystem. But before I grab this part for a project, Microchip really needs to fix the extremely slow, bloated peripheral library, and update their code-gen tool to do proper error-checking of clock and peripheral configurations.As it is, whenever I use Atmel START on the D10, I want to STOP almost immediately. And there are no current, stand-alone peripheral drivers that Microchip has released for this part, so unless you want to do register programming from scratch, youll be relying on third-party, open-source projects like Alex Taradovs code examples.Infineon XMC1100: Interesting peripheral perks make this Cortex-M0 stand outThe most interesting Arm chip was, without a doubt, the Infineon XMC1100 and I think professionals who may be wary of getting out of the ST/NXP/Atmel Arm ecosystem need to take a second look at these XMC1000 (and XMC4000) parts.The timer options are amazingly flexible, and you can squeeze fantastic performance out of the USIC module.Im going to go out on a limb and recommend that serious hobbyists who are building motor / lighting control projects look into these parts, too. DAVE makes setting up these complex peripherals painless, and the 38-pin TSSOP chips will be substantially easier to solder than the 0.5mm QFNs and QFPs you usually end up with in these pin counts.Like many of the parts reviewed here, the biggest problem for hobbyists and indie developers is the tiny online communities and lack of GitHub repos with open-source projects that use these chips. My advice be bold, and post in the forums. Infineon employees monitor and usually respond within a day or so.PIC16: Tons of peripherals with a slower, power-efficient coreWhen you compare the PIC16 with other 8-bit parts out there, its obviously a part built for low-power applications, and not processing power. And while the development ecosystem is workable, there are other parts more friendlier pathways especially for smaller shops, hobbyists, and students who need extremely low-cost tools (and free software).To add fuel to the PIC-vs-AVR debate, my testing found that a 32 MHz PIC16 is roughly equivalent to an AVR part running at 1.4 MHz (in terms of math performance), and 9 MHz (in terms of bit-shuffling performance).Having said that, the DMX-512 receiver seems a perfect match for the PIC16, and thats where it looks best in my testing: the PIC16 was the lowest-power 8-bit part in my testing.Its also full of timers and digital logic-oriented peripherals that make it suitable for funky special-purpose projects that require some crafty use of configurable logic and and the numerically-controlled oscillator these peripherals help offload the (relatively slow) CPU, at the expense of requiring more developer familiarity with the device and these peripherals.The usual Microchip gotchas apply: clunky IDE, expensive compilers, and expensive debuggers.The usual Microchip advantages apply: huge online community, seemingly infinite product lifetime guarantees, and DIP, SOIC, QFP, and QFN package availability.PIC24: An expensive MSP430 wannabe that doesnt hit the markThe PIC24 is nearly forgettable. In the biquad test, its marginally faster than the Renesas RL-78 but uses almost three times as much power. In the DMX-512 test, both the RL-78 and MSP430 beat it, too. It was also one of the least-endowed parts in the round-up (which really just means its expensive higher-end PIC24 parts have no shortage of peripherals).The usual Microchip gotchas apply: clunky IDE, expensive compilers, and expensive debuggers.The usual Microchip advantages apply: huge online community, seemingly infinite product lifetime guarantees, and DIP, SOIC, QFP, and QFN package availability.PIC32: An excellent 32-bit part that balances performance and power consumptionThe PIC32MM was my favorite Microchip part in the review. It brought in the lowest-power performance of every 32-bit part tested. Unfortunately, it was also the least-efficient 32-bit part tested in terms of math performance (well, excluding the couldnt-care-less-about-power Nuvoton M051), and its pretty spartan on peripherals it doesnt even have a hardware I2C controller.But PIC32MM parts have good flash / RAM density, and have simpler clocking / peripheral gating configurations than some of the more-flexible Arm parts, which makes them feel easier to program at a register level.Plus, they have a lot of headroom: I think the high-end PIC32MZ DA devices have a home among small industrial dev shops that need Linux-like HMI functionality but dont have the resources to bring a product like that to market.The usual Microchip gotchas apply: clunky IDE, expensive compilers, and expensive debuggers.The usual Microchip advantages apply: huge online community, seemingly infinite product lifetime guarantees, and DIP, SOIC, QFP, and QFN package availability.Renesas RL-78: An agile, low-power, easy-to-use 16-bit part you really ought to tryI had never picked up a Renesas part before, and when I went shopping for dev kits and stumbled on only a smattering of expensive, traditional systems, I was a little anxious. But I found the $25 RL78L1A Promotion Board, gave it a shot, and really enjoyed it.The RL-78 is a snappy architecture that competes with Arm parts in math performance, yet its also relatively inexpensive especially compared to the MSP430 and PIC24. It cant quite hit the MSP430 sleep-mode power consumption figures, but it gets close and is, by far, the most power-efficient 5V-capable part in the review.The code generator tool produces readable yet efficient code, and the IDE, e2studio, is Eclipse-based and is getting Linux and macOS support in the next release.Id complain about the dev board, but the new YRPBRL78G13 RL78/G13 development kit should remedy basically all my complains with it I cant wait for U.S. distributors to start carrying these. They could use a more active community and more people publishing code online, but I hope this article will help inspire some remedies for that.N76, HT66, and STM8: Low-cost parts with a smattering of development headachesThe STM8 is probably the nicest of the cheapie parts. It has nice peripherals and really good performance for an 8-bit part running at its frequency, but I think the entry-level 38-cent STM8S103F2P6 is a more compelling part than the higher-end one reviewed here simply because of its ultra-low price. The part I reviewed here looks a lot like the other 8-bit microcontrollers but with an ancient-looking IDE thats not nearly as productive as the competition. And almost everything out there has better power consumption figures.Still, this part is relatively cheap to get going (ahoy, $5 ST-Link clones), and the IDE and toolchain are completely free. But in that regard, you get what you pay for: STVD feels trapped in 2002, and theres no way to set up a more modern development and debugging environment for it 15Though, theres some emerging SDCC / GDB support for pairing it with an ST-Link in an open-source fashion, which might help it make inroads with the classic tinyAVR and megaAVR users.Youll be forking over quite a bit of money for a Keil C51 license to develop for the N76 all for a part that doesnt look much different than some entry-level EFM8s that have a day-and-night difference in ease of development. Still, at 23 cents per unit, its tough to beat for volume applications and hobbyists and hackers can probably get by with the 2K code limit of Keils evaluation version. SDCC users need not apply: theres no stand-alone tools for loading code into this part, and I doubt Vision can be coaxed into loading an SDCC-compiled hex file.The Holtek HT-66 has terrible processing performance, but barely uses any run-mode current theres plenty of application-specific models to choose from, and while the IDE is goofy, I found it to be fairly productive and its completely free. Careful, though: only the more-expensive V parts have on-chip debugging.STC8: A neat performance-heavy part for hacking but probably not for serious, professional workI think every hacker and advanced hobbyist really ought to throw $10 at AliExpress/Taobao and get some STC15 and STC8 parts just for fun.Both are jam-packed full of peripherals and memory (more than every other part reviewed), and the STC8 is also really fast. There are some interesting projects you can do with a part that hits your C interrupt code 10 clock cycles after an interrupt occurs thats 320 nanoseconds. Both these parts support debugging over UART, so theres no proprietary debugger to purchase.I wouldnt seriously consider using these parts in U.S-based commercial work, as we have no access to STC inventory here, but the part is just plain fun to play with.ON Semiconductor LC-87: SkipYou can probably skip over the ON Semiconductor LC87. This is a rare part outside the Japanese market, and it looks like its on its way out the door. I called Altium to try to get an evaluation version of the Tasking LC87 toolset, and the person I talked to had never heard of the LC87 before, and was almost positive they hadnt made a compiler for it for at least three years. This part has terrible power consumption, few peripherals, and the worst development environment I saw in this review. Skip.Kinetis KL03: Sleep-mode specialist not for beginnersWhile the Kinetis KL03 has excellent deep-sleep current and ultra-tiny CSP package availability, it definitely feels like a specialized part not useful for the applications I evaluated. It has far fewer peripherals than the other parts reviewed, and despite NXPs low-power claims, was consistently in the middle of my Arm rankings for the DMX-512 receiver test though it nearly matches the SAM D10 in full-speed active mode.Kinetis SDK is awkward to use, and the dev boards are terrible requiring a lot of reverse-engineering and hacking to get the board doing anything other than running pre-written demos (especially if youre interested in measuring power consumption). Still, MCUXpresso is a productive, modern Eclipse-based IDE, and the KL03 has some of the lowest-leakage power modes out there, which means you can get 8-bit-like performance when youre running an RTC or interrupt wake-up project from a coin-cell battery.Kinetis KE04: Decent peripheral assortment with a powerful yet clunky code gen toolThe Kinetis KE04 had pretty heavy power consumption in my testing but this was largely due to the heavy-handed Processor Expert code that Kinetis Design Studio generated. This environment is really suited to much larger, faster microcontrollers running RTOSes and not needing especially good low-level I/O performance.But, hey, if you dont really care about performance, the nice thing about Processor Expert is it abstracts the peripherals to such a high level that youll never need to crack open a datasheet for the part if youre using the peripherals in normal configurations.Plus, the KE04 (and KE02) are 5V-compatible parts, and theyre available in old-school, easy-to-solder 1.27mm SOIC and 0.8mm packages so I could imagine hobbyists would find this part useful.LPC811: Few perks, and less interesting than the LPC810The LPC810 drew people in with its odd, 8-pin DIP form-factor. That chip has since been discontinued, but the LPC81x line remains. The LPC811 reviewed here is sparse on peripherals not even having an ADC and brought in poor performance. Theres really nothing that this part does that you cant get from one of the other vendors; but dont discredit NXP completely their higher-end offerings have some interesting capabilities (like dual-core Cortex-M4/M0 designs), and their development environment, MCUXpresso, is an inoffensive Eclipse system.PSoC 4000S & MSP430: Bottom-of-the-barrel parts that offer a glimpse into nice ecosystemsI hesitated to review the PSoC and MSP430 because they tend to be relatively expensive parts, so in a $1 shoot-out, you end up with bottom-end parts that dont look nearly as useful as their higher-cost relatives. If you really want to get a feel for what the MSP430 or PSoC parts can do, I recommend buying into a higher-end part preferably on one of the excellent dev boards that these manufacturers make.PSoC Creator and the reconfigurable digital and analog blocks in the PSoC line draw many professional and hobbyist users into the architecture but instead of grabbing the 4000S from this review, reach for a PSoC5 (or soon-to-launch PSoC6) dev board to get a feel for the platform.Same with the MSP430. In the DMX-512 test, it dominated in power consumption, but barely put up marks in any other category (this is especially challenging when you have no hardware multiplier, and only a smattering of peripherals).Still, the part has a solid development ecosystem with Code Composer Studio and a choice between the proprietary (but now free) TI compiler, and the open-source GCC one. Plus, hobbyists will love the easy Arduino migration path (with Code Composer Studio directly supporting Energia *.ino sketch projects) and $10 dev boards.And really, everyone starting a battery-based product needs to go buy an MSP430 Launch Pad and play around with it these really are amazing parts that still have a lot of relevance in 2017.Nuvoton M051: Ecosystem issues stifle a performance-packed partThe Nuvoton M051 one of the most-endowed parts reviewed suffers ecosystem issues that Nuvoton could easily remedy in the future, so Ill reserve judgment. Theres no manufacturer-provided Eclipse-based IDE instead, the only IDE options are CooCox, and Keil Vision neither of which Im particularly fond of.I was able to get CooCox working (though the peripheral libraries that are in the CooCox repo are old and full of bugs). The M0 had some of the worst power-consumption figures in the review, but it makes up for that with tons of communications peripherals, beautiful 32-bit control-friendly timers, and easily-digestible runtime libraries and documentation that are far easier to use than other vendors. When Nuvoton fixes the IDE absence, Ill definitely move this part from the meh to yeah column since it accomplishes all of these feats while remaining one of the lowest-cost Arm microcontrollers out there.I had a ton of fun playing with all these different parts for the last few months for this microcontroller review, and in many ways, came away thinking what I already knew: there is no perfect microcontroller no magic bullet that will please all users. What I did learn, however, is its getting easier and easier to pick up a new architecture youve never used before, and there have never been more exciting ecosystems to choose from.And thats what I want people to think about as they walk away from this microcontroller review. If youre an Arduino hobbyist looking where to go next, I hope you realize there are a ton of great, easy-to-use choices. And for professional developers and hardcore hackers, perhaps theres an odd-ball architecture youve noticed before, but never quite felt like plunging into nows the time.Its an exciting time to be involved with electronics whatever parts you choose to pick up, I hope youve enjoyed learning about whats out there, and can get inspired to go build something great. Definitely leave a note in the comments below if youve got something to contribute to the discussion! "],
        "story_type":"Normal",
        "url_raw":"https://jaycarlson.net/microcontrollers/",
        "comments.comment_id":[19851932,
          19852605],
        "comments.comment_author":["neya",
          "blhack"],
        "comments.comment_descendants":[15,
          5],
        "comments.comment_time":["2019-05-07T18:09:12Z",
          "2019-05-07T19:24:40Z"],
        "comments.comment_text":["My favorite is the ATTINY 85. It's so cheap and yet, it can do almost most of the things you can do an Arduino. And yes, you can program it using the Arduino IDE as well.<p>Last year, I started out to digitalize my entire house with these tiny ATTINY 85 chips and with some nrF wireless chips. It has worked really well for me and I'm able to control and monitor any power port in my house as well as my mains.<p>The overall setup still costed less than a Nest, yet, vastly more powerful. Of course, the downside is you need to write code, have a running server to control it on the cloud. But, no problem for me as I'm a (Google Cloud) consultant and this is what I do for a living.<p>I use Google Cloud's IAP with AppEngine (I may switch to cloud run now, since it's cheaper tho) and I have a private REST server running that I can access from my own Android app.<p>The whole thing took me a year to finish, but that's because I worked on it maybe once or twice a week in the weekends and didn't focus on it much.<p>But, my effort has paid off and it's amazing to me that what is possible with just a dollar investment.",
          "I feel obligated to mention in every one of these threads the wemos D1.<p>I actually have a tough time explaining just how incredible this board is.  You can program it in the Arduino environment, meaning if you are doing any hobby electronics, you're probably already really familiar with the programming modalities.<p>It comes with a programmer, and power regulator.  No fussing with anything like you might have to with a bare ESP8266.<p>It also has WiFi.  It is <i>the</i> magic internet of things that I swear everybody was dreaming about 5 years ago.  Go buy 10 of them.  They're my favorite favorite favorite general purpose dev board right now, and actually they're so cheap that I have no problem putting them into \"finished products\"[1].<p>Here they are for about $3: (<a href=\"https://www.aliexpress.com/item/ESP8266-ESP-12F-CH340-CH340G-V2-1-0-WIFI-Expansion-Board-Module-Based-ESP8266-Micro-USB/32963411851.html?spm=2114.search0104.3.3.226f11cc81cPtk&ws_ab_test=searchweb0_0,searchweb201602_6_10065_10130_10068_10890_10547_319_10546_317_10548_10545_10696_453_10084_454_10083_10618_10307_537_536_10059_10884_10887_321_322_10103,searchweb201603_52,ppcSwitch_0&algo_expid=a10ec08c-7bb2-4299-a8eb-79e1f5bc42ad-0&algo_pvid=a10ec08c-7bb2-4299-a8eb-79e1f5bc42ad\" rel=\"nofollow\">https://www.aliexpress.com/item/ESP8266-ESP-12F-CH340-CH340G...</a>)<p>[1]: I build custom/one-off large scale installation pieces and hardware prototypes.  Almost every single one of them has an arduino-ish device in it somewhere."],
        "id":"13abeae4-f69b-4a8f-94e1-b4d336dee444",
        "url_text":"Silcon Labs EFM8: Fantastic value and ease-of-use from the only 8-bit part with a totally-free cross-platform vendor ecosystemThe EFM8 was the fastest 8-bit part in my round-up, and admittedly, my favorite 8-bit architecture to develop with overall. What these parts lack in brains they make up for in brawns 14-bit ADCs, 12-bit DACs, lots of timers, and a 72 MHz core clock speed that gives you timing options not found in any other part in the round-up.Plus, this is the only 8-bit part with a totally-free, cross-platform, vendor-provided ecosystem. Let that sink in.Keil C51 is a silly compiler, but Silicon Labs does an excellent job hiding it under the hood even when running its Eclipse-based Simplicity Studio on Linux or macOS.Simplicity Configurator is the lightest-weight code generator in our round-up, using only 534 bytes of flash to house the entire DMX-512 receiver project. It was one of the easiest to use, and seemed to strike a good balance between abstraction, performance, and ease of use.Debugging speeds are snappy with a J-Link debugger, but at $35, the official Silicon Labs USB Debug Adapter is one of the cheapest first-party debugger in the round-up, and clones of the hardware are even cheaper.And call me old-fashioned, but I think the 8051 definitely has a place in 2017 especially among hobbyists and students, where its bit-addressable memory, easy-to-use peripherals, and fuse-free configuration help get students comfortable with microcontrollers quickly.Microchip megaAVR & tinyAVR 1-Series: Different strokes for different folks still with the best 8-bit toolchain availableThe megaAVR came in surprisingly flat for me: especially when compared with its lower-cost, new sibling, the tinyAVR 1-Series.Theres no comparison when it comes to price: tinyAVR has incredible value packing in a nice assortment of timers, analog peripherals (including a DAC), and a new 20 MHz internal oscillator while costing 20-40% less than the megaAVR.While the megaAVR has a perplexing debugging experience that requires two completely different interfaces and protocols to work with the part, the new one-wire UPDI interface the tinyAVR sports worked flawlessly in my testing.But thats the crux of the problem for the tinyAVR by shedding many of its megaAVR roots, Microchip ended up with a wonderful microcontroller that will be challenging to use for a large base of Atmel fans: indie developers and hobbyists who use low-cost, open-source programmers (which dont support the UPDI interface).While the tinyAVR wasnt the fastest part in the round-up (even among 8-bitters), it was the most efficient both in terms of active-mode power and clock efficiency. Amazingly, the AVR only uses about twice as many instructions as 16- and 32-bit parts when performing 16-bit math.Unfortunately, the AVR system as a whole is not without its issues. The Windows-only Atmel Studio is still buggy (especially with older megaAVR devices and AVR Dragon stuff in my tests), and there isnt an under-$50 low-cost debugger available (other than hacking apart Xplained Mini dev boards).In many ways, there seems to be a tacit demarcation Atmel creates between its hobbyist/indie developers, and the professional shops that use Atmel parts.As a professional embedded developer, I most definitely have access to Windows computers, and I have no problem blowing a few billable hours worth of pay on a $140 debugger.But even as popular as Atmel is among hobbyists, Atmel has largely stayed out of this space directly. Instead, theyve secured small-volume AVR sales by relying on the open-source community to build their own tools for themselves: turning out a slew of hardware and software used to program the megaAVR devices.While I applaud the efforts of these developers, these tools are inferior to Atmels. Their programming speeds are terrible, they dont support the new tinyAVR 1-Series devices, and they have absolutely no debug capability.Having said that, both the megaAVR and tinyAVR have the best toolchain available for 8-bit MCU development. The part supports a full, end-to-end Makefile-based GCC toolchain.If you love printf() debugging, would never touch a proprietary toolchain, and hate IDEs, megaAVR and old tinyAVR parts are definitely for you. The older ones are still available in DIP packages, and as you probably know, there are a ton of low-cost programmers available across the world. The online community is massive, and as clunky as I find Atmel START to be, I have to applaud its support for Makefile-based project generation.Consequently, the megaAVR remains the most open-source 8-bit microcontroller on the market by a long shot.But Id really like to see Microchip provide a PicKit-priced debugger with UPDI support and allow off-board debugging the way their PIC Curiosity Boards do.I also hope these open-source projects can add UPDI support to their tools, so that hobbyists and indie developers can start integrating the tinyAVR into their projects its a much better part, and if youre an AVR user with access to Atmel Studio, you really ought to buy an Xplained Mini board and take it for a spin.STM32F0: A low-cost, no-nonsense part with arguably the best Arm development ecosystem testedThe STM32F0 was the lowest-power Arm microcontroller in the round-up, and also one of the easiest to use. STM32CubeMX doesnt generate the most compact code on Arm (that honor belongs to Cypress PSoC Creator and Infineon DAVE), but it has a snappy interface, and the generated code is easy enough to manipulate for your own goals.I love the nearly-stock Eclipse-based environment that System Workbench for STM32 provides, and the ST-Link and excellent Discovery/Nucleo boards seals the deal for me.Most pros have used ST parts in their work, but for all these reasons, any hobbyist looking at moving to Arm should probably pick up a dev board from this ecosystem, too. ST has a huge market footprint, so theres tons of resources online aimed at both hobbyists and professionals.SAM D10: Killer performance & peripherals, but with runtime library hiccupsThe Microchip/Atmel SAM D10 (and the broader D11/D20/D21 ecosystem) has good value (considering their analog portfolio includes a DAC, and they have good timing options), and the SAM D10 was the most efficient part tested when running at full speed.Professionals will like the easy-to-use, well-documented header files, and hobbyists will appreciate the 1.27mm-pitch SOIC package options and GCC compilers that come with the Arm ecosystem. But before I grab this part for a project, Microchip really needs to fix the extremely slow, bloated peripheral library, and update their code-gen tool to do proper error-checking of clock and peripheral configurations.As it is, whenever I use Atmel START on the D10, I want to STOP almost immediately. And there are no current, stand-alone peripheral drivers that Microchip has released for this part, so unless you want to do register programming from scratch, youll be relying on third-party, open-source projects like Alex Taradovs code examples.Infineon XMC1100: Interesting peripheral perks make this Cortex-M0 stand outThe most interesting Arm chip was, without a doubt, the Infineon XMC1100 and I think professionals who may be wary of getting out of the ST/NXP/Atmel Arm ecosystem need to take a second look at these XMC1000 (and XMC4000) parts.The timer options are amazingly flexible, and you can squeeze fantastic performance out of the USIC module.Im going to go out on a limb and recommend that serious hobbyists who are building motor / lighting control projects look into these parts, too. DAVE makes setting up these complex peripherals painless, and the 38-pin TSSOP chips will be substantially easier to solder than the 0.5mm QFNs and QFPs you usually end up with in these pin counts.Like many of the parts reviewed here, the biggest problem for hobbyists and indie developers is the tiny online communities and lack of GitHub repos with open-source projects that use these chips. My advice be bold, and post in the forums. Infineon employees monitor and usually respond within a day or so.PIC16: Tons of peripherals with a slower, power-efficient coreWhen you compare the PIC16 with other 8-bit parts out there, its obviously a part built for low-power applications, and not processing power. And while the development ecosystem is workable, there are other parts more friendlier pathways especially for smaller shops, hobbyists, and students who need extremely low-cost tools (and free software).To add fuel to the PIC-vs-AVR debate, my testing found that a 32 MHz PIC16 is roughly equivalent to an AVR part running at 1.4 MHz (in terms of math performance), and 9 MHz (in terms of bit-shuffling performance).Having said that, the DMX-512 receiver seems a perfect match for the PIC16, and thats where it looks best in my testing: the PIC16 was the lowest-power 8-bit part in my testing.Its also full of timers and digital logic-oriented peripherals that make it suitable for funky special-purpose projects that require some crafty use of configurable logic and and the numerically-controlled oscillator these peripherals help offload the (relatively slow) CPU, at the expense of requiring more developer familiarity with the device and these peripherals.The usual Microchip gotchas apply: clunky IDE, expensive compilers, and expensive debuggers.The usual Microchip advantages apply: huge online community, seemingly infinite product lifetime guarantees, and DIP, SOIC, QFP, and QFN package availability.PIC24: An expensive MSP430 wannabe that doesnt hit the markThe PIC24 is nearly forgettable. In the biquad test, its marginally faster than the Renesas RL-78 but uses almost three times as much power. In the DMX-512 test, both the RL-78 and MSP430 beat it, too. It was also one of the least-endowed parts in the round-up (which really just means its expensive higher-end PIC24 parts have no shortage of peripherals).The usual Microchip gotchas apply: clunky IDE, expensive compilers, and expensive debuggers.The usual Microchip advantages apply: huge online community, seemingly infinite product lifetime guarantees, and DIP, SOIC, QFP, and QFN package availability.PIC32: An excellent 32-bit part that balances performance and power consumptionThe PIC32MM was my favorite Microchip part in the review. It brought in the lowest-power performance of every 32-bit part tested. Unfortunately, it was also the least-efficient 32-bit part tested in terms of math performance (well, excluding the couldnt-care-less-about-power Nuvoton M051), and its pretty spartan on peripherals it doesnt even have a hardware I2C controller.But PIC32MM parts have good flash / RAM density, and have simpler clocking / peripheral gating configurations than some of the more-flexible Arm parts, which makes them feel easier to program at a register level.Plus, they have a lot of headroom: I think the high-end PIC32MZ DA devices have a home among small industrial dev shops that need Linux-like HMI functionality but dont have the resources to bring a product like that to market.The usual Microchip gotchas apply: clunky IDE, expensive compilers, and expensive debuggers.The usual Microchip advantages apply: huge online community, seemingly infinite product lifetime guarantees, and DIP, SOIC, QFP, and QFN package availability.Renesas RL-78: An agile, low-power, easy-to-use 16-bit part you really ought to tryI had never picked up a Renesas part before, and when I went shopping for dev kits and stumbled on only a smattering of expensive, traditional systems, I was a little anxious. But I found the $25 RL78L1A Promotion Board, gave it a shot, and really enjoyed it.The RL-78 is a snappy architecture that competes with Arm parts in math performance, yet its also relatively inexpensive especially compared to the MSP430 and PIC24. It cant quite hit the MSP430 sleep-mode power consumption figures, but it gets close and is, by far, the most power-efficient 5V-capable part in the review.The code generator tool produces readable yet efficient code, and the IDE, e2studio, is Eclipse-based and is getting Linux and macOS support in the next release.Id complain about the dev board, but the new YRPBRL78G13 RL78/G13 development kit should remedy basically all my complains with it I cant wait for U.S. distributors to start carrying these. They could use a more active community and more people publishing code online, but I hope this article will help inspire some remedies for that.N76, HT66, and STM8: Low-cost parts with a smattering of development headachesThe STM8 is probably the nicest of the cheapie parts. It has nice peripherals and really good performance for an 8-bit part running at its frequency, but I think the entry-level 38-cent STM8S103F2P6 is a more compelling part than the higher-end one reviewed here simply because of its ultra-low price. The part I reviewed here looks a lot like the other 8-bit microcontrollers but with an ancient-looking IDE thats not nearly as productive as the competition. And almost everything out there has better power consumption figures.Still, this part is relatively cheap to get going (ahoy, $5 ST-Link clones), and the IDE and toolchain are completely free. But in that regard, you get what you pay for: STVD feels trapped in 2002, and theres no way to set up a more modern development and debugging environment for it 15Though, theres some emerging SDCC / GDB support for pairing it with an ST-Link in an open-source fashion, which might help it make inroads with the classic tinyAVR and megaAVR users.Youll be forking over quite a bit of money for a Keil C51 license to develop for the N76 all for a part that doesnt look much different than some entry-level EFM8s that have a day-and-night difference in ease of development. Still, at 23 cents per unit, its tough to beat for volume applications and hobbyists and hackers can probably get by with the 2K code limit of Keils evaluation version. SDCC users need not apply: theres no stand-alone tools for loading code into this part, and I doubt Vision can be coaxed into loading an SDCC-compiled hex file.The Holtek HT-66 has terrible processing performance, but barely uses any run-mode current theres plenty of application-specific models to choose from, and while the IDE is goofy, I found it to be fairly productive and its completely free. Careful, though: only the more-expensive V parts have on-chip debugging.STC8: A neat performance-heavy part for hacking but probably not for serious, professional workI think every hacker and advanced hobbyist really ought to throw $10 at AliExpress/Taobao and get some STC15 and STC8 parts just for fun.Both are jam-packed full of peripherals and memory (more than every other part reviewed), and the STC8 is also really fast. There are some interesting projects you can do with a part that hits your C interrupt code 10 clock cycles after an interrupt occurs thats 320 nanoseconds. Both these parts support debugging over UART, so theres no proprietary debugger to purchase.I wouldnt seriously consider using these parts in U.S-based commercial work, as we have no access to STC inventory here, but the part is just plain fun to play with.ON Semiconductor LC-87: SkipYou can probably skip over the ON Semiconductor LC87. This is a rare part outside the Japanese market, and it looks like its on its way out the door. I called Altium to try to get an evaluation version of the Tasking LC87 toolset, and the person I talked to had never heard of the LC87 before, and was almost positive they hadnt made a compiler for it for at least three years. This part has terrible power consumption, few peripherals, and the worst development environment I saw in this review. Skip.Kinetis KL03: Sleep-mode specialist not for beginnersWhile the Kinetis KL03 has excellent deep-sleep current and ultra-tiny CSP package availability, it definitely feels like a specialized part not useful for the applications I evaluated. It has far fewer peripherals than the other parts reviewed, and despite NXPs low-power claims, was consistently in the middle of my Arm rankings for the DMX-512 receiver test though it nearly matches the SAM D10 in full-speed active mode.Kinetis SDK is awkward to use, and the dev boards are terrible requiring a lot of reverse-engineering and hacking to get the board doing anything other than running pre-written demos (especially if youre interested in measuring power consumption). Still, MCUXpresso is a productive, modern Eclipse-based IDE, and the KL03 has some of the lowest-leakage power modes out there, which means you can get 8-bit-like performance when youre running an RTC or interrupt wake-up project from a coin-cell battery.Kinetis KE04: Decent peripheral assortment with a powerful yet clunky code gen toolThe Kinetis KE04 had pretty heavy power consumption in my testing but this was largely due to the heavy-handed Processor Expert code that Kinetis Design Studio generated. This environment is really suited to much larger, faster microcontrollers running RTOSes and not needing especially good low-level I/O performance.But, hey, if you dont really care about performance, the nice thing about Processor Expert is it abstracts the peripherals to such a high level that youll never need to crack open a datasheet for the part if youre using the peripherals in normal configurations.Plus, the KE04 (and KE02) are 5V-compatible parts, and theyre available in old-school, easy-to-solder 1.27mm SOIC and 0.8mm packages so I could imagine hobbyists would find this part useful.LPC811: Few perks, and less interesting than the LPC810The LPC810 drew people in with its odd, 8-pin DIP form-factor. That chip has since been discontinued, but the LPC81x line remains. The LPC811 reviewed here is sparse on peripherals not even having an ADC and brought in poor performance. Theres really nothing that this part does that you cant get from one of the other vendors; but dont discredit NXP completely their higher-end offerings have some interesting capabilities (like dual-core Cortex-M4/M0 designs), and their development environment, MCUXpresso, is an inoffensive Eclipse system.PSoC 4000S & MSP430: Bottom-of-the-barrel parts that offer a glimpse into nice ecosystemsI hesitated to review the PSoC and MSP430 because they tend to be relatively expensive parts, so in a $1 shoot-out, you end up with bottom-end parts that dont look nearly as useful as their higher-cost relatives. If you really want to get a feel for what the MSP430 or PSoC parts can do, I recommend buying into a higher-end part preferably on one of the excellent dev boards that these manufacturers make.PSoC Creator and the reconfigurable digital and analog blocks in the PSoC line draw many professional and hobbyist users into the architecture but instead of grabbing the 4000S from this review, reach for a PSoC5 (or soon-to-launch PSoC6) dev board to get a feel for the platform.Same with the MSP430. In the DMX-512 test, it dominated in power consumption, but barely put up marks in any other category (this is especially challenging when you have no hardware multiplier, and only a smattering of peripherals).Still, the part has a solid development ecosystem with Code Composer Studio and a choice between the proprietary (but now free) TI compiler, and the open-source GCC one. Plus, hobbyists will love the easy Arduino migration path (with Code Composer Studio directly supporting Energia *.ino sketch projects) and $10 dev boards.And really, everyone starting a battery-based product needs to go buy an MSP430 Launch Pad and play around with it these really are amazing parts that still have a lot of relevance in 2017.Nuvoton M051: Ecosystem issues stifle a performance-packed partThe Nuvoton M051 one of the most-endowed parts reviewed suffers ecosystem issues that Nuvoton could easily remedy in the future, so Ill reserve judgment. Theres no manufacturer-provided Eclipse-based IDE instead, the only IDE options are CooCox, and Keil Vision neither of which Im particularly fond of.I was able to get CooCox working (though the peripheral libraries that are in the CooCox repo are old and full of bugs). The M0 had some of the worst power-consumption figures in the review, but it makes up for that with tons of communications peripherals, beautiful 32-bit control-friendly timers, and easily-digestible runtime libraries and documentation that are far easier to use than other vendors. When Nuvoton fixes the IDE absence, Ill definitely move this part from the meh to yeah column since it accomplishes all of these feats while remaining one of the lowest-cost Arm microcontrollers out there.I had a ton of fun playing with all these different parts for the last few months for this microcontroller review, and in many ways, came away thinking what I already knew: there is no perfect microcontroller no magic bullet that will please all users. What I did learn, however, is its getting easier and easier to pick up a new architecture youve never used before, and there have never been more exciting ecosystems to choose from.And thats what I want people to think about as they walk away from this microcontroller review. If youre an Arduino hobbyist looking where to go next, I hope you realize there are a ton of great, easy-to-use choices. And for professional developers and hardcore hackers, perhaps theres an odd-ball architecture youve noticed before, but never quite felt like plunging into nows the time.Its an exciting time to be involved with electronics whatever parts you choose to pick up, I hope youve enjoyed learning about whats out there, and can get inspired to go build something great. Definitely leave a note in the comments below if youve got something to contribute to the discussion! ",
        "_version_":1718441032205467648},
      {
        "story_id":20447378,
        "story_author":"susam",
        "story_descendants":65,
        "story_score":150,
        "story_time":"2019-07-16T04:03:25Z",
        "story_title":"Lunar Eclipse: An Email to a Daughter and Son-in-Law",
        "search":["Lunar Eclipse: An Email to a Daughter and Son-in-Law",
          "https://groups.google.com/d/msg/b-a-s/9rcz9MbC5p8/2Q8txQgGBAAJ",
          "Vasanth BRunread,Jul 15, 2019, 11:50:39 PM7/15/19to TS...@yahoogroups.com, subramanya cr, absb...@googlegroups.com, b-...@googlegroups.com, Shyamal Sarkar, ind...@googlegroups.com, Military...@yahoogroups.com, ma...@randhirmishra.com, s_chowdh...@yahoo.com, Swaga...@gmail.com, vas...@learningaccord.com AN EMAIL TO A DAUGHTER & SON-IN-LAW. Tue, 16 Jul 19 / 01:32 Dear Shirin & Arun, Place Flight M British Standard Time Indian Standard Time Duration Distance A/C Speed Alt London BA 119 D Tue, 16 Jul / 14:25 (+01:00) Tue, 16 Jul / 18:55 (+05:30) B 777 Bengaluru A Wed, 17 Jul / 00:05 (+01:00) Wed, 17 Jul / 04:35 (+05:30) 09:40 hr 8,010 km 869 km/h 11,280 mt On Wed, 17 Jul, a Partial Lunar Eclipse begins at (all IST) 01:31:43, reaches Maximum 65% at 03:00:44 and Ends at 04:29:39. You will be landing in Bengaluru, at 04:35; just 5 minutes after the Eclipse ends. Solar Eclipses are Virtual. They really do not exist. They are Localised OUR Views of the sun, from an individual on the earth. They are no different from holding an umbrella over your head, when walking under the sun. Instead of the umbrella, you are \"holding\"the moon over your head, blocking the sun, only to you, locally; only about 30 km around you. It is about 30 km radius circle, because your umbrella is about 1 mt diametre and the moon's diametre is 3,476 km; but then, it is 362,600 km to 405,600 km away. Lunar Eclipses are real. They are actually happening of an event on the moon. The earth's shadow, is actually falling on the moon. All the persons in half earth who can see the moon, can see the Eclipse, in the same manner. When you take off from London, set your watch 04 hr 30 mt head. In that watch, when it isIndia's time, 01:31:43 (correct to the second), the Eclipse begins. You can calibrate your watch, correct to the second. Please try to get a seat on the right side, so that you can get a view on the West side. Do not sit over the wing. When the Eclipse starts at 01:32, you will be over Oman (wave to Arun's sister Anita). From Oman to Bengaluru, you willfly due South-East. You do not have to crane your neck. See the moon at level, straight through your window. This is a rare opportunity, to see the entire Lunar Eclipse of 02 hr 57 mt 56 sc through the window of a plane. Take your BEST binoculars with you. If you do not have one, purchase it in Belfast or London. At 11.28 km high, above the atmosphere, your moon sight will be in crystal clear sky. As the Earth's sharply edged shadow races across the moon's surface, at about 1,000 km per hour, through your binoculars, you will see the edge of the shadow climbing up the moon's mountains and dipping below into the valleys. From the Earth's surface, the moon appears flat. In reality, the centre of the moon is 1,738 km nearer to you. From a plane, at 11 km above the Earth's surface, as the Earth's shadow's edge races across the moon's surface, your eyes will suddenly see the moon in 3 dimensions. Just intently stare at the moon. Your brain will suddenly pop up the moon into 3 dimensions. This is a million pounds worth ofexperience, which you can NEVER have fromthe Earth's surface. Looking forward to meeting you. DAD BHU1 LORDunread,Jul 16, 2019, 7:09:25 AM7/16/19to b-...@googlegroups.comNo offence but why is this on a public mailing list keerthi kiranunread,Jul 16, 2019, 7:38:52 AM7/16/19to BASHi Vasanth,Nice to see your mail after a long time :)It was a nice read... I wish I was on that flight :)I saw one error in your mail. The eclipse will happen in the eastern sky. So they should get a view on the \"East side of the plane\".Also, they can inform the crew so that the crew informs the other passengers. All the best to them. Hope they get to witness a wonderful eclipse (and may be take a photo from the cockpit).Thanks and Regards,Keerthi -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. keerthi kiranunread,Jul 16, 2019, 7:51:22 AM7/16/19to BASHi Bhu1 Lord,The mail (although written in a personal context) is informational. Also, it is a good read :)Regards,Keerthi -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. Message has been deletedSusam Palunread,Jul 16, 2019, 8:15:54 AM7/16/19to Bangalore Astronomical SocietyThe eclipse would begin at 01:32 AM IST, i.e., 00:02 AM GST at Oman when the plane is likely flying over Oman. In Muscat (Oman), the moon would rise at 06:39 PM GST in the East-southeast, cross the local meridian at 12:09 AM GST, and set at 05:40 AM GST in the West-southwest [1]. Therefore, when the eclipse would start at 00:02 AM GST, the moon would be in the eastern sky close to the local meridian at Muscat.But then most of the remaining eclipse seems to take place in the western sky. For example, in Bangalore, the moon would rise at 06:30 PM IST in the East-southeast, cross the local meridian at 12:20 AM IST, and set at 06:10 AM IST in the West-southwest [2]. Therefore, when the eclipse would end at 04:29 AM IST, the moon would be in the western sky ready to set in about 1.5 hours. It looks like most of the eclipse would indeed be visible in the western sky from this particular flight.[1]: https://www.timeanddate.com/moon/oman/muscat[2]: https://www.timeanddate.com/moon/india/bangaloreThanks,Susam -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group.Shashikiran Kolarunread,Jul 16, 2019, 2:50:55 PM7/16/19to Bangalore Astronomical SocietyYes, as Susam says, with respect to Muscat and India, the Moon spends most of its time in the Western half of the sky when it enters the Earth's umbral shadow.Muscat is 22, or about 90 minutes, West of Bangalore. So, if the partial eclipse begins at 1.30AM here, it would do so at 12.00AM at Muscat. Since this is the full moon, it's in opposition and hence transits to Western part of the sky at (local) 12.00AM. Also, the plane would be flying Eastwards, causing the Moon would always hang in the Western part of the sky thereon.That lunar eclipses always begin at the Eastern limb of the Moon was perhaps confused with the Moon being in the Eastern half of the sky?As an aside, it is likely that certain depictions on temples indicate eclipses. In the attached picture, a snake is seen moving towards a moon (shown as a crescentto perhaps distinguish it from the Sun) and likely indicates Rahu/Ketu about to swallow the Moon causing a lunar eclipse. The temple may have been dedicated during such an event.Thanks,-ShashiRameshwara temple, Hale Palya, Hassan District, Karnataka.JPGBHU1 LORDunread,Jul 16, 2019, 2:56:15 PM7/16/19to b-...@googlegroups.comI guess I misunderstood the mail when I first read it as it was written in personal context and thought it was meant to be sent to somebody personallyBut I really appreciate the result especially regarding the accurate timings mentioned in the mailI'm eagerly waiting for the eclipse and wish all of you clear skieskeerthi kiranunread,Jul 17, 2019, 10:08:39 AM7/17/19to BASHi Susam, Shashikiran,You guys are right. I didn't think it through.For Europe, the eclipse was already underway when the moon rose. So I was only thinking about that.But for Asia, as you guys said, the eclipse was mostly in the western sky.Regards,Keerthi You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. BHU1 LORDunread,Jul 17, 2019, 10:11:43 AM7/17/19to b-...@googlegroups.comDid anybody get some good pictures of the eclipseHere the clouds started moving SW during max eclipse and the moon was completely coveredI would really appreciate if somebody got a pic of the moon during max eclipsekeerthi kiranunread,Jul 17, 2019, 10:15:24 AM7/17/19to b-...@googlegroups.comI can't call them good photos. But I got a few from Paris from my balcony.Regards,Keerthi You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. 20190717030019__MG_1631.JPG.jpgBHU1 LORDunread,Jul 17, 2019, 10:20:27 AM7/17/19to b-...@googlegroups.comthe pictures are awesomeI really wished I had that crystal clear sky hereAjay Talwarunread,Jul 17, 2019, 10:50:55 AM7/17/19to b-...@googlegroups.comHi Bhuvan,Here is an image of last night's eclipse. This image of the Lunar eclipse was shot at 02:52 AM (I.S.T.) on 17 July, using the 14inch telescope and Hyperstar lens. Location - Tijara, Rajasthan.This image is a HDR composite of 5 images, clicked in quick succession, and varying exposures. The longer exposure was good for the darker portion of the eclipse, and the shorter exposure was good for the brighter part of the Moon, which was not inside the umbra. The 5 images were combined to show the brighter as well as the darker portions of this partial lunar eclipse together in an HDR image. Monsoon clouds can be seen all around the eclipsed moon.Ajay Talwar -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group.2019-07-17 Partial Lunar Eclipse Hyperstar HDR.jpgBHU1 LORDunread,Jul 17, 2019, 11:05:55 AM7/17/19to b-...@googlegroups.comThis picture is worth all the effort you put into itI like how to incorporated the clouds into the picture, it gives kind of a mysterious feel to itkeerthi kiranunread,Jul 17, 2019, 12:58:22 PM7/17/19to b-...@googlegroups.comHere are some good photos clicked by Prabhu Kutti, Naveen and Niranjan from our WhatsApp group.Regards,Keerthikeerthi kiranunread,Jul 17, 2019, 12:59:50 PM7/17/19to b-...@googlegroups.comAwesome image Ajay :)But don't share it with TV channels, they will use it will special effects to scare people... :D Huligun .comunread,Jul 17, 2019, 6:46:28 PM7/17/19to b-...@googlegroups.comGood photos all, thanks for sharing !!! Akarsh Simhaunread,Jul 18, 2019, 8:14:52 AM7/18/19to b-...@googlegroups.comYes, very nice to hear from Vasanth after a long long time!ashiqunread,Jul 18, 2019, 2:16:32 PM7/18/19to b-...@googlegroups.com "],
        "story_type":"Normal",
        "url_raw":"https://groups.google.com/d/msg/b-a-s/9rcz9MbC5p8/2Q8txQgGBAAJ",
        "comments.comment_id":[20448578,
          20449793],
        "comments.comment_author":["bnegreve",
          "hluska"],
        "comments.comment_descendants":[2,
          2],
        "comments.comment_time":["2019-07-16T09:15:44Z",
          "2019-07-16T13:26:35Z"],
        "comments.comment_text":["> Solar Eclipses are Virtual. They really do not exist. [..] Lunar Eclipses are real. They are actually happening of an event on the moon.<p>During a lunar eclipse, the earth projects its shadow on the moon. During a Solar Eclipse, the moon projects its shadow on the earth. Only the shadow is smaller. I'm not sure what makes lunar eclipses more real.",
          "In May, my three year old daughter and I flew out to Vancouver Island, Canada to spend a couple of weeks with my Dad. It was my daughters second time out to the island, though the first time was for my Grandmas funeral when she was six months old.<p>This email from a father to a child reminds me of the emails my Dad sent us in the lead up to our trip. Ive never seen my Dad so excited for anything. The day of the flight, he was awake at 1am his local time to start watching the flight schedule and the weather. He wrote lots of reminders, to make sure that I packed Laurens identification, Pull Ups and wipes. And he wanted to make sure that Lauren had a window seat where she wouldnt just stare out at a wing. No no no, his only granddaughter was going to have a view. (Strangely, the day of the flight was cloudy as hell until we got to the mountains, then we flew over the Rockies with barely a cloud in sight. It was as if Grandpa put in his order for weather that would give his granddaughter the best view imaginable).<p>I was pretty scared in the lead up. I hate flying at the best of times and this was my first solo flight with a three year old. Shit, Im hardly qualified to deal with Webpack when it has a tantrum. This is a bloody human life in a plane.<p>The flight was amazing and reading my Dads dispatches every ten minutes added to our excitement. When we landed, my Dad and stepmother were visibly excited. My daughter ran to them and the three of them had a wonderful moment.<p>My Dads emails were purely about his excitement. And this dads email makes him sound just as excited as my Dad was. I am only going to have one child and I pray that one day, I get to be that excited about my little girl hopping on a flight to visit me. If my excited message happens to go public and it contains some factual issues, I hope to hell that the people who read it recognize it for what it is...Ill just be a really excited dad who is doing whatever I can to pass the time before I get to see someone I love more than life! :)"],
        "id":"dcc71f51-9476-47f3-ab47-a9d76e958b10",
        "url_text":"Vasanth BRunread,Jul 15, 2019, 11:50:39 PM7/15/19to TS...@yahoogroups.com, subramanya cr, absb...@googlegroups.com, b-...@googlegroups.com, Shyamal Sarkar, ind...@googlegroups.com, Military...@yahoogroups.com, ma...@randhirmishra.com, s_chowdh...@yahoo.com, Swaga...@gmail.com, vas...@learningaccord.com AN EMAIL TO A DAUGHTER & SON-IN-LAW. Tue, 16 Jul 19 / 01:32 Dear Shirin & Arun, Place Flight M British Standard Time Indian Standard Time Duration Distance A/C Speed Alt London BA 119 D Tue, 16 Jul / 14:25 (+01:00) Tue, 16 Jul / 18:55 (+05:30) B 777 Bengaluru A Wed, 17 Jul / 00:05 (+01:00) Wed, 17 Jul / 04:35 (+05:30) 09:40 hr 8,010 km 869 km/h 11,280 mt On Wed, 17 Jul, a Partial Lunar Eclipse begins at (all IST) 01:31:43, reaches Maximum 65% at 03:00:44 and Ends at 04:29:39. You will be landing in Bengaluru, at 04:35; just 5 minutes after the Eclipse ends. Solar Eclipses are Virtual. They really do not exist. They are Localised OUR Views of the sun, from an individual on the earth. They are no different from holding an umbrella over your head, when walking under the sun. Instead of the umbrella, you are \"holding\"the moon over your head, blocking the sun, only to you, locally; only about 30 km around you. It is about 30 km radius circle, because your umbrella is about 1 mt diametre and the moon's diametre is 3,476 km; but then, it is 362,600 km to 405,600 km away. Lunar Eclipses are real. They are actually happening of an event on the moon. The earth's shadow, is actually falling on the moon. All the persons in half earth who can see the moon, can see the Eclipse, in the same manner. When you take off from London, set your watch 04 hr 30 mt head. In that watch, when it isIndia's time, 01:31:43 (correct to the second), the Eclipse begins. You can calibrate your watch, correct to the second. Please try to get a seat on the right side, so that you can get a view on the West side. Do not sit over the wing. When the Eclipse starts at 01:32, you will be over Oman (wave to Arun's sister Anita). From Oman to Bengaluru, you willfly due South-East. You do not have to crane your neck. See the moon at level, straight through your window. This is a rare opportunity, to see the entire Lunar Eclipse of 02 hr 57 mt 56 sc through the window of a plane. Take your BEST binoculars with you. If you do not have one, purchase it in Belfast or London. At 11.28 km high, above the atmosphere, your moon sight will be in crystal clear sky. As the Earth's sharply edged shadow races across the moon's surface, at about 1,000 km per hour, through your binoculars, you will see the edge of the shadow climbing up the moon's mountains and dipping below into the valleys. From the Earth's surface, the moon appears flat. In reality, the centre of the moon is 1,738 km nearer to you. From a plane, at 11 km above the Earth's surface, as the Earth's shadow's edge races across the moon's surface, your eyes will suddenly see the moon in 3 dimensions. Just intently stare at the moon. Your brain will suddenly pop up the moon into 3 dimensions. This is a million pounds worth ofexperience, which you can NEVER have fromthe Earth's surface. Looking forward to meeting you. DAD BHU1 LORDunread,Jul 16, 2019, 7:09:25 AM7/16/19to b-...@googlegroups.comNo offence but why is this on a public mailing list keerthi kiranunread,Jul 16, 2019, 7:38:52 AM7/16/19to BASHi Vasanth,Nice to see your mail after a long time :)It was a nice read... I wish I was on that flight :)I saw one error in your mail. The eclipse will happen in the eastern sky. So they should get a view on the \"East side of the plane\".Also, they can inform the crew so that the crew informs the other passengers. All the best to them. Hope they get to witness a wonderful eclipse (and may be take a photo from the cockpit).Thanks and Regards,Keerthi -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. keerthi kiranunread,Jul 16, 2019, 7:51:22 AM7/16/19to BASHi Bhu1 Lord,The mail (although written in a personal context) is informational. Also, it is a good read :)Regards,Keerthi -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. Message has been deletedSusam Palunread,Jul 16, 2019, 8:15:54 AM7/16/19to Bangalore Astronomical SocietyThe eclipse would begin at 01:32 AM IST, i.e., 00:02 AM GST at Oman when the plane is likely flying over Oman. In Muscat (Oman), the moon would rise at 06:39 PM GST in the East-southeast, cross the local meridian at 12:09 AM GST, and set at 05:40 AM GST in the West-southwest [1]. Therefore, when the eclipse would start at 00:02 AM GST, the moon would be in the eastern sky close to the local meridian at Muscat.But then most of the remaining eclipse seems to take place in the western sky. For example, in Bangalore, the moon would rise at 06:30 PM IST in the East-southeast, cross the local meridian at 12:20 AM IST, and set at 06:10 AM IST in the West-southwest [2]. Therefore, when the eclipse would end at 04:29 AM IST, the moon would be in the western sky ready to set in about 1.5 hours. It looks like most of the eclipse would indeed be visible in the western sky from this particular flight.[1]: https://www.timeanddate.com/moon/oman/muscat[2]: https://www.timeanddate.com/moon/india/bangaloreThanks,Susam -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group.Shashikiran Kolarunread,Jul 16, 2019, 2:50:55 PM7/16/19to Bangalore Astronomical SocietyYes, as Susam says, with respect to Muscat and India, the Moon spends most of its time in the Western half of the sky when it enters the Earth's umbral shadow.Muscat is 22, or about 90 minutes, West of Bangalore. So, if the partial eclipse begins at 1.30AM here, it would do so at 12.00AM at Muscat. Since this is the full moon, it's in opposition and hence transits to Western part of the sky at (local) 12.00AM. Also, the plane would be flying Eastwards, causing the Moon would always hang in the Western part of the sky thereon.That lunar eclipses always begin at the Eastern limb of the Moon was perhaps confused with the Moon being in the Eastern half of the sky?As an aside, it is likely that certain depictions on temples indicate eclipses. In the attached picture, a snake is seen moving towards a moon (shown as a crescentto perhaps distinguish it from the Sun) and likely indicates Rahu/Ketu about to swallow the Moon causing a lunar eclipse. The temple may have been dedicated during such an event.Thanks,-ShashiRameshwara temple, Hale Palya, Hassan District, Karnataka.JPGBHU1 LORDunread,Jul 16, 2019, 2:56:15 PM7/16/19to b-...@googlegroups.comI guess I misunderstood the mail when I first read it as it was written in personal context and thought it was meant to be sent to somebody personallyBut I really appreciate the result especially regarding the accurate timings mentioned in the mailI'm eagerly waiting for the eclipse and wish all of you clear skieskeerthi kiranunread,Jul 17, 2019, 10:08:39 AM7/17/19to BASHi Susam, Shashikiran,You guys are right. I didn't think it through.For Europe, the eclipse was already underway when the moon rose. So I was only thinking about that.But for Asia, as you guys said, the eclipse was mostly in the western sky.Regards,Keerthi You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. BHU1 LORDunread,Jul 17, 2019, 10:11:43 AM7/17/19to b-...@googlegroups.comDid anybody get some good pictures of the eclipseHere the clouds started moving SW during max eclipse and the moon was completely coveredI would really appreciate if somebody got a pic of the moon during max eclipsekeerthi kiranunread,Jul 17, 2019, 10:15:24 AM7/17/19to b-...@googlegroups.comI can't call them good photos. But I got a few from Paris from my balcony.Regards,Keerthi You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. 20190717030019__MG_1631.JPG.jpgBHU1 LORDunread,Jul 17, 2019, 10:20:27 AM7/17/19to b-...@googlegroups.comthe pictures are awesomeI really wished I had that crystal clear sky hereAjay Talwarunread,Jul 17, 2019, 10:50:55 AM7/17/19to b-...@googlegroups.comHi Bhuvan,Here is an image of last night's eclipse. This image of the Lunar eclipse was shot at 02:52 AM (I.S.T.) on 17 July, using the 14inch telescope and Hyperstar lens. Location - Tijara, Rajasthan.This image is a HDR composite of 5 images, clicked in quick succession, and varying exposures. The longer exposure was good for the darker portion of the eclipse, and the shorter exposure was good for the brighter part of the Moon, which was not inside the umbra. The 5 images were combined to show the brighter as well as the darker portions of this partial lunar eclipse together in an HDR image. Monsoon clouds can be seen all around the eclipsed moon.Ajay Talwar -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group. -- You received this message because you are subscribed to the Google Groups \"Bangalore Astronomical Society\" group.2019-07-17 Partial Lunar Eclipse Hyperstar HDR.jpgBHU1 LORDunread,Jul 17, 2019, 11:05:55 AM7/17/19to b-...@googlegroups.comThis picture is worth all the effort you put into itI like how to incorporated the clouds into the picture, it gives kind of a mysterious feel to itkeerthi kiranunread,Jul 17, 2019, 12:58:22 PM7/17/19to b-...@googlegroups.comHere are some good photos clicked by Prabhu Kutti, Naveen and Niranjan from our WhatsApp group.Regards,Keerthikeerthi kiranunread,Jul 17, 2019, 12:59:50 PM7/17/19to b-...@googlegroups.comAwesome image Ajay :)But don't share it with TV channels, they will use it will special effects to scare people... :D Huligun .comunread,Jul 17, 2019, 6:46:28 PM7/17/19to b-...@googlegroups.comGood photos all, thanks for sharing !!! Akarsh Simhaunread,Jul 18, 2019, 8:14:52 AM7/18/19to b-...@googlegroups.comYes, very nice to hear from Vasanth after a long long time!ashiqunread,Jul 18, 2019, 2:16:32 PM7/18/19to b-...@googlegroups.com ",
        "_version_":1718441045355659264},
      {
        "story_id":18972516,
        "story_author":"curtis",
        "story_descendants":142,
        "story_score":600,
        "story_time":"2019-01-22T21:11:20Z",
        "story_title":"A meteorite hit the moon during yesterday's total lunar eclipse",
        "search":["A meteorite hit the moon during yesterday's total lunar eclipse",
          "https://www.newscientist.com/article/2191526-a-meteorite-hit-the-moon-during-yesterdays-total-lunar-eclipse/",
          "The meteorite impact caused a bright flash, indicated by the arrowJose M. Madiedo Observers of Mondays lunar eclipse were blessed with the first known sighting of a meteorite impact during such an event. The so-called super wolf blood moon was eagerly watched by millions of people around the world, mostly via live streaming video.During the eclipse,some people noticed a tiny flash, a brief yellow-white speck, popping up on the lunar surface during the online broadcasts. One Reddit user raised the possibility that this was a meteorite impactand othersscoured eclipse footage for evidence of the event. A flash is visible in at least three different videos.Advertisement Jose Maria Madiedo at the University of Huelva in Spain has confirmed that the impact is genuine.For years,he and his colleagues have been hoping to observe a meteorite impact on the moon during a lunar eclipse, but the brightness of these events can make that very difficult lunar meteorite impacts have been filmed before, but not during an eclipse. Read more: NASA puzzles over invisible moon impact On this occasion, Madiedo doubled the number of telescopes trained on different parts of the moon from four to eight in the hope of seeing an impact. I had a feeling, this time will be the time it will happen, says Madiedo. After the eclipse, software automatically pinpointed a flash in imagery recorded by several of his telescopes. This helps to confirm that the flashes seen by live stream-watchers were not just optical anomalies on camera sensors. I was really, really happy when this happened, says Madiedo. He notes that the flash was quite bright and it struck the moon at a moment when the eclipse was not overly luminous itself, perhaps making the strike easier to detect. Although he has not yet formally calculated an estimate for the size of the space rock that collided with the moon, Madiedo thinks it probably had a mass of about 2 kilograms and was roughly the size of a football. Experience Argentinas 2020 total solar eclipse: Witness a rare celestial event on a New Scientist Discovery Tour The combination of a darkened surface and a lot of people watching made it much more likely that the flash of impact was seen and it reminds us that the solar system is still a very dynamic place, says Robert Massey at the Royal Astronomical Society. Article amended on 23 January 2019 When this article was first published it included a video of a collision recorded in December 2018. It has been replaced with the correct video showing the January meteorite impact during the lunar eclipse. Article amended on 6 February 2019 We corrected the measure of the mass of the meteorite More on these topics: the moon eclipses "],
        "story_type":"Normal",
        "url_raw":"https://www.newscientist.com/article/2191526-a-meteorite-hit-the-moon-during-yesterdays-total-lunar-eclipse/",
        "url_text":"The meteorite impact caused a bright flash, indicated by the arrowJose M. Madiedo Observers of Mondays lunar eclipse were blessed with the first known sighting of a meteorite impact during such an event. The so-called super wolf blood moon was eagerly watched by millions of people around the world, mostly via live streaming video.During the eclipse,some people noticed a tiny flash, a brief yellow-white speck, popping up on the lunar surface during the online broadcasts. One Reddit user raised the possibility that this was a meteorite impactand othersscoured eclipse footage for evidence of the event. A flash is visible in at least three different videos.Advertisement Jose Maria Madiedo at the University of Huelva in Spain has confirmed that the impact is genuine.For years,he and his colleagues have been hoping to observe a meteorite impact on the moon during a lunar eclipse, but the brightness of these events can make that very difficult lunar meteorite impacts have been filmed before, but not during an eclipse. Read more: NASA puzzles over invisible moon impact On this occasion, Madiedo doubled the number of telescopes trained on different parts of the moon from four to eight in the hope of seeing an impact. I had a feeling, this time will be the time it will happen, says Madiedo. After the eclipse, software automatically pinpointed a flash in imagery recorded by several of his telescopes. This helps to confirm that the flashes seen by live stream-watchers were not just optical anomalies on camera sensors. I was really, really happy when this happened, says Madiedo. He notes that the flash was quite bright and it struck the moon at a moment when the eclipse was not overly luminous itself, perhaps making the strike easier to detect. Although he has not yet formally calculated an estimate for the size of the space rock that collided with the moon, Madiedo thinks it probably had a mass of about 2 kilograms and was roughly the size of a football. Experience Argentinas 2020 total solar eclipse: Witness a rare celestial event on a New Scientist Discovery Tour The combination of a darkened surface and a lot of people watching made it much more likely that the flash of impact was seen and it reminds us that the solar system is still a very dynamic place, says Robert Massey at the Royal Astronomical Society. Article amended on 23 January 2019 When this article was first published it included a video of a collision recorded in December 2018. It has been replaced with the correct video showing the January meteorite impact during the lunar eclipse. Article amended on 6 February 2019 We corrected the measure of the mass of the meteorite More on these topics: the moon eclipses ",
        "comments.comment_id":[18972967,
          18973434],
        "comments.comment_author":["scrumbledober",
          "sizzzzlerz"],
        "comments.comment_descendants":[2,
          9],
        "comments.comment_time":["2019-01-22T21:56:02Z",
          "2019-01-22T22:48:18Z"],
        "comments.comment_text":["Scott Manley did a great quick video about this<p><a href=\"https://www.youtube.com/watch?v=Smp7TqccTpY\" rel=\"nofollow\">https://www.youtube.com/watch?v=Smp7TqccTpY</a>",
          "I know its just a coincidence that the meteor hit during a lunar eclipse while millions of people might have been watching (and probably not seeing it). But, it gives me chills to think that lump of rock has been orbiting through the solar system for possibly billions of years with its unknown fate already sealed. All it took was for the Sun and all the planets and moons to go through their cycles an uncountable number of times to bring it to its ultimate conclusion.<p>Now, unknown asteroid, your watch is done."],
        "id":"5005e6d8-2a7b-4cfa-94a6-d93e80ddfa26",
        "_version_":1718441007734849537},
      {
        "story_id":20351953,
        "story_author":"CaliforniaKarl",
        "story_descendants":13,
        "story_score":53,
        "story_time":"2019-07-04T06:03:03Z",
        "story_title":"During a Solar Eclipse, What Are Plants Doing?",
        "search":["During a Solar Eclipse, What Are Plants Doing?",
          "https://www.nytimes.com/2019/07/02/science/plants-solar-eclipse.html",
          "TrilobitesResearch conducted during the Great American Eclipse of 2017 suggested the suns midday disappearance shocked some plants.Credit...Dr. Mario BretfeldJuly 2, 2019As the total solar eclipse crosses South America on Tuesday, it wont just be people oohing and ahhing as the sun is blotted out.Other living things will have their own responses, too some of which we are just beginning to understand. As some scientists used the Great American Eclipse in August 2017 to watch how bees and birds dealt with sudden midday darkness, researchers in Wyoming investigated big sagebrush, a shaggy, aromatic desert shrub that grows throughout the western United States.Tracking its reactions at the leaf level, scientists saw it experience a slowdown in activity as darkness fell, followed by shock at the suns surprise return. The study, published in June in Scientific Reports, adds to a small clump of botanical eclipse research, all produced by people with the ability to wonder, even as a celestial event occurs: What are the plants getting up to?[Like the Science Times page on Facebook. | Sign up for the Science Times newsletter.]Plants havent really been well-documented during the eclipse, said Daniel Beverly, a Ph.D. student in botany and hydrology at the University of Wyoming and the papers lead author. When he heard an eclipse was coming, he saw the chance for a once-in-a-lifetime data set.Big sagebrush spends much of its life in the sun, and it covers such a large portion of the country, from Oregon down to New Mexico, Mr. Beverly said, making it a good subject for study. He and his colleagues chose a patch close to Yellowstone National Park. They set up instruments that could measure photosynthetic rate, as well as the speed of transpiration how quickly its leaves lose water.As the light faded and the temperature dropped during the 80 or so minutes before totality, Mr. Beverly and his team saw a corresponding decrease in photosynthesis and transpiration. The plant responded as if it was dusk, he said.During the two minutes and 18 seconds of complete darkness, both rates fell further. Although they did not quite reach the slowdown level of an average nighttime, it was a much more precipitous drop than would have occurred for a passing cloud.But when the sun appeared again, the shrubs were blindsided.Thats when we saw the most evident signals of stress, said Mr. Beverly. It caused a disruption in the photosynthetic pathways.Although they could not test this directly, he guesses that the shock of sudden sunlight messed with each plants circadian rhythm the internal clock that determines an organisms daily cycle of activity.Over the course of the eclipse day, the team estimated, your average big sagebrush managed about 14 percent less photosynthesis than it would have if the sun hadnt been blocked. If a plant is already drought-stressed, an eclipse might be bad news, like losing 14 percent of a days income when youre already broke, Mr. Beverly said.Their findings follow a few other studies, including one done during an eclipse over Europe in 1999 that showed changes in sap flow and transpiration in a beech tree near Ghent, Belgium. The tree held its breath during the solar eclipse, said Kathy Steppe, now a professor of plant ecology at Ghent University meaning it stopped releasing water, even as its sap kept flowing.Mr. Beverly looks forward to the 2024 eclipse, which will sweep from Mexico through to the Eastern United States, when he plans to take a closer look at crop and tree responses.Hell do one thing differently: I would try to use as much automation as possible so I could actually enjoy the eclipse, he said. Thats a big thing I missed. "],
        "story_type":"Normal",
        "url_raw":"https://www.nytimes.com/2019/07/02/science/plants-solar-eclipse.html",
        "url_text":"TrilobitesResearch conducted during the Great American Eclipse of 2017 suggested the suns midday disappearance shocked some plants.Credit...Dr. Mario BretfeldJuly 2, 2019As the total solar eclipse crosses South America on Tuesday, it wont just be people oohing and ahhing as the sun is blotted out.Other living things will have their own responses, too some of which we are just beginning to understand. As some scientists used the Great American Eclipse in August 2017 to watch how bees and birds dealt with sudden midday darkness, researchers in Wyoming investigated big sagebrush, a shaggy, aromatic desert shrub that grows throughout the western United States.Tracking its reactions at the leaf level, scientists saw it experience a slowdown in activity as darkness fell, followed by shock at the suns surprise return. The study, published in June in Scientific Reports, adds to a small clump of botanical eclipse research, all produced by people with the ability to wonder, even as a celestial event occurs: What are the plants getting up to?[Like the Science Times page on Facebook. | Sign up for the Science Times newsletter.]Plants havent really been well-documented during the eclipse, said Daniel Beverly, a Ph.D. student in botany and hydrology at the University of Wyoming and the papers lead author. When he heard an eclipse was coming, he saw the chance for a once-in-a-lifetime data set.Big sagebrush spends much of its life in the sun, and it covers such a large portion of the country, from Oregon down to New Mexico, Mr. Beverly said, making it a good subject for study. He and his colleagues chose a patch close to Yellowstone National Park. They set up instruments that could measure photosynthetic rate, as well as the speed of transpiration how quickly its leaves lose water.As the light faded and the temperature dropped during the 80 or so minutes before totality, Mr. Beverly and his team saw a corresponding decrease in photosynthesis and transpiration. The plant responded as if it was dusk, he said.During the two minutes and 18 seconds of complete darkness, both rates fell further. Although they did not quite reach the slowdown level of an average nighttime, it was a much more precipitous drop than would have occurred for a passing cloud.But when the sun appeared again, the shrubs were blindsided.Thats when we saw the most evident signals of stress, said Mr. Beverly. It caused a disruption in the photosynthetic pathways.Although they could not test this directly, he guesses that the shock of sudden sunlight messed with each plants circadian rhythm the internal clock that determines an organisms daily cycle of activity.Over the course of the eclipse day, the team estimated, your average big sagebrush managed about 14 percent less photosynthesis than it would have if the sun hadnt been blocked. If a plant is already drought-stressed, an eclipse might be bad news, like losing 14 percent of a days income when youre already broke, Mr. Beverly said.Their findings follow a few other studies, including one done during an eclipse over Europe in 1999 that showed changes in sap flow and transpiration in a beech tree near Ghent, Belgium. The tree held its breath during the solar eclipse, said Kathy Steppe, now a professor of plant ecology at Ghent University meaning it stopped releasing water, even as its sap kept flowing.Mr. Beverly looks forward to the 2024 eclipse, which will sweep from Mexico through to the Eastern United States, when he plans to take a closer look at crop and tree responses.Hell do one thing differently: I would try to use as much automation as possible so I could actually enjoy the eclipse, he said. Thats a big thing I missed. ",
        "comments.comment_id":[20352477,
          20355944],
        "comments.comment_author":["bactisme",
          "fridgamarator"],
        "comments.comment_descendants":[2,
          0],
        "comments.comment_time":["2019-07-04T07:47:34Z",
          "2019-07-04T16:40:50Z"],
        "comments.comment_text":["Is there a difference between eclipse and bringing tour plant inside? How is it difficult to simulate a eclipse all year round ?",
          "Some data my company collected during the August 2017 eclipse in Nebraska - <a href=\"http://www.licor.com/env/newsline/2017/08/solar-eclipse-data-from-li-cor/\" rel=\"nofollow\">http://www.licor.com/env/newsline/2017/08/solar-eclipse-data...</a>"],
        "id":"5dc9c7ba-a9a6-41ea-b8d9-b058ee645ade",
        "_version_":1718441043146309632},
      {
        "story_id":19623087,
        "story_author":"okket",
        "story_descendants":7,
        "story_score":30,
        "story_time":"2019-04-10T09:29:36Z",
        "story_title":"Event Horizon Telescope Live Press Conference",
        "search":["Event Horizon Telescope Live Press Conference",
          "https://www.eso.org/public/live/",
          "On 2 July 2019 the European Southern Observatory (ESO) provided a 4-hour live webcast of the 2019 La Silla Total Solar Eclipse. This was a unique opportunity to be part of the only total solar eclipse visible from an ESO Observatory for the next 212 years. The webcast was a raw feed without commentary switching between sources regularly, featuring views from three small telescopes and two camera views of spectators at the Observatory. The webcast finishes with a beautiful view of the Sun setting over the Pacific Ocean. A nice way to end a spectacular day at La Silla. Production: Francois Glasser (APICAL) Engineering: Lionel Gauze (APICAL) & Francois Glasser (APICAL) Cinematography: Francois Glasser (APICAL) & Alexandre Santerne (Aix-Marseille University/LAM) Director: Lars Lindberg Christensen The Eclipse Schedule Event02.07.2019 Local Time Chile CLT Time UT Time CEST Start of webcast 13:56:00 17:56:00 19:56:00 Start of partial eclipse (C1) 15:23:51 19:23:51 21:23:51 Start of total eclipse (C2) 16:39:24 20:39:24 22:39:24 Maximum eclipse 16:40:20 20:40:20 22:40:20 End of total eclipse (C3) 16:41:15 20:41:15 22:41:15 End of partial eclipse (C4) 17:47:16 21:47:16 23:47:16 Sunset 17:54:00 21:54:00 23:54:00 End of webcast 18:12:00 22:12:00 00:12:00 Duration, totality 01:51.7 "],
        "story_type":"Normal",
        "url_raw":"https://www.eso.org/public/live/",
        "url_text":"On 2 July 2019 the European Southern Observatory (ESO) provided a 4-hour live webcast of the 2019 La Silla Total Solar Eclipse. This was a unique opportunity to be part of the only total solar eclipse visible from an ESO Observatory for the next 212 years. The webcast was a raw feed without commentary switching between sources regularly, featuring views from three small telescopes and two camera views of spectators at the Observatory. The webcast finishes with a beautiful view of the Sun setting over the Pacific Ocean. A nice way to end a spectacular day at La Silla. Production: Francois Glasser (APICAL) Engineering: Lionel Gauze (APICAL) & Francois Glasser (APICAL) Cinematography: Francois Glasser (APICAL) & Alexandre Santerne (Aix-Marseille University/LAM) Director: Lars Lindberg Christensen The Eclipse Schedule Event02.07.2019 Local Time Chile CLT Time UT Time CEST Start of webcast 13:56:00 17:56:00 19:56:00 Start of partial eclipse (C1) 15:23:51 19:23:51 21:23:51 Start of total eclipse (C2) 16:39:24 20:39:24 22:39:24 Maximum eclipse 16:40:20 20:40:20 22:40:20 End of total eclipse (C3) 16:41:15 20:41:15 22:41:15 End of partial eclipse (C4) 17:47:16 21:47:16 23:47:16 Sunset 17:54:00 21:54:00 23:54:00 End of webcast 18:12:00 22:12:00 00:12:00 Duration, totality 01:51.7 ",
        "comments.comment_id":[19624060,
          19624702],
        "comments.comment_author":["wolfram74",
          "okket"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-04-10T12:48:25Z",
          "2019-04-10T14:03:26Z"],
        "comments.comment_text":["I had a brief conversation about this and the Falcon Heavy launch with my quantum professor, who is very deep into black holes. I completely understand why they're very excited about this announcement, and I am pumped as well. But they should be a bit excited for the falcon heavy launch as well, if we had a radio telescope on the far side of the moon with a decent clock the resolution on this announcement would an order of magnitude better. That kind of project will be more feasible as launches get cheaper.\nedit: The radio telescope on the moon's data would be integrated into the rest of the networks, this would permit a virtual aperture diameter about ~50 times larger.",
          "The papers with the scientific details are here (open access):<p><a href=\"https://iopscience.iop.org/journal/2041-8205/page/Focus_on_EHT\" rel=\"nofollow\">https://iopscience.iop.org/journal/2041-8205/page/Focus_on_E...</a><p>Article in physics world with comparisons to simulations:<p><a href=\"https://physicsworld.com/a/first-images-of-a-black-hole-unveiled-by-astronomers-in-landmark-discovery/\" rel=\"nofollow\">https://physicsworld.com/a/first-images-of-a-black-hole-unve...</a><p>\"AskScience\" AMA on Reddit about the breakthrough:<p><a href=\"https://www.reddit.com/r/askscience/comments/bbknik/askscience_ama_series_we_are_scientists_here_to/\" rel=\"nofollow\">https://www.reddit.com/r/askscience/comments/bbknik/askscien...</a>"],
        "id":"f48cbf95-32c5-48f6-96e0-414098797f87",
        "_version_":1718441025437958144},
      {
        "story_id":21195196,
        "story_author":"HNLurker2",
        "story_descendants":24,
        "story_score":54,
        "story_time":"2019-10-08T18:10:47Z",
        "story_title":"Decoding the Ancient Greek Astronomical Calculator: Antikythera Mechanism",
        "search":["Decoding the Ancient Greek Astronomical Calculator: Antikythera Mechanism",
          "https://fermatslibrary.com/s/decoding-the-ancient-greek-astronomical-calculator-known-as-the-antikythera-mechanism",
          "LETTERS Decoding the ancient Greek astronomical calculator known as the Antikythera Mechanism T. Freeth 1,2 , Y. Bitsakis 3,5 , X. Moussas 3 , J. H. Seiradakis 4 , A. Tselikas 5 , H. Mangou 6 , M. Zafeiropoulou 6 , R. Hadland 7 , D. Bate 7 , A. Ramsey 7 , M. Allen 7 , A. Crawley 7 , P. Hockley 7 , T. Malzbender 8 , D. Gelb 8 , W. Ambrisco 9 & M. G. Edmunds 1 The Antikythera Mechanism is a unique Greek geared device, con- structed around the end of the second century BC. It is known 19 that it calculated and displayed celestial information, particularly cycles such as the phases of the moon and a luni-solar calendar. Calendars were important to ancient societies 10 for timing agricul- tural activity and fixing religious festivals. Eclipses and planetary motions were often interpreted as omens, while the calm regular- ity of the astronomical cycles must have been philosophically attractive in an uncertain and violent world. Named after its place of discovery in 1901 in a Roman shipwreck, the Antikythera Mechanism is technically more complex than any known device for at least a millennium afterwards. Its specific functions have remained controversial 1114 because its gears and the inscriptions upon its faces are only fragmentary. Here we report surface imaging and high-resolution X-ray tomography of the surviving fragments, enabling us to reconstruct the gear function and double the number of deciphered inscriptions. The mechanism predicted lunar and solar eclipses on the basis of Babylonian arithmetic- progression cycles. The inscriptions support suggestions of mech- anical display of planetary positions 9,14,15 , now lost. In the second century BC, Hipparchos developed a theory to explain the irregu- larities of the Moons motion across the sky caused by its elliptic orbit. We find a mechanical realization of this theory in the gear- ing of the mechanism, revealing an unexpected degree of technical sophistication for the period. The bronze mechanism (Fig. 1), probably hand-driven, was ori- ginally housed in a wooden-framed case 1 of (uncertain) overall size 315 3 190 3 100 mm (Fig. 2). It had front and back doors, with astronomical inscriptions covering much of the exterior of the mech- anism. Our new transcriptions and translations of the Greek texts are given in Supplementary Note 2 (glyphs and inscriptions). The detailed form of the lettering can be dated to the second half of the second century BC, implying that the mechanism was constructed during the period 150100 BC, slightly earlier than previously sug- gested 1 . This is consistent with a date of around 8060 BC for the wreck 1,16 from which the mechanism was recovered by some of the first underwater archaeology. We are able to complete the recon- struction 1 of the back door inscription with text from fragment E, and characters from fragments A and F (see Fig. 1 legend for fragment nomenclature). The front door is mainly from fragment G. The text is astronomical, with many numbers that could be related to planetary motions; the word sterigmos (STGRICMOS, translated as sta- tion or stationary point) is found, meaning where a planets appar- ent motion changes direction, and the numbers may relate to planetary cycles. We note that a major aim of this investigation is to set up a data archive to allow non-invasive future research, and access to this will start in 2007. Details will be available on www.an- tikythera-mechanism.gr. The back door inscription mixes mechanical terms about con- struction (trunnions, gnomon, perforations) with astronom- ical periods. Of the periods, 223 is the Saros eclipse cycle (see Box 1 for a brief explanation of astronomical cycles and periods). We discover the inscription spiral divided into 235 sections, which is 1 Cardiff University, School of Physics and Astronomy, Queens Buildings, The Parade, Cardiff CF24 3AA, UK. 2 Images First Ltd, 10 Hereford Road, South Ealing, London W5 4SE, UK. 3 National and Kapodistrian University of Athens, Department of Astrophysics, Astronomy and Mechanics, Panepistimiopolis, GR-15783, Zographos, G reece. 4 Aristotle University of Thessaloniki, Department of Physics, Section of Astrophysics, Astronomy and Mechanics, GR-54124 Thessaloniki, Greece. 5 Centre for History and Palaeography, National Bank of Greece Cultural Foundation, P. Skouze 3, GR-10560 Athens, Greece. 6 National Archaeological Museum of Athens, 1 Tositsa Str., GR-10682 Athens, Greece. 7 X-Tek Systems Ltd, Tring Business Centre, Icknield Way, Tring, Hertfordshire HP23 4JX, UK. 8 Hewlett-Packard Laboratories, 1501 Page Mill Road, Palo Alto, California 94304, USA. 9 Foxhollow Technologies Inc., 740 Bay Road, Redwood City, California 94063, USA. Figure 1 | The surviving fragments of the Antikythera Mechanism. The 82 fragments that survive in the National Archaeological Museum in Athens are shown to scale. A key and dimensions are provided in Supplementary Note 1 (fragments). The major fragments A, B, C, D are across the top, starting at top left, with E, F, G immediately below them. 27 hand-cut bronze gears are in fragment A and one gear in each of fragments B, C and D. Segments of display scales are in fragments B, C, E and F. A schematic reconstruction is given in Fig. 2. It is not certain that every one of the remaining fragments (numbered 175) belong to the mechanism. The distinctive fragment A, which contains most of the gears, is approximately 180 3 150 mm in size. We have used three principal techniques to investigate the structure and inscriptions of the Antikythera Mechanism. (1) Three-dimensional X-ray microfocus computed tomography 24 (CT), developed by X-Tek Systems Ltd. The use of CT has been crucial in making the text legible just beneath the current surfaces. (2) Digital optical imaging to reveal faint surface detail using polynomial texture mapping (PTM) 25,26 , developed by Hewlett- Packard Inc. (3) Digitized high-quality conventional film photography. Vol 444 | 30 November 2006 | doi:10.1038/nature05357 587 Natu re Pu blishin g Gro u p 2 0 0 6 the key to understanding the function 6 of the upper back dial. The references to golden little sphere and little sphere probably refer to the front zodiac display for the Sun and Moonincluding phase for the latter. The text near the lower back dial includes Pharos and from south (about/around).Spain (ISPANIA) ten. These geograph- ical references, together with previous readings 1 of towards the east, west-north-west and west-south-west suggest an eclipse function for the dial, as solar eclipses occur only at limited geograph- ical sites, and winds were often recorded 1719 in antiquity with eclipse observations. Possibly this information was added to the mechanism during use. Turning to the dials themselves, the front dial displays the position of the Sun and Moon in the zodiac, and a corresponding calendar 1 of 365 days that could be adjusted for leap years. Previously 1 , it was suggested that the upper back dial might have five concentric rings with 47 divisions per turn, showing the 235 months of the 19-year Metonic cycle. A later proposal 5 augments this with the upper sub- sidiary dial showing the 76-year Callippic cycle. Our optical and X-ray microfocus computed tomography (CT) imaging confirms these proposals, with 34 scale markings discovered on the upper back dial. On the basis of a statistical analysis analogous to that described for gear tooth counts below, we confirm the 235 total divisions. We also find from the CT that the subsidiary dial is indeed divided into quadrants 1,6 , as required for a Callippic dial. In agreement with the back door inscription, we also substantiate the perceptive proposal 5,20 that the dial is in fact a spiral, made from semicircular arcs displaced Figure 2 | A schematic view of the mechanism to illustrate the position of major inscriptions and dials. The front dial has two concentric scales. The inner scale shows the Greek zodiac with 360 divisions. There are occasional Greek letters denoting references to the Parapegma inscription, and we add three further reference letters (Z, H, H) to Prices description 1 . The Parapegma is a star almanac showing rising and settings at dawn or evening of particular stars or constellations, which we will discuss elsewhere. Its form is consistent with a date of late second century BC. The outer (originally) movable scale is a calendar carrying the Egyptian names of the months with Greek letters. The Egyptian calendar of 365 days, with twelve 30-day months and 5 extra (epagomenai) days was in standard use in Greek astronomy. The effect of the extra quarter day in a year could be corrected by turning the scale one day every four yearsand a sequence of holes to take a locking pin is observed under the scale. We find that spacing of the holes is indeed what would be expected for a total of 365 days, with a possible range 363365. The position of the Sun and Moon would have been indicated by pointers across the dial scales, and a device 7 showing the phase of the Moon was probably carried round on the lunar pointer. It is not clear whether the Sun position pointer would have been separated from a date pointer, or whether any planetary positions might have been displayed. The spiral upper back dial displays the luni-solar Metonic sequence of 235 lunar months with a subsidiary dial showing the Callippic cycle, while the spiral lower back dial displays the 223-lunar-month Saros eclipse cycle with a subsidiary dial showing the Exeligmos cycle. Box 1 | Astronomical cycles known to the Babylonians The lunar (or synodic) month is the interval between the Moon being at the same phasefor example, full moon to full moon. The Metonic cycle results from the close equality of 19 years to 235 lunar months. It represents the return to the same phase of the Moon on the same date in the year. After the cycle, the Sun, Moon and Earth are back in nearly the same relative orientations. The Moon appears to return to the same point in the sky relative to the zodiac in a sidereal month, and in 19 years there are 235 1 19 5 254 sidereal months. The 76-year Callippic cycle is four Metonic cycles minus one dayand improves the accuracy of reconciling solar years with whole numbers of lunar months. The Saros is an eclipse repeat cycle. If either a solar or lunar eclipse occurs, a very similar eclipse will occur 223 lunar months later 23 .A record of past eclipses can thus be used to predict future occurrences. The cycle arises from the coincidence of three orbital periods of the Moon. These are: (1) same phase to same phase, 223 synodic monthseclipses will of course only occur at new or full Moon in the month; (2) the lunar crossing of the Earth Sun orbital plane, 242 draconitic monthseclipses can only occur near these points (nodes) of co-alignment; (3) similar Earth Moon distances which occur on the period from apogee to apogee of the Moons orbit, 239 anomalistic months. The distance will determine the magnitude of the eclipse, ensuring the similarity of eclipses at the period of the cycle. The Saros cycle is not an integer number of days (6,585M), causing the eclipses in successive cycles to be displaced by eight hours in time (and solar eclipses, only visible at limited geographical locations, to be displaced by 120u in longitude). True repeats come after 3 Saros cycles, the 54-year Exeligmos cycle, but not with identical solar eclipse paths. LETTERS NATURE | Vol 444 | 30 November 2006 588 Natu re Pu blishin g Gro u p 2 0 0 6 to two centres on the vertical midline. In the CT of fragment B we find a new feature that explains why the dial is a spiral: a pointer-follower device (Fig. 3) travelled around the spiral groove to indicate which month (across the five turns of the scale) should be read. From our CT data of the 48 scale divisions observed in fragments A, E and F, we establish 223 divisions in the four-turn 5,20 spiral on the lower back dial, the spiral starting at the bottom of the dial. This is the Saros eclipse cycle, whose number is on the back door inscription. The 54-year Exeligmos cycle of three Saros cycles is shown on the lower subsidiary dial. Between the scale divisions of the Saros dial we have identified 16 blocks of characters, or glyphs (see Supplementary Note 2 (glyphs and inscriptions)) at intervals of one, five and six months. These are eclipse predictions and contain either S for a lunar eclipse (from SELGNG, Moon) or G for a solar eclipse (from GLIOS, Sun) or both. A correlation analysis (analogous to DNA sequence matching) with historic eclipse data 21 (all modern eclipse data and predictions in our work are from this reference) indicates that over a period of 400 1 BC, the sequence of eclipses marked by the identified glyphs would be exactly matched by 121 possible start dates. The matching only occurs if the lunar month starts at first crescent, and confirms this choice of month start in the mechanism. The sequences of eclipses can then be used to predict the expected position of glyphs on the whole dial, as seen in Fig. 4. The dial starts and finishes with an eclipse. Although Ptolemy indicates that the Greeks recorded eclipses in the second century BC, the Babylonian Saros canon 1719 is the only known source of sufficient data to construct the dial. The functions of the mechanism are determined by the tooth counts of the gears. These are based mainly on the CT, using angular measurement from a nominal centre to the remains of tooth tips. In a few cases all teeth can be seen, but many gears are incomplete. Counts are established by fitting models with regularly spaced teeth and minimising the r.m.s. deviation from the measurementsvarying the centre in software (when unclear) to find the best-fit solution or solutions (see Supplementary Note 3 (gears)). We have adopted a systematic nomenclature of lower case letters for the axis of the gear, with numbering increasing with ordering from the front of the mech- anism. Hypothetical (lost) gears are denoted by italics. Several models have been proposed for the gear trains 1,2,46,8 . We agree with the assumption of four missing gears (n1, n2, p1, p2) to drive the Metonic and Callippic dials 4 . We propose a new reconstruc- tion for the other trains, which uses all extant gears (except the lone r1 0 1 2 3 4 5 cm Figure 4 | Reconstruction of the back dials. A composite of fragments A, B, E and F. The Metonic calendar is at top, with its subsidiary Callippic dial. The Saros eclipse cycle is below, with its subsidiary Exeligmos dial. The 16 observed eclipse glyphs are shown in turquoise on the Saros dial, with 35 hypothetical glyphs in violet. The hypothetical glyphs are based on the criterion that 99% of the 121 sequences exactly matching the observed glyphs have an eclipse at the month position. Both main dials would have a pointer-follower (see Fig. 3) to indicate the relevant lunar month on the spiral. The monthly divisions on the Metonic upper back dial are not simply scribed directly across all five turns, as might be expected for simplicity of construction. There are small misalignments, implying a systematic attempt at marking full (30-day) and hollow (29-day) months. The incomplete data do not allow good analysis, other than a hint of bimodality in the interval distribution. If the marking out of the scale were carried out using the mechanisms gearing, then this would greatly pre-date known dividing engines 27 by many centuries. Figure 3 | The pointer-follower lunar month indicator of the upper back dial. On the left, false-colour sections through CT images, analysed with VGStudio Max software by Volume Graphics GmbH. These show two views at right angles of the pointer-follower in the Metonic dial in fragment B. On the right, a computer reconstruction of the device from two different angles (with the Metonic scale omitted for clarity). The pin was constrained to follow the groove between the spiral scales (the scale is shown in Fig. 4), causing the device to slide along the month pointer to indicate which ring on the spiral scale specified the month. A similar pointer-follower would have been present on the lower back (Saros) dial. The Metonic dial would have required re- setting every 19 years, the Saros dial after 18 years. The groove-pin may have been held in place by the small pin through the front of the device, enabling its removal for re-setting. NATURE | Vol 444 | 30 November 2006 LETTERS 589 Natu re Pu blishin g Gro u p 2 0 0 6 from the separate fragment D). The proposed model is shown in Fig. 5. We require the assumption of only one further gear (m3), whose proposed shaft is clearly broken off in the CT. A detailed description is contained in Supplementary Note 3 (gears). Of particular note is the dual use of the large gear, e3, at the back of the mechanism, which has found no use in previous models. In our model, it is powered by m3 as part of a fixed-axis train that turns the Saros and Exeligmos dials for eclipse prediction, and also doubles as the epicyclic table for the gears k1, k2. These are part of epicyclic gearing that calculates the theory of the irregular motion of the moon, developed by Hipparchos some time between 146 and 128 BC (ref. 22)the first anomaly, caused by its elliptical orbit about the Earth. The period of this anomaly is the period from apogee to apogee (the anomalistic month). To realize this theory, the mean sidereal lunar motion is first calculated by gears on axes c, d and e and this is then fed into the epicyclic system. As explained in Fig. 6, a pin- and-slot device on the epicyclic gears k1 and k2, clearly seen in the CT, provides the variation. This was previously identified 4 , but rejected as a lunar mechanism. The remarkable purpose of mounting the pin-and-slot mechanism on the gear e3 is to change the period of variation from sidereal month (that is, the time taken for the Moon to orbit the Earth relative to the zodiac), which would occur if k1 and k2 were on fixed axes, to anomalistic monthby carrying the gears epicyclically at a rate that is the difference between the rates of the Front dials Lunar phase Lost epicyclic gearing Pin and slot Hipparchos lunar mechanism Possibly Hipparchos solar mechanism and planetary mechanisms Hipparchos sidereal month Year Back dials Luni-solar calendar Input Eclipse prediction Saros 4Metonic 5Callippic Exeligmos Zodiac Egyptian calendar Parapegma Figure 5 | New reconstruction of the gear trains. A schematic sectional diagram (not to scale) of the gearing, following the style of Price 1 and Wright 4 . The viewpoint is looking down from the top right of the mechanism, and is stretched in the direction of the main axes to show the structure. Features that are outlined or labelled in red are hypothetical. Gears are lettered with their shaft, and numbered with increasing distance from the front dial. The two-or-three digit number on the gear is its actual or assumed tooth count (see Supplementary Note 3 (gears)). Hypothetical gears n1, n2, p1, p2 have been proposed previously, the gear m3 on the broken-off shaft m is our addition. All gears, except the lone one in fragment D, are now accounted for in the mechanism. The function of the trains is outlined in the text. We find no evidence in the CT for an idler wheel carried on e3 and between e5 and k1 or between k2 and e6, as has been previously proposed 1,2,4 . The CT shows a pin through axis e between gears e1 and e2. We believe its purpose is to retain the square-bossed e1 on the shaft, but its passage right through the axis rules out previous reconstructions 1,2,4 where e1 and e2 were joined by an outer pipe rotating around the shaft e. Figure 6 | The Hipparchos lunar mechanism mounted on gear e3. The figure is based on a CT slice of part of fragment A, showing (top) shaft e and (bottom) shaft k. The complete geometry cannot be seen in a single CT slice. The two gears on the e axis (e5 and e6) are coaxial, while the two k gears rotate on slightly displaced axes. k1 has a pin on its face that engages with a radial slot in k2 (and this was previously reported 5 ). In the figure the pitch circles of e5 and k1 are shown in turquoise and those of e6 and k2 in pink. The gear e5 drives k1, which drives k2 via the pin-and-slot, introducing a quasi-sinusoidal variation in the motion, which is then transmitted to e6. Our estimate of the distance between the arbors on the k gears is about 1.1 mm, with a pin distance of 9.6 mm, giving an angular variation of 6.5u. According to Ptolemy 28 , Hipparchos made two estimates for a lunar anomaly parameter, based on eclipse data, which would require angular variations of 5.9u or 4.5u herealthough estimates of the anomaly from Babylonian astronomy were generally larger. The difference from our estimated value is probably not significant given the difficulty of precise measurement of the axes in the CT. The harmonic variation, together with the effect of carrying the gears on e3 (which rotates at the period of the Moons apogee around the Earth), would simulate the correct variation for the Moons mean (sidereal) rotation rate on the front dial. An (unexplained) regular pentagon is visible at the centre of gear e5. It is tempting to associate the conception of the mechanism with Hipparchos himself, but he was not the first to assume eccentric or epicyclic models. LETTERS NATURE | Vol 444 | 30 November 2006 590 Natu re Pu blishin g Gro u p 2 0 0 6 sidereal and anomalistic months, that is, at the rate of rotation of about 9 years of the Moons apogee. Gears with 53 teeth are awkward to divide. So it may seem sur- prising that the gearing includes two such gears (f1, l2), whose effects cancel in the train leading to the Saros dial. But the gearing has been specifically designed so that the epicyclic table e3 turns at the rate of rotation of the Moons apogeethe factor 53 being derived from the calculation of this rotation from the Metonic and Saros cycles, which are the bases for all the prime factors in the tooth counts of the gears. The establishment of the 53-tooth count of these gears is powerful confirmation of our proposed model of Hipparchos lunar theory. The output of this complex system is carried from e6 back through e3 and thence, via e1 and b3, to the zodiac scale on the front dial and the lunar phase 7 mechanism. Our CT confirms the complex structure of axis e that this model entails. The Antikythera Mechanism shows great economy and ingenuity of design. It stands as a witness to the extraordinary technological potential of Ancient Greece, apparently lost within the Roman Empire. Received 10 August; accepted 17 October 2006. 1. Price, D. de S. Gears from the Greeks: The Antikythera Mechanism A calendar computer from ca. 80 BC. Trans Am. Phil. Soc. New Ser. 64, 1 70 (1974); reprinted by Science History Publications, New York (1975). 2. Wright, M. T. Epicyclic gearing and the Antikythera Mechanism, Part I. Antiquar. Horol. 27, 270 279 (2003). 3. Wright, M. T., Bromley, A. G. & Magou, H. Simple X-ray tomography and the Antikythera Mechanism. PACT J. Eur. Study Group Phys. Chem. Biol. Math. Tech. Appl. Archaeol. 45, 531 543 (1995). 4. Wright, M. T. The Antikythera Mechanism: a New Gearing Scheme. Bull. Sci. Instrum. Soc. 85, 2 7 (2005). 5. Wright, M. T. Epicyclic gearing and the Antikythera Mechanism, Part II. Antiquar. Horol. 29, 51 63 (2005). 6. Wright, M. T. Counting months and years: The upper back dial of the Antikythera Mechanism. Bull. Sci. Instrum. Soc. 87, 8 13 (2005). 7. Wright, M. T. The Antikythera Mechanism and the early history of the moon- phase display. Antiquar. Horol. 29, 319 329 (2006). 8. Wright, M. T. in Proc. 2nd Int. Conf. on Ancient Greek Technology (ed. Tassios, Th.) 49 60 (Technical Chamber of Greece, Athens, 2006). 9. Wright, M. T. A planetarium display for the Antikythera Mechanism. Horol. J. 144(5), 169 173 (2002); erratum 144, 193 (2002). 10. North, J. D. The Fontana History of Astronomy and Cosmology (Fontana, London, 1994). 11. Bromley, A. G. The Antikythera Mechanism. Horol. J. 132, 412 415 (1990). 12. Bromley, A. G. in Bassernet Vol. 2, No. 3 (Basser Department of Computer Science, Univ. Sydney, June, 1993). 13. Freeth, T. The Antikythera Mechanism: 1. Challenging the classic research. Mediterr. Archaeol. Archaeom. 2, 21 35 (2002). 14. Edmunds, M. & Morgan, P. The Antikythera Mechanism: still a mystery of Greek astronomy? Astron. Geophys. 41, 6.10 6.17 (2000). 15. Freeth, T. The Antikythera Mechanism: 2. Is it Posidonius orrery? Mediterr. Archaeol. Archaeom. 2, 45 58 (2002). 16. Illsley, J. S. History and Archaeology of the ShipLecture Notes. http:// cma.soton.ac.uk/HistShip/shlect36.htm (1998). 17. Stephenson, F. R. Historical Eclipses and Earths Rotation (Cambridge Univ. Press, Cambridge, UK, 1997). 18. Steele, J. M. Observations and Predictions of Eclipse Times by Early Astronomers (Kluwer Academic, Dordrecht, 2000). 19. Steele, J. M. Eclipse prediction in Mesopotamia. Arch. Hist. Exact Sci. 54, 421 454 (2000). 20. Wright, M. T. The Scholar, the Mechanic and the Antikythera Mechanism. Bull. Sci. Instrum . Soc. 80, 4 11 (2003). 21. Espenak, F. NASA Eclipse Home Page. http://sunearth.gsfc.nasa.gov/eclipse/ eclipse.html (2005). 22. Jones, A. The adaptation of Babylonian methods in Greek numerical astronomy. Isis 82, 440 453 (1991). 23. Britton, J. P. in Die Rolle der Astronomie in den Kulturen Mesopotamiens (ed. Galter, H. D.) 61 76 (rm-Druck & Vergansgesellschaft, Graz, 1993). 24. X-Tek Systems Ltd. 3-D computed tomography. http://www.xtek.co.uk/ct/ (2006). 25. Malzbender, T. & Gelb, D. Polynomial texture mapping. http:// www.hpl.hp.com/research/ptm/ (2006). 26. Brooks, M. Tricks of the light. New Sci. 170, 38 49 (2001). 27. Chapman, A. Dividing the Circle (Wiley, Chichester, 1995). 28. Toomer, G. J. Ptolemys Almagest (transl. Toomer, G. J.) (Princeton Univ. Press, Princeton, New Jersey, 1998). Supplementary Information is linked to the online version of the paper at www.nature.com/nature. Acknowledgements This work was financed by the Leverhulme Trust, the Walter Hudson Bequest, the University of Athens Research Committee and the Cultural Foundation of the National Bank of Greece. For essential support we thank the Ministry of Culture, Greece (P. Tatoulis), and the National Archaeological Museum of Athens (N. Kalts as). We acknowledge help and advice from J. Ambers, J. Austin, G. Dermody, H. Forsyth, I. Freestone, P. Haycock, V. Horie, A. Jones, M. Jones, P. Kipouros, H. Kritzas, J. Lossl, G. Makris, A. Ray, C. Reinhart, A. Valassopoulos, R. Westgate, T. Whiteside, S. Wright and C. Xenikakis. Author Contributions T.F. carried out most of the CT analysis of structure and its interpretation. Y.B., A.T. and X.M. read, transcribed and translated the inscriptions. H.M and M.Z. catalogued the fragments, provided guidance on X-ray examination, and measured the fragments with J.H.S. R.H. led the team (D.B., A.R., M.A., A.C. and P.H.) that built and operated the Bladerunner CT machine, and provided CT reconstructions and advice. T.M., D.G. and W.A. built, operated and provided software for the PTM. M.G.E. was academic lead, and undertook the statistical analysis. T.F. and Y.B. organised the logistics of the experimental work, with inter-agency liaison by X.M. and J.H.S. The manuscript was written by T.F. and M.G.E. including material from Y.B., A.T., X.M., J.H.S., H.M. and M.Z. T.F. designed the illustrations. Author Information Reprints and permissions information is available at www.nature.com/reprints. The authors declare no competing financial interests. Correspondence and requests for materials should be addressed to M.G.E. (mge@astro.cf.ac.uk). NATURE | Vol 444 | 30 November 2006 LETTERS 591 Natu re Pu blishin g Gro u p 2 0 0 6 View publication statsView publication stats "],
        "story_type":"Normal",
        "url_raw":"https://fermatslibrary.com/s/decoding-the-ancient-greek-astronomical-calculator-known-as-the-antikythera-mechanism",
        "comments.comment_id":[21195796,
          21195905],
        "comments.comment_author":["arnado",
          "coldcode"],
        "comments.comment_descendants":[4,
          2],
        "comments.comment_time":["2019-10-08T18:58:56Z",
          "2019-10-08T19:06:43Z"],
        "comments.comment_text":["He's been scarce for a while, but Clickspring on youtube has a series on rebuilding the Antikythera Mechanism. It's quite fascinating.<p><a href=\"https://www.youtube.com/channel/UCworsKCR-Sx6R6-BnIjS2MA/videos\" rel=\"nofollow\">https://www.youtube.com/channel/UCworsKCR-Sx6R6-BnIjS2MA/vid...</a>",
          "It's easy today to imagine we are smarter than people 2000+ years ago, but it's more like we are standing on the shoulders of a ton of generations before us. I always wonder if I went back in time 2000 years from today, if I would be able to translate my knowledge at all into the limitations of back then. The folks who built this only knew what they knew back then or could imagine, but were still able to build this complex machine."],
        "id":"9c519373-8159-418c-b77e-a2d3e3cb0bf1",
        "url_text":"LETTERS Decoding the ancient Greek astronomical calculator known as the Antikythera Mechanism T. Freeth 1,2 , Y. Bitsakis 3,5 , X. Moussas 3 , J. H. Seiradakis 4 , A. Tselikas 5 , H. Mangou 6 , M. Zafeiropoulou 6 , R. Hadland 7 , D. Bate 7 , A. Ramsey 7 , M. Allen 7 , A. Crawley 7 , P. Hockley 7 , T. Malzbender 8 , D. Gelb 8 , W. Ambrisco 9 & M. G. Edmunds 1 The Antikythera Mechanism is a unique Greek geared device, con- structed around the end of the second century BC. It is known 19 that it calculated and displayed celestial information, particularly cycles such as the phases of the moon and a luni-solar calendar. Calendars were important to ancient societies 10 for timing agricul- tural activity and fixing religious festivals. Eclipses and planetary motions were often interpreted as omens, while the calm regular- ity of the astronomical cycles must have been philosophically attractive in an uncertain and violent world. Named after its place of discovery in 1901 in a Roman shipwreck, the Antikythera Mechanism is technically more complex than any known device for at least a millennium afterwards. Its specific functions have remained controversial 1114 because its gears and the inscriptions upon its faces are only fragmentary. Here we report surface imaging and high-resolution X-ray tomography of the surviving fragments, enabling us to reconstruct the gear function and double the number of deciphered inscriptions. The mechanism predicted lunar and solar eclipses on the basis of Babylonian arithmetic- progression cycles. The inscriptions support suggestions of mech- anical display of planetary positions 9,14,15 , now lost. In the second century BC, Hipparchos developed a theory to explain the irregu- larities of the Moons motion across the sky caused by its elliptic orbit. We find a mechanical realization of this theory in the gear- ing of the mechanism, revealing an unexpected degree of technical sophistication for the period. The bronze mechanism (Fig. 1), probably hand-driven, was ori- ginally housed in a wooden-framed case 1 of (uncertain) overall size 315 3 190 3 100 mm (Fig. 2). It had front and back doors, with astronomical inscriptions covering much of the exterior of the mech- anism. Our new transcriptions and translations of the Greek texts are given in Supplementary Note 2 (glyphs and inscriptions). The detailed form of the lettering can be dated to the second half of the second century BC, implying that the mechanism was constructed during the period 150100 BC, slightly earlier than previously sug- gested 1 . This is consistent with a date of around 8060 BC for the wreck 1,16 from which the mechanism was recovered by some of the first underwater archaeology. We are able to complete the recon- struction 1 of the back door inscription with text from fragment E, and characters from fragments A and F (see Fig. 1 legend for fragment nomenclature). The front door is mainly from fragment G. The text is astronomical, with many numbers that could be related to planetary motions; the word sterigmos (STGRICMOS, translated as sta- tion or stationary point) is found, meaning where a planets appar- ent motion changes direction, and the numbers may relate to planetary cycles. We note that a major aim of this investigation is to set up a data archive to allow non-invasive future research, and access to this will start in 2007. Details will be available on www.an- tikythera-mechanism.gr. The back door inscription mixes mechanical terms about con- struction (trunnions, gnomon, perforations) with astronom- ical periods. Of the periods, 223 is the Saros eclipse cycle (see Box 1 for a brief explanation of astronomical cycles and periods). We discover the inscription spiral divided into 235 sections, which is 1 Cardiff University, School of Physics and Astronomy, Queens Buildings, The Parade, Cardiff CF24 3AA, UK. 2 Images First Ltd, 10 Hereford Road, South Ealing, London W5 4SE, UK. 3 National and Kapodistrian University of Athens, Department of Astrophysics, Astronomy and Mechanics, Panepistimiopolis, GR-15783, Zographos, G reece. 4 Aristotle University of Thessaloniki, Department of Physics, Section of Astrophysics, Astronomy and Mechanics, GR-54124 Thessaloniki, Greece. 5 Centre for History and Palaeography, National Bank of Greece Cultural Foundation, P. Skouze 3, GR-10560 Athens, Greece. 6 National Archaeological Museum of Athens, 1 Tositsa Str., GR-10682 Athens, Greece. 7 X-Tek Systems Ltd, Tring Business Centre, Icknield Way, Tring, Hertfordshire HP23 4JX, UK. 8 Hewlett-Packard Laboratories, 1501 Page Mill Road, Palo Alto, California 94304, USA. 9 Foxhollow Technologies Inc., 740 Bay Road, Redwood City, California 94063, USA. Figure 1 | The surviving fragments of the Antikythera Mechanism. The 82 fragments that survive in the National Archaeological Museum in Athens are shown to scale. A key and dimensions are provided in Supplementary Note 1 (fragments). The major fragments A, B, C, D are across the top, starting at top left, with E, F, G immediately below them. 27 hand-cut bronze gears are in fragment A and one gear in each of fragments B, C and D. Segments of display scales are in fragments B, C, E and F. A schematic reconstruction is given in Fig. 2. It is not certain that every one of the remaining fragments (numbered 175) belong to the mechanism. The distinctive fragment A, which contains most of the gears, is approximately 180 3 150 mm in size. We have used three principal techniques to investigate the structure and inscriptions of the Antikythera Mechanism. (1) Three-dimensional X-ray microfocus computed tomography 24 (CT), developed by X-Tek Systems Ltd. The use of CT has been crucial in making the text legible just beneath the current surfaces. (2) Digital optical imaging to reveal faint surface detail using polynomial texture mapping (PTM) 25,26 , developed by Hewlett- Packard Inc. (3) Digitized high-quality conventional film photography. Vol 444 | 30 November 2006 | doi:10.1038/nature05357 587 Natu re Pu blishin g Gro u p 2 0 0 6 the key to understanding the function 6 of the upper back dial. The references to golden little sphere and little sphere probably refer to the front zodiac display for the Sun and Moonincluding phase for the latter. The text near the lower back dial includes Pharos and from south (about/around).Spain (ISPANIA) ten. These geograph- ical references, together with previous readings 1 of towards the east, west-north-west and west-south-west suggest an eclipse function for the dial, as solar eclipses occur only at limited geograph- ical sites, and winds were often recorded 1719 in antiquity with eclipse observations. Possibly this information was added to the mechanism during use. Turning to the dials themselves, the front dial displays the position of the Sun and Moon in the zodiac, and a corresponding calendar 1 of 365 days that could be adjusted for leap years. Previously 1 , it was suggested that the upper back dial might have five concentric rings with 47 divisions per turn, showing the 235 months of the 19-year Metonic cycle. A later proposal 5 augments this with the upper sub- sidiary dial showing the 76-year Callippic cycle. Our optical and X-ray microfocus computed tomography (CT) imaging confirms these proposals, with 34 scale markings discovered on the upper back dial. On the basis of a statistical analysis analogous to that described for gear tooth counts below, we confirm the 235 total divisions. We also find from the CT that the subsidiary dial is indeed divided into quadrants 1,6 , as required for a Callippic dial. In agreement with the back door inscription, we also substantiate the perceptive proposal 5,20 that the dial is in fact a spiral, made from semicircular arcs displaced Figure 2 | A schematic view of the mechanism to illustrate the position of major inscriptions and dials. The front dial has two concentric scales. The inner scale shows the Greek zodiac with 360 divisions. There are occasional Greek letters denoting references to the Parapegma inscription, and we add three further reference letters (Z, H, H) to Prices description 1 . The Parapegma is a star almanac showing rising and settings at dawn or evening of particular stars or constellations, which we will discuss elsewhere. Its form is consistent with a date of late second century BC. The outer (originally) movable scale is a calendar carrying the Egyptian names of the months with Greek letters. The Egyptian calendar of 365 days, with twelve 30-day months and 5 extra (epagomenai) days was in standard use in Greek astronomy. The effect of the extra quarter day in a year could be corrected by turning the scale one day every four yearsand a sequence of holes to take a locking pin is observed under the scale. We find that spacing of the holes is indeed what would be expected for a total of 365 days, with a possible range 363365. The position of the Sun and Moon would have been indicated by pointers across the dial scales, and a device 7 showing the phase of the Moon was probably carried round on the lunar pointer. It is not clear whether the Sun position pointer would have been separated from a date pointer, or whether any planetary positions might have been displayed. The spiral upper back dial displays the luni-solar Metonic sequence of 235 lunar months with a subsidiary dial showing the Callippic cycle, while the spiral lower back dial displays the 223-lunar-month Saros eclipse cycle with a subsidiary dial showing the Exeligmos cycle. Box 1 | Astronomical cycles known to the Babylonians The lunar (or synodic) month is the interval between the Moon being at the same phasefor example, full moon to full moon. The Metonic cycle results from the close equality of 19 years to 235 lunar months. It represents the return to the same phase of the Moon on the same date in the year. After the cycle, the Sun, Moon and Earth are back in nearly the same relative orientations. The Moon appears to return to the same point in the sky relative to the zodiac in a sidereal month, and in 19 years there are 235 1 19 5 254 sidereal months. The 76-year Callippic cycle is four Metonic cycles minus one dayand improves the accuracy of reconciling solar years with whole numbers of lunar months. The Saros is an eclipse repeat cycle. If either a solar or lunar eclipse occurs, a very similar eclipse will occur 223 lunar months later 23 .A record of past eclipses can thus be used to predict future occurrences. The cycle arises from the coincidence of three orbital periods of the Moon. These are: (1) same phase to same phase, 223 synodic monthseclipses will of course only occur at new or full Moon in the month; (2) the lunar crossing of the Earth Sun orbital plane, 242 draconitic monthseclipses can only occur near these points (nodes) of co-alignment; (3) similar Earth Moon distances which occur on the period from apogee to apogee of the Moons orbit, 239 anomalistic months. The distance will determine the magnitude of the eclipse, ensuring the similarity of eclipses at the period of the cycle. The Saros cycle is not an integer number of days (6,585M), causing the eclipses in successive cycles to be displaced by eight hours in time (and solar eclipses, only visible at limited geographical locations, to be displaced by 120u in longitude). True repeats come after 3 Saros cycles, the 54-year Exeligmos cycle, but not with identical solar eclipse paths. LETTERS NATURE | Vol 444 | 30 November 2006 588 Natu re Pu blishin g Gro u p 2 0 0 6 to two centres on the vertical midline. In the CT of fragment B we find a new feature that explains why the dial is a spiral: a pointer-follower device (Fig. 3) travelled around the spiral groove to indicate which month (across the five turns of the scale) should be read. From our CT data of the 48 scale divisions observed in fragments A, E and F, we establish 223 divisions in the four-turn 5,20 spiral on the lower back dial, the spiral starting at the bottom of the dial. This is the Saros eclipse cycle, whose number is on the back door inscription. The 54-year Exeligmos cycle of three Saros cycles is shown on the lower subsidiary dial. Between the scale divisions of the Saros dial we have identified 16 blocks of characters, or glyphs (see Supplementary Note 2 (glyphs and inscriptions)) at intervals of one, five and six months. These are eclipse predictions and contain either S for a lunar eclipse (from SELGNG, Moon) or G for a solar eclipse (from GLIOS, Sun) or both. A correlation analysis (analogous to DNA sequence matching) with historic eclipse data 21 (all modern eclipse data and predictions in our work are from this reference) indicates that over a period of 400 1 BC, the sequence of eclipses marked by the identified glyphs would be exactly matched by 121 possible start dates. The matching only occurs if the lunar month starts at first crescent, and confirms this choice of month start in the mechanism. The sequences of eclipses can then be used to predict the expected position of glyphs on the whole dial, as seen in Fig. 4. The dial starts and finishes with an eclipse. Although Ptolemy indicates that the Greeks recorded eclipses in the second century BC, the Babylonian Saros canon 1719 is the only known source of sufficient data to construct the dial. The functions of the mechanism are determined by the tooth counts of the gears. These are based mainly on the CT, using angular measurement from a nominal centre to the remains of tooth tips. In a few cases all teeth can be seen, but many gears are incomplete. Counts are established by fitting models with regularly spaced teeth and minimising the r.m.s. deviation from the measurementsvarying the centre in software (when unclear) to find the best-fit solution or solutions (see Supplementary Note 3 (gears)). We have adopted a systematic nomenclature of lower case letters for the axis of the gear, with numbering increasing with ordering from the front of the mech- anism. Hypothetical (lost) gears are denoted by italics. Several models have been proposed for the gear trains 1,2,46,8 . We agree with the assumption of four missing gears (n1, n2, p1, p2) to drive the Metonic and Callippic dials 4 . We propose a new reconstruc- tion for the other trains, which uses all extant gears (except the lone r1 0 1 2 3 4 5 cm Figure 4 | Reconstruction of the back dials. A composite of fragments A, B, E and F. The Metonic calendar is at top, with its subsidiary Callippic dial. The Saros eclipse cycle is below, with its subsidiary Exeligmos dial. The 16 observed eclipse glyphs are shown in turquoise on the Saros dial, with 35 hypothetical glyphs in violet. The hypothetical glyphs are based on the criterion that 99% of the 121 sequences exactly matching the observed glyphs have an eclipse at the month position. Both main dials would have a pointer-follower (see Fig. 3) to indicate the relevant lunar month on the spiral. The monthly divisions on the Metonic upper back dial are not simply scribed directly across all five turns, as might be expected for simplicity of construction. There are small misalignments, implying a systematic attempt at marking full (30-day) and hollow (29-day) months. The incomplete data do not allow good analysis, other than a hint of bimodality in the interval distribution. If the marking out of the scale were carried out using the mechanisms gearing, then this would greatly pre-date known dividing engines 27 by many centuries. Figure 3 | The pointer-follower lunar month indicator of the upper back dial. On the left, false-colour sections through CT images, analysed with VGStudio Max software by Volume Graphics GmbH. These show two views at right angles of the pointer-follower in the Metonic dial in fragment B. On the right, a computer reconstruction of the device from two different angles (with the Metonic scale omitted for clarity). The pin was constrained to follow the groove between the spiral scales (the scale is shown in Fig. 4), causing the device to slide along the month pointer to indicate which ring on the spiral scale specified the month. A similar pointer-follower would have been present on the lower back (Saros) dial. The Metonic dial would have required re- setting every 19 years, the Saros dial after 18 years. The groove-pin may have been held in place by the small pin through the front of the device, enabling its removal for re-setting. NATURE | Vol 444 | 30 November 2006 LETTERS 589 Natu re Pu blishin g Gro u p 2 0 0 6 from the separate fragment D). The proposed model is shown in Fig. 5. We require the assumption of only one further gear (m3), whose proposed shaft is clearly broken off in the CT. A detailed description is contained in Supplementary Note 3 (gears). Of particular note is the dual use of the large gear, e3, at the back of the mechanism, which has found no use in previous models. In our model, it is powered by m3 as part of a fixed-axis train that turns the Saros and Exeligmos dials for eclipse prediction, and also doubles as the epicyclic table for the gears k1, k2. These are part of epicyclic gearing that calculates the theory of the irregular motion of the moon, developed by Hipparchos some time between 146 and 128 BC (ref. 22)the first anomaly, caused by its elliptical orbit about the Earth. The period of this anomaly is the period from apogee to apogee (the anomalistic month). To realize this theory, the mean sidereal lunar motion is first calculated by gears on axes c, d and e and this is then fed into the epicyclic system. As explained in Fig. 6, a pin- and-slot device on the epicyclic gears k1 and k2, clearly seen in the CT, provides the variation. This was previously identified 4 , but rejected as a lunar mechanism. The remarkable purpose of mounting the pin-and-slot mechanism on the gear e3 is to change the period of variation from sidereal month (that is, the time taken for the Moon to orbit the Earth relative to the zodiac), which would occur if k1 and k2 were on fixed axes, to anomalistic monthby carrying the gears epicyclically at a rate that is the difference between the rates of the Front dials Lunar phase Lost epicyclic gearing Pin and slot Hipparchos lunar mechanism Possibly Hipparchos solar mechanism and planetary mechanisms Hipparchos sidereal month Year Back dials Luni-solar calendar Input Eclipse prediction Saros 4Metonic 5Callippic Exeligmos Zodiac Egyptian calendar Parapegma Figure 5 | New reconstruction of the gear trains. A schematic sectional diagram (not to scale) of the gearing, following the style of Price 1 and Wright 4 . The viewpoint is looking down from the top right of the mechanism, and is stretched in the direction of the main axes to show the structure. Features that are outlined or labelled in red are hypothetical. Gears are lettered with their shaft, and numbered with increasing distance from the front dial. The two-or-three digit number on the gear is its actual or assumed tooth count (see Supplementary Note 3 (gears)). Hypothetical gears n1, n2, p1, p2 have been proposed previously, the gear m3 on the broken-off shaft m is our addition. All gears, except the lone one in fragment D, are now accounted for in the mechanism. The function of the trains is outlined in the text. We find no evidence in the CT for an idler wheel carried on e3 and between e5 and k1 or between k2 and e6, as has been previously proposed 1,2,4 . The CT shows a pin through axis e between gears e1 and e2. We believe its purpose is to retain the square-bossed e1 on the shaft, but its passage right through the axis rules out previous reconstructions 1,2,4 where e1 and e2 were joined by an outer pipe rotating around the shaft e. Figure 6 | The Hipparchos lunar mechanism mounted on gear e3. The figure is based on a CT slice of part of fragment A, showing (top) shaft e and (bottom) shaft k. The complete geometry cannot be seen in a single CT slice. The two gears on the e axis (e5 and e6) are coaxial, while the two k gears rotate on slightly displaced axes. k1 has a pin on its face that engages with a radial slot in k2 (and this was previously reported 5 ). In the figure the pitch circles of e5 and k1 are shown in turquoise and those of e6 and k2 in pink. The gear e5 drives k1, which drives k2 via the pin-and-slot, introducing a quasi-sinusoidal variation in the motion, which is then transmitted to e6. Our estimate of the distance between the arbors on the k gears is about 1.1 mm, with a pin distance of 9.6 mm, giving an angular variation of 6.5u. According to Ptolemy 28 , Hipparchos made two estimates for a lunar anomaly parameter, based on eclipse data, which would require angular variations of 5.9u or 4.5u herealthough estimates of the anomaly from Babylonian astronomy were generally larger. The difference from our estimated value is probably not significant given the difficulty of precise measurement of the axes in the CT. The harmonic variation, together with the effect of carrying the gears on e3 (which rotates at the period of the Moons apogee around the Earth), would simulate the correct variation for the Moons mean (sidereal) rotation rate on the front dial. An (unexplained) regular pentagon is visible at the centre of gear e5. It is tempting to associate the conception of the mechanism with Hipparchos himself, but he was not the first to assume eccentric or epicyclic models. LETTERS NATURE | Vol 444 | 30 November 2006 590 Natu re Pu blishin g Gro u p 2 0 0 6 sidereal and anomalistic months, that is, at the rate of rotation of about 9 years of the Moons apogee. Gears with 53 teeth are awkward to divide. So it may seem sur- prising that the gearing includes two such gears (f1, l2), whose effects cancel in the train leading to the Saros dial. But the gearing has been specifically designed so that the epicyclic table e3 turns at the rate of rotation of the Moons apogeethe factor 53 being derived from the calculation of this rotation from the Metonic and Saros cycles, which are the bases for all the prime factors in the tooth counts of the gears. The establishment of the 53-tooth count of these gears is powerful confirmation of our proposed model of Hipparchos lunar theory. The output of this complex system is carried from e6 back through e3 and thence, via e1 and b3, to the zodiac scale on the front dial and the lunar phase 7 mechanism. Our CT confirms the complex structure of axis e that this model entails. The Antikythera Mechanism shows great economy and ingenuity of design. It stands as a witness to the extraordinary technological potential of Ancient Greece, apparently lost within the Roman Empire. Received 10 August; accepted 17 October 2006. 1. Price, D. de S. Gears from the Greeks: The Antikythera Mechanism A calendar computer from ca. 80 BC. Trans Am. Phil. Soc. New Ser. 64, 1 70 (1974); reprinted by Science History Publications, New York (1975). 2. Wright, M. T. Epicyclic gearing and the Antikythera Mechanism, Part I. Antiquar. Horol. 27, 270 279 (2003). 3. Wright, M. T., Bromley, A. G. & Magou, H. Simple X-ray tomography and the Antikythera Mechanism. PACT J. Eur. Study Group Phys. Chem. Biol. Math. Tech. Appl. Archaeol. 45, 531 543 (1995). 4. Wright, M. T. The Antikythera Mechanism: a New Gearing Scheme. Bull. Sci. Instrum. Soc. 85, 2 7 (2005). 5. Wright, M. T. Epicyclic gearing and the Antikythera Mechanism, Part II. Antiquar. Horol. 29, 51 63 (2005). 6. Wright, M. T. Counting months and years: The upper back dial of the Antikythera Mechanism. Bull. Sci. Instrum. Soc. 87, 8 13 (2005). 7. Wright, M. T. The Antikythera Mechanism and the early history of the moon- phase display. Antiquar. Horol. 29, 319 329 (2006). 8. Wright, M. T. in Proc. 2nd Int. Conf. on Ancient Greek Technology (ed. Tassios, Th.) 49 60 (Technical Chamber of Greece, Athens, 2006). 9. Wright, M. T. A planetarium display for the Antikythera Mechanism. Horol. J. 144(5), 169 173 (2002); erratum 144, 193 (2002). 10. North, J. D. The Fontana History of Astronomy and Cosmology (Fontana, London, 1994). 11. Bromley, A. G. The Antikythera Mechanism. Horol. J. 132, 412 415 (1990). 12. Bromley, A. G. in Bassernet Vol. 2, No. 3 (Basser Department of Computer Science, Univ. Sydney, June, 1993). 13. Freeth, T. The Antikythera Mechanism: 1. Challenging the classic research. Mediterr. Archaeol. Archaeom. 2, 21 35 (2002). 14. Edmunds, M. & Morgan, P. The Antikythera Mechanism: still a mystery of Greek astronomy? Astron. Geophys. 41, 6.10 6.17 (2000). 15. Freeth, T. The Antikythera Mechanism: 2. Is it Posidonius orrery? Mediterr. Archaeol. Archaeom. 2, 45 58 (2002). 16. Illsley, J. S. History and Archaeology of the ShipLecture Notes. http:// cma.soton.ac.uk/HistShip/shlect36.htm (1998). 17. Stephenson, F. R. Historical Eclipses and Earths Rotation (Cambridge Univ. Press, Cambridge, UK, 1997). 18. Steele, J. M. Observations and Predictions of Eclipse Times by Early Astronomers (Kluwer Academic, Dordrecht, 2000). 19. Steele, J. M. Eclipse prediction in Mesopotamia. Arch. Hist. Exact Sci. 54, 421 454 (2000). 20. Wright, M. T. The Scholar, the Mechanic and the Antikythera Mechanism. Bull. Sci. Instrum . Soc. 80, 4 11 (2003). 21. Espenak, F. NASA Eclipse Home Page. http://sunearth.gsfc.nasa.gov/eclipse/ eclipse.html (2005). 22. Jones, A. The adaptation of Babylonian methods in Greek numerical astronomy. Isis 82, 440 453 (1991). 23. Britton, J. P. in Die Rolle der Astronomie in den Kulturen Mesopotamiens (ed. Galter, H. D.) 61 76 (rm-Druck & Vergansgesellschaft, Graz, 1993). 24. X-Tek Systems Ltd. 3-D computed tomography. http://www.xtek.co.uk/ct/ (2006). 25. Malzbender, T. & Gelb, D. Polynomial texture mapping. http:// www.hpl.hp.com/research/ptm/ (2006). 26. Brooks, M. Tricks of the light. New Sci. 170, 38 49 (2001). 27. Chapman, A. Dividing the Circle (Wiley, Chichester, 1995). 28. Toomer, G. J. Ptolemys Almagest (transl. Toomer, G. J.) (Princeton Univ. Press, Princeton, New Jersey, 1998). Supplementary Information is linked to the online version of the paper at www.nature.com/nature. Acknowledgements This work was financed by the Leverhulme Trust, the Walter Hudson Bequest, the University of Athens Research Committee and the Cultural Foundation of the National Bank of Greece. For essential support we thank the Ministry of Culture, Greece (P. Tatoulis), and the National Archaeological Museum of Athens (N. Kalts as). We acknowledge help and advice from J. Ambers, J. Austin, G. Dermody, H. Forsyth, I. Freestone, P. Haycock, V. Horie, A. Jones, M. Jones, P. Kipouros, H. Kritzas, J. Lossl, G. Makris, A. Ray, C. Reinhart, A. Valassopoulos, R. Westgate, T. Whiteside, S. Wright and C. Xenikakis. Author Contributions T.F. carried out most of the CT analysis of structure and its interpretation. Y.B., A.T. and X.M. read, transcribed and translated the inscriptions. H.M and M.Z. catalogued the fragments, provided guidance on X-ray examination, and measured the fragments with J.H.S. R.H. led the team (D.B., A.R., M.A., A.C. and P.H.) that built and operated the Bladerunner CT machine, and provided CT reconstructions and advice. T.M., D.G. and W.A. built, operated and provided software for the PTM. M.G.E. was academic lead, and undertook the statistical analysis. T.F. and Y.B. organised the logistics of the experimental work, with inter-agency liaison by X.M. and J.H.S. The manuscript was written by T.F. and M.G.E. including material from Y.B., A.T., X.M., J.H.S., H.M. and M.Z. T.F. designed the illustrations. Author Information Reprints and permissions information is available at www.nature.com/reprints. The authors declare no competing financial interests. Correspondence and requests for materials should be addressed to M.G.E. (mge@astro.cf.ac.uk). NATURE | Vol 444 | 30 November 2006 LETTERS 591 Natu re Pu blishin g Gro u p 2 0 0 6 View publication statsView publication stats ",
        "_version_":1718441064933621761},
      {
        "story_id":20755974,
        "story_author":"nnnmnten",
        "story_descendants":86,
        "story_score":141,
        "story_time":"2019-08-21T10:25:24Z",
        "story_title":"AdoptOpenJDK: Open-source, prebuilt OpenJDK binaries",
        "search":["AdoptOpenJDK: Open-source, prebuilt OpenJDK binaries",
          "https://adoptopenjdk.net/",
          "HotSpot is the VM from the OpenJDK community. It is the most widely used VM today and is used in Oracles JDK. It is suitable for all workloads. For more details see OpenJDK HotSpot. Eclipse OpenJ9 is the VM from the Eclipse community. It is an enterprise-grade VM designed for low memory footprint and fast start-up and is used in IBMs JDK. It is suitable for running all workloads. For more details see Eclipse OpenJ9. LTS (Long Term Support). These versions have a longer support timeframe. Suitable for enterprise customers. See Support for more information. "],
        "story_type":"Normal",
        "url_raw":"https://adoptopenjdk.net/",
        "url_text":"HotSpot is the VM from the OpenJDK community. It is the most widely used VM today and is used in Oracles JDK. It is suitable for all workloads. For more details see OpenJDK HotSpot. Eclipse OpenJ9 is the VM from the Eclipse community. It is an enterprise-grade VM designed for low memory footprint and fast start-up and is used in IBMs JDK. It is suitable for running all workloads. For more details see Eclipse OpenJ9. LTS (Long Term Support). These versions have a longer support timeframe. Suitable for enterprise customers. See Support for more information. ",
        "comments.comment_id":[20757799,
          20758542],
        "comments.comment_author":["hs86",
          "latchkey"],
        "comments.comment_descendants":[1,
          3],
        "comments.comment_time":["2019-08-21T14:55:13Z",
          "2019-08-21T16:02:38Z"],
        "comments.comment_text":["On macOS and Windows their various builds are also available over the Homebrew and Scoop package managers:<p><a href=\"https://github.com/AdoptOpenJDK/homebrew-openjdk\" rel=\"nofollow\">https://github.com/AdoptOpenJDK/homebrew-openjdk</a><p><a href=\"https://github.com/lukesampson/scoop/wiki/Java\" rel=\"nofollow\">https://github.com/lukesampson/scoop/wiki/Java</a>",
          "You don't need extra projects to manage this stuff anymore.<p>There is a great answer here... <a href=\"https://stackoverflow.com/questions/52524112/how-do-i-install-java-on-mac-osx-allowing-version-switching\" rel=\"nofollow\">https://stackoverflow.com/questions/52524112/how-do-i-instal...</a><p><pre><code>  brew install java (12)\n  brew install java11\n</code></pre>\nAnd you can simplify things...<p>In your .bashrc:<p><pre><code>  export JDK_HOME=`/usr/libexec/java_home -v 12` # v9+ change to the version you installed\n  export JAVA_PATH=$JDK_HOME\n  export JAVA_HOME=$JDK_HOME\n  export PATH=\".\"\n  export PATH=\"${PATH}:${JAVA_HOME}/bin\"</code></pre>"],
        "id":"b39c956f-c2c1-4edc-8b44-8ebd061784da",
        "_version_":1718441053500997632},
      {
        "story_id":21907168,
        "story_author":"another",
        "story_descendants":18,
        "story_score":86,
        "story_time":"2019-12-29T17:36:03Z",
        "story_title":"DIY Meta Clock with 24 Analog Clocks",
        "search":["DIY Meta Clock with 24 Analog Clocks",
          "https://mcuoneclipse.com/2019/12/29/diy-meta-clock-with-24-analog-clocks/",
          "MCU on Eclipse Everything on Eclipse, Microcontrollers and Software Main menu Home Eclipse PEx Components FreeRTOS USB FatFs I2C SPI CDE Compendium Pictures Q&A Books! Bucket List About Requests Mark Dunnett Guest Blog Post navigation Previous Next Human since 1982 claims Human since 1982 have the copyright to works displaying digital time using a grid arrangement of analog clocks Im not a lawyer, but without obligations (imho) I have removed the content. You can read more of the details here: Copyright Law for Makers and Educators Thanks for understanding, Erich "],
        "story_type":"Normal",
        "url_raw":"https://mcuoneclipse.com/2019/12/29/diy-meta-clock-with-24-analog-clocks/",
        "url_text":"MCU on Eclipse Everything on Eclipse, Microcontrollers and Software Main menu Home Eclipse PEx Components FreeRTOS USB FatFs I2C SPI CDE Compendium Pictures Q&A Books! Bucket List About Requests Mark Dunnett Guest Blog Post navigation Previous Next Human since 1982 claims Human since 1982 have the copyright to works displaying digital time using a grid arrangement of analog clocks Im not a lawyer, but without obligations (imho) I have removed the content. You can read more of the details here: Copyright Law for Makers and Educators Thanks for understanding, Erich ",
        "comments.comment_id":[21909389,
          21913416],
        "comments.comment_author":["sunsetMurk",
          "reaperducer"],
        "comments.comment_descendants":[1,
          0],
        "comments.comment_time":["2019-12-29T23:28:24Z",
          "2019-12-30T14:11:45Z"],
        "comments.comment_text":["Dang. I want one of these on my wall!<p>But, I don't have the $$$ for the real ones [0], nor the knowhow to build one using the info in this post.<p>Maybe I'll try to get some time together to learn more about PCB design/hardware... or even try and turn this into a side project to try an open-source collab to make it a more complete kit/tutorial.<p>0 - <a href=\"https://clockclock.com/\" rel=\"nofollow\">https://clockclock.com/</a>",
          "There was an iOS version of this from a Swedish company called Chapel. It appears to be officially licensed from Humans Since 1982.<p>Unfortunately, the app company seems to be gone, so the app is no longer on the App Store (I still have it installed on my devices, though.)<p>But the originators are still around: <a href=\"https://www.humanssince1982.com/\" rel=\"nofollow\">https://www.humanssince1982.com/</a>"],
        "id":"a7706b94-f14d-442e-a3d5-418a5d191c74",
        "_version_":1718441081947815936},
      {
        "story_id":20602318,
        "story_author":"mr_puzzled",
        "story_descendants":13,
        "story_score":14,
        "story_time":"2019-08-03T19:56:36Z",
        "story_title":"My experience using Spring boot for website development",
        "search":["My experience using Spring boot for website development",
          "At work, I was asked to learn spring boot and the first task was to build a simple crud app, just end to end functionality for a model with automated tests, builds, static code analysis etc. I hate to admit it, but for the first few days (and even now) I couldn't tell which was up and which way was down, nothing made sense.<p>- Installation was a nightmare. Workplace uses windows, so the usual dance of installing the jdk, setting the env variables, creating a project in eclipse and for some reason it uses the jre instead of the jdk. Some plugins/packages such as lombok work in eclipse but not in vs code! And even in eclipse there's some annotation (@Data I think) that's supposed to generate getters and setters but it doesn't work.<p>- Flexible architecture : some use service, serviceimpl and controllers. Some skip the service interface. And it turns out you can extend crudrepository and have a fully functioning rest endpoint without writing any code using spring data rest!<p>- Magic : this is incredibly frustrating. Some repository types are automatically exported, some are not and the docs do an atrocious job of explaing what's going on. Place some file abc.xyz in the root and magic happens!<p>- Docs : the single worst part of learning spring boot, especially for newcomers. How can the docs possibly be so insensitive to newcomers? Or do they expect only experienced devs at big cos to use spring boot?<p>- I hate to admit this, but I still don't know what a god damn bean is, what AOP is, what inversion of control is.<p>This was mainly a rant and a bad one at that, but I had to vent."],
        "story_text":"At work, I was asked to learn spring boot and the first task was to build a simple crud app, just end to end functionality for a model with automated tests, builds, static code analysis etc. I hate to admit it, but for the first few days (and even now) I couldn't tell which was up and which way was down, nothing made sense.<p>- Installation was a nightmare. Workplace uses windows, so the usual dance of installing the jdk, setting the env variables, creating a project in eclipse and for some reason it uses the jre instead of the jdk. Some plugins/packages such as lombok work in eclipse but not in vs code! And even in eclipse there's some annotation (@Data I think) that's supposed to generate getters and setters but it doesn't work.<p>- Flexible architecture : some use service, serviceimpl and controllers. Some skip the service interface. And it turns out you can extend crudrepository and have a fully functioning rest endpoint without writing any code using spring data rest!<p>- Magic : this is incredibly frustrating. Some repository types are automatically exported, some are not and the docs do an atrocious job of explaing what's going on. Place some file abc.xyz in the root and magic happens!<p>- Docs : the single worst part of learning spring boot, especially for newcomers. How can the docs possibly be so insensitive to newcomers? Or do they expect only experienced devs at big cos to use spring boot?<p>- I hate to admit this, but I still don't know what a god damn bean is, what AOP is, what inversion of control is.<p>This was mainly a rant and a bad one at that, but I had to vent.",
        "story_type":"AskHN",
        "comments.comment_id":[20604369,
          20631954],
        "comments.comment_author":["karmakaze",
          "tmm84"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-08-04T04:07:05Z",
          "2019-08-07T04:22:52Z"],
        "comments.comment_text":["I would say that you were likely more successful than the typical first time exposure using Sprint Boot.<p>Your comments also sound spot-on to much of my first time exposure to Spring (pre Boot). I've used Spring and Spring Boot on many projects since and it wouldn't be my first choice. It tends to be selected for its popularity which means there are StackOverflow answers, pretty good support for common use cases, and a large pool to hire from.<p>At the same time, Spring Boot (or even just Spring) aren't the worst once you get used to them. I myself have come to dislike other aspects of it (e.g. slow startup, JPQL and Hibernate quirks, excessive gc, poor scaling). Most of these things probably won't have much of an impact until the application gets to a certain level of complexity or scale. But if it wasn't about solving for the future, Ruby or Python would be better to start.",
          "I've been programming in Java since I started working as a programmer. Learning to setup a Windows machine for Java development is difficult if you aren't familiar with environment variables and the like.<p>Java, like C++, has gotten more and more features/keywords over the years. Frameworks like Spring Boot tend to make use of those features. Also, Spring Boot is a sit down and read the guides compared to something like Rails or ExpressJS and the like. Depending on your Java knowledge the documentation is going to be hit or miss I think. Also, convention over configuration makes it harder for new developers to get the hang of developing with a framework like Spring Boot because they don't know the conventions usually."],
        "id":"6d3e9c51-6acf-419a-963d-bf86fdf85f84",
        "_version_":1718441049391628288},
      {
        "story_id":20490580,
        "story_author":"pps",
        "story_descendants":49,
        "story_score":95,
        "story_time":"2019-07-21T09:26:44Z",
        "story_title":"Anaxagoras Was Exiled for Claiming the Moon Was a Rock",
        "search":["Anaxagoras Was Exiled for Claiming the Moon Was a Rock",
          "https://www.smithsonianmag.com/science-nature/ancient-greek-philosopher-was-exiled-claiming-moon-was-rock-not-god-180972447/",
          "Anaxagoras, who lived in the fifth century B.C., was one of the first people in recorded history to recognize that the moon was a rocky, mountainous body. Eduard Lebiedzki / Public Domain Close to the north pole of the moon lies the crater Anaxagoras, named for a Greek philosopher who lived in the fifth century B.C. The eponym is fitting, as Anaxagoras the man was one of the first people in history to suggest the moon was a rocky body, not all too dissimilar from Earth. Streaks of material thrown out during the impact that formed the crater extend 560 miles southward to the rim of another crater, this one named for Plato. Like Plato, Anaxagoras the scholar did most of his work in Athens, but the similarities between the two men stop there. Influenced strongly by the Pythagoreans, Plato posited a mystical universe based on sacred geometric forms, including perfectly circular orbits. Plato eschewed observation and experimentation, preferring to pursue a pure knowledge he believed was innate in all humans. But Anaxagoras, who died around the time Plato was born, had a knack for astronomy, an area of study that requires careful observational and calculation to unlock the mysteries of the universe. During his time in Athens, Anaxagoras made several fundamental discoveries about the moon. He reiterated and expended upon an idea that likely emerged among his predecessors but was not widely accepted in antiquity: that the moon and sun were not gods, but rather objects. This seemingly innocuous belief would ultimately result in Anaxagoras arrest and exile. Anaxagoras crater near the north pole of the moon, imaged by the Lunar Orbiter 4 spacecraft in 1967. NASA Piecing together the lives of early philosophers such as Anaxagoras, who is thought to have written just one book, lost to us today, can be a major challenge for historians. Modern scholars have only fragments to describe the life of Anaxagorasbrief quotes from his teachings and short summaries of his ideas, cited within the works of scholars from later generations, such as Plato and Aristotle. Through persistent observation, Anaxagoras came to believe that the moon was a rock, not totally unlike the Earth, and he even described mountains on the lunar surface. The sun, he thought, was a burning rock. In fragment 18, Anaxagoras says, It is the sun that puts brightness into the moon. While Anaxagoras was not the first to realize that moonlight is reflected light from the sun, he was able to use this concept to correctly explain additional natural phenomena, such as eclipses and lunar phases. Hailing from Clazomenae in the Ionian lands east of the Greek mainland, Anaxagoras grew up during the Ionian Enlightenment, an intellectual revolution that began around 600 B.C. As a young man, he saw Athens and Sparta align to drive the Persian Empire out of Ionia. When he relocated to Athens, Anaxagoras and his contemporaries brought philosophy to the budding Athenian democracy. Although many Greek philosophers of the sixth and fifth centuries B.C. believed in one or a few fundamental elementssuch as water, air, fire and earthAnaxagoras thought there must be an infinite number of elements. This idea was his way of resolving an intellectual dispute concerning the nature of existence that had emerged between the naturalistic-minded philosophers of Ionia to the east and the mystical-minded philosophers to the west, in Greek-colonized Italy, such as Pythagoras and his followers. Daniel Graham, a professor of philosophy at Brigham Young University and one of the few Anaxagoras experts in the world, says that of the Italian-based philosophers, Parmenides in particular influenced Anaxagoras and his ideas about astronomy. Anaxagoras turns the problem of lunar light into a problem of geometry, Graham says. He noted that when the moon is on the opposite side of the Earth than the sun, the full face is illuminated, [producing] a model of the heavens that predicts not only phases of the moon, but how eclipses are possible. The moons phases, Anaxagoras realized, were the result of different portions of the celestial object being illuminated by the sun from Earths perspective. The philosopher also realized that the occasional darkening of the moon must result from the moon, sun and Earth lining up such that the moon passes into the Earths shadowa lunar eclipse. When the moon passes directly in front of the sun, the skies darken during the day, a phenomenon Anaxagoras also described and we now call a solar eclipse. The total lunar eclipse of October 8, 2014, as photographed from California. When the shadow of the Earth covers the moon, only light filtered through Earth's atmosphere reaches the lunar surface, casting the moon in a reddish glow. Alfredo Garcia, Jr. / Flickr under CC BY-SA 2.0 Anaxagoras also wrestled with the origins and formation of the moon, a mystery that still challenges scientists today. The philosopher proposed that the moon was a big rock which the early Earth had flung into space. This concept anticipated a scenario for the moons origin that physicist George Darwin, son of Charles Darwin, would propose 23 centuries later. Known as the fission hypothesis, Darwins idea was that the moon began as a chunk of Earth and was hurled into space by the Earths rapid rotation, leaving behind the Pacific basin. (Today, many astronomers believe that a Mars-sized body slammed into the early Earth, expelling material that then coalesced into the moon, though other theories exist for the origin of our natural satellite.) By describing the moon as a rock of terrestrial origin, and the sun as a burning rock, Anaxagoras moved beyond earlier thinkers, even those who realized the moon was a kind of reflector. This forward thinking got Anaxagoras labeled as a chief denier of the idea that the moon and sun were deities. Such an idea should have been welcome in democratic Athens, but Anaxagoras was a teacher and friend of the influential statesman Pericles, and political factions would soon conspire against him. In power for over 30 years, Pericles would lead Athens into the Peloponnesian wars against Sparta. While the exact causes of these conflicts are a matter of debate, Pericles political opponents in the years leading to the wars blamed him for excessive aggression and arrogance. Unable to hurt the Athenian leader directly, Pericles enemies went after his friends. Anaxagoras was arrested, tried and sentenced to death, ostensibly for breaking impiety laws while promoting his ideas about the moon and sun. In the Athenian democracy, with its democratic trials before large juries on criminal charges being brought by private citizensthere was no district attorneyall trials were basically political trials, Graham says. They were often disguised as being about religion or morality, but they aimed at embarrassing some public figure by going after him directly if he was vulnerable, or a member of his circle if he was not. If you wanted to attack Pericles, but he was too popular to attack directly, you found the weakest link in his group. As a foreigner and intellectual with unorthodox new ideas, Pericles friend and science advisor Anaxagoras was an obvious target. Still holding some political sway, Pericles was able to free Anaxagoras and prevent his execution. Though his life was spared, the philosopher who questioned the divinity of the moon found himself in exile in Lampsacus at the edge of the Hellespont. But his ideas regarding eclipses and lunar phases would live on to this day, and for his recognition of the true nature of the moon, a lunar crater, visited by orbiting spacecraft some 2,400 years later, bears the name Anaxagoras. Ancient Greece Astronomy Moon Outer Space Recommended Videos "],
        "story_type":"Normal",
        "url_raw":"https://www.smithsonianmag.com/science-nature/ancient-greek-philosopher-was-exiled-claiming-moon-was-rock-not-god-180972447/",
        "comments.comment_id":[20494002,
          20497472],
        "comments.comment_author":["perl4ever",
          "Agentlien"],
        "comments.comment_descendants":[2,
          5],
        "comments.comment_time":["2019-07-21T22:14:17Z",
          "2019-07-22T12:48:16Z"],
        "comments.comment_text":["I find the picture of Anaxagoras (the crater) rather annoying, because they didn't orient it with the light source coming from the upper left, as is (or used to be) kind of the convention on a computer screen.",
          "The title of the article doesn't really match what the article has to say about his trial. It does, however, work nicely as an example of a general rule I have:<p>I'm always suspicious when hearing tales about scientists who were punished for their discoveries by less enlightened minds. Looking into it, it mostly turns out they were either sentenced for other crimes or, as this article claims was the case here, that it was most likely a political move masquerading as being motivated by theology.<p>Which itself falls under another similar guideline: if any big societal undertaking (wars unfortunately seem to be the most common example) seems motivated by religion, that's usually not the true reason behind it.<p>Perhaps my own beliefs are colouring my interpretations, but it just seems like people's actual motivations tend to be pragmatic but they market their actions as being out of belief or morality."],
        "id":"aa16701e-40b8-48b0-b08b-9e71fc577126",
        "url_text":"Anaxagoras, who lived in the fifth century B.C., was one of the first people in recorded history to recognize that the moon was a rocky, mountainous body. Eduard Lebiedzki / Public Domain Close to the north pole of the moon lies the crater Anaxagoras, named for a Greek philosopher who lived in the fifth century B.C. The eponym is fitting, as Anaxagoras the man was one of the first people in history to suggest the moon was a rocky body, not all too dissimilar from Earth. Streaks of material thrown out during the impact that formed the crater extend 560 miles southward to the rim of another crater, this one named for Plato. Like Plato, Anaxagoras the scholar did most of his work in Athens, but the similarities between the two men stop there. Influenced strongly by the Pythagoreans, Plato posited a mystical universe based on sacred geometric forms, including perfectly circular orbits. Plato eschewed observation and experimentation, preferring to pursue a pure knowledge he believed was innate in all humans. But Anaxagoras, who died around the time Plato was born, had a knack for astronomy, an area of study that requires careful observational and calculation to unlock the mysteries of the universe. During his time in Athens, Anaxagoras made several fundamental discoveries about the moon. He reiterated and expended upon an idea that likely emerged among his predecessors but was not widely accepted in antiquity: that the moon and sun were not gods, but rather objects. This seemingly innocuous belief would ultimately result in Anaxagoras arrest and exile. Anaxagoras crater near the north pole of the moon, imaged by the Lunar Orbiter 4 spacecraft in 1967. NASA Piecing together the lives of early philosophers such as Anaxagoras, who is thought to have written just one book, lost to us today, can be a major challenge for historians. Modern scholars have only fragments to describe the life of Anaxagorasbrief quotes from his teachings and short summaries of his ideas, cited within the works of scholars from later generations, such as Plato and Aristotle. Through persistent observation, Anaxagoras came to believe that the moon was a rock, not totally unlike the Earth, and he even described mountains on the lunar surface. The sun, he thought, was a burning rock. In fragment 18, Anaxagoras says, It is the sun that puts brightness into the moon. While Anaxagoras was not the first to realize that moonlight is reflected light from the sun, he was able to use this concept to correctly explain additional natural phenomena, such as eclipses and lunar phases. Hailing from Clazomenae in the Ionian lands east of the Greek mainland, Anaxagoras grew up during the Ionian Enlightenment, an intellectual revolution that began around 600 B.C. As a young man, he saw Athens and Sparta align to drive the Persian Empire out of Ionia. When he relocated to Athens, Anaxagoras and his contemporaries brought philosophy to the budding Athenian democracy. Although many Greek philosophers of the sixth and fifth centuries B.C. believed in one or a few fundamental elementssuch as water, air, fire and earthAnaxagoras thought there must be an infinite number of elements. This idea was his way of resolving an intellectual dispute concerning the nature of existence that had emerged between the naturalistic-minded philosophers of Ionia to the east and the mystical-minded philosophers to the west, in Greek-colonized Italy, such as Pythagoras and his followers. Daniel Graham, a professor of philosophy at Brigham Young University and one of the few Anaxagoras experts in the world, says that of the Italian-based philosophers, Parmenides in particular influenced Anaxagoras and his ideas about astronomy. Anaxagoras turns the problem of lunar light into a problem of geometry, Graham says. He noted that when the moon is on the opposite side of the Earth than the sun, the full face is illuminated, [producing] a model of the heavens that predicts not only phases of the moon, but how eclipses are possible. The moons phases, Anaxagoras realized, were the result of different portions of the celestial object being illuminated by the sun from Earths perspective. The philosopher also realized that the occasional darkening of the moon must result from the moon, sun and Earth lining up such that the moon passes into the Earths shadowa lunar eclipse. When the moon passes directly in front of the sun, the skies darken during the day, a phenomenon Anaxagoras also described and we now call a solar eclipse. The total lunar eclipse of October 8, 2014, as photographed from California. When the shadow of the Earth covers the moon, only light filtered through Earth's atmosphere reaches the lunar surface, casting the moon in a reddish glow. Alfredo Garcia, Jr. / Flickr under CC BY-SA 2.0 Anaxagoras also wrestled with the origins and formation of the moon, a mystery that still challenges scientists today. The philosopher proposed that the moon was a big rock which the early Earth had flung into space. This concept anticipated a scenario for the moons origin that physicist George Darwin, son of Charles Darwin, would propose 23 centuries later. Known as the fission hypothesis, Darwins idea was that the moon began as a chunk of Earth and was hurled into space by the Earths rapid rotation, leaving behind the Pacific basin. (Today, many astronomers believe that a Mars-sized body slammed into the early Earth, expelling material that then coalesced into the moon, though other theories exist for the origin of our natural satellite.) By describing the moon as a rock of terrestrial origin, and the sun as a burning rock, Anaxagoras moved beyond earlier thinkers, even those who realized the moon was a kind of reflector. This forward thinking got Anaxagoras labeled as a chief denier of the idea that the moon and sun were deities. Such an idea should have been welcome in democratic Athens, but Anaxagoras was a teacher and friend of the influential statesman Pericles, and political factions would soon conspire against him. In power for over 30 years, Pericles would lead Athens into the Peloponnesian wars against Sparta. While the exact causes of these conflicts are a matter of debate, Pericles political opponents in the years leading to the wars blamed him for excessive aggression and arrogance. Unable to hurt the Athenian leader directly, Pericles enemies went after his friends. Anaxagoras was arrested, tried and sentenced to death, ostensibly for breaking impiety laws while promoting his ideas about the moon and sun. In the Athenian democracy, with its democratic trials before large juries on criminal charges being brought by private citizensthere was no district attorneyall trials were basically political trials, Graham says. They were often disguised as being about religion or morality, but they aimed at embarrassing some public figure by going after him directly if he was vulnerable, or a member of his circle if he was not. If you wanted to attack Pericles, but he was too popular to attack directly, you found the weakest link in his group. As a foreigner and intellectual with unorthodox new ideas, Pericles friend and science advisor Anaxagoras was an obvious target. Still holding some political sway, Pericles was able to free Anaxagoras and prevent his execution. Though his life was spared, the philosopher who questioned the divinity of the moon found himself in exile in Lampsacus at the edge of the Hellespont. But his ideas regarding eclipses and lunar phases would live on to this day, and for his recognition of the true nature of the moon, a lunar crater, visited by orbiting spacecraft some 2,400 years later, bears the name Anaxagoras. Ancient Greece Astronomy Moon Outer Space Recommended Videos ",
        "_version_":1718441046561521664},
      {
        "story_id":21202341,
        "story_author":"Tomte",
        "story_descendants":10,
        "story_score":50,
        "story_time":"2019-10-09T12:02:48Z",
        "story_title":"The KeY Project",
        "search":["The KeY Project",
          "https://www.key-project.org/",
          "Skip to content The KeY to Software Correctness Decades of experience in proving program correctness for Java. Download now Available now: The KeY Book The definitive resource about the KeY system. Tell me more! Exception in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 40 at java.util.TimSort.pushRun(TimSort.java:413) at java.util.TimSort.sort(TimSort.java:240) at java.util.Arrays.sort(Arrays.java:1438) at TestTimSort.main(TestTimSort.java:18) The TimSort Success Story Read about how KeY was used to find a well-concealed bug in the Java standard library. Read the blog post Teaching material available! There is a variety of courses around KeY. Have a look Previous Next Functional Verification Tired of experiencing bugs that weren't covered by your test cases before? KeY lets you augment your Java program with a specification written in the Java Modeling Language JML and helps you proving that your program behaves as it should. View details Symbolic Debugging Debugging sucks! Well, not if you're using our Symbolic Execution Debugger. Profit from a graphical visualization of the program's execution flow, state inspection and various other features to quickly uncover undesired behavior! View details Teaching and Research KeY is already used extensively for teaching at different universities. Use the tool's possibilities for teaching your students important concepts like design by contract, applied logics and program verification. Or use KeY for realizing your research project. View details Most Recent Posts The KeYNote series is a virtual workshop where teams from Germany, the Netherlands, Norway and Sweden (in lexicographic order) take part and present recent work which uses or extends the KeY verification system. Continue reading KeYNote Series Just in time for the holiday season, the new book has arrived. The LNCS volume 12345* Deductive Software Verification: Future Perspectives contains a collection of articles reflections on the occasion of 20 years of KeY. Continue reading The new book has arrived! This is a short comparison report about a verification task solved with KeY, Why3, Dafny and Frama-C. The original challenge comes from a real-world situation. There is no particular trick needed for the specification and verification; it is rather straightforward. Yet, the required annotations to achieve the specification are not too few making the example a good opportunity to compare different specification languages. Continue reading Proving Line Wrapping in KeY, Why3, Dafny and Frama-C During the VerifyThis competition 2021, KeY was invited to present itself as a Java verification tool. It is customary for Verify This that for one tool after a brief introduction into the concepts of the tool, the participants are invited to solve a micro challenge with a little help from present KeY developers. Continue reading KeY Tutorial at VerifyThis 2021 After almost 2 years of active development, we present now KeY 2.8 just before the years end. The new KeY version comes with significant improvements on the calculus side, but features also a major overhaul of the user interface. You can try it out here We thank all contributors for reaching this milestone. Nice holidays and a happy new year!The KeY Team Functional Verification We can do better than testing! KeY lets you specify the desired behavior of your program in the well-known specification language JML, and helps you proving that your programs conforms to its specification. That way, you did not only show that your program behaves as expected for some set of test values - you proved that it works correctly for all possible values! Go beyond testing - start proving! Symbolic Debugging The next-generation debugger. Imagine that you start debugging without having to think of particular input values. Your debugger instantly tells you that part of your code is unreachable, shows you all feasible runs in a beautiful graph and gives you graphical hints on where the problem you're after could possibly hide. While performing all this magic, it gracefully integrates within the Eclipse IDE and offers tools (breakpoints etc.) that you know from the debuggers of yesterday. Meet the Symbolic Execution Debugger! Teaching & Research It was never easier to fall in love with formal methods. Use our tool to demonstrate to your students that formal concepts, like first-order logic, Hoare logic and symbolic execution, are not just dry theoretical ideas, but are practically used and even integrated into the IDE they know. KeY is already extensively used in different universities, in several countries. Currently, our team is spending a lot of effort on making KeY even more attractive to new research projects by increasing its extensibility. Use KeY for your own research project or in your courses! "],
        "story_type":"Normal",
        "url_raw":"https://www.key-project.org/",
        "comments.comment_id":[21202794,
          21212670],
        "comments.comment_author":["giancarlostoro",
          "LeanderK"],
        "comments.comment_descendants":[0,
          2],
        "comments.comment_time":["2019-10-09T13:09:07Z",
          "2019-10-10T10:02:28Z"],
        "comments.comment_text":["Looks interesting, surprised to see it runs on Eclipse. I had to try Eclipse again for Garmin after years of not using it. Man Eclipse is buggier and less useful than I remembered it being. I love IntelliJ and can't replace it, my only exception would be Neovim (which I'm still waiting on JetBrains to allow embedding) or VS Code.",
          "Anybody used it? I attended the lecture \"introduction to formal methods\" by one of the profs behing the project at KIT. It was a fantastic lecture and the prof was very good, but the scriptum was 600 pages long just for a few ECTS. I always wondered whether this project has industrial use somewhere or not."],
        "id":"221f4fcb-0d89-4a3e-91b5-bb586aeb5c58",
        "url_text":"Skip to content The KeY to Software Correctness Decades of experience in proving program correctness for Java. Download now Available now: The KeY Book The definitive resource about the KeY system. Tell me more! Exception in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 40 at java.util.TimSort.pushRun(TimSort.java:413) at java.util.TimSort.sort(TimSort.java:240) at java.util.Arrays.sort(Arrays.java:1438) at TestTimSort.main(TestTimSort.java:18) The TimSort Success Story Read about how KeY was used to find a well-concealed bug in the Java standard library. Read the blog post Teaching material available! There is a variety of courses around KeY. Have a look Previous Next Functional Verification Tired of experiencing bugs that weren't covered by your test cases before? KeY lets you augment your Java program with a specification written in the Java Modeling Language JML and helps you proving that your program behaves as it should. View details Symbolic Debugging Debugging sucks! Well, not if you're using our Symbolic Execution Debugger. Profit from a graphical visualization of the program's execution flow, state inspection and various other features to quickly uncover undesired behavior! View details Teaching and Research KeY is already used extensively for teaching at different universities. Use the tool's possibilities for teaching your students important concepts like design by contract, applied logics and program verification. Or use KeY for realizing your research project. View details Most Recent Posts The KeYNote series is a virtual workshop where teams from Germany, the Netherlands, Norway and Sweden (in lexicographic order) take part and present recent work which uses or extends the KeY verification system. Continue reading KeYNote Series Just in time for the holiday season, the new book has arrived. The LNCS volume 12345* Deductive Software Verification: Future Perspectives contains a collection of articles reflections on the occasion of 20 years of KeY. Continue reading The new book has arrived! This is a short comparison report about a verification task solved with KeY, Why3, Dafny and Frama-C. The original challenge comes from a real-world situation. There is no particular trick needed for the specification and verification; it is rather straightforward. Yet, the required annotations to achieve the specification are not too few making the example a good opportunity to compare different specification languages. Continue reading Proving Line Wrapping in KeY, Why3, Dafny and Frama-C During the VerifyThis competition 2021, KeY was invited to present itself as a Java verification tool. It is customary for Verify This that for one tool after a brief introduction into the concepts of the tool, the participants are invited to solve a micro challenge with a little help from present KeY developers. Continue reading KeY Tutorial at VerifyThis 2021 After almost 2 years of active development, we present now KeY 2.8 just before the years end. The new KeY version comes with significant improvements on the calculus side, but features also a major overhaul of the user interface. You can try it out here We thank all contributors for reaching this milestone. Nice holidays and a happy new year!The KeY Team Functional Verification We can do better than testing! KeY lets you specify the desired behavior of your program in the well-known specification language JML, and helps you proving that your programs conforms to its specification. That way, you did not only show that your program behaves as expected for some set of test values - you proved that it works correctly for all possible values! Go beyond testing - start proving! Symbolic Debugging The next-generation debugger. Imagine that you start debugging without having to think of particular input values. Your debugger instantly tells you that part of your code is unreachable, shows you all feasible runs in a beautiful graph and gives you graphical hints on where the problem you're after could possibly hide. While performing all this magic, it gracefully integrates within the Eclipse IDE and offers tools (breakpoints etc.) that you know from the debuggers of yesterday. Meet the Symbolic Execution Debugger! Teaching & Research It was never easier to fall in love with formal methods. Use our tool to demonstrate to your students that formal concepts, like first-order logic, Hoare logic and symbolic execution, are not just dry theoretical ideas, but are practically used and even integrated into the IDE they know. KeY is already extensively used in different universities, in several countries. Currently, our team is spending a lot of effort on making KeY even more attractive to new research projects by increasing its extensibility. Use KeY for your own research project or in your courses! ",
        "_version_":1718441065091956737},
      {
        "story_id":21567800,
        "story_author":"jhallenworld",
        "story_descendants":23,
        "story_score":60,
        "story_time":"2019-11-18T20:22:50Z",
        "story_title":"How to set up Xilinx Vivado for source control",
        "search":["How to set up Xilinx Vivado for source control",
          "https://github.com/jhallen/vivado_setup",
          "Xilinx Vivado setup for source control This was tested on Vivado version 2019.1 Contents Project setup Build steps Save the script Xilinx IP IP Integrator / Block design XSDK XSDK review Source control for XSDK Rebuild steps Project setup Here is one way to structure your FPGA project so that it is compatible with both Xilinx Vivado GUI in project mode and source control. This setup allows you to check in the minimum number of files needed so that you can easily recreate the project in another sandbox. To save you some time: Once you have a Vivado project, you can not move it to a different directory. The project files are filled with absolute paths. It is therefore pointless to check in the entire project. There is a TCL command \"write_project_tcl\" which generates a TCL script which rebuilds the project in a new directory. This method is shown here. Even with \"write_project_tcl\", the project has to be structured correctly or the script will not work. In particular, none of the files needed to recreate the project should be in the project subdirectory. The TCL file could be used as a starting point for your own build script. However, it may be better to not modify it at all. Instead, repeatedly execute \"write_project_tcl\" throughout development to pick up the latest project changes. Although it's possible to script the entire FPGA build process, it is also important to be able to use project mode to access the Vivado GUI for such tasks as debugging with the integrated logic analyzer and pin-planning. If your setup is incorrect, you will waste a lot of time fixing it, or create a mess in your repository by checking in too many files. Unfortunately, Vivado really would like your source code to be in the project directory, so you have to actively fight it to prevent this. The directory structure should be this: example_fpga/ rebuild.tcl - TCL script created by Vivado. Checked in. rtl/ - Your Verilog (or VHDL) source code. All checked in. cons/ - Your constraint files. All checked in. project_1/ - Vivado project. Nothing here is checked in. ip/ - Xilinx IP. Some files checked in. The idea is to have the minimal set of files checked in so that you get the Vivado project back after a fresh clone. You should only have to type these commands: git clone https://github.com/jhallen/example_fpga.git cd example_fpga vivado -source rebuild.tcl Also, you can always delete project_1/ and rebuild it. rm -rf project_1 vivado -source rebuild.tcl The ip/ directory gets polluted with derived files, but at least Vivado tells you which ones need to be saved in source control. If the correct files are checked in, you can delete all derived files and rebuild: rm -rf project_1 rm -rf ip - Delete all Xilinx IP files (only after you've checked in the source) git checkout ip - Restore the actual source files vivado -source rebuild.tcl Build steps Suppose you don't have the project_1/ and rebuild.tcl script. This is how to create them. Start Vivado. Change directory to example_fpga first. Go through the usual sequence of creating a project, but do not check any of the boxes that say \"copy into project\": You need your Verilog or VHDL source files: You need the .xci files for Xilinx \"IP Catalog\" IP. You also need the wrapper file for block designs (see below for more about this). Do not check the box here: You need your constraint files: Don't check it here either! Then we wait a long time, why is Vivado so slow? Now finally the project appears: Save the script Use the write_project_tcl command to save the script. You will likely need to add the \"-force\" option to allow it overwrite the previous version of the script: Here are the messages this command prints: That's it. Now you can regenerate the project with: rm -rf project_1 vivado -source rebuild.tcl Note that the rebuild.tcl script shows you the files that need to be saved in source control in comments. This is in the file: # 3. The following remote source files that were added to the original project:- # # \"/home/jallen/example_fpga/ip/clk_wiz_0/clk_wiz_0.xci\" # \"/home/jallen/example_fpga/rtl/testtop.v\" # \"/home/jallen/example_fpga/cons/mycons.xdc\" You will have big problems if any of these are in the project directory (example_fpga/project_1 in this case). It does not work to delete the project_1/ directory, and sneakily write the files back. First the rebuild.tcl script will fail in the line with create_project because the directory already exists. You can try to get around this by adding -force the line with create_project. Unfortunately, create_project -force deletes the entire directory, so then Vivado will complain that the files are missing. You really need to create the source files outside of the project directory in the first place. Xilinx IP The next difficulty is that Xilinx IP from the \"IP Catalog\" is written by default to the project directory. An example is the clock wizard IP to use a PLL or MMCM. Take note of the \"IP Location\" in the top left of the window. It should just show you the location, but the GUI design is bad here so have to click on it. When you click on it, you will see that it wants to put it in the project: Instead put it in the ip/ directory. You need to generete the output products, even though they do not have to be saved in git. When you generate them here, then Vivado knows to regenerate them when you rebuild the project. Just the .xci file needs to be saved in source control, but it's a good idea to run the write_project_tcl command and check the comments to be sure. Unfortunately the output products are generated in the same tree as the .xci files, so you have to pay close attention to the ip/ directory. You should know that there is a setting to override the default \"IP Location\". It's here: Seems good right? But it isn't. The problem is that this is a system-wide setting, not a per-project setting. This default does not end up in the project tcl file, and if you change it, it will apply to all of your projects. You could end up building IP from one project in another project's IP directory. I found that this setting is stored here: ~/.Xilinx find . -type f | xargs grep newip ./Vivado/2019.1/vivado.xml: <recent index=\"0\" path=\"/home/jallen/quicktest1/newip\"/> ./Vivado/2019.1/vivado.xml: <CUSTOMIZED_IP_DEFAULT_LOCATION value=\"/home/jallen/quicktest1/newip\"/> My recommendation for this setting is to change the path to something you don't have permission to write to, perhaps \"/root\". This way, if you attempt to save IP to the default location, it will fail and inform you. IP Integrator / Block design Block designs must also be created outside of the project directory (for example, in our ip/ directory). This is odd, because the block design ends up as a set of commands in the project tcl script produced by write_project_tcl. The problem is that the HDL wrapper file is not produced by the project tcl, but must exist for this script to work without failing. When block designs are produced in ip/, many files are generated, but only the wrapper file has to be checked into the source control. This file is in the list in the comments of the project tcl script: # 3. The following remote source files that were added to the original project:- # # \"/home/jallen/tryit/ip/design_1/hdl/design_1_wrapper.v\" # \"/home/jallen/tryit/cons/mycons.xdc\" Anyway, when you create a block design, the default is for it to be produced inside the project: Change this to ip/: Now we create the block design: Populate the block design canvas: Right click on the block design and create the HDL wrapper: That's it. Now you have the files necessary for checking into source control so you can use the write_project_tcl command. Xilinx SDK / Eclipse Review XSDK allows you to develop software for the system you create on the FPGA. Let's review a little about how XSDK works. First thing you must do is export the hardware .hdf file from Vivado: As usual, Vivado will want to put it in the project: You should save it somewhere else, perhaps at the top level since this is the one file that the software team will care about: The .hdf file is a .zip file containing the list of peripherals in your design, the address map and possibly the .bit file- basically everything the software needs to know about the hardware: ~/quicktest1 unzip -v design_2_wrapper.hdf Archive: design_2_wrapper.hdf Length Method Size Cmpr Date Time CRC-32 Name -------- ------ ------- ---- ---------- ----- -------- ---- 620 Defl:N 353 43% 2019-11-18 16:34 619e2f0a sysdef.xml 261182 Defl:S 24932 91% 2019-11-18 16:34 0959ea2b design_1.hwh 15855 Defl:S 4021 75% 2019-11-18 16:34 00750531 design_1_bd.tcl 1241916 Defl:S 58412 95% 2019-11-18 16:34 de68834d design_1_wrapper.bit 1436 Defl:S 629 56% 2019-11-18 16:34 014b194a design_1_wrapper.mmi -------- ------- --- ------- 1521009 88347 94% 5 files Now we start XSDK, either through Vivado: Of course, it assumes you want the files in the project: You should put the workspace directory (where XSDK will put the software projects you create) outside of the Vivado project. The \"Exported Location\" is the directory where the .hdf file was placed: Alternatively, you can start XSDK from the command line: xsdk -workspace sw -hwspec design_1_wrapper.hdf XSDK starts, and automatically runs a sequence to create a hardware wrapper project from the .hdf file: You finally end up with one project in the workspace, the hardware wrapper project: You then may create an application project based on a template: You can select the OS: standalone, linux or freertos. We pick standalone: And you select one of the template projects: XSDK creates the application project and a BSP project for it. The BSP (Board Support Package) project has Xilinx provided source code for the peripherals that are included in the hardware. You end up with three projects: The application project (\"fred\" above) has your code. It is linked with a library produced from the BSP project. The BSP project in turn depends on the hardware wrapper project which was created based on the .hdf file. Source control for XSDK Here is one way to structure your Xilinx SDK project so that it compatible with version control. The goal here is to check in the minimum number of files so that an XSDK software project can be recreated in a new repository. Only files that we create should be checked in, not derived files. We want the project to work through the XSDK Eclipse GUI to enhance the convenience of certain tasks: XSDK GUI allows you to casually create new software projects. For example you may need to create the Xilinx Zynq memory test project when your board comes in to validate the DRAM or you may want to create a standalone project instead of full Linux for board bringup tasks. XSDK GUI allows you to connect to the target and debug without using any other tools. On the other hand, for larger software projects and Linux you will almost certainly want to use scripting. You should include in your Linux build script the steps needed to create the device tree and the FSBL (first stage bootloader) from the .hdf file. Here is a quick summary of what we are going to do: Check in only the application project source code plus the Eclipse project files for it. Leave out the BSP project and hardware wrapper project. On a fresh clone, you need to rebuild the BSP project that the application project references. Mainly this means that you have to remember to use the name it expects (usually \"fred_bsp\" for an application called \"fred\"). Alternatively, you could include the BSP project in the repository. You may want this if you customize it in any way. In this case, you will have to import both the BSP and the application project. Assuming you do not put the BSP in the respository, the directory structure will look like this: quicktest1/ rebuild.tcl - TCL script created by Vivado. Checked in. rtl/ - Your Verilog (or VHDL) source code. All checked in. cons/ - Your constraint files. All checked in. project_1/ - Vivado project. Nothing here is checked in. ip/ - Xilinx IP. Some files checked in. design_1_wrapper.hdf - The exported hardware. sw/ - Becomes the XSDK / Eclipse workspace sw/fred/ - The application project sw/fred/src/ - Source code for the application project. All checked in. sw/fred/.project - Eclipse project file. Checked in. sw/fred/.cproject - Eclipse project file. Checked in. sw/fred/Debug/ - Application binary. Not checked in. sw/design_1_wrapper_hw_platform_0 - Hardware wrapper project derived from .hdf file. Not checked in. sw/fred_bsp/ - BSP project derived from wrapper. Not checked in. sw/.metadata - Eclipse workspace crap. Not checked in. It is important to understand some issues with XSDK: When you start it with the -hwspec option (as it is when you \"Launch SDK\" from Vivado), there is a very high chance that it will import the hdf as a new design wrapper project instead of updating your existing one. But sometimes it does what you want, and just updates the existing wrapper project- it works correctly if you leave XSDK running when you overwrite the .hdf file with Vivado, or even if XSDK is not running but there are no path changes. It certainly will not work if you had the wrapper project checked in and you start XSDK after a fresh clone. Bottom line is that it is pointless to check in the wrapper project. Then if you do end up with multiple design wrapper projects, it will be unclear which one is being referenced by the BSP project. The \"system.mss\" within the BSP shows it, but will you remember to look? Chances are high that you will compile the code with the old hardware. Related things are broken. For example, you can right-click on an applicaiton project for \"Change Referenced BSP\", but it's broken- the window comes up blank. This implies that you do not want to end up with multiple BSPs. Anyway, you should only have a single hardware wrapper project and a single BSP in your workspace to keep things clear. If you do end up with multiple wrapper projects, you should delete the ones you don't want by selected them and hitting the Delete key. Then review the inter-project references by right-clicking on the BSP project, click Properties and click on Project References: Check the correct one. Then clean and rebuild all. Note that even this window is broken- none of the wrappers were checked to begin with. Rebuild steps Luckily, the application project references the BSP project with a relative path, so you can move the application project as long as the BSP project is created in the same relative position. Also, Eclipse allows you to import projects which are already physically in the workspace. So it works for Eclipse to start a new workspace with your application source code already there. Suppose we have a repository structured as above. After a fresh clone, you should follow these steps: git clone https://github.com/jhallen/quicktest1.git cd quicktest1 xsdk -workspace sw -hwspec design_1_wrapper.hdf When it launches, you will only have the hardware wrapper project (no fred_bsp and no fred even though the fred/ directory is already in the workspace directory): Create the BSP: Set the name to what the application project expects: Select the same BSP options. If any of these are selected, you are better off checking in the BSP. Now we have the BSP project recreated: Next we import the application project. It's already in the workspace directory, but XSDK doesn't know about it yet. Select the current workspace directory to search for projects: Select your application project (\"fred\" in this case) that was checked out with git: Now we have all three required projects: From here you can rebuild the application. "],
        "story_type":"Normal",
        "url_raw":"https://github.com/jhallen/vivado_setup",
        "comments.comment_id":[21570164,
          21571190],
        "comments.comment_author":["cmrdporcupine",
          "tcherasaro"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-11-19T02:12:56Z",
          "2019-11-19T07:15:28Z"],
        "comments.comment_text":["Use fusesoc. <a href=\"https://github.com/olofk/fusesoc\" rel=\"nofollow\">https://github.com/olofk/fusesoc</a><p>Use your own editor and source control tools outside of Xilinx's 1990s straightjacket. It creates Vivado projects and invokes it to do synthesis.  You can open the generated projects in Vivado if you really need their GUI.",
          "FPGA Pro here.  I checked out the README.md to see if I could pick up any tips.  It is a good guide for a first time through, but I recommend also reviewing the Xilix documentation on this.<p>There's Xilinx App Note 1165 [Circa 2013] which is getting long in the tooth but still valid:\n<a href=\"https://www.xilinx.com/support/documentation/application_notes/xapp1165.pdf\" rel=\"nofollow\">https://www.xilinx.com/support/documentation/application_not...</a><p>And, Chapter 5 in Xilinx User Guide 892 is \"Source Management and Revision Control Recommendations\"\n<a href=\"https://www.xilinx.com/support/documentation/sw_manuals/xilinx2019_1/ug892-vivado-design-flows-overview.pdf\" rel=\"nofollow\">https://www.xilinx.com/support/documentation/sw_manuals/xili...</a><p>There is also a .gitignore example which can be helpful but is not geared towards a minimal file set without alot of tweaking:\n<a href=\"https://www.xilinx.com/support/answers/61232.html\" rel=\"nofollow\">https://www.xilinx.com/support/answers/61232.html</a><p>A few words of warning here:\nThe documentation recommends using the write_project_tcl and write_bd_tcl scripts.  I have experienced problems with both of these not faithfully recreating the .xpr and .bd files so exercise caution with them.  A better approach is to avoid .xpr files and to use a non-project or scripted flow.  If you have to use a Vivado project for some reason (I can think of a couple) then create and maintain tcl script to generate the project by hand.  One can use the write_project_tcl to get started and then update it by copying the tcl generated by Vivado after each GUI operation out of the Vivado tcl console into the project generation script when making changes to the project one wants to keep.<p>I have had the most problems with write_bd_tcl so I am now experimenting with just checking in the .bd file and setting the .bd extension to \"binary\" and various merge strategies in the .gitattributes file after several projects worth of frustration with the write_xx_tcl nonsense.<p>Finally, take all other Xilinx recommendation of source files to check in with a grain of salt.  The documentation recommends checking in way too much.  I recommend sticking with the \"Minimum Set of Source Files to Manage\" with archival backups of all output products for critical tags (eg. releases) somewhere outside the VCS."],
        "id":"4c46558e-74b0-4cdb-8980-586b9adf10be",
        "url_text":"Xilinx Vivado setup for source control This was tested on Vivado version 2019.1 Contents Project setup Build steps Save the script Xilinx IP IP Integrator / Block design XSDK XSDK review Source control for XSDK Rebuild steps Project setup Here is one way to structure your FPGA project so that it is compatible with both Xilinx Vivado GUI in project mode and source control. This setup allows you to check in the minimum number of files needed so that you can easily recreate the project in another sandbox. To save you some time: Once you have a Vivado project, you can not move it to a different directory. The project files are filled with absolute paths. It is therefore pointless to check in the entire project. There is a TCL command \"write_project_tcl\" which generates a TCL script which rebuilds the project in a new directory. This method is shown here. Even with \"write_project_tcl\", the project has to be structured correctly or the script will not work. In particular, none of the files needed to recreate the project should be in the project subdirectory. The TCL file could be used as a starting point for your own build script. However, it may be better to not modify it at all. Instead, repeatedly execute \"write_project_tcl\" throughout development to pick up the latest project changes. Although it's possible to script the entire FPGA build process, it is also important to be able to use project mode to access the Vivado GUI for such tasks as debugging with the integrated logic analyzer and pin-planning. If your setup is incorrect, you will waste a lot of time fixing it, or create a mess in your repository by checking in too many files. Unfortunately, Vivado really would like your source code to be in the project directory, so you have to actively fight it to prevent this. The directory structure should be this: example_fpga/ rebuild.tcl - TCL script created by Vivado. Checked in. rtl/ - Your Verilog (or VHDL) source code. All checked in. cons/ - Your constraint files. All checked in. project_1/ - Vivado project. Nothing here is checked in. ip/ - Xilinx IP. Some files checked in. The idea is to have the minimal set of files checked in so that you get the Vivado project back after a fresh clone. You should only have to type these commands: git clone https://github.com/jhallen/example_fpga.git cd example_fpga vivado -source rebuild.tcl Also, you can always delete project_1/ and rebuild it. rm -rf project_1 vivado -source rebuild.tcl The ip/ directory gets polluted with derived files, but at least Vivado tells you which ones need to be saved in source control. If the correct files are checked in, you can delete all derived files and rebuild: rm -rf project_1 rm -rf ip - Delete all Xilinx IP files (only after you've checked in the source) git checkout ip - Restore the actual source files vivado -source rebuild.tcl Build steps Suppose you don't have the project_1/ and rebuild.tcl script. This is how to create them. Start Vivado. Change directory to example_fpga first. Go through the usual sequence of creating a project, but do not check any of the boxes that say \"copy into project\": You need your Verilog or VHDL source files: You need the .xci files for Xilinx \"IP Catalog\" IP. You also need the wrapper file for block designs (see below for more about this). Do not check the box here: You need your constraint files: Don't check it here either! Then we wait a long time, why is Vivado so slow? Now finally the project appears: Save the script Use the write_project_tcl command to save the script. You will likely need to add the \"-force\" option to allow it overwrite the previous version of the script: Here are the messages this command prints: That's it. Now you can regenerate the project with: rm -rf project_1 vivado -source rebuild.tcl Note that the rebuild.tcl script shows you the files that need to be saved in source control in comments. This is in the file: # 3. The following remote source files that were added to the original project:- # # \"/home/jallen/example_fpga/ip/clk_wiz_0/clk_wiz_0.xci\" # \"/home/jallen/example_fpga/rtl/testtop.v\" # \"/home/jallen/example_fpga/cons/mycons.xdc\" You will have big problems if any of these are in the project directory (example_fpga/project_1 in this case). It does not work to delete the project_1/ directory, and sneakily write the files back. First the rebuild.tcl script will fail in the line with create_project because the directory already exists. You can try to get around this by adding -force the line with create_project. Unfortunately, create_project -force deletes the entire directory, so then Vivado will complain that the files are missing. You really need to create the source files outside of the project directory in the first place. Xilinx IP The next difficulty is that Xilinx IP from the \"IP Catalog\" is written by default to the project directory. An example is the clock wizard IP to use a PLL or MMCM. Take note of the \"IP Location\" in the top left of the window. It should just show you the location, but the GUI design is bad here so have to click on it. When you click on it, you will see that it wants to put it in the project: Instead put it in the ip/ directory. You need to generete the output products, even though they do not have to be saved in git. When you generate them here, then Vivado knows to regenerate them when you rebuild the project. Just the .xci file needs to be saved in source control, but it's a good idea to run the write_project_tcl command and check the comments to be sure. Unfortunately the output products are generated in the same tree as the .xci files, so you have to pay close attention to the ip/ directory. You should know that there is a setting to override the default \"IP Location\". It's here: Seems good right? But it isn't. The problem is that this is a system-wide setting, not a per-project setting. This default does not end up in the project tcl file, and if you change it, it will apply to all of your projects. You could end up building IP from one project in another project's IP directory. I found that this setting is stored here: ~/.Xilinx find . -type f | xargs grep newip ./Vivado/2019.1/vivado.xml: <recent index=\"0\" path=\"/home/jallen/quicktest1/newip\"/> ./Vivado/2019.1/vivado.xml: <CUSTOMIZED_IP_DEFAULT_LOCATION value=\"/home/jallen/quicktest1/newip\"/> My recommendation for this setting is to change the path to something you don't have permission to write to, perhaps \"/root\". This way, if you attempt to save IP to the default location, it will fail and inform you. IP Integrator / Block design Block designs must also be created outside of the project directory (for example, in our ip/ directory). This is odd, because the block design ends up as a set of commands in the project tcl script produced by write_project_tcl. The problem is that the HDL wrapper file is not produced by the project tcl, but must exist for this script to work without failing. When block designs are produced in ip/, many files are generated, but only the wrapper file has to be checked into the source control. This file is in the list in the comments of the project tcl script: # 3. The following remote source files that were added to the original project:- # # \"/home/jallen/tryit/ip/design_1/hdl/design_1_wrapper.v\" # \"/home/jallen/tryit/cons/mycons.xdc\" Anyway, when you create a block design, the default is for it to be produced inside the project: Change this to ip/: Now we create the block design: Populate the block design canvas: Right click on the block design and create the HDL wrapper: That's it. Now you have the files necessary for checking into source control so you can use the write_project_tcl command. Xilinx SDK / Eclipse Review XSDK allows you to develop software for the system you create on the FPGA. Let's review a little about how XSDK works. First thing you must do is export the hardware .hdf file from Vivado: As usual, Vivado will want to put it in the project: You should save it somewhere else, perhaps at the top level since this is the one file that the software team will care about: The .hdf file is a .zip file containing the list of peripherals in your design, the address map and possibly the .bit file- basically everything the software needs to know about the hardware: ~/quicktest1 unzip -v design_2_wrapper.hdf Archive: design_2_wrapper.hdf Length Method Size Cmpr Date Time CRC-32 Name -------- ------ ------- ---- ---------- ----- -------- ---- 620 Defl:N 353 43% 2019-11-18 16:34 619e2f0a sysdef.xml 261182 Defl:S 24932 91% 2019-11-18 16:34 0959ea2b design_1.hwh 15855 Defl:S 4021 75% 2019-11-18 16:34 00750531 design_1_bd.tcl 1241916 Defl:S 58412 95% 2019-11-18 16:34 de68834d design_1_wrapper.bit 1436 Defl:S 629 56% 2019-11-18 16:34 014b194a design_1_wrapper.mmi -------- ------- --- ------- 1521009 88347 94% 5 files Now we start XSDK, either through Vivado: Of course, it assumes you want the files in the project: You should put the workspace directory (where XSDK will put the software projects you create) outside of the Vivado project. The \"Exported Location\" is the directory where the .hdf file was placed: Alternatively, you can start XSDK from the command line: xsdk -workspace sw -hwspec design_1_wrapper.hdf XSDK starts, and automatically runs a sequence to create a hardware wrapper project from the .hdf file: You finally end up with one project in the workspace, the hardware wrapper project: You then may create an application project based on a template: You can select the OS: standalone, linux or freertos. We pick standalone: And you select one of the template projects: XSDK creates the application project and a BSP project for it. The BSP (Board Support Package) project has Xilinx provided source code for the peripherals that are included in the hardware. You end up with three projects: The application project (\"fred\" above) has your code. It is linked with a library produced from the BSP project. The BSP project in turn depends on the hardware wrapper project which was created based on the .hdf file. Source control for XSDK Here is one way to structure your Xilinx SDK project so that it compatible with version control. The goal here is to check in the minimum number of files so that an XSDK software project can be recreated in a new repository. Only files that we create should be checked in, not derived files. We want the project to work through the XSDK Eclipse GUI to enhance the convenience of certain tasks: XSDK GUI allows you to casually create new software projects. For example you may need to create the Xilinx Zynq memory test project when your board comes in to validate the DRAM or you may want to create a standalone project instead of full Linux for board bringup tasks. XSDK GUI allows you to connect to the target and debug without using any other tools. On the other hand, for larger software projects and Linux you will almost certainly want to use scripting. You should include in your Linux build script the steps needed to create the device tree and the FSBL (first stage bootloader) from the .hdf file. Here is a quick summary of what we are going to do: Check in only the application project source code plus the Eclipse project files for it. Leave out the BSP project and hardware wrapper project. On a fresh clone, you need to rebuild the BSP project that the application project references. Mainly this means that you have to remember to use the name it expects (usually \"fred_bsp\" for an application called \"fred\"). Alternatively, you could include the BSP project in the repository. You may want this if you customize it in any way. In this case, you will have to import both the BSP and the application project. Assuming you do not put the BSP in the respository, the directory structure will look like this: quicktest1/ rebuild.tcl - TCL script created by Vivado. Checked in. rtl/ - Your Verilog (or VHDL) source code. All checked in. cons/ - Your constraint files. All checked in. project_1/ - Vivado project. Nothing here is checked in. ip/ - Xilinx IP. Some files checked in. design_1_wrapper.hdf - The exported hardware. sw/ - Becomes the XSDK / Eclipse workspace sw/fred/ - The application project sw/fred/src/ - Source code for the application project. All checked in. sw/fred/.project - Eclipse project file. Checked in. sw/fred/.cproject - Eclipse project file. Checked in. sw/fred/Debug/ - Application binary. Not checked in. sw/design_1_wrapper_hw_platform_0 - Hardware wrapper project derived from .hdf file. Not checked in. sw/fred_bsp/ - BSP project derived from wrapper. Not checked in. sw/.metadata - Eclipse workspace crap. Not checked in. It is important to understand some issues with XSDK: When you start it with the -hwspec option (as it is when you \"Launch SDK\" from Vivado), there is a very high chance that it will import the hdf as a new design wrapper project instead of updating your existing one. But sometimes it does what you want, and just updates the existing wrapper project- it works correctly if you leave XSDK running when you overwrite the .hdf file with Vivado, or even if XSDK is not running but there are no path changes. It certainly will not work if you had the wrapper project checked in and you start XSDK after a fresh clone. Bottom line is that it is pointless to check in the wrapper project. Then if you do end up with multiple design wrapper projects, it will be unclear which one is being referenced by the BSP project. The \"system.mss\" within the BSP shows it, but will you remember to look? Chances are high that you will compile the code with the old hardware. Related things are broken. For example, you can right-click on an applicaiton project for \"Change Referenced BSP\", but it's broken- the window comes up blank. This implies that you do not want to end up with multiple BSPs. Anyway, you should only have a single hardware wrapper project and a single BSP in your workspace to keep things clear. If you do end up with multiple wrapper projects, you should delete the ones you don't want by selected them and hitting the Delete key. Then review the inter-project references by right-clicking on the BSP project, click Properties and click on Project References: Check the correct one. Then clean and rebuild all. Note that even this window is broken- none of the wrappers were checked to begin with. Rebuild steps Luckily, the application project references the BSP project with a relative path, so you can move the application project as long as the BSP project is created in the same relative position. Also, Eclipse allows you to import projects which are already physically in the workspace. So it works for Eclipse to start a new workspace with your application source code already there. Suppose we have a repository structured as above. After a fresh clone, you should follow these steps: git clone https://github.com/jhallen/quicktest1.git cd quicktest1 xsdk -workspace sw -hwspec design_1_wrapper.hdf When it launches, you will only have the hardware wrapper project (no fred_bsp and no fred even though the fred/ directory is already in the workspace directory): Create the BSP: Set the name to what the application project expects: Select the same BSP options. If any of these are selected, you are better off checking in the BSP. Now we have the BSP project recreated: Next we import the application project. It's already in the workspace directory, but XSDK doesn't know about it yet. Select the current workspace directory to search for projects: Select your application project (\"fred\" in this case) that was checked out with git: Now we have all three required projects: From here you can rebuild the application. ",
        "_version_":1718441073607442432},
      {
        "story_id":18946601,
        "story_author":"bobowzki",
        "story_descendants":154,
        "story_score":429,
        "story_time":"2019-01-19T12:05:27Z",
        "story_title":"B612 is a highly legible open source font to be used on aircraft cockpit screens",
        "search":["B612 is a highly legible open source font to be used on aircraft cockpit screens",
          "http://b612-font.com",
          "The genesis of B612 In 2010, Airbus initiated a research collaboration with ENAC and Universit de Toulouse III on a prospective study to define and validate an Aeronautical Font: the challenge was to improve the display of information on the cockpit screens, in particular in terms of legibility and comfort of reading, and to optimize the overall homogeneity of the cockpit. 2 years later, Airbus came to find Intactile DESIGN to work on the design of the eight typographic variants of the font. This one, baptized B612 in reference to the imaginary asteroid of the aviator SaintExupry, benefited from a complete hinting on all the characters. Read the full story B612 B612 Mono Glyphset ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789abcdefghijklmnopqrstuvwxyz!&()*?@[#]{|}~ License In 2017, Airbus agreed to publish the font with an open source license (Eclipse Public License) within the Polarsys project, an industry oriented project hosted by the Eclipse foundation. In December 2018, B612 has been published on Google Fonts with an open source license (OFL) and the source has been put on Github. B612 on Google Font B612 Github repository More on OFL License Authors B612 is the result of a research project initiated by Airbus. The font was designed by Nicolas Chauveau, Thomas Paillot and Jonathan Favre-Lamarine (intactile DESIGN) with the support of JeanLuc Vinot (ENAC). Prior research by JeanLuc Vinot (DGAC/DSNA) and Sylvie Athnes (Universit de Toulouse III). "],
        "story_type":"Normal",
        "url_raw":"http://b612-font.com",
        "url_text":"The genesis of B612 In 2010, Airbus initiated a research collaboration with ENAC and Universit de Toulouse III on a prospective study to define and validate an Aeronautical Font: the challenge was to improve the display of information on the cockpit screens, in particular in terms of legibility and comfort of reading, and to optimize the overall homogeneity of the cockpit. 2 years later, Airbus came to find Intactile DESIGN to work on the design of the eight typographic variants of the font. This one, baptized B612 in reference to the imaginary asteroid of the aviator SaintExupry, benefited from a complete hinting on all the characters. Read the full story B612 B612 Mono Glyphset ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789abcdefghijklmnopqrstuvwxyz!&()*?@[#]{|}~ License In 2017, Airbus agreed to publish the font with an open source license (Eclipse Public License) within the Polarsys project, an industry oriented project hosted by the Eclipse foundation. In December 2018, B612 has been published on Google Fonts with an open source license (OFL) and the source has been put on Github. B612 on Google Font B612 Github repository More on OFL License Authors B612 is the result of a research project initiated by Airbus. The font was designed by Nicolas Chauveau, Thomas Paillot and Jonathan Favre-Lamarine (intactile DESIGN) with the support of JeanLuc Vinot (ENAC). Prior research by JeanLuc Vinot (DGAC/DSNA) and Sylvie Athnes (Universit de Toulouse III). ",
        "comments.comment_id":[18956114,
          18956511],
        "comments.comment_author":["oakwhiz",
          "jmwilson"],
        "comments.comment_descendants":[9,
          5],
        "comments.comment_time":["2019-01-20T22:47:46Z",
          "2019-01-21T00:12:10Z"],
        "comments.comment_text":["I'm surprised they didn't cross or dot the zero. Surely differentiating O from 0 is important when eye fatigue is a problem?",
          "Their own legibility study shows it's about around the same as Verdana: <a href=\"https://i.imgur.com/M0C3TkY.png\" rel=\"nofollow\">https://i.imgur.com/M0C3TkY.png</a> (excerpt from the PDF in the download .zip, vertical axis is misidentifications). Their design goal was beating their current font (CDS), which seems a bit of a sandbagged goal if you look at the sample in the report. I wonder if we've reached a plateau of legibility for technical fonts after identifying the easy wins in typeface design for screens."],
        "id":"328efbe9-8504-4e1f-8ddd-633f21ccb47f",
        "_version_":1718441007160229888},
      {
        "story_id":19069526,
        "story_author":"ktr",
        "story_descendants":134,
        "story_score":149,
        "story_time":"2019-02-03T13:48:05Z",
        "story_title":"Stack Overflow: Helping One Million Developers Exit Vim (2017)",
        "search":["Stack Overflow: Helping One Million Developers Exit Vim (2017)",
          "https://stackoverflow.blog/2017/05/23/stack-overflow-helping-one-million-developers-exit-vim/",
          "This morning, a popular Stack Overflow question hit a major milestone: Youre not alone, jclancy. In the five years since this question was asked, there have been over a million other developers who got stuck in Vim and couldnt escape without a bit of help. Indeed, the difficulty of quitting the Vim editor is a common joke among developers. Ive been told by experienced Vim users that this reputation is unfair, and Im sure theyre right (even Ive gotten the hang of it in the last few years). I think there are two reasons its easy to forget how to exit Vim: developers are often dropped into Vim from a git command or another situation where they didnt expect to be, and they run into it infrequently enough to forget how they solved it last time. In honor of this milestone, we decided to take a look at the data surrounding this question. In particular, well try measuring who is most likely to get stuck in Vim as opposed to using it intentionally, and examining how that balance varies by country and by programming language. How many people have been struggling to exit Vim? In the last year, How to exit the Vim editor has made up about .005% of question traffic: that is, one out of every 20,000 visits to Stack Overflow questions. That means during peak traffic hours on weekdays, there are about 80 people per hour that need help getting out of Vim. Has the percentage of traffic it makes up changed over time? That is, have developers started learning to exit Vim on their own? It doesnt look like it. The question was asked in August 2012, and for a few months it got very little traffic. Then it began growing in the following two years, presumably as more sources linked to it online and it moved to the top of search engine results. Its been relatively steady for the last two years. This doesnt necessarily mean the same people visited it again and again, of course; it could represent relatively new programmers getting stuck in Vim for the first time. Differences across countries As we saw in a previous blog post, we can use Stack Overflow traffic to learn a lot about the geographical distribution of developers. Lets consider what percentage of visits to Vim this question comprises within each country. In countries with a lot of experienced Vim users, wed expect this percentage to be low. When its high, it indicates many users got stuck in Vim when they didnt necessarily expect to. It looks like developers in Ukraine, Turkey and Indonesia are getting stuck in Vim quite a bit: it makes up a larger portion of their Vim questions than in any other country. In contrast, in China, Korea and Japan the fraction going to this question is one-tenth as much. That might indicate that when developers in these countries enter Vim, they usually meant to do so, and they know how to get out of it. What kind of programmers get stuck in Vim? Its also likely that users of different programming languages will have different experiences with Vim. We can investigate this by stratifying the Exit Vim / Total Vim percentage across each users main programming technology. Well define this based on what Stack Overflow tag they visit most often. (For instance, my most visited tag is R: it makes up 52% of my question views). Its not a perfect measure, but its reliable enough to give a sense of the breakdown by language. (For this analysis, we considered only registered users with at least 100 visits to the site). The developers who are most likely to get stuck in Vim are front-end web developers: those who primarily visit tags like JQuery, CSS, and AngularJS. Theyre followed by Microsoft developers (C# and SQL Server) and mobile (Android and iOS). These developers usually work with an IDE (Visual Studio, Eclipse, Xcode, and so on), rather than a plain text editor, so it makes sense that theyre relatively more likely to get stuck in Vim rather than to open it intentionally. The developers least likely to get stuck in Vim are those who program in C, C++, Python and Ruby. These languages make sense to me: they are a combination of low-level languages and scripting languages that are often used with a plain text editor rather than an IDE, so they have the experience to escape it without a Google search. Conclusion I was amused when I saw this question approach a million visits, but I was also proud that I work for, and contribute answers to, a site that helps so many developers. You never know when an answer you contribute could help millions of people, whether it shares how to undo a git commit or how the yield keyword in Python works. If you want to contribute yourself, we encourage you to join the worlds largest developer community, whether its to ask and answer questions, get your next job, or build your online presence with a Developer Story. You can also use tools like Stack Overflow Trends to learn more about what our data can tell you about software developers. In any case, next time you solve your problem through Stack Overflow, remember the hundreds of thousands of users who regularly ask, answer, edit, and moderate the site to make it all possible. "],
        "story_type":"Normal",
        "url_raw":"https://stackoverflow.blog/2017/05/23/stack-overflow-helping-one-million-developers-exit-vim/",
        "comments.comment_id":[19069729,
          19069855],
        "comments.comment_author":["alexpetralia",
          "Datenstrom"],
        "comments.comment_descendants":[2,
          3],
        "comments.comment_time":["2019-02-03T14:34:37Z",
          "2019-02-03T14:55:52Z"],
        "comments.comment_text":["This is probably the #1 problem with Unix-based systems in general for new users. There are so many ways to exit a command prompt depending on the application!<p>Off the top of my head: Ctrl+D, q, :q, exit, quit(), Ctrl+C, Ctrl+X.<p>Countless times have I been locked in an application and needed to cycle through those memorized commands to escape. Imagine doing that as a newbie.",
          "Whenever I end up on another computer without my environment setup and end up in nano because it is the default, it always takes me a while. There is the cheat sheet at the bottom but as soon as the text appears I'll start manipulating it using vim keybindings and delete things and whatever else randomly. I always assume my fingers were not on the homerow correctly or something first. It is very disorienting.<p>So I imagine it is far worse for someone who ends up in Vim the same way."],
        "id":"92f00d21-8daf-4002-8cd6-2a9cdb52e450",
        "url_text":"This morning, a popular Stack Overflow question hit a major milestone: Youre not alone, jclancy. In the five years since this question was asked, there have been over a million other developers who got stuck in Vim and couldnt escape without a bit of help. Indeed, the difficulty of quitting the Vim editor is a common joke among developers. Ive been told by experienced Vim users that this reputation is unfair, and Im sure theyre right (even Ive gotten the hang of it in the last few years). I think there are two reasons its easy to forget how to exit Vim: developers are often dropped into Vim from a git command or another situation where they didnt expect to be, and they run into it infrequently enough to forget how they solved it last time. In honor of this milestone, we decided to take a look at the data surrounding this question. In particular, well try measuring who is most likely to get stuck in Vim as opposed to using it intentionally, and examining how that balance varies by country and by programming language. How many people have been struggling to exit Vim? In the last year, How to exit the Vim editor has made up about .005% of question traffic: that is, one out of every 20,000 visits to Stack Overflow questions. That means during peak traffic hours on weekdays, there are about 80 people per hour that need help getting out of Vim. Has the percentage of traffic it makes up changed over time? That is, have developers started learning to exit Vim on their own? It doesnt look like it. The question was asked in August 2012, and for a few months it got very little traffic. Then it began growing in the following two years, presumably as more sources linked to it online and it moved to the top of search engine results. Its been relatively steady for the last two years. This doesnt necessarily mean the same people visited it again and again, of course; it could represent relatively new programmers getting stuck in Vim for the first time. Differences across countries As we saw in a previous blog post, we can use Stack Overflow traffic to learn a lot about the geographical distribution of developers. Lets consider what percentage of visits to Vim this question comprises within each country. In countries with a lot of experienced Vim users, wed expect this percentage to be low. When its high, it indicates many users got stuck in Vim when they didnt necessarily expect to. It looks like developers in Ukraine, Turkey and Indonesia are getting stuck in Vim quite a bit: it makes up a larger portion of their Vim questions than in any other country. In contrast, in China, Korea and Japan the fraction going to this question is one-tenth as much. That might indicate that when developers in these countries enter Vim, they usually meant to do so, and they know how to get out of it. What kind of programmers get stuck in Vim? Its also likely that users of different programming languages will have different experiences with Vim. We can investigate this by stratifying the Exit Vim / Total Vim percentage across each users main programming technology. Well define this based on what Stack Overflow tag they visit most often. (For instance, my most visited tag is R: it makes up 52% of my question views). Its not a perfect measure, but its reliable enough to give a sense of the breakdown by language. (For this analysis, we considered only registered users with at least 100 visits to the site). The developers who are most likely to get stuck in Vim are front-end web developers: those who primarily visit tags like JQuery, CSS, and AngularJS. Theyre followed by Microsoft developers (C# and SQL Server) and mobile (Android and iOS). These developers usually work with an IDE (Visual Studio, Eclipse, Xcode, and so on), rather than a plain text editor, so it makes sense that theyre relatively more likely to get stuck in Vim rather than to open it intentionally. The developers least likely to get stuck in Vim are those who program in C, C++, Python and Ruby. These languages make sense to me: they are a combination of low-level languages and scripting languages that are often used with a plain text editor rather than an IDE, so they have the experience to escape it without a Google search. Conclusion I was amused when I saw this question approach a million visits, but I was also proud that I work for, and contribute answers to, a site that helps so many developers. You never know when an answer you contribute could help millions of people, whether it shares how to undo a git commit or how the yield keyword in Python works. If you want to contribute yourself, we encourage you to join the worlds largest developer community, whether its to ask and answer questions, get your next job, or build your online presence with a Developer Story. You can also use tools like Stack Overflow Trends to learn more about what our data can tell you about software developers. In any case, next time you solve your problem through Stack Overflow, remember the hundreds of thousands of users who regularly ask, answer, edit, and moderate the site to make it all possible. ",
        "_version_":1718441009940004865},
      {
        "story_id":21744863,
        "story_author":"cmi_cloud",
        "story_descendants":4,
        "story_score":28,
        "story_time":"2019-12-09T18:10:38Z",
        "story_title":"Top key takeaways and announcements from KubeCon and CloudNative Con 2019",
        "search":["Top key takeaways and announcements from KubeCon and CloudNative Con 2019",
          "https://www.cloudmanagementinsider.com/key-takeaways-announcements-kubecon-cloudnative-con-2019-san-diego/",
          "Cloud Native Computing Foundation (CNCF) hosted KubeCon, one of the biggest cloud events of 2019, in San Diego. CNCF came into existence with the motive to build sustainable ecosystems and foster communities to support the growth and health of cloud-native open-source software. The foundation governs many of the fastest-growing projects. The most prominent ones are Kubernetes (everybodys heard of this), Prometheus, and Envoy. KubeCon is growing bigger KubeCon has positioned itself as the most exclusive event for open source communities embracing cloud-native possibilities. Companies utilize KubeCon to announce all the new products and features. The event saw a rise in the percentage of attendees by 50%. This year the total announcements also crossed the mark of 100, so many things to highlight. We covered the live updates of the KubeCon + CloudNative Con. Here we are bringing to you the top 6 announcements, significant updates, and key takeaways from KubeCon + CloudNative Con 2019 San Diego. 1. Launch ofHelm 3 CNCF governs Helm as an incubating project. It is one of the most popular package managers for Kubernetes. Well,the announcement for Helm 3was made on their official blog on Nov. 13. To present it to the cloud-native community, Helm organized two maintainers track sessions, during KubeCon, focused on Introduction to Helm and a Helm 3 Deep Dive (for which everyone was waiting). The notable Helm 3 announcements that got attention during KubeCon and also mentionedon their Github accountare: Releases are stored in a new format There is no in-cluster (Tiller) component Helm 3 includes support for a new version of Helm charts (Charts v2) Helm 3 also supports library charts charts that are used primarily as a resource for other charts. Experimental support for storing Helm charts in OCI registries (e.g.,Docker Distribution) is available for testing. A 3-way strategic merge patch is now applied when upgrading Kubernetes resources. A charts supplied values can now be validated against a JSON schema A number of small improvements have been made to make Helm more secure, usable, and robust. 2. AWS + Intuit + Weaveworks + Argo Flux Gitops Engine is a collaborated project by AWS, Intuit, and WeaveWorks on Argo Flux. The idea for GitOps Engine came into existence when two of the most significant GitOps projects joined forces, i.e., Argo CD and Flux CD, which became Argo Flux. GitOps Engine will be responsible for access to Git repositories, Kubernetes resource cache manifest generation, resources reconciliation, and sync planning. Argo Flux is taking the GitOps engines development at the next level by collaborating with Intuit and WeaveWorks. These efforts will lead to a unified continuous deployment tool based on GitOps. AWS is also integrating GitOps in EKS and Flagger for AWS App Mesh. Suggested Read:StackRox Survey reveals security as the main concern for Kubernetes 3. Confidential Computing for Kubernetes from Microsoft Azure became thefirst major cloud platform to support confidential computingsupported by Intel Software Guard Extensions (Intel SGX). The confidential computing capabilities provide an additional layer of protection from malicious insiders at a cloud providers end. It will reduce the chances of data leaks and helpful in addressing compliance needs. Microsoft announced to made confidential computing available for Kubernetes to serve a broad set of users. Now, Kubernetes users can schedule pods and containers that are based on the Open Enclave SDK along with hardware that supports Trusted Execution Environments (TEE). The scheduled pods in the clusters can then run containers using secure enclaves and take advantage of confidential computing. 4. Red Hat announced the CodeReady Workspaces 2.0 As the name suggests, Red Hats CodeReady Workspaces provides a Kubernetes-native development solution that delivers a workspace environment and in-browser IDE (based on Eclipse Che) for rapid cloud application development. It provides any member of the development or IT team with a consistent and secure development environment with zero-configuration required. CodeReady Workspace is available for Red Hat OpenShift and Enterprise Linux. With the latest release of CodeReady Workspaces, developers can create and build applications (or any other services) in an environment that mirrors that of production environments running on Red Hat OpeShift. In the most simple words, a developer can experience streamline the handling of development processes by integrating IDE with a production deployment environment. 5. Mirantis launches Kubernetes as a Service (KaaS) Last Week, Mirantis recently acquired Dockers enterprise business. Mirantis efforts to go all-in for containerization got wings with the announcement of continuously updated multi-cloud Kubernetes as a Service (KaaS). It is a pure-play K8s without API extensions, which ensures your apps can run on any cloud. Mirantis KaaS promises to be resilient, scalable, secure, easy to integrate and operate. Suggested Read:Containerization vs Virtualization | Everything you need to know 6. OReilly Acquires Katacoda Katacoda is an interactive technology platform that enables hands-on learning. Through Katacodas integration, OReilly users can master critical new technologies. It will enable tech professionals to learn in real environments with the actual tools used in production that, too, in their web browser. So, its a wrap from KubeCon + CloudNative Con 2019. If you find the information relevant, share it with your peers and colleagues. Subscribe to our blog to get all these articles directly in your inbox. Cloud EvangelistCloud Evangelists are CMI's in house ambassadors for the entire Cloud ecosystem. They are responsible for propagating the doctrine of cloud computing and help community members make informed decisions. "],
        "story_type":"Normal",
        "url_raw":"https://www.cloudmanagementinsider.com/key-takeaways-announcements-kubecon-cloudnative-con-2019-san-diego/",
        "comments.comment_id":[21745524,
          21746331],
        "comments.comment_author":["ntolia",
          "Aqua"],
        "comments.comment_descendants":[1,
          2],
        "comments.comment_time":["2019-12-09T19:26:15Z",
          "2019-12-09T20:43:33Z"],
        "comments.comment_text":["Apart from the announcements highlighted above, if you want a little bit more color and my take on the community, marketing vs. reality, and some of the tech-related controversy I saw, feel free to check out my notes on the conference here - <a href=\"https://blog.kasten.io/posts/our-top-highlights-from-kubecon-cloudnativecon-san-diego-2019/\" rel=\"nofollow\">https://blog.kasten.io/posts/our-top-highlights-from-kubecon...</a>",
          ">  Mirantis launches Kubernetes as a Service (KaaS)<p>How is this a <i>key</i> takeaway? There are dozens of Kubernetes as a service offerings, why is this one considered a key takeaway? This is a genuine question, not being sarcastic or cynical."],
        "id":"d49c2d02-beb6-49de-b4d6-f6b6ec9f71b3",
        "url_text":"Cloud Native Computing Foundation (CNCF) hosted KubeCon, one of the biggest cloud events of 2019, in San Diego. CNCF came into existence with the motive to build sustainable ecosystems and foster communities to support the growth and health of cloud-native open-source software. The foundation governs many of the fastest-growing projects. The most prominent ones are Kubernetes (everybodys heard of this), Prometheus, and Envoy. KubeCon is growing bigger KubeCon has positioned itself as the most exclusive event for open source communities embracing cloud-native possibilities. Companies utilize KubeCon to announce all the new products and features. The event saw a rise in the percentage of attendees by 50%. This year the total announcements also crossed the mark of 100, so many things to highlight. We covered the live updates of the KubeCon + CloudNative Con. Here we are bringing to you the top 6 announcements, significant updates, and key takeaways from KubeCon + CloudNative Con 2019 San Diego. 1. Launch ofHelm 3 CNCF governs Helm as an incubating project. It is one of the most popular package managers for Kubernetes. Well,the announcement for Helm 3was made on their official blog on Nov. 13. To present it to the cloud-native community, Helm organized two maintainers track sessions, during KubeCon, focused on Introduction to Helm and a Helm 3 Deep Dive (for which everyone was waiting). The notable Helm 3 announcements that got attention during KubeCon and also mentionedon their Github accountare: Releases are stored in a new format There is no in-cluster (Tiller) component Helm 3 includes support for a new version of Helm charts (Charts v2) Helm 3 also supports library charts charts that are used primarily as a resource for other charts. Experimental support for storing Helm charts in OCI registries (e.g.,Docker Distribution) is available for testing. A 3-way strategic merge patch is now applied when upgrading Kubernetes resources. A charts supplied values can now be validated against a JSON schema A number of small improvements have been made to make Helm more secure, usable, and robust. 2. AWS + Intuit + Weaveworks + Argo Flux Gitops Engine is a collaborated project by AWS, Intuit, and WeaveWorks on Argo Flux. The idea for GitOps Engine came into existence when two of the most significant GitOps projects joined forces, i.e., Argo CD and Flux CD, which became Argo Flux. GitOps Engine will be responsible for access to Git repositories, Kubernetes resource cache manifest generation, resources reconciliation, and sync planning. Argo Flux is taking the GitOps engines development at the next level by collaborating with Intuit and WeaveWorks. These efforts will lead to a unified continuous deployment tool based on GitOps. AWS is also integrating GitOps in EKS and Flagger for AWS App Mesh. Suggested Read:StackRox Survey reveals security as the main concern for Kubernetes 3. Confidential Computing for Kubernetes from Microsoft Azure became thefirst major cloud platform to support confidential computingsupported by Intel Software Guard Extensions (Intel SGX). The confidential computing capabilities provide an additional layer of protection from malicious insiders at a cloud providers end. It will reduce the chances of data leaks and helpful in addressing compliance needs. Microsoft announced to made confidential computing available for Kubernetes to serve a broad set of users. Now, Kubernetes users can schedule pods and containers that are based on the Open Enclave SDK along with hardware that supports Trusted Execution Environments (TEE). The scheduled pods in the clusters can then run containers using secure enclaves and take advantage of confidential computing. 4. Red Hat announced the CodeReady Workspaces 2.0 As the name suggests, Red Hats CodeReady Workspaces provides a Kubernetes-native development solution that delivers a workspace environment and in-browser IDE (based on Eclipse Che) for rapid cloud application development. It provides any member of the development or IT team with a consistent and secure development environment with zero-configuration required. CodeReady Workspace is available for Red Hat OpenShift and Enterprise Linux. With the latest release of CodeReady Workspaces, developers can create and build applications (or any other services) in an environment that mirrors that of production environments running on Red Hat OpeShift. In the most simple words, a developer can experience streamline the handling of development processes by integrating IDE with a production deployment environment. 5. Mirantis launches Kubernetes as a Service (KaaS) Last Week, Mirantis recently acquired Dockers enterprise business. Mirantis efforts to go all-in for containerization got wings with the announcement of continuously updated multi-cloud Kubernetes as a Service (KaaS). It is a pure-play K8s without API extensions, which ensures your apps can run on any cloud. Mirantis KaaS promises to be resilient, scalable, secure, easy to integrate and operate. Suggested Read:Containerization vs Virtualization | Everything you need to know 6. OReilly Acquires Katacoda Katacoda is an interactive technology platform that enables hands-on learning. Through Katacodas integration, OReilly users can master critical new technologies. It will enable tech professionals to learn in real environments with the actual tools used in production that, too, in their web browser. So, its a wrap from KubeCon + CloudNative Con 2019. If you find the information relevant, share it with your peers and colleagues. Subscribe to our blog to get all these articles directly in your inbox. Cloud EvangelistCloud Evangelists are CMI's in house ambassadors for the entire Cloud ecosystem. They are responsible for propagating the doctrine of cloud computing and help community members make informed decisions. ",
        "_version_":1718441078123659264},
      {
        "story_id":21305843,
        "story_author":"MilnerRoute",
        "story_descendants":5,
        "story_score":11,
        "story_time":"2019-10-20T18:05:45Z",
        "story_title":"Microsoft vs. IBM: A major shift in Java support",
        "search":["Microsoft vs. IBM: A major shift in Java support",
          "https://www.theserverside.com/opinion/Microsoft-vs-IBM-A-major-shift-in-Java-support",
          "Once an afterthought in the Java community, Microsoft has seemingly overtaken IBM as the preeminent advocate among developers at the Oracle Code One conference. SAN FRANCISCO -- There once was a time when IBM was arguably the most dominant force in the enterprise Java community. And yet at Oracle Code One 2019, signs all pointed to how Microsoft wants in, while IBM wants out -- a major shift in the Microsoft vs. IBM discussion. It was always IBM, after all, that invested heavily in Java development, while Microsoft didn't bother. As IBM pushes itself away from the Java table, Microsoft appears ready to take a seat. Microsoft vs. IBM: A reversal of roles IBM invented the Eclipse IDE. IBM pushed Fortune 500 clients onto WebSphere, which drove widespread adoption of server-side Java. And when Oracle bought Sun Microsystems a decade ago, it was IBM that put in a serious bid for Sun's technologies. If Oracle hadn't sweetened its offer, IBM likely would have become the steward of the Java language. But IBM's interest waned over the years, and the company has severely neglected its WebSphere user base, providing inadequate updates to the server, portal and web content management (WCM) tools. Even today, the WCM editor doesn't support multiple browser tabs. The web-based server administration UI blows up when you click the back button, and the portal configuration tool is severely outdated. Nobody in the industry was surprised when IBM sold its on-premises WebSphere offerings to HCL Industries earlier this year. From a user perspective, it feels like IBM gave up on WebSphere and sever-side Java a long time ago. Instead, Big Blue is focused on AI, the cloud and its bewildering assortment of Watson-branded tools. IBM certainly didn't present as strong a front at Oracle Code One 2019 as it did when the conference was called JavaOne, and that contrasts starkly with Microsoft. Microsoft vs. IBM: One lays inroads for Java developers, the other retracts Microsoft's .NET platform has always been a direct competitor to Java EE, and any tool that flew under Bill Gates' flag was ripe for criticism from the Java community -- no matter how technically sound it might be. Despite a position behind the 8-ball in terms of Java mindshare, Microsoft has done everything it can over the past 18 months to endear itself to the developer community. Microsoft became an AdoptOpenJDK sponsor in June 2018. More recently, Microsoft acquired jClarity, which meant respected Java Champions such as Martijn Verburg and Ben Evans were brought into the Microsoft fold. And, despite owning Team Foundation Server, a popular and capable version-control system in its own right, Microsoft spent $7.5 billion on GitHub, a DVCS tool that hosts a variety of Apache and other open source Java projects. Microsoft servers have never been the primary deployment target of Java EE apps. But a cloud-native Java application that runs within Docker can be hosted on Microsoft Azure with ease. Containerization has opened up the playing field, and Microsoft can salivate over the formerly inaccessible revenue potential the enterprise Java space represents. Oracle Code One 2019 may well be remembered as the turning point in the Microsoft vs. IBM discussion. This is the year that saw Microsoft start to make serious inroads to the Java community and transform itself from a punching bag to a respected advocate. This notion really hit home when I saw a session at Code One with Kirk Pepperdine -- a Java Champion who has always been fiercely independent -- and realized that he's now a principal engineer at Microsoft. It's equally as elucidating to see Reza Rahman -- the former Java EE evangelist for Oracle -- represent Microsoft in his \"Birds of a Feather\" sessions. It's pretty clear. Microsoft has made a serious play at the enterprise Java space, and IBM has drifted off in other directions. Dig Deeper on JSRs and APIs Is Apache Tomcat the right Java application server for you? By: TimCulverhouse Tomcat vs WebSphere: How these application servers compare By: CameronMcKenzie Azure tools for cloud-based development Hot skills: IBMs WAS CE offers free route to develop and deploy applications By: NickLangley "],
        "story_type":"Normal",
        "url_raw":"https://www.theserverside.com/opinion/Microsoft-vs-IBM-A-major-shift-in-Java-support",
        "comments.comment_id":[21306161,
          21306258],
        "comments.comment_author":["mshockwave",
          "__initbrian__"],
        "comments.comment_descendants":[1,
          0],
        "comments.comment_time":["2019-10-20T18:45:51Z",
          "2019-10-20T18:57:47Z"],
        "comments.comment_text":["I don't think donating huge money to Github can be an evidence that M$ are moving toward Java",
          "Buying developers via github seemed focused toward JS. I went to [0] to prove it but turns out Java is the second most popular language on GitHub projects<p>[0] <a href=\"https://octoverse.github.com/projects\" rel=\"nofollow\">https://octoverse.github.com/projects</a>"],
        "id":"e809b289-889c-40e3-aa0a-a8bd5a2b9800",
        "url_text":"Once an afterthought in the Java community, Microsoft has seemingly overtaken IBM as the preeminent advocate among developers at the Oracle Code One conference. SAN FRANCISCO -- There once was a time when IBM was arguably the most dominant force in the enterprise Java community. And yet at Oracle Code One 2019, signs all pointed to how Microsoft wants in, while IBM wants out -- a major shift in the Microsoft vs. IBM discussion. It was always IBM, after all, that invested heavily in Java development, while Microsoft didn't bother. As IBM pushes itself away from the Java table, Microsoft appears ready to take a seat. Microsoft vs. IBM: A reversal of roles IBM invented the Eclipse IDE. IBM pushed Fortune 500 clients onto WebSphere, which drove widespread adoption of server-side Java. And when Oracle bought Sun Microsystems a decade ago, it was IBM that put in a serious bid for Sun's technologies. If Oracle hadn't sweetened its offer, IBM likely would have become the steward of the Java language. But IBM's interest waned over the years, and the company has severely neglected its WebSphere user base, providing inadequate updates to the server, portal and web content management (WCM) tools. Even today, the WCM editor doesn't support multiple browser tabs. The web-based server administration UI blows up when you click the back button, and the portal configuration tool is severely outdated. Nobody in the industry was surprised when IBM sold its on-premises WebSphere offerings to HCL Industries earlier this year. From a user perspective, it feels like IBM gave up on WebSphere and sever-side Java a long time ago. Instead, Big Blue is focused on AI, the cloud and its bewildering assortment of Watson-branded tools. IBM certainly didn't present as strong a front at Oracle Code One 2019 as it did when the conference was called JavaOne, and that contrasts starkly with Microsoft. Microsoft vs. IBM: One lays inroads for Java developers, the other retracts Microsoft's .NET platform has always been a direct competitor to Java EE, and any tool that flew under Bill Gates' flag was ripe for criticism from the Java community -- no matter how technically sound it might be. Despite a position behind the 8-ball in terms of Java mindshare, Microsoft has done everything it can over the past 18 months to endear itself to the developer community. Microsoft became an AdoptOpenJDK sponsor in June 2018. More recently, Microsoft acquired jClarity, which meant respected Java Champions such as Martijn Verburg and Ben Evans were brought into the Microsoft fold. And, despite owning Team Foundation Server, a popular and capable version-control system in its own right, Microsoft spent $7.5 billion on GitHub, a DVCS tool that hosts a variety of Apache and other open source Java projects. Microsoft servers have never been the primary deployment target of Java EE apps. But a cloud-native Java application that runs within Docker can be hosted on Microsoft Azure with ease. Containerization has opened up the playing field, and Microsoft can salivate over the formerly inaccessible revenue potential the enterprise Java space represents. Oracle Code One 2019 may well be remembered as the turning point in the Microsoft vs. IBM discussion. This is the year that saw Microsoft start to make serious inroads to the Java community and transform itself from a punching bag to a respected advocate. This notion really hit home when I saw a session at Code One with Kirk Pepperdine -- a Java Champion who has always been fiercely independent -- and realized that he's now a principal engineer at Microsoft. It's equally as elucidating to see Reza Rahman -- the former Java EE evangelist for Oracle -- represent Microsoft in his \"Birds of a Feather\" sessions. It's pretty clear. Microsoft has made a serious play at the enterprise Java space, and IBM has drifted off in other directions. Dig Deeper on JSRs and APIs Is Apache Tomcat the right Java application server for you? By: TimCulverhouse Tomcat vs WebSphere: How these application servers compare By: CameronMcKenzie Azure tools for cloud-based development Hot skills: IBMs WAS CE offers free route to develop and deploy applications By: NickLangley ",
        "_version_":1718441067744854016},
      {
        "story_id":20589102,
        "story_author":"rspivak",
        "story_descendants":95,
        "story_score":178,
        "story_time":"2019-08-01T22:50:55Z",
        "story_title":"Big O Notation  Using not-boring math to measure codes efficiency",
        "search":["Big O Notation  Using not-boring math to measure codes efficiency",
          "https://www.interviewcake.com/article/python/big-o-notation-time-and-space-complexity",
          "Using not-boring math to measure code's efficiency Get the 7-day crash course! In this free email course, I'll teach you the right way of thinking for breaking down tricky algorithmic coding interview questions. The idea behind big O notation Big O notation is the language we use for talking about how long an algorithm takes to run. It's how we compare the efficiency of different approaches to a problem. It's like math except it's an awesome, not-boring kind of math where you get to wave your hands through the details and just focus on what's basically happening. With big O notation we express the runtime in terms ofbrace yourselfhow quickly it grows relative to the input, as the input gets arbitrarily large. Let's break that down: how quickly the runtime growsIt's hard to pin down the exact runtime of an algorithm. It depends on the speed of the processor, what else the computer is running, etc. So instead of talking about the runtime directly, we use big O notation to talk about how quickly the runtime grows. relative to the inputIf we were measuring our runtime directly, we could express our speed in seconds. Since we're measuring how quickly our runtime grows, we need to express our speed in terms of...something else. With Big O notation, we use the size of the input, which we call \"n.\" So we can say things like the runtime grows \"on the order of the size of the input\" () or \"on the order of the square of the size of the input\" (). as the input gets arbitrarily largeOur algorithm may have steps that seem expensive when n is small but are eclipsed eventually by other steps as n gets huge. For big O analysis, we care most about the stuff that grows fastest as the input grows, because everything else is quickly eclipsed as n gets very large. (If you know what an asymptote is, you might see why \"big O analysis\" is sometimes called \"asymptotic analysis.\") If this seems abstract so far, that's because it is. Let's look at some examples. Some examples def print_first_item(items): print items[0] This function runs in time (or \"constant time\") relative to its input. The input list could be 1 item or 1,000 items, but this function would still just require one \"step.\" def print_all_items(items): for item in items: print item This function runs in time (or \"linear time\"), where n is the number of items in the list. If the list has 10 items, we have to print 10 times. If it has 1,000 items, we have to print 1,000 times. def print_all_possible_ordered_pairs(items): for first_item in items: for second_item in items: print first_item, second_item Here we're nesting two loops. If our list has n items, our outer loop runs n times and our inner loop runs n times for each iteration of the outer loop, giving us n^2 total prints. Thus this function runs in time (or \"quadratic time\"). If the list has 10 items, we have to print 100 times. If it has 1,000 items, we have to print 1,000,000 times. N could be the actual input, or the size of the input Both of these functions have runtime, even though one takes an integer as its input and the other takes a list: def say_hi_n_times(n): for time in xrange(n): print \"hi\" def print_all_items(items): for item in items: print item So sometimes n is an actual number that's an input to our function, and other times n is the number of items in an input list (or an input map, or an input object, etc.). Drop the constants This is why big O notation rules. When you're calculating the big O complexity of something, you just throw out the constants. So like: def print_all_items_twice(items): for item in items: print item # Once more, with feeling for item in items: print item This is , which we just call . def print_first_item_then_first_half_then_say_hi_100_times(items): print items[0] middle_index = len(items) / 2 index = 0 while index < middle_index: print items[index] index += 1 for time in xrange(100): print \"hi\" This is , which we just call . Why can we get away with this? Remember, for big O notation we're looking at what happens as n gets arbitrarily large. As n gets really big, adding 100 or dividing by 2 has a decreasingly significant effect. Keep up the momentum! Sign up to get a data structures and algorithms practice question sent to you every week. Drop the less significant terms For example: def print_all_numbers_then_all_pair_sums(numbers): print \"these are the numbers:\" for number in numbers: print number print \"and these are their sums:\" for first_number in numbers: for second_number in numbers: print first_number + second_number Here our runtime is , which we just call . Even if it was , it would still be . Similarly: is is Again, we can get away with this because the less significant terms quickly become, well, less significant as n gets big. We're usually talking about the \"worst case\" Often this \"worst case\" stipulation is implied. But sometimes you can impress your interviewer by saying it explicitly. Sometimes the worst case runtime is significantly worse than the best case runtime: def contains(haystack, needle): # Does the haystack contain the needle? for item in haystack: if item == needle: return True return False Here we might have 100 items in our haystack, but the first item might be the needle, in which case we would return in just 1 iteration of our loop. In general we'd say this is runtime and the \"worst case\" part would be implied. But to be more specific we could say this is worst case and best case runtime. For some algorithms we can also make rigorous statements about the \"average case\" runtime. Space complexity: the final frontier Sometimes we want to optimize for using less memory instead of (or in addition to) using less time. Talking about memory cost (or \"space complexity\") is very similar to talking about time cost. We simply look at the total size (relative to the size of the input) of any new variables we're allocating. This function takes space (we use a fixed number of variables): def say_hi_n_times(n): for time in xrange(n): print \"hi\" This function takes space (the size of hi_list scales with the size of the input): def list_of_hi_n_times(n): hi_list = [] for time in xrange(n): hi_list.append(\"hi\") return hi_list Usually when we talk about space complexity, we're talking about additional space, so we don't include space taken up by the inputs. For example, this function takes constant space even though the input has n items: def get_largest_item(items): largest = float('-inf') for item in items: if item > largest: largest = item return largest Sometimes there's a tradeoff between saving time and saving space, so you have to decide which one you're optimizing for. Big O analysis is awesome except when it's not You should make a habit of thinking about the time and space complexity of algorithms as you design them. Before long this'll become second nature, allowing you to see optimizations and potential performance issues right away. Asymptotic analysis is a powerful tool, but wield it wisely. Big O ignores constants, but sometimes the constants matter. If we have a script that takes 5 hours to run, an optimization that divides the runtime by 5 might not affect big O, but it still saves you 4 hours of waiting. Beware of premature optimization. Sometimes optimizing time or space negatively impacts readability or coding time. For a young startup it might be more important to write code that's easy to ship quickly or easy to understand later, even if this means it's less time and space efficient than it could be. But that doesn't mean startups don't care about big O analysis. A great engineer (startup or otherwise) knows how to strike the right balance between runtime, space, implementation time, maintainability, and readability. You should develop the skill to see time and space optimizations, as well as the wisdom to judge if those optimizations are worthwhile. Don't miss part 2: logarithms, amortized analysis, and more. Sign up for updates. Free practice problems every week! {\"id\":22689693,\"username\":\"2021-11-06_00:30:16_wc%j5n\",\"email\":null,\"date_joined\":\"2021-11-06T00:30:16.875874+00:00\",\"first_name\":\"\",\"last_name\":\"\",\"full_name\":\"\",\"short_name\":\"friend\",\"is_anonymous\":true,\"is_on_last_question\":false,\"percent_done\":0,\"num_questions_done\":0,\"num_questions_remaining\":46,\"is_full_access\":false,\"is_student\":false,\"first_payment_date\":null,\"last_payment_date\":null,\"num_free_questions_left\":3,\"terms_has_agreed_to_latest\":false,\"preferred_content_language\":\"\",\"preferred_editor_language\":\"\",\"is_staff\":false,\"auth_providers_human_readable_list\":\"\",\"num_auth_providers\":0,\"auth_email\":\"\"} . . . "],
        "story_type":"Normal",
        "url_raw":"https://www.interviewcake.com/article/python/big-o-notation-time-and-space-complexity",
        "comments.comment_id":[20589687,
          20590005],
        "comments.comment_author":["curiousgal",
          "abalone"],
        "comments.comment_descendants":[2,
          7],
        "comments.comment_time":["2019-08-02T00:23:55Z",
          "2019-08-02T01:27:31Z"],
        "comments.comment_text":["> <i>not-boring math</i><p>What's next? Gradient decent with no gradients? Fourrier transform with no functions?<p>If math is do boring just skip the whole thing, don't try to kid yourself by saying you're not doing it when you are in fact doing it.<p>/rant",
          "I've noticed there are SO many of these \"coding interview prep\" courses lately. Like, obviously it's a hot job market, but there's just so many and of a certain vibe that it seems like some are selling a dream.<p>There's a common narrative too, that it's all about algorithm question prep and it's a \"game\" you can win. One youtuber who recently landed a ~$275K TC gig said it's not about being \"intelligent\" it's about practice and rapid pattern recognition (he also was pitching his own coding interview prep course). Said he forgot half of it after getting the job.<p>This could all be true but this narrative is coming from sources that have a financial interest in making you believe that. What do you think HN? I mean I know algorithm prep is an important part of the interview but.. there's just something sketch about all these outfits I can't quite put my finger on.<p>(I also wonder if the people who get hired based on prep vs. aptitude then have a kind of survivor bias and hire others who are strong on this limited range of pattern recognition... could it be a problem for the industry?)"],
        "id":"ec912419-4011-48f3-841d-cd0dd7ff5d0d",
        "url_text":"Using not-boring math to measure code's efficiency Get the 7-day crash course! In this free email course, I'll teach you the right way of thinking for breaking down tricky algorithmic coding interview questions. The idea behind big O notation Big O notation is the language we use for talking about how long an algorithm takes to run. It's how we compare the efficiency of different approaches to a problem. It's like math except it's an awesome, not-boring kind of math where you get to wave your hands through the details and just focus on what's basically happening. With big O notation we express the runtime in terms ofbrace yourselfhow quickly it grows relative to the input, as the input gets arbitrarily large. Let's break that down: how quickly the runtime growsIt's hard to pin down the exact runtime of an algorithm. It depends on the speed of the processor, what else the computer is running, etc. So instead of talking about the runtime directly, we use big O notation to talk about how quickly the runtime grows. relative to the inputIf we were measuring our runtime directly, we could express our speed in seconds. Since we're measuring how quickly our runtime grows, we need to express our speed in terms of...something else. With Big O notation, we use the size of the input, which we call \"n.\" So we can say things like the runtime grows \"on the order of the size of the input\" () or \"on the order of the square of the size of the input\" (). as the input gets arbitrarily largeOur algorithm may have steps that seem expensive when n is small but are eclipsed eventually by other steps as n gets huge. For big O analysis, we care most about the stuff that grows fastest as the input grows, because everything else is quickly eclipsed as n gets very large. (If you know what an asymptote is, you might see why \"big O analysis\" is sometimes called \"asymptotic analysis.\") If this seems abstract so far, that's because it is. Let's look at some examples. Some examples def print_first_item(items): print items[0] This function runs in time (or \"constant time\") relative to its input. The input list could be 1 item or 1,000 items, but this function would still just require one \"step.\" def print_all_items(items): for item in items: print item This function runs in time (or \"linear time\"), where n is the number of items in the list. If the list has 10 items, we have to print 10 times. If it has 1,000 items, we have to print 1,000 times. def print_all_possible_ordered_pairs(items): for first_item in items: for second_item in items: print first_item, second_item Here we're nesting two loops. If our list has n items, our outer loop runs n times and our inner loop runs n times for each iteration of the outer loop, giving us n^2 total prints. Thus this function runs in time (or \"quadratic time\"). If the list has 10 items, we have to print 100 times. If it has 1,000 items, we have to print 1,000,000 times. N could be the actual input, or the size of the input Both of these functions have runtime, even though one takes an integer as its input and the other takes a list: def say_hi_n_times(n): for time in xrange(n): print \"hi\" def print_all_items(items): for item in items: print item So sometimes n is an actual number that's an input to our function, and other times n is the number of items in an input list (or an input map, or an input object, etc.). Drop the constants This is why big O notation rules. When you're calculating the big O complexity of something, you just throw out the constants. So like: def print_all_items_twice(items): for item in items: print item # Once more, with feeling for item in items: print item This is , which we just call . def print_first_item_then_first_half_then_say_hi_100_times(items): print items[0] middle_index = len(items) / 2 index = 0 while index < middle_index: print items[index] index += 1 for time in xrange(100): print \"hi\" This is , which we just call . Why can we get away with this? Remember, for big O notation we're looking at what happens as n gets arbitrarily large. As n gets really big, adding 100 or dividing by 2 has a decreasingly significant effect. Keep up the momentum! Sign up to get a data structures and algorithms practice question sent to you every week. Drop the less significant terms For example: def print_all_numbers_then_all_pair_sums(numbers): print \"these are the numbers:\" for number in numbers: print number print \"and these are their sums:\" for first_number in numbers: for second_number in numbers: print first_number + second_number Here our runtime is , which we just call . Even if it was , it would still be . Similarly: is is Again, we can get away with this because the less significant terms quickly become, well, less significant as n gets big. We're usually talking about the \"worst case\" Often this \"worst case\" stipulation is implied. But sometimes you can impress your interviewer by saying it explicitly. Sometimes the worst case runtime is significantly worse than the best case runtime: def contains(haystack, needle): # Does the haystack contain the needle? for item in haystack: if item == needle: return True return False Here we might have 100 items in our haystack, but the first item might be the needle, in which case we would return in just 1 iteration of our loop. In general we'd say this is runtime and the \"worst case\" part would be implied. But to be more specific we could say this is worst case and best case runtime. For some algorithms we can also make rigorous statements about the \"average case\" runtime. Space complexity: the final frontier Sometimes we want to optimize for using less memory instead of (or in addition to) using less time. Talking about memory cost (or \"space complexity\") is very similar to talking about time cost. We simply look at the total size (relative to the size of the input) of any new variables we're allocating. This function takes space (we use a fixed number of variables): def say_hi_n_times(n): for time in xrange(n): print \"hi\" This function takes space (the size of hi_list scales with the size of the input): def list_of_hi_n_times(n): hi_list = [] for time in xrange(n): hi_list.append(\"hi\") return hi_list Usually when we talk about space complexity, we're talking about additional space, so we don't include space taken up by the inputs. For example, this function takes constant space even though the input has n items: def get_largest_item(items): largest = float('-inf') for item in items: if item > largest: largest = item return largest Sometimes there's a tradeoff between saving time and saving space, so you have to decide which one you're optimizing for. Big O analysis is awesome except when it's not You should make a habit of thinking about the time and space complexity of algorithms as you design them. Before long this'll become second nature, allowing you to see optimizations and potential performance issues right away. Asymptotic analysis is a powerful tool, but wield it wisely. Big O ignores constants, but sometimes the constants matter. If we have a script that takes 5 hours to run, an optimization that divides the runtime by 5 might not affect big O, but it still saves you 4 hours of waiting. Beware of premature optimization. Sometimes optimizing time or space negatively impacts readability or coding time. For a young startup it might be more important to write code that's easy to ship quickly or easy to understand later, even if this means it's less time and space efficient than it could be. But that doesn't mean startups don't care about big O analysis. A great engineer (startup or otherwise) knows how to strike the right balance between runtime, space, implementation time, maintainability, and readability. You should develop the skill to see time and space optimizations, as well as the wisdom to judge if those optimizations are worthwhile. Don't miss part 2: logarithms, amortized analysis, and more. Sign up for updates. Free practice problems every week! {\"id\":22689693,\"username\":\"2021-11-06_00:30:16_wc%j5n\",\"email\":null,\"date_joined\":\"2021-11-06T00:30:16.875874+00:00\",\"first_name\":\"\",\"last_name\":\"\",\"full_name\":\"\",\"short_name\":\"friend\",\"is_anonymous\":true,\"is_on_last_question\":false,\"percent_done\":0,\"num_questions_done\":0,\"num_questions_remaining\":46,\"is_full_access\":false,\"is_student\":false,\"first_payment_date\":null,\"last_payment_date\":null,\"num_free_questions_left\":3,\"terms_has_agreed_to_latest\":false,\"preferred_content_language\":\"\",\"preferred_editor_language\":\"\",\"is_staff\":false,\"auth_providers_human_readable_list\":\"\",\"num_auth_providers\":0,\"auth_email\":\"\"} . . . ",
        "_version_":1718441049057132544},
      {
        "story_id":19437245,
        "story_author":"quickthrower2",
        "story_descendants":55,
        "story_score":110,
        "story_time":"2019-03-19T23:25:50Z",
        "story_title":"Setting Up a Software Development Environment on Alpine Linux",
        "search":["Setting Up a Software Development Environment on Alpine Linux",
          "https://blog.overops.com/my-alpine-desktop-setting-up-a-software-development-environment-on-alpine-linux/",
          "Posted by Shahar Valiano, a C++ engineer from OverOps microagent team developing our low-level JVM agent. Whether you are into software development, DevOps or test engineering, if you have some work experience with Docker, you are most likely already familiar with Alpine Linux. Alpine has gained great popularity in recent years and nowadays is probably the most favored Linux for Dockers. Originally designed for routers, it is a secure, fast, feather-light Linux: a basic Alpine base image takes as little as 5 MB, orders of magnitude less than other popular Linux distros (Ubuntu is 188 MB, for comparison). That fact makes it an ideal choice as the base system of Docker images, where small size is desirable, and specifically for OpenJDK Docker images that otherwise take up several hundred MBs. Im a big Linux fan, and enjoy experimenting with new environments, so I was very excited when I was tasked with porting the OverOps agent to native Alpine Linux! But before I could get started with any actual porting work, I had to set myself a proper Alpine development environment. In this post, Ill cover my experience in setting up an Alpine Linux workstation for C++ and Java development, with some hopefully useful Alpine know-hows, tips and resources. So What Do You Get With Alpine Linux? First, lets take a glimpse of the main features of Alpine Linux. The project homepage summarizes it nicely: Alpine Linux is an independent, non-commercial, general purpose Linux distribution designed for power users who appreciate security, simplicity and resource efficiency. The most notable security feature is a hardened, specially patched Linux kernel. Additional security features include: PIE executables. All Alpine executables are built as PIE Position Independent Executables, which do not depend on absolute memory addresses for their correct operation, much like shared libraries. Such executables are subject to Address Space Layout Randomization (ASLR), a technique in which the kernel dynamically shifts programs between random memory locations, which is useful for preventing certain kinds of attacks. Stack smashing protection. This feature is implemented in all Alpine binaries, and allows stack overflowed programs to exit gracefully instead of crashing horribly. It is typically implemented by building with a special C/C++ compiler option (-fstack-protector-fstrong), that adds stack canaries to the program code. These are compiler generated variables that mark the end of the allocated stack frame in each function, and when overridden, indicate that the stack has overflowed and trouble is ahead, much like canaries in a coal mine. Less is more. Though not a feature per se, Alpines minimalistic nature also makes it safer and less prone to attacks, since it carries little to no excess baggage and therefore the possible attack surface is much smaller, especially compared to other Linuxes. Unfortunately, as with any other system, it is not 100% bulletproof, as uncovered by this recently found and fixed exploit. So, Alpine is as secure as it can be, but its small footprint is what really sets it apart from the other Linuxes. It is built around two components that make it especially compact: Busybox. Busybox is an all-in-one, multi-purpose binary. Named by its authors The Swiss Army Knife of Embedded Linux, it provides the core functionalities for dozens of standard programs, such as awk, cp, grep, gzip, sh and top. All programs are symlinked to /bin/busybox, which identifies the program to run according the name it was executed with. It offers high compatibility to the GNU counterparts, typically with a reduced feature set, but worry not you can usually get the full GNU functionalities by installing the package of the same name, such as grep or tar, or bundle packages like coreutils. Busybox is significant to Alpines small size using a single static binary reduces the overhead of multiple executables and allows the resulting binary to be effectively size optimized. On Alpine 3.8, it is squeezed into just 780 KB of goodness. musl libc. The musl libc library is a compact and compelling alternative to GNUs libc library, glibc, which is the defacto libc implementation used in most Linux distributions. Compared to glibc, musl is tiny in size: it weighs only 572 KB on Alpine 3.8, compared to glibcs 3 MB on Ubuntu 16.04, for example (3 MB is the combined size of libc.so and libm.so; in Alpine libm.so is symlinked to musl.so). But, theres much more to musl than its small size: it offers stricter POSIX conformance, improved safety and competitive performance. This comparison chart from ETA labs, authors of musl, gives a very detailed picture of the differences between the libc implementations and highlights musls strengths. Sadly, theres a catch: having musl as the default libc implementation introduces a major compatibility issue with other Linuxes that are based on glibc. Almost any Linux program is dynamically linked with libc, and while glibc Linux binaries will link against libc.so, Alpine binaries will link against musl.so. Consequently, Linux executables that were built on glibc distros such as Ubuntu and RedHat, will not be able to run on Alpine, at least not out of the box. [optin-monster-shortcode id=ora6yvsrf1pjk2fmq5ee] What else is in Alpine? A neat and simple package manager called apk. If youre already familiar with the apt package manager, theres no getting used to apk: the basic commands are add, del, search and update, and youre good to go. If youve written Alpine Dockerfiles, you should already know apk pretty well. The default shell in Alpine is the busybox provided ash. As with anything in Alpine, it is also quite minimalistic, and lacks some shell convenience features, such as auto completion. If youre into bash, like myself, simply install the bash apk package, and use it as your default shell. Lets Install Alpine So, after some sniffing around, it was time to get started with setting up an Alpine work environment. But what environment should it be? Docker or workstation? As I saw it, there were two options: one, using an Alpine Docker for my development work, and two, bringing up a proper Alpine workstation on my laptop. This was my first real encounter with Docker, so I wasnt sure whether I should deep dive in that direction, since I wanted to get up and running quickly. Also, at that point, I thought that Docker is essentially a disposable environment that may not be ideal for day to day development (only later I realized that Docker could come handy for that purpose, too). The Alpine workstation option seemed much more compelling, but having no experience with Alpine either, I wasnt sure how comfortable it would be for development; with the Docker option, at least I still had my Ubuntu desktop to work with. Finally, I chose option two, and rushed into installing Alpine on my already dual-booted Ubuntu-Windows Dell XPS 15. Installing Alpine Linux Happy and jolly, I headed over to the Alpine downloads page. It offers the latest Alpine version (3.8.1, as of writing these lines), available in 8 different ISO flavours. It turns out, Alpine runs on pretty much anything, from servers, dockers and VMs, to embedded ARM devices and Raspberry Pi. Out of the x86_64 images, I contemplated between Standard and Extended. Since I was aiming for a full fledged desktop environment, I figured Extended would make a better starting point (which proved correct later on) and hit the download button. Alpine ISO check. Now comes the real fun part install time! The first Google hit for install alpine is the installation page at the Alpine Linux Wiki. The Wiki is an incredible source of information, with just about everything there is to know about Alpine. Really awesome job by the Alpine team. As described in the installation page, Alpine could be installed in three different modes: Diskless mode In that mode, Alpine boots from the installation media, such as a CD or a USB stick, and loaded into RAM. No data is written to disk, and session is discarded upon reboot. Data mode Similar to Diskless, but with the /var folder mounted to a writable media, which is persistent between reboots. Sys mode A traditional hard disk install. Clearly, option 3 was the most suitable one for me. Therefore, I proceeded with the below steps on Ubuntu for installing Alpine: Created a bootable Alpine installation USB (guide); Used gparted for shrinking my existing volumes, making room for the new Alpine partition, and formatted it as ext4; Copied the Alpine system to the new partition (guide); Regenerated my GRUB menu entries, by running update-grub2; Tweaked GRUB entry to allow booting into Alpine (see below). That was about it. Just a few hours work, and I could now log in into my brand new Alpine Linux installation, triple booted with Ubuntu and Windows Sweet! Hellooo, Alpine! GRUB Tweaking The GRUB part was a bit tricky, though. After installing Alpine and updating GRUB, a new menu entry appeared: unknown Linux distribution (on /dev/nvme0n1p9). Optimistically, I clicked it, but instead of the expected Welcome to Alpine prompt, after a few seconds of booting I got a kernel panic Yikes! It took some experimenting to figure out what was wrong: I did not take care of loading the appropriate kernel modules for my filesystem, ext4 specifically, and update-grub2 didnt take care of that either. So, I used the Ubuntu GRUB entry as a starting point, and dropped the following menu entry in a new file under /etc/grub.d: Getting Started With Alpine Now that I was able to successfully boot into Alpine, it was time for some basic setup steps. Connecting to Wi-Fi First and foremost, I needed to get a working internet connection. My laptop doesnt have a wired network socket, and my USB RJ45 adapter was broken, so my only option was Wi-Fi. This Alpine Wiki page, Connecting to a Wireless Access Point, describes the Wi-Fi setup procedure very clearly, so I was very quickly able to connect to my work Wi-Fi network (specifically, I followed the Manual Configuration steps). For getting started with Wi-Fi on Alpine, there are two essential packages that need to be installed: wireless-tools and wpa_supplicant. It turns out, my bet on the Extended ISO paid off: it includes these packages in its offline apk database, in contrast to the standard ISO. This saved me the trouble of using a colleagues computer for downloading these packages from the Alpine packages catalogue (which is, BTW, pretty awesome!), copying them to a USB stick, mounting it to Alpine and installing the .apk files manually. Now that I had network connectivity, I was able to update the apk repository indices and install some convenience packages, such as bash, coreutils and nano. Feels like home already! Preparing The Build Environment At this point, I had a decent working Alpine command line, so I was good for prepping the build environment. Similar to Ubuntus build-essential, the build-base meta-package is a good starting point for installing the most common build tools and utilities, including g++, make and binutils. For building our C++ stack, I needed several other packages, including cmake and linux-headers. Some C++ projects are only compatible with certain compiler versions, and specifically, Alpine 3.8 ships with gcc 6.4.0. The official Alpine repositories do not contain previous major gcc versions, as far as I could find, so if youre needing a different gcc version on Alpine, chances are youll find it (or a compatible version) at the musl-cross-make project page: https://github.com/just-containers/musl-cross-make/releases. Ive personally tested gcc 4.9.4, and it worked perfectly. Great job by these guys, which could be a real lifesaver for some. Next, installing Java. The official Alpine JDK implementation is OpenJDK, brought to us by the OpenJDK IcedTea project. OpenJDK versions 7 and 8 are available, and could be downloaded from the Alpine apk repositories. Unfortunately, there are currently no GA builds for Java 9, 10 and 11 (see project Portola homepage and this github discussion for current status). For now, I was satisfied with the openjdk8 package. Finally, expecting times of fierce debugging, I went ahead and installed gdb and strace, as well as musl and OpenJDK debug symbols, available in the musl-dbg and openjdk8-dbg packages respectively. Bring it on, Alpine! Tip: for bleeding edge and latest packages, you may want to add the edge repository to your /etc/apk/repositories file, as described in the package management page. For example, lldb is currently only available in edge. However, edge packages are experimental, so use them with care. Installing a Desktop and IDE I must admit, I first imagined Alpine as an exotic command line environment and didnt see myself even using the mouse, so I was happy to learn Alpine is a capable desktop environment, too! Alpine has a variety of desktops available, including GNOME, MATE and Xfce. I wasnt yet familiar with Xfce, so I decided to give it a try. As always, the Alpine Wiki is a great place to start. The Xfce setup page details all the steps needed for installing and starting Xfce. Most importantly, the packages to install are xfce4 the desktop itself, and alpine-desktop. The latter is a very comprehensive meta-package that delivers the default Alpine desktop experience, including the Firefox web browser, AbiWord editor, Audacious sound player, and many others. Tip: Dont forget to install a font package as well, such as font-noto. Otherwise, youll end up with weird blank squares for text, and some GUI apps may crash or misbehave. This took me a decent amount of time to figure out! The final piece missing in my Alpine puzzle was finding proper IDEs for both C/C++ and Java development. Here, the musl-glibc compatibility issue becomes really painful. For example, Eclipse CDT builds compatible with musl are not to be found, so without the glibc compatibility layer, its not possible to run Eclipse on Alpine. Im comfortable with the command line, but I really wanted a proper IDE at hand, especially for debugging. Luckily, we have the JetBrains IDE family. IntelliJ, CLion and other IDEs from JetBrains all run on the JVM, and do not depend directly on the underlying libc implementation. Since the OpenJDK JVM is fully compatible with musl, any standard Java app should run fine out of the box, and so do IntelliJ and CLion. Im a big IntelliJ fan, so I was thrilled to see that its working just as well on Alpine. What a relief! Its important to note that while IntelliJ is freely available to everyone in its community edition, CLion is only free for a 30-day evaluation period (though, students can apply for a free academic license). For now, this was enough. That was it I was set up with my Alpine desktop environment! My very own Alpine XFCE Desktop Summary Alpine has become one of the most important Linux distros around, thanks to its widespread use in containers. While being very slim and minimalistic at its core, it could be easily stacked up with most programs and functionalities were accustomed to from other popular Linux distros. With some effort, an Alpine installation could be turned into a proper desktop environment that has nothing to be ashamed of. However, theres a learning curve to Alpine, so its surely not for everyone. Since it has a relatively small user base, theres less compatible software available, and you may find that the package of need is not yet supported on musl Alpine. Alpine comes with excellent documentation, but on the other hand, there are much less troubleshooting resources and discussions (think StackOverflow), so you may end up having to diagnose and solve problems on your own. All in all, Alpine has many great things going for it, and I currently take much pleasure in working on it. Hope you will, too! Bonus Alpine 3.8 IntelliJ Dockerfile Fast forward a few weeks, as I got more familiar with Docker, I became acquainted with data volumes, which are useful for sharing folders between the host and the docker container. I also learned that when properly set up, Docker will happily run GUI apps, too. Therefore, essentially, the entire Alpine build and desktop environment could be encapsulated in a reusable Docker image, and our projects source could be shared to the Docker and used for development on the Alpine container. So, without further ado, heres an Alpine 3.8 Dockerfile with IntelliJ Community edition + the tools and utilities mentioned earlier. While not perfect or complete, this image may be a good starting point for your very own Alpine dockered desktop. For running the image and starting IntelliJ, follow the instructions at the top of the Dockerfile. GUI is enabled by granting Docker access to the host X11 server. Enjoy! Link to Dockerfile on GitHub:https://github.com/shaharv/docker/blob/master/alpine/dev/Dockerfile "],
        "story_type":"Normal",
        "url_raw":"https://blog.overops.com/my-alpine-desktop-setting-up-a-software-development-environment-on-alpine-linux/",
        "comments.comment_id":[19437567,
          19437896],
        "comments.comment_author":["vector_spaces",
          "deathanatos"],
        "comments.comment_descendants":[0,
          4],
        "comments.comment_time":["2019-03-20T00:15:14Z",
          "2019-03-20T01:06:45Z"],
        "comments.comment_text":["I have Alpine+gnome running on my little Dell Inspiron 5000 and I love it. It's super snappy where even Lubuntu was sluggish.<p>There are some missing luxuries like obconf and Emacs gui, but I mainly use it for browsing and sshing into Debian servers where I can just use vim anyway.<p>It was actually really straightforward to setup, too.",
          "> <i>Ubuntu is 188 MB</i><p>Ubuntu is 88MB:<p><pre><code>  ubuntu  bionic  47b19964fb50  5 weeks ago  88.1MB\n</code></pre>\nDefinitely not Alpine levels of small, sure, but the size in the article is off by a digit. But not <i>that</i> big, either, and I find a huge number of devs are more familiar with Ubuntu.<p>I'm also throwing Python into the container, so the base container size is very quickly dwarfed by that. I think if you're using something like golang or Rust, the gains would be a lot more meaningful."],
        "id":"b6f3c95b-3599-460b-83b5-c706298e01bb",
        "url_text":"Posted by Shahar Valiano, a C++ engineer from OverOps microagent team developing our low-level JVM agent. Whether you are into software development, DevOps or test engineering, if you have some work experience with Docker, you are most likely already familiar with Alpine Linux. Alpine has gained great popularity in recent years and nowadays is probably the most favored Linux for Dockers. Originally designed for routers, it is a secure, fast, feather-light Linux: a basic Alpine base image takes as little as 5 MB, orders of magnitude less than other popular Linux distros (Ubuntu is 188 MB, for comparison). That fact makes it an ideal choice as the base system of Docker images, where small size is desirable, and specifically for OpenJDK Docker images that otherwise take up several hundred MBs. Im a big Linux fan, and enjoy experimenting with new environments, so I was very excited when I was tasked with porting the OverOps agent to native Alpine Linux! But before I could get started with any actual porting work, I had to set myself a proper Alpine development environment. In this post, Ill cover my experience in setting up an Alpine Linux workstation for C++ and Java development, with some hopefully useful Alpine know-hows, tips and resources. So What Do You Get With Alpine Linux? First, lets take a glimpse of the main features of Alpine Linux. The project homepage summarizes it nicely: Alpine Linux is an independent, non-commercial, general purpose Linux distribution designed for power users who appreciate security, simplicity and resource efficiency. The most notable security feature is a hardened, specially patched Linux kernel. Additional security features include: PIE executables. All Alpine executables are built as PIE Position Independent Executables, which do not depend on absolute memory addresses for their correct operation, much like shared libraries. Such executables are subject to Address Space Layout Randomization (ASLR), a technique in which the kernel dynamically shifts programs between random memory locations, which is useful for preventing certain kinds of attacks. Stack smashing protection. This feature is implemented in all Alpine binaries, and allows stack overflowed programs to exit gracefully instead of crashing horribly. It is typically implemented by building with a special C/C++ compiler option (-fstack-protector-fstrong), that adds stack canaries to the program code. These are compiler generated variables that mark the end of the allocated stack frame in each function, and when overridden, indicate that the stack has overflowed and trouble is ahead, much like canaries in a coal mine. Less is more. Though not a feature per se, Alpines minimalistic nature also makes it safer and less prone to attacks, since it carries little to no excess baggage and therefore the possible attack surface is much smaller, especially compared to other Linuxes. Unfortunately, as with any other system, it is not 100% bulletproof, as uncovered by this recently found and fixed exploit. So, Alpine is as secure as it can be, but its small footprint is what really sets it apart from the other Linuxes. It is built around two components that make it especially compact: Busybox. Busybox is an all-in-one, multi-purpose binary. Named by its authors The Swiss Army Knife of Embedded Linux, it provides the core functionalities for dozens of standard programs, such as awk, cp, grep, gzip, sh and top. All programs are symlinked to /bin/busybox, which identifies the program to run according the name it was executed with. It offers high compatibility to the GNU counterparts, typically with a reduced feature set, but worry not you can usually get the full GNU functionalities by installing the package of the same name, such as grep or tar, or bundle packages like coreutils. Busybox is significant to Alpines small size using a single static binary reduces the overhead of multiple executables and allows the resulting binary to be effectively size optimized. On Alpine 3.8, it is squeezed into just 780 KB of goodness. musl libc. The musl libc library is a compact and compelling alternative to GNUs libc library, glibc, which is the defacto libc implementation used in most Linux distributions. Compared to glibc, musl is tiny in size: it weighs only 572 KB on Alpine 3.8, compared to glibcs 3 MB on Ubuntu 16.04, for example (3 MB is the combined size of libc.so and libm.so; in Alpine libm.so is symlinked to musl.so). But, theres much more to musl than its small size: it offers stricter POSIX conformance, improved safety and competitive performance. This comparison chart from ETA labs, authors of musl, gives a very detailed picture of the differences between the libc implementations and highlights musls strengths. Sadly, theres a catch: having musl as the default libc implementation introduces a major compatibility issue with other Linuxes that are based on glibc. Almost any Linux program is dynamically linked with libc, and while glibc Linux binaries will link against libc.so, Alpine binaries will link against musl.so. Consequently, Linux executables that were built on glibc distros such as Ubuntu and RedHat, will not be able to run on Alpine, at least not out of the box. [optin-monster-shortcode id=ora6yvsrf1pjk2fmq5ee] What else is in Alpine? A neat and simple package manager called apk. If youre already familiar with the apt package manager, theres no getting used to apk: the basic commands are add, del, search and update, and youre good to go. If youve written Alpine Dockerfiles, you should already know apk pretty well. The default shell in Alpine is the busybox provided ash. As with anything in Alpine, it is also quite minimalistic, and lacks some shell convenience features, such as auto completion. If youre into bash, like myself, simply install the bash apk package, and use it as your default shell. Lets Install Alpine So, after some sniffing around, it was time to get started with setting up an Alpine work environment. But what environment should it be? Docker or workstation? As I saw it, there were two options: one, using an Alpine Docker for my development work, and two, bringing up a proper Alpine workstation on my laptop. This was my first real encounter with Docker, so I wasnt sure whether I should deep dive in that direction, since I wanted to get up and running quickly. Also, at that point, I thought that Docker is essentially a disposable environment that may not be ideal for day to day development (only later I realized that Docker could come handy for that purpose, too). The Alpine workstation option seemed much more compelling, but having no experience with Alpine either, I wasnt sure how comfortable it would be for development; with the Docker option, at least I still had my Ubuntu desktop to work with. Finally, I chose option two, and rushed into installing Alpine on my already dual-booted Ubuntu-Windows Dell XPS 15. Installing Alpine Linux Happy and jolly, I headed over to the Alpine downloads page. It offers the latest Alpine version (3.8.1, as of writing these lines), available in 8 different ISO flavours. It turns out, Alpine runs on pretty much anything, from servers, dockers and VMs, to embedded ARM devices and Raspberry Pi. Out of the x86_64 images, I contemplated between Standard and Extended. Since I was aiming for a full fledged desktop environment, I figured Extended would make a better starting point (which proved correct later on) and hit the download button. Alpine ISO check. Now comes the real fun part install time! The first Google hit for install alpine is the installation page at the Alpine Linux Wiki. The Wiki is an incredible source of information, with just about everything there is to know about Alpine. Really awesome job by the Alpine team. As described in the installation page, Alpine could be installed in three different modes: Diskless mode In that mode, Alpine boots from the installation media, such as a CD or a USB stick, and loaded into RAM. No data is written to disk, and session is discarded upon reboot. Data mode Similar to Diskless, but with the /var folder mounted to a writable media, which is persistent between reboots. Sys mode A traditional hard disk install. Clearly, option 3 was the most suitable one for me. Therefore, I proceeded with the below steps on Ubuntu for installing Alpine: Created a bootable Alpine installation USB (guide); Used gparted for shrinking my existing volumes, making room for the new Alpine partition, and formatted it as ext4; Copied the Alpine system to the new partition (guide); Regenerated my GRUB menu entries, by running update-grub2; Tweaked GRUB entry to allow booting into Alpine (see below). That was about it. Just a few hours work, and I could now log in into my brand new Alpine Linux installation, triple booted with Ubuntu and Windows Sweet! Hellooo, Alpine! GRUB Tweaking The GRUB part was a bit tricky, though. After installing Alpine and updating GRUB, a new menu entry appeared: unknown Linux distribution (on /dev/nvme0n1p9). Optimistically, I clicked it, but instead of the expected Welcome to Alpine prompt, after a few seconds of booting I got a kernel panic Yikes! It took some experimenting to figure out what was wrong: I did not take care of loading the appropriate kernel modules for my filesystem, ext4 specifically, and update-grub2 didnt take care of that either. So, I used the Ubuntu GRUB entry as a starting point, and dropped the following menu entry in a new file under /etc/grub.d: Getting Started With Alpine Now that I was able to successfully boot into Alpine, it was time for some basic setup steps. Connecting to Wi-Fi First and foremost, I needed to get a working internet connection. My laptop doesnt have a wired network socket, and my USB RJ45 adapter was broken, so my only option was Wi-Fi. This Alpine Wiki page, Connecting to a Wireless Access Point, describes the Wi-Fi setup procedure very clearly, so I was very quickly able to connect to my work Wi-Fi network (specifically, I followed the Manual Configuration steps). For getting started with Wi-Fi on Alpine, there are two essential packages that need to be installed: wireless-tools and wpa_supplicant. It turns out, my bet on the Extended ISO paid off: it includes these packages in its offline apk database, in contrast to the standard ISO. This saved me the trouble of using a colleagues computer for downloading these packages from the Alpine packages catalogue (which is, BTW, pretty awesome!), copying them to a USB stick, mounting it to Alpine and installing the .apk files manually. Now that I had network connectivity, I was able to update the apk repository indices and install some convenience packages, such as bash, coreutils and nano. Feels like home already! Preparing The Build Environment At this point, I had a decent working Alpine command line, so I was good for prepping the build environment. Similar to Ubuntus build-essential, the build-base meta-package is a good starting point for installing the most common build tools and utilities, including g++, make and binutils. For building our C++ stack, I needed several other packages, including cmake and linux-headers. Some C++ projects are only compatible with certain compiler versions, and specifically, Alpine 3.8 ships with gcc 6.4.0. The official Alpine repositories do not contain previous major gcc versions, as far as I could find, so if youre needing a different gcc version on Alpine, chances are youll find it (or a compatible version) at the musl-cross-make project page: https://github.com/just-containers/musl-cross-make/releases. Ive personally tested gcc 4.9.4, and it worked perfectly. Great job by these guys, which could be a real lifesaver for some. Next, installing Java. The official Alpine JDK implementation is OpenJDK, brought to us by the OpenJDK IcedTea project. OpenJDK versions 7 and 8 are available, and could be downloaded from the Alpine apk repositories. Unfortunately, there are currently no GA builds for Java 9, 10 and 11 (see project Portola homepage and this github discussion for current status). For now, I was satisfied with the openjdk8 package. Finally, expecting times of fierce debugging, I went ahead and installed gdb and strace, as well as musl and OpenJDK debug symbols, available in the musl-dbg and openjdk8-dbg packages respectively. Bring it on, Alpine! Tip: for bleeding edge and latest packages, you may want to add the edge repository to your /etc/apk/repositories file, as described in the package management page. For example, lldb is currently only available in edge. However, edge packages are experimental, so use them with care. Installing a Desktop and IDE I must admit, I first imagined Alpine as an exotic command line environment and didnt see myself even using the mouse, so I was happy to learn Alpine is a capable desktop environment, too! Alpine has a variety of desktops available, including GNOME, MATE and Xfce. I wasnt yet familiar with Xfce, so I decided to give it a try. As always, the Alpine Wiki is a great place to start. The Xfce setup page details all the steps needed for installing and starting Xfce. Most importantly, the packages to install are xfce4 the desktop itself, and alpine-desktop. The latter is a very comprehensive meta-package that delivers the default Alpine desktop experience, including the Firefox web browser, AbiWord editor, Audacious sound player, and many others. Tip: Dont forget to install a font package as well, such as font-noto. Otherwise, youll end up with weird blank squares for text, and some GUI apps may crash or misbehave. This took me a decent amount of time to figure out! The final piece missing in my Alpine puzzle was finding proper IDEs for both C/C++ and Java development. Here, the musl-glibc compatibility issue becomes really painful. For example, Eclipse CDT builds compatible with musl are not to be found, so without the glibc compatibility layer, its not possible to run Eclipse on Alpine. Im comfortable with the command line, but I really wanted a proper IDE at hand, especially for debugging. Luckily, we have the JetBrains IDE family. IntelliJ, CLion and other IDEs from JetBrains all run on the JVM, and do not depend directly on the underlying libc implementation. Since the OpenJDK JVM is fully compatible with musl, any standard Java app should run fine out of the box, and so do IntelliJ and CLion. Im a big IntelliJ fan, so I was thrilled to see that its working just as well on Alpine. What a relief! Its important to note that while IntelliJ is freely available to everyone in its community edition, CLion is only free for a 30-day evaluation period (though, students can apply for a free academic license). For now, this was enough. That was it I was set up with my Alpine desktop environment! My very own Alpine XFCE Desktop Summary Alpine has become one of the most important Linux distros around, thanks to its widespread use in containers. While being very slim and minimalistic at its core, it could be easily stacked up with most programs and functionalities were accustomed to from other popular Linux distros. With some effort, an Alpine installation could be turned into a proper desktop environment that has nothing to be ashamed of. However, theres a learning curve to Alpine, so its surely not for everyone. Since it has a relatively small user base, theres less compatible software available, and you may find that the package of need is not yet supported on musl Alpine. Alpine comes with excellent documentation, but on the other hand, there are much less troubleshooting resources and discussions (think StackOverflow), so you may end up having to diagnose and solve problems on your own. All in all, Alpine has many great things going for it, and I currently take much pleasure in working on it. Hope you will, too! Bonus Alpine 3.8 IntelliJ Dockerfile Fast forward a few weeks, as I got more familiar with Docker, I became acquainted with data volumes, which are useful for sharing folders between the host and the docker container. I also learned that when properly set up, Docker will happily run GUI apps, too. Therefore, essentially, the entire Alpine build and desktop environment could be encapsulated in a reusable Docker image, and our projects source could be shared to the Docker and used for development on the Alpine container. So, without further ado, heres an Alpine 3.8 Dockerfile with IntelliJ Community edition + the tools and utilities mentioned earlier. While not perfect or complete, this image may be a good starting point for your very own Alpine dockered desktop. For running the image and starting IntelliJ, follow the instructions at the top of the Dockerfile. GUI is enabled by granting Docker access to the host X11 server. Enjoy! Link to Dockerfile on GitHub:https://github.com/shaharv/docker/blob/master/alpine/dev/Dockerfile ",
        "_version_":1718441019508260864},
      {
        "story_id":19704448,
        "story_author":"Alupis",
        "story_descendants":39,
        "story_score":171,
        "story_time":"2019-04-20T02:21:45Z",
        "story_title":"Red Hat Replaces Oracle as OpenJDK 8, OpenJDK 11 Steward",
        "search":["Red Hat Replaces Oracle as OpenJDK 8, OpenJDK 11 Steward",
          "https://www.theserverside.com/news/252461945/Red-Hat-replaces-Oracle-as-OpenJDK-8-OpenJDK-11-steward",
          "Red Hat has taken control of two popular versions of the open source Java implementation, so developers can continue to build apps after Oracle's support ends. Red Hat has assumed stewardship from Oracle over OpenJDK 8 and OpenJDK 11, which strengthens Red Hat's support for the Java community -- particularly enterprise Java developers. OpenJDK 8 and OpenJDK 11 are strategic releases of OpenJDK, the free open source implementation of Java Platform, Standard Edition (Java SE). Java 8 is still the most widely used version of Java, and Java 11 is the first long-term support (LTS) version of Java. Oracle signaled its intent to get out of the enterprise Java business when it transitioned support and maintenance of Java Platform, Enterprise Edition to the Eclipse Foundation, where it is now known as Jakarta EE. Oracle increased to a six-month release cadence for new versions of Java in 2017. LTS versions arrive every three years. Oracle ended commercial support for Java 8 and the Oracle JDK 8 implementation of Java SE in January 2018. Serving Oracle's interests Red Hat's stewardship of OpenJDK 11 is a win for the Java community, said Mike Milinkovich, executive director of the Eclipse Foundation, based in Ottawa. Mike Milinkovich \"Both Red Hat and [parent company] IBM are going to be interested and willing to support the public and free maintenance of these Java LTS releases for a longer period than Oracle [intended],\" he said. Red Hat's OpenJDK contributions began in 2007 with the IcedTea integration project. The company was a steward of OpenJDK 7, which came out in 2011, and of OpenJDK 6, from 2013 until 2017. The OpenJDK community selected the company's technical lead on Java, Andrew Haley, as project lead for OpenJDK 8 and OpenJDK 11 in February 2019. He has been active on the OpenJDK governing board for seven years. At the same time, Red Hat's OpenJDK 11 stewardship likely serves Oracle's interests. For starters, this move potentially slows the trend toward new OpenJDK distributions from companies such as Amazon, SAP and Alibaba. \"There are so many other companies jumping on the OpenJDK support bandwagon, which is nice to see, but very few of them really have the expertise to do full-stack support of OpenJDK,\" said an enterprise Java developer for a systems provider, who asked not to be named. \"I feel far better about 8 and 11 being under Red Hat's stewardship than Oracle or almost any of the others. They've done more to keep Java 'honest' than just about anyone else.\" Meanwhile, Oracle has little interest in older versions of Java, other than to charge customers for them, and it's happy to let Red Hat fix and maintain OpenJDK forks, said Cameron Purdy, CEO of xqiz.it, a cloud software startup in Lexington, Mass., and former senior vice president of development at Oracle. \"Companies like IBM and Red Hat are basically forced to do this. Otherwise, they cannot provide long-term support for their own products,\" he said. Red Hat takes over Companies like IBM and Red Hat are basically forced to [fix and maintain OpenJDK forks]. Otherwise, they cannot provide long-term support for their own products. Cameron PurdyCEO, xqiz.it With its stewardship of OpenJDK 8 and OpenJDK 11, Red Hat will maintain releases and determine what it does or doesn't add. That is important, because others -- including AWS, Azul Systems, AdoptOpenJDK and SAP -- will also base their releases on OpenJDK 11. Red Hat will also provide bug and security fixes, which is a major benefit for users of a supported OpenJDK environment. Unsupported environments are unlikely to be current on security. \"There will not be any radical changes to OpenJDK 8, which is in maintenance mode, or OpenJDK 11. But Red Hat will ensure long-term support for each,\" said John Doyle, senior principal product manager at Red Hat, based in Raleigh, N.C. Red Hat also plans to support and enable more innovation in Java. For instance, the company leads the development of the Shenandoah garbage collector that is in OpenJDK 12. It's also facilitating the use of OpenJDK on Microsoft Windows, with commercial support for OpenJDK on Windows released in December 2018. Within the next few weeks, Red Hat will release OpenJDK in a Microsoft installer and distribute IcedTea-Web, a free software implementation of Java Web Start and the Java web browser plugin, as part of the Windows OpenJDK distribution. Beyond Java Despite Java's continued popularity, some believe the programming language's best days are behind it. \"While still important and definitely not legacy, Java is not one of today's growth technologies,\" said Torsten Volk, an analyst at Enterprise Management Associates in Boulder, Colo. \"All of these traditional Java vendors should be focusing on pivoting toward today's fast-growing stuff, such as Python, JavaScript and Node.js.\" Dig Deeper on Java performance tuning Microsoft previews OpenJDK distro to the delight of devs By: DarrylTaft AdoptOpenJDK moves to Eclipse Foundation as Adoptium By: DarrylTaft Oracle moves OpenJDK to Git and GitHub By: DarrylTaft Oracle's release cadence opens door for Java support rivals By: DarrylTaft "],
        "story_type":"Normal",
        "url_raw":"https://www.theserverside.com/news/252461945/Red-Hat-replaces-Oracle-as-OpenJDK-8-OpenJDK-11-steward",
        "comments.comment_id":[19705381,
          19706198],
        "comments.comment_author":["narnianal",
          "Recurecur"],
        "comments.comment_descendants":[0,
          1],
        "comments.comment_time":["2019-04-20T08:08:58Z",
          "2019-04-20T12:34:24Z"],
        "comments.comment_text":["That's great news. I'm personally more a Python fan, but I see all around me how good, stable enterprise problems are solved by Java coders. Last Java version I used was still pre-Oracle, so I can't say if they did a good job or not. But I can say when it comes to open source I have much more trust in Red Hat as a company.",
          "\"While still important and definitely not legacy, Java is not one of today's growth technologies,\" said Torsten Volk, an analyst at Enterprise Management Associates in Boulder, Colo. \"All of these traditional Java vendors should be focusing on pivoting toward today's fast-growing stuff, such as Python, JavaScript and Node.js.\"<p>That strikes me as a profoundly ignorant statement. Most of the value in the Java ecosystem lies in the JVM. There are many languages running on it, Java is only one. Java has improved a lot, but Scala and Kotlin both provide alternatives with their own strengths. Clojure also has a vibrant community.<p>As more focus centers on datacenter efficiency, I suspect there will be a move away from JS and Python as they both have performance limitations based on design.<p>Java is much stronger in that regard, but it'll be interesting to see how the newer AOT compiled languages do - Julia, Rust and Swift."],
        "id":"b61f44b8-c9b4-4b76-8cca-be3a1139b208",
        "url_text":"Red Hat has taken control of two popular versions of the open source Java implementation, so developers can continue to build apps after Oracle's support ends. Red Hat has assumed stewardship from Oracle over OpenJDK 8 and OpenJDK 11, which strengthens Red Hat's support for the Java community -- particularly enterprise Java developers. OpenJDK 8 and OpenJDK 11 are strategic releases of OpenJDK, the free open source implementation of Java Platform, Standard Edition (Java SE). Java 8 is still the most widely used version of Java, and Java 11 is the first long-term support (LTS) version of Java. Oracle signaled its intent to get out of the enterprise Java business when it transitioned support and maintenance of Java Platform, Enterprise Edition to the Eclipse Foundation, where it is now known as Jakarta EE. Oracle increased to a six-month release cadence for new versions of Java in 2017. LTS versions arrive every three years. Oracle ended commercial support for Java 8 and the Oracle JDK 8 implementation of Java SE in January 2018. Serving Oracle's interests Red Hat's stewardship of OpenJDK 11 is a win for the Java community, said Mike Milinkovich, executive director of the Eclipse Foundation, based in Ottawa. Mike Milinkovich \"Both Red Hat and [parent company] IBM are going to be interested and willing to support the public and free maintenance of these Java LTS releases for a longer period than Oracle [intended],\" he said. Red Hat's OpenJDK contributions began in 2007 with the IcedTea integration project. The company was a steward of OpenJDK 7, which came out in 2011, and of OpenJDK 6, from 2013 until 2017. The OpenJDK community selected the company's technical lead on Java, Andrew Haley, as project lead for OpenJDK 8 and OpenJDK 11 in February 2019. He has been active on the OpenJDK governing board for seven years. At the same time, Red Hat's OpenJDK 11 stewardship likely serves Oracle's interests. For starters, this move potentially slows the trend toward new OpenJDK distributions from companies such as Amazon, SAP and Alibaba. \"There are so many other companies jumping on the OpenJDK support bandwagon, which is nice to see, but very few of them really have the expertise to do full-stack support of OpenJDK,\" said an enterprise Java developer for a systems provider, who asked not to be named. \"I feel far better about 8 and 11 being under Red Hat's stewardship than Oracle or almost any of the others. They've done more to keep Java 'honest' than just about anyone else.\" Meanwhile, Oracle has little interest in older versions of Java, other than to charge customers for them, and it's happy to let Red Hat fix and maintain OpenJDK forks, said Cameron Purdy, CEO of xqiz.it, a cloud software startup in Lexington, Mass., and former senior vice president of development at Oracle. \"Companies like IBM and Red Hat are basically forced to do this. Otherwise, they cannot provide long-term support for their own products,\" he said. Red Hat takes over Companies like IBM and Red Hat are basically forced to [fix and maintain OpenJDK forks]. Otherwise, they cannot provide long-term support for their own products. Cameron PurdyCEO, xqiz.it With its stewardship of OpenJDK 8 and OpenJDK 11, Red Hat will maintain releases and determine what it does or doesn't add. That is important, because others -- including AWS, Azul Systems, AdoptOpenJDK and SAP -- will also base their releases on OpenJDK 11. Red Hat will also provide bug and security fixes, which is a major benefit for users of a supported OpenJDK environment. Unsupported environments are unlikely to be current on security. \"There will not be any radical changes to OpenJDK 8, which is in maintenance mode, or OpenJDK 11. But Red Hat will ensure long-term support for each,\" said John Doyle, senior principal product manager at Red Hat, based in Raleigh, N.C. Red Hat also plans to support and enable more innovation in Java. For instance, the company leads the development of the Shenandoah garbage collector that is in OpenJDK 12. It's also facilitating the use of OpenJDK on Microsoft Windows, with commercial support for OpenJDK on Windows released in December 2018. Within the next few weeks, Red Hat will release OpenJDK in a Microsoft installer and distribute IcedTea-Web, a free software implementation of Java Web Start and the Java web browser plugin, as part of the Windows OpenJDK distribution. Beyond Java Despite Java's continued popularity, some believe the programming language's best days are behind it. \"While still important and definitely not legacy, Java is not one of today's growth technologies,\" said Torsten Volk, an analyst at Enterprise Management Associates in Boulder, Colo. \"All of these traditional Java vendors should be focusing on pivoting toward today's fast-growing stuff, such as Python, JavaScript and Node.js.\" Dig Deeper on Java performance tuning Microsoft previews OpenJDK distro to the delight of devs By: DarrylTaft AdoptOpenJDK moves to Eclipse Foundation as Adoptium By: DarrylTaft Oracle moves OpenJDK to Git and GitHub By: DarrylTaft Oracle's release cadence opens door for Java support rivals By: DarrylTaft ",
        "_version_":1718441027949297664},
      {
        "story_id":21427363,
        "story_author":"yogthos",
        "story_descendants":5,
        "story_score":116,
        "story_time":"2019-11-02T12:50:55Z",
        "story_title":"Show HN: Open sourcing a data flow engine for Clojure/Script",
        "search":["Show HN: Open sourcing a data flow engine for Clojure/Script",
          "https://domino-clj.github.io/",
          "See here for interactive documentation.Domino is a data flow engine that helps you organize the interactions between your data model and events. Domino allows you to declare your business logic using a directed acyclic graph of events and effects. Whenever an external change is transacted to the data model, the graph determines the chain of events that will be executed, and side effects triggered as a result of the computation.Without a way to formalize the interactions between different parts of the application, relationships in code become implicit. This results in code that's difficult to maintain because of the mental overhead involved in tracking these relationships. Domino makes the interactions between pieces of business logic explicit and centralized.Domino explicitly separates logic that makes changes to the data model from side effectful functions. Business logic functions in Domino explicitly declare how they interact with the data model by declaring their inputs and outputs. Domino then uses these declarations to build a graphs of related events. This approach handles cascading business logic out of the box, and provides a data specification for relationships in code. Once the changes are transacted, the effectful functions are called against the new state.Version 0.4.0Version 0.4.0 is in pre-alpha and is not on the master branch. We do not recommend using it at this point (or even looking at it really). Please continue to use version 0.3.3 for the time being.ConceptsDomino consists of three main concepts:1. ModelThe model represents the paths within an EDN data structure. These paths will typically represent fields within a document. Each path entry is a tuple where the first value is the path segment, and the second value is the metadata associated with it. If the path is to be used for effects and/or events, the metadata must contain the :id key.For example, [:amount {:id :amount}] is the path entry to the :amount key within the data model and can be referenced in your events and effects as :amount (defined by the :id). You can nest paths within each other, such as the following model definition:[[:patient [:first-name {:id :fname}]]] 2. EventsThe events define the business logic associated with the changes of the model. Whenever a value is transacted, associated events are computed. Events are defined by three keys; an :inputs vector, an :outputs vector, and a :handler function.The handler accepts three arguments: a context containing the current state of the engine, a list of the input values, and a list of the output values. The function should produce a vector of outputs matching the declared :outputs key. For example:{:inputs [:amount] :outputs [:total] :handler (fn [ctx {:keys [amount]} {:keys [total]}] {:total (+ total amount)})} Domino also provides a domino.core/event helper for declaring events, so the above event can also be written as follows:(domino.core/event [ctx {:keys [amount]} {:keys [total]}] {:total (+ total amount)}) The macro requires that the :keys destructuring syntax is used for input and outputs, and expands the the event map with the :inputs and :outputs keys being inferred from the ones specified using the :keys in the event declaration.It's also possible to declare async events by providing the :async? key, e.g:{:async? true :inputs [:amount] :outputs [:total] :handler (fn [ctx {:keys [amount]} {:keys [total]} callback] (callback {:total (+ total amount)}))} Async event handler takes an additional argument that specifies the callback function that should be called with the result.3. EffectsEffects are used for effectful operations, such as IO, that happen at the edges of the computation. The effects do not cascade. An effect can contain the following keys::id - optional unique identifier for the event:inputs - optional set of inputs that trigger the event to run when changed:outputs - optional set of outpus that the event will produce when running the handler:handler - a function that handles the business logic for the effectIncoming EffectsEffects that declare :outputs are used to generate the initial input to the engine. For example, an effect that injects a timestamp can look as follows:{:id :timestamp :outputs [:ts] :handler (fn [_ {:keys [ts]}] {:ts (.getTime (java.util.Date.))})} The effect has an :id key specifying the unique identifier that is used be trigger the event by calling the domino.core/trigger-effects function. This function accepts a collection of event ids, e.g: (trigger-effects ctx [:timestamp]).The handler accepts two arguments: a context containing the current state of the engine, and a list of output values.Outgoing EffectsEffects that declare :inputs will be run after events have been transacted and the new context is produced. These effects are defined as a map of :inputs and a :handler function.The handler accepts two arguments: a context containing the current state of the engine, and a list of input values. For example:{:inputs [:total] :handler (fn [ctx {:keys [total]}] (when (> total 1337) (println \"Woah. That's a lot.\")))} Usage1. Require domino.core (require '[domino.core :as domino]) 2. Declare your schemaLet's take a look at a simple engine that accumulates a total. Whenever an amount is set, this value is added to the current value of the total. If the total exceeds 1337 at any point, it prints out a statement that says \"Woah. That's a lot.\"(def schema {:model [[:amount {:id :amount}] [:total {:id :total}]] :events [{:id :update-total :inputs [:amount] :outputs [:total] :handler (fn [ctx {:keys [amount]} {:keys [total]}] {:total (+ total amount)})}] :effects [{:inputs [:total] :handler (fn [ctx {:keys [total]}] (when (> total 1337) (js/alert \"Woah. That's a lot.\")))}]}) This schema declaration is a map containing three keys:The :model key declares the shape of the data model used by Domino.The :events key contains pure functions that represent events that are triggered when their inputs change. The events produce updated values that are persisted in the state.The :effects key contains the functions that produce side effects based on the updated state.Using a unified model referenced by the event functions allows us to easily tell how a particular piece of business logic is triggered.The event engine generates a direct acyclic graph (DAG) based on the :input keys declared by each event that's used to compute the new state in a transaction. This approach removes any ambiguity regarding when and how business logic is executed.Domino explicitly separates the code that modifies the state of the data from the code that causes side effects. This encourages keeping business logic pure and keeping the effects at the edges of the application.3. Initialize the engineThe schema that we declared above provides a specification for the internal data model and the code that operates on it. Once we've created a schema, we will need to initialize the data flow engine. This is done by calling the domino/initialize function. This function can be called by providing a schema along with an optional initial state map. In our example, we will give it the schema that we defined above, and an initial value for the state with the :total set to 0.(def ctx (atom (domino/initialize schema {:total 0}))) Calling the initialize function creates a context ctx that's used as the initial state for the engine. The context will contain the model, events, effects, event graph, and db (state). In our example we use an atom in order to easily update the state of the engine.4. Transact your external data changesWe can update the state of the data by calling domino/transact that accepts the current ctx along with an inputs vector, returning the updated ctx. The input vector is a collection of path-value pairs. For example, to set the value of :amount to 10, you would pass in the following input vector [[[:amount] 10]].(swap! ctx domino/transact [[[:amount] 10]]) The updated ctx contains :domino.core/change-history key which is a simple vector of all the changes as they were applied to the data in execution order of the events that were triggered.(:domino.core/change-history @ctx) We can see the new context contains the updated total amount and the change history shows the order in which the changes were applied.The :domino.core/db key in the context will contain the updated state reflecting the changes applied by running the events.(:domino.core/db @ctx) Finally, let's update the :amount to a value that triggers an effect.(require '[reagent.core :as reagent]) (defn button [] [:button {:on-click #(swap! ctx domino/transact [[[:amount] 2000]])} \"trigger effect\"]) (reagent/render-component [button] js/klipse-container) InterceptorsDomino provides the ability to add interceptors pre and post event execution. Interceptors are defined in the schema's model. If there are multiple interceptors applicable, they are composed together.In the metadata map for a model key, you can add a :pre and :post key to define these interceptors. Returning a nil value from an interceptor will short circuit execution. For example, we could check if the context is authorized before running the events as follows:(let [ctx (domino/initialize {:model [[:foo {:id :foo :pre [(fn [handler] (fn [ctx inputs outputs] ;; only run the handler if ctx contains ;; :authorized key (when (:authorized ctx) (handler ctx inputs outputs))))] :post [(fn [handler] (fn [result] (handler (update result :foo #(or % -1)))))]}]] :events [{:inputs [:foo] :outputs [:foo] :handler (fn [ctx {:keys [foo]} outputs] {:foo (inc foo)})}]})] (map :domino.core/db [(domino/transact ctx [[[:foo] 0]]) (domino/transact (assoc ctx :authorized true) [[[:foo] 0]])])) Triggering EffectsEffects can act as inputs to the data flow engine. For example, this might happen when a button is clicked and you want a value to increment. This can be accomplished with a call to trigger-effects.trigger-effects takes a list of effects that you would like trigger and calls transact with the current state of the data from all the inputs of the effects. For example:(let [ctx (domino.core/initialize {:model [[:total {:id :total}]] :effects [{:id :increment-total :outputs [:total] :handler (fn [_ current-state] (update current-state :total inc))}]} {:total 0})] (:domino.core/db (domino.core/trigger-effects ctx [:increment-total]))) This wraps up everything you need to know to start using Domino. You can see a more detailed example using Domino with re-frame here.Possible Use CasesUI state managementFSMReactive systems / spreadsheet-like modelsExample Appdemo applications can be found hereInspirationsre-framejavelinreititLicenseCopyright 2019Distributed under the Eclipse Public License either version 1.0 or (at your option) any later version. "],
        "story_type":"ShowHN",
        "url_raw":"https://domino-clj.github.io/",
        "comments.comment_id":[21428855,
          21430023],
        "comments.comment_author":["sundbry",
          "thom"],
        "comments.comment_descendants":[1,
          0],
        "comments.comment_time":["2019-11-02T17:24:48Z",
          "2019-11-02T20:55:03Z"],
        "comments.comment_text":["Very neat. Looks like something similar to a lightweight Onyx. Can it handle async event handlers in this version?",
          "This could be cool, I was sad when nothing came of the old dataflow project in Clojure contrib. Readers might also be interested in:<p><a href=\"https://github.com/sixthnormal/clj-3df\" rel=\"nofollow\">https://github.com/sixthnormal/clj-3df</a><p>Which offers Datalog queries that deliver you fast, minimal diffs of data upon updates from external sources (e.g. Kafka). Build on differential dataflow, which is very cool."],
        "id":"1f968037-06eb-418b-8e1d-fc4d653df5db",
        "url_text":"See here for interactive documentation.Domino is a data flow engine that helps you organize the interactions between your data model and events. Domino allows you to declare your business logic using a directed acyclic graph of events and effects. Whenever an external change is transacted to the data model, the graph determines the chain of events that will be executed, and side effects triggered as a result of the computation.Without a way to formalize the interactions between different parts of the application, relationships in code become implicit. This results in code that's difficult to maintain because of the mental overhead involved in tracking these relationships. Domino makes the interactions between pieces of business logic explicit and centralized.Domino explicitly separates logic that makes changes to the data model from side effectful functions. Business logic functions in Domino explicitly declare how they interact with the data model by declaring their inputs and outputs. Domino then uses these declarations to build a graphs of related events. This approach handles cascading business logic out of the box, and provides a data specification for relationships in code. Once the changes are transacted, the effectful functions are called against the new state.Version 0.4.0Version 0.4.0 is in pre-alpha and is not on the master branch. We do not recommend using it at this point (or even looking at it really). Please continue to use version 0.3.3 for the time being.ConceptsDomino consists of three main concepts:1. ModelThe model represents the paths within an EDN data structure. These paths will typically represent fields within a document. Each path entry is a tuple where the first value is the path segment, and the second value is the metadata associated with it. If the path is to be used for effects and/or events, the metadata must contain the :id key.For example, [:amount {:id :amount}] is the path entry to the :amount key within the data model and can be referenced in your events and effects as :amount (defined by the :id). You can nest paths within each other, such as the following model definition:[[:patient [:first-name {:id :fname}]]] 2. EventsThe events define the business logic associated with the changes of the model. Whenever a value is transacted, associated events are computed. Events are defined by three keys; an :inputs vector, an :outputs vector, and a :handler function.The handler accepts three arguments: a context containing the current state of the engine, a list of the input values, and a list of the output values. The function should produce a vector of outputs matching the declared :outputs key. For example:{:inputs [:amount] :outputs [:total] :handler (fn [ctx {:keys [amount]} {:keys [total]}] {:total (+ total amount)})} Domino also provides a domino.core/event helper for declaring events, so the above event can also be written as follows:(domino.core/event [ctx {:keys [amount]} {:keys [total]}] {:total (+ total amount)}) The macro requires that the :keys destructuring syntax is used for input and outputs, and expands the the event map with the :inputs and :outputs keys being inferred from the ones specified using the :keys in the event declaration.It's also possible to declare async events by providing the :async? key, e.g:{:async? true :inputs [:amount] :outputs [:total] :handler (fn [ctx {:keys [amount]} {:keys [total]} callback] (callback {:total (+ total amount)}))} Async event handler takes an additional argument that specifies the callback function that should be called with the result.3. EffectsEffects are used for effectful operations, such as IO, that happen at the edges of the computation. The effects do not cascade. An effect can contain the following keys::id - optional unique identifier for the event:inputs - optional set of inputs that trigger the event to run when changed:outputs - optional set of outpus that the event will produce when running the handler:handler - a function that handles the business logic for the effectIncoming EffectsEffects that declare :outputs are used to generate the initial input to the engine. For example, an effect that injects a timestamp can look as follows:{:id :timestamp :outputs [:ts] :handler (fn [_ {:keys [ts]}] {:ts (.getTime (java.util.Date.))})} The effect has an :id key specifying the unique identifier that is used be trigger the event by calling the domino.core/trigger-effects function. This function accepts a collection of event ids, e.g: (trigger-effects ctx [:timestamp]).The handler accepts two arguments: a context containing the current state of the engine, and a list of output values.Outgoing EffectsEffects that declare :inputs will be run after events have been transacted and the new context is produced. These effects are defined as a map of :inputs and a :handler function.The handler accepts two arguments: a context containing the current state of the engine, and a list of input values. For example:{:inputs [:total] :handler (fn [ctx {:keys [total]}] (when (> total 1337) (println \"Woah. That's a lot.\")))} Usage1. Require domino.core (require '[domino.core :as domino]) 2. Declare your schemaLet's take a look at a simple engine that accumulates a total. Whenever an amount is set, this value is added to the current value of the total. If the total exceeds 1337 at any point, it prints out a statement that says \"Woah. That's a lot.\"(def schema {:model [[:amount {:id :amount}] [:total {:id :total}]] :events [{:id :update-total :inputs [:amount] :outputs [:total] :handler (fn [ctx {:keys [amount]} {:keys [total]}] {:total (+ total amount)})}] :effects [{:inputs [:total] :handler (fn [ctx {:keys [total]}] (when (> total 1337) (js/alert \"Woah. That's a lot.\")))}]}) This schema declaration is a map containing three keys:The :model key declares the shape of the data model used by Domino.The :events key contains pure functions that represent events that are triggered when their inputs change. The events produce updated values that are persisted in the state.The :effects key contains the functions that produce side effects based on the updated state.Using a unified model referenced by the event functions allows us to easily tell how a particular piece of business logic is triggered.The event engine generates a direct acyclic graph (DAG) based on the :input keys declared by each event that's used to compute the new state in a transaction. This approach removes any ambiguity regarding when and how business logic is executed.Domino explicitly separates the code that modifies the state of the data from the code that causes side effects. This encourages keeping business logic pure and keeping the effects at the edges of the application.3. Initialize the engineThe schema that we declared above provides a specification for the internal data model and the code that operates on it. Once we've created a schema, we will need to initialize the data flow engine. This is done by calling the domino/initialize function. This function can be called by providing a schema along with an optional initial state map. In our example, we will give it the schema that we defined above, and an initial value for the state with the :total set to 0.(def ctx (atom (domino/initialize schema {:total 0}))) Calling the initialize function creates a context ctx that's used as the initial state for the engine. The context will contain the model, events, effects, event graph, and db (state). In our example we use an atom in order to easily update the state of the engine.4. Transact your external data changesWe can update the state of the data by calling domino/transact that accepts the current ctx along with an inputs vector, returning the updated ctx. The input vector is a collection of path-value pairs. For example, to set the value of :amount to 10, you would pass in the following input vector [[[:amount] 10]].(swap! ctx domino/transact [[[:amount] 10]]) The updated ctx contains :domino.core/change-history key which is a simple vector of all the changes as they were applied to the data in execution order of the events that were triggered.(:domino.core/change-history @ctx) We can see the new context contains the updated total amount and the change history shows the order in which the changes were applied.The :domino.core/db key in the context will contain the updated state reflecting the changes applied by running the events.(:domino.core/db @ctx) Finally, let's update the :amount to a value that triggers an effect.(require '[reagent.core :as reagent]) (defn button [] [:button {:on-click #(swap! ctx domino/transact [[[:amount] 2000]])} \"trigger effect\"]) (reagent/render-component [button] js/klipse-container) InterceptorsDomino provides the ability to add interceptors pre and post event execution. Interceptors are defined in the schema's model. If there are multiple interceptors applicable, they are composed together.In the metadata map for a model key, you can add a :pre and :post key to define these interceptors. Returning a nil value from an interceptor will short circuit execution. For example, we could check if the context is authorized before running the events as follows:(let [ctx (domino/initialize {:model [[:foo {:id :foo :pre [(fn [handler] (fn [ctx inputs outputs] ;; only run the handler if ctx contains ;; :authorized key (when (:authorized ctx) (handler ctx inputs outputs))))] :post [(fn [handler] (fn [result] (handler (update result :foo #(or % -1)))))]}]] :events [{:inputs [:foo] :outputs [:foo] :handler (fn [ctx {:keys [foo]} outputs] {:foo (inc foo)})}]})] (map :domino.core/db [(domino/transact ctx [[[:foo] 0]]) (domino/transact (assoc ctx :authorized true) [[[:foo] 0]])])) Triggering EffectsEffects can act as inputs to the data flow engine. For example, this might happen when a button is clicked and you want a value to increment. This can be accomplished with a call to trigger-effects.trigger-effects takes a list of effects that you would like trigger and calls transact with the current state of the data from all the inputs of the effects. For example:(let [ctx (domino.core/initialize {:model [[:total {:id :total}]] :effects [{:id :increment-total :outputs [:total] :handler (fn [_ current-state] (update current-state :total inc))}]} {:total 0})] (:domino.core/db (domino.core/trigger-effects ctx [:increment-total]))) This wraps up everything you need to know to start using Domino. You can see a more detailed example using Domino with re-frame here.Possible Use CasesUI state managementFSMReactive systems / spreadsheet-like modelsExample Appdemo applications can be found hereInspirationsre-framejavelinreititLicenseCopyright 2019Distributed under the Eclipse Public License either version 1.0 or (at your option) any later version. ",
        "_version_":1718441070297088002},
      {
        "story_id":21180154,
        "story_author":"user5994461",
        "story_descendants":163,
        "story_score":122,
        "story_time":"2019-10-07T12:52:56Z",
        "story_title":"Perl is dying",
        "search":["Perl is dying",
          "https://thehftguy.com/2019/10/07/perl-is-dying-quick-could-be-extinct-by-2023/",
          "Some personal reflections about the evolution and death or programming languages. By the forces a circumstances, I had to reverse engineer and decommission a few Perl scripts in an old company earlier this year. That makes one wonder about who else is still using Perl? if any? Cant remember the last time Ive heard about it. What is Perl? If you want to see the numbers right away, scroll down to the next section. For the young readers who may have never heard of it. Perl was a popular programming language about 30 years ago. COBOL 1959BASIC 1964C++ 1985Perl 1987Python 1989Delphi 1995PHP 1995JavaScript 1995Java 1996C# 2001Ruby on Rails 2005 Having used many languages over decades, more or less professionally (C, C++, Java, python, Haskell, Ada, PHP). Perl is truly unique in that it is genuinely unique and exotic. For example, it doesnt support functions with arguments, well, not like what exists today in mainstream languages. Its also based on an extensive use of symbols whereas todays languages are more about letters (keywords, variable names, function names, etc). I think its fair to say that Perl is about magic symbols that do stuff, so much that 93% of random characters are valid Perl programs. Here are some sample cgi scripts in Perl, Python and PHP for comparison. Straight copy/paste from perl.com and stack overflow. Can you understand what they do? Plot-Twist: Stack overflow answers never work and this time is no exception. Can you find the bug(s)? (answer at the end) Source: https://www.perl.com/article/perl-and-cgi/ #!/usr/bin/perl use strict; use warnings; use CGI; my $cgi = CGI->new; my %param = map { $_ => scalar $cgi->param($_) } $cgi->param() ; print $cgi->header( -type => 'text/plain' ); print qq{PARAM:\\N}; for my $k ( sort keys %param ) { print join \": \", $k, $param{$k}; print \"\\n\"; } # PARAM: # foo: bar Source: https://stackoverflow.com/questions/3582398/getting-http-get-arguments-in-python import cgi import cgitb; cgitb.enable() # Optional; for debugging only print \"Content-Type: text/html\" print \"\" arguments = cgi.FieldStorage() for i in arguments.keys(): print arguments[i].value Source: https://stackoverflow.com/questions/2160382/how-do-i-grab-all-parameters-from-a-url-and-print-it-out-in-php <?php foreach($_GET as $key => $value){ echo $key . \" : \" . $value . \"<br />\\r\\n\"; } ?> What was the bug? HTTP headers MUST be separated by \\r\\n line ending. These scripts do a basic print() generating a \\n only when running on Linux. The output is thus invalid, although there is a chance the CGI server is robust to that specific issue. Note that PHP doesnt expose raw headers and avoid this class of issues entirely. Stats The below charts show programming languages popularity relative to one another. Data from Google trend. Most Common Languages Link to Google Trends: Perl, PHP, Python, Java, C++ Interesting finding from this chart, Perl was somewhat popular 2 decades ago, on-par with C++. Certainly popular enough to be considered among the major programming languages at the time. Perl has been on the decline for a while. Its reaching zero market share on this chart, what youre seeing from 2018 onward is a single pixel as google trend is rounding up a near-zero value. I think its fair to say that Perl can be considered a dead language. Definitely not something to use for new projects. Niche Languages Lets compare Perl to other niche languages with low adoption. Link to Google Trend: Delphi, Haskell, COBOL, Perl, Rust Well, still more popular than COBOL! COBOL: A programming language running on mainframes used by financial applications. Specifically designed with support for integer arithmetic, transactions and records. Often considered legacy but really hard to migrate from because of how large, old and critical the typical application is. Delphi: A programming language and IDE, based on Pascal, like C++ is based on C. Designed to write desktop applications. The IDE had a GUI editor to drag and drop widgets, very efficient and easy to use, maybe the first to provide that. Theres a good story on how it came to be eclipsed by C++, C# and Java over 20 years. Meanwhile in a Perl conference. Lifetime Projection Curious whats dying faster between COBOL, Delphi and Perl? Lets find out. The graph below shows the linear and the polynomial trends for each programming language. Note that both regression curves happen to overlap for COBOL. A stable and predictable language indeed. If we are to believe the linear projections, Perl could be extinct by 2023, a few months after Delphi. COBOL would outlive both by far and go on to 2030. Of course, languages dont go extinct. Maybe think of it as having order(s) of magnitude less developers, up to a stage where theres really not much left. Typically those making/selling the language/tooling will be among the last to know about it, as well as a few curious googlers. All unmaintained software decay, because of hardware and software evolution, up to a point where it suddenly stops working and its unfixable. For example, the executable doesnt run on Windows 12 or Ubuntu 24 for whatever reason, looking into it, turns out the compiler doesnt run anymore either, ouch. Software lifecycle is coupled to the lifecycle of the platform its developed with and runs on, so there is strong risk associated with little used or divested platforms. When will Perl go away? Perl and Python have always come pre-installed on Linux. If I remember well, there was actually a Posix specification or something that had it as a requirement. Times have changed though and theyre both actively being removed from major OS. Apple has announced officially that all interpreters are deprecated (Perl, Python, Ruby) and will not be available with the OS in the next version of MacOS. RedHat has announced officially that python is not setup out-of-the-box in RHEL 8 and future versions. Its only a matter of time before other distributions follow. The next logical step is for Perl to go away, the only question is when? Apple announcement: Future versions of macOS wont include scripting language runtimes by default + discussion on Hacker NewsRedHat announcement: Python in RHEL 8 Conclusion If youre thinking of learning Perl or starting a new project in Perl, you might want to reconsider. Nothing personal with Perl. Just doing stats. Next, need to figure out which one took longer to build, Microsoft Windows or the Great Pyramid. "],
        "story_type":"Normal",
        "url_raw":"https://thehftguy.com/2019/10/07/perl-is-dying-quick-could-be-extinct-by-2023/",
        "comments.comment_id":[21181130,
          21181215],
        "comments.comment_author":["tadzik_",
          "robinhouston"],
        "comments.comment_descendants":[8,
          0],
        "comments.comment_time":["2019-10-07T14:21:53Z",
          "2019-10-07T14:27:15Z"],
        "comments.comment_text":["> That makes one wonder about who else is still using Perl? if any? Cant remember the last time Ive heard about it.<p>Perl, like many other languages, has its own bubble. I'm using Perl daily at work. I used Python for a while, but the Perl job market was just too nice to pass. I have a backlog of clients interested in Perl work longer than I can handle, and they don't mind my requirements because they have no one to replace me with.<p>Is that a sign of a dying language? Probably yes. But to be honest, if it's about to become the next COBOL then I'm looking at a lifetime of profitable work with little to no competition :)<p>BUT:\n> Nothing personal with Perl. Just doing stats<p>Perhaps a little bit of research could help your point as well. The CGI example you mentioned is broken, but is also using a module (CGI.pm) that's been bad and wrong for so long that even Perl, with its eternal love for backwards compatibility has removed it from Perl Core. That's a sign of Perl's marketing problem, yes, but it's hardly a real indication of a language that's too old to care about fixing itself.",
          "I love Perl. I first used it in 1994, around the time the early versions of Perl 5 were released. Its hard to overstate what a breath of fresh air it was compared to the incumbent languages at the time, which were either very low-level (mainly C, and increasingly C++, at least in the Unix world) or else brittle and limited (various flavours of shell script, sed, awk).<p>Perl was a joy to use. Things that would have needed dozens of bug-prone lines of C could be done in just a few characters. Regular expressions and associative arrays were first-class language features! And, with the advent of Perl 5, there was enough scope for structuring code in a modular way that it wasnt insane to write pretty large and serious programs.<p>The culture was delightful too. In those days it was centred on Usenet and mailing lists, and the general attitude was freewheeling and fun. When the first user group was set up in London, where I was living by then, I signed up immediately. Thats still the most fun Ive ever had hanging out with programmers, and Perl was the glue that bound us together.<p>At some point I joined the developers mailing list (perl5-porters), just to see what they were up to. After a while I started to contribute: bug fixes at first, and later language features. That was fun.<p>So, Perl has been a big part of my programming life. But I havent really used it in anger for about 15 years. I still write Perl one-liners on the command line for ad hoc data processing, and actually I think Perl is still the best available tool for that, although younger coders tend not to be familiar with it any more.<p>But as for writing real programs? The world has moved on. There are newer languages that have taken the good ideas from Perl, but without so many confusing idiosyncrasies.<p>Perl 6 is another story, one that I didnt have much part in. Its a fun bundle of ideas, and maybe some of those ideas will spread and be useful elsewhere, but I doubt it will ever catch on itself.<p>Ill always <i>love</i> Perl, but I doubt Ill ever write another serious program in it  except maybe out of nostalgia."],
        "id":"9bf5a1f5-43bb-4fe5-b9cf-f6a4e4c3a827",
        "url_text":"Some personal reflections about the evolution and death or programming languages. By the forces a circumstances, I had to reverse engineer and decommission a few Perl scripts in an old company earlier this year. That makes one wonder about who else is still using Perl? if any? Cant remember the last time Ive heard about it. What is Perl? If you want to see the numbers right away, scroll down to the next section. For the young readers who may have never heard of it. Perl was a popular programming language about 30 years ago. COBOL 1959BASIC 1964C++ 1985Perl 1987Python 1989Delphi 1995PHP 1995JavaScript 1995Java 1996C# 2001Ruby on Rails 2005 Having used many languages over decades, more or less professionally (C, C++, Java, python, Haskell, Ada, PHP). Perl is truly unique in that it is genuinely unique and exotic. For example, it doesnt support functions with arguments, well, not like what exists today in mainstream languages. Its also based on an extensive use of symbols whereas todays languages are more about letters (keywords, variable names, function names, etc). I think its fair to say that Perl is about magic symbols that do stuff, so much that 93% of random characters are valid Perl programs. Here are some sample cgi scripts in Perl, Python and PHP for comparison. Straight copy/paste from perl.com and stack overflow. Can you understand what they do? Plot-Twist: Stack overflow answers never work and this time is no exception. Can you find the bug(s)? (answer at the end) Source: https://www.perl.com/article/perl-and-cgi/ #!/usr/bin/perl use strict; use warnings; use CGI; my $cgi = CGI->new; my %param = map { $_ => scalar $cgi->param($_) } $cgi->param() ; print $cgi->header( -type => 'text/plain' ); print qq{PARAM:\\N}; for my $k ( sort keys %param ) { print join \": \", $k, $param{$k}; print \"\\n\"; } # PARAM: # foo: bar Source: https://stackoverflow.com/questions/3582398/getting-http-get-arguments-in-python import cgi import cgitb; cgitb.enable() # Optional; for debugging only print \"Content-Type: text/html\" print \"\" arguments = cgi.FieldStorage() for i in arguments.keys(): print arguments[i].value Source: https://stackoverflow.com/questions/2160382/how-do-i-grab-all-parameters-from-a-url-and-print-it-out-in-php <?php foreach($_GET as $key => $value){ echo $key . \" : \" . $value . \"<br />\\r\\n\"; } ?> What was the bug? HTTP headers MUST be separated by \\r\\n line ending. These scripts do a basic print() generating a \\n only when running on Linux. The output is thus invalid, although there is a chance the CGI server is robust to that specific issue. Note that PHP doesnt expose raw headers and avoid this class of issues entirely. Stats The below charts show programming languages popularity relative to one another. Data from Google trend. Most Common Languages Link to Google Trends: Perl, PHP, Python, Java, C++ Interesting finding from this chart, Perl was somewhat popular 2 decades ago, on-par with C++. Certainly popular enough to be considered among the major programming languages at the time. Perl has been on the decline for a while. Its reaching zero market share on this chart, what youre seeing from 2018 onward is a single pixel as google trend is rounding up a near-zero value. I think its fair to say that Perl can be considered a dead language. Definitely not something to use for new projects. Niche Languages Lets compare Perl to other niche languages with low adoption. Link to Google Trend: Delphi, Haskell, COBOL, Perl, Rust Well, still more popular than COBOL! COBOL: A programming language running on mainframes used by financial applications. Specifically designed with support for integer arithmetic, transactions and records. Often considered legacy but really hard to migrate from because of how large, old and critical the typical application is. Delphi: A programming language and IDE, based on Pascal, like C++ is based on C. Designed to write desktop applications. The IDE had a GUI editor to drag and drop widgets, very efficient and easy to use, maybe the first to provide that. Theres a good story on how it came to be eclipsed by C++, C# and Java over 20 years. Meanwhile in a Perl conference. Lifetime Projection Curious whats dying faster between COBOL, Delphi and Perl? Lets find out. The graph below shows the linear and the polynomial trends for each programming language. Note that both regression curves happen to overlap for COBOL. A stable and predictable language indeed. If we are to believe the linear projections, Perl could be extinct by 2023, a few months after Delphi. COBOL would outlive both by far and go on to 2030. Of course, languages dont go extinct. Maybe think of it as having order(s) of magnitude less developers, up to a stage where theres really not much left. Typically those making/selling the language/tooling will be among the last to know about it, as well as a few curious googlers. All unmaintained software decay, because of hardware and software evolution, up to a point where it suddenly stops working and its unfixable. For example, the executable doesnt run on Windows 12 or Ubuntu 24 for whatever reason, looking into it, turns out the compiler doesnt run anymore either, ouch. Software lifecycle is coupled to the lifecycle of the platform its developed with and runs on, so there is strong risk associated with little used or divested platforms. When will Perl go away? Perl and Python have always come pre-installed on Linux. If I remember well, there was actually a Posix specification or something that had it as a requirement. Times have changed though and theyre both actively being removed from major OS. Apple has announced officially that all interpreters are deprecated (Perl, Python, Ruby) and will not be available with the OS in the next version of MacOS. RedHat has announced officially that python is not setup out-of-the-box in RHEL 8 and future versions. Its only a matter of time before other distributions follow. The next logical step is for Perl to go away, the only question is when? Apple announcement: Future versions of macOS wont include scripting language runtimes by default + discussion on Hacker NewsRedHat announcement: Python in RHEL 8 Conclusion If youre thinking of learning Perl or starting a new project in Perl, you might want to reconsider. Nothing personal with Perl. Just doing stats. Next, need to figure out which one took longer to build, Microsoft Windows or the Great Pyramid. ",
        "_version_":1718441064679866368},
      {
        "story_id":21833393,
        "story_author":"weinzierl",
        "story_descendants":53,
        "story_score":72,
        "story_time":"2019-12-19T09:43:21Z",
        "story_title":"Quantum Dominance, Hegemony, and Superiority",
        "search":["Quantum Dominance, Hegemony, and Superiority",
          "https://www.scottaaronson.com/blog/?p=4450",
          "Yay! Im now a Fellow of the ACM. Along with my fellow new inductee Peter Shor, who I hear is a real up-and-comer in the quantum computing field. I will seek to use this awesome responsibility to steer the ACM along the path of good rather than evil. Also, last week, I attended the Q2B conference in San Jose, where a central theme was the outlook for practical quantum computing in the wake of the first clear demonstration of quantum computational supremacy. Thanks to the folks at QC Ware for organizing a fun conference (full disclosure: Im QC Wares Chief Scientific Advisor). Ill have more to say about the actual scientific things discussed at Q2B in future posts. None of that is why youre here, though. Youre here because of the battle over quantum supremacy. A week ago, my good friend and collaborator Zach Weinersmith, of SMBC Comics, put out a cartoon with a dark-curly-haired scientist named Dr. Aaronson, whos revealed on a hot mic to be an evil quantum supremacist. Apparently a rush job, this cartoon is far from Zachs finest work. For one thing, if the character is supposed to be me, why not draw him as me, and if he isnt, why call him Dr. Aaronson? In any case, I learned from talking to Zach that the cartoons timing was purely coincidental: Zach didnt even realize what a hornets-nest he was poking with this. Ever since John Preskill coined it in 2012, quantum supremacy has been an awkward term. Much as I admire John Preskills wisdom, brilliance, generosity, and good sense, in physics as in everything elseyeah, quantum supremacy is not a term I wouldve coined, and its certainly not a hill Id choose to die on. Once it had gained common currency, though, I sort of took a liking to it, mostly because I realized that I could mine it for dark one-liners in my talks. The thinking was: even as white supremacy was making its horrific resurgence in the US and around the world, here we were, physicists and computer scientists and mathematicians of varied skin tones and accents and genders, coming together to pursue a different and better kind of supremacya small reflection of the better world that we still believed was possible. You might say that we were reclaiming the word supremacywhich, after all, just means a state of being supremefor something non-sexist and non-racist and inclusive and good. In the world of 2019, alas, perhaps it was inevitable that people wouldnt leave things there. My first intimation came a month ago, when Leonie Muecksomeone who Id gotten to know and like when she was an editor at Nature handling quantum information papersemailed me about her view that our community should abandon the term quantum supremacy, because of its potential to make women and minorities uncomfortable in our field. She advocated using quantum advantage instead. So I sent Leonie back a friendly reply, explaining that, as the father of a math-loving 6-year-old girl, I understood and shared her concernsbut also, that I didnt know an alternative term that really worked. See, its like this. Preskill meant quantum supremacy to refer to a momentous event that seemed likely to arrive in a matter of years: namely, the moment when programmable quantum computers would first outpace the ability of the fastest classical supercomputers on earth, running the fastest algorithms known by humans, to simulate what the quantum computers were doing (at least on special, contrived problems). And the historic milestone of quantum advantage? It just doesnt sound right. Plus, as many others pointed out, the term quantum advantage is already used to refer to well, quantum advantages, which might fall well short of supremacy. But one could go further. Suppose we did switch to quantum advantage. Couldnt that term, too, remind vulnerable people about the unfair advantages that some groups have over others? Indeed, while advantage is certainly subtler than supremacy, couldnt that make it all the more insidious, and therefore dangerous? Oblivious though I sometimes am, I realized Leonie would be unhappy if I offered that, because of my wholehearted agreement, I would henceforth never again call it quantum supremacy, but only quantum superiority, quantum dominance, or quantum hegemony. But maybe you now see the problem. What word does the English language provide to describe one thing decisively beating or being better than a different thing for some purpose, and which doesnt have unsavory connotations? Ive heard quantum ascendancy, but that makes it sound like were a UFO cultwaiting to ascend, like ytterbium ions caught in a laser beam, to a vast quantum computer in the sky. Ive heard quantum inimitability (that is, inability to imitate using a classical computer), but who can pronounce that? Yesterday, my brilliant former student Ewin Tang (yes, that one) relayed to me a suggestion by Kevin Tian: quantum eclipse (that is, the moment when quantum computers first eclipse classical ones for some task). But would one want to speak of a quantum eclipse experiment? And shouldnt we expect that, the cuter and cleverer the term, the harder it will be to use unironically? In summary, while someone might think of a term so inspired that it immediately supplants quantum supremacy (and while I welcome suggestions), I currently regard it as an open problem. Anyway, evidently dissatisfied with my response, last week Leonie teamed up with 13 others to publish a letter in Nature, which was originally entitled Supremacy is for racistsuse quantum advantage,' but whose title I see has now been changed to the less inflammatory Instead of supremacy use quantum advantage.' Leonies co-signatories included four of my good friends and colleagues: Alan Aspuru-Guzik, Helmut Katzgraber, Anne Broadbent, and Chris Granade (the last of whom got started in the field by helping me edit Quantum Computing Since Democritus). (Update: Leonie pointed me to a longer list of signatories here, at their website called quantumresponsibility.org. A few names that might be known to Shtetl-Optimized readers are Andrew White, David Yonge-Mallo, Debbie Leung, Matt Leifer, Matthias Troyer.) Their letter says: The community claims that quantum supremacy is a technical term with a specified meaning. However, any technical justification for this descriptor could get swamped as it enters the public arena after the intense media coverage of the past few months.In our view, supremacy has overtones of violence, neocolonialism and racism through its association with white supremacy. Inherently violent language has crept into other branches of science as well in human and robotic spaceflight, for example, terms such as conquest, colonization and settlement evoke theterra nulliusarguments of settler colonialism and must be contextualized against ongoing issues of neocolonialism.Instead, quantum computing should be an open arena and an inspiration for a new generation of scientists. When I did an Ask Me Anything session, as the closing event at Q2B, Sarah Kaiser asked me to comment on the Nature petition. So I repeated what Id said in my emailed response to Leonierunning through the problems with each proposed alternative term, talking about the value of reclaiming the word supremacy, and mostly just trying to diffuse the tension by getting everyone laughing together. Sarah later tweeted that she was really disappointed in my response. Then the Wall Street Journal got in on the action, with a brief editorial (warning: paywalled) mocking the Nature petition: There it is, folks: Mankind has hit quantum wokeness. Our species, akin to Schrdingers cat, is simultaneously brilliant and brain-dead. We built a quantum computer and then argued about whether the write-up was linguistically racist.Taken seriously, the renaming game will never end. First put a Sharpie to the Supremacy Clause of the U.S. Constitution, which says federal laws trump state laws. Cancel Matt Damon for his 2004 role in The Bourne Supremacy. Make the Air Force give up the term air supremacy. Tell lovers of supreme pizza to quit being so chauvinistic about their toppings. Please inform Motown legend Diana Ross that the Supremes are problematic.The quirks of quantum mechanics, some people argue, are explained by the existence of many universes. How did we get stuck in this one? Steven Pinker also weighed in, with a linguistically-informed tweetstorm: This sounds like something from The Onion but actually appeared in Nature It follows the wokified stigmatization of other innocent words, like House Master (now, at Harvard, Residential Dean) and NIPS (Neural Information Processing Society, now NeurIPS). Its a familiar linguistic phenomenon, a lexical version of Greshams Law: bad meanings drive good ones out of circulation. Examples: the doomed niggardly (no relation to the n-word) and the original senses of cock, ass, prick, pussy, and booty. Still, the prissy banning of words by academics should be resisted. It dumbs down understanding of language: word meanings are conventions, not spells with magical powers, and all words have multiple senses, which are distinguished in context. Also, it makes academia a laughingstock, tars the innocent, and does nothing to combat actual racism & sexism. Others had a stronger reaction. Curtis Yarvin, better known as Mencius Moldbug, is one of the founders of neoreaction (and a significant influence on Steve Bannon, Michael Anton, and other Trumpists). Regulars might remember that Yarvin argued with me in Shtetl-Optimizeds comment section, under a post in which I denounced Trumps travel ban and its effects on my Iranian PhD student. Since then, Yarvin has sent me many emails, which have ranged from long to extremely long, and whose message could be summarized as: [labored breathing] Abandon your liberal Enlightenment pretensions, young Nerdwalker. Come over the Dark Side. After the supremacy is for racists letter came out in Nature, though, Yarvin sent me his shortest email ever. It was simply a link to the letter, along with the comment I knew it would come to this. He meant: What more proof do you need, young Nerdawan, that this performative wokeness is a cancer that will eventually infect everything you valueeven totally apolitical research in quantum information? And by extension, that my whole worldview, which warned of this, is fundamentally correct, while your faith in liberal academia is nave, and will be repaid only with backstabbing? In a subsequent email, Yarvin predicted that in two years, the whole community will be saying quantum advantage instead of quantum supremacy, and in five years Ill be saying quantum advantage too. As Yarvin famously wrote: Cthulhu may swim slowly. But he only swims left. So what do I really think about this epic battle for (and against) supremacy? Truthfully, half of me just wants to switch to quantum advantage right now and be done with it. As I said, I know some of the signatories of the Nature letter to be smart and reasonable and kind. They dont wish to rid the planet of everyone like me. Theyre not Amanda Marcottes or Arthur Chus. Furthermore, theres little I despise more than a meaty scientific debate devolving into a pointless semantic one, with brilliant friend after brilliant friend getting sucked into the vortex (you too?). Im strongly in the Pinkerian camp, which holds that words are just arbitrary designators, devoid of the totemic power to dictate thoughts. So if friends and colleagueseven just a few of themtell me that they find some word I use to be offensive, why not just be a mensch, apologize for any unintended hurt, switch words midsentence, and continue discussing the matter at hand? But then the other half of me wonders: once weve ceded an open-ended veto over technical terms that remind anyone of anything bad, where does it stop? How do we ever certify a word as kosher? At what point do we all get to stop arguing and laugh together? To make this worry concrete, look back at Sarah Kaisers Twitter threadthe one where she expresses disappointment in me. Below her tweet, someone remarks that, besides quantum supremacy, the word ancilla (as in ancilla qubit, a qubit used for intermediate computation or other auxiliary purposes) is problematic as well. Heres Sarahs response: I agree, but I wanted to start by focusing on the obvious one, Its harder for them to object to just one to start with, then once they admit the logic, we can expand the list (What would Curtis Yarvin say about that?) Youre probably now wondering: whats wrong with ancilla? Apparently, in ancient Rome, an ancilla was a female slave, and indeed thats the Latin root of the English adjective ancillary (as in providing support to). I confess that I hadnt known thathad you? Admittedly, once you do know, you might never again look at a Controlled-NOT gatepitilessly flipping an ancilla qubit, subject only to the whims of a nearby control qubitin quite the same way. (Ah, but the ancilla can fight back against her controller! And she doesin the Hadamard basis.) The thing is, if were gonna play this game: what about annihilation operators? Wont those need to be annihilated from physics? And what about unitary matrices? Doesnt their very name negate the multiplicity of perspectives and cultures? What about Diracs oddly-named bra/ket notation, with its limitless potential for puerile jokes, about the bra vectors displaying their contents horizontally and so forth? (Did you smile at that, you hateful pig?) What about daggers? Dont we need a less violent conjugate tranpose? Not to beat a dead horse, but once you hunt for examples, you realize that the whole dictionary is shot through with domination and brutalitythat youd have to massacre the English language to take it out. Theres nothing special about math or physics in this respect. The same half of me also thinks about my friends and colleagues who oppose claims of quantum supremacy, or even the quest for quantum supremacy, on various scientific grounds. I.e., either they dont think that the Google team achieved what it said, or they think that the task wasnt hard enough for classical computers, or they think that the entire goal is misguided or irrelevant or uninteresting. Which is finethese are precisely the arguments we should be havingexcept that Ive personally seen some of my respected colleagues, while arguing for these positions, opportunistically tack on ideological objections to the term quantum supremacy. Just to goose up their case, I guess. And I confess that every time they did this, it made me want to keep saying quantum supremacy from now till the end of timesolely to deny these colleagues a cheap and unearned victory, one they apparently felt they couldnt obtain on the merits alone. I realize that this is childish and irrational. Most of all, though, the half of me that Im talking about thinks about Curtis Yarvin and the Wall Street Journal editorial board, cackling with glee to see their worldview so dramatically confirmedas theatrical wokeness, that self-parodying modern monstrosity, turns its gaze on (of all things) quantum computing research. More red meat to fire up the baseor at least that sliver of the base nerdy enough to care. And the left, as usual, walks right into the trap, sacrificing its credibility with the outside world to pursue a runaway virtue-signaling spiral. The same half of me thinks: do we really want to fight racism and sexism? Then lets work together to assemble a broad coalition that can defeat Trump. And Jair Bolsonaro, and Viktor Orbn, and all the other ghastly manifestations of humanitys collective lizard-brain. Then, if were really fantasizing, we could liberalize the drug laws, and get contraception and loans and education to women in the Third World, and stop the systematic disenfranchisement of black voters, and open up the worlds richer, whiter, and higher-elevation countries to climate refugees, and protect the worlds remaining indigenous lands (those that didnt burn to the ground this year). In this context, the trouble with obsessing over terms like quantum supremacy is not merely that it diverts attention, while contributing nothing to fighting the worlds actual racism and sexism. The trouble is that the obsessions are actually harmful. For they make academicsalong with progressive activistslook silly. They make people think that we must not have meant it when we talked about the existential urgency of climate change and the worlds other crises. They pump oxygen into right-wing echo chambers. But its worse than ridiculous, because of the message that I fear is received by many outside the activists bubble. When you say stuff like [quantum] supremacy is for racists, whats heard might be something more like: Watch your back, you disgusting supremacist. Yes, you. You claim that you mentor women and minorities, donate to good causes, try hard to confront the demons in your own character? Ha! None of that counts for anything with us. Youll never be with-it enough to be our ally, so dont bother trying. Well see to it that youre never safe, not even in the most abstruse and apolitical fields. Well comb through your wordseven words like ancilla qubitlooking for any that we can cast as offensive by our opaque and ever-shifting standards. And once we find some, well have it within our power to end your career, and youll be reduced to groveling that we dont. Remember those popular kids who bullied you in second grade, giving you nightmares of social ostracism that persist to this day? We plan to achieve what even those bullies couldnt: to shame you with the full backing of the modern worlds moral code. See, were the good guys of this story. Its goodness itself thats branding you as racist scum. In short, I claim that the messagenot the message intended, of course, by anyone other than a Chu or a Marcotte or a SneerClubber, but the message receivedis basically a Trump campaign ad. I claim further that our civilizations current self-inflicted catastrophe will endi.e., the believers in science and reason and progress and rule of law will claw their way back to powerwhen, and only when, a generation of activists emerges that understands these dynamics as well as Barack Obama did. Wouldnt it be awesome if, five years from now, I could say to Curtis Yarvin: you were wrong? If I could say to him: my colleagues and I still use the term quantum supremacy whenever we care to, and none of us have been cancelled or ostracized for itso maybe you should revisit your paranoid theories about Cthulhu and the Cathedral and so forth? If I could say: quantum computing researchers now have bigger fish to fry than arguments over wordslike moving beyond quantum supremacy to the first useful quantum simulations, as well as the race for scalability and fault-tolerance? And even: progressive activists now have bigger fish to fry toolike retaking actual power all over the world? Anyway, as I said, thats how half of me feels. The other half is ready to switch to quantum advantage or any other serviceable term and get back to doing science. This entry was posted on Thursday, December 19th, 2019 at 3:17 am and is filed under Obviously I'm Not Defending Aaronson, Quantum, The Fate of Humanity. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site. Comment Policy: All comments are placed in moderation and reviewed prior to appearing. Comments can be left in moderation for any reason, but in particular, for ad-hominem attacks, hatred of groups of people, or snide and patronizing tone. Also: comments that link to a paper or article and, in effect, challenge me to respond to it are at severe risk of being left in moderation, as such comments place demands on my time that I can no longer meet. You'll have a much better chance of a response from me if you formulate your own argument here, rather than outsourcing the job to someone else. I sometimes accidentally miss perfectly reasonable comments in the moderation queue, or they get caught in the spam filter. If you feel this may have been the case with your comment, shoot me an email. You can now use rich HTML in comments! You can also use basic TeX, by enclosing it within $$ $$ for displayed equations or \\( \\) for inline equations. "],
        "story_type":"Normal",
        "url_raw":"https://www.scottaaronson.com/blog/?p=4450",
        "comments.comment_id":[21835706,
          21835867],
        "comments.comment_author":["marcus_holmes",
          "javajosh"],
        "comments.comment_descendants":[5,
          1],
        "comments.comment_time":["2019-12-19T15:31:29Z",
          "2019-12-19T15:46:33Z"],
        "comments.comment_text":["Pinker also talks about the \"Euphemism Treadmill\", where words that were specific medical terms turned into general insults and became unusable because of it (\"spastic\" in my lifetime has gone from a medical description, to a playground insult, to being effectively unusable, a non-word).<p>We'll see the same happen here. As \"supremacy\" becomes unacceptable, other words will rise to describe the same phenomenon, and then they'll become unacceptable, and we'll rotate gradually through ever-more-complex words and phrases to describe things.<p>I'm all in support of reclaiming words. If we use \"supremacy\" for lots of other things, then its association with racists fades. Giving them sole use of such a useful word seems to be letting the bad people win, somehow.",
          "Oy ve, if you're going to get upset about names, please get upset about the Standard Model particle names, which are a dumpster fire. The problem is that a tiny group signals outrage on behalf of a much larger group, and sensitive, empathic people like Aaronson take the bait.<p>This just doesn't matter, Scott. Protecting others from outrage is a fool's game, because you will always (always!) lose. Outrage is NOT a signal that someone has been harmed; 99% of the time outraged offense is a signal that someone's ego is getting fed that sweet sweet nectar of negative, powerful emotion.<p>Honestly, if you just wrote <i>another</i> post about how QM is a very natural consequence of the reality of negative probability, instead of this post about nothing, the world would be a better place."],
        "id":"daca0024-b97b-4fbd-a8c5-a446d546afe8",
        "url_text":"Yay! Im now a Fellow of the ACM. Along with my fellow new inductee Peter Shor, who I hear is a real up-and-comer in the quantum computing field. I will seek to use this awesome responsibility to steer the ACM along the path of good rather than evil. Also, last week, I attended the Q2B conference in San Jose, where a central theme was the outlook for practical quantum computing in the wake of the first clear demonstration of quantum computational supremacy. Thanks to the folks at QC Ware for organizing a fun conference (full disclosure: Im QC Wares Chief Scientific Advisor). Ill have more to say about the actual scientific things discussed at Q2B in future posts. None of that is why youre here, though. Youre here because of the battle over quantum supremacy. A week ago, my good friend and collaborator Zach Weinersmith, of SMBC Comics, put out a cartoon with a dark-curly-haired scientist named Dr. Aaronson, whos revealed on a hot mic to be an evil quantum supremacist. Apparently a rush job, this cartoon is far from Zachs finest work. For one thing, if the character is supposed to be me, why not draw him as me, and if he isnt, why call him Dr. Aaronson? In any case, I learned from talking to Zach that the cartoons timing was purely coincidental: Zach didnt even realize what a hornets-nest he was poking with this. Ever since John Preskill coined it in 2012, quantum supremacy has been an awkward term. Much as I admire John Preskills wisdom, brilliance, generosity, and good sense, in physics as in everything elseyeah, quantum supremacy is not a term I wouldve coined, and its certainly not a hill Id choose to die on. Once it had gained common currency, though, I sort of took a liking to it, mostly because I realized that I could mine it for dark one-liners in my talks. The thinking was: even as white supremacy was making its horrific resurgence in the US and around the world, here we were, physicists and computer scientists and mathematicians of varied skin tones and accents and genders, coming together to pursue a different and better kind of supremacya small reflection of the better world that we still believed was possible. You might say that we were reclaiming the word supremacywhich, after all, just means a state of being supremefor something non-sexist and non-racist and inclusive and good. In the world of 2019, alas, perhaps it was inevitable that people wouldnt leave things there. My first intimation came a month ago, when Leonie Muecksomeone who Id gotten to know and like when she was an editor at Nature handling quantum information papersemailed me about her view that our community should abandon the term quantum supremacy, because of its potential to make women and minorities uncomfortable in our field. She advocated using quantum advantage instead. So I sent Leonie back a friendly reply, explaining that, as the father of a math-loving 6-year-old girl, I understood and shared her concernsbut also, that I didnt know an alternative term that really worked. See, its like this. Preskill meant quantum supremacy to refer to a momentous event that seemed likely to arrive in a matter of years: namely, the moment when programmable quantum computers would first outpace the ability of the fastest classical supercomputers on earth, running the fastest algorithms known by humans, to simulate what the quantum computers were doing (at least on special, contrived problems). And the historic milestone of quantum advantage? It just doesnt sound right. Plus, as many others pointed out, the term quantum advantage is already used to refer to well, quantum advantages, which might fall well short of supremacy. But one could go further. Suppose we did switch to quantum advantage. Couldnt that term, too, remind vulnerable people about the unfair advantages that some groups have over others? Indeed, while advantage is certainly subtler than supremacy, couldnt that make it all the more insidious, and therefore dangerous? Oblivious though I sometimes am, I realized Leonie would be unhappy if I offered that, because of my wholehearted agreement, I would henceforth never again call it quantum supremacy, but only quantum superiority, quantum dominance, or quantum hegemony. But maybe you now see the problem. What word does the English language provide to describe one thing decisively beating or being better than a different thing for some purpose, and which doesnt have unsavory connotations? Ive heard quantum ascendancy, but that makes it sound like were a UFO cultwaiting to ascend, like ytterbium ions caught in a laser beam, to a vast quantum computer in the sky. Ive heard quantum inimitability (that is, inability to imitate using a classical computer), but who can pronounce that? Yesterday, my brilliant former student Ewin Tang (yes, that one) relayed to me a suggestion by Kevin Tian: quantum eclipse (that is, the moment when quantum computers first eclipse classical ones for some task). But would one want to speak of a quantum eclipse experiment? And shouldnt we expect that, the cuter and cleverer the term, the harder it will be to use unironically? In summary, while someone might think of a term so inspired that it immediately supplants quantum supremacy (and while I welcome suggestions), I currently regard it as an open problem. Anyway, evidently dissatisfied with my response, last week Leonie teamed up with 13 others to publish a letter in Nature, which was originally entitled Supremacy is for racistsuse quantum advantage,' but whose title I see has now been changed to the less inflammatory Instead of supremacy use quantum advantage.' Leonies co-signatories included four of my good friends and colleagues: Alan Aspuru-Guzik, Helmut Katzgraber, Anne Broadbent, and Chris Granade (the last of whom got started in the field by helping me edit Quantum Computing Since Democritus). (Update: Leonie pointed me to a longer list of signatories here, at their website called quantumresponsibility.org. A few names that might be known to Shtetl-Optimized readers are Andrew White, David Yonge-Mallo, Debbie Leung, Matt Leifer, Matthias Troyer.) Their letter says: The community claims that quantum supremacy is a technical term with a specified meaning. However, any technical justification for this descriptor could get swamped as it enters the public arena after the intense media coverage of the past few months.In our view, supremacy has overtones of violence, neocolonialism and racism through its association with white supremacy. Inherently violent language has crept into other branches of science as well in human and robotic spaceflight, for example, terms such as conquest, colonization and settlement evoke theterra nulliusarguments of settler colonialism and must be contextualized against ongoing issues of neocolonialism.Instead, quantum computing should be an open arena and an inspiration for a new generation of scientists. When I did an Ask Me Anything session, as the closing event at Q2B, Sarah Kaiser asked me to comment on the Nature petition. So I repeated what Id said in my emailed response to Leonierunning through the problems with each proposed alternative term, talking about the value of reclaiming the word supremacy, and mostly just trying to diffuse the tension by getting everyone laughing together. Sarah later tweeted that she was really disappointed in my response. Then the Wall Street Journal got in on the action, with a brief editorial (warning: paywalled) mocking the Nature petition: There it is, folks: Mankind has hit quantum wokeness. Our species, akin to Schrdingers cat, is simultaneously brilliant and brain-dead. We built a quantum computer and then argued about whether the write-up was linguistically racist.Taken seriously, the renaming game will never end. First put a Sharpie to the Supremacy Clause of the U.S. Constitution, which says federal laws trump state laws. Cancel Matt Damon for his 2004 role in The Bourne Supremacy. Make the Air Force give up the term air supremacy. Tell lovers of supreme pizza to quit being so chauvinistic about their toppings. Please inform Motown legend Diana Ross that the Supremes are problematic.The quirks of quantum mechanics, some people argue, are explained by the existence of many universes. How did we get stuck in this one? Steven Pinker also weighed in, with a linguistically-informed tweetstorm: This sounds like something from The Onion but actually appeared in Nature It follows the wokified stigmatization of other innocent words, like House Master (now, at Harvard, Residential Dean) and NIPS (Neural Information Processing Society, now NeurIPS). Its a familiar linguistic phenomenon, a lexical version of Greshams Law: bad meanings drive good ones out of circulation. Examples: the doomed niggardly (no relation to the n-word) and the original senses of cock, ass, prick, pussy, and booty. Still, the prissy banning of words by academics should be resisted. It dumbs down understanding of language: word meanings are conventions, not spells with magical powers, and all words have multiple senses, which are distinguished in context. Also, it makes academia a laughingstock, tars the innocent, and does nothing to combat actual racism & sexism. Others had a stronger reaction. Curtis Yarvin, better known as Mencius Moldbug, is one of the founders of neoreaction (and a significant influence on Steve Bannon, Michael Anton, and other Trumpists). Regulars might remember that Yarvin argued with me in Shtetl-Optimizeds comment section, under a post in which I denounced Trumps travel ban and its effects on my Iranian PhD student. Since then, Yarvin has sent me many emails, which have ranged from long to extremely long, and whose message could be summarized as: [labored breathing] Abandon your liberal Enlightenment pretensions, young Nerdwalker. Come over the Dark Side. After the supremacy is for racists letter came out in Nature, though, Yarvin sent me his shortest email ever. It was simply a link to the letter, along with the comment I knew it would come to this. He meant: What more proof do you need, young Nerdawan, that this performative wokeness is a cancer that will eventually infect everything you valueeven totally apolitical research in quantum information? And by extension, that my whole worldview, which warned of this, is fundamentally correct, while your faith in liberal academia is nave, and will be repaid only with backstabbing? In a subsequent email, Yarvin predicted that in two years, the whole community will be saying quantum advantage instead of quantum supremacy, and in five years Ill be saying quantum advantage too. As Yarvin famously wrote: Cthulhu may swim slowly. But he only swims left. So what do I really think about this epic battle for (and against) supremacy? Truthfully, half of me just wants to switch to quantum advantage right now and be done with it. As I said, I know some of the signatories of the Nature letter to be smart and reasonable and kind. They dont wish to rid the planet of everyone like me. Theyre not Amanda Marcottes or Arthur Chus. Furthermore, theres little I despise more than a meaty scientific debate devolving into a pointless semantic one, with brilliant friend after brilliant friend getting sucked into the vortex (you too?). Im strongly in the Pinkerian camp, which holds that words are just arbitrary designators, devoid of the totemic power to dictate thoughts. So if friends and colleagueseven just a few of themtell me that they find some word I use to be offensive, why not just be a mensch, apologize for any unintended hurt, switch words midsentence, and continue discussing the matter at hand? But then the other half of me wonders: once weve ceded an open-ended veto over technical terms that remind anyone of anything bad, where does it stop? How do we ever certify a word as kosher? At what point do we all get to stop arguing and laugh together? To make this worry concrete, look back at Sarah Kaisers Twitter threadthe one where she expresses disappointment in me. Below her tweet, someone remarks that, besides quantum supremacy, the word ancilla (as in ancilla qubit, a qubit used for intermediate computation or other auxiliary purposes) is problematic as well. Heres Sarahs response: I agree, but I wanted to start by focusing on the obvious one, Its harder for them to object to just one to start with, then once they admit the logic, we can expand the list (What would Curtis Yarvin say about that?) Youre probably now wondering: whats wrong with ancilla? Apparently, in ancient Rome, an ancilla was a female slave, and indeed thats the Latin root of the English adjective ancillary (as in providing support to). I confess that I hadnt known thathad you? Admittedly, once you do know, you might never again look at a Controlled-NOT gatepitilessly flipping an ancilla qubit, subject only to the whims of a nearby control qubitin quite the same way. (Ah, but the ancilla can fight back against her controller! And she doesin the Hadamard basis.) The thing is, if were gonna play this game: what about annihilation operators? Wont those need to be annihilated from physics? And what about unitary matrices? Doesnt their very name negate the multiplicity of perspectives and cultures? What about Diracs oddly-named bra/ket notation, with its limitless potential for puerile jokes, about the bra vectors displaying their contents horizontally and so forth? (Did you smile at that, you hateful pig?) What about daggers? Dont we need a less violent conjugate tranpose? Not to beat a dead horse, but once you hunt for examples, you realize that the whole dictionary is shot through with domination and brutalitythat youd have to massacre the English language to take it out. Theres nothing special about math or physics in this respect. The same half of me also thinks about my friends and colleagues who oppose claims of quantum supremacy, or even the quest for quantum supremacy, on various scientific grounds. I.e., either they dont think that the Google team achieved what it said, or they think that the task wasnt hard enough for classical computers, or they think that the entire goal is misguided or irrelevant or uninteresting. Which is finethese are precisely the arguments we should be havingexcept that Ive personally seen some of my respected colleagues, while arguing for these positions, opportunistically tack on ideological objections to the term quantum supremacy. Just to goose up their case, I guess. And I confess that every time they did this, it made me want to keep saying quantum supremacy from now till the end of timesolely to deny these colleagues a cheap and unearned victory, one they apparently felt they couldnt obtain on the merits alone. I realize that this is childish and irrational. Most of all, though, the half of me that Im talking about thinks about Curtis Yarvin and the Wall Street Journal editorial board, cackling with glee to see their worldview so dramatically confirmedas theatrical wokeness, that self-parodying modern monstrosity, turns its gaze on (of all things) quantum computing research. More red meat to fire up the baseor at least that sliver of the base nerdy enough to care. And the left, as usual, walks right into the trap, sacrificing its credibility with the outside world to pursue a runaway virtue-signaling spiral. The same half of me thinks: do we really want to fight racism and sexism? Then lets work together to assemble a broad coalition that can defeat Trump. And Jair Bolsonaro, and Viktor Orbn, and all the other ghastly manifestations of humanitys collective lizard-brain. Then, if were really fantasizing, we could liberalize the drug laws, and get contraception and loans and education to women in the Third World, and stop the systematic disenfranchisement of black voters, and open up the worlds richer, whiter, and higher-elevation countries to climate refugees, and protect the worlds remaining indigenous lands (those that didnt burn to the ground this year). In this context, the trouble with obsessing over terms like quantum supremacy is not merely that it diverts attention, while contributing nothing to fighting the worlds actual racism and sexism. The trouble is that the obsessions are actually harmful. For they make academicsalong with progressive activistslook silly. They make people think that we must not have meant it when we talked about the existential urgency of climate change and the worlds other crises. They pump oxygen into right-wing echo chambers. But its worse than ridiculous, because of the message that I fear is received by many outside the activists bubble. When you say stuff like [quantum] supremacy is for racists, whats heard might be something more like: Watch your back, you disgusting supremacist. Yes, you. You claim that you mentor women and minorities, donate to good causes, try hard to confront the demons in your own character? Ha! None of that counts for anything with us. Youll never be with-it enough to be our ally, so dont bother trying. Well see to it that youre never safe, not even in the most abstruse and apolitical fields. Well comb through your wordseven words like ancilla qubitlooking for any that we can cast as offensive by our opaque and ever-shifting standards. And once we find some, well have it within our power to end your career, and youll be reduced to groveling that we dont. Remember those popular kids who bullied you in second grade, giving you nightmares of social ostracism that persist to this day? We plan to achieve what even those bullies couldnt: to shame you with the full backing of the modern worlds moral code. See, were the good guys of this story. Its goodness itself thats branding you as racist scum. In short, I claim that the messagenot the message intended, of course, by anyone other than a Chu or a Marcotte or a SneerClubber, but the message receivedis basically a Trump campaign ad. I claim further that our civilizations current self-inflicted catastrophe will endi.e., the believers in science and reason and progress and rule of law will claw their way back to powerwhen, and only when, a generation of activists emerges that understands these dynamics as well as Barack Obama did. Wouldnt it be awesome if, five years from now, I could say to Curtis Yarvin: you were wrong? If I could say to him: my colleagues and I still use the term quantum supremacy whenever we care to, and none of us have been cancelled or ostracized for itso maybe you should revisit your paranoid theories about Cthulhu and the Cathedral and so forth? If I could say: quantum computing researchers now have bigger fish to fry than arguments over wordslike moving beyond quantum supremacy to the first useful quantum simulations, as well as the race for scalability and fault-tolerance? And even: progressive activists now have bigger fish to fry toolike retaking actual power all over the world? Anyway, as I said, thats how half of me feels. The other half is ready to switch to quantum advantage or any other serviceable term and get back to doing science. This entry was posted on Thursday, December 19th, 2019 at 3:17 am and is filed under Obviously I'm Not Defending Aaronson, Quantum, The Fate of Humanity. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site. Comment Policy: All comments are placed in moderation and reviewed prior to appearing. Comments can be left in moderation for any reason, but in particular, for ad-hominem attacks, hatred of groups of people, or snide and patronizing tone. Also: comments that link to a paper or article and, in effect, challenge me to respond to it are at severe risk of being left in moderation, as such comments place demands on my time that I can no longer meet. You'll have a much better chance of a response from me if you formulate your own argument here, rather than outsourcing the job to someone else. I sometimes accidentally miss perfectly reasonable comments in the moderation queue, or they get caught in the spam filter. If you feel this may have been the case with your comment, shoot me an email. You can now use rich HTML in comments! You can also use basic TeX, by enclosing it within $$ $$ for displayed equations or \\( \\) for inline equations. ",
        "_version_":1718441080011096064},
      {
        "story_id":19316228,
        "story_author":"magnetic",
        "story_descendants":11,
        "story_score":96,
        "story_time":"2019-03-06T03:38:05Z",
        "story_title":"SimFix: Automatically fix programs with existing patches from other projects",
        "search":["SimFix: Automatically fix programs with existing patches from other projects",
          "https://github.com/xgdsmileboy/SimFix",
          "NOTE: SimFix depends on several Defects4J commands. Therefore, if you want to conduct your experiment on other projects outside Defects4J, please adapt the project to the Defects4J framework. As an alternative, if you don't want to compile SimFix by youself, you also can download the replication package [HERE] directly, which will save your time for setting up the running environment. SimFix I. Introduction of SimFix II. Environment setup III. Run SimFix Step-by-Step IV. Evaluation Result V. Structure of the project I. Introduction SimFix is an automatic program repair technique, which leverages exisiting patches from other projects and similar code snippets in the same project to generate patches. The following figure is the workflow of our approach. Mining Stage Mine repair patterns from existing open-source projects, after which we can obtain a set of frequent repair patterns. Those patterns can be reused for other repairing scenarios as well. Repairing Stage Fault Localization : obtain a ranking list of candidate faulty statements and extract corresponding code snippets. Donor Snippet Identification : identify the similarity between faulty code snippet and each candidate similar code snippet by leveraging three similarity metrics, according to which we obtain a list of candidate similar snippets with decending order of similarity value. Variable Mapping : establish the mapping relationship between variables in faulty and similar code snippets by leveraging similarity metrics and then obtain a mapping table, based on which the variables in the donor code snippet will be replaced with the corresponding variables. Modification Extraction and Intersection : extract code modifications to faulty code snippet via AST (Abstract Syntax Tree) matching and differencing against the donor snippet, and then the frequent patterns from the mining stage will be used to take intersection with those modifications to further ruled out invalid ones. Patch Generation & Validation : generate repair patches by applying extracted code modifications to the faulty code snippet with combining and ranking whose modifications, then using the test suite to validate the correctness of candidate patches until a correct patch found or timeout. If you want to use this project, please cite our technical paper published at ISSTA'18. @inproceedings{Simfix:2018, author = {Jiang, Jiajun and Xiong, Yingfei and Zhang, Hongyu and Gao, Qing and Chen, Xiangqun}, title = {Shaping Program Repair Space with Existing Patches and Similar Code}, series = {ISSTA}, year = {2018}, location = {Amsterdam, Netherlands}, doi = {10.1145/3213846.3213871}, } II. Environment OS: Linux (Tested on Ubuntu 16.04.2 LTS) JDK: Oracle jdk1.7 (important!) Download and configure Defects4J (branch fee5ddf020) running environment. Configure the following path. DEFECTS4J_HOME=\"home_of_defects4j\" III. How to run SimFix was traditionally developed as an Eclipse Java project, you can simply import this project to your workspace and run it as a common Java program. The main class is cofix.main.Main, and for the running option please refer to the Running Options. Before running unzip file sbfl/data.zip to sbfl/data : used for fault localization using the command line provided by Defects4J to checkout a buggy version of benchmark for testing. Example: defects4j checkout -p Chart -v 1b -w ${buggy_program_path}/chart/chart_1_buggy NOTE : the path of the buggy version of benchmark have to be set as: /projectName/projectName_id_buggy => Example: /home/user/chart/chart_1_buggy Step 1, Build The Project Originally, SimFix was developed as an Eclipse Java Project, you can simply import the project into your workspace and the class cofix.main.Main is the entry of the whole program. Step 2, Running Options Our prototype of SimFix needs three input options for running. --proj_home : the home of buggy program of benchmark. (${buggy_program_path} for the example) --proj_name : the project name of buggy program of benchmark. (chart for the example) --bug_id : the identifier of the buggy program. (1 for the example) The option of --bugy_id supports multiple formats: single_id : repair single bug, e.g., 1. startId-endId : repair a series of bugs with consecutive identifiers, e.g., 1-3. single_id,single_id,single_id : repair any bugs for the specific program, e.g., 1,5,9. all : repair all buggy versions of a specific project, i.e., all. Usage: --proj_home=${proj_home} --proj_name=${proj_name} --bug_id=${bug_id} Example: --proj_home=/home/user --proj_name=chart --bug_id=1 Another: --proj_home=/home/user --proj_name=chart --bug_id=1,4,8 OPTION 1 : Run within eclipse (please use the old version: tested on Mars, which depends on Java7). From the Main class: Run AsRun Configurations Arguments : set the above arguments as Program Arguments. OPTION 2 : run using command line. We also provide runnable jar file of SimFix in the home folder of the project i.e., simfix.jar. Set the home directory of the SimFix project as your correct path and then run as: java -jar simfix.jar --proj_home=/home/user --proj_name=chart --bug_id=1 Step 3, Result Analysis After finishing the repair, there will be two additional folders: log and patch. log : debug output, including buggy statements already tried, patches and reference code snippet for correct patch generation. patch : a single source file repaired by SimFix that can pass the test suite. In the source file, you can find the patch, which is formatted as (example of Chart_1): // start of generated patch int index=this.plot.getIndexOf(this); CategoryDataset dataset=this.plot.getDataset(index); if(dataset==null){ return result; } // end of generated patch /* start of original code int index = this.plot.getIndexOf(this); CategoryDataset dataset = this.plot.getDataset(index); if (dataset != null) { return result; } end of original code*/ IV. Evaluation Result Totally, SimFix successfully repair 34 bugs among 357 bugs in Defects4J v1.0 benchmark with generating 22 plausible but incorrect patches. The following table and venn diagram are comparison withexisting approaches. More details are presented in the sub-folder final (latest). The comparison with existing approaches. Intersections. V. Structure of the project |--- README.md : user guidance |--- bin : binary code |--- d4j-info : defects4j information |--- doc : document |--- final : evaluation result |--- lib : dependent libraries |--- sbfl : fault localization tool |--- src : source code |--- test : test suite ALL suggestions are welcomed. "],
        "story_type":"Normal",
        "url_raw":"https://github.com/xgdsmileboy/SimFix",
        "comments.comment_id":[19319878,
          19320798],
        "comments.comment_author":["sitkack",
          "jboggan"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-03-06T16:01:04Z",
          "2019-03-06T17:34:49Z"],
        "comments.comment_text":["My favorite paper on the subject of automatic program repair is \"A Systematic Study of Automated Program Repair:Fixing 55 out of 105 Bugs for $8 Each\" [0][1] see also <a href=\"http://program-repair.org/\" rel=\"nofollow\">http://program-repair.org/</a><p>[0] <a href=\"http://web.eecs.umich.edu/~weimerw/p/weimer-icse2012-genprog-preprint.pdf\" rel=\"nofollow\">http://web.eecs.umich.edu/~weimerw/p/weimer-icse2012-genprog...</a><p>[1] <a href=\"https://squareslab.github.io/genprog-code/\" rel=\"nofollow\">https://squareslab.github.io/genprog-code/</a>",
          "We were attempting something similar last year but with ML (training from error messages to patchfiles to fix the given bug) but found it didn't scale well beyond simple scripts. This is very interesting though. There's a large potential market for this with continuous integration platforms."],
        "id":"7a587289-1b77-40c8-8bf5-78493b3a6a19",
        "url_text":"NOTE: SimFix depends on several Defects4J commands. Therefore, if you want to conduct your experiment on other projects outside Defects4J, please adapt the project to the Defects4J framework. As an alternative, if you don't want to compile SimFix by youself, you also can download the replication package [HERE] directly, which will save your time for setting up the running environment. SimFix I. Introduction of SimFix II. Environment setup III. Run SimFix Step-by-Step IV. Evaluation Result V. Structure of the project I. Introduction SimFix is an automatic program repair technique, which leverages exisiting patches from other projects and similar code snippets in the same project to generate patches. The following figure is the workflow of our approach. Mining Stage Mine repair patterns from existing open-source projects, after which we can obtain a set of frequent repair patterns. Those patterns can be reused for other repairing scenarios as well. Repairing Stage Fault Localization : obtain a ranking list of candidate faulty statements and extract corresponding code snippets. Donor Snippet Identification : identify the similarity between faulty code snippet and each candidate similar code snippet by leveraging three similarity metrics, according to which we obtain a list of candidate similar snippets with decending order of similarity value. Variable Mapping : establish the mapping relationship between variables in faulty and similar code snippets by leveraging similarity metrics and then obtain a mapping table, based on which the variables in the donor code snippet will be replaced with the corresponding variables. Modification Extraction and Intersection : extract code modifications to faulty code snippet via AST (Abstract Syntax Tree) matching and differencing against the donor snippet, and then the frequent patterns from the mining stage will be used to take intersection with those modifications to further ruled out invalid ones. Patch Generation & Validation : generate repair patches by applying extracted code modifications to the faulty code snippet with combining and ranking whose modifications, then using the test suite to validate the correctness of candidate patches until a correct patch found or timeout. If you want to use this project, please cite our technical paper published at ISSTA'18. @inproceedings{Simfix:2018, author = {Jiang, Jiajun and Xiong, Yingfei and Zhang, Hongyu and Gao, Qing and Chen, Xiangqun}, title = {Shaping Program Repair Space with Existing Patches and Similar Code}, series = {ISSTA}, year = {2018}, location = {Amsterdam, Netherlands}, doi = {10.1145/3213846.3213871}, } II. Environment OS: Linux (Tested on Ubuntu 16.04.2 LTS) JDK: Oracle jdk1.7 (important!) Download and configure Defects4J (branch fee5ddf020) running environment. Configure the following path. DEFECTS4J_HOME=\"home_of_defects4j\" III. How to run SimFix was traditionally developed as an Eclipse Java project, you can simply import this project to your workspace and run it as a common Java program. The main class is cofix.main.Main, and for the running option please refer to the Running Options. Before running unzip file sbfl/data.zip to sbfl/data : used for fault localization using the command line provided by Defects4J to checkout a buggy version of benchmark for testing. Example: defects4j checkout -p Chart -v 1b -w ${buggy_program_path}/chart/chart_1_buggy NOTE : the path of the buggy version of benchmark have to be set as: /projectName/projectName_id_buggy => Example: /home/user/chart/chart_1_buggy Step 1, Build The Project Originally, SimFix was developed as an Eclipse Java Project, you can simply import the project into your workspace and the class cofix.main.Main is the entry of the whole program. Step 2, Running Options Our prototype of SimFix needs three input options for running. --proj_home : the home of buggy program of benchmark. (${buggy_program_path} for the example) --proj_name : the project name of buggy program of benchmark. (chart for the example) --bug_id : the identifier of the buggy program. (1 for the example) The option of --bugy_id supports multiple formats: single_id : repair single bug, e.g., 1. startId-endId : repair a series of bugs with consecutive identifiers, e.g., 1-3. single_id,single_id,single_id : repair any bugs for the specific program, e.g., 1,5,9. all : repair all buggy versions of a specific project, i.e., all. Usage: --proj_home=${proj_home} --proj_name=${proj_name} --bug_id=${bug_id} Example: --proj_home=/home/user --proj_name=chart --bug_id=1 Another: --proj_home=/home/user --proj_name=chart --bug_id=1,4,8 OPTION 1 : Run within eclipse (please use the old version: tested on Mars, which depends on Java7). From the Main class: Run AsRun Configurations Arguments : set the above arguments as Program Arguments. OPTION 2 : run using command line. We also provide runnable jar file of SimFix in the home folder of the project i.e., simfix.jar. Set the home directory of the SimFix project as your correct path and then run as: java -jar simfix.jar --proj_home=/home/user --proj_name=chart --bug_id=1 Step 3, Result Analysis After finishing the repair, there will be two additional folders: log and patch. log : debug output, including buggy statements already tried, patches and reference code snippet for correct patch generation. patch : a single source file repaired by SimFix that can pass the test suite. In the source file, you can find the patch, which is formatted as (example of Chart_1): // start of generated patch int index=this.plot.getIndexOf(this); CategoryDataset dataset=this.plot.getDataset(index); if(dataset==null){ return result; } // end of generated patch /* start of original code int index = this.plot.getIndexOf(this); CategoryDataset dataset = this.plot.getDataset(index); if (dataset != null) { return result; } end of original code*/ IV. Evaluation Result Totally, SimFix successfully repair 34 bugs among 357 bugs in Defects4J v1.0 benchmark with generating 22 plausible but incorrect patches. The following table and venn diagram are comparison withexisting approaches. More details are presented in the sub-folder final (latest). The comparison with existing approaches. Intersections. V. Structure of the project |--- README.md : user guidance |--- bin : binary code |--- d4j-info : defects4j information |--- doc : document |--- final : evaluation result |--- lib : dependent libraries |--- sbfl : fault localization tool |--- src : source code |--- test : test suite ALL suggestions are welcomed. ",
        "_version_":1718441016184274944},
      {
        "story_id":21126267,
        "story_author":"wellactually",
        "story_descendants":263,
        "story_score":373,
        "story_time":"2019-10-01T15:17:33Z",
        "story_title":"I used to fear being a nobody, then I left social media",
        "search":["I used to fear being a nobody, then I left social media",
          "https://www.nytimes.com/2019/10/01/opinion/quit-social-media.html",
          "My wanting to share every waking thought became eclipsed by a desire for an increasingly rare commodity a private life.Oct. 1, 2019Credit...Natalie Keyssar for The New York TimesBianca Vivion BrooksMs. Brooks hosts a weekly culture podcast, Ask Viv.Whats happening?I stare blankly at the little box as I try to think of something clever for my first tweet. I settle on whats at the top of my mind: My only #fear is being a nobody. How could I know this exchange would begin a dialogue that would continue nearly every day for the next nine years of my life?I began using Twitter in 2010 as a newly minted high school freshman. Though it began as a hub for my quirky adolescent thoughts, over the years it became an archive of my emotional and intellectual voice a kind of virtual display for the evolution of my politics and artistic identity. But after nine years, it was time to close the archive. My wanting to share my every waking thought became eclipsed by a desire for an increasingly rare commodity a private life.Though I thought disappearing from social media would be as simple as logging off, my refusal to post anything caused a bit of a stir among my small but loyal following. I began to receive emails from strangers asking me where I had gone and when I would return. One message read: Not to be over familiar, but you have to come back eventually. Youre a writer after all. How will we read your writing? Another follower inquired, Where will you go? The truth is I have not gone anywhere. I am, in fact, more present than ever.Over time, I have begun to sense these messages reveal more than a lack of respect for privacy. I realize that to many millennials, a life without a social media presence is not simply a private life; it is no life at all: We possess a widespread, genuine fear of obscurity.When I consider the near-decade I have spent on social media, this worry makes sense. As with many in my generation, Twitter was my entry into conversations happening on a global scale; long before my byline graced any publication, tweeting was how I felt a part of the world. Twitter functions much like an echo chamber dependent on likes and retweets, and gaining notoriety is as easy as finding someone to agree with you. For years I poured my opinions, musings and outrage onto my timeline, believing I held an indispensable place in a vital sociopolitical experiment. But these passionate, public observations were born of more than just a desire to speak my mind I was measuring my individual worth in constant visibility. Implicit in my followers question Where will you go? is the resounding question How will we know where youve gone? Privacy is considered a small exchange for the security of being well known and well liked. After all, a private life boasts no location markers or story updates. The idea that the happenings of our lives would be constrained to our immediate families, friends and real-life communities is akin to social death in a world measured by followers, views, likes and shares.I grow weary when I think of this as the new normal for what is considered to be a fruitful personal life. Social media is no longer a mere public extension of our private socialization; it has become a replacement for it. What happens to our humanity when we relegate our real lives to props for the performance of our virtual ones?For one, a predominantly online existence can lull us into a dubious sense of having enacted concrete change, simply because of a tweet or Instagram post. As hashtag activism has obscured longstanding traditions of assembly and protest, theres concern that a failure to transition from the keyboard to in-person organization will effectively stall or kill the momentum of political movements. (See: Occupy Wall Street.) The sanctity of our most intimate experiences is also diminished. My grandfather Charles Shaw a notable trumpeter whose wisdoms and jazz scene tales I often shared on Twitter died last year. Rather than take adequate time to privately mourn the loss of his giant influence in my life alongside those who loved him most, I quickly posted a lengthy tribute to him to my followers. At the time I thought, How will they remember him if I dont acknowledge his passing?Perhaps at the root of this anxiety over being forgotten is an urgent question of how one ought to form a legacy; with the rise of automation, a widening wealth gap and an unstable political climate, it is easy to feel unimportant. It is almost as if the world is too big and we are much too small to excel in it in any meaningful way. We feel we need as many people as possible to witness our lives, so as not to be left out of a story that is being written too fast by people much more significant than ourselves.The secret of a full life is to live and relate to others as if they might not be there tomorrow, as if you might not be there tomorrow, the writer Anas Nin said. This feeling has become a rarity, and rarer every day now that we have reached a hastier and more superficial rhythm, now that we believe we are in touch with a greater amount of people. This is the illusion which might cheat us of being in touch deeply with the one breathing next to us.I think of those words and at once any fear of obscurity is eclipsed by much deeper ones the fear of forgoing the sacred moments of life, of never learning to be completely alone, of not bearing witness to the incredible lives of those who surround me.I observe the world around me. It is big and moving fast. Whats happening? I think to myself.Im just beginning to find out.Bianca Vivion Brooks is a writer based in Harlem.The Times is committed to publishing a diversity of letters to the editor. Wed like to hear what you think about this or any of our articles. Here are some tips. And heres our email: letters@nytimes.com.Follow @privacyproject on Twitter and The New York Times Opinion Section on Facebook and Instagram. "],
        "story_type":"Normal",
        "url_raw":"https://www.nytimes.com/2019/10/01/opinion/quit-social-media.html",
        "comments.comment_id":[21126518,
          21126723],
        "comments.comment_author":["mabbo",
          "vonseel"],
        "comments.comment_descendants":[33,
          25],
        "comments.comment_time":["2019-10-01T15:34:13Z",
          "2019-10-01T15:48:37Z"],
        "comments.comment_text":["I used to excessively post on Facebook. I started to worry that it was unhealthy- why do I need the constant validation from strangers? What is wrong with me psychologically that I crave that more than my own privacy? So I quit last year and I've felt good about that choice.<p>Now I have 17000 HN points and I'm starting to feel like I gave up meth and replaced it with cocaine. Maybe better, but still not good.<p>Going to stop posting here now too. Thanks everyone for the good times.",
          "I quit social media for a long time too. And then I noticed how disconnected I was from all my old school friends and contacts. Even close family that still uses Facebook but I otherwise dont talk to much directly (brothers wives posting about their kids and stuff like that, maybe even more extended than that).<p>Really, Im starting to think that opting out of social media altogether is a bit like opting to be a hermit in todays society. Sure, Im not advocating posting every 5 minutes on Facebook and getting into dumb arguments, but it really feels like opting out is a way of putting walls between yourself and the rest of the world (that is on social media). You will miss out on news of friends having kids, graduating college, going to college, getting new jobs, and even planning events together since many events are organized officially on Facebook - from free local concerts to your friends bachelors party. Many people just go through Facebook and create a private event and invite their Facebook friends when they have a birthday gathering nowadays. You will miss out on these if you opt-out completely. I did."],
        "id":"b502862d-3aa6-4550-9f94-5f0c18fe4092",
        "url_text":"My wanting to share every waking thought became eclipsed by a desire for an increasingly rare commodity a private life.Oct. 1, 2019Credit...Natalie Keyssar for The New York TimesBianca Vivion BrooksMs. Brooks hosts a weekly culture podcast, Ask Viv.Whats happening?I stare blankly at the little box as I try to think of something clever for my first tweet. I settle on whats at the top of my mind: My only #fear is being a nobody. How could I know this exchange would begin a dialogue that would continue nearly every day for the next nine years of my life?I began using Twitter in 2010 as a newly minted high school freshman. Though it began as a hub for my quirky adolescent thoughts, over the years it became an archive of my emotional and intellectual voice a kind of virtual display for the evolution of my politics and artistic identity. But after nine years, it was time to close the archive. My wanting to share my every waking thought became eclipsed by a desire for an increasingly rare commodity a private life.Though I thought disappearing from social media would be as simple as logging off, my refusal to post anything caused a bit of a stir among my small but loyal following. I began to receive emails from strangers asking me where I had gone and when I would return. One message read: Not to be over familiar, but you have to come back eventually. Youre a writer after all. How will we read your writing? Another follower inquired, Where will you go? The truth is I have not gone anywhere. I am, in fact, more present than ever.Over time, I have begun to sense these messages reveal more than a lack of respect for privacy. I realize that to many millennials, a life without a social media presence is not simply a private life; it is no life at all: We possess a widespread, genuine fear of obscurity.When I consider the near-decade I have spent on social media, this worry makes sense. As with many in my generation, Twitter was my entry into conversations happening on a global scale; long before my byline graced any publication, tweeting was how I felt a part of the world. Twitter functions much like an echo chamber dependent on likes and retweets, and gaining notoriety is as easy as finding someone to agree with you. For years I poured my opinions, musings and outrage onto my timeline, believing I held an indispensable place in a vital sociopolitical experiment. But these passionate, public observations were born of more than just a desire to speak my mind I was measuring my individual worth in constant visibility. Implicit in my followers question Where will you go? is the resounding question How will we know where youve gone? Privacy is considered a small exchange for the security of being well known and well liked. After all, a private life boasts no location markers or story updates. The idea that the happenings of our lives would be constrained to our immediate families, friends and real-life communities is akin to social death in a world measured by followers, views, likes and shares.I grow weary when I think of this as the new normal for what is considered to be a fruitful personal life. Social media is no longer a mere public extension of our private socialization; it has become a replacement for it. What happens to our humanity when we relegate our real lives to props for the performance of our virtual ones?For one, a predominantly online existence can lull us into a dubious sense of having enacted concrete change, simply because of a tweet or Instagram post. As hashtag activism has obscured longstanding traditions of assembly and protest, theres concern that a failure to transition from the keyboard to in-person organization will effectively stall or kill the momentum of political movements. (See: Occupy Wall Street.) The sanctity of our most intimate experiences is also diminished. My grandfather Charles Shaw a notable trumpeter whose wisdoms and jazz scene tales I often shared on Twitter died last year. Rather than take adequate time to privately mourn the loss of his giant influence in my life alongside those who loved him most, I quickly posted a lengthy tribute to him to my followers. At the time I thought, How will they remember him if I dont acknowledge his passing?Perhaps at the root of this anxiety over being forgotten is an urgent question of how one ought to form a legacy; with the rise of automation, a widening wealth gap and an unstable political climate, it is easy to feel unimportant. It is almost as if the world is too big and we are much too small to excel in it in any meaningful way. We feel we need as many people as possible to witness our lives, so as not to be left out of a story that is being written too fast by people much more significant than ourselves.The secret of a full life is to live and relate to others as if they might not be there tomorrow, as if you might not be there tomorrow, the writer Anas Nin said. This feeling has become a rarity, and rarer every day now that we have reached a hastier and more superficial rhythm, now that we believe we are in touch with a greater amount of people. This is the illusion which might cheat us of being in touch deeply with the one breathing next to us.I think of those words and at once any fear of obscurity is eclipsed by much deeper ones the fear of forgoing the sacred moments of life, of never learning to be completely alone, of not bearing witness to the incredible lives of those who surround me.I observe the world around me. It is big and moving fast. Whats happening? I think to myself.Im just beginning to find out.Bianca Vivion Brooks is a writer based in Harlem.The Times is committed to publishing a diversity of letters to the editor. Wed like to hear what you think about this or any of our articles. Here are some tips. And heres our email: letters@nytimes.com.Follow @privacyproject on Twitter and The New York Times Opinion Section on Facebook and Instagram. ",
        "_version_":1718441063382777856},
      {
        "story_id":20564911,
        "story_author":"La-ang",
        "story_descendants":2,
        "story_score":5,
        "story_time":"2019-07-30T14:01:25Z",
        "story_title":"Astronomers Discover Record-Breaking White Dwarf Binary System",
        "search":["Astronomers Discover Record-Breaking White Dwarf Binary System",
          "http://www.sci-news.com/astronomy/eclipsing-white-dwarf-binary-shortest-orbital-period-07429.html",
          "Astronomers using the KPED (Kitt Peak Electron Multiplying CCD demonstrator) instrument at NSFs Kitt Peak National Observatory have discovered ZTF J153932.16+502738.8, a white dwarf eclipsing binary with the shortest orbital period known to date. Located nearly 8,000 light-years away in the constellation of Botes, the system is also the second fastest pair of orbiting white dwarfs found to date. An artists impression of a pair of orbiting white dwarfs, ZTF J153932.16+502738.8. Image credit: Caltech / IPAC. White dwarf binaries with very tight orbits are expected to be strong sources of gravitational wave radiation. Although anticipated to be relatively common, such systems have proven elusive, with only a few identified to date, the researchers said. Caltechs Zwicky Transient Facility (ZTF), a new survey of the night sky, currently underway at Kitt Peak National Observatory and Palomar Observatory, is changing this situation. ZTF J153932.16+502738.8 (J1539 for short) is comprised of two white dwarfs that circle each other every 6.91 minutes. The system has an orbit so compact that the entire binary could fit within the diameter of the planet Saturn. As the dimmer star passes in front of the brighter one, it blocks most of the light, resulting in the seven-minute blinking pattern we see in the data, said lead author Kevin Burdge, a graduate student at Caltech. Closely orbiting white dwarfs are predicted to spiral together closer and faster, as the system loses energy by emitting gravitational waves. Sometimes these binary white dwarfs merge into one star, and other times the orbit widens as the lighter white dwarf is gradually shredded by the heavier one, said Dr. James Fuller, a theoretical astrophysicist at Caltech. Were not sure what will happen in this case, but finding more such systems will tell us how often these stars survive their close encounters. J1539s orbital period is predicted to become measurably shorter after only a few years. Burdge and colleagues were able to confirm the prediction from general relativity of a shrinking orbit, by comparing their new results with archival data acquired over the past ten years. J1539 is a rare gem. It is one of only a few known sources of gravitational waves that will be detected by the future European space mission LISA (Laser Interferometer Space Antenna), which is expected to launch in 2034, they noted. Another mystery they hope to answer in the future involves the temperature of the hotter white dwarf, which is estimated to be 90,000 degrees Fahrenheit (50,000 degrees Celsius). This white dwarf is thought to be so hot because it is starting to feed off its companion and pull material onto it, a process that heats material to sizzling-hot temperatures. But this feeding, or accretion process, is usually associated with X-rays, and the scientists are not seeing any. Its strange that we arent seeing X-rays in this system. One possibility is that the accretion spots on the white dwarf the areas the material is falling on are bigger than what is typical, and this could result in the emission of ultraviolet light and optical light instead of X-rays, Burdge said. The discovery is described in a paper published in the July 25, 2019 issue of the journal Nature. _____ Kevin B. Burdge et al. 2019. General relativistic orbital decay in a seven-minute-orbital-period eclipsing binary system. Nature 571: 528-531; doi: 10.1038/s41586-019-1403-0 "],
        "story_type":"Normal",
        "url_raw":"http://www.sci-news.com/astronomy/eclipsing-white-dwarf-binary-shortest-orbital-period-07429.html",
        "url_text":"Astronomers using the KPED (Kitt Peak Electron Multiplying CCD demonstrator) instrument at NSFs Kitt Peak National Observatory have discovered ZTF J153932.16+502738.8, a white dwarf eclipsing binary with the shortest orbital period known to date. Located nearly 8,000 light-years away in the constellation of Botes, the system is also the second fastest pair of orbiting white dwarfs found to date. An artists impression of a pair of orbiting white dwarfs, ZTF J153932.16+502738.8. Image credit: Caltech / IPAC. White dwarf binaries with very tight orbits are expected to be strong sources of gravitational wave radiation. Although anticipated to be relatively common, such systems have proven elusive, with only a few identified to date, the researchers said. Caltechs Zwicky Transient Facility (ZTF), a new survey of the night sky, currently underway at Kitt Peak National Observatory and Palomar Observatory, is changing this situation. ZTF J153932.16+502738.8 (J1539 for short) is comprised of two white dwarfs that circle each other every 6.91 minutes. The system has an orbit so compact that the entire binary could fit within the diameter of the planet Saturn. As the dimmer star passes in front of the brighter one, it blocks most of the light, resulting in the seven-minute blinking pattern we see in the data, said lead author Kevin Burdge, a graduate student at Caltech. Closely orbiting white dwarfs are predicted to spiral together closer and faster, as the system loses energy by emitting gravitational waves. Sometimes these binary white dwarfs merge into one star, and other times the orbit widens as the lighter white dwarf is gradually shredded by the heavier one, said Dr. James Fuller, a theoretical astrophysicist at Caltech. Were not sure what will happen in this case, but finding more such systems will tell us how often these stars survive their close encounters. J1539s orbital period is predicted to become measurably shorter after only a few years. Burdge and colleagues were able to confirm the prediction from general relativity of a shrinking orbit, by comparing their new results with archival data acquired over the past ten years. J1539 is a rare gem. It is one of only a few known sources of gravitational waves that will be detected by the future European space mission LISA (Laser Interferometer Space Antenna), which is expected to launch in 2034, they noted. Another mystery they hope to answer in the future involves the temperature of the hotter white dwarf, which is estimated to be 90,000 degrees Fahrenheit (50,000 degrees Celsius). This white dwarf is thought to be so hot because it is starting to feed off its companion and pull material onto it, a process that heats material to sizzling-hot temperatures. But this feeding, or accretion process, is usually associated with X-rays, and the scientists are not seeing any. Its strange that we arent seeing X-rays in this system. One possibility is that the accretion spots on the white dwarf the areas the material is falling on are bigger than what is typical, and this could result in the emission of ultraviolet light and optical light instead of X-rays, Burdge said. The discovery is described in a paper published in the July 25, 2019 issue of the journal Nature. _____ Kevin B. Burdge et al. 2019. General relativistic orbital decay in a seven-minute-orbital-period eclipsing binary system. Nature 571: 528-531; doi: 10.1038/s41586-019-1403-0 ",
        "comments.comment_id":[20564916,
          20566083],
        "comments.comment_author":["La-ang",
          "eutropia"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-07-30T14:02:21Z",
          "2019-07-30T15:54:14Z"],
        "comments.comment_text":["About White Dwarf Binary System (WDBS)\n<a href=\"https://phys.org/news/2011-05-binary-white-dwarf-stars.html\" rel=\"nofollow\">https://phys.org/news/2011-05-binary-white-dwarf-stars.html</a>",
          "Two ~planetary sized balls of nucleonic plasma orbiting one another inside an area the size of saturn every 7 minutes.<p>That's mindbogglingly ridiculous."],
        "id":"6f3c709e-9982-4d05-a9b7-cb7f16c69548",
        "_version_":1718441048515018752},
      {
        "story_id":21501902,
        "story_author":"bharatsb",
        "story_descendants":63,
        "story_score":170,
        "story_time":"2019-11-11T03:30:49Z",
        "story_title":"The transit of Mercury starts at 7:35 am ET and will last for 5.5 hours",
        "search":["The transit of Mercury starts at 7:35 am ET and will last for 5.5 hours",
          "https://solarsystem.nasa.gov/whats-up-skywatching-tips-from-nasa/",
          "Skywatching Home What's Up! Tips & Guides FAQ What's Up: November 2021 November 2021 skywatching highlights:Planets after sunset, a partial lunar eclipse, and the familiar stars of winter return. More Eyes on the Solar System Rainy day? Can't get outdoors? Explore our solar system's planets, moons, asteroids, and comets - without leaving the house. Explore More with NASA Night Sky Network Find astronomy clubs & events in your area Spot the Station Find out when the ISS will be visible in your sky Watch the Skies Blog Timely skywatching insights about meteors, the Moon, and more "],
        "story_type":"Normal",
        "url_raw":"https://solarsystem.nasa.gov/whats-up-skywatching-tips-from-nasa/",
        "url_text":"Skywatching Home What's Up! Tips & Guides FAQ What's Up: November 2021 November 2021 skywatching highlights:Planets after sunset, a partial lunar eclipse, and the familiar stars of winter return. More Eyes on the Solar System Rainy day? Can't get outdoors? Explore our solar system's planets, moons, asteroids, and comets - without leaving the house. Explore More with NASA Night Sky Network Find astronomy clubs & events in your area Spot the Station Find out when the ISS will be visible in your sky Watch the Skies Blog Timely skywatching insights about meteors, the Moon, and more ",
        "comments.comment_id":[21501931,
          21502551],
        "comments.comment_author":["ColinWright",
          "nexuist"],
        "comments.comment_descendants":[0,
          6],
        "comments.comment_time":["2019-11-11T03:37:45Z",
          "2019-11-11T06:06:08Z"],
        "comments.comment_text":["<p><pre><code>  12:35Z : Starts\n  15:20Z : Mid-point\n  18:04Z : Ends\n</code></pre>\nGutted to be missing this ... it's been in my diary for years.",
          "Interesting to think about where we'll be when this happens again in 2049.<p>It's a difference of only 30 years. 30 years ago was 1989. Yet life today compared to then is simply alien - technology has impacted the way everyone and every <i>thing</i> exists.<p>The first self-powered flight was in 1903. Man landed on the Moon just 66 years later. In one lifetime you went from being perpetually constrained to the surface to literally flying above the sky, transcending all known heights, and you wouldn't even have retired yet.<p>SpaceX was started in 2002, a mere 17 years ago. It is now, on this very day, on the verge of delivering hardware that can send people to Mars.<p>In 30 years from now, if the optimists are correct, we'll be living on the Moon and probably Mars. If the optimists are correct, we'll be able to take a shuttle from Mars to Mercury itself by the time this event happens again.<p>The optimists are probably wrong. But it doesn't hurt to dream."],
        "id":"4f9f5a94-846c-4b8e-94b5-2205f3123ed4",
        "_version_":1718441072113221632},
      {
        "story_id":20009037,
        "story_author":"Tomte",
        "story_descendants":76,
        "story_score":93,
        "story_time":"2019-05-25T13:51:43Z",
        "story_title":"A history of tea, the second most-consumed beverage in the world (2017)",
        "search":["A history of tea, the second most-consumed beverage in the world (2017)",
          "http://kottke.org/17/05/a-history-of-tea-the-second-most-consumed-beverage-in-the-world",
          "From TED-ed and tea expert Shunan Teng, a short video on the history of tea, from its invention in China to its role in globalization. Our history of tea begins with the legend of the divine famer Shen Nong who is credited in many ancient Chinese texts with various agricultural accomplishments. However, some scholars of ancient China now believe Shen Nong might in fact originally have referred to a group of people, living within China and utilizing particularly advanced agricultural techniques for the era. Over time this peoples knowledge of farming was canonized in the form of legends about a divine farmer who shared their name, and whose fame ultimately eclipsed their own. More about... foodShunan Tengvideo "],
        "story_type":"Normal",
        "url_raw":"http://kottke.org/17/05/a-history-of-tea-the-second-most-consumed-beverage-in-the-world",
        "url_text":"From TED-ed and tea expert Shunan Teng, a short video on the history of tea, from its invention in China to its role in globalization. Our history of tea begins with the legend of the divine famer Shen Nong who is credited in many ancient Chinese texts with various agricultural accomplishments. However, some scholars of ancient China now believe Shen Nong might in fact originally have referred to a group of people, living within China and utilizing particularly advanced agricultural techniques for the era. Over time this peoples knowledge of farming was canonized in the form of legends about a divine farmer who shared their name, and whose fame ultimately eclipsed their own. More about... foodShunan Tengvideo ",
        "comments.comment_id":[20013308,
          20015140],
        "comments.comment_author":["elliotec",
          "themagician"],
        "comments.comment_descendants":[2,
          3],
        "comments.comment_time":["2019-05-26T05:28:05Z",
          "2019-05-26T14:42:11Z"],
        "comments.comment_text":["Weird to post this youtube video embedded on kottke as if it was a post from kottke, here's the original TedEd video on youtube: <a href=\"https://www.youtube.com/watch?v=LaLvVc1sS20\" rel=\"nofollow\">https://www.youtube.com/watch?v=LaLvVc1sS20</a>",
          "I prefer the greentext history:<p>> Tea is the coolest thing in human history.<p>> Grows in China, only really exported to Japan.<p>> 1500s, some dues bring it to Europe.<p>> Britain goes fucking nuts for it.<p>> Start naval empire for the sole purposes of buying tea.<p>> Britain buys all the tea in China for all its silver bullion, utterly bankrupting itself.<p>> Britain gets China addicted to opium to balance debt. Ha.<p>> China gets pissed, colossal war ensues.<p>> Meanwhile Brits decide they want sugar in their tea, basically kick start the slave trade in earnest.<p>> Stick sugar plantations all over Americas.<p>> Kill half of the people in Africa just to grow that sweet, sweet sugar.<p>> Blitz through India, be like, Grow tea or everyone dies.<p>> okay.jpg<p>> Chinas fucked, broke from opium wars, monopoly on tea gone,<p>> Africas fucked, millions dead, millions in chains halfway around the world.<p>> The Americas rolling in wealth because of slavery and plantations.<p>> Everyone and their mum now drinks tea, Brtis make mad bank.<p>> Wake up to drink sweet brew every morning, imagine the untold amount of violence that went into that cup."],
        "id":"038734da-1e33-4f2a-b3b6-22df0f58266e",
        "_version_":1718441035453956096},
      {
        "story_id":18896649,
        "story_author":"gyre007",
        "story_descendants":20,
        "story_score":45,
        "story_time":"2019-01-13T13:44:52Z",
        "story_title":"An ancient Greek calendar was ahead of its time (2006)",
        "search":["An ancient Greek calendar was ahead of its time (2006)",
          "https://www.smithsonianmag.com/science-nature/old-world-high-tech-141284744/",
          "/ These 82 bronze fragments of the original mechanism were found in a Roman shipwreck by sponge divers in 1900. Copyright of the Antikythera Mechanism Research Project / In this reconstruction of the mechanical calendar, the area in the center measured the moon's position, the gears on the left predicted eclipses and the gears on the right reconciled lunar months with solar years. Copyright of the Antikythera Mechanism Research Project / A calendar with Egyptian names of months, written in Greek letters, appeared on the front dials. Copyright of the Antikythera Mechanism Research Project / The front gears were built around the Egyptian calendar of 365 days - the standard in Greek astronomy. Copyright of the Antikythera Mechanism Research Project / The back dials sat below a pointer that indicated lunar months. Copyright of the Antikythera Mechanism Research Project / The complexity of the back gears likely pre-dates similar engines by many centuries, the researchers say. Copyright of the Antikythera Mechanism Research Project / The fragments now rest in the National Archaeological Museum in Athens. Jo Marchant, Nature In 1900, Greek divers rifling through an ancient shipwreck recovered dozens of bronze fragments that turned out to be parts of a 2,000-year-old mechanical calendar. Now, more than a century after that discovery, scientists who studied these pieces are hailing the device as remarkably advanced for its time. Using computer imaging techniques previously unavailable to researchers, a team led by mathematician Tony Freeth of the University of Cardiff reconstructed the Greek instrument, known as the Antikythera Mechanism, from the 82 recovered fragments. The original mechanism involved 37 gear-wheels held together by a complex pin-and-slot system, the researchers report in the Nov. 30 Nature. One section of the instrument's gear systems predicted lunar and solar eclipses, another synchronized lunar months and solar years. A large gear in the middle indicated the position of the moon. \"The Antikythera Mechanism is the most sophisticated such object yet found from the ancient and medieval periods,\" writes Franois Charette of Ludwig-Maximilians University in Germany in a commentary accompanying the paper. Freeth's team deciphered previously illegible inscriptions on the wooden walls that housed the mechanism. Based on these markings, the calendar likely dates to about 150 B.C., the researchers report. Physical evidence of an ancient technology is rare, says professor of Greek and Roman studies John Humphrey of the University of Calgary, who recently published a book on the subject. Most early devices are known from writings describing the machines, which may or may not have been built. \"That's what makes the Antikythera Mechanism different,\" he says. Humphrey, who was not part of the new research study, highlights some other intriguing early technologies: The Bottomless Wine Glass Inventor: Hero of Alexandria Date: Around 65 A.D. How it works: The goblet is connected to a reservoir by a tube. As a person drinks wine, the liquid level in the reservoir falls, releasing a plug coming from a hidden reservoir. As the liquid levels replenishperhaps between gulpsthe plug once again stops the hidden reservoir. Proof of complexity: Several of Humphrey's engineering students have been unable to replicate the device's complex plug. Quirk: \"The trouble is, you have to drink the wine through a straw,\" Humphrey says. Automatic Temple Door Inventor: Hero of Alexandria, again Date: First century A.D. How it works: A worshipper lights a fire on an altar. The fire heats the air, which increases its volume. The heavier air causes a container of water to flow into a bucket. As the bucket fills, a series of pulleys and gears lifts the temple door. Proof of complexity: The applied physical principles of pneumatics. Quirk: \"I doubt it if was ever constructed,\" Humphrey says. World's First Vending Machine Inventor: Hero (busy man) Date: First century A.D. How it works: A person puts a coin in a slot at the top of a box. The coin hits a metal lever, like a balance beam. On the other end of the beam is a string tied to a plug that stops a container of liquid. As the beam tilts from the weight of the coin, the string lifts the plug and dispenses the desired drink until the coin drops off the beam. Proof of complexity: Early modern vending machines actually used a similar system, before electrical machines took over. Quirk: It was devised to distribute Holy Water at temples, because \"people were taking more Holy Water than they were paying for,\" Humphrey says. Double-action Piston Pump Inventor: Ctesibus Date: Third century B.C. How it works: Two pistons rest in cylinders attached to a handle. As one piston is raised, the other falls. The rising piston allows water to enter a chamber. The falling piston presses the water out of the other side in a constant stream. Proof of complexity: Such a device is a predecessor of the modern engine. Quirk: With the addition of a nozzle on one end, the device was subsequently turned into a fire-fighting toolby none other than Hero. Tools Recommended Videos "],
        "story_type":"Normal",
        "url_raw":"https://www.smithsonianmag.com/science-nature/old-world-high-tech-141284744/",
        "comments.comment_id":[18897609,
          18899030],
        "comments.comment_author":["TomK32",
          "dr_dshiv"],
        "comments.comment_descendants":[1,
          1],
        "comments.comment_time":["2019-01-13T17:30:44Z",
          "2019-01-13T22:09:06Z"],
        "comments.comment_text":["There's an ongoing youtube series about recreating the Antikythera Mechanism<p><a href=\"https://www.youtube.com/watch?v=ML4tw_UzqZE&list=PLZioPDnFPNsHnyxfygxA0to4RXv4_jDU2\" rel=\"nofollow\">https://www.youtube.com/watch?v=ML4tw_UzqZE&list=PLZioPDnFPN...</a>",
          "Check out Archytas, the so-called \"father of mechanical engineering.\" He was the (philosopher) king of Tarentum and a major influence on Plato. He was one of the last Pythagoreans and probably authored the first treatise of mech Eng, \"mechanical problems\" (Winter, 2007). Archytas is attested to have built the first self-propelled, steam powered flying machine. He was also a major influence on Vitruvius.<p>The Pythagoreans were erased from history several times - but essentially introduced the modern world view that the natural world, including the human mind, has a mathematical basis.<p>This is completely off topic but I also discovered recently (from the source text) that Philo of Alexandria, contemporary to Jesus Christ, twice described the Essenes of Israel as \"Pythagoreans\". That is super strange, people! Jesus was described as a Nazorean, a sect of the Essenes. Iamblicus claims Pythagoras visited Mt Caramel (later the home of the Nazoreans) around the time that the Jewish \"babylonian captivity\" ended. So, that's an alternate narrative for the basis of Christianity that has better historical documentation than most of the new testament. And, via Pythagorean ideas, is far more commensurable with modern science. Just saying! It's a real rabbit hole, these original source texts...."],
        "id":"765ff4d4-d7d6-4b08-8e66-73c8494e5abd",
        "url_text":"/ These 82 bronze fragments of the original mechanism were found in a Roman shipwreck by sponge divers in 1900. Copyright of the Antikythera Mechanism Research Project / In this reconstruction of the mechanical calendar, the area in the center measured the moon's position, the gears on the left predicted eclipses and the gears on the right reconciled lunar months with solar years. Copyright of the Antikythera Mechanism Research Project / A calendar with Egyptian names of months, written in Greek letters, appeared on the front dials. Copyright of the Antikythera Mechanism Research Project / The front gears were built around the Egyptian calendar of 365 days - the standard in Greek astronomy. Copyright of the Antikythera Mechanism Research Project / The back dials sat below a pointer that indicated lunar months. Copyright of the Antikythera Mechanism Research Project / The complexity of the back gears likely pre-dates similar engines by many centuries, the researchers say. Copyright of the Antikythera Mechanism Research Project / The fragments now rest in the National Archaeological Museum in Athens. Jo Marchant, Nature In 1900, Greek divers rifling through an ancient shipwreck recovered dozens of bronze fragments that turned out to be parts of a 2,000-year-old mechanical calendar. Now, more than a century after that discovery, scientists who studied these pieces are hailing the device as remarkably advanced for its time. Using computer imaging techniques previously unavailable to researchers, a team led by mathematician Tony Freeth of the University of Cardiff reconstructed the Greek instrument, known as the Antikythera Mechanism, from the 82 recovered fragments. The original mechanism involved 37 gear-wheels held together by a complex pin-and-slot system, the researchers report in the Nov. 30 Nature. One section of the instrument's gear systems predicted lunar and solar eclipses, another synchronized lunar months and solar years. A large gear in the middle indicated the position of the moon. \"The Antikythera Mechanism is the most sophisticated such object yet found from the ancient and medieval periods,\" writes Franois Charette of Ludwig-Maximilians University in Germany in a commentary accompanying the paper. Freeth's team deciphered previously illegible inscriptions on the wooden walls that housed the mechanism. Based on these markings, the calendar likely dates to about 150 B.C., the researchers report. Physical evidence of an ancient technology is rare, says professor of Greek and Roman studies John Humphrey of the University of Calgary, who recently published a book on the subject. Most early devices are known from writings describing the machines, which may or may not have been built. \"That's what makes the Antikythera Mechanism different,\" he says. Humphrey, who was not part of the new research study, highlights some other intriguing early technologies: The Bottomless Wine Glass Inventor: Hero of Alexandria Date: Around 65 A.D. How it works: The goblet is connected to a reservoir by a tube. As a person drinks wine, the liquid level in the reservoir falls, releasing a plug coming from a hidden reservoir. As the liquid levels replenishperhaps between gulpsthe plug once again stops the hidden reservoir. Proof of complexity: Several of Humphrey's engineering students have been unable to replicate the device's complex plug. Quirk: \"The trouble is, you have to drink the wine through a straw,\" Humphrey says. Automatic Temple Door Inventor: Hero of Alexandria, again Date: First century A.D. How it works: A worshipper lights a fire on an altar. The fire heats the air, which increases its volume. The heavier air causes a container of water to flow into a bucket. As the bucket fills, a series of pulleys and gears lifts the temple door. Proof of complexity: The applied physical principles of pneumatics. Quirk: \"I doubt it if was ever constructed,\" Humphrey says. World's First Vending Machine Inventor: Hero (busy man) Date: First century A.D. How it works: A person puts a coin in a slot at the top of a box. The coin hits a metal lever, like a balance beam. On the other end of the beam is a string tied to a plug that stops a container of liquid. As the beam tilts from the weight of the coin, the string lifts the plug and dispenses the desired drink until the coin drops off the beam. Proof of complexity: Early modern vending machines actually used a similar system, before electrical machines took over. Quirk: It was devised to distribute Holy Water at temples, because \"people were taking more Holy Water than they were paying for,\" Humphrey says. Double-action Piston Pump Inventor: Ctesibus Date: Third century B.C. How it works: Two pistons rest in cylinders attached to a handle. As one piston is raised, the other falls. The rising piston allows water to enter a chamber. The falling piston presses the water out of the other side in a constant stream. Proof of complexity: Such a device is a predecessor of the modern engine. Quirk: With the addition of a nozzle on one end, the device was subsequently turned into a fire-fighting toolby none other than Hero. Tools Recommended Videos ",
        "_version_":1718441005820149760},
      {
        "story_id":21682358,
        "story_author":"lelf",
        "story_descendants":13,
        "story_score":185,
        "story_time":"2019-12-02T14:07:17Z",
        "story_title":"The soul of a new computer company",
        "search":["The soul of a new computer company",
          "http://dtrace.org/blogs/bmc/2019/12/02/the-soul-of-a-new-computer-company/",
          "Over the summer, I described preparing for my next expedition. Im thrilled to announce that the expedition is now plotted, the funds are raised, and the bags are packed: together with Steve Tuck and Jess Frazelle, we have started Oxide Computer Company. Starting a computer company may sound crazy (and you would certainly be forgiven a double-take!), but it stems from a belief that I hold in my marrow: that hardware and software should each be built with the other in mind. For me, this belief dates back a quarter century: when I first came to Sun Microsystems in the mid-1990s, it was explicitly to work on operating system kernel development at a computer company at a time when that very idea was iconoclastic. And when we started Fishworks a decade later, the belief in fully integrated software and hardware was so deeply rooted into our endeavor as to be eponymous: it was the FISH in Fishworks. In working at a cloud computing company over the past decade, economic realities forced me to suppress this belief to a degree but it now burns hotter than ever after having endured the consequences of a world divided: in running a cloud, our most vexing problems emanated from the deepest bowels of the stack, when hardware and (especially) firmware operated at cross purposes with our systems software. As I began to think about what was next, I was haunted by the pain and futility of trying to build a cloud with PC-era systems. At the same time, seeing the kinds of solutions that the hyperscalers had developed for themselves had always left me with equal parts admiration and frustration: their rack-level designs are a clear win why are these designs cloistered among so few? And even in as much as the hardware could be found through admirable efforts like the Open Compute Project, the software necessary to realize its full potential has remained cruelly unavailable. Alongside my inescapable technical beliefs has been a commercial one: even as the world is moving (or has moved) to elastic, API-driven computing, there remain good reasons to run on ones own equipment! Further, as cloud-borne SaaS companies mature from being strictly growth focused to being more margin focused, it seems likely that more will consider buying machines instead of always renting them. It was in the confluence of these sentiments that an idea began to take shape: the world needed a company to develop and deliver integrated, hyperscaler-class infrastructure to the broader market that we needed to start a computer company. The we here is paramount: in Steve and Jess, I feel blessed to not only share a vision of our future, but to have diverse perspectives on how infrastructure is designed, built, sold, operated and run. And most important of all (with the emphasis itself being a reflection of hard-won wisdom), we three share deeply-held values: we have the same principled approach, with shared aspirations for building the kind of company that customers will love to buy from and employees will be proud to work for. Together, as we looked harder at the problem, we saw the opportunity more and more clearly: the rise of open firmware and the broadening of the Open Compute Project made this more technically feasible than ever; the sharpening desire among customers for a true cloud-like on-prem experience (and the neglect those customers felt in the market) made it more in demand than ever. With accelerating conviction that we would build a company to do this, we needed a name and once we hit on Oxide, we knew it was us: oxides form much of the earths crust, giving a connotation of foundation; silicon, the element that is the foundation of all of computing, is found in nature in its oxide; and (yes!) iron oxide is also known as Rust, a programming language we see playing a substantial role for us. Were there any doubt, that Oxide can also be pseudo-written in hexadecimal as 0x1de pretty much sealed the deal! There was just one question left, and it was an existential one: could we find an investor who saw what we saw in Oxide? Fortunately, the answer to this question had been emphatic and unequivocal: in the incredible team at Eclipse Ventures, we found investors that not only understood the space and the market, but also the challenges of solving hard technical problems. And we are deeply honored to have Eclipses singular Pierre Lamond joining us on our board; we can imagine no better a start for a new computer company! So while there is a long and rocky path ahead, we are at last underway on our improbable journey! If you havent yet, read Jesss blog on Oxide being born on a garage. If you find yourself battling the problems were aiming to fix, please join our mailing list. If you are a technologist who feels this problem in your bones as we do, consider joining us. And if nothing else, and you would like to hear some terrific stories of life at the hardware/software interface, check out our incredible podcast On the Metal! "],
        "story_type":"Normal",
        "url_raw":"http://dtrace.org/blogs/bmc/2019/12/02/the-soul-of-a-new-computer-company/",
        "comments.comment_id":[21687172,
          21687186],
        "comments.comment_author":["dang",
          "ChuckMcM"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-12-02T21:30:31Z",
          "2019-12-02T21:31:13Z"],
        "comments.comment_text":["Comments moved to <a href=\"https://news.ycombinator.com/item?id=21682360\" rel=\"nofollow\">https://news.ycombinator.com/item?id=21682360</a>.",
          "FWIW I registered the domain name \"moonmicrosystems.com\" thinking along these same lines :-). I share their passion that a \"real\" computer company need exists. That said, Google and Facebook at least design their own computers because they have the scale. Not many companies need 10,000 servers at a pop."],
        "id":"2a1f1002-375e-40ff-b9f2-f2fb71c579c6",
        "url_text":"Over the summer, I described preparing for my next expedition. Im thrilled to announce that the expedition is now plotted, the funds are raised, and the bags are packed: together with Steve Tuck and Jess Frazelle, we have started Oxide Computer Company. Starting a computer company may sound crazy (and you would certainly be forgiven a double-take!), but it stems from a belief that I hold in my marrow: that hardware and software should each be built with the other in mind. For me, this belief dates back a quarter century: when I first came to Sun Microsystems in the mid-1990s, it was explicitly to work on operating system kernel development at a computer company at a time when that very idea was iconoclastic. And when we started Fishworks a decade later, the belief in fully integrated software and hardware was so deeply rooted into our endeavor as to be eponymous: it was the FISH in Fishworks. In working at a cloud computing company over the past decade, economic realities forced me to suppress this belief to a degree but it now burns hotter than ever after having endured the consequences of a world divided: in running a cloud, our most vexing problems emanated from the deepest bowels of the stack, when hardware and (especially) firmware operated at cross purposes with our systems software. As I began to think about what was next, I was haunted by the pain and futility of trying to build a cloud with PC-era systems. At the same time, seeing the kinds of solutions that the hyperscalers had developed for themselves had always left me with equal parts admiration and frustration: their rack-level designs are a clear win why are these designs cloistered among so few? And even in as much as the hardware could be found through admirable efforts like the Open Compute Project, the software necessary to realize its full potential has remained cruelly unavailable. Alongside my inescapable technical beliefs has been a commercial one: even as the world is moving (or has moved) to elastic, API-driven computing, there remain good reasons to run on ones own equipment! Further, as cloud-borne SaaS companies mature from being strictly growth focused to being more margin focused, it seems likely that more will consider buying machines instead of always renting them. It was in the confluence of these sentiments that an idea began to take shape: the world needed a company to develop and deliver integrated, hyperscaler-class infrastructure to the broader market that we needed to start a computer company. The we here is paramount: in Steve and Jess, I feel blessed to not only share a vision of our future, but to have diverse perspectives on how infrastructure is designed, built, sold, operated and run. And most important of all (with the emphasis itself being a reflection of hard-won wisdom), we three share deeply-held values: we have the same principled approach, with shared aspirations for building the kind of company that customers will love to buy from and employees will be proud to work for. Together, as we looked harder at the problem, we saw the opportunity more and more clearly: the rise of open firmware and the broadening of the Open Compute Project made this more technically feasible than ever; the sharpening desire among customers for a true cloud-like on-prem experience (and the neglect those customers felt in the market) made it more in demand than ever. With accelerating conviction that we would build a company to do this, we needed a name and once we hit on Oxide, we knew it was us: oxides form much of the earths crust, giving a connotation of foundation; silicon, the element that is the foundation of all of computing, is found in nature in its oxide; and (yes!) iron oxide is also known as Rust, a programming language we see playing a substantial role for us. Were there any doubt, that Oxide can also be pseudo-written in hexadecimal as 0x1de pretty much sealed the deal! There was just one question left, and it was an existential one: could we find an investor who saw what we saw in Oxide? Fortunately, the answer to this question had been emphatic and unequivocal: in the incredible team at Eclipse Ventures, we found investors that not only understood the space and the market, but also the challenges of solving hard technical problems. And we are deeply honored to have Eclipses singular Pierre Lamond joining us on our board; we can imagine no better a start for a new computer company! So while there is a long and rocky path ahead, we are at last underway on our improbable journey! If you havent yet, read Jesss blog on Oxide being born on a garage. If you find yourself battling the problems were aiming to fix, please join our mailing list. If you are a technologist who feels this problem in your bones as we do, consider joining us. And if nothing else, and you would like to hear some terrific stories of life at the hardware/software interface, check out our incredible podcast On the Metal! ",
        "_version_":1718441076428111872},
      {
        "story_id":20491876,
        "story_author":"spiffytech",
        "story_descendants":5,
        "story_score":36,
        "story_time":"2019-07-21T15:53:39Z",
        "story_title":"Programmer Competency Matrix",
        "search":["Programmer Competency Matrix",
          "https://sijinjoseph.com/programmer-competency-matrix/",
          "Note that the knowledge for each level is cumulative; being atlevel n implies that you also know everything from thelevels lower than n.Computer Science2n (Level 0)n2 (Level 1)n (Level 2)log(n) (Level 3)Commentsdata structuresDoesnt know the difference between Array and LinkedListAble to explain and use Arrays, LinkedLists, Dictionaries etc in practical programming tasksKnows space and time tradeoffs of the basic data structures, Arrays vs LinkedLists, Able to explain how hashtables can be implemented and can handle collisions, Priority queues and ways to implement them etc.Knowledge of advanced data structures like B-trees, binomial and fibonacci heaps, AVL/Red Black trees, Splay Trees, Skip Lists, tries etc.algorithmsUnable to find the average of numbers in an array (Its hard to believe but Ive interviewed such candidates)Basic sorting, searching and data structure traversal and retrieval algorithmsTree, Graph, simple greedy and divide and conquer algorithms, is able to understand the relevance of the levels of this matrix.Able to recognize and code dynamic programming solutions, good knowledge of graph algorithms, good knowledge of numerical computation algorithms, able to identify NP problems etc.systems programmingDoesnt know what a compiler, linker or interpreter isBasic understanding of compilers, linker and interpreters. Understands what assembly code is and how things work at the hardware level. Some knowledge of virtual memory and paging.Understands kernel mode vs. user mode, multi-threading, synchronization primitives and how theyre implemented, able to read assembly code. Understands how networks work, understanding of network protocols and socket level programming.Understands the entire programming stack, hardware (CPU + Memory + Cache + Interrupts + microcode), binary code, assembly, static and dynamic linking, compilation, interpretation, JIT compilation, garbage collection, heap, stack, memory addressingSoftware Engineering2n (Level 0)n2 (Level 1)n (Level 2)log(n) (Level 3)Commentssource code version controlFolder backups by dateVSS and beginning CVS/SVN userProficient in using CVS and SVN features. Knows how to branch and merge, use patches setup repository properties etc.Knowledge of distributed VCS systems. Has tried out Bzr/Mercurial/Darcs/Gitbuild automationOnly knows how to build from IDEKnows how to build the system from the command lineCan setup a script to build the basic systemCan setup a script to build the system and also documentation, installers, generate release notes and tag the code in source controlautomated testingThinks that all testing is the job of the testerHas written automated unit tests and comes up with good unit test cases for the code that is being writtenHas written code in TDD mannerUnderstands and is able to setup automated functional, load/performance and UI testsProgramming2n (Level 0)n2 (Level 1)n (Level 2)log(n) (Level 3)Commentsproblem decompositionOnly straight line code with copy paste for reuseAble to break up problem into multiple functionsAble to come up with reusable functions/objects that solve the overall problemUse of appropriate data structures and algorithms and comes up with generic/object-oriented code that encapsulate aspects of the problem that are subject to change.systems decompositionNot able to think above the level of a single file/classAble to break up problem space and design solution as long as it is within the same platform/technologyAble to design systems that span multiple technologies/platforms.Able to visualize and design complex systems with multiple product lines and integrations with external systems. Also should be able to design operations support systems like monitoring, reporting, fail overs etc.communicationCannot express thoughts/ideas to peers. Poor spelling and grammar.Peers can understand what is being said. Good spelling and grammar.Is able to effectively communicate with peersAble to understand and communicate thoughts/design/ideas/specs in a unambiguous manner and adjusts communication as per the contextThis is an often under rated but very critical criteria for judging a programmer. With the increase in outsourcing of programming tasks to places where English is not the native tongue this issue has become more prominent. I know of several projects that failed because the programmers could not understand what the intent of the communication was.code organization within a fileno evidence of organization within a fileMethods are grouped logically or by accessibilityCode is grouped into regions and well commented with references to other source filesFile has license header, summary, well commented, consistent white space usage. The file should look beautiful.code organization across filesNo thought given to organizing code across filesRelated files are grouped into a folderEach physical file has a unique purpose, for e.g. one class definition, one feature implementation etc.Code organization at a physical level closely matches design and looking at file names and folder distribution provides insights into designsource tree organizationEverything in one folderBasic separation of code into logical folders.No circular dependencies, binaries, libs, docs, builds, third-party code all organized into appropriate foldersPhysical layout of source tree matches logical hierarchy and organization. The directory names and organization provide insights into the design of the system.The difference between this and the previous item is in the scale of organization, source tree organization relates to the entire set of artifacts that define the system.code readabilityMono-syllable namesGood names for files, variables classes, methods etc.No long functions, comments explaining unusual code, bug fixes, code assumptionsCode assumptions are verified using asserts, code flows naturally no deep nesting of conditionals or methodsdefensive codingDoesnt understand the conceptChecks all arguments and asserts critical assumptions in codeMakes sure to check return values and check for exceptions around code that can fail.Has his own library to help with defensive coding, writes unit tests that simulate faultserror handlingOnly codes the happy caseBasic error handling around code that can throw exceptions/generate errorsEnsures that error/exceptions leave program in good state, resources, connections and memory is all cleaned up properlyCodes to detect possible exception before, maintain consistent exception handling strategy in all layers of code, come up with guidelines on exception handling for entire system.IDEMostly uses IDE for text editingKnows their way around the interface, able to effectively use the IDE using menus.Knows keyboard shortcuts for most used operations.Has written custom macrosAPINeeds to look up the documentation frequentlyHas the most frequently used APIs in memoryVast and In-depth knowledge of the APIHas written libraries that sit on top of the API to simplify frequently used tasks and to fill in gaps in the APIE.g. of API can be Java library, .net framework or the custom API for the applicationframeworksHas not used any framework outside of the core platformHas heard about but not used the popular frameworks available for the platform.Has used more than one framework in a professional capacity and is well-versed with the idioms of the frameworks.Author of frameworkrequirementsTakes the given requirements and codes to specCome up with questions regarding missed cases in the specUnderstand complete picture and come up with entire areas that need to be specedAble to suggest better alternatives and flows to given requirements based on experiencescriptingNo knowledge of scripting toolsBatch files/shell scriptsPerl/Python/Ruby/VBScript/PowershellHas written and published reusable codedatabaseThinks that Excel is a databaseKnows basic database concepts, normalization, ACID, transactions and can write simple selectsAble to design good and normalized database schemas keeping in mind the queries thatll have to be run, proficient in use of views, stored procedures, triggers and user defined types. Knows difference between clustered and non-clustered indexes. Proficient in use of ORM tools.Can do basic database administration, performance optimization, index optimization, write advanced select queries, able to replace cursor usage with relational sql, understands how data is stored internally, understands how indexes are stored internally, understands how databases can be mirrored, replicated etc. Understands how the two phase commit works.Experience2n (Level 0)n2 (Level 1)n (Level 2)log(n) (Level 3)Commentslanguages with professional experienceImperative or Object OrientedImperative, Object-Oriented and declarative (SQL), added bonus if they understand static vs dynamic typing, weak vs strong typing and static inferred typesFunctional, added bonus if they understand lazy evaluation, currying, continuationsConcurrent (Erlang, Oz) and Logic (Prolog)platforms with professional experience12-34-56+years of professional experience12-56-910+domain knowledgeNo knowledge of the domainHas worked on at least one product in the domain.Has worked on multiple products in the same domain.Domain expert. Has designed and implemented several products/solutions in the domain. Well versed with standard terms, protocols used in the domain.Knowledgetool knowledgeLimited to primary IDE (VS.Net, Eclipse etc.)Knows about some alternatives to popular and standard tools.Good knowledge of editors, debuggers, IDEs, open source alternatives etc. etc. For e.g. someone who knows most of the tools from Scott Hanselmans power tools list. Has used ORM tools.Has actually written tools and scripts, added bonus if theyve been published.languages exposed toImperative or Object OrientedImperative, Object-Oriented and declarative (SQL), added bonus if they understand static vs dynamic typing, weak vs strong typing and static inferred typesFunctional, added bonus if they understand lazy evaluation, currying, continuationsConcurrent (Erlang, Oz) and Logic (Prolog)codebase knowledgeHas never looked at the codebaseBasic knowledge of the code layout and how to build the systemGood working knowledge of code base, has implemented several bug fixes and maybe some small features.Has implemented multiple big features in the codebase and can easily visualize the changes required for most features or bug fixes.knowledge of upcoming technologiesHas not heard of the upcoming technologiesHas heard of upcoming technologies in the fieldHas downloaded the alpha preview/CTP/beta and read some articles/manualsHas played with the previews and has actually built something with it and as a bonus shared that with everyone elseplatform internalsZero knowledge of platform internalsHas basic knowledge of how the platform works internallyDeep knowledge of platform internals and can visualize how the platform takes the program and converts it into executable code.Has written tools to enhance or provide information on platform internals. For e.g. disassemblers, decompilers, debuggers etc.booksUnleashed series, 21 days series, 24 hour series, dummies seriesCode Complete, Dont Make me Think, Mastering Regular ExpressionsDesign Patterns, Peopleware, Programming Pearls, Algorithm Design Manual, Pragmatic Programmer, Mythical Man monthStructure and Interpretation of Computer Programs, Concepts Techniques, Models of Computer Programming, Art of Computer Programming, Database systems , by C. J Date, Thinking Forth, Little SchemerblogsHas heard of them but never got the time.Reads tech/programming/software engineering blogs and listens to podcasts regularly.Maintains a link blog with some collection of useful articles and tools that he/she has collectedMaintains a blog in which personal insights and thoughts on programming are shared "],
        "story_type":"Normal",
        "url_raw":"https://sijinjoseph.com/programmer-competency-matrix/",
        "comments.comment_id":[20495872,
          20495945],
        "comments.comment_author":["knz42",
          "frigfog"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-07-22T07:06:30Z",
          "2019-07-22T07:28:41Z"],
        "comments.comment_text":["Just accumulating knowledge says nothing about what programmers actually can <i>do</i> on the workplace.<p>this is a better matrix: <a href=\"https://science.raphael.poss.name/programming-levels.html\" rel=\"nofollow\">https://science.raphael.poss.name/programming-levels.html</a>",
          "I'd say Fibonacci heaps are not worth learning for programmers.<p>I think augmenting datastructures should be added somewhere there.<p>> About people with high tc rating<p>In Theory, yes,that would ne great. But in practice high gaps in comptence are not good for team moral.<p>- Just like in highschool, people feel threatened and don't like looking stupid. So They overcompensate.( Nuclear plant problmes). In Germany,This made worse by a culture of never admitting that one doesn't understand. ( especially in Theory heavy stuff).<p>- object-oriented vs problem-oriented leads to communication problems.<p>- Comptitive programmers are not ( usually / on average) the most socially competent people. Which hurts team atmosphere.<p>- it's not worth the hassle: to ne honest most of what we do ils Boring CRUD. The programmer gets bored and the improvements are not worth the social cost."],
        "id":"df1d11de-ab27-4a1b-82ef-72fd0e32f6a5",
        "url_text":"Note that the knowledge for each level is cumulative; being atlevel n implies that you also know everything from thelevels lower than n.Computer Science2n (Level 0)n2 (Level 1)n (Level 2)log(n) (Level 3)Commentsdata structuresDoesnt know the difference between Array and LinkedListAble to explain and use Arrays, LinkedLists, Dictionaries etc in practical programming tasksKnows space and time tradeoffs of the basic data structures, Arrays vs LinkedLists, Able to explain how hashtables can be implemented and can handle collisions, Priority queues and ways to implement them etc.Knowledge of advanced data structures like B-trees, binomial and fibonacci heaps, AVL/Red Black trees, Splay Trees, Skip Lists, tries etc.algorithmsUnable to find the average of numbers in an array (Its hard to believe but Ive interviewed such candidates)Basic sorting, searching and data structure traversal and retrieval algorithmsTree, Graph, simple greedy and divide and conquer algorithms, is able to understand the relevance of the levels of this matrix.Able to recognize and code dynamic programming solutions, good knowledge of graph algorithms, good knowledge of numerical computation algorithms, able to identify NP problems etc.systems programmingDoesnt know what a compiler, linker or interpreter isBasic understanding of compilers, linker and interpreters. Understands what assembly code is and how things work at the hardware level. Some knowledge of virtual memory and paging.Understands kernel mode vs. user mode, multi-threading, synchronization primitives and how theyre implemented, able to read assembly code. Understands how networks work, understanding of network protocols and socket level programming.Understands the entire programming stack, hardware (CPU + Memory + Cache + Interrupts + microcode), binary code, assembly, static and dynamic linking, compilation, interpretation, JIT compilation, garbage collection, heap, stack, memory addressingSoftware Engineering2n (Level 0)n2 (Level 1)n (Level 2)log(n) (Level 3)Commentssource code version controlFolder backups by dateVSS and beginning CVS/SVN userProficient in using CVS and SVN features. Knows how to branch and merge, use patches setup repository properties etc.Knowledge of distributed VCS systems. Has tried out Bzr/Mercurial/Darcs/Gitbuild automationOnly knows how to build from IDEKnows how to build the system from the command lineCan setup a script to build the basic systemCan setup a script to build the system and also documentation, installers, generate release notes and tag the code in source controlautomated testingThinks that all testing is the job of the testerHas written automated unit tests and comes up with good unit test cases for the code that is being writtenHas written code in TDD mannerUnderstands and is able to setup automated functional, load/performance and UI testsProgramming2n (Level 0)n2 (Level 1)n (Level 2)log(n) (Level 3)Commentsproblem decompositionOnly straight line code with copy paste for reuseAble to break up problem into multiple functionsAble to come up with reusable functions/objects that solve the overall problemUse of appropriate data structures and algorithms and comes up with generic/object-oriented code that encapsulate aspects of the problem that are subject to change.systems decompositionNot able to think above the level of a single file/classAble to break up problem space and design solution as long as it is within the same platform/technologyAble to design systems that span multiple technologies/platforms.Able to visualize and design complex systems with multiple product lines and integrations with external systems. Also should be able to design operations support systems like monitoring, reporting, fail overs etc.communicationCannot express thoughts/ideas to peers. Poor spelling and grammar.Peers can understand what is being said. Good spelling and grammar.Is able to effectively communicate with peersAble to understand and communicate thoughts/design/ideas/specs in a unambiguous manner and adjusts communication as per the contextThis is an often under rated but very critical criteria for judging a programmer. With the increase in outsourcing of programming tasks to places where English is not the native tongue this issue has become more prominent. I know of several projects that failed because the programmers could not understand what the intent of the communication was.code organization within a fileno evidence of organization within a fileMethods are grouped logically or by accessibilityCode is grouped into regions and well commented with references to other source filesFile has license header, summary, well commented, consistent white space usage. The file should look beautiful.code organization across filesNo thought given to organizing code across filesRelated files are grouped into a folderEach physical file has a unique purpose, for e.g. one class definition, one feature implementation etc.Code organization at a physical level closely matches design and looking at file names and folder distribution provides insights into designsource tree organizationEverything in one folderBasic separation of code into logical folders.No circular dependencies, binaries, libs, docs, builds, third-party code all organized into appropriate foldersPhysical layout of source tree matches logical hierarchy and organization. The directory names and organization provide insights into the design of the system.The difference between this and the previous item is in the scale of organization, source tree organization relates to the entire set of artifacts that define the system.code readabilityMono-syllable namesGood names for files, variables classes, methods etc.No long functions, comments explaining unusual code, bug fixes, code assumptionsCode assumptions are verified using asserts, code flows naturally no deep nesting of conditionals or methodsdefensive codingDoesnt understand the conceptChecks all arguments and asserts critical assumptions in codeMakes sure to check return values and check for exceptions around code that can fail.Has his own library to help with defensive coding, writes unit tests that simulate faultserror handlingOnly codes the happy caseBasic error handling around code that can throw exceptions/generate errorsEnsures that error/exceptions leave program in good state, resources, connections and memory is all cleaned up properlyCodes to detect possible exception before, maintain consistent exception handling strategy in all layers of code, come up with guidelines on exception handling for entire system.IDEMostly uses IDE for text editingKnows their way around the interface, able to effectively use the IDE using menus.Knows keyboard shortcuts for most used operations.Has written custom macrosAPINeeds to look up the documentation frequentlyHas the most frequently used APIs in memoryVast and In-depth knowledge of the APIHas written libraries that sit on top of the API to simplify frequently used tasks and to fill in gaps in the APIE.g. of API can be Java library, .net framework or the custom API for the applicationframeworksHas not used any framework outside of the core platformHas heard about but not used the popular frameworks available for the platform.Has used more than one framework in a professional capacity and is well-versed with the idioms of the frameworks.Author of frameworkrequirementsTakes the given requirements and codes to specCome up with questions regarding missed cases in the specUnderstand complete picture and come up with entire areas that need to be specedAble to suggest better alternatives and flows to given requirements based on experiencescriptingNo knowledge of scripting toolsBatch files/shell scriptsPerl/Python/Ruby/VBScript/PowershellHas written and published reusable codedatabaseThinks that Excel is a databaseKnows basic database concepts, normalization, ACID, transactions and can write simple selectsAble to design good and normalized database schemas keeping in mind the queries thatll have to be run, proficient in use of views, stored procedures, triggers and user defined types. Knows difference between clustered and non-clustered indexes. Proficient in use of ORM tools.Can do basic database administration, performance optimization, index optimization, write advanced select queries, able to replace cursor usage with relational sql, understands how data is stored internally, understands how indexes are stored internally, understands how databases can be mirrored, replicated etc. Understands how the two phase commit works.Experience2n (Level 0)n2 (Level 1)n (Level 2)log(n) (Level 3)Commentslanguages with professional experienceImperative or Object OrientedImperative, Object-Oriented and declarative (SQL), added bonus if they understand static vs dynamic typing, weak vs strong typing and static inferred typesFunctional, added bonus if they understand lazy evaluation, currying, continuationsConcurrent (Erlang, Oz) and Logic (Prolog)platforms with professional experience12-34-56+years of professional experience12-56-910+domain knowledgeNo knowledge of the domainHas worked on at least one product in the domain.Has worked on multiple products in the same domain.Domain expert. Has designed and implemented several products/solutions in the domain. Well versed with standard terms, protocols used in the domain.Knowledgetool knowledgeLimited to primary IDE (VS.Net, Eclipse etc.)Knows about some alternatives to popular and standard tools.Good knowledge of editors, debuggers, IDEs, open source alternatives etc. etc. For e.g. someone who knows most of the tools from Scott Hanselmans power tools list. Has used ORM tools.Has actually written tools and scripts, added bonus if theyve been published.languages exposed toImperative or Object OrientedImperative, Object-Oriented and declarative (SQL), added bonus if they understand static vs dynamic typing, weak vs strong typing and static inferred typesFunctional, added bonus if they understand lazy evaluation, currying, continuationsConcurrent (Erlang, Oz) and Logic (Prolog)codebase knowledgeHas never looked at the codebaseBasic knowledge of the code layout and how to build the systemGood working knowledge of code base, has implemented several bug fixes and maybe some small features.Has implemented multiple big features in the codebase and can easily visualize the changes required for most features or bug fixes.knowledge of upcoming technologiesHas not heard of the upcoming technologiesHas heard of upcoming technologies in the fieldHas downloaded the alpha preview/CTP/beta and read some articles/manualsHas played with the previews and has actually built something with it and as a bonus shared that with everyone elseplatform internalsZero knowledge of platform internalsHas basic knowledge of how the platform works internallyDeep knowledge of platform internals and can visualize how the platform takes the program and converts it into executable code.Has written tools to enhance or provide information on platform internals. For e.g. disassemblers, decompilers, debuggers etc.booksUnleashed series, 21 days series, 24 hour series, dummies seriesCode Complete, Dont Make me Think, Mastering Regular ExpressionsDesign Patterns, Peopleware, Programming Pearls, Algorithm Design Manual, Pragmatic Programmer, Mythical Man monthStructure and Interpretation of Computer Programs, Concepts Techniques, Models of Computer Programming, Art of Computer Programming, Database systems , by C. J Date, Thinking Forth, Little SchemerblogsHas heard of them but never got the time.Reads tech/programming/software engineering blogs and listens to podcasts regularly.Maintains a link blog with some collection of useful articles and tools that he/she has collectedMaintains a blog in which personal insights and thoughts on programming are shared ",
        "_version_":1718441046590881792},
      {
        "story_id":19597910,
        "story_author":"mooreds",
        "story_descendants":9,
        "story_score":28,
        "story_time":"2019-04-07T16:35:27Z",
        "story_title":"Why the Cool Kids Are Playing Dungeons and Dragons",
        "search":["Why the Cool Kids Are Playing Dungeons and Dragons",
          "https://www.nytimes.com/2019/04/06/opinion/sunday/dungeons-and-dragons.html",
          "Fighting the dragon queen Tiamat is a much more satisfying way to spend time with my friends than social media ever was. April 6, 2019Credit...Kelsey BorchAnnalee NewitzMs. Newitz is a science journalist and novelist.I started playing Dungeons & Dragons right around the time I completely gave up on Facebook. It was a little less than a year ago, as the first stories broke about the Cambridge Analytica scandal. I was sick of the social media idea of friendship, defined as likes or shares or X knows the same 50 people you know. So when my friend Kate suggested we start a game of Dungeons & Dragons, I thought, Yes, Im going to get together with people face-to-face, without any hearting or retweeting, and were going to eat chips and fight those damn cultists who are trying to resurrect the evil, five-headed dragon queen Tiamat.Until then, I had played a little D&D as an adult, but I hadnt joined a group that met regularly. But I am basically the target demographic for Stranger Things. Like the characters on that show, I played D&D in the 1980s with a group of geeky guys every day at lunch throughout the sixth grade, slaying vegepygmies in a crashed spaceship and meeting the great demon Lolth in her sticky transdimensional web. Kate became our dungeon master, the narrator of our adventure, who sets the scene using maps, dice, flowery language and silly accents. We were joined by seven other friends around my dining room table, eager to take on the roles of fighting monk, rogue, sorcerer, warlock, paladin, bard and cleric. As soon as Kate told us to fill out our character sheets, I remembered the feeling of sheer awesomeness that had drawn me to the game when I was 11. I was about to become an Aarakocra cleric, a bird person with a divine connection to nature who could call down lightning, raise winds, grow plants from the barren earth and heal the dying with a touch.But D&D isnt only about inventing a more badass version of myself, with wings and magic powers instead of sneakers and a laptop. I was also drawn to the idea of building a social group whose baseline assumption was that wed see one another regularly. Theres a sense of purpose to the gathering.Using a few maps spread on the table, we chart our course, explaining to Kate and one another what we want to do next. And when Kate leaves us on a cliffhanger, theres no Hey, Ill text you later and maybe we can meet up. Of course well meet up again. The point of the game isnt to win; its to go adventuring together.Wizards of the Coast, the parent company of Dungeons & Dragons, reported that 8.6 million people played the game in 2017, its biggest year of sales in two decades. That mark was eclipsed in 2018, when D&D sales reportedly grew 30 percent. All of those D&D consumers are snapping up the Fifth Edition, a new rule set released in 2014 that emphasizes a flexible approach to combat and decision-making. New players dont need to learn as many arcane rules to get started, and sales of D&D starter kits skyrocketed. Adding to the newfound popularity are thousands of D&D games broadcast on YouTube and the live-stream service Twitch. Critical Role, a popular livestream and podcast, features actors playing the game. This surge of interest is no doubt also inspired by shows like Stranger Things and the D&D-esque world of Game of Thrones. We want to escape into fantasy worlds where we know who the bad guys are and our spells to banish evil actually work. In this way, D&D is similar to online games like World of Warcraft, where people take on imaginary identities, form a guild and shout at one another using headsets while fighting orcs. What makes D & D different is that we can never forget about the human beings behind the avatars. When a member of my group makes a bad choice, I cant look into his face and shout insults the way I would if we were playing online. Hes a person, and my friend, even if he also inexplicably decided to open an obviously booby-trapped trunk, get a faceful of poison and use up my last remaining healing spell.[As technology advances, will it continue to blur the lines between public and private? Sign up for Charlie Warzels limited-run newsletter to explore whats at stake and what you can do about it.] But online, my friend would be just another dude with leathery blue skin, not someone whose face might crumple in sadness if Im a jerk. Theres a toxic distance created by online gaming and social networks that allows us to pretend were not socializing with friends. Our empathy gets switched off. That may be one reason gamer arguments over fake countries and nonexistent knights can morph all too easily into hate-based social movements in the real world.Plus, even when things get heated during our D&D game and they do none of us can win by getting 10,000 of our friends to harass the person we disagree with.There are, of course, genuine friendships forged in online game worlds and on social media, and I dont mean to dismiss those. But after months of playing D&D with my friends, Im socializing on Twitter and other social media less than I did before. I dont click to see hundreds of half-lives flash before me in an instant. Instead, I look forward to an evening with a handful of people.What drove me away from Facebook wasnt just the fake friending. It was that fake friendship could be weaponized, used by a hostile government or group to manipulate us. When we fantasize together, in person, we always know that the bot army isnt real. We know that an insult can hurt. But online, we wear masks over masks. I still love the internet, but Id rather have a real friendship with a half-elf bard than a thousand faceless followers. "],
        "story_type":"Normal",
        "url_raw":"https://www.nytimes.com/2019/04/06/opinion/sunday/dungeons-and-dragons.html",
        "comments.comment_id":[19602429,
          19606347],
        "comments.comment_author":["diybrad",
          "wishinghand"],
        "comments.comment_descendants":[0,
          1],
        "comments.comment_time":["2019-04-08T05:40:42Z",
          "2019-04-08T15:55:13Z"],
        "comments.comment_text":["I literally started a group 2 weeks ago. Six 30-40s adults played for 5 hours straight and not a single person touched their phone the entire time.<p>Fucking magic",
          "Ive always considered myself nerdy, but when I was younger I thought I wasnt <i>that</i> nerdy to play Dungeons and Dragons. Years later a woman I had a crush on was starting a campaign with some mutual friends so of course I jumped in. When she left the group prematurely to go to a university, I stayed because it was just so much fun. These days I get bummed out if I dont have a weekly game of some sort to play.<p>The reason why its cool is that its essentially an unlimited budget, sandbox video game/film of any setting or dynamics you want. Most people think of Dungeons and Dragons as the only game of its type but there are so many more. Ive played in a futuristic cyberpunk dystopia, an episode of Star Trek, a blasted radioactive Mad Max landscape where different realities have messily converged, a medieval land where dragons are the ruling class, infiltrating a clockwork/steampunk aristocrat empire, and a bunch more that Im forgetting.<p>You also engage with people face to face, make food together, catch up on things, exercise improv skills, and goof around too. I once played a female Druid (Im a man) but used a weird version of my voice and claimed I had been cursed to speak like that).<p>With the exception of a trending multiplayer shooter game or two, I generally dont play computer games anymore. With the right people theres so much more nuance, depth, and variety with table top role playing games.<p>Im currently getting ready to be the GM of a Dungeon World campaign. I really like the variety of community made supplements available to it, and its philosophy of player->GM->player feedback loop driving the action."],
        "id":"f175e920-e4f0-43f8-a693-64b6181763d9",
        "url_text":"Fighting the dragon queen Tiamat is a much more satisfying way to spend time with my friends than social media ever was. April 6, 2019Credit...Kelsey BorchAnnalee NewitzMs. Newitz is a science journalist and novelist.I started playing Dungeons & Dragons right around the time I completely gave up on Facebook. It was a little less than a year ago, as the first stories broke about the Cambridge Analytica scandal. I was sick of the social media idea of friendship, defined as likes or shares or X knows the same 50 people you know. So when my friend Kate suggested we start a game of Dungeons & Dragons, I thought, Yes, Im going to get together with people face-to-face, without any hearting or retweeting, and were going to eat chips and fight those damn cultists who are trying to resurrect the evil, five-headed dragon queen Tiamat.Until then, I had played a little D&D as an adult, but I hadnt joined a group that met regularly. But I am basically the target demographic for Stranger Things. Like the characters on that show, I played D&D in the 1980s with a group of geeky guys every day at lunch throughout the sixth grade, slaying vegepygmies in a crashed spaceship and meeting the great demon Lolth in her sticky transdimensional web. Kate became our dungeon master, the narrator of our adventure, who sets the scene using maps, dice, flowery language and silly accents. We were joined by seven other friends around my dining room table, eager to take on the roles of fighting monk, rogue, sorcerer, warlock, paladin, bard and cleric. As soon as Kate told us to fill out our character sheets, I remembered the feeling of sheer awesomeness that had drawn me to the game when I was 11. I was about to become an Aarakocra cleric, a bird person with a divine connection to nature who could call down lightning, raise winds, grow plants from the barren earth and heal the dying with a touch.But D&D isnt only about inventing a more badass version of myself, with wings and magic powers instead of sneakers and a laptop. I was also drawn to the idea of building a social group whose baseline assumption was that wed see one another regularly. Theres a sense of purpose to the gathering.Using a few maps spread on the table, we chart our course, explaining to Kate and one another what we want to do next. And when Kate leaves us on a cliffhanger, theres no Hey, Ill text you later and maybe we can meet up. Of course well meet up again. The point of the game isnt to win; its to go adventuring together.Wizards of the Coast, the parent company of Dungeons & Dragons, reported that 8.6 million people played the game in 2017, its biggest year of sales in two decades. That mark was eclipsed in 2018, when D&D sales reportedly grew 30 percent. All of those D&D consumers are snapping up the Fifth Edition, a new rule set released in 2014 that emphasizes a flexible approach to combat and decision-making. New players dont need to learn as many arcane rules to get started, and sales of D&D starter kits skyrocketed. Adding to the newfound popularity are thousands of D&D games broadcast on YouTube and the live-stream service Twitch. Critical Role, a popular livestream and podcast, features actors playing the game. This surge of interest is no doubt also inspired by shows like Stranger Things and the D&D-esque world of Game of Thrones. We want to escape into fantasy worlds where we know who the bad guys are and our spells to banish evil actually work. In this way, D&D is similar to online games like World of Warcraft, where people take on imaginary identities, form a guild and shout at one another using headsets while fighting orcs. What makes D & D different is that we can never forget about the human beings behind the avatars. When a member of my group makes a bad choice, I cant look into his face and shout insults the way I would if we were playing online. Hes a person, and my friend, even if he also inexplicably decided to open an obviously booby-trapped trunk, get a faceful of poison and use up my last remaining healing spell.[As technology advances, will it continue to blur the lines between public and private? Sign up for Charlie Warzels limited-run newsletter to explore whats at stake and what you can do about it.] But online, my friend would be just another dude with leathery blue skin, not someone whose face might crumple in sadness if Im a jerk. Theres a toxic distance created by online gaming and social networks that allows us to pretend were not socializing with friends. Our empathy gets switched off. That may be one reason gamer arguments over fake countries and nonexistent knights can morph all too easily into hate-based social movements in the real world.Plus, even when things get heated during our D&D game and they do none of us can win by getting 10,000 of our friends to harass the person we disagree with.There are, of course, genuine friendships forged in online game worlds and on social media, and I dont mean to dismiss those. But after months of playing D&D with my friends, Im socializing on Twitter and other social media less than I did before. I dont click to see hundreds of half-lives flash before me in an instant. Instead, I look forward to an evening with a handful of people.What drove me away from Facebook wasnt just the fake friending. It was that fake friendship could be weaponized, used by a hostile government or group to manipulate us. When we fantasize together, in person, we always know that the bot army isnt real. We know that an insult can hurt. But online, we wear masks over masks. I still love the internet, but Id rather have a real friendship with a half-elf bard than a thousand faceless followers. ",
        "_version_":1718441024666206209},
      {
        "story_id":19601673,
        "story_author":"sohkamyung",
        "story_descendants":3,
        "story_score":9,
        "story_time":"2019-04-08T03:18:06Z",
        "story_title":"Cool Kids Are Playing Dungeons and Dragons",
        "search":["Cool Kids Are Playing Dungeons and Dragons",
          "https://www.nytimes.com/2019/04/06/opinion/sunday/dungeons-and-dragons.html",
          "Fighting the dragon queen Tiamat is a much more satisfying way to spend time with my friends than social media ever was. April 6, 2019Credit...Kelsey BorchAnnalee NewitzMs. Newitz is a science journalist and novelist.I started playing Dungeons & Dragons right around the time I completely gave up on Facebook. It was a little less than a year ago, as the first stories broke about the Cambridge Analytica scandal. I was sick of the social media idea of friendship, defined as likes or shares or X knows the same 50 people you know. So when my friend Kate suggested we start a game of Dungeons & Dragons, I thought, Yes, Im going to get together with people face-to-face, without any hearting or retweeting, and were going to eat chips and fight those damn cultists who are trying to resurrect the evil, five-headed dragon queen Tiamat.Until then, I had played a little D&D as an adult, but I hadnt joined a group that met regularly. But I am basically the target demographic for Stranger Things. Like the characters on that show, I played D&D in the 1980s with a group of geeky guys every day at lunch throughout the sixth grade, slaying vegepygmies in a crashed spaceship and meeting the great demon Lolth in her sticky transdimensional web. Kate became our dungeon master, the narrator of our adventure, who sets the scene using maps, dice, flowery language and silly accents. We were joined by seven other friends around my dining room table, eager to take on the roles of fighting monk, rogue, sorcerer, warlock, paladin, bard and cleric. As soon as Kate told us to fill out our character sheets, I remembered the feeling of sheer awesomeness that had drawn me to the game when I was 11. I was about to become an Aarakocra cleric, a bird person with a divine connection to nature who could call down lightning, raise winds, grow plants from the barren earth and heal the dying with a touch.But D&D isnt only about inventing a more badass version of myself, with wings and magic powers instead of sneakers and a laptop. I was also drawn to the idea of building a social group whose baseline assumption was that wed see one another regularly. Theres a sense of purpose to the gathering.Using a few maps spread on the table, we chart our course, explaining to Kate and one another what we want to do next. And when Kate leaves us on a cliffhanger, theres no Hey, Ill text you later and maybe we can meet up. Of course well meet up again. The point of the game isnt to win; its to go adventuring together.Wizards of the Coast, the parent company of Dungeons & Dragons, reported that 8.6 million people played the game in 2017, its biggest year of sales in two decades. That mark was eclipsed in 2018, when D&D sales reportedly grew 30 percent. All of those D&D consumers are snapping up the Fifth Edition, a new rule set released in 2014 that emphasizes a flexible approach to combat and decision-making. New players dont need to learn as many arcane rules to get started, and sales of D&D starter kits skyrocketed. Adding to the newfound popularity are thousands of D&D games broadcast on YouTube and the live-stream service Twitch. Critical Role, a popular livestream and podcast, features actors playing the game. This surge of interest is no doubt also inspired by shows like Stranger Things and the D&D-esque world of Game of Thrones. We want to escape into fantasy worlds where we know who the bad guys are and our spells to banish evil actually work. In this way, D&D is similar to online games like World of Warcraft, where people take on imaginary identities, form a guild and shout at one another using headsets while fighting orcs. What makes D & D different is that we can never forget about the human beings behind the avatars. When a member of my group makes a bad choice, I cant look into his face and shout insults the way I would if we were playing online. Hes a person, and my friend, even if he also inexplicably decided to open an obviously booby-trapped trunk, get a faceful of poison and use up my last remaining healing spell.[As technology advances, will it continue to blur the lines between public and private? Sign up for Charlie Warzels limited-run newsletter to explore whats at stake and what you can do about it.] But online, my friend would be just another dude with leathery blue skin, not someone whose face might crumple in sadness if Im a jerk. Theres a toxic distance created by online gaming and social networks that allows us to pretend were not socializing with friends. Our empathy gets switched off. That may be one reason gamer arguments over fake countries and nonexistent knights can morph all too easily into hate-based social movements in the real world.Plus, even when things get heated during our D&D game and they do none of us can win by getting 10,000 of our friends to harass the person we disagree with.There are, of course, genuine friendships forged in online game worlds and on social media, and I dont mean to dismiss those. But after months of playing D&D with my friends, Im socializing on Twitter and other social media less than I did before. I dont click to see hundreds of half-lives flash before me in an instant. Instead, I look forward to an evening with a handful of people.What drove me away from Facebook wasnt just the fake friending. It was that fake friendship could be weaponized, used by a hostile government or group to manipulate us. When we fantasize together, in person, we always know that the bot army isnt real. We know that an insult can hurt. But online, we wear masks over masks. I still love the internet, but Id rather have a real friendship with a half-elf bard than a thousand faceless followers. "],
        "story_type":"Normal",
        "url_raw":"https://www.nytimes.com/2019/04/06/opinion/sunday/dungeons-and-dragons.html",
        "comments.comment_id":[19602603,
          19605022],
        "comments.comment_author":["polotics",
          "arx1422"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-04-08T06:20:59Z",
          "2019-04-08T13:35:22Z"],
        "comments.comment_text":["I disagree! Cool kids have always made <i>their own</i> RPGs in high school!",
          "At least in NYC, there are now after school programs where they pick you up directly from elementary school and take you to play in professionally run D&D campaigns.  Plus there is no social stigma attached for the kids playing - its seen as \"cool\" as any other activity.  I am sad to bequeath to my children a climate ravaged over-indebted husk of a planet but at least in some crucial ways they have it better than I did!"],
        "id":"640b1235-8ce8-4d84-910d-5a0d97f97576",
        "url_text":"Fighting the dragon queen Tiamat is a much more satisfying way to spend time with my friends than social media ever was. April 6, 2019Credit...Kelsey BorchAnnalee NewitzMs. Newitz is a science journalist and novelist.I started playing Dungeons & Dragons right around the time I completely gave up on Facebook. It was a little less than a year ago, as the first stories broke about the Cambridge Analytica scandal. I was sick of the social media idea of friendship, defined as likes or shares or X knows the same 50 people you know. So when my friend Kate suggested we start a game of Dungeons & Dragons, I thought, Yes, Im going to get together with people face-to-face, without any hearting or retweeting, and were going to eat chips and fight those damn cultists who are trying to resurrect the evil, five-headed dragon queen Tiamat.Until then, I had played a little D&D as an adult, but I hadnt joined a group that met regularly. But I am basically the target demographic for Stranger Things. Like the characters on that show, I played D&D in the 1980s with a group of geeky guys every day at lunch throughout the sixth grade, slaying vegepygmies in a crashed spaceship and meeting the great demon Lolth in her sticky transdimensional web. Kate became our dungeon master, the narrator of our adventure, who sets the scene using maps, dice, flowery language and silly accents. We were joined by seven other friends around my dining room table, eager to take on the roles of fighting monk, rogue, sorcerer, warlock, paladin, bard and cleric. As soon as Kate told us to fill out our character sheets, I remembered the feeling of sheer awesomeness that had drawn me to the game when I was 11. I was about to become an Aarakocra cleric, a bird person with a divine connection to nature who could call down lightning, raise winds, grow plants from the barren earth and heal the dying with a touch.But D&D isnt only about inventing a more badass version of myself, with wings and magic powers instead of sneakers and a laptop. I was also drawn to the idea of building a social group whose baseline assumption was that wed see one another regularly. Theres a sense of purpose to the gathering.Using a few maps spread on the table, we chart our course, explaining to Kate and one another what we want to do next. And when Kate leaves us on a cliffhanger, theres no Hey, Ill text you later and maybe we can meet up. Of course well meet up again. The point of the game isnt to win; its to go adventuring together.Wizards of the Coast, the parent company of Dungeons & Dragons, reported that 8.6 million people played the game in 2017, its biggest year of sales in two decades. That mark was eclipsed in 2018, when D&D sales reportedly grew 30 percent. All of those D&D consumers are snapping up the Fifth Edition, a new rule set released in 2014 that emphasizes a flexible approach to combat and decision-making. New players dont need to learn as many arcane rules to get started, and sales of D&D starter kits skyrocketed. Adding to the newfound popularity are thousands of D&D games broadcast on YouTube and the live-stream service Twitch. Critical Role, a popular livestream and podcast, features actors playing the game. This surge of interest is no doubt also inspired by shows like Stranger Things and the D&D-esque world of Game of Thrones. We want to escape into fantasy worlds where we know who the bad guys are and our spells to banish evil actually work. In this way, D&D is similar to online games like World of Warcraft, where people take on imaginary identities, form a guild and shout at one another using headsets while fighting orcs. What makes D & D different is that we can never forget about the human beings behind the avatars. When a member of my group makes a bad choice, I cant look into his face and shout insults the way I would if we were playing online. Hes a person, and my friend, even if he also inexplicably decided to open an obviously booby-trapped trunk, get a faceful of poison and use up my last remaining healing spell.[As technology advances, will it continue to blur the lines between public and private? Sign up for Charlie Warzels limited-run newsletter to explore whats at stake and what you can do about it.] But online, my friend would be just another dude with leathery blue skin, not someone whose face might crumple in sadness if Im a jerk. Theres a toxic distance created by online gaming and social networks that allows us to pretend were not socializing with friends. Our empathy gets switched off. That may be one reason gamer arguments over fake countries and nonexistent knights can morph all too easily into hate-based social movements in the real world.Plus, even when things get heated during our D&D game and they do none of us can win by getting 10,000 of our friends to harass the person we disagree with.There are, of course, genuine friendships forged in online game worlds and on social media, and I dont mean to dismiss those. But after months of playing D&D with my friends, Im socializing on Twitter and other social media less than I did before. I dont click to see hundreds of half-lives flash before me in an instant. Instead, I look forward to an evening with a handful of people.What drove me away from Facebook wasnt just the fake friending. It was that fake friendship could be weaponized, used by a hostile government or group to manipulate us. When we fantasize together, in person, we always know that the bot army isnt real. We know that an insult can hurt. But online, we wear masks over masks. I still love the internet, but Id rather have a real friendship with a half-elf bard than a thousand faceless followers. ",
        "_version_":1718441024747995136},
      {
        "story_id":19812690,
        "story_author":"prostoalex",
        "story_descendants":73,
        "story_score":185,
        "story_time":"2019-05-02T20:46:01Z",
        "story_title":"Renewables predicted to beat coal for the first time in US later this year",
        "search":["Renewables predicted to beat coal for the first time in US later this year",
          "https://qz.com/1610977/solar-wind-plus-other-renewables-beat-coal-for-first-time-in-us/",
          "Coal began firing US homes and factories in the 1880s. A century later, the cheap, plentiful fuel was Americas primary one (pdf) for electricity generation.But its long reign is slowly coming to an end.In April, renewables eclipsed coal generation in the US for the first time. The Energy Information Administration estimates renewables outperformed coal by 16% in April and will generate 1.4% more in May.The seasonal nature of the business means electricity generation from coal will again exceed that of hydro, biomass, wind, solar, and geothermal sources later this year. But the trend is clear. In 2020, annual coal and renewable generation will approach parity.Coals proponents may dismiss these monthly and quarterly ups and downs in generation share as unimportant, but we believe they are indicative of the fundamental disruption happening across the electric generation sector, writes the energy-and-environment nonprofit Institute for Energy Economics and Financial Analysis. Renewable generation is catching up to coal, and faster than forecast. "],
        "story_type":"Normal",
        "url_raw":"https://qz.com/1610977/solar-wind-plus-other-renewables-beat-coal-for-first-time-in-us/",
        "url_text":"Coal began firing US homes and factories in the 1880s. A century later, the cheap, plentiful fuel was Americas primary one (pdf) for electricity generation.But its long reign is slowly coming to an end.In April, renewables eclipsed coal generation in the US for the first time. The Energy Information Administration estimates renewables outperformed coal by 16% in April and will generate 1.4% more in May.The seasonal nature of the business means electricity generation from coal will again exceed that of hydro, biomass, wind, solar, and geothermal sources later this year. But the trend is clear. In 2020, annual coal and renewable generation will approach parity.Coals proponents may dismiss these monthly and quarterly ups and downs in generation share as unimportant, but we believe they are indicative of the fundamental disruption happening across the electric generation sector, writes the energy-and-environment nonprofit Institute for Energy Economics and Financial Analysis. Renewable generation is catching up to coal, and faster than forecast. ",
        "comments.comment_id":[19813637,
          19813677],
        "comments.comment_author":["tedsanders",
          "cydonian_monk"],
        "comments.comment_descendants":[1,
          1],
        "comments.comment_time":["2019-05-02T22:36:47Z",
          "2019-05-02T22:42:57Z"],
        "comments.comment_text":["The article doesn't even get the units right. Generation is megawatt-hours per day, not megawatts per day. The source data from EIA shows this here: <a href=\"https://www.eia.gov/outlooks/steo/data/browser/#/?v=22&f=M&s=&start=201112&end=202012&id=&linechart=CLTO_US~RTTO_US&ctype=linechart&maptype=0&map=\" rel=\"nofollow\">https://www.eia.gov/outlooks/steo/data/browser/#/?v=22&f=M&s...</a><p>Also, the unnamed strawman coal proponents are probably correct that short-term variations are unimportant. What matters is the long-term trend. And the long-term trend is more renewables and less coal.",
          "Renewables continuing to grow in output is great news. Yet that's only part of the story.<p>Coal, at least domestic steam coal, is still being actively displaced by natural gas. That's also a significant factor in the downward trend this projection shows. This is covered in more detail in the original source [1] the linked QZ article draws from.<p>1: <a href=\"http://ieefa.org/ieefa-u-s-april-is-shaping-up-to-be-momentous-in-transition-from-coal-to-renewables/\" rel=\"nofollow\">http://ieefa.org/ieefa-u-s-april-is-shaping-up-to-be-momento...</a>"],
        "id":"2057f623-f98f-44b8-bbcb-db269ca38674",
        "_version_":1718441031281672193},
      {
        "story_id":21337020,
        "story_author":"fortran77",
        "story_descendants":14,
        "story_score":57,
        "story_time":"2019-10-23T17:48:06Z",
        "story_title":"A Visit to the Large Scale Systems Museum",
        "search":["A Visit to the Large Scale Systems Museum",
          "http://www.righto.com/2019/10/a-visit-to-large-scale-systems-museum.html?m=1",
          "I didn't expect to find two floors filled with vintage computers in a small town outside Pittsburgh. But that's the location of the Large Scale System Museum, housed in a former department store. The ground floor of this private collection concentrates on mainframes and minicomputers from the 1970s to 1990s featuring IBM, Cray, and DEC systems, along with less common computers. Amazingly, most of these vintage systems are working. Upstairs, the museum is filled with vintage home computers from the pre-PC era. IBM IBM set the standard for the mainframe computer with its introduction of the System/360 in 1964, a line of computers designed to support the full circle (i.e. 360) of business and scientific applications. The System/360 evolved into the System/370 in the 1970s and the System/390 in the 1990s. Most of these mainframes filled a data center, but the museum has some smaller S/370 and S/390 mainframes designed for offices. The IBM System/370 9375 (1986; below), is described as a \"baby mainframe\" or \"super-mini computer\" for engineering or commercial applications. IBM System/370 9375. The computer itself is in the middle rack. The left rack has a 3490E tape cartridge storage system, while the right rack holds 9335 disk controllers and disk drives (856 MB per drive). The System/390 line is represented by the IBM System/390 Multiprise-2003 (1997; below). This mainframe could not boot up on its own, but required a special desktop PC called the Mainframe Service Element (photo) to initialize the mainframe with microcode and start it up. This low-end IBM System/390 Multiprise-2003 had 1 GB of memory and supported hundreds of simultaneous database transactions. To support smaller customers, IBM also produced minicomputers, which they called \"midrange systems\". The IBM System/32 (1975; below) is a minicomputer built into a desk, designed for small businesses. IBM's midrange systems evolved into the IBM AS/400 (1992; photo). This IBM System/32 had 16 KB of memory and 13 MB of disk storage. It leased for $1200 per month. The museum has many disk drives and tape drives. One example is the massive 3380E disk drive (below; 1985), providing 5 gigabytes of storage. It's amazing to think that you can now hold a thousand times as much storage in your hand. The IBM 3380E disk system stored 5 gigabytes of data. The 14-inch disk platter is in the center, labeled \"E\". Cray Computer designer Seymour Cray and his company Cray Research were famed for building the world's fastest supercomputers. The museum has several Cray computers from the 1990s. The Cray YMP-EL supercomputer (1992; below) was an \"Entry Level\" Cray, costing $300,000. It was built from CMOS chips rather than the fast but hot ECL chips in earlier Crays, allowing it to be air-cooled rather than Freon cooled. The museum also has the related, low-end Cray EL-94, packaged in an ugly box (photo; 1992); The Cray YMP-EL supercomputer. The Cray J90 (1996; below) was a popular low-end Cray, an evolution of the Y-MP EL. This one holds 1 GB of memory and cost $300,000. Cray J 90 supercomputer. The Cray SV1 (1999; below) followed the J90. It introduced more high-performance features such as a vector cache and multi-streaming. This one has 16 processors and 16 GB of memory, and cost about $1 million. The Cray SV1 supercomputer. Digital Equipment Corporation (DEC) Dave McGuire, curator of the large systems, in front of PDP \"Straight 8\" minicomputers. Digital Equipment Corporation was founded in 1957 and became the second-largest computer manufacturer, concentrating on minicomputers. DEC's PDP-8 was a very popular 12-bit minicomputer that essentially created the \"minicomputer\" category of computers. The first PDP-8 was the Straight-8 (1966; photos above and below), a compact all-transistor computer built from circuit cards plugged into a wire-wrapped backplane. The \"Straight 8\" PDP-8 was built from transistorized circuits on small cards. The PDP-8/E (1969; below) used integrated circuits (7400-series TTL) in place of discrete transistors as did the compact and cheaper PDP-8/A (1974; photo). PDP-8/E minicomputer. The paper tape reader is at the top, above the front panel. An RK05 DECpack is at the bottom, storing 2.4 megabytes on a removable disk pack. DEC started producing mainframes in 1966 with the PDP-10, a 36-bit computer that popularized time-sharing. The museum has a DECsystem-2020 (1978), the smallest member of the PDP-10 family. A DECsystem-2020 mainframe next to an RM02 disk drive. The drive's removable disk packs each store 67 megabytes. In 1970, DEC introduced the 16-bit PDP-11, which became the most popular minicomputer with about 600,000 sold. The museum has many different PDP-11 models including the PDP-11/05 (1972; photo, console), the fast PDP-11/50 (1972; below, photo), the compact and popular PDP-11/34 (1976; photo), and the PDP-11/44 (1981; photo). Console of the PDP-11/50 minicomputer. DEC's PDP-11 evolved into the VAX line of 32-bit computers. Larger and more powerful than earlier minicomputers, these systems were known as superminicomputers. The VAX-11/780 (1978; below) was the first member of the VAX family, and was implemented with TTL chips. The museum has a VAX-11/750 (1980) and the cheap single-cabinet VAX-11/730 (1982; photo), the powerful VAX-6000 (1991; photo), and top-of-the-line VAX-7000 (1992; photo). The VAXstation 4000 Model 90 (1991; photo) was a workstation implementing the VAX instruction set. The VAX 11/780 \"superminicomputer\". DEC started struggling in the 1990s as the market shifted to personal computers. DEC was acquired in 1998 by personal computer manufacturer Compaq, which in turn was soon acquired by Hewlett-Packard in 2002. Other systems The museum has systems from many other companies such as Varian, Control Data, Wang, Panasonic, Silicon Graphics, Singer, and Tektronix, but I'll just touch on some highlights. Data General was a major producer of minicomputers, third behind DEC and IBM. The Data General Eclipse was the successor to the popular Data General Nova 16-bit minicomputer. It is represented in the museum by the Eclipse S/280 (1975; below) and Eclipse S/120 (1982; photo). Data General moved into the microcomputer market with the microNOVA (1977; photo), but it wasn't commercially successful. Data General Eclipse S/280 minicomputer. In the late 1970s, Hewlett-Packard was the fourth-largest producer of minicomputers. The HP 2116B minicomputer (1968; photo) was part of the HP 1000 (photo) family of 16-bit minicomputers designed for instrument control and automation. The HP 2645A terminal (below) was part of HP's line of terminals. HP 2645A terminal Another interesting terminal is the Friden Flexowriter from the early 1960s (below). It has a paper tape reader and punch on the left. Flexowriters were often used as console terminals for computers. Friden Flexowriter The Burroughs B80 is a multi-user office minicomputer (1978; below). It has as dot-matrix printer above the keyboard. The computer on display was used by a funeral home, and has a paper product list taped above the keyboard with products such as \"Tranquility urn\", \"Open/Close grave\", and \"Move dirt more than 25 miles\". The Burroughs B80 office minicomputer. The collection also includes analog computers, such as the Heathkit H-1 (1950s) which used vacuum tube amplifiers and represented values by signals from -100 to 100 volts. It could be programmed to solve differential equations by wiring the patch board. The museum also has a Comdyna GP-6 (photo), a more modern transistorized analog computer from the late 1960s. A Heathkit H1 analog computer. Vacuum tubes are on top, the plugboard is in the middle, and potentiometer controls are in the front. Microcomputers in the Large Scale Integration Museum Upstairs is the \"Large Scale Integration Museum\", a large collection of microcomputers of the 1970s and 1980s. The collection focuses on microcomputers before to the IBM PC and x86 processors. Since I'm more interested in the larger computers, I'll discuss this collection briefly, but I don't want to downplay its impressive scope. Corey Little, curator of the microcomputer collection, in front of Imsai, ASR-33 teletype, Kenbek-1 replica, and Altair. The first commercial microprocessor was Intel's 4-bit 4004, introduced in 1971. The Intel Intellec 4/40 development system (below), used the 4040 microprocessor (1974), an improved version of the 4004. This system was intended for engineers to develop software for embedded systems using the 4040 chip. Intel Intellec 4/40 development system. An EPROM socket below the key allowed software to be burned into EPROM chips. The microcomputer revolution took off when Intel released the 8-bit 8080 microprocessor in 1974, leading to the first commercially successful personal computer, the MITS Altair 8800 kit (1975). In addition to the Altair 8800, the museum has the updated Altair 8800b and the more obscure Altair 680, which uses the Motorola 6800 microprocessor. Altair 8800 (with the famous manifesto Computer Lib on top), Altair 680, Altair 8800b, and disk drive for Altair. Single-board computers also helped popularize microprocessors. Companies produced development kits for engineers to experiment with new microprocessors and hobbyists often used them due to their low cost. The museum has several racks of these development boards; the rack below includes the Intel SDK-85 System Design Kit for the 8085 microprocessor, Artisan Electronics Model 85 microcalculator (a single-board scientific calculator that could be interfaced to a microcomputer), Rockwell's 6502-based AIM-65, Synertek's 6502-based SYM-1, and Transputer parallel processor boards. A variety of development boards and single-board computers. By the late 1970s, microcomputers became mass-market products, with the introduction of home computers that were more affordable and usable by the general public. The museum has many other popular home computers from manufacturers such as Atari, Sinclair, Radio Shack, Heathkit, and Texas Instruments. The photo below shows part of the Commodore collection. The Commodore collection includes calculators, Commodore Super PET, Educator 64, PET 4032, and PET 2001 Early portable computers were suitcase-sized and often called luggables. The museum has a large collection including the IBM 5100 (1975; below), Osborne One (1981), Osborne Executive, Osborne Vixen, and Kaypro II, as well as more obscure machines such as the Telcon Zorba and General Electric Workmaster. The IBM 5100 portable computer was introduced in 1975, six years before the IBM PC. Its keyboard has special characters for the APL language. Apple is represented by a variety of Apple II, Apple III, Lisa, and Macintosh systems. The collection also includes a NeXTcube, the workstation developed by Steve Jobs in the 1980s after he was forced out of Apple. Steve Jobs returned to Apple when Apple purchased NeXT in 1997, leading to Apple's dramatic rise. The NeXTcube's operating system led to Apple's current macOS and iOS operating systems. The NeXTcube workstation was packaged in a 1-foot magnesium cube. The museum has various toys and educational devices that were produced to explain computers, including the CALCULO Analog Computer (1959), Minivac 6010 (1962) created by the father of information theory Claude Shannon, Radio Shack Science Fair Digital Computer Kit (1977), and Digi-Comp 1 (1963). The collection includes toy computers such as the CALCULO Analog Computer, MINIVAC 6010, Radio Shack ScienceFair Digital Computer, and Digi-Comp 1. Heathkit introduced the HERO-1 kit robot in 1982, providing a way for hobbyists to experiment with robotics. Nowadays, Arduinos and cheap servos and stepper motors make it easy to build a simple robot, but in 1982, robotics was much more difficult. The HERO-1 kit cost $1500 (equivalent to about $4000 today). Three Heathkit HERO robots. The HERO 2000 (1986, left) included multiple processors and speech synthesis, while the older HERO-1 robots have a single 6808 processor. The \"eyes\" are an ultrasonic distance sensor. Conclusion The Large Scale Systems Museum contains a remarkable collection of large computer systems and microcomputers from the 1970s to 1990s The museum, hidden behind a storefront on a quiet small-town main street, illustrates an interesting period in computer history. During this time, mainframes, minicomputers, and supercomputers reached their peak and then went into steep decline. Meanwhile, the microprocessor passed through the hobbyist phase and the home computer phase before achieving its dominance. Amazingly most of the systems at the museum are up and running, giving the visitor a feel for the computers of that era. The museum is open by appointment only; details are here and on their Facebook page. If you ever find yourself near New Kensington, PA (half an hour outside Pittsburgh), get in touch with them. I've only presented the highlights of the museum; more photos are here. I announce my latest blog posts on Twitter, so follow me @kenshirriff for future articles. I also have an RSS feed. "],
        "story_type":"Normal",
        "url_raw":"http://www.righto.com/2019/10/a-visit-to-large-scale-systems-museum.html?m=1",
        "comments.comment_id":[21339038,
          21339183],
        "comments.comment_author":["EvanAnderson",
          "geephroh"],
        "comments.comment_descendants":[2,
          1],
        "comments.comment_time":["2019-10-23T21:08:09Z",
          "2019-10-23T21:27:30Z"],
        "comments.comment_text":["This looks really cool, and it's only a 4.5 hour drive.<p>As an aside: Is anybody familiar with a curated list anywhere of interesting \"geek\" tourist destinations? Just today I've seen this museum and the \"rotary jail\"-turned museums [1]. I would love to plan road trips around these kinds of attractions.<p>[1] - <a href=\"https://news.ycombinator.com/item?id=21337410\" rel=\"nofollow\">https://news.ycombinator.com/item?id=21337410</a>",
          "And check out the Living Computer Museum if you're in Seattle (<a href=\"https://livingcomputers.org/\" rel=\"nofollow\">https://livingcomputers.org/</a>). They have a ton of systems you can play with including a CDC 6500 and a DEC PDP-8e."],
        "id":"035641e2-56b7-4f35-99a6-fbe2d47e31cd",
        "url_text":"I didn't expect to find two floors filled with vintage computers in a small town outside Pittsburgh. But that's the location of the Large Scale System Museum, housed in a former department store. The ground floor of this private collection concentrates on mainframes and minicomputers from the 1970s to 1990s featuring IBM, Cray, and DEC systems, along with less common computers. Amazingly, most of these vintage systems are working. Upstairs, the museum is filled with vintage home computers from the pre-PC era. IBM IBM set the standard for the mainframe computer with its introduction of the System/360 in 1964, a line of computers designed to support the full circle (i.e. 360) of business and scientific applications. The System/360 evolved into the System/370 in the 1970s and the System/390 in the 1990s. Most of these mainframes filled a data center, but the museum has some smaller S/370 and S/390 mainframes designed for offices. The IBM System/370 9375 (1986; below), is described as a \"baby mainframe\" or \"super-mini computer\" for engineering or commercial applications. IBM System/370 9375. The computer itself is in the middle rack. The left rack has a 3490E tape cartridge storage system, while the right rack holds 9335 disk controllers and disk drives (856 MB per drive). The System/390 line is represented by the IBM System/390 Multiprise-2003 (1997; below). This mainframe could not boot up on its own, but required a special desktop PC called the Mainframe Service Element (photo) to initialize the mainframe with microcode and start it up. This low-end IBM System/390 Multiprise-2003 had 1 GB of memory and supported hundreds of simultaneous database transactions. To support smaller customers, IBM also produced minicomputers, which they called \"midrange systems\". The IBM System/32 (1975; below) is a minicomputer built into a desk, designed for small businesses. IBM's midrange systems evolved into the IBM AS/400 (1992; photo). This IBM System/32 had 16 KB of memory and 13 MB of disk storage. It leased for $1200 per month. The museum has many disk drives and tape drives. One example is the massive 3380E disk drive (below; 1985), providing 5 gigabytes of storage. It's amazing to think that you can now hold a thousand times as much storage in your hand. The IBM 3380E disk system stored 5 gigabytes of data. The 14-inch disk platter is in the center, labeled \"E\". Cray Computer designer Seymour Cray and his company Cray Research were famed for building the world's fastest supercomputers. The museum has several Cray computers from the 1990s. The Cray YMP-EL supercomputer (1992; below) was an \"Entry Level\" Cray, costing $300,000. It was built from CMOS chips rather than the fast but hot ECL chips in earlier Crays, allowing it to be air-cooled rather than Freon cooled. The museum also has the related, low-end Cray EL-94, packaged in an ugly box (photo; 1992); The Cray YMP-EL supercomputer. The Cray J90 (1996; below) was a popular low-end Cray, an evolution of the Y-MP EL. This one holds 1 GB of memory and cost $300,000. Cray J 90 supercomputer. The Cray SV1 (1999; below) followed the J90. It introduced more high-performance features such as a vector cache and multi-streaming. This one has 16 processors and 16 GB of memory, and cost about $1 million. The Cray SV1 supercomputer. Digital Equipment Corporation (DEC) Dave McGuire, curator of the large systems, in front of PDP \"Straight 8\" minicomputers. Digital Equipment Corporation was founded in 1957 and became the second-largest computer manufacturer, concentrating on minicomputers. DEC's PDP-8 was a very popular 12-bit minicomputer that essentially created the \"minicomputer\" category of computers. The first PDP-8 was the Straight-8 (1966; photos above and below), a compact all-transistor computer built from circuit cards plugged into a wire-wrapped backplane. The \"Straight 8\" PDP-8 was built from transistorized circuits on small cards. The PDP-8/E (1969; below) used integrated circuits (7400-series TTL) in place of discrete transistors as did the compact and cheaper PDP-8/A (1974; photo). PDP-8/E minicomputer. The paper tape reader is at the top, above the front panel. An RK05 DECpack is at the bottom, storing 2.4 megabytes on a removable disk pack. DEC started producing mainframes in 1966 with the PDP-10, a 36-bit computer that popularized time-sharing. The museum has a DECsystem-2020 (1978), the smallest member of the PDP-10 family. A DECsystem-2020 mainframe next to an RM02 disk drive. The drive's removable disk packs each store 67 megabytes. In 1970, DEC introduced the 16-bit PDP-11, which became the most popular minicomputer with about 600,000 sold. The museum has many different PDP-11 models including the PDP-11/05 (1972; photo, console), the fast PDP-11/50 (1972; below, photo), the compact and popular PDP-11/34 (1976; photo), and the PDP-11/44 (1981; photo). Console of the PDP-11/50 minicomputer. DEC's PDP-11 evolved into the VAX line of 32-bit computers. Larger and more powerful than earlier minicomputers, these systems were known as superminicomputers. The VAX-11/780 (1978; below) was the first member of the VAX family, and was implemented with TTL chips. The museum has a VAX-11/750 (1980) and the cheap single-cabinet VAX-11/730 (1982; photo), the powerful VAX-6000 (1991; photo), and top-of-the-line VAX-7000 (1992; photo). The VAXstation 4000 Model 90 (1991; photo) was a workstation implementing the VAX instruction set. The VAX 11/780 \"superminicomputer\". DEC started struggling in the 1990s as the market shifted to personal computers. DEC was acquired in 1998 by personal computer manufacturer Compaq, which in turn was soon acquired by Hewlett-Packard in 2002. Other systems The museum has systems from many other companies such as Varian, Control Data, Wang, Panasonic, Silicon Graphics, Singer, and Tektronix, but I'll just touch on some highlights. Data General was a major producer of minicomputers, third behind DEC and IBM. The Data General Eclipse was the successor to the popular Data General Nova 16-bit minicomputer. It is represented in the museum by the Eclipse S/280 (1975; below) and Eclipse S/120 (1982; photo). Data General moved into the microcomputer market with the microNOVA (1977; photo), but it wasn't commercially successful. Data General Eclipse S/280 minicomputer. In the late 1970s, Hewlett-Packard was the fourth-largest producer of minicomputers. The HP 2116B minicomputer (1968; photo) was part of the HP 1000 (photo) family of 16-bit minicomputers designed for instrument control and automation. The HP 2645A terminal (below) was part of HP's line of terminals. HP 2645A terminal Another interesting terminal is the Friden Flexowriter from the early 1960s (below). It has a paper tape reader and punch on the left. Flexowriters were often used as console terminals for computers. Friden Flexowriter The Burroughs B80 is a multi-user office minicomputer (1978; below). It has as dot-matrix printer above the keyboard. The computer on display was used by a funeral home, and has a paper product list taped above the keyboard with products such as \"Tranquility urn\", \"Open/Close grave\", and \"Move dirt more than 25 miles\". The Burroughs B80 office minicomputer. The collection also includes analog computers, such as the Heathkit H-1 (1950s) which used vacuum tube amplifiers and represented values by signals from -100 to 100 volts. It could be programmed to solve differential equations by wiring the patch board. The museum also has a Comdyna GP-6 (photo), a more modern transistorized analog computer from the late 1960s. A Heathkit H1 analog computer. Vacuum tubes are on top, the plugboard is in the middle, and potentiometer controls are in the front. Microcomputers in the Large Scale Integration Museum Upstairs is the \"Large Scale Integration Museum\", a large collection of microcomputers of the 1970s and 1980s. The collection focuses on microcomputers before to the IBM PC and x86 processors. Since I'm more interested in the larger computers, I'll discuss this collection briefly, but I don't want to downplay its impressive scope. Corey Little, curator of the microcomputer collection, in front of Imsai, ASR-33 teletype, Kenbek-1 replica, and Altair. The first commercial microprocessor was Intel's 4-bit 4004, introduced in 1971. The Intel Intellec 4/40 development system (below), used the 4040 microprocessor (1974), an improved version of the 4004. This system was intended for engineers to develop software for embedded systems using the 4040 chip. Intel Intellec 4/40 development system. An EPROM socket below the key allowed software to be burned into EPROM chips. The microcomputer revolution took off when Intel released the 8-bit 8080 microprocessor in 1974, leading to the first commercially successful personal computer, the MITS Altair 8800 kit (1975). In addition to the Altair 8800, the museum has the updated Altair 8800b and the more obscure Altair 680, which uses the Motorola 6800 microprocessor. Altair 8800 (with the famous manifesto Computer Lib on top), Altair 680, Altair 8800b, and disk drive for Altair. Single-board computers also helped popularize microprocessors. Companies produced development kits for engineers to experiment with new microprocessors and hobbyists often used them due to their low cost. The museum has several racks of these development boards; the rack below includes the Intel SDK-85 System Design Kit for the 8085 microprocessor, Artisan Electronics Model 85 microcalculator (a single-board scientific calculator that could be interfaced to a microcomputer), Rockwell's 6502-based AIM-65, Synertek's 6502-based SYM-1, and Transputer parallel processor boards. A variety of development boards and single-board computers. By the late 1970s, microcomputers became mass-market products, with the introduction of home computers that were more affordable and usable by the general public. The museum has many other popular home computers from manufacturers such as Atari, Sinclair, Radio Shack, Heathkit, and Texas Instruments. The photo below shows part of the Commodore collection. The Commodore collection includes calculators, Commodore Super PET, Educator 64, PET 4032, and PET 2001 Early portable computers were suitcase-sized and often called luggables. The museum has a large collection including the IBM 5100 (1975; below), Osborne One (1981), Osborne Executive, Osborne Vixen, and Kaypro II, as well as more obscure machines such as the Telcon Zorba and General Electric Workmaster. The IBM 5100 portable computer was introduced in 1975, six years before the IBM PC. Its keyboard has special characters for the APL language. Apple is represented by a variety of Apple II, Apple III, Lisa, and Macintosh systems. The collection also includes a NeXTcube, the workstation developed by Steve Jobs in the 1980s after he was forced out of Apple. Steve Jobs returned to Apple when Apple purchased NeXT in 1997, leading to Apple's dramatic rise. The NeXTcube's operating system led to Apple's current macOS and iOS operating systems. The NeXTcube workstation was packaged in a 1-foot magnesium cube. The museum has various toys and educational devices that were produced to explain computers, including the CALCULO Analog Computer (1959), Minivac 6010 (1962) created by the father of information theory Claude Shannon, Radio Shack Science Fair Digital Computer Kit (1977), and Digi-Comp 1 (1963). The collection includes toy computers such as the CALCULO Analog Computer, MINIVAC 6010, Radio Shack ScienceFair Digital Computer, and Digi-Comp 1. Heathkit introduced the HERO-1 kit robot in 1982, providing a way for hobbyists to experiment with robotics. Nowadays, Arduinos and cheap servos and stepper motors make it easy to build a simple robot, but in 1982, robotics was much more difficult. The HERO-1 kit cost $1500 (equivalent to about $4000 today). Three Heathkit HERO robots. The HERO 2000 (1986, left) included multiple processors and speech synthesis, while the older HERO-1 robots have a single 6808 processor. The \"eyes\" are an ultrasonic distance sensor. Conclusion The Large Scale Systems Museum contains a remarkable collection of large computer systems and microcomputers from the 1970s to 1990s The museum, hidden behind a storefront on a quiet small-town main street, illustrates an interesting period in computer history. During this time, mainframes, minicomputers, and supercomputers reached their peak and then went into steep decline. Meanwhile, the microprocessor passed through the hobbyist phase and the home computer phase before achieving its dominance. Amazingly most of the systems at the museum are up and running, giving the visitor a feel for the computers of that era. The museum is open by appointment only; details are here and on their Facebook page. If you ever find yourself near New Kensington, PA (half an hour outside Pittsburgh), get in touch with them. I've only presented the highlights of the museum; more photos are here. I announce my latest blog posts on Twitter, so follow me @kenshirriff for future articles. I also have an RSS feed. ",
        "_version_":1718441068288016384},
      {
        "story_id":20543223,
        "story_author":"EndXA",
        "story_descendants":35,
        "story_score":83,
        "story_time":"2019-07-27T18:02:03Z",
        "story_title":"Antikythera Mechanism",
        "search":["Antikythera Mechanism",
          "https://en.wikipedia.org/wiki/Antikythera_mechanism",
          "Antikythera mechanismThe Antikythera mechanism (Fragment A front and rear); visible is the largest gear in the mechanism, approximately 13 centimetres (5.1in) in diameter.WritingAncient GreekPeriod/cultureHellenisticDiscovered1901Antikythera, GreecePresent locationNational Archaeological Museum, Athens The Antikythera mechanism ( AN-tih-kih-THEER-) is an ancient Greek hand-powered orrery, described as the oldest example of an analogue computer[1][2][3] used to predict astronomical positions and eclipses decades in advance.[4][5][6] It could also be used to track the four-year cycle of athletic games which was similar to an Olympiad, the cycle of the ancient Olympic Games.[7][8][9] This artefact was among wreckage retrieved from a shipwreck off the coast of the Greek island Antikythera in 1901.[10][11] On 17 May 1902 it was identified as containing a gear by archaeologist Valerios Stais.[12] The device, housed in the remains of a 34cm 18cm 9cm (13.4in 7.1in 3.5in) wooden box, was found as one lump, later separated into three main fragments which are now divided into 82 separate fragments after conservation efforts. Four of these fragments contain gears, while inscriptions are found on many others.[13][14] The largest gear is approximately 13 centimetres (5.1in) in diameter and originally had 223 teeth.[15] In 2008, a team led by Mike Edmunds and Tony Freeth at Cardiff University used modern computer x-ray tomography and high resolution surface scanning to image inside fragments of the crust-encased mechanism and read the faintest inscriptions that once covered the outer casing of the machine. This suggests it had 37 meshing bronze gears enabling it to follow the movements of the Moon and the Sun through the zodiac, to predict eclipses and to model the irregular orbit of the Moon, where the Moon's velocity is higher in its perigee than in its apogee. This motion was studied in the 2nd century BC by astronomer Hipparchus of Rhodes, and it is speculated that he may have been consulted in the machine's construction.[16] There is speculation that a portion of the mechanism is missing and it also calculated the positions of the five classical planets. The instrument is believed to have been designed and constructed by Greek scientists and has been variously dated to about 87BC,[17] or between 150 and 100BC,[4] or to 205BC,[18][19] or to within a generation before the shipwreck, which has been dated to approximately 7060BC.[20][21] Later clockwork is known from the medieval Byzantine and Islamic worlds, but works with similar complexity did not appear again until the development of mechanical astronomical clocks in Europe in the fourteenth century.[22] All known fragments of the Antikythera mechanism are now kept at the National Archaeological Museum in Athens, along with a number of artistic reconstructions and replicas[23][24] to demonstrate how it may have looked and worked.[25] History[edit] Discovery[edit] Captain Dimitrios Kontos ( ) and a crew of sponge divers from Symi island discovered the Antikythera shipwreck during the spring of 1900, and recovered artefacts during the first expedition with the Hellenic Royal Navy, in 190001.[26] This wreck of a Roman cargo ship was found at a depth of 45 metres (148ft) off Point Glyphadia on the Greek island of Antikythera. The team retrieved numerous large artefacts, including bronze and marble statues, pottery, unique glassware, jewellery, coins, and the mechanism. The mechanism was retrieved from the wreckage in 1901, most probably in July of that year.[27] It is not known how the mechanism came to be on the cargo ship, but it has been suggested that it was being taken from Rhodes to Rome, together with other looted treasure, to support a triumphal parade being staged by Julius Caesar.[28] All of the items retrieved from the wreckage were transferred to the National Museum of Archaeology in Athens for storage and analysis. The mechanism appeared at the time to be little more than a lump of corroded bronze and wood; it went unnoticed for two years, while museum staff worked on piecing together more obvious treasures, such as the statues.[22] On 17 May 1902, archaeologist Valerios Stais found that one of the pieces of rock had a gear wheel embedded in it. He initially believed that it was an astronomical clock, but most scholars considered the device to be prochronistic, too complex to have been constructed during the same period as the other pieces that had been discovered. Investigations into the object were dropped until British science historian and Yale University professor Derek J. de Solla Price became interested in it in 1951.[29] In 1971, Price and Greek nuclear physicist Charalampos Karakalos made X-ray and gamma-ray images of the 82 fragments. Price published an extensive 70-page paper on their findings in 1974.[11] Two other searches for items at the Antikythera wreck site in 2012 and 2015 have yielded a number of fascinating art objects and a second ship which may or may not be connected with the treasure ship on which the Mechanism was found.[30] Also found was a bronze disc, embellished with the image of a bull. The disc has four \"ears\" which have holes in them, and it was thought by some that it may have been part of the Antikythera Mechanism itself, as a \"cog wheel\". However, there appears to be little evidence that it was part of the Mechanism; it is more likely that the disc was a bronze decoration on a piece of furniture.[31] Origin[edit] The Antikythera mechanism is generally referred to as the first known analogue computer.[32] The quality and complexity of the mechanism's manufacture suggests that it must have had undiscovered predecessors made during the Hellenistic period.[33] Its construction relied on theories of astronomy and mathematics developed by Greek astronomers during the second century BC, and it is estimated to have been built in the late second century BC[4] or the early first century BC.[34][5] In 2008, continued research by the Antikythera Mechanism Research Project suggested that the concept for the mechanism may have originated in the colonies of Corinth, since they identified the calendar on the Metonic Spiral as coming from Corinth or one of its colonies in northwest Greece or Sicily.[7] Syracuse was a colony of Corinth and the home of Archimedes, and the Antikythera Mechanism Research project argued in 2008 that it might imply a connection with the school of Archimedes.[7] However, it was demonstrated in 2017 that the calendar on the Metonic Spiral is indeed of the Corinthian type but cannot be that of Syracuse.[35] Another theory suggests that coins found by Jacques Cousteau at the wreck site in the 1970s date to the time of the device's construction, and posits that its origin may have been from the ancient Greek city of Pergamon,[36] home of the Library of Pergamum. With its many scrolls of art and science, it was second in importance only to the Library of Alexandria during the Hellenistic period.[37] The ship carrying the device also contained vases in the Rhodian style, leading to a hypothesis that it was constructed at an academy founded by Stoic philosopher Posidonius on that Greek island.[38] Rhodes was a busy trading port in antiquity and a centre of astronomy and mechanical engineering, home to astronomer Hipparchus, who was active from about 140 BC to 120 BC. The mechanism uses Hipparchus' theory for the motion of the Moon, which suggests the possibility that he may have designed it or at least worked on it.[22] In addition, it has recently been argued that the astronomical events on the Parapegma of the Antikythera mechanism work best for latitudes in the range of 33.337.0 degrees north;[39] the island of Rhodes is located between the latitudes of 35.85 and 36.50 degrees north. In 2014, a study by Carman and Evans argued for a new dating of approximately 200 BC based on identifying the start-up date on the Saros Dial as the astronomical lunar month that began shortly after the new moon of 28 April 205 BC.[18][19] Moreover, according to Carman and Evans, the Babylonian arithmetic style of prediction fits much better with the device's predictive models than the traditional Greek trigonometric style.[18] A study by Paul Iversen published in 2017 reasons that the prototype for the device was indeed from Rhodes, but that this particular model was modified for a client from Epirus in northwestern Greece; Iversen argues that it was probably constructed no earlier than a generation before the shipwreck, a date supported also by Jones.[40] Further dives were undertaken in 2014, with plans to continue in 2015, in the hope of discovering more of the mechanism.[19] A five-year programme of investigations began in 2014 and ended in October 2019, with a new five-year session starting in May 2020.[41][42] Description[edit] The original mechanism apparently came out of the Mediterranean as a single encrusted piece. Soon afterward it fractured into three major pieces. Other small pieces have broken off in the interim from cleaning and handling,[43] and still others were found on the sea floor by the Cousteau expedition. Other fragments may still be in storage, undiscovered since their initial recovery; Fragment F was discovered in that way in 2005. Of the 82 known fragments, seven are mechanically significant and contain the majority of the mechanism and inscriptions. There are also 16 smaller parts that contain fractional and incomplete inscriptions.[4][7][44] Major fragments[edit] Fragment Size [mm] Weight [g] Gears Inscriptions Notes A 180150 369.1 27 Yes The main fragment contains the majority of the known mechanism. Clearly visible on the front is the large b1 gear, and under closer inspection further gears behind said gear (parts of the l, m, c, and d trains are clearly visible as gears to the naked eye). The crank mechanism socket and the side-mounted gear that meshes with b1 is on Fragment A. The back of the fragment contains the rearmost e and k gears for synthesis of the moon anomaly, noticeable also is the pin and slot mechanism of the k train. It is noticed from detailed scans of the fragment that all gears are very closely packed and have sustained damage and displacement due to their years in the sea. The fragment is approximately 30mm thick at its thickest point. Fragment A also contains divisions of the upper left quarter of the Saros spiral and 14 inscriptions from said spiral. The fragment also contains inscriptions for the Exeligmos dial and visible on the back surface the remnants of the dial face. Finally, this fragment contains some back door inscriptions. B 12560 99.4 1 Yes Contains approximately the bottom right third of the Metonic spiral and inscriptions of both the spiral and back door of the mechanism. The Metonic scale would have consisted of 235 cells of which 49 have been deciphered from fragment B either in whole or partially. The rest so far are assumed from knowledge of the Metonic cycle. This fragment also contains a single gear (o1) used in the Olympic train. C 120110 63.8 1 Yes Contains parts of the upper right of the front dial face showing calendar and zodiac inscriptions. This fragment also contains the Moon indicator dial assembly including the Moon phase sphere in its housing and a single bevel gear (ma1) used in the Moon phase indication system. D 4535 15.0 1 Contains at least one unknown gear; according to Michael T. Wright it contains possibly two, and according to Xenophon Moussas[45] it contains one gear (numbered 45 \"ME\") inside a hollow gear giving the position of Jupiter reproducing it with epicyclic motion. Their purpose and position has not been ascertained to any accuracy or consensus, but lends to the debate for the possible planet displays on the face of the mechanism. E 6035 22.1 Yes Found in 1976 and contains six inscriptions from the upper right of the Saros spiral. F 9080 86.2 Yes Found in 2005 and contains 16 inscriptions from the lower right of the Saros spiral. It also contains remnants of the mechanism's wooden housing. G 125110 31.7 Yes A combination of fragments taken from fragment C while cleaning. Minor fragments[edit] Many of the smaller fragments that have been found contain nothing of apparent value; however, a few have some inscriptions on them. Fragment 19 contains significant back door inscriptions including one reading \"...76 years...\" which refers to the Callippic cycle. Other inscriptions seem to describe the function of the back dials. In addition to this important minor fragment, 15 further minor fragments have remnants of inscriptions on them.[15]:7 Mechanics[edit] Information on the specific data gleaned from the ruins by the latest inquiries is detailed in the supplement to Freeth's 2006 Nature article.[4] Operation[edit] On the front face of the mechanism there is a fixed ring dial representing the ecliptic, the twelve zodiacal signs marked off with equal 30-degree sectors. This matched with the Babylonian custom of assigning one twelfth of the ecliptic to each zodiac sign equally, even though the constellation boundaries were variable. Outside that dial is another ring which is rotatable, marked off with the months and days of the Sothic Egyptian calendar, twelve months of 30 days plus five intercalary days. The months are marked with the Egyptian names for the months transcribed into the Greek alphabet. The first task, then, is to rotate the Egyptian calendar ring to match the current zodiac points. The Egyptian calendar ignored leap days, so it advanced through a full zodiac sign in about 120 years.[5] The mechanism was operated by turning a small hand crank (now lost) which was linked via a crown gear to the largest gear, the four-spoked gear visible on the front of fragment A, the gear named b1. This moved the date pointer on the front dial, which would be set to the correct Egyptian calendar day. The year is not selectable, so it is necessary to know the year currently set, or by looking up the cycles indicated by the various calendar cycle indicators on the back in the Babylonian ephemeris tables for the day of the year currently set, since most of the calendar cycles are not synchronous with the year. The crank moves the date pointer about 78 days per full rotation, so hitting a particular day on the dial would be easily possible if the mechanism were in good working condition. The action of turning the hand crank would also cause all interlocked gears within the mechanism to rotate, resulting in the simultaneous calculation of the position of the Sun and Moon, the moon phase, eclipse, and calendar cycles, and perhaps the locations of planets.[46] The operator also had to be aware of the position of the spiral dial pointers on the two large dials on the back. The pointer had a \"follower\" that tracked the spiral incisions in the metal as the dials incorporated four and five full rotations of the pointers. When a pointer reached the terminal month location at either end of the spiral, the pointer's follower had to be manually moved to the other end of the spiral before proceeding further.[4]:10 Faces[edit] Computer-generated front panel of the Freeth model Front face[edit] The front dial has two concentric circular scales. The inner scale marks the Greek signs of the Zodiac, with division in degrees. The outer scale, which is a moveable ring that sits flush with the surface and runs in a channel, is marked off with what appear to be days and has a series of corresponding holes beneath the ring in the channel. Since the discovery of the Mechanism, this outer ring has been presumed to represent the 365-day Egyptian civil calendar. However, recent research challenges this presumption and gives evidence it is most likely divided into 354 intervals.[47] If one subscribes to the 365-day presumption, it is recognized the Mechanism predates the Julian calendar reform, but the Sothic and Callippic cycles had already pointed to a 365 14-day solar year, as seen in Ptolemy III's abortive calendrical reform of 238 BC. The dials are not believed to reflect his proposed leap day (Epag. 6), but the outer calendar dial may be moved against the inner dial to compensate for the effect of the extra quarter-day in the solar year by turning the scale backward one day every four years. However, if one subscribes to the 354-day evidence, then the most likely interpretation is that the ring is a manifestation of a 354-day lunar calendar. Given the era of the Mechanism's presumed construction and the presence of Egyptian month names, it is possibly the first example of the Egyptian civil-based lunar calendar proposed by Richard Anthony Parker in 1950.[48] The lunar calendars purpose was to serve as a day-to-day indicator of successive lunations, and would also have assisted with the interpretation of the Lunar phase pointer, and the Metonic and Saros dials. Undiscovered gearing, synchronous with the rest of the Metonic gearing of the mechanism, is implied to drive a pointer around this scale. Movement and registration of the ring relative to the underlying holes served to facilitate both a one-in-76-year Callippic cycle correction, as well as convenient lunisolar intercalation. The dial also marks the position of the Sun on the ecliptic corresponds to the current date in the year. The orbits of the Moon and the five planets known to the Greeks are close enough to the ecliptic to make it a convenient reference for defining their positions as well. The following three Egyptian months are inscribed in Greek letters on the surviving pieces of the outer ring:[49] (Pachon) (Payni) (Epiphi) The other months have been reconstructed, although some reconstructions of the mechanism omit the five days of the Egyptian intercalary month. The Zodiac dial contains Greek inscriptions of the members of the zodiac, which is believed to be adapted to the tropical month version rather than the sidereal:[15]:8[failed verification] Front panel of a 2007 recreation (Krios [Ram], Aries) (Tauros [Bull], Taurus) (Didymoi [Twins], Gemini) (Karkinos [Crab], Cancer) (Leon [Lion], Leo) (Parthenos [Maiden], Virgo) (Chelai [Scorpio's Claw or Zygos], Libra) (Skorpios [Scorpion], Scorpio) (Toxotes [Archer], Sagittarius) (Aigokeros [Goat-horned], Capricorn) (Hydrokhoos [Water carrier], Aquarius) (Ichthyes [Fish], Pisces) Also on the zodiac dial are a number of single characters at specific points (see reconstruction here:[50]). They are keyed to a parapegma, a precursor of the modern day almanac inscribed on the front face above and beneath the dials. They mark the locations of longitudes on the ecliptic for specific stars. The parapegma above the dials reads (square brackets indicate inferred text): [...] Capricorn begins to rise [...] Aries begins to rise [...] Winter solstice [...] Vernal equinox [...] ... evening [...] [...] ... evening [...] ... evening [...] The Hyades set in the evening [...] Aquarius begins to rise {} Taurus begins to rise [...] [...] {} ... evening [...] Lyra rises in the evening [...] [...] ... {evening} [...] The Pleiades rise in the morning [...] Pisces begins to rise [...] The Hyades rise in the morning [...] {} [...] Gemini begins to rise Altair rises in the evening {}{} Arcturus sets in the morning The parapegma beneath the dials reads: [...] Libra begins to rise [...] Cancer begins {to rise} [...] Autumnal equinox [...] Summer solstice [...] ... rise in the evening Orion precedes the morning [...] ... rise in the evening {} Canis Major precedes the morning [...] {} ... rise Altair sets in the morning Scorpio begins to rise [...] Leo begins to rise [...] [...] [...] [...] [...] [...] [...] Sagittarius begins to rise [...] [...] [...] [...] [...] At least two pointers indicated positions of bodies upon the ecliptic. A lunar pointer indicated the position of the Moon, and a mean Sun pointer also was shown, perhaps doubling as the current date pointer. The Moon position was not a simple mean Moon indicator that would indicate movement uniformly around a circular orbit; it approximated the acceleration and deceleration of the Moon's elliptical orbit, through the earliest extant use of epicyclic gearing. It also tracked the precession of the elliptical orbit around the ecliptic in an 8.88-year cycle. The mean Sun position is, by definition, the current date. It is speculated that since such pains were taken to get the position of the Moon correct,[15]:20,24 then there also was likely to have been a \"true sun\" pointer in addition to the mean Sun pointer likewise, to track the elliptical anomaly of the Sun (the orbit of Earth around the Sun), but there is no evidence of it among the ruins of the mechanism found to date.[5] Similarly, neither is there the evidence of planetary orbit pointers for the five planets known to the Greeks among the ruins. See Proposed planet indication gearing schemes below. Mechanical engineer Michael Wright demonstrated that there was a mechanism to supply the lunar phase in addition to the position.[51] The indicator was a small ball embedded in the lunar pointer, half-white and half-black, which rotated to show the phase (new, first quarter, half, third quarter, full, and back) graphically. The data to support this function is available given the Sun and Moon positions as angular rotations; essentially, it is the angle between the two, translated into the rotation of the ball. It requires a differential gear, a gearing arrangement that sums or differences two angular inputs. Rear face[edit] Computer-generated back panel In July 2008, scientists reported new findings in the journal Nature showing that the mechanism not only tracked the Metonic calendar and predicted solar eclipses, but also calculated the timing of several panhellenic athletic games, including the Ancient Olympic Games.[7] Inscriptions on the instrument closely match the names of the months that are used on calendars from Epirus in northwestern Greece and with the island of Corfu, which in antiquity was known as Corcyra.[52][53][54] On the back of the mechanism, there are five dials: the two large displays, the Metonic and the Saros, and three smaller indicators, the so-called Olympiad Dial,[7] which has recently been renamed the Games dial as it did not track Olympiad years (the four-year cycle it tracks most closely is the Halieiad),[9] the Callippic, and the Exeligmos.[4]:11 The Metonic Dial is the main upper dial on the rear of the mechanism. The Metonic cycle, defined in several physical units, is 235 synodic months, which is very close (to within less than 13 one-millionths) to 19 tropical years. It is therefore a convenient interval over which to convert between lunar and solar calendars. The Metonic dial covers 235 months in five rotations of the dial, following a spiral track with a follower on the pointer that keeps track of the layer of the spiral. The pointer points to the synodic month, counted from new moon to new moon, and the cell contains the Corinthian month names.[7][55][56] (Phoinikaios) (Kraneios) (Lanotropios) (Machaneus, \"mechanic\", referring to Zeus the inventor) (Dodekateus) (Eukleios) (Artemisios) (Psydreus) (Gameilios) (Agrianios) (Panamos) (Apellaios) Thus, setting the correct solar time (in days) on the front panel indicates the current lunar month on the back panel, with resolution to within a week or so. Based on the fact that the calendar month names are consistent with all the evidence of the Epirote calendar and that the Games dial mentions the very minor Naa games of Dodona (in Epirus), it has recently been argued that the calendar on the Antikythera Mechanism is likely to be the Epirote calendar, and that this calendar was probably adopted from a Corinthian colony in Epirus, possibly Ambracia.[56] It has also been argued that the first month of the calendar, Phoinikaios, was ideally the month in which the autumn equinox fell, and that the start-up date of the calendar began shortly after the astronomical new moon of 23 August 205 BC.[57] The Callippic dial is the left secondary upper dial, which follows a 76-year cycle. The Callippic cycle is four Metonic cycles, and so this dial indicates the current Metonic cycle in the overall Callippic cycle.[citation needed] The Games dial is the right secondary upper dial; it is the only pointer on the instrument that travels in a counter-clockwise direction as time advances. The dial is divided into four sectors, each of which is inscribed with a year indicator and the name of two Panhellenic Games: the \"crown\" games of Isthmia, Olympia, Nemea, and Pythia; and two lesser games: Naa (held at Dodona),[58] and the sixth and final set of Games recently deciphered as the Halieia of Rhodes.[59] The inscriptions on each one of the four divisions are:[4][7] Olympic dial Year of the cycle Inside the dial inscription Outside the dial inscription 1 L (Isthmia) (Olympia) 2 L (Nemea)NAA (Naa) 3 L (Isthmia) (Pythia) 4 L (Nemea) (Halieia) The Saros dial is the main lower spiral dial on the rear of the mechanism.[4]:45,10 The Saros cycle is 18 years and 11+13 days long (6585.333... days), which is very close to 223 synodic months (6585.3211 days). It is defined as the cycle of repetition of the positions required to cause solar and lunar eclipses, and therefore, it could be used to predict themnot only the month, but the day and time of day. Note that the cycle is approximately 8 hours longer than an integer number of days. Translated into global spin, that means an eclipse occurs not only eight hours later, but one-third of a rotation farther to the west. Glyphs in 51 of the 223 synodic month cells of the dial specify the occurrence of 38 lunar and 27 solar eclipses. Some of the abbreviations in the glyphs read:[citation needed] = (\"Selene\", Moon) = (\"Helios\", Sun) H\\M = (\"Hemeras\", of the day) \\ = (\"hora\", hour) N\\Y = (\"Nuktos\", of the night) The glyphs show whether the designated eclipse is solar or lunar, and give the day of the month and hour. Solar eclipses may not be visible at any given point, and lunar eclipses are visible only if the moon is above the horizon at the appointed hour.[15]:6 In addition, the inner lines at the cardinal points of the Saros dial indicate the start of a new full moon cycle. Based on the distribution of the times of the eclipses, it has recently been argued that the start-up date of the Saros dial was shortly after the astronomical new moon of 28 April 205 BC.[18] The Exeligmos Dial is the secondary lower dial on the rear of the mechanism. The Exeligmos cycle is a 54-year triple Saros cycle that is 19,756 days long. Since the length of the Saros cycle is to a third of a day (eight hours), so a full Exeligmos cycle returns counting to integer days, hence the inscriptions. The labels on its three divisions are:[4]:10 Blank or o? (representing the number zero, assumed, not yet observed) H (number 8) means add 8 hours to the time mentioned in the display I (number 16) means add 16 hours to the time mentioned in the display Thus the dial pointer indicates how many hours must be added to the glyph times of the Saros dial in order to calculate the exact eclipse times.[citation needed] Doors[edit] The mechanism has a wooden casing with a front and a back door, both containing inscriptions.[7][15] The back door appears to be the \"instruction manual\". On one of its fragments is written \"76 years, 19 years\" representing the Callippic and Metonic cycles. Also written is \"223\" for the Saros cycle. On another one of its fragments, it is written \"on the spiral subdivisions 235\" referring to the Metonic dial. Gearing[edit] The mechanism is remarkable for the level of miniaturisation and the complexity of its parts, which is comparable to that of fourteenth-century astronomical clocks. It has at least 30 gears, although mechanism expert Michael Wright has suggested that the Greeks of this period were capable of implementing a system with many more gears.[46] There is much debate as to whether the mechanism had indicators for all five of the planets known to the ancient Greeks. No gearing for such a planetary display survives and all gears are accounted forwith the exception of one 63-toothed gear (r1) otherwise unaccounted for in fragment D.[5] Fragment D is a small quasi-circular constriction that, according to Xenophon Moussas, has a gear inside a somewhat larger hollow gear. The inner gear moves inside the outer gear reproducing an epicyclical motion that, with a pointer, gives the position of planet Jupiter.[60][61] The inner gear is numbered 45, \"ME\" in Greek and the same number is written on two surfaces of this small cylindrical box. The purpose of the front face was to position astronomical bodies with respect to the celestial sphere along the ecliptic, in reference to the observer's position on the Earth. That is irrelevant to the question of whether that position was computed using a heliocentric or geocentric view of the Solar System; either computational method should, and does, result in the same position (ignoring ellipticity), within the error factors of the mechanism. The epicyclic Solar System of Ptolemy (c. AD 100170)still 300 years in the future from the apparent date of the mechanismcarried forward with more epicycles, and was more accurate predicting the positions of planets than the view of Copernicus (14731543), until Kepler (15711630) introduced the possibility that orbits are ellipses.[62] Evans et al. suggest that to display the mean positions of the five classical planets would require only 17 further gears that could be positioned in front of the large driving gear and indicated using individual circular dials on the face.[63] Tony Freeth and Alexander Jones have modelled and published details of a version using several gear trains mechanically-similar to the lunar anomaly system allowing for indication of the positions of the planets as well as synthesis of the Sun anomaly. Their system, they claim, is more authentic than Wright's model as it uses the known skill sets of the Greeks of that period and does not add excessive complexity or internal stresses to the machine.[5] The gear teeth were in the form of equilateral triangles with an average circular pitch of 1.6mm, an average wheel thickness of 1.4mm and an average air gap between gears of 1.2mm. The teeth probably were created from a blank bronze round using hand tools; this is evident because not all of them are even.[5] Due to advances in imaging and X-ray technology it is now possible to know the precise number of teeth and size of the gears within the located fragments. Thus the basic operation of the device is no longer a mystery and has been replicated accurately. The major unknown remains the question of the presence and nature of any planet indicators.[15]:8 A table of the gears, their teeth, and the expected and computed rotations of various important gears follows. The gear functions come from Freeth et al. (2008)[7] and those for the lower half of the table from Freeth and Jones 2012.[5] The computed values start with 1 year/revolution for the b1 gear, and the remainder are computed directly from gear teeth ratios. The gears marked with an asterisk (*) are missing, or have predecessors missing, from the known mechanism; these gears have been calculated with reasonable gear teeth counts.[7][15] The Antikythera Mechanism: known gears and accuracy of computation Gear name[table 1] Function of the gear/pointer Expected simulated interval of a full circular revolution Mechanism formula[table 2] Computed interval Gear direction[table 3] x Year gear 1 tropical year 1 (by definition) 1 year (presumed) cw[table 4] b the Moon's orbit 1 sidereal month (27.321661 days) Time(b) = Time(x) * (c1 / b2) * (d1 / c2) * (e2 / d2) * (k1 / e5) * (e6 / k2) * (b3 / e1) 27.321 days[table 5] cw r lunar phase display 1 synodic month (29.530589 days) Time(r) = 1 / (1 / Time(b2 [mean sun] or sun3 [true sun])) (1 / Time(b))) 29.530 days[table 5] n* Metonic pointer Metonic cycle () / 5 spirals around the dial = 1387.94 days Time(n) = Time(x) * (l1 / b2) * (m1 /l2) * (n1 / m2) 1387.9 days ccw[table 6] o* Games dial pointer 4 years Time(o) = Time(n) * (o1 / n2) 4.00 years cw[table 6][table 7] q* Callippic pointer 27758.8 days Time(q) = Time(n) * (p1 / n3) * (q1 /p2) 27758 days ccw[table 6] e* lunar orbit precession 8.85 years Time(e) = Time(x) * (l1 / b2) * (m1 / l2) * (e3 / m3) 8.8826 years ccw[table 8] g* Saros cycle Saros time / 4 turns = 1646.33 days Time(g) = Time(e) * (f1 / e4) * (g1 / f2) 1646.3 days ccw[table 6] i* Exeligmos pointer 19755.8 days Time(i) = Time(g) * (h1 / g2) * (i1 / h2) 19756 days ccw[table 6] The following are proposed gearing from the 2012 Freeth and Jones reconstruction: sun3* True sun pointer 1 mean year Time(sun3) = Time(x) * (sun3 / sun1) * (sun2 / sun3) 1 mean year[table 5] cw[table 9] mer2* Mercury pointer 115.88 days (synodic period) Time(mer2) = Time(x) * (mer2 / mer1) 115.89 days[table 5] cw[table 9] ven2* Venus pointer 583.93 days (synodic period) Time(ven2) = Time(x) * (ven1 / sun1) 584.39 days[table 5] cw[table 9] mars4* Mars pointer 779.96 days (synodic period) Time(mars4) = Time(x) * (mars2 / mars1) * (mars4 / mars3) 779.84 days[table 5] cw[table 9] jup4* Jupiter pointer 398.88 days (synodic period) Time(jup4) = Time(x) * (jup2 / jup1) * (jup4 / jup3) 398.88 days[table 5] cw[table 9] sat4* Saturn pointer 378.09 days (synodic period) Time(sat4) = Time(x) * (sat2 / sat1) * (sat4 / sat3) 378.06 days[table 5] cw[table 9] Table notes: ^ Change from traditional naming: X is the main year axis, turns once per year with gear B1. The B axis is the axis with gears B3 and B6, while the E axis is the axis with gears E3 and E4. Other axes on E (E1/E6 and E2/E5) are irrelevant to this table. ^ \"Time\" is the interval represented by one complete revolution of the gear. ^ As viewed from the front of the Mechanism. The \"natural\" view is viewing the side of the Mechanism the dial/pointer in question is actually displayed on. ^ The Greeks, being in the northern hemisphere, assumed proper daily motion of the stars was from east to west, ccw when the ecliptic and zodiac is viewed to the south. As viewed on the front of the Mechanism. ^ a b c d e f g h On average, due to epicyclic gearing causing accelerations and decelerations. ^ a b c d e Being on the reverse side of the box, the \"natural\" rotation is the opposite ^ This was the only visual pointer naturally travelling in the counter-clockwise direction. ^ Internal and not visible. ^ a b c d e f Prograde motion; retrograde is obviously the opposite direction. There are several gear ratios for each planet that result in close matches to the correct values for synodic periods of the planets and the Sun. The ones chosen above seem to provide good accuracy with reasonable tooth counts, but the specific gears that may have been used are, and probably will remain, unknown.[5] Known gear scheme[edit] A hypothetical schematic representation of the gearing of the Antikythera Mechanism, including the 2012 published interpretation of existing gearing, gearing added to complete known functions, and proposed gearing to accomplish additional functions, namely true sun pointer and pointers for the five then-known planets, as proposed by Freeth and Jones, 2012.[5] Based also upon similar drawing in the Freeth 2006 Supplement[15] and Wright 2005, Epicycles Part 2.[64] Proposed (as opposed to known from the artefact) gearing crosshatched. It is very probable that there were planetary dials, as the complicated motions and periodicities of all planets are mentioned in the manual of the mechanism. The exact position and mechanisms for the gears of the planets is not known. There is no coaxial system but only for the Moon. Fragment D that is an epicycloidal system is considered as a planetary gear for Jupiter (Moussas, 2011, 2012, 2014) or a gear for the motion of the Sun (University of Thessaloniki group). The Sun gear is operated from the hand-operated crank (connected to gear a1, driving the large four-spoked mean Sun gear, b1) and in turn drives the rest of the gear sets. The Sun gear is b1/b2 and b2 has 64 teeth. It directly drives the date/mean sun pointer (there may have been a second, \"true sun\" pointer that displayed the Sun's elliptical anomaly; it is discussed below in the Freeth reconstruction). In this discussion, reference is to modelled rotational period of various pointers and indicators; they all assume the input rotation of the b1 gear of 360 degrees, corresponding with one tropical year, and are computed solely on the basis of the gear ratios of the gears named.[4][7][65] The Moon train starts with gear b1 and proceeds through c1, c2, d1, d2, e2, e5, k1, k2, e6, e1, and b3 to the Moon pointer on the front face. The gears k1 and k2 form an epicyclic gear system; they are an identical pair of gears that don't mesh, but rather, they operate face-to-face, with a short pin on k1 inserted into a slot in k2. The two gears have different centres of rotation, so the pin must move back and forth in the slot. That increases and decreases the radius at which k2 is driven, also necessarily varying its angular velocity (presuming the velocity of k1 is even) faster in some parts of the rotation than others. Over an entire revolution the average velocities are the same, but the fast-slow variation models the effects of the elliptical orbit of the Moon, in consequence of Kepler's second and third laws. The modelled rotational period of the Moon pointer (averaged over a year) is 27.321 days, compared to the modern length of a lunar sidereal month of 27.321661 days. As mentioned, the pin/slot driving of the k1/k2 gears varies the displacement over a year's time, and the mounting of those two gears on the e3 gear supplies a precessional advancement to the ellipticity modelling with a period of 8.8826 years, compared with the current value of precession period of the moon of 8.85 years.[4][7][65] The system also models the phases of the Moon. The Moon pointer holds a shaft along its length, on which is mounted a small gear named r, which meshes to the Sun pointer at B0 (the connection between B0 and the rest of B is not visible in the original mechanism, so whether b0 is the current date/mean Sun pointer or a hypothetical true Sun pointer is not known). The gear rides around the dial with the Moon, but is also geared to the Sunthe effect is to perform a differential gear operation, so the gear turns at the synodic month period, measuring in effect, the angle of the difference between the Sun and Moon pointers. The gear drives a small ball that appears through an opening in the Moon pointer's face, painted longitudinally half white and half black, displaying the phases pictorially. It turns with a modelled rotational period of 29.53 days; the modern value for the synodic month is 29.530589 days.[4][7][65] The Metonic train is driven by the drive train b1, b2, l1, l2, m1, m2, and n1, which is connected to the pointer. The modelled rotational period of the pointer is the length of the 6939.5 days (over the whole five-rotation spiral), while the modern value for the Metonic cycle is 6939.69 days.[4][7][65] The Olympiad train is driven by b1, b2, l1, l2, m1, m2, n1, n2, and o1, which mounts the pointer. It has a computed modelled rotational period of exactly four years, as expected. Incidentally, it is the only pointer on the mechanism that rotates counter-clockwise; all of the others rotate clockwise.[4][7][65] The Callippic train is driven by b1, b2, l1, l2, m1, m2, n1, n3, p1, p2, and q1, which mounts the pointer. It has a computed modelled rotational period of 27758 days, while the modern value is 27758.8 days.[4][7][65] The Saros train is driven by b1, b2, l1, l2, m1, m3, e3, e4, f1, f2, and g1, which mounts the pointer. The modelled rotational period of the Saros pointer is 1646.3 days (in four rotations along the spiral pointer track); the modern value is 1646.33 days.[4][7][65] The Exeligmos train is driven by b1, b2, l1, l2, m1, m3, e3, e4, f1, f2, g1, g2, h1, h2, and i1, which mounts the pointer. The modelled rotational period of the Exeligmos pointer is 19,756 days; the modern value is 19755.96 days.[4][7][65] Apparently, gears m3, n1-3, p1-2, and q1 did not survive in the wreckage. The functions of the pointers were deduced from the remains of the dials on the back face, and reasonable, appropriate gearage to fulfill the functions was proposed, and is generally accepted.[4][7][65] Reconstruction efforts[edit] Proposed gear schemes[edit] Because of the large space between the mean Sun gear and the front of the case and the size of and mechanical features on the mean Sun gear it is very likely that the mechanism contained further gearing that either has been lost in or subsequent to the shipwreck or was removed before being loaded onto the ship.[5] This lack of evidence and nature of the front part of the mechanism has led to numerous attempts to emulate what the Greeks of the period would have done and, of course, because of the lack of evidence many solutions have been put forward. Wright proposalEvans et al. proposalFreeth et al. proposal Michael Wright was the first person to design and build a model with not only the known mechanism, but also, with his emulation of a potential planetarium system. He suggested that along with the lunar anomaly, adjustments would have been made for the deeper, more basic solar anomaly (known as the \"first anomaly\"). He included pointers for this \"true sun\", Mercury, Venus, Mars, Jupiter, and Saturn, in addition to the known \"mean sun\" (current time) and lunar pointers.[5] Evans, Carman, and Thorndike published a solution with significant differences from Wright's.[63] Their proposal centred on what they observed as irregular spacing of the inscriptions on the front dial face, which to them seemed to indicate an off-centre sun indicator arrangement; this would simplify the mechanism by removing the need to simulate the solar anomaly. They also suggested that rather than accurate planetary indication (rendered impossible by the offset inscriptions) there would be simple dials for each individual planet showing information such as key events in the cycle of planet, initial and final appearances in the night sky, and apparent direction changes. This system would lead to a much simplified gear system, with much reduced forces and complexity, as compared to Wright's model.[63] Their proposal used simple meshed gear trains and accounted for the previously unexplained 63 toothed gear in fragment D. They proposed two face plate layouts, one with evenly spaced dials, and another with a gap in the top of the face to account for criticism regarding their not using the apparent fixtures on the b1 gear. They proposed that rather than bearings and pillars for gears and axles, they simply held weather and seasonal icons to be displayed through a window.[63] In a paper published in 2012 Carman, Thorndike, and Evans also proposed a system of epicyclic gearing with pin and slot followers.[66] Freeth and Jones published their proposal in 2012 after extensive research and work. They came up with a compact and feasible solution to the question of planetary indication. They also propose indicating the solar anomaly (that is, the sun's apparent position in the zodiac dial) on a separate pointer from the date pointer, which indicates the mean position of the Sun, as well as the date on the month dial. If the two dials are synchronised correctly, their front panel display is essentially the same as Wright's. Unlike Wright's model however, this model has not been built physically, and is only a 3-D computer model.[5] Internal gearing relationships of the Antikythera Mechanism, based on the Freeth and Jones proposal The system to synthesise the solar anomaly is very similar to that used in Wright's proposal: three gears, one fixed in the centre of the b1 gear and attached to the Sun spindle, the second fixed on one of the spokes (in their proposal the one on the bottom left) acting as an idle gear, and the final positioned next to that one; the final gear is fitted with an offset pin and, over said pin, an arm with a slot that in turn, is attached to the sun spindle, inducing anomaly as the mean Sun wheel turns.[5] The inferior planet mechanism includes the Sun (treated as a planet in this context), Mercury, and Venus.[5] For each of the three systems there is an epicyclic gear whose axis is mounted on b1, thus the basic frequency is the Earth year (as it is, in truth, for epicyclic motion in the Sun and all the planetsexcepting only the Moon). Each meshes with a gear grounded to the mechanism frame. Each has a pin mounted, potentially on an extension of one side of the gear that enlarges the gear, but doesn't interfere with the teeth; in some cases the needed distance between the gear's centre and the pin is farther than the radius of the gear itself. A bar with a slot along its length extends from the pin toward the appropriate coaxial tube, at whose other end is the object pointer, out in front of the front dials. The bars could have been full gears, although there is no need for the waste of metal, since the only working part is the slot. Also, using the bars avoids interference between the three mechanisms, each of which are set on one of the four spokes of b1. Thus there is one new grounded gear (one was identified in the wreckage, and the second is shared by two of the planets), one gear used to reverse the direction of the sun anomaly, three epicyclic gears and three bars/coaxial tubes/pointers, which would qualify as another gear each: five gears and three slotted bars in all.[5] The superior planet systemsMars, Jupiter, and Saturnall follow the same general principle of the lunar anomaly mechanism.[5] Similar to the inferior systems, each has a gear whose centre pivot is on an extension of b1, and which meshes with a grounded gear. It presents a pin and a centre pivot for the epicyclic gear which has a slot for the pin, and which meshes with a gear fixed to a coaxial tube and thence to the pointer. Each of the three mechanisms can fit within a quadrant of the b1 extension, and they are thus all on a single plane parallel with the front dial plate. Each one uses a ground gear, a driving gear, a driven gear, and a gear/coaxial tube/pointer, thus, twelve gears additional in all. In total, there are eight coaxial spindles of various nested sizes to transfer the rotations in the mechanism to the eight pointers. So in all, there are 30 original gears, seven gears added to complete calendar functionality, 17 gears and three slotted bars to support the six new pointers, for a grand total of 54 gears, three bars, and eight pointers in Freeth and Jones' design.[5] On the visual representation Freeth supplies in the paper, the pointers on the front zodiac dial have small, round identifying stones. He mentions a quote from an ancient papyrus: ...a voice comes to you speaking. Let the stars be set upon the board in accordance with [their] nature except for the Sun and Moon. And let the Sun be golden, the Moon silver, Kronos [Saturn] of obsidian, Ares [Mars] of reddish onyx, Aphrodite [Venus] lapis lazuli veined with gold, Hermes [Mercury] turquoise; let Zeus [Jupiter] be of (whitish?) stone, crystalline (?)...[67] In March 2021, the Antikythera Research Team at University College London, led by Freeth, published their proposed reconstruction of the entire Antikythera Mechanism.[68][69][70] Accuracy[edit] Investigations by Freeth and Jones reveal that their simulated mechanism is not particularly accurate, the Mars pointer being up to 38 off at times (these inaccuracies occur at the nodal points of Mars' retrograde motion, and the error recedes at other locations in the orbit). This is not due to inaccuracies in gearing ratios in the mechanism, but rather due to inadequacies in the Greek theory of planetary movements. The accuracy could not have been improved until first Ptolemy put forth his Planetary Hypotheses in the second half of the second century AD (particularly adding the concept of the equant to his theory) and then finally by the introduction of Kepler's Second Law in the early 17th century.[5] In short, the Antikythera Mechanism was a machine designed to predict celestial phenomena according to the sophisticated astronomical theories current in its day, the sole witness to a lost history of brilliant engineering, a conception of pure genius, one of the great wonders of the ancient worldbut it didn't really work very well![5] In addition to theoretical accuracy, there is the matter of mechanical accuracy. Freeth and Jones note that the inevitable \"looseness\" in the mechanism due to the hand-built gears, with their triangular teeth and the frictions between gears, and in bearing surfaces, probably would have swamped the finer solar and lunar correction mechanisms built into it: Though the engineering was remarkable for its era, recent research indicates that its design conception exceeded the engineering precision of its manufacture by a wide marginwith considerable cumulative inaccuracies in the gear trains, which would have cancelled out many of the subtle anomalies built into its design.[5][71] While the device itself may have struggled with inaccuracies due to the triangular teeth being hand-made, the calculations used and the technology implemented to create the elliptical paths of the planets and retrograde motion of the Moon and Mars by using a clockwork-type gear train with the addition of a pin-and-slot epicyclic mechanism predated that of the first known clocks found in antiquity in Medieval Europe by more than 1000 years.[72] Archimedes' development of the approximate value of pi and his theory of centres of gravity, along with the steps he made towards developing the calculus,[73] all suggest that the Greeks had access to more than enough mathematical knowledge beyond that of just Babylonian algebra in order to be able to model the elliptical nature of planetary motion. Of special delight to physicists, the Moon mechanism uses a special train of bronze gears, two of them linked with a slightly offset axis, to indicate the position and phase of the moon. As is known today from Kepler's Laws of Planetary Motion, the moon travels at different speeds as it orbits the Earth, and this speed differential is modelled by the Antikythera Mechanism, even though the ancient Greeks were not aware of the actual elliptical shape of the orbit.[74] Similar devices in ancient literature[edit] Cicero's De re publica, a first century BC philosophical dialogue, mentions two machines that some modern authors consider as some kind of planetarium or orrery, predicting the movements of the Sun, the Moon, and the five planets known at that time. They were both built by Archimedes and brought to Rome by the Roman general Marcus Claudius Marcellus after the death of Archimedes at the siege of Syracuse in 212 BC. Marcellus had great respect for Archimedes and one of these machines was the only item he kept from the siege (the second was placed in the Temple of Virtue). The device was kept as a family heirloom, and Cicero has Philus (one of the participants in a conversation that Cicero imagined had taken place in a villa belonging to Scipio Aemilianus in the year 129 BC) saying that Gaius Sulpicius Gallus (consul with Marcellus's nephew in 166 BC, and credited by Pliny the Elder as the first Roman to have written a book explaining solar and lunar eclipses) gave both a \"learned explanation\" and a working demonstration of the device. I had often heard this celestial globe or sphere mentioned on account of the great fame of Archimedes. Its appearance, however, did not seem to me particularly striking. There is another, more elegant in form, and more generally known, moulded by the same Archimedes, and deposited by the same Marcellus, in the Temple of Virtue at Rome. But as soon as Gallus had begun to explain, by his sublime science, the composition of this machine, I felt that the Sicilian geometrician must have possessed a genius superior to any thing we usually conceive to belong to our nature. Gallus assured us, that the solid and compact globe, was a very ancient invention, and that the first model of it had been presented by Thales of Miletus. That afterwards Eudoxus of Cnidus, a disciple of Plato, had traced on its surface the stars that appear in the sky, and that many years subsequent, borrowing from Eudoxus this beautiful design and representation, Aratus had illustrated them in his verses, not by any science of astronomy, but the ornament of poetic description. He added, that the figure of the sphere, which displayed the motions of the Sun and Moon, and the five planets, or wandering stars, could not be represented by the primitive solid globe. And that in this, the invention of Archimedes was admirable, because he had calculated how a single revolution should maintain unequal and diversified progressions in dissimilar motions. When Gallus moved this globe it showed the relationship of the Moon with the Sun, and there were exactly the same number of turns on the bronze device as the number of days in the real globe of the sky. Thus it showed the same eclipse of the Sun as in the globe [of the sky], as well as showing the Moon entering the area of the Earth's shadow when the Sun is in line... [missing text] [i.e. It showed both solar and lunar eclipses.][75] Pappus of Alexandria stated that Archimedes had written a now lost manuscript on the construction of these devices entitled On Sphere-Making.[76][77] The surviving texts from ancient times describe many of his creations, some even containing simple drawings. One such device is his odometer, the exact model later used by the Romans to place their mile markers (described by Vitruvius, Heron of Alexandria and in the time of Emperor Commodus).[78] The drawings in the text appeared functional, but attempts to build them as pictured had failed. When the gears pictured, which had square teeth, were replaced with gears of the type in the Antikythera mechanism, which were angled, the device was perfectly functional.[79] If Cicero's account is correct, then this technology existed as early as the third century BC. Archimedes' device is also mentioned by later Roman era writers such as Lactantius (Divinarum Institutionum Libri VII), Claudian (In sphaeram Archimedes), and Proclus (Commentary on the first book of Euclid's Elements of Geometry) in the fourth and fifth centuries. Cicero also said that another such device was built \"recently\" by his friend Posidonius, \"... each one of the revolutions of which brings about the same movement in the Sun and Moon and five wandering stars [planets] as is brought about each day and night in the heavens...\"[80] It is unlikely that any one of these machines was the Antikythera mechanism found in the shipwreck since both the devices fabricated by Archimedes and mentioned by Cicero were located in Rome at least 30 years later than the estimated date of the shipwreck, and the third device was almost certainly in the hands of Posidonius by that date. The scientists who have reconstructed the Antikythera mechanism also agree that it was too sophisticated to have been a unique device. This evidence that the Antikythera mechanism was not unique adds support to the idea that there was an ancient Greek tradition of complex mechanical technology that was later, at least in part, transmitted to the Byzantine and Islamic worlds, where mechanical devices which were complex, albeit simpler than the Antikythera mechanism, were built during the Middle Ages.[81] Fragments of a geared calendar attached to a sundial, from the fifth or sixth century Byzantine Empire, have been found; the calendar may have been used to assist in telling time.[82] In the Islamic world, Ban Ms's Kitab al-Hiyal, or Book of Ingenious Devices, was commissioned by the Caliph of Baghdad in the early 9th century AD. This text described over a hundred mechanical devices, some of which may date back to ancient Greek texts preserved in monasteries. A geared calendar similar to the Byzantine device was described by the scientist al-Biruni around 1000, and a surviving 13th-century astrolabe also contains a similar clockwork device.[82] It is possible that this medieval technology may have been transmitted to Europe and contributed to the development of mechanical clocks there.[22] In the 11th century, Chinese polymath Su Song constructed a mechanical clock tower that told (among other measurements) the position of some stars and planets, which where shown on a mechanically rotated armillary sphere.[83] Popular culture[edit] On 17 May 2017, Google marked the 115th anniversary of the discovery with a Google Doodle.[84][85] As of 2012, the Antikythera mechanism was displayed as part of a temporary exhibition about the Antikythera Shipwreck,[86] accompanied by reconstructions made by Ioannis Theofanidis, Derek de Solla Price, Michael Wright, the Thessaloniki University and Dionysios Kriaris. Other reconstructions are on display at the American Computer Museum in Bozeman, Montana, at the Children's Museum of Manhattan in New York, at Astronomisch-Physikalisches Kabinett in Kassel, Germany, and at the Muse des Arts et Mtiers in Paris. The National Geographic documentary series Naked Science had an episode dedicated to the Antikythera Mechanism entitled \"Star Clock BC\" that aired on 20 January 2011.[87] A documentary, The World's First Computer, was produced in 2012 by the Antikythera mechanism researcher and film-maker Tony Freeth.[88] In 2012 BBC Four aired The Two-Thousand-Year-Old Computer;[89] it was also aired on 3 April 2013 in the United States on NOVA, the PBS science series, under the name Ancient Computer.[90] It documents the discovery and 2005 investigation of the mechanism by the Antikythera Mechanism Research Project. A fully functioning Lego reconstruction of the Antikythera mechanism was built in 2010 by hobbyist Andy Carol, and featured in a short film produced by Small Mammal in 2011.[91] Several exhibitions have been staged worldwide,[92] leading to the main \"Antikythera shipwreck\" exhibition at the National Archaeological Museum in Athens, Greece. A fictionalised version of the device was a central plot point in the film Stonehenge Apocalypse (2010), where it was used as the artefact that saved the world from impending doom.[93] The massively multiplayer video game Eve Online contains an item named \"Antikythera Element\" obtained from game content surrounding a mysterious group of non-player characters themed as ancient Greeks.[94] See also[edit] Archimedes Palimpsest Astrarium Automaton Ctesibius Reverse engineering References[edit] ^ Efstathiou, Kyriakos; Efstathiou, Marianna (1 September 2018). \"Celestial Gearbox: Oldest Known Computer is a Mechanism Designed to Calculate the Location of the Sun, Moon, and Planets\". Mechanical Engineering. 140 (9): 3135. doi:10.1115/1.2018-SEP1. ISSN0025-6501. ^ Ken Steiglitz (2019). The Discrete Charm of the Machine: Why the World Became Digital. Princeton University Press. p.108. ISBN978-0-691-18417-3. The Antkythera Mechanism [The first computer worthy of the name...] ^ Paphitis, Nicholas (30 November 2006). \"Experts: Fragments an Ancient Computer\". Washington Post. Archived from the original on 8 June 2017. Imagine tossing a top-notch laptop into the sea, leaving scientists from a foreign culture to scratch their heads over its corroded remains centuries later. A Roman shipmaster inadvertently did something just like it 2,000 years ago off southern Greece, experts said late Thursday. ^ a b c d e f g h i j k l m n o p q r s Freeth, Tony; Bitsakis, Yanis; Moussas, Xenophon; Seiradakis, John. H.; Tselikas, A.; Mangou, H.; Zafeiropoulou, M.; Hadland, R.; etal. (30 November 2006). \"Decoding the ancient Greek astronomical calculator known as the Antikythera Mechanism\" (PDF). Nature. 444 (7119): 58791. Bibcode:2006Natur.444..587F. doi:10.1038/nature05357. PMID17136087. S2CID4424998. Archived from the original (PDF) on 20 July 2015. Retrieved 20 May 2014. ^ a b c d e f g h i j k l m n o p q r s t u Freeth, Tony; Jones, Alexander (2012). \"The Cosmos in the Antikythera Mechanism\". Institute for the Study of the Ancient World. Retrieved 19 May 2014. ^ Pinotsis, A. D. (30 August 2007). \"The Antikythera mechanism: who was its creator and what was its use and purpose?\". Astronomical and Astrophysical Transactions. 26 (45): 21126. Bibcode:2007A&AT...26..211P. doi:10.1080/10556790601136925. S2CID56126896. ^ a b c d e f g h i j k l m n o p q r s t Freeth, Tony; Jones, Alexander; Steele, John M.; Bitsakis, Yanis (31 July 2008). \"Calendars with Olympiad display and eclipse prediction on the Antikythera Mechanism\" (PDF). Nature. 454 (7204): 61417. Bibcode:2008Natur.454..614F. doi:10.1038/nature07130. PMID18668103. S2CID4400693. Archived from the original (PDF) on 27 September 2013. Retrieved 20 May 2014. ^ Kaplan, Sarah (14 June 2016). \"The World's Oldest Computer Is Still Revealing Its Secrets\", The Washington Post. Retrieved 16 June 2016. ^ a b Iversen 2017, p.130 and note 4 ^ Alexander Jones, A Portable Cosmos, Oxford: Oxford University Press, pp. 1011. ^ a b Price, Derek de Solla (1974). \"Gears from the Greeks. The Antikythera Mechanism: A Calendar Computer from ca. 80 B. C.\". Transactions of the American Philosophical Society. New Series. 64 (7): 170. doi:10.2307/1006146. JSTOR1006146. ^ Palazzo, Chiara (17 May 2017). \"What is the Antikythera Mechanism? How was this ancient 'computer' discovered?\". The Telegraph. Retrieved 10 June 2017. ^ Freeth, T.; Bitsakis, Y.; Moussas, X.; Seiradakis, J.H.; Tselikas, A.; Mangou, E.; Zafeiropoulou, M.; Hadland, R.; Bate, D.; Ramsey, A.; Allen, M.; Crawley, A.; Hockley, P.; Malzbender, T.; Gelb, D.; Ambrisco, W.; Edmunds, M.G. \"Decoding The Antikythera Mechanism Investigation of An Ancient Astronomical Calculator\". Retrieved 27 June 2020. ^ Vetenskapens vrld: Bronsklumpen som kan frutsga framtiden. SVT. 17 October 2012. Archived 20 October 2012 at the Wayback Machine ^ a b c d e f g h i Freeth, Tony (2006). \"Decoding the Antikythera Mechanism: Supplementary Notes 2\" (PDF). Nature. 444 (7119): 58791. Bibcode:2006Natur.444..587F. doi:10.1038/nature05357. PMID17136087. S2CID4424998. Archived from the original (PDF) on 26 January 2013. Retrieved 20 May 2014. ^ Sample, Ian. \"Mysteries of computer from 65 BC are solved\". The Guardian. One of the remaining mysteries is why the Greek technology invented for the machine seemed to disappear...\"This device is extraordinary, the only thing of its kind,\" said Professor Edmunds. \"The astronomy is exactly right ... in terms of historic and scarcity value, I have to regard this mechanism as being more valuable than the Mona Lisa.\" ^ Price, Derek de Solla (1974). \"Gears from the Greeks. The Antikythera Mechanism: A Calendar Computer from ca. 80 BC\" Transactions of the American Philosophical Society, New Series. 64 (7): 19. ^ a b c d Carman, Christin C.; Evans, James (15 November 2014). \"On the epoch of the Antikythera mechanism and its eclipse predictor\". Archive for History of Exact Sciences. 68 (6): 693774. doi:10.1007/s00407-014-0145-5. S2CID120548493. ^ a b c Markoff, John (24 November 2014). \"On the Trail of an Ancient Mystery Solving the Riddles of an Early Astronomical Calculator\". The New York Times. Retrieved 25 November 2014. ^ Iversen 2017, pp.18283 ^ Jones 2017, pp.93, 15760, 23346 ^ a b c d Marchant, Jo (30 November 2006). \"In search of lost time\". Nature. 444 (7119): 53438. Bibcode:2006Natur.444..534M. doi:10.1038/444534a. PMID17136067. ^ Efstathiou, M.; Basiakoulis, A.; Efstathiou, K.; Anastasiou, M.; Boutbaras, P.; Seiradakis, J.H. (September 2013). \"The Reconstruction of the Antikythera Mechanism\" (PDF). International Journal of Heritage in the Digital Era. 2 (3): 30734. doi:10.1260/2047-4970.2.3.307. S2CID111280754. ^ Efstathiou, K.; Basiakoulis, A.; Efstathiou, M.; Anastasiou, M.; Seiradakis, J.H. (June 2012). \"Determination of the gears geometrical parameters necessary for the construction of an operational model of the Antikythera Mechanism\". Mechanism and Machine Theory. 52: 21931. doi:10.1016/j.mechmachtheory.2012.01.020. ^ \"The Antikythera Mechanism at the National Archaeological Museum\" Archived 21 February 2017 at the Wayback Machine. Retrieved 8 August 2015. ^ Dimitrios (Dimitris) Kontos ^ \"History Antikythera Mechanism Research Project\". www.antikythera-mechanism.gr. ^ \"Ancient 'computer' starts to yield secrets\". IOL: Technology. Independent Media. 7 June 2006. Archived from the original on 13 March 2007. Retrieved 16 July 2017. ^ Haughton, Brian (26 December 2006). Hidden History: Lost Civilizations, Secret Knowledge, and Ancient Mysteries. Career Press. pp.4344. ISBN978-1-56414-897-1. Retrieved 16 May 2011. ^ Bohstrom, Philippe (18 November 2018), Missing Piece of Antikythera Mechanism Found on Aegean Seabed, Haaretz, retrieved 26 June 2020. ^ Daley, Jason (15 November 2018), No, Archaeologists Probably Did Not Find a New Piece of the Antikythera Mechanism, Smithsonian Magazine, retrieved 15 November 2018. ^ Angelakis, Dimitris G. (2 May 2005). Quantum Information Processing: From Theory to Experiment. Proceedings of the NATO Advanced Study Institute on Quantum Computation and Quantum Information. Chania, Crete, Greece: IOS Press (published 2006). p.5. ISBN978-1-58603-611-9. Retrieved 28 May 2013. The Antikythera mechanism, as it is now known, was probably the world's first 'analog computer'a sophisticated device for calculating the motions of stars and planets. This remarkable assembly of more than 30 gears with a differential... ^ Allen, Martin (27 May 2007). \"Were there others? The Antikythera Mechanism Research Project\". Antikythera-mechanism.gr. Archived from the original on 21 July 2011. Retrieved 24 August 2011. ^ Iversen 2017 ^ Iversen 2017, pp.13441 ^ Freeth, Tony (December 2009). \"Decoding an Ancient Computer\" (PDF). Scientific American. 301 (6): 78. Bibcode:2009SciAm.301f..76F. doi:10.1038/scientificamerican1209-76. PMID20058643. Retrieved 26 November 2014. ^ Article \"Pergamum\", Columbia Electronic Encyclopedia, 6th Edition, 1. ^ Price, Derek de Solla (1974). \"Gears from the Greeks. The Antikythera Mechanism: A Calendar Computer from ca. 80 BC\". Transactions of the American Philosophical Society, New Series. 64 (7): 13; 5762. ^ Bitsakis, Yannis; Jones, Alexander (2013). \"The Inscriptions of the Antikythera Mechanism 3: The Front Dial and Parapegma Inscriptions\", Almagest 7 (2016), pp. 11719. See also Magdalini Anastasiou et al. \"The Astronomical Events of the Parapegma of the Antikythera Mechanism\". Journal for the History of Astronomy. 44: 17386. ^ Iversen 2017, pp.14147; Jones 2017, p.93 ^ Kampouris, Nick (18 October 2019). \"Important New Discoveries from Greece's Ancient Antikythera Shipwreck\". Greek Reporter. Retrieved 26 June 2020. ^ \"The new findings from the underwater archaeological research at the Antikythera Shipwreck\". Aikaterini Laskaridis Foundation. 18 October 2019. Retrieved 23 January 2020. ^ Marchant, Jo (2006). Decoding the Heavens. Da Capo Press. p.180. mechanical engineer and former curator of London's Science Museum Michael Wright tells of a piece breaking off in his inspection, which was glued back into place by the museum staff. ^ Wright, Michael T. (2007). \"The Antikythera Mechanism reconsidered\". Interdisciplinary Science Reviews. 32 (1): 2143. doi:10.1179/030801807X163670. S2CID54663891. ^ X. Moussas. Antikythera Mechanism, \"PINAX\", Greek Physical Society, Athens, 2011.2012 and X. Moussas Antikythera Mechanism the oldest computer, ed. Canto Mediterraneo, 2018, Athens, ^ a b Freeth, T. (2009). \"Decoding an Ancient Computer\". Scientific American. 301 (6): 7683. Bibcode:2009SciAm.301f..76F. doi:10.1038/scientificamerican1209-76. PMID20058643. ^ Budiselic et al., Antikythera Mechanism: Evidence of a Lunar Calendar, https://bhi.co.uk/wp-content/uploads/2020/12/BHI-Antikythera-Mechanism-Evidence-of-a-Lunar-Calendar.pdf ^ Parker, Richard Anthony, \"The Calendars of Ancient Egypt,\" (Chicago: University of Chicago Press, 1950). ^ Jones 2017, p.97. ^ \"The Cosmos on the front of the Antikythera Mechanism\". Archived from the original on 17 May 2018. Retrieved 21 May 2014.CS1 maint: bot: original URL status unknown (link) ^ Wright, Michael T. (March 2006). \"The Antikythera Mechanism and the early history of the moon phase display\" (PDF). Antiquarian Horology. 29 (3): 31929. Retrieved 16 June 2014. ^ Wilford, J. N. (31 July 2008). \"Discovering how Greeks computed in 100 B.C.\" The New York Times. ^ Connor, S. (31 July 2008). \"Ancient Device Was Used To Predict Olympic Games\". The Independent. London. Retrieved 27 March 2010. ^ Iversen 2017, pp.14868 ^ Freeth, T (2009). \"Decoding an Ancient Computer\". Scientific American. 301 (6): 7683. Bibcode:2009SciAm.301f..76F. doi:10.1038/scientificamerican1209-76. PMID20058643. ^ a b Iversen 2017, pp.14864 ^ Iversen 2017, pp.16585 ^ \"Olympic link to early 'computer'\". BBC News. Retrieved 15 December 2008. ^ Iversen 2017, pp.14147 ^ Moussas, Xenophon (2011). \"Antikythera Mechanism\". Greek Physical Society, Athens. ^ Moussas, Xenophon (2018). \"Antikythera Mechanism the oldest computer\". Canto Mediterraneo, Athens. ^ \"Does it favour a Heliocentric, or Geocentric Universe?\". Antikythera Mechanism Research Project. 27 July 2007. Archived from the original on 21 July 2011. Retrieved 24 August 2011. ^ a b c d Evans, James; Carman, Christin C.; Thorndyke, Alan (February 2010). \"Solar anomaly and planetary displays in the Antikythera Mechanism\" (PDF). Journal for the History of Astronomy. xli (1): 139. Bibcode:2010JHA....41....1E. doi:10.1177/002182861004100101. S2CID14000634. Retrieved 20 May 2014. ^ Wright, Michael T. (June 2005). \"The Antikythera Mechanism: a new gearing scheme\" (PDF). Bulletin of the Scientific Instrument Society. 85: 27. Retrieved 12 March 2017. ^ a b c d e f g h i Edmunds, Mike G.; Freeth, Tony (July 2011). \"Using Computation to Decode the First Known Computer\". Computer. 20117 (7): 3239. doi:10.1109/MC.2011.134. S2CID8574856. ^ Carman, Christin C.; Thorndyke, Alan; Evans, James (2012). \"On the Pin-and-Slot Device of the Antikythera Mechanism, with a New Application to the Superior Planets\" (PDF). Journal for the History of Astronomy. 43 (1): 93116. Bibcode:2012JHA....43...93C. doi:10.1177/002182861204300106. S2CID41930968. Retrieved 21 May 2014. ^ An extract from a 2nd or 3rd century AD papyrus (P.Wash.Univ.inv. 181+221) about an \"Astrologer's Board\", where the astrologer lays out particular stones to represent the Sun, Moon and planets ^ Freeth, Tony; Higgon, David; Dacanalis, Aris; MacDonald, Lindsay; Georgakopoulou, Myrto; Wojcik, Adam (12 March 2021). \"A Model of the Cosmos in the ancient Greek Antikythera Mechanism\". Scientific Reports. 11 (1): 5821. doi:10.1038/s41598-021-84310-w. PMC7955085. PMID33712674. ^ Freeth, Tony (2 March 2021). \"The Antikythera Cosmos (video: 25:56)\". Retrieved 12 March 2021. ^ Freeth, Tony; Higgon, David; Dacanalis, Aris; MacDonald, Lindsay; Georgakopoulou, Myrto; Wojcik, Adam (2 March 2021). \"A Model of the Cosmos in the ancient Greek Antikythera Mechanism\". Scientific Reports. 11 (1): 5821. doi:10.1038/s41598-021-84310-w. PMC7955085. PMID33712674. Retrieved 12 March 2021. ^ Geoffrey, Edmunds, Michael (1 August 2011). \"An Initial Assessment of the Accuracy of the Gear Trains in the Antikythera Mechanism\". Journal for the History of Astronomy. 42 (3): 30720. Bibcode:2011JHA....42..307E. doi:10.1177/002182861104200302. S2CID120883936. Retrieved 10 May 2016. ^ Marchant, Jo (2009). Decoding the Heavens. First Da Capo Press. p.40. ISBN978-0-306-81742-7. ^ Netz & Noel, Reviel & William (2007). The Archimedes Codex. Da Capo Press. p.1. ISBN978-0-306-81580-5. ^ Pickover, Clifford (2011). The Physics Book. Sterling. p.52. ISBN978-1-4027-7861-2. ^ \"M. TVLLI CICERONIS DE RE PVBLICA LIBER PRIMVS\" (in Latin). Archived from the original on 22 March 2007. Retrieved 23 March 2007. ^ Rorres, Chris. \"Archimedes: Spheres and Planetaria (Introduction)\". New York University. Archived from the original on 10 May 2011. Retrieved 27 March 2011. ^ Fildes, Jonathan (29 November 2006). \"Ancient Moon 'computer' revisited\". BBC News. Retrieved 25 April 2010. ^ Needham, Joseph (2000). Science and Civilisation in China. 4, Part 2. Cambridge. p.285. ISBN0-521-05803-1. ^ Sleeswyk, Andre (October 1981). \"Vitruvius' odometer\". Scientific American. 252 (4). pp.188200. See also: Andre Wegener Sleeswyk, \"Vitruvius' waywiser\", Archives internationales d'histoire des sciences, vol. 29, pp. 1122 (1979). ^ \"Cicero, De Natura Deorum II.88 (or 3334)\". Archived from the original on 16 March 2007. Retrieved 23 March 2007. ^ Charette, F (November 2006). \"Archaeology: high tech from Ancient Greece\". Nature. 444 (7119): 55152. Bibcode:2006Natur.444..551C. doi:10.1038/444551a. PMID17136077. S2CID33513516.. ^ a b Maddison, Francis (28 March 1985). \"Early mathematical wheelwork: Byzantine calendrical gearing\". Nature. 314 (6009): 31617. Bibcode:1985Natur.314..316M. doi:10.1038/314316b0. S2CID4229697.. ^ https://web.archive.org/web/20210826131722/http://afe.easia.columbia.edu/songdynasty-module/tech-experiment.html ^ Staff (17 May 2017). \"115 Anniversary of the Antikythera Mechanism Discovery\". Google. Retrieved 17 May 2017. ^ Smith, Reiss (17 May 2017). \"What is the Antikythera mechanism? Google Doodle marks discovery of ancient Greek computer\". BBC. Retrieved 17 May 2017. ^ \"The Antikythera Shipwreck: the Ship, the Treasures, the Mechanism\". Antikythera Mechanism Research Project. 6 June 2012. Retrieved 16 April 2013. ^ \"Naked Science Star Clock BC (TV Episode)\". 2011.[unreliable source?] ^ \"The World's First Computer\". Antikythera Mechanism Research Project. Retrieved 21 January 2013. ^ \"BBC Four The Two-Thousand-Year-Old Computer\". ^ \"Ancient Computer\". Nova. PBS. Retrieved 13 May 2014. ^ Pavlus, John. \"Small Mammal, Behind the Scenes: Lego Antikythera Mechanism\". Small Mammal. Retrieved 19 July 2018. ^ \"Exhibitions\". The Antikythera Mechanism Research Project. ^ Gracey, James (23 May 2011). \"Behind The Couch: 'Stonehenge Apocalypse'\". BehindTheCouch web site. Retrieved 23 May 2011. ^ \"Antikythera Element\". EVE Online Reference. Further reading[edit] Books[edit] Allen, M.; Ambrisco, W.; e.a. (2016). \"The Inscriptions of the Antikythera Mechanism\". Almagest. Almagest 7.1. Turnhout, Belgium: Brepols Publishers. ISSN1792-2593. James, Peter; Thorpe, Nick (1995). Ancient Inventions. Ballantine. ISBN978-0-345-40102-1. Jones, Alexander (2017). A Portable Cosmos: Revealing the Antikythera Mechanism, Scientific Wonder of the Ancient World. Oxford University Press. ISBN978-0199739349. Lin, Jian-Liang; Yan, Hong-Sen (2016). Decoding the Mechanisms of Antikythera Astronomical Device. Berlin [u.a.]: Springer. ISBN978-3662484456. Marchant, Jo (2008). Decoding the Heavens: Solving the Mystery of the World's First Computer. William Heinemann. ISBN978-0-434-01835-2. Price, Derek De Solla (1975). Gears from the Greeks: The Antikythera Mechanism; A Calendar Computer from ca. 80 B.C. Science History Publications. ISBN0-87169-647-9. Rosheim, Mark E. (1994). Robot Evolution: The Development of Anthrobotics. Wiley. ISBN978-0-471-02622-8. Russo, Lucio (2004). The Forgotten Revolution: How Science Was Born in 300 BC and Why It Had To Be Reborn. Springer. ISBN978-3-540-20396-4. Steele, J. M. (2000). Observations and Predictions of Eclipse Times by Early Astronomers. Kluwer. ISBN978-0-7923-6298-2. Stephenson, F. R. (1997). Historical Eclipses and the Earth's Rotation. Cambridge University Press. ISBN978-0-521-46194-8. Ptolemy (1998). Ptolemy's Almagest. Translated by Toomer, G. J. Princeton University Press. ISBN978-0-691-00260-6. Journals[edit] Bromley, A. G. (1990). \"The Antikythera Mechanism\". Horological Journal. 132: 41215. ISSN0018-5108. Bromley, A. G. (1990). \"The Antikythera Mechanism: A Reconstruction\". Horological Journal. 133 (1): 2831. Bromley, A. G. (1990). \"Observations of the Antikythera Mechanism\". Antiquarian Horology. 18 (6): 64152. OCLC900191459. Carman, C. C.; Di Cocco, M. (2016). \"The Moon Phase Anomaly in the Antikythera Mechanism\". ISAW Papers. 11. Archived from the original on 10 October 2019. Retrieved 6 June 2018. Charette, Franois (2006). \"High tech from Ancient Greece\". Nature. 444 (7119): 55152. Bibcode:2006Natur.444..551C. doi:10.1038/444551a. PMID17136077. S2CID33513516. Edmunds, M. G. (2014). \"The Antikythera Mechanism and the Mechanical Universe\". Contemporary Physics. 55 (4): 26385. Bibcode:2014ConPh..55..263E. doi:10.1080/00107514.2014.927280. S2CID122403901. Edmunds, Mike & Morgan, Philip (2000). \"The Antikythera Mechanism: Still a Mystery of Greek Astronomy\". Astronomy & Geophysics. 41 (6): 610. Bibcode:2000A&G....41f..10E. doi:10.1046/j.1468-4004.2000.41610.x. (The authors mention that an \"extended account\" of their researches titled \"Computing Aphrodite\" is forthcoming in 2001, but it does not seem to have appeared yet.) Freeth, T. (2002). \"The Antikythera Mechanism: 1. Challenging the Classic Research\" (PDF). Mediterranean Archeology and Archeaometry. 2 (1): 2135. Freeth, T. (2002). \"The Antikythera Mechanism: 2. Is it Posidonius' Orrery?\". Mediterranean Archeology and Archeaometry. 2 (2): 4558. Bibcode:2002MAA.....2...45F. Freeth, T.; Bitsakis, Y.; Moussas, X.; Seiradakis, J. H.; etal. (2006). \"Decoding the ancient Greek astronomical calculator known as the Antikythera Mechanism\". Nature. 444 (7119): 58791. Bibcode:2006Natur.444..587F. doi:10.1038/nature05357. PMID17136087. S2CID4424998. Freeth, T. (2009). \"Decoding an Ancient Computer\". Scientific American. 301 (6): 7683. Bibcode:2009SciAm.301f..76F. doi:10.1038/scientificamerican1209-76. PMID20058643. Freeth, T.; Jones, A. (2012). \"The Cosmos in the Antikythera Mechanism\". ISAW Papers. 4. Iversen, Paul A. (2017). \"The Calendar on the Antikythera Mechanism and the Corinthian Family of Calendars\". Hesperia. 86 (1): 129203. doi:10.2972/hesperia.86.1.0129. Jones, A. (1991). \"The adaptation of Babylonian methods in Greek numerical astronomy\". Isis. 82 (3): 44053. Bibcode:1991Isis...82..441J. doi:10.1086/355836. S2CID92988054. Koulouris, John A. (2008). \"The Heavens of Poseidon: The History and Discovery of the AntiKythera Mechanism\" (PDF). In Nomine Portal (in Greek). 1: 112. Price, D. de S. (1959). \"An Ancient Greek Computer\". Scientific American. 200 (6): 6067. Bibcode:1959SciAm.200f..60P. doi:10.1038/scientificamerican0659-60. Price, D. de S. (1974). \"Gears from the Greeks: The Antikythera Mechanism, a Calendar Computer from c. 80 B.C.\". Transactions of the American Philosophical Society. 64 (7): 170. doi:10.2307/1006146. JSTOR1006146. Spinellis, Diomidis (May 2008). \"The Antikythera Mechanism: A Computer Science Perspective\". Computer. 41 (5): 2227. CiteSeerX10.1.1.144.2297. doi:10.1109/MC.2008.166. S2CID25254859. Steele, J. M. (2000). \"Eclipse prediction in Mesopotamia\". Arch. Hist. Exact Sci. 54 (5): 42154. Bibcode:2000AHES...54..421S. doi:10.1007/s004070050007. JSTOR41134091. S2CID118299511. Weinberg, G. D.; Grace, V. R.; Edwards, G. R.; Robinson, H. S.; etal. (1965). \"The Antikythera Shipwreck Reconsidered\". Trans. Am. Philos. Soc. 55 (New Series) (3): 348. doi:10.2307/1005929. JSTOR1005929. Other[edit] Blain, Loz (16 November 2011). \"Hublot painstakingly recreates a mysterious, 2,100-year-old clockwork relic - but why?\". New Atlas. Retrieved 26 June 2020. Hublot. Marchant, Jo (12 December 2008). \"Archimedes and the 2000-year-old computer\". New Scientist (2686). Panos, Kristina (2015). \"The Antikythera Mechanism\". Hackaday. Retrieved 24 November 2015. Rice, Rob S. (47 September 1997). The Antikythera Mechanism: Physical and Intellectual Salvage from the 1st Century B.C. USNA Eleventh Naval History Symposium. Thessaloniki. pp.1925. This audio file was created from a revision of this article dated 30July2019, and does not reflect subsequent edits. External links[edit] A Model of the Cosmos in the ancient Greek Antikythera Mechanism, Scientific Reports volume 11, Article number: 5821 (2021) Asimakopoulos, Fivos. \"3D model simulation\". Manos Roumeliotis's Simulation and Animation of the Antikythera Mechanism page. The Antikythera Mechanism Research Project. The Antikythera Mechanism Research Project. \"Videos\". YouTube. Retrieved 24 July 2017. \"The Antikythera Mechanism Exhibitions\". National Hellenic Research Foundation. Archived from the original on 23 April 2012. YAAS Een 3D interactive virtual reality simulator in VRML Wright, M.; Vicentini, M. (25 August 2009). \"Virtual Reconstruction of the Antikythera Mechanism\". Heritage Key via YouTube. \"Antikythera\" (Adobe Flash). Nature. 30 July 2008. ClickSpring: Machining The Antikythera Mechanism playlist on YouTube Metapage with links at antikythera.org "],
        "story_type":"Normal",
        "url_raw":"https://en.wikipedia.org/wiki/Antikythera_mechanism",
        "comments.comment_id":[20543624,
          20544259],
        "comments.comment_author":["jhalstead",
          "summner"],
        "comments.comment_descendants":[2,
          0],
        "comments.comment_time":["2019-07-27T19:19:40Z",
          "2019-07-27T21:21:31Z"],
        "comments.comment_text":["For anyone interested, the Clickspring YouTube channel is recreating the Antikythera Mechanism.<p><a href=\"https://www.youtube.com/channel/UCworsKCR-Sx6R6-BnIjS2MA\" rel=\"nofollow\">https://www.youtube.com/channel/UCworsKCR-Sx6R6-BnIjS2MA</a>",
          "On my recent vacations to Athens I visited National Archeological Museum where the Antikythera is displayed.\nI highly recomend seeing this device and its recreations live there.<p>Especially seeing the recontructioned devices gave me completely new perspective about ancient Greece."],
        "id":"af37f159-2123-44fc-a9bb-24535f25ced5",
        "url_text":"Antikythera mechanismThe Antikythera mechanism (Fragment A front and rear); visible is the largest gear in the mechanism, approximately 13 centimetres (5.1in) in diameter.WritingAncient GreekPeriod/cultureHellenisticDiscovered1901Antikythera, GreecePresent locationNational Archaeological Museum, Athens The Antikythera mechanism ( AN-tih-kih-THEER-) is an ancient Greek hand-powered orrery, described as the oldest example of an analogue computer[1][2][3] used to predict astronomical positions and eclipses decades in advance.[4][5][6] It could also be used to track the four-year cycle of athletic games which was similar to an Olympiad, the cycle of the ancient Olympic Games.[7][8][9] This artefact was among wreckage retrieved from a shipwreck off the coast of the Greek island Antikythera in 1901.[10][11] On 17 May 1902 it was identified as containing a gear by archaeologist Valerios Stais.[12] The device, housed in the remains of a 34cm 18cm 9cm (13.4in 7.1in 3.5in) wooden box, was found as one lump, later separated into three main fragments which are now divided into 82 separate fragments after conservation efforts. Four of these fragments contain gears, while inscriptions are found on many others.[13][14] The largest gear is approximately 13 centimetres (5.1in) in diameter and originally had 223 teeth.[15] In 2008, a team led by Mike Edmunds and Tony Freeth at Cardiff University used modern computer x-ray tomography and high resolution surface scanning to image inside fragments of the crust-encased mechanism and read the faintest inscriptions that once covered the outer casing of the machine. This suggests it had 37 meshing bronze gears enabling it to follow the movements of the Moon and the Sun through the zodiac, to predict eclipses and to model the irregular orbit of the Moon, where the Moon's velocity is higher in its perigee than in its apogee. This motion was studied in the 2nd century BC by astronomer Hipparchus of Rhodes, and it is speculated that he may have been consulted in the machine's construction.[16] There is speculation that a portion of the mechanism is missing and it also calculated the positions of the five classical planets. The instrument is believed to have been designed and constructed by Greek scientists and has been variously dated to about 87BC,[17] or between 150 and 100BC,[4] or to 205BC,[18][19] or to within a generation before the shipwreck, which has been dated to approximately 7060BC.[20][21] Later clockwork is known from the medieval Byzantine and Islamic worlds, but works with similar complexity did not appear again until the development of mechanical astronomical clocks in Europe in the fourteenth century.[22] All known fragments of the Antikythera mechanism are now kept at the National Archaeological Museum in Athens, along with a number of artistic reconstructions and replicas[23][24] to demonstrate how it may have looked and worked.[25] History[edit] Discovery[edit] Captain Dimitrios Kontos ( ) and a crew of sponge divers from Symi island discovered the Antikythera shipwreck during the spring of 1900, and recovered artefacts during the first expedition with the Hellenic Royal Navy, in 190001.[26] This wreck of a Roman cargo ship was found at a depth of 45 metres (148ft) off Point Glyphadia on the Greek island of Antikythera. The team retrieved numerous large artefacts, including bronze and marble statues, pottery, unique glassware, jewellery, coins, and the mechanism. The mechanism was retrieved from the wreckage in 1901, most probably in July of that year.[27] It is not known how the mechanism came to be on the cargo ship, but it has been suggested that it was being taken from Rhodes to Rome, together with other looted treasure, to support a triumphal parade being staged by Julius Caesar.[28] All of the items retrieved from the wreckage were transferred to the National Museum of Archaeology in Athens for storage and analysis. The mechanism appeared at the time to be little more than a lump of corroded bronze and wood; it went unnoticed for two years, while museum staff worked on piecing together more obvious treasures, such as the statues.[22] On 17 May 1902, archaeologist Valerios Stais found that one of the pieces of rock had a gear wheel embedded in it. He initially believed that it was an astronomical clock, but most scholars considered the device to be prochronistic, too complex to have been constructed during the same period as the other pieces that had been discovered. Investigations into the object were dropped until British science historian and Yale University professor Derek J. de Solla Price became interested in it in 1951.[29] In 1971, Price and Greek nuclear physicist Charalampos Karakalos made X-ray and gamma-ray images of the 82 fragments. Price published an extensive 70-page paper on their findings in 1974.[11] Two other searches for items at the Antikythera wreck site in 2012 and 2015 have yielded a number of fascinating art objects and a second ship which may or may not be connected with the treasure ship on which the Mechanism was found.[30] Also found was a bronze disc, embellished with the image of a bull. The disc has four \"ears\" which have holes in them, and it was thought by some that it may have been part of the Antikythera Mechanism itself, as a \"cog wheel\". However, there appears to be little evidence that it was part of the Mechanism; it is more likely that the disc was a bronze decoration on a piece of furniture.[31] Origin[edit] The Antikythera mechanism is generally referred to as the first known analogue computer.[32] The quality and complexity of the mechanism's manufacture suggests that it must have had undiscovered predecessors made during the Hellenistic period.[33] Its construction relied on theories of astronomy and mathematics developed by Greek astronomers during the second century BC, and it is estimated to have been built in the late second century BC[4] or the early first century BC.[34][5] In 2008, continued research by the Antikythera Mechanism Research Project suggested that the concept for the mechanism may have originated in the colonies of Corinth, since they identified the calendar on the Metonic Spiral as coming from Corinth or one of its colonies in northwest Greece or Sicily.[7] Syracuse was a colony of Corinth and the home of Archimedes, and the Antikythera Mechanism Research project argued in 2008 that it might imply a connection with the school of Archimedes.[7] However, it was demonstrated in 2017 that the calendar on the Metonic Spiral is indeed of the Corinthian type but cannot be that of Syracuse.[35] Another theory suggests that coins found by Jacques Cousteau at the wreck site in the 1970s date to the time of the device's construction, and posits that its origin may have been from the ancient Greek city of Pergamon,[36] home of the Library of Pergamum. With its many scrolls of art and science, it was second in importance only to the Library of Alexandria during the Hellenistic period.[37] The ship carrying the device also contained vases in the Rhodian style, leading to a hypothesis that it was constructed at an academy founded by Stoic philosopher Posidonius on that Greek island.[38] Rhodes was a busy trading port in antiquity and a centre of astronomy and mechanical engineering, home to astronomer Hipparchus, who was active from about 140 BC to 120 BC. The mechanism uses Hipparchus' theory for the motion of the Moon, which suggests the possibility that he may have designed it or at least worked on it.[22] In addition, it has recently been argued that the astronomical events on the Parapegma of the Antikythera mechanism work best for latitudes in the range of 33.337.0 degrees north;[39] the island of Rhodes is located between the latitudes of 35.85 and 36.50 degrees north. In 2014, a study by Carman and Evans argued for a new dating of approximately 200 BC based on identifying the start-up date on the Saros Dial as the astronomical lunar month that began shortly after the new moon of 28 April 205 BC.[18][19] Moreover, according to Carman and Evans, the Babylonian arithmetic style of prediction fits much better with the device's predictive models than the traditional Greek trigonometric style.[18] A study by Paul Iversen published in 2017 reasons that the prototype for the device was indeed from Rhodes, but that this particular model was modified for a client from Epirus in northwestern Greece; Iversen argues that it was probably constructed no earlier than a generation before the shipwreck, a date supported also by Jones.[40] Further dives were undertaken in 2014, with plans to continue in 2015, in the hope of discovering more of the mechanism.[19] A five-year programme of investigations began in 2014 and ended in October 2019, with a new five-year session starting in May 2020.[41][42] Description[edit] The original mechanism apparently came out of the Mediterranean as a single encrusted piece. Soon afterward it fractured into three major pieces. Other small pieces have broken off in the interim from cleaning and handling,[43] and still others were found on the sea floor by the Cousteau expedition. Other fragments may still be in storage, undiscovered since their initial recovery; Fragment F was discovered in that way in 2005. Of the 82 known fragments, seven are mechanically significant and contain the majority of the mechanism and inscriptions. There are also 16 smaller parts that contain fractional and incomplete inscriptions.[4][7][44] Major fragments[edit] Fragment Size [mm] Weight [g] Gears Inscriptions Notes A 180150 369.1 27 Yes The main fragment contains the majority of the known mechanism. Clearly visible on the front is the large b1 gear, and under closer inspection further gears behind said gear (parts of the l, m, c, and d trains are clearly visible as gears to the naked eye). The crank mechanism socket and the side-mounted gear that meshes with b1 is on Fragment A. The back of the fragment contains the rearmost e and k gears for synthesis of the moon anomaly, noticeable also is the pin and slot mechanism of the k train. It is noticed from detailed scans of the fragment that all gears are very closely packed and have sustained damage and displacement due to their years in the sea. The fragment is approximately 30mm thick at its thickest point. Fragment A also contains divisions of the upper left quarter of the Saros spiral and 14 inscriptions from said spiral. The fragment also contains inscriptions for the Exeligmos dial and visible on the back surface the remnants of the dial face. Finally, this fragment contains some back door inscriptions. B 12560 99.4 1 Yes Contains approximately the bottom right third of the Metonic spiral and inscriptions of both the spiral and back door of the mechanism. The Metonic scale would have consisted of 235 cells of which 49 have been deciphered from fragment B either in whole or partially. The rest so far are assumed from knowledge of the Metonic cycle. This fragment also contains a single gear (o1) used in the Olympic train. C 120110 63.8 1 Yes Contains parts of the upper right of the front dial face showing calendar and zodiac inscriptions. This fragment also contains the Moon indicator dial assembly including the Moon phase sphere in its housing and a single bevel gear (ma1) used in the Moon phase indication system. D 4535 15.0 1 Contains at least one unknown gear; according to Michael T. Wright it contains possibly two, and according to Xenophon Moussas[45] it contains one gear (numbered 45 \"ME\") inside a hollow gear giving the position of Jupiter reproducing it with epicyclic motion. Their purpose and position has not been ascertained to any accuracy or consensus, but lends to the debate for the possible planet displays on the face of the mechanism. E 6035 22.1 Yes Found in 1976 and contains six inscriptions from the upper right of the Saros spiral. F 9080 86.2 Yes Found in 2005 and contains 16 inscriptions from the lower right of the Saros spiral. It also contains remnants of the mechanism's wooden housing. G 125110 31.7 Yes A combination of fragments taken from fragment C while cleaning. Minor fragments[edit] Many of the smaller fragments that have been found contain nothing of apparent value; however, a few have some inscriptions on them. Fragment 19 contains significant back door inscriptions including one reading \"...76 years...\" which refers to the Callippic cycle. Other inscriptions seem to describe the function of the back dials. In addition to this important minor fragment, 15 further minor fragments have remnants of inscriptions on them.[15]:7 Mechanics[edit] Information on the specific data gleaned from the ruins by the latest inquiries is detailed in the supplement to Freeth's 2006 Nature article.[4] Operation[edit] On the front face of the mechanism there is a fixed ring dial representing the ecliptic, the twelve zodiacal signs marked off with equal 30-degree sectors. This matched with the Babylonian custom of assigning one twelfth of the ecliptic to each zodiac sign equally, even though the constellation boundaries were variable. Outside that dial is another ring which is rotatable, marked off with the months and days of the Sothic Egyptian calendar, twelve months of 30 days plus five intercalary days. The months are marked with the Egyptian names for the months transcribed into the Greek alphabet. The first task, then, is to rotate the Egyptian calendar ring to match the current zodiac points. The Egyptian calendar ignored leap days, so it advanced through a full zodiac sign in about 120 years.[5] The mechanism was operated by turning a small hand crank (now lost) which was linked via a crown gear to the largest gear, the four-spoked gear visible on the front of fragment A, the gear named b1. This moved the date pointer on the front dial, which would be set to the correct Egyptian calendar day. The year is not selectable, so it is necessary to know the year currently set, or by looking up the cycles indicated by the various calendar cycle indicators on the back in the Babylonian ephemeris tables for the day of the year currently set, since most of the calendar cycles are not synchronous with the year. The crank moves the date pointer about 78 days per full rotation, so hitting a particular day on the dial would be easily possible if the mechanism were in good working condition. The action of turning the hand crank would also cause all interlocked gears within the mechanism to rotate, resulting in the simultaneous calculation of the position of the Sun and Moon, the moon phase, eclipse, and calendar cycles, and perhaps the locations of planets.[46] The operator also had to be aware of the position of the spiral dial pointers on the two large dials on the back. The pointer had a \"follower\" that tracked the spiral incisions in the metal as the dials incorporated four and five full rotations of the pointers. When a pointer reached the terminal month location at either end of the spiral, the pointer's follower had to be manually moved to the other end of the spiral before proceeding further.[4]:10 Faces[edit] Computer-generated front panel of the Freeth model Front face[edit] The front dial has two concentric circular scales. The inner scale marks the Greek signs of the Zodiac, with division in degrees. The outer scale, which is a moveable ring that sits flush with the surface and runs in a channel, is marked off with what appear to be days and has a series of corresponding holes beneath the ring in the channel. Since the discovery of the Mechanism, this outer ring has been presumed to represent the 365-day Egyptian civil calendar. However, recent research challenges this presumption and gives evidence it is most likely divided into 354 intervals.[47] If one subscribes to the 365-day presumption, it is recognized the Mechanism predates the Julian calendar reform, but the Sothic and Callippic cycles had already pointed to a 365 14-day solar year, as seen in Ptolemy III's abortive calendrical reform of 238 BC. The dials are not believed to reflect his proposed leap day (Epag. 6), but the outer calendar dial may be moved against the inner dial to compensate for the effect of the extra quarter-day in the solar year by turning the scale backward one day every four years. However, if one subscribes to the 354-day evidence, then the most likely interpretation is that the ring is a manifestation of a 354-day lunar calendar. Given the era of the Mechanism's presumed construction and the presence of Egyptian month names, it is possibly the first example of the Egyptian civil-based lunar calendar proposed by Richard Anthony Parker in 1950.[48] The lunar calendars purpose was to serve as a day-to-day indicator of successive lunations, and would also have assisted with the interpretation of the Lunar phase pointer, and the Metonic and Saros dials. Undiscovered gearing, synchronous with the rest of the Metonic gearing of the mechanism, is implied to drive a pointer around this scale. Movement and registration of the ring relative to the underlying holes served to facilitate both a one-in-76-year Callippic cycle correction, as well as convenient lunisolar intercalation. The dial also marks the position of the Sun on the ecliptic corresponds to the current date in the year. The orbits of the Moon and the five planets known to the Greeks are close enough to the ecliptic to make it a convenient reference for defining their positions as well. The following three Egyptian months are inscribed in Greek letters on the surviving pieces of the outer ring:[49] (Pachon) (Payni) (Epiphi) The other months have been reconstructed, although some reconstructions of the mechanism omit the five days of the Egyptian intercalary month. The Zodiac dial contains Greek inscriptions of the members of the zodiac, which is believed to be adapted to the tropical month version rather than the sidereal:[15]:8[failed verification] Front panel of a 2007 recreation (Krios [Ram], Aries) (Tauros [Bull], Taurus) (Didymoi [Twins], Gemini) (Karkinos [Crab], Cancer) (Leon [Lion], Leo) (Parthenos [Maiden], Virgo) (Chelai [Scorpio's Claw or Zygos], Libra) (Skorpios [Scorpion], Scorpio) (Toxotes [Archer], Sagittarius) (Aigokeros [Goat-horned], Capricorn) (Hydrokhoos [Water carrier], Aquarius) (Ichthyes [Fish], Pisces) Also on the zodiac dial are a number of single characters at specific points (see reconstruction here:[50]). They are keyed to a parapegma, a precursor of the modern day almanac inscribed on the front face above and beneath the dials. They mark the locations of longitudes on the ecliptic for specific stars. The parapegma above the dials reads (square brackets indicate inferred text): [...] Capricorn begins to rise [...] Aries begins to rise [...] Winter solstice [...] Vernal equinox [...] ... evening [...] [...] ... evening [...] ... evening [...] The Hyades set in the evening [...] Aquarius begins to rise {} Taurus begins to rise [...] [...] {} ... evening [...] Lyra rises in the evening [...] [...] ... {evening} [...] The Pleiades rise in the morning [...] Pisces begins to rise [...] The Hyades rise in the morning [...] {} [...] Gemini begins to rise Altair rises in the evening {}{} Arcturus sets in the morning The parapegma beneath the dials reads: [...] Libra begins to rise [...] Cancer begins {to rise} [...] Autumnal equinox [...] Summer solstice [...] ... rise in the evening Orion precedes the morning [...] ... rise in the evening {} Canis Major precedes the morning [...] {} ... rise Altair sets in the morning Scorpio begins to rise [...] Leo begins to rise [...] [...] [...] [...] [...] [...] [...] Sagittarius begins to rise [...] [...] [...] [...] [...] At least two pointers indicated positions of bodies upon the ecliptic. A lunar pointer indicated the position of the Moon, and a mean Sun pointer also was shown, perhaps doubling as the current date pointer. The Moon position was not a simple mean Moon indicator that would indicate movement uniformly around a circular orbit; it approximated the acceleration and deceleration of the Moon's elliptical orbit, through the earliest extant use of epicyclic gearing. It also tracked the precession of the elliptical orbit around the ecliptic in an 8.88-year cycle. The mean Sun position is, by definition, the current date. It is speculated that since such pains were taken to get the position of the Moon correct,[15]:20,24 then there also was likely to have been a \"true sun\" pointer in addition to the mean Sun pointer likewise, to track the elliptical anomaly of the Sun (the orbit of Earth around the Sun), but there is no evidence of it among the ruins of the mechanism found to date.[5] Similarly, neither is there the evidence of planetary orbit pointers for the five planets known to the Greeks among the ruins. See Proposed planet indication gearing schemes below. Mechanical engineer Michael Wright demonstrated that there was a mechanism to supply the lunar phase in addition to the position.[51] The indicator was a small ball embedded in the lunar pointer, half-white and half-black, which rotated to show the phase (new, first quarter, half, third quarter, full, and back) graphically. The data to support this function is available given the Sun and Moon positions as angular rotations; essentially, it is the angle between the two, translated into the rotation of the ball. It requires a differential gear, a gearing arrangement that sums or differences two angular inputs. Rear face[edit] Computer-generated back panel In July 2008, scientists reported new findings in the journal Nature showing that the mechanism not only tracked the Metonic calendar and predicted solar eclipses, but also calculated the timing of several panhellenic athletic games, including the Ancient Olympic Games.[7] Inscriptions on the instrument closely match the names of the months that are used on calendars from Epirus in northwestern Greece and with the island of Corfu, which in antiquity was known as Corcyra.[52][53][54] On the back of the mechanism, there are five dials: the two large displays, the Metonic and the Saros, and three smaller indicators, the so-called Olympiad Dial,[7] which has recently been renamed the Games dial as it did not track Olympiad years (the four-year cycle it tracks most closely is the Halieiad),[9] the Callippic, and the Exeligmos.[4]:11 The Metonic Dial is the main upper dial on the rear of the mechanism. The Metonic cycle, defined in several physical units, is 235 synodic months, which is very close (to within less than 13 one-millionths) to 19 tropical years. It is therefore a convenient interval over which to convert between lunar and solar calendars. The Metonic dial covers 235 months in five rotations of the dial, following a spiral track with a follower on the pointer that keeps track of the layer of the spiral. The pointer points to the synodic month, counted from new moon to new moon, and the cell contains the Corinthian month names.[7][55][56] (Phoinikaios) (Kraneios) (Lanotropios) (Machaneus, \"mechanic\", referring to Zeus the inventor) (Dodekateus) (Eukleios) (Artemisios) (Psydreus) (Gameilios) (Agrianios) (Panamos) (Apellaios) Thus, setting the correct solar time (in days) on the front panel indicates the current lunar month on the back panel, with resolution to within a week or so. Based on the fact that the calendar month names are consistent with all the evidence of the Epirote calendar and that the Games dial mentions the very minor Naa games of Dodona (in Epirus), it has recently been argued that the calendar on the Antikythera Mechanism is likely to be the Epirote calendar, and that this calendar was probably adopted from a Corinthian colony in Epirus, possibly Ambracia.[56] It has also been argued that the first month of the calendar, Phoinikaios, was ideally the month in which the autumn equinox fell, and that the start-up date of the calendar began shortly after the astronomical new moon of 23 August 205 BC.[57] The Callippic dial is the left secondary upper dial, which follows a 76-year cycle. The Callippic cycle is four Metonic cycles, and so this dial indicates the current Metonic cycle in the overall Callippic cycle.[citation needed] The Games dial is the right secondary upper dial; it is the only pointer on the instrument that travels in a counter-clockwise direction as time advances. The dial is divided into four sectors, each of which is inscribed with a year indicator and the name of two Panhellenic Games: the \"crown\" games of Isthmia, Olympia, Nemea, and Pythia; and two lesser games: Naa (held at Dodona),[58] and the sixth and final set of Games recently deciphered as the Halieia of Rhodes.[59] The inscriptions on each one of the four divisions are:[4][7] Olympic dial Year of the cycle Inside the dial inscription Outside the dial inscription 1 L (Isthmia) (Olympia) 2 L (Nemea)NAA (Naa) 3 L (Isthmia) (Pythia) 4 L (Nemea) (Halieia) The Saros dial is the main lower spiral dial on the rear of the mechanism.[4]:45,10 The Saros cycle is 18 years and 11+13 days long (6585.333... days), which is very close to 223 synodic months (6585.3211 days). It is defined as the cycle of repetition of the positions required to cause solar and lunar eclipses, and therefore, it could be used to predict themnot only the month, but the day and time of day. Note that the cycle is approximately 8 hours longer than an integer number of days. Translated into global spin, that means an eclipse occurs not only eight hours later, but one-third of a rotation farther to the west. Glyphs in 51 of the 223 synodic month cells of the dial specify the occurrence of 38 lunar and 27 solar eclipses. Some of the abbreviations in the glyphs read:[citation needed] = (\"Selene\", Moon) = (\"Helios\", Sun) H\\M = (\"Hemeras\", of the day) \\ = (\"hora\", hour) N\\Y = (\"Nuktos\", of the night) The glyphs show whether the designated eclipse is solar or lunar, and give the day of the month and hour. Solar eclipses may not be visible at any given point, and lunar eclipses are visible only if the moon is above the horizon at the appointed hour.[15]:6 In addition, the inner lines at the cardinal points of the Saros dial indicate the start of a new full moon cycle. Based on the distribution of the times of the eclipses, it has recently been argued that the start-up date of the Saros dial was shortly after the astronomical new moon of 28 April 205 BC.[18] The Exeligmos Dial is the secondary lower dial on the rear of the mechanism. The Exeligmos cycle is a 54-year triple Saros cycle that is 19,756 days long. Since the length of the Saros cycle is to a third of a day (eight hours), so a full Exeligmos cycle returns counting to integer days, hence the inscriptions. The labels on its three divisions are:[4]:10 Blank or o? (representing the number zero, assumed, not yet observed) H (number 8) means add 8 hours to the time mentioned in the display I (number 16) means add 16 hours to the time mentioned in the display Thus the dial pointer indicates how many hours must be added to the glyph times of the Saros dial in order to calculate the exact eclipse times.[citation needed] Doors[edit] The mechanism has a wooden casing with a front and a back door, both containing inscriptions.[7][15] The back door appears to be the \"instruction manual\". On one of its fragments is written \"76 years, 19 years\" representing the Callippic and Metonic cycles. Also written is \"223\" for the Saros cycle. On another one of its fragments, it is written \"on the spiral subdivisions 235\" referring to the Metonic dial. Gearing[edit] The mechanism is remarkable for the level of miniaturisation and the complexity of its parts, which is comparable to that of fourteenth-century astronomical clocks. It has at least 30 gears, although mechanism expert Michael Wright has suggested that the Greeks of this period were capable of implementing a system with many more gears.[46] There is much debate as to whether the mechanism had indicators for all five of the planets known to the ancient Greeks. No gearing for such a planetary display survives and all gears are accounted forwith the exception of one 63-toothed gear (r1) otherwise unaccounted for in fragment D.[5] Fragment D is a small quasi-circular constriction that, according to Xenophon Moussas, has a gear inside a somewhat larger hollow gear. The inner gear moves inside the outer gear reproducing an epicyclical motion that, with a pointer, gives the position of planet Jupiter.[60][61] The inner gear is numbered 45, \"ME\" in Greek and the same number is written on two surfaces of this small cylindrical box. The purpose of the front face was to position astronomical bodies with respect to the celestial sphere along the ecliptic, in reference to the observer's position on the Earth. That is irrelevant to the question of whether that position was computed using a heliocentric or geocentric view of the Solar System; either computational method should, and does, result in the same position (ignoring ellipticity), within the error factors of the mechanism. The epicyclic Solar System of Ptolemy (c. AD 100170)still 300 years in the future from the apparent date of the mechanismcarried forward with more epicycles, and was more accurate predicting the positions of planets than the view of Copernicus (14731543), until Kepler (15711630) introduced the possibility that orbits are ellipses.[62] Evans et al. suggest that to display the mean positions of the five classical planets would require only 17 further gears that could be positioned in front of the large driving gear and indicated using individual circular dials on the face.[63] Tony Freeth and Alexander Jones have modelled and published details of a version using several gear trains mechanically-similar to the lunar anomaly system allowing for indication of the positions of the planets as well as synthesis of the Sun anomaly. Their system, they claim, is more authentic than Wright's model as it uses the known skill sets of the Greeks of that period and does not add excessive complexity or internal stresses to the machine.[5] The gear teeth were in the form of equilateral triangles with an average circular pitch of 1.6mm, an average wheel thickness of 1.4mm and an average air gap between gears of 1.2mm. The teeth probably were created from a blank bronze round using hand tools; this is evident because not all of them are even.[5] Due to advances in imaging and X-ray technology it is now possible to know the precise number of teeth and size of the gears within the located fragments. Thus the basic operation of the device is no longer a mystery and has been replicated accurately. The major unknown remains the question of the presence and nature of any planet indicators.[15]:8 A table of the gears, their teeth, and the expected and computed rotations of various important gears follows. The gear functions come from Freeth et al. (2008)[7] and those for the lower half of the table from Freeth and Jones 2012.[5] The computed values start with 1 year/revolution for the b1 gear, and the remainder are computed directly from gear teeth ratios. The gears marked with an asterisk (*) are missing, or have predecessors missing, from the known mechanism; these gears have been calculated with reasonable gear teeth counts.[7][15] The Antikythera Mechanism: known gears and accuracy of computation Gear name[table 1] Function of the gear/pointer Expected simulated interval of a full circular revolution Mechanism formula[table 2] Computed interval Gear direction[table 3] x Year gear 1 tropical year 1 (by definition) 1 year (presumed) cw[table 4] b the Moon's orbit 1 sidereal month (27.321661 days) Time(b) = Time(x) * (c1 / b2) * (d1 / c2) * (e2 / d2) * (k1 / e5) * (e6 / k2) * (b3 / e1) 27.321 days[table 5] cw r lunar phase display 1 synodic month (29.530589 days) Time(r) = 1 / (1 / Time(b2 [mean sun] or sun3 [true sun])) (1 / Time(b))) 29.530 days[table 5] n* Metonic pointer Metonic cycle () / 5 spirals around the dial = 1387.94 days Time(n) = Time(x) * (l1 / b2) * (m1 /l2) * (n1 / m2) 1387.9 days ccw[table 6] o* Games dial pointer 4 years Time(o) = Time(n) * (o1 / n2) 4.00 years cw[table 6][table 7] q* Callippic pointer 27758.8 days Time(q) = Time(n) * (p1 / n3) * (q1 /p2) 27758 days ccw[table 6] e* lunar orbit precession 8.85 years Time(e) = Time(x) * (l1 / b2) * (m1 / l2) * (e3 / m3) 8.8826 years ccw[table 8] g* Saros cycle Saros time / 4 turns = 1646.33 days Time(g) = Time(e) * (f1 / e4) * (g1 / f2) 1646.3 days ccw[table 6] i* Exeligmos pointer 19755.8 days Time(i) = Time(g) * (h1 / g2) * (i1 / h2) 19756 days ccw[table 6] The following are proposed gearing from the 2012 Freeth and Jones reconstruction: sun3* True sun pointer 1 mean year Time(sun3) = Time(x) * (sun3 / sun1) * (sun2 / sun3) 1 mean year[table 5] cw[table 9] mer2* Mercury pointer 115.88 days (synodic period) Time(mer2) = Time(x) * (mer2 / mer1) 115.89 days[table 5] cw[table 9] ven2* Venus pointer 583.93 days (synodic period) Time(ven2) = Time(x) * (ven1 / sun1) 584.39 days[table 5] cw[table 9] mars4* Mars pointer 779.96 days (synodic period) Time(mars4) = Time(x) * (mars2 / mars1) * (mars4 / mars3) 779.84 days[table 5] cw[table 9] jup4* Jupiter pointer 398.88 days (synodic period) Time(jup4) = Time(x) * (jup2 / jup1) * (jup4 / jup3) 398.88 days[table 5] cw[table 9] sat4* Saturn pointer 378.09 days (synodic period) Time(sat4) = Time(x) * (sat2 / sat1) * (sat4 / sat3) 378.06 days[table 5] cw[table 9] Table notes: ^ Change from traditional naming: X is the main year axis, turns once per year with gear B1. The B axis is the axis with gears B3 and B6, while the E axis is the axis with gears E3 and E4. Other axes on E (E1/E6 and E2/E5) are irrelevant to this table. ^ \"Time\" is the interval represented by one complete revolution of the gear. ^ As viewed from the front of the Mechanism. The \"natural\" view is viewing the side of the Mechanism the dial/pointer in question is actually displayed on. ^ The Greeks, being in the northern hemisphere, assumed proper daily motion of the stars was from east to west, ccw when the ecliptic and zodiac is viewed to the south. As viewed on the front of the Mechanism. ^ a b c d e f g h On average, due to epicyclic gearing causing accelerations and decelerations. ^ a b c d e Being on the reverse side of the box, the \"natural\" rotation is the opposite ^ This was the only visual pointer naturally travelling in the counter-clockwise direction. ^ Internal and not visible. ^ a b c d e f Prograde motion; retrograde is obviously the opposite direction. There are several gear ratios for each planet that result in close matches to the correct values for synodic periods of the planets and the Sun. The ones chosen above seem to provide good accuracy with reasonable tooth counts, but the specific gears that may have been used are, and probably will remain, unknown.[5] Known gear scheme[edit] A hypothetical schematic representation of the gearing of the Antikythera Mechanism, including the 2012 published interpretation of existing gearing, gearing added to complete known functions, and proposed gearing to accomplish additional functions, namely true sun pointer and pointers for the five then-known planets, as proposed by Freeth and Jones, 2012.[5] Based also upon similar drawing in the Freeth 2006 Supplement[15] and Wright 2005, Epicycles Part 2.[64] Proposed (as opposed to known from the artefact) gearing crosshatched. It is very probable that there were planetary dials, as the complicated motions and periodicities of all planets are mentioned in the manual of the mechanism. The exact position and mechanisms for the gears of the planets is not known. There is no coaxial system but only for the Moon. Fragment D that is an epicycloidal system is considered as a planetary gear for Jupiter (Moussas, 2011, 2012, 2014) or a gear for the motion of the Sun (University of Thessaloniki group). The Sun gear is operated from the hand-operated crank (connected to gear a1, driving the large four-spoked mean Sun gear, b1) and in turn drives the rest of the gear sets. The Sun gear is b1/b2 and b2 has 64 teeth. It directly drives the date/mean sun pointer (there may have been a second, \"true sun\" pointer that displayed the Sun's elliptical anomaly; it is discussed below in the Freeth reconstruction). In this discussion, reference is to modelled rotational period of various pointers and indicators; they all assume the input rotation of the b1 gear of 360 degrees, corresponding with one tropical year, and are computed solely on the basis of the gear ratios of the gears named.[4][7][65] The Moon train starts with gear b1 and proceeds through c1, c2, d1, d2, e2, e5, k1, k2, e6, e1, and b3 to the Moon pointer on the front face. The gears k1 and k2 form an epicyclic gear system; they are an identical pair of gears that don't mesh, but rather, they operate face-to-face, with a short pin on k1 inserted into a slot in k2. The two gears have different centres of rotation, so the pin must move back and forth in the slot. That increases and decreases the radius at which k2 is driven, also necessarily varying its angular velocity (presuming the velocity of k1 is even) faster in some parts of the rotation than others. Over an entire revolution the average velocities are the same, but the fast-slow variation models the effects of the elliptical orbit of the Moon, in consequence of Kepler's second and third laws. The modelled rotational period of the Moon pointer (averaged over a year) is 27.321 days, compared to the modern length of a lunar sidereal month of 27.321661 days. As mentioned, the pin/slot driving of the k1/k2 gears varies the displacement over a year's time, and the mounting of those two gears on the e3 gear supplies a precessional advancement to the ellipticity modelling with a period of 8.8826 years, compared with the current value of precession period of the moon of 8.85 years.[4][7][65] The system also models the phases of the Moon. The Moon pointer holds a shaft along its length, on which is mounted a small gear named r, which meshes to the Sun pointer at B0 (the connection between B0 and the rest of B is not visible in the original mechanism, so whether b0 is the current date/mean Sun pointer or a hypothetical true Sun pointer is not known). The gear rides around the dial with the Moon, but is also geared to the Sunthe effect is to perform a differential gear operation, so the gear turns at the synodic month period, measuring in effect, the angle of the difference between the Sun and Moon pointers. The gear drives a small ball that appears through an opening in the Moon pointer's face, painted longitudinally half white and half black, displaying the phases pictorially. It turns with a modelled rotational period of 29.53 days; the modern value for the synodic month is 29.530589 days.[4][7][65] The Metonic train is driven by the drive train b1, b2, l1, l2, m1, m2, and n1, which is connected to the pointer. The modelled rotational period of the pointer is the length of the 6939.5 days (over the whole five-rotation spiral), while the modern value for the Metonic cycle is 6939.69 days.[4][7][65] The Olympiad train is driven by b1, b2, l1, l2, m1, m2, n1, n2, and o1, which mounts the pointer. It has a computed modelled rotational period of exactly four years, as expected. Incidentally, it is the only pointer on the mechanism that rotates counter-clockwise; all of the others rotate clockwise.[4][7][65] The Callippic train is driven by b1, b2, l1, l2, m1, m2, n1, n3, p1, p2, and q1, which mounts the pointer. It has a computed modelled rotational period of 27758 days, while the modern value is 27758.8 days.[4][7][65] The Saros train is driven by b1, b2, l1, l2, m1, m3, e3, e4, f1, f2, and g1, which mounts the pointer. The modelled rotational period of the Saros pointer is 1646.3 days (in four rotations along the spiral pointer track); the modern value is 1646.33 days.[4][7][65] The Exeligmos train is driven by b1, b2, l1, l2, m1, m3, e3, e4, f1, f2, g1, g2, h1, h2, and i1, which mounts the pointer. The modelled rotational period of the Exeligmos pointer is 19,756 days; the modern value is 19755.96 days.[4][7][65] Apparently, gears m3, n1-3, p1-2, and q1 did not survive in the wreckage. The functions of the pointers were deduced from the remains of the dials on the back face, and reasonable, appropriate gearage to fulfill the functions was proposed, and is generally accepted.[4][7][65] Reconstruction efforts[edit] Proposed gear schemes[edit] Because of the large space between the mean Sun gear and the front of the case and the size of and mechanical features on the mean Sun gear it is very likely that the mechanism contained further gearing that either has been lost in or subsequent to the shipwreck or was removed before being loaded onto the ship.[5] This lack of evidence and nature of the front part of the mechanism has led to numerous attempts to emulate what the Greeks of the period would have done and, of course, because of the lack of evidence many solutions have been put forward. Wright proposalEvans et al. proposalFreeth et al. proposal Michael Wright was the first person to design and build a model with not only the known mechanism, but also, with his emulation of a potential planetarium system. He suggested that along with the lunar anomaly, adjustments would have been made for the deeper, more basic solar anomaly (known as the \"first anomaly\"). He included pointers for this \"true sun\", Mercury, Venus, Mars, Jupiter, and Saturn, in addition to the known \"mean sun\" (current time) and lunar pointers.[5] Evans, Carman, and Thorndike published a solution with significant differences from Wright's.[63] Their proposal centred on what they observed as irregular spacing of the inscriptions on the front dial face, which to them seemed to indicate an off-centre sun indicator arrangement; this would simplify the mechanism by removing the need to simulate the solar anomaly. They also suggested that rather than accurate planetary indication (rendered impossible by the offset inscriptions) there would be simple dials for each individual planet showing information such as key events in the cycle of planet, initial and final appearances in the night sky, and apparent direction changes. This system would lead to a much simplified gear system, with much reduced forces and complexity, as compared to Wright's model.[63] Their proposal used simple meshed gear trains and accounted for the previously unexplained 63 toothed gear in fragment D. They proposed two face plate layouts, one with evenly spaced dials, and another with a gap in the top of the face to account for criticism regarding their not using the apparent fixtures on the b1 gear. They proposed that rather than bearings and pillars for gears and axles, they simply held weather and seasonal icons to be displayed through a window.[63] In a paper published in 2012 Carman, Thorndike, and Evans also proposed a system of epicyclic gearing with pin and slot followers.[66] Freeth and Jones published their proposal in 2012 after extensive research and work. They came up with a compact and feasible solution to the question of planetary indication. They also propose indicating the solar anomaly (that is, the sun's apparent position in the zodiac dial) on a separate pointer from the date pointer, which indicates the mean position of the Sun, as well as the date on the month dial. If the two dials are synchronised correctly, their front panel display is essentially the same as Wright's. Unlike Wright's model however, this model has not been built physically, and is only a 3-D computer model.[5] Internal gearing relationships of the Antikythera Mechanism, based on the Freeth and Jones proposal The system to synthesise the solar anomaly is very similar to that used in Wright's proposal: three gears, one fixed in the centre of the b1 gear and attached to the Sun spindle, the second fixed on one of the spokes (in their proposal the one on the bottom left) acting as an idle gear, and the final positioned next to that one; the final gear is fitted with an offset pin and, over said pin, an arm with a slot that in turn, is attached to the sun spindle, inducing anomaly as the mean Sun wheel turns.[5] The inferior planet mechanism includes the Sun (treated as a planet in this context), Mercury, and Venus.[5] For each of the three systems there is an epicyclic gear whose axis is mounted on b1, thus the basic frequency is the Earth year (as it is, in truth, for epicyclic motion in the Sun and all the planetsexcepting only the Moon). Each meshes with a gear grounded to the mechanism frame. Each has a pin mounted, potentially on an extension of one side of the gear that enlarges the gear, but doesn't interfere with the teeth; in some cases the needed distance between the gear's centre and the pin is farther than the radius of the gear itself. A bar with a slot along its length extends from the pin toward the appropriate coaxial tube, at whose other end is the object pointer, out in front of the front dials. The bars could have been full gears, although there is no need for the waste of metal, since the only working part is the slot. Also, using the bars avoids interference between the three mechanisms, each of which are set on one of the four spokes of b1. Thus there is one new grounded gear (one was identified in the wreckage, and the second is shared by two of the planets), one gear used to reverse the direction of the sun anomaly, three epicyclic gears and three bars/coaxial tubes/pointers, which would qualify as another gear each: five gears and three slotted bars in all.[5] The superior planet systemsMars, Jupiter, and Saturnall follow the same general principle of the lunar anomaly mechanism.[5] Similar to the inferior systems, each has a gear whose centre pivot is on an extension of b1, and which meshes with a grounded gear. It presents a pin and a centre pivot for the epicyclic gear which has a slot for the pin, and which meshes with a gear fixed to a coaxial tube and thence to the pointer. Each of the three mechanisms can fit within a quadrant of the b1 extension, and they are thus all on a single plane parallel with the front dial plate. Each one uses a ground gear, a driving gear, a driven gear, and a gear/coaxial tube/pointer, thus, twelve gears additional in all. In total, there are eight coaxial spindles of various nested sizes to transfer the rotations in the mechanism to the eight pointers. So in all, there are 30 original gears, seven gears added to complete calendar functionality, 17 gears and three slotted bars to support the six new pointers, for a grand total of 54 gears, three bars, and eight pointers in Freeth and Jones' design.[5] On the visual representation Freeth supplies in the paper, the pointers on the front zodiac dial have small, round identifying stones. He mentions a quote from an ancient papyrus: ...a voice comes to you speaking. Let the stars be set upon the board in accordance with [their] nature except for the Sun and Moon. And let the Sun be golden, the Moon silver, Kronos [Saturn] of obsidian, Ares [Mars] of reddish onyx, Aphrodite [Venus] lapis lazuli veined with gold, Hermes [Mercury] turquoise; let Zeus [Jupiter] be of (whitish?) stone, crystalline (?)...[67] In March 2021, the Antikythera Research Team at University College London, led by Freeth, published their proposed reconstruction of the entire Antikythera Mechanism.[68][69][70] Accuracy[edit] Investigations by Freeth and Jones reveal that their simulated mechanism is not particularly accurate, the Mars pointer being up to 38 off at times (these inaccuracies occur at the nodal points of Mars' retrograde motion, and the error recedes at other locations in the orbit). This is not due to inaccuracies in gearing ratios in the mechanism, but rather due to inadequacies in the Greek theory of planetary movements. The accuracy could not have been improved until first Ptolemy put forth his Planetary Hypotheses in the second half of the second century AD (particularly adding the concept of the equant to his theory) and then finally by the introduction of Kepler's Second Law in the early 17th century.[5] In short, the Antikythera Mechanism was a machine designed to predict celestial phenomena according to the sophisticated astronomical theories current in its day, the sole witness to a lost history of brilliant engineering, a conception of pure genius, one of the great wonders of the ancient worldbut it didn't really work very well![5] In addition to theoretical accuracy, there is the matter of mechanical accuracy. Freeth and Jones note that the inevitable \"looseness\" in the mechanism due to the hand-built gears, with their triangular teeth and the frictions between gears, and in bearing surfaces, probably would have swamped the finer solar and lunar correction mechanisms built into it: Though the engineering was remarkable for its era, recent research indicates that its design conception exceeded the engineering precision of its manufacture by a wide marginwith considerable cumulative inaccuracies in the gear trains, which would have cancelled out many of the subtle anomalies built into its design.[5][71] While the device itself may have struggled with inaccuracies due to the triangular teeth being hand-made, the calculations used and the technology implemented to create the elliptical paths of the planets and retrograde motion of the Moon and Mars by using a clockwork-type gear train with the addition of a pin-and-slot epicyclic mechanism predated that of the first known clocks found in antiquity in Medieval Europe by more than 1000 years.[72] Archimedes' development of the approximate value of pi and his theory of centres of gravity, along with the steps he made towards developing the calculus,[73] all suggest that the Greeks had access to more than enough mathematical knowledge beyond that of just Babylonian algebra in order to be able to model the elliptical nature of planetary motion. Of special delight to physicists, the Moon mechanism uses a special train of bronze gears, two of them linked with a slightly offset axis, to indicate the position and phase of the moon. As is known today from Kepler's Laws of Planetary Motion, the moon travels at different speeds as it orbits the Earth, and this speed differential is modelled by the Antikythera Mechanism, even though the ancient Greeks were not aware of the actual elliptical shape of the orbit.[74] Similar devices in ancient literature[edit] Cicero's De re publica, a first century BC philosophical dialogue, mentions two machines that some modern authors consider as some kind of planetarium or orrery, predicting the movements of the Sun, the Moon, and the five planets known at that time. They were both built by Archimedes and brought to Rome by the Roman general Marcus Claudius Marcellus after the death of Archimedes at the siege of Syracuse in 212 BC. Marcellus had great respect for Archimedes and one of these machines was the only item he kept from the siege (the second was placed in the Temple of Virtue). The device was kept as a family heirloom, and Cicero has Philus (one of the participants in a conversation that Cicero imagined had taken place in a villa belonging to Scipio Aemilianus in the year 129 BC) saying that Gaius Sulpicius Gallus (consul with Marcellus's nephew in 166 BC, and credited by Pliny the Elder as the first Roman to have written a book explaining solar and lunar eclipses) gave both a \"learned explanation\" and a working demonstration of the device. I had often heard this celestial globe or sphere mentioned on account of the great fame of Archimedes. Its appearance, however, did not seem to me particularly striking. There is another, more elegant in form, and more generally known, moulded by the same Archimedes, and deposited by the same Marcellus, in the Temple of Virtue at Rome. But as soon as Gallus had begun to explain, by his sublime science, the composition of this machine, I felt that the Sicilian geometrician must have possessed a genius superior to any thing we usually conceive to belong to our nature. Gallus assured us, that the solid and compact globe, was a very ancient invention, and that the first model of it had been presented by Thales of Miletus. That afterwards Eudoxus of Cnidus, a disciple of Plato, had traced on its surface the stars that appear in the sky, and that many years subsequent, borrowing from Eudoxus this beautiful design and representation, Aratus had illustrated them in his verses, not by any science of astronomy, but the ornament of poetic description. He added, that the figure of the sphere, which displayed the motions of the Sun and Moon, and the five planets, or wandering stars, could not be represented by the primitive solid globe. And that in this, the invention of Archimedes was admirable, because he had calculated how a single revolution should maintain unequal and diversified progressions in dissimilar motions. When Gallus moved this globe it showed the relationship of the Moon with the Sun, and there were exactly the same number of turns on the bronze device as the number of days in the real globe of the sky. Thus it showed the same eclipse of the Sun as in the globe [of the sky], as well as showing the Moon entering the area of the Earth's shadow when the Sun is in line... [missing text] [i.e. It showed both solar and lunar eclipses.][75] Pappus of Alexandria stated that Archimedes had written a now lost manuscript on the construction of these devices entitled On Sphere-Making.[76][77] The surviving texts from ancient times describe many of his creations, some even containing simple drawings. One such device is his odometer, the exact model later used by the Romans to place their mile markers (described by Vitruvius, Heron of Alexandria and in the time of Emperor Commodus).[78] The drawings in the text appeared functional, but attempts to build them as pictured had failed. When the gears pictured, which had square teeth, were replaced with gears of the type in the Antikythera mechanism, which were angled, the device was perfectly functional.[79] If Cicero's account is correct, then this technology existed as early as the third century BC. Archimedes' device is also mentioned by later Roman era writers such as Lactantius (Divinarum Institutionum Libri VII), Claudian (In sphaeram Archimedes), and Proclus (Commentary on the first book of Euclid's Elements of Geometry) in the fourth and fifth centuries. Cicero also said that another such device was built \"recently\" by his friend Posidonius, \"... each one of the revolutions of which brings about the same movement in the Sun and Moon and five wandering stars [planets] as is brought about each day and night in the heavens...\"[80] It is unlikely that any one of these machines was the Antikythera mechanism found in the shipwreck since both the devices fabricated by Archimedes and mentioned by Cicero were located in Rome at least 30 years later than the estimated date of the shipwreck, and the third device was almost certainly in the hands of Posidonius by that date. The scientists who have reconstructed the Antikythera mechanism also agree that it was too sophisticated to have been a unique device. This evidence that the Antikythera mechanism was not unique adds support to the idea that there was an ancient Greek tradition of complex mechanical technology that was later, at least in part, transmitted to the Byzantine and Islamic worlds, where mechanical devices which were complex, albeit simpler than the Antikythera mechanism, were built during the Middle Ages.[81] Fragments of a geared calendar attached to a sundial, from the fifth or sixth century Byzantine Empire, have been found; the calendar may have been used to assist in telling time.[82] In the Islamic world, Ban Ms's Kitab al-Hiyal, or Book of Ingenious Devices, was commissioned by the Caliph of Baghdad in the early 9th century AD. This text described over a hundred mechanical devices, some of which may date back to ancient Greek texts preserved in monasteries. A geared calendar similar to the Byzantine device was described by the scientist al-Biruni around 1000, and a surviving 13th-century astrolabe also contains a similar clockwork device.[82] It is possible that this medieval technology may have been transmitted to Europe and contributed to the development of mechanical clocks there.[22] In the 11th century, Chinese polymath Su Song constructed a mechanical clock tower that told (among other measurements) the position of some stars and planets, which where shown on a mechanically rotated armillary sphere.[83] Popular culture[edit] On 17 May 2017, Google marked the 115th anniversary of the discovery with a Google Doodle.[84][85] As of 2012, the Antikythera mechanism was displayed as part of a temporary exhibition about the Antikythera Shipwreck,[86] accompanied by reconstructions made by Ioannis Theofanidis, Derek de Solla Price, Michael Wright, the Thessaloniki University and Dionysios Kriaris. Other reconstructions are on display at the American Computer Museum in Bozeman, Montana, at the Children's Museum of Manhattan in New York, at Astronomisch-Physikalisches Kabinett in Kassel, Germany, and at the Muse des Arts et Mtiers in Paris. The National Geographic documentary series Naked Science had an episode dedicated to the Antikythera Mechanism entitled \"Star Clock BC\" that aired on 20 January 2011.[87] A documentary, The World's First Computer, was produced in 2012 by the Antikythera mechanism researcher and film-maker Tony Freeth.[88] In 2012 BBC Four aired The Two-Thousand-Year-Old Computer;[89] it was also aired on 3 April 2013 in the United States on NOVA, the PBS science series, under the name Ancient Computer.[90] It documents the discovery and 2005 investigation of the mechanism by the Antikythera Mechanism Research Project. A fully functioning Lego reconstruction of the Antikythera mechanism was built in 2010 by hobbyist Andy Carol, and featured in a short film produced by Small Mammal in 2011.[91] Several exhibitions have been staged worldwide,[92] leading to the main \"Antikythera shipwreck\" exhibition at the National Archaeological Museum in Athens, Greece. A fictionalised version of the device was a central plot point in the film Stonehenge Apocalypse (2010), where it was used as the artefact that saved the world from impending doom.[93] The massively multiplayer video game Eve Online contains an item named \"Antikythera Element\" obtained from game content surrounding a mysterious group of non-player characters themed as ancient Greeks.[94] See also[edit] Archimedes Palimpsest Astrarium Automaton Ctesibius Reverse engineering References[edit] ^ Efstathiou, Kyriakos; Efstathiou, Marianna (1 September 2018). \"Celestial Gearbox: Oldest Known Computer is a Mechanism Designed to Calculate the Location of the Sun, Moon, and Planets\". Mechanical Engineering. 140 (9): 3135. doi:10.1115/1.2018-SEP1. ISSN0025-6501. ^ Ken Steiglitz (2019). The Discrete Charm of the Machine: Why the World Became Digital. Princeton University Press. p.108. ISBN978-0-691-18417-3. The Antkythera Mechanism [The first computer worthy of the name...] ^ Paphitis, Nicholas (30 November 2006). \"Experts: Fragments an Ancient Computer\". Washington Post. Archived from the original on 8 June 2017. Imagine tossing a top-notch laptop into the sea, leaving scientists from a foreign culture to scratch their heads over its corroded remains centuries later. A Roman shipmaster inadvertently did something just like it 2,000 years ago off southern Greece, experts said late Thursday. ^ a b c d e f g h i j k l m n o p q r s Freeth, Tony; Bitsakis, Yanis; Moussas, Xenophon; Seiradakis, John. H.; Tselikas, A.; Mangou, H.; Zafeiropoulou, M.; Hadland, R.; etal. (30 November 2006). \"Decoding the ancient Greek astronomical calculator known as the Antikythera Mechanism\" (PDF). Nature. 444 (7119): 58791. Bibcode:2006Natur.444..587F. doi:10.1038/nature05357. PMID17136087. S2CID4424998. Archived from the original (PDF) on 20 July 2015. Retrieved 20 May 2014. ^ a b c d e f g h i j k l m n o p q r s t u Freeth, Tony; Jones, Alexander (2012). \"The Cosmos in the Antikythera Mechanism\". Institute for the Study of the Ancient World. Retrieved 19 May 2014. ^ Pinotsis, A. D. (30 August 2007). \"The Antikythera mechanism: who was its creator and what was its use and purpose?\". Astronomical and Astrophysical Transactions. 26 (45): 21126. Bibcode:2007A&AT...26..211P. doi:10.1080/10556790601136925. S2CID56126896. ^ a b c d e f g h i j k l m n o p q r s t Freeth, Tony; Jones, Alexander; Steele, John M.; Bitsakis, Yanis (31 July 2008). \"Calendars with Olympiad display and eclipse prediction on the Antikythera Mechanism\" (PDF). Nature. 454 (7204): 61417. Bibcode:2008Natur.454..614F. doi:10.1038/nature07130. PMID18668103. S2CID4400693. Archived from the original (PDF) on 27 September 2013. Retrieved 20 May 2014. ^ Kaplan, Sarah (14 June 2016). \"The World's Oldest Computer Is Still Revealing Its Secrets\", The Washington Post. Retrieved 16 June 2016. ^ a b Iversen 2017, p.130 and note 4 ^ Alexander Jones, A Portable Cosmos, Oxford: Oxford University Press, pp. 1011. ^ a b Price, Derek de Solla (1974). \"Gears from the Greeks. The Antikythera Mechanism: A Calendar Computer from ca. 80 B. C.\". Transactions of the American Philosophical Society. New Series. 64 (7): 170. doi:10.2307/1006146. JSTOR1006146. ^ Palazzo, Chiara (17 May 2017). \"What is the Antikythera Mechanism? How was this ancient 'computer' discovered?\". The Telegraph. Retrieved 10 June 2017. ^ Freeth, T.; Bitsakis, Y.; Moussas, X.; Seiradakis, J.H.; Tselikas, A.; Mangou, E.; Zafeiropoulou, M.; Hadland, R.; Bate, D.; Ramsey, A.; Allen, M.; Crawley, A.; Hockley, P.; Malzbender, T.; Gelb, D.; Ambrisco, W.; Edmunds, M.G. \"Decoding The Antikythera Mechanism Investigation of An Ancient Astronomical Calculator\". Retrieved 27 June 2020. ^ Vetenskapens vrld: Bronsklumpen som kan frutsga framtiden. SVT. 17 October 2012. Archived 20 October 2012 at the Wayback Machine ^ a b c d e f g h i Freeth, Tony (2006). \"Decoding the Antikythera Mechanism: Supplementary Notes 2\" (PDF). Nature. 444 (7119): 58791. Bibcode:2006Natur.444..587F. doi:10.1038/nature05357. PMID17136087. S2CID4424998. Archived from the original (PDF) on 26 January 2013. Retrieved 20 May 2014. ^ Sample, Ian. \"Mysteries of computer from 65 BC are solved\". The Guardian. One of the remaining mysteries is why the Greek technology invented for the machine seemed to disappear...\"This device is extraordinary, the only thing of its kind,\" said Professor Edmunds. \"The astronomy is exactly right ... in terms of historic and scarcity value, I have to regard this mechanism as being more valuable than the Mona Lisa.\" ^ Price, Derek de Solla (1974). \"Gears from the Greeks. The Antikythera Mechanism: A Calendar Computer from ca. 80 BC\" Transactions of the American Philosophical Society, New Series. 64 (7): 19. ^ a b c d Carman, Christin C.; Evans, James (15 November 2014). \"On the epoch of the Antikythera mechanism and its eclipse predictor\". Archive for History of Exact Sciences. 68 (6): 693774. doi:10.1007/s00407-014-0145-5. S2CID120548493. ^ a b c Markoff, John (24 November 2014). \"On the Trail of an Ancient Mystery Solving the Riddles of an Early Astronomical Calculator\". The New York Times. Retrieved 25 November 2014. ^ Iversen 2017, pp.18283 ^ Jones 2017, pp.93, 15760, 23346 ^ a b c d Marchant, Jo (30 November 2006). \"In search of lost time\". Nature. 444 (7119): 53438. Bibcode:2006Natur.444..534M. doi:10.1038/444534a. PMID17136067. ^ Efstathiou, M.; Basiakoulis, A.; Efstathiou, K.; Anastasiou, M.; Boutbaras, P.; Seiradakis, J.H. (September 2013). \"The Reconstruction of the Antikythera Mechanism\" (PDF). International Journal of Heritage in the Digital Era. 2 (3): 30734. doi:10.1260/2047-4970.2.3.307. S2CID111280754. ^ Efstathiou, K.; Basiakoulis, A.; Efstathiou, M.; Anastasiou, M.; Seiradakis, J.H. (June 2012). \"Determination of the gears geometrical parameters necessary for the construction of an operational model of the Antikythera Mechanism\". Mechanism and Machine Theory. 52: 21931. doi:10.1016/j.mechmachtheory.2012.01.020. ^ \"The Antikythera Mechanism at the National Archaeological Museum\" Archived 21 February 2017 at the Wayback Machine. Retrieved 8 August 2015. ^ Dimitrios (Dimitris) Kontos ^ \"History Antikythera Mechanism Research Project\". www.antikythera-mechanism.gr. ^ \"Ancient 'computer' starts to yield secrets\". IOL: Technology. Independent Media. 7 June 2006. Archived from the original on 13 March 2007. Retrieved 16 July 2017. ^ Haughton, Brian (26 December 2006). Hidden History: Lost Civilizations, Secret Knowledge, and Ancient Mysteries. Career Press. pp.4344. ISBN978-1-56414-897-1. Retrieved 16 May 2011. ^ Bohstrom, Philippe (18 November 2018), Missing Piece of Antikythera Mechanism Found on Aegean Seabed, Haaretz, retrieved 26 June 2020. ^ Daley, Jason (15 November 2018), No, Archaeologists Probably Did Not Find a New Piece of the Antikythera Mechanism, Smithsonian Magazine, retrieved 15 November 2018. ^ Angelakis, Dimitris G. (2 May 2005). Quantum Information Processing: From Theory to Experiment. Proceedings of the NATO Advanced Study Institute on Quantum Computation and Quantum Information. Chania, Crete, Greece: IOS Press (published 2006). p.5. ISBN978-1-58603-611-9. Retrieved 28 May 2013. The Antikythera mechanism, as it is now known, was probably the world's first 'analog computer'a sophisticated device for calculating the motions of stars and planets. This remarkable assembly of more than 30 gears with a differential... ^ Allen, Martin (27 May 2007). \"Were there others? The Antikythera Mechanism Research Project\". Antikythera-mechanism.gr. Archived from the original on 21 July 2011. Retrieved 24 August 2011. ^ Iversen 2017 ^ Iversen 2017, pp.13441 ^ Freeth, Tony (December 2009). \"Decoding an Ancient Computer\" (PDF). Scientific American. 301 (6): 78. Bibcode:2009SciAm.301f..76F. doi:10.1038/scientificamerican1209-76. PMID20058643. Retrieved 26 November 2014. ^ Article \"Pergamum\", Columbia Electronic Encyclopedia, 6th Edition, 1. ^ Price, Derek de Solla (1974). \"Gears from the Greeks. The Antikythera Mechanism: A Calendar Computer from ca. 80 BC\". Transactions of the American Philosophical Society, New Series. 64 (7): 13; 5762. ^ Bitsakis, Yannis; Jones, Alexander (2013). \"The Inscriptions of the Antikythera Mechanism 3: The Front Dial and Parapegma Inscriptions\", Almagest 7 (2016), pp. 11719. See also Magdalini Anastasiou et al. \"The Astronomical Events of the Parapegma of the Antikythera Mechanism\". Journal for the History of Astronomy. 44: 17386. ^ Iversen 2017, pp.14147; Jones 2017, p.93 ^ Kampouris, Nick (18 October 2019). \"Important New Discoveries from Greece's Ancient Antikythera Shipwreck\". Greek Reporter. Retrieved 26 June 2020. ^ \"The new findings from the underwater archaeological research at the Antikythera Shipwreck\". Aikaterini Laskaridis Foundation. 18 October 2019. Retrieved 23 January 2020. ^ Marchant, Jo (2006). Decoding the Heavens. Da Capo Press. p.180. mechanical engineer and former curator of London's Science Museum Michael Wright tells of a piece breaking off in his inspection, which was glued back into place by the museum staff. ^ Wright, Michael T. (2007). \"The Antikythera Mechanism reconsidered\". Interdisciplinary Science Reviews. 32 (1): 2143. doi:10.1179/030801807X163670. S2CID54663891. ^ X. Moussas. Antikythera Mechanism, \"PINAX\", Greek Physical Society, Athens, 2011.2012 and X. Moussas Antikythera Mechanism the oldest computer, ed. Canto Mediterraneo, 2018, Athens, ^ a b Freeth, T. (2009). \"Decoding an Ancient Computer\". Scientific American. 301 (6): 7683. Bibcode:2009SciAm.301f..76F. doi:10.1038/scientificamerican1209-76. PMID20058643. ^ Budiselic et al., Antikythera Mechanism: Evidence of a Lunar Calendar, https://bhi.co.uk/wp-content/uploads/2020/12/BHI-Antikythera-Mechanism-Evidence-of-a-Lunar-Calendar.pdf ^ Parker, Richard Anthony, \"The Calendars of Ancient Egypt,\" (Chicago: University of Chicago Press, 1950). ^ Jones 2017, p.97. ^ \"The Cosmos on the front of the Antikythera Mechanism\". Archived from the original on 17 May 2018. Retrieved 21 May 2014.CS1 maint: bot: original URL status unknown (link) ^ Wright, Michael T. (March 2006). \"The Antikythera Mechanism and the early history of the moon phase display\" (PDF). Antiquarian Horology. 29 (3): 31929. Retrieved 16 June 2014. ^ Wilford, J. N. (31 July 2008). \"Discovering how Greeks computed in 100 B.C.\" The New York Times. ^ Connor, S. (31 July 2008). \"Ancient Device Was Used To Predict Olympic Games\". The Independent. London. Retrieved 27 March 2010. ^ Iversen 2017, pp.14868 ^ Freeth, T (2009). \"Decoding an Ancient Computer\". Scientific American. 301 (6): 7683. Bibcode:2009SciAm.301f..76F. doi:10.1038/scientificamerican1209-76. PMID20058643. ^ a b Iversen 2017, pp.14864 ^ Iversen 2017, pp.16585 ^ \"Olympic link to early 'computer'\". BBC News. Retrieved 15 December 2008. ^ Iversen 2017, pp.14147 ^ Moussas, Xenophon (2011). \"Antikythera Mechanism\". Greek Physical Society, Athens. ^ Moussas, Xenophon (2018). \"Antikythera Mechanism the oldest computer\". Canto Mediterraneo, Athens. ^ \"Does it favour a Heliocentric, or Geocentric Universe?\". Antikythera Mechanism Research Project. 27 July 2007. Archived from the original on 21 July 2011. Retrieved 24 August 2011. ^ a b c d Evans, James; Carman, Christin C.; Thorndyke, Alan (February 2010). \"Solar anomaly and planetary displays in the Antikythera Mechanism\" (PDF). Journal for the History of Astronomy. xli (1): 139. Bibcode:2010JHA....41....1E. doi:10.1177/002182861004100101. S2CID14000634. Retrieved 20 May 2014. ^ Wright, Michael T. (June 2005). \"The Antikythera Mechanism: a new gearing scheme\" (PDF). Bulletin of the Scientific Instrument Society. 85: 27. Retrieved 12 March 2017. ^ a b c d e f g h i Edmunds, Mike G.; Freeth, Tony (July 2011). \"Using Computation to Decode the First Known Computer\". Computer. 20117 (7): 3239. doi:10.1109/MC.2011.134. S2CID8574856. ^ Carman, Christin C.; Thorndyke, Alan; Evans, James (2012). \"On the Pin-and-Slot Device of the Antikythera Mechanism, with a New Application to the Superior Planets\" (PDF). Journal for the History of Astronomy. 43 (1): 93116. Bibcode:2012JHA....43...93C. doi:10.1177/002182861204300106. S2CID41930968. Retrieved 21 May 2014. ^ An extract from a 2nd or 3rd century AD papyrus (P.Wash.Univ.inv. 181+221) about an \"Astrologer's Board\", where the astrologer lays out particular stones to represent the Sun, Moon and planets ^ Freeth, Tony; Higgon, David; Dacanalis, Aris; MacDonald, Lindsay; Georgakopoulou, Myrto; Wojcik, Adam (12 March 2021). \"A Model of the Cosmos in the ancient Greek Antikythera Mechanism\". Scientific Reports. 11 (1): 5821. doi:10.1038/s41598-021-84310-w. PMC7955085. PMID33712674. ^ Freeth, Tony (2 March 2021). \"The Antikythera Cosmos (video: 25:56)\". Retrieved 12 March 2021. ^ Freeth, Tony; Higgon, David; Dacanalis, Aris; MacDonald, Lindsay; Georgakopoulou, Myrto; Wojcik, Adam (2 March 2021). \"A Model of the Cosmos in the ancient Greek Antikythera Mechanism\". Scientific Reports. 11 (1): 5821. doi:10.1038/s41598-021-84310-w. PMC7955085. PMID33712674. Retrieved 12 March 2021. ^ Geoffrey, Edmunds, Michael (1 August 2011). \"An Initial Assessment of the Accuracy of the Gear Trains in the Antikythera Mechanism\". Journal for the History of Astronomy. 42 (3): 30720. Bibcode:2011JHA....42..307E. doi:10.1177/002182861104200302. S2CID120883936. Retrieved 10 May 2016. ^ Marchant, Jo (2009). Decoding the Heavens. First Da Capo Press. p.40. ISBN978-0-306-81742-7. ^ Netz & Noel, Reviel & William (2007). The Archimedes Codex. Da Capo Press. p.1. ISBN978-0-306-81580-5. ^ Pickover, Clifford (2011). The Physics Book. Sterling. p.52. ISBN978-1-4027-7861-2. ^ \"M. TVLLI CICERONIS DE RE PVBLICA LIBER PRIMVS\" (in Latin). Archived from the original on 22 March 2007. Retrieved 23 March 2007. ^ Rorres, Chris. \"Archimedes: Spheres and Planetaria (Introduction)\". New York University. Archived from the original on 10 May 2011. Retrieved 27 March 2011. ^ Fildes, Jonathan (29 November 2006). \"Ancient Moon 'computer' revisited\". BBC News. Retrieved 25 April 2010. ^ Needham, Joseph (2000). Science and Civilisation in China. 4, Part 2. Cambridge. p.285. ISBN0-521-05803-1. ^ Sleeswyk, Andre (October 1981). \"Vitruvius' odometer\". Scientific American. 252 (4). pp.188200. See also: Andre Wegener Sleeswyk, \"Vitruvius' waywiser\", Archives internationales d'histoire des sciences, vol. 29, pp. 1122 (1979). ^ \"Cicero, De Natura Deorum II.88 (or 3334)\". Archived from the original on 16 March 2007. Retrieved 23 March 2007. ^ Charette, F (November 2006). \"Archaeology: high tech from Ancient Greece\". Nature. 444 (7119): 55152. Bibcode:2006Natur.444..551C. doi:10.1038/444551a. PMID17136077. S2CID33513516.. ^ a b Maddison, Francis (28 March 1985). \"Early mathematical wheelwork: Byzantine calendrical gearing\". Nature. 314 (6009): 31617. Bibcode:1985Natur.314..316M. doi:10.1038/314316b0. S2CID4229697.. ^ https://web.archive.org/web/20210826131722/http://afe.easia.columbia.edu/songdynasty-module/tech-experiment.html ^ Staff (17 May 2017). \"115 Anniversary of the Antikythera Mechanism Discovery\". Google. Retrieved 17 May 2017. ^ Smith, Reiss (17 May 2017). \"What is the Antikythera mechanism? Google Doodle marks discovery of ancient Greek computer\". BBC. Retrieved 17 May 2017. ^ \"The Antikythera Shipwreck: the Ship, the Treasures, the Mechanism\". Antikythera Mechanism Research Project. 6 June 2012. Retrieved 16 April 2013. ^ \"Naked Science Star Clock BC (TV Episode)\". 2011.[unreliable source?] ^ \"The World's First Computer\". Antikythera Mechanism Research Project. Retrieved 21 January 2013. ^ \"BBC Four The Two-Thousand-Year-Old Computer\". ^ \"Ancient Computer\". Nova. PBS. Retrieved 13 May 2014. ^ Pavlus, John. \"Small Mammal, Behind the Scenes: Lego Antikythera Mechanism\". Small Mammal. Retrieved 19 July 2018. ^ \"Exhibitions\". The Antikythera Mechanism Research Project. ^ Gracey, James (23 May 2011). \"Behind The Couch: 'Stonehenge Apocalypse'\". BehindTheCouch web site. Retrieved 23 May 2011. ^ \"Antikythera Element\". EVE Online Reference. Further reading[edit] Books[edit] Allen, M.; Ambrisco, W.; e.a. (2016). \"The Inscriptions of the Antikythera Mechanism\". Almagest. Almagest 7.1. Turnhout, Belgium: Brepols Publishers. ISSN1792-2593. James, Peter; Thorpe, Nick (1995). Ancient Inventions. Ballantine. ISBN978-0-345-40102-1. Jones, Alexander (2017). A Portable Cosmos: Revealing the Antikythera Mechanism, Scientific Wonder of the Ancient World. Oxford University Press. ISBN978-0199739349. Lin, Jian-Liang; Yan, Hong-Sen (2016). Decoding the Mechanisms of Antikythera Astronomical Device. Berlin [u.a.]: Springer. ISBN978-3662484456. Marchant, Jo (2008). Decoding the Heavens: Solving the Mystery of the World's First Computer. William Heinemann. ISBN978-0-434-01835-2. Price, Derek De Solla (1975). Gears from the Greeks: The Antikythera Mechanism; A Calendar Computer from ca. 80 B.C. Science History Publications. ISBN0-87169-647-9. Rosheim, Mark E. (1994). Robot Evolution: The Development of Anthrobotics. Wiley. ISBN978-0-471-02622-8. Russo, Lucio (2004). The Forgotten Revolution: How Science Was Born in 300 BC and Why It Had To Be Reborn. Springer. ISBN978-3-540-20396-4. Steele, J. M. (2000). Observations and Predictions of Eclipse Times by Early Astronomers. Kluwer. ISBN978-0-7923-6298-2. Stephenson, F. R. (1997). Historical Eclipses and the Earth's Rotation. Cambridge University Press. ISBN978-0-521-46194-8. Ptolemy (1998). Ptolemy's Almagest. Translated by Toomer, G. J. Princeton University Press. ISBN978-0-691-00260-6. Journals[edit] Bromley, A. G. (1990). \"The Antikythera Mechanism\". Horological Journal. 132: 41215. ISSN0018-5108. Bromley, A. G. (1990). \"The Antikythera Mechanism: A Reconstruction\". Horological Journal. 133 (1): 2831. Bromley, A. G. (1990). \"Observations of the Antikythera Mechanism\". Antiquarian Horology. 18 (6): 64152. OCLC900191459. Carman, C. C.; Di Cocco, M. (2016). \"The Moon Phase Anomaly in the Antikythera Mechanism\". ISAW Papers. 11. Archived from the original on 10 October 2019. Retrieved 6 June 2018. Charette, Franois (2006). \"High tech from Ancient Greece\". Nature. 444 (7119): 55152. Bibcode:2006Natur.444..551C. doi:10.1038/444551a. PMID17136077. S2CID33513516. Edmunds, M. G. (2014). \"The Antikythera Mechanism and the Mechanical Universe\". Contemporary Physics. 55 (4): 26385. Bibcode:2014ConPh..55..263E. doi:10.1080/00107514.2014.927280. S2CID122403901. Edmunds, Mike & Morgan, Philip (2000). \"The Antikythera Mechanism: Still a Mystery of Greek Astronomy\". Astronomy & Geophysics. 41 (6): 610. Bibcode:2000A&G....41f..10E. doi:10.1046/j.1468-4004.2000.41610.x. (The authors mention that an \"extended account\" of their researches titled \"Computing Aphrodite\" is forthcoming in 2001, but it does not seem to have appeared yet.) Freeth, T. (2002). \"The Antikythera Mechanism: 1. Challenging the Classic Research\" (PDF). Mediterranean Archeology and Archeaometry. 2 (1): 2135. Freeth, T. (2002). \"The Antikythera Mechanism: 2. Is it Posidonius' Orrery?\". Mediterranean Archeology and Archeaometry. 2 (2): 4558. Bibcode:2002MAA.....2...45F. Freeth, T.; Bitsakis, Y.; Moussas, X.; Seiradakis, J. H.; etal. (2006). \"Decoding the ancient Greek astronomical calculator known as the Antikythera Mechanism\". Nature. 444 (7119): 58791. Bibcode:2006Natur.444..587F. doi:10.1038/nature05357. PMID17136087. S2CID4424998. Freeth, T. (2009). \"Decoding an Ancient Computer\". Scientific American. 301 (6): 7683. Bibcode:2009SciAm.301f..76F. doi:10.1038/scientificamerican1209-76. PMID20058643. Freeth, T.; Jones, A. (2012). \"The Cosmos in the Antikythera Mechanism\". ISAW Papers. 4. Iversen, Paul A. (2017). \"The Calendar on the Antikythera Mechanism and the Corinthian Family of Calendars\". Hesperia. 86 (1): 129203. doi:10.2972/hesperia.86.1.0129. Jones, A. (1991). \"The adaptation of Babylonian methods in Greek numerical astronomy\". Isis. 82 (3): 44053. Bibcode:1991Isis...82..441J. doi:10.1086/355836. S2CID92988054. Koulouris, John A. (2008). \"The Heavens of Poseidon: The History and Discovery of the AntiKythera Mechanism\" (PDF). In Nomine Portal (in Greek). 1: 112. Price, D. de S. (1959). \"An Ancient Greek Computer\". Scientific American. 200 (6): 6067. Bibcode:1959SciAm.200f..60P. doi:10.1038/scientificamerican0659-60. Price, D. de S. (1974). \"Gears from the Greeks: The Antikythera Mechanism, a Calendar Computer from c. 80 B.C.\". Transactions of the American Philosophical Society. 64 (7): 170. doi:10.2307/1006146. JSTOR1006146. Spinellis, Diomidis (May 2008). \"The Antikythera Mechanism: A Computer Science Perspective\". Computer. 41 (5): 2227. CiteSeerX10.1.1.144.2297. doi:10.1109/MC.2008.166. S2CID25254859. Steele, J. M. (2000). \"Eclipse prediction in Mesopotamia\". Arch. Hist. Exact Sci. 54 (5): 42154. Bibcode:2000AHES...54..421S. doi:10.1007/s004070050007. JSTOR41134091. S2CID118299511. Weinberg, G. D.; Grace, V. R.; Edwards, G. R.; Robinson, H. S.; etal. (1965). \"The Antikythera Shipwreck Reconsidered\". Trans. Am. Philos. Soc. 55 (New Series) (3): 348. doi:10.2307/1005929. JSTOR1005929. Other[edit] Blain, Loz (16 November 2011). \"Hublot painstakingly recreates a mysterious, 2,100-year-old clockwork relic - but why?\". New Atlas. Retrieved 26 June 2020. Hublot. Marchant, Jo (12 December 2008). \"Archimedes and the 2000-year-old computer\". New Scientist (2686). Panos, Kristina (2015). \"The Antikythera Mechanism\". Hackaday. Retrieved 24 November 2015. Rice, Rob S. (47 September 1997). The Antikythera Mechanism: Physical and Intellectual Salvage from the 1st Century B.C. USNA Eleventh Naval History Symposium. Thessaloniki. pp.1925. This audio file was created from a revision of this article dated 30July2019, and does not reflect subsequent edits. External links[edit] A Model of the Cosmos in the ancient Greek Antikythera Mechanism, Scientific Reports volume 11, Article number: 5821 (2021) Asimakopoulos, Fivos. \"3D model simulation\". Manos Roumeliotis's Simulation and Animation of the Antikythera Mechanism page. The Antikythera Mechanism Research Project. The Antikythera Mechanism Research Project. \"Videos\". YouTube. Retrieved 24 July 2017. \"The Antikythera Mechanism Exhibitions\". National Hellenic Research Foundation. Archived from the original on 23 April 2012. YAAS Een 3D interactive virtual reality simulator in VRML Wright, M.; Vicentini, M. (25 August 2009). \"Virtual Reconstruction of the Antikythera Mechanism\". Heritage Key via YouTube. \"Antikythera\" (Adobe Flash). Nature. 30 July 2008. ClickSpring: Machining The Antikythera Mechanism playlist on YouTube Metapage with links at antikythera.org ",
        "_version_":1718441048052596736},
      {
        "story_id":19238277,
        "story_author":"tosh",
        "story_descendants":8,
        "story_score":71,
        "story_time":"2019-02-24T12:09:20Z",
        "story_title":"Asami: Datomic-Like Graph Database",
        "search":["Asami: Datomic-Like Graph Database",
          "https://github.com/threatgrid/asami/blob/master/README.md",
          "asami A graph database, for Clojure and ClojureScript. The latest version is : Asami is a schemaless database, meaning that data may be inserted with no predefined schema. This flexibility has advantages and disadvantages. It is easier to load and evolve data over time without a schema. However, functionality like upsert and basic integrity checking is not available in the same way as with a graph with a predefined schema. Asami also follows an Open World Assumption model, in the same way that RDF does. In practice, this has very little effect on the database, beyond what being schemaless provides. If you are new to graph databases, then please read our Introduction page. Asami has a query API that looks very similar to a simplified Datomic. More details are available in the Query documentation. Features There are several other graph databases available in the Clojure ecosystem, with each having their own focus. Asami is characterized by the following: Clojure and ClojureScript: Asami runs identically in both systems. Schema-less: Asami does not require a schema to insert data. Query planner: Queries are analyzed to find an efficient execution plan. This can be turned off. Analytics: Supports fast graph traversal operations, such as transitive closures, and can identify subgraphs. Integrated with Loom: Asami graphs are valid Loom graphs, via Asami-Loom. Open World Assumption: Related to being schema-less, Asami borrows semantics from RDF to lean towards an open world model. Pluggable Storage: Like Datomic, storage in Asami can be implemented in multiple ways. There are currently 2 in-memory graph systems, and durable storage available on the JVM. Usage Installing Using Asami requires Clojure or ClojureScript. Asami can be made available to clojure by adding the following to a deps.edn file: { :deps { org.clojars.quoll/asami {:mvn/version \"2.2.2\"} } } This makes Asami available to a repl that is launched with the clj or clojure commands. Alternatively, Asami can be added for the Leiningen build tool by adding this to the :dependencies section of the project.clj file: [org.clojars.quoll/asami \"2.2.2\"] Important Note for databases before 2.1.0 Asami 2.1.0 now uses fewer files to manage data. This makes it incompatible with previous versions. To port data from an older store to a new one, use the asami.core/export-data function on a database on the previous version of Asami, and asami.core/import-data to load the data into a new connection. Running The Asami API tries to look a little like Datomic. Once a repl has been configured for Asami, the following can be copy/pasted to test the API: (require '[asami.core :as d]) ;; Create an in-memory database, named dbname (def db-uri \"asami:mem://dbname\") (d/create-database db-uri) ;; Create a connection to the database (def conn (d/connect db-uri)) ;; Data can be loaded into a database either as objects, or \"add\" statements: (def first-movies [{:movie/title \"Explorers\" :movie/genre \"adventure/comedy/family\" :movie/release-year 1985} {:movie/title \"Demolition Man\" :movie/genre \"action/sci-fi/thriller\" :movie/release-year 1993} {:movie/title \"Johnny Mnemonic\" :movie/genre \"cyber-punk/action\" :movie/release-year 1995} {:movie/title \"Toy Story\" :movie/genre \"animation/adventure\" :movie/release-year 1995}]) @(d/transact conn {:tx-data first-movies}) The transact operation returns an object that can be dereferenced (via clojure.core/deref or the @ macro) to provide information about the state of the database before and after the transaction. (A future in Clojure, or a delay in ClojureScript). Note that the transaction data can be provided as the :tx-data in a map object if other parameters are to be provided, or just as a raw sequence without the wrapping map. For more information about loading data and executing transact see the Transactions documentation. With the data loaded, a database value can be retrieved from the database and then queried. NB: The transact operation will be executed asynchronously on the JVM. Retrieving a database immediately after executing a transact will not retrieve the latest database. If the updated database is needed, then perform the deref operation as shown above, since this will wait until the operation is complete. (def db (d/db conn)) (d/q '[:find ?movie-title :where [?m :movie/title ?movie-title]] db) This returns a sequence of results, with each result being a sequence of the selected vars in the :find clause (just ?movie-title in this case): ([\"Explorers\"] [\"Demolition Man\"] [\"Johnny Mnemonic\"] [\"Toy Story\"]) A more complex query could be to get the title, year and genre for all movies after 1990: (d/q '[:find ?title ?year ?genre :where [?m :movie/title ?title] [?m :movie/release-year ?year] [?m :movie/genre ?genre] [(> ?year 1990)]] db) Entities found in a query can be extracted back out as objects using the entity function. For instance, the following is a repl session that looks up the movies released in 1995, and then gets the associated entities: ;; find the entity IDs. This variation in the :find clause asks for a list of just the ?m variable => (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db) (:tg/node-10327 :tg/node-10326) ;; get a single entity => (d/entity db :tg/node-10327) #:movie{:title \"Toy Story\", :genre \"animation/adventure\", :release-year 1995} ;; get all the entities from the query => (map #(d/entity db %) (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db)) (#:movie{:title \"Toy Story\", :genre \"animation/adventure\", :release-year 1995} #:movie{:title \"Johnny Mnemonic\", :genre \"cyber-punk/action\", :release-year 1995}) See the Query Documentation for more information on querying. Refer to the Entity Structure documentation to understand how entities are stored and how to construct queries for them. Local Storage The above code uses an in-memory database, specified with a URL of the form asami:mem://dbname. Creating a database on disk is done the same way, but with the URL scheme changed to asami:local://dbname. This would create a database in the dbname directory. Local databases do not use keywords as entity IDs, as keywords use up memory, and a local database could be gigabytes in size. Instead, these are InternalNode objects. These can be created with asami.graph/new-node, or by using the readers in asami.graph. For instance, if the above code were all done with a local graph instead of a memory graph: => (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db) (#a/n \"3\" #a/n \"4\") ;; get a single entity => (require '[asami.graph :as graph]) => (d/entity db (graph/new-node 4)) #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} ;; nodes can also be read from a string, with the appropriate reader => (set! *data-readers* graph/node-reader) => (d/entity db #a/n \"4\") #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} Updates The Open World Assumption allows each attribute to be multi-arity. In a Closed World database an object may be loaded to replace those attributes that can only appear once. To do the same thing with Asami, annotate the attributes to be replaced with a quote character at the end of the attribute name. => (def toy-story (d/q '[:find ?ts . :where [?ts :movie/title \"Toy Story\"]] db)) => (d/transact conn [{:db/id toy-story :movie/genre' \"animation/adventure/comedy\"}]) => (d/entity (d/db conn) toy-story) #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} Addressing nodes by their internal ID can be cumbersome. They can also be addressed by a :db/ident field if one is provided. (def tx (d/transact conn [{:db/ident \"sense\" :movie/title \"Sense and Sensibility\" :movie/genre \"drama/romance\" :movie/release-year 1996}])) ;; ask the transaction for the node ID, instead of querying (def sense (get (:tempids @tx) \"sense\")) (d/entity (d/db conn) sense) This returns the new movie. The :db/ident attribute does not appear in the entity: #:movie{:title \"Sense and Sensibility\", :genre \"drama/romance\", :release-year 1996} However, all of the attributes are still present in the graph: => (d/q '[:find ?a ?v :in $ ?s :where [?s ?a ?v]] (d/db conn) sense) ([:db/ident \"sense\"] [:movie/title \"Sense and Sensibility\"] [:movie/genre \"drama/romance\"] [:movie/release-year 1996]) The release year of this movie is incorrectly set to the release in the USA, and not the initial release. That can be updated using the :db/ident field: => (d/transact conn [{:db/ident \"sense\" :movie/release-year' 1995}]) => (d/entity (d/db conn) sense) #:movie{:title \"Sense and Sensibility\", :genre \"drama/romance\", :release-year 1995} More details are provided in Entity Updates. Analytics Asami also has some support for graph analytics. These all operate on the graph part of a database value, which can be retrieved with the asami.core/graph function. NB: local graphs on disk are not yet supported. These will be available soon. Start by populating a graph with the cast of \"The Flintstones\". So that we can refer to entities after they have been created, we can provide them with temporary ID values. These are just negative numbers, and can be used elsewhere in the transaction to refer to the same entity. We will also avoid the :tx-data wrapper in the transaction: (require '[asami.core :as d]) (require '[asami.analytics :as aa]) (def db-uri \"asami:mem://data\") (d/create-database db-uri) (def conn (d/connect db-uri)) (def data [{:db/id -1 :name \"Fred\"} {:db/id -2 :name \"Wilma\"} {:db/id -3 :name \"Pebbles\"} {:db/id -4 :name \"Dino\" :species \"Dinosaur\"} {:db/id -5 :name \"Barney\"} {:db/id -6 :name \"Betty\"} {:db/id -7 :name \"Bamm-Bamm\"} [:db/add -1 :spouse -2] [:db/add -2 :spouse -1] [:db/add -1 :child -3] [:db/add -2 :child -3] [:db/add -1 :pet -4] [:db/add -5 :spouse -6] [:db/add -6 :spouse -5] [:db/add -5 :child -7] [:db/add -6 :child -7]]) (d/transact conn data) Fred, Wilma, Pebbles, and Dino are all connected in a subgraph. Barney, Betty and Bamm-Bamm are connected in a separate subgraph. Let's find the subgraph from Fred: (def db (d/db conn)) (def graph (d/graph db)) (def fred (d/q '[:find ?e . :where [?e :name \"Fred\"]] db)) (aa/subgraph-from-node graph fred) This returns the nodes in the graph, but not the scalar values. For instance: #{:tg/node-10330 :tg/node-10329 :tg/node-10331 :tg/node-10332} These nodes can be used as the input to a query to get their names: => (d/q '[:find [?name ...] :in $ [?n ...] :where [?n :name ?name]] db (aa/subgraph-from-node graph fred)) (\"Fred\" \"Pebbles\" \"Dino\" \"Wilma\") We can also get all the subgraphs: => (count (aa/subgraphs graph)) 2 ;; execute the same query for each subgraph => (map (partial d/q '[:find [?name ...] :where [?e :name ?name]]) (aa/subgraphs graph)) ((\"Fred\" \"Wilma\" \"Pebbles\" \"Dino\") (\"Barney\" \"Betty\" \"Bamm-Bamm\")) Transitive Queries Asami supports transitive properties in queries. A property (or attribute) is treated as transitive if it is followed by a + or a * character. (d/q '[:find ?friend-of-a-friend :where [?person :name \"Fred\"] [?person :friend+ ?foaf] [?foaf :name ?friend-of-a-friend]] db) This will find all friends, and friends of friends for Fred. Loom Asami also implements Loom via the Asami-Loom package. Include the following dependency for your project: [org.clojars.quoll/asami-loom \"0.2.0\"] Graphs can now be analyzed with Loom functions. If functions are provided to Loom, then they can be used to provide labels for creating a visual graph. The following creates some simple queries to get the labels for edges and nodes: (require '[asami-loom.index]) (require '[asami-loom.label]) (require '[loom.io]) (defn edge-label [g s d] (str (d/q '[:find ?e . :in $ ?a ?b :where (or [?a ?e ?b] [?b ?e ?a])] g s d))) (defn node-label [g n] (or (d/q '[:find ?name . :where [?n :name ?name]] g n) \"-\")) ;; create a PDF of the graph (loom-io/view (graph db) :fmt :pdg :alg :sfpd :edge-label edge-label :node-label node-label) Command Line Tool A command line tool is available to load data into an Asami graph and query it. This requires GraalVM CE 21.1.0 or later, and the native-image executable. Leiningen needs to see GraalVM on the classpath first, so if there are problems with building, check to see if this is the case. To build from sources: lein with-profile native uberjar lein with-profile native native This will create a binary called asami in the target directory. Execute with the -? flag for help: $ ./target/asami -? Usage: asami URL [-f filename] [-e query] [--help | -?] -? | --help: This help URL: the URL of the database to use. Must start with asami:mem://, asami:multi:// or asami:local:// -f filename: loads the filename into the database. A filename of \"-\" will use stdin. Data defaults to EDN. Filenames ending in .json are treated as JSON. -e query: executes a query. \"-\" (the default) will read from stdin instead of a command line argument. Multiple queries can be specified as edn (vector of query vectors) or ; separated. Available EDN readers: internal nodes - #a/n \"node-id\" regex - #a/r \"[Tt]his is a (regex|regular expression)\" Example: Loading a json file, and querying for keys (attributes) that are strings with spaces in them: asami asami:mem://tmp -f data.json -e ':find ?a :where [?e ?a ?v][(string? ?a)][(re-find #a/r \" \" ?a)]' The command will also work on local stores, which means that they can be loaded once and then queried multiple times. License Copyright 2016-2021 Cisco Systems Copyright 2015-2021 Paula Gearon Portions of src/asami/cache.cljc are Copyright Rich Hickey Distributed under the Eclipse Public License either version 1.0 or (at your option) any later version. "],
        "story_type":"Normal",
        "url_raw":"https://github.com/threatgrid/asami/blob/master/README.md",
        "comments.comment_id":[19239013,
          19239827],
        "comments.comment_author":["lmeyerov",
          "Scarbutt"],
        "comments.comment_descendants":[0,
          2],
        "comments.comment_time":["2019-02-24T15:53:16Z",
          "2019-02-24T18:30:57Z"],
        "comments.comment_text":["Ah neat -- We're always looking for new graph databases to connect Graphistry to!<p>Any guidance on wire protocol (BOLT, TinkerPop, some custom HTTP, ...?) + early users? Guessing ThreatGrid..",
          "For CRUD stuff I have found datomic to be hard to maintain if your domain demands lots of attributes (even if trying to be as generic as possible), you need discipline (good docs) and lots of application code for checks and to force constraints. In sql/rdbms world you have all the atrributes nicely organized(within tables) with good check/constraints available at hand (waits for easy vs simple comment).<p>With datomic you get fleixble schema but at a high cost IMO."],
        "id":"a3746ce4-d65d-4e15-8315-49a968572144",
        "url_text":"asami A graph database, for Clojure and ClojureScript. The latest version is : Asami is a schemaless database, meaning that data may be inserted with no predefined schema. This flexibility has advantages and disadvantages. It is easier to load and evolve data over time without a schema. However, functionality like upsert and basic integrity checking is not available in the same way as with a graph with a predefined schema. Asami also follows an Open World Assumption model, in the same way that RDF does. In practice, this has very little effect on the database, beyond what being schemaless provides. If you are new to graph databases, then please read our Introduction page. Asami has a query API that looks very similar to a simplified Datomic. More details are available in the Query documentation. Features There are several other graph databases available in the Clojure ecosystem, with each having their own focus. Asami is characterized by the following: Clojure and ClojureScript: Asami runs identically in both systems. Schema-less: Asami does not require a schema to insert data. Query planner: Queries are analyzed to find an efficient execution plan. This can be turned off. Analytics: Supports fast graph traversal operations, such as transitive closures, and can identify subgraphs. Integrated with Loom: Asami graphs are valid Loom graphs, via Asami-Loom. Open World Assumption: Related to being schema-less, Asami borrows semantics from RDF to lean towards an open world model. Pluggable Storage: Like Datomic, storage in Asami can be implemented in multiple ways. There are currently 2 in-memory graph systems, and durable storage available on the JVM. Usage Installing Using Asami requires Clojure or ClojureScript. Asami can be made available to clojure by adding the following to a deps.edn file: { :deps { org.clojars.quoll/asami {:mvn/version \"2.2.2\"} } } This makes Asami available to a repl that is launched with the clj or clojure commands. Alternatively, Asami can be added for the Leiningen build tool by adding this to the :dependencies section of the project.clj file: [org.clojars.quoll/asami \"2.2.2\"] Important Note for databases before 2.1.0 Asami 2.1.0 now uses fewer files to manage data. This makes it incompatible with previous versions. To port data from an older store to a new one, use the asami.core/export-data function on a database on the previous version of Asami, and asami.core/import-data to load the data into a new connection. Running The Asami API tries to look a little like Datomic. Once a repl has been configured for Asami, the following can be copy/pasted to test the API: (require '[asami.core :as d]) ;; Create an in-memory database, named dbname (def db-uri \"asami:mem://dbname\") (d/create-database db-uri) ;; Create a connection to the database (def conn (d/connect db-uri)) ;; Data can be loaded into a database either as objects, or \"add\" statements: (def first-movies [{:movie/title \"Explorers\" :movie/genre \"adventure/comedy/family\" :movie/release-year 1985} {:movie/title \"Demolition Man\" :movie/genre \"action/sci-fi/thriller\" :movie/release-year 1993} {:movie/title \"Johnny Mnemonic\" :movie/genre \"cyber-punk/action\" :movie/release-year 1995} {:movie/title \"Toy Story\" :movie/genre \"animation/adventure\" :movie/release-year 1995}]) @(d/transact conn {:tx-data first-movies}) The transact operation returns an object that can be dereferenced (via clojure.core/deref or the @ macro) to provide information about the state of the database before and after the transaction. (A future in Clojure, or a delay in ClojureScript). Note that the transaction data can be provided as the :tx-data in a map object if other parameters are to be provided, or just as a raw sequence without the wrapping map. For more information about loading data and executing transact see the Transactions documentation. With the data loaded, a database value can be retrieved from the database and then queried. NB: The transact operation will be executed asynchronously on the JVM. Retrieving a database immediately after executing a transact will not retrieve the latest database. If the updated database is needed, then perform the deref operation as shown above, since this will wait until the operation is complete. (def db (d/db conn)) (d/q '[:find ?movie-title :where [?m :movie/title ?movie-title]] db) This returns a sequence of results, with each result being a sequence of the selected vars in the :find clause (just ?movie-title in this case): ([\"Explorers\"] [\"Demolition Man\"] [\"Johnny Mnemonic\"] [\"Toy Story\"]) A more complex query could be to get the title, year and genre for all movies after 1990: (d/q '[:find ?title ?year ?genre :where [?m :movie/title ?title] [?m :movie/release-year ?year] [?m :movie/genre ?genre] [(> ?year 1990)]] db) Entities found in a query can be extracted back out as objects using the entity function. For instance, the following is a repl session that looks up the movies released in 1995, and then gets the associated entities: ;; find the entity IDs. This variation in the :find clause asks for a list of just the ?m variable => (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db) (:tg/node-10327 :tg/node-10326) ;; get a single entity => (d/entity db :tg/node-10327) #:movie{:title \"Toy Story\", :genre \"animation/adventure\", :release-year 1995} ;; get all the entities from the query => (map #(d/entity db %) (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db)) (#:movie{:title \"Toy Story\", :genre \"animation/adventure\", :release-year 1995} #:movie{:title \"Johnny Mnemonic\", :genre \"cyber-punk/action\", :release-year 1995}) See the Query Documentation for more information on querying. Refer to the Entity Structure documentation to understand how entities are stored and how to construct queries for them. Local Storage The above code uses an in-memory database, specified with a URL of the form asami:mem://dbname. Creating a database on disk is done the same way, but with the URL scheme changed to asami:local://dbname. This would create a database in the dbname directory. Local databases do not use keywords as entity IDs, as keywords use up memory, and a local database could be gigabytes in size. Instead, these are InternalNode objects. These can be created with asami.graph/new-node, or by using the readers in asami.graph. For instance, if the above code were all done with a local graph instead of a memory graph: => (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db) (#a/n \"3\" #a/n \"4\") ;; get a single entity => (require '[asami.graph :as graph]) => (d/entity db (graph/new-node 4)) #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} ;; nodes can also be read from a string, with the appropriate reader => (set! *data-readers* graph/node-reader) => (d/entity db #a/n \"4\") #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} Updates The Open World Assumption allows each attribute to be multi-arity. In a Closed World database an object may be loaded to replace those attributes that can only appear once. To do the same thing with Asami, annotate the attributes to be replaced with a quote character at the end of the attribute name. => (def toy-story (d/q '[:find ?ts . :where [?ts :movie/title \"Toy Story\"]] db)) => (d/transact conn [{:db/id toy-story :movie/genre' \"animation/adventure/comedy\"}]) => (d/entity (d/db conn) toy-story) #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} Addressing nodes by their internal ID can be cumbersome. They can also be addressed by a :db/ident field if one is provided. (def tx (d/transact conn [{:db/ident \"sense\" :movie/title \"Sense and Sensibility\" :movie/genre \"drama/romance\" :movie/release-year 1996}])) ;; ask the transaction for the node ID, instead of querying (def sense (get (:tempids @tx) \"sense\")) (d/entity (d/db conn) sense) This returns the new movie. The :db/ident attribute does not appear in the entity: #:movie{:title \"Sense and Sensibility\", :genre \"drama/romance\", :release-year 1996} However, all of the attributes are still present in the graph: => (d/q '[:find ?a ?v :in $ ?s :where [?s ?a ?v]] (d/db conn) sense) ([:db/ident \"sense\"] [:movie/title \"Sense and Sensibility\"] [:movie/genre \"drama/romance\"] [:movie/release-year 1996]) The release year of this movie is incorrectly set to the release in the USA, and not the initial release. That can be updated using the :db/ident field: => (d/transact conn [{:db/ident \"sense\" :movie/release-year' 1995}]) => (d/entity (d/db conn) sense) #:movie{:title \"Sense and Sensibility\", :genre \"drama/romance\", :release-year 1995} More details are provided in Entity Updates. Analytics Asami also has some support for graph analytics. These all operate on the graph part of a database value, which can be retrieved with the asami.core/graph function. NB: local graphs on disk are not yet supported. These will be available soon. Start by populating a graph with the cast of \"The Flintstones\". So that we can refer to entities after they have been created, we can provide them with temporary ID values. These are just negative numbers, and can be used elsewhere in the transaction to refer to the same entity. We will also avoid the :tx-data wrapper in the transaction: (require '[asami.core :as d]) (require '[asami.analytics :as aa]) (def db-uri \"asami:mem://data\") (d/create-database db-uri) (def conn (d/connect db-uri)) (def data [{:db/id -1 :name \"Fred\"} {:db/id -2 :name \"Wilma\"} {:db/id -3 :name \"Pebbles\"} {:db/id -4 :name \"Dino\" :species \"Dinosaur\"} {:db/id -5 :name \"Barney\"} {:db/id -6 :name \"Betty\"} {:db/id -7 :name \"Bamm-Bamm\"} [:db/add -1 :spouse -2] [:db/add -2 :spouse -1] [:db/add -1 :child -3] [:db/add -2 :child -3] [:db/add -1 :pet -4] [:db/add -5 :spouse -6] [:db/add -6 :spouse -5] [:db/add -5 :child -7] [:db/add -6 :child -7]]) (d/transact conn data) Fred, Wilma, Pebbles, and Dino are all connected in a subgraph. Barney, Betty and Bamm-Bamm are connected in a separate subgraph. Let's find the subgraph from Fred: (def db (d/db conn)) (def graph (d/graph db)) (def fred (d/q '[:find ?e . :where [?e :name \"Fred\"]] db)) (aa/subgraph-from-node graph fred) This returns the nodes in the graph, but not the scalar values. For instance: #{:tg/node-10330 :tg/node-10329 :tg/node-10331 :tg/node-10332} These nodes can be used as the input to a query to get their names: => (d/q '[:find [?name ...] :in $ [?n ...] :where [?n :name ?name]] db (aa/subgraph-from-node graph fred)) (\"Fred\" \"Pebbles\" \"Dino\" \"Wilma\") We can also get all the subgraphs: => (count (aa/subgraphs graph)) 2 ;; execute the same query for each subgraph => (map (partial d/q '[:find [?name ...] :where [?e :name ?name]]) (aa/subgraphs graph)) ((\"Fred\" \"Wilma\" \"Pebbles\" \"Dino\") (\"Barney\" \"Betty\" \"Bamm-Bamm\")) Transitive Queries Asami supports transitive properties in queries. A property (or attribute) is treated as transitive if it is followed by a + or a * character. (d/q '[:find ?friend-of-a-friend :where [?person :name \"Fred\"] [?person :friend+ ?foaf] [?foaf :name ?friend-of-a-friend]] db) This will find all friends, and friends of friends for Fred. Loom Asami also implements Loom via the Asami-Loom package. Include the following dependency for your project: [org.clojars.quoll/asami-loom \"0.2.0\"] Graphs can now be analyzed with Loom functions. If functions are provided to Loom, then they can be used to provide labels for creating a visual graph. The following creates some simple queries to get the labels for edges and nodes: (require '[asami-loom.index]) (require '[asami-loom.label]) (require '[loom.io]) (defn edge-label [g s d] (str (d/q '[:find ?e . :in $ ?a ?b :where (or [?a ?e ?b] [?b ?e ?a])] g s d))) (defn node-label [g n] (or (d/q '[:find ?name . :where [?n :name ?name]] g n) \"-\")) ;; create a PDF of the graph (loom-io/view (graph db) :fmt :pdg :alg :sfpd :edge-label edge-label :node-label node-label) Command Line Tool A command line tool is available to load data into an Asami graph and query it. This requires GraalVM CE 21.1.0 or later, and the native-image executable. Leiningen needs to see GraalVM on the classpath first, so if there are problems with building, check to see if this is the case. To build from sources: lein with-profile native uberjar lein with-profile native native This will create a binary called asami in the target directory. Execute with the -? flag for help: $ ./target/asami -? Usage: asami URL [-f filename] [-e query] [--help | -?] -? | --help: This help URL: the URL of the database to use. Must start with asami:mem://, asami:multi:// or asami:local:// -f filename: loads the filename into the database. A filename of \"-\" will use stdin. Data defaults to EDN. Filenames ending in .json are treated as JSON. -e query: executes a query. \"-\" (the default) will read from stdin instead of a command line argument. Multiple queries can be specified as edn (vector of query vectors) or ; separated. Available EDN readers: internal nodes - #a/n \"node-id\" regex - #a/r \"[Tt]his is a (regex|regular expression)\" Example: Loading a json file, and querying for keys (attributes) that are strings with spaces in them: asami asami:mem://tmp -f data.json -e ':find ?a :where [?e ?a ?v][(string? ?a)][(re-find #a/r \" \" ?a)]' The command will also work on local stores, which means that they can be loaded once and then queried multiple times. License Copyright 2016-2021 Cisco Systems Copyright 2015-2021 Paula Gearon Portions of src/asami/cache.cljc are Copyright Rich Hickey Distributed under the Eclipse Public License either version 1.0 or (at your option) any later version. ",
        "_version_":1718441013950808064},
      {
        "story_id":21325962,
        "story_author":"jrepinc",
        "story_descendants":104,
        "story_score":171,
        "story_time":"2019-10-22T16:58:39Z",
        "story_title":"Slimbook collaborates with the PowerPC laptop",
        "search":["Slimbook collaborates with the PowerPC laptop",
          "https://slimbook.es/en/noticias-notas-de-prensa-y-reviews/435-slimbook-collaborates-with-the-powerpc-laptop",
          "Details Created: 19 October 2019 We are pleased to announce our collaboration with Power Progress Community (PPC), the association behind the creation of a PowerPC processor laptop, in this case the NXP T2080. The PowerPC instruction set has been recently released by IBM as open source and given to the Linux Foundation. The PPC project aims to create and promote open source hardware and software. The project was born in Italy in 2016. In the software part, PPC adapts and compiles a well known version of GNU / Linuxl, Debian. You can find some applications in its repository. In the hardware part, its efforts are focused on collaborating withAcube Systems to manufacture the motherboard, which will be powered by the aforementioned processor. Slimbook joined the project at the beginning of 2019, providing the entire body of the laptop. The body of the laptop is actually the entire case, the cooling system, the screen, the keyboard, the backlight, the webcam, the speakers and the battery. After several months of work, sharing with them electronic schemes of all the components we contribute, its time to be prepared to manufacture the motherboard and that is why the PPC association has initiated the fundraising campaign. (Do not hesitate to make your contribution to the project) Externally, the design of the computer is our Eclipse model, because having its large cabin, adapts to the needs of the notebook, which among other features will have MXM removable graphics card. Below you can see a picture of it and then the electronic schematics. Lastly, if you want to see the processor in action, here is a video of how it behaves by browsing or when playing some games: "],
        "story_type":"Normal",
        "url_raw":"https://slimbook.es/en/noticias-notas-de-prensa-y-reviews/435-slimbook-collaborates-with-the-powerpc-laptop",
        "url_text":"Details Created: 19 October 2019 We are pleased to announce our collaboration with Power Progress Community (PPC), the association behind the creation of a PowerPC processor laptop, in this case the NXP T2080. The PowerPC instruction set has been recently released by IBM as open source and given to the Linux Foundation. The PPC project aims to create and promote open source hardware and software. The project was born in Italy in 2016. In the software part, PPC adapts and compiles a well known version of GNU / Linuxl, Debian. You can find some applications in its repository. In the hardware part, its efforts are focused on collaborating withAcube Systems to manufacture the motherboard, which will be powered by the aforementioned processor. Slimbook joined the project at the beginning of 2019, providing the entire body of the laptop. The body of the laptop is actually the entire case, the cooling system, the screen, the keyboard, the backlight, the webcam, the speakers and the battery. After several months of work, sharing with them electronic schemes of all the components we contribute, its time to be prepared to manufacture the motherboard and that is why the PPC association has initiated the fundraising campaign. (Do not hesitate to make your contribution to the project) Externally, the design of the computer is our Eclipse model, because having its large cabin, adapts to the needs of the notebook, which among other features will have MXM removable graphics card. Below you can see a picture of it and then the electronic schematics. Lastly, if you want to see the processor in action, here is a video of how it behaves by browsing or when playing some games: ",
        "comments.comment_id":[21327908,
          21328012],
        "comments.comment_author":["dragontamer",
          "filmgirlcw"],
        "comments.comment_descendants":[1,
          3],
        "comments.comment_time":["2019-10-22T19:57:22Z",
          "2019-10-22T20:07:10Z"],
        "comments.comment_text":["The link seems to be getting the YCombinator hug-of-death. I can't read the article at all.<p>Since POWER9 only exists in the server / HEDT sphere, I doubt this will be a modern POWER9 chip. Maybe something like a NXP-chip, which would still be interesting. Something on the scale of a Chromebook maybe?<p>If binaries were compatible with the Talos II POWER9 HEDT / Server, then there is still a good use of this laptop. To serve as a portable development platform, much like Intel Atom can serve as a portable development platform for Intel Xeons.<p>Depends on a lot of details however. Honestly, its probably more important to rack-up a POWER9 Talos II and just SSH into it every once in a while... modern cloud-based services makes this sort of thing much easier.<p>----------<p>EDIT: Site finally worked for me. NXP T2080 chip, 4-core / 8-thread 28nm class chip. e6500 core, which is some form of PowerPC for sure... 128-bit vector units (Altivec). 32kB L1 d$ and 32kB L1 I$, 2MB L2$ for the whole chip.<p>This is older-tech for sure, definitely \"Chromebook\" level of tech, maybe a touch weaker even.<p>EDIT2: I think there's something to be said about an \"open\" design, where the schematics are available for the community to use and extend. The NXP T2080 has good GPIO pins and connectivity, so there's a chance that this laptop will be easier to interface with electronically than other devices. Guys over at hackaday probably would love something like this. <a href=\"https://gitlab.com/oshw-powerpc-notebook/powerpc-laptop-mobo/blob/master/electrical_schematic_ppc_notebook_0_2_version_09_2019.pdf\" rel=\"nofollow\">https://gitlab.com/oshw-powerpc-notebook/powerpc-laptop-mobo...</a>",
          "People are getting down-voted for asking what year it is, but Im going to what year is this?<p>Im not discounting the effort that goes into place here  at least Im trying not to  but like, why would anyone look at PowerPC for portables in 2019 when the whole reason Apple shifted to Intel 14 years ago was thermals? Wouldnt ARM be better?<p>Im sure Im missing something but I just dont understand this on any level except for some obsession with openness at the expense of actually doing anything."],
        "id":"80b91bf3-5b1d-4967-8a07-2572b7749d01",
        "_version_":1718441068045795328},
      {
        "story_id":19711732,
        "story_author":"BerislavLopac",
        "story_descendants":16,
        "story_score":46,
        "story_time":"2019-04-21T12:14:07Z",
        "story_title":"Ark II",
        "search":["Ark II",
          "https://en.wikipedia.org/wiki/Ark_II",
          "For other uses, see Ark 2. Ark IITitle cardGenreScience fictionCreated byTed Post & Martin RothStarringTerry LesterJean Marie HonJose FloresMoochie the chimpanzee(owned and trained by Darrell Keener)ComposersYvette BlaisJeff MichaelCountry of originUnited StatesOriginal languageEnglishNo. of seasons1No. of episodes15 (list of episodes)ProductionProducersNorm PrescottLou ScheimerCamera setupSingle-cameraRunning time2224 minutes (without commercials)30 minutes (with commercials)Production companyFilmationDistributorNBCUniversal Television DistributionReleaseOriginal networkCBSAudio formatMonauralOriginal releaseSeptember 11December 18, 1976ChronologyRelated showsSpace Academy Ark II is an American live-action science fiction television series, aimed at children, that aired on CBS from September 11 to December 18, 1976, (with reruns continuing through November 13, 1977 and reruns returning from September 16, 1978, through August 25, 1979) as part of its weekend line-up. Only 15 half-hour episodes were ever produced. The program's central characters were created by Martin Roth; Ted Post helped Roth develop its core format. Series overview[edit] The opening titles for each episode, as narrated first by executive producer Lou Scheimer (using his then uncredited pseudonym Erik Gunden), then by the voice of Terry Lester, who portrayed Jonah, summarized the show's backstory: For millions of years, Earth was fertile and rich. Then pollution and waste began to take their toll. Civilization fell into ruin. This is the world of the 25th century. Only a handful of scientists remain, men who have vowed to rebuild what has been destroyed. This is their achievement: Ark II, a mobile storehouse of scientific knowledge, manned by a highly trained crew of young people. Their mission: to bring the hope of a new future to mankind. (Voice of Jonah): Ark II log, Entry Number 1. I, Jonah,...Ruth,...Samuel,...and Adam are fully aware of the dangers we face as we venture into unknown, maybe even hostile, areas. But were determined to bring the promise of a new civilization to our people and our planet. Ark II had a racially mixed cast starring Terry Lester as Ark II's commander, Jonah, Jean Marie Hon as Ruth, Jos Flores as Samuel, and a chimpanzee named Moochie (owned and trained by Darrell Keener) responding to the name of Adam (voiced by Lou Scheimer).[1] The show's premise was inspired by the story of Noah's Ark, and the characters were given names taken from the Hebrew Bible. It was set in a post-apocalyptic 25th Century (specifically, 2476, the show having debuted in 1976), after Earth's civilizations had been decimated by the effects of waste, pollution, and warfare, falling back to a civilization comparable to the Dark Ages. The surviving scientists pooled their knowledge and resources, training three young people (and the chimp, who was capable of speech and abstract reasoning) to search for remnants of humanity, reintroducing lost ideas as they traveled the barren landscape in the high-tech Ark II.[2] The show mentions a \"headquarters\" and that the crew are scientists. The titles \"Commander\" and \"Captain\" are both used to refer to Jonah. All the installments began and ended with numbered entries in the Ark II's log, which Lester, in character as Jonah, narrated in voiceover. Like other Filmation shows, Ark II also had a moral lesson derived from the preceding events seen in the episode. However, the epilogue log entry narrated by Jonah is used to deliver the moral rather than have any of the characters break the fourth wall (i.e. speak directly to the camera and TV viewers) to deliver the moral which was common in most Filmation shows. Production[edit] In \"The Launch of Ark II,\" the documentary filmed for the release of the DVD set, Lou Scheimer and others mention that the program was filmed during the summer of 1976 predominantly on location at Paramount Ranch near Malibu, California. Technology[edit] The series is best-remembered for its eponymous vehicle: a futuristic-looking six-wheeled combination RV and mobile laboratory. The 44ft long vehicle was a fiberglass body on a 1971 Ford C-Series (C-700) cabover, by the Brubaker Group. The front end of the Ark II prop was later re-used as the nose portion of the Seeker spacecraft in the Filmation series Space Academy and Jason of Star Command.[citation needed] It is sometimes incorrectly reported that the Ark II was built by Dean Jeffries, who constructed various fantastic vehicles for science-fiction films and television. These include the Landmaster for the film Damnation Alley, with which the Ark II is sometimes confused. In addition, the series also featured a jetpack called the Jet Jumper, and the Ark Roamer, a smaller, 4-wheeled all-terrain vehicle built by Brubaker from a modified Brubaker Box, a kit car using a 1968 Volkswagen Beetle sedan chassis. The Roamer was carried in the rear of the Ark II.[3] Guest stars[edit] Guest stars included Jonathan Harris, Malachi Throne, Jim Backus, Geoffrey Lewis, Philip Abbott, Robert Ridgely, Helen Hunt, and Robby the Robot as the title character built by Samuel in the episode \"The Robot.\" Helen Hunt appears in the episode \"Omega.\" Actor Daniel Selby auditioned for the role of Samuel, but Jose Flores won the role. Episodes[edit] Home media[edit] BCI Eclipse LLC (under its Ink & Paint classic animation entertainment brand), under a license it had obtained from Entertainment Rights, released Ark II: The Complete Series on DVD in Region 1 on November 7, 2006.[4] The BCI Eclipse 4-disc set included many special features, and the episodes were presented uncut, digitally remastered and presented in their original production order. Unlike many of the in-house Filmation DVD releases in Region 1 by BCI Entertainments Ink & Paint brand, this series was sourced from the original NTSC film elements, with correct speed and pitch. Savor Ediciones, S.A. released Ark II: La Serie Completa as a 4-disc Region 2 DVD box set on May 20, 2009.[5] Unlike the BCI set, this release only contains the episodes, no bonus features. Being a Region 2 release for Spain, the soundtrack is the dubbed Spanish version. Unfortunately, the original English soundtrack was not included, even as a secondary option. The discs are encoded in the PAL video format. References[edit] ^ Woolery, George W. (1985). Children's Television: The First Thirty-Five Years, 1946-1981, Part II: Live, Film, and Tape Series. The Scarecrow Press. pp.4950. ISBN0-8108-1651-2. ^ \"Ark II\". TV.com. CBS Interactive. Retrieved 20 January 2016. ^ Pickup Van & 4WD magazine, January 1977, Vol. 5, NO. 4 ^ \"Ark II DVD news: BCI issues Ark II press release\". tvshowsondvd.com. Archived from the original on 9 August 2015. Retrieved 20 January 2016. ^ https://www.amazon.es/Ark-II-Serie-completa-DVD/dp/B0053CAJ10/ref=sr_1_1?__mk_es_ES=%C3%85M%C3%85%C5%BD%C3%95%C3%91&dchild=1&keywords=%22Ark+II%22+la+serie+completa&qid=1591709919&s=dvd&sr=1-1 External links[edit] Ark II at IMDb Ark II at Retro Junk (has the intro video) Unofficial Ark II Appreciation Site Ark II at 70's Live Kid DVD Memorable TV: Ark II episode guide Ark II at OldFutures.com "],
        "story_type":"Normal",
        "url_raw":"https://en.wikipedia.org/wiki/Ark_II",
        "comments.comment_id":[19712210,
          19712458],
        "comments.comment_author":["BerislavLopac",
          "stcredzero"],
        "comments.comment_descendants":[0,
          1],
        "comments.comment_time":["2019-04-21T14:32:37Z",
          "2019-04-21T15:22:27Z"],
        "comments.comment_text":["Apparently, it is available on YouTube: <a href=\"https://www.youtube.com/watch?v=_lj8z28Mshw\" rel=\"nofollow\">https://www.youtube.com/watch?v=_lj8z28Mshw</a>",
          "I used to watch that, along with Shazam/Captain Marvel, and The Mighty Isis. Also the Kroft Super Show. The Kroft puppet shows were pretty subversive. HR Puff'n Stuff basically had Puff the Magic Dragon as a main character, basically a walking marijuana reference. Electra Woman and Dyna Girl were a superhero duo with their own storyline on the Kroft Super Show, IIRC. Basically Batman and Robin in a flying car, where the men were support characters and the action duo was female.<p>Where I grew up, binge watching all of the Saturday Morning cartoons and shows in a row (not all episodes in a show, but all shows with an episode that day) was viewed as something kinda bad. People talked about it as if it was going to be a part of the destruction of the culture. I always felt guilty, and mom would kick us out of the house after lunch. Then we'd spend the whole day outside unless it was raining.<p>Now, watching a whole bunch of television in a multi-hour block is just kind of a normal thing, so long as you don't overdo it.<p>The parents of the day might not have been wrong about the destruction of the culture part, though."],
        "id":"2b919a60-8d72-4e8c-8005-11848a47ff80",
        "url_text":"For other uses, see Ark 2. Ark IITitle cardGenreScience fictionCreated byTed Post & Martin RothStarringTerry LesterJean Marie HonJose FloresMoochie the chimpanzee(owned and trained by Darrell Keener)ComposersYvette BlaisJeff MichaelCountry of originUnited StatesOriginal languageEnglishNo. of seasons1No. of episodes15 (list of episodes)ProductionProducersNorm PrescottLou ScheimerCamera setupSingle-cameraRunning time2224 minutes (without commercials)30 minutes (with commercials)Production companyFilmationDistributorNBCUniversal Television DistributionReleaseOriginal networkCBSAudio formatMonauralOriginal releaseSeptember 11December 18, 1976ChronologyRelated showsSpace Academy Ark II is an American live-action science fiction television series, aimed at children, that aired on CBS from September 11 to December 18, 1976, (with reruns continuing through November 13, 1977 and reruns returning from September 16, 1978, through August 25, 1979) as part of its weekend line-up. Only 15 half-hour episodes were ever produced. The program's central characters were created by Martin Roth; Ted Post helped Roth develop its core format. Series overview[edit] The opening titles for each episode, as narrated first by executive producer Lou Scheimer (using his then uncredited pseudonym Erik Gunden), then by the voice of Terry Lester, who portrayed Jonah, summarized the show's backstory: For millions of years, Earth was fertile and rich. Then pollution and waste began to take their toll. Civilization fell into ruin. This is the world of the 25th century. Only a handful of scientists remain, men who have vowed to rebuild what has been destroyed. This is their achievement: Ark II, a mobile storehouse of scientific knowledge, manned by a highly trained crew of young people. Their mission: to bring the hope of a new future to mankind. (Voice of Jonah): Ark II log, Entry Number 1. I, Jonah,...Ruth,...Samuel,...and Adam are fully aware of the dangers we face as we venture into unknown, maybe even hostile, areas. But were determined to bring the promise of a new civilization to our people and our planet. Ark II had a racially mixed cast starring Terry Lester as Ark II's commander, Jonah, Jean Marie Hon as Ruth, Jos Flores as Samuel, and a chimpanzee named Moochie (owned and trained by Darrell Keener) responding to the name of Adam (voiced by Lou Scheimer).[1] The show's premise was inspired by the story of Noah's Ark, and the characters were given names taken from the Hebrew Bible. It was set in a post-apocalyptic 25th Century (specifically, 2476, the show having debuted in 1976), after Earth's civilizations had been decimated by the effects of waste, pollution, and warfare, falling back to a civilization comparable to the Dark Ages. The surviving scientists pooled their knowledge and resources, training three young people (and the chimp, who was capable of speech and abstract reasoning) to search for remnants of humanity, reintroducing lost ideas as they traveled the barren landscape in the high-tech Ark II.[2] The show mentions a \"headquarters\" and that the crew are scientists. The titles \"Commander\" and \"Captain\" are both used to refer to Jonah. All the installments began and ended with numbered entries in the Ark II's log, which Lester, in character as Jonah, narrated in voiceover. Like other Filmation shows, Ark II also had a moral lesson derived from the preceding events seen in the episode. However, the epilogue log entry narrated by Jonah is used to deliver the moral rather than have any of the characters break the fourth wall (i.e. speak directly to the camera and TV viewers) to deliver the moral which was common in most Filmation shows. Production[edit] In \"The Launch of Ark II,\" the documentary filmed for the release of the DVD set, Lou Scheimer and others mention that the program was filmed during the summer of 1976 predominantly on location at Paramount Ranch near Malibu, California. Technology[edit] The series is best-remembered for its eponymous vehicle: a futuristic-looking six-wheeled combination RV and mobile laboratory. The 44ft long vehicle was a fiberglass body on a 1971 Ford C-Series (C-700) cabover, by the Brubaker Group. The front end of the Ark II prop was later re-used as the nose portion of the Seeker spacecraft in the Filmation series Space Academy and Jason of Star Command.[citation needed] It is sometimes incorrectly reported that the Ark II was built by Dean Jeffries, who constructed various fantastic vehicles for science-fiction films and television. These include the Landmaster for the film Damnation Alley, with which the Ark II is sometimes confused. In addition, the series also featured a jetpack called the Jet Jumper, and the Ark Roamer, a smaller, 4-wheeled all-terrain vehicle built by Brubaker from a modified Brubaker Box, a kit car using a 1968 Volkswagen Beetle sedan chassis. The Roamer was carried in the rear of the Ark II.[3] Guest stars[edit] Guest stars included Jonathan Harris, Malachi Throne, Jim Backus, Geoffrey Lewis, Philip Abbott, Robert Ridgely, Helen Hunt, and Robby the Robot as the title character built by Samuel in the episode \"The Robot.\" Helen Hunt appears in the episode \"Omega.\" Actor Daniel Selby auditioned for the role of Samuel, but Jose Flores won the role. Episodes[edit] Home media[edit] BCI Eclipse LLC (under its Ink & Paint classic animation entertainment brand), under a license it had obtained from Entertainment Rights, released Ark II: The Complete Series on DVD in Region 1 on November 7, 2006.[4] The BCI Eclipse 4-disc set included many special features, and the episodes were presented uncut, digitally remastered and presented in their original production order. Unlike many of the in-house Filmation DVD releases in Region 1 by BCI Entertainments Ink & Paint brand, this series was sourced from the original NTSC film elements, with correct speed and pitch. Savor Ediciones, S.A. released Ark II: La Serie Completa as a 4-disc Region 2 DVD box set on May 20, 2009.[5] Unlike the BCI set, this release only contains the episodes, no bonus features. Being a Region 2 release for Spain, the soundtrack is the dubbed Spanish version. Unfortunately, the original English soundtrack was not included, even as a secondary option. The discs are encoded in the PAL video format. References[edit] ^ Woolery, George W. (1985). Children's Television: The First Thirty-Five Years, 1946-1981, Part II: Live, Film, and Tape Series. The Scarecrow Press. pp.4950. ISBN0-8108-1651-2. ^ \"Ark II\". TV.com. CBS Interactive. Retrieved 20 January 2016. ^ Pickup Van & 4WD magazine, January 1977, Vol. 5, NO. 4 ^ \"Ark II DVD news: BCI issues Ark II press release\". tvshowsondvd.com. Archived from the original on 9 August 2015. Retrieved 20 January 2016. ^ https://www.amazon.es/Ark-II-Serie-completa-DVD/dp/B0053CAJ10/ref=sr_1_1?__mk_es_ES=%C3%85M%C3%85%C5%BD%C3%95%C3%91&dchild=1&keywords=%22Ark+II%22+la+serie+completa&qid=1591709919&s=dvd&sr=1-1 External links[edit] Ark II at IMDb Ark II at Retro Junk (has the intro video) Unofficial Ark II Appreciation Site Ark II at 70's Live Kid DVD Memorable TV: Ark II episode guide Ark II at OldFutures.com ",
        "_version_":1718441028096098304},
      {
        "story_id":20432968,
        "story_author":"edent",
        "story_descendants":19,
        "story_score":78,
        "story_time":"2019-07-14T11:59:17Z",
        "story_title":"Sending 1.2M Tweets",
        "search":["Sending 1.2M Tweets",
          "https://shkspr.mobi/blog/2019/07/sending-1-2-million-tweets/",
          "Back in 2014, I set up a rather silly Twitter account - @OxfordSolarLive.The premise was simple. A camera took a photo of the sky above my house. It took a reading from my solar panels to see how much electricity they were generating. It superimposed the reading on the photo. Then posted it on Twitter.1206 watts. pic.twitter.com/ip2Cp5rRao Solar Realtime Edent (@OxfordSolarLive) August 24, 2018When I had a solar battery fitted, it also added that into the mix.2586 watts.68% Battery = 1.36kWh pic.twitter.com/GpQh5tURuY Solar Realtime Edent (@OxfordSolarLive) July 2, 2017It tweeted once per minute from local sunrise to sunset. With the occasional spot of downtime due to my server rebooting, or local weather conditions.Solar panels? More like SNOWLAR PANELS! pic.twitter.com/BIy8nQcfDn Terence Eden (@edent) February 1, 2019Looks like that was the minimum for the #eclipse \"@OxfordSolarLive: 144 watts. pic.twitter.com/5KTbPE9Bad\" Terence Eden (@edent) March 20, 2015Why? Part of it was convenience - rather than log in to my systems, it was easier to check it on Twitter. Part of it was public engagement - I could quickly share how well my solar installation was doing. It also got some media attention.Being interviewed by a nice man from the BBC about @Edent_Solar, @OxfordSolarLive and @edent_car!Thanks @JonDgls! pic.twitter.com/uvPiXQRM2r Terence Eden (@edent) October 18, 2016Talking Solar Batteries With BBCClick https://t.co/LAUEDf8UJU pic.twitter.com/gEkHFn9HhF Terence Eden (@edent) November 9, 2017But, mostly, because I didn't want to store all those images myself! I figured that Twitter would quickly ban the account for breaking the rate limits. But that never happened.The rate limit is posting 300 times per 3 hours. So once per minute during daylight hours is fine.But it is time to shut it down. We're moving house and can't take our panels with us.If you would like to do anything with the images or data, I'm happy to licence them as CC BY-SA.As an aside - I'd love to know if any other account has sent more tweets, or uploaded more photos than OxfordSolarLive has. "],
        "story_type":"Normal",
        "url_raw":"https://shkspr.mobi/blog/2019/07/sending-1-2-million-tweets/",
        "url_text":"Back in 2014, I set up a rather silly Twitter account - @OxfordSolarLive.The premise was simple. A camera took a photo of the sky above my house. It took a reading from my solar panels to see how much electricity they were generating. It superimposed the reading on the photo. Then posted it on Twitter.1206 watts. pic.twitter.com/ip2Cp5rRao Solar Realtime Edent (@OxfordSolarLive) August 24, 2018When I had a solar battery fitted, it also added that into the mix.2586 watts.68% Battery = 1.36kWh pic.twitter.com/GpQh5tURuY Solar Realtime Edent (@OxfordSolarLive) July 2, 2017It tweeted once per minute from local sunrise to sunset. With the occasional spot of downtime due to my server rebooting, or local weather conditions.Solar panels? More like SNOWLAR PANELS! pic.twitter.com/BIy8nQcfDn Terence Eden (@edent) February 1, 2019Looks like that was the minimum for the #eclipse \"@OxfordSolarLive: 144 watts. pic.twitter.com/5KTbPE9Bad\" Terence Eden (@edent) March 20, 2015Why? Part of it was convenience - rather than log in to my systems, it was easier to check it on Twitter. Part of it was public engagement - I could quickly share how well my solar installation was doing. It also got some media attention.Being interviewed by a nice man from the BBC about @Edent_Solar, @OxfordSolarLive and @edent_car!Thanks @JonDgls! pic.twitter.com/uvPiXQRM2r Terence Eden (@edent) October 18, 2016Talking Solar Batteries With BBCClick https://t.co/LAUEDf8UJU pic.twitter.com/gEkHFn9HhF Terence Eden (@edent) November 9, 2017But, mostly, because I didn't want to store all those images myself! I figured that Twitter would quickly ban the account for breaking the rate limits. But that never happened.The rate limit is posting 300 times per 3 hours. So once per minute during daylight hours is fine.But it is time to shut it down. We're moving house and can't take our panels with us.If you would like to do anything with the images or data, I'm happy to licence them as CC BY-SA.As an aside - I'd love to know if any other account has sent more tweets, or uploaded more photos than OxfordSolarLive has. ",
        "comments.comment_id":[20437827,
          20438376],
        "comments.comment_author":["SimeVidas",
          "azhenley"],
        "comments.comment_descendants":[3,
          2],
        "comments.comment_time":["2019-07-15T03:05:57Z",
          "2019-07-15T05:29:47Z"],
        "comments.comment_text":["I remember the days when your ISP would give you 5 MB of hosting space.<p>If each of the 1.2 million tweets includes a ~150 KB image, thats 180 GB of images hosted on Twitter for free.",
          "Does this mean that I can make a Twitter account to backup all my photos and then use the \"Download your data\" feature [1] to download all of them?<p>[1] <a href=\"https://help.twitter.com/en/managing-your-account/how-to-download-your-twitter-archive\" rel=\"nofollow\">https://help.twitter.com/en/managing-your-account/how-to-dow...</a>"],
        "id":"97008df5-7604-4cb8-b633-29dd0388b5fc",
        "_version_":1718441045028503552}]
  }}
