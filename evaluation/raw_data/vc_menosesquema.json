{
  "responseHeader":{
    "status":0,
    "QTime":101},
  "response":{"numFound":130,"start":0,"numFoundExact":true,"docs":[
      {
        "story_id":[20745393],
        "story_author":["ingve"],
        "story_descendants":[355],
        "story_score":[357],
        "story_time":["2019-08-20T10:39:18Z"],
        "story_title":"Sunsetting Mercurial Support in Bitbucket",
        "search":["Sunsetting Mercurial Support in Bitbucket",
          "https://bitbucket.org/blog/sunsetting-mercurial-support-in-bitbucket",
          "[Update Aug 26, 2020] All hg repos have now been disabled and cannot be accessed.[Update July 1, 2020] Today, mercurial repositories, snippets, and wikis will turn to read-only mode. After July 8th, 2020 they will no longer be accessible. The version control software market has evolved a lot since Bitbucket began in 2008. When we launched, centralized version control was the norm and we only supported Mercurial repos. But Git adoption has grown over the years to become the default system, helping teams of all sizes work faster as they become more distributed.As we surpass 10 million registered users on the platform, we're at a point in our growth where we are conducting a deeper evaluation of the market and how we can best support our users going forward. After much consideration, we've decided to remove Mercurial support from Bitbucket Cloud and its API. Mercurial features and repositories will be officially deprecated on July 1, 2020.Read on to learn more about this decision, the important timelines, and get migration resources and support. The timeline and how this may affect your team Here are the key dates as we sunset Mercurial functionality: February 1, 2020: users will no longer be able to create new Mercurial repositories [Extended] July 1, 2020: users will not be able to use Mercurial features. All hg repos, wikis, and snippets will be in read-only mode. Heres why were focusing on Git This wasnt an easy decision, and Mercurial will always have a special place in Bitbuckets history. DevOps adoption has skyrocketed over the last decade and our customers are adopting this new way of working at an exponential rate. In this time, Bitbucket has steadily grown from being just a version control management tool to being a place to manage the entire software development lifecycle. And there's always more work to be done. This year we will concentrate on building deeper integrations to enhance automation and collaboration. Our improvements will make it even easier and safer to plan, code, test, and deploy all from within Bitbucket. Building quality features requires intense focus, and supporting two version control systems means splitting focus doubling shipping time and technical overhead. With Git being the more popularly used tool, Mercurial runs the risk of overlooked issues as we scale. According to a Stack Overflow Developer Survey, almost 90% of developers use Git, while Mercurial is the least popular version control system with only about 3% developer adoption. In fact, Mercurial usage on Bitbucket is steadily declining, and the percentage of new Bitbucket users choosing Mercurial has fallen to less than 1%. This deprecation will enable us to focus on building the best possible experience for our users. How to migrate and export We recommend that teams migrate their existing Mercurial repos to Git. There are various Git conversion tools in the market, including hg-fast-export and hg-git mercurial plugin. We are happy to support your migration, and you can find a discussion about available options in our dedicated Community thread. If you prefer to continue using the Mercurial system, there are a number of free and paid Mercurial hosting services. We realize that there is no one-size-fits-all solution. That's why we've created the following resources to best equip you with the knowledge and tools for a seamless transition: A Community thread to discuss conversion tools, migration, tips, and offer troubleshooting help A Git tutorial that covers anywhere from the basics of creating pull requests to rebasing and Git hooks We want to thank all the loyal users who have grown with us over the years. We look forward to this new focus on our roadmap and to introducing exciting new features. ",
          "It's funny to see how the whole world concentrates on this Git thing, while there is a treasure trove called Mercurial.<p>Mercurial was made for humans. It is seriously convenient and productive. Something I cannot say about Git, which more reminds me of an adhoc job.<p>I use both Git and Mercurial on daily basis. But my preference goes to Mercurial: it is just more sane in a big way. It is clearly a piece of art and love.",
          "It's very sad to see bitbucket dropping mercurial support. Now only Facebook and volunteers are keeping mercurial alive. \nSometimes technically better architecture and user interface lose to a non user friendly hard solutions due to inertia of mass adoption.<p>So a lesson in Software development is similar to betamax and VHS, so marketing is still a winner over technically superior architecture and ease of use. GitHub successfully marketed git, so git and GitHub are synonymous for most developers. Now majority of open source projects are reliant on a single proprietary solution Github by Microsoft, for managing code and project. Can understand the difficulty of bitbucket, when Python language itself moved out of mercurial due to the same inertia.<p>Hopefully gitlab can come out with mercurial support to migrate projects using it from bitbucket.<p>For people who believe in self hosted solution can install Kallithea (<a href=\"https://kallithea-scm.org\" rel=\"nofollow\">https://kallithea-scm.org</a>) or Rhodecode open source edition. Kallithea is used by Unity engine to manage their source code internally with mercurial."],
        "story_type":["Normal"],
        "url":"https://bitbucket.org/blog/sunsetting-mercurial-support-in-bitbucket",
        "url_text":"[Update Aug 26, 2020] All hg repos have now been disabled and cannot be accessed.[Update July 1, 2020] Today, mercurial repositories, snippets, and wikis will turn to read-only mode. After July 8th, 2020 they will no longer be accessible. The version control software market has evolved a lot since Bitbucket began in 2008. When we launched, centralized version control was the norm and we only supported Mercurial repos. But Git adoption has grown over the years to become the default system, helping teams of all sizes work faster as they become more distributed.As we surpass 10 million registered users on the platform, we're at a point in our growth where we are conducting a deeper evaluation of the market and how we can best support our users going forward. After much consideration, we've decided to remove Mercurial support from Bitbucket Cloud and its API. Mercurial features and repositories will be officially deprecated on July 1, 2020.Read on to learn more about this decision, the important timelines, and get migration resources and support. The timeline and how this may affect your team Here are the key dates as we sunset Mercurial functionality: February 1, 2020: users will no longer be able to create new Mercurial repositories [Extended] July 1, 2020: users will not be able to use Mercurial features. All hg repos, wikis, and snippets will be in read-only mode. Heres why were focusing on Git This wasnt an easy decision, and Mercurial will always have a special place in Bitbuckets history. DevOps adoption has skyrocketed over the last decade and our customers are adopting this new way of working at an exponential rate. In this time, Bitbucket has steadily grown from being just a version control management tool to being a place to manage the entire software development lifecycle. And there's always more work to be done. This year we will concentrate on building deeper integrations to enhance automation and collaboration. Our improvements will make it even easier and safer to plan, code, test, and deploy all from within Bitbucket. Building quality features requires intense focus, and supporting two version control systems means splitting focus doubling shipping time and technical overhead. With Git being the more popularly used tool, Mercurial runs the risk of overlooked issues as we scale. According to a Stack Overflow Developer Survey, almost 90% of developers use Git, while Mercurial is the least popular version control system with only about 3% developer adoption. In fact, Mercurial usage on Bitbucket is steadily declining, and the percentage of new Bitbucket users choosing Mercurial has fallen to less than 1%. This deprecation will enable us to focus on building the best possible experience for our users. How to migrate and export We recommend that teams migrate their existing Mercurial repos to Git. There are various Git conversion tools in the market, including hg-fast-export and hg-git mercurial plugin. We are happy to support your migration, and you can find a discussion about available options in our dedicated Community thread. If you prefer to continue using the Mercurial system, there are a number of free and paid Mercurial hosting services. We realize that there is no one-size-fits-all solution. That's why we've created the following resources to best equip you with the knowledge and tools for a seamless transition: A Community thread to discuss conversion tools, migration, tips, and offer troubleshooting help A Git tutorial that covers anywhere from the basics of creating pull requests to rebasing and Git hooks We want to thank all the loyal users who have grown with us over the years. We look forward to this new focus on our roadmap and to introducing exciting new features. ",
        "comments.comment_id":[20745989,
          20746077],
        "comments.comment_author":["garganzol",
          "dragonsh"],
        "comments.comment_descendants":[4,
          19],
        "comments.comment_time":["2019-08-20T12:13:11Z",
          "2019-08-20T12:22:53Z"],
        "comments.comment_text":["It's funny to see how the whole world concentrates on this Git thing, while there is a treasure trove called Mercurial.<p>Mercurial was made for humans. It is seriously convenient and productive. Something I cannot say about Git, which more reminds me of an adhoc job.<p>I use both Git and Mercurial on daily basis. But my preference goes to Mercurial: it is just more sane in a big way. It is clearly a piece of art and love.",
          "It's very sad to see bitbucket dropping mercurial support. Now only Facebook and volunteers are keeping mercurial alive. \nSometimes technically better architecture and user interface lose to a non user friendly hard solutions due to inertia of mass adoption.<p>So a lesson in Software development is similar to betamax and VHS, so marketing is still a winner over technically superior architecture and ease of use. GitHub successfully marketed git, so git and GitHub are synonymous for most developers. Now majority of open source projects are reliant on a single proprietary solution Github by Microsoft, for managing code and project. Can understand the difficulty of bitbucket, when Python language itself moved out of mercurial due to the same inertia.<p>Hopefully gitlab can come out with mercurial support to migrate projects using it from bitbucket.<p>For people who believe in self hosted solution can install Kallithea (<a href=\"https://kallithea-scm.org\" rel=\"nofollow\">https://kallithea-scm.org</a>) or Rhodecode open source edition. Kallithea is used by Unity engine to manage their source code internally with mercurial."],
        "id":"9f22ad8e-bf30-4a4e-b1a4-e8bd641d790f",
        "_version_":1718939859380338688},
      {
        "story_id":[19669153],
        "story_author":["themlaiguy"],
        "story_descendants":[5],
        "story_score":[7],
        "story_time":["2019-04-15T21:37:08Z"],
        "story_title":"Why is version control in Jupyter notebooks so hard?",
        "search":["Why is version control in Jupyter notebooks so hard?",
          "Are there any tools that help with version control on notebooks?",
          "I've been clearing my output using nbconvert before putting the notebook into version control. I have a precommit hook and a check in CI. This works for my use case but I can understand needing to preserve output.<p>jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace my_notebook_name.ipynb",
          "You bet. I built ReviewNB[1] specifically for Jupyter Notebook code reviews.<p>There's also,<p>- nbstripout[2] for stripping outputs automatically before every commit<p>- nbdime[3] for diff'ing notebooks locally<p>- jupytext[4] for converting notebooks to markdown and vice-a-versa<p>[1] <a href=\"https://www.reviewnb.com/\" rel=\"nofollow\">https://www.reviewnb.com/</a><p>[2] <a href=\"https://github.com/kynan/nbstripout\" rel=\"nofollow\">https://github.com/kynan/nbstripout</a><p>[3] <a href=\"https://github.com/jupyter/nbdime\" rel=\"nofollow\">https://github.com/jupyter/nbdime</a><p>[4] <a href=\"https://github.com/mwouts/jupytext\" rel=\"nofollow\">https://github.com/mwouts/jupytext</a>"],
        "story_text":"Are there any tools that help with version control on notebooks?",
        "story_type":["AskHN"],
        "comments.comment_id":[19670482,
          19674241],
        "comments.comment_author":["snilzzor",
          "amirathi"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-04-16T01:47:18Z",
          "2019-04-16T15:04:38Z"],
        "comments.comment_text":["I've been clearing my output using nbconvert before putting the notebook into version control. I have a precommit hook and a check in CI. This works for my use case but I can understand needing to preserve output.<p>jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace my_notebook_name.ipynb",
          "You bet. I built ReviewNB[1] specifically for Jupyter Notebook code reviews.<p>There's also,<p>- nbstripout[2] for stripping outputs automatically before every commit<p>- nbdime[3] for diff'ing notebooks locally<p>- jupytext[4] for converting notebooks to markdown and vice-a-versa<p>[1] <a href=\"https://www.reviewnb.com/\" rel=\"nofollow\">https://www.reviewnb.com/</a><p>[2] <a href=\"https://github.com/kynan/nbstripout\" rel=\"nofollow\">https://github.com/kynan/nbstripout</a><p>[3] <a href=\"https://github.com/jupyter/nbdime\" rel=\"nofollow\">https://github.com/jupyter/nbdime</a><p>[4] <a href=\"https://github.com/mwouts/jupytext\" rel=\"nofollow\">https://github.com/mwouts/jupytext</a>"],
        "id":"421cd1a4-7490-4035-9246-d8945351d691",
        "_version_":1718939835411988480},
      {
        "story_id":[21071181],
        "story_author":["JA7Cal"],
        "story_descendants":[15],
        "story_score":[43],
        "story_time":["2019-09-25T14:36:38Z"],
        "story_title":"Automated Reports with Jupyter Notebooks (Using Jupytext and Papermill)",
        "search":["Automated Reports with Jupyter Notebooks (Using Jupytext and Papermill)",
          "https://medium.com/capital-fund-management/automated-reports-with-jupyter-notebooks-using-jupytext-and-papermill-619e60c37330",
          "Jupyter notebooks are one of the best available tools for running code interactively and writing a narrative with data and plots. What is less known is that they can be conveniently versioned and run automatically.Do you have a Jupyter notebook with plots and figures that you regularly run manually? Wouldnt it be nice to use the same notebook and instead have an automated reporting system, launched from a script? What if this script could even pass some parameters to the notebook it runs?This post explains in a few steps how this can be done concretely, including within a production environment.Example notebookWe will show you how to version control, automatically run and publish a notebook that depends on a parameter. As an example, we will use a notebook that describes the world population and the gross domestic product for a given year. It is simple to use: just change the year variable in the first cell, re-run, and you get the plots for the chosen year. But this requires a manual intervention. It would be much more convenient if the update could be automated and produce a report for each possible value of the year parameter (more generally, a notebook can update its results based not only on some user-provided parameters, but also through a connection to a database, etc.).Version controlIn a professional environment, notebooks are designed by, say, a data scientist, but the task of running them in production may be handled by a different team. So in general people have to share notebooks. This is best done through a version control system.Jupyter notebooks are famous for the difficulty of their version control. Lets consider our notebook above, with a file size of 3 MB, much of it being contributed by the embedded Plotly library. The notebook is less than 80 KB if we remove the output of the second code cell. And as small as 1.75 KB when all outputs are removed. This shows how much of its contents is unrelated to pure code! If we dont pay attention, code changes in the notebook will be lost in an ocean of binary contents.To get meaningful diffs, we use Jupytext (disclaimer: Im the author of Jupytext). Jupytext can be installed with pip or conda. Once the notebook server is restarted, a Jupytext menu appears in Jupyter:We click on Pair Notebook with Markdown, save the notebook and we obtain two representations of the notebook: world_fact.ipynb (with both input and output cells) and world_fact.md (with only the input cells).Jupytexts representation of notebooks as Markdown files is compatible with all major Markdown editors and viewers, including GitHub and VS Code. The Markdown version is for example rendered by GitHub as:As you can see, the Markdown file does not include any output. Indeed, we dont want it at this stage since we only need to share the notebook code. The Markdown file also has a very clear diff history, which makes versioning notebooks simple.The world_facts.md file is automatically updated by Jupyter when you save the notebook. And the other way round also works! If you modify world_facts.md with either a text editor, or by pulling the latest contributions from the version control system, then the changes appear in Jupyter when you refresh the notebook in the browser.In our version control system, we only need to track the Markdown file (and we even explicitly ignore all .ipynb files). Obviously, the team that will execute the notebook needs to regenerate the world_fact.ipynb document. For this they use Jupytext in the command line:$ jupytext world_facts.md --to ipynb[jupytext] Reading world_facts.md[jupytext] Writing world_facts.ipynbWe are now properly versioning the notebook. The diff history is much clearer. See for instance how the addition of the gross domestic products to our report looks like:Jupyter notebooks as Scripts?As an alternative to the Markdown representation, we could have paired the notebook to a world_facts.py script using Jupytext. You should give it a try if your notebook contains more code than text. That's often a first good step towards a complete and efficient refactoring of long notebooks: once the notebook is represented as a script, you can extract any complex code and move it to a (unit-tested) library using the refactoring tools in your IDE.JupyterLab, JupyterHub, Binder, Nteract, Colab & Cloud notebooks?Do you use JupyterLab and not Jupyter Notebook? No worries: the method above also applies in this case. You will just have to use the Jupytext extension for JupyterLab instead of the Jupytext menu. And in case you were wondering, Jupytext also work in JupyterHub and Binder.If you use other notebook editors like Nteract desktop, CoCalc, Google Colab, or another cloud notebook editor, you may not be able to use Jupytext as a plugin in the editor. In this case you can simply use Jupytext in the command line. Close your notebook and inject the pairing information into world_facts.ipynb with$ jupytext --set-formats ipynb,md world_facts.ipynband then keep the two representations synchronised with$ jupytext --sync world_facts.ipynbNotebook parametersPapermill is the reference library for executing notebooks with parameters.Papermill needs to know which cell contains the notebook parameters. This is simply done by adding a parameter tag in that cell with the cell toolbar in Jupyter Notebook:In JupyterLab you can use the celltags extension.And if you prefer you can also directly edit world_facts.md and add the tag there:```python tags=[\"parameters\"]year = 2000```Automated executionWe now have all the information required to execute the notebook on a production server.Production environmentIn order to execute the notebook, we need to know in which environment it should run. As we are working with a Python notebook in this example, we list its dependencies in a requirements.txt file, as is standard for Python projects.For simplicity, we also include the notebook tools in the same environment, i.e. add jupytext and papermill to the same requirements.txt file. Strictly speaking, these tools could be installed and executed in another Python environment.The corresponding Python environment is created with either$ conda create -n run_notebook --file requirements.txt -yor$ pip install -r requirements.txt(if in a virtual environment).Please note that the requirements.txt file is just one way of specifying an execution environment. The Reproducible Execution Environment Specification by the Binder team is one of the most complete references on the subject.Continuous IntegrationIt is a good practice to test each new contribution to either the notebook or its requirements. For this you can use for example Travis CI, a continuous integration solution. You will need only these two commands:pip install -r requirements.txt to install the dependenciesjupytext world_facts.md --set-kernel - --execute to test the execution of the notebook in the current Python environment.You can find a concrete example in our .travis.yml file.We are already executing the notebook automatically, arent we? Travis will tell us if a regression is introduced in the project What progress! But were not 100% done yet, as we promised to execute the notebook with parameters.Using the right kernelJupyter notebooks are associated with a kernel (i.e. a pointer to a local Python environment), but that kernel might not be available on your production machine. In this case, we simply update the notebook kernel so as to point to the environment that we have just created:$ jupytext world_facts.ipynb --set-kernel -Note that the minus sign in --set-kernel - above represents the current Python environment. In our example this yields:[jupytext] Reading world_facts.ipynb[jupytext] Updating notebook metadata with '{\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}}' [jupytext] Writing world_facts.ipynb (destination file replaced)In case you want to use another kernel just pass the kernel name to the --set-kernel option (you can get the list of all available kernels with jupyter kernelspec list and/or declare a new kernel with python -m ipykernel install --name kernel_name --user).Executing the notebook with parametersWe are now ready to use Papermill for executing the notebook.$ papermill world_facts.ipynb world_facts_2017.ipynb -p year 2017Input Notebook: world_facts.ipynb Output Notebook: world_facts_2017.ipynb 100%|| 8/8 [00:04<00:00, 1.41it/s]Were done! The notebook has been executed and the file world_facts_2017.ipynb contains the outputs.Publishing the NotebookIts time to deliver the notebook that was just executed. Maybe you want it in your mailbox? Or maybe you prefer to get a URL where you can see the result? We cover a few ways of doing that.GitHub can display Jupyter notebooks. This is a convenient solution, as you can easily choose who can access repositories. This works well as long as you dont include any interactive JavaScript plots or widgets in the notebook (the JavaScript parts are ignored by GitHub). In the case of our notebook, the interactive plots do not appear on GitHub, so we need another approach.Another option is to use the Jupyter Notebook Viewer. The nbviewer service can render any notebook which is publicly available on GitHub. Our notebook is thus rendered correctly there. If your notebook is not public, you can choose to install nbviewer locally.Alternatively, you can convert the executed notebook to HTML, and publish it on GitHub pages, or on your own HTML server, or send it over email. Converting the notebook to HTML is easily done with$ jupyter nbconvert world_facts_2017.ipynb --to html[NbConvertApp] Converting notebook world_facts_2017.ipynb to html [NbConvertApp] Writing 3361863 bytes to world_facts_2017.htmlThe resulting HTML file includes the code cells as below:But maybe you dont want to see the input cells in the HTML? You just need to add --no-input:$ jupyter nbconvert --to html --no-input world_facts_2017.ipynb --output world_facts_2017_report.htmlAnd youll get a cleaner report:Sending the standalone HTML file as an attachment in an email is an easy exercise. Embedding the report in the body of the email is also possible (but interactive plots wont work).Finally, if you are looking for a polished report and have some knowledge of LaTeX, you can give the PDF export option of Jupyters nbconvert command a try.Using pipesAn alternative to using named files would be to use pipes. jupytext, nbconvert and papermill all support them. A one-liner substitute for the previous commands is:$ cat world_facts.md \\ | jupytext --from md --to ipynb --set-kernel - \\ | papermill -p year 2017 \\ | jupyter nbconvert --stdin --output world_facts_2017_report.htmlConclusionYou should now be able to set up a full pipeline for generating reports in production, based on Jupyter notebooks. We have seen how to:version control a notebook with Jupytextshare a notebook and its dependencies between various userstest a notebook with continuous integrationexecute a notebook with parameters using Papermilland finally, how to publish the notebook (on GitHub or nbviewer), or render it as a static HTML page.The technology used in this example is fully based on the Jupyter Project, which is the de facto standard for Data Science. The tools used here are all open source and work well with any continuous integration framework.You have everything you need to schedule and deliver fine-tuned, code-free reports!EpilogueThe tools used here are written in Python. But they are language agnostic. Thanks to the Jupyter framework, they actually apply to any of the 40+ programming language for which a Jupyter kernel exists.Now, imagine that you have authored a document containing a few Bash command lines, just like this blog post. Install Jupytext and the bash kernel, and the blog post becomes this interactive Jupyter notebook!Going further, shouldnt we make sure that every instruction in our post actually works? We do that via our continuous integration spoiler alert: thats as simple as jupytext --execute README.md!AcknowledgmentsMarc would like to thank Eric Lebigot and Florent Zara for their contributions to this article, and to CFM for supporting this work through their Open-Source Program.About the authorThis article was written by Marc Wouts. Marc joined the research team of CFM in 2012 and has worked on a range of research projects, from optimal trading to portfolio construction.Marc has always been interested in finding efficient workflows for doing collaborative research involving data and code. In 2015 he authored an internal tool for publishing Jupyter and R Markdown notebooks on Atlassians Confluence wiki, providing a first solution for collaborating on notebooks. In 2018, he authored Jupytext, an open-source program that facilitates the version control of Jupyter notebooks. Marc is also interested in data visualisation, and coordinates a working group on this subject at CFM.Marc obtained a PhD in Probability Theory from the Paris Diderot University in 2007.DisclaimerAll views included in this document constitute judgments of its author(s) and do not necessarily reflect the views of Capital Fund Management or any of its affiliates. The information provided in this document is general information only, does not constitute investment or other advice, and is subject to change without notice. ",
          "Awesome article - I'm wondering, for \"Publishing the Notebook\" part of the workflow, have you ever seen Kyso (<a href=\"https://kyso.io\" rel=\"nofollow\">https://kyso.io</a>) - disclaimer, I'm a founder. We started Kyso to make it easier to communicate insights gained from analysis to non-technical people by converting data science tools (e.g. Jupyter Notebooks) into conversational tools in the form of blog posts. You can make public posts or have an internal \"data blog\" for your team, where you push your work to Github and it is reflected on Kyso. Would love to hear your thoughts on how it could fit into existing workflows.",
          "I don't think Jupyter notebooks should be used for automated jobs. They're great for exploratory stuff but once things are getting fleshed out and cleaned up, one should move to proper python files that can be unit tested and versioned without having to go to crazy lengths..."],
        "story_type":["Normal"],
        "url":"https://medium.com/capital-fund-management/automated-reports-with-jupyter-notebooks-using-jupytext-and-papermill-619e60c37330",
        "comments.comment_id":[21071306,
          21072228],
        "comments.comment_author":["KyleOS",
          "brummm"],
        "comments.comment_descendants":[0,
          1],
        "comments.comment_time":["2019-09-25T14:49:46Z",
          "2019-09-25T16:21:21Z"],
        "comments.comment_text":["Awesome article - I'm wondering, for \"Publishing the Notebook\" part of the workflow, have you ever seen Kyso (<a href=\"https://kyso.io\" rel=\"nofollow\">https://kyso.io</a>) - disclaimer, I'm a founder. We started Kyso to make it easier to communicate insights gained from analysis to non-technical people by converting data science tools (e.g. Jupyter Notebooks) into conversational tools in the form of blog posts. You can make public posts or have an internal \"data blog\" for your team, where you push your work to Github and it is reflected on Kyso. Would love to hear your thoughts on how it could fit into existing workflows.",
          "I don't think Jupyter notebooks should be used for automated jobs. They're great for exploratory stuff but once things are getting fleshed out and cleaned up, one should move to proper python files that can be unit tested and versioned without having to go to crazy lengths..."],
        "id":"8af3dc0b-b106-49e2-b807-321853b5e767",
        "url_text":"Jupyter notebooks are one of the best available tools for running code interactively and writing a narrative with data and plots. What is less known is that they can be conveniently versioned and run automatically.Do you have a Jupyter notebook with plots and figures that you regularly run manually? Wouldnt it be nice to use the same notebook and instead have an automated reporting system, launched from a script? What if this script could even pass some parameters to the notebook it runs?This post explains in a few steps how this can be done concretely, including within a production environment.Example notebookWe will show you how to version control, automatically run and publish a notebook that depends on a parameter. As an example, we will use a notebook that describes the world population and the gross domestic product for a given year. It is simple to use: just change the year variable in the first cell, re-run, and you get the plots for the chosen year. But this requires a manual intervention. It would be much more convenient if the update could be automated and produce a report for each possible value of the year parameter (more generally, a notebook can update its results based not only on some user-provided parameters, but also through a connection to a database, etc.).Version controlIn a professional environment, notebooks are designed by, say, a data scientist, but the task of running them in production may be handled by a different team. So in general people have to share notebooks. This is best done through a version control system.Jupyter notebooks are famous for the difficulty of their version control. Lets consider our notebook above, with a file size of 3 MB, much of it being contributed by the embedded Plotly library. The notebook is less than 80 KB if we remove the output of the second code cell. And as small as 1.75 KB when all outputs are removed. This shows how much of its contents is unrelated to pure code! If we dont pay attention, code changes in the notebook will be lost in an ocean of binary contents.To get meaningful diffs, we use Jupytext (disclaimer: Im the author of Jupytext). Jupytext can be installed with pip or conda. Once the notebook server is restarted, a Jupytext menu appears in Jupyter:We click on Pair Notebook with Markdown, save the notebook and we obtain two representations of the notebook: world_fact.ipynb (with both input and output cells) and world_fact.md (with only the input cells).Jupytexts representation of notebooks as Markdown files is compatible with all major Markdown editors and viewers, including GitHub and VS Code. The Markdown version is for example rendered by GitHub as:As you can see, the Markdown file does not include any output. Indeed, we dont want it at this stage since we only need to share the notebook code. The Markdown file also has a very clear diff history, which makes versioning notebooks simple.The world_facts.md file is automatically updated by Jupyter when you save the notebook. And the other way round also works! If you modify world_facts.md with either a text editor, or by pulling the latest contributions from the version control system, then the changes appear in Jupyter when you refresh the notebook in the browser.In our version control system, we only need to track the Markdown file (and we even explicitly ignore all .ipynb files). Obviously, the team that will execute the notebook needs to regenerate the world_fact.ipynb document. For this they use Jupytext in the command line:$ jupytext world_facts.md --to ipynb[jupytext] Reading world_facts.md[jupytext] Writing world_facts.ipynbWe are now properly versioning the notebook. The diff history is much clearer. See for instance how the addition of the gross domestic products to our report looks like:Jupyter notebooks as Scripts?As an alternative to the Markdown representation, we could have paired the notebook to a world_facts.py script using Jupytext. You should give it a try if your notebook contains more code than text. That's often a first good step towards a complete and efficient refactoring of long notebooks: once the notebook is represented as a script, you can extract any complex code and move it to a (unit-tested) library using the refactoring tools in your IDE.JupyterLab, JupyterHub, Binder, Nteract, Colab & Cloud notebooks?Do you use JupyterLab and not Jupyter Notebook? No worries: the method above also applies in this case. You will just have to use the Jupytext extension for JupyterLab instead of the Jupytext menu. And in case you were wondering, Jupytext also work in JupyterHub and Binder.If you use other notebook editors like Nteract desktop, CoCalc, Google Colab, or another cloud notebook editor, you may not be able to use Jupytext as a plugin in the editor. In this case you can simply use Jupytext in the command line. Close your notebook and inject the pairing information into world_facts.ipynb with$ jupytext --set-formats ipynb,md world_facts.ipynband then keep the two representations synchronised with$ jupytext --sync world_facts.ipynbNotebook parametersPapermill is the reference library for executing notebooks with parameters.Papermill needs to know which cell contains the notebook parameters. This is simply done by adding a parameter tag in that cell with the cell toolbar in Jupyter Notebook:In JupyterLab you can use the celltags extension.And if you prefer you can also directly edit world_facts.md and add the tag there:```python tags=[\"parameters\"]year = 2000```Automated executionWe now have all the information required to execute the notebook on a production server.Production environmentIn order to execute the notebook, we need to know in which environment it should run. As we are working with a Python notebook in this example, we list its dependencies in a requirements.txt file, as is standard for Python projects.For simplicity, we also include the notebook tools in the same environment, i.e. add jupytext and papermill to the same requirements.txt file. Strictly speaking, these tools could be installed and executed in another Python environment.The corresponding Python environment is created with either$ conda create -n run_notebook --file requirements.txt -yor$ pip install -r requirements.txt(if in a virtual environment).Please note that the requirements.txt file is just one way of specifying an execution environment. The Reproducible Execution Environment Specification by the Binder team is one of the most complete references on the subject.Continuous IntegrationIt is a good practice to test each new contribution to either the notebook or its requirements. For this you can use for example Travis CI, a continuous integration solution. You will need only these two commands:pip install -r requirements.txt to install the dependenciesjupytext world_facts.md --set-kernel - --execute to test the execution of the notebook in the current Python environment.You can find a concrete example in our .travis.yml file.We are already executing the notebook automatically, arent we? Travis will tell us if a regression is introduced in the project What progress! But were not 100% done yet, as we promised to execute the notebook with parameters.Using the right kernelJupyter notebooks are associated with a kernel (i.e. a pointer to a local Python environment), but that kernel might not be available on your production machine. In this case, we simply update the notebook kernel so as to point to the environment that we have just created:$ jupytext world_facts.ipynb --set-kernel -Note that the minus sign in --set-kernel - above represents the current Python environment. In our example this yields:[jupytext] Reading world_facts.ipynb[jupytext] Updating notebook metadata with '{\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}}' [jupytext] Writing world_facts.ipynb (destination file replaced)In case you want to use another kernel just pass the kernel name to the --set-kernel option (you can get the list of all available kernels with jupyter kernelspec list and/or declare a new kernel with python -m ipykernel install --name kernel_name --user).Executing the notebook with parametersWe are now ready to use Papermill for executing the notebook.$ papermill world_facts.ipynb world_facts_2017.ipynb -p year 2017Input Notebook: world_facts.ipynb Output Notebook: world_facts_2017.ipynb 100%|| 8/8 [00:04<00:00, 1.41it/s]Were done! The notebook has been executed and the file world_facts_2017.ipynb contains the outputs.Publishing the NotebookIts time to deliver the notebook that was just executed. Maybe you want it in your mailbox? Or maybe you prefer to get a URL where you can see the result? We cover a few ways of doing that.GitHub can display Jupyter notebooks. This is a convenient solution, as you can easily choose who can access repositories. This works well as long as you dont include any interactive JavaScript plots or widgets in the notebook (the JavaScript parts are ignored by GitHub). In the case of our notebook, the interactive plots do not appear on GitHub, so we need another approach.Another option is to use the Jupyter Notebook Viewer. The nbviewer service can render any notebook which is publicly available on GitHub. Our notebook is thus rendered correctly there. If your notebook is not public, you can choose to install nbviewer locally.Alternatively, you can convert the executed notebook to HTML, and publish it on GitHub pages, or on your own HTML server, or send it over email. Converting the notebook to HTML is easily done with$ jupyter nbconvert world_facts_2017.ipynb --to html[NbConvertApp] Converting notebook world_facts_2017.ipynb to html [NbConvertApp] Writing 3361863 bytes to world_facts_2017.htmlThe resulting HTML file includes the code cells as below:But maybe you dont want to see the input cells in the HTML? You just need to add --no-input:$ jupyter nbconvert --to html --no-input world_facts_2017.ipynb --output world_facts_2017_report.htmlAnd youll get a cleaner report:Sending the standalone HTML file as an attachment in an email is an easy exercise. Embedding the report in the body of the email is also possible (but interactive plots wont work).Finally, if you are looking for a polished report and have some knowledge of LaTeX, you can give the PDF export option of Jupyters nbconvert command a try.Using pipesAn alternative to using named files would be to use pipes. jupytext, nbconvert and papermill all support them. A one-liner substitute for the previous commands is:$ cat world_facts.md \\ | jupytext --from md --to ipynb --set-kernel - \\ | papermill -p year 2017 \\ | jupyter nbconvert --stdin --output world_facts_2017_report.htmlConclusionYou should now be able to set up a full pipeline for generating reports in production, based on Jupyter notebooks. We have seen how to:version control a notebook with Jupytextshare a notebook and its dependencies between various userstest a notebook with continuous integrationexecute a notebook with parameters using Papermilland finally, how to publish the notebook (on GitHub or nbviewer), or render it as a static HTML page.The technology used in this example is fully based on the Jupyter Project, which is the de facto standard for Data Science. The tools used here are all open source and work well with any continuous integration framework.You have everything you need to schedule and deliver fine-tuned, code-free reports!EpilogueThe tools used here are written in Python. But they are language agnostic. Thanks to the Jupyter framework, they actually apply to any of the 40+ programming language for which a Jupyter kernel exists.Now, imagine that you have authored a document containing a few Bash command lines, just like this blog post. Install Jupytext and the bash kernel, and the blog post becomes this interactive Jupyter notebook!Going further, shouldnt we make sure that every instruction in our post actually works? We do that via our continuous integration spoiler alert: thats as simple as jupytext --execute README.md!AcknowledgmentsMarc would like to thank Eric Lebigot and Florent Zara for their contributions to this article, and to CFM for supporting this work through their Open-Source Program.About the authorThis article was written by Marc Wouts. Marc joined the research team of CFM in 2012 and has worked on a range of research projects, from optimal trading to portfolio construction.Marc has always been interested in finding efficient workflows for doing collaborative research involving data and code. In 2015 he authored an internal tool for publishing Jupyter and R Markdown notebooks on Atlassians Confluence wiki, providing a first solution for collaborating on notebooks. In 2018, he authored Jupytext, an open-source program that facilitates the version control of Jupyter notebooks. Marc is also interested in data visualisation, and coordinates a working group on this subject at CFM.Marc obtained a PhD in Probability Theory from the Paris Diderot University in 2007.DisclaimerAll views included in this document constitute judgments of its author(s) and do not necessarily reflect the views of Capital Fund Management or any of its affiliates. The information provided in this document is general information only, does not constitute investment or other advice, and is subject to change without notice. ",
        "_version_":1718939868685402113},
      {
        "story_id":[20646674],
        "story_author":["tosh"],
        "story_descendants":[68],
        "story_score":[107],
        "story_time":["2019-08-08T17:40:55Z"],
        "story_title":"The market figured out Gitlab’s secret",
        "search":["The market figured out Gitlab’s secret",
          "https://about.gitlab.com/2019/08/08/built-in-ci-cd-version-control-secret/",
          "Theres a movement in the DevOps industry and the world right now: to do more in a simple way that inspires us to innovate. GitLab started this trend in the DevOps space by simplifying the delivery of code by combining GitLab CI and GitLab version control. We didn't originally buy into the idea that this was the right way to do things, but it became our secret capability that weve doubled down on. Lets combine applications The story starts with Kamil Trzciski, now a distinguished engineer at GitLab. Soon after Kamil came to work for GitLab full time, he began talking with me and my co-founder, Dmitriy Zaporozhets, suggesting that we bring our two projects together GitLab Version Control and GitLab CI, making it into one application. Dmitriy didnt think it was a good idea. GitLab version control and CI were already perfectly integrated with single sign-on and APIs that fit like a glove. He thought that combining them would make GitLab a monolith of an application, that it would be disastrous for our code quality, and an unfortunate user experience. After time though, Dmitriy started to think it was the right idea as it would deliver a seamless experience for developers to deliver code quickly. After Dmitriy was convinced, they came to me. I also didnt think it was a good idea. At the time I believed we needed to have tools that are composable and that could integrate with other tools, in line with the Unix philosophy. Kamil convinced me to think about the efficiencies of having a single application. Well, if you dont believe that its better for a user, at least believe its more efficient for us, because we only have to release one application instead of two. Efficiency is in our values. - Kamil Trzcinski, distinguished engineer at GitLab Realizing the future of DevOps is a single application That made sense to me and I no longer stood in their way. The two projects merged and the results were beyond my expectations. The efficiencies that were so appealing to us, also made it appealing to our customers. We realized we stumbled on a big secret because nobody believed that the two combined together would be a better way of continuously delivering code to market. We doubled down on this philosophy and we started doing continuous delivery. From that day on, I saw the value of having a single application. For example, a new feature we are implementing is auto-remediation. When a vulnerability comes out, say a heart bleed, GitLab will automatically detect where in your codebase that vulnerability exists, update the dependency, and deliver it to your production environment. This level of automation would be hard to implement without being in a single application. By combining the projects we unified teams helping them realize the original intent of DevOps and that is magical to see. The market validates our secret And while we bet on this philosophy the industry is now seeing it as well. In September of 2015 we combined GitLab CI and GitLab version control to create a single application. By March of 2017, Bitbucket also realized the advantages of this architecture and released Pipelines as a built-in part of Bitbucket. In 2018, GitHub announced Actions with CI-like functionality built into a single application offering. In the last six months, JFrog acquired Shippable and Idera acquired Travis CI, showing a consolidation of the DevOps market and a focus on CI. The market is validating what we continually hear from our users and customers: that a simple, single DevOps application meets their needs better. We hope you will continue to join us in our effort to bring teams together to innovate. Everyone can contribute here at GitLab and as always, we value your feedback, thoughts, and contributions. Want to hear me talk through the origin story? Listen to the Software Engineering Daily podcast where I talk about combining GitLab CI and GitLab Version Control. The industry has caught onto @GitLabs secret. Learn more about why GitLab combined GitLab CI and GitLab version control Sid Sijbrandij Click to tweet Sign up for GitLabs twice-monthly newsletter ",
          "Every time github releases a feature this is the response. These posts are so pathetic. I’m surprised that they continue to play this angle. It’s a very polarizing way to address the community that will definitely continue to stir up us vs them mentality between GitHub and Gitlab users.",
          "For all the bashing GitLab gets, personally, I want GitLab to survive and keep competing at some level with GitHub.<p>Should GitHub dominate the market and gobble up competition, we all know how it goes for its parent company."],
        "story_type":["Normal"],
        "url":"https://about.gitlab.com/2019/08/08/built-in-ci-cd-version-control-secret/",
        "comments.comment_id":[20647790,
          20648739],
        "comments.comment_author":["whalesalad",
          "asadkn"],
        "comments.comment_descendants":[5,
          1],
        "comments.comment_time":["2019-08-08T19:32:16Z",
          "2019-08-08T21:04:04Z"],
        "comments.comment_text":["Every time github releases a feature this is the response. These posts are so pathetic. I’m surprised that they continue to play this angle. It’s a very polarizing way to address the community that will definitely continue to stir up us vs them mentality between GitHub and Gitlab users.",
          "For all the bashing GitLab gets, personally, I want GitLab to survive and keep competing at some level with GitHub.<p>Should GitHub dominate the market and gobble up competition, we all know how it goes for its parent company."],
        "id":"6cca37cd-352b-4c93-be0e-8464ac8549bd",
        "url_text":"Theres a movement in the DevOps industry and the world right now: to do more in a simple way that inspires us to innovate. GitLab started this trend in the DevOps space by simplifying the delivery of code by combining GitLab CI and GitLab version control. We didn't originally buy into the idea that this was the right way to do things, but it became our secret capability that weve doubled down on. Lets combine applications The story starts with Kamil Trzciski, now a distinguished engineer at GitLab. Soon after Kamil came to work for GitLab full time, he began talking with me and my co-founder, Dmitriy Zaporozhets, suggesting that we bring our two projects together GitLab Version Control and GitLab CI, making it into one application. Dmitriy didnt think it was a good idea. GitLab version control and CI were already perfectly integrated with single sign-on and APIs that fit like a glove. He thought that combining them would make GitLab a monolith of an application, that it would be disastrous for our code quality, and an unfortunate user experience. After time though, Dmitriy started to think it was the right idea as it would deliver a seamless experience for developers to deliver code quickly. After Dmitriy was convinced, they came to me. I also didnt think it was a good idea. At the time I believed we needed to have tools that are composable and that could integrate with other tools, in line with the Unix philosophy. Kamil convinced me to think about the efficiencies of having a single application. Well, if you dont believe that its better for a user, at least believe its more efficient for us, because we only have to release one application instead of two. Efficiency is in our values. - Kamil Trzcinski, distinguished engineer at GitLab Realizing the future of DevOps is a single application That made sense to me and I no longer stood in their way. The two projects merged and the results were beyond my expectations. The efficiencies that were so appealing to us, also made it appealing to our customers. We realized we stumbled on a big secret because nobody believed that the two combined together would be a better way of continuously delivering code to market. We doubled down on this philosophy and we started doing continuous delivery. From that day on, I saw the value of having a single application. For example, a new feature we are implementing is auto-remediation. When a vulnerability comes out, say a heart bleed, GitLab will automatically detect where in your codebase that vulnerability exists, update the dependency, and deliver it to your production environment. This level of automation would be hard to implement without being in a single application. By combining the projects we unified teams helping them realize the original intent of DevOps and that is magical to see. The market validates our secret And while we bet on this philosophy the industry is now seeing it as well. In September of 2015 we combined GitLab CI and GitLab version control to create a single application. By March of 2017, Bitbucket also realized the advantages of this architecture and released Pipelines as a built-in part of Bitbucket. In 2018, GitHub announced Actions with CI-like functionality built into a single application offering. In the last six months, JFrog acquired Shippable and Idera acquired Travis CI, showing a consolidation of the DevOps market and a focus on CI. The market is validating what we continually hear from our users and customers: that a simple, single DevOps application meets their needs better. We hope you will continue to join us in our effort to bring teams together to innovate. Everyone can contribute here at GitLab and as always, we value your feedback, thoughts, and contributions. Want to hear me talk through the origin story? Listen to the Software Engineering Daily podcast where I talk about combining GitLab CI and GitLab Version Control. The industry has caught onto @GitLabs secret. Learn more about why GitLab combined GitLab CI and GitLab version control Sid Sijbrandij Click to tweet Sign up for GitLabs twice-monthly newsletter ",
        "_version_":1718939856952885248},
      {
        "story_id":[19706396],
        "story_author":["beefhash"],
        "story_descendants":[10],
        "story_score":[125],
        "story_time":["2019-04-20T13:22:38Z"],
        "story_title":"Fh: File history with ed, diff, awk, sed, and sh",
        "search":["Fh: File history with ed, diff, awk, sed, and sh",
          "https://github.com/xorhash/fh",
          "fh records changes to a file on a per-file basis, similar to RCS and SCCS. It is, however, considerably more primitive. Design goals: no support for multi-user environments (no locking, etc.) implemented in shell script must use ed(1) should work or easily be made to work on 7th Edition UNIX I've taken care not to use any shell scripting constructions that didn't exist in the Bourne shell, but I may have missed things; however, the shebang needs to be removed on 7th Edition UNIX. fh uses a chain of ed(1) scripts to construct any version of a file. It even allows recording of commit messages. Why? I saw the following passage in diff(1) of 7th Edition UNIX: The -e option produces a script of a, c and d commands for the editor ed, which will recreate file2 from file1. The -f option produces a similar script, not useful with ed, in the opposite order. In connection with -e, the following shell program may help maintain multiple versions of a file. Only an ancestral file ($1) and a chain of version-to-version ed scripts ($2,$3,...) made by diff need be on hand. A `latest version' appears on the standard output. (shift; cat $*; echo 1,$p) ed - $1 After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious. License ISC, see LICENSE. Installation and usage Copy the fl, fr and fu files to a directory in $PATH; make sure they are marked as executable. Copy the man pages fl.1, fr.1, fu.1 and fh.5 to a directory in $MANPATH. For usage, see the supplied man pages. man.md is available on the web. For 7th Edition UNIX, the man pages written in mdoc macrosneed to be converted to old man macros first: mkdir man mandoc -Tman fl.1 > man/fl.1 mandoc -Tman fr.1 > man/fr.1 mandoc -Tman fu.1 > man/fu.1 mandoc -Tman fh.5 > man/fh.5 ",
          "Wait, what?<p><i>\"After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious.\"</i><p>Ah, ok.<p>Also, if you want to experiment with diffs and recreating versions:<p>Patch: <a href=\"https://en.m.wikipedia.org/wiki/Patch_(Unix)\" rel=\"nofollow\">https://en.m.wikipedia.org/wiki/Patch_(Unix)</a><p>Xdelta:  <a href=\"https://github.com/jmacd/xdelta\" rel=\"nofollow\">https://github.com/jmacd/xdelta</a>",
          "It's not a bad exercise.  In a Unix class I took, we had to implement an RDBMS using only the traditional Unix software tools.  A little later, for work, I had to learn to write highly portable and fairly secure shell scripts.<p>A few reasons to do exercises like this:<p>* To understand the Unix software tools and Bourne/etc. shell scripting models.  There are good lessons, and also examples of what not to do (or the tradeoffs), which I don't think you'll find anywhere else.  (For example, the simple Unix streams model is very powerful, and also often used in very kludgey ways in practice, and the Bourne evaluation model is probably much worse and dangerous than a beginner might think.)<p>* Knowing how to do things with just shell scripts is great for some kinds of bootstrapping, commands in build tools, or other situations in which you can only assume a shell interpreter and possibly certain command-line tools.<p>* Handy for configuring your interactive shell to do what you want.<p>* It's one way to familiarize with parts of a Unix-ish system that you might not normally, especially if you're spending all your time learning your way around huge stacks of tools that obscure the lower-level mechanics.<p>* You'll appreciate Perl and other languages much more, in some ways."],
        "story_type":["Normal"],
        "url":"https://github.com/xorhash/fh",
        "url_text":"fh records changes to a file on a per-file basis, similar to RCS and SCCS. It is, however, considerably more primitive. Design goals: no support for multi-user environments (no locking, etc.) implemented in shell script must use ed(1) should work or easily be made to work on 7th Edition UNIX I've taken care not to use any shell scripting constructions that didn't exist in the Bourne shell, but I may have missed things; however, the shebang needs to be removed on 7th Edition UNIX. fh uses a chain of ed(1) scripts to construct any version of a file. It even allows recording of commit messages. Why? I saw the following passage in diff(1) of 7th Edition UNIX: The -e option produces a script of a, c and d commands for the editor ed, which will recreate file2 from file1. The -f option produces a similar script, not useful with ed, in the opposite order. In connection with -e, the following shell program may help maintain multiple versions of a file. Only an ancestral file ($1) and a chain of version-to-version ed scripts ($2,$3,...) made by diff need be on hand. A `latest version' appears on the standard output. (shift; cat $*; echo 1,$p) ed - $1 After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious. License ISC, see LICENSE. Installation and usage Copy the fl, fr and fu files to a directory in $PATH; make sure they are marked as executable. Copy the man pages fl.1, fr.1, fu.1 and fh.5 to a directory in $MANPATH. For usage, see the supplied man pages. man.md is available on the web. For 7th Edition UNIX, the man pages written in mdoc macrosneed to be converted to old man macros first: mkdir man mandoc -Tman fl.1 > man/fl.1 mandoc -Tman fr.1 > man/fr.1 mandoc -Tman fu.1 > man/fu.1 mandoc -Tman fh.5 > man/fh.5 ",
        "comments.comment_id":[19706631,
          19706816],
        "comments.comment_author":["tyingq",
          "neilv"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-04-20T14:22:11Z",
          "2019-04-20T14:54:29Z"],
        "comments.comment_text":["Wait, what?<p><i>\"After some thinking, I figured it would be hilarious to actually implement a basic version control system on these primitives. In hindsight, it's probably closer to terrifying than hilarious.\"</i><p>Ah, ok.<p>Also, if you want to experiment with diffs and recreating versions:<p>Patch: <a href=\"https://en.m.wikipedia.org/wiki/Patch_(Unix)\" rel=\"nofollow\">https://en.m.wikipedia.org/wiki/Patch_(Unix)</a><p>Xdelta:  <a href=\"https://github.com/jmacd/xdelta\" rel=\"nofollow\">https://github.com/jmacd/xdelta</a>",
          "It's not a bad exercise.  In a Unix class I took, we had to implement an RDBMS using only the traditional Unix software tools.  A little later, for work, I had to learn to write highly portable and fairly secure shell scripts.<p>A few reasons to do exercises like this:<p>* To understand the Unix software tools and Bourne/etc. shell scripting models.  There are good lessons, and also examples of what not to do (or the tradeoffs), which I don't think you'll find anywhere else.  (For example, the simple Unix streams model is very powerful, and also often used in very kludgey ways in practice, and the Bourne evaluation model is probably much worse and dangerous than a beginner might think.)<p>* Knowing how to do things with just shell scripts is great for some kinds of bootstrapping, commands in build tools, or other situations in which you can only assume a shell interpreter and possibly certain command-line tools.<p>* Handy for configuring your interactive shell to do what you want.<p>* It's one way to familiarize with parts of a Unix-ish system that you might not normally, especially if you're spending all your time learning your way around huge stacks of tools that obscure the lower-level mechanics.<p>* You'll appreciate Perl and other languages much more, in some ways."],
        "id":"21968fc7-7631-4069-b399-ed81561aaa9b",
        "_version_":1718939836236169216},
      {
        "story_id":[19470064],
        "story_author":["anewhnaccount2"],
        "story_descendants":[42],
        "story_score":[204],
        "story_time":["2019-03-23T11:52:03Z"],
        "story_title":"Qri: A global dataset version control system built on the distributed web",
        "search":["Qri: A global dataset version control system built on the distributed web",
          "https://github.com/qri-io/qri",
          "Qri CLI a dataset version control system built on the distributed web Website | Packages | Contribute | Issues | Docs | Download Welcome Question Answer \"I want to learn about Qri\" Read the official documentation \"I want to download Qri\" Download Qri or brew install qri-io/qri/qri \"I have a question\" Create an issue and use the label 'question' \"I found a bug\" Create an issue and use the label 'bug' \"I want to help build the Qri backend\" Read the Contributing guides \"I want to build Qri from source\" Build Qri from source qri is a global dataset version control system built on the distributed web Breaking that down: global so that if anyone, anywhere has published work with the same or similar datasets, you can discover it. Specific to datasets because data deserves purpose-built tools version control to keep data in sync, attributing all changes to authors On the distributed web to make all of the data published on qri simultaneously available, letting peers work on data together. If youre unfamiliar with version control, particularly the distributed kind, well you're probably viewing this document on github which is a version control system intended for code. Its underlying technology git popularized some magic sauce that has inspired a generation of programmers and popularized concepts at the heart of the distributed web. Qri is applying that family of concepts to four common data problems: Discovery Can I find data Im looking for? Trust Can I trust what Ive found? Friction Can I make this work with my other stuff? Sync How do I handle changes in data? Because qri is global and content-addressed, adding data to qri also checks the entire network to see if someone has added it before. Since qri is focused solely on datasets, it can provide meaningful search results. Every change on qri is associated with a peer, creating an audit-able trail you can use to quickly see what has changed and who has changed it. All datasets on qri are automatically described at the time of ingest using a flexible schema that makes data naturally inter-operate. Qri comes with tools to turn all datasets on the network into a JSON API with a single command. Finally, all changes in qri are tracked & synced. Building From Source To build qri you'll need the go programming language on your machine. $ git clone https://github.com/qri-io/qri $ cd qri $ make install If this is your first time building, this command will have a lot of output. That's good! Its means it's working :) It'll take a minute or two to build. After this is done, there will be a new binary qri in your ~/go/bin directory if using go modules, and $GOPATH/bin directory otherwise. You should be able to run: and see help output. Building on Windows To start, make sure that you have enabled Developer Mode. A library that we depend on needs it enabled in order to properly handle symlinks. If not done, you'll likely get the error message \"A required privilege is not held by the client\". You should not need to Run As Administrator to build or run qri. We do not recommend using administrator to run qri. Shell For your shell, we recommend using msys2. Other shells, such as cmd, Powershell, or cygwin may also be usable, but msys2 makes it easy to install our required dependencies. IPFS also recommends msys2, and qri is built on top of IPFS. Dependencies Building depends upon having git and make installed. If using msys2, you can easily install these by using the package manager \"pacman\". In a shell, type: Assuming you've also installed go using the official Windows installer linked above, you will also need to add go to your PATH by modifying your environment variable. See the next section on \"Environment variables\" for more information. Due to how msys2 treats the PATH variable, you also need to add a new environment variable MSYS2_PATH_TYPE, with the value inherit, using the same procedure. Once these steps are complete, proceed to building. Building on Rasberry PI On a Raspberry PI, you'll need to increase your swap file size in order to build. Normal desktop and server linux OSes should be fine to proceed to building. One symptom of having not enough swap space is the go install command producing an error message ending with: To increase your swapfile size, first turn off the swapfile: sudo dphys-swapfile swapoff Then edit /etc/dphys-swapfile as root and set CONF_SWAPSIZE to 1024. Finally turn on the swapfile again: sudo dphys-swapfile swapon Otherwise linux machines with reduced memory will have other ways to increase their swap file sizes. Check documentation for your particular machine. Packages Qri is comprised of many specialized packages. Below you will find a summary of each package. Package Go Docs Go Report Card Description api user accessible layer, primarily made for communication with our frontend webapp cmd our command line interface config user configuration details, includes peer's profile lib takes arguments from the cmd and api layer and forms proper requests to call to the action layer p2p the peer to peer communication layer of qri repo the repository: saving, removing, and storing datasets, profiles, and the config dataset the blueprint for a dataset, the atoms that make up qri registry the blueprint for a registry: the service that allows profiles to be unique and datasets to be searchable starlib the starlark standard library available for qri transform scripts qfs \"qri file sytem\" is Qri's file system abstraction for getting & storing data from different sources ioes package to handle in, out, and error streams: gives us better control of where we send output and errors jsonschema used to describe the structure of a dataset, so we can validate datasets and determine dataset interop Outside Libraries The following packages are not under Qri, but are important dependencies, so we display their latest versions for convenience. Package Version ipfs This documentation has been adapted from the Cycle.js documentation. ",
          "Interesting project, particularly with the choice of IPFS and DCAT -- something I'll have to look into.  There have been other efforts to handle mostly file-based scientific data with versioning in both distributed (Dat <a href=\"https://blog.datproject.org/tag/science/\" rel=\"nofollow\">https://blog.datproject.org/tag/science/</a>) and centralized ways (DataHub <a href=\"https://datahub.csail.mit.edu/www/\" rel=\"nofollow\">https://datahub.csail.mit.edu/www/</a>).  Juan Benet visited our research center to give a talk about IPFS a few years ago. Really fantastic stuff.<p>I'm the creator of DVID (<a href=\"http://dvid.io\" rel=\"nofollow\">http://dvid.io</a>), which has an entirely different approach to how we might handle distributed versioning of scientific data primarily at a larger scale (100 GB to petabytes).  Like Qri and IPFS, DVID is written in Go.  Our research group works in Connectomics.  We start with massive 3D brain image volumes and apply automated and manual segmentation to mine the neurons and synapses of all that data.  There's also a lot of associated data to manage the production of connectomes.<p>One of our requirements, though, is having low-latency reads and writes to the data.  We decided to create a Science API that shields clients from how the data is actually represented, and for now, have used an ordered key-value stores for the backend.  Pluggable \"datatypes\" provide the Science API and also translate requests into the underlying key-value pairs, which are the units for versioning.  It's worked out pretty well for us and I'm now working on overhauling the store interface and improving the movement of versions between servers.  At our scale, it's useful to be able to mail a hard drive to a collaborator to establish the base DAG data and then let them eventually do a \"pull request\" for their relatively small modifications.<p>We've published some of our data online (<a href=\"http://emdata.janelia.org\" rel=\"nofollow\">http://emdata.janelia.org</a>) and visitors can actually browse through the 3d images using a Google-developed web app, Neuroglancer.  It's running on a relatively small VM so I imagine any significant HN traffic might crush it :/  We are still figuring out the best way to handle the public-facing side.<p>I think a lot of people are coming up with their own ideas about how to version scientific data, so maybe we should establish a meeting or workshop to discuss how some of these systems might interoperate?  The RDA (<a href=\"https://rd-alliance.org/\" rel=\"nofollow\">https://rd-alliance.org/</a>) \nhas been trying to establish working groups and standards, although they weren't really looking at distributed versioning a few years ago.  We need something like a Github for scientific data where papers can reference data at a particular commit and then offer improvements through pull requests.",
          "I really love the design and style qri! It is fun!<p>Can I ask why, for a git-style system, IPFS was chosen instead of GUN or SSB?<p>Certainly, images/files/etc. are better in IPFS than GUN or SSB.<p>But, you're gonna have a nightmare doing any git-style index/patch/object/etc. operations with it - both GUN & SSB's algorithms are meant to handle this type of stuff.<p>Did you guys do any analysis?"],
        "story_type":["Normal"],
        "url":"https://github.com/qri-io/qri",
        "comments.comment_id":[19470305,
          19472145],
        "comments.comment_author":["DocSavage",
          "marknadal"],
        "comments.comment_descendants":[2,
          1],
        "comments.comment_time":["2019-03-23T13:11:31Z",
          "2019-03-23T20:17:31Z"],
        "comments.comment_text":["Interesting project, particularly with the choice of IPFS and DCAT -- something I'll have to look into.  There have been other efforts to handle mostly file-based scientific data with versioning in both distributed (Dat <a href=\"https://blog.datproject.org/tag/science/\" rel=\"nofollow\">https://blog.datproject.org/tag/science/</a>) and centralized ways (DataHub <a href=\"https://datahub.csail.mit.edu/www/\" rel=\"nofollow\">https://datahub.csail.mit.edu/www/</a>).  Juan Benet visited our research center to give a talk about IPFS a few years ago. Really fantastic stuff.<p>I'm the creator of DVID (<a href=\"http://dvid.io\" rel=\"nofollow\">http://dvid.io</a>), which has an entirely different approach to how we might handle distributed versioning of scientific data primarily at a larger scale (100 GB to petabytes).  Like Qri and IPFS, DVID is written in Go.  Our research group works in Connectomics.  We start with massive 3D brain image volumes and apply automated and manual segmentation to mine the neurons and synapses of all that data.  There's also a lot of associated data to manage the production of connectomes.<p>One of our requirements, though, is having low-latency reads and writes to the data.  We decided to create a Science API that shields clients from how the data is actually represented, and for now, have used an ordered key-value stores for the backend.  Pluggable \"datatypes\" provide the Science API and also translate requests into the underlying key-value pairs, which are the units for versioning.  It's worked out pretty well for us and I'm now working on overhauling the store interface and improving the movement of versions between servers.  At our scale, it's useful to be able to mail a hard drive to a collaborator to establish the base DAG data and then let them eventually do a \"pull request\" for their relatively small modifications.<p>We've published some of our data online (<a href=\"http://emdata.janelia.org\" rel=\"nofollow\">http://emdata.janelia.org</a>) and visitors can actually browse through the 3d images using a Google-developed web app, Neuroglancer.  It's running on a relatively small VM so I imagine any significant HN traffic might crush it :/  We are still figuring out the best way to handle the public-facing side.<p>I think a lot of people are coming up with their own ideas about how to version scientific data, so maybe we should establish a meeting or workshop to discuss how some of these systems might interoperate?  The RDA (<a href=\"https://rd-alliance.org/\" rel=\"nofollow\">https://rd-alliance.org/</a>) \nhas been trying to establish working groups and standards, although they weren't really looking at distributed versioning a few years ago.  We need something like a Github for scientific data where papers can reference data at a particular commit and then offer improvements through pull requests.",
          "I really love the design and style qri! It is fun!<p>Can I ask why, for a git-style system, IPFS was chosen instead of GUN or SSB?<p>Certainly, images/files/etc. are better in IPFS than GUN or SSB.<p>But, you're gonna have a nightmare doing any git-style index/patch/object/etc. operations with it - both GUN & SSB's algorithms are meant to handle this type of stuff.<p>Did you guys do any analysis?"],
        "id":"387a40e7-84bd-4548-af5a-0d4dda9170fb",
        "url_text":"Qri CLI a dataset version control system built on the distributed web Website | Packages | Contribute | Issues | Docs | Download Welcome Question Answer \"I want to learn about Qri\" Read the official documentation \"I want to download Qri\" Download Qri or brew install qri-io/qri/qri \"I have a question\" Create an issue and use the label 'question' \"I found a bug\" Create an issue and use the label 'bug' \"I want to help build the Qri backend\" Read the Contributing guides \"I want to build Qri from source\" Build Qri from source qri is a global dataset version control system built on the distributed web Breaking that down: global so that if anyone, anywhere has published work with the same or similar datasets, you can discover it. Specific to datasets because data deserves purpose-built tools version control to keep data in sync, attributing all changes to authors On the distributed web to make all of the data published on qri simultaneously available, letting peers work on data together. If youre unfamiliar with version control, particularly the distributed kind, well you're probably viewing this document on github which is a version control system intended for code. Its underlying technology git popularized some magic sauce that has inspired a generation of programmers and popularized concepts at the heart of the distributed web. Qri is applying that family of concepts to four common data problems: Discovery Can I find data Im looking for? Trust Can I trust what Ive found? Friction Can I make this work with my other stuff? Sync How do I handle changes in data? Because qri is global and content-addressed, adding data to qri also checks the entire network to see if someone has added it before. Since qri is focused solely on datasets, it can provide meaningful search results. Every change on qri is associated with a peer, creating an audit-able trail you can use to quickly see what has changed and who has changed it. All datasets on qri are automatically described at the time of ingest using a flexible schema that makes data naturally inter-operate. Qri comes with tools to turn all datasets on the network into a JSON API with a single command. Finally, all changes in qri are tracked & synced. Building From Source To build qri you'll need the go programming language on your machine. $ git clone https://github.com/qri-io/qri $ cd qri $ make install If this is your first time building, this command will have a lot of output. That's good! Its means it's working :) It'll take a minute or two to build. After this is done, there will be a new binary qri in your ~/go/bin directory if using go modules, and $GOPATH/bin directory otherwise. You should be able to run: and see help output. Building on Windows To start, make sure that you have enabled Developer Mode. A library that we depend on needs it enabled in order to properly handle symlinks. If not done, you'll likely get the error message \"A required privilege is not held by the client\". You should not need to Run As Administrator to build or run qri. We do not recommend using administrator to run qri. Shell For your shell, we recommend using msys2. Other shells, such as cmd, Powershell, or cygwin may also be usable, but msys2 makes it easy to install our required dependencies. IPFS also recommends msys2, and qri is built on top of IPFS. Dependencies Building depends upon having git and make installed. If using msys2, you can easily install these by using the package manager \"pacman\". In a shell, type: Assuming you've also installed go using the official Windows installer linked above, you will also need to add go to your PATH by modifying your environment variable. See the next section on \"Environment variables\" for more information. Due to how msys2 treats the PATH variable, you also need to add a new environment variable MSYS2_PATH_TYPE, with the value inherit, using the same procedure. Once these steps are complete, proceed to building. Building on Rasberry PI On a Raspberry PI, you'll need to increase your swap file size in order to build. Normal desktop and server linux OSes should be fine to proceed to building. One symptom of having not enough swap space is the go install command producing an error message ending with: To increase your swapfile size, first turn off the swapfile: sudo dphys-swapfile swapoff Then edit /etc/dphys-swapfile as root and set CONF_SWAPSIZE to 1024. Finally turn on the swapfile again: sudo dphys-swapfile swapon Otherwise linux machines with reduced memory will have other ways to increase their swap file sizes. Check documentation for your particular machine. Packages Qri is comprised of many specialized packages. Below you will find a summary of each package. Package Go Docs Go Report Card Description api user accessible layer, primarily made for communication with our frontend webapp cmd our command line interface config user configuration details, includes peer's profile lib takes arguments from the cmd and api layer and forms proper requests to call to the action layer p2p the peer to peer communication layer of qri repo the repository: saving, removing, and storing datasets, profiles, and the config dataset the blueprint for a dataset, the atoms that make up qri registry the blueprint for a registry: the service that allows profiles to be unique and datasets to be searchable starlib the starlark standard library available for qri transform scripts qfs \"qri file sytem\" is Qri's file system abstraction for getting & storing data from different sources ioes package to handle in, out, and error streams: gives us better control of where we send output and errors jsonschema used to describe the structure of a dataset, so we can validate datasets and determine dataset interop Outside Libraries The following packages are not under Qri, but are important dependencies, so we display their latest versions for convenience. Package Version ipfs This documentation has been adapted from the Cycle.js documentation. ",
        "_version_":1718939831551131648},
      {
        "story_id":[19078281],
        "story_author":["anishathalye"],
        "story_descendants":[79],
        "story_score":[1028],
        "story_time":["2019-02-04T17:21:56Z"],
        "story_title":"MIT Hacker Tools: a lecture series on programmer tools",
        "search":["MIT Hacker Tools: a lecture series on programmer tools",
          "https://hacker-tools.github.io/",
          "This class has moved to https://missing.csail.mit.edu/. You should go there to see the newest version of the material. This site is being left up for archival purposes. You can see all the lectures from the IAP 2019 course here. ",
          "Hi all! We (@anishathalye, @jjgo, and @jonhoo) have long felt that while university CS classes are great at teaching specific topics, they often leave it to students to figure out a lot of the common knowledge about how to actually use your computer. And in particular, how to use it efficiently.<p>There’s just no class in the undergrad curriculum that teaches you how to become familiar with the system you’re working with! Students are expected to know about, or figure out, the shell, editors, remote access and file management, version control, debugging and profiling utilities, and all sorts of other useful tools on their own. Often times, they won’t even know that many of these tools exist, and instead do things in roundabout ways or simply be left frustrated about their development environment.<p>To help mitigate this, we decided to run this short lecture series at MIT during the January Independent Activities Period that we called “Hacker Tools” (in reference to “hacker culture”, not hacking computers). Our hope was that through this class, and the resulting lecture materials and videos, we might be able to bootstrap students’ knowledge about the tools that are available to them, which they can then put to use throughout their time at university, and beyond.<p>We’ve shared both the lecture notes and the recordings of the lectures in the hopes that people outside of MIT may also find these resources useful in making better use of their tools. If that turns out to be true, we’re also thinking of re-doing the videos in screen-cast style with live chat and a proper microphone when we get the time. If that sounds interesting to you, and if you have ideas about other things you’d like to see us cover, please leave a comment below; we’d love to hear from you!<p>We’re sure there are also plenty of cool tools that we didn’t get to cover in this series that you all know and love. Please share them below along with a short description so we can all learn something new!<p>Anish, Jose, and Jon",
          "The equivalent UCLA course is CS35L: Software Construction Laboratory (<a href=\"https://web.cs.ucla.edu/classes/winter19/cs35L/\" rel=\"nofollow\">https://web.cs.ucla.edu/classes/winter19/cs35L/</a>). It's taught by Paul Eggert (big open source/coreutils/emacs contributor + author of diff/sort)."],
        "story_type":["Normal"],
        "url":"https://hacker-tools.github.io/",
        "url_text":"This class has moved to https://missing.csail.mit.edu/. You should go there to see the newest version of the material. This site is being left up for archival purposes. You can see all the lectures from the IAP 2019 course here. ",
        "comments.comment_id":[19078338,
          19080440],
        "comments.comment_author":["Jonhoo",
          "bhchiang"],
        "comments.comment_descendants":[6,
          2],
        "comments.comment_time":["2019-02-04T17:25:35Z",
          "2019-02-04T20:44:16Z"],
        "comments.comment_text":["Hi all! We (@anishathalye, @jjgo, and @jonhoo) have long felt that while university CS classes are great at teaching specific topics, they often leave it to students to figure out a lot of the common knowledge about how to actually use your computer. And in particular, how to use it efficiently.<p>There’s just no class in the undergrad curriculum that teaches you how to become familiar with the system you’re working with! Students are expected to know about, or figure out, the shell, editors, remote access and file management, version control, debugging and profiling utilities, and all sorts of other useful tools on their own. Often times, they won’t even know that many of these tools exist, and instead do things in roundabout ways or simply be left frustrated about their development environment.<p>To help mitigate this, we decided to run this short lecture series at MIT during the January Independent Activities Period that we called “Hacker Tools” (in reference to “hacker culture”, not hacking computers). Our hope was that through this class, and the resulting lecture materials and videos, we might be able to bootstrap students’ knowledge about the tools that are available to them, which they can then put to use throughout their time at university, and beyond.<p>We’ve shared both the lecture notes and the recordings of the lectures in the hopes that people outside of MIT may also find these resources useful in making better use of their tools. If that turns out to be true, we’re also thinking of re-doing the videos in screen-cast style with live chat and a proper microphone when we get the time. If that sounds interesting to you, and if you have ideas about other things you’d like to see us cover, please leave a comment below; we’d love to hear from you!<p>We’re sure there are also plenty of cool tools that we didn’t get to cover in this series that you all know and love. Please share them below along with a short description so we can all learn something new!<p>Anish, Jose, and Jon",
          "The equivalent UCLA course is CS35L: Software Construction Laboratory (<a href=\"https://web.cs.ucla.edu/classes/winter19/cs35L/\" rel=\"nofollow\">https://web.cs.ucla.edu/classes/winter19/cs35L/</a>). It's taught by Paul Eggert (big open source/coreutils/emacs contributor + author of diff/sort)."],
        "id":"ae6aaa36-ef48-494c-abe3-2c37b55815e7",
        "_version_":1718939819379261440},
      {
        "story_id":[21158487],
        "story_author":["danicgross"],
        "story_descendants":[65],
        "story_score":[470],
        "story_time":["2019-10-04T15:15:12Z"],
        "story_title":"Streamlit: Turn a Python script into an interactive data analysis tool",
        "search":["Streamlit: Turn a Python script into an interactive data analysis tool",
          "https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace",
          "Introducing Streamlit, an app framework built for ML engineersCoding a semantic search engine with real-time neural-net inference in 300 lines of Python.In my experience, every nontrivial machine learning project is eventually stitched together with bug-ridden and unmaintainable internal tools. These tools often a patchwork of Jupyter Notebooks and Flask apps are difficult to deploy, require reasoning about client-server architecture, and dont integrate well with machine learning constructs like Tensorflow GPU sessions.I saw this first at Carnegie Mellon, then at Berkeley, Google X, and finally while building autonomous robots at Zoox. These tools were often born as little Jupyter notebooks: the sensor calibration tool, the simulation comparison app, the LIDAR alignment app, the scenario replay tool, and so on.As a tool grew in importance, project managers stepped in. Processes sprouted. Requirements flowered. These solo projects gestated into scripts, and matured into gangly maintenance nightmares.The machine learning engineers ad-hoc app building flow.When a tool became crucial, we called in the tools team. They wrote fluent Vue and React. They blinged their laptops with stickers about declarative frameworks. They had a design process:The tools teams clean-slate app building flow.Which was awesome. But these tools all needed new features, like weekly. And the tools team was supporting ten other projects. They would say, well update your tool again in two months.So we were back to building our own tools, deploying Flask apps, writing HTML, CSS, and JavaScript, and trying to version control everything from notebooks to stylesheets. So my old Google X friend, Thiago Teixeira, and I began thinking about the following question: What if we could make building tools as easy as writing Python scripts?We wanted machine learning engineers to be able to create beautiful apps without needing a tools team. These internal tools should arise as a natural byproduct of the ML workflow. Writing such tools should feel like training a neural net or performing an ad-hoc analysis in Jupyter! At the same time, we wanted to preserve all of the flexibility of a powerful app framework. We wanted to create beautiful, performant tools that engineers could show off. Basically, we wanted this:The Streamlit app building flow.With an amazing beta community including engineers from Uber, Twitter, Stitch Fix, and Dropbox, we worked for a year to create Streamlit, a completely free and open source app framework for ML engineers. With each prototype, the core principles of Streamlit became simpler and purer. They are:#1: Embrace Python scripting. Streamlit apps are really just scripts that run from top to bottom. Theres no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. For example, this is how you write to the screen:import streamlit as stst.write('Hello, world!')Nice to meet you.#2: Treat widgets as variables. There are no callbacks in Streamlit! Every interaction simply reruns the script from top to bottom. This approach leads to really clean code:import streamlit as stx = st.slider('x')st.write(x, 'squared is', x * x)An interactive Streamlit app in three lines of code.#3: Reuse data and computation. What if you download lots of data or perform complex computation? The key is to safely reuse information across runs. Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default, data store that lets Streamlit apps safely and effortlessly reuse information. For example, this code downloads data only once from the Udacity Self-driving car project, yielding a simple, fast app:Using st.cache to persist data across Streamlit runs. To run this code, please follow these instructions.The output of running the st.cache example above.In short, Streamlit works like this:The entire script is run from scratch for each user interaction.Streamlit assigns each variable an up-to-date value given widget states.Caching allows Streamlit to skip redundant data fetches and computation.Or in pictures:User events trigger Streamlit to rerun the script from scratch. Only the cache persists across runs.If this sounds intriguing, you can try it right now! Just run:$ pip install --upgrade streamlit $ streamlit hello You can now view your Streamlit app in your browser. Local URL: http://localhost:8501 Network URL: http://10.0.1.29:8501This will automatically pop open a web browser pointing to your local Streamlit app. If not, just click the link.To see more examples like this fractal animation, run streamlit hello from the command line.Ok. Are you back from playing with fractals? Those can be mesmerizing.The simplicity of these ideas does not prevent you from creating incredibly rich and useful apps with Streamlit. During my time at Zoox and Google X, I watched as self-driving car projects ballooned into gigabytes of visual data, which needed to be searched and understood, including running models on images to compare performance. Every self-driving car project Ive seen eventually has had entire teams working on this tooling.Building such a tool in Streamlit is easy. This Streamlit demo lets you perform semantic search across the entire Udacity self-driving car photo dataset, visualize human-annotated ground truth labels, and run a complete neural net (YOLO) in real time from within the app [1].This 300-line Streamlit demo combines semantic visual search with interactive neural net inference.The whole app is a completely self-contained, 300-line Python script, most of which is machine learning code. In fact, there are only 23 Streamlit calls in the whole app. You can run it yourself right now!$ pip install --upgrade streamlit opencv-python$ streamlit runhttps://raw.githubusercontent.com/streamlit/demo-self-driving/master/app.pyAs we worked with machine learning teams on their own projects, we came to realize that these simple ideas yield a number of important benefits:Streamlit apps are pure Python files. So you can use your favorite editor and debugger with Streamlit.My favorite layout for writing Streamlit apps has VSCode on the left and Chrome on the right.Pure Python scripts work seamlessly with Git and other source control software, including commits, pull requests, issues, and comments. Because Streamlits underlying language is pure Python, you get all the benefits of these amazing collaboration tools for free .Because Streamlit apps are just Python scripts, you can easily version control them with Git.Streamlit provides an immediate-mode live coding environment. Just click Always rerun when Streamlit detects a source file change.Click Always rerun to enable live coding.Caching simplifies setting up computation pipelines. Amazingly, chaining cached functions automatically creates efficient computation pipelines! Consider this code adapted from our Udacity demo:A simple computation pipeline in Streamlit. To run this code, please follow these instructions.Basically, the pipeline is load_metadata create_summary. Every time the script is run Streamlit only recomputes whatever subset of the pipeline is required to get the right answer. Cool!To make apps performant, Streamlit only recomputes whatever is necessary to update the UI.Streamlit is built for GPUs. Streamlit allows direct access to machine-level primitives like TensorFlow and PyTorch and complements these libraries. For example in this demo, Streamlits cache stores the entire NVIDIA celebrity face GAN [2]. This approach enables nearly instantaneous inference as the user updates sliders.This Streamlit app demonstrates NVIDIA celebrity face GAN [2] model using Shaobo Guans TL-GAN [3].Streamlit is a free and open-source library rather than a proprietary web app. You can serve Streamlit apps on-prem without contacting us. You can even run Streamlit locally on a laptop without an Internet connection! Furthermore, existing projects can adopt Streamlit incrementally.Several ways incrementally adopt Streamlit. (Icons courtesy of fullvector / Freepik.)This just scratches the surface of what you can do with Streamlit. One of the most exciting aspects of Streamlit is how these primitives can be easily composed into complex apps that look like scripts. Theres a lot more we could say about how our architecture works and the features we have planned, but well save that for future posts.Block diagram of Streamlits components. More coming soon!Were excited to finally share Streamlit with the community today and see what you all build with it. We hope that youll find it easy and delightful to turn your Python scripts into beautiful ML apps.Thanks to Amanda Kelly, Thiago Teixeira, TC Ricks, Seth Weidman, Regan Carey, Beverly Treuille, Genevive Wachtell, and Barney Pell for their helpful input on this article.References:[1] J. Redmon and A. Farhadi, YOLOv3: An Incremental Improvement (2018), arXiv.[2] T. Karras, T. Aila, S. Laine, and J. Lehtinen, Progressive Growing of GANs for Improved Quality, Stability, and Variation (2018), ICLR.[3] S. Guan, Controlled image synthesis and editing using a novel TL-GAN model (2018), Insight Data Science Blog. ",
          "This looks really slick, can't wait to try it out!<p>If anyone is curious about other tools in the same space, our data scientists use Dash[1] and plotly to build interactive exploration and visualization apps. We set up a Git repo that deploys their apps internally with every merge to master, so they're actually building and updating tools that our operations, marketing, etc teams use every day.<p>[1] <a href=\"https://plot.ly/dash/\" rel=\"nofollow\">https://plot.ly/dash/</a>",
          "Interesting project, but why does an open source developer tool needs browser telemetry?<p>You should ask for telemetry permissions _before_ the process starts up (as you do for email address), and keep the default as \"No\", instead of start to send the data transparently unless non user friendly steps are taken by the user."],
        "story_type":["Normal"],
        "url":"https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace",
        "comments.comment_id":[21169495,
          21170226],
        "comments.comment_author":["_gwlb",
          "random42"],
        "comments.comment_descendants":[2,
          0],
        "comments.comment_time":["2019-10-06T02:43:38Z",
          "2019-10-06T06:33:03Z"],
        "comments.comment_text":["This looks really slick, can't wait to try it out!<p>If anyone is curious about other tools in the same space, our data scientists use Dash[1] and plotly to build interactive exploration and visualization apps. We set up a Git repo that deploys their apps internally with every merge to master, so they're actually building and updating tools that our operations, marketing, etc teams use every day.<p>[1] <a href=\"https://plot.ly/dash/\" rel=\"nofollow\">https://plot.ly/dash/</a>",
          "Interesting project, but why does an open source developer tool needs browser telemetry?<p>You should ask for telemetry permissions _before_ the process starts up (as you do for email address), and keep the default as \"No\", instead of start to send the data transparently unless non user friendly steps are taken by the user."],
        "id":"97ad6304-f48f-4778-a87d-6302acf97c84",
        "url_text":"Introducing Streamlit, an app framework built for ML engineersCoding a semantic search engine with real-time neural-net inference in 300 lines of Python.In my experience, every nontrivial machine learning project is eventually stitched together with bug-ridden and unmaintainable internal tools. These tools often a patchwork of Jupyter Notebooks and Flask apps are difficult to deploy, require reasoning about client-server architecture, and dont integrate well with machine learning constructs like Tensorflow GPU sessions.I saw this first at Carnegie Mellon, then at Berkeley, Google X, and finally while building autonomous robots at Zoox. These tools were often born as little Jupyter notebooks: the sensor calibration tool, the simulation comparison app, the LIDAR alignment app, the scenario replay tool, and so on.As a tool grew in importance, project managers stepped in. Processes sprouted. Requirements flowered. These solo projects gestated into scripts, and matured into gangly maintenance nightmares.The machine learning engineers ad-hoc app building flow.When a tool became crucial, we called in the tools team. They wrote fluent Vue and React. They blinged their laptops with stickers about declarative frameworks. They had a design process:The tools teams clean-slate app building flow.Which was awesome. But these tools all needed new features, like weekly. And the tools team was supporting ten other projects. They would say, well update your tool again in two months.So we were back to building our own tools, deploying Flask apps, writing HTML, CSS, and JavaScript, and trying to version control everything from notebooks to stylesheets. So my old Google X friend, Thiago Teixeira, and I began thinking about the following question: What if we could make building tools as easy as writing Python scripts?We wanted machine learning engineers to be able to create beautiful apps without needing a tools team. These internal tools should arise as a natural byproduct of the ML workflow. Writing such tools should feel like training a neural net or performing an ad-hoc analysis in Jupyter! At the same time, we wanted to preserve all of the flexibility of a powerful app framework. We wanted to create beautiful, performant tools that engineers could show off. Basically, we wanted this:The Streamlit app building flow.With an amazing beta community including engineers from Uber, Twitter, Stitch Fix, and Dropbox, we worked for a year to create Streamlit, a completely free and open source app framework for ML engineers. With each prototype, the core principles of Streamlit became simpler and purer. They are:#1: Embrace Python scripting. Streamlit apps are really just scripts that run from top to bottom. Theres no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. For example, this is how you write to the screen:import streamlit as stst.write('Hello, world!')Nice to meet you.#2: Treat widgets as variables. There are no callbacks in Streamlit! Every interaction simply reruns the script from top to bottom. This approach leads to really clean code:import streamlit as stx = st.slider('x')st.write(x, 'squared is', x * x)An interactive Streamlit app in three lines of code.#3: Reuse data and computation. What if you download lots of data or perform complex computation? The key is to safely reuse information across runs. Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default, data store that lets Streamlit apps safely and effortlessly reuse information. For example, this code downloads data only once from the Udacity Self-driving car project, yielding a simple, fast app:Using st.cache to persist data across Streamlit runs. To run this code, please follow these instructions.The output of running the st.cache example above.In short, Streamlit works like this:The entire script is run from scratch for each user interaction.Streamlit assigns each variable an up-to-date value given widget states.Caching allows Streamlit to skip redundant data fetches and computation.Or in pictures:User events trigger Streamlit to rerun the script from scratch. Only the cache persists across runs.If this sounds intriguing, you can try it right now! Just run:$ pip install --upgrade streamlit $ streamlit hello You can now view your Streamlit app in your browser. Local URL: http://localhost:8501 Network URL: http://10.0.1.29:8501This will automatically pop open a web browser pointing to your local Streamlit app. If not, just click the link.To see more examples like this fractal animation, run streamlit hello from the command line.Ok. Are you back from playing with fractals? Those can be mesmerizing.The simplicity of these ideas does not prevent you from creating incredibly rich and useful apps with Streamlit. During my time at Zoox and Google X, I watched as self-driving car projects ballooned into gigabytes of visual data, which needed to be searched and understood, including running models on images to compare performance. Every self-driving car project Ive seen eventually has had entire teams working on this tooling.Building such a tool in Streamlit is easy. This Streamlit demo lets you perform semantic search across the entire Udacity self-driving car photo dataset, visualize human-annotated ground truth labels, and run a complete neural net (YOLO) in real time from within the app [1].This 300-line Streamlit demo combines semantic visual search with interactive neural net inference.The whole app is a completely self-contained, 300-line Python script, most of which is machine learning code. In fact, there are only 23 Streamlit calls in the whole app. You can run it yourself right now!$ pip install --upgrade streamlit opencv-python$ streamlit runhttps://raw.githubusercontent.com/streamlit/demo-self-driving/master/app.pyAs we worked with machine learning teams on their own projects, we came to realize that these simple ideas yield a number of important benefits:Streamlit apps are pure Python files. So you can use your favorite editor and debugger with Streamlit.My favorite layout for writing Streamlit apps has VSCode on the left and Chrome on the right.Pure Python scripts work seamlessly with Git and other source control software, including commits, pull requests, issues, and comments. Because Streamlits underlying language is pure Python, you get all the benefits of these amazing collaboration tools for free .Because Streamlit apps are just Python scripts, you can easily version control them with Git.Streamlit provides an immediate-mode live coding environment. Just click Always rerun when Streamlit detects a source file change.Click Always rerun to enable live coding.Caching simplifies setting up computation pipelines. Amazingly, chaining cached functions automatically creates efficient computation pipelines! Consider this code adapted from our Udacity demo:A simple computation pipeline in Streamlit. To run this code, please follow these instructions.Basically, the pipeline is load_metadata create_summary. Every time the script is run Streamlit only recomputes whatever subset of the pipeline is required to get the right answer. Cool!To make apps performant, Streamlit only recomputes whatever is necessary to update the UI.Streamlit is built for GPUs. Streamlit allows direct access to machine-level primitives like TensorFlow and PyTorch and complements these libraries. For example in this demo, Streamlits cache stores the entire NVIDIA celebrity face GAN [2]. This approach enables nearly instantaneous inference as the user updates sliders.This Streamlit app demonstrates NVIDIA celebrity face GAN [2] model using Shaobo Guans TL-GAN [3].Streamlit is a free and open-source library rather than a proprietary web app. You can serve Streamlit apps on-prem without contacting us. You can even run Streamlit locally on a laptop without an Internet connection! Furthermore, existing projects can adopt Streamlit incrementally.Several ways incrementally adopt Streamlit. (Icons courtesy of fullvector / Freepik.)This just scratches the surface of what you can do with Streamlit. One of the most exciting aspects of Streamlit is how these primitives can be easily composed into complex apps that look like scripts. Theres a lot more we could say about how our architecture works and the features we have planned, but well save that for future posts.Block diagram of Streamlits components. More coming soon!Were excited to finally share Streamlit with the community today and see what you all build with it. We hope that youll find it easy and delightful to turn your Python scripts into beautiful ML apps.Thanks to Amanda Kelly, Thiago Teixeira, TC Ricks, Seth Weidman, Regan Carey, Beverly Treuille, Genevive Wachtell, and Barney Pell for their helpful input on this article.References:[1] J. Redmon and A. Farhadi, YOLOv3: An Incremental Improvement (2018), arXiv.[2] T. Karras, T. Aila, S. Laine, and J. Lehtinen, Progressive Growing of GANs for Improved Quality, Stability, and Variation (2018), ICLR.[3] S. Guan, Controlled image synthesis and editing using a novel TL-GAN model (2018), Insight Data Science Blog. ",
        "_version_":1718939870777311233},
      {
        "story_id":[21585050],
        "story_author":["redm"],
        "story_descendants":[3],
        "story_score":[14],
        "story_time":["2019-11-20T16:06:12Z"],
        "story_title":"Launch HN: Fast.io, Simple Enterprise File Hosting Alternative to S3",
        "search":["Launch HN: Fast.io, Simple Enterprise File Hosting Alternative to S3",
          "The Fast.io team is excited to be officially launching today!<p>Our platform is a solution for designers, developers, and marketers looking to automate everything needed to host and track files and static websites at an enterprise scale.<p>We developed Fast.io because we recognized that the process of deploying static content today is complicated and time-consuming. Solutions like S3 require uploading, a separate CDN configuration, a manual review of raw data logs, and tedious cache flushing each time you make an update.<p>We want to get content online quickly with the simplicity and ease of use of cloud storage without sacrificing the reliable scalability and performance of a CDN.<p>Fast.io tightly integrates with your current workflows and preferred cloud storage service (Google Drive, Dropbox, OneDrive, MediaFire, Box, and GitHub) to manage files. It includes an integrated CDN, using Cloudflare and Akamai for lightning-fast global deploys, a visual dashboard, and detailed, accurate analytics data sent to Google Analytics and Mixpanel.<p>We’d love for you to check it out and we appreciate any and all feedback - https://www.producthunt.com/posts/fast-io<p>Here is an overview of what our simple enterprise file distribution network delivers for free:<p>- Continuous integration from cloud storage or version control<p>- Files up to 500MB each<p>- 100GB free transfer per month<p>- Foolproof Analytics collected directly on the CDN and reported to Google Analytics<p>- Custom domains and free included SSL (HTTPS)<p>- Automatic updates and global deployment from cloud storage or version control<p>- Automatic image optimization<p>- Automatic code minification<p>- Automatic directory listings<p>- Slack integration",
          "This is pretty useful! Putting blinders on and working with a project on my laptop and knowing the folder it's in is automagically getting synced with an actual CDN as well as everything being updated in real-time as I make changes??? Sounds pretty sweet. On my last project I wasted an entire damn day dealing with setting up GH-Pages and another project with Coudflair, a regular hosting company and a few tools to automate my workflow. I really like that all of this is out of the box in this product!",
          "\"Solutions like S3 require uploading, a separate CDN configuration, a manual review of raw data logs, and tedious cache flushing each time you make an update.\"<p>I love all types of automation that lets me avoid doing any of the above. The pricing plans are also very generous... Time to start playing with the free tier :)"],
        "story_text":"The Fast.io team is excited to be officially launching today!<p>Our platform is a solution for designers, developers, and marketers looking to automate everything needed to host and track files and static websites at an enterprise scale.<p>We developed Fast.io because we recognized that the process of deploying static content today is complicated and time-consuming. Solutions like S3 require uploading, a separate CDN configuration, a manual review of raw data logs, and tedious cache flushing each time you make an update.<p>We want to get content online quickly with the simplicity and ease of use of cloud storage without sacrificing the reliable scalability and performance of a CDN.<p>Fast.io tightly integrates with your current workflows and preferred cloud storage service (Google Drive, Dropbox, OneDrive, MediaFire, Box, and GitHub) to manage files. It includes an integrated CDN, using Cloudflare and Akamai for lightning-fast global deploys, a visual dashboard, and detailed, accurate analytics data sent to Google Analytics and Mixpanel.<p>We’d love for you to check it out and we appreciate any and all feedback - https://www.producthunt.com/posts/fast-io<p>Here is an overview of what our simple enterprise file distribution network delivers for free:<p>- Continuous integration from cloud storage or version control<p>- Files up to 500MB each<p>- 100GB free transfer per month<p>- Foolproof Analytics collected directly on the CDN and reported to Google Analytics<p>- Custom domains and free included SSL (HTTPS)<p>- Automatic updates and global deployment from cloud storage or version control<p>- Automatic image optimization<p>- Automatic code minification<p>- Automatic directory listings<p>- Slack integration",
        "story_type":["LanchHN"],
        "comments.comment_id":[21587894,
          21590425],
        "comments.comment_author":["telaport",
          "pineapplecake"],
        "comments.comment_descendants":[1,
          0],
        "comments.comment_time":["2019-11-20T20:05:41Z",
          "2019-11-21T00:57:36Z"],
        "comments.comment_text":["This is pretty useful! Putting blinders on and working with a project on my laptop and knowing the folder it's in is automagically getting synced with an actual CDN as well as everything being updated in real-time as I make changes??? Sounds pretty sweet. On my last project I wasted an entire damn day dealing with setting up GH-Pages and another project with Coudflair, a regular hosting company and a few tools to automate my workflow. I really like that all of this is out of the box in this product!",
          "\"Solutions like S3 require uploading, a separate CDN configuration, a manual review of raw data logs, and tedious cache flushing each time you make an update.\"<p>I love all types of automation that lets me avoid doing any of the above. The pricing plans are also very generous... Time to start playing with the free tier :)"],
        "id":"daa4651c-a984-4a11-b0ab-6d4b9a579e7e",
        "_version_":1718939881522069504},
      {
        "story_id":[21522522],
        "story_author":["mr_golyadkin"],
        "story_descendants":[78],
        "story_score":[277],
        "story_time":["2019-11-13T09:58:14Z"],
        "story_title":"Developing open-source FPGA tools",
        "search":["Developing open-source FPGA tools",
          "https://twitter.com/TinyFPGA/status/1177940755530207232",
          "Something went wrong, but dont fret lets give it another shot. ",
          "For those who are not familiar with silicon vendors' toolchain, it really is very poor quality especially when they started pushing for GUI based Graphical/System development.<p>Vendors make money selling silicon, they see the toolchain as a necessary evil.<p>There are 2018 tools that are still unable to fully support VHDL-2008 standard. Many bugs reported years again remain open. And often the GUI centric\n approach means you are left manually changing ad nauseam several GUI fields and ticking boxes, until you eventually determine what is the tcl 'equivalent' to at least try to ease your pain (tcl scripting often is another pain all together but arguably a lesser evil).<p>Also specifically with Xilinx they seem to have zero consideration for basic version control and create/duplicate/modify an explosion of files and cached versions of files.<p>Projects like GHDL and this are a breath of fresh are.",
          "I'm surprised no one has mentioned Icarus Verilog [0]. I used this extensively when I was in college around 2002. Open source, used to only support FreeBSD (my first BSD exposure), but is cross platform. It saved my ass, as I didnt have to compete for limited lab time on the few Solaris boxes with the proprietary tools we used in my VLSI class while studying EE.<p>I was sitting on my living room floor with my FreeBSD laptop punching out my project while others were waiting in line at 2am for lab time.<p>Fun times.<p>Wrote all of the components for a basic 8-bit ALU while watching with Mothman Propechies and sipping some Bourbon. Wrote a C++ program in that time, too, to generate exhaustive tests for all components. Icarus was fucking awesome for all this. As a commuter student my last 2 years, not having to hang out waiting for time was awesome. I got a lot more sleep, and a lot more done.<p>I've not kept up with its development, but its apparently now cross platform and still under active development. Any amateur interested in Verilog should definitely give it a look.<p>[0] <a href=\"http://iverilog.icarus.com\" rel=\"nofollow\">http://iverilog.icarus.com</a>"],
        "story_type":["Normal"],
        "url":"https://twitter.com/TinyFPGA/status/1177940755530207232",
        "url_text":"Something went wrong, but dont fret lets give it another shot. ",
        "comments.comment_id":[21523219,
          21531956],
        "comments.comment_author":["DoingIsLearning",
          "hermitdev"],
        "comments.comment_descendants":[8,
          1],
        "comments.comment_time":["2019-11-13T12:14:32Z",
          "2019-11-14T01:40:20Z"],
        "comments.comment_text":["For those who are not familiar with silicon vendors' toolchain, it really is very poor quality especially when they started pushing for GUI based Graphical/System development.<p>Vendors make money selling silicon, they see the toolchain as a necessary evil.<p>There are 2018 tools that are still unable to fully support VHDL-2008 standard. Many bugs reported years again remain open. And often the GUI centric\n approach means you are left manually changing ad nauseam several GUI fields and ticking boxes, until you eventually determine what is the tcl 'equivalent' to at least try to ease your pain (tcl scripting often is another pain all together but arguably a lesser evil).<p>Also specifically with Xilinx they seem to have zero consideration for basic version control and create/duplicate/modify an explosion of files and cached versions of files.<p>Projects like GHDL and this are a breath of fresh are.",
          "I'm surprised no one has mentioned Icarus Verilog [0]. I used this extensively when I was in college around 2002. Open source, used to only support FreeBSD (my first BSD exposure), but is cross platform. It saved my ass, as I didnt have to compete for limited lab time on the few Solaris boxes with the proprietary tools we used in my VLSI class while studying EE.<p>I was sitting on my living room floor with my FreeBSD laptop punching out my project while others were waiting in line at 2am for lab time.<p>Fun times.<p>Wrote all of the components for a basic 8-bit ALU while watching with Mothman Propechies and sipping some Bourbon. Wrote a C++ program in that time, too, to generate exhaustive tests for all components. Icarus was fucking awesome for all this. As a commuter student my last 2 years, not having to hang out waiting for time was awesome. I got a lot more sleep, and a lot more done.<p>I've not kept up with its development, but its apparently now cross platform and still under active development. Any amateur interested in Verilog should definitely give it a look.<p>[0] <a href=\"http://iverilog.icarus.com\" rel=\"nofollow\">http://iverilog.icarus.com</a>"],
        "id":"9f547ff8-92a1-46ce-bd75-32bbf61e3393",
        "_version_":1718939880285798400},
      {
        "story_id":[20069596],
        "story_author":["methusala8"],
        "story_descendants":[15],
        "story_score":[18],
        "story_time":["2019-06-01T15:20:33Z"],
        "story_title":"Ask HN:Skills/Tools to Stand Out as a Data Scientist",
        "search":["Ask HN:Skills/Tools to Stand Out as a Data Scientist",
          "Apart from Conventional tools like Python/R and Knowledge  of Machine Learning/Statistics/SQL are there any other skills that I can pick up in order to up skill myself as a Data Scientist?<p>I have nearly three years experience in this field and would like to level up. \nThanks.",
          "A provable ability to conduct Bayesian data analysis: from experiment design, to modelling, to evaluation and back again.<p>I think it’s what makes a “data scientist” legitimate.",
          "Depending on the role, data science is generally either [data analysis (with very little modelling) + business understanding + communication and presentation skills], OR it's [statistics + software development]. There can be some deviation and mixing between the two, but to help with the latter:<p>- linear algebra<p>- calculus<p>- software development - best practices, version control, design patterns etc."],
        "story_text":"Apart from Conventional tools like Python/R and Knowledge  of Machine Learning/Statistics/SQL are there any other skills that I can pick up in order to up skill myself as a Data Scientist?<p>I have nearly three years experience in this field and would like to level up. \nThanks.",
        "story_type":["AskHN"],
        "comments.comment_id":[20069867,
          20074208],
        "comments.comment_author":["usgroup",
          "vxpzx"],
        "comments.comment_descendants":[2,
          1],
        "comments.comment_time":["2019-06-01T16:14:12Z",
          "2019-06-02T08:18:27Z"],
        "comments.comment_text":["A provable ability to conduct Bayesian data analysis: from experiment design, to modelling, to evaluation and back again.<p>I think it’s what makes a “data scientist” legitimate.",
          "Depending on the role, data science is generally either [data analysis (with very little modelling) + business understanding + communication and presentation skills], OR it's [statistics + software development]. There can be some deviation and mixing between the two, but to help with the latter:<p>- linear algebra<p>- calculus<p>- software development - best practices, version control, design patterns etc."],
        "id":"bc228a85-48b7-4f23-968b-971a56049aa4",
        "_version_":1718939845625118720},
      {
        "story_id":[21196107],
        "story_author":["joebaf"],
        "story_descendants":[145],
        "story_score":[194],
        "story_time":["2019-10-08T19:25:11Z"],
        "story_title":"C++ Ecosystem: Compilers, IDEs, Tools, Testing",
        "search":["C++ Ecosystem: Compilers, IDEs, Tools, Testing",
          "https://www.bfilipek.com/2019/10/cppecosystem.html",
          "Table of Contents Introduction Compilers GCC Microsoft Visual C++ Clang Intel C++ Compiler Build Tools & Package Managers Make Cmake Ninja Microsoft Build Engine (MSBuild) Conan, Vcpkg, Buckaroo Integrated Development Environments Sublime Text, Atom, And Visual Studio Code Vi/Vim & Emacs Clion Qt Creator C++Builder Visual Studio Xcode KDevelop Eclipse CDT IDE Cevelop Android Studio Oracle Studio Extra: Compiler Explorer & Online Tools Debugging & Testing GDB LLDB Debugging Tools For Windows Mozillas RR CATCH/CATCH2 BOOST.TEST GOOGLE TEST CUTE DocTest Mull Sanitizers Valgrind HeapTrack Dr. Memory Deleaker Summary & More Your Turn To write a professional C++ application, you not only need a basic text editor and a compiler. You require some more tooling. In this blog post, youll see a broad list of tools that make C++ programming possible: compilers, IDEs, debuggers and other. Last Update: 14th October 2019. Note: This is a blog post based on the White Paper created by Embarcadero, see the full paper here: C++ Ecosystem White Paper. Introduction The C++ computer programming language has become one of the most widely used modern programming languages. Software built with C++ is known for its performance and efficiency. C++ has been used to build numerous vastly popular core libraries, applications such as Microsoft Office, game engines such as Unreal, software tools like Adobe Photoshop, compilers like Clang, databases like MySQL, and even operating systems such as Windows across a wide variety of platforms as it continues to evolve and grow. Modern C++ is generally defined as C++ code that utilizes language features in C++11, C++14, and C++17 based compilers. These are language standards named after the year they were defined (2011, 2014 and 2017 respectively) and include a number of significant additions and enhancements to the original core language for powerful, highly performant, and bug-free code. Modern C++ has high-level features that support object-oriented programming, functional programming, generic programming, and low-level memory manipulation features. Big names in the computer industry such as Microsoft, Intel, the Free Software Foundation, and others have their modern C++ compilers. Companies such as Microsoft, The QT Company, JetBrains, and Embarcadero provide integrated development environments for writing code in modern C++. Popular libraries are available for C++ across a wide range of computer disciplines including Artificial Intelligence, Machine Learning, Robotics, Math, Scientific Computing, Audio Processing, and Image Processing. In this blog post, we are going to cover a number of these compilers, build tools, IDEs, libraries, frameworks, coding assistants, and much more that can support and enhance your development with modern C++. Lets get started! Compilers There are a number of popular compilers that support modern C++ including GCC/g++, MSVC (Microsoft Visual C++), Clang and Intel Compiler. Each compiler has varying support for each of the major operating systems with the open-source GCC/g++ originating in the late 1980s, Microsofts Visual C++ in the early 1990s, and Clang in the late 2000s. All four compilers support modern C++ up to at least C++17, but the source code licenses for each of them vary greatly. GCC GCC is a general-use compiler developed and maintained and regularly updated by the GCC Steering committee as part of the GNU Project. GCC describes a large growing family of compilers targeting many hardware platforms and several languages. While it mainly targets Unix-like platforms, Windows support is provided through the Cygwin or MinGW runtime libraries. GCC compiles modern C++ code up to C++17 with experimental support for some C++20 features. It also compiles with a variety of language extensions that build upon C++ standards. It is free and open-source (GPL3) with the GCC Runtime Library Exception. GCC has support from build tools such as CMake and Ninja and many IDEs such as CLion, Qt Creator, and Visual Studio Code. https://gcc.gnu.org/ https://gcc.gnu.org/projects/cxx-status.html Microsoft Visual C++ Microsoft Visual C++ (MSVC) is Microsofts compiler for their custom implementation of the C++ standard, known as Visual C++. It is regularly updated, and like GCC and Clang, supports modern C++ standards up to C++17 with experimental support for some C++20 features. MSVC is the primary method for building C++ applications in Microsofts own Visual Studio. It generally targets a number of architectures on Windows, Android, iOS, and Linux. Support for build tools and IDEs are limited but growing. CMake extensions are available in Visual Studio 2019. MSVC can be used with Visual Studio Code, with limited support from CLion and Qt Creator with additional extensions. MSVC is proprietary and available under a commercial license, but theres also a Community edition. https://en.wikipedia.org/wiki/Microsoft_Visual_C%2B%2B https://devblogs.microsoft.com/visualstudio/ https://visualstudio.microsoft.com/vs/community/ Clang Clang describes a large family of compilers for the C family of languages maintained and regularly developed as part of the LLVM project. Although it targets many popular architectures, it generally targets fewer platforms than GCC. The LLVM project defines Clang through key design principles - strict adherence to C++ standards (although support for GCC extensions is offered), modular design, and minimal modification to the source codes structure during compilation, to name a few. Like GCC, Clang compiles modern C++ code with support for the C++17 standard with experimental C++20 support. It is available under an open-source (Apache License Version 2.0) license. Clang also has widespread support from build tools such as CMake and Ninja and IDEs such as CLion, Qt Creator, Xcode, and others. https://clang.llvm.org/ https://clang.llvm.org/cxx_status.html Intel C++ Compiler Intel C++ Compiler can generate highly optimized code for various Intel CPUs (including Xeon, Core, and Atom processors). The compiler can seamlessly integrate with popular IDE like Visual Studio, GCC toolchain and others. It can leverage advanced instruction set (even AVX512) and generate parallel code (for example, thanks to OpenMP 5.0 support). Intel doesnt ship the compiler with the Standard Library implementation, so it uses the library you provide on your platform. The compiler is available as a part of Intel Parallel Studio XE or Intel System Studio. https://software.intel.com/en-us/c-compilers https://software.intel.com/en-us/articles/c17-features-supported-by-intel-c-compiler On top of compilers, you need an infrastructure that helps to build a whole application: build tools, pipelines and package managers. Make Make is a well-known build system widely used, especially in Unix and Unix-like operating systems. Make is typically used to build executable programs and libraries from source code. But the tool applies to any process that involves executing arbitrary commands to transform a source file to a target result. Make is not tight to any particular programming language. It automatically determines which source files has been changed and then performs the minimal build process to get the final output. It also helps with the installation of the results in the system https://www.gnu.org/software/make/ Cmake CMake is a cross-platform tool for managing your build process. Building, especially large apps and with dependent libraries, can be a very complex process, especially when you support multiple compilers; CMake abstracts this. You can define complex build processes in one common language and convert them to native build directives for any number of supported compilers, IDEs, and build tools, including Ninja (below.) There are versions of CMake available for Windows, macOS, and Linux. https://cmake.org/ Note: Heres a good answer about the differences between Make and Cmake: Difference between using Makefile and CMake to compile the code - Stack Overflow. Ninja The Ninja build system is used for the actual process of building apps and is similar to Make. It focuses on running as fast as possible by parallelizing builds. It is commonly used paired with CMake, which supports creating build files for the Ninja build system. The feature set of Ninja is intentionally kept minimal because the focus is on speed. https://ninja-build.org/ Microsoft Build Engine (MSBuild) MSBuild is a command-line based built platform available from Microsoft under an open-source (MIT) license. It can be used to automate the process of compiling and deploying projects. It is available standalone, packaged with Visual Studio, or from Github. The structure and function of MSBuild files is very similar to Make. MSBuild has an XML based file format and mainly has support for Windows but also macOS and Linux. IDEs such as CLion and C++Builder can integrate with MSBuild as well. https://docs.microsoft.com/en-us/visualstudio/msbuild/msbuild Conan, Vcpkg, Buckaroo Package managers such as Conan, vcpkg, Buckaroo and NIX have been gaining popularity in the C++ community. A package manager is a tool to install libraries or components. Conan is a decentralized open-source (MIT) package manager that supports multiple platforms and all build systems (such as CMake and MSBuild). Conan supports binaries with a goal of automating dependency management to save time in development and continuous integration. Microsofts vcpkg is open source under an MIT license and supports Windows, macOS, and Linux. Out of the box, it makes installed libraries available in Visual Studio, but it also supports CMake build recipes. It can build libs for every toolchain that can be fitted into CMake. Buckaroo is a lesser-known open-source package manager that can pull dependencies from GitHub, BitBucket, GitLab, and others. Buckaroo offers integrations for a number of IDEs including CLion, Visual Studio Code, XCode, and others. Here are the links for the mentioned package managers: https://conan.io/ https://github.com/microsoft/vcpkg https://buckaroo.pm/ Integrated Development Environments A host of editors and integrated development environments (IDEs) can be used for developing with modern C++. Text editors are typically lightweight, but are less featureful than a full IDE and so are used only for the process of writing code, not debugging or testing it. Full development requires other tools, and an IDE contains those and integrates into a cohesive, integrated development environment. Any number of text editors like Sublime Text, Atom, Visual Studio Code, vi/vim, and Emacs can be used for writing C++ code. However, some IDEs are specifically designed with modern C++ in mind like CLion, Qt Creator, and C++Builder, while IDEs like Xcode and Visual Studio also support other languages. You can also compare various IDE for C++ in this handy table on Wikipedia: Comparison of integrated development environments - C++ - Wikipedia Sublime Text, Atom, And Visual Studio Code The list below summarises a set of advanced source code editors that thanks to various plugins and extensions allow creating applications in almost all programming languages. Sublime Text is a commercial text editor with extended support for modern C++ available via plugins. Atom is an open-source (MIT license) text editor that supports modern C++ via packages with integrations available for debugging and compiling. Visual Studio Code is a popular open-source (MIT license) source-code editor from Microsoft. A wide variety of extensions are available that bring features such as debugging and code completion for modern C++ to Visual Studio Code. Sublime Text, Atom, and Visual Studio Code are all available for Windows, macOS, and Linux. Here are the links for the above tools: https://www.sublimetext.com/ https://atom.io/ https://code.visualstudio.com/ Vi/Vim & Emacs Vi/Vim and Emacs are free command-line based text editors that are mainly used on Linux but are also available for macOS and Windows. Modern C++ support can be added to Vi/Vim through the use of scripts while modern C++ support can be added to Emacs through the use of modules. https://www.vim.org/ https://www.gnu.org/software/emacs/ Clion CLion is a commercial IDE from JetBrains that supports modern C++. It can be used with build tools like CMake and Gradle, integrates with the GDB and LLDB debuggers, can be used with version control systems like Git, test libraries like Boost.Test, and various documentation tools. It has features such as code generation, refactoring, on the fly code analysis, symbol navigation, and more. https://www.jetbrains.com/clion/ Qt Creator Qt Creator is a (non)commercial IDE from The Qt Company which supports Windows, macOS, and Linux. Qt Creator has features such as a UI designer, syntax highlighting, auto-completion, and integration with a number of different modern C++ compilers like GCC and Clang. Qt Creator tightly integrates with the Qt library for rapidly building cross-platform applications. Additionally, it integrates with standard version control systems like Git, debuggers like GDB and LLDB, build systems like CMake, and can deploy cross-platform to iOS and Android devices. https://www.qt.io/ C++Builder C++Builder is a commercial IDE from Embarcadero Technologies which runs on Windows and supports modern C++. It features the award-winning Visual Component Library (VCL) for Windows development and FireMonkey (FMX) for cross-platform development for Windows, iOS and Android. The C++Builder compiler features an enhanced version of Clang, an integrated debugger, visual UI designer, database library, comprehensive RTL, and standard features like syntax highlighting, code completion, and refactoring. C++Builder has integrations for CMake, can be used with Ninja, and also MSBuild. https://www.embarcadero.com/products/cbuilder https://www.embarcadero.com/products/cbuilder/starter Visual Studio Visual C++ is a commercial Visual Studio IDE from Microsoft. Visual Studio integrates building, debugging, and testing within the IDE. It provides the Microsoft Foundation Class (MFC) library which gives access to the Win32 APIs. Visual Studio features a visual UI designer for certain platforms, comes with MSBuild, supports CMake, and provides standard features such as code completion, refactoring, and syntax highlighting. Additionally, Visual Studio supports a number of other programming languages, and the C++ side of it is focused on Windows, with other platforms slowly being added. https://visualstudio.microsoft.com/ Xcode Xcode is a multi-language IDE from Apple available only on macOS that supports modern C++. Xcode is proprietary but available for free from Apple. Xcode has an integrated debugger, supports version control systems like Git, features a Clang compiler, and utilizes libc++ as its standard library. It supports standard features such as syntax highlighting, code completion, and finally, Xcode supports external build systems like CMake and utilizes the LLDB debugger. https://developer.apple.com/xcode/ KDevelop KDevelop (its 0.1 version was released in 1998) is a cross-platform IDE for C, C++, Python, QML/JavaScript and PHP. This IDE is part of the KDE project, and is based on KDE Frameworks and Qt. The C/C++ backend uses Clang and LLVM. It has UI integration with several version control systems: Git, SVN, Bazaar and more, build process based on CMake, QMake or custom makefiles. Among many interesting features, its essential to mention advanced syntax colouring and Context-sensitive, semantic code completion. https://www.kdevelop.org/ https://www.kdevelop.org/features Eclipse CDT IDE The Eclipse C/C++ Development Toolkit (CDT) is a combination of the Eclipse IDE with a C++ toolchain (usually GNU - GCC). This IDE supports project creation and build management for various toolchains, like the standard make build. CDT IDE offers source navigation, various source knowledge tools, such as type hierarchy, call graph, include browser, macro definition browser, code editor with syntax highlighting, folding and hyperlink navigation, source code refactoring and code generation, visual debugging tools, including memory, registers, and disassembly viewers. https://www.eclipse.org/cdt/ Cevelop Cevelop is a powerful IDE based Eclipse CDT. Its main strength lies in the powerful refactoring and static analysis support for code modernization. In addition, it comes with unit testing and TDD support for the CUTE unit testing framework. Whats more, you can easily visualize your template instantiation/function overload resolution and optimize includes. https://www.cevelop.com/ Android Studio Android Studio is the official IDE for Googles Android operating system, built on JetBrains IntelliJ IDEA software and designed specifically for Android development. It is available for download on Windows, macOS and Linux based operating systems. It is a replacement for the Eclipse Android Development Tools (ADT) as the primary IDE for native Android application development. Android Studio focuses mainly on Kotlin but you can also write applications in C++. Oracle Studio Oracle Developer Studio is Oracle Corporations flagship software development product for the Solaris and Linux operating systems. It includes optimizing C, C++, and Fortran compilers, libraries, and performance analysis and debugging tools, for Solaris on SPARC and x86 platforms, and Linux on x86/x64 platforms, including multi-core systems. You can download Developer Studio at no charge but if you want the full support and patch updates, then you need a paid support contract. The C++ Compiler supports C++14. https://www.oracle.com/technetwork/server-storage/developerstudio/overview/index.html https://www.oracle.com/technetwork/server-storage/solarisstudio/features/compilers-2332272.html If you want to check some shorter code samples and you dont want to install the whole compiler/.IDE suite then we have lots of online tools that can make those tests super simple. Just open a web browser and put the code Compiler Explorer is a web-based tool that allows you to select from a wide variety of C++ compilers and different versions of the same compiler to test out your code. This allows developers to compare the generated code for specific C++ constructs among compilers, and test for correct behaviour. Clang, GCC, and MSVC are all there but also lesser-known compilers such as DJGPP, ELLCC, Intel C++, and others. https://godbolt.org/ Extra: Heres a list of handy online compilers that you can use: like Coliru, Wandbox, CppInsighs and more: https://arnemertz.github.io/online-compilers/ Debugging & Testing GDB GDB is a portable command-line based debugging platform that supports modern C++ and is available under an open-source license (GPL). A number of editors and IDEs like Visual Studio, Qt Creator, and CLion support integration with GDB. It can also be used to debug applications remotely where GDB is running on one device, and the application being debugged is running on another device. It supports a number of platforms including Windows, macOS, and Linux. https://www.gnu.org/software/gdb/ LLDB LLDB is an open-source debugging interface that supports modern C++ and integrates with the Clang compiler. It has a number of optional performance-enhancing features such as JIT but also supports debugging memory, multiple threads, and machine code analysis. It is built in C++. LLDB is the default debugger for Xcode and can be used with Visual Studio Code, CLion, and Qt Creator. It supports a number of platforms including Windows, macOS, and Linux. https://lldb.llvm.org/ Debugging Tools For Windows On Windows, you can use several debuggers, ranging from Visual Studio Debugger (integrated and one of the most user-friendly), WinDBG, CDB and several more. WinDbg is a multipurpose debugger for the Microsoft Windows Platform. It can be used to debug user-mode applications, device drivers, and the operating system itself in kernel mode. It has a graphical user interface (GUI) and is more powerful than Visual Studio Debugger. You can debug memory dumps obtained even from kernel drivers. One of the recent exciting features in Debugging on Windows is called Time Travel Debugging (Available in WinDBG Preview and also in Visual Studio Ultimate). It allows you to record the execution of the process and then replay the steps backwards or forwards. This flexibility enables us to easily tracks back the state that caused a bug. https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/ https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/time-travel-debugging-overview Mozillas RR RR is an advanced debugger that aims to replace GDB on Linux. It offers the full state recordings of the application so that you can replay the action backwards and forwards (similarly to Time Travel Debugging). The debugger is used to work with large applications like Chrome, OpenOffice or even Firefox code bases. https://rr-project.org/ CATCH/CATCH2 Catch2 is a cross-platform open-source (BSL-1.0) testing framework for modern C++. It is very lightweight because only a header file needs to be included. Unit tests can be tagged and run in groups. It supports both test-driven development and behaviour-driven development. Catch2 also easily integrates with CLion. https://github.com/catchorg/Catch2 BOOST.TEST Boost.Test is a feature-rich open-source (BSL-1.0) testing framework that utilizes modern C++ standards. It can be used to quickly detect errors, failures, and time outs through customizable logging and real-time monitoring. Tests can be grouped into suites, and the framework supports both small scale testing and large scale testing. https://github.com/boostorg/test GOOGLE TEST Google Test is Googles C++ testing and mocking framework, which is available under an open-source (BSD) license. Google test can be used on a broad range of platforms, including Linux, macOS, Windows, and others. It contains a unit testing framework, assertions, death tests, detects failures, handles parameterized tests, and creates XML test reports. https://github.com/google/googletest CUTE CUTE is a unit testing framework integrated into Cevelop, but it can also be used standalone. It spans C++ versions from c++98 to c++2a and is header-only. While not as popular as Google Test it is less macro-ridden and uses macros only, where no appropriate C++ feature is available. In addition, it features a mode that easily allows it to run on embedded platforms, by sidestepping some of the I/O formatting features. https://cute-test.com/ DocTest DocTest is a single-header unit testing framework. Available for C++11 up to C++20 and is easy to configure and works on probably all platforms. It offers regular TDD testing macros (also with subcases) as well as BDD-style test cases. http://bit.ly/doctest-docs https://github.com/onqtam/doctest Mull Mull is an LLVM-based tool for Mutation Testing with a strong focus on C and C++ languages. In general, it creates many variations of the input source code (using LLVM bytecode) and then checks it against the test cases. Thanks to this advanced testing technique, you can make your code more secure. https://github.com/mull-project/mull PDF: https://lowlevelbits.org/pdfs/Mull_Mutation_2018.pdf Sanitizers AddressSanitizer - https://clang.llvm.org/docs/AddressSanitizer.html (supported in Clang, GCC and XCode) UndefinedBehaviorSanitizer - https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html LeakSanitizer - https://clang.llvm.org/docs/LeakSanitizer.html Application Verifier for Windows - https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/application-verifier Sanitizers are relatively new tools that add extra instrumentation to your application (for example they replace new/malloc/delete calls) and can detect various runtime errors: leaks, use after delete, double free and many others. To improve your build pipeline, many guides advice to add sanitizers steps when doing tests. Most sanitizers come from the LLVM/Clang platform, but now they also work with GCC. Unfortunately not yet with Visual Studio (but you can try Application Verifier). Valgrind Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. When you run a program through Valgrind its run on a virtual machine that emulates your host environment. Having that abstraction the tools can leverage various information about the source code and its execution. http://valgrind.org/ http://valgrind.org/info/about.html http://valgrind.org/docs/manual/quick-start.html HeapTrack HeapTrack is a FOSS project and a heap memory profiler for Linux. It traces all memory allocations and annotates these events with stack traces. The tool has two forms the command line version that grabs the data, and then the UI part that you can use to read and analyze the results. This tool is comparable to Valgrinds massif; its easier to use and should be faster to load and analyze for large projects. https://github.com/KDE/heaptrack Dr. Memory Dr. Memory is an LGPL licenced tool that allows you to monitor and intensify memory -related errors for binaries on Windows, Linux, Mac, Android. Its based on the DynamoRIO dynamic instrumentation tool platform. With the tool, you can find errors like double frees, memory leaks, handle leaks (on Windows), GDI issues, access to uninitialized memory or even errors in multithreading memory scenarios. http://drmemory.org/ https://github.com/DynamoRIO/drmemory Deleaker The primary role of Deleaker is to find leaks in your native applications. It supports Visual Studio (since 2008 till the latest 2019 version), Delphi/C++ Builder, Qt Creator, CLion (soon!). Can be used as an extension in Visual Studio or as a standalone application. Deleaker tracks leaks in C/C++ applications (Native and CLR), plus .NET code. Memory (new/delete, malloc), GDI objects, User32 objects, Handles, File views, Fibres, Critical Sections, and even more. It gathers full call stack, ability to take snapshots, compare them, view source files related to allocation. https://www.deleaker.com/ https://www.deleaker.com/docs/deleaker/tutorial.html Summary & More I hope that with the above list, you get a useful overview of the tools that are essential for C++ development. If you want to read more about other ecosystem elements: libraries, frameworks, and other tools, then please see the full report from Embarcadero: C++ Ecosystem White Paper (Its a nice looking pdf, with more than 20 pages of content!) You might check this Resource for a super long list of tools, libs, frameworks that enhance C++ development: https://github.com/fffaraz/awesome-cpp Your Turn What are your favourite tools that you use when writing C++ apps? ",
          "CLion is an amazing tool -- I've purchased licenses for my personal self in the past, but my employer pays for it these days.<p>My problem is they've done a terrible job of making it scale up to large code bases. I work on the chromium tree -- CLion is completely useless on it.  I have a dual 24-core xeon with 128GB of RAM and SSD and I've given it a wackload of memory, and it becomes completely inoperable, freezing all over the place.<p>Awful because I have such muscle memory for the JetBrains tools, and such a fondness for them.<p>I've gone back to using Emacs, but now with Eclim. I just couldn't get into VSCode.",
          "I had to use Qt as the UI lib for a project, it made me discover that QtCreator was actually not a Qt-only tool but a very good lightweight and generic C++ IDE.<p>That's my choice now. I need something that can navigate easily in a code base, I don't really like learning all the oddities around emacs and vim (even though I am a bit competent at vim) and I don't see what is so bad in using a mouse.<p>At first I thought annoying to have to manually edit the .includes and .config to add the includes and the macro I needed in our complex, hard-to-parse CMake based project, but now I really enjoy the freedom it gives."],
        "story_type":["Normal"],
        "url":"https://www.bfilipek.com/2019/10/cppecosystem.html",
        "comments.comment_id":[21199733,
          21201412],
        "comments.comment_author":["cmrdporcupine",
          "Iv"],
        "comments.comment_descendants":[9,
          0],
        "comments.comment_time":["2019-10-09T03:27:23Z",
          "2019-10-09T09:10:25Z"],
        "comments.comment_text":["CLion is an amazing tool -- I've purchased licenses for my personal self in the past, but my employer pays for it these days.<p>My problem is they've done a terrible job of making it scale up to large code bases. I work on the chromium tree -- CLion is completely useless on it.  I have a dual 24-core xeon with 128GB of RAM and SSD and I've given it a wackload of memory, and it becomes completely inoperable, freezing all over the place.<p>Awful because I have such muscle memory for the JetBrains tools, and such a fondness for them.<p>I've gone back to using Emacs, but now with Eclim. I just couldn't get into VSCode.",
          "I had to use Qt as the UI lib for a project, it made me discover that QtCreator was actually not a Qt-only tool but a very good lightweight and generic C++ IDE.<p>That's my choice now. I need something that can navigate easily in a code base, I don't really like learning all the oddities around emacs and vim (even though I am a bit competent at vim) and I don't see what is so bad in using a mouse.<p>At first I thought annoying to have to manually edit the .includes and .config to add the includes and the macro I needed in our complex, hard-to-parse CMake based project, but now I really enjoy the freedom it gives."],
        "id":"ae81c4cf-2356-4c7e-9d79-9ba58794b222",
        "url_text":"Table of Contents Introduction Compilers GCC Microsoft Visual C++ Clang Intel C++ Compiler Build Tools & Package Managers Make Cmake Ninja Microsoft Build Engine (MSBuild) Conan, Vcpkg, Buckaroo Integrated Development Environments Sublime Text, Atom, And Visual Studio Code Vi/Vim & Emacs Clion Qt Creator C++Builder Visual Studio Xcode KDevelop Eclipse CDT IDE Cevelop Android Studio Oracle Studio Extra: Compiler Explorer & Online Tools Debugging & Testing GDB LLDB Debugging Tools For Windows Mozillas RR CATCH/CATCH2 BOOST.TEST GOOGLE TEST CUTE DocTest Mull Sanitizers Valgrind HeapTrack Dr. Memory Deleaker Summary & More Your Turn To write a professional C++ application, you not only need a basic text editor and a compiler. You require some more tooling. In this blog post, youll see a broad list of tools that make C++ programming possible: compilers, IDEs, debuggers and other. Last Update: 14th October 2019. Note: This is a blog post based on the White Paper created by Embarcadero, see the full paper here: C++ Ecosystem White Paper. Introduction The C++ computer programming language has become one of the most widely used modern programming languages. Software built with C++ is known for its performance and efficiency. C++ has been used to build numerous vastly popular core libraries, applications such as Microsoft Office, game engines such as Unreal, software tools like Adobe Photoshop, compilers like Clang, databases like MySQL, and even operating systems such as Windows across a wide variety of platforms as it continues to evolve and grow. Modern C++ is generally defined as C++ code that utilizes language features in C++11, C++14, and C++17 based compilers. These are language standards named after the year they were defined (2011, 2014 and 2017 respectively) and include a number of significant additions and enhancements to the original core language for powerful, highly performant, and bug-free code. Modern C++ has high-level features that support object-oriented programming, functional programming, generic programming, and low-level memory manipulation features. Big names in the computer industry such as Microsoft, Intel, the Free Software Foundation, and others have their modern C++ compilers. Companies such as Microsoft, The QT Company, JetBrains, and Embarcadero provide integrated development environments for writing code in modern C++. Popular libraries are available for C++ across a wide range of computer disciplines including Artificial Intelligence, Machine Learning, Robotics, Math, Scientific Computing, Audio Processing, and Image Processing. In this blog post, we are going to cover a number of these compilers, build tools, IDEs, libraries, frameworks, coding assistants, and much more that can support and enhance your development with modern C++. Lets get started! Compilers There are a number of popular compilers that support modern C++ including GCC/g++, MSVC (Microsoft Visual C++), Clang and Intel Compiler. Each compiler has varying support for each of the major operating systems with the open-source GCC/g++ originating in the late 1980s, Microsofts Visual C++ in the early 1990s, and Clang in the late 2000s. All four compilers support modern C++ up to at least C++17, but the source code licenses for each of them vary greatly. GCC GCC is a general-use compiler developed and maintained and regularly updated by the GCC Steering committee as part of the GNU Project. GCC describes a large growing family of compilers targeting many hardware platforms and several languages. While it mainly targets Unix-like platforms, Windows support is provided through the Cygwin or MinGW runtime libraries. GCC compiles modern C++ code up to C++17 with experimental support for some C++20 features. It also compiles with a variety of language extensions that build upon C++ standards. It is free and open-source (GPL3) with the GCC Runtime Library Exception. GCC has support from build tools such as CMake and Ninja and many IDEs such as CLion, Qt Creator, and Visual Studio Code. https://gcc.gnu.org/ https://gcc.gnu.org/projects/cxx-status.html Microsoft Visual C++ Microsoft Visual C++ (MSVC) is Microsofts compiler for their custom implementation of the C++ standard, known as Visual C++. It is regularly updated, and like GCC and Clang, supports modern C++ standards up to C++17 with experimental support for some C++20 features. MSVC is the primary method for building C++ applications in Microsofts own Visual Studio. It generally targets a number of architectures on Windows, Android, iOS, and Linux. Support for build tools and IDEs are limited but growing. CMake extensions are available in Visual Studio 2019. MSVC can be used with Visual Studio Code, with limited support from CLion and Qt Creator with additional extensions. MSVC is proprietary and available under a commercial license, but theres also a Community edition. https://en.wikipedia.org/wiki/Microsoft_Visual_C%2B%2B https://devblogs.microsoft.com/visualstudio/ https://visualstudio.microsoft.com/vs/community/ Clang Clang describes a large family of compilers for the C family of languages maintained and regularly developed as part of the LLVM project. Although it targets many popular architectures, it generally targets fewer platforms than GCC. The LLVM project defines Clang through key design principles - strict adherence to C++ standards (although support for GCC extensions is offered), modular design, and minimal modification to the source codes structure during compilation, to name a few. Like GCC, Clang compiles modern C++ code with support for the C++17 standard with experimental C++20 support. It is available under an open-source (Apache License Version 2.0) license. Clang also has widespread support from build tools such as CMake and Ninja and IDEs such as CLion, Qt Creator, Xcode, and others. https://clang.llvm.org/ https://clang.llvm.org/cxx_status.html Intel C++ Compiler Intel C++ Compiler can generate highly optimized code for various Intel CPUs (including Xeon, Core, and Atom processors). The compiler can seamlessly integrate with popular IDE like Visual Studio, GCC toolchain and others. It can leverage advanced instruction set (even AVX512) and generate parallel code (for example, thanks to OpenMP 5.0 support). Intel doesnt ship the compiler with the Standard Library implementation, so it uses the library you provide on your platform. The compiler is available as a part of Intel Parallel Studio XE or Intel System Studio. https://software.intel.com/en-us/c-compilers https://software.intel.com/en-us/articles/c17-features-supported-by-intel-c-compiler On top of compilers, you need an infrastructure that helps to build a whole application: build tools, pipelines and package managers. Make Make is a well-known build system widely used, especially in Unix and Unix-like operating systems. Make is typically used to build executable programs and libraries from source code. But the tool applies to any process that involves executing arbitrary commands to transform a source file to a target result. Make is not tight to any particular programming language. It automatically determines which source files has been changed and then performs the minimal build process to get the final output. It also helps with the installation of the results in the system https://www.gnu.org/software/make/ Cmake CMake is a cross-platform tool for managing your build process. Building, especially large apps and with dependent libraries, can be a very complex process, especially when you support multiple compilers; CMake abstracts this. You can define complex build processes in one common language and convert them to native build directives for any number of supported compilers, IDEs, and build tools, including Ninja (below.) There are versions of CMake available for Windows, macOS, and Linux. https://cmake.org/ Note: Heres a good answer about the differences between Make and Cmake: Difference between using Makefile and CMake to compile the code - Stack Overflow. Ninja The Ninja build system is used for the actual process of building apps and is similar to Make. It focuses on running as fast as possible by parallelizing builds. It is commonly used paired with CMake, which supports creating build files for the Ninja build system. The feature set of Ninja is intentionally kept minimal because the focus is on speed. https://ninja-build.org/ Microsoft Build Engine (MSBuild) MSBuild is a command-line based built platform available from Microsoft under an open-source (MIT) license. It can be used to automate the process of compiling and deploying projects. It is available standalone, packaged with Visual Studio, or from Github. The structure and function of MSBuild files is very similar to Make. MSBuild has an XML based file format and mainly has support for Windows but also macOS and Linux. IDEs such as CLion and C++Builder can integrate with MSBuild as well. https://docs.microsoft.com/en-us/visualstudio/msbuild/msbuild Conan, Vcpkg, Buckaroo Package managers such as Conan, vcpkg, Buckaroo and NIX have been gaining popularity in the C++ community. A package manager is a tool to install libraries or components. Conan is a decentralized open-source (MIT) package manager that supports multiple platforms and all build systems (such as CMake and MSBuild). Conan supports binaries with a goal of automating dependency management to save time in development and continuous integration. Microsofts vcpkg is open source under an MIT license and supports Windows, macOS, and Linux. Out of the box, it makes installed libraries available in Visual Studio, but it also supports CMake build recipes. It can build libs for every toolchain that can be fitted into CMake. Buckaroo is a lesser-known open-source package manager that can pull dependencies from GitHub, BitBucket, GitLab, and others. Buckaroo offers integrations for a number of IDEs including CLion, Visual Studio Code, XCode, and others. Here are the links for the mentioned package managers: https://conan.io/ https://github.com/microsoft/vcpkg https://buckaroo.pm/ Integrated Development Environments A host of editors and integrated development environments (IDEs) can be used for developing with modern C++. Text editors are typically lightweight, but are less featureful than a full IDE and so are used only for the process of writing code, not debugging or testing it. Full development requires other tools, and an IDE contains those and integrates into a cohesive, integrated development environment. Any number of text editors like Sublime Text, Atom, Visual Studio Code, vi/vim, and Emacs can be used for writing C++ code. However, some IDEs are specifically designed with modern C++ in mind like CLion, Qt Creator, and C++Builder, while IDEs like Xcode and Visual Studio also support other languages. You can also compare various IDE for C++ in this handy table on Wikipedia: Comparison of integrated development environments - C++ - Wikipedia Sublime Text, Atom, And Visual Studio Code The list below summarises a set of advanced source code editors that thanks to various plugins and extensions allow creating applications in almost all programming languages. Sublime Text is a commercial text editor with extended support for modern C++ available via plugins. Atom is an open-source (MIT license) text editor that supports modern C++ via packages with integrations available for debugging and compiling. Visual Studio Code is a popular open-source (MIT license) source-code editor from Microsoft. A wide variety of extensions are available that bring features such as debugging and code completion for modern C++ to Visual Studio Code. Sublime Text, Atom, and Visual Studio Code are all available for Windows, macOS, and Linux. Here are the links for the above tools: https://www.sublimetext.com/ https://atom.io/ https://code.visualstudio.com/ Vi/Vim & Emacs Vi/Vim and Emacs are free command-line based text editors that are mainly used on Linux but are also available for macOS and Windows. Modern C++ support can be added to Vi/Vim through the use of scripts while modern C++ support can be added to Emacs through the use of modules. https://www.vim.org/ https://www.gnu.org/software/emacs/ Clion CLion is a commercial IDE from JetBrains that supports modern C++. It can be used with build tools like CMake and Gradle, integrates with the GDB and LLDB debuggers, can be used with version control systems like Git, test libraries like Boost.Test, and various documentation tools. It has features such as code generation, refactoring, on the fly code analysis, symbol navigation, and more. https://www.jetbrains.com/clion/ Qt Creator Qt Creator is a (non)commercial IDE from The Qt Company which supports Windows, macOS, and Linux. Qt Creator has features such as a UI designer, syntax highlighting, auto-completion, and integration with a number of different modern C++ compilers like GCC and Clang. Qt Creator tightly integrates with the Qt library for rapidly building cross-platform applications. Additionally, it integrates with standard version control systems like Git, debuggers like GDB and LLDB, build systems like CMake, and can deploy cross-platform to iOS and Android devices. https://www.qt.io/ C++Builder C++Builder is a commercial IDE from Embarcadero Technologies which runs on Windows and supports modern C++. It features the award-winning Visual Component Library (VCL) for Windows development and FireMonkey (FMX) for cross-platform development for Windows, iOS and Android. The C++Builder compiler features an enhanced version of Clang, an integrated debugger, visual UI designer, database library, comprehensive RTL, and standard features like syntax highlighting, code completion, and refactoring. C++Builder has integrations for CMake, can be used with Ninja, and also MSBuild. https://www.embarcadero.com/products/cbuilder https://www.embarcadero.com/products/cbuilder/starter Visual Studio Visual C++ is a commercial Visual Studio IDE from Microsoft. Visual Studio integrates building, debugging, and testing within the IDE. It provides the Microsoft Foundation Class (MFC) library which gives access to the Win32 APIs. Visual Studio features a visual UI designer for certain platforms, comes with MSBuild, supports CMake, and provides standard features such as code completion, refactoring, and syntax highlighting. Additionally, Visual Studio supports a number of other programming languages, and the C++ side of it is focused on Windows, with other platforms slowly being added. https://visualstudio.microsoft.com/ Xcode Xcode is a multi-language IDE from Apple available only on macOS that supports modern C++. Xcode is proprietary but available for free from Apple. Xcode has an integrated debugger, supports version control systems like Git, features a Clang compiler, and utilizes libc++ as its standard library. It supports standard features such as syntax highlighting, code completion, and finally, Xcode supports external build systems like CMake and utilizes the LLDB debugger. https://developer.apple.com/xcode/ KDevelop KDevelop (its 0.1 version was released in 1998) is a cross-platform IDE for C, C++, Python, QML/JavaScript and PHP. This IDE is part of the KDE project, and is based on KDE Frameworks and Qt. The C/C++ backend uses Clang and LLVM. It has UI integration with several version control systems: Git, SVN, Bazaar and more, build process based on CMake, QMake or custom makefiles. Among many interesting features, its essential to mention advanced syntax colouring and Context-sensitive, semantic code completion. https://www.kdevelop.org/ https://www.kdevelop.org/features Eclipse CDT IDE The Eclipse C/C++ Development Toolkit (CDT) is a combination of the Eclipse IDE with a C++ toolchain (usually GNU - GCC). This IDE supports project creation and build management for various toolchains, like the standard make build. CDT IDE offers source navigation, various source knowledge tools, such as type hierarchy, call graph, include browser, macro definition browser, code editor with syntax highlighting, folding and hyperlink navigation, source code refactoring and code generation, visual debugging tools, including memory, registers, and disassembly viewers. https://www.eclipse.org/cdt/ Cevelop Cevelop is a powerful IDE based Eclipse CDT. Its main strength lies in the powerful refactoring and static analysis support for code modernization. In addition, it comes with unit testing and TDD support for the CUTE unit testing framework. Whats more, you can easily visualize your template instantiation/function overload resolution and optimize includes. https://www.cevelop.com/ Android Studio Android Studio is the official IDE for Googles Android operating system, built on JetBrains IntelliJ IDEA software and designed specifically for Android development. It is available for download on Windows, macOS and Linux based operating systems. It is a replacement for the Eclipse Android Development Tools (ADT) as the primary IDE for native Android application development. Android Studio focuses mainly on Kotlin but you can also write applications in C++. Oracle Studio Oracle Developer Studio is Oracle Corporations flagship software development product for the Solaris and Linux operating systems. It includes optimizing C, C++, and Fortran compilers, libraries, and performance analysis and debugging tools, for Solaris on SPARC and x86 platforms, and Linux on x86/x64 platforms, including multi-core systems. You can download Developer Studio at no charge but if you want the full support and patch updates, then you need a paid support contract. The C++ Compiler supports C++14. https://www.oracle.com/technetwork/server-storage/developerstudio/overview/index.html https://www.oracle.com/technetwork/server-storage/solarisstudio/features/compilers-2332272.html If you want to check some shorter code samples and you dont want to install the whole compiler/.IDE suite then we have lots of online tools that can make those tests super simple. Just open a web browser and put the code Compiler Explorer is a web-based tool that allows you to select from a wide variety of C++ compilers and different versions of the same compiler to test out your code. This allows developers to compare the generated code for specific C++ constructs among compilers, and test for correct behaviour. Clang, GCC, and MSVC are all there but also lesser-known compilers such as DJGPP, ELLCC, Intel C++, and others. https://godbolt.org/ Extra: Heres a list of handy online compilers that you can use: like Coliru, Wandbox, CppInsighs and more: https://arnemertz.github.io/online-compilers/ Debugging & Testing GDB GDB is a portable command-line based debugging platform that supports modern C++ and is available under an open-source license (GPL). A number of editors and IDEs like Visual Studio, Qt Creator, and CLion support integration with GDB. It can also be used to debug applications remotely where GDB is running on one device, and the application being debugged is running on another device. It supports a number of platforms including Windows, macOS, and Linux. https://www.gnu.org/software/gdb/ LLDB LLDB is an open-source debugging interface that supports modern C++ and integrates with the Clang compiler. It has a number of optional performance-enhancing features such as JIT but also supports debugging memory, multiple threads, and machine code analysis. It is built in C++. LLDB is the default debugger for Xcode and can be used with Visual Studio Code, CLion, and Qt Creator. It supports a number of platforms including Windows, macOS, and Linux. https://lldb.llvm.org/ Debugging Tools For Windows On Windows, you can use several debuggers, ranging from Visual Studio Debugger (integrated and one of the most user-friendly), WinDBG, CDB and several more. WinDbg is a multipurpose debugger for the Microsoft Windows Platform. It can be used to debug user-mode applications, device drivers, and the operating system itself in kernel mode. It has a graphical user interface (GUI) and is more powerful than Visual Studio Debugger. You can debug memory dumps obtained even from kernel drivers. One of the recent exciting features in Debugging on Windows is called Time Travel Debugging (Available in WinDBG Preview and also in Visual Studio Ultimate). It allows you to record the execution of the process and then replay the steps backwards or forwards. This flexibility enables us to easily tracks back the state that caused a bug. https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/ https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/time-travel-debugging-overview Mozillas RR RR is an advanced debugger that aims to replace GDB on Linux. It offers the full state recordings of the application so that you can replay the action backwards and forwards (similarly to Time Travel Debugging). The debugger is used to work with large applications like Chrome, OpenOffice or even Firefox code bases. https://rr-project.org/ CATCH/CATCH2 Catch2 is a cross-platform open-source (BSL-1.0) testing framework for modern C++. It is very lightweight because only a header file needs to be included. Unit tests can be tagged and run in groups. It supports both test-driven development and behaviour-driven development. Catch2 also easily integrates with CLion. https://github.com/catchorg/Catch2 BOOST.TEST Boost.Test is a feature-rich open-source (BSL-1.0) testing framework that utilizes modern C++ standards. It can be used to quickly detect errors, failures, and time outs through customizable logging and real-time monitoring. Tests can be grouped into suites, and the framework supports both small scale testing and large scale testing. https://github.com/boostorg/test GOOGLE TEST Google Test is Googles C++ testing and mocking framework, which is available under an open-source (BSD) license. Google test can be used on a broad range of platforms, including Linux, macOS, Windows, and others. It contains a unit testing framework, assertions, death tests, detects failures, handles parameterized tests, and creates XML test reports. https://github.com/google/googletest CUTE CUTE is a unit testing framework integrated into Cevelop, but it can also be used standalone. It spans C++ versions from c++98 to c++2a and is header-only. While not as popular as Google Test it is less macro-ridden and uses macros only, where no appropriate C++ feature is available. In addition, it features a mode that easily allows it to run on embedded platforms, by sidestepping some of the I/O formatting features. https://cute-test.com/ DocTest DocTest is a single-header unit testing framework. Available for C++11 up to C++20 and is easy to configure and works on probably all platforms. It offers regular TDD testing macros (also with subcases) as well as BDD-style test cases. http://bit.ly/doctest-docs https://github.com/onqtam/doctest Mull Mull is an LLVM-based tool for Mutation Testing with a strong focus on C and C++ languages. In general, it creates many variations of the input source code (using LLVM bytecode) and then checks it against the test cases. Thanks to this advanced testing technique, you can make your code more secure. https://github.com/mull-project/mull PDF: https://lowlevelbits.org/pdfs/Mull_Mutation_2018.pdf Sanitizers AddressSanitizer - https://clang.llvm.org/docs/AddressSanitizer.html (supported in Clang, GCC and XCode) UndefinedBehaviorSanitizer - https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html LeakSanitizer - https://clang.llvm.org/docs/LeakSanitizer.html Application Verifier for Windows - https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/application-verifier Sanitizers are relatively new tools that add extra instrumentation to your application (for example they replace new/malloc/delete calls) and can detect various runtime errors: leaks, use after delete, double free and many others. To improve your build pipeline, many guides advice to add sanitizers steps when doing tests. Most sanitizers come from the LLVM/Clang platform, but now they also work with GCC. Unfortunately not yet with Visual Studio (but you can try Application Verifier). Valgrind Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. When you run a program through Valgrind its run on a virtual machine that emulates your host environment. Having that abstraction the tools can leverage various information about the source code and its execution. http://valgrind.org/ http://valgrind.org/info/about.html http://valgrind.org/docs/manual/quick-start.html HeapTrack HeapTrack is a FOSS project and a heap memory profiler for Linux. It traces all memory allocations and annotates these events with stack traces. The tool has two forms the command line version that grabs the data, and then the UI part that you can use to read and analyze the results. This tool is comparable to Valgrinds massif; its easier to use and should be faster to load and analyze for large projects. https://github.com/KDE/heaptrack Dr. Memory Dr. Memory is an LGPL licenced tool that allows you to monitor and intensify memory -related errors for binaries on Windows, Linux, Mac, Android. Its based on the DynamoRIO dynamic instrumentation tool platform. With the tool, you can find errors like double frees, memory leaks, handle leaks (on Windows), GDI issues, access to uninitialized memory or even errors in multithreading memory scenarios. http://drmemory.org/ https://github.com/DynamoRIO/drmemory Deleaker The primary role of Deleaker is to find leaks in your native applications. It supports Visual Studio (since 2008 till the latest 2019 version), Delphi/C++ Builder, Qt Creator, CLion (soon!). Can be used as an extension in Visual Studio or as a standalone application. Deleaker tracks leaks in C/C++ applications (Native and CLR), plus .NET code. Memory (new/delete, malloc), GDI objects, User32 objects, Handles, File views, Fibres, Critical Sections, and even more. It gathers full call stack, ability to take snapshots, compare them, view source files related to allocation. https://www.deleaker.com/ https://www.deleaker.com/docs/deleaker/tutorial.html Summary & More I hope that with the above list, you get a useful overview of the tools that are essential for C++ development. If you want to read more about other ecosystem elements: libraries, frameworks, and other tools, then please see the full report from Embarcadero: C++ Ecosystem White Paper (Its a nice looking pdf, with more than 20 pages of content!) You might check this Resource for a super long list of tools, libs, frameworks that enhance C++ development: https://github.com/fffaraz/awesome-cpp Your Turn What are your favourite tools that you use when writing C++ apps? ",
        "_version_":1718939871511314432},
      {
        "story_id":[19256059],
        "story_author":["Tomte"],
        "story_descendants":[11],
        "story_score":[34],
        "story_time":["2019-02-26T17:00:43Z"],
        "story_title":"Semantic Linefeeds (2012)",
        "search":["Semantic Linefeeds (2012)",
          "https://rhodesmill.org/brandon/2012/one-sentence-per-line/",
          "Date: 3 April 2012 Tags:python, computing, document-processing I give some advice each year in my annual Sphinx tutorial at PyCon. Agrateful student asked where I myself had learned the tip. Ihave done some archology and finally have an answer. Let me share what I teach them about semantic linefeeds, then I will reveal its source which turns out to have been written when I was only a few months old! In the tutorial, I ask students whether or not the Sphinx text files in their project will be read by end-users. If not, then I encourage students to treat the files as private source code that they are free to format semantically. Instead of fussing with the lines of each paragraph so that they all end near the right margin, they can add linefeeds anywhere that there is a break between ideas. The result can be spectacular. By starting a new line at the end of each sentence, and splitting sentences themselves at natural breaks between clauses, a text file becomes far easier to edit and version control. Text editors are very good at manipulating lines so when each sentence is a contiguous block of lines, your editor suddenly becomes a very powerful mechanism for quickly rearranging clauses and ideas. And your version-control system will love semantic linefeeds. Have you ever changed a few words at the beginning of a paragraph, only to discover that version control now thinks the whole text has changed? ... the definition in place of it. -The beauteous scheme is that now, if you change -your mind about what a paragraph should look -like, you can change the formatted output merely -by changing the definition of .PP and -re-running the formatter. +The beauty of this scheme is that now, if you +change your mind about what a paragraph should +look like, you can change the formatted output +merely by changing the definition of .PP +and re-running the formatter. As a rule of thumb, for all but the most ... With every sentence and clause on its own line, you can make exactly the same change to the same paragraph without the rest of the paragraph even noticing: ... the definition in place of it. -The beauteous scheme is that now, +The beauty of this scheme is that now, if you change your mind about what a paragraph should look like, you can change the formatted output merely by changing the definition of .PP and re-running the formatter. As a rule of thumb, for all but the most ... Semantic linefeeds, as I call them, have been making my life easier for more than twenty years, and have governed how my text files look behind-the-scenes whether my markup format is HTML, TeX, RST, or the venerable troff macro typesetter. So where did I learn the trick? For a long time I believed that my source must have been the UNIX Documenter's Workbench manual. The Workbench was an attempt by AT&T to market the operating system that had become such a cult hit internally among Bell Labs engineers, by bundling the system with its most powerful typesetting tools. The attempt failed, of course I am told that AT&T was terrible at marketing computers, just as Xerox had no idea what to do with the ideas that were bubbling at PARC in the 1970s but my father worked at Bell Labs and had a copy of the Workbench documentation around the house. (Icannot find a copy on the Internet were all public copies destroyed during the devastating copyright battle that justly brought SCO to its ruin?) But after an extensive search, I have found an earlier source and I could not be any happier to discover that my inspiration is none other than Brian W. Kernighan! He published UNIX for Beginners [PDF] as Bell Labs Technical Memorandum 74-1273-18 on 29 October 1974. It describes a far more primitive version of the operating system than his more famous and more widely available UNIX for Beginners Second Edition from 1978. After a long search I have found the lone copy linked above, hosted on an obscure Japanese web page about UNIX 6th Edition which has now disappeared but can still be viewed on the Internet Archives Wayback Machine (to which both of the links above point). In the section Hints for Preparing Documents, Kernighan shares this wisdom: Hints for Preparing Documents Most documents go through several versions (always more than you expected) before they are finally finished. Accordingly, you should do whatever possible to make the job of changing them easy. First, when you do the purely mechanical operations of typing, type so subsequent editing will be easy. Start each sentence on a new line. Make lines short, and break lines at natural places, such as after commas and semicolons, rather than randomly. Since most people change documents by rewriting phrases and adding, deleting and rearranging sentences, these precautions simplify any editing you have to do later. Brian W. Kernighan, 1974 Note how Pythonic his advice sounds he replaces the fiction of write-once documents with a realistic focus on making text that is easy to edit later! I must have read this when I was first learning UNIX and somehow carried it with me all of these years. It says something very powerful about the UNIX plain-text approach that advice given in 1974, and basically targeted at making text easier to edit in the terribly cramped ed text editor, applies just as well to our modern world of colorful full-screen editors like Emacs and Vim and distributed version control systems that were not even imagined in the 1970s. If you are interested in more early UNIX documentation including the Second Edition of Kernighan's Beginners guide check out the 7th Edition manuals which Bell Labs has kindly made available online, both as PDF files and also as plain-text files marked up for the troff typesetter. Note that you can still compile the troff files successfully on a modern system try that with any other richly-formatted text from the1970s! comments powered by 2021 ",
          "Previous discussion: <a href=\"https://news.ycombinator.com/item?id=4642395\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4642395</a>",
          "Maybe I'm misunderstanding the point of this article, but I always put linefeeds where they should logically be. If people would like to view it in a certain way, it's always easy to put it through a text-processing program, which will do a much better job on semantically separated content rather that that which has had formatting hard-coded into it.<p>(Incidentally, this is also why I use tabs for indentation, since it's a lot easier for tooling to relayout. But I'm not trying to start the holy war here.)"],
        "story_type":["Normal"],
        "url":"https://rhodesmill.org/brandon/2012/one-sentence-per-line/",
        "comments.comment_id":[19258490,
          19259344],
        "comments.comment_author":["emddudley",
          "saagarjha"],
        "comments.comment_descendants":[0,
          2],
        "comments.comment_time":["2019-02-26T21:33:26Z",
          "2019-02-26T23:18:07Z"],
        "comments.comment_text":["Previous discussion: <a href=\"https://news.ycombinator.com/item?id=4642395\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4642395</a>",
          "Maybe I'm misunderstanding the point of this article, but I always put linefeeds where they should logically be. If people would like to view it in a certain way, it's always easy to put it through a text-processing program, which will do a much better job on semantically separated content rather that that which has had formatting hard-coded into it.<p>(Incidentally, this is also why I use tabs for indentation, since it's a lot easier for tooling to relayout. But I'm not trying to start the holy war here.)"],
        "id":"7f0c74ca-05a9-43f1-8aa7-9da822806d6c",
        "url_text":"Date: 3 April 2012 Tags:python, computing, document-processing I give some advice each year in my annual Sphinx tutorial at PyCon. Agrateful student asked where I myself had learned the tip. Ihave done some archology and finally have an answer. Let me share what I teach them about semantic linefeeds, then I will reveal its source which turns out to have been written when I was only a few months old! In the tutorial, I ask students whether or not the Sphinx text files in their project will be read by end-users. If not, then I encourage students to treat the files as private source code that they are free to format semantically. Instead of fussing with the lines of each paragraph so that they all end near the right margin, they can add linefeeds anywhere that there is a break between ideas. The result can be spectacular. By starting a new line at the end of each sentence, and splitting sentences themselves at natural breaks between clauses, a text file becomes far easier to edit and version control. Text editors are very good at manipulating lines so when each sentence is a contiguous block of lines, your editor suddenly becomes a very powerful mechanism for quickly rearranging clauses and ideas. And your version-control system will love semantic linefeeds. Have you ever changed a few words at the beginning of a paragraph, only to discover that version control now thinks the whole text has changed? ... the definition in place of it. -The beauteous scheme is that now, if you change -your mind about what a paragraph should look -like, you can change the formatted output merely -by changing the definition of .PP and -re-running the formatter. +The beauty of this scheme is that now, if you +change your mind about what a paragraph should +look like, you can change the formatted output +merely by changing the definition of .PP +and re-running the formatter. As a rule of thumb, for all but the most ... With every sentence and clause on its own line, you can make exactly the same change to the same paragraph without the rest of the paragraph even noticing: ... the definition in place of it. -The beauteous scheme is that now, +The beauty of this scheme is that now, if you change your mind about what a paragraph should look like, you can change the formatted output merely by changing the definition of .PP and re-running the formatter. As a rule of thumb, for all but the most ... Semantic linefeeds, as I call them, have been making my life easier for more than twenty years, and have governed how my text files look behind-the-scenes whether my markup format is HTML, TeX, RST, or the venerable troff macro typesetter. So where did I learn the trick? For a long time I believed that my source must have been the UNIX Documenter's Workbench manual. The Workbench was an attempt by AT&T to market the operating system that had become such a cult hit internally among Bell Labs engineers, by bundling the system with its most powerful typesetting tools. The attempt failed, of course I am told that AT&T was terrible at marketing computers, just as Xerox had no idea what to do with the ideas that were bubbling at PARC in the 1970s but my father worked at Bell Labs and had a copy of the Workbench documentation around the house. (Icannot find a copy on the Internet were all public copies destroyed during the devastating copyright battle that justly brought SCO to its ruin?) But after an extensive search, I have found an earlier source and I could not be any happier to discover that my inspiration is none other than Brian W. Kernighan! He published UNIX for Beginners [PDF] as Bell Labs Technical Memorandum 74-1273-18 on 29 October 1974. It describes a far more primitive version of the operating system than his more famous and more widely available UNIX for Beginners Second Edition from 1978. After a long search I have found the lone copy linked above, hosted on an obscure Japanese web page about UNIX 6th Edition which has now disappeared but can still be viewed on the Internet Archives Wayback Machine (to which both of the links above point). In the section Hints for Preparing Documents, Kernighan shares this wisdom: Hints for Preparing Documents Most documents go through several versions (always more than you expected) before they are finally finished. Accordingly, you should do whatever possible to make the job of changing them easy. First, when you do the purely mechanical operations of typing, type so subsequent editing will be easy. Start each sentence on a new line. Make lines short, and break lines at natural places, such as after commas and semicolons, rather than randomly. Since most people change documents by rewriting phrases and adding, deleting and rearranging sentences, these precautions simplify any editing you have to do later. Brian W. Kernighan, 1974 Note how Pythonic his advice sounds he replaces the fiction of write-once documents with a realistic focus on making text that is easy to edit later! I must have read this when I was first learning UNIX and somehow carried it with me all of these years. It says something very powerful about the UNIX plain-text approach that advice given in 1974, and basically targeted at making text easier to edit in the terribly cramped ed text editor, applies just as well to our modern world of colorful full-screen editors like Emacs and Vim and distributed version control systems that were not even imagined in the 1970s. If you are interested in more early UNIX documentation including the Second Edition of Kernighan's Beginners guide check out the 7th Edition manuals which Bell Labs has kindly made available online, both as PDF files and also as plain-text files marked up for the troff typesetter. Note that you can still compile the troff files successfully on a modern system try that with any other richly-formatted text from the1970s! comments powered by 2021 ",
        "_version_":1718939822419083264},
      {
        "story_id":[19367379],
        "story_author":["kanishkdudeja"],
        "story_descendants":[3],
        "story_score":[10],
        "story_time":["2019-03-12T12:15:08Z"],
        "story_title":"Manifesto for Async Software Development",
        "search":["Manifesto for Async Software Development",
          "http://asyncmanifesto.org/",
          "It's time for a 21st century successor to Agile And its most popular incarnation, Scrum After many years of developing software using Agile methodologies like Scrum, the time has come to value: Modern tools and flexible work environments over meetings and office hours Flexibility in prioritization over detailed planning Comprehensive documentation over tribal knowledge That is, while there is value in the latter items, there is more value in the former items. Principles of Async Software Development Modern tools Whether you're a fan of GitLab, GitHub, Bitbucket, or something else, good async collaboration tools like these all share a few important things in common: Each deeply integrates version control with issue tracking. Each offers rich prioritization and assignment features. Each wraps all that up into a slick user experience that anyone on the team can use, including nontechnical people. Meetings only as a last resort Meetings are very costly to your business. That's because creative professionals need long stretches of uninterrupted time to get meaningful work done. Thus, async communication should be your default, because it prevents context switching. Forget the planning meetings. Product owners can replace planning meetings by simply filing issues in the issue tracker, assigning priority, assigning them to people, and setting a release milestone. People will know what to work on by simply working on whatever the highest priority issue is in their queue. Skip the daily standups. Product owners can ascertain status by reading the comment threads of issues currently being worked on and posting questions as needed. Retire the backlog grooming sessions. Product owners should own the issue queue and frequently reassess priority on their own. They should loop in other people on an as-needed basis for advice. Call a meeting only when all other channels of communication aren't suitable for a specific issue. Flexible work environments Since modern tools and async communication makes 1950s-style meetings-centric office cultures obsolete, we can enjoy much more flexible work environments now. Adopt a hotelling policy at your office. Don't assign desks to anyone by default. Anyone who requests an assigned desk should get to choose whether it's a private office or in a communal space. Discourage one-size-fits-all space management. Some people work better in crowds, others work better at home. Let people decide for themselves. Document everything The better documented your workflow, the less your workers will need to interrupt each other to seek out tribal knowledge. A question answered in a FAQ or some other form of async communication is much better than one answered by a shoulder tap. ",
          "> A question answered in a FAQ or some other form of async communication is much better than one answered by a shoulder tap.<p>This doesn't scale. Eventually, you get to a point where you don't know where to look for context and context ends up getting spread around multiple areas. Complexity cannot be wrapped up in a wiki obviously enough, that you can fully grasp every possible issue without comprehending the whole wiki.",
          "This is entirely from an engineer's perspective. The reason agile and scrum are so popular is because it works for managers.<p>Managers need to herd all the cats, keep them on task, and give them continuous deadlines to report status in order to keep a low level fire burning under their employees' butts.<p>For open source software projects and hobby projects, this type of \"asynchronous\" development is perfectly fine.<p>For corporate environments, the problem is that employees are not necessarily intrinsically motivated and also need a way to communicate regularly up a chain of authority. This is the point of meetings.<p>Furthermore, managers generally succeed based on social intelligence. By regularly keeping tabs on employees using real meetings, they can assess the emotional state of the team... such as the morale or whether anyone is struggling or going through personal problems that they are reluctant to broadcast."],
        "story_type":["Normal"],
        "url":"http://asyncmanifesto.org/",
        "url_text":"It's time for a 21st century successor to Agile And its most popular incarnation, Scrum After many years of developing software using Agile methodologies like Scrum, the time has come to value: Modern tools and flexible work environments over meetings and office hours Flexibility in prioritization over detailed planning Comprehensive documentation over tribal knowledge That is, while there is value in the latter items, there is more value in the former items. Principles of Async Software Development Modern tools Whether you're a fan of GitLab, GitHub, Bitbucket, or something else, good async collaboration tools like these all share a few important things in common: Each deeply integrates version control with issue tracking. Each offers rich prioritization and assignment features. Each wraps all that up into a slick user experience that anyone on the team can use, including nontechnical people. Meetings only as a last resort Meetings are very costly to your business. That's because creative professionals need long stretches of uninterrupted time to get meaningful work done. Thus, async communication should be your default, because it prevents context switching. Forget the planning meetings. Product owners can replace planning meetings by simply filing issues in the issue tracker, assigning priority, assigning them to people, and setting a release milestone. People will know what to work on by simply working on whatever the highest priority issue is in their queue. Skip the daily standups. Product owners can ascertain status by reading the comment threads of issues currently being worked on and posting questions as needed. Retire the backlog grooming sessions. Product owners should own the issue queue and frequently reassess priority on their own. They should loop in other people on an as-needed basis for advice. Call a meeting only when all other channels of communication aren't suitable for a specific issue. Flexible work environments Since modern tools and async communication makes 1950s-style meetings-centric office cultures obsolete, we can enjoy much more flexible work environments now. Adopt a hotelling policy at your office. Don't assign desks to anyone by default. Anyone who requests an assigned desk should get to choose whether it's a private office or in a communal space. Discourage one-size-fits-all space management. Some people work better in crowds, others work better at home. Let people decide for themselves. Document everything The better documented your workflow, the less your workers will need to interrupt each other to seek out tribal knowledge. A question answered in a FAQ or some other form of async communication is much better than one answered by a shoulder tap. ",
        "comments.comment_id":[19371539,
          19373363],
        "comments.comment_author":["reallydude",
          "nwah1"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-03-12T18:41:55Z",
          "2019-03-12T21:43:42Z"],
        "comments.comment_text":["> A question answered in a FAQ or some other form of async communication is much better than one answered by a shoulder tap.<p>This doesn't scale. Eventually, you get to a point where you don't know where to look for context and context ends up getting spread around multiple areas. Complexity cannot be wrapped up in a wiki obviously enough, that you can fully grasp every possible issue without comprehending the whole wiki.",
          "This is entirely from an engineer's perspective. The reason agile and scrum are so popular is because it works for managers.<p>Managers need to herd all the cats, keep them on task, and give them continuous deadlines to report status in order to keep a low level fire burning under their employees' butts.<p>For open source software projects and hobby projects, this type of \"asynchronous\" development is perfectly fine.<p>For corporate environments, the problem is that employees are not necessarily intrinsically motivated and also need a way to communicate regularly up a chain of authority. This is the point of meetings.<p>Furthermore, managers generally succeed based on social intelligence. By regularly keeping tabs on employees using real meetings, they can assess the emotional state of the team... such as the morale or whether anyone is struggling or going through personal problems that they are reluctant to broadcast."],
        "id":"454f27b9-7ee5-4fa7-8c48-91aabcf3c8f1",
        "_version_":1718939828248117248},
      {
        "story_id":[20377175],
        "story_author":["azhenley"],
        "story_descendants":[2],
        "story_score":[64],
        "story_time":["2019-07-07T19:21:10Z"],
        "story_title":"Quick start guide to research on human factors of software engineering",
        "search":["Quick start guide to research on human factors of software engineering",
          "http://web.eecs.utk.edu/~azh/blog/guidehciseresearch.html",
          "Austin Z. Henley Assistant Professor azh@utk.edu @austinzhenley github/AZHenley This guide is meant to help new graduate students get a short introduction to research at the intersection of human-computer interaction (HCI) and software engineering (SE). By reading the materials listed below, you will get a small taste of the field. ",
          "In my experience for most developers the behaviors boil down to just a few questions?<p>1. Are the developers willing to write original code or must they be limited to configurations or tooling?<p>2. Are the developers willing to accept any API (RTFM) or must the developers be limited to prior known APIs?<p>3. Are the developers willing to accept failure for missing unspecified, but commonly known, requirements such as accessibility, usability, security, or domain specific knowledge?<p>4. Are the developers willing to alter their personal routines to accommodate a shift in requirements and delivery dates or do the developers shift requirements to accommodate their routines, such as work-life balance?<p>5. Will the developers write technical documentation to describe process and approach or will the developers shift requirements in anticipation of forth-coming technical documentation?<p>6. Will the developers refactor code to achieve simplicity (imperative), descriptive concepts (declarative), code style, or not at all? The motivations generally do not overlap.<p>7. Will the developers accept code that better achieves business concerns or end-user preferences (object measures) in violation to preferred structures or styles (subjective measures)?<p>8. Are the developers willing to read the existing code (RTFC) before recommending new tools or solutions?",
          "I really want this kind of research to flourish, but the first click I made on an interesting looking paper led me to: \"To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support.  We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers.\"<p>To be fair, I haven't read the paper and the rest of the abstract looks reasonable: \"To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits.\"<p>But what the heck is the first bit for?  Of 12 people I know, the vast majority don't use refactoring tools.  That's not the basis on which to launch a study.  I've skimmed the abstracts of the other papers as well and I'm not all that impressed.  Everything seems to be doing comparisons against Eclipse.  While Eclipse has a variety of different usability features, I'm actually not convinced any of them help at all (which is why I don't use it ;-) ).  So at the very least, I'd like to see a baseline against a traditional text editor like Emacs or Vim.  The \"improvements\" in usability that they are measuring may simply be the avoidance of problems in Eclipse.  I'm not trying to start an editor war here, I'm just saying you can't assume that any single editor is a good baseline.  I'd argue strenuously that feature rich editors like Eclipse, IntelliJ, VS Code, etc, etc are particularly bad candidates because nobody has really measured the effectiveness of their features.<p>Without being too negative, I hope there are better papers on these kinds of topics because I think it's incredibly important."],
        "story_type":["Normal"],
        "url":"http://web.eecs.utk.edu/~azh/blog/guidehciseresearch.html",
        "url_text":"Austin Z. Henley Assistant Professor azh@utk.edu @austinzhenley github/AZHenley This guide is meant to help new graduate students get a short introduction to research at the intersection of human-computer interaction (HCI) and software engineering (SE). By reading the materials listed below, you will get a small taste of the field. ",
        "comments.comment_id":[20379783,
          20380549],
        "comments.comment_author":["austincheney",
          "mikekchar"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-07-08T03:56:33Z",
          "2019-07-08T07:11:40Z"],
        "comments.comment_text":["In my experience for most developers the behaviors boil down to just a few questions?<p>1. Are the developers willing to write original code or must they be limited to configurations or tooling?<p>2. Are the developers willing to accept any API (RTFM) or must the developers be limited to prior known APIs?<p>3. Are the developers willing to accept failure for missing unspecified, but commonly known, requirements such as accessibility, usability, security, or domain specific knowledge?<p>4. Are the developers willing to alter their personal routines to accommodate a shift in requirements and delivery dates or do the developers shift requirements to accommodate their routines, such as work-life balance?<p>5. Will the developers write technical documentation to describe process and approach or will the developers shift requirements in anticipation of forth-coming technical documentation?<p>6. Will the developers refactor code to achieve simplicity (imperative), descriptive concepts (declarative), code style, or not at all? The motivations generally do not overlap.<p>7. Will the developers accept code that better achieves business concerns or end-user preferences (object measures) in violation to preferred structures or styles (subjective measures)?<p>8. Are the developers willing to read the existing code (RTFC) before recommending new tools or solutions?",
          "I really want this kind of research to flourish, but the first click I made on an interesting looking paper led me to: \"To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support.  We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers.\"<p>To be fair, I haven't read the paper and the rest of the abstract looks reasonable: \"To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits.\"<p>But what the heck is the first bit for?  Of 12 people I know, the vast majority don't use refactoring tools.  That's not the basis on which to launch a study.  I've skimmed the abstracts of the other papers as well and I'm not all that impressed.  Everything seems to be doing comparisons against Eclipse.  While Eclipse has a variety of different usability features, I'm actually not convinced any of them help at all (which is why I don't use it ;-) ).  So at the very least, I'd like to see a baseline against a traditional text editor like Emacs or Vim.  The \"improvements\" in usability that they are measuring may simply be the avoidance of problems in Eclipse.  I'm not trying to start an editor war here, I'm just saying you can't assume that any single editor is a good baseline.  I'd argue strenuously that feature rich editors like Eclipse, IntelliJ, VS Code, etc, etc are particularly bad candidates because nobody has really measured the effectiveness of their features.<p>Without being too negative, I hope there are better papers on these kinds of topics because I think it's incredibly important."],
        "id":"30333443-211a-4d19-8918-9eb0412b2a73",
        "_version_":1718939851425841152},
      {
        "story_id":[21668975],
        "story_author":["networked"],
        "story_descendants":[55],
        "story_score":[195],
        "story_time":["2019-11-30T07:20:56Z"],
        "story_title":"GitHub Trip Report",
        "search":["GitHub Trip Report",
          "https://fossil-scm.org/forum/forumpost/536ce98d85",
          "(1.2) By Richard Hipp (drh) on 2019-12-02 01:07:22 edited from 1.1 [source] I, the founder of Fossil, was invited and attended the first meeting of the GitHub Open Source Advisory Board in San Francisco on 2019-11-12. This post is a report of my adventures. The only skeptic in the room GitHub was a very gracious host, providing lunch before and dinner after the 5-hour meeting. There were 24 people in the room, 5 or 6 of whom were GitHub employees. To nobody's surprise, I was the only Git skeptic present. But everyone was very cordial and welcoming. It was a productive meeting. It is not about Git The GitHub employees seem to have all read at least some of my writings concerning Git and rebase and comparing Git and Fossil and so they knew where I stand on those issues. They are not in 100% agreement with me, but they also seem to understand and appreciate my point of view. They all seemed to agree that Git has UX issues and that it is not the optimal Distributed Version Control System (DVCS) design. Their argument is that none of that matters, at least not any more. Git is used by nearly everyone and is the defacto standard, and so that is what they use. Though I disagree, it is still a reasonable argument. I offered a number of suggestions on how Git might be improved (for example, by adding the ability to check-in revisions to comments on prior check-ins) and their response was mostly \"We don't control Git\". I replied that GitHub designed and implement the v2-http sync protocol used by Git which has since become the only sync protocol that most Git user will ever experience and which significantly improved the usability of Git. I encouraged them to continue to try to push Git to improve. I'm not sure how persuasive my arguments were though. Because: It isn't really about Git anymore. GitHub used to be a Git repository hosting company. But now, they are about providing other software development and management infrastructure that augments the version control. They might as well change their name to Hub at this point. For example, the main topics of discussion at this meeting where: Issue tracking and tickets Code review Automated Continuous Integration Maintainer workflow and triage The GitHub staff says that the four pillars of their organization are DevOps Security Collaboration Insights You will notice that version control is not on either of those lists. There has been a culture shift at GitHub. They are now all about the tooling that supports Git and not Git itself. On the other hand, GitHub showed no hint of any desire to support alternative version control systems like Mercurial or Fossil. They openly assume that Open Source (mostly) runs on Git. It is just that the version control is no longer their focus. They have moved on to providing other infrastructure in support of Open Source. Other Take-Aways Documentation Is The Best Way To Say \"No\" One of the most important and also the hardest jobs of a project maintainer is saying \"no\" to enhancement requests. If you try to take on every requested enhancement, your project will quickly loss focus and become too unwieldy to maintain. Participant \"Paris\" (whose full contact information I was unable to obtain) says: \"Documentation is the best way to say 'no'.\" In other words, it is important to document why a project does things the way it does, as this will tend to prevent enhancement requests that cause the project to diverge from its original intent. I have also found that writing about the \"why\" tends to help one focus on the real purpose of the project as well. Separately it was observed that documentation is the single most important factor in open-source adaption. The better your documentation, the more likely people are to use and contribute to your project. Implications for Fossil: I'd like to add the ability to include PIC and EQN markup in the middle of both Markdown and Fossil-Wiki documents. I think this Forum feature has been very successful, but there are still many opportunities to improve the UX. Additional ideas on how Fossil might be enhanced to support better project documentation are welcomed. \"The Tools Make The Rules\" This quote (from Tom Ted Kremenek of the Swift project) is saying that your workflow will be defined and constrained by the tools you use. If you use inferior tools your productivity will suffer. This is the argument I've made for years about the importance of being about to amend check-in comments and to view the descendants of check-ins. Git partisans offer all kinds of excuses about how their workflow does not need that. I tend to counter with \"I never needed bisect until I had the capability.\" The point is that once you have the ability to amend check-in comments and view the descendants of a check-in, your workflow becomes more efficient and you wonder how you ever survived without those capabilities. What limitations or restrictions in Fossil are limiting productivity? Some possible ideas: Poor tracking of files across renames Inability to do shallow clones Poor ticket UX Inability to subscribe to notifications for changes to specific tickets or files. Difficulty implementing actions that are triggered by new check-ins and/or new tickets. This is needed for Continuous Integration (CI). Enhancements to the backoffice feature of Fossil would be welcomed. General agreement that rebasing is not a good thing Even among Git partisans, there seems to be a general agreement that rebase ought to be avoided. The Rebase Considered Harmful document is not especially controversial. An interesting insight from the LLVM developers: They use rebase extensively. But the reason is that downstream tooling controlled by third-parties and which was developed back when LLVM was still using SVN expects a linear sequence of changes. The LLVM developers would like to do more with branching, but that would break the downstream tools over which they have no control, and so they are stuck with having to rebase everything. Thus rebase supports historical compatibility of legacy tooling. It's a reasonable argument in support of rebase, not one that I necessarily agree with, but one that I understand. Summary And Concluding Thoughts GitHub is changing. It is important to understand that GitHub is moving away from being a Git Repository hosting company and towards a company that provides an entire ecosystem for software development. Git is still at the core, but the focus is no longer on Git. Fossil already provides many of the features that GitHub wraps around Git. Prior to the meeting, someone told me that \"GitHub is what make Git usable.\" Fossil has a lot of integrated capabilities that make GitHub unnecessary. Even so, there is always room for improvement and Fossil should be adapting and integrating ideas from other software development systems. One GitHub-er asked me: \"What would it take to get SQLite to move off of Fossil and on to Git.\" Just to be clear to everyone reading this: That will never happen. Fossil was specifically designed to support SQLite development and does so remarkably well. Fossil fills a different niche than does Git/GitHub. Fossil will continue to be supported and enhanced by me (as well as others) well into the foreseeable future. (2) By Richard Hipp (drh) on 2019-11-27 15:30:32 in reply to 1.0 [link] [source] Prior discussion of PIC markup in Fossil documentation was at post db217184de. The EQN markup would work much the same way. Since posting that a couple of months ago, I did actually attempt to write a PIC language interpreter in Javascript. But I discovered that Javascript is not a particularly good language with which to write a parser/compiler - or at least my limited knowledge of Javascript was insufficient for the task. Input from readers with better Javascript skills would be appreciated here. I have heard that it is now possible to compile C code into WASM. In particular, I hear that people are compiling \"sqlite3.c\" into WASM and running it in browsers. Perhaps an alternative approach would be to write a compact PIC translator in C (using Lemon to implement the parser) then compile the result into WASM for download to the browser. But I don't understand how all of that works. If anybody has any insights and can help guide me through the process, that would be appreciated. (4) By Andreas Kupries (aku) on 2019-11-27 17:52:34 in reply to 2 [link] [source] A question about lemon. IIRC it is a LALR parser generator similar to Bison. Is the generated parser able to execute semantic actions as the AST is built ? This and the ability to reach back to the lexer to control its execution will be necessary for PIC. The reason for that is PIC's macro system, which it has in lieu of proper procedures/functions. Macros can recursively call each other, and to not have this end in tears of infinite recursion this expansion has to be/is curtailed in code not executed, i.e. the untaken branches of if/else conditionals. For the implementation of the language this means that semantic actions have to be executed as the input is lexed and parsed, and then reach back to the lexer with the outcomes of conditions to selectively skip/suppress macro expansion. See pages 6-8 of the dpic docs. With that in mind I wonder if it might be suitable to reuse an existing parser like dpic, instead of writing a new PIC parser, for compilation to WASM. (5) By Richard Hipp (drh) on 2019-11-27 18:05:49 in reply to 4 [link] [source] If you can compile dpic down into a dpic.js file that we can source and use to render PIC on the client-side, that would be great! (7) By Warren Young (wyoung) on 2019-11-27 18:44:20 in reply to 2 [link] [source] it is now possible to compile C code into WASM Yes, and more to the point, WebAssembly support is approaching universal adoption. It won't run in ELinks, but this is a non-core feature that those with such niche needs can ignore. I do wonder if server-side rendering to a virtual SVG would be a better plan, though. (13) By aitap on 2019-11-27 21:35:13 in reply to 2 [link] [source] Is there a place (someone's repo on ChiselApp?) where we could collaborate on the PIC renderer? I seem to have created a Lemon grammar for PIC, but I cannot test it on real inputs yet, because first I'll have to hack the lexer to understand defines and substitute macros. (According to Kernighan and Raymond, they have to be done outside the parser, by pure sting substitution.) Also, I haven't started the work on the \"virtual machine\" to interpret the AST and output SVG. (14) By Stephan Beal (stephan) on 2019-11-27 21:45:42 in reply to 13 [link] [source] If you can wait about 12 hours i can set up a repo for you on fossil.wanderinghorse.net and make you the admin so you can manage collaborators. i have no idea whether the backoffice (forum) parts will work on that site, though, so no guarantees on that. If you want to go ahead and set up a repo, set up your user(s), and email me a link (see https://wanderinghorse.net/home/stephan) where i can download it, i can have it installed in no time tomorrow. My only requirement is that i be allowed to set myself up as an admin/superuser solely for maintenance purposes. (15) By Stephan Beal (stephan) on 2019-11-27 22:07:17 in reply to 13 [link] [source] Alternately, i can set you up an empty repo. Just email me (see previous post) with your preference. (3) By Stephan Beal (stephan) on 2019-11-27 15:41:14 in reply to 1.0 [link] [source] The only skeptic in the room But you are not alone! You will notice that version control is not on either of those lists. That's... kind of mind-blowing. Perhaps they consider it an implicit entry in the list, since it's a level of infrastructure which underpins every aspect of what they do. A \"can't see the forest for the trees\" kind of thing. What limitations or restrictions in Fossil are limiting productivity? My own personal entry in that list is not new: the lack of a library interface. Obviously (for those who don't know this tidbit of history) i intended to tackle that with libfossil, and was well on the way to feature-parity with the fossil binary when chronic RSI (specifically, C8 nerve inflammation/damage in both arms) effectively kicked me out of the game in late 2014 (even now, my typing capacity is still reduced to about 10-15% of what it previously was, and i am still in forced early retirement due to the years-long duration of my medical leave). Coincidentally enough, since yesterday i've been milling over a reboot of that work, focusing initially on features which would make it easier/possible to create custom read-only clients, ignoring the ability to update/merge/etc. for the time being, simply to try to keep the scope in check. The motivation for that came from user elechak, who posted a query based on the mlink table. Being able to plug that type of feature one's own app/page/whatever would empower all sorts of apps we don't/can't currently have. Obviously once we have a library, the gateway is opened for any number of clients, e.g. tooling for reports, CI, and IDEs. That said... my hands cannot reasonably commit (as it were) to that scope of project for the foreseeable future :(. Getting the library up and running again, now that fossil supports multiple hash types (libfossil predates those changes), would require an as-yet-unknown amount of effort. That said, i don't think it would take too awful much to get the read-only functionality working again. So... i'll take this opportunity to bang the drum again and call for \"seriously interested\" coders to get in touch and help make that happen. (Forewarning: it wouldn't be a small amount of code.) While i cannot personally commit to more than \"casual\" amounts of coding, my capacity for technical support is far less bounded and my capacity for moral support is practically limitless ;). One GitHub-er asked me: \"What would it take to get SQLite to move off of Fossil and on to Git.\" LOL! i haven't laughed this hard since you explained the logic behind you getting taller with age! (8) By Warren Young (wyoung) on 2019-11-27 18:59:43 in reply to 3 [link] [source] i intended to tackle that with libfossil Let's not make the same mistake twice: libfossil fell into disrepair because it's a second implementation of the formats and protocols within an ecosystem that's not able to progress as fast as it wants to on a single implementation. (Thus this thread.) Rather than resurrect libfossil, I'd rather see that work went into refactoring the internals of Fossil to extract a library, divorcing the CGI and CLI interfaces from a core that only deals with the repo formats. Some of this is already done well enough that all that's needed is to draw the lines more explicitly, but I want to get to a world where the build process results in a static libfossil2.a that links to the Fossil binary, which can optionally be built as a shared object and linked into an app without linking to libnss, libssl, etc. because the UI is a native GUI tool that only works on local repos and thus doesn't need network clone and sync support. The resulting library should be modular, so that if the third-party app also wants clone and sync support, it can include that as well. Or, Plan C: expand the current web APIs to the point that one could write a full-function Fossil library that did nothing other than make HTTP API calls to a running Fossil instance. (9) By Richard Hipp (drh) on 2019-11-27 19:15:14 in reply to 8 [link] [source] Proposed Forum Enhancement I would like to start a new Forum thread named something like \"Libfossil Reboot\" based on the previous comment. The new \"Libfossil Reboot\" thread would link back to Warren's post, and Warren's post should link forward to the new thread. But it would be a completely new thread, in as much as it is a tangent off of the original. In other words, I'd like to create a simple mechanism to prevent thread hijacking. Somebody with good UX sense, please suggest a suitable interface for this new feature, or if the new idea makes no sense from a UX standpoint, please talk me out of it. (11) By Warren Young (wyoung) on 2019-11-27 19:23:24 in reply to 9 [link] [source] At risk of creating a hijack of a hijack () what if a Moderator or higher level user could mark a post as being a topic fork, which then prompts the user for a new thread title. The existing post becomes the starting post of the new thread under the new title, and unlike current first-post-in-thread, it has an \"in reply to\" in its header pointing back to the other thread. For normal users, all they see is that the Reply button changes to Moved or similar. Clicking it takes them to the start of the other thread, where they can now click Reply. Don't send the user straight to a reply: they need to see the new context first. They might then choose to reply further down-thread in the child topic. (10.1) By Stephan Beal (stephan) on 2019-11-27 19:24:51 edited from 10.0 in reply to 8 [link] [source] (Edit: sorry, Richard - i didn't see your note about the threadjacking until posting this :/. Further discussion will take place in the new thread if/when it starts.) Fair enough. Rather than resurrect libfossil, I'd rather see that work went into refactoring the internals of Fossil to extract a library, divorcing the CGI and CLI interfaces from a core that only deals with the repo formats. That's more or less originally what the goal was. There are, however, fundamental design decisions in fossil which make \"bending it\" into a library unfeasible. First and foremost is its allocation failure policy: simply die. That drastically reduces the amount of code needed (as can be seen when comparing the same algos in fossil core to libfossil - a huge number of lines of code in libfossil are checking/handling result codes and allocation results, where fossil would simply die in most such cases). A library cannot feasibly have a die-on-OOM policy. (That is to say: i wouldn't personally use a library which i knew would outright kill my app if the library encountered an error code or couldn't allocate memory.) That said: i would be completely on-board with such an effort. As Richard put it when it was proposed it in 2011, though, it would require \"a Herculean effort.\" (Though dictionaries/style guides differ on whether Herculean is capitalized or not when used that way, it would certainly have a massive capital H in our context!) Or, Plan C: expand the current web APIs to the point that one could write a full-function Fossil library that did nothing other than make HTTP API calls to a running Fossil instance. That's kinda/sorta what the JSON bits do, and an HTTP-based API is essentially a link-at-call-time DLL. However, a JSON-based API would be a particularly poor choice for certain functionality, most notably transferring binary files of arbitrary sizes, as doing so requires encoding into a form JSON can use (huge strings or massive arrays of integers (byte values), both of which are larger than the binaries themselves). (12) By Warren Young (wyoung) on 2019-11-27 19:37:21 in reply to 10.0 [link] [source] Understand, I don't mean that your intent in creating the library was a mistake, only that we now know that it clearly didn't attract a big enough user community that you had to fend off the tendency to fork the libary when you had to give up on it, because you had multiple people all wanting to continue it. Knowing this, we shouldn't repeat the decision. Contrast Fossil: I can see this project forking if something drastic happened to drh or to the project direction. We've got enough people actively interested in working on it that we would have to fight off a tendency to create multiple forks and not just one new successor project. I point that out not to show a danger to Fossil, but to show that it has a bus factor above 1. Years now after libfossil stopped moving, I think we can say that it had a bus factor of exactly 1. :( Therefore, efforts going forward should make Fossil itself more powerful, not divide precious resources. As to the matter of difficulty, that's why I proposed my previously-unnamed Plan B as a refactoring effort, rather than reimplementation. It's something we can simply choose to strive toward, one bite at a time. On the efficiency of a JSON API, many HTTP APIs switch content types to match the needs of the call. JSON returns may be the default, but binary and HTML returns are also legal based on context. Some calls may even have multiple forms. In that sense, Fossil UI is the HTML API to this library already, and /raw is the binary interface. All I'm proposing is wrapping this in a way that you can use inside, say, a Fossil plugin for a programmer's text editor. (6) By Warren Young (wyoung) on 2019-11-27 18:40:00 in reply to 1.1 [link] [source] Git is used by nearly everyone and is the defacto standard, and so that is what they use. That's pretty much the \"Windows\" argument. Windows hasn't gone away in the face of smartphones, Chromebooks, and the cloud, but it has been pushed to the margins wherever practical. It's probably a large part of the reason that the Windows phone and tablet efforts didn't go anywhere. It's the point of view of an entrenched victor who becomes blind to the fact that they are now entrenched, and consequently can no longer move. I'm not even sure this is a conscious choice. Perhaps a better metaphor is Gulliver strapped down by thousands of Lilliputians using ropes woven from process, standardization, and interdependency. Gulliver could choose to rise, but he'd destroy all the ropes in doing so. 1. Poor tracking of files across renames That's not the number one issue, just the most recent request. :) Fixing this would be a nice quality of life improvement for those of us already using Fossil, but I doubt it factors very high in anyone's decision about whether to use Fossil, or whether to leave Fossil for something else. 2. Inability to do shallow clones That goes to scalability, and it probably does have a lot to do with Fossil's adoption curve. It is true that most software projects aren't Linux-scale in terms of SLOC and complexity of development and thus don't benefit from elements of Git's design that serve the rare characteristics of the Linux kernel development project. Nevertheless, it is also true that there are a lot of software projects bigger and more complex than SQLite yet still much smaller than Linux and thus could benefit if Fossil supported large-scale projects better. 3. Poor ticket UX Now that we have Markdown in tickets (!!) my only serious complaint about it is the two-step filing process. I see the value of the simplified filing interface for normal users, but when I'm logged in as a Setup user, it's annoying to not have all of the ticket fields available to me to fill out at the same time as the initial filing. This could be controlled by a user cap saying whether that user or category of users sees the full ticket UI on filing. Speaking of user caps, another thing I anticipate is support for various SSO technologies: LDAP, OpenID, Oauth, SAML, etc. That in turn would require some way to map groups in those systems to user categories in Fossil, which would probably have to expand beyond the current hard-coded set of four. 4. Inability to subscribe to notifications for changes to specific tickets or files. That sounds like it'd be pretty easy to implement. All it wants is someone to scratch the itch. Not me; don't care. :) 5. Difficulty implementing actions that are triggered by new check-ins and/or new tickets. Server-side Tcl hooks? I think the hardest part is making such hooks enforcing in the face of distributed clones. If I check something into my local repo but a push to the parent repo fails because of a Tcl hook refusing the new material, what then? LLVM...downstream tools...stuck with having to rebase everything. Wouldn't a release branch (as opposed to release and version tags) solve this? Downstream projects could point at the branch they care about, which would get only a linear sequence of changes, pushed there by a release manager. You can implement development, feature freeze, slush, and code freeze branches in this way within Fossil today, progressively restricting changes until a frozen blob of code lands on the release branch. The only thing that might be nice in this regard is locking branches to particular users. Fossil's DVCS nature makes absolute locking impossible, but part of such organized thinking is an assumption that the users aren't actively malicious. If low-level Dev A checks something into the feature freeze branch directly, Fossil already has tools to fix that, and beyond that, it's a social/admin issue, not a tech issue. GitHub is moving away from being a Git Repository hosting company They're just moving up-stack, which is necessary to avoid becoming another commodity provider of Git service. If they'd continued to focus on Git, they'd be in direct competition with Digital Ocean and /usr/bin/git. They have to move up-stack to continue propping up those Silicon Valley salaries. Fossil doesn't have the same need. It merely needs to remain useful to its users. I think this discussion should remain focused on that and possibly on attracting new users. But Fossil doesn't have the same business needs as GitHub, Inc., a Subsidiary of Microsoft Corporation. (24) By anonymous on 2020-01-27 22:04:38 in reply to 6 [link] [source] If I check something into my local repo but a push to the parent repo fails because of a Tcl hook refusing the new material, what then? What happens when you push to a parent repo where you don't have commit privilege? (25) By Warren Young (wyetr) on 2020-01-27 22:31:29 in reply to 24 [link] [source] I see where you're trying to go with that point, but the difference is that when you try to push without check-in capability it's expected to be permanent until you get an account with that capability, then permanently allowed as long as your account remains active. To compare that with the denial from hooks, you're saying that a remote piece of software can effectively activate or deactivate my check-in capability on each artifact pushed. Keep in mind that Fossil writes its check-ins durably first to the local blockchain, then syncs the result to the remote system. This means that once local artifacts are denied by the remote, they'll be denied until the remote changes its rules. If we assume that the rules implemented by the Tcl commit hook are good and wisely crafted, then this means the local repo will forever have durable artifacts that can never be pushed. The local repo will keep pushing them, and the remote will keep running the commit hook logic over them, which will keep rejecting them. All of this is fairly anti-Fossil, which tries very hard to make all clones identical. If Fossil gets anything like this feature, I think it must be done by the hook scripts being somehow sent down with the clone and kept up to date there, so that the hook can run locally before the local blockchain gets modified. (26) By Warren Young (wyoung) on 2020-01-28 19:15:48 in reply to 25 [link] [source] I think it must be done by the hook scripts being somehow sent down with the clone ...which of course introduces portability problems: you have to ensure that the hook scripts will in fact run correctly on all of the clones, else you break those clones. \"But it worked on my machine!\" doesn't fly when your remote contributors stop being able to check work in just because they're on a different host OS, or are missing some third-party Tcl package, or whatever. However, a night's sleep and a shower have provided an answer: instead of the hook script outright rejecting \"bad\" checkins, what if Fossil treats commit hook rejection as a signal to automatically create a branch for the new artifacts so that other work can continue on the original branch? Then the problem can be corrected in public, which is very much pro-Fossil. We could obviously do this in a post-facto way, running such problem-detecting scripts on the server and having it automatically move artifacts, but making it part of the sync protocol with integrated hook scripts means the remote clone could be notified immediately that their check-in has been moved to a branch. The hook script could even return a detailed message explaining the problem, which could guide the remote user to a fix. This new scheme means the content is never lost, it is committed locally and sync'd remotely, but it's set aside until the problem can be resolved. (27) By anonymous on 2020-01-28 21:18:18 in reply to 26 [link] [source] what if Fossil treats commit hook rejection as a signal to automatically create a branch for the new artifacts so that other work can continue on the original branch? I like this idea a lot. but making it part of the sync protocol with integrated hook scripts means the remote clone could be notified immediately that their check-in has been moved to a branch. I would hope that the hook script would be able to supply the new branch name, \"calculated\" by whatever rules the core project team deems appropriate. Maybe the hook script could return a string with a branch name and a message separated by some reasonable delimiter. (28) By Warren Young (wyetr) on 2020-01-28 21:51:28 in reply to 27 [link] [source] the hook script would be able to supply the new branch name Sure. It'd allow a form of namespacing, where each hook script gets its own branch naming prefix so you can tell which script shunted the commit aside. Examples: code-style-abcd1234: code rejected by a site-configured automatic code style checker ci-cd-abcd1234: this commit breaks the CI/CD build; implicitly an async hook temp-code-abcd1234: this commit appears to contain some sort of temporary code: debug logging, FIXME comments, #if 0 blocks, etc. Maybe the hook script could return a string with a branch name and a message separated by some reasonable delimiter. Tcl lets you return a list of values; it isn't limited to returning a single scalar value: return { 401, \"code style check failed\", \"code-style-$nonce\", $output_from_style_checker } That is, an error code, a brief message, a branch name to move the commit to if not null, and an optional long message that expands on the brief message. (16) By Kevin (KevinYouren) on 2019-11-29 23:17:40 in reply to 1.1 [link] [source] Richard, I think \"rebase\" may be useful to entities who want to remove or hide errors or worse. (22) By anonymous on 2019-12-05 19:34:16 in reply to 16 [link] [source] I think \"rebase\" may be useful to entities who want to remove or hide errors or worse. If you develop on a private branch, then merge the final commit to trunk then no one need ever see any mess there might be on the branch. (Down side: Fossil doesn't track merges from a private branch, not even in the repo where the private branch exists.) (23) By Kevin (KevinYouren) on 2019-12-06 01:28:25 in reply to 22 [link] [source] Thank you, I didn't know that. (29.1) Originally by anonymous with edits by Stephan Beal (stephan) on 2020-12-15 16:37:59 from 29.0 in reply to 23 [link] [source] Deleted (30) By Stephan Beal (stephan) on 2020-12-15 16:43:07 in reply to 23 [link] [source] Reminder to moderators: We have had one spammer today who's tried to sneak in hyperlinks to a \"bathroom mold removal\" product as a sneaky link on a single period, making it effectively invisible when reading the markdown-processed post but clearly visible in the unparsed content. Please always check the unparsed content of posts for new and anonymous users before approving them. This particular piece of spam was first posted by a new account (which i deleted) and then again, later in the day, as an anonymous post (which snuck past moderation but i coincidentally recognized as identical to the previous post, so deleted it). The devious part is that such posts appear to be topical, but they try to sneak in spam links which are easy for the eye to overlook. (17) By Saagar Jha (saagarjha) on 2019-11-30 18:11:51 in reply to 1.1 [link] [source] This quote (from Tom Kremenek of the Swift project) is saying that your workflow will be defined and constrained by the tools you use. Might you be talking about Ted, the Swift Project Lead? (18) By Richard Hipp (drh) on 2019-11-30 18:49:20 in reply to 17 [link] [source] Yes, it seems likely that I wrote his name down incorrectly in my notes. Sorry about that, Ted. (19) By anonymous on 2019-12-01 23:45:14 in reply to 1.1 [link] [source] I'd like to copy this post for a personal blogpost (of course with reference) is such doing welcomed and permitted? (20) By Richard Hipp (drh) on 2019-12-02 01:07:45 in reply to 19 [link] [source] (21) By Offray (offray) on 2019-12-02 21:41:56 in reply to 1.2 [link] [source] One GitHub-er asked me: \"What would it take to get SQLite to move off of Fossil and on to Git.\" Just to be clear to everyone reading this: That will never happen. Fossil was specifically designed to support SQLite development and does so remarkably well. Fossil fills a different niche than does Git/GitHub. Fossil will continue to be supported and enhanced by me (as well as others) well into the foreseeable future. You could counter ask him/her: \"What would it take to get full support to Fossil inside GitHub and make it a first class citizen there?\" ;-) Cheers, ",
          "Maybe i’m crazy but i find it completely unremarkable that a GitHub strategy session leaves core git completely alone and instead focuses on improving the ecosystem around it.",
          "This is a pretty interesting writeup, even though I don’t necessarily agree with everything. Makes you think, at least.<p>I’ve been out of version control for a bit, and GitHub itself for quite a bit longer, but this part was interesting to me:<p>> Because: It isn't really about Git anymore. GitHub used to be a Git repository hosting company. But now, they are about providing other software development and management infrastructure that augments the version control. They might as well change their name to Hub at this point.<p>Just for added color: starting around 2010 or 2011 or so (around when we added Subversion to GitHub), we had a pretty solid idea that version control wasn’t “the thing”. Mostly because Mercurial was a strong alternative, and there always felt like there might be something new in the wings that could dominate in the next few years. Version control felt really dynamic, and deeply susceptible to change. And if something was better, we’d totally ditch Git for the new thing. The name was a bit of a misnomer, from that perspective.<p>I think that changed over time — I had a hell of a time trying to get movement on basic version control improvements back in 2014 — and now they’re clearly much more about the ecosystem rather than version control itself. It’s where the money is, of course, but it’s also where the bigger innovation is, at least for the time being.<p>I think the author is right to say that Microsoft is targeting a different area than version control specifically, though you could argue if the outcome of that is good or bad. It’s certainly different, though- they’re especially growing headcount right now, and the company makeup is wildly different than what many customers tend think it is today, imo."],
        "story_type":["Normal"],
        "url":"https://fossil-scm.org/forum/forumpost/536ce98d85",
        "comments.comment_id":[21669341,
          21669354],
        "comments.comment_author":["jcims",
          "holman"],
        "comments.comment_descendants":[1,
          2],
        "comments.comment_time":["2019-11-30T10:03:22Z",
          "2019-11-30T10:07:56Z"],
        "comments.comment_text":["Maybe i’m crazy but i find it completely unremarkable that a GitHub strategy session leaves core git completely alone and instead focuses on improving the ecosystem around it.",
          "This is a pretty interesting writeup, even though I don’t necessarily agree with everything. Makes you think, at least.<p>I’ve been out of version control for a bit, and GitHub itself for quite a bit longer, but this part was interesting to me:<p>> Because: It isn't really about Git anymore. GitHub used to be a Git repository hosting company. But now, they are about providing other software development and management infrastructure that augments the version control. They might as well change their name to Hub at this point.<p>Just for added color: starting around 2010 or 2011 or so (around when we added Subversion to GitHub), we had a pretty solid idea that version control wasn’t “the thing”. Mostly because Mercurial was a strong alternative, and there always felt like there might be something new in the wings that could dominate in the next few years. Version control felt really dynamic, and deeply susceptible to change. And if something was better, we’d totally ditch Git for the new thing. The name was a bit of a misnomer, from that perspective.<p>I think that changed over time — I had a hell of a time trying to get movement on basic version control improvements back in 2014 — and now they’re clearly much more about the ecosystem rather than version control itself. It’s where the money is, of course, but it’s also where the bigger innovation is, at least for the time being.<p>I think the author is right to say that Microsoft is targeting a different area than version control specifically, though you could argue if the outcome of that is good or bad. It’s certainly different, though- they’re especially growing headcount right now, and the company makeup is wildly different than what many customers tend think it is today, imo."],
        "id":"15ed7526-c883-4001-9823-1cf7c91bbdc8",
        "url_text":"(1.2) By Richard Hipp (drh) on 2019-12-02 01:07:22 edited from 1.1 [source] I, the founder of Fossil, was invited and attended the first meeting of the GitHub Open Source Advisory Board in San Francisco on 2019-11-12. This post is a report of my adventures. The only skeptic in the room GitHub was a very gracious host, providing lunch before and dinner after the 5-hour meeting. There were 24 people in the room, 5 or 6 of whom were GitHub employees. To nobody's surprise, I was the only Git skeptic present. But everyone was very cordial and welcoming. It was a productive meeting. It is not about Git The GitHub employees seem to have all read at least some of my writings concerning Git and rebase and comparing Git and Fossil and so they knew where I stand on those issues. They are not in 100% agreement with me, but they also seem to understand and appreciate my point of view. They all seemed to agree that Git has UX issues and that it is not the optimal Distributed Version Control System (DVCS) design. Their argument is that none of that matters, at least not any more. Git is used by nearly everyone and is the defacto standard, and so that is what they use. Though I disagree, it is still a reasonable argument. I offered a number of suggestions on how Git might be improved (for example, by adding the ability to check-in revisions to comments on prior check-ins) and their response was mostly \"We don't control Git\". I replied that GitHub designed and implement the v2-http sync protocol used by Git which has since become the only sync protocol that most Git user will ever experience and which significantly improved the usability of Git. I encouraged them to continue to try to push Git to improve. I'm not sure how persuasive my arguments were though. Because: It isn't really about Git anymore. GitHub used to be a Git repository hosting company. But now, they are about providing other software development and management infrastructure that augments the version control. They might as well change their name to Hub at this point. For example, the main topics of discussion at this meeting where: Issue tracking and tickets Code review Automated Continuous Integration Maintainer workflow and triage The GitHub staff says that the four pillars of their organization are DevOps Security Collaboration Insights You will notice that version control is not on either of those lists. There has been a culture shift at GitHub. They are now all about the tooling that supports Git and not Git itself. On the other hand, GitHub showed no hint of any desire to support alternative version control systems like Mercurial or Fossil. They openly assume that Open Source (mostly) runs on Git. It is just that the version control is no longer their focus. They have moved on to providing other infrastructure in support of Open Source. Other Take-Aways Documentation Is The Best Way To Say \"No\" One of the most important and also the hardest jobs of a project maintainer is saying \"no\" to enhancement requests. If you try to take on every requested enhancement, your project will quickly loss focus and become too unwieldy to maintain. Participant \"Paris\" (whose full contact information I was unable to obtain) says: \"Documentation is the best way to say 'no'.\" In other words, it is important to document why a project does things the way it does, as this will tend to prevent enhancement requests that cause the project to diverge from its original intent. I have also found that writing about the \"why\" tends to help one focus on the real purpose of the project as well. Separately it was observed that documentation is the single most important factor in open-source adaption. The better your documentation, the more likely people are to use and contribute to your project. Implications for Fossil: I'd like to add the ability to include PIC and EQN markup in the middle of both Markdown and Fossil-Wiki documents. I think this Forum feature has been very successful, but there are still many opportunities to improve the UX. Additional ideas on how Fossil might be enhanced to support better project documentation are welcomed. \"The Tools Make The Rules\" This quote (from Tom Ted Kremenek of the Swift project) is saying that your workflow will be defined and constrained by the tools you use. If you use inferior tools your productivity will suffer. This is the argument I've made for years about the importance of being about to amend check-in comments and to view the descendants of check-ins. Git partisans offer all kinds of excuses about how their workflow does not need that. I tend to counter with \"I never needed bisect until I had the capability.\" The point is that once you have the ability to amend check-in comments and view the descendants of a check-in, your workflow becomes more efficient and you wonder how you ever survived without those capabilities. What limitations or restrictions in Fossil are limiting productivity? Some possible ideas: Poor tracking of files across renames Inability to do shallow clones Poor ticket UX Inability to subscribe to notifications for changes to specific tickets or files. Difficulty implementing actions that are triggered by new check-ins and/or new tickets. This is needed for Continuous Integration (CI). Enhancements to the backoffice feature of Fossil would be welcomed. General agreement that rebasing is not a good thing Even among Git partisans, there seems to be a general agreement that rebase ought to be avoided. The Rebase Considered Harmful document is not especially controversial. An interesting insight from the LLVM developers: They use rebase extensively. But the reason is that downstream tooling controlled by third-parties and which was developed back when LLVM was still using SVN expects a linear sequence of changes. The LLVM developers would like to do more with branching, but that would break the downstream tools over which they have no control, and so they are stuck with having to rebase everything. Thus rebase supports historical compatibility of legacy tooling. It's a reasonable argument in support of rebase, not one that I necessarily agree with, but one that I understand. Summary And Concluding Thoughts GitHub is changing. It is important to understand that GitHub is moving away from being a Git Repository hosting company and towards a company that provides an entire ecosystem for software development. Git is still at the core, but the focus is no longer on Git. Fossil already provides many of the features that GitHub wraps around Git. Prior to the meeting, someone told me that \"GitHub is what make Git usable.\" Fossil has a lot of integrated capabilities that make GitHub unnecessary. Even so, there is always room for improvement and Fossil should be adapting and integrating ideas from other software development systems. One GitHub-er asked me: \"What would it take to get SQLite to move off of Fossil and on to Git.\" Just to be clear to everyone reading this: That will never happen. Fossil was specifically designed to support SQLite development and does so remarkably well. Fossil fills a different niche than does Git/GitHub. Fossil will continue to be supported and enhanced by me (as well as others) well into the foreseeable future. (2) By Richard Hipp (drh) on 2019-11-27 15:30:32 in reply to 1.0 [link] [source] Prior discussion of PIC markup in Fossil documentation was at post db217184de. The EQN markup would work much the same way. Since posting that a couple of months ago, I did actually attempt to write a PIC language interpreter in Javascript. But I discovered that Javascript is not a particularly good language with which to write a parser/compiler - or at least my limited knowledge of Javascript was insufficient for the task. Input from readers with better Javascript skills would be appreciated here. I have heard that it is now possible to compile C code into WASM. In particular, I hear that people are compiling \"sqlite3.c\" into WASM and running it in browsers. Perhaps an alternative approach would be to write a compact PIC translator in C (using Lemon to implement the parser) then compile the result into WASM for download to the browser. But I don't understand how all of that works. If anybody has any insights and can help guide me through the process, that would be appreciated. (4) By Andreas Kupries (aku) on 2019-11-27 17:52:34 in reply to 2 [link] [source] A question about lemon. IIRC it is a LALR parser generator similar to Bison. Is the generated parser able to execute semantic actions as the AST is built ? This and the ability to reach back to the lexer to control its execution will be necessary for PIC. The reason for that is PIC's macro system, which it has in lieu of proper procedures/functions. Macros can recursively call each other, and to not have this end in tears of infinite recursion this expansion has to be/is curtailed in code not executed, i.e. the untaken branches of if/else conditionals. For the implementation of the language this means that semantic actions have to be executed as the input is lexed and parsed, and then reach back to the lexer with the outcomes of conditions to selectively skip/suppress macro expansion. See pages 6-8 of the dpic docs. With that in mind I wonder if it might be suitable to reuse an existing parser like dpic, instead of writing a new PIC parser, for compilation to WASM. (5) By Richard Hipp (drh) on 2019-11-27 18:05:49 in reply to 4 [link] [source] If you can compile dpic down into a dpic.js file that we can source and use to render PIC on the client-side, that would be great! (7) By Warren Young (wyoung) on 2019-11-27 18:44:20 in reply to 2 [link] [source] it is now possible to compile C code into WASM Yes, and more to the point, WebAssembly support is approaching universal adoption. It won't run in ELinks, but this is a non-core feature that those with such niche needs can ignore. I do wonder if server-side rendering to a virtual SVG would be a better plan, though. (13) By aitap on 2019-11-27 21:35:13 in reply to 2 [link] [source] Is there a place (someone's repo on ChiselApp?) where we could collaborate on the PIC renderer? I seem to have created a Lemon grammar for PIC, but I cannot test it on real inputs yet, because first I'll have to hack the lexer to understand defines and substitute macros. (According to Kernighan and Raymond, they have to be done outside the parser, by pure sting substitution.) Also, I haven't started the work on the \"virtual machine\" to interpret the AST and output SVG. (14) By Stephan Beal (stephan) on 2019-11-27 21:45:42 in reply to 13 [link] [source] If you can wait about 12 hours i can set up a repo for you on fossil.wanderinghorse.net and make you the admin so you can manage collaborators. i have no idea whether the backoffice (forum) parts will work on that site, though, so no guarantees on that. If you want to go ahead and set up a repo, set up your user(s), and email me a link (see https://wanderinghorse.net/home/stephan) where i can download it, i can have it installed in no time tomorrow. My only requirement is that i be allowed to set myself up as an admin/superuser solely for maintenance purposes. (15) By Stephan Beal (stephan) on 2019-11-27 22:07:17 in reply to 13 [link] [source] Alternately, i can set you up an empty repo. Just email me (see previous post) with your preference. (3) By Stephan Beal (stephan) on 2019-11-27 15:41:14 in reply to 1.0 [link] [source] The only skeptic in the room But you are not alone! You will notice that version control is not on either of those lists. That's... kind of mind-blowing. Perhaps they consider it an implicit entry in the list, since it's a level of infrastructure which underpins every aspect of what they do. A \"can't see the forest for the trees\" kind of thing. What limitations or restrictions in Fossil are limiting productivity? My own personal entry in that list is not new: the lack of a library interface. Obviously (for those who don't know this tidbit of history) i intended to tackle that with libfossil, and was well on the way to feature-parity with the fossil binary when chronic RSI (specifically, C8 nerve inflammation/damage in both arms) effectively kicked me out of the game in late 2014 (even now, my typing capacity is still reduced to about 10-15% of what it previously was, and i am still in forced early retirement due to the years-long duration of my medical leave). Coincidentally enough, since yesterday i've been milling over a reboot of that work, focusing initially on features which would make it easier/possible to create custom read-only clients, ignoring the ability to update/merge/etc. for the time being, simply to try to keep the scope in check. The motivation for that came from user elechak, who posted a query based on the mlink table. Being able to plug that type of feature one's own app/page/whatever would empower all sorts of apps we don't/can't currently have. Obviously once we have a library, the gateway is opened for any number of clients, e.g. tooling for reports, CI, and IDEs. That said... my hands cannot reasonably commit (as it were) to that scope of project for the foreseeable future :(. Getting the library up and running again, now that fossil supports multiple hash types (libfossil predates those changes), would require an as-yet-unknown amount of effort. That said, i don't think it would take too awful much to get the read-only functionality working again. So... i'll take this opportunity to bang the drum again and call for \"seriously interested\" coders to get in touch and help make that happen. (Forewarning: it wouldn't be a small amount of code.) While i cannot personally commit to more than \"casual\" amounts of coding, my capacity for technical support is far less bounded and my capacity for moral support is practically limitless ;). One GitHub-er asked me: \"What would it take to get SQLite to move off of Fossil and on to Git.\" LOL! i haven't laughed this hard since you explained the logic behind you getting taller with age! (8) By Warren Young (wyoung) on 2019-11-27 18:59:43 in reply to 3 [link] [source] i intended to tackle that with libfossil Let's not make the same mistake twice: libfossil fell into disrepair because it's a second implementation of the formats and protocols within an ecosystem that's not able to progress as fast as it wants to on a single implementation. (Thus this thread.) Rather than resurrect libfossil, I'd rather see that work went into refactoring the internals of Fossil to extract a library, divorcing the CGI and CLI interfaces from a core that only deals with the repo formats. Some of this is already done well enough that all that's needed is to draw the lines more explicitly, but I want to get to a world where the build process results in a static libfossil2.a that links to the Fossil binary, which can optionally be built as a shared object and linked into an app without linking to libnss, libssl, etc. because the UI is a native GUI tool that only works on local repos and thus doesn't need network clone and sync support. The resulting library should be modular, so that if the third-party app also wants clone and sync support, it can include that as well. Or, Plan C: expand the current web APIs to the point that one could write a full-function Fossil library that did nothing other than make HTTP API calls to a running Fossil instance. (9) By Richard Hipp (drh) on 2019-11-27 19:15:14 in reply to 8 [link] [source] Proposed Forum Enhancement I would like to start a new Forum thread named something like \"Libfossil Reboot\" based on the previous comment. The new \"Libfossil Reboot\" thread would link back to Warren's post, and Warren's post should link forward to the new thread. But it would be a completely new thread, in as much as it is a tangent off of the original. In other words, I'd like to create a simple mechanism to prevent thread hijacking. Somebody with good UX sense, please suggest a suitable interface for this new feature, or if the new idea makes no sense from a UX standpoint, please talk me out of it. (11) By Warren Young (wyoung) on 2019-11-27 19:23:24 in reply to 9 [link] [source] At risk of creating a hijack of a hijack () what if a Moderator or higher level user could mark a post as being a topic fork, which then prompts the user for a new thread title. The existing post becomes the starting post of the new thread under the new title, and unlike current first-post-in-thread, it has an \"in reply to\" in its header pointing back to the other thread. For normal users, all they see is that the Reply button changes to Moved or similar. Clicking it takes them to the start of the other thread, where they can now click Reply. Don't send the user straight to a reply: they need to see the new context first. They might then choose to reply further down-thread in the child topic. (10.1) By Stephan Beal (stephan) on 2019-11-27 19:24:51 edited from 10.0 in reply to 8 [link] [source] (Edit: sorry, Richard - i didn't see your note about the threadjacking until posting this :/. Further discussion will take place in the new thread if/when it starts.) Fair enough. Rather than resurrect libfossil, I'd rather see that work went into refactoring the internals of Fossil to extract a library, divorcing the CGI and CLI interfaces from a core that only deals with the repo formats. That's more or less originally what the goal was. There are, however, fundamental design decisions in fossil which make \"bending it\" into a library unfeasible. First and foremost is its allocation failure policy: simply die. That drastically reduces the amount of code needed (as can be seen when comparing the same algos in fossil core to libfossil - a huge number of lines of code in libfossil are checking/handling result codes and allocation results, where fossil would simply die in most such cases). A library cannot feasibly have a die-on-OOM policy. (That is to say: i wouldn't personally use a library which i knew would outright kill my app if the library encountered an error code or couldn't allocate memory.) That said: i would be completely on-board with such an effort. As Richard put it when it was proposed it in 2011, though, it would require \"a Herculean effort.\" (Though dictionaries/style guides differ on whether Herculean is capitalized or not when used that way, it would certainly have a massive capital H in our context!) Or, Plan C: expand the current web APIs to the point that one could write a full-function Fossil library that did nothing other than make HTTP API calls to a running Fossil instance. That's kinda/sorta what the JSON bits do, and an HTTP-based API is essentially a link-at-call-time DLL. However, a JSON-based API would be a particularly poor choice for certain functionality, most notably transferring binary files of arbitrary sizes, as doing so requires encoding into a form JSON can use (huge strings or massive arrays of integers (byte values), both of which are larger than the binaries themselves). (12) By Warren Young (wyoung) on 2019-11-27 19:37:21 in reply to 10.0 [link] [source] Understand, I don't mean that your intent in creating the library was a mistake, only that we now know that it clearly didn't attract a big enough user community that you had to fend off the tendency to fork the libary when you had to give up on it, because you had multiple people all wanting to continue it. Knowing this, we shouldn't repeat the decision. Contrast Fossil: I can see this project forking if something drastic happened to drh or to the project direction. We've got enough people actively interested in working on it that we would have to fight off a tendency to create multiple forks and not just one new successor project. I point that out not to show a danger to Fossil, but to show that it has a bus factor above 1. Years now after libfossil stopped moving, I think we can say that it had a bus factor of exactly 1. :( Therefore, efforts going forward should make Fossil itself more powerful, not divide precious resources. As to the matter of difficulty, that's why I proposed my previously-unnamed Plan B as a refactoring effort, rather than reimplementation. It's something we can simply choose to strive toward, one bite at a time. On the efficiency of a JSON API, many HTTP APIs switch content types to match the needs of the call. JSON returns may be the default, but binary and HTML returns are also legal based on context. Some calls may even have multiple forms. In that sense, Fossil UI is the HTML API to this library already, and /raw is the binary interface. All I'm proposing is wrapping this in a way that you can use inside, say, a Fossil plugin for a programmer's text editor. (6) By Warren Young (wyoung) on 2019-11-27 18:40:00 in reply to 1.1 [link] [source] Git is used by nearly everyone and is the defacto standard, and so that is what they use. That's pretty much the \"Windows\" argument. Windows hasn't gone away in the face of smartphones, Chromebooks, and the cloud, but it has been pushed to the margins wherever practical. It's probably a large part of the reason that the Windows phone and tablet efforts didn't go anywhere. It's the point of view of an entrenched victor who becomes blind to the fact that they are now entrenched, and consequently can no longer move. I'm not even sure this is a conscious choice. Perhaps a better metaphor is Gulliver strapped down by thousands of Lilliputians using ropes woven from process, standardization, and interdependency. Gulliver could choose to rise, but he'd destroy all the ropes in doing so. 1. Poor tracking of files across renames That's not the number one issue, just the most recent request. :) Fixing this would be a nice quality of life improvement for those of us already using Fossil, but I doubt it factors very high in anyone's decision about whether to use Fossil, or whether to leave Fossil for something else. 2. Inability to do shallow clones That goes to scalability, and it probably does have a lot to do with Fossil's adoption curve. It is true that most software projects aren't Linux-scale in terms of SLOC and complexity of development and thus don't benefit from elements of Git's design that serve the rare characteristics of the Linux kernel development project. Nevertheless, it is also true that there are a lot of software projects bigger and more complex than SQLite yet still much smaller than Linux and thus could benefit if Fossil supported large-scale projects better. 3. Poor ticket UX Now that we have Markdown in tickets (!!) my only serious complaint about it is the two-step filing process. I see the value of the simplified filing interface for normal users, but when I'm logged in as a Setup user, it's annoying to not have all of the ticket fields available to me to fill out at the same time as the initial filing. This could be controlled by a user cap saying whether that user or category of users sees the full ticket UI on filing. Speaking of user caps, another thing I anticipate is support for various SSO technologies: LDAP, OpenID, Oauth, SAML, etc. That in turn would require some way to map groups in those systems to user categories in Fossil, which would probably have to expand beyond the current hard-coded set of four. 4. Inability to subscribe to notifications for changes to specific tickets or files. That sounds like it'd be pretty easy to implement. All it wants is someone to scratch the itch. Not me; don't care. :) 5. Difficulty implementing actions that are triggered by new check-ins and/or new tickets. Server-side Tcl hooks? I think the hardest part is making such hooks enforcing in the face of distributed clones. If I check something into my local repo but a push to the parent repo fails because of a Tcl hook refusing the new material, what then? LLVM...downstream tools...stuck with having to rebase everything. Wouldn't a release branch (as opposed to release and version tags) solve this? Downstream projects could point at the branch they care about, which would get only a linear sequence of changes, pushed there by a release manager. You can implement development, feature freeze, slush, and code freeze branches in this way within Fossil today, progressively restricting changes until a frozen blob of code lands on the release branch. The only thing that might be nice in this regard is locking branches to particular users. Fossil's DVCS nature makes absolute locking impossible, but part of such organized thinking is an assumption that the users aren't actively malicious. If low-level Dev A checks something into the feature freeze branch directly, Fossil already has tools to fix that, and beyond that, it's a social/admin issue, not a tech issue. GitHub is moving away from being a Git Repository hosting company They're just moving up-stack, which is necessary to avoid becoming another commodity provider of Git service. If they'd continued to focus on Git, they'd be in direct competition with Digital Ocean and /usr/bin/git. They have to move up-stack to continue propping up those Silicon Valley salaries. Fossil doesn't have the same need. It merely needs to remain useful to its users. I think this discussion should remain focused on that and possibly on attracting new users. But Fossil doesn't have the same business needs as GitHub, Inc., a Subsidiary of Microsoft Corporation. (24) By anonymous on 2020-01-27 22:04:38 in reply to 6 [link] [source] If I check something into my local repo but a push to the parent repo fails because of a Tcl hook refusing the new material, what then? What happens when you push to a parent repo where you don't have commit privilege? (25) By Warren Young (wyetr) on 2020-01-27 22:31:29 in reply to 24 [link] [source] I see where you're trying to go with that point, but the difference is that when you try to push without check-in capability it's expected to be permanent until you get an account with that capability, then permanently allowed as long as your account remains active. To compare that with the denial from hooks, you're saying that a remote piece of software can effectively activate or deactivate my check-in capability on each artifact pushed. Keep in mind that Fossil writes its check-ins durably first to the local blockchain, then syncs the result to the remote system. This means that once local artifacts are denied by the remote, they'll be denied until the remote changes its rules. If we assume that the rules implemented by the Tcl commit hook are good and wisely crafted, then this means the local repo will forever have durable artifacts that can never be pushed. The local repo will keep pushing them, and the remote will keep running the commit hook logic over them, which will keep rejecting them. All of this is fairly anti-Fossil, which tries very hard to make all clones identical. If Fossil gets anything like this feature, I think it must be done by the hook scripts being somehow sent down with the clone and kept up to date there, so that the hook can run locally before the local blockchain gets modified. (26) By Warren Young (wyoung) on 2020-01-28 19:15:48 in reply to 25 [link] [source] I think it must be done by the hook scripts being somehow sent down with the clone ...which of course introduces portability problems: you have to ensure that the hook scripts will in fact run correctly on all of the clones, else you break those clones. \"But it worked on my machine!\" doesn't fly when your remote contributors stop being able to check work in just because they're on a different host OS, or are missing some third-party Tcl package, or whatever. However, a night's sleep and a shower have provided an answer: instead of the hook script outright rejecting \"bad\" checkins, what if Fossil treats commit hook rejection as a signal to automatically create a branch for the new artifacts so that other work can continue on the original branch? Then the problem can be corrected in public, which is very much pro-Fossil. We could obviously do this in a post-facto way, running such problem-detecting scripts on the server and having it automatically move artifacts, but making it part of the sync protocol with integrated hook scripts means the remote clone could be notified immediately that their check-in has been moved to a branch. The hook script could even return a detailed message explaining the problem, which could guide the remote user to a fix. This new scheme means the content is never lost, it is committed locally and sync'd remotely, but it's set aside until the problem can be resolved. (27) By anonymous on 2020-01-28 21:18:18 in reply to 26 [link] [source] what if Fossil treats commit hook rejection as a signal to automatically create a branch for the new artifacts so that other work can continue on the original branch? I like this idea a lot. but making it part of the sync protocol with integrated hook scripts means the remote clone could be notified immediately that their check-in has been moved to a branch. I would hope that the hook script would be able to supply the new branch name, \"calculated\" by whatever rules the core project team deems appropriate. Maybe the hook script could return a string with a branch name and a message separated by some reasonable delimiter. (28) By Warren Young (wyetr) on 2020-01-28 21:51:28 in reply to 27 [link] [source] the hook script would be able to supply the new branch name Sure. It'd allow a form of namespacing, where each hook script gets its own branch naming prefix so you can tell which script shunted the commit aside. Examples: code-style-abcd1234: code rejected by a site-configured automatic code style checker ci-cd-abcd1234: this commit breaks the CI/CD build; implicitly an async hook temp-code-abcd1234: this commit appears to contain some sort of temporary code: debug logging, FIXME comments, #if 0 blocks, etc. Maybe the hook script could return a string with a branch name and a message separated by some reasonable delimiter. Tcl lets you return a list of values; it isn't limited to returning a single scalar value: return { 401, \"code style check failed\", \"code-style-$nonce\", $output_from_style_checker } That is, an error code, a brief message, a branch name to move the commit to if not null, and an optional long message that expands on the brief message. (16) By Kevin (KevinYouren) on 2019-11-29 23:17:40 in reply to 1.1 [link] [source] Richard, I think \"rebase\" may be useful to entities who want to remove or hide errors or worse. (22) By anonymous on 2019-12-05 19:34:16 in reply to 16 [link] [source] I think \"rebase\" may be useful to entities who want to remove or hide errors or worse. If you develop on a private branch, then merge the final commit to trunk then no one need ever see any mess there might be on the branch. (Down side: Fossil doesn't track merges from a private branch, not even in the repo where the private branch exists.) (23) By Kevin (KevinYouren) on 2019-12-06 01:28:25 in reply to 22 [link] [source] Thank you, I didn't know that. (29.1) Originally by anonymous with edits by Stephan Beal (stephan) on 2020-12-15 16:37:59 from 29.0 in reply to 23 [link] [source] Deleted (30) By Stephan Beal (stephan) on 2020-12-15 16:43:07 in reply to 23 [link] [source] Reminder to moderators: We have had one spammer today who's tried to sneak in hyperlinks to a \"bathroom mold removal\" product as a sneaky link on a single period, making it effectively invisible when reading the markdown-processed post but clearly visible in the unparsed content. Please always check the unparsed content of posts for new and anonymous users before approving them. This particular piece of spam was first posted by a new account (which i deleted) and then again, later in the day, as an anonymous post (which snuck past moderation but i coincidentally recognized as identical to the previous post, so deleted it). The devious part is that such posts appear to be topical, but they try to sneak in spam links which are easy for the eye to overlook. (17) By Saagar Jha (saagarjha) on 2019-11-30 18:11:51 in reply to 1.1 [link] [source] This quote (from Tom Kremenek of the Swift project) is saying that your workflow will be defined and constrained by the tools you use. Might you be talking about Ted, the Swift Project Lead? (18) By Richard Hipp (drh) on 2019-11-30 18:49:20 in reply to 17 [link] [source] Yes, it seems likely that I wrote his name down incorrectly in my notes. Sorry about that, Ted. (19) By anonymous on 2019-12-01 23:45:14 in reply to 1.1 [link] [source] I'd like to copy this post for a personal blogpost (of course with reference) is such doing welcomed and permitted? (20) By Richard Hipp (drh) on 2019-12-02 01:07:45 in reply to 19 [link] [source] (21) By Offray (offray) on 2019-12-02 21:41:56 in reply to 1.2 [link] [source] One GitHub-er asked me: \"What would it take to get SQLite to move off of Fossil and on to Git.\" Just to be clear to everyone reading this: That will never happen. Fossil was specifically designed to support SQLite development and does so remarkably well. Fossil fills a different niche than does Git/GitHub. Fossil will continue to be supported and enhanced by me (as well as others) well into the foreseeable future. You could counter ask him/her: \"What would it take to get full support to Fossil inside GitHub and make it a first class citizen there?\" ;-) Cheers, ",
        "_version_":1718939883097030656},
      {
        "story_id":[21534619],
        "story_author":["tombrm"],
        "story_descendants":[3],
        "story_score":[10],
        "story_time":["2019-11-14T11:39:21Z"],
        "story_title":"Show HN: Respresso – localization and design asset optimizer for iOS and Android",
        "search":["Show HN: Respresso – localization and design asset optimizer for iOS and Android",
          "https://respresso.io/",
          "Save hours with efficient collaboration Manage your resources in multiplatform environments. Fonts Easy integration of custom fonts Localizations Modify localization texts or add a new language to your project, without developers Images Change or resize an image anytime and keep in sync on all platforms App icons No more generating thousands of icon sizes, just use one SVG for all platforms. Colors No more incorrect guideline colors. Your designers will have the ability to set the perfect colors to be used on all platforms Raw Easy access to all your common config files(JSON, XML, YAML) Customize Respresso Extend Respressos functionality or connect your work tools like Slack, Teams, Jenkins etc. Version control All resources are under version control. You can easily lock your assets version and reuse it later. Be agile and spare development time Collaborate on assets with your team members or customer from anywhere in a transparent way to boost your productivity. It automatically transforms and delivers to your project without assistance. Your assets will be ready for use almost immediately. It takes care of your digital assets (images, texts, colors, fonts, etc.) across multiple platforms and projects. Help your team focus during the development stage. Developers code, designers deliver graphics, the marketing team, translators manage your localization and this is just the beginning of stress-free development. Start using in 3 simple steps Easy as pie 1-minute setup Upload your origin resources Sync converted resources across multiple platforms Respresso is simple but powerful Respresso can easily be integrated into your build process and also works well with your Continuous Integration tools. Respresso manages your resources Convert your resources automatically to platform-specific formats such as VectorDrawable for Android, PDF for iOS etc. Synchronize with your project in build-time regardless of which platform you use What else is it good for? Versioned resources and repeatable build support for CI & CD A better team experience via team and project creation. It provides you with a way to track every change in each project and send feedback. For example you will be notified when someone changes the key of a localization you use Well-separated roles spell correction and translation without developer intervention fix any graphics files and change icons without coding skills rebrand the application by changing colors and app icons Enforce naming conventions to enhance code quality Try our image converter Simply upload your chosen SVG file and download the generated resources as VectorDrawable for Android and PDF for iOS. Get access for Free Once you create an account you can try Respresso for free, for iOS, Android and Web frontend projects, too. Do you have any question? ",
          "Respresso helps mobile developers by automatically optimizing their design assets for iOS and Android and comes with a live localization feature.<p>It's still in beta and all feedback is highly appreciated.",
          "What is the pricing?"],
        "story_type":["ShowHN"],
        "url":"https://respresso.io/",
        "url_text":"Save hours with efficient collaboration Manage your resources in multiplatform environments. Fonts Easy integration of custom fonts Localizations Modify localization texts or add a new language to your project, without developers Images Change or resize an image anytime and keep in sync on all platforms App icons No more generating thousands of icon sizes, just use one SVG for all platforms. Colors No more incorrect guideline colors. Your designers will have the ability to set the perfect colors to be used on all platforms Raw Easy access to all your common config files(JSON, XML, YAML) Customize Respresso Extend Respressos functionality or connect your work tools like Slack, Teams, Jenkins etc. Version control All resources are under version control. You can easily lock your assets version and reuse it later. Be agile and spare development time Collaborate on assets with your team members or customer from anywhere in a transparent way to boost your productivity. It automatically transforms and delivers to your project without assistance. Your assets will be ready for use almost immediately. It takes care of your digital assets (images, texts, colors, fonts, etc.) across multiple platforms and projects. Help your team focus during the development stage. Developers code, designers deliver graphics, the marketing team, translators manage your localization and this is just the beginning of stress-free development. Start using in 3 simple steps Easy as pie 1-minute setup Upload your origin resources Sync converted resources across multiple platforms Respresso is simple but powerful Respresso can easily be integrated into your build process and also works well with your Continuous Integration tools. Respresso manages your resources Convert your resources automatically to platform-specific formats such as VectorDrawable for Android, PDF for iOS etc. Synchronize with your project in build-time regardless of which platform you use What else is it good for? Versioned resources and repeatable build support for CI & CD A better team experience via team and project creation. It provides you with a way to track every change in each project and send feedback. For example you will be notified when someone changes the key of a localization you use Well-separated roles spell correction and translation without developer intervention fix any graphics files and change icons without coding skills rebrand the application by changing colors and app icons Enforce naming conventions to enhance code quality Try our image converter Simply upload your chosen SVG file and download the generated resources as VectorDrawable for Android and PDF for iOS. Get access for Free Once you create an account you can try Respresso for free, for iOS, Android and Web frontend projects, too. Do you have any question? ",
        "comments.comment_id":[21534748,
          21554836],
        "comments.comment_author":["tombrm",
          "deca6cda37d0"],
        "comments.comment_descendants":[0,
          1],
        "comments.comment_time":["2019-11-14T12:06:56Z",
          "2019-11-16T20:42:53Z"],
        "comments.comment_text":["Respresso helps mobile developers by automatically optimizing their design assets for iOS and Android and comes with a live localization feature.<p>It's still in beta and all feedback is highly appreciated.",
          "What is the pricing?"],
        "id":"4c04cc13-5014-4e0e-8465-2611a4c2322a",
        "_version_":1718939880443084800},
      {
        "story_id":[21270269],
        "story_author":["fastbmk_com"],
        "story_descendants":[10],
        "story_score":[12],
        "story_time":["2019-10-16T13:48:43Z"],
        "story_title":"It’s Time to Get over That Stored Procedure Aversion You Have",
        "search":["It’s Time to Get over That Stored Procedure Aversion You Have",
          "https://rob.conery.io/2015/02/20/its-time-to-get-over-that-stored-procedure-aversion-you-have/",
          "404 Page does not exist! Please use the search bar at the top or visit our homepage! Explore Microsoft (3) Travel (4) Opinion (17) Ruby (2) Movies (1) Database (9) Postgres (31) Node (22) Tekpub (10) JavaScript (13) Speaking (3) Screencasts (4) Databsae (1) Elixir (12) Career (11) Writing (8) Firebase (5) Syndication (15) CS (1) Books (1) News (1) ",
          "2015...",
          "Erm. No. There's plenty of reasons why \"rock star developers\" and plenty of blogs discuss this. ORM usage is only as good as the stored procedures that are written. If devs don't understand an ORM they're bound it abuse it, because it's easy to do so.<p>My biggest issue with stored procedures is change management. I've seen plenty of crazy custom tools that try to use Git, SVN, and just file system copies of Stores procedures. All of em have been 'lovely' to work with to say the least.<p>Having your core business logic on the server is amazing due to version control. If it's performance that's a bottle neck I would say 95% (my POV) it's a developer that doesn't understand what the ORM is doing. The beauty of using EF or another micro ORM (dapper) is that SQL performance is optimized from caching the SQL hash. And the option is still there to execute a stored procedure.<p>Theres absolutely times for using stored procedures, but I'd sacrifice a little bit of performance for maintainability."],
        "story_type":["Normal"],
        "url":"https://rob.conery.io/2015/02/20/its-time-to-get-over-that-stored-procedure-aversion-you-have/",
        "url_text":"404 Page does not exist! Please use the search bar at the top or visit our homepage! Explore Microsoft (3) Travel (4) Opinion (17) Ruby (2) Movies (1) Database (9) Postgres (31) Node (22) Tekpub (10) JavaScript (13) Speaking (3) Screencasts (4) Databsae (1) Elixir (12) Career (11) Writing (8) Firebase (5) Syndication (15) CS (1) Books (1) News (1) ",
        "comments.comment_id":[21275626,
          21277550],
        "comments.comment_author":["purple_ducks",
          "alunchbox"],
        "comments.comment_descendants":[1,
          1],
        "comments.comment_time":["2019-10-16T21:01:20Z",
          "2019-10-17T00:04:11Z"],
        "comments.comment_text":["2015...",
          "Erm. No. There's plenty of reasons why \"rock star developers\" and plenty of blogs discuss this. ORM usage is only as good as the stored procedures that are written. If devs don't understand an ORM they're bound it abuse it, because it's easy to do so.<p>My biggest issue with stored procedures is change management. I've seen plenty of crazy custom tools that try to use Git, SVN, and just file system copies of Stores procedures. All of em have been 'lovely' to work with to say the least.<p>Having your core business logic on the server is amazing due to version control. If it's performance that's a bottle neck I would say 95% (my POV) it's a developer that doesn't understand what the ORM is doing. The beauty of using EF or another micro ORM (dapper) is that SQL performance is optimized from caching the SQL hash. And the option is still there to execute a stored procedure.<p>Theres absolutely times for using stored procedures, but I'd sacrifice a little bit of performance for maintainability."],
        "id":"e5b66f9b-3b0c-4156-b9be-b3a6bb20b540",
        "_version_":1718939872984563712},
      {
        "story_id":[19508974],
        "story_author":["Sami_Lehtinen"],
        "story_descendants":[30],
        "story_score":[40],
        "story_time":["2019-03-28T03:31:47Z"],
        "story_title":"New Bitbucket Cloud features",
        "search":["New Bitbucket Cloud features",
          "https://bitbucket.org/blog/6-new-bitbucket-cloud-features-that-spark-joy",
          "The return of warmer weather in recent weeks means spring has finally sprung. For the Bitbucket team, this is the time of year we are especially reminded of the potential in clearing out clutter in your day-to-day, giving you back time to do the things that truly spark joy. Over the past few months, we've been burrowed indoors busy working on new features that will help you and your team automate the most tedious parts of your workflows, connect more seamlessly with third-party tools, and work more collaboratively. Let's jump in and get tidying: Branching: 1. Smart Feature Branching: Good branching practices arekey to a strong Git workflow. That's why Bitbucket has alwaysbeen invested in helping teams build powerful and efficient branching workflows. Last year, we announcedbranching model support, which made it easier to make consistent naming decisions when creating branches. Now, we're releasing additional functionalityto make it easier to build and use a branching workflow with your team. When you configure a new branch,we will take your existing branching model settings into account to make smarter defaults for your branching flow.We've also made some additional changes to support standard Gitflow and other branching flows from planning through to production. Here's what you can expect in this latest release: Smarter Defaults:Bring smart, automated source and target branch selection to your flow.Based off your configuration choices, Bitbucket will now save you timeby automatically suggesting the source when you create a new branch. When you're finished working on the branch and create a PR, we will also suggest the target branch from your branch type. Filtering in Branch List Page:Save time with more granular and accurate search results on the Branch List page. Weve now added the ability to select the branch type. Before, if you were to look up release branch types, you would see results by text match (i.e. everything that matched the word \"release). Now, if you select \"Release,\" only release branch types will be shown on the page.If you want to multi-task, you can now also search for multiple branch types at one time! Branch Permissions by Branch Type: If you have your branching model configured, you can quickly browse and set your permissions based off these naming conventions. The permissions will be linked to your configuration, eliminating manual typing, browsing, and chance for error. After you select the branch type, simply select the group you'd like to give permission to and lock that workflow down. Because branch permissions and your configuration are linked, even if you decide to change your branch name settings, say from \"hotfix\" to \"hot,\" your permissions will remain unaffected. Integrations 2.Bitbucket for Gmail:Our new feature-packed Atlassian Cloud for Gmail add-on allows you to take action on and get contextual Bitbucket information without leaving your inbox. You can read all the comments on a Bitbucket issue, view information about a pull request or build, or even see who has or hasn't approved a pull request, all from within Gmail. Best of all, you can take action on the notifications you receive via email, meaning less switching between apps to get stuff done. Perform the following actions from your inbox and see it automatically update in Bitbucket Cloud: Comment directly on an issue Comment on a pull request Merge a pull request Re-run a pipeline build The Atlassian app also integrates with Jira, allowing joint Jira and Bitbucket Cloud users to get a full picture of the status of their work on the go. 3.Further Integration With Jira:We've now leveled up the integration between Jira Software Cloud and Bitbucket Cloud so teams can gain continuous visibility from their backlog through to release. Teams can now seeBitbucketPipelines release information within Jira Software. In Bitbucket, teams will be able to see Jira issues within the Deployment preview screen. With this new integration, you'll have: Release status at your fingertips:Release information that was once locked in your deployment tools, only visible to your dev team, is now visible to any team member working in Jira Software. Deeper history of releases: Understand the history of releases by clicking on the release status and view all of the environments ? test, staging or production ? that each issue has been deployed to, along with information about when the changes were deployed to that environment. Jira x Bitbucket in Deployment:Using the Deployment preview, development teams can quickly validate what code they are about to promote to another environment before hitting Deploy. This preview provides them with all the associated commits, diff, and now Jira issues, for a higher level of context. By clicking on the issue key, they can view and even edit the Jira issue without leaving Bitbucket. Pipelines 4.After scripts in Pipelines:Bitbucket Pipelines now supports the ability to run scripts at the end of a step execution, regardless of whether the step has failed or succeeded, using the after-script keyword. A BITBUCKET_EXIT_CODE environment variable is also exported, which contains the status code returned from the last command run in script. The commands in theafter-scriptsection run until one fails, and the result of this section does not affect the overall build status. 5.Faster feedback on merges:We're excited to be delivering on our highest voted feature request for Bitbucket Pipelines! You can now configure Bitbucket Pipelines to run when you create or update a pull request, providing developers a faster feedback loop on breaking changes and giving reviewers more confidence when approving changes. With a pull-requests pipeline, a temporary merge containing the changes in the pull request and the destination branch is created, and the defined pipeline is run on the merge result. By doing this, both the pull request creator and reviewers can confirm changes are valid, that the code builds, and tests pass before the pull request is merged into the destination branch. This helps developers prevent errors earlier in their workflow and gives reviewers more confidence when approving pull requests. The pull-requests pipeline also gives you the flexibility to configure certain pieces to run only when the changes are ready to be merged. Pull Requests 6.Default Pull Request Descriptions:Repository admins can now add predefined text to all new pull requests with the Default description repository setting. Remind authors to include additional information (i.e. screenshots, links, business context about the change, testing notes) to help reviewers understand the proposed changes more thoroughly and complete code review more quickly. Ready to take these features for a spin? If you're new to Bitbucket,sign up for a Bitbucket Cloud account. Already a Bitbucket customer? Check out the individual blogs for instructions on enabling your desired feature. Have more specific questions about this post? Reach out to us on Twitter to get the information you need. ",
          "I'm amazed bitbucket hasn't got a move on and actually made the product good since Github and Gitlab are 10000x better and speeding away at rocket pace.",
          "For people only criticizing Bitbucket, take a look at this costs comparison <a href=\"https://imgur.com/a/eZPIHXW\" rel=\"nofollow\">https://imgur.com/a/eZPIHXW</a><p>Source: <a href=\"https://www.process.st/bitbucket-vs-github-version-control-software/\" rel=\"nofollow\">https://www.process.st/bitbucket-vs-github-version-control-s...</a>"],
        "story_type":["Normal"],
        "url":"https://bitbucket.org/blog/6-new-bitbucket-cloud-features-that-spark-joy",
        "comments.comment_id":[19509212,
          19511083],
        "comments.comment_author":["baroffoos",
          "edpichler"],
        "comments.comment_descendants":[3,
          1],
        "comments.comment_time":["2019-03-28T04:19:19Z",
          "2019-03-28T11:23:42Z"],
        "comments.comment_text":["I'm amazed bitbucket hasn't got a move on and actually made the product good since Github and Gitlab are 10000x better and speeding away at rocket pace.",
          "For people only criticizing Bitbucket, take a look at this costs comparison <a href=\"https://imgur.com/a/eZPIHXW\" rel=\"nofollow\">https://imgur.com/a/eZPIHXW</a><p>Source: <a href=\"https://www.process.st/bitbucket-vs-github-version-control-software/\" rel=\"nofollow\">https://www.process.st/bitbucket-vs-github-version-control-s...</a>"],
        "id":"8361f5d5-b9c3-421c-aee2-414268e09555",
        "url_text":"The return of warmer weather in recent weeks means spring has finally sprung. For the Bitbucket team, this is the time of year we are especially reminded of the potential in clearing out clutter in your day-to-day, giving you back time to do the things that truly spark joy. Over the past few months, we've been burrowed indoors busy working on new features that will help you and your team automate the most tedious parts of your workflows, connect more seamlessly with third-party tools, and work more collaboratively. Let's jump in and get tidying: Branching: 1. Smart Feature Branching: Good branching practices arekey to a strong Git workflow. That's why Bitbucket has alwaysbeen invested in helping teams build powerful and efficient branching workflows. Last year, we announcedbranching model support, which made it easier to make consistent naming decisions when creating branches. Now, we're releasing additional functionalityto make it easier to build and use a branching workflow with your team. When you configure a new branch,we will take your existing branching model settings into account to make smarter defaults for your branching flow.We've also made some additional changes to support standard Gitflow and other branching flows from planning through to production. Here's what you can expect in this latest release: Smarter Defaults:Bring smart, automated source and target branch selection to your flow.Based off your configuration choices, Bitbucket will now save you timeby automatically suggesting the source when you create a new branch. When you're finished working on the branch and create a PR, we will also suggest the target branch from your branch type. Filtering in Branch List Page:Save time with more granular and accurate search results on the Branch List page. Weve now added the ability to select the branch type. Before, if you were to look up release branch types, you would see results by text match (i.e. everything that matched the word \"release). Now, if you select \"Release,\" only release branch types will be shown on the page.If you want to multi-task, you can now also search for multiple branch types at one time! Branch Permissions by Branch Type: If you have your branching model configured, you can quickly browse and set your permissions based off these naming conventions. The permissions will be linked to your configuration, eliminating manual typing, browsing, and chance for error. After you select the branch type, simply select the group you'd like to give permission to and lock that workflow down. Because branch permissions and your configuration are linked, even if you decide to change your branch name settings, say from \"hotfix\" to \"hot,\" your permissions will remain unaffected. Integrations 2.Bitbucket for Gmail:Our new feature-packed Atlassian Cloud for Gmail add-on allows you to take action on and get contextual Bitbucket information without leaving your inbox. You can read all the comments on a Bitbucket issue, view information about a pull request or build, or even see who has or hasn't approved a pull request, all from within Gmail. Best of all, you can take action on the notifications you receive via email, meaning less switching between apps to get stuff done. Perform the following actions from your inbox and see it automatically update in Bitbucket Cloud: Comment directly on an issue Comment on a pull request Merge a pull request Re-run a pipeline build The Atlassian app also integrates with Jira, allowing joint Jira and Bitbucket Cloud users to get a full picture of the status of their work on the go. 3.Further Integration With Jira:We've now leveled up the integration between Jira Software Cloud and Bitbucket Cloud so teams can gain continuous visibility from their backlog through to release. Teams can now seeBitbucketPipelines release information within Jira Software. In Bitbucket, teams will be able to see Jira issues within the Deployment preview screen. With this new integration, you'll have: Release status at your fingertips:Release information that was once locked in your deployment tools, only visible to your dev team, is now visible to any team member working in Jira Software. Deeper history of releases: Understand the history of releases by clicking on the release status and view all of the environments ? test, staging or production ? that each issue has been deployed to, along with information about when the changes were deployed to that environment. Jira x Bitbucket in Deployment:Using the Deployment preview, development teams can quickly validate what code they are about to promote to another environment before hitting Deploy. This preview provides them with all the associated commits, diff, and now Jira issues, for a higher level of context. By clicking on the issue key, they can view and even edit the Jira issue without leaving Bitbucket. Pipelines 4.After scripts in Pipelines:Bitbucket Pipelines now supports the ability to run scripts at the end of a step execution, regardless of whether the step has failed or succeeded, using the after-script keyword. A BITBUCKET_EXIT_CODE environment variable is also exported, which contains the status code returned from the last command run in script. The commands in theafter-scriptsection run until one fails, and the result of this section does not affect the overall build status. 5.Faster feedback on merges:We're excited to be delivering on our highest voted feature request for Bitbucket Pipelines! You can now configure Bitbucket Pipelines to run when you create or update a pull request, providing developers a faster feedback loop on breaking changes and giving reviewers more confidence when approving changes. With a pull-requests pipeline, a temporary merge containing the changes in the pull request and the destination branch is created, and the defined pipeline is run on the merge result. By doing this, both the pull request creator and reviewers can confirm changes are valid, that the code builds, and tests pass before the pull request is merged into the destination branch. This helps developers prevent errors earlier in their workflow and gives reviewers more confidence when approving pull requests. The pull-requests pipeline also gives you the flexibility to configure certain pieces to run only when the changes are ready to be merged. Pull Requests 6.Default Pull Request Descriptions:Repository admins can now add predefined text to all new pull requests with the Default description repository setting. Remind authors to include additional information (i.e. screenshots, links, business context about the change, testing notes) to help reviewers understand the proposed changes more thoroughly and complete code review more quickly. Ready to take these features for a spin? If you're new to Bitbucket,sign up for a Bitbucket Cloud account. Already a Bitbucket customer? Check out the individual blogs for instructions on enabling your desired feature. Have more specific questions about this post? Reach out to us on Twitter to get the information you need. ",
        "_version_":1718939832149868544},
      {
        "story_id":[21055373],
        "story_author":["gk1"],
        "story_descendants":[5],
        "story_score":[26],
        "story_time":["2019-09-24T00:47:23Z"],
        "story_title":"Building a Modern CI/CD Pipeline in the Serverless Era with GitOps",
        "search":["Building a Modern CI/CD Pipeline in the Serverless Era with GitOps",
          "https://aws.amazon.com/blogs/aws/building-a-modern-ci-cd-pipeline-in-the-serverless-era-with-gitops/",
          "Guest post by AWS Community Hero Shimon Tolts, CTO and co-founder at Datree.io. He specializes in developer tools and infrastructure, running a company that is 100% serverless. In recent years, there was a major transition in the way you build and ship software. This was mainly around microservices, splitting code into small components, using infrastructure as code, and using Git as the single source of truth that glues it all together. In this post, I discuss the transition and the different steps of modern software development to showcase the possible solutions for the serverless world. In addition, I list useful tools that were designed for this era. What is serverless? Before I dive into the wonderful world of serverless development and tooling, heres what I mean by serverless. The AWS website talks about four main benefits: No server management. Flexible scaling. Pay for value. Automated high availability. To me, serverless is any infrastructure that you dont have to manage and scale yourself. At my company Datree.io, we run 95% of our workload on AWS Fargate and 5% on AWS Lambda. We are a serverless company; we have zero Amazon EC2 instances in our AWS account. For more information, see the following: Datree.io case study Migrating to AWS ECS Fargate in production CON320: Operational Excellence w/ Containerized Workloads Using AWS Fargate (re:Invent 2018) What is GitOps? Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. According to Luis Faceira, a CI/CD consultant, GitOps is a way of working. You might look at it as an approach in which everything starts and ends with Git. Here are some key concepts: Git as the SINGLE source of truth of a system Git as the SINGLE place where we operate (create, change and destroy) ALL environments ALL changes are observable/verifiable. How you built software before the cloud Back in the waterfall pre-cloud era, you used to have separate teams for development, testing, security, operations, monitoring, and so on. Nowadays, in most organizations, there is a transition to full developer autonomy and developers owning the entire production path. The developer is the King or Queen :) Those teams (Ops/Security/IT/etc) used to be gatekeepers to validate and control every developer change. Now they have become more of a satellite unit that drives policy and sets best practices and standards. They are no longer the production bottleneck, so they provide organization-wide platforms and enablement solutions. Everything is codified With the transition into full developer ownership of the entire pipeline, developers automated everything. We have more code than ever, and processes that used to be manual are now described in code. This is a good transition, in my opinion. Here are some of the benefits: Automation: By storing all things as code, everything can be automated, reused, and re-created in moments. Immutable: If anything goes wrong, create it again from the stored configuration. Versioning: Changes can be applied and reverted, and are tracked to a single user who made the change. GitOps: Git has become the single source of truth The second major transition is that now everything is in one place! Git is the place where all of the code is stored and where all operations are initiated. Whether its testing, building, packaging, or releasing, nowadays everything is triggered through pull requests. This is amplified by the codification of everything. Useful tools in the serverless era There are many useful tools in the market, here is a list of ones that were designed for serverless. Code Always store your code in a source control system. In recent years, more and more functions are codified, such as, BI, ops, security, and AI. For new developers, it is not always obvious that they should use source control for some functionality. GitHub AWS CodeCommit GitLab BitBucket Build and test The most common mistake I see is manually configuring build jobs in the GUI. This might be good for a small POC but it is not scalable. You should have your job codified and inside your Git repository. Here are some tools to help with building and testing: AWS CodeBuild CodeFresh GitHub Actions Jenkins-x CircleCI TravisCI Security and governance When working in a serverless way, you end up having many Git repos. The number of code packages can be overwhelming. The demand for unified code standards remains as it was but now it is much harder to enforce it on top of your R&D org. Here are some tools that might help you with the challenge: Snyk Datree PureSec Aqua Protego Bundle and release Building a serverless application is connecting microservices into one unit. For example, you might be using Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. Instead of configuring each one separately, you should use a bundler to hold the configuration in one place. That allows for easy versioning and replication of the app for several environments. Here are a couple of bundlers: Serverless Framework AWS Serverless Application Model (AWS SAM) Package When working with many different serverless components, you should create small packages of tools to be able to import across different Lambda functions. You can use a language-specific store like npm or RubyGems, or use a more holistic solution. Here are several package artifact stores that allow hosting for multiple programming languages: GitHub Package Registry Jfrog Artifactory Sonatype Nexus Monitor This part is especially tricky when working with serverless applications, as everything is split into small pieces. Its important to use monitoring tools that support this mode of work. Here are some tools that can handle serverless: Rookout Amazon CloudWatch Epsagon Lumigo NewRelic DataDog Summary The serverless era brings many transitions along with it like a codification of the entire pipeline and Git being the single source of truth. This doesnt mean that the same problems that we use to have like security, logging and more disappeared, you should continue addressing them and leveraging tools that enable you to focus on your business. ",
          "Lol at the image where before microservices, it was a single monolithic application.<p>FaaS has its use cases but this “serverless for every solution” or 100% serverless marketing is annoying and NOT customer centric (Amazon said they were earths most customer centric company).",
          "This post was disappointing when it first ran: I was expecting some content after the basic intro but then it’s just a couple of saved Google searches with no discussion or analysis. It would have been a lot more interesting if they’d discussed anything about the trade offs of the different services or what they liked about a particular combination."],
        "story_type":["Normal"],
        "url":"https://aws.amazon.com/blogs/aws/building-a-modern-ci-cd-pipeline-in-the-serverless-era-with-gitops/",
        "comments.comment_id":[21057480,
          21063347],
        "comments.comment_author":["tracer4201",
          "acdha"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-09-24T07:36:59Z",
          "2019-09-24T18:46:43Z"],
        "comments.comment_text":["Lol at the image where before microservices, it was a single monolithic application.<p>FaaS has its use cases but this “serverless for every solution” or 100% serverless marketing is annoying and NOT customer centric (Amazon said they were earths most customer centric company).",
          "This post was disappointing when it first ran: I was expecting some content after the basic intro but then it’s just a couple of saved Google searches with no discussion or analysis. It would have been a lot more interesting if they’d discussed anything about the trade offs of the different services or what they liked about a particular combination."],
        "id":"0ce15098-c7bd-42fa-96b1-175823bf6874",
        "url_text":"Guest post by AWS Community Hero Shimon Tolts, CTO and co-founder at Datree.io. He specializes in developer tools and infrastructure, running a company that is 100% serverless. In recent years, there was a major transition in the way you build and ship software. This was mainly around microservices, splitting code into small components, using infrastructure as code, and using Git as the single source of truth that glues it all together. In this post, I discuss the transition and the different steps of modern software development to showcase the possible solutions for the serverless world. In addition, I list useful tools that were designed for this era. What is serverless? Before I dive into the wonderful world of serverless development and tooling, heres what I mean by serverless. The AWS website talks about four main benefits: No server management. Flexible scaling. Pay for value. Automated high availability. To me, serverless is any infrastructure that you dont have to manage and scale yourself. At my company Datree.io, we run 95% of our workload on AWS Fargate and 5% on AWS Lambda. We are a serverless company; we have zero Amazon EC2 instances in our AWS account. For more information, see the following: Datree.io case study Migrating to AWS ECS Fargate in production CON320: Operational Excellence w/ Containerized Workloads Using AWS Fargate (re:Invent 2018) What is GitOps? Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. According to Luis Faceira, a CI/CD consultant, GitOps is a way of working. You might look at it as an approach in which everything starts and ends with Git. Here are some key concepts: Git as the SINGLE source of truth of a system Git as the SINGLE place where we operate (create, change and destroy) ALL environments ALL changes are observable/verifiable. How you built software before the cloud Back in the waterfall pre-cloud era, you used to have separate teams for development, testing, security, operations, monitoring, and so on. Nowadays, in most organizations, there is a transition to full developer autonomy and developers owning the entire production path. The developer is the King or Queen :) Those teams (Ops/Security/IT/etc) used to be gatekeepers to validate and control every developer change. Now they have become more of a satellite unit that drives policy and sets best practices and standards. They are no longer the production bottleneck, so they provide organization-wide platforms and enablement solutions. Everything is codified With the transition into full developer ownership of the entire pipeline, developers automated everything. We have more code than ever, and processes that used to be manual are now described in code. This is a good transition, in my opinion. Here are some of the benefits: Automation: By storing all things as code, everything can be automated, reused, and re-created in moments. Immutable: If anything goes wrong, create it again from the stored configuration. Versioning: Changes can be applied and reverted, and are tracked to a single user who made the change. GitOps: Git has become the single source of truth The second major transition is that now everything is in one place! Git is the place where all of the code is stored and where all operations are initiated. Whether its testing, building, packaging, or releasing, nowadays everything is triggered through pull requests. This is amplified by the codification of everything. Useful tools in the serverless era There are many useful tools in the market, here is a list of ones that were designed for serverless. Code Always store your code in a source control system. In recent years, more and more functions are codified, such as, BI, ops, security, and AI. For new developers, it is not always obvious that they should use source control for some functionality. GitHub AWS CodeCommit GitLab BitBucket Build and test The most common mistake I see is manually configuring build jobs in the GUI. This might be good for a small POC but it is not scalable. You should have your job codified and inside your Git repository. Here are some tools to help with building and testing: AWS CodeBuild CodeFresh GitHub Actions Jenkins-x CircleCI TravisCI Security and governance When working in a serverless way, you end up having many Git repos. The number of code packages can be overwhelming. The demand for unified code standards remains as it was but now it is much harder to enforce it on top of your R&D org. Here are some tools that might help you with the challenge: Snyk Datree PureSec Aqua Protego Bundle and release Building a serverless application is connecting microservices into one unit. For example, you might be using Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. Instead of configuring each one separately, you should use a bundler to hold the configuration in one place. That allows for easy versioning and replication of the app for several environments. Here are a couple of bundlers: Serverless Framework AWS Serverless Application Model (AWS SAM) Package When working with many different serverless components, you should create small packages of tools to be able to import across different Lambda functions. You can use a language-specific store like npm or RubyGems, or use a more holistic solution. Here are several package artifact stores that allow hosting for multiple programming languages: GitHub Package Registry Jfrog Artifactory Sonatype Nexus Monitor This part is especially tricky when working with serverless applications, as everything is split into small pieces. Its important to use monitoring tools that support this mode of work. Here are some tools that can handle serverless: Rookout Amazon CloudWatch Epsagon Lumigo NewRelic DataDog Summary The serverless era brings many transitions along with it like a codification of the entire pipeline and Git being the single source of truth. This doesnt mean that the same problems that we use to have like security, logging and more disappeared, you should continue addressing them and leveraging tools that enable you to focus on your business. ",
        "_version_":1718939868401238016}]
  }}
