{
  "responseHeader":{
    "status":0,
    "QTime":9},
  "response":{"numFound":54,"start":0,"numFoundExact":true,"docs":[
      {
        "story_id":[20559240],
        "story_author":["mooreds"],
        "story_descendants":[7],
        "story_score":[23],
        "story_time":["2019-07-29T20:45:37Z"],
        "story_title":"Learn a little jq, Awk and sed",
        "search":["Learn a little jq, Awk and sed",
          "https://letterstoanewdeveloper.com/2019/07/29/learn-a-little-jq-awk-and-sed/",
          "Dear new developer, You are probably going to be dealing with text files sometime during your development career. These could be plain text, csv, or json. They may have data you want to get out, or log files you want to examine. You may be transforming from one format to another. Now, if this is a regular occurrence, you may want to build a script or a program around this problem (or use a third party service which aggregates everything together). But sometimes these files are one offs. Or you use them once in a blue moon. And it can take a little while to write a script, look at the libraries, and put it all together. Another alternative is to learn some of the unix tools available on the command line. Here are three that I consider table stakes. awk This is a multi purpose line processing utility. I often want to grab lines of a log file and figure out what is going on. Heres a few lines of a log file: 54.147.20.92 - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" \"Slackbot 1.0 (+https://api.slack.com/robots)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" If I want to see only the ip addresses (assuming these are all in a file called logs.txt), Id run something like: $ awk '{print $1}' logs.txt 54.147.20.92 185.24.234.106 185.24.234.106 Theres lots more, but you can see that youd be able to slice and dice delimited data pretty easily. Heres a great article which dives in further. sed This is another line utility. You can use it for all kinds of things, but I primarily use it to do search and replace on a file. Suppose you had the same log file, but you wanted to anonymize the the ip address and the user agent. Perhaps youre going to ship them off for long term storage or something. You can easily remove this with a couple of sed commands. $ sed 's/^[^ ]*//' logs.txt |sed 's/\"[^\"]*\"$//' - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" Yes, it looks like line noise, but this is the power of regular expressions. Theyre in every language (though with slight variations) and worth learning. sed gives you the power of regular expressions at the command line for processing files. I dont have a great sed tutorial Ive found, but googling shows a number. jq If you work on the command line with modern software at all, you have encountered json. Its used for configuration files and data transmission. Sometimes you get an array of json and you just want to pick out certain attributes of it. Tools like sed and awk fail at this, because they are used to newlines separating records, not curly braces and commas. Sure, you could use regular expressions to parse simple json, and there are times when Ive done this. But a far better tool is jq. Im not as savvy with this as with the others, but have used it whenever Im dealing with an API that delivers json (which is most modern ones). I can pull the API down with curl (another great tool) and parse it out with jq. I can put these all in a script and have the exploration be repeatable. I did this a few months ago when I was doing some exploration of an elastic search system. I crafted the queries with curl and then used jq to parse out the results so that I could make some sense of this. Yes, I could have done this with a real programming language, but it would have taken longer. I could also have used a gui tool like postman, but then it would not have been replicable. sed and awk should be on every system you run across; jq is non standard, but easy to install. Its worth spending some time getting to know these tools. So next time you are processing a text file and need to extract just a bit of it, reach for sed and awk. Next time you get a hairy json file and you are peering at it, look at jq. I think youll be happy with the result. Sincerely, Dan Published July 29, 2019October 17, 2020 ",
          "When I first saw someone using zsh (omz), I was awe-struck.<p>Same thing happens to the person sitting next to me when I pipe an output to jq.",
          "however, I find jq not so friendly piping its output to other programs."],
        "story_type":["Normal"],
        "url":"https://letterstoanewdeveloper.com/2019/07/29/learn-a-little-jq-awk-and-sed/",
        "comments.comment_id":[20562095,
          20562592],
        "comments.comment_author":["envolt",
          "magoon"],
        "comments.comment_descendants":[0,
          3],
        "comments.comment_time":["2019-07-30T04:14:52Z",
          "2019-07-30T06:42:37Z"],
        "comments.comment_text":["When I first saw someone using zsh (omz), I was awe-struck.<p>Same thing happens to the person sitting next to me when I pipe an output to jq.",
          "however, I find jq not so friendly piping its output to other programs."],
        "id":"85e67926-007e-40ad-a80f-4911316b72fa",
        "url_text":"Dear new developer, You are probably going to be dealing with text files sometime during your development career. These could be plain text, csv, or json. They may have data you want to get out, or log files you want to examine. You may be transforming from one format to another. Now, if this is a regular occurrence, you may want to build a script or a program around this problem (or use a third party service which aggregates everything together). But sometimes these files are one offs. Or you use them once in a blue moon. And it can take a little while to write a script, look at the libraries, and put it all together. Another alternative is to learn some of the unix tools available on the command line. Here are three that I consider table stakes. awk This is a multi purpose line processing utility. I often want to grab lines of a log file and figure out what is going on. Heres a few lines of a log file: 54.147.20.92 - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" \"Slackbot 1.0 (+https://api.slack.com/robots)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" If I want to see only the ip addresses (assuming these are all in a file called logs.txt), Id run something like: $ awk '{print $1}' logs.txt 54.147.20.92 185.24.234.106 185.24.234.106 Theres lots more, but you can see that youd be able to slice and dice delimited data pretty easily. Heres a great article which dives in further. sed This is another line utility. You can use it for all kinds of things, but I primarily use it to do search and replace on a file. Suppose you had the same log file, but you wanted to anonymize the the ip address and the user agent. Perhaps youre going to ship them off for long term storage or something. You can easily remove this with a couple of sed commands. $ sed 's/^[^ ]*//' logs.txt |sed 's/\"[^\"]*\"$//' - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" Yes, it looks like line noise, but this is the power of regular expressions. Theyre in every language (though with slight variations) and worth learning. sed gives you the power of regular expressions at the command line for processing files. I dont have a great sed tutorial Ive found, but googling shows a number. jq If you work on the command line with modern software at all, you have encountered json. Its used for configuration files and data transmission. Sometimes you get an array of json and you just want to pick out certain attributes of it. Tools like sed and awk fail at this, because they are used to newlines separating records, not curly braces and commas. Sure, you could use regular expressions to parse simple json, and there are times when Ive done this. But a far better tool is jq. Im not as savvy with this as with the others, but have used it whenever Im dealing with an API that delivers json (which is most modern ones). I can pull the API down with curl (another great tool) and parse it out with jq. I can put these all in a script and have the exploration be repeatable. I did this a few months ago when I was doing some exploration of an elastic search system. I crafted the queries with curl and then used jq to parse out the results so that I could make some sense of this. Yes, I could have done this with a real programming language, but it would have taken longer. I could also have used a gui tool like postman, but then it would not have been replicable. sed and awk should be on every system you run across; jq is non standard, but easy to install. Its worth spending some time getting to know these tools. So next time you are processing a text file and need to extract just a bit of it, reach for sed and awk. Next time you get a hairy json file and you are peering at it, look at jq. I think youll be happy with the result. Sincerely, Dan Published July 29, 2019October 17, 2020 ",
        "_version_":1718939855262580736},
      {
        "story_id":[20811829],
        "story_author":["weinzierl"],
        "story_descendants":[59],
        "story_score":[273],
        "story_time":["2019-08-27T16:54:20Z"],
        "story_title":"Curl exercises",
        "search":["Curl exercises",
          "https://jvns.ca/blog/2019/08/27/curl-exercises/",
          "Recently Ive been interested in how people learn things. I was reading Kathy Sierras great book Badass: Making Users Awesome. It talks about the idea of deliberate practice. The idea is that you find a small micro-skill that can be learned in maybe 3 sessions of 45 minutes, and focus on learning that micro-skill. So, as an exercise, I was trying to think of a computer skill that I thought could be learned in 3 45-minute sessions. I thought that making HTTP requests with curl might be a skill like that, so here are some curl exercises as an experiment! whats curl? curl is a command line tool for making HTTP requests. I like it because its an easy way to test that servers or APIs are doing what I think, but its a little confusing at first! Heres a drawing explaining curls most important command line arguments (which is page 6 of my Bite Size Networking zine). You can click to make it bigger. fluency is valuable With any command line tool, I think having fluency is really helpful. Its really nice to be able to just type in the thing you need. For example recently I was testing out the Gumroad API and I was able to just type in: curl https://api.gumroad.com/v2/sales \\ -d \"access_token=<SECRET>\" \\ -X GET -d \"before=2016-09-03\" and get things working from the command line. 21 curl exercises These exercises are about understanding how to make different kinds of HTTP requests with curl. Theyre a little repetitive on purpose. They exercise basically everything I do with curl. To keep it simple, were going to make a lot of our requests to the same website: https://httpbin.org. httpbin is a service that accepts HTTP requests and then tells you what request you made. Request https://httpbin.org Request https://httpbin.org/anything. httpbin.org/anything will look at the request you made, parse it, and echo back to you what you requested. curls default is to make a GET request. Make a POST request to https://httpbin.org/anything Make a GET request to https://httpbin.org/anything, but this time add some query parameters (set value=panda). Request googles robots.txt file (www.google.com/robots.txt) Make a GET request to https://httpbin.org/anything and set the header User-Agent: elephant. Make a DELETE request to https://httpbin.org/anything Request https://httpbin.org/anything and also get the response headers Make a POST request to https://httpbin.org/anything with the JSON body {\"value\": \"panda\"} Make the same POST request as the previous exercise, but set the Content-Type header to application/json (because POST requests need to have a content type that matches their body). Look at the json field in the response to see the difference from the previous one. Make a GET request to https://httpbin.org/anything and set the header Accept-Encoding: gzip (what happens? why?) Put a bunch of a JSON in a file and then make a POST request to https://httpbin.org/anything with the JSON in that file as the body Make a request to https://httpbin.org/image and set the header Accept: image/png. Save the output to a PNG file and open the file in an image viewer. Try the same thing with different Accept: headers. Make a PUT request to https://httpbin.org/anything Request https://httpbin.org/image/jpeg, save it to a file, and open that file in your image editor. Request https://www.twitter.com. Youll get an empty response. Get curl to show you the response headers too, and try to figure out why the response was empty. Make any request to https://httpbin.org/anything and just set some nonsense headers (like panda: elephant) Request https://httpbin.org/status/404 and https://httpbin.org/status/200. Request them again and get curl to show the response headers. Request https://httpbin.org/anything and set a username and password (with -u username:password) Download the Twitter homepage (https://twitter.com) in Spanish by setting the Accept-Language: es-ES header. Make a request to the Stripe API with curl. (see https://stripe.com/docs/development for how, they give you a test API key). Try making exactly the same request to https://httpbin.org/anything. ",
          "I really like using HTTPie (<a href=\"https://httpie.org\" rel=\"nofollow\">https://httpie.org</a>). Much friendlier syntax then curl, formats the output...<p>Also works great with fx (<a href=\"https://github.com/antonmedv/fx\" rel=\"nofollow\">https://github.com/antonmedv/fx</a>). Just pipe the output from HTTPie and you get easy json processing.",
          "This is more SysAdmin related, but one power-curl function I use atleast 30 times a day is this alias I have in my .bash_aliases<p>This will output the HTTP status code for a given URL.<p><pre><code>     alias hstat=\"curl -o /dev/null --silent --head --write-out '%{http_code}\\n'\" $1  \n</code></pre>\nExample:<p><pre><code>  $ hstat google.com\n  301\n</code></pre>\nI also use curl as an 'uptime monitor' by adding onto that code section.  (a file with a list of URLs, and \"if http status code !=200\" then email me.)<p>There are variations on this all over the place but I really depend on it and I like it."],
        "story_type":["Normal"],
        "url":"https://jvns.ca/blog/2019/08/27/curl-exercises/",
        "url_text":"Recently Ive been interested in how people learn things. I was reading Kathy Sierras great book Badass: Making Users Awesome. It talks about the idea of deliberate practice. The idea is that you find a small micro-skill that can be learned in maybe 3 sessions of 45 minutes, and focus on learning that micro-skill. So, as an exercise, I was trying to think of a computer skill that I thought could be learned in 3 45-minute sessions. I thought that making HTTP requests with curl might be a skill like that, so here are some curl exercises as an experiment! whats curl? curl is a command line tool for making HTTP requests. I like it because its an easy way to test that servers or APIs are doing what I think, but its a little confusing at first! Heres a drawing explaining curls most important command line arguments (which is page 6 of my Bite Size Networking zine). You can click to make it bigger. fluency is valuable With any command line tool, I think having fluency is really helpful. Its really nice to be able to just type in the thing you need. For example recently I was testing out the Gumroad API and I was able to just type in: curl https://api.gumroad.com/v2/sales \\ -d \"access_token=<SECRET>\" \\ -X GET -d \"before=2016-09-03\" and get things working from the command line. 21 curl exercises These exercises are about understanding how to make different kinds of HTTP requests with curl. Theyre a little repetitive on purpose. They exercise basically everything I do with curl. To keep it simple, were going to make a lot of our requests to the same website: https://httpbin.org. httpbin is a service that accepts HTTP requests and then tells you what request you made. Request https://httpbin.org Request https://httpbin.org/anything. httpbin.org/anything will look at the request you made, parse it, and echo back to you what you requested. curls default is to make a GET request. Make a POST request to https://httpbin.org/anything Make a GET request to https://httpbin.org/anything, but this time add some query parameters (set value=panda). Request googles robots.txt file (www.google.com/robots.txt) Make a GET request to https://httpbin.org/anything and set the header User-Agent: elephant. Make a DELETE request to https://httpbin.org/anything Request https://httpbin.org/anything and also get the response headers Make a POST request to https://httpbin.org/anything with the JSON body {\"value\": \"panda\"} Make the same POST request as the previous exercise, but set the Content-Type header to application/json (because POST requests need to have a content type that matches their body). Look at the json field in the response to see the difference from the previous one. Make a GET request to https://httpbin.org/anything and set the header Accept-Encoding: gzip (what happens? why?) Put a bunch of a JSON in a file and then make a POST request to https://httpbin.org/anything with the JSON in that file as the body Make a request to https://httpbin.org/image and set the header Accept: image/png. Save the output to a PNG file and open the file in an image viewer. Try the same thing with different Accept: headers. Make a PUT request to https://httpbin.org/anything Request https://httpbin.org/image/jpeg, save it to a file, and open that file in your image editor. Request https://www.twitter.com. Youll get an empty response. Get curl to show you the response headers too, and try to figure out why the response was empty. Make any request to https://httpbin.org/anything and just set some nonsense headers (like panda: elephant) Request https://httpbin.org/status/404 and https://httpbin.org/status/200. Request them again and get curl to show the response headers. Request https://httpbin.org/anything and set a username and password (with -u username:password) Download the Twitter homepage (https://twitter.com) in Spanish by setting the Accept-Language: es-ES header. Make a request to the Stripe API with curl. (see https://stripe.com/docs/development for how, they give you a test API key). Try making exactly the same request to https://httpbin.org/anything. ",
        "comments.comment_id":[20813303,
          20813591],
        "comments.comment_author":["AndrejKolar",
          "UI_at_80x24"],
        "comments.comment_descendants":[1,
          2],
        "comments.comment_time":["2019-08-27T19:03:29Z",
          "2019-08-27T19:33:10Z"],
        "comments.comment_text":["I really like using HTTPie (<a href=\"https://httpie.org\" rel=\"nofollow\">https://httpie.org</a>). Much friendlier syntax then curl, formats the output...<p>Also works great with fx (<a href=\"https://github.com/antonmedv/fx\" rel=\"nofollow\">https://github.com/antonmedv/fx</a>). Just pipe the output from HTTPie and you get easy json processing.",
          "This is more SysAdmin related, but one power-curl function I use atleast 30 times a day is this alias I have in my .bash_aliases<p>This will output the HTTP status code for a given URL.<p><pre><code>     alias hstat=\"curl -o /dev/null --silent --head --write-out '%{http_code}\\n'\" $1  \n</code></pre>\nExample:<p><pre><code>  $ hstat google.com\n  301\n</code></pre>\nI also use curl as an 'uptime monitor' by adding onto that code section.  (a file with a list of URLs, and \"if http status code !=200\" then email me.)<p>There are variations on this all over the place but I really depend on it and I like it."],
        "id":"479aad0d-1e38-41c8-bd9e-8e0d8e2dd933",
        "_version_":1718939863438327808},
      {
        "story_id":[20062064],
        "story_author":["lelf"],
        "story_descendants":[25],
        "story_score":[193],
        "story_time":["2019-05-31T15:57:18Z"],
        "story_title":"Semantic: Parsing, analyzing, and comparing source code across many languages",
        "search":["Semantic: Parsing, analyzing, and comparing source code across many languages",
          "https://github.com/github/semantic",
          "semantic is a Haskell library and command line tool for parsing, analyzing, and comparing source code. In a hurry? Check out our documentation of example uses for the semantic command line tool. Table of Contents Usage Language support Development Technology and architecture Licensing Usage Run semantic --help for complete list of up-to-date options. Parse Usage: semantic parse [--sexpression | (--json-symbols|--symbols) | --proto-symbols | --show | --quiet] [FILES...] Generate parse trees for path(s) Available options: --sexpression Output s-expression parse trees (default) --json-symbols,--symbols Output JSON symbol list --proto-symbols Output protobufs symbol list --show Output using the Show instance (debug only, format subject to change without notice) --quiet Don't produce output, but show timing stats -h,--help Show this help text Language support Language Parse AST Symbols Stack graphs Ruby JavaScript TypeScript Python Go PHP Java JSON JSX TSX CodeQL Haskell Used for code navigation on github.com. Supported Partial support Under development - N/A Development semantic requires at least GHC 8.10.1 and Cabal 3.0. We strongly recommend using ghcup to sandbox GHC versions, as GHC packages installed through your OS's package manager may not install statically-linked versions of the GHC boot libraries. semantic currently builds only on Unix systems; users of other operating systems may wish to use the Docker images. We use cabal's Nix-style local builds for development. To get started quickly: git clone git@github.com:github/semantic.git cd semantic script/bootstrap cabal v2-build all cabal v2-run semantic:test cabal v2-run semantic:semantic -- --help You can also use the Bazel build system for development. To learn more about Bazel and why it might give you a better development experience, check the build documentation. git clone git@github.com:github/semantic.git cd semantic script/bootstrap-bazel bazel build //... stack as a build tool is not officially supported; there is unofficial stack.yaml support available, though we cannot make guarantees as to its stability. Technology and architecture Architecturally, semantic: Generates per-language Haskell syntax types based on tree-sitter grammar definitions. Reads blobs from a filesystem or provided via a protocol buffer request. Returns blobs or performs analysis. Renders output in one of many supported formats. Throughout its lifestyle, semantic has leveraged a number of interesting algorithms and techniques, including: Myers' algorithm (SES) as described in the paper An O(ND) Difference Algorithm and Its Variations RWS as described in the paper RWS-Diff: Flexible and Efficient Change Detection in Hierarchical Data. Open unions and data types la carte. An implementation of Abstracting Definitional Interpreters extended to work with an la carte representation of syntax terms. Contributions Contributions are welcome! Please see our contribution guidelines and our code of conduct for details on how to participate in our community. Licensing Semantic is licensed under the MIT license. ",
          "Looks very interesting - would benefit from showing some examples and/or use cases",
          "The late 20th century, early 00's version:<p><a href=\"http://www.program-transformation.org/Transform/CodeCrawler\" rel=\"nofollow\">http://www.program-transformation.org/Transform/CodeCrawler</a><p>(And MOOSE)"],
        "story_type":["Normal"],
        "url":"https://github.com/github/semantic",
        "url_text":"semantic is a Haskell library and command line tool for parsing, analyzing, and comparing source code. In a hurry? Check out our documentation of example uses for the semantic command line tool. Table of Contents Usage Language support Development Technology and architecture Licensing Usage Run semantic --help for complete list of up-to-date options. Parse Usage: semantic parse [--sexpression | (--json-symbols|--symbols) | --proto-symbols | --show | --quiet] [FILES...] Generate parse trees for path(s) Available options: --sexpression Output s-expression parse trees (default) --json-symbols,--symbols Output JSON symbol list --proto-symbols Output protobufs symbol list --show Output using the Show instance (debug only, format subject to change without notice) --quiet Don't produce output, but show timing stats -h,--help Show this help text Language support Language Parse AST Symbols Stack graphs Ruby JavaScript TypeScript Python Go PHP Java JSON JSX TSX CodeQL Haskell Used for code navigation on github.com. Supported Partial support Under development - N/A Development semantic requires at least GHC 8.10.1 and Cabal 3.0. We strongly recommend using ghcup to sandbox GHC versions, as GHC packages installed through your OS's package manager may not install statically-linked versions of the GHC boot libraries. semantic currently builds only on Unix systems; users of other operating systems may wish to use the Docker images. We use cabal's Nix-style local builds for development. To get started quickly: git clone git@github.com:github/semantic.git cd semantic script/bootstrap cabal v2-build all cabal v2-run semantic:test cabal v2-run semantic:semantic -- --help You can also use the Bazel build system for development. To learn more about Bazel and why it might give you a better development experience, check the build documentation. git clone git@github.com:github/semantic.git cd semantic script/bootstrap-bazel bazel build //... stack as a build tool is not officially supported; there is unofficial stack.yaml support available, though we cannot make guarantees as to its stability. Technology and architecture Architecturally, semantic: Generates per-language Haskell syntax types based on tree-sitter grammar definitions. Reads blobs from a filesystem or provided via a protocol buffer request. Returns blobs or performs analysis. Renders output in one of many supported formats. Throughout its lifestyle, semantic has leveraged a number of interesting algorithms and techniques, including: Myers' algorithm (SES) as described in the paper An O(ND) Difference Algorithm and Its Variations RWS as described in the paper RWS-Diff: Flexible and Efficient Change Detection in Hierarchical Data. Open unions and data types la carte. An implementation of Abstracting Definitional Interpreters extended to work with an la carte representation of syntax terms. Contributions Contributions are welcome! Please see our contribution guidelines and our code of conduct for details on how to participate in our community. Licensing Semantic is licensed under the MIT license. ",
        "comments.comment_id":[20062368,
          20062741],
        "comments.comment_author":["anentropic",
          "stcredzero"],
        "comments.comment_descendants":[1,
          0],
        "comments.comment_time":["2019-05-31T16:27:42Z",
          "2019-05-31T17:01:44Z"],
        "comments.comment_text":["Looks very interesting - would benefit from showing some examples and/or use cases",
          "The late 20th century, early 00's version:<p><a href=\"http://www.program-transformation.org/Transform/CodeCrawler\" rel=\"nofollow\">http://www.program-transformation.org/Transform/CodeCrawler</a><p>(And MOOSE)"],
        "id":"63eae11f-6ef8-484d-941d-a9a87e1b8bd9",
        "_version_":1718939845461540864},
      {
        "story_id":[19427332],
        "story_author":["an-tao"],
        "story_descendants":[105],
        "story_score":[189],
        "story_time":["2019-03-19T01:47:40Z"],
        "story_title":"Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
        "search":["Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
          "https://github.com/an-tao/drogon",
          "English | | Overview Drogon is a C++14/17-based HTTP application framework. Drogon can be used to easily build various types of web application server programs using C++. Drogon is the name of a dragon in the American TV series \"Game of Thrones\" that I really like. Drogon is a cross-platform framework, It supports Linux, macOS, FreeBSD, OpenBSD, HaikuOS, and Windows. Its main features are as follows: Use a non-blocking I/O network lib based on epoll (kqueue under macOS/FreeBSD) to provide high-concurrency, high-performance network IO, please visit the TFB Tests Results for more details; Provide a completely asynchronous programming mode; Support Http1.0/1.1 (server side and client side); Based on template, a simple reflection mechanism is implemented to completely decouple the main program framework, controllers and views. Support cookies and built-in sessions; Support back-end rendering, the controller generates the data to the view to generate the Html page. Views are described by CSP template files, C++ codes are embedded into Html pages through CSP tags. And the drogon command-line tool automatically generates the C++ code files for compilation; Support view page dynamic loading (dynamic compilation and loading at runtime); Provide a convenient and flexible routing solution from the path to the controller handler; Support filter chains to facilitate the execution of unified logic (such as login verification, Http Method constraint verification, etc.) before handling HTTP requests; Support https (based on OpenSSL); Support WebSocket (server side and client side); Support JSON format request and response, very friendly to the Restful API application development; Support file download and upload; Support gzip, brotli compression transmission; Support pipelining; Provide a lightweight command line tool, drogon_ctl, to simplify the creation of various classes in Drogon and the generation of view code; Support non-blocking I/O based asynchronously reading and writing database (PostgreSQL and MySQL(MariaDB) database); Support asynchronously reading and writing sqlite3 database based on thread pool; Support Redis with asynchronous reading and writing; Support ARM Architecture; Provide a convenient lightweight ORM implementation that supports for regular object-to-database bidirectional mapping; Support plugins which can be installed by the configuration file at load time; Support AOP with build-in joinpoints. Support C++ coroutines A very simple example Unlike most C++ frameworks, the main program of the drogon application can be kept clean and simple. Drogon uses a few tricks to decouple controllers from the main program. The routing settings of controllers can be done through macros or configuration file. Below is the main program of a typical drogon application: #include <drogon/drogon.h> using namespace drogon; int main() { app().setLogPath(\"./\") .setLogLevel(trantor::Logger::kWarn) .addListener(\"0.0.0.0\", 80) .setThreadNum(16) .enableRunAsDaemon() .run(); } It can be further simplified by using configuration file as follows: #include <drogon/drogon.h> using namespace drogon; int main() { app().loadConfigFile(\"./config.json\").run(); } Drogon provides some interfaces for adding controller logic directly in the main() function, for example, user can register a handler like this in Drogon: app().registerHandler(\"/test?username={name}\", [](const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback, const std::string &name) { Json::Value json; json[\"result\"]=\"ok\"; json[\"message\"]=std::string(\"hello,\")+name; auto resp=HttpResponse::newHttpJsonResponse(json); callback(resp); }, {Get,\"LoginFilter\"}); While such interfaces look intuitive, they are not suitable for complex business logic scenarios. Assuming there are tens or even hundreds of handlers that need to be registered in the framework, isn't it a better practice to implement them separately in their respective classes? So unless your logic is very simple, we don't recommend using above interfaces. Instead, we can create an HttpSimpleController as follows: /// The TestCtrl.h file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class TestCtrl:public drogon::HttpSimpleController<TestCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN PATH_ADD(\"/test\",Get); PATH_LIST_END }; /// The TestCtrl.cc file #include \"TestCtrl.h\" void TestCtrl::asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) { //write your application logic here auto resp = HttpResponse::newHttpResponse(); resp->setBody(\"<p>Hello, world!</p>\"); resp->setExpiredTime(0); callback(resp); } Most of the above programs can be automatically generated by the command line tool drogon_ctl provided by drogon (The command is drogon_ctl create controller TestCtrl). All the user needs to do is add their own business logic. In the example, the controller returns a Hello, world! string when the client accesses the http://ip/test URL. For JSON format response, we create the controller as follows: /// The header file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class JsonCtrl : public drogon::HttpSimpleController<JsonCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN //list path definitions here; PATH_ADD(\"/json\", Get); PATH_LIST_END }; /// The source file #include \"JsonCtrl.h\" void JsonCtrl::asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) { Json::Value ret; ret[\"message\"] = \"Hello, World!\"; auto resp = HttpResponse::newHttpJsonResponse(ret); callback(resp); } Let's go a step further and create a demo RESTful API with the HttpController class, as shown below (Omit the source file): /// The header file #pragma once #include <drogon/HttpController.h> using namespace drogon; namespace api { namespace v1 { class User : public drogon::HttpController<User> { public: METHOD_LIST_BEGIN //use METHOD_ADD to add your custom processing function here; METHOD_ADD(User::getInfo, \"/{id}\", Get); //path is /api/v1/User/{arg1} METHOD_ADD(User::getDetailInfo, \"/{id}/detailinfo\", Get); //path is /api/v1/User/{arg1}/detailinfo METHOD_ADD(User::newUser, \"/{name}\", Post); //path is /api/v1/User/{arg1} METHOD_LIST_END //your declaration of processing function maybe like this: void getInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void getDetailInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void newUser(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, std::string &&userName); public: User() { LOG_DEBUG << \"User constructor!\"; } }; } // namespace v1 } // namespace api As you can see, users can use the HttpController to map paths and parameters at the same time. This is a very convenient way to create a RESTful API application. In addition, you can also find that all handler interfaces are in asynchronous mode, where the response is returned by a callback object. This design is for performance reasons because in asynchronous mode the drogon application can handle a large number of concurrent requests with a small number of threads. After compiling all of the above source files, we get a very simple web application. This is a good start. For more information, please visit the wiki or DocsForge Contributions Every contribution is welcome. Please refer to the contribution guidelines for more information. ",
          "2 small things.<p>a) Show me the code to start with, don't send me digging for it. A SimpleController example as the very first thing would help give a feel for the project, and makes me more likely to consider the project.<p>b) If there's an easier way (like the drogon_ctl utility at the start of Quickstart [0]), show that first, and the more detailed way second.<p>Other than that, it looks great. I've used libmicrohttpd a few times, so a bit less of an overhead always looks great.<p>[0] <a href=\"https://github.com/an-tao/drogon/wiki/quick-start\" rel=\"nofollow\">https://github.com/an-tao/drogon/wiki/quick-start</a>",
          "This is cool, and I like it. Very Haskell like, which is a compliment in my book.<p>But one thing that surprises me is that folks are essentially sleeping on HTTP/2. HTTP/2 is just a hell of a lot better in most every dimension. It's better for handshake latency, it's better for bandwidth in most cases, it's better for eliminating excess SSL overhead and also, it's kinda easier to write client libraries for, because it's so much simpler (although the parallel and concurrent nature of connections will challenge a lot of programmers).<p>It's not bad to see a new contender in this space, but it's surprising that it isn't http/2 first. Is there a good reason for this? It's busted through 90% support on caniuse, so it's hard to make an argument that adoption holds it back."],
        "story_type":["ShowHN"],
        "url":"https://github.com/an-tao/drogon",
        "comments.comment_id":[19427737,
          19427925],
        "comments.comment_author":["shakna",
          "KirinDave"],
        "comments.comment_descendants":[1,
          8],
        "comments.comment_time":["2019-03-19T03:06:56Z",
          "2019-03-19T03:45:52Z"],
        "comments.comment_text":["2 small things.<p>a) Show me the code to start with, don't send me digging for it. A SimpleController example as the very first thing would help give a feel for the project, and makes me more likely to consider the project.<p>b) If there's an easier way (like the drogon_ctl utility at the start of Quickstart [0]), show that first, and the more detailed way second.<p>Other than that, it looks great. I've used libmicrohttpd a few times, so a bit less of an overhead always looks great.<p>[0] <a href=\"https://github.com/an-tao/drogon/wiki/quick-start\" rel=\"nofollow\">https://github.com/an-tao/drogon/wiki/quick-start</a>",
          "This is cool, and I like it. Very Haskell like, which is a compliment in my book.<p>But one thing that surprises me is that folks are essentially sleeping on HTTP/2. HTTP/2 is just a hell of a lot better in most every dimension. It's better for handshake latency, it's better for bandwidth in most cases, it's better for eliminating excess SSL overhead and also, it's kinda easier to write client libraries for, because it's so much simpler (although the parallel and concurrent nature of connections will challenge a lot of programmers).<p>It's not bad to see a new contender in this space, but it's surprising that it isn't http/2 first. Is there a good reason for this? It's busted through 90% support on caniuse, so it's hard to make an argument that adoption holds it back."],
        "id":"49e580cd-0cf9-46a2-aaf1-c325e441d818",
        "url_text":"English | | Overview Drogon is a C++14/17-based HTTP application framework. Drogon can be used to easily build various types of web application server programs using C++. Drogon is the name of a dragon in the American TV series \"Game of Thrones\" that I really like. Drogon is a cross-platform framework, It supports Linux, macOS, FreeBSD, OpenBSD, HaikuOS, and Windows. Its main features are as follows: Use a non-blocking I/O network lib based on epoll (kqueue under macOS/FreeBSD) to provide high-concurrency, high-performance network IO, please visit the TFB Tests Results for more details; Provide a completely asynchronous programming mode; Support Http1.0/1.1 (server side and client side); Based on template, a simple reflection mechanism is implemented to completely decouple the main program framework, controllers and views. Support cookies and built-in sessions; Support back-end rendering, the controller generates the data to the view to generate the Html page. Views are described by CSP template files, C++ codes are embedded into Html pages through CSP tags. And the drogon command-line tool automatically generates the C++ code files for compilation; Support view page dynamic loading (dynamic compilation and loading at runtime); Provide a convenient and flexible routing solution from the path to the controller handler; Support filter chains to facilitate the execution of unified logic (such as login verification, Http Method constraint verification, etc.) before handling HTTP requests; Support https (based on OpenSSL); Support WebSocket (server side and client side); Support JSON format request and response, very friendly to the Restful API application development; Support file download and upload; Support gzip, brotli compression transmission; Support pipelining; Provide a lightweight command line tool, drogon_ctl, to simplify the creation of various classes in Drogon and the generation of view code; Support non-blocking I/O based asynchronously reading and writing database (PostgreSQL and MySQL(MariaDB) database); Support asynchronously reading and writing sqlite3 database based on thread pool; Support Redis with asynchronous reading and writing; Support ARM Architecture; Provide a convenient lightweight ORM implementation that supports for regular object-to-database bidirectional mapping; Support plugins which can be installed by the configuration file at load time; Support AOP with build-in joinpoints. Support C++ coroutines A very simple example Unlike most C++ frameworks, the main program of the drogon application can be kept clean and simple. Drogon uses a few tricks to decouple controllers from the main program. The routing settings of controllers can be done through macros or configuration file. Below is the main program of a typical drogon application: #include <drogon/drogon.h> using namespace drogon; int main() { app().setLogPath(\"./\") .setLogLevel(trantor::Logger::kWarn) .addListener(\"0.0.0.0\", 80) .setThreadNum(16) .enableRunAsDaemon() .run(); } It can be further simplified by using configuration file as follows: #include <drogon/drogon.h> using namespace drogon; int main() { app().loadConfigFile(\"./config.json\").run(); } Drogon provides some interfaces for adding controller logic directly in the main() function, for example, user can register a handler like this in Drogon: app().registerHandler(\"/test?username={name}\", [](const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback, const std::string &name) { Json::Value json; json[\"result\"]=\"ok\"; json[\"message\"]=std::string(\"hello,\")+name; auto resp=HttpResponse::newHttpJsonResponse(json); callback(resp); }, {Get,\"LoginFilter\"}); While such interfaces look intuitive, they are not suitable for complex business logic scenarios. Assuming there are tens or even hundreds of handlers that need to be registered in the framework, isn't it a better practice to implement them separately in their respective classes? So unless your logic is very simple, we don't recommend using above interfaces. Instead, we can create an HttpSimpleController as follows: /// The TestCtrl.h file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class TestCtrl:public drogon::HttpSimpleController<TestCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN PATH_ADD(\"/test\",Get); PATH_LIST_END }; /// The TestCtrl.cc file #include \"TestCtrl.h\" void TestCtrl::asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) { //write your application logic here auto resp = HttpResponse::newHttpResponse(); resp->setBody(\"<p>Hello, world!</p>\"); resp->setExpiredTime(0); callback(resp); } Most of the above programs can be automatically generated by the command line tool drogon_ctl provided by drogon (The command is drogon_ctl create controller TestCtrl). All the user needs to do is add their own business logic. In the example, the controller returns a Hello, world! string when the client accesses the http://ip/test URL. For JSON format response, we create the controller as follows: /// The header file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class JsonCtrl : public drogon::HttpSimpleController<JsonCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN //list path definitions here; PATH_ADD(\"/json\", Get); PATH_LIST_END }; /// The source file #include \"JsonCtrl.h\" void JsonCtrl::asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) { Json::Value ret; ret[\"message\"] = \"Hello, World!\"; auto resp = HttpResponse::newHttpJsonResponse(ret); callback(resp); } Let's go a step further and create a demo RESTful API with the HttpController class, as shown below (Omit the source file): /// The header file #pragma once #include <drogon/HttpController.h> using namespace drogon; namespace api { namespace v1 { class User : public drogon::HttpController<User> { public: METHOD_LIST_BEGIN //use METHOD_ADD to add your custom processing function here; METHOD_ADD(User::getInfo, \"/{id}\", Get); //path is /api/v1/User/{arg1} METHOD_ADD(User::getDetailInfo, \"/{id}/detailinfo\", Get); //path is /api/v1/User/{arg1}/detailinfo METHOD_ADD(User::newUser, \"/{name}\", Post); //path is /api/v1/User/{arg1} METHOD_LIST_END //your declaration of processing function maybe like this: void getInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void getDetailInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void newUser(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, std::string &&userName); public: User() { LOG_DEBUG << \"User constructor!\"; } }; } // namespace v1 } // namespace api As you can see, users can use the HttpController to map paths and parameters at the same time. This is a very convenient way to create a RESTful API application. In addition, you can also find that all handler interfaces are in asynchronous mode, where the response is returned by a callback object. This design is for performance reasons because in asynchronous mode the drogon application can handle a large number of concurrent requests with a small number of threads. After compiling all of the above source files, we get a very simple web application. This is a good start. For more information, please visit the wiki or DocsForge Contributions Every contribution is welcome. Please refer to the contribution guidelines for more information. ",
        "_version_":1718939829292498946},
      {
        "story_id":[18937195],
        "story_author":["kenshaw"],
        "story_descendants":[39],
        "story_score":[73],
        "story_time":["2019-01-18T05:13:09Z"],
        "story_title":"Gunk: Modern front end and syntax for Protocol Buffers",
        "search":["Gunk: Modern front end and syntax for Protocol Buffers",
          "https://github.com/gunk/gunk",
          "Gunk is a modern frontend and syntax for Protocol Buffers. Quickstart | Installing | Syntax | Configuring | About | Releases Overview Gunk provides a modern project-based workflow along with a Go-derived syntax for defining types and services for use with Protocol Buffers. Gunk is designed to integrate cleanly with existing protoc based build pipelines, while standardizing workflows in a way that is familiar/accessible to Go developers. Quickstart Create a working directory for a project: $ mkdir -p ~/src/example && cd ~/src/example Install gunk and place the following Gunk definitions in example/util.gunk: package util // Util is a utility service. type Util interface { // Echo returns the passed message. Echo(Message) Message } // Message contains an echo message. type Message struct { // Msg is a message from a client. Msg string `pb:\"1\"` } Create the corresponding project configuration in example/.gunkconfig: [generate go] [generate js] import_style=commonjs binary Then, generate protocol buffer definitions/code: $ ls -A .gunkconfig util.gunk $ gunk generate $ ls -A all.pb.go all_pb.js .gunkconfig util.gunk As seen above, gunk generated the corresponding Go and JavaScript protobuf code using the options defined in the .gunkconfig. End-to-end Example A end-to-end example gRPC server implementation, using Gunk definitions is available for review. Debugging protoc commands Underlying commands executed by gunk can be viewed with the following: $ gunk generate -x protoc-gen-go protoc --js_out=import_style=commonjs,binary:/home/user/example --descriptor_set_in=/dev/stdin all.proto Installing The gunk command-line tool can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the gunk or gunk.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) Installing via Homebrew (macOS) gunk is available in the gunk/gunk tap, and can be installed in the usual way with the brew command: # add tap $ brew tap gunk/gunk # install gunk $ brew install gunk Installing via Scoop (Windows) gunk can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install gunk Installing via Go gunk can be installed in the usual Go fashion: # install gunk $ go get -u github.com/gunk/gunk Protobuf Dependency and Caching The gunk command-line tool uses the protoc command-line tool. gunk can be configured to use protoc at a specified path. If it isn't available, gunk will download the latest protobuf release to the user's cache, for use. It's also possible to pin a specific version, see the section on protoc configuration. Protocol Types and Messages Gunk provides an alternate, Go-derived syntax for defining protocol buffers. As such, Gunk definitions are a subset of the Go programming language. Additionally, a special +gunk annotation is recognized by gunk, to allow the declaration of protocol buffer options: package message import \"github.com/gunk/opt/http\" // Message is a Echo message. type Message struct { // Msg holds a message. Msg string `pb:\"1\" json:\"msg\"` Code int `pb:\"2\" json:\"code\"` } // Util is a utility service. type Util interface { // Echo echoes a message. // // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo(Message) Message } Technically speaking, gunk is not actually strict subset of go, as gunk allows unused imports; it actually requires them for some features. See the example above; in pure go, this would not be a valid go code, as http is not used outside of the comment. Scalars Gunk's Go-derived syntax uses the canonical Go scalar types of the proto3 syntax, defined by the protocol buffer project: Proto3 Type Gunk Type double float64 float float32 int32 int int32 int32 int64 int64 uint32 uint uint32 uint32 uint64 uint64 bool bool string string bytes []byte Note: Variable-length scalars will be enabled in the future using a tag parameter. Messages Gunk's Go-derived syntax uses Go's struct type declarations for declaring messages, and require a pb:\"<field_number>\" tag to indicate the field number: type Message struct { FieldA string `pb:\"1\"` } type Envelope struct { Message Message `pb:\"1\" json:\"msg\"` } There are additional tags (for example, the json: tag above), that will be recognized by gunk format, and passed on to generators, where possible. Note: When using gunk format, a valid pb:\"<field_number>\" tag will be automatically inserted if not declared. Services Gunk's Go-derived syntax uses Go's interface syntax for declaring services: type SearchService interface { Search(SearchRequest) SearchResponse } The above is equivalent to the following protobuf syntax: service SearchService { rpc Search (SearchRequest) returns (SearchResponse); } Enums Gunk's Go-derived syntax uses Go const's for declaring enums: type MyEnum int const ( MYENUM MyEnum = iota MYENUM2 ) Note: values can also be fixed numeric values or a calculated value (using iota). Maps Gunk's Go-derived syntax uses Go map's for declaring map fields: type Project struct { ProjectID string `pb:\"1\" json:\"project_id\"` } type GetProjectResponse struct { Projects map[string]Project `pb:\"1\"` } Repeated Values Gunk's Go-derived syntax uses Go's slice syntax ([]) for declaring a repeated field: type MyMessage struct { FieldA []string `pb:\"1\"` } Message Streams Gunk's Go-derived syntax uses Go chan syntax for declaring streams: type MessageService interface { List(chan Message) chan Message } The above is equivalent to the following protobuf syntax: service MessageService { rpc List(stream Message) returns (stream Message); } Protocol Options Protocol buffer options are standard messages (ie, a struct), and can be attached to any service, message, enum, or other other type declaration in a Gunk file via the doccomment preceding the type, field, or service: // MyOption is an option. type MyOption struct { Name string `pb:\"1\"` } // +gunk MyOption { // Name: \"test\", // } type MyMessage struct { /* ... */ } Project Configuration Files Gunk uses a top-level .gunkconfig configuration file for managing the Gunk protocol definitons for a project: # Example .gunkconfig for Go, grpc-gateway, Python and JS [generate go] out=v1/go plugins=grpc [generate] out=v1/go command=protoc-gen-grpc-gateway logtostderr=true [generate python] out=v1/python [generate js] out=v1/js import_style=commonjs binary Project Search Path When gunk is invoked from the command-line, it searches the passed package spec (or current working directory) for a .gunkconfig file, and walks up the directory hierarchy until a .gunkconfig is found, or the project's root is encountered. The project root is defined as the top-most directory containing a .git subdirectory, or where a go.mod file is located. Format The .gunkconfig file format is compatible with Git config syntax, and in turn is compatible with the INI file format: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Global section import_path - see \"Converting Existing Protobuf Files\" strip_enum_type_names - with this option on, enums with their type prefixed will be renamed to the version without prefix. Note that this might produce invalid protobuf that stops compiling in 1.4.* protoc-gen-go, if the enum names clash. Section [protoc] The path where to check for (or where to download) the protoc binary can be configured. The version can also be pinned. Parameters version - the version of protoc to use. If unspecified, defaults to the latest release available. Otherwise, gunk will either download the specified version, or check that the version of protoc at the specified path matches what was configured. path - the path to check for the protoc binary. If unspecified, defaults appropriate user cache directory for the user's OS. If no file exists at the path, gunk will attempt to download protoc. Section [generate[ <type>]] Each [generate] or [generate <type>] section in a .gunkconfig corresponds to a invocation of the protoc-gen-<type> tool. Parameters Each name[=value] parameter defined within a [generate] section will be passed as a parameter to the protoc-gen-<type> tool, with the exception of the following special parameters that override the behavior of the gunk generate tool: command - overrides the protoc-gen-* command executable used by gunk generate. The executable must be findable on $PATH (Linux/macOS) or %PATH% (Windows), or may be the full path to the executable. If not defined, then command will be protoc-gen-<type>, when <type> is the value in [generate <type>]. protoc - overrides the <type> value, causing gunk generate to use the protoc value in place of <type>. out - overrides the output path of protoc. If not defined, output will be the same directory as the location of the .gunk files. plugin_version - specify version of plugin. The plugin is downloaded from github/maven, built in cache and used. It is not installed in $PATH. This currently works with the following plugins: protoc-gen-go protoc-gen-grpc-java protoc-gen-grpc-gateway protoc-gen-openapiv2 (protoc-gen-swagger support is deprecated) protoc-gen-swift (installing swift itself first is necessary) protoc-gen-grpc-swift (installing swift itself first is necessary) protoc-gen-ts (installing node and npm first is necessary) protoc-gen-grpc-python (cmake, gcc is necessary; takes ~10 minutes to clone build) It is recommended to use this function everywhere, for reproducible builds, together with version for protoc. json_tag_postproc - uses json tags defined in gunk file also for go-generated file fix_paths_postproc - for js and ts - by default, gunk generates wrong paths for other imported gunk packages, because of the way gunk moves files around. Works only if js also has import_style=commonjs option. All other name[=value] pairs specified within the generate section will be passed as plugin parameters to protoc and the protoc-gen-<type> generators. Short Form The following .gunkconfig: [generate go] [generate js] out=v1/js is equivalent to: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Different forms of invocation There are three different forms of gunkconfig sections that have three different semantics. [generate] command=protoc-gen-go [generate] protoc=go [generate go] The first one uses protoc-gen-go plugin directly, without using protoc. It also attempts to move files to the same directory as the gunk file. The second one uses protoc and does not attempt to move any files. Protoc attempts to load plugin from $PATH, if it is not one of the built-in protoc plugins; this will not work together with pinned version and other gunk features and is not recommended outside of built-in protoc generators. The third version is reccomended. It will try to detect whether language is one of built-in protoc generators, in that case behaves like the second way, otherwise behaves like the first. The built-in protoc generators are: cpp java python php ruby csharp objc js Third-Party Protobuf Options Gunk provides the +gunk annotation syntax for declaring protobuf options, and specially recognizes some third-party API annotations, such as Google HTTP options, including all builtin/standard protoc options for code generation: // +gunk java.Package(\"com.example.message\") // +gunk java.MultipleFiles(true) package message import ( \"github.com/gunk/opt/http\" \"github.com/gunk/opt/file/java\" ) type Util interface { // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo() } Further documentation on available options can be found at the Gunk options project. Formatting Gunk Files Gunk provides the gunk format command to format .gunk files (akin to gofmt): $ gunk format /path/to/file.gunk $ gunk format <pathspec> Converting Existing Protobuf Files Gunk provides the gunk convert command that will converting existing .proto files (or a directory) to the Go-derived Gunk syntax: $ gunk convert /path/to/file.proto $ gunk convert /path/to/protobuf/directory If your .proto is referencing another .proto from another directory, you can add import_path in the global section of your .gunkconfig. If you don't provide import_path it will only search in the root directory. import_path=relative/path/to/protobuf/directory The path to provide is relative from the .gunkconfig location. Furthermore, the referenced files must contain: option go_package=\"path/of/go/package\"; The resulting .gunk file will contain the import path as defined in go_package: import ( name \"path/of/go/package\" ) About Gunk is developed by the team at Brankas, and was designed to streamline API design and development. History From the beginning of the company, the Brankas team defined API types and services in .proto files, leveraging ad-hoc Makefile's, shell scripts, and other non-standardized mechanisms for generating Protocol Buffer code. As development exploded in 2017 (and beyond) with continued addition of backend microservices/APIs, more code repositories and projects, and team members, it became necessary to standardize tooling for the organization as well as reduce the cognitive load of developers (who for the most part were working almost exclusively with Go) when declaring gRPC and REST services. Naming The Gunk name has a cheeky, backronym \"Gunk Unified N-terface Kompiler\", however the name was chosen because it was possible to secure the GitHub gunk project name, was short, concise, and not used by other projects. Additionally, \"gunk\" is an apt description for the \"gunk\" surrounding protocol definition, generation, compilation, and delivery. Contributing Issues, Pull Requests, and other contributions are greatly welcomed and appreciated! Get started with building and running gunk: # clone source repository $ git clone https://github.com/gunk/gunk.git && cd gunk # force GO111MODULES $ export GO111MODULE=on # build and run $ go build && ./gunk Dependency Management Gunk uses Go modules for dependency management, and as such requires Go 1.11+. Please run go mod tidy before submitting any PRs: $ export GO111MODULE=on $ cd gunk && go mod tidy ",
          "I'm not seeing what makes this \"modern\". proto3 is only a few years old and nothing about it strikes me as unusually archaic. Protobuf in general isn't that much older than Go. I can see why Go-compatible syntax would be attractive to Go developers, so maybe that should be in the description rather than \"modern\"?",
          "Not sure why I’d want to define a language independent interchange format in a language specific way and remove all of the tooling help at the same time. Why is this better? A why section/motivations would help greatly."],
        "story_type":["Normal"],
        "url":"https://github.com/gunk/gunk",
        "comments.comment_id":[18945203,
          18945486],
        "comments.comment_author":["djur",
          "grogenaut"],
        "comments.comment_descendants":[1,
          1],
        "comments.comment_time":["2019-01-19T03:44:30Z",
          "2019-01-19T05:20:46Z"],
        "comments.comment_text":["I'm not seeing what makes this \"modern\". proto3 is only a few years old and nothing about it strikes me as unusually archaic. Protobuf in general isn't that much older than Go. I can see why Go-compatible syntax would be attractive to Go developers, so maybe that should be in the description rather than \"modern\"?",
          "Not sure why I’d want to define a language independent interchange format in a language specific way and remove all of the tooling help at the same time. Why is this better? A why section/motivations would help greatly."],
        "id":"ca607c94-4183-45ec-89f5-43c8d214dd64",
        "url_text":"Gunk is a modern frontend and syntax for Protocol Buffers. Quickstart | Installing | Syntax | Configuring | About | Releases Overview Gunk provides a modern project-based workflow along with a Go-derived syntax for defining types and services for use with Protocol Buffers. Gunk is designed to integrate cleanly with existing protoc based build pipelines, while standardizing workflows in a way that is familiar/accessible to Go developers. Quickstart Create a working directory for a project: $ mkdir -p ~/src/example && cd ~/src/example Install gunk and place the following Gunk definitions in example/util.gunk: package util // Util is a utility service. type Util interface { // Echo returns the passed message. Echo(Message) Message } // Message contains an echo message. type Message struct { // Msg is a message from a client. Msg string `pb:\"1\"` } Create the corresponding project configuration in example/.gunkconfig: [generate go] [generate js] import_style=commonjs binary Then, generate protocol buffer definitions/code: $ ls -A .gunkconfig util.gunk $ gunk generate $ ls -A all.pb.go all_pb.js .gunkconfig util.gunk As seen above, gunk generated the corresponding Go and JavaScript protobuf code using the options defined in the .gunkconfig. End-to-end Example A end-to-end example gRPC server implementation, using Gunk definitions is available for review. Debugging protoc commands Underlying commands executed by gunk can be viewed with the following: $ gunk generate -x protoc-gen-go protoc --js_out=import_style=commonjs,binary:/home/user/example --descriptor_set_in=/dev/stdin all.proto Installing The gunk command-line tool can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the gunk or gunk.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) Installing via Homebrew (macOS) gunk is available in the gunk/gunk tap, and can be installed in the usual way with the brew command: # add tap $ brew tap gunk/gunk # install gunk $ brew install gunk Installing via Scoop (Windows) gunk can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install gunk Installing via Go gunk can be installed in the usual Go fashion: # install gunk $ go get -u github.com/gunk/gunk Protobuf Dependency and Caching The gunk command-line tool uses the protoc command-line tool. gunk can be configured to use protoc at a specified path. If it isn't available, gunk will download the latest protobuf release to the user's cache, for use. It's also possible to pin a specific version, see the section on protoc configuration. Protocol Types and Messages Gunk provides an alternate, Go-derived syntax for defining protocol buffers. As such, Gunk definitions are a subset of the Go programming language. Additionally, a special +gunk annotation is recognized by gunk, to allow the declaration of protocol buffer options: package message import \"github.com/gunk/opt/http\" // Message is a Echo message. type Message struct { // Msg holds a message. Msg string `pb:\"1\" json:\"msg\"` Code int `pb:\"2\" json:\"code\"` } // Util is a utility service. type Util interface { // Echo echoes a message. // // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo(Message) Message } Technically speaking, gunk is not actually strict subset of go, as gunk allows unused imports; it actually requires them for some features. See the example above; in pure go, this would not be a valid go code, as http is not used outside of the comment. Scalars Gunk's Go-derived syntax uses the canonical Go scalar types of the proto3 syntax, defined by the protocol buffer project: Proto3 Type Gunk Type double float64 float float32 int32 int int32 int32 int64 int64 uint32 uint uint32 uint32 uint64 uint64 bool bool string string bytes []byte Note: Variable-length scalars will be enabled in the future using a tag parameter. Messages Gunk's Go-derived syntax uses Go's struct type declarations for declaring messages, and require a pb:\"<field_number>\" tag to indicate the field number: type Message struct { FieldA string `pb:\"1\"` } type Envelope struct { Message Message `pb:\"1\" json:\"msg\"` } There are additional tags (for example, the json: tag above), that will be recognized by gunk format, and passed on to generators, where possible. Note: When using gunk format, a valid pb:\"<field_number>\" tag will be automatically inserted if not declared. Services Gunk's Go-derived syntax uses Go's interface syntax for declaring services: type SearchService interface { Search(SearchRequest) SearchResponse } The above is equivalent to the following protobuf syntax: service SearchService { rpc Search (SearchRequest) returns (SearchResponse); } Enums Gunk's Go-derived syntax uses Go const's for declaring enums: type MyEnum int const ( MYENUM MyEnum = iota MYENUM2 ) Note: values can also be fixed numeric values or a calculated value (using iota). Maps Gunk's Go-derived syntax uses Go map's for declaring map fields: type Project struct { ProjectID string `pb:\"1\" json:\"project_id\"` } type GetProjectResponse struct { Projects map[string]Project `pb:\"1\"` } Repeated Values Gunk's Go-derived syntax uses Go's slice syntax ([]) for declaring a repeated field: type MyMessage struct { FieldA []string `pb:\"1\"` } Message Streams Gunk's Go-derived syntax uses Go chan syntax for declaring streams: type MessageService interface { List(chan Message) chan Message } The above is equivalent to the following protobuf syntax: service MessageService { rpc List(stream Message) returns (stream Message); } Protocol Options Protocol buffer options are standard messages (ie, a struct), and can be attached to any service, message, enum, or other other type declaration in a Gunk file via the doccomment preceding the type, field, or service: // MyOption is an option. type MyOption struct { Name string `pb:\"1\"` } // +gunk MyOption { // Name: \"test\", // } type MyMessage struct { /* ... */ } Project Configuration Files Gunk uses a top-level .gunkconfig configuration file for managing the Gunk protocol definitons for a project: # Example .gunkconfig for Go, grpc-gateway, Python and JS [generate go] out=v1/go plugins=grpc [generate] out=v1/go command=protoc-gen-grpc-gateway logtostderr=true [generate python] out=v1/python [generate js] out=v1/js import_style=commonjs binary Project Search Path When gunk is invoked from the command-line, it searches the passed package spec (or current working directory) for a .gunkconfig file, and walks up the directory hierarchy until a .gunkconfig is found, or the project's root is encountered. The project root is defined as the top-most directory containing a .git subdirectory, or where a go.mod file is located. Format The .gunkconfig file format is compatible with Git config syntax, and in turn is compatible with the INI file format: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Global section import_path - see \"Converting Existing Protobuf Files\" strip_enum_type_names - with this option on, enums with their type prefixed will be renamed to the version without prefix. Note that this might produce invalid protobuf that stops compiling in 1.4.* protoc-gen-go, if the enum names clash. Section [protoc] The path where to check for (or where to download) the protoc binary can be configured. The version can also be pinned. Parameters version - the version of protoc to use. If unspecified, defaults to the latest release available. Otherwise, gunk will either download the specified version, or check that the version of protoc at the specified path matches what was configured. path - the path to check for the protoc binary. If unspecified, defaults appropriate user cache directory for the user's OS. If no file exists at the path, gunk will attempt to download protoc. Section [generate[ <type>]] Each [generate] or [generate <type>] section in a .gunkconfig corresponds to a invocation of the protoc-gen-<type> tool. Parameters Each name[=value] parameter defined within a [generate] section will be passed as a parameter to the protoc-gen-<type> tool, with the exception of the following special parameters that override the behavior of the gunk generate tool: command - overrides the protoc-gen-* command executable used by gunk generate. The executable must be findable on $PATH (Linux/macOS) or %PATH% (Windows), or may be the full path to the executable. If not defined, then command will be protoc-gen-<type>, when <type> is the value in [generate <type>]. protoc - overrides the <type> value, causing gunk generate to use the protoc value in place of <type>. out - overrides the output path of protoc. If not defined, output will be the same directory as the location of the .gunk files. plugin_version - specify version of plugin. The plugin is downloaded from github/maven, built in cache and used. It is not installed in $PATH. This currently works with the following plugins: protoc-gen-go protoc-gen-grpc-java protoc-gen-grpc-gateway protoc-gen-openapiv2 (protoc-gen-swagger support is deprecated) protoc-gen-swift (installing swift itself first is necessary) protoc-gen-grpc-swift (installing swift itself first is necessary) protoc-gen-ts (installing node and npm first is necessary) protoc-gen-grpc-python (cmake, gcc is necessary; takes ~10 minutes to clone build) It is recommended to use this function everywhere, for reproducible builds, together with version for protoc. json_tag_postproc - uses json tags defined in gunk file also for go-generated file fix_paths_postproc - for js and ts - by default, gunk generates wrong paths for other imported gunk packages, because of the way gunk moves files around. Works only if js also has import_style=commonjs option. All other name[=value] pairs specified within the generate section will be passed as plugin parameters to protoc and the protoc-gen-<type> generators. Short Form The following .gunkconfig: [generate go] [generate js] out=v1/js is equivalent to: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Different forms of invocation There are three different forms of gunkconfig sections that have three different semantics. [generate] command=protoc-gen-go [generate] protoc=go [generate go] The first one uses protoc-gen-go plugin directly, without using protoc. It also attempts to move files to the same directory as the gunk file. The second one uses protoc and does not attempt to move any files. Protoc attempts to load plugin from $PATH, if it is not one of the built-in protoc plugins; this will not work together with pinned version and other gunk features and is not recommended outside of built-in protoc generators. The third version is reccomended. It will try to detect whether language is one of built-in protoc generators, in that case behaves like the second way, otherwise behaves like the first. The built-in protoc generators are: cpp java python php ruby csharp objc js Third-Party Protobuf Options Gunk provides the +gunk annotation syntax for declaring protobuf options, and specially recognizes some third-party API annotations, such as Google HTTP options, including all builtin/standard protoc options for code generation: // +gunk java.Package(\"com.example.message\") // +gunk java.MultipleFiles(true) package message import ( \"github.com/gunk/opt/http\" \"github.com/gunk/opt/file/java\" ) type Util interface { // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo() } Further documentation on available options can be found at the Gunk options project. Formatting Gunk Files Gunk provides the gunk format command to format .gunk files (akin to gofmt): $ gunk format /path/to/file.gunk $ gunk format <pathspec> Converting Existing Protobuf Files Gunk provides the gunk convert command that will converting existing .proto files (or a directory) to the Go-derived Gunk syntax: $ gunk convert /path/to/file.proto $ gunk convert /path/to/protobuf/directory If your .proto is referencing another .proto from another directory, you can add import_path in the global section of your .gunkconfig. If you don't provide import_path it will only search in the root directory. import_path=relative/path/to/protobuf/directory The path to provide is relative from the .gunkconfig location. Furthermore, the referenced files must contain: option go_package=\"path/of/go/package\"; The resulting .gunk file will contain the import path as defined in go_package: import ( name \"path/of/go/package\" ) About Gunk is developed by the team at Brankas, and was designed to streamline API design and development. History From the beginning of the company, the Brankas team defined API types and services in .proto files, leveraging ad-hoc Makefile's, shell scripts, and other non-standardized mechanisms for generating Protocol Buffer code. As development exploded in 2017 (and beyond) with continued addition of backend microservices/APIs, more code repositories and projects, and team members, it became necessary to standardize tooling for the organization as well as reduce the cognitive load of developers (who for the most part were working almost exclusively with Go) when declaring gRPC and REST services. Naming The Gunk name has a cheeky, backronym \"Gunk Unified N-terface Kompiler\", however the name was chosen because it was possible to secure the GitHub gunk project name, was short, concise, and not used by other projects. Additionally, \"gunk\" is an apt description for the \"gunk\" surrounding protocol definition, generation, compilation, and delivery. Contributing Issues, Pull Requests, and other contributions are greatly welcomed and appreciated! Get started with building and running gunk: # clone source repository $ git clone https://github.com/gunk/gunk.git && cd gunk # force GO111MODULES $ export GO111MODULE=on # build and run $ go build && ./gunk Dependency Management Gunk uses Go modules for dependency management, and as such requires Go 1.11+. Please run go mod tidy before submitting any PRs: $ export GO111MODULE=on $ cd gunk && go mod tidy ",
        "_version_":1718939816853241856},
      {
        "story_id":[18895856],
        "story_author":["ingve"],
        "story_descendants":[41],
        "story_score":[96],
        "story_time":["2019-01-13T09:04:55Z"],
        "story_title":"Writing custom tools with Swift",
        "search":["Writing custom tools with Swift",
          "https://paul-samuels.com/blog/2019/01/12/writing-custom-tools-with-swift/",
          "12 Jan 2019I write a lot of custom command line tools that live alongside my projects. The tools vary in complexity and implementation. From simplest to most involved heres my high level implementation strategy: A single file containing a shebang #!/usr/bin/swift. A Swift Package Manager project of type executable. A Swift Package Manager project of type executable that builds using sources from the main project (Ive written about this here). The same as above but special care has been taken to ensure that the tool can be dockerized and run on Linux. The hardest part with writing custom tools is knowing how to get started, this post will run through creating a single file tool. Problem Outline Lets imagine that we want to grab our most recent app store reviews, get a high level overview of star distribution of the recent reviews and look at any comments that have a rating of 3 stars or below. Skeleton Lets start by making sure we can get an executable Swift file. In your terminal you can do the following: echo '#!/usr/bin/swift\\nprint(\"It works!!\")' > reviews chmod u+x reviews ./reviews The result will be It works!! The first line is equivalent to just creating a file called reviews with the following contents #!/usr/bin/swift print(\"It works!!\") Its not the most exciting file but its good enough to get us rolling. The next command chmod u+x reviews makes the file executable and finally we execute it with ./reviews. Now that we have an executable file lets figure out what our data looks like. Source data Before we progress with writing the rest of the script we need to figure out how to get the data, Im going to do this using curl and jq. This is a useful step because it helps me figure out what the structure of the data is and allows me to experiment with the transformations that I need to apply in my tool. First lets checkout the URL that I grabbed from Stack Overflow (for this example Im just using the Apple Support apps id for reviews): curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" To see how this looks I can pretty print it by piping it through jq: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq . Response structure ``` { \"feed\": { \"author\": { \"name\": { \"label\": \"...\" }, \"uri\": { \"label\": \"...\" } }, \"entry\": [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ], \"updated\": { \"label\": \"...\" }, \"rights\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"icon\": { \"label\": \"...\" }, \"link\": [ { \"attributes\": { \"rel\": \"...\", \"type\": \"text/html\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"self\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"first\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"last\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"previous\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"next\", \"href\": \"...\" } } ], \"id\": { \"label\": \"...\" } } } ``` Looking at the structure I can see that the data I really care about is under feed.entry so I update my jq filter to scope the data a little better: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry' Response structure ``` [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ] ``` Finally I pull out the fields that I feel will be important for the tool we are writing: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label}' Response structure ``` [ { \"title\" : \"...\", \"rating\" : \"...\", \"comment\" : \"...\" } ] ``` This is a really fast way of experimenting with data and as well see later its helpful when we come to write the Swift code. The result of the jq filter above is that the large feed will be reduced down to an array of objects with just the title, rating and comment. At this point Im feeling pretty confident that I know what my data will look like so I can go ahead and write this in Swift. Network Request in swift Well use URLSession to make our request - a first attempt might look like: 1 2 3 4 5 6 7 8 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() 2 we need to import Foundation in order to use URLSession and URL. 6 well use the default session as we dont need anything custom. 7 to start well just print anything to check this works. 8 lets not forget to resume the task or nothing will happen. Taking the above we can return to terminal and run ./reviews. nothing happened. The issue here is that dataTask is an asynchronous operation and our script will exit immediately without waiting for the completion to be called. Modifying the code to call dispatchMain() at the end resolves this: #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() dispatchMain() Heading back to terminal and running ./reviews we should get some output like Optional(41678 bytes) but weve also introduced a new problem - the programme didnt terminate. Lets fix this and then we can crack on with the rest of our tasks: 1 2 3 4 5 6 7 8 9 10 11 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) exit(EXIT_SUCCESS) }).resume() dispatchMain() On line 8 Ive added an exit, well provide different exit codes later on depending on whether the tool succeeded or not. To prepare for the next steps well just add some error handling: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { print(data as Any) exit(EXIT_SUCCESS) } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Lines 7-10 are covering cases where there is a failure at the task level. Lines 12-15 are covering errors at the http level. Lines 20-23 are covering cases where there is no data returned. The happy path is hidden in lines 17-20. Side note: Depending on the usage of your scripts you may choose to tailor the level of error reporting and decide if things like force unwraps are acceptable. I tend to find its worth putting error handling in as Ill rarely look at this code, so when it goes wrong it will be a pain to debug without some guidance. Parsing the data We can look at the jq filter we created earlier to guide us on what we need to build. jq .feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label} We need to dive into the JSON through feed and entry - we can do this by mirroring this structure and using Swifts Decodable: struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { } } } In order to decode an Entry well provide a custom implementation of init(from:) - this will allow us to flatten the data e.g. instead of having entry.title.label we end up with just entry.title. We can do this with the following: struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } With this done we can wire it all up - well go back to the happy path and add: do { print(try JSONDecoder().decode(Response.self, from: data)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } Thats the complicated stuff out of the way - the next part is the data manipulation that makes the tool actually useful. Processing the data Lets start by printing a summary of the different star ratings. The high level approach will be to loop over all the reviews and keep a track of how many times each star rating was used. Well then return a string that shows the rating number and then an asterisk to represent the number of ratings. func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } This will yield output like: 5: ***************** 4: ** 3: * 2: **** 1: ************************** The other task we wanted to do was print all the comments that had a rating of 3 or less. This is the simpler of the two tasks as we just need to filter the entries and then format for printing: func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } This will yield output like: (3) - Love it > This is my favourite app. Putting it all together we end up with: #!/usr/bin/swift import Foundation struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } } } func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { do { let entries = try JSONDecoder().decode(Response.self, from: data).feed.entry print(ratings(entries: entries)) print() print(reviews(entries: entries)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Conclusion Creating tools is a lot of fun and isnt as scary as it might seem at first. Weve done networking, data parsing and some data munging all in one file with not too much effort, which is very rewarding. The single file approach is probably best for shorter tasks. In the example above its already becoming unwieldy and it would be worth considering moving to a Swift Package Manager tool (maybe thats a future post). ",
          "Swift tries to be a jack of all trades, but I think as a scripting language (as in this example) it falls short. Having to start a run loop, write asynchronous callbacks (completion handlers), and implement custom JSON decoders just to make a web request is introducing a ton of complexity that might make sense in an event-driven interactive GUI application, but not so much in a quick shell script.<p>Swift tools might be a good choice for use cases where you need to integrate with an existing Swift project, or if you need lower level APIs.<p>In this example, a Bash script would have been almost done by the time you worked out the `curl | jq` command. But in other similar cases I would suggest Python Requests, which will take perhaps 10% as much code and avoid issues of Linux compatibility and mistakes like forgetting to call `resume()` on your download task or `exit()` in some branch (there are five calls just to keep the program from looping forever).<p>That said, I think this blog post is very informative and well-made for a beginner interested in talking to a web-based JSON API from Swift.",
          "I like it, being able to start with a shell script and then evolve the tool to a static binary is pretty neat, I do that all the time with ActionScript<p>I don't think it's trying to be a \"jack of all trades\", sure Bash can do plenty but when you reach few hundred lines of Bash and start to fight against the syntax to do simple things well...<p>The shebang line is there to be used, you can use sh, bash, etc. but there are other citizen like perl, php, python, etc. so why not swift or anything else if it help you build quickly command-line tools?"],
        "story_type":["Normal"],
        "url":"https://paul-samuels.com/blog/2019/01/12/writing-custom-tools-with-swift/",
        "comments.comment_id":[18898126,
          18899738],
        "comments.comment_author":["rgovostes",
          "zwetan"],
        "comments.comment_descendants":[6,
          0],
        "comments.comment_time":["2019-01-13T19:08:41Z",
          "2019-01-14T00:46:57Z"],
        "comments.comment_text":["Swift tries to be a jack of all trades, but I think as a scripting language (as in this example) it falls short. Having to start a run loop, write asynchronous callbacks (completion handlers), and implement custom JSON decoders just to make a web request is introducing a ton of complexity that might make sense in an event-driven interactive GUI application, but not so much in a quick shell script.<p>Swift tools might be a good choice for use cases where you need to integrate with an existing Swift project, or if you need lower level APIs.<p>In this example, a Bash script would have been almost done by the time you worked out the `curl | jq` command. But in other similar cases I would suggest Python Requests, which will take perhaps 10% as much code and avoid issues of Linux compatibility and mistakes like forgetting to call `resume()` on your download task or `exit()` in some branch (there are five calls just to keep the program from looping forever).<p>That said, I think this blog post is very informative and well-made for a beginner interested in talking to a web-based JSON API from Swift.",
          "I like it, being able to start with a shell script and then evolve the tool to a static binary is pretty neat, I do that all the time with ActionScript<p>I don't think it's trying to be a \"jack of all trades\", sure Bash can do plenty but when you reach few hundred lines of Bash and start to fight against the syntax to do simple things well...<p>The shebang line is there to be used, you can use sh, bash, etc. but there are other citizen like perl, php, python, etc. so why not swift or anything else if it help you build quickly command-line tools?"],
        "id":"2dd30077-a097-447f-9487-bd38e491efe5",
        "url_text":"12 Jan 2019I write a lot of custom command line tools that live alongside my projects. The tools vary in complexity and implementation. From simplest to most involved heres my high level implementation strategy: A single file containing a shebang #!/usr/bin/swift. A Swift Package Manager project of type executable. A Swift Package Manager project of type executable that builds using sources from the main project (Ive written about this here). The same as above but special care has been taken to ensure that the tool can be dockerized and run on Linux. The hardest part with writing custom tools is knowing how to get started, this post will run through creating a single file tool. Problem Outline Lets imagine that we want to grab our most recent app store reviews, get a high level overview of star distribution of the recent reviews and look at any comments that have a rating of 3 stars or below. Skeleton Lets start by making sure we can get an executable Swift file. In your terminal you can do the following: echo '#!/usr/bin/swift\\nprint(\"It works!!\")' > reviews chmod u+x reviews ./reviews The result will be It works!! The first line is equivalent to just creating a file called reviews with the following contents #!/usr/bin/swift print(\"It works!!\") Its not the most exciting file but its good enough to get us rolling. The next command chmod u+x reviews makes the file executable and finally we execute it with ./reviews. Now that we have an executable file lets figure out what our data looks like. Source data Before we progress with writing the rest of the script we need to figure out how to get the data, Im going to do this using curl and jq. This is a useful step because it helps me figure out what the structure of the data is and allows me to experiment with the transformations that I need to apply in my tool. First lets checkout the URL that I grabbed from Stack Overflow (for this example Im just using the Apple Support apps id for reviews): curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" To see how this looks I can pretty print it by piping it through jq: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq . Response structure ``` { \"feed\": { \"author\": { \"name\": { \"label\": \"...\" }, \"uri\": { \"label\": \"...\" } }, \"entry\": [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ], \"updated\": { \"label\": \"...\" }, \"rights\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"icon\": { \"label\": \"...\" }, \"link\": [ { \"attributes\": { \"rel\": \"...\", \"type\": \"text/html\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"self\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"first\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"last\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"previous\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"next\", \"href\": \"...\" } } ], \"id\": { \"label\": \"...\" } } } ``` Looking at the structure I can see that the data I really care about is under feed.entry so I update my jq filter to scope the data a little better: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry' Response structure ``` [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ] ``` Finally I pull out the fields that I feel will be important for the tool we are writing: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label}' Response structure ``` [ { \"title\" : \"...\", \"rating\" : \"...\", \"comment\" : \"...\" } ] ``` This is a really fast way of experimenting with data and as well see later its helpful when we come to write the Swift code. The result of the jq filter above is that the large feed will be reduced down to an array of objects with just the title, rating and comment. At this point Im feeling pretty confident that I know what my data will look like so I can go ahead and write this in Swift. Network Request in swift Well use URLSession to make our request - a first attempt might look like: 1 2 3 4 5 6 7 8 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() 2 we need to import Foundation in order to use URLSession and URL. 6 well use the default session as we dont need anything custom. 7 to start well just print anything to check this works. 8 lets not forget to resume the task or nothing will happen. Taking the above we can return to terminal and run ./reviews. nothing happened. The issue here is that dataTask is an asynchronous operation and our script will exit immediately without waiting for the completion to be called. Modifying the code to call dispatchMain() at the end resolves this: #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() dispatchMain() Heading back to terminal and running ./reviews we should get some output like Optional(41678 bytes) but weve also introduced a new problem - the programme didnt terminate. Lets fix this and then we can crack on with the rest of our tasks: 1 2 3 4 5 6 7 8 9 10 11 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) exit(EXIT_SUCCESS) }).resume() dispatchMain() On line 8 Ive added an exit, well provide different exit codes later on depending on whether the tool succeeded or not. To prepare for the next steps well just add some error handling: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { print(data as Any) exit(EXIT_SUCCESS) } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Lines 7-10 are covering cases where there is a failure at the task level. Lines 12-15 are covering errors at the http level. Lines 20-23 are covering cases where there is no data returned. The happy path is hidden in lines 17-20. Side note: Depending on the usage of your scripts you may choose to tailor the level of error reporting and decide if things like force unwraps are acceptable. I tend to find its worth putting error handling in as Ill rarely look at this code, so when it goes wrong it will be a pain to debug without some guidance. Parsing the data We can look at the jq filter we created earlier to guide us on what we need to build. jq .feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label} We need to dive into the JSON through feed and entry - we can do this by mirroring this structure and using Swifts Decodable: struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { } } } In order to decode an Entry well provide a custom implementation of init(from:) - this will allow us to flatten the data e.g. instead of having entry.title.label we end up with just entry.title. We can do this with the following: struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } With this done we can wire it all up - well go back to the happy path and add: do { print(try JSONDecoder().decode(Response.self, from: data)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } Thats the complicated stuff out of the way - the next part is the data manipulation that makes the tool actually useful. Processing the data Lets start by printing a summary of the different star ratings. The high level approach will be to loop over all the reviews and keep a track of how many times each star rating was used. Well then return a string that shows the rating number and then an asterisk to represent the number of ratings. func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } This will yield output like: 5: ***************** 4: ** 3: * 2: **** 1: ************************** The other task we wanted to do was print all the comments that had a rating of 3 or less. This is the simpler of the two tasks as we just need to filter the entries and then format for printing: func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } This will yield output like: (3) - Love it > This is my favourite app. Putting it all together we end up with: #!/usr/bin/swift import Foundation struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } } } func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { do { let entries = try JSONDecoder().decode(Response.self, from: data).feed.entry print(ratings(entries: entries)) print() print(reviews(entries: entries)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Conclusion Creating tools is a lot of fun and isnt as scary as it might seem at first. Weve done networking, data parsing and some data munging all in one file with not too much effort, which is very rewarding. The single file approach is probably best for shorter tasks. In the example above its already becoming unwieldy and it would be worth considering moving to a Swift Package Manager tool (maybe thats a future post). ",
        "_version_":1718939816165376000},
      {
        "story_id":[21572308],
        "story_author":["dragondax"],
        "story_descendants":[47],
        "story_score":[98],
        "story_time":["2019-11-19T12:23:01Z"],
        "story_title":"Run an Internet Speed Test from the Command Line",
        "search":["Run an Internet Speed Test from the Command Line",
          "https://www.putorius.net/speed-test-command-line.html",
          "We have all used tools like speedtest.net to test upload and download speeds. Whether it was to test the WiFi in that coffee shop (I use my own tether, never unknown hot spots), preparing for a LAN party (do people still do that?), or just a step in troubleshooting, we have all been there. For one reason or another you simply think you are being cheated of bandwidth, so you want independent verification of your speeds. This typically means opening a browser and going to a website to test your connection. But what if you want to run a speed test on a remote server? In this article we will discuss running an internet speed test from the Linux command line, and skipping the browser. There is something about the raw efficiency of the command line that I am really attracted to. As I discussed in the article 5 Command Line Tool to Break Your Dependence on the GUI, I try my best to stay away from the browser. It usually creates an unnecessary distraction. The internet is designed to grab your attention like a laser pointer does to a cat. So lets get started, and figure out one more way to stay away from the GUI. Different Speed Test Packages There are a few different tools you can use to run a speed test from the command line. To make things even more confusing the two most popular share the same exact name, but both use the speedtest.net service. Unofficial Speedtest-CLI Python Script The first one is an independently written Python script that is simple to install and use. It is available in the default repositories for some popular Linux distributions. Pros: Easy to installWide AvailabilityFull list of serversCan specify upload test, download test, or both Cons: Minimal output format optionsNo verbose output option Jump to Installing speedtest-cli Python Script or How to Use speedtest-cli Python Script. Official Ookla Speedtest CLI The second tool is built by Ookla, the people who bring you the speedtest.net website and service. Installing it requires you to add a repo for your package manager. But the maintainers offer simple instructions for installation. Pros: Official release from OoklaMore robust formatting optionsOutput easier to read, better layoutVerbose output availableHas repo making it easy to get updates Cons: Use limited to nearby serversCannot specify download or upload only Jump to Official Ookla Speedtest CLI The Speedtest-cli Python Script This is an easy way to get started running a speed test on the Linux command line. Installing the Speedtest-cli Python Script Simply use your package manager to install the package. Install on Fedora using DNF sudo dnf install speedtest-cli Ubuntu or Debian using APT sudo apt-get install speedtest-cli CentOS/Red Hat 7 / 8 Unfortunately, CentOS does not offer the rpm in their repos. It can still be easily installed. Change to /usr/bin directory to make command available to all users: cd /usr/bin Install dependencies: sudo yum install -y python wget Fetch script from github: sudo wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli Make script executable: sudo chmod +x speedtest-cli Or just copy and paste the whole thing below as a single line: cd /usr/bin; sudo yum install -y python wget && wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli && sudo chmod +x speedtest-cli How to Use the Python Script to Run a Speed Test The most basic usage is to simply run the command. It will automatically select the best server based on ping responses. Speedtest-cli Python Script Options There are several options available to change the default behavior. Here we will outline the most popular options. List Available Speed Test Servers You can use the list option to find a list of available servers to run your test against. At the time of writing this list is pretty extensive with 8829 possible servers. NOTE: The servers are sorted by distance, closest first. [[emailprotected] ~]$ speedtest-cli --list Retrieving speedtest.net configuration 4847) Hotwire Fision (Philadelphia, PA, United States) [10.92 km] 10979) School District of Philadelphia (Philadelphia, PA, United States) [10.92 km] ...OUTPUT TRUNCATED... Specify Specific Server to Test Against Once you have found the server you want to test against, you can use the server <SERVER ID> to select it. The server ID is the first column in the output of the list option above. [[emailprotected] ~]$ speedtest-cli --server 4847 Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by School District of Philadelphia (Philadelphia, PA) [10.92 km]: 25.033 ms Testing download speed.. Download: 384.07 Mbit/s Testing upload speed Upload: 417.93 Mbit/s Only Test Upload or Download Speeds The option is actually designed to exclude a test. But since there are only two options it is effectively the same as selecting only one. To run only the download test, you exclude the upload, and vice versa. [[emailprotected] ~]$ speedtest-cli --no-upload Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by KamaTera INC (New Jersey, NJ) [62.36 km]: 19.785 ms Testing download speed.. Download: 600.89 Mbit/s Skipping upload test Format Output in JSON or CSV You can specify the output format in JSON or CSV. You also have the opton to use CSV with a custom delimiter. This is handy if you are going to use the output in some other script or application. [[emailprotected] ~]$ speedtest-cli --json {\"download\": 597726146.0529929, \"upload\": 562476134.8046777, \"ping\": 17.004, \"server\": {\"url\": \"http://speedtest.us-ny2.kamatera.com:8080/speedtest/upload.php\", \"lat\": \"40.0583\", \"lon\": \"-74.4057\", \"name\": \"New Jersey, NJ\", \"country\": \"United States\", \"cc\": \"US\", \"sponsor\": \"KamaTera INC\", \"id\": \"11612\" ...OUTPUT TRUNCATED... Using CSV with a custom delimiter. The default delimiter is a comma, which is implied by the name CSV. Here we use the csv-delimiter option to change the delimiter to a pipe character. [[emailprotected] ~]$ speedtest-cli --csv --csv-delimiter \"|\" 11612|KamaTera INC|New Jersey, NJ|2019-11-17T14:51:53.636981Z|62.35865439150934|8.546|588013638.8767571|512001168.48230773||x.x.x.x The Official Ookla Speedtest CLI The official Speedtest CLI (Command Line Interface) from Ookla is a little more robust. It has all of the options of the python script and more. There are also several output formats not available with the unofficial python script. Ooklas speedtest is also a little easier on the eyes. It spreads the information out which makes it easier to read and displays a neat little progress bar. A URL you can use to share the results is also displayed by default. Installing the Official Speedtest CLI Install Speedtest CLI on Ubuntu / Debian: The Speedtest CLI from Ookla is supported on Ubuntu (xenial & bionic) and Debian (jessie, stretch, buster). $ sudo apt-get install gnupg1 apt-transport-https dirmngr $ export INSTALL_KEY=379CE192D401AB61 $ export DEB_DISTRO=$(lsb_release -sc) $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys $INSTALL_KEY $ echo \"deb https://ookla.bintray.com/debian ${DEB_DISTRO} main\" | sudo tee /etc/apt/sources.list.d/speedtest.list $ sudo apt-get update $ sudo apt-get install speedtest Install Speedtest CLI on Fedora / Redhat / CentOS: Fedora has moved on to DNF for package management, but is still compatible with YUM. These instructions were tested on Fedora 31, CentOS 7 and Red Hat 8. $ sudo yum install wget $ wget https://bintray.com/ookla/rhel/rpm -O bintray-ookla-rhel.repo $ sudo mv bintray-ookla-rhel.repo /etc/yum.repos.d/ $ sudo yum install speedtest How to Use the Official Speedtest CLI Once installed you can simply call the utility by typing speedtest at the command line. This will give you all the default information that you would see on the web version of speedtest.net. Official Speedtest CLI Options The options available in the official release are more robust. Here we will outline the popular options and how to use them. List Available Speed Test Servers Using the -L (servers) option will give you a list of servers available to run a test against. This option will only show you servers that are nearby. What exactly determines nearby is undefined. But for me it looks like they are staying in the tri-state area (PA, NJ, DE). [[emailprotected] ~]$ speedtest -L Closest servers: ID Name Location Country 4847 Hotwire Fision Philadelphia, PA United States 10979 School District of Philadelphia Philadelphia, PA United States 9840 Comcast New Castle, DE United States 11612 KamaTera INC New Jersey, NJ United States ...OUTPUT TRUNCATED... Optionally, you can use the -o (host) option and specify the FQDN of the server instead of the ID. But oddly, I dont see a way to get the FQDN of the servers on the list. I am guessing this option is available for using a custom server. I havent found a way to list all servers. If you are looking to test against a server on the other side of the country, you will have to find it another way. Select Specific Server to Run Speed Test Against You can use the -s (server-id) option to select a server to use from the list. You must supply the server ID with this option. The server ID is the number in the first column of the list output above. [[emailprotected] ~]$ speedtest -s 4847 Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) Change Unit Used for Speed Output The -u (unit) option can display the speed output in many different formats. Decimal prefix, bits per second: bps, kbps, Mbps, Gbps Decimal prefix, bytes per second: B/s, kB/s, MB/s, GB/s Binary prefix, bits per second: kibps, Mibps, Gibps Binary prefix, bytes per second: kiB/s, MiB/s, GiB/s [[emailprotected] ~]$ speedtest -u MiB/s Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) ISP: Verizon Fios Latency: 10.67 ms ( 0.95 ms jitter) Download: 63.45 MiB/s (data used: 700.2 MiB) ...OUTPUT TRUNCATED... Output Formatting Options The Ookla Speedtest CLI offers decent options for output formats. Human Readable DefaultCSV Comma Separated ValueTSV Tab Separated ValueJSON JavaScript Object NotationJSONL JSON LinesJSON-PRETTY JSON Pretty Printed Here is an example using json-pretty. [[emailprotected] ~]$ speedtest -f json-pretty { \"type\": \"result\", \"timestamp\": \"2019-11-17T16:42:06Z\", \"ping\": { \"jitter\": 0.29899999999999999, \"latency\": 17.474 }, \"download\": { \"bandwidth\": 92184614, \"bytes\": 491967724, \"elapsed\": 5303 }, \"upload\": { \"bandwidth\": 45010100, \"bytes\": 313859035, \"elapsed\": 6714 }, ...OUTPUT TRUNCATED... Conclusion Running a speed test from the command line may not be something that is needed on a daily basis for most people. However, it may prove useful in some troubleshooting situations. In this article we cover how to run a speed test from the command line using two similar tools. The unofficial python script and the official Ookla Speedtest CLI. We discussed installing, using and setting options for each one. This should be enough to get you started. For more information on these tools, visit their respective home pages found in the resources section below. Resources and Links: Ookla Speedtest CLISpeedtest-cli (Python Version) on GitHub ",
          "Note that some ISPs prioritise traffic to known speedtest targets, turning off traffic shaping rules that might otherwise slow bulk transfers. When this happens it means you are testing the likely maximum throughput of your connection not necessarily the throughput you will see more generally.<p>This is why Netflix started fast.com - because it draws data from the same distribution points as their video streaming apps it means you can't prioritise the speedtest without also doing so for the video traffic or (more likely) you can't de-prioritise the video traffic without also getting bad scores in that particular speedtest. From Netflix's point of view it is an answer to people contacting support with \"my speedtest results are fine, the problem must be your servers\" when they are experiencing video lag/drops and other such problems and the issue is due to ISP traffic shaping or the ISP simply not having enough backhaul bandwidth.<p>A more reliable test might be taking part in a busy public torrent: that way you are testing against arbitrary locations so your ISP can't be setting different shaping rules for them. Just remember to throttle upstream when testing downstream and vice-versa or saturation in the other direction will slow control packets that will in turn give you lower results for the one you are testing. This may fall into another trap though: unless you limit the number of active streams it may be an unrealistic test as more generally most processes use a small number of streams (or just a single one), and if you limit the number of streams too much you might get a lower result because each swarm member you connect to may be fairly saturated and sharing its bandwidth amongst many connections.",
          "speedtest-cli is <i>garbage</i> if you have >100Mbps Speeds. The dev refuses to acknowledge this: <a href=\"https://github.com/sivel/speedtest-cli/issues/226\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/226</a><p>Also not just me:\n<a href=\"https://github.com/sivel/speedtest-cli/issues/649\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/649</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/648\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/648</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/641\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/641</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/616\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/616</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/601\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/601</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/588\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/588</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/546\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/546</a>"],
        "story_type":["Normal"],
        "url":"https://www.putorius.net/speed-test-command-line.html",
        "comments.comment_id":[21582731,
          21583805],
        "comments.comment_author":["dspillett",
          "virtuallynathan"],
        "comments.comment_descendants":[5,
          4],
        "comments.comment_time":["2019-11-20T10:49:21Z",
          "2019-11-20T13:46:04Z"],
        "comments.comment_text":["Note that some ISPs prioritise traffic to known speedtest targets, turning off traffic shaping rules that might otherwise slow bulk transfers. When this happens it means you are testing the likely maximum throughput of your connection not necessarily the throughput you will see more generally.<p>This is why Netflix started fast.com - because it draws data from the same distribution points as their video streaming apps it means you can't prioritise the speedtest without also doing so for the video traffic or (more likely) you can't de-prioritise the video traffic without also getting bad scores in that particular speedtest. From Netflix's point of view it is an answer to people contacting support with \"my speedtest results are fine, the problem must be your servers\" when they are experiencing video lag/drops and other such problems and the issue is due to ISP traffic shaping or the ISP simply not having enough backhaul bandwidth.<p>A more reliable test might be taking part in a busy public torrent: that way you are testing against arbitrary locations so your ISP can't be setting different shaping rules for them. Just remember to throttle upstream when testing downstream and vice-versa or saturation in the other direction will slow control packets that will in turn give you lower results for the one you are testing. This may fall into another trap though: unless you limit the number of active streams it may be an unrealistic test as more generally most processes use a small number of streams (or just a single one), and if you limit the number of streams too much you might get a lower result because each swarm member you connect to may be fairly saturated and sharing its bandwidth amongst many connections.",
          "speedtest-cli is <i>garbage</i> if you have >100Mbps Speeds. The dev refuses to acknowledge this: <a href=\"https://github.com/sivel/speedtest-cli/issues/226\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/226</a><p>Also not just me:\n<a href=\"https://github.com/sivel/speedtest-cli/issues/649\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/649</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/648\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/648</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/641\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/641</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/616\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/616</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/601\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/601</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/588\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/588</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/546\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/546</a>"],
        "id":"40a85b3b-5773-49a8-a2fc-629df51ff478",
        "url_text":"We have all used tools like speedtest.net to test upload and download speeds. Whether it was to test the WiFi in that coffee shop (I use my own tether, never unknown hot spots), preparing for a LAN party (do people still do that?), or just a step in troubleshooting, we have all been there. For one reason or another you simply think you are being cheated of bandwidth, so you want independent verification of your speeds. This typically means opening a browser and going to a website to test your connection. But what if you want to run a speed test on a remote server? In this article we will discuss running an internet speed test from the Linux command line, and skipping the browser. There is something about the raw efficiency of the command line that I am really attracted to. As I discussed in the article 5 Command Line Tool to Break Your Dependence on the GUI, I try my best to stay away from the browser. It usually creates an unnecessary distraction. The internet is designed to grab your attention like a laser pointer does to a cat. So lets get started, and figure out one more way to stay away from the GUI. Different Speed Test Packages There are a few different tools you can use to run a speed test from the command line. To make things even more confusing the two most popular share the same exact name, but both use the speedtest.net service. Unofficial Speedtest-CLI Python Script The first one is an independently written Python script that is simple to install and use. It is available in the default repositories for some popular Linux distributions. Pros: Easy to installWide AvailabilityFull list of serversCan specify upload test, download test, or both Cons: Minimal output format optionsNo verbose output option Jump to Installing speedtest-cli Python Script or How to Use speedtest-cli Python Script. Official Ookla Speedtest CLI The second tool is built by Ookla, the people who bring you the speedtest.net website and service. Installing it requires you to add a repo for your package manager. But the maintainers offer simple instructions for installation. Pros: Official release from OoklaMore robust formatting optionsOutput easier to read, better layoutVerbose output availableHas repo making it easy to get updates Cons: Use limited to nearby serversCannot specify download or upload only Jump to Official Ookla Speedtest CLI The Speedtest-cli Python Script This is an easy way to get started running a speed test on the Linux command line. Installing the Speedtest-cli Python Script Simply use your package manager to install the package. Install on Fedora using DNF sudo dnf install speedtest-cli Ubuntu or Debian using APT sudo apt-get install speedtest-cli CentOS/Red Hat 7 / 8 Unfortunately, CentOS does not offer the rpm in their repos. It can still be easily installed. Change to /usr/bin directory to make command available to all users: cd /usr/bin Install dependencies: sudo yum install -y python wget Fetch script from github: sudo wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli Make script executable: sudo chmod +x speedtest-cli Or just copy and paste the whole thing below as a single line: cd /usr/bin; sudo yum install -y python wget && wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli && sudo chmod +x speedtest-cli How to Use the Python Script to Run a Speed Test The most basic usage is to simply run the command. It will automatically select the best server based on ping responses. Speedtest-cli Python Script Options There are several options available to change the default behavior. Here we will outline the most popular options. List Available Speed Test Servers You can use the list option to find a list of available servers to run your test against. At the time of writing this list is pretty extensive with 8829 possible servers. NOTE: The servers are sorted by distance, closest first. [[emailprotected] ~]$ speedtest-cli --list Retrieving speedtest.net configuration 4847) Hotwire Fision (Philadelphia, PA, United States) [10.92 km] 10979) School District of Philadelphia (Philadelphia, PA, United States) [10.92 km] ...OUTPUT TRUNCATED... Specify Specific Server to Test Against Once you have found the server you want to test against, you can use the server <SERVER ID> to select it. The server ID is the first column in the output of the list option above. [[emailprotected] ~]$ speedtest-cli --server 4847 Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by School District of Philadelphia (Philadelphia, PA) [10.92 km]: 25.033 ms Testing download speed.. Download: 384.07 Mbit/s Testing upload speed Upload: 417.93 Mbit/s Only Test Upload or Download Speeds The option is actually designed to exclude a test. But since there are only two options it is effectively the same as selecting only one. To run only the download test, you exclude the upload, and vice versa. [[emailprotected] ~]$ speedtest-cli --no-upload Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by KamaTera INC (New Jersey, NJ) [62.36 km]: 19.785 ms Testing download speed.. Download: 600.89 Mbit/s Skipping upload test Format Output in JSON or CSV You can specify the output format in JSON or CSV. You also have the opton to use CSV with a custom delimiter. This is handy if you are going to use the output in some other script or application. [[emailprotected] ~]$ speedtest-cli --json {\"download\": 597726146.0529929, \"upload\": 562476134.8046777, \"ping\": 17.004, \"server\": {\"url\": \"http://speedtest.us-ny2.kamatera.com:8080/speedtest/upload.php\", \"lat\": \"40.0583\", \"lon\": \"-74.4057\", \"name\": \"New Jersey, NJ\", \"country\": \"United States\", \"cc\": \"US\", \"sponsor\": \"KamaTera INC\", \"id\": \"11612\" ...OUTPUT TRUNCATED... Using CSV with a custom delimiter. The default delimiter is a comma, which is implied by the name CSV. Here we use the csv-delimiter option to change the delimiter to a pipe character. [[emailprotected] ~]$ speedtest-cli --csv --csv-delimiter \"|\" 11612|KamaTera INC|New Jersey, NJ|2019-11-17T14:51:53.636981Z|62.35865439150934|8.546|588013638.8767571|512001168.48230773||x.x.x.x The Official Ookla Speedtest CLI The official Speedtest CLI (Command Line Interface) from Ookla is a little more robust. It has all of the options of the python script and more. There are also several output formats not available with the unofficial python script. Ooklas speedtest is also a little easier on the eyes. It spreads the information out which makes it easier to read and displays a neat little progress bar. A URL you can use to share the results is also displayed by default. Installing the Official Speedtest CLI Install Speedtest CLI on Ubuntu / Debian: The Speedtest CLI from Ookla is supported on Ubuntu (xenial & bionic) and Debian (jessie, stretch, buster). $ sudo apt-get install gnupg1 apt-transport-https dirmngr $ export INSTALL_KEY=379CE192D401AB61 $ export DEB_DISTRO=$(lsb_release -sc) $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys $INSTALL_KEY $ echo \"deb https://ookla.bintray.com/debian ${DEB_DISTRO} main\" | sudo tee /etc/apt/sources.list.d/speedtest.list $ sudo apt-get update $ sudo apt-get install speedtest Install Speedtest CLI on Fedora / Redhat / CentOS: Fedora has moved on to DNF for package management, but is still compatible with YUM. These instructions were tested on Fedora 31, CentOS 7 and Red Hat 8. $ sudo yum install wget $ wget https://bintray.com/ookla/rhel/rpm -O bintray-ookla-rhel.repo $ sudo mv bintray-ookla-rhel.repo /etc/yum.repos.d/ $ sudo yum install speedtest How to Use the Official Speedtest CLI Once installed you can simply call the utility by typing speedtest at the command line. This will give you all the default information that you would see on the web version of speedtest.net. Official Speedtest CLI Options The options available in the official release are more robust. Here we will outline the popular options and how to use them. List Available Speed Test Servers Using the -L (servers) option will give you a list of servers available to run a test against. This option will only show you servers that are nearby. What exactly determines nearby is undefined. But for me it looks like they are staying in the tri-state area (PA, NJ, DE). [[emailprotected] ~]$ speedtest -L Closest servers: ID Name Location Country 4847 Hotwire Fision Philadelphia, PA United States 10979 School District of Philadelphia Philadelphia, PA United States 9840 Comcast New Castle, DE United States 11612 KamaTera INC New Jersey, NJ United States ...OUTPUT TRUNCATED... Optionally, you can use the -o (host) option and specify the FQDN of the server instead of the ID. But oddly, I dont see a way to get the FQDN of the servers on the list. I am guessing this option is available for using a custom server. I havent found a way to list all servers. If you are looking to test against a server on the other side of the country, you will have to find it another way. Select Specific Server to Run Speed Test Against You can use the -s (server-id) option to select a server to use from the list. You must supply the server ID with this option. The server ID is the number in the first column of the list output above. [[emailprotected] ~]$ speedtest -s 4847 Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) Change Unit Used for Speed Output The -u (unit) option can display the speed output in many different formats. Decimal prefix, bits per second: bps, kbps, Mbps, Gbps Decimal prefix, bytes per second: B/s, kB/s, MB/s, GB/s Binary prefix, bits per second: kibps, Mibps, Gibps Binary prefix, bytes per second: kiB/s, MiB/s, GiB/s [[emailprotected] ~]$ speedtest -u MiB/s Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) ISP: Verizon Fios Latency: 10.67 ms ( 0.95 ms jitter) Download: 63.45 MiB/s (data used: 700.2 MiB) ...OUTPUT TRUNCATED... Output Formatting Options The Ookla Speedtest CLI offers decent options for output formats. Human Readable DefaultCSV Comma Separated ValueTSV Tab Separated ValueJSON JavaScript Object NotationJSONL JSON LinesJSON-PRETTY JSON Pretty Printed Here is an example using json-pretty. [[emailprotected] ~]$ speedtest -f json-pretty { \"type\": \"result\", \"timestamp\": \"2019-11-17T16:42:06Z\", \"ping\": { \"jitter\": 0.29899999999999999, \"latency\": 17.474 }, \"download\": { \"bandwidth\": 92184614, \"bytes\": 491967724, \"elapsed\": 5303 }, \"upload\": { \"bandwidth\": 45010100, \"bytes\": 313859035, \"elapsed\": 6714 }, ...OUTPUT TRUNCATED... Conclusion Running a speed test from the command line may not be something that is needed on a daily basis for most people. However, it may prove useful in some troubleshooting situations. In this article we cover how to run a speed test from the command line using two similar tools. The unofficial python script and the official Ookla Speedtest CLI. We discussed installing, using and setting options for each one. This should be enough to get you started. For more information on these tools, visit their respective home pages found in the resources section below. Resources and Links: Ookla Speedtest CLISpeedtest-cli (Python Version) on GitHub ",
        "_version_":1718939881292431360},
      {
        "story_id":[20449610],
        "story_author":["cube2222"],
        "story_descendants":[70],
        "story_score":[310],
        "story_time":["2019-07-16T13:04:35Z"],
        "story_title":"Show HN: OctoSQL – Query and join multiple databases and files, written in Go",
        "search":["Show HN: OctoSQL – Query and join multiple databases and files, written in Go",
          "https://github.com/cube2222/octosql",
          "OctoSQL is a query tool that allows you to join, analyse and transform data from multiple databases, streaming sources and file formats using SQL. OctoSQL is currently being rewritten on the redesign branch. Problems OctoSQL Solves You need to join / analyze data from multiple datasources. Think of enriching an Excel file by joining it with a PostgreSQL database. You need stream aggregates over time, with live output updates. Think of a live-updated leaderboard with cat images based on a \"like\" event stream. You need aggregate streams per time window, with live output updates. Think of a unique user count per hour, per country live summary. Table of Contents What is OctoSQL? Installation Quickstart Temporal SQL Features Watermarks Triggers Retractions Example Durability Configuration JSON CSV Excel Parquet PostgreSQL MySQL Redis Kafka Documentation Architecture Datasource Pushdown Operations Roadmap What is OctoSQL? OctoSQL is a SQL query engine which allows you to write standard SQL queries on data stored in multiple SQL databases, NoSQL databases, streaming sources and files in various formats trying to push down as much of the work as possible to the source databases, not transferring unnecessary data. OctoSQL does that by creating an internal representation of your query and later translating parts of it into the query languages or APIs of the source databases. Whenever a datasource doesn't support a given operation, OctoSQL will execute it in memory, so you don't have to worry about the specifics of the underlying datasources. OctoSQL also includes temporal SQL extensions, to operate ergonomically on streams and respect their event-time (not the current system-time when the records are being processed). With OctoSQL you don't need O(n) client tools or a large data analysis system deployment. Everything's contained in a single binary. Why the name? OctoSQL stems from Octopus SQL. Octopus, because octopi have many arms, so they can grasp and manipulate multiple objects, like OctoSQL is able to handle multiple datasources simultaneously. Installation Either download the binary for your operating system (Linux, OS X and Windows are supported) from the Releases page, or install using the go command line tool: GO111MODULE=on go get -u github.com/cube2222/octosql/cmd/octosql Quickstart Let's say we have a csv file with cats, and a redis database with people (potential cat owners). Now we want to get a list of cities with the number of distinct cat names in them and the cumulative number of cat lives (as each cat has up to 9 lives left). First, create a configuration file (Configuration Syntax) For example: dataSources: - name: cats type: csv config: path: \"~/Documents/cats.csv\" - name: people type: redis config: address: \"localhost:6379\" password: \"\" databaseIndex: 0 databaseKeyName: \"id\" Then, set the OCTOSQL_CONFIG environment variable to point to the configuration file. export OCTOSQL_CONFIG=~/octosql.yaml You can also use the --config command line argument. Finally, query to your hearts desire: octosql \"SELECT p.city, FIRST(c.name), COUNT(DISTINCT c.name) cats, SUM(c.livesleft) catlives FROM cats c JOIN people p ON c.ownerid = p.id GROUP BY p.city ORDER BY catlives DESC LIMIT 9\" Example output: +---------+--------------+------+----------+ | p.city | c.name_first | cats | catlives | +---------+--------------+------+----------+ | Warren | Zoey | 68 | 570 | | Gadsden | Snickers | 52 | 388 | | Staples | Harley | 54 | 383 | | Buxton | Lucky | 45 | 373 | | Bethany | Princess | 46 | 366 | | Noxen | Sheba | 49 | 361 | | Yorklyn | Scooter | 45 | 359 | | Tuttle | Toby | 57 | 356 | | Ada | Jasmine | 49 | 351 | +---------+--------------+------+----------+ You can choose between live-table batch-table live-csv batch-csv stream-json output formats. (The live-* types will update the terminal view repeatedly every second, the batch-* ones will write the output once before exiting, the stream-* ones will print records whenever they are available) Temporal SQL Features OctoSQL features temporal SQL extensions inspired by the paper One SQL to Rule Them All. Introduction Often when you're working with streams of events, you'd like to use the time dimension somehow: Calculate average values for a day sliced by hours. Get unique user counts per day. and others All those examples have one thing in common: The time value of an event is crucial for correctness. A naive system could just use the current clock time whenever it receives an event. The correctness of this approach however, degrades quickly in the face of network problems, delivery delays, clock skew. This can be solved by using a value from the event as its time value. A new problem arises though: how do I know that I've received all events up to time X and can publish results for a given hour. You never know if there isn't somewhere a delayed event which should be factored in. This is where watermarks come into play. Watermarks Watermarks are a heuristic which try to approximate the \"current time\" when processing events. Said differently: When I receive a watermark for 12:00 I can be sure enough I've received all events of interest up to 12:00. To achieve this, they are generated at streaming sources and propagate downstream through the whole processing pipeline. The generation of watermarks usually relies on heuristics which provide satisfactory results for our given use case. OctoSQL currently contains the following watermark generators: Maximum difference watermark generator (with an offset argument) With an offset of 10 seconds, this generator says: When I've received an event for 12:00:00, then I'm sure I won't receive any event older than 11:59:50. Percentile watermark generator (with a percentile argument) With a percentile of 99.5, it will look at a specified number of recent events, and generate a watermark so that 99.5% of those events are after the watermark (not yet triggered), and the remaining 0.5% are before it. This way we set the watermark so that only a fraction of the recently seen events is potentially ignored as being late. Watermark generators are specified using table valued functions and are documented in the wiki. Triggers Another matter is triggering of keys in aggregations. Sometimes you'd like to only see the value for a given key (hour) when you know it's done, but othertimes you'd like to see partial results (how's the unique user count going this hour). That's where you can use triggers. Triggers allow you to specify when a given aggregate (or join window for that matter) is emitted or updated. OctoSQL contains multiple triggers: Watermark Trigger This is the most straightforward trigger. It emits a value whenever the watermark for a given key (or the end of the stream) is reached. So basically the \"show me when it's done\". Counting Trigger (with a count argument) This trigger will emit a value for a key every time it receives count records with this key. The count is reset whenever the key is triggered. Delay Trigger (with a delay argument) This trigger will emit a value for a key whenever the key has been inactive for the delay period. You can use multiple triggers simultaneously. (Show me the current sum every 10 received events, but also the final value after having received the watermark.) Retractions A key can be triggered multiple times with partial results. How do we know a given record is a retriggering of some key, and not a new unrelated record? OctoSQL solves this problem using a dataflow-like architecture. This means whenever a new value is sent for a key, a retraction is send for the old value. In practice this means every update is accompanied by the old record with an undo flag set. This can be visible when using a stream-* output format with partial results. Example Now we can see how it all fits together. In this example we have an events file, which contains records about points being scored in a game by multiple teams. WITH with_watermark AS (SELECT * FROM max_diff_watermark(source=>TABLE(events), offset=>INTERVAL 5 SECONDS, time_field=>DESCRIPTOR(time)) e), with_tumble AS (SELECT * FROM tumble(source=>TABLE(with_watermark), time_field=>DESCRIPTOR(e.time), window_length=> INTERVAL 1 MINUTE, offset => INTERVAL 0 SECONDS) e), counts_per_team AS (SELECT e.window_end, e.team, COUNT(*) as goals FROM with_tumble e GROUP BY e.window_end, e.team TRIGGER COUNTING 100, ON WATERMARK) SELECT * FROM counts_per_team cpt ORDER BY cpt.window_end DESC, cpt.goals ASC, cpt.team DESC We use common table expressions to break the query up into multiple stages. First we create the with_watermark intermediate table/stream. Here we use the table valued function max_diff_watermark to add watermarks to the events table - with an offset of 5 seconds based on the time record field. Then we use this intermediate table to create the with_tumble table, where we use the tumble table valued function to add a window_start and window_end field to each record, based on the record's time field. This assigns the records to 1 minute long windows. Next we create the counts_per_team table, which groups the records by their window end and team. Finally, we order those results by window end, goal count and team. Durability OctoSQL in its current design is based on on-disk transactional storage. All state is saved this way. All interactions with datasources are designed so that no records get duplicated in the face of errors or application restarts. You can also kill the OctoSQL process and start it again with the same query and storage-directory (command line argument), it will start where it left off. By default, OctoSQL will create a temporary directory for the state and delete it after termination. Configuration The configuration file has the following form dataSources: - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... ... physical: physical_plan_option: <value> Available OctoSQL-wide configuration options are: physical groupByParallelism: The parallelism of group by's and distinct queries. Will default to the CPU core count of your machine. streamJoinParallelism: The parallelism of streaming joins. Will default to the CPU core count of your machine. execution lookupJoinPrefetchCount: The count of simultaneously processed records in a lookup join. Supported Datasources JSON JSON file in one of the following forms: one record per line, no commas JSON list of records options: path - path to file containing the data, required arrayFormat - if the JSON list of records format should be used, optional: defaults to false batchSize - number of records extracted from json file in one storage transaction, optional: defaults to 1000 CSV CSV file separated using commas. The file may or may not have column names as it's first row. options: path - path to file containing the data, required headerRow - whether the first row of the CSV file contains column names or not, optional: defaults to true separator - columns separator, optional: defaults to \",\" batchSize - number of records extracted from csv file in one storage transaction, optional: defaults to 1000 Excel A single table in an Excel spreadsheet. The table may or may not have column names as it's first row. The table can be in any sheet, and start at any point, but it cannot contain spaces between columns nor spaces between rows. options: path - path to file, required headerRow - does the first row contain column names, optional: defaults to true sheet - name of the sheet in which data is stored, optional: defaults to \"Sheet1\" rootCell - name of cell (i.e \"A3\", \"BA14\") which is the leftmost cell of the first, optional: defaults to \"A1\" timeColumns - a list of columns to parse as datetime values with second precision row, optional: defaults to [] batchSize - number of records extracted from excel file in one storage transaction, optional: defaults to 1000 Parquet A single Parquet file. Nested repeated elements are not supported. Otherwise repeated xor nested elements are supported. Currently unsupported logical types, they will get parsed as the underlying primitive type: - ENUM - TIME with NANOS precision - TIMESTAMP with NANOS precision (both UTC and non-UTC) - INTERVAL - MAP options path - path to file, required batchSize - number of records extracted from parquet file in one storage transaction, optional: defaults to 1000 PostgreSQL Single PostgreSQL database table. options: address - address including port number, optional: defaults to localhost:5432 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from PostgreSQL database in one storage transaction, optional: defaults to 1000 MySQL Single MySQL database table. options: address - address including port number, optional: defaults to localhost:3306 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from MySQL database in one storage transaction, optional: defaults to 1000 Redis Redis database with the given index. Currently only hashes are supported. options: address - address including port number, optional: defaults to localhost:6379 password - optional: defaults to \"\" databaseIndex - index number of Redis database, optional: defaults to 0 databaseKeyName - column name of Redis key in OctoSQL records, optional: defaults to \"key\" batchSize - number of records extracted from Redis database in one storage transaction, optional: defaults to 1000 Kafka Multi-partition kafka topic. optional brokers - list of broker addresses (separately hosts and ports) used to connect to the kafka cluster, optional: defaults to [\"localhost:9092\"] topic - name of topic to read messages from, required partitions - topic partition count, optional: defaults to 1 startOffset - offset from which the first batch of messages will be read, optional: defaults to -1 batchSize - number of records extracted from Kafka in one storage transaction, optional: defaults to 1 json - should the messages be decoded as JSON, optional: defaults to false Documentation Documentation for the available functions: https://github.com/cube2222/octosql/wiki/Function-Documentation Documentation for the available aggregates: https://github.com/cube2222/octosql/wiki/Aggregate-Documentation Documentation for the available triggers: https://github.com/cube2222/octosql/wiki/Trigger-Documentation Documentation for the available table valued functions: https://github.com/cube2222/octosql/wiki/Table-Valued-Functions-Documentation The SQL dialect documentation: TODO ;) in short though: Available SQL constructs: Select, Where, Order By, Group By, Offset, Limit, Left Join, Right Join, Inner Join, Distinct, Union, Union All, Subqueries, Operators, Table Valued Functions, Trigger, Common Table Expressions. Available SQL types: Int, Float, String, Bool, Time, Duration, Tuple (array), Object (e.g. JSON) Describe You can describe the current plan in graphviz format using the -describe flag, like this: octosql \"...\" --describe | dot -Tpng > output.png Architecture An OctoSQL invocation gets processed in multiple phases. SQL AST First, the SQL query gets parsed into an abstract syntax tree. This phase only rules out syntax errors. Logical Plan The SQL AST gets converted into a logical query plan. This plan is still mostly a syntactic validation. It's the most naive possible translation of the SQL query. However, this plan already has more of a map-filter-reduce form. If you wanted to add a new query language to OctoSQL, the only problem you'd have to solve is translating it to this logical plan. Physical Plan The logical plan gets converted into a physical plan. This conversion finds any semantic errors in the query. If this phase is reached, then the input is correct and OctoSQL will be able execute it. This phase already understands the specifics of the underlying datasources. So it's here where the optimizer will iteratively transform the plan, pushing computation nodes down to the datasources, and deduplicating unnecessary parts. The optimizer uses a pattern matching approach, where it has rules for matching parts of the physical plan tree and how those patterns can be restructured into a more efficient version. The rules are meant to be as simple as possible and make the smallest possible changes. For example, pushing filters under maps, if they don't use any mapped variables. This way, the optimizer just keeps on iterating on the whole tree, until it can't change anything anymore. (each iteration tries to apply each rule in each possible place in the tree) This ensures that the plan reaches a local performance minimum, and the rules should be structured so that this local minimum is equal - or close to - the global minimum. (i.e. one optimization, shouldn't make another - much more useful one - impossible) Here is an example diagram of an optimized physical plan: Execution Plan The physical plan gets materialized into an execution plan. This phase has to be able to connect to the actual datasources. It may initialize connections, open files, etc. Stream Starting the execution plan creates a stream, which underneath may hold more streams, or parts of the execution plan to create streams in the future. This stream works in a pull based model. Datasource Pushdown Operations Datasource Equality In > < <= >= MySQL supported supported supported PostgreSQL supported supported supported Redis supported supported scan Kafka scan scan scan Parquet scan scan scan JSON scan scan scan CSV scan scan scan Where scan means that the whole table needs to be scanned for each access. Telemetry OctoSQL sends application telemetry on each run to help us gauge user interest and feature use. This way we know somebody uses our software, feel our work is actually useful and can prioritize features based on actual usefulness. You can turn it off (though please don't) by setting the OCTOSQL_TELEMETRY environment variable to 0. Telemetry is also fully printed in the output log of OctoSQL, if you want to see what precisely is being sent. Roadmap Additional Datasources. SQL Constructs: JSON Query HAVING, ALL, ANY Push down functions, aggregates to databases that support them. An in-memory index to save values of subqueries and save on rescanning tables which don't support a given operation, so as not to recalculate them each time. Runtime statistics Server mode Querying a json or csv table from standard input. Integration test suite Tuple splitter, returning the row for each tuple element, with the given element instead of the tuple. ",
          "Hey, one of the authors here.<p>The motivation behind this project is that I always wanted a simple commandline tool allowing me to join data from different places, without needing to set up stuff like presto or spark. On another hand, I never encountered any tool which allows me to easily query csv and json data using SQL (which at least in my opinion is fairly ergonomic to use).<p>This started as an university project, but we're now continuing it as an open source one, as it's been a great success so far.<p>Anyways, feedback greatly requested and appreciated!",
          "Really cool, thanks for sharing. You all might want to look at Apache Calcite (<a href=\"http://calcite.apache.org/\" rel=\"nofollow\">http://calcite.apache.org/</a>) as well for inspiration, which has similar functionality as a subset of its features!"],
        "story_type":["ShowHN"],
        "url":"https://github.com/cube2222/octosql",
        "comments.comment_id":[20449703,
          20450152],
        "comments.comment_author":["cube2222",
          "MoOmer"],
        "comments.comment_descendants":[12,
          3],
        "comments.comment_time":["2019-07-16T13:16:54Z",
          "2019-07-16T14:11:16Z"],
        "comments.comment_text":["Hey, one of the authors here.<p>The motivation behind this project is that I always wanted a simple commandline tool allowing me to join data from different places, without needing to set up stuff like presto or spark. On another hand, I never encountered any tool which allows me to easily query csv and json data using SQL (which at least in my opinion is fairly ergonomic to use).<p>This started as an university project, but we're now continuing it as an open source one, as it's been a great success so far.<p>Anyways, feedback greatly requested and appreciated!",
          "Really cool, thanks for sharing. You all might want to look at Apache Calcite (<a href=\"http://calcite.apache.org/\" rel=\"nofollow\">http://calcite.apache.org/</a>) as well for inspiration, which has similar functionality as a subset of its features!"],
        "id":"ac1a4b24-39c5-44eb-9723-d1f0994109b4",
        "url_text":"OctoSQL is a query tool that allows you to join, analyse and transform data from multiple databases, streaming sources and file formats using SQL. OctoSQL is currently being rewritten on the redesign branch. Problems OctoSQL Solves You need to join / analyze data from multiple datasources. Think of enriching an Excel file by joining it with a PostgreSQL database. You need stream aggregates over time, with live output updates. Think of a live-updated leaderboard with cat images based on a \"like\" event stream. You need aggregate streams per time window, with live output updates. Think of a unique user count per hour, per country live summary. Table of Contents What is OctoSQL? Installation Quickstart Temporal SQL Features Watermarks Triggers Retractions Example Durability Configuration JSON CSV Excel Parquet PostgreSQL MySQL Redis Kafka Documentation Architecture Datasource Pushdown Operations Roadmap What is OctoSQL? OctoSQL is a SQL query engine which allows you to write standard SQL queries on data stored in multiple SQL databases, NoSQL databases, streaming sources and files in various formats trying to push down as much of the work as possible to the source databases, not transferring unnecessary data. OctoSQL does that by creating an internal representation of your query and later translating parts of it into the query languages or APIs of the source databases. Whenever a datasource doesn't support a given operation, OctoSQL will execute it in memory, so you don't have to worry about the specifics of the underlying datasources. OctoSQL also includes temporal SQL extensions, to operate ergonomically on streams and respect their event-time (not the current system-time when the records are being processed). With OctoSQL you don't need O(n) client tools or a large data analysis system deployment. Everything's contained in a single binary. Why the name? OctoSQL stems from Octopus SQL. Octopus, because octopi have many arms, so they can grasp and manipulate multiple objects, like OctoSQL is able to handle multiple datasources simultaneously. Installation Either download the binary for your operating system (Linux, OS X and Windows are supported) from the Releases page, or install using the go command line tool: GO111MODULE=on go get -u github.com/cube2222/octosql/cmd/octosql Quickstart Let's say we have a csv file with cats, and a redis database with people (potential cat owners). Now we want to get a list of cities with the number of distinct cat names in them and the cumulative number of cat lives (as each cat has up to 9 lives left). First, create a configuration file (Configuration Syntax) For example: dataSources: - name: cats type: csv config: path: \"~/Documents/cats.csv\" - name: people type: redis config: address: \"localhost:6379\" password: \"\" databaseIndex: 0 databaseKeyName: \"id\" Then, set the OCTOSQL_CONFIG environment variable to point to the configuration file. export OCTOSQL_CONFIG=~/octosql.yaml You can also use the --config command line argument. Finally, query to your hearts desire: octosql \"SELECT p.city, FIRST(c.name), COUNT(DISTINCT c.name) cats, SUM(c.livesleft) catlives FROM cats c JOIN people p ON c.ownerid = p.id GROUP BY p.city ORDER BY catlives DESC LIMIT 9\" Example output: +---------+--------------+------+----------+ | p.city | c.name_first | cats | catlives | +---------+--------------+------+----------+ | Warren | Zoey | 68 | 570 | | Gadsden | Snickers | 52 | 388 | | Staples | Harley | 54 | 383 | | Buxton | Lucky | 45 | 373 | | Bethany | Princess | 46 | 366 | | Noxen | Sheba | 49 | 361 | | Yorklyn | Scooter | 45 | 359 | | Tuttle | Toby | 57 | 356 | | Ada | Jasmine | 49 | 351 | +---------+--------------+------+----------+ You can choose between live-table batch-table live-csv batch-csv stream-json output formats. (The live-* types will update the terminal view repeatedly every second, the batch-* ones will write the output once before exiting, the stream-* ones will print records whenever they are available) Temporal SQL Features OctoSQL features temporal SQL extensions inspired by the paper One SQL to Rule Them All. Introduction Often when you're working with streams of events, you'd like to use the time dimension somehow: Calculate average values for a day sliced by hours. Get unique user counts per day. and others All those examples have one thing in common: The time value of an event is crucial for correctness. A naive system could just use the current clock time whenever it receives an event. The correctness of this approach however, degrades quickly in the face of network problems, delivery delays, clock skew. This can be solved by using a value from the event as its time value. A new problem arises though: how do I know that I've received all events up to time X and can publish results for a given hour. You never know if there isn't somewhere a delayed event which should be factored in. This is where watermarks come into play. Watermarks Watermarks are a heuristic which try to approximate the \"current time\" when processing events. Said differently: When I receive a watermark for 12:00 I can be sure enough I've received all events of interest up to 12:00. To achieve this, they are generated at streaming sources and propagate downstream through the whole processing pipeline. The generation of watermarks usually relies on heuristics which provide satisfactory results for our given use case. OctoSQL currently contains the following watermark generators: Maximum difference watermark generator (with an offset argument) With an offset of 10 seconds, this generator says: When I've received an event for 12:00:00, then I'm sure I won't receive any event older than 11:59:50. Percentile watermark generator (with a percentile argument) With a percentile of 99.5, it will look at a specified number of recent events, and generate a watermark so that 99.5% of those events are after the watermark (not yet triggered), and the remaining 0.5% are before it. This way we set the watermark so that only a fraction of the recently seen events is potentially ignored as being late. Watermark generators are specified using table valued functions and are documented in the wiki. Triggers Another matter is triggering of keys in aggregations. Sometimes you'd like to only see the value for a given key (hour) when you know it's done, but othertimes you'd like to see partial results (how's the unique user count going this hour). That's where you can use triggers. Triggers allow you to specify when a given aggregate (or join window for that matter) is emitted or updated. OctoSQL contains multiple triggers: Watermark Trigger This is the most straightforward trigger. It emits a value whenever the watermark for a given key (or the end of the stream) is reached. So basically the \"show me when it's done\". Counting Trigger (with a count argument) This trigger will emit a value for a key every time it receives count records with this key. The count is reset whenever the key is triggered. Delay Trigger (with a delay argument) This trigger will emit a value for a key whenever the key has been inactive for the delay period. You can use multiple triggers simultaneously. (Show me the current sum every 10 received events, but also the final value after having received the watermark.) Retractions A key can be triggered multiple times with partial results. How do we know a given record is a retriggering of some key, and not a new unrelated record? OctoSQL solves this problem using a dataflow-like architecture. This means whenever a new value is sent for a key, a retraction is send for the old value. In practice this means every update is accompanied by the old record with an undo flag set. This can be visible when using a stream-* output format with partial results. Example Now we can see how it all fits together. In this example we have an events file, which contains records about points being scored in a game by multiple teams. WITH with_watermark AS (SELECT * FROM max_diff_watermark(source=>TABLE(events), offset=>INTERVAL 5 SECONDS, time_field=>DESCRIPTOR(time)) e), with_tumble AS (SELECT * FROM tumble(source=>TABLE(with_watermark), time_field=>DESCRIPTOR(e.time), window_length=> INTERVAL 1 MINUTE, offset => INTERVAL 0 SECONDS) e), counts_per_team AS (SELECT e.window_end, e.team, COUNT(*) as goals FROM with_tumble e GROUP BY e.window_end, e.team TRIGGER COUNTING 100, ON WATERMARK) SELECT * FROM counts_per_team cpt ORDER BY cpt.window_end DESC, cpt.goals ASC, cpt.team DESC We use common table expressions to break the query up into multiple stages. First we create the with_watermark intermediate table/stream. Here we use the table valued function max_diff_watermark to add watermarks to the events table - with an offset of 5 seconds based on the time record field. Then we use this intermediate table to create the with_tumble table, where we use the tumble table valued function to add a window_start and window_end field to each record, based on the record's time field. This assigns the records to 1 minute long windows. Next we create the counts_per_team table, which groups the records by their window end and team. Finally, we order those results by window end, goal count and team. Durability OctoSQL in its current design is based on on-disk transactional storage. All state is saved this way. All interactions with datasources are designed so that no records get duplicated in the face of errors or application restarts. You can also kill the OctoSQL process and start it again with the same query and storage-directory (command line argument), it will start where it left off. By default, OctoSQL will create a temporary directory for the state and delete it after termination. Configuration The configuration file has the following form dataSources: - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... ... physical: physical_plan_option: <value> Available OctoSQL-wide configuration options are: physical groupByParallelism: The parallelism of group by's and distinct queries. Will default to the CPU core count of your machine. streamJoinParallelism: The parallelism of streaming joins. Will default to the CPU core count of your machine. execution lookupJoinPrefetchCount: The count of simultaneously processed records in a lookup join. Supported Datasources JSON JSON file in one of the following forms: one record per line, no commas JSON list of records options: path - path to file containing the data, required arrayFormat - if the JSON list of records format should be used, optional: defaults to false batchSize - number of records extracted from json file in one storage transaction, optional: defaults to 1000 CSV CSV file separated using commas. The file may or may not have column names as it's first row. options: path - path to file containing the data, required headerRow - whether the first row of the CSV file contains column names or not, optional: defaults to true separator - columns separator, optional: defaults to \",\" batchSize - number of records extracted from csv file in one storage transaction, optional: defaults to 1000 Excel A single table in an Excel spreadsheet. The table may or may not have column names as it's first row. The table can be in any sheet, and start at any point, but it cannot contain spaces between columns nor spaces between rows. options: path - path to file, required headerRow - does the first row contain column names, optional: defaults to true sheet - name of the sheet in which data is stored, optional: defaults to \"Sheet1\" rootCell - name of cell (i.e \"A3\", \"BA14\") which is the leftmost cell of the first, optional: defaults to \"A1\" timeColumns - a list of columns to parse as datetime values with second precision row, optional: defaults to [] batchSize - number of records extracted from excel file in one storage transaction, optional: defaults to 1000 Parquet A single Parquet file. Nested repeated elements are not supported. Otherwise repeated xor nested elements are supported. Currently unsupported logical types, they will get parsed as the underlying primitive type: - ENUM - TIME with NANOS precision - TIMESTAMP with NANOS precision (both UTC and non-UTC) - INTERVAL - MAP options path - path to file, required batchSize - number of records extracted from parquet file in one storage transaction, optional: defaults to 1000 PostgreSQL Single PostgreSQL database table. options: address - address including port number, optional: defaults to localhost:5432 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from PostgreSQL database in one storage transaction, optional: defaults to 1000 MySQL Single MySQL database table. options: address - address including port number, optional: defaults to localhost:3306 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from MySQL database in one storage transaction, optional: defaults to 1000 Redis Redis database with the given index. Currently only hashes are supported. options: address - address including port number, optional: defaults to localhost:6379 password - optional: defaults to \"\" databaseIndex - index number of Redis database, optional: defaults to 0 databaseKeyName - column name of Redis key in OctoSQL records, optional: defaults to \"key\" batchSize - number of records extracted from Redis database in one storage transaction, optional: defaults to 1000 Kafka Multi-partition kafka topic. optional brokers - list of broker addresses (separately hosts and ports) used to connect to the kafka cluster, optional: defaults to [\"localhost:9092\"] topic - name of topic to read messages from, required partitions - topic partition count, optional: defaults to 1 startOffset - offset from which the first batch of messages will be read, optional: defaults to -1 batchSize - number of records extracted from Kafka in one storage transaction, optional: defaults to 1 json - should the messages be decoded as JSON, optional: defaults to false Documentation Documentation for the available functions: https://github.com/cube2222/octosql/wiki/Function-Documentation Documentation for the available aggregates: https://github.com/cube2222/octosql/wiki/Aggregate-Documentation Documentation for the available triggers: https://github.com/cube2222/octosql/wiki/Trigger-Documentation Documentation for the available table valued functions: https://github.com/cube2222/octosql/wiki/Table-Valued-Functions-Documentation The SQL dialect documentation: TODO ;) in short though: Available SQL constructs: Select, Where, Order By, Group By, Offset, Limit, Left Join, Right Join, Inner Join, Distinct, Union, Union All, Subqueries, Operators, Table Valued Functions, Trigger, Common Table Expressions. Available SQL types: Int, Float, String, Bool, Time, Duration, Tuple (array), Object (e.g. JSON) Describe You can describe the current plan in graphviz format using the -describe flag, like this: octosql \"...\" --describe | dot -Tpng > output.png Architecture An OctoSQL invocation gets processed in multiple phases. SQL AST First, the SQL query gets parsed into an abstract syntax tree. This phase only rules out syntax errors. Logical Plan The SQL AST gets converted into a logical query plan. This plan is still mostly a syntactic validation. It's the most naive possible translation of the SQL query. However, this plan already has more of a map-filter-reduce form. If you wanted to add a new query language to OctoSQL, the only problem you'd have to solve is translating it to this logical plan. Physical Plan The logical plan gets converted into a physical plan. This conversion finds any semantic errors in the query. If this phase is reached, then the input is correct and OctoSQL will be able execute it. This phase already understands the specifics of the underlying datasources. So it's here where the optimizer will iteratively transform the plan, pushing computation nodes down to the datasources, and deduplicating unnecessary parts. The optimizer uses a pattern matching approach, where it has rules for matching parts of the physical plan tree and how those patterns can be restructured into a more efficient version. The rules are meant to be as simple as possible and make the smallest possible changes. For example, pushing filters under maps, if they don't use any mapped variables. This way, the optimizer just keeps on iterating on the whole tree, until it can't change anything anymore. (each iteration tries to apply each rule in each possible place in the tree) This ensures that the plan reaches a local performance minimum, and the rules should be structured so that this local minimum is equal - or close to - the global minimum. (i.e. one optimization, shouldn't make another - much more useful one - impossible) Here is an example diagram of an optimized physical plan: Execution Plan The physical plan gets materialized into an execution plan. This phase has to be able to connect to the actual datasources. It may initialize connections, open files, etc. Stream Starting the execution plan creates a stream, which underneath may hold more streams, or parts of the execution plan to create streams in the future. This stream works in a pull based model. Datasource Pushdown Operations Datasource Equality In > < <= >= MySQL supported supported supported PostgreSQL supported supported supported Redis supported supported scan Kafka scan scan scan Parquet scan scan scan JSON scan scan scan CSV scan scan scan Where scan means that the whole table needs to be scanned for each access. Telemetry OctoSQL sends application telemetry on each run to help us gauge user interest and feature use. This way we know somebody uses our software, feel our work is actually useful and can prioritize features based on actual usefulness. You can turn it off (though please don't) by setting the OCTOSQL_TELEMETRY environment variable to 0. Telemetry is also fully printed in the output log of OctoSQL, if you want to see what precisely is being sent. Roadmap Additional Datasources. SQL Constructs: JSON Query HAVING, ALL, ANY Push down functions, aggregates to databases that support them. An in-memory index to save values of subqueries and save on rescanning tables which don't support a given operation, so as not to recalculate them each time. Runtime statistics Server mode Querying a json or csv table from standard input. Integration test suite Tuple splitter, returning the row for each tuple element, with the given element instead of the tuple. ",
        "_version_":1718939852806815744},
      {
        "story_id":[21676256],
        "story_author":["osprojects"],
        "story_descendants":[4],
        "story_score":[32],
        "story_time":["2019-12-01T16:42:50Z"],
        "story_title":"Open Source Webhook Server",
        "search":["Open Source Webhook Server",
          "https://github.com/adnanh/webhook",
          "What is webhook? webhook is a lightweight configurable tool written in Go, that allows you to easily create HTTP endpoints (hooks) on your server, which you can use to execute configured commands. You can also pass data from the HTTP request (such as headers, payload or query variables) to your commands. webhook also allows you to specify rules which have to be satisfied in order for the hook to be triggered. For example, if you're using Github or Bitbucket, you can use webhook to set up a hook that runs a redeploy script for your project on your staging server, whenever you push changes to the master branch of your project. If you use Mattermost or Slack, you can set up an \"Outgoing webhook integration\" or \"Slash command\" to run various commands on your server, which can then report back directly to you or your channels using the \"Incoming webhook integrations\", or the appropriate response body. webhook aims to do nothing more than it should do, and that is: receive the request, parse the headers, payload and query variables, check if the specified rules for the hook are satisfied, and finally, pass the specified arguments to the specified command via command line arguments or via environment variables. Everything else is the responsibility of the command's author. Hookdoo If you don't have time to waste configuring, hosting, debugging and maintaining your webhook instance, we offer a SaaS solution that has all of the capabilities webhook provides, plus a lot more, and all that packaged in a nice friendly web interface. If you are interested, find out more at hookdoo website. If you have any questions, you can contact us at info@hookdoo.com If you need a way of inspecting, monitoring and replaying webhooks without the back and forth troubleshooting, give Hookdeck a try! Getting started Installation Building from source To get started, first make sure you've properly set up your Go 1.14 or newer environment and then run $ go build github.com/adnanh/webhook to build the latest version of the webhook. Using package manager Snap store Ubuntu If you are using Ubuntu linux (17.04 or later), you can install webhook using sudo apt-get install webhook which will install community packaged version. Debian If you are using Debian linux (\"stretch\" or later), you can install webhook using sudo apt-get install webhook which will install community packaged version (thanks @freeekanayaka) from https://packages.debian.org/sid/webhook Download prebuilt binaries Prebuilt binaries for different architectures are available at GitHub Releases. Configuration Next step is to define some hooks you want webhook to serve. webhook supports JSON or YAML configuration files, but we'll focus primarily on JSON in the following example. Begin by creating an empty file named hooks.json. This file will contain an array of hooks the webhook will serve. Check Hook definition page to see the detailed description of what properties a hook can contain, and how to use them. Let's define a simple hook named redeploy-webhook that will run a redeploy script located in /var/scripts/redeploy.sh. Make sure that your bash script has #!/bin/sh shebang on top. Our hooks.json file will now look like this: [ { \"id\": \"redeploy-webhook\", \"execute-command\": \"/var/scripts/redeploy.sh\", \"command-working-directory\": \"/var/webhook\" } ] NOTE: If you prefer YAML, the equivalent hooks.yaml file would be: - id: redeploy-webhook execute-command: \"/var/scripts/redeploy.sh\" command-working-directory: \"/var/webhook\" You can now run webhook using $ /path/to/webhook -hooks hooks.json -verbose It will start up on default port 9000 and will provide you with one HTTP endpoint http://yourserver:9000/hooks/redeploy-webhook Check webhook parameters page to see how to override the ip, port and other settings such as hook hotreload, verbose output, etc, when starting the webhook. By performing a simple HTTP GET or POST request to that endpoint, your specified redeploy script would be executed. Neat! However, hook defined like that could pose a security threat to your system, because anyone who knows your endpoint, can send a request and execute your command. To prevent that, you can use the \"trigger-rule\" property for your hook, to specify the exact circumstances under which the hook would be triggered. For example, you can use them to add a secret that you must supply as a parameter in order to successfully trigger the hook. Please check out the Hook rules page for detailed list of available rules and their usage. Multipart Form Data webhook provides limited support the parsing of multipart form data. Multipart form data can contain two types of parts: values and files. All form values are automatically added to the payload scope. Use the parse-parameters-as-json settings to parse a given value as JSON. All files are ignored unless they match one of the following criteria: The Content-Type header is application/json. The part is named in the parse-parameters-as-json setting. In either case, the given file part will be parsed as JSON and added to the payload map. Templates webhook can parse the hooks configuration file as a Go template when given the -template CLI parameter. See the Templates page for more details on template usage. Using HTTPS webhook by default serves hooks using http. If you want webhook to serve secure content using https, you can use the -secure flag while starting webhook. Files containing a certificate and matching private key for the server must be provided using the -cert /path/to/cert.pem and -key /path/to/key.pem flags. If the certificate is signed by a certificate authority, the cert file should be the concatenation of the server's certificate followed by the CA's certificate. TLS version and cipher suite selection flags are available from the command line. To list available cipher suites, use the -list-cipher-suites flag. The -tls-min-version flag can be used with -list-cipher-suites. CORS Headers If you want to set CORS headers, you can use the -header name=value flag while starting webhook to set the appropriate CORS headers that will be returned with each response. Interested in running webhook inside of a Docker container? You can use one of the following Docker images, or create your own (please read this discussion): almir/webhook roxedus/webhook thecatlady/webhook Examples Check out Hook examples page for more complex examples of hooks. Guides featuring webhook Plex 2 Telegram by @psyhomb Webhook & JIRA by @perfecto25 Trigger Ansible AWX job runs on SCM (e.g. git) commit by @jpmens Deploy using GitHub webhooks by @awea Setting up Automatic Deployment and Builds Using Webhooks by Will Browning Auto deploy your Node.js app on push to GitHub in 3 simple steps by Karolis Rusenas Automate Static Site Deployments with Salt, Git, and Webhooks by Linode Using Prometheus to Automatically Scale WebLogic Clusters on Kubernetes by Marina Kogan Github Pages and Jekyll - A New Platform for LACNIC Labs by Carlos Martnez Cagnazzo How to Deploy React Apps Using Webhooks and Integrating Slack on Ubuntu by Arslan Ud Din Shafiq Private webhooks by Thomas Adventures in webhooks by Drake GitHub pro tips by Spencer Lyon XiaoMi Vacuum + Amazon Button = Dash Cleaning by c0mmensal Set up Automated Deployments From Github With Webhook by Maxim Orlov VIDEO: Gitlab CI/CD configuration using Docker and adnanh/webhook to deploy on VPS - Tutorial #1 by Yes! Let's Learn Software Engineering Integrate automatic deployment in 20 minutes using webhooks + Nginx setup by Anksus ... Want to add your own? Open an Issue or create a PR :-) Community Contributions See the webhook-contrib repository for a collections of tools and helpers related to webhook that have been contributed by the webhook community. Need help? Check out existing issues to see if someone else also had the same problem, or open a new one. Support active development Sponsors DigitalOcean is a simple and robust cloud computing platform, designed for developers. BrowserStack is a cloud-based cross-browser testing tool that enables developers to test their websites across various browsers on different operating systems and mobile devices, without requiring users to install virtual machines, devices or emulators. Support this project by becoming a sponsor. Your logo will show up here with a link to your website. By contributing This project exists thanks to all the people who contribute. Contribute!. By giving money OpenCollective Backer OpenCollective Sponsor PayPal Patreon Faircode Flattr Thank you to all our backers! License The MIT License (MIT) Copyright (c) 2015 Adnan Hajdarevic adnanh@gmail.com Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ",
          "Interesting, could be useful to interface some of my stuff with IFTTT or Integromat.",
          "I have been using this webhook server in prod for a few years and its been easy to setup/maintain ... you just define  github.com repo to publish `git push` or whatever then this webhook server listens to every git push my team makes to launch a code recompile/redeploy ...  foolproof and solid ... I highly recommend"],
        "story_type":["Normal"],
        "url":"https://github.com/adnanh/webhook",
        "comments.comment_id":[21679205,
          21688092],
        "comments.comment_author":["m-p-3",
          "AtomicOrbital"],
        "comments.comment_descendants":[1,
          0],
        "comments.comment_time":["2019-12-02T01:32:48Z",
          "2019-12-02T23:09:42Z"],
        "comments.comment_text":["Interesting, could be useful to interface some of my stuff with IFTTT or Integromat.",
          "I have been using this webhook server in prod for a few years and its been easy to setup/maintain ... you just define  github.com repo to publish `git push` or whatever then this webhook server listens to every git push my team makes to launch a code recompile/redeploy ...  foolproof and solid ... I highly recommend"],
        "id":"2c16faa2-3ac8-4509-af87-d3df5d37247b",
        "url_text":"What is webhook? webhook is a lightweight configurable tool written in Go, that allows you to easily create HTTP endpoints (hooks) on your server, which you can use to execute configured commands. You can also pass data from the HTTP request (such as headers, payload or query variables) to your commands. webhook also allows you to specify rules which have to be satisfied in order for the hook to be triggered. For example, if you're using Github or Bitbucket, you can use webhook to set up a hook that runs a redeploy script for your project on your staging server, whenever you push changes to the master branch of your project. If you use Mattermost or Slack, you can set up an \"Outgoing webhook integration\" or \"Slash command\" to run various commands on your server, which can then report back directly to you or your channels using the \"Incoming webhook integrations\", or the appropriate response body. webhook aims to do nothing more than it should do, and that is: receive the request, parse the headers, payload and query variables, check if the specified rules for the hook are satisfied, and finally, pass the specified arguments to the specified command via command line arguments or via environment variables. Everything else is the responsibility of the command's author. Hookdoo If you don't have time to waste configuring, hosting, debugging and maintaining your webhook instance, we offer a SaaS solution that has all of the capabilities webhook provides, plus a lot more, and all that packaged in a nice friendly web interface. If you are interested, find out more at hookdoo website. If you have any questions, you can contact us at info@hookdoo.com If you need a way of inspecting, monitoring and replaying webhooks without the back and forth troubleshooting, give Hookdeck a try! Getting started Installation Building from source To get started, first make sure you've properly set up your Go 1.14 or newer environment and then run $ go build github.com/adnanh/webhook to build the latest version of the webhook. Using package manager Snap store Ubuntu If you are using Ubuntu linux (17.04 or later), you can install webhook using sudo apt-get install webhook which will install community packaged version. Debian If you are using Debian linux (\"stretch\" or later), you can install webhook using sudo apt-get install webhook which will install community packaged version (thanks @freeekanayaka) from https://packages.debian.org/sid/webhook Download prebuilt binaries Prebuilt binaries for different architectures are available at GitHub Releases. Configuration Next step is to define some hooks you want webhook to serve. webhook supports JSON or YAML configuration files, but we'll focus primarily on JSON in the following example. Begin by creating an empty file named hooks.json. This file will contain an array of hooks the webhook will serve. Check Hook definition page to see the detailed description of what properties a hook can contain, and how to use them. Let's define a simple hook named redeploy-webhook that will run a redeploy script located in /var/scripts/redeploy.sh. Make sure that your bash script has #!/bin/sh shebang on top. Our hooks.json file will now look like this: [ { \"id\": \"redeploy-webhook\", \"execute-command\": \"/var/scripts/redeploy.sh\", \"command-working-directory\": \"/var/webhook\" } ] NOTE: If you prefer YAML, the equivalent hooks.yaml file would be: - id: redeploy-webhook execute-command: \"/var/scripts/redeploy.sh\" command-working-directory: \"/var/webhook\" You can now run webhook using $ /path/to/webhook -hooks hooks.json -verbose It will start up on default port 9000 and will provide you with one HTTP endpoint http://yourserver:9000/hooks/redeploy-webhook Check webhook parameters page to see how to override the ip, port and other settings such as hook hotreload, verbose output, etc, when starting the webhook. By performing a simple HTTP GET or POST request to that endpoint, your specified redeploy script would be executed. Neat! However, hook defined like that could pose a security threat to your system, because anyone who knows your endpoint, can send a request and execute your command. To prevent that, you can use the \"trigger-rule\" property for your hook, to specify the exact circumstances under which the hook would be triggered. For example, you can use them to add a secret that you must supply as a parameter in order to successfully trigger the hook. Please check out the Hook rules page for detailed list of available rules and their usage. Multipart Form Data webhook provides limited support the parsing of multipart form data. Multipart form data can contain two types of parts: values and files. All form values are automatically added to the payload scope. Use the parse-parameters-as-json settings to parse a given value as JSON. All files are ignored unless they match one of the following criteria: The Content-Type header is application/json. The part is named in the parse-parameters-as-json setting. In either case, the given file part will be parsed as JSON and added to the payload map. Templates webhook can parse the hooks configuration file as a Go template when given the -template CLI parameter. See the Templates page for more details on template usage. Using HTTPS webhook by default serves hooks using http. If you want webhook to serve secure content using https, you can use the -secure flag while starting webhook. Files containing a certificate and matching private key for the server must be provided using the -cert /path/to/cert.pem and -key /path/to/key.pem flags. If the certificate is signed by a certificate authority, the cert file should be the concatenation of the server's certificate followed by the CA's certificate. TLS version and cipher suite selection flags are available from the command line. To list available cipher suites, use the -list-cipher-suites flag. The -tls-min-version flag can be used with -list-cipher-suites. CORS Headers If you want to set CORS headers, you can use the -header name=value flag while starting webhook to set the appropriate CORS headers that will be returned with each response. Interested in running webhook inside of a Docker container? You can use one of the following Docker images, or create your own (please read this discussion): almir/webhook roxedus/webhook thecatlady/webhook Examples Check out Hook examples page for more complex examples of hooks. Guides featuring webhook Plex 2 Telegram by @psyhomb Webhook & JIRA by @perfecto25 Trigger Ansible AWX job runs on SCM (e.g. git) commit by @jpmens Deploy using GitHub webhooks by @awea Setting up Automatic Deployment and Builds Using Webhooks by Will Browning Auto deploy your Node.js app on push to GitHub in 3 simple steps by Karolis Rusenas Automate Static Site Deployments with Salt, Git, and Webhooks by Linode Using Prometheus to Automatically Scale WebLogic Clusters on Kubernetes by Marina Kogan Github Pages and Jekyll - A New Platform for LACNIC Labs by Carlos Martnez Cagnazzo How to Deploy React Apps Using Webhooks and Integrating Slack on Ubuntu by Arslan Ud Din Shafiq Private webhooks by Thomas Adventures in webhooks by Drake GitHub pro tips by Spencer Lyon XiaoMi Vacuum + Amazon Button = Dash Cleaning by c0mmensal Set up Automated Deployments From Github With Webhook by Maxim Orlov VIDEO: Gitlab CI/CD configuration using Docker and adnanh/webhook to deploy on VPS - Tutorial #1 by Yes! Let's Learn Software Engineering Integrate automatic deployment in 20 minutes using webhooks + Nginx setup by Anksus ... Want to add your own? Open an Issue or create a PR :-) Community Contributions See the webhook-contrib repository for a collections of tools and helpers related to webhook that have been contributed by the webhook community. Need help? Check out existing issues to see if someone else also had the same problem, or open a new one. Support active development Sponsors DigitalOcean is a simple and robust cloud computing platform, designed for developers. BrowserStack is a cloud-based cross-browser testing tool that enables developers to test their websites across various browsers on different operating systems and mobile devices, without requiring users to install virtual machines, devices or emulators. Support this project by becoming a sponsor. Your logo will show up here with a link to your website. By contributing This project exists thanks to all the people who contribute. Contribute!. By giving money OpenCollective Backer OpenCollective Sponsor PayPal Patreon Faircode Flattr Thank you to all our backers! License The MIT License (MIT) Copyright (c) 2015 Adnan Hajdarevic adnanh@gmail.com Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ",
        "_version_":1718939883350786048},
      {
        "story_id":[19023196],
        "story_author":["l2g"],
        "story_descendants":[22],
        "story_score":[163],
        "story_time":["2019-01-29T02:30:20Z"],
        "story_title":"Show HN: Apprise – A lightweight all-in-one notification solution",
        "search":["Show HN: Apprise – A lightweight all-in-one notification solution",
          "https://github.com/caronc/apprise",
          "apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations ",
          "Cool stuff! But what about AWS SNS (<a href=\"https://aws.amazon.com/sns/\" rel=\"nofollow\">https://aws.amazon.com/sns/</a>) or Firebase Cloud Messaging (<a href=\"https://firebase.google.com/docs/cloud-messaging/\" rel=\"nofollow\">https://firebase.google.com/docs/cloud-messaging/</a>)?",
          "This is pretty cool, I drop the following shell script on all my servers:<p><pre><code>    #!/bin/bash\n    \n    if [[ -z $1 && -z $2 ]]; then\n        echo \"No Message passed\"\n    else\n        if [[ -z $2 ]]; then\n            curl -s --form-string \"token=MYAPPTOKEN\" --form-string \"user=MYUSERTOKEN\" --form-string \"message=$1\" https://api.pushover.net/1/messages.json\n        else\n            curl -s --form-string \"token=MYAPPTOKEN\" --form-string \"user=MYUSERTOKEN\" --form-string \"title=$1\" --form-string \"message=$2\"  https://api.pushover.net/1/messages.json\n        fi\n    fi\n</code></pre>\nIt's SUPER basic and probably shitty but for me it's perfect. I can add \" && push 'Command is Done'\" to the end of any command and get a notification on my watch/phone/(and desktop? But I don't have pushover on my desktop installed). Great for things you threw into a screen/tmux session and want to know when they finished."],
        "story_type":["ShowHN"],
        "url":"https://github.com/caronc/apprise",
        "comments.comment_id":[19026288,
          19027306],
        "comments.comment_author":["harryf",
          "joshstrange"],
        "comments.comment_descendants":[1,
          0],
        "comments.comment_time":["2019-01-29T14:00:21Z",
          "2019-01-29T15:54:22Z"],
        "comments.comment_text":["Cool stuff! But what about AWS SNS (<a href=\"https://aws.amazon.com/sns/\" rel=\"nofollow\">https://aws.amazon.com/sns/</a>) or Firebase Cloud Messaging (<a href=\"https://firebase.google.com/docs/cloud-messaging/\" rel=\"nofollow\">https://firebase.google.com/docs/cloud-messaging/</a>)?",
          "This is pretty cool, I drop the following shell script on all my servers:<p><pre><code>    #!/bin/bash\n    \n    if [[ -z $1 && -z $2 ]]; then\n        echo \"No Message passed\"\n    else\n        if [[ -z $2 ]]; then\n            curl -s --form-string \"token=MYAPPTOKEN\" --form-string \"user=MYUSERTOKEN\" --form-string \"message=$1\" https://api.pushover.net/1/messages.json\n        else\n            curl -s --form-string \"token=MYAPPTOKEN\" --form-string \"user=MYUSERTOKEN\" --form-string \"title=$1\" --form-string \"message=$2\"  https://api.pushover.net/1/messages.json\n        fi\n    fi\n</code></pre>\nIt's SUPER basic and probably shitty but for me it's perfect. I can add \" && push 'Command is Done'\" to the end of any command and get a notification on my watch/phone/(and desktop? But I don't have pushover on my desktop installed). Great for things you threw into a screen/tmux session and want to know when they finished."],
        "id":"56548859-472e-4712-bbfa-795dd035932f",
        "url_text":"apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations ",
        "_version_":1718939818468048898},
      {
        "story_id":[20587541],
        "story_author":["l2g"],
        "story_descendants":[6],
        "story_score":[35],
        "story_time":["2019-08-01T19:44:25Z"],
        "story_title":"Show HN: Apprise – A lightweight all-in-one notification solution (update)",
        "search":["Show HN: Apprise – A lightweight all-in-one notification solution (update)",
          "https://github.com/caronc/apprise/#showhn-one-last-time",
          "apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations ",
          "6 months ago [I posted about Apprise here](<a href=\"https://news.ycombinator.com/item?id=19023196\" rel=\"nofollow\">https://news.ycombinator.com/item?id=19023196</a>) and got a lot of amazing and encouraging feedback!  I since took just about everyone's comments and ideas at the time and implemented most of them.<p>Apprise now supports over 40+ different notification services, including configuration files that can be read from disk and the cloud! The library remains incredible light weight and easy to use.<p>I just wanted to share an almost completed solution and hope to hit you all up for more of your thoughts and advice!",
          "Worked about 10 years for a company named Appriss - <a href=\"https://appriss.com\" rel=\"nofollow\">https://appriss.com</a> - no real comment there other than to share the founding CEO used to say the board was presented with a bunch of names \"<i>and picked the worst one</i>\".<p>No real comment other than that, but nostalgia alone will make me play with this, so thanks for sharing."],
        "story_type":["ShowHN"],
        "url":"https://github.com/caronc/apprise/#showhn-one-last-time",
        "comments.comment_id":[20587555,
          20597895],
        "comments.comment_author":["l2g",
          "MaxwellsDaemon"],
        "comments.comment_descendants":[1,
          1],
        "comments.comment_time":["2019-08-01T19:45:59Z",
          "2019-08-03T00:50:08Z"],
        "comments.comment_text":["6 months ago [I posted about Apprise here](<a href=\"https://news.ycombinator.com/item?id=19023196\" rel=\"nofollow\">https://news.ycombinator.com/item?id=19023196</a>) and got a lot of amazing and encouraging feedback!  I since took just about everyone's comments and ideas at the time and implemented most of them.<p>Apprise now supports over 40+ different notification services, including configuration files that can be read from disk and the cloud! The library remains incredible light weight and easy to use.<p>I just wanted to share an almost completed solution and hope to hit you all up for more of your thoughts and advice!",
          "Worked about 10 years for a company named Appriss - <a href=\"https://appriss.com\" rel=\"nofollow\">https://appriss.com</a> - no real comment there other than to share the founding CEO used to say the board was presented with a bunch of names \"<i>and picked the worst one</i>\".<p>No real comment other than that, but nostalgia alone will make me play with this, so thanks for sharing."],
        "id":"39ca47c9-3094-49bc-ab11-1ffe22821054",
        "url_text":"apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations ",
        "_version_":1718939855808888832},
      {
        "story_id":[19074170],
        "story_author":["nosarthur"],
        "story_descendants":[36],
        "story_score":[79],
        "story_time":["2019-02-04T05:22:20Z"],
        "story_title":"Show HN: Gita – a CLI tool to manage multiple Git repos",
        "search":["Show HN: Gita – a CLI tool to manage multiple Git repos",
          "https://github.com/nosarthur/gita",
          "_______________________________ ( ____ \\__ __|__ __( ___ ) | ( \\/ ) ( ) ( | ( ) | | | | | | | | (___) | | | ____ | | | | | ___ | | | \\_ ) | | | | | ( ) | | (___) |__) (___ | | | ) ( | (_______)_______/ )_( |/ \\| v0.15 Gita: a command-line tool to manage multiple git repos This tool does two things display the status of multiple git repos such as branch, modification, commit message side by side (batch) delegate git commands/aliases from any working directory If several repos are related, it helps to see their status together. I also hate to change directories to execute git commands. In this screenshot, the gita ll command displays the status of all repos. The gita remote dotfiles command translates to git remote -v for the dotfiles repo, even though we are not in the repo. The gita fetch command fetches from all repos and two of them have updates. To see the pre-defined commands, run gita -h or take a look at cmds.json. To add your own commands, see the customization section. To run arbitrary git command, see the superman mode section. To run arbitrary shell command, see the shell mode section. The branch color distinguishes 5 situations between local and remote branches: color meaning white local has no remote green local is the same as remote red local has diverged from remote purple local is ahead of remote (good for push) yellow local is behind remote (good for merge) The choice of purple for ahead and yellow for behind is motivated by blueshift and redshift, using green as baseline. You can change the color scheme using the gita color command. See the customization section. The additional status symbols denote symbol meaning + staged changes * unstaged changes _ untracked files/folders The bookkeeping sub-commands are gita add <repo-path(s)> [-g <groupname>]: add repo(s) to gita, optionally into an existing group gita add -a <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively and automatically generate hierarchical groups. See the customization section for more details. gita add -b <bare-repo-path(s)>: add bare repo(s) to gita. See the customization section for more details on setting custom worktree. gita add -r <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively gita clone <config-file>: clone repos in config-file (generated by gita freeze) to current directory. gita clone -p <config-file>: clone repos in config-file to prescribed paths. gita context: context sub-command gita context: show current context gita context <group-name>: set context to group-name, all operations then only apply to repos in this group gita context auto: set context automatically according to the current working directory gita context none: remove context gita color: color sub-command gita color [ll]: Show available colors and the current coloring scheme gita color reset: Reset to the default coloring scheme gita color set <situation> <color>: Use the specified color for the local-remote situation gita flags: flags sub-command gita flags set <repo-name> <flags>: add custom flags to repo gita flags [ll]: display repos with custom flags gita freeze: print information of all repos such as URL, name, and path. Use with gita clone. gita group: group sub-command gita group add <repo-name(s)> -n <group-name>: add repo(s) to a new or existing group gita group [ll]: display existing groups with repos gita group ls: display existing group names gita group rename <group-name> <new-name>: change group name gita group rm <group-name(s)>: delete group(s) gita group rmrepo <repo-name(s)> -n <group-name>: remove repo(s) from existing group gita info: info sub-command gita info [ll]: display the used and unused information items gita info add <info-item>: enable information item gita info rm <info-item>: disable information item gita ll: display the status of all repos gita ll <group-name>: display the status of repos in a group gita ll -g: display the repo summaries by groups gita ls: display the names of all repos gita ls <repo-name>: display the absolute path of one repo gita rename <repo-name> <new-name>: rename a repo gita rm <repo-name(s)>: remove repo(s) from gita (won't remove files on disk) gita -v: display gita version The git delegating sub-commands are of two formats gita <sub-command> [repo-name(s) or group-name(s)]: optional repo or group input, and no input means all repos. gita <sub-command> <repo-name(s) or groups-name(s)>: required repo name(s) or group name(s) input They translate to git <sub-command> for the corresponding repos. By default, only fetch and pull take optional input. In other words, gita fetch and gita pull apply to all repos. To see the pre-defined sub-commands, run gita -h or take a look at cmds.json. To add your own sub-commands or override the default behaviors, see the customization section. To run arbitrary git command, see the superman mode section. If more than one repos are specified, the git command runs asynchronously, with the exception of log, difftool and mergetool, which require non-trivial user input. Repo configuration is saved in $XDG_CONFIG_HOME/gita/repos.csv (most likely ~/.config/gita/repos.csv). Installation To install the latest version, run If you prefer development mode, download the source code and run pip3 install -e <gita-source-folder> In either case, calling gita in terminal may not work, then put the following line in the .bashrc file. alias gita=\"python3 -m gita\" Windows users may need to enable the ANSI escape sequence in terminal for the branch color to work. See this stackoverflow post for details. Auto-completion Download .gita-completion.bash or .gita-completion.zsh and source it in shell. Superman mode The superman mode delegates any git command or alias. Usage: gita super [repo-name(s) or group-name(s)] <any-git-command-with-or-without-options> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita super checkout master puts all repos on the master branch gita super frontend-repo backend-repo commit -am 'implement a new feature' executes git commit -am 'implement a new feature' for frontend-repo and backend-repo Shell mode The shell mode delegates any shell command. Usage: gita shell [repo-name(s) or group-name(s)] <any-shell-command> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita shell ll lists contents for all repos gita shell repo1 repo2 mkdir docs create a new directory docs in repo1 and repo2 gita shell \"git describe --abbrev=0 --tags | xargs git checkout\": check out the latest tag for all repos Customization define repo group and context When the project contains several independent but related repos, we can define a group and execute gita command on this group. For example, gita group add repo1 repo2 -n my-group gita ll my-group gita pull my-group To save more typing, one can set a group as context, then any gita command is scoped to the group gita context my-group gita ll gita pull The most useful context maybe auto. In this mode, the context is automatically determined from the current working directory (CWD): the context is the group whose member repo's path contains CWD. To set it, run To remove the context, run It is also possible to recursively add repos within a directory and generate hierarchical groups automatically. For example, running on the following folder structure src project1 repo1 repo2 repo3 project2 repo4 repo5 repo6 gives rise to 3 groups: src:repo1,repo2,repo3,repo4,repo5,repo6 src-project1:repo1,repo2 src-project2:repo4,repo5 add user-defined sub-command using json file Custom delegating sub-commands can be defined in $XDG_CONFIG_HOME/gita/cmds.json (most likely ~/.config/gita/cmds.json) And they shadow the default ones if name collisions exist. Default delegating sub-commands are defined in cmds.json. For example, gita stat <repo-name(s)> is registered as \"stat\":{ \"cmd\": \"git diff --stat\", \"help\": \"show edit statistics\" } which executes git diff --stat for the specified repo(s). To disable asynchronous execution, set disable_async to be true. See the difftool example: \"difftool\":{ \"cmd\": \"git difftool\", \"disable_async\": true, \"help\": \"show differences using a tool\" } If you want a custom command to behave like gita fetch, i.e., to apply to all repos when no repo is specified, set allow_all to be true. For example, the following snippet creates a new command gita comaster [repo-name(s)] with optional repo name input. \"comaster\":{ \"cmd\": \"checkout master\", \"allow_all\": true, \"help\": \"checkout the master branch\" } Any command that runs in the superman mode mode or the shell mode can be defined in this json format. For example, the following command runs in shell mode and fetches only the current branch from upstream. \"fetchcrt\":{ \"cmd\": \"git rev-parse --abbrev-ref HEAD | xargs git fetch --prune upstream\", \"allow_all\": true, \"shell\": true, \"help\": \"fetch current branch only\" } customize the local/remote relationship coloring displayed by the gita ll command You can see the default color scheme and the available colors via gita color. To change the color coding, use gita color set <situation> <color>. The configuration is saved in $XDG_CONFIG_HOME/gita/color.csv. customize information displayed by the gita ll command You can customize the information displayed by gita ll. The used and unused information items are shown with gita info, and the configuration is saved in $XDG_CONFIG_HOME/gita/info.csv. For example, the default setting corresponds to branch,commit_msg,commit_time customize git command flags One can set custom flags to run git commands. For example, with gita flags set my-repo --git-dir=`gita ls dotfiles` --work-tree=$HOME any git command/alias triggered from gita on dotfiles will use these flags. Note that the flags are applied immediately after git. For example, gita st dotfiles translates to git --git-dir=$HOME/somefolder --work-tree=$HOME status running from the dotfiles directory. This feature was originally added to deal with bare repo dotfiles. Requirements Gita requires Python 3.6 or higher, due to the use of f-string and asyncio module. Under the hood, gita uses subprocess to run git commands/aliases. Thus the installed git version may matter. I have git 1.8.3.1, 2.17.2, and 2.20.1 on my machines, and their results agree. Tips effect shell command enter <repo> directory cd `gita ls <repo>` delete repos in <group> gita group ll <group> | xargs gita rm Contributing To contribute, you can report/fix bugs request/implement features star/recommend this project Read this article if you have never contribute code to open source project before. Chat room is available on To run tests locally, simply pytest in the source code folder. Note that context should be set as none. More implementation details are in design.md. A step-by-step guide to reproduce this project is here. You can also sponsor me on GitHub. Any amount is appreciated! Other multi-repo tools I haven't tried them but I heard good things about them. myrepos repo ",
          "One thing this project does really well is to start the readme with a screenshot. I open the link, scroll down to the readme, and I immediately see what sort of user interface/experience I will get. Some commenters have noted that Gita is similar to other multi-repo tools, but both Repo and wstool are more effort to evaluate, because their readmes don't have pictures.",
          "Nice, but is there a way to just run any command? I.e. just `gita <optional repo names/paths> <pass entire command line git -C repodir>`. This has advantages in that you don't have to go round via a command file, don't have to keep it synced across machines, don't have to remember what you put in the file etc, and can just use the git syntax which already took long enough to learn by heart :P<p>I've used multiple multiple repository tools and in the end all I happen to use is one (usually versioned) file to store a list of repositories and then a command which just loops over all repos and applies anything to it. If I need custom commands I use git aliases so that works both for normal git and whatever tool used."],
        "story_type":["ShowHN"],
        "url":"https://github.com/nosarthur/gita",
        "comments.comment_id":[19075272,
          19075476],
        "comments.comment_author":["fyhn",
          "stinos"],
        "comments.comment_descendants":[1,
          3],
        "comments.comment_time":["2019-02-04T10:43:55Z",
          "2019-02-04T11:34:37Z"],
        "comments.comment_text":["One thing this project does really well is to start the readme with a screenshot. I open the link, scroll down to the readme, and I immediately see what sort of user interface/experience I will get. Some commenters have noted that Gita is similar to other multi-repo tools, but both Repo and wstool are more effort to evaluate, because their readmes don't have pictures.",
          "Nice, but is there a way to just run any command? I.e. just `gita <optional repo names/paths> <pass entire command line git -C repodir>`. This has advantages in that you don't have to go round via a command file, don't have to keep it synced across machines, don't have to remember what you put in the file etc, and can just use the git syntax which already took long enough to learn by heart :P<p>I've used multiple multiple repository tools and in the end all I happen to use is one (usually versioned) file to store a list of repositories and then a command which just loops over all repos and applies anything to it. If I need custom commands I use git aliases so that works both for normal git and whatever tool used."],
        "id":"00defdde-cb4c-43ab-bb93-a20909c07a80",
        "url_text":"_______________________________ ( ____ \\__ __|__ __( ___ ) | ( \\/ ) ( ) ( | ( ) | | | | | | | | (___) | | | ____ | | | | | ___ | | | \\_ ) | | | | | ( ) | | (___) |__) (___ | | | ) ( | (_______)_______/ )_( |/ \\| v0.15 Gita: a command-line tool to manage multiple git repos This tool does two things display the status of multiple git repos such as branch, modification, commit message side by side (batch) delegate git commands/aliases from any working directory If several repos are related, it helps to see their status together. I also hate to change directories to execute git commands. In this screenshot, the gita ll command displays the status of all repos. The gita remote dotfiles command translates to git remote -v for the dotfiles repo, even though we are not in the repo. The gita fetch command fetches from all repos and two of them have updates. To see the pre-defined commands, run gita -h or take a look at cmds.json. To add your own commands, see the customization section. To run arbitrary git command, see the superman mode section. To run arbitrary shell command, see the shell mode section. The branch color distinguishes 5 situations between local and remote branches: color meaning white local has no remote green local is the same as remote red local has diverged from remote purple local is ahead of remote (good for push) yellow local is behind remote (good for merge) The choice of purple for ahead and yellow for behind is motivated by blueshift and redshift, using green as baseline. You can change the color scheme using the gita color command. See the customization section. The additional status symbols denote symbol meaning + staged changes * unstaged changes _ untracked files/folders The bookkeeping sub-commands are gita add <repo-path(s)> [-g <groupname>]: add repo(s) to gita, optionally into an existing group gita add -a <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively and automatically generate hierarchical groups. See the customization section for more details. gita add -b <bare-repo-path(s)>: add bare repo(s) to gita. See the customization section for more details on setting custom worktree. gita add -r <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively gita clone <config-file>: clone repos in config-file (generated by gita freeze) to current directory. gita clone -p <config-file>: clone repos in config-file to prescribed paths. gita context: context sub-command gita context: show current context gita context <group-name>: set context to group-name, all operations then only apply to repos in this group gita context auto: set context automatically according to the current working directory gita context none: remove context gita color: color sub-command gita color [ll]: Show available colors and the current coloring scheme gita color reset: Reset to the default coloring scheme gita color set <situation> <color>: Use the specified color for the local-remote situation gita flags: flags sub-command gita flags set <repo-name> <flags>: add custom flags to repo gita flags [ll]: display repos with custom flags gita freeze: print information of all repos such as URL, name, and path. Use with gita clone. gita group: group sub-command gita group add <repo-name(s)> -n <group-name>: add repo(s) to a new or existing group gita group [ll]: display existing groups with repos gita group ls: display existing group names gita group rename <group-name> <new-name>: change group name gita group rm <group-name(s)>: delete group(s) gita group rmrepo <repo-name(s)> -n <group-name>: remove repo(s) from existing group gita info: info sub-command gita info [ll]: display the used and unused information items gita info add <info-item>: enable information item gita info rm <info-item>: disable information item gita ll: display the status of all repos gita ll <group-name>: display the status of repos in a group gita ll -g: display the repo summaries by groups gita ls: display the names of all repos gita ls <repo-name>: display the absolute path of one repo gita rename <repo-name> <new-name>: rename a repo gita rm <repo-name(s)>: remove repo(s) from gita (won't remove files on disk) gita -v: display gita version The git delegating sub-commands are of two formats gita <sub-command> [repo-name(s) or group-name(s)]: optional repo or group input, and no input means all repos. gita <sub-command> <repo-name(s) or groups-name(s)>: required repo name(s) or group name(s) input They translate to git <sub-command> for the corresponding repos. By default, only fetch and pull take optional input. In other words, gita fetch and gita pull apply to all repos. To see the pre-defined sub-commands, run gita -h or take a look at cmds.json. To add your own sub-commands or override the default behaviors, see the customization section. To run arbitrary git command, see the superman mode section. If more than one repos are specified, the git command runs asynchronously, with the exception of log, difftool and mergetool, which require non-trivial user input. Repo configuration is saved in $XDG_CONFIG_HOME/gita/repos.csv (most likely ~/.config/gita/repos.csv). Installation To install the latest version, run If you prefer development mode, download the source code and run pip3 install -e <gita-source-folder> In either case, calling gita in terminal may not work, then put the following line in the .bashrc file. alias gita=\"python3 -m gita\" Windows users may need to enable the ANSI escape sequence in terminal for the branch color to work. See this stackoverflow post for details. Auto-completion Download .gita-completion.bash or .gita-completion.zsh and source it in shell. Superman mode The superman mode delegates any git command or alias. Usage: gita super [repo-name(s) or group-name(s)] <any-git-command-with-or-without-options> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita super checkout master puts all repos on the master branch gita super frontend-repo backend-repo commit -am 'implement a new feature' executes git commit -am 'implement a new feature' for frontend-repo and backend-repo Shell mode The shell mode delegates any shell command. Usage: gita shell [repo-name(s) or group-name(s)] <any-shell-command> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita shell ll lists contents for all repos gita shell repo1 repo2 mkdir docs create a new directory docs in repo1 and repo2 gita shell \"git describe --abbrev=0 --tags | xargs git checkout\": check out the latest tag for all repos Customization define repo group and context When the project contains several independent but related repos, we can define a group and execute gita command on this group. For example, gita group add repo1 repo2 -n my-group gita ll my-group gita pull my-group To save more typing, one can set a group as context, then any gita command is scoped to the group gita context my-group gita ll gita pull The most useful context maybe auto. In this mode, the context is automatically determined from the current working directory (CWD): the context is the group whose member repo's path contains CWD. To set it, run To remove the context, run It is also possible to recursively add repos within a directory and generate hierarchical groups automatically. For example, running on the following folder structure src project1 repo1 repo2 repo3 project2 repo4 repo5 repo6 gives rise to 3 groups: src:repo1,repo2,repo3,repo4,repo5,repo6 src-project1:repo1,repo2 src-project2:repo4,repo5 add user-defined sub-command using json file Custom delegating sub-commands can be defined in $XDG_CONFIG_HOME/gita/cmds.json (most likely ~/.config/gita/cmds.json) And they shadow the default ones if name collisions exist. Default delegating sub-commands are defined in cmds.json. For example, gita stat <repo-name(s)> is registered as \"stat\":{ \"cmd\": \"git diff --stat\", \"help\": \"show edit statistics\" } which executes git diff --stat for the specified repo(s). To disable asynchronous execution, set disable_async to be true. See the difftool example: \"difftool\":{ \"cmd\": \"git difftool\", \"disable_async\": true, \"help\": \"show differences using a tool\" } If you want a custom command to behave like gita fetch, i.e., to apply to all repos when no repo is specified, set allow_all to be true. For example, the following snippet creates a new command gita comaster [repo-name(s)] with optional repo name input. \"comaster\":{ \"cmd\": \"checkout master\", \"allow_all\": true, \"help\": \"checkout the master branch\" } Any command that runs in the superman mode mode or the shell mode can be defined in this json format. For example, the following command runs in shell mode and fetches only the current branch from upstream. \"fetchcrt\":{ \"cmd\": \"git rev-parse --abbrev-ref HEAD | xargs git fetch --prune upstream\", \"allow_all\": true, \"shell\": true, \"help\": \"fetch current branch only\" } customize the local/remote relationship coloring displayed by the gita ll command You can see the default color scheme and the available colors via gita color. To change the color coding, use gita color set <situation> <color>. The configuration is saved in $XDG_CONFIG_HOME/gita/color.csv. customize information displayed by the gita ll command You can customize the information displayed by gita ll. The used and unused information items are shown with gita info, and the configuration is saved in $XDG_CONFIG_HOME/gita/info.csv. For example, the default setting corresponds to branch,commit_msg,commit_time customize git command flags One can set custom flags to run git commands. For example, with gita flags set my-repo --git-dir=`gita ls dotfiles` --work-tree=$HOME any git command/alias triggered from gita on dotfiles will use these flags. Note that the flags are applied immediately after git. For example, gita st dotfiles translates to git --git-dir=$HOME/somefolder --work-tree=$HOME status running from the dotfiles directory. This feature was originally added to deal with bare repo dotfiles. Requirements Gita requires Python 3.6 or higher, due to the use of f-string and asyncio module. Under the hood, gita uses subprocess to run git commands/aliases. Thus the installed git version may matter. I have git 1.8.3.1, 2.17.2, and 2.20.1 on my machines, and their results agree. Tips effect shell command enter <repo> directory cd `gita ls <repo>` delete repos in <group> gita group ll <group> | xargs gita rm Contributing To contribute, you can report/fix bugs request/implement features star/recommend this project Read this article if you have never contribute code to open source project before. Chat room is available on To run tests locally, simply pytest in the source code folder. Note that context should be set as none. More implementation details are in design.md. A step-by-step guide to reproduce this project is here. You can also sponsor me on GitHub. Any amount is appreciated! Other multi-repo tools I haven't tried them but I heard good things about them. myrepos repo ",
        "_version_":1718939819292229632},
      {
        "story_id":[21671843],
        "story_author":["Sn0wlizz4rd"],
        "story_descendants":[83],
        "story_score":[172],
        "story_time":["2019-11-30T19:53:00Z"],
        "story_title":"Sherlock: Find usernames across social networks",
        "search":["Sherlock: Find usernames across social networks",
          "https://github.com/sherlock-project/sherlock",
          "Hunt down social media accounts by username across social networks Installation | Usage | Docker Notes | Contributing Installation # clone the repo $ git clone https://github.com/sherlock-project/sherlock.git # change the working directory to sherlock $ cd sherlock # install the requirements $ python3 -m pip install -r requirements.txt Usage $ python3 sherlock --help usage: sherlock [-h] [--version] [--verbose] [--folderoutput FOLDEROUTPUT] [--output OUTPUT] [--tor] [--unique-tor] [--csv] [--site SITE_NAME] [--proxy PROXY_URL] [--json JSON_FILE] [--timeout TIMEOUT] [--print-all] [--print-found] [--no-color] [--browse] [--local] USERNAMES [USERNAMES ...] Sherlock: Find Usernames Across Social Networks (Version 0.14.0) positional arguments: USERNAMES One or more usernames to check with social networks. optional arguments: -h, --help show this help message and exit --version Display version information and dependencies. --verbose, -v, -d, --debug Display extra debugging information and metrics. --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT If using multiple usernames, the output of the results will be saved to this folder. --output OUTPUT, -o OUTPUT If using single username, the output of the result will be saved to this file. --tor, -t Make requests over Tor; increases runtime; requires Tor to be installed and in system path. --unique-tor, -u Make requests over Tor with new Tor circuit after each request; increases runtime; requires Tor to be installed and in system path. --csv Create Comma-Separated Values (CSV) File. --site SITE_NAME Limit analysis to just the listed sites. Add multiple options to specify more than one site. --proxy PROXY_URL, -p PROXY_URL Make requests over a proxy. e.g. socks5://127.0.0.1:1080 --json JSON_FILE, -j JSON_FILE Load data from a JSON file or an online, valid, JSON file. --timeout TIMEOUT Time (in seconds) to wait for response to requests. Default timeout is infinity. A longer timeout will be more likely to get results from slow sites. On the other hand, this may cause a long delay to gather all results. --print-all Output sites where the username was not found. --print-found Output sites where the username was found. --no-color Don't color terminal output --browse, -b Browse to all results on default browser. --local, -l Force the use of the local data.json file. To search for only one user: To search for more than one user: python3 sherlock user1 user2 user3 Accounts found will be stored in an individual text file with the corresponding username (e.g user123.txt). Anaconda (Windows) Notes If you are using Anaconda in Windows, using 'python3' might not work. Use 'python' instead. Docker Notes If docker is installed you can build an image and run this as a container. docker build -t mysherlock-image . Once the image is built, sherlock can be invoked by running the following: docker run --rm -t mysherlock-image user123 The optional --rm flag removes the container filesystem on completion to prevent cruft build-up. See: https://docs.docker.com/engine/reference/run/#clean-up---rm The optional -t flag allocates a pseudo-TTY which allows colored output. See: https://docs.docker.com/engine/reference/run/#foreground Use the following command to access the saved results: docker run --rm -t -v \"$PWD/results:/opt/sherlock/results\" mysherlock-image -o /opt/sherlock/results/text.txt user123 The -v \"$PWD/results:/opt/sherlock/results\" options tell docker to create (or use) the folder results in the present working directory and to mount it at /opt/sherlock/results on the docker container. The -o /opt/sherlock/results/text.txt option tells sherlock to output the result. Or you can use \"Docker Hub\" to run sherlock: docker run theyahya/sherlock user123 Using docker-compose You can use the docker-compose.yml file from the repository and use this command: docker-compose run sherlock -o /opt/sherlock/results/text.txt user123 Contributing We would love to have you help us with the development of Sherlock. Each and every contribution is greatly valued! Here are some things we would appreciate your help on: Addition of new site support Bringing back site support of sites that have been removed in the past due to false positives [1] Please look at the Wiki entry on adding new sites to understand the issues. Tests Thank you for contributing to Sherlock! Before creating a pull request with new development, please run the tests to ensure that everything is working great. It would also be a good idea to run the tests before starting development to distinguish problems between your environment and the Sherlock software. The following is an example of the command line to run all the tests for Sherlock. This invocation hides the progress text that Sherlock normally outputs, and instead shows the verbose output of the tests. $ cd sherlock/sherlock $ python3 -m unittest tests.all --verbose Note that we do currently have 100% test coverage. Unfortunately, some of the sites that Sherlock checks are not always reliable, so it is common to get response problems. Any problems in connection will show up as warnings in the tests instead of true errors. If some sites are failing due to connection problems (site is down, in maintenance, etc) you can exclude them from tests by creating a tests/.excluded_sites file with a list of sites to ignore (one site name per line). Stargazers over time License MIT Sherlock Project Original Creator - Siddharth Dushantha ",
          "<i>CODE_OF_CONDUCT.md</i><p><i>Examples of unacceptable behavior by participants include:</i><p><i>Publishing others' private information, such as a physical or electronic address, without explicit permission</i><p>How is this tool not a violation of its own CoC?",
          "I don't get this? It checks against a limited list of websites if a username is taken. So what? This is hardly doxxing or \"smart\". Simply a faster method than the manual way, except it doesn't exhaust all avenues of search."],
        "story_type":["Normal"],
        "url":"https://github.com/sherlock-project/sherlock",
        "comments.comment_id":[21672262,
          21672990],
        "comments.comment_author":["lsb",
          "comfymatrix"],
        "comments.comment_descendants":[6,
          1],
        "comments.comment_time":["2019-11-30T21:34:33Z",
          "2019-12-01T00:30:56Z"],
        "comments.comment_text":["<i>CODE_OF_CONDUCT.md</i><p><i>Examples of unacceptable behavior by participants include:</i><p><i>Publishing others' private information, such as a physical or electronic address, without explicit permission</i><p>How is this tool not a violation of its own CoC?",
          "I don't get this? It checks against a limited list of websites if a username is taken. So what? This is hardly doxxing or \"smart\". Simply a faster method than the manual way, except it doesn't exhaust all avenues of search."],
        "id":"bcde0ad3-0529-41cd-9808-e3261e5444d8",
        "url_text":"Hunt down social media accounts by username across social networks Installation | Usage | Docker Notes | Contributing Installation # clone the repo $ git clone https://github.com/sherlock-project/sherlock.git # change the working directory to sherlock $ cd sherlock # install the requirements $ python3 -m pip install -r requirements.txt Usage $ python3 sherlock --help usage: sherlock [-h] [--version] [--verbose] [--folderoutput FOLDEROUTPUT] [--output OUTPUT] [--tor] [--unique-tor] [--csv] [--site SITE_NAME] [--proxy PROXY_URL] [--json JSON_FILE] [--timeout TIMEOUT] [--print-all] [--print-found] [--no-color] [--browse] [--local] USERNAMES [USERNAMES ...] Sherlock: Find Usernames Across Social Networks (Version 0.14.0) positional arguments: USERNAMES One or more usernames to check with social networks. optional arguments: -h, --help show this help message and exit --version Display version information and dependencies. --verbose, -v, -d, --debug Display extra debugging information and metrics. --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT If using multiple usernames, the output of the results will be saved to this folder. --output OUTPUT, -o OUTPUT If using single username, the output of the result will be saved to this file. --tor, -t Make requests over Tor; increases runtime; requires Tor to be installed and in system path. --unique-tor, -u Make requests over Tor with new Tor circuit after each request; increases runtime; requires Tor to be installed and in system path. --csv Create Comma-Separated Values (CSV) File. --site SITE_NAME Limit analysis to just the listed sites. Add multiple options to specify more than one site. --proxy PROXY_URL, -p PROXY_URL Make requests over a proxy. e.g. socks5://127.0.0.1:1080 --json JSON_FILE, -j JSON_FILE Load data from a JSON file or an online, valid, JSON file. --timeout TIMEOUT Time (in seconds) to wait for response to requests. Default timeout is infinity. A longer timeout will be more likely to get results from slow sites. On the other hand, this may cause a long delay to gather all results. --print-all Output sites where the username was not found. --print-found Output sites where the username was found. --no-color Don't color terminal output --browse, -b Browse to all results on default browser. --local, -l Force the use of the local data.json file. To search for only one user: To search for more than one user: python3 sherlock user1 user2 user3 Accounts found will be stored in an individual text file with the corresponding username (e.g user123.txt). Anaconda (Windows) Notes If you are using Anaconda in Windows, using 'python3' might not work. Use 'python' instead. Docker Notes If docker is installed you can build an image and run this as a container. docker build -t mysherlock-image . Once the image is built, sherlock can be invoked by running the following: docker run --rm -t mysherlock-image user123 The optional --rm flag removes the container filesystem on completion to prevent cruft build-up. See: https://docs.docker.com/engine/reference/run/#clean-up---rm The optional -t flag allocates a pseudo-TTY which allows colored output. See: https://docs.docker.com/engine/reference/run/#foreground Use the following command to access the saved results: docker run --rm -t -v \"$PWD/results:/opt/sherlock/results\" mysherlock-image -o /opt/sherlock/results/text.txt user123 The -v \"$PWD/results:/opt/sherlock/results\" options tell docker to create (or use) the folder results in the present working directory and to mount it at /opt/sherlock/results on the docker container. The -o /opt/sherlock/results/text.txt option tells sherlock to output the result. Or you can use \"Docker Hub\" to run sherlock: docker run theyahya/sherlock user123 Using docker-compose You can use the docker-compose.yml file from the repository and use this command: docker-compose run sherlock -o /opt/sherlock/results/text.txt user123 Contributing We would love to have you help us with the development of Sherlock. Each and every contribution is greatly valued! Here are some things we would appreciate your help on: Addition of new site support Bringing back site support of sites that have been removed in the past due to false positives [1] Please look at the Wiki entry on adding new sites to understand the issues. Tests Thank you for contributing to Sherlock! Before creating a pull request with new development, please run the tests to ensure that everything is working great. It would also be a good idea to run the tests before starting development to distinguish problems between your environment and the Sherlock software. The following is an example of the command line to run all the tests for Sherlock. This invocation hides the progress text that Sherlock normally outputs, and instead shows the verbose output of the tests. $ cd sherlock/sherlock $ python3 -m unittest tests.all --verbose Note that we do currently have 100% test coverage. Unfortunately, some of the sites that Sherlock checks are not always reliable, so it is common to get response problems. Any problems in connection will show up as warnings in the tests instead of true errors. If some sites are failing due to connection problems (site is down, in maintenance, etc) you can exclude them from tests by creating a tests/.excluded_sites file with a list of sites to ignore (one site name per line). Stargazers over time License MIT Sherlock Project Original Creator - Siddharth Dushantha ",
        "_version_":1718939883261657088},
      {
        "story_id":[19108787],
        "story_author":["jaxxstorm"],
        "story_descendants":[346],
        "story_score":[263],
        "story_time":["2019-02-07T21:31:59Z"],
        "story_title":"Why are we templating YAML?",
        "search":["Why are we templating YAML?",
          "https://leebriggs.co.uk/blog/2019/02/07/why-are-we-templating-yaml.html",
          "Published Feb 7, 2019 by Lee Briggs #kubernetes #configuration mgmt #jsonnet #helm #kr8 I was at cfgmgmtcamp 2019 in Ghent, and did a talk which I think was well received about the need for some Kubernetes configuration management as well as the solution we built for it at $work, kr8. I made a statement during the talk which ignited some fairly fierce discussion both online, and at the conference: \"If you're starting to template yaml, ask yourself the question: why am I not *generating* json?\" - @briggsl spitting straight fire at #cfgmgmtcamp eric sorenson (@ahpook) February 5, 2019 To put this into my own words: At some point, we decided it was okay for us to template yaml. When did this happen? How is this acceptable? After some conversation, I figured it was probably best to back up my claims in some way. This blog post is going to try to do that. The configuration problem Once the applications and infrastructure youre going to manage grows past a certain size, you inevitably end up in some form of configuration complexity hell. If youre only deploying 1 or maybe 2 things, you can write a yaml configuration file and be done with it. However once you grow beyond that, you need to figure out how to manage this complexity. Its incredibly likely that the reason you have multiple configuration files is because the $thing that uses that config is slightly different from its companions. Examples of this include: Applications deployed in different environments, like dev, stg and prod Applications deployed in different regions, like Europe or North American Obviously, not all the configuration is different here, but its likely the configuration differs enough that you want to be able to differentiate between the two. This configuration complexity has been well known for Operators (System Administrators, DevOps engineers, whatever you want to call them) for some years now. An entire discpline grew up around this in Configuration Management, and each tool solved this problem in their own way, but ultimately, they used YAML to get the job done. My favourite method has always been hiera which comes bundled with Puppet. Having the ability to hierarchically look up the variables of specific config needs is incredibly powerful and flexible, and has generally meant you dont actually need to do any templating of yaml at all, except perhaps for embedding Puppet facts into the yaml. Did we go backwards? Then, as our industries needs moved above the operating system and into cloud computing, we had a whole new data plane to configure. The tooling to configure this changed, and tools like CloudFormation and Helm appeared. These tools are excellent configuration tools, but I firmly believe we (as an industry) got something really, really wrong when we designed them. To examine that, lets take a look at example of a helm chart taking a custom parameter Helm Charts Helm charts can take external parameters defined by an values.yaml file which you specify when rendering the chart. A simple example might look like this: Lets say my external parameter is simple - its a string. Itd look a bit like this: image: \"{{ .Values.image }}\" Thats not so bad right? You just specify a value for image in your values.yaml and youre on your way. The real problem starts to get highlighted when you want to do more complicated and complex things. In this particular example, youre doing okay because you know you have to specify an image for a Kubernetes deployment. However, what if youre working with something like an optional field? Well, then it gets a little more unwieldy: {{- with .resourceGroup }} resourceGroup: {{ . }} {{- end }} Optional values just make things ugly in templating languages, and you cant just leave the value blank, so you have to resort to ugly loops and conditionals that are probably going to bite you later. Lets say you need to go a step further, and you need to push an array or map into the config. With helm, youd do something like this. {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 8 }} {{- end }} Firstly, lets ignore the madness of having a templating function toYaml to convert yaml to yaml and focus more on the whitespace issue here. YAML has strict requirements and whitespace implementation rules. The following, for example, is not valid or complete yaml: something: nothing hello: goodbye Generally, if youre handwriting something, this isnt necessarily a problem because you just hit backspace twice and its fixed. However, if youre generating YAML using a templating system, you cant do that - and if youre operating above 5 or 10 configuration files, you probably want to be generating your config rather than writing it. So, in the above example, you want to embed the values of .Values.podAnnotations under the annotations field, which is indented already. So youre having to not only indent your values, but indent them correctly. What makes this even more confusing is that the go parser doesnt actually know anything about YAML at all, so if you try to keep the syntax clean and indent the templates like this: {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 6 }} {{- end }} You actually cant do that, because the templating system gets confused. This is a singular example of the complexity and difficulty you end up facing when generating config data in YAML, but when you really start to do more complex work, it really starts to become obvious that this isnt the way to go. Needless to say, this isnt what I want to spend my time doing. If fiddling around with whitespace requirements in a templating system doing something its not really designed for is what suits you, then Im not going to stop you. I also dont want to spend my time writing configuration in JSON without comments and accidentally missing commas all over the shop. We (as an industry) decided a long time ago that shit wasnt going to work and thats why YAML exists. So what should we do instead? Thats where jsonnet comes in. JSON, Jsonnet & YAML Before we actually talk about Jsonnet, its worth reminding people of a very important (but oft forgotten point). YAML is a superset of JSON and converting between the two is trivial. Many applications and programming languages will parse JSON and YAML natively, and many can convert between the two very simple. For example, in Python: python -c 'import json, sys, yaml ; y=yaml.safe_load(sys.stdin.read()) ; print(json.dumps(y))' So with that in mind, lets talk about Jsonnet. Welcome to the church of Jsonnet Jsonnet is a relatively new, little known (outside the Kubernetes community?) language that calls itself a data templating language. Its definitely a good exercise to read and consume the Jsonnet design rationale page to get an idea why it exists, but if I was going to define in a nutshell what its purpose is - its to generate JSON config. So, how does it help, exactly? Well, lets take our earlier example - we want to generate some JSON config specifying a parameter (ie, the image string). We can do that very very easily with Jsonnet using external variables. Firstly, lets define some Jsonnet: { image: std.extVar('image'), } Then, we can generate it using the Jsonnet command line tool, passing in the external variable as we need to: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Easy! Optional fields Before, I noted that if you wanted to define an optional field, with YAML templating you had to define if statements for everything. With Jsonnet, youre just defining code! // define a variable - yes, jsonnet also has comments local rg = null; { image: std.extVar('image'), // if the variable is null, this will be blank [if rg != null then 'resourceGroup']: rg, } The output here, because our variable is null, means that we never actually populate resourceGroup. If you specify a value, it will appear: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Maps and parameters Okay, now lets look at our previous annotation example. We want to define some pod annotations, which takes a YAML map as its input. You want this map to be configurable by specifying external data, and obviously doing that on the command line sucks (youd be very unlikely to specify this with Helm on the command line, for example) so generally youd use Jsonnet imports to this. Im going to specify this config as a variable and then load that variable into the annotation: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { // annotations are nested under the metadata of a pod annotations: annotations, }, } This might just be my bias towards Jsonnet talking, but this is so dramatically easier than faffing about with indentation that I cant even begin to describe it. Additional goodies The final thing I wanted to quickly explore, which is something that I feel cant really be done with Helm and other yaml templating tools, is the concept of manipulating existing objects in config. Lets take our example above with the annotations, and look at the result file: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true } } } Now, lets say for example I wanted to append a set of annotations to this annotations map. In any templating system, Id probably have to rewrite the whole map. Jsonnet makes this trivial. I can simply use the + operator to add something to this. Heres a (poor) example: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { annotations: annotations, }, } + { // this adds another JSON object metadata+: { // I'm using the + operator, so we'll append to the existing metadata annotations+: { // same as above something: 'nothing', }, }, } The end result is this: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true, \"something\": \"nothing\" } } } Obviously, in this case, its more code to this, but as your example get more complex, it becomes extremely useful to be able to manipulate objects this way. Kr8 We use all of these methods in kr8 to make creating and manipulating configuration for multiple Kubernetes clusters easy and simple. I highly recommend you check it out if any of the concepts youve found here have found you nodding your head. ",
          "I know I'm in a minority, but I really dislike YAML... I recently did a lot of Ansible and boy, at the beginning, I was just struggling a lot. Syntactic whitespace kills me.<p>I don't like it in Python either, but for some reason, when I write Python, it's a lot easier. Maybe YAML is just a bit more complex (and Python has better IDE support..?)",
          "My belief is that we've been slowly building up to using general purpose languages, one small step at a time, throughout the infrastructure as code, DevOps, and SRE journeys this past 10 years. INI files, XML, JSON, and YAML aren't sufficiently expressive -- lacking for loops, conditionals, variable references, and any sort of abstraction -- so, of course, we add templates to it. But as the author (IMHO rightfully) points out, we just end up with a funky, poor approximation of a language.<p>I think this approach is a byproduct of thinking about infrastructure and configuration -- and the cloud generally -- as an \"afterthought,\" not a core part of an application's infrastructure. Containers, Kubernetes, serverless, and more hosted services all change this, and Chef, Puppet, and others laid the groundwork to think differently about what the future looks like. More developers today than ever before need to think about how to build and configure cloud software.<p>We started the Pulumi project to solve this very problem, so I'm admittedly biased, and I hope you forgive the plug -- I only mention it here because I think it contributes to the discussion. Our approach is to simply use general purpose languages like TypeScript, Python, and Go, while still having infrastructure as code. An important thing to realize is that infrastructure as code is based on the idea of a <i>goal state</i>. Using a full blown language to generate that goal state generally doesn't threaten the repeatability, determinism, or robustness of the solution, provided you've got an engine handling state management, diffing, resource CRUD, and so on. We've been able to apply this universally across AWS, Azure, GCP, <i>and</i> Kubernetes, often mixing their configuration in the same program.<p>Again, I'm biased and want to admit that, however if you're sick of YAML, it's definitely worth checking out. We'd love your feedback:<p>- Project website: <a href=\"https://pulumi.io/\" rel=\"nofollow\">https://pulumi.io/</a><p>- All open source on GitHub: <a href=\"https://github.com/pulumi/pulumi\" rel=\"nofollow\">https://github.com/pulumi/pulumi</a><p>- Example of abstractions: <a href=\"https://blog.pulumi.com/the-fastest-path-to-deploying-kubernetes-on-aws-with-eks-and-pulumi\" rel=\"nofollow\">https://blog.pulumi.com/the-fastest-path-to-deploying-kubern...</a><p>- Example of serverless as event handlers: <a href=\"https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simple-serverless-functions\" rel=\"nofollow\">https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simp...</a><p>Pulumi may not be <i>the</i> solution for everyone, but I'm fairly optimistic that this is where we're all heading.<p>Joe"],
        "story_type":["Normal"],
        "url":"https://leebriggs.co.uk/blog/2019/02/07/why-are-we-templating-yaml.html",
        "comments.comment_id":[19109135,
          19109540],
        "comments.comment_author":["BossingAround",
          "joeduffy"],
        "comments.comment_descendants":[12,
          19],
        "comments.comment_time":["2019-02-07T22:14:16Z",
          "2019-02-07T22:58:45Z"],
        "comments.comment_text":["I know I'm in a minority, but I really dislike YAML... I recently did a lot of Ansible and boy, at the beginning, I was just struggling a lot. Syntactic whitespace kills me.<p>I don't like it in Python either, but for some reason, when I write Python, it's a lot easier. Maybe YAML is just a bit more complex (and Python has better IDE support..?)",
          "My belief is that we've been slowly building up to using general purpose languages, one small step at a time, throughout the infrastructure as code, DevOps, and SRE journeys this past 10 years. INI files, XML, JSON, and YAML aren't sufficiently expressive -- lacking for loops, conditionals, variable references, and any sort of abstraction -- so, of course, we add templates to it. But as the author (IMHO rightfully) points out, we just end up with a funky, poor approximation of a language.<p>I think this approach is a byproduct of thinking about infrastructure and configuration -- and the cloud generally -- as an \"afterthought,\" not a core part of an application's infrastructure. Containers, Kubernetes, serverless, and more hosted services all change this, and Chef, Puppet, and others laid the groundwork to think differently about what the future looks like. More developers today than ever before need to think about how to build and configure cloud software.<p>We started the Pulumi project to solve this very problem, so I'm admittedly biased, and I hope you forgive the plug -- I only mention it here because I think it contributes to the discussion. Our approach is to simply use general purpose languages like TypeScript, Python, and Go, while still having infrastructure as code. An important thing to realize is that infrastructure as code is based on the idea of a <i>goal state</i>. Using a full blown language to generate that goal state generally doesn't threaten the repeatability, determinism, or robustness of the solution, provided you've got an engine handling state management, diffing, resource CRUD, and so on. We've been able to apply this universally across AWS, Azure, GCP, <i>and</i> Kubernetes, often mixing their configuration in the same program.<p>Again, I'm biased and want to admit that, however if you're sick of YAML, it's definitely worth checking out. We'd love your feedback:<p>- Project website: <a href=\"https://pulumi.io/\" rel=\"nofollow\">https://pulumi.io/</a><p>- All open source on GitHub: <a href=\"https://github.com/pulumi/pulumi\" rel=\"nofollow\">https://github.com/pulumi/pulumi</a><p>- Example of abstractions: <a href=\"https://blog.pulumi.com/the-fastest-path-to-deploying-kubernetes-on-aws-with-eks-and-pulumi\" rel=\"nofollow\">https://blog.pulumi.com/the-fastest-path-to-deploying-kubern...</a><p>- Example of serverless as event handlers: <a href=\"https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simple-serverless-functions\" rel=\"nofollow\">https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simp...</a><p>Pulumi may not be <i>the</i> solution for everyone, but I'm fairly optimistic that this is where we're all heading.<p>Joe"],
        "id":"8f04b79e-2fc6-4202-bdbe-2a2665ebd156",
        "url_text":"Published Feb 7, 2019 by Lee Briggs #kubernetes #configuration mgmt #jsonnet #helm #kr8 I was at cfgmgmtcamp 2019 in Ghent, and did a talk which I think was well received about the need for some Kubernetes configuration management as well as the solution we built for it at $work, kr8. I made a statement during the talk which ignited some fairly fierce discussion both online, and at the conference: \"If you're starting to template yaml, ask yourself the question: why am I not *generating* json?\" - @briggsl spitting straight fire at #cfgmgmtcamp eric sorenson (@ahpook) February 5, 2019 To put this into my own words: At some point, we decided it was okay for us to template yaml. When did this happen? How is this acceptable? After some conversation, I figured it was probably best to back up my claims in some way. This blog post is going to try to do that. The configuration problem Once the applications and infrastructure youre going to manage grows past a certain size, you inevitably end up in some form of configuration complexity hell. If youre only deploying 1 or maybe 2 things, you can write a yaml configuration file and be done with it. However once you grow beyond that, you need to figure out how to manage this complexity. Its incredibly likely that the reason you have multiple configuration files is because the $thing that uses that config is slightly different from its companions. Examples of this include: Applications deployed in different environments, like dev, stg and prod Applications deployed in different regions, like Europe or North American Obviously, not all the configuration is different here, but its likely the configuration differs enough that you want to be able to differentiate between the two. This configuration complexity has been well known for Operators (System Administrators, DevOps engineers, whatever you want to call them) for some years now. An entire discpline grew up around this in Configuration Management, and each tool solved this problem in their own way, but ultimately, they used YAML to get the job done. My favourite method has always been hiera which comes bundled with Puppet. Having the ability to hierarchically look up the variables of specific config needs is incredibly powerful and flexible, and has generally meant you dont actually need to do any templating of yaml at all, except perhaps for embedding Puppet facts into the yaml. Did we go backwards? Then, as our industries needs moved above the operating system and into cloud computing, we had a whole new data plane to configure. The tooling to configure this changed, and tools like CloudFormation and Helm appeared. These tools are excellent configuration tools, but I firmly believe we (as an industry) got something really, really wrong when we designed them. To examine that, lets take a look at example of a helm chart taking a custom parameter Helm Charts Helm charts can take external parameters defined by an values.yaml file which you specify when rendering the chart. A simple example might look like this: Lets say my external parameter is simple - its a string. Itd look a bit like this: image: \"{{ .Values.image }}\" Thats not so bad right? You just specify a value for image in your values.yaml and youre on your way. The real problem starts to get highlighted when you want to do more complicated and complex things. In this particular example, youre doing okay because you know you have to specify an image for a Kubernetes deployment. However, what if youre working with something like an optional field? Well, then it gets a little more unwieldy: {{- with .resourceGroup }} resourceGroup: {{ . }} {{- end }} Optional values just make things ugly in templating languages, and you cant just leave the value blank, so you have to resort to ugly loops and conditionals that are probably going to bite you later. Lets say you need to go a step further, and you need to push an array or map into the config. With helm, youd do something like this. {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 8 }} {{- end }} Firstly, lets ignore the madness of having a templating function toYaml to convert yaml to yaml and focus more on the whitespace issue here. YAML has strict requirements and whitespace implementation rules. The following, for example, is not valid or complete yaml: something: nothing hello: goodbye Generally, if youre handwriting something, this isnt necessarily a problem because you just hit backspace twice and its fixed. However, if youre generating YAML using a templating system, you cant do that - and if youre operating above 5 or 10 configuration files, you probably want to be generating your config rather than writing it. So, in the above example, you want to embed the values of .Values.podAnnotations under the annotations field, which is indented already. So youre having to not only indent your values, but indent them correctly. What makes this even more confusing is that the go parser doesnt actually know anything about YAML at all, so if you try to keep the syntax clean and indent the templates like this: {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 6 }} {{- end }} You actually cant do that, because the templating system gets confused. This is a singular example of the complexity and difficulty you end up facing when generating config data in YAML, but when you really start to do more complex work, it really starts to become obvious that this isnt the way to go. Needless to say, this isnt what I want to spend my time doing. If fiddling around with whitespace requirements in a templating system doing something its not really designed for is what suits you, then Im not going to stop you. I also dont want to spend my time writing configuration in JSON without comments and accidentally missing commas all over the shop. We (as an industry) decided a long time ago that shit wasnt going to work and thats why YAML exists. So what should we do instead? Thats where jsonnet comes in. JSON, Jsonnet & YAML Before we actually talk about Jsonnet, its worth reminding people of a very important (but oft forgotten point). YAML is a superset of JSON and converting between the two is trivial. Many applications and programming languages will parse JSON and YAML natively, and many can convert between the two very simple. For example, in Python: python -c 'import json, sys, yaml ; y=yaml.safe_load(sys.stdin.read()) ; print(json.dumps(y))' So with that in mind, lets talk about Jsonnet. Welcome to the church of Jsonnet Jsonnet is a relatively new, little known (outside the Kubernetes community?) language that calls itself a data templating language. Its definitely a good exercise to read and consume the Jsonnet design rationale page to get an idea why it exists, but if I was going to define in a nutshell what its purpose is - its to generate JSON config. So, how does it help, exactly? Well, lets take our earlier example - we want to generate some JSON config specifying a parameter (ie, the image string). We can do that very very easily with Jsonnet using external variables. Firstly, lets define some Jsonnet: { image: std.extVar('image'), } Then, we can generate it using the Jsonnet command line tool, passing in the external variable as we need to: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Easy! Optional fields Before, I noted that if you wanted to define an optional field, with YAML templating you had to define if statements for everything. With Jsonnet, youre just defining code! // define a variable - yes, jsonnet also has comments local rg = null; { image: std.extVar('image'), // if the variable is null, this will be blank [if rg != null then 'resourceGroup']: rg, } The output here, because our variable is null, means that we never actually populate resourceGroup. If you specify a value, it will appear: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Maps and parameters Okay, now lets look at our previous annotation example. We want to define some pod annotations, which takes a YAML map as its input. You want this map to be configurable by specifying external data, and obviously doing that on the command line sucks (youd be very unlikely to specify this with Helm on the command line, for example) so generally youd use Jsonnet imports to this. Im going to specify this config as a variable and then load that variable into the annotation: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { // annotations are nested under the metadata of a pod annotations: annotations, }, } This might just be my bias towards Jsonnet talking, but this is so dramatically easier than faffing about with indentation that I cant even begin to describe it. Additional goodies The final thing I wanted to quickly explore, which is something that I feel cant really be done with Helm and other yaml templating tools, is the concept of manipulating existing objects in config. Lets take our example above with the annotations, and look at the result file: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true } } } Now, lets say for example I wanted to append a set of annotations to this annotations map. In any templating system, Id probably have to rewrite the whole map. Jsonnet makes this trivial. I can simply use the + operator to add something to this. Heres a (poor) example: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { annotations: annotations, }, } + { // this adds another JSON object metadata+: { // I'm using the + operator, so we'll append to the existing metadata annotations+: { // same as above something: 'nothing', }, }, } The end result is this: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true, \"something\": \"nothing\" } } } Obviously, in this case, its more code to this, but as your example get more complex, it becomes extremely useful to be able to manipulate objects this way. Kr8 We use all of these methods in kr8 to make creating and manipulating configuration for multiple Kubernetes clusters easy and simple. I highly recommend you check it out if any of the concepts youve found here have found you nodding your head. ",
        "_version_":1718939819855314944},
      {
        "story_id":[21509373],
        "story_author":["nexuist"],
        "story_descendants":[26],
        "story_score":[147],
        "story_time":["2019-11-11T22:02:51Z"],
        "story_title":"Usql – A Universal CLI for Databases",
        "search":["Usql – A Universal CLI for Databases",
          "https://github.com/xo/usql",
          "Installing | Building | Using | Database Support | Features and Compatibility | Releases | Contributing usql is a universal command-line interface for PostgreSQL, MySQL, Oracle Database, SQLite3, Microsoft SQL Server, and many other databases including NoSQL and non-relational databases! usql provides a simple way to work with SQL and NoSQL databases via a command-line inspired by PostgreSQL's psql. usql supports most of the core psql features, such as variables, backticks, and commands and has additional features that psql does not, such as syntax highlighting, context-based completion, and multiple database support. Database administrators and developers that would prefer to work with a tool like psql with non-PostgreSQL databases, will find usql intuitive, easy-to-use, and a great replacement for the command-line clients/tools for other databases. Installing usql can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the usql or usql.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) macOS Notes The recommended installation method on macOS is via brew (see below). If the following or similar error is encountered when attempting to run usql: $ usql dyld: Library not loaded: /usr/local/opt/icu4c/lib/libicuuc.68.dylib Referenced from: /Users/user/.local/bin/usql Reason: image not found Abort trap: 6 Then the ICU lib needs to be installed. This can be accomplished using brew: Installing via Homebrew (macOS and Linux) usql is available in the xo/xo tap, and can be installed in the usual way with the brew command: # install usql with \"most\" drivers $ brew install xo/xo/usql Additional support for ODBC databases can be installed by passing --with-odbc option during install: # install usql with odbc support $ brew install --with-odbc usql Installing via Scoop (Windows) usql can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install usql Installing via Go usql can be installed in the usual Go fashion: # install usql from master branch with basic database support # includes PostgreSQL, Oracle Database, MySQL, MS SQL, and SQLite3 drivers $ go install github.com/xo/usql@master Building When building usql with Go, only drivers for PostgreSQL, MySQL, SQLite3 and Microsoft SQL Server will be enabled by default. Other databases can be enabled by specifying the build tag for their database driver. Additionally, the most and all build tags include most, and all SQL drivers, respectively: # install all drivers $ go install -tags all github.com/xo/usql@master # install with most drivers (excludes unsupported drivers) $ go install -tags most github.com/xo/usql@master # install with base drivers and additional support for Oracle Database and ODBC $ go install -tags 'godror odbc' github.com/xo/usql@master For every build tag <driver>, there is also a no_<driver> build tag disabling the driver: # install all drivers excluding avatica and couchbase $ go install -tags 'all no_avatica no_couchbase' github.com/xo/usql@master Release Builds Release builds are built with the most build tag. Additional SQLite3 build tags are also specified for releases. Embedding An effort has been made to keep usql's packages modular, and reusable by other developers wishing to leverage the usql code base. As such, it is possible to embed or create a SQL command-line interface (e.g, for use by some other project as an \"official\" client) using the core usql source tree. Please refer to main.go to see how usql puts together its packages. usql's code is also well-documented -- please refer to the Go reference for an overview of the various packages and APIs. Database Support usql works with all Go standard library compatible SQL drivers supported by github.com/xo/dburl. The list of drivers that usql was built with can be displayed using the \\drivers command: $ cd $GOPATH/src/github.com/xo/usql $ export GO111MODULE=on # build excluding the base drivers, and including cassandra and moderncsqlite $ go build -tags 'no_postgres no_oracle no_sqlserver no_sqlite3 cassandra moderncsqlite' # show built driver support $ ./usql -c '\\drivers' Available Drivers: cql [ca, scy, scylla, datastax, cassandra] memsql (mysql) [me] moderncsqlite [mq, sq, file, sqlite, sqlite3, modernsqlite] mysql [my, maria, aurora, mariadb, percona] tidb (mysql) [ti] vitess (mysql) [vt] The above shows that usql was built with only the mysql, cassandra (ie, cql), and moderncsqlite drivers. The output above reflects information about the drivers available to usql, specifically the internal driver name, its primary URL scheme, the driver's available scheme aliases (shown in [...]), and the real/underlying driver (shown in (...)) for wire compatible drivers. Supported Database Schemes and Aliases The following are the Go SQL drivers that usql supports, the associated database, scheme / build tag, and scheme aliases: Database Scheme / Tag Scheme Aliases Driver Package / Notes Microsoft SQL Server sqlserver ms, mssql github.com/denisenkom/go-mssqldb MySQL mysql my, maria, aurora, mariadb, percona github.com/go-sql-driver/mysql Oracle Database oracle or, ora, oci, oci8, odpi, odpi-c github.com/sijms/go-ora/v2 PostgreSQL postgres pg, pgsql, postgresql github.com/lib/pq SQLite3 sqlite3 sq, file, sqlite github.com/mattn/go-sqlite3 Alibaba MaxCompute maxcompute mc sqlflow.org/gomaxcompute Apache Avatica avatica av, phoenix github.com/apache/calcite-avatica-go/v5 Apache H2 h2 github.com/jmrobles/h2go Apache Ignite ignite ig, gridgain github.com/amsokol/ignite-go-client/sql AWS Athena athena s3, aws github.com/uber/athenadriver/go Cassandra cassandra ca, scy, scylla, datastax, cql github.com/MichaelS11/go-cql-driver ClickHouse clickhouse ch github.com/ClickHouse/clickhouse-go Couchbase couchbase n1, n1ql github.com/couchbase/go_n1ql CSVQ csvq cs, csv, tsv, json github.com/mithrandie/csvq-driver Cznic QL ql cznic, cznicql modernc.org/ql Exasol exasol ex, exa github.com/exasol/exasol-driver-go Firebird firebird fb, firebirdsql github.com/nakagami/firebirdsql Genji genji gj github.com/genjidb/genji/driver Google BigQuery bigquery bq gorm.io/driver/bigquery/driver Google Spanner spanner sp github.com/cloudspannerecosystem/go-sql-spanner Microsoft ADODB adodb ad, ado github.com/mattn/go-adodb ModernC SQLite3 moderncsqlite mq, modernsqlite modernc.org/sqlite MySQL MyMySQL mymysql zm, mymy github.com/ziutek/mymysql/godrv Netezza netezza nz, nzgo github.com/IBM/nzgo PostgreSQL PGX pgx px github.com/jackc/pgx/v4/stdlib Presto presto pr, prs, prestos, prestodb, prestodbs github.com/prestodb/presto-go-client/presto SAP ASE sapase ax, ase, tds github.com/thda/tds SAP HANA saphana sa, sap, hana, hdb github.com/SAP/go-hdb/driver Trino trino tr, trs, trinos github.com/trinodb/trino-go-client/trino Vertica vertica ve github.com/vertica/vertica-sql-go VoltDB voltdb vo, vdb, volt github.com/VoltDB/voltdb-client-go/voltdbclient Apache Hive hive hi sqlflow.org/gohive Apache Impala impala im github.com/bippio/go-impala Azure CosmosDB cosmos cm github.com/btnguyen2k/gocosmos GO DRiver for ORacle godror gr github.com/godror/godror ODBC odbc od github.com/alexbrainman/odbc Snowflake snowflake sf github.com/snowflakedb/gosnowflake Amazon Redshift postgres rs, redshift github.com/lib/pq CockroachDB postgres cr, cdb, crdb, cockroach, cockroachdb github.com/lib/pq OLE ODBC adodb oo, ole, oleodbc github.com/mattn/go-adodb SingleStore MemSQL mysql me, memsql github.com/go-sql-driver/mysql TiDB mysql ti, tidb github.com/go-sql-driver/mysql Vitess Database mysql vt, vitess github.com/go-sql-driver/mysql NO DRIVERS no_base no base drivers (useful for development) MOST DRIVERS most all stable drivers ALL DRIVERS all all drivers NO <TAG> no_<tag> exclude driver with <tag> Requires CGO Wire compatible (see respective driver) Any of the protocol schemes/aliases shown above can be used in conjunction when connecting to a database via the command-line or with the \\connect command: # connect to a vitess database: $ usql vt://user:pass@host:3306/mydatabase $ usql (not connected)=> \\c vitess://user:pass@host:3306/mydatabase See the section below on connecting to databases for further details building DSNs/URLs for use with usql. Using After installing, usql can be used similarly to the following: # connect to a postgres database $ usql postgres://booktest@localhost/booktest # connect to an oracle database $ usql oracle://user:pass@host/oracle.sid # connect to a postgres database and run the commands contained in script.sql $ usql pg://localhost/ -f script.sql Command-line Options Supported command-line options: $ usql --help usql, the universal command-line interface for SQL databases Usage: usql [OPTIONS]... [DSN] Arguments: DSN database url Options: -c, --command=COMMAND ... run only single command (SQL or internal) and exit -f, --file=FILE ... execute commands from file and exit -w, --no-password never prompt for password -X, --no-rc do not read start up file -o, --out=OUT output file -W, --password force password prompt (should happen automatically) -1, --single-transaction execute as a single transaction (if non-interactive) -v, --set=, --variable=NAME=VALUE ... set variable NAME to VALUE -P, --pset=VAR[=ARG] ... set printing option VAR to ARG (see \\pset command) -F, --field-separator=FIELD-SEPARATOR ... field separator for unaligned output (default, \"|\") -R, --record-separator=RECORD-SEPARATOR ... record separator for unaligned output (default, \\n) -T, --table-attr=TABLE-ATTR ... set HTML table tag attributes (e.g., width, border) -A, --no-align unaligned table output mode -H, --html HTML table output mode -t, --tuples-only print rows only -x, --expanded turn on expanded table output -z, --field-separator-zero set field separator for unaligned output to zero byte -0, --record-separator-zero set record separator for unaligned output to zero byte -J, --json JSON output mode -C, --csv CSV output mode -G, --vertical vertical output mode -V, --version display version and exit Connecting to Databases usql opens a database connection by parsing a URL and passing the resulting connection string to a database driver. Database connection strings (aka \"data source name\" or DSNs) have the same parsing rules as URLs, and can be passed to usql via command-line, or to the \\connect or \\c commands. Connection strings look like the following: driver+transport://user:pass@host/dbname?opt1=a&opt2=b driver:/path/to/file /path/to/file Where the above are: Component Description driver driver scheme name or scheme alias transport tcp, udp, unix or driver name (for ODBC and ADODB) user username pass password host hostname dbname database name, instance, or service name/ID ?opt1=a&... additional database driver options (see respective SQL driver for available options) /path/to/file a path on disk Some databases, such as Microsoft SQL Server, or Oracle Database support a path component (ie, /dbname) in the form of /instance/dbname, where /instance is the optional service identifier (aka \"SID\") or database instance Driver Aliases usql supports the same driver names and aliases from the dburl package. Most databases have at least one or more alias - please refer to the dburl documentation for all supported aliases. Short Aliases All database drivers have a two character short form that is usually the first two letters of the database driver. For example, pg for postgres, my for mysql, ms for sqlserver (formerly known as mssql), or for oracle, or sq for sqlite3. Passing Driver Options Driver options are specified as standard URL query options in the form of ?opt1=a&obt2=b. Please refer to the relevant database driver's documentation for available options. Paths on Disk If a URL does not have a driver: scheme, usql will check if it is a path on disk. If the path exists, usql will attempt to use an appropriate database driver to open the path. If the specified path is a Unix Domain Socket, usql will attempt to open it using the MySQL driver. If the path is a directory, usql will attempt to open it using the PostgreSQL driver. If the path is a regular file, usql will attempt to open the file using the SQLite3 driver. Driver Defaults As with URLs, most components in the URL are optional and many components can be left out. usql will attempt connecting using defaults where possible: # connect to postgres using the local $USER and the unix domain socket in /var/run/postgresql $ usql pg:// Please see documentation for the database driver you are connecting with for more information. Connection Examples The following are example connection strings and additional ways to connect to databases using usql: # connect to a postgres database $ usql pg://user:pass@host/dbname $ usql pgsql://user:pass@host/dbname $ usql postgres://user:pass@host:port/dbname $ usql pg:// $ usql /var/run/postgresql $ usql pg://user:pass@host/dbname?sslmode=disable # Connect without SSL # connect to a mysql database $ usql my://user:pass@host/dbname $ usql mysql://user:pass@host:port/dbname $ usql my:// $ usql /var/run/mysqld/mysqld.sock # connect to a sqlserver database $ usql sqlserver://user:pass@host/instancename/dbname $ usql ms://user:pass@host/dbname $ usql ms://user:pass@host/instancename/dbname $ usql mssql://user:pass@host:port/dbname $ usql ms:// # connect to a sqlserver database using Windows domain authentication $ runas /user:ACME\\wiley /netonly \"usql mssql://host/dbname/\" # connect to a oracle database $ usql or://user:pass@host/sid $ usql oracle://user:pass@host:port/sid $ usql or:// # connect to a cassandra database $ usql ca://user:pass@host/keyspace $ usql cassandra://host/keyspace $ usql cql://host/ $ usql ca:// # connect to a sqlite database that exists on disk $ usql dbname.sqlite3 # NOTE: when connecting to a SQLite database, if the \"<driver>://\" or # \"<driver>:\" scheme/alias is omitted, the file must already exist on disk. # # if the file does not yet exist, the URL must incorporate file:, sq:, sqlite3:, # or any other recognized sqlite3 driver alias to force usql to create a new, # empty database at the specified path: $ usql sq://path/to/dbname.sqlite3 $ usql sqlite3://path/to/dbname.sqlite3 $ usql file:/path/to/dbname.sqlite3 # connect to a adodb ole resource (windows only) $ usql adodb://Microsoft.Jet.OLEDB.4.0/myfile.mdb $ usql \"adodb://Microsoft.ACE.OLEDB.12.0/?Extended+Properties=\\\"Text;HDR=NO;FMT=Delimited\\\"\" # connect with ODBC driver (requires building with odbc tag) $ cat /etc/odbcinst.ini [DB2] Description=DB2 driver Driver=/opt/db2/clidriver/lib/libdb2.so FileUsage = 1 DontDLClose = 1 [PostgreSQL ANSI] Description=PostgreSQL ODBC driver (ANSI version) Driver=psqlodbca.so Setup=libodbcpsqlS.so Debug=0 CommLog=1 UsageCount=1 # connect to db2, postgres databases using ODBC $ usql odbc+DB2://user:pass@localhost/dbname $ usql odbc+PostgreSQL+ANSI://user:pass@localhost/dbname?TraceFile=/path/to/trace.log Executing Queries and Commands The interactive intrepreter reads queries and meta (\\ ) commands, sending the query to the connected database: $ usql sqlite://example.sqlite3 Connected with driver sqlite3 (SQLite3 3.17.0) Type \"help\" for help. sq:example.sqlite3=> create table test (test_id int, name string); CREATE TABLE sq:example.sqlite3=> insert into test (test_id, name) values (1, 'hello'); INSERT 1 sq:example.sqlite3=> select * from test; test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> select * from test sq:example.sqlite3-> \\p select * from test sq:example.sqlite3-> \\g test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> \\c postgres://booktest@localhost error: pq: 28P01: password authentication failed for user \"booktest\" Enter password: Connected with driver postgres (PostgreSQL 9.6.6) pg:booktest@localhost=> select * from authors; author_id | name +-----------+----------------+ 1 | Unknown Master 2 | blah 3 | aoeu (3 rows) pg:booktest@localhost=> Commands may accept one or more parameter, and can be quoted using either ' or \". Command parameters may also be backtick'd. Backslash Commands Currently available commands: $ usql Type \"help\" for help. (not connected)=> \\? General \\q quit usql \\copyright show usql usage and distribution terms \\drivers display information about available database drivers Query Execute \\g [(OPTIONS)] [FILE] or ; execute query (and send results to file or |pipe) \\crosstabview [(OPTIONS)] [COLUMNS] execute query and display results in crosstab \\G [(OPTIONS)] [FILE] as \\g, but forces vertical output mode \\gexec execute query and execute each value of the result \\gset [PREFIX] execute query and store results in usql variables \\gx [(OPTIONS)] [FILE] as \\g, but forces expanded output mode \\watch [(OPTIONS)] [DURATION] execute query every specified interval Query Buffer \\e [FILE] [LINE] edit the query buffer (or file) with external editor \\p show the contents of the query buffer \\raw show the raw (non-interpolated) contents of the query buffer \\r reset (clear) the query buffer \\w FILE write query buffer to file Help \\? [commands] show help on backslash commands \\? options show help on usql command-line options \\? variables show help on special variables Input/Output \\echo [-n] [STRING] write string to standard output (-n for no newline) \\qecho [-n] [STRING] write string to \\o output stream (-n for no newline) \\warn [-n] [STRING] write string to standard error (-n for no newline) \\o [FILE] send all query results to file or |pipe \\i FILE execute commands from file \\ir FILE as \\i, but relative to location of current script Informational \\d[S+] [NAME] list tables, views, and sequences or describe table, view, sequence, or index \\da[S+] [PATTERN] list aggregates \\df[S+] [PATTERN] list functions \\di[S+] [PATTERN] list indexes \\dm[S+] [PATTERN] list materialized views \\dn[S+] [PATTERN] list schemas \\ds[S+] [PATTERN] list sequences \\dt[S+] [PATTERN] list tables \\dv[S+] [PATTERN] list views \\l[+] list databases \\ss[+] [TABLE|QUERY] [k] show stats for a table or a query Formatting \\pset [NAME [VALUE]] set table output option \\a toggle between unaligned and aligned output mode \\C [STRING] set table title, or unset if none \\f [STRING] show or set field separator for unaligned query output \\H toggle HTML output mode \\T [STRING] set HTML <table> tag attributes, or unset if none \\t [on|off] show only rows \\x [on|off|auto] toggle expanded output Transaction \\begin begin a transaction \\commit commit current transaction \\rollback rollback (abort) current transaction Connection \\c URL connect to database with url \\c DRIVER PARAMS... connect to database with SQL driver and parameters \\Z close database connection \\password [USERNAME] change the password for a user \\conninfo display information about the current database connection Operating System \\cd [DIR] change the current working directory \\setenv NAME [VALUE] set or unset environment variable \\! [COMMAND] execute command in shell or start interactive shell \\timing [on|off] toggle timing of commands Variables \\prompt [-TYPE] <VAR> [PROMPT] prompt user to set variable \\set [NAME [VALUE]] set internal variable, or list all if no parameters \\unset NAME unset (delete) internal variable Features and Compatibility The usql project's goal is to support all standard psql commands and features. Pull Requests are always appreciated! Variables and Interpolation usql supports client-side interpolation of variables that can be \\set and \\unset: $ usql (not connected)=> \\set (not connected)=> \\set FOO bar (not connected)=> \\set FOO = 'bar' (not connected)=> \\unset FOO (not connected)=> \\set (not connected)=> A \\set variable, NAME, will be directly interpolated (by string substitution) into the query when prefixed with : and optionally surrounded by quotation marks (' or \"): pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from authors where name = :'FOO'; author_id | name +-----------+------+ 7 | bar (1 rows) The three forms, :NAME, :'NAME', and :\"NAME\", are used to interpolate a variable in parts of a query that may require quoting, such as for a column name, or when doing concatenation in a query: pg:booktest@localhost=> \\set TBLNAME authors pg:booktest@localhost=> \\set COLNAME name pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\p select * from authors where \"name\" = 'bar' pg:booktest@localhost-> \\raw select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\g author_id | name +-----------+------+ 7 | bar (1 rows) pg:booktest@localhost=> Note: variables contained within other strings will NOT be interpolated: pg:booktest@localhost=> select ':FOO'; ?column? +----------+ :FOO (1 rows) pg:booktest@localhost=> \\p select ':FOO'; pg:booktest@localhost=> Backtick'd parameters Meta (\\ ) commands support backticks on parameters: (not connected)=> \\echo Welcome `echo $USER` -- 'currently:' \"(\" `date` \")\" Welcome ken -- currently: ( Wed Jun 13 12:10:27 WIB 2018 ) (not connected)=> Backtick'd parameters will be passed to the user's SHELL, exactly as written, and can be combined with \\set: pg:booktest@localhost=> \\set MYVAR `date` pg:booktest@localhost=> \\set MYVAR = 'Wed Jun 13 12:17:11 WIB 2018' pg:booktest@localhost=> \\echo :MYVAR Wed Jun 13 12:17:11 WIB 2018 pg:booktest@localhost=> Passwords usql supports reading passwords for databases from a .usqlpass file contained in the user's HOME directory at startup: $ cat $HOME/.usqlpass # format is: # protocol:host:port:dbname:user:pass postgres:*:*:*:booktest:booktest $ usql pg:// Connected with driver postgres (PostgreSQL 9.6.9) Type \"help\" for help. pg:booktest@=> Note: the .usqlpass file cannot be readable by other users. Please set the permissions accordingly: Runtime Configuration (RC) File usql supports executing a .usqlrc contained in the user's HOME directory: $ cat $HOME/.usqlrc \\echo WELCOME TO THE JUNGLE `date` \\set SYNTAX_HL_STYLE paraiso-dark $ usql WELCOME TO THE JUNGLE Thu Jun 14 02:36:53 WIB 2018 Type \"help\" for help. (not connected)=> \\set SYNTAX_HL_STYLE = 'paraiso-dark' (not connected)=> The .usqlrc file is read by usql at startup in the same way as a file passed on the command-line with -f / --file. It is commonly used to set startup environment variables and settings. You can temporarily disable the RC-file by passing -X or --no-rc on the command-line: Host Connection Information By default, usql displays connection information when connecting to a database. This might cause problems with some databases or connections. This can be disabled by setting the system environment variable USQL_SHOW_HOST_INFORMATION to false: $ export USQL_SHOW_HOST_INFORMATION=false $ usql pg://booktest@localhost Type \"help\" for help. pg:booktest@=> SHOW_HOST_INFORMATION is a standard usql variable, and can be \\set or \\unset. Additionally, it can be passed via the command-line using -v or --set: $ usql --set SHOW_HOST_INFORMATION=false pg:// Type \"help\" for help. pg:booktest@=> \\set SHOW_HOST_INFORMATION true pg:booktest@=> \\connect pg:// Connected with driver postgres (PostgreSQL 9.6.9) pg:booktest@=> Syntax Highlighting Interactive queries will be syntax highlighted by default, using Chroma. There are a number of variables that control syntax highlighting: Variable Default Values Description SYNTAX_HL true true or false enables syntax highlighting SYNTAX_HL_FORMAT dependent on terminal support formatter name Chroma formatter name SYNTAX_HL_OVERRIDE_BG true true or false enables overriding the background color of the chroma styles SYNTAX_HL_STYLE monokai style name Chroma style name Time Formatting Some databases support time/date columns that support formatting. By default, usql formats time/date columns as RFC3339Nano, and can be set using \\pset time <FORMAT>: $ usql pg:// Connected with driver postgres (PostgreSQL 13.2 (Debian 13.2-1.pgdg100+1)) Type \"help\" for help. pg:postgres@=> \\pset time RFC3339Nano pg:postgres@=> select now(); now ----------------------------- 2021-05-01T22:21:44.710385Z (1 row) pg:postgres@=> \\pset time Kitchen Time display is \"Kitchen\" (\"3:04PM\"). pg:postgres@=> select now(); now --------- 10:22PM (1 row) pg:postgres@=> Any Go supported time format or the standard Go const name (for example, Kitchen, in the above). Constants Constant Name Value ANSIC Mon Jan _2 15:04:05 2006 UnixDate Mon Jan _2 15:04:05 MST 2006 RubyDate Mon Jan 02 15:04:05 -0700 2006 RFC822 02 Jan 06 15:04 MST RFC822Z 02 Jan 06 15:04 -0700 RFC850 Monday, 02-Jan-06 15:04:05 MST RFC1123 Mon, 02 Jan 2006 15:04:05 MST RFC1123Z Mon, 02 Jan 2006 15:04:05 -0700 RFC3339 2006-01-02T15:04:05Z07:00 RFC3339Nano 2006-01-02T15:04:05.999999999Z07:00 Kitchen 3:04PM Stamp Jan _2 15:04:05 StampMilli Jan _2 15:04:05.000 StampMicro Jan _2 15:04:05.000000 StampNano Jan _2 15:04:05.000000000 Copy usql implements the \\copy command that reads data from a database connection and writes it into another one. It requires 4 parameters: source connection string destination connection string source query destination table name, optionally with columns Connection strings support same syntax as in \\connect. Source query needs to be quoted. Source query must select same number of columns and in same order as they're defined in the destination table, unless they're specified for the destination, as table_name(column1, column2, ...). Quote the whole expression, if it contains spaces. \\copy does not attempt to perform any data type conversion. Use CAST in the source query to ensure data types compatible with destination table. Some drivers may have limited data type support, and they might not work at all when combined with other limited drivers. Unlike psql, \\copy in usql cannot read data directly from files. Drivers like csvq can help with this, since they support reading CSV and JSON files. $ cat books.csv book_id,author_id,isbn,title,year,available,tags 3,1,3,one,2018,\"2018-06-01 00:00:00\",{} 4,2,4,two,2019,\"2019-06-01 00:00:00\",{} $ usql -c \"\\copy csvq://. sqlite3://test.db 'select * from books' 'books'\" Copied 2 rows Note that it might be a better idea to use tools dedicated to the destination database to load data in a robust way. \\copy reads data from plain SELECT queries. Most drivers that have \\copy enabled use INSERT statements, except for PostgreSQL ones, which use COPY TO. Because data needs to be downloaded from one database and uploaded into another, don't expect same performance as in psql. For loading large amount of data efficiently, use tools native to the destination database. You can use \\copy with variables. Better yet, put those \\set commands in your runtime configuration file at $HOME/.usqlrc and passwords at $HOME/.usqlpass. $ usql Type \"help\" for help. (not connected)=> \\set pglocal postgres://postgres@localhost:49153?sslmode=disable (not connected)=> \\set oralocal godror://system@localhost:1521/orasid (not connected)=> \\copy :pglocal :oralocal 'select staff_id, first_name from staff' 'staff(staff_id, first_name)' Contributing usql is currently a WIP, and is aiming towards a 1.0 release soon. Well-written PRs are always welcome -- and there is a clear backlog of issues marked help wanted on the GitHub issue tracker! Please pick up an issue today, and submit a PR tomorrow! For more technical details, see CONTRIBUTING.md. Related Projects dburl - Go package providing a standard, URL-style mechanism for parsing and opening database connection URLs xo - Go command-line tool to generate Go code from a database schema ",
          "I have been looking for a faster alternative to pgcli [1] ( with  features like auto-complete for table names and columns) and was getting excited.<p>Sadly this is \"just\" a plain old CLI.<p>Also: had to install with `go get -tags \"no_sqlite3\" -u github.com/xo/usql` since the sqlite3 package did not build.<p>[1] <a href=\"https://www.pgcli.com/\" rel=\"nofollow\">https://www.pgcli.com/</a>",
          "I was really hoping for \\dt but looks like they haven't implemented it yet. The number one most common thing I look up when I'm using a SQL variant is how to show the tables. I suppose there would be complications with NoSQL but you could just show available collections or whatever else it maps to in that case."],
        "story_type":["Normal"],
        "url":"https://github.com/xo/usql",
        "comments.comment_id":[21509885,
          21509956],
        "comments.comment_author":["the_duke",
          "lwb"],
        "comments.comment_descendants":[1,
          1],
        "comments.comment_time":["2019-11-11T23:05:33Z",
          "2019-11-11T23:14:50Z"],
        "comments.comment_text":["I have been looking for a faster alternative to pgcli [1] ( with  features like auto-complete for table names and columns) and was getting excited.<p>Sadly this is \"just\" a plain old CLI.<p>Also: had to install with `go get -tags \"no_sqlite3\" -u github.com/xo/usql` since the sqlite3 package did not build.<p>[1] <a href=\"https://www.pgcli.com/\" rel=\"nofollow\">https://www.pgcli.com/</a>",
          "I was really hoping for \\dt but looks like they haven't implemented it yet. The number one most common thing I look up when I'm using a SQL variant is how to show the tables. I suppose there would be complications with NoSQL but you could just show available collections or whatever else it maps to in that case."],
        "id":"b096ebc9-bde9-47c3-a5e0-f480e0b2770a",
        "url_text":"Installing | Building | Using | Database Support | Features and Compatibility | Releases | Contributing usql is a universal command-line interface for PostgreSQL, MySQL, Oracle Database, SQLite3, Microsoft SQL Server, and many other databases including NoSQL and non-relational databases! usql provides a simple way to work with SQL and NoSQL databases via a command-line inspired by PostgreSQL's psql. usql supports most of the core psql features, such as variables, backticks, and commands and has additional features that psql does not, such as syntax highlighting, context-based completion, and multiple database support. Database administrators and developers that would prefer to work with a tool like psql with non-PostgreSQL databases, will find usql intuitive, easy-to-use, and a great replacement for the command-line clients/tools for other databases. Installing usql can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the usql or usql.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) macOS Notes The recommended installation method on macOS is via brew (see below). If the following or similar error is encountered when attempting to run usql: $ usql dyld: Library not loaded: /usr/local/opt/icu4c/lib/libicuuc.68.dylib Referenced from: /Users/user/.local/bin/usql Reason: image not found Abort trap: 6 Then the ICU lib needs to be installed. This can be accomplished using brew: Installing via Homebrew (macOS and Linux) usql is available in the xo/xo tap, and can be installed in the usual way with the brew command: # install usql with \"most\" drivers $ brew install xo/xo/usql Additional support for ODBC databases can be installed by passing --with-odbc option during install: # install usql with odbc support $ brew install --with-odbc usql Installing via Scoop (Windows) usql can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install usql Installing via Go usql can be installed in the usual Go fashion: # install usql from master branch with basic database support # includes PostgreSQL, Oracle Database, MySQL, MS SQL, and SQLite3 drivers $ go install github.com/xo/usql@master Building When building usql with Go, only drivers for PostgreSQL, MySQL, SQLite3 and Microsoft SQL Server will be enabled by default. Other databases can be enabled by specifying the build tag for their database driver. Additionally, the most and all build tags include most, and all SQL drivers, respectively: # install all drivers $ go install -tags all github.com/xo/usql@master # install with most drivers (excludes unsupported drivers) $ go install -tags most github.com/xo/usql@master # install with base drivers and additional support for Oracle Database and ODBC $ go install -tags 'godror odbc' github.com/xo/usql@master For every build tag <driver>, there is also a no_<driver> build tag disabling the driver: # install all drivers excluding avatica and couchbase $ go install -tags 'all no_avatica no_couchbase' github.com/xo/usql@master Release Builds Release builds are built with the most build tag. Additional SQLite3 build tags are also specified for releases. Embedding An effort has been made to keep usql's packages modular, and reusable by other developers wishing to leverage the usql code base. As such, it is possible to embed or create a SQL command-line interface (e.g, for use by some other project as an \"official\" client) using the core usql source tree. Please refer to main.go to see how usql puts together its packages. usql's code is also well-documented -- please refer to the Go reference for an overview of the various packages and APIs. Database Support usql works with all Go standard library compatible SQL drivers supported by github.com/xo/dburl. The list of drivers that usql was built with can be displayed using the \\drivers command: $ cd $GOPATH/src/github.com/xo/usql $ export GO111MODULE=on # build excluding the base drivers, and including cassandra and moderncsqlite $ go build -tags 'no_postgres no_oracle no_sqlserver no_sqlite3 cassandra moderncsqlite' # show built driver support $ ./usql -c '\\drivers' Available Drivers: cql [ca, scy, scylla, datastax, cassandra] memsql (mysql) [me] moderncsqlite [mq, sq, file, sqlite, sqlite3, modernsqlite] mysql [my, maria, aurora, mariadb, percona] tidb (mysql) [ti] vitess (mysql) [vt] The above shows that usql was built with only the mysql, cassandra (ie, cql), and moderncsqlite drivers. The output above reflects information about the drivers available to usql, specifically the internal driver name, its primary URL scheme, the driver's available scheme aliases (shown in [...]), and the real/underlying driver (shown in (...)) for wire compatible drivers. Supported Database Schemes and Aliases The following are the Go SQL drivers that usql supports, the associated database, scheme / build tag, and scheme aliases: Database Scheme / Tag Scheme Aliases Driver Package / Notes Microsoft SQL Server sqlserver ms, mssql github.com/denisenkom/go-mssqldb MySQL mysql my, maria, aurora, mariadb, percona github.com/go-sql-driver/mysql Oracle Database oracle or, ora, oci, oci8, odpi, odpi-c github.com/sijms/go-ora/v2 PostgreSQL postgres pg, pgsql, postgresql github.com/lib/pq SQLite3 sqlite3 sq, file, sqlite github.com/mattn/go-sqlite3 Alibaba MaxCompute maxcompute mc sqlflow.org/gomaxcompute Apache Avatica avatica av, phoenix github.com/apache/calcite-avatica-go/v5 Apache H2 h2 github.com/jmrobles/h2go Apache Ignite ignite ig, gridgain github.com/amsokol/ignite-go-client/sql AWS Athena athena s3, aws github.com/uber/athenadriver/go Cassandra cassandra ca, scy, scylla, datastax, cql github.com/MichaelS11/go-cql-driver ClickHouse clickhouse ch github.com/ClickHouse/clickhouse-go Couchbase couchbase n1, n1ql github.com/couchbase/go_n1ql CSVQ csvq cs, csv, tsv, json github.com/mithrandie/csvq-driver Cznic QL ql cznic, cznicql modernc.org/ql Exasol exasol ex, exa github.com/exasol/exasol-driver-go Firebird firebird fb, firebirdsql github.com/nakagami/firebirdsql Genji genji gj github.com/genjidb/genji/driver Google BigQuery bigquery bq gorm.io/driver/bigquery/driver Google Spanner spanner sp github.com/cloudspannerecosystem/go-sql-spanner Microsoft ADODB adodb ad, ado github.com/mattn/go-adodb ModernC SQLite3 moderncsqlite mq, modernsqlite modernc.org/sqlite MySQL MyMySQL mymysql zm, mymy github.com/ziutek/mymysql/godrv Netezza netezza nz, nzgo github.com/IBM/nzgo PostgreSQL PGX pgx px github.com/jackc/pgx/v4/stdlib Presto presto pr, prs, prestos, prestodb, prestodbs github.com/prestodb/presto-go-client/presto SAP ASE sapase ax, ase, tds github.com/thda/tds SAP HANA saphana sa, sap, hana, hdb github.com/SAP/go-hdb/driver Trino trino tr, trs, trinos github.com/trinodb/trino-go-client/trino Vertica vertica ve github.com/vertica/vertica-sql-go VoltDB voltdb vo, vdb, volt github.com/VoltDB/voltdb-client-go/voltdbclient Apache Hive hive hi sqlflow.org/gohive Apache Impala impala im github.com/bippio/go-impala Azure CosmosDB cosmos cm github.com/btnguyen2k/gocosmos GO DRiver for ORacle godror gr github.com/godror/godror ODBC odbc od github.com/alexbrainman/odbc Snowflake snowflake sf github.com/snowflakedb/gosnowflake Amazon Redshift postgres rs, redshift github.com/lib/pq CockroachDB postgres cr, cdb, crdb, cockroach, cockroachdb github.com/lib/pq OLE ODBC adodb oo, ole, oleodbc github.com/mattn/go-adodb SingleStore MemSQL mysql me, memsql github.com/go-sql-driver/mysql TiDB mysql ti, tidb github.com/go-sql-driver/mysql Vitess Database mysql vt, vitess github.com/go-sql-driver/mysql NO DRIVERS no_base no base drivers (useful for development) MOST DRIVERS most all stable drivers ALL DRIVERS all all drivers NO <TAG> no_<tag> exclude driver with <tag> Requires CGO Wire compatible (see respective driver) Any of the protocol schemes/aliases shown above can be used in conjunction when connecting to a database via the command-line or with the \\connect command: # connect to a vitess database: $ usql vt://user:pass@host:3306/mydatabase $ usql (not connected)=> \\c vitess://user:pass@host:3306/mydatabase See the section below on connecting to databases for further details building DSNs/URLs for use with usql. Using After installing, usql can be used similarly to the following: # connect to a postgres database $ usql postgres://booktest@localhost/booktest # connect to an oracle database $ usql oracle://user:pass@host/oracle.sid # connect to a postgres database and run the commands contained in script.sql $ usql pg://localhost/ -f script.sql Command-line Options Supported command-line options: $ usql --help usql, the universal command-line interface for SQL databases Usage: usql [OPTIONS]... [DSN] Arguments: DSN database url Options: -c, --command=COMMAND ... run only single command (SQL or internal) and exit -f, --file=FILE ... execute commands from file and exit -w, --no-password never prompt for password -X, --no-rc do not read start up file -o, --out=OUT output file -W, --password force password prompt (should happen automatically) -1, --single-transaction execute as a single transaction (if non-interactive) -v, --set=, --variable=NAME=VALUE ... set variable NAME to VALUE -P, --pset=VAR[=ARG] ... set printing option VAR to ARG (see \\pset command) -F, --field-separator=FIELD-SEPARATOR ... field separator for unaligned output (default, \"|\") -R, --record-separator=RECORD-SEPARATOR ... record separator for unaligned output (default, \\n) -T, --table-attr=TABLE-ATTR ... set HTML table tag attributes (e.g., width, border) -A, --no-align unaligned table output mode -H, --html HTML table output mode -t, --tuples-only print rows only -x, --expanded turn on expanded table output -z, --field-separator-zero set field separator for unaligned output to zero byte -0, --record-separator-zero set record separator for unaligned output to zero byte -J, --json JSON output mode -C, --csv CSV output mode -G, --vertical vertical output mode -V, --version display version and exit Connecting to Databases usql opens a database connection by parsing a URL and passing the resulting connection string to a database driver. Database connection strings (aka \"data source name\" or DSNs) have the same parsing rules as URLs, and can be passed to usql via command-line, or to the \\connect or \\c commands. Connection strings look like the following: driver+transport://user:pass@host/dbname?opt1=a&opt2=b driver:/path/to/file /path/to/file Where the above are: Component Description driver driver scheme name or scheme alias transport tcp, udp, unix or driver name (for ODBC and ADODB) user username pass password host hostname dbname database name, instance, or service name/ID ?opt1=a&... additional database driver options (see respective SQL driver for available options) /path/to/file a path on disk Some databases, such as Microsoft SQL Server, or Oracle Database support a path component (ie, /dbname) in the form of /instance/dbname, where /instance is the optional service identifier (aka \"SID\") or database instance Driver Aliases usql supports the same driver names and aliases from the dburl package. Most databases have at least one or more alias - please refer to the dburl documentation for all supported aliases. Short Aliases All database drivers have a two character short form that is usually the first two letters of the database driver. For example, pg for postgres, my for mysql, ms for sqlserver (formerly known as mssql), or for oracle, or sq for sqlite3. Passing Driver Options Driver options are specified as standard URL query options in the form of ?opt1=a&obt2=b. Please refer to the relevant database driver's documentation for available options. Paths on Disk If a URL does not have a driver: scheme, usql will check if it is a path on disk. If the path exists, usql will attempt to use an appropriate database driver to open the path. If the specified path is a Unix Domain Socket, usql will attempt to open it using the MySQL driver. If the path is a directory, usql will attempt to open it using the PostgreSQL driver. If the path is a regular file, usql will attempt to open the file using the SQLite3 driver. Driver Defaults As with URLs, most components in the URL are optional and many components can be left out. usql will attempt connecting using defaults where possible: # connect to postgres using the local $USER and the unix domain socket in /var/run/postgresql $ usql pg:// Please see documentation for the database driver you are connecting with for more information. Connection Examples The following are example connection strings and additional ways to connect to databases using usql: # connect to a postgres database $ usql pg://user:pass@host/dbname $ usql pgsql://user:pass@host/dbname $ usql postgres://user:pass@host:port/dbname $ usql pg:// $ usql /var/run/postgresql $ usql pg://user:pass@host/dbname?sslmode=disable # Connect without SSL # connect to a mysql database $ usql my://user:pass@host/dbname $ usql mysql://user:pass@host:port/dbname $ usql my:// $ usql /var/run/mysqld/mysqld.sock # connect to a sqlserver database $ usql sqlserver://user:pass@host/instancename/dbname $ usql ms://user:pass@host/dbname $ usql ms://user:pass@host/instancename/dbname $ usql mssql://user:pass@host:port/dbname $ usql ms:// # connect to a sqlserver database using Windows domain authentication $ runas /user:ACME\\wiley /netonly \"usql mssql://host/dbname/\" # connect to a oracle database $ usql or://user:pass@host/sid $ usql oracle://user:pass@host:port/sid $ usql or:// # connect to a cassandra database $ usql ca://user:pass@host/keyspace $ usql cassandra://host/keyspace $ usql cql://host/ $ usql ca:// # connect to a sqlite database that exists on disk $ usql dbname.sqlite3 # NOTE: when connecting to a SQLite database, if the \"<driver>://\" or # \"<driver>:\" scheme/alias is omitted, the file must already exist on disk. # # if the file does not yet exist, the URL must incorporate file:, sq:, sqlite3:, # or any other recognized sqlite3 driver alias to force usql to create a new, # empty database at the specified path: $ usql sq://path/to/dbname.sqlite3 $ usql sqlite3://path/to/dbname.sqlite3 $ usql file:/path/to/dbname.sqlite3 # connect to a adodb ole resource (windows only) $ usql adodb://Microsoft.Jet.OLEDB.4.0/myfile.mdb $ usql \"adodb://Microsoft.ACE.OLEDB.12.0/?Extended+Properties=\\\"Text;HDR=NO;FMT=Delimited\\\"\" # connect with ODBC driver (requires building with odbc tag) $ cat /etc/odbcinst.ini [DB2] Description=DB2 driver Driver=/opt/db2/clidriver/lib/libdb2.so FileUsage = 1 DontDLClose = 1 [PostgreSQL ANSI] Description=PostgreSQL ODBC driver (ANSI version) Driver=psqlodbca.so Setup=libodbcpsqlS.so Debug=0 CommLog=1 UsageCount=1 # connect to db2, postgres databases using ODBC $ usql odbc+DB2://user:pass@localhost/dbname $ usql odbc+PostgreSQL+ANSI://user:pass@localhost/dbname?TraceFile=/path/to/trace.log Executing Queries and Commands The interactive intrepreter reads queries and meta (\\ ) commands, sending the query to the connected database: $ usql sqlite://example.sqlite3 Connected with driver sqlite3 (SQLite3 3.17.0) Type \"help\" for help. sq:example.sqlite3=> create table test (test_id int, name string); CREATE TABLE sq:example.sqlite3=> insert into test (test_id, name) values (1, 'hello'); INSERT 1 sq:example.sqlite3=> select * from test; test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> select * from test sq:example.sqlite3-> \\p select * from test sq:example.sqlite3-> \\g test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> \\c postgres://booktest@localhost error: pq: 28P01: password authentication failed for user \"booktest\" Enter password: Connected with driver postgres (PostgreSQL 9.6.6) pg:booktest@localhost=> select * from authors; author_id | name +-----------+----------------+ 1 | Unknown Master 2 | blah 3 | aoeu (3 rows) pg:booktest@localhost=> Commands may accept one or more parameter, and can be quoted using either ' or \". Command parameters may also be backtick'd. Backslash Commands Currently available commands: $ usql Type \"help\" for help. (not connected)=> \\? General \\q quit usql \\copyright show usql usage and distribution terms \\drivers display information about available database drivers Query Execute \\g [(OPTIONS)] [FILE] or ; execute query (and send results to file or |pipe) \\crosstabview [(OPTIONS)] [COLUMNS] execute query and display results in crosstab \\G [(OPTIONS)] [FILE] as \\g, but forces vertical output mode \\gexec execute query and execute each value of the result \\gset [PREFIX] execute query and store results in usql variables \\gx [(OPTIONS)] [FILE] as \\g, but forces expanded output mode \\watch [(OPTIONS)] [DURATION] execute query every specified interval Query Buffer \\e [FILE] [LINE] edit the query buffer (or file) with external editor \\p show the contents of the query buffer \\raw show the raw (non-interpolated) contents of the query buffer \\r reset (clear) the query buffer \\w FILE write query buffer to file Help \\? [commands] show help on backslash commands \\? options show help on usql command-line options \\? variables show help on special variables Input/Output \\echo [-n] [STRING] write string to standard output (-n for no newline) \\qecho [-n] [STRING] write string to \\o output stream (-n for no newline) \\warn [-n] [STRING] write string to standard error (-n for no newline) \\o [FILE] send all query results to file or |pipe \\i FILE execute commands from file \\ir FILE as \\i, but relative to location of current script Informational \\d[S+] [NAME] list tables, views, and sequences or describe table, view, sequence, or index \\da[S+] [PATTERN] list aggregates \\df[S+] [PATTERN] list functions \\di[S+] [PATTERN] list indexes \\dm[S+] [PATTERN] list materialized views \\dn[S+] [PATTERN] list schemas \\ds[S+] [PATTERN] list sequences \\dt[S+] [PATTERN] list tables \\dv[S+] [PATTERN] list views \\l[+] list databases \\ss[+] [TABLE|QUERY] [k] show stats for a table or a query Formatting \\pset [NAME [VALUE]] set table output option \\a toggle between unaligned and aligned output mode \\C [STRING] set table title, or unset if none \\f [STRING] show or set field separator for unaligned query output \\H toggle HTML output mode \\T [STRING] set HTML <table> tag attributes, or unset if none \\t [on|off] show only rows \\x [on|off|auto] toggle expanded output Transaction \\begin begin a transaction \\commit commit current transaction \\rollback rollback (abort) current transaction Connection \\c URL connect to database with url \\c DRIVER PARAMS... connect to database with SQL driver and parameters \\Z close database connection \\password [USERNAME] change the password for a user \\conninfo display information about the current database connection Operating System \\cd [DIR] change the current working directory \\setenv NAME [VALUE] set or unset environment variable \\! [COMMAND] execute command in shell or start interactive shell \\timing [on|off] toggle timing of commands Variables \\prompt [-TYPE] <VAR> [PROMPT] prompt user to set variable \\set [NAME [VALUE]] set internal variable, or list all if no parameters \\unset NAME unset (delete) internal variable Features and Compatibility The usql project's goal is to support all standard psql commands and features. Pull Requests are always appreciated! Variables and Interpolation usql supports client-side interpolation of variables that can be \\set and \\unset: $ usql (not connected)=> \\set (not connected)=> \\set FOO bar (not connected)=> \\set FOO = 'bar' (not connected)=> \\unset FOO (not connected)=> \\set (not connected)=> A \\set variable, NAME, will be directly interpolated (by string substitution) into the query when prefixed with : and optionally surrounded by quotation marks (' or \"): pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from authors where name = :'FOO'; author_id | name +-----------+------+ 7 | bar (1 rows) The three forms, :NAME, :'NAME', and :\"NAME\", are used to interpolate a variable in parts of a query that may require quoting, such as for a column name, or when doing concatenation in a query: pg:booktest@localhost=> \\set TBLNAME authors pg:booktest@localhost=> \\set COLNAME name pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\p select * from authors where \"name\" = 'bar' pg:booktest@localhost-> \\raw select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\g author_id | name +-----------+------+ 7 | bar (1 rows) pg:booktest@localhost=> Note: variables contained within other strings will NOT be interpolated: pg:booktest@localhost=> select ':FOO'; ?column? +----------+ :FOO (1 rows) pg:booktest@localhost=> \\p select ':FOO'; pg:booktest@localhost=> Backtick'd parameters Meta (\\ ) commands support backticks on parameters: (not connected)=> \\echo Welcome `echo $USER` -- 'currently:' \"(\" `date` \")\" Welcome ken -- currently: ( Wed Jun 13 12:10:27 WIB 2018 ) (not connected)=> Backtick'd parameters will be passed to the user's SHELL, exactly as written, and can be combined with \\set: pg:booktest@localhost=> \\set MYVAR `date` pg:booktest@localhost=> \\set MYVAR = 'Wed Jun 13 12:17:11 WIB 2018' pg:booktest@localhost=> \\echo :MYVAR Wed Jun 13 12:17:11 WIB 2018 pg:booktest@localhost=> Passwords usql supports reading passwords for databases from a .usqlpass file contained in the user's HOME directory at startup: $ cat $HOME/.usqlpass # format is: # protocol:host:port:dbname:user:pass postgres:*:*:*:booktest:booktest $ usql pg:// Connected with driver postgres (PostgreSQL 9.6.9) Type \"help\" for help. pg:booktest@=> Note: the .usqlpass file cannot be readable by other users. Please set the permissions accordingly: Runtime Configuration (RC) File usql supports executing a .usqlrc contained in the user's HOME directory: $ cat $HOME/.usqlrc \\echo WELCOME TO THE JUNGLE `date` \\set SYNTAX_HL_STYLE paraiso-dark $ usql WELCOME TO THE JUNGLE Thu Jun 14 02:36:53 WIB 2018 Type \"help\" for help. (not connected)=> \\set SYNTAX_HL_STYLE = 'paraiso-dark' (not connected)=> The .usqlrc file is read by usql at startup in the same way as a file passed on the command-line with -f / --file. It is commonly used to set startup environment variables and settings. You can temporarily disable the RC-file by passing -X or --no-rc on the command-line: Host Connection Information By default, usql displays connection information when connecting to a database. This might cause problems with some databases or connections. This can be disabled by setting the system environment variable USQL_SHOW_HOST_INFORMATION to false: $ export USQL_SHOW_HOST_INFORMATION=false $ usql pg://booktest@localhost Type \"help\" for help. pg:booktest@=> SHOW_HOST_INFORMATION is a standard usql variable, and can be \\set or \\unset. Additionally, it can be passed via the command-line using -v or --set: $ usql --set SHOW_HOST_INFORMATION=false pg:// Type \"help\" for help. pg:booktest@=> \\set SHOW_HOST_INFORMATION true pg:booktest@=> \\connect pg:// Connected with driver postgres (PostgreSQL 9.6.9) pg:booktest@=> Syntax Highlighting Interactive queries will be syntax highlighted by default, using Chroma. There are a number of variables that control syntax highlighting: Variable Default Values Description SYNTAX_HL true true or false enables syntax highlighting SYNTAX_HL_FORMAT dependent on terminal support formatter name Chroma formatter name SYNTAX_HL_OVERRIDE_BG true true or false enables overriding the background color of the chroma styles SYNTAX_HL_STYLE monokai style name Chroma style name Time Formatting Some databases support time/date columns that support formatting. By default, usql formats time/date columns as RFC3339Nano, and can be set using \\pset time <FORMAT>: $ usql pg:// Connected with driver postgres (PostgreSQL 13.2 (Debian 13.2-1.pgdg100+1)) Type \"help\" for help. pg:postgres@=> \\pset time RFC3339Nano pg:postgres@=> select now(); now ----------------------------- 2021-05-01T22:21:44.710385Z (1 row) pg:postgres@=> \\pset time Kitchen Time display is \"Kitchen\" (\"3:04PM\"). pg:postgres@=> select now(); now --------- 10:22PM (1 row) pg:postgres@=> Any Go supported time format or the standard Go const name (for example, Kitchen, in the above). Constants Constant Name Value ANSIC Mon Jan _2 15:04:05 2006 UnixDate Mon Jan _2 15:04:05 MST 2006 RubyDate Mon Jan 02 15:04:05 -0700 2006 RFC822 02 Jan 06 15:04 MST RFC822Z 02 Jan 06 15:04 -0700 RFC850 Monday, 02-Jan-06 15:04:05 MST RFC1123 Mon, 02 Jan 2006 15:04:05 MST RFC1123Z Mon, 02 Jan 2006 15:04:05 -0700 RFC3339 2006-01-02T15:04:05Z07:00 RFC3339Nano 2006-01-02T15:04:05.999999999Z07:00 Kitchen 3:04PM Stamp Jan _2 15:04:05 StampMilli Jan _2 15:04:05.000 StampMicro Jan _2 15:04:05.000000 StampNano Jan _2 15:04:05.000000000 Copy usql implements the \\copy command that reads data from a database connection and writes it into another one. It requires 4 parameters: source connection string destination connection string source query destination table name, optionally with columns Connection strings support same syntax as in \\connect. Source query needs to be quoted. Source query must select same number of columns and in same order as they're defined in the destination table, unless they're specified for the destination, as table_name(column1, column2, ...). Quote the whole expression, if it contains spaces. \\copy does not attempt to perform any data type conversion. Use CAST in the source query to ensure data types compatible with destination table. Some drivers may have limited data type support, and they might not work at all when combined with other limited drivers. Unlike psql, \\copy in usql cannot read data directly from files. Drivers like csvq can help with this, since they support reading CSV and JSON files. $ cat books.csv book_id,author_id,isbn,title,year,available,tags 3,1,3,one,2018,\"2018-06-01 00:00:00\",{} 4,2,4,two,2019,\"2019-06-01 00:00:00\",{} $ usql -c \"\\copy csvq://. sqlite3://test.db 'select * from books' 'books'\" Copied 2 rows Note that it might be a better idea to use tools dedicated to the destination database to load data in a robust way. \\copy reads data from plain SELECT queries. Most drivers that have \\copy enabled use INSERT statements, except for PostgreSQL ones, which use COPY TO. Because data needs to be downloaded from one database and uploaded into another, don't expect same performance as in psql. For loading large amount of data efficiently, use tools native to the destination database. You can use \\copy with variables. Better yet, put those \\set commands in your runtime configuration file at $HOME/.usqlrc and passwords at $HOME/.usqlpass. $ usql Type \"help\" for help. (not connected)=> \\set pglocal postgres://postgres@localhost:49153?sslmode=disable (not connected)=> \\set oralocal godror://system@localhost:1521/orasid (not connected)=> \\copy :pglocal :oralocal 'select staff_id, first_name from staff' 'staff(staff_id, first_name)' Contributing usql is currently a WIP, and is aiming towards a 1.0 release soon. Well-written PRs are always welcome -- and there is a clear backlog of issues marked help wanted on the GitHub issue tracker! Please pick up an issue today, and submit a PR tomorrow! For more technical details, see CONTRIBUTING.md. Related Projects dburl - Go package providing a standard, URL-style mechanism for parsing and opening database connection URLs xo - Go command-line tool to generate Go code from a database schema ",
        "_version_":1718939880026800128},
      {
        "story_id":[18906834],
        "story_author":["byxorna"],
        "story_descendants":[17],
        "story_score":[72],
        "story_time":["2019-01-14T21:46:15Z"],
        "story_title":"Tumblr Opensources Kubernetes Utilities",
        "search":["Tumblr Opensources Kubernetes Utilities",
          "https://engineering.tumblr.com/post/182013497734/open-sourcing-our-kubernetes-tools",
          "At Tumblr, we are avid fans of Kubernetes. We have been using Kubernetes for all manner of workloads, like critical-path web requests handling for tumblr.com, background task executions like sending queued posts and push notifications, and scheduled jobs for spam detection and content moderation. Throughout our journey to move our 11 year old (almost 12! ) platform to a container-native architecture, we have made innumerable changes to how our applications are designed and run. Inspired by a lot of existing Kubernetes APIs and best practices, were excited to share with the community some of the tools weve developed at Tumblr as our infrastructure has evolved to work with Kubernetes.To help us integrate Kubernetes into our workflows, we have built a handful of tools of which we are open-sourcing three today! Each tool is a small, focused utility, designed to solve specific integration needs Tumblr had while migrating our workflows to Kubernetes. The tools were built to handle our needs internally, but we believe they are useful to the wider Kubernetes community.github.com/tumblr/k8s-sidecar-injectorgithub.com/tumblr/k8s-config-projectorgithub.com/tumblr/k8s-secret-projectork8s-sidecar-injectorAny company that has containerized an application as large and complex as Tumblr knows that it requires a tremendous amount of effort. Applications dont become container-native overnight, and sidecars can be useful to help emulate older deployments with colocated services on physical hosts or VMs. To reduce the amount of fragile copy-paste code by developers adding in sidecars to their Deployments and CronJobs, we created a service to dynamically inject sidecars, volumes, and environment data into pods as they are launched.The k8s-sidecar-injector listens to the Kubernetes API for Pod launches that contain annotations requesting a specific sidecar to be injected. For example, the annotation injector.tumblr.com/request=sidecar-prod-v1 will add any environment variables, volumes, and containers defined in the sidecar-prod-v1 configuration. We use this to add sidecars like logging and metrics daemons, cluster-wide environment variables like DATACENTER and HTTP_PROXY settings, and volumes for shared configuration data. By centralizing configuration of sidecars, we were able to reduce complexity in CronJobs and Deployments by hundreds of lines, eliminated copy-paste errors, and made rolling out updates to shared components in our sidecars effortless.An example sidecar ConfigMap is below, which adds a logging container, a volume from a logger-config ConfigMap, and some environment variables into the Pod.--- apiVersion: v1 kind: ConfigMap metadata: name: example-sidecars namespace: kube-system labels app: k8s-sidecar-injector data: logger-v1: | name: logger-v1 containers: - name: logger image: some/logger:2.2.3 imagePullPolicy: IfNotPresent ports: - containerPort: 8888 volumeMounts: - name: logger-conf mountPath: /etc/logger volumes: - name: logger-conf configMap: name: logger-config env: - name: DATACENTER value: dc01 - name: HTTP_PROXY value: http://my-proxy.org:8080/ - name: HTTPS_PROXY value: http://my-proxy.org:8080/This configuration will add the logger container into each pod with the annotation injector.tumblr.com/request: logger-v1, with a ConfigMap projected as a volume in /etc/logger. Additionally, every container in the Pod will get the DATACENTER=dc01 and HTTP_PROXY environment variables added, if they were not already set. This has allowed us to drastically reduce our boilerplate configuration when containerizing legacy applications that require a complex sidecar configuration.k8s-config-projectorInternally, we have many types of configuration data that is needed by a variety of applications. We store canonical settings data like feature flags, lists of hosts/IPs+ports, and application settings in git. This allows automated generation/manipulation of these settings by bots, cron jobs, Collins, and humans alike. Applications want to know about some subset of this configuration data, and they want to be informed when this data changes as quickly as possible. Kubernetes provides the ConfigMap resource, which enables users to provide their service with configuration data and update the data in running pods without requiring a redeployment. We wanted to use this to configure our services and jobs in a Kubernetes-native manner, but needed a way to bridge the gap between our canonical configuration store (git repo of config files) to ConfigMaps. Thus, was k8s-config-projector born.The Config Projector (github.com/tumblr/k8s-config-projector)[github.com/tumblr/k8s-config-projector] is a command line tool, meant to be run by CI processes. It combines a git repo hosting configuration data (feature flags, lists of hostnames+ports, application settings) with a set of projection manifest files that describe how to group/extract settings from the config repo and transmute them into ConfigMaps. The config projector allows developers to encode a set of configuration data the application needs to run into a projection manifest. As the configuration data changes in the git repository, CI will run the projector, projecting and deploying new ConfigMaps containing this updated data, without needing the application to be redeployed. Projection datasources can handle both structured and unstructured configuration files (YAML, JSON, and raw text/binary).An example projection manifest is below, describing how a fictitious notification application could request some configuration data that may dynamically change (memcached hosts, log level, launch flags, etc):--- name: notifications-us-east-1-production namespace: notification-production data: # extract some fields from JSON - source: generated/us-east-1/production/config.json output_file: config.json field_extraction: - memcached_hosts: $.memcached.notifications.production.hosts - settings: $.applications.notification.production.settings - datacenter: $.datacenter - environment: $.environment # extract a scalar value from a YAML - source: apps/us-east-1/production/notification.yaml output_file: launch_flags extract: $.launch_flags After processing by the config projector, the following ConfigMap is generated, which can then be posted to a Kubernetes cluster with kubectl create -f <generatedfile>.kind: ConfigMap apiVersion: v1 metadata name: notifications-us-east-1-production namespace: notification-production labels: tumblr.com/config-version: \"1539778254\" tumblr.com/managed-configmap: \"true\" data: config.json: | { \"memcached_hosts\": [\"2.3.4.5:11211\",\"4.5.6.7:11211\",\"6.7.8.9:11211\"], \"settings\": { \"debug\": false, \"buffer\": \"2000\", \"flavor\": \"out of control\", \"log_level\": \"INFO\", }, \"datacenter\": \"us-east-1\", \"environment\": \"production\" } launch_flags: \"-Xmx5g -Dsun.net.inetaddr.ttl=10\" With this tool, we have enabled our applications running in kubernetes to receive dynamic configuration updates without requiring container rebuilds or deployments. More examples can be found here.k8s-secret-projectorSimilar to our configuration repository, we store secure credentials in access controlled vaults, divided by production levels. We wanted to enable developers to request access to subsets of credentials for a given application without needing to grant the user access to the secrets themselves. Additionally, we wanted to make certificate and password rotation transparent to all applications, enabling us to rotate credentials in an application-agnostic manner, without needing to redeploy applications. Lastly, we wanted to introduce a mechanism where application developers would explicitly describe which credentials their services need, and enable a framework to audit and grant permissions for a service to consume a secret.The k8s-secret-projector operates similarly to the k8s-config-projector, albeit with a few differences. The secret projector combines a repository of projection manifests with a set of credential repositories. A Continuous Integration (CI) tool like Jenkins will run the k8s-secret-projector against any changes in the projection manifests repository to generate new Kubernetes Secret YAML files. Then, Continuous Deployment can deploy the generated and validated Secret files to any number of Kubernetes clusters.Take this file in the production credentials repository, named aws/credentials.json:{ \"us-east-1\": { \"region\": \"us-east-1\", \"aws\": { \"key\": \"somethignSekri7T!\", }, \"s3\": { \"key\": \"passW0rD!\", }, \"redshift\": { \"key\": \"ello0liv3r!\", \"database\": \"mydatabase\" } }, \"us-west-2\": { \"region\": \"us-west-2\", \"aws\": { \"key\": \"anotherPasswr09d!\", }, \"s3\": { \"key\": \"sueprSekur#\", } } } We need to create an amazon.yaml configuration file containing the s3.key and aws.key for us-east-1, as well as a text file containing our region. The projection manifest below will extract only the fields we need, and output them in the format desired.name: aws-credentials namespace: myteam repo: production data: # create an amazon.yaml config with the secrets we care about - name: amazon.yaml source: format: yaml json: aws/credentials.json jsonpaths: s3: $.us-east-1.s3.key aws: $.us-east-1.aws.key region: $.us-east-1.region # create a item containing just the name of the region we are in - name: region source: json: aws/credentials.json jsonpath: $.us-east-1.region Projecting this manifest with the above credentials results in the following Kubernetes Secret YAML file:apiVersion: v1 kind: Secret metadata: labels: tumblr.com/managed-secret: \"true\" tumblr.com/secret-version: master-741-7459d1abcc120 name: aws-credentials namespace: myteam data: region: dXMtZWFzdC0x # region decoded for clarity: us-east-1 amazon.yaml: LS0tCnMzOiAicGFzc1cwckQhIgphd3M6ICJzb21ldGhpZ25TZWtyaTdUISIKcmVnaW9uOiB1cy1lYXN0LTEK # amazon.yaml decoded for clarity: # --- # s3: \"passW0rD!\" # aws: \"somethignSekri7T!\" # region: us-east-1 In addition to being able to extract fields from structured YAML and JSON sources, we gave it the ability to encrypt generated Secrets before they touch disk. This allows Secrets to be deployed in shared Kubernetes environments, where users are colocated with other users, and do not feel comfortable with their Secret resources being unencrypted in etcd. Please note, this requires decryption by your applications before use. More details on how the encryption modules work can be found here.For more examples of how to use this, check out examples here!Whats NextWe are excited to share these tools with the Kubernetes open source community, and we hope they can help your organization adopt container-native thinking when managing application lifecycle like they helped Tumblr. Feature enhancements and bug fixes are welcome! And, shameless plug: if you are interested in Kubernetes, containerization technology, open source, and scaling a massive website with industry leading technologies and practices? Come join us!.- @pipefail ",
          "Not a cryptographer, but a few things irked me about the secret projector encryption features:<p>1. Calls it AES-CBC, but it's using AES-GCM <a href=\"https://github.com/tumblr/k8s-secret-projector/blob/master/pkg/encryption/cbc/block.go#L88\" rel=\"nofollow\">https://github.com/tumblr/k8s-secret-projector/blob/master/p...</a><p>2. Encryption key generation (edited: from a password) uses a hash function rather than using a KDF. Short hashes are padded (takes first N bytes required to hit max length, appends to end) or trimmed to the appropriate length. <a href=\"https://github.com/tumblr/k8s-secret-projector/blob/master/pkg/encryption/cbc/key.go#L43\" rel=\"nofollow\">https://github.com/tumblr/k8s-secret-projector/blob/master/p...</a>",
          "My tinfoil hat is telling me that the engineers are pushing to open source as much as possible before the company inevitably goes under due to the recent banning of NSFW content."],
        "story_type":["Normal"],
        "url":"https://engineering.tumblr.com/post/182013497734/open-sourcing-our-kubernetes-tools",
        "comments.comment_id":[18907172,
          18907340],
        "comments.comment_author":["throwaway_129",
          "p1necone"],
        "comments.comment_descendants":[1,
          2],
        "comments.comment_time":["2019-01-14T22:32:25Z",
          "2019-01-14T22:53:24Z"],
        "comments.comment_text":["Not a cryptographer, but a few things irked me about the secret projector encryption features:<p>1. Calls it AES-CBC, but it's using AES-GCM <a href=\"https://github.com/tumblr/k8s-secret-projector/blob/master/pkg/encryption/cbc/block.go#L88\" rel=\"nofollow\">https://github.com/tumblr/k8s-secret-projector/blob/master/p...</a><p>2. Encryption key generation (edited: from a password) uses a hash function rather than using a KDF. Short hashes are padded (takes first N bytes required to hit max length, appends to end) or trimmed to the appropriate length. <a href=\"https://github.com/tumblr/k8s-secret-projector/blob/master/pkg/encryption/cbc/key.go#L43\" rel=\"nofollow\">https://github.com/tumblr/k8s-secret-projector/blob/master/p...</a>",
          "My tinfoil hat is telling me that the engineers are pushing to open source as much as possible before the company inevitably goes under due to the recent banning of NSFW content."],
        "id":"4c9447ee-ba5a-4858-b22c-48383b4a59c8",
        "url_text":"At Tumblr, we are avid fans of Kubernetes. We have been using Kubernetes for all manner of workloads, like critical-path web requests handling for tumblr.com, background task executions like sending queued posts and push notifications, and scheduled jobs for spam detection and content moderation. Throughout our journey to move our 11 year old (almost 12! ) platform to a container-native architecture, we have made innumerable changes to how our applications are designed and run. Inspired by a lot of existing Kubernetes APIs and best practices, were excited to share with the community some of the tools weve developed at Tumblr as our infrastructure has evolved to work with Kubernetes.To help us integrate Kubernetes into our workflows, we have built a handful of tools of which we are open-sourcing three today! Each tool is a small, focused utility, designed to solve specific integration needs Tumblr had while migrating our workflows to Kubernetes. The tools were built to handle our needs internally, but we believe they are useful to the wider Kubernetes community.github.com/tumblr/k8s-sidecar-injectorgithub.com/tumblr/k8s-config-projectorgithub.com/tumblr/k8s-secret-projectork8s-sidecar-injectorAny company that has containerized an application as large and complex as Tumblr knows that it requires a tremendous amount of effort. Applications dont become container-native overnight, and sidecars can be useful to help emulate older deployments with colocated services on physical hosts or VMs. To reduce the amount of fragile copy-paste code by developers adding in sidecars to their Deployments and CronJobs, we created a service to dynamically inject sidecars, volumes, and environment data into pods as they are launched.The k8s-sidecar-injector listens to the Kubernetes API for Pod launches that contain annotations requesting a specific sidecar to be injected. For example, the annotation injector.tumblr.com/request=sidecar-prod-v1 will add any environment variables, volumes, and containers defined in the sidecar-prod-v1 configuration. We use this to add sidecars like logging and metrics daemons, cluster-wide environment variables like DATACENTER and HTTP_PROXY settings, and volumes for shared configuration data. By centralizing configuration of sidecars, we were able to reduce complexity in CronJobs and Deployments by hundreds of lines, eliminated copy-paste errors, and made rolling out updates to shared components in our sidecars effortless.An example sidecar ConfigMap is below, which adds a logging container, a volume from a logger-config ConfigMap, and some environment variables into the Pod.--- apiVersion: v1 kind: ConfigMap metadata: name: example-sidecars namespace: kube-system labels app: k8s-sidecar-injector data: logger-v1: | name: logger-v1 containers: - name: logger image: some/logger:2.2.3 imagePullPolicy: IfNotPresent ports: - containerPort: 8888 volumeMounts: - name: logger-conf mountPath: /etc/logger volumes: - name: logger-conf configMap: name: logger-config env: - name: DATACENTER value: dc01 - name: HTTP_PROXY value: http://my-proxy.org:8080/ - name: HTTPS_PROXY value: http://my-proxy.org:8080/This configuration will add the logger container into each pod with the annotation injector.tumblr.com/request: logger-v1, with a ConfigMap projected as a volume in /etc/logger. Additionally, every container in the Pod will get the DATACENTER=dc01 and HTTP_PROXY environment variables added, if they were not already set. This has allowed us to drastically reduce our boilerplate configuration when containerizing legacy applications that require a complex sidecar configuration.k8s-config-projectorInternally, we have many types of configuration data that is needed by a variety of applications. We store canonical settings data like feature flags, lists of hosts/IPs+ports, and application settings in git. This allows automated generation/manipulation of these settings by bots, cron jobs, Collins, and humans alike. Applications want to know about some subset of this configuration data, and they want to be informed when this data changes as quickly as possible. Kubernetes provides the ConfigMap resource, which enables users to provide their service with configuration data and update the data in running pods without requiring a redeployment. We wanted to use this to configure our services and jobs in a Kubernetes-native manner, but needed a way to bridge the gap between our canonical configuration store (git repo of config files) to ConfigMaps. Thus, was k8s-config-projector born.The Config Projector (github.com/tumblr/k8s-config-projector)[github.com/tumblr/k8s-config-projector] is a command line tool, meant to be run by CI processes. It combines a git repo hosting configuration data (feature flags, lists of hostnames+ports, application settings) with a set of projection manifest files that describe how to group/extract settings from the config repo and transmute them into ConfigMaps. The config projector allows developers to encode a set of configuration data the application needs to run into a projection manifest. As the configuration data changes in the git repository, CI will run the projector, projecting and deploying new ConfigMaps containing this updated data, without needing the application to be redeployed. Projection datasources can handle both structured and unstructured configuration files (YAML, JSON, and raw text/binary).An example projection manifest is below, describing how a fictitious notification application could request some configuration data that may dynamically change (memcached hosts, log level, launch flags, etc):--- name: notifications-us-east-1-production namespace: notification-production data: # extract some fields from JSON - source: generated/us-east-1/production/config.json output_file: config.json field_extraction: - memcached_hosts: $.memcached.notifications.production.hosts - settings: $.applications.notification.production.settings - datacenter: $.datacenter - environment: $.environment # extract a scalar value from a YAML - source: apps/us-east-1/production/notification.yaml output_file: launch_flags extract: $.launch_flags After processing by the config projector, the following ConfigMap is generated, which can then be posted to a Kubernetes cluster with kubectl create -f <generatedfile>.kind: ConfigMap apiVersion: v1 metadata name: notifications-us-east-1-production namespace: notification-production labels: tumblr.com/config-version: \"1539778254\" tumblr.com/managed-configmap: \"true\" data: config.json: | { \"memcached_hosts\": [\"2.3.4.5:11211\",\"4.5.6.7:11211\",\"6.7.8.9:11211\"], \"settings\": { \"debug\": false, \"buffer\": \"2000\", \"flavor\": \"out of control\", \"log_level\": \"INFO\", }, \"datacenter\": \"us-east-1\", \"environment\": \"production\" } launch_flags: \"-Xmx5g -Dsun.net.inetaddr.ttl=10\" With this tool, we have enabled our applications running in kubernetes to receive dynamic configuration updates without requiring container rebuilds or deployments. More examples can be found here.k8s-secret-projectorSimilar to our configuration repository, we store secure credentials in access controlled vaults, divided by production levels. We wanted to enable developers to request access to subsets of credentials for a given application without needing to grant the user access to the secrets themselves. Additionally, we wanted to make certificate and password rotation transparent to all applications, enabling us to rotate credentials in an application-agnostic manner, without needing to redeploy applications. Lastly, we wanted to introduce a mechanism where application developers would explicitly describe which credentials their services need, and enable a framework to audit and grant permissions for a service to consume a secret.The k8s-secret-projector operates similarly to the k8s-config-projector, albeit with a few differences. The secret projector combines a repository of projection manifests with a set of credential repositories. A Continuous Integration (CI) tool like Jenkins will run the k8s-secret-projector against any changes in the projection manifests repository to generate new Kubernetes Secret YAML files. Then, Continuous Deployment can deploy the generated and validated Secret files to any number of Kubernetes clusters.Take this file in the production credentials repository, named aws/credentials.json:{ \"us-east-1\": { \"region\": \"us-east-1\", \"aws\": { \"key\": \"somethignSekri7T!\", }, \"s3\": { \"key\": \"passW0rD!\", }, \"redshift\": { \"key\": \"ello0liv3r!\", \"database\": \"mydatabase\" } }, \"us-west-2\": { \"region\": \"us-west-2\", \"aws\": { \"key\": \"anotherPasswr09d!\", }, \"s3\": { \"key\": \"sueprSekur#\", } } } We need to create an amazon.yaml configuration file containing the s3.key and aws.key for us-east-1, as well as a text file containing our region. The projection manifest below will extract only the fields we need, and output them in the format desired.name: aws-credentials namespace: myteam repo: production data: # create an amazon.yaml config with the secrets we care about - name: amazon.yaml source: format: yaml json: aws/credentials.json jsonpaths: s3: $.us-east-1.s3.key aws: $.us-east-1.aws.key region: $.us-east-1.region # create a item containing just the name of the region we are in - name: region source: json: aws/credentials.json jsonpath: $.us-east-1.region Projecting this manifest with the above credentials results in the following Kubernetes Secret YAML file:apiVersion: v1 kind: Secret metadata: labels: tumblr.com/managed-secret: \"true\" tumblr.com/secret-version: master-741-7459d1abcc120 name: aws-credentials namespace: myteam data: region: dXMtZWFzdC0x # region decoded for clarity: us-east-1 amazon.yaml: LS0tCnMzOiAicGFzc1cwckQhIgphd3M6ICJzb21ldGhpZ25TZWtyaTdUISIKcmVnaW9uOiB1cy1lYXN0LTEK # amazon.yaml decoded for clarity: # --- # s3: \"passW0rD!\" # aws: \"somethignSekri7T!\" # region: us-east-1 In addition to being able to extract fields from structured YAML and JSON sources, we gave it the ability to encrypt generated Secrets before they touch disk. This allows Secrets to be deployed in shared Kubernetes environments, where users are colocated with other users, and do not feel comfortable with their Secret resources being unencrypted in etcd. Please note, this requires decryption by your applications before use. More details on how the encryption modules work can be found here.For more examples of how to use this, check out examples here!Whats NextWe are excited to share these tools with the Kubernetes open source community, and we hope they can help your organization adopt container-native thinking when managing application lifecycle like they helped Tumblr. Feature enhancements and bug fixes are welcome! And, shameless plug: if you are interested in Kubernetes, containerization technology, open source, and scaling a massive website with industry leading technologies and practices? Come join us!.- @pipefail ",
        "_version_":1718939816424374272},
      {
        "story_id":[19988548],
        "story_author":["axiomdata316"],
        "story_descendants":[169],
        "story_score":[597],
        "story_time":["2019-05-23T04:42:59Z"],
        "story_title":"The Art of Command Line (2015)",
        "search":["The Art of Command Line (2015)",
          "https://github.com/jlevy/the-art-of-command-line",
          " etina Deutsch English Espaol Franais Indonesia Italiano polski Portugus Romn Slovenina The Art of Command Line Note: I'm planning to revise this and looking for a new co-author to help with expanding this into a more comprehensive guide. While it's very popular, it could be broader and a bit deeper. If you like to write and are close to being an expert on this material and willing to consider helping, please drop me a note at josh (0x40) holloway.com. jlevy, Holloway. Thank you! Meta Basics Everyday use Processing files and data System debugging One-liners Obscure but useful macOS only Windows only More resources Disclaimer Fluency on the command line is a skill often neglected or considered arcane, but it improves your flexibility and productivity as an engineer in both obvious and subtle ways. This is a selection of notes and tips on using the command-line that we've found useful when working on Linux. Some tips are elementary, and some are fairly specific, sophisticated, or obscure. This page is not long, but if you can use and recall all the items here, you know a lot. This work is the result of many authors and translators. Some of this originally appeared on Quora, but it has since moved to GitHub, where people more talented than the original author have made numerous improvements. Please submit a question if you have a question related to the command line. Please contribute if you see an error or something that could be better! Meta Scope: This guide is for both beginners and experienced users. The goals are breadth (everything important), specificity (give concrete examples of the most common case), and brevity (avoid things that aren't essential or digressions you can easily look up elsewhere). Every tip is essential in some situation or significantly saves time over alternatives. This is written for Linux, with the exception of the \"macOS only\" and \"Windows only\" sections. Many of the other items apply or can be installed on other Unices or macOS (or even Cygwin). The focus is on interactive Bash, though many tips apply to other shells and to general Bash scripting. It includes both \"standard\" Unix commands as well as ones that require special package installs -- so long as they are important enough to merit inclusion. Notes: To keep this to one page, content is implicitly included by reference. You're smart enough to look up more detail elsewhere once you know the idea or command to Google. Use apt, yum, dnf, pacman, pip or brew (as appropriate) to install new programs. Use Explainshell to get a helpful breakdown of what commands, options, pipes etc. do. Basics Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long. Alternate shells can be nice, but Bash is powerful and always available (learning only zsh, fish, etc., while tempting on your own laptop, restricts you in many situations, such as using existing servers). Learn at least one text-based editor well. The nano editor is one of the simplest for basic editing (opening, editing, saving, searching). However, for the power user in a text terminal, there is no substitute for Vim (vi), the hard-to-learn but venerable, fast, and full-featured editor. Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.) Finding documentation: Know how to read official documentation with man (for the inquisitive, man man lists the section numbers, e.g. 1 is \"regular\" commands, 5 is files/conventions, and 8 are for administration). Find man pages with apropos. Know that some commands are not executables, but Bash builtins, and that you can get help on them with help and help -d. You can find out whether a command is an executable, shell builtin or an alias by using type command. curl cheat.sh/command will give a brief \"cheat sheet\" with common examples of how to use a shell command. Learn about redirection of output and input using > and < and pipes using |. Know > overwrites the output file and >> appends. Learn about stdout and stderr. Learn about file glob expansion with * (and perhaps ? and [...]) and quoting and the difference between double \" and single ' quotes. (See more on variable expansion below.) Be familiar with Bash job management: &, ctrl-z, ctrl-c, jobs, fg, bg, kill, etc. Know ssh, and the basics of passwordless authentication, via ssh-agent, ssh-add, etc. Basic file management: ls and ls -l (in particular, learn what every column in ls -l means), less, head, tail and tail -f (or even better, less +F), ln and ln -s (learn the differences and advantages of hard versus soft links), chown, chmod, du (for a quick summary of disk usage: du -hs *). For filesystem management, df, mount, fdisk, mkfs, lsblk. Learn what an inode is (ls -i or df -i). Basic network management: ip or ifconfig, dig, traceroute, route. Learn and use a version control management system, such as git. Know regular expressions well, and the various flags to grep/egrep. The -i, -o, -v, -A, -B, and -C options are worth knowing. Learn to use apt-get, yum, dnf or pacman (depending on distro) to find and install packages. And make sure you have pip to install Python-based command-line tools (a few below are easiest to install via pip). Everyday use In Bash, use Tab to complete arguments or list all available commands and ctrl-r to search through command history (after pressing, type to search, press ctrl-r repeatedly to cycle through more matches, press Enter to execute the found command, or hit the right arrow to put the result in the current line to allow editing). In Bash, use ctrl-w to delete the last word, and ctrl-u to delete the content from current cursor back to the start of the line. Use alt-b and alt-f to move by word, ctrl-a to move cursor to beginning of line, ctrl-e to move cursor to end of line, ctrl-k to kill to the end of the line, ctrl-l to clear the screen. See man readline for all the default keybindings in Bash. There are a lot. For example alt-. cycles through previous arguments, and alt-* expands a glob. Alternatively, if you love vi-style key-bindings, use set -o vi (and set -o emacs to put it back). For editing long commands, after setting your editor (for example export EDITOR=vim), ctrl-x ctrl-e will open the current command in an editor for multi-line editing. Or in vi style, escape-v. To see recent commands, use history. Follow with !n (where n is the command number) to execute again. There are also many abbreviations you can use, the most useful probably being !$ for last argument and !! for last command (see \"HISTORY EXPANSION\" in the man page). However, these are often easily replaced with ctrl-r and alt-.. Go to your home directory with cd. Access files relative to your home directory with the ~ prefix (e.g. ~/.bashrc). In sh scripts refer to the home directory as $HOME. To go back to the previous working directory: cd -. If you are halfway through typing a command but change your mind, hit alt-# to add a # at the beginning and enter it as a comment (or use ctrl-a, #, enter). You can then return to it later via command history. Use xargs (or parallel). It's very powerful. Note you can control how many items execute per line (-L) as well as parallelism (-P). If you're not sure if it'll do the right thing, use xargs echo first. Also, -I{} is handy. Examples: find . -name '*.py' | xargs grep some_function cat hosts | xargs -I{} ssh root@{} hostname pstree -p is a helpful display of the process tree. Use pgrep and pkill to find or signal processes by name (-f is helpful). Know the various signals you can send processes. For example, to suspend a process, use kill -STOP [pid]. For the full list, see man 7 signal Use nohup or disown if you want a background process to keep running forever. Check what processes are listening via netstat -lntp or ss -plat (for TCP; add -u for UDP) or lsof -iTCP -sTCP:LISTEN -P -n (which also works on macOS). See also lsof and fuser for open sockets and files. See uptime or w to know how long the system has been running. Use alias to create shortcuts for commonly used commands. For example, alias ll='ls -latr' creates a new alias ll. Save aliases, shell settings, and functions you commonly use in ~/.bashrc, and arrange for login shells to source it. This will make your setup available in all your shell sessions. Put the settings of environment variables as well as commands that should be executed when you login in ~/.bash_profile. Separate configuration will be needed for shells you launch from graphical environment logins and cron jobs. Synchronize your configuration files (e.g. .bashrc and .bash_profile) among various computers with Git. Understand that care is needed when variables and filenames include whitespace. Surround your Bash variables with quotes, e.g. \"$FOO\". Prefer the -0 or -print0 options to enable null characters to delimit filenames, e.g. locate -0 pattern | xargs -0 ls -al or find / -print0 -type d | xargs -0 ls -al. To iterate on filenames containing whitespace in a for loop, set your IFS to be a newline only using IFS=$'\\n'. In Bash scripts, use set -x (or the variant set -v, which logs raw input, including unexpanded variables and comments) for debugging output. Use strict modes unless you have a good reason not to: Use set -e to abort on errors (nonzero exit code). Use set -u to detect unset variable usages. Consider set -o pipefail too, to abort on errors within pipes (though read up on it more if you do, as this topic is a bit subtle). For more involved scripts, also use trap on EXIT or ERR. A useful habit is to start a script like this, which will make it detect and abort on common errors and print a message: set -euo pipefail trap \"echo 'error: Script failed: see failed command above'\" ERR In Bash scripts, subshells (written with parentheses) are convenient ways to group commands. A common example is to temporarily move to a different working directory, e.g. # do something in current dir (cd /some/other/dir && other-command) # continue in original dir In Bash, note there are lots of kinds of variable expansion. Checking a variable exists: ${name:?error message}. For example, if a Bash script requires a single argument, just write input_file=${1:?usage: $0 input_file}. Using a default value if a variable is empty: ${name:-default}. If you want to have an additional (optional) parameter added to the previous example, you can use something like output_file=${2:-logfile}. If $2 is omitted and thus empty, output_file will be set to logfile. Arithmetic expansion: i=$(( (i + 1) % 5 )). Sequences: {1..10}. Trimming of strings: ${var%suffix} and ${var#prefix}. For example if var=foo.pdf, then echo ${var%.pdf}.txt prints foo.txt. Brace expansion using {...} can reduce having to re-type similar text and automate combinations of items. This is helpful in examples like mv foo.{txt,pdf} some-dir (which moves both files), cp somefile{,.bak} (which expands to cp somefile somefile.bak) or mkdir -p test-{a,b,c}/subtest-{1,2,3} (which expands all possible combinations and creates a directory tree). Brace expansion is performed before any other expansion. The order of expansions is: brace expansion; tilde expansion, parameter and variable expansion, arithmetic expansion, and command substitution (done in a left-to-right fashion); word splitting; and filename expansion. (For example, a range like {1..20} cannot be expressed with variables using {$a..$b}. Use seq or a for loop instead, e.g., seq $a $b or for((i=a; i<=b; i++)); do ... ; done.) The output of a command can be treated like a file via <(some command) (known as process substitution). For example, compare local /etc/hosts with a remote one: diff /etc/hosts <(ssh somehost cat /etc/hosts) When writing scripts you may want to put all of your code in curly braces. If the closing brace is missing, your script will be prevented from executing due to a syntax error. This makes sense when your script is going to be downloaded from the web, since it prevents partially downloaded scripts from executing: A \"here document\" allows redirection of multiple lines of input as if from a file: cat <<EOF input on multiple lines EOF In Bash, redirect both standard output and standard error via: some-command >logfile 2>&1 or some-command &>logfile. Often, to ensure a command does not leave an open file handle to standard input, tying it to the terminal you are in, it is also good practice to add </dev/null. Use man ascii for a good ASCII table, with hex and decimal values. For general encoding info, man unicode, man utf-8, and man latin1 are helpful. Use screen or tmux to multiplex the screen, especially useful on remote ssh sessions and to detach and re-attach to a session. byobu can enhance screen or tmux by providing more information and easier management. A more minimal alternative for session persistence only is dtach. In ssh, knowing how to port tunnel with -L or -D (and occasionally -R) is useful, e.g. to access web sites from a remote server. It can be useful to make a few optimizations to your ssh configuration; for example, this ~/.ssh/config contains settings to avoid dropped connections in certain network environments, uses compression (which is helpful with scp over low-bandwidth connections), and multiplex channels to the same server with a local control file: TCPKeepAlive=yes ServerAliveInterval=15 ServerAliveCountMax=6 Compression=yes ControlMaster auto ControlPath /tmp/%r@%h:%p ControlPersist yes A few other options relevant to ssh are security sensitive and should be enabled with care, e.g. per subnet or host or in trusted networks: StrictHostKeyChecking=no, ForwardAgent=yes Consider mosh an alternative to ssh that uses UDP, avoiding dropped connections and adding convenience on the road (requires server-side setup). To get the permissions on a file in octal form, which is useful for system configuration but not available in ls and easy to bungle, use something like stat -c '%A %a %n' /etc/timezone For interactive selection of values from the output of another command, use percol or fzf. For interaction with files based on the output of another command (like git), use fpp (PathPicker). For a simple web server for all files in the current directory (and subdirs), available to anyone on your network, use: python -m SimpleHTTPServer 7777 (for port 7777 and Python 2) and python -m http.server 7777 (for port 7777 and Python 3). For running a command as another user, use sudo. Defaults to running as root; use -u to specify another user. Use -i to login as that user (you will be asked for your password). For switching the shell to another user, use su username or su - username. The latter with \"-\" gets an environment as if another user just logged in. Omitting the username defaults to root. You will be asked for the password of the user you are switching to. Know about the 128K limit on command lines. This \"Argument list too long\" error is common when wildcard matching large numbers of files. (When this happens alternatives like find and xargs may help.) For a basic calculator (and of course access to Python in general), use the python interpreter. For example, Processing files and data To locate a file by name in the current directory, find . -iname '*something*' (or similar). To find a file anywhere by name, use locate something (but bear in mind updatedb may not have indexed recently created files). For general searching through source or data files, there are several options more advanced or faster than grep -r, including (in rough order from older to newer) ack, ag (\"the silver searcher\"), and rg (ripgrep). To convert HTML to text: lynx -dump -stdin For Markdown, HTML, and all kinds of document conversion, try pandoc. For example, to convert a Markdown document to Word format: pandoc README.md --from markdown --to docx -o temp.docx If you must handle XML, xmlstarlet is old but good. For JSON, use jq. For interactive use, also see jid and jiq. For YAML, use shyaml. For Excel or CSV files, csvkit provides in2csv, csvcut, csvjoin, csvgrep, etc. For Amazon S3, s3cmd is convenient and s4cmd is faster. Amazon's aws and the improved saws are essential for other AWS-related tasks. Know about sort and uniq, including uniq's -u and -d options -- see one-liners below. See also comm. Know about cut, paste, and join to manipulate text files. Many people use cut but forget about join. Know about wc to count newlines (-l), characters (-m), words (-w) and bytes (-c). Know about tee to copy from stdin to a file and also to stdout, as in ls -al | tee file.txt. For more complex calculations, including grouping, reversing fields, and statistical calculations, consider datamash. Know that locale affects a lot of command line tools in subtle ways, including sorting order (collation) and performance. Most Linux installations will set LANG or other locale variables to a local setting like US English. But be aware sorting will change if you change locale. And know i18n routines can make sort or other commands run many times slower. In some situations (such as the set operations or uniqueness operations below) you can safely ignore slow i18n routines entirely and use traditional byte-based sort order, using export LC_ALL=C. You can set a specific command's environment by prefixing its invocation with the environment variable settings, as in TZ=Pacific/Fiji date. Know basic awk and sed for simple data munging. See One-liners for examples. To replace all occurrences of a string in place, in one or more files: perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt To rename multiple files and/or search and replace within files, try repren. (In some cases the rename command also allows multiple renames, but be careful as its functionality is not the same on all Linux distributions.) # Full rename of filenames, directories, and contents foo -> bar: repren --full --preserve-case --from foo --to bar . # Recover backup files whatever.bak -> whatever: repren --renames --from '(.*)\\.bak' --to '\\1' *.bak # Same as above, using rename, if available: rename 's/\\.bak$//' *.bak As the man page says, rsync really is a fast and extraordinarily versatile file copying tool. It's known for synchronizing between machines but is equally useful locally. When security restrictions allow, using rsync instead of scp allows recovery of a transfer without restarting from scratch. It also is among the fastest ways to delete large numbers of files: mkdir empty && rsync -r --delete empty/ some-dir && rmdir some-dir For monitoring progress when processing files, use pv, pycp, pmonitor, progress, rsync --progress, or, for block-level copying, dd status=progress. Use shuf to shuffle or select random lines from a file. Know sort's options. For numbers, use -n, or -h for handling human-readable numbers (e.g. from du -h). Know how keys work (-t and -k). In particular, watch out that you need to write -k1,1 to sort by only the first field; -k1 means sort according to the whole line. Stable sort (sort -s) can be useful. For example, to sort first by field 2, then secondarily by field 1, you can use sort -k1,1 | sort -s -k2,2. If you ever need to write a tab literal in a command line in Bash (e.g. for the -t argument to sort), press ctrl-v [Tab] or write $'\\t' (the latter is better as you can copy/paste it). The standard tools for patching source code are diff and patch. See also diffstat for summary statistics of a diff and sdiff for a side-by-side diff. Note diff -r works for entire directories. Use diff -r tree1 tree2 | diffstat for a summary of changes. Use vimdiff to compare and edit files. For binary files, use hd, hexdump or xxd for simple hex dumps and bvi, hexedit or biew for binary editing. Also for binary files, strings (plus grep, etc.) lets you find bits of text. For binary diffs (delta compression), use xdelta3. To convert text encodings, try iconv. Or uconv for more advanced use; it supports some advanced Unicode things. For example: # Displays hex codes or actual names of characters (useful for debugging): uconv -f utf-8 -t utf-8 -x '::Any-Hex;' < input.txt uconv -f utf-8 -t utf-8 -x '::Any-Name;' < input.txt # Lowercase and removes all accents (by expanding and dropping them): uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] >; ::Any-NFC;' < input.txt > output.txt To split files into pieces, see split (to split by size) and csplit (to split by a pattern). Date and time: To get the current date and time in the helpful ISO 8601 format, use date -u +\"%Y-%m-%dT%H:%M:%SZ\" (other options are problematic). To manipulate date and time expressions, use dateadd, datediff, strptime etc. from dateutils. Use zless, zmore, zcat, and zgrep to operate on compressed files. File attributes are settable via chattr and offer a lower-level alternative to file permissions. For example, to protect against accidental file deletion the immutable flag: sudo chattr +i /critical/directory/or/file Use getfacl and setfacl to save and restore file permissions. For example: getfacl -R /some/path > permissions.txt setfacl --restore=permissions.txt To create empty files quickly, use truncate (creates sparse file), fallocate (ext4, xfs, btrfs and ocfs2 filesystems), xfs_mkfile (almost any filesystems, comes in xfsprogs package), mkfile (for Unix-like systems like Solaris, Mac OS). System debugging For web debugging, curl and curl -I are handy, or their wget equivalents, or the more modern httpie. To know current cpu/disk status, the classic tools are top (or the better htop), iostat, and iotop. Use iostat -mxz 15 for basic CPU and detailed per-partition disk stats and performance insight. For network connection details, use netstat and ss. For a quick overview of what's happening on a system, dstat is especially useful. For broadest overview with details, use glances. To know memory status, run and understand the output of free and vmstat. In particular, be aware the \"cached\" value is memory held by the Linux kernel as file cache, so effectively counts toward the \"free\" value. Java system debugging is a different kettle of fish, but a simple trick on Oracle's and some other JVMs is that you can run kill -3 <pid> and a full stack trace and heap summary (including generational garbage collection details, which can be highly informative) will be dumped to stderr/logs. The JDK's jps, jstat, jstack, jmap are useful. SJK tools are more advanced. Use mtr as a better traceroute, to identify network issues. For looking at why a disk is full, ncdu saves time over the usual commands like du -sh *. To find which socket or process is using bandwidth, try iftop or nethogs. The ab tool (comes with Apache) is helpful for quick-and-dirty checking of web server performance. For more complex load testing, try siege. For more serious network debugging, wireshark, tshark, or ngrep. Know about strace and ltrace. These can be helpful if a program is failing, hanging, or crashing, and you don't know why, or if you want to get a general idea of performance. Note the profiling option (-c), and the ability to attach to a running process (-p). Use trace child option (-f) to avoid missing important calls. Know about ldd to check shared libraries etc but never run it on untrusted files. Know how to connect to a running process with gdb and get its stack traces. Use /proc. It's amazingly helpful sometimes when debugging live problems. Examples: /proc/cpuinfo, /proc/meminfo, /proc/cmdline, /proc/xxx/cwd, /proc/xxx/exe, /proc/xxx/fd/, /proc/xxx/smaps (where xxx is the process id or pid). When debugging why something went wrong in the past, sar can be very helpful. It shows historic statistics on CPU, memory, network, etc. For deeper systems and performance analyses, look at stap (SystemTap), perf, and sysdig. Check what OS you're on with uname or uname -a (general Unix/kernel info) or lsb_release -a (Linux distro info). Use dmesg whenever something's acting really funny (it could be hardware or driver issues). If you delete a file and it doesn't free up expected disk space as reported by du, check whether the file is in use by a process: lsof | grep deleted | grep \"filename-of-my-big-file\" One-liners A few examples of piecing together commands: It is remarkably helpful sometimes that you can do set intersection, union, and difference of text files via sort/uniq. Suppose a and b are text files that are already uniqued. This is fast, and works on files of arbitrary size, up to many gigabytes. (Sort is not limited by memory, though you may need to use the -T option if /tmp is on a small root partition.) See also the note about LC_ALL above and sort's -u option (left out for clarity below). sort a b | uniq > c # c is a union b sort a b | uniq -d > c # c is a intersect b sort a b b | uniq -u > c # c is set difference a - b Pretty-print two JSON files, normalizing their syntax, then coloring and paginating the result: diff <(jq --sort-keys . < file1.json) <(jq --sort-keys . < file2.json) | colordiff | less -R Use grep . * to quickly examine the contents of all files in a directory (so each line is paired with the filename), or head -100 * (so each file has a heading). This can be useful for directories filled with config settings like those in /sys, /proc, /etc. Summing all numbers in the third column of a text file (this is probably 3X faster and 3X less code than equivalent Python): awk '{ x += $3 } END { print x }' myfile To see sizes/dates on a tree of files, this is like a recursive ls -l but is easier to read than ls -lR: Say you have a text file, like a web server log, and a certain value that appears on some lines, such as an acct_id parameter that is present in the URL. If you want a tally of how many requests for each acct_id: egrep -o 'acct_id=[0-9]+' access.log | cut -d= -f2 | sort | uniq -c | sort -rn To continuously monitor changes, use watch, e.g. check changes to files in a directory with watch -d -n 2 'ls -rtlh | tail' or to network settings while troubleshooting your wifi settings with watch -d -n 2 ifconfig. Run this function to get a random tip from this document (parses Markdown and extracts an item): function taocl() { curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md | sed '/cowsay[.]png/d' | pandoc -f markdown -t html | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v \"(html/body/ul/li[count(p)>0])[$RANDOM mod last()+1]\" | xmlstarlet unesc | fmt -80 | iconv -t US } Obscure but useful expr: perform arithmetic or boolean operations or evaluate regular expressions m4: simple macro processor yes: print a string a lot cal: nice calendar env: run a command (useful in scripts) printenv: print out environment variables (useful in debugging and scripts) look: find English words (or lines in a file) beginning with a string cut, paste and join: data manipulation fmt: format text paragraphs pr: format text into pages/columns fold: wrap lines of text column: format text fields into aligned, fixed-width columns or tables expand and unexpand: convert between tabs and spaces nl: add line numbers seq: print numbers bc: calculator factor: factor integers gpg: encrypt and sign files toe: table of terminfo entries nc: network debugging and data transfer socat: socket relay and tcp port forwarder (similar to netcat) slurm: network traffic visualization dd: moving data between files or devices file: identify type of a file tree: display directories and subdirectories as a nesting tree; like ls but recursive stat: file info time: execute and time a command timeout: execute a command for specified amount of time and stop the process when the specified amount of time completes. lockfile: create semaphore file that can only be removed by rm -f logrotate: rotate, compress and mail logs. watch: run a command repeatedly, showing results and/or highlighting changes when-changed: runs any command you specify whenever it sees file changed. See inotifywait and entr as well. tac: print files in reverse comm: compare sorted files line by line strings: extract text from binary files tr: character translation or manipulation iconv or uconv: conversion for text encodings split and csplit: splitting files sponge: read all input before writing it, useful for reading from then writing to the same file, e.g., grep -v something some-file | sponge some-file units: unit conversions and calculations; converts furlongs per fortnight to twips per blink (see also /usr/share/units/definitions.units) apg: generates random passwords xz: high-ratio file compression ldd: dynamic library info nm: symbols from object files ab or wrk: benchmarking web servers strace: system call debugging mtr: better traceroute for network debugging cssh: visual concurrent shell rsync: sync files and folders over SSH or in local file system wireshark and tshark: packet capture and network debugging ngrep: grep for the network layer host and dig: DNS lookups lsof: process file descriptor and socket info dstat: useful system stats glances: high level, multi-subsystem overview iostat: Disk usage stats mpstat: CPU usage stats vmstat: Memory usage stats htop: improved version of top last: login history w: who's logged on id: user/group identity info sar: historic system stats iftop or nethogs: network utilization by socket or process ss: socket statistics dmesg: boot and system error messages sysctl: view and configure Linux kernel parameters at run time hdparm: SATA/ATA disk manipulation/performance lsblk: list block devices: a tree view of your disks and disk partitions lshw, lscpu, lspci, lsusb, dmidecode: hardware information, including CPU, BIOS, RAID, graphics, devices, etc. lsmod and modinfo: List and show details of kernel modules. fortune, ddate, and sl: um, well, it depends on whether you consider steam locomotives and Zippy quotations \"useful\" macOS only These are items relevant only on macOS. Package management with brew (Homebrew) and/or port (MacPorts). These can be used to install on macOS many of the above commands. Copy output of any command to a desktop app with pbcopy and paste input from one with pbpaste. To enable the Option key in macOS Terminal as an alt key (such as used in the commands above like alt-b, alt-f, etc.), open Preferences -> Profiles -> Keyboard and select \"Use Option as Meta key\". To open a file with a desktop app, use open or open -a /Applications/Whatever.app. Spotlight: Search files with mdfind and list metadata (such as photo EXIF info) with mdls. Be aware macOS is based on BSD Unix, and many commands (for example ps, ls, tail, awk, sed) have many subtle variations from Linux, which is largely influenced by System V-style Unix and GNU tools. You can often tell the difference by noting a man page has the heading \"BSD General Commands Manual.\" In some cases GNU versions can be installed, too (such as gawk and gsed for GNU awk and sed). If writing cross-platform Bash scripts, avoid such commands (for example, consider Python or perl) or test carefully. To get macOS release information, use sw_vers. Windows only These items are relevant only on Windows. Ways to obtain Unix tools under Windows Access the power of the Unix shell under Microsoft Windows by installing Cygwin. Most of the things described in this document will work out of the box. On Windows 10, you can use Windows Subsystem for Linux (WSL), which provides a familiar Bash environment with Unix command line utilities. If you mainly want to use GNU developer tools (such as GCC) on Windows, consider MinGW and its MSYS package, which provides utilities such as bash, gawk, make and grep. MSYS doesn't have all the features compared to Cygwin. MinGW is particularly useful for creating native Windows ports of Unix tools. Another option to get Unix look and feel under Windows is Cash. Note that only very few Unix commands and command-line options are available in this environment. Useful Windows command-line tools You can perform and script most Windows system administration tasks from the command line by learning and using wmic. Native command-line Windows networking tools you may find useful include ping, ipconfig, tracert, and netstat. You can perform many useful Windows tasks by invoking the Rundll32 command. Cygwin tips and tricks Install additional Unix programs with the Cygwin's package manager. Use mintty as your command-line window. Access the Windows clipboard through /dev/clipboard. Run cygstart to open an arbitrary file through its registered application. Access the Windows registry with regtool. Note that a C:\\ Windows drive path becomes /cygdrive/c under Cygwin, and that Cygwin's / appears under C:\\cygwin on Windows. Convert between Cygwin and Windows-style file paths with cygpath. This is most useful in scripts that invoke Windows programs. More resources awesome-shell: A curated list of shell tools and resources. awesome-osx-command-line: A more in-depth guide for the macOS command line. Strict mode for writing better shell scripts. shellcheck: A shell script static analysis tool. Essentially, lint for bash/sh/zsh. Filenames and Pathnames in Shell: The sadly complex minutiae on how to handle filenames correctly in shell scripts. Data Science at the Command Line: More commands and tools helpful for doing data science, from the book of the same name Disclaimer With the exception of very small tasks, code is written so others can read it. With power comes responsibility. The fact you can do something in Bash doesn't necessarily mean you should! ;) License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. ",
          "> Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long<p>Skimming <i>The Grapes of Wrath</i> would be shorter. Thank you, but no thank you.",
          "<i>Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.)</i><p>Huh?! Unlikely?!"],
        "story_type":["Normal"],
        "url":"https://github.com/jlevy/the-art-of-command-line",
        "comments.comment_id":[19989314,
          19989338],
        "comments.comment_author":["smith-kyle",
          "molteanu"],
        "comments.comment_descendants":[8,
          4],
        "comments.comment_time":["2019-05-23T07:16:13Z",
          "2019-05-23T07:21:38Z"],
        "comments.comment_text":["> Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long<p>Skimming <i>The Grapes of Wrath</i> would be shorter. Thank you, but no thank you.",
          "<i>Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.)</i><p>Huh?! Unlikely?!"],
        "id":"af8626d2-188b-44bd-b74a-1e5cf0dd9e95",
        "url_text":" etina Deutsch English Espaol Franais Indonesia Italiano polski Portugus Romn Slovenina The Art of Command Line Note: I'm planning to revise this and looking for a new co-author to help with expanding this into a more comprehensive guide. While it's very popular, it could be broader and a bit deeper. If you like to write and are close to being an expert on this material and willing to consider helping, please drop me a note at josh (0x40) holloway.com. jlevy, Holloway. Thank you! Meta Basics Everyday use Processing files and data System debugging One-liners Obscure but useful macOS only Windows only More resources Disclaimer Fluency on the command line is a skill often neglected or considered arcane, but it improves your flexibility and productivity as an engineer in both obvious and subtle ways. This is a selection of notes and tips on using the command-line that we've found useful when working on Linux. Some tips are elementary, and some are fairly specific, sophisticated, or obscure. This page is not long, but if you can use and recall all the items here, you know a lot. This work is the result of many authors and translators. Some of this originally appeared on Quora, but it has since moved to GitHub, where people more talented than the original author have made numerous improvements. Please submit a question if you have a question related to the command line. Please contribute if you see an error or something that could be better! Meta Scope: This guide is for both beginners and experienced users. The goals are breadth (everything important), specificity (give concrete examples of the most common case), and brevity (avoid things that aren't essential or digressions you can easily look up elsewhere). Every tip is essential in some situation or significantly saves time over alternatives. This is written for Linux, with the exception of the \"macOS only\" and \"Windows only\" sections. Many of the other items apply or can be installed on other Unices or macOS (or even Cygwin). The focus is on interactive Bash, though many tips apply to other shells and to general Bash scripting. It includes both \"standard\" Unix commands as well as ones that require special package installs -- so long as they are important enough to merit inclusion. Notes: To keep this to one page, content is implicitly included by reference. You're smart enough to look up more detail elsewhere once you know the idea or command to Google. Use apt, yum, dnf, pacman, pip or brew (as appropriate) to install new programs. Use Explainshell to get a helpful breakdown of what commands, options, pipes etc. do. Basics Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long. Alternate shells can be nice, but Bash is powerful and always available (learning only zsh, fish, etc., while tempting on your own laptop, restricts you in many situations, such as using existing servers). Learn at least one text-based editor well. The nano editor is one of the simplest for basic editing (opening, editing, saving, searching). However, for the power user in a text terminal, there is no substitute for Vim (vi), the hard-to-learn but venerable, fast, and full-featured editor. Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.) Finding documentation: Know how to read official documentation with man (for the inquisitive, man man lists the section numbers, e.g. 1 is \"regular\" commands, 5 is files/conventions, and 8 are for administration). Find man pages with apropos. Know that some commands are not executables, but Bash builtins, and that you can get help on them with help and help -d. You can find out whether a command is an executable, shell builtin or an alias by using type command. curl cheat.sh/command will give a brief \"cheat sheet\" with common examples of how to use a shell command. Learn about redirection of output and input using > and < and pipes using |. Know > overwrites the output file and >> appends. Learn about stdout and stderr. Learn about file glob expansion with * (and perhaps ? and [...]) and quoting and the difference between double \" and single ' quotes. (See more on variable expansion below.) Be familiar with Bash job management: &, ctrl-z, ctrl-c, jobs, fg, bg, kill, etc. Know ssh, and the basics of passwordless authentication, via ssh-agent, ssh-add, etc. Basic file management: ls and ls -l (in particular, learn what every column in ls -l means), less, head, tail and tail -f (or even better, less +F), ln and ln -s (learn the differences and advantages of hard versus soft links), chown, chmod, du (for a quick summary of disk usage: du -hs *). For filesystem management, df, mount, fdisk, mkfs, lsblk. Learn what an inode is (ls -i or df -i). Basic network management: ip or ifconfig, dig, traceroute, route. Learn and use a version control management system, such as git. Know regular expressions well, and the various flags to grep/egrep. The -i, -o, -v, -A, -B, and -C options are worth knowing. Learn to use apt-get, yum, dnf or pacman (depending on distro) to find and install packages. And make sure you have pip to install Python-based command-line tools (a few below are easiest to install via pip). Everyday use In Bash, use Tab to complete arguments or list all available commands and ctrl-r to search through command history (after pressing, type to search, press ctrl-r repeatedly to cycle through more matches, press Enter to execute the found command, or hit the right arrow to put the result in the current line to allow editing). In Bash, use ctrl-w to delete the last word, and ctrl-u to delete the content from current cursor back to the start of the line. Use alt-b and alt-f to move by word, ctrl-a to move cursor to beginning of line, ctrl-e to move cursor to end of line, ctrl-k to kill to the end of the line, ctrl-l to clear the screen. See man readline for all the default keybindings in Bash. There are a lot. For example alt-. cycles through previous arguments, and alt-* expands a glob. Alternatively, if you love vi-style key-bindings, use set -o vi (and set -o emacs to put it back). For editing long commands, after setting your editor (for example export EDITOR=vim), ctrl-x ctrl-e will open the current command in an editor for multi-line editing. Or in vi style, escape-v. To see recent commands, use history. Follow with !n (where n is the command number) to execute again. There are also many abbreviations you can use, the most useful probably being !$ for last argument and !! for last command (see \"HISTORY EXPANSION\" in the man page). However, these are often easily replaced with ctrl-r and alt-.. Go to your home directory with cd. Access files relative to your home directory with the ~ prefix (e.g. ~/.bashrc). In sh scripts refer to the home directory as $HOME. To go back to the previous working directory: cd -. If you are halfway through typing a command but change your mind, hit alt-# to add a # at the beginning and enter it as a comment (or use ctrl-a, #, enter). You can then return to it later via command history. Use xargs (or parallel). It's very powerful. Note you can control how many items execute per line (-L) as well as parallelism (-P). If you're not sure if it'll do the right thing, use xargs echo first. Also, -I{} is handy. Examples: find . -name '*.py' | xargs grep some_function cat hosts | xargs -I{} ssh root@{} hostname pstree -p is a helpful display of the process tree. Use pgrep and pkill to find or signal processes by name (-f is helpful). Know the various signals you can send processes. For example, to suspend a process, use kill -STOP [pid]. For the full list, see man 7 signal Use nohup or disown if you want a background process to keep running forever. Check what processes are listening via netstat -lntp or ss -plat (for TCP; add -u for UDP) or lsof -iTCP -sTCP:LISTEN -P -n (which also works on macOS). See also lsof and fuser for open sockets and files. See uptime or w to know how long the system has been running. Use alias to create shortcuts for commonly used commands. For example, alias ll='ls -latr' creates a new alias ll. Save aliases, shell settings, and functions you commonly use in ~/.bashrc, and arrange for login shells to source it. This will make your setup available in all your shell sessions. Put the settings of environment variables as well as commands that should be executed when you login in ~/.bash_profile. Separate configuration will be needed for shells you launch from graphical environment logins and cron jobs. Synchronize your configuration files (e.g. .bashrc and .bash_profile) among various computers with Git. Understand that care is needed when variables and filenames include whitespace. Surround your Bash variables with quotes, e.g. \"$FOO\". Prefer the -0 or -print0 options to enable null characters to delimit filenames, e.g. locate -0 pattern | xargs -0 ls -al or find / -print0 -type d | xargs -0 ls -al. To iterate on filenames containing whitespace in a for loop, set your IFS to be a newline only using IFS=$'\\n'. In Bash scripts, use set -x (or the variant set -v, which logs raw input, including unexpanded variables and comments) for debugging output. Use strict modes unless you have a good reason not to: Use set -e to abort on errors (nonzero exit code). Use set -u to detect unset variable usages. Consider set -o pipefail too, to abort on errors within pipes (though read up on it more if you do, as this topic is a bit subtle). For more involved scripts, also use trap on EXIT or ERR. A useful habit is to start a script like this, which will make it detect and abort on common errors and print a message: set -euo pipefail trap \"echo 'error: Script failed: see failed command above'\" ERR In Bash scripts, subshells (written with parentheses) are convenient ways to group commands. A common example is to temporarily move to a different working directory, e.g. # do something in current dir (cd /some/other/dir && other-command) # continue in original dir In Bash, note there are lots of kinds of variable expansion. Checking a variable exists: ${name:?error message}. For example, if a Bash script requires a single argument, just write input_file=${1:?usage: $0 input_file}. Using a default value if a variable is empty: ${name:-default}. If you want to have an additional (optional) parameter added to the previous example, you can use something like output_file=${2:-logfile}. If $2 is omitted and thus empty, output_file will be set to logfile. Arithmetic expansion: i=$(( (i + 1) % 5 )). Sequences: {1..10}. Trimming of strings: ${var%suffix} and ${var#prefix}. For example if var=foo.pdf, then echo ${var%.pdf}.txt prints foo.txt. Brace expansion using {...} can reduce having to re-type similar text and automate combinations of items. This is helpful in examples like mv foo.{txt,pdf} some-dir (which moves both files), cp somefile{,.bak} (which expands to cp somefile somefile.bak) or mkdir -p test-{a,b,c}/subtest-{1,2,3} (which expands all possible combinations and creates a directory tree). Brace expansion is performed before any other expansion. The order of expansions is: brace expansion; tilde expansion, parameter and variable expansion, arithmetic expansion, and command substitution (done in a left-to-right fashion); word splitting; and filename expansion. (For example, a range like {1..20} cannot be expressed with variables using {$a..$b}. Use seq or a for loop instead, e.g., seq $a $b or for((i=a; i<=b; i++)); do ... ; done.) The output of a command can be treated like a file via <(some command) (known as process substitution). For example, compare local /etc/hosts with a remote one: diff /etc/hosts <(ssh somehost cat /etc/hosts) When writing scripts you may want to put all of your code in curly braces. If the closing brace is missing, your script will be prevented from executing due to a syntax error. This makes sense when your script is going to be downloaded from the web, since it prevents partially downloaded scripts from executing: A \"here document\" allows redirection of multiple lines of input as if from a file: cat <<EOF input on multiple lines EOF In Bash, redirect both standard output and standard error via: some-command >logfile 2>&1 or some-command &>logfile. Often, to ensure a command does not leave an open file handle to standard input, tying it to the terminal you are in, it is also good practice to add </dev/null. Use man ascii for a good ASCII table, with hex and decimal values. For general encoding info, man unicode, man utf-8, and man latin1 are helpful. Use screen or tmux to multiplex the screen, especially useful on remote ssh sessions and to detach and re-attach to a session. byobu can enhance screen or tmux by providing more information and easier management. A more minimal alternative for session persistence only is dtach. In ssh, knowing how to port tunnel with -L or -D (and occasionally -R) is useful, e.g. to access web sites from a remote server. It can be useful to make a few optimizations to your ssh configuration; for example, this ~/.ssh/config contains settings to avoid dropped connections in certain network environments, uses compression (which is helpful with scp over low-bandwidth connections), and multiplex channels to the same server with a local control file: TCPKeepAlive=yes ServerAliveInterval=15 ServerAliveCountMax=6 Compression=yes ControlMaster auto ControlPath /tmp/%r@%h:%p ControlPersist yes A few other options relevant to ssh are security sensitive and should be enabled with care, e.g. per subnet or host or in trusted networks: StrictHostKeyChecking=no, ForwardAgent=yes Consider mosh an alternative to ssh that uses UDP, avoiding dropped connections and adding convenience on the road (requires server-side setup). To get the permissions on a file in octal form, which is useful for system configuration but not available in ls and easy to bungle, use something like stat -c '%A %a %n' /etc/timezone For interactive selection of values from the output of another command, use percol or fzf. For interaction with files based on the output of another command (like git), use fpp (PathPicker). For a simple web server for all files in the current directory (and subdirs), available to anyone on your network, use: python -m SimpleHTTPServer 7777 (for port 7777 and Python 2) and python -m http.server 7777 (for port 7777 and Python 3). For running a command as another user, use sudo. Defaults to running as root; use -u to specify another user. Use -i to login as that user (you will be asked for your password). For switching the shell to another user, use su username or su - username. The latter with \"-\" gets an environment as if another user just logged in. Omitting the username defaults to root. You will be asked for the password of the user you are switching to. Know about the 128K limit on command lines. This \"Argument list too long\" error is common when wildcard matching large numbers of files. (When this happens alternatives like find and xargs may help.) For a basic calculator (and of course access to Python in general), use the python interpreter. For example, Processing files and data To locate a file by name in the current directory, find . -iname '*something*' (or similar). To find a file anywhere by name, use locate something (but bear in mind updatedb may not have indexed recently created files). For general searching through source or data files, there are several options more advanced or faster than grep -r, including (in rough order from older to newer) ack, ag (\"the silver searcher\"), and rg (ripgrep). To convert HTML to text: lynx -dump -stdin For Markdown, HTML, and all kinds of document conversion, try pandoc. For example, to convert a Markdown document to Word format: pandoc README.md --from markdown --to docx -o temp.docx If you must handle XML, xmlstarlet is old but good. For JSON, use jq. For interactive use, also see jid and jiq. For YAML, use shyaml. For Excel or CSV files, csvkit provides in2csv, csvcut, csvjoin, csvgrep, etc. For Amazon S3, s3cmd is convenient and s4cmd is faster. Amazon's aws and the improved saws are essential for other AWS-related tasks. Know about sort and uniq, including uniq's -u and -d options -- see one-liners below. See also comm. Know about cut, paste, and join to manipulate text files. Many people use cut but forget about join. Know about wc to count newlines (-l), characters (-m), words (-w) and bytes (-c). Know about tee to copy from stdin to a file and also to stdout, as in ls -al | tee file.txt. For more complex calculations, including grouping, reversing fields, and statistical calculations, consider datamash. Know that locale affects a lot of command line tools in subtle ways, including sorting order (collation) and performance. Most Linux installations will set LANG or other locale variables to a local setting like US English. But be aware sorting will change if you change locale. And know i18n routines can make sort or other commands run many times slower. In some situations (such as the set operations or uniqueness operations below) you can safely ignore slow i18n routines entirely and use traditional byte-based sort order, using export LC_ALL=C. You can set a specific command's environment by prefixing its invocation with the environment variable settings, as in TZ=Pacific/Fiji date. Know basic awk and sed for simple data munging. See One-liners for examples. To replace all occurrences of a string in place, in one or more files: perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt To rename multiple files and/or search and replace within files, try repren. (In some cases the rename command also allows multiple renames, but be careful as its functionality is not the same on all Linux distributions.) # Full rename of filenames, directories, and contents foo -> bar: repren --full --preserve-case --from foo --to bar . # Recover backup files whatever.bak -> whatever: repren --renames --from '(.*)\\.bak' --to '\\1' *.bak # Same as above, using rename, if available: rename 's/\\.bak$//' *.bak As the man page says, rsync really is a fast and extraordinarily versatile file copying tool. It's known for synchronizing between machines but is equally useful locally. When security restrictions allow, using rsync instead of scp allows recovery of a transfer without restarting from scratch. It also is among the fastest ways to delete large numbers of files: mkdir empty && rsync -r --delete empty/ some-dir && rmdir some-dir For monitoring progress when processing files, use pv, pycp, pmonitor, progress, rsync --progress, or, for block-level copying, dd status=progress. Use shuf to shuffle or select random lines from a file. Know sort's options. For numbers, use -n, or -h for handling human-readable numbers (e.g. from du -h). Know how keys work (-t and -k). In particular, watch out that you need to write -k1,1 to sort by only the first field; -k1 means sort according to the whole line. Stable sort (sort -s) can be useful. For example, to sort first by field 2, then secondarily by field 1, you can use sort -k1,1 | sort -s -k2,2. If you ever need to write a tab literal in a command line in Bash (e.g. for the -t argument to sort), press ctrl-v [Tab] or write $'\\t' (the latter is better as you can copy/paste it). The standard tools for patching source code are diff and patch. See also diffstat for summary statistics of a diff and sdiff for a side-by-side diff. Note diff -r works for entire directories. Use diff -r tree1 tree2 | diffstat for a summary of changes. Use vimdiff to compare and edit files. For binary files, use hd, hexdump or xxd for simple hex dumps and bvi, hexedit or biew for binary editing. Also for binary files, strings (plus grep, etc.) lets you find bits of text. For binary diffs (delta compression), use xdelta3. To convert text encodings, try iconv. Or uconv for more advanced use; it supports some advanced Unicode things. For example: # Displays hex codes or actual names of characters (useful for debugging): uconv -f utf-8 -t utf-8 -x '::Any-Hex;' < input.txt uconv -f utf-8 -t utf-8 -x '::Any-Name;' < input.txt # Lowercase and removes all accents (by expanding and dropping them): uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] >; ::Any-NFC;' < input.txt > output.txt To split files into pieces, see split (to split by size) and csplit (to split by a pattern). Date and time: To get the current date and time in the helpful ISO 8601 format, use date -u +\"%Y-%m-%dT%H:%M:%SZ\" (other options are problematic). To manipulate date and time expressions, use dateadd, datediff, strptime etc. from dateutils. Use zless, zmore, zcat, and zgrep to operate on compressed files. File attributes are settable via chattr and offer a lower-level alternative to file permissions. For example, to protect against accidental file deletion the immutable flag: sudo chattr +i /critical/directory/or/file Use getfacl and setfacl to save and restore file permissions. For example: getfacl -R /some/path > permissions.txt setfacl --restore=permissions.txt To create empty files quickly, use truncate (creates sparse file), fallocate (ext4, xfs, btrfs and ocfs2 filesystems), xfs_mkfile (almost any filesystems, comes in xfsprogs package), mkfile (for Unix-like systems like Solaris, Mac OS). System debugging For web debugging, curl and curl -I are handy, or their wget equivalents, or the more modern httpie. To know current cpu/disk status, the classic tools are top (or the better htop), iostat, and iotop. Use iostat -mxz 15 for basic CPU and detailed per-partition disk stats and performance insight. For network connection details, use netstat and ss. For a quick overview of what's happening on a system, dstat is especially useful. For broadest overview with details, use glances. To know memory status, run and understand the output of free and vmstat. In particular, be aware the \"cached\" value is memory held by the Linux kernel as file cache, so effectively counts toward the \"free\" value. Java system debugging is a different kettle of fish, but a simple trick on Oracle's and some other JVMs is that you can run kill -3 <pid> and a full stack trace and heap summary (including generational garbage collection details, which can be highly informative) will be dumped to stderr/logs. The JDK's jps, jstat, jstack, jmap are useful. SJK tools are more advanced. Use mtr as a better traceroute, to identify network issues. For looking at why a disk is full, ncdu saves time over the usual commands like du -sh *. To find which socket or process is using bandwidth, try iftop or nethogs. The ab tool (comes with Apache) is helpful for quick-and-dirty checking of web server performance. For more complex load testing, try siege. For more serious network debugging, wireshark, tshark, or ngrep. Know about strace and ltrace. These can be helpful if a program is failing, hanging, or crashing, and you don't know why, or if you want to get a general idea of performance. Note the profiling option (-c), and the ability to attach to a running process (-p). Use trace child option (-f) to avoid missing important calls. Know about ldd to check shared libraries etc but never run it on untrusted files. Know how to connect to a running process with gdb and get its stack traces. Use /proc. It's amazingly helpful sometimes when debugging live problems. Examples: /proc/cpuinfo, /proc/meminfo, /proc/cmdline, /proc/xxx/cwd, /proc/xxx/exe, /proc/xxx/fd/, /proc/xxx/smaps (where xxx is the process id or pid). When debugging why something went wrong in the past, sar can be very helpful. It shows historic statistics on CPU, memory, network, etc. For deeper systems and performance analyses, look at stap (SystemTap), perf, and sysdig. Check what OS you're on with uname or uname -a (general Unix/kernel info) or lsb_release -a (Linux distro info). Use dmesg whenever something's acting really funny (it could be hardware or driver issues). If you delete a file and it doesn't free up expected disk space as reported by du, check whether the file is in use by a process: lsof | grep deleted | grep \"filename-of-my-big-file\" One-liners A few examples of piecing together commands: It is remarkably helpful sometimes that you can do set intersection, union, and difference of text files via sort/uniq. Suppose a and b are text files that are already uniqued. This is fast, and works on files of arbitrary size, up to many gigabytes. (Sort is not limited by memory, though you may need to use the -T option if /tmp is on a small root partition.) See also the note about LC_ALL above and sort's -u option (left out for clarity below). sort a b | uniq > c # c is a union b sort a b | uniq -d > c # c is a intersect b sort a b b | uniq -u > c # c is set difference a - b Pretty-print two JSON files, normalizing their syntax, then coloring and paginating the result: diff <(jq --sort-keys . < file1.json) <(jq --sort-keys . < file2.json) | colordiff | less -R Use grep . * to quickly examine the contents of all files in a directory (so each line is paired with the filename), or head -100 * (so each file has a heading). This can be useful for directories filled with config settings like those in /sys, /proc, /etc. Summing all numbers in the third column of a text file (this is probably 3X faster and 3X less code than equivalent Python): awk '{ x += $3 } END { print x }' myfile To see sizes/dates on a tree of files, this is like a recursive ls -l but is easier to read than ls -lR: Say you have a text file, like a web server log, and a certain value that appears on some lines, such as an acct_id parameter that is present in the URL. If you want a tally of how many requests for each acct_id: egrep -o 'acct_id=[0-9]+' access.log | cut -d= -f2 | sort | uniq -c | sort -rn To continuously monitor changes, use watch, e.g. check changes to files in a directory with watch -d -n 2 'ls -rtlh | tail' or to network settings while troubleshooting your wifi settings with watch -d -n 2 ifconfig. Run this function to get a random tip from this document (parses Markdown and extracts an item): function taocl() { curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md | sed '/cowsay[.]png/d' | pandoc -f markdown -t html | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v \"(html/body/ul/li[count(p)>0])[$RANDOM mod last()+1]\" | xmlstarlet unesc | fmt -80 | iconv -t US } Obscure but useful expr: perform arithmetic or boolean operations or evaluate regular expressions m4: simple macro processor yes: print a string a lot cal: nice calendar env: run a command (useful in scripts) printenv: print out environment variables (useful in debugging and scripts) look: find English words (or lines in a file) beginning with a string cut, paste and join: data manipulation fmt: format text paragraphs pr: format text into pages/columns fold: wrap lines of text column: format text fields into aligned, fixed-width columns or tables expand and unexpand: convert between tabs and spaces nl: add line numbers seq: print numbers bc: calculator factor: factor integers gpg: encrypt and sign files toe: table of terminfo entries nc: network debugging and data transfer socat: socket relay and tcp port forwarder (similar to netcat) slurm: network traffic visualization dd: moving data between files or devices file: identify type of a file tree: display directories and subdirectories as a nesting tree; like ls but recursive stat: file info time: execute and time a command timeout: execute a command for specified amount of time and stop the process when the specified amount of time completes. lockfile: create semaphore file that can only be removed by rm -f logrotate: rotate, compress and mail logs. watch: run a command repeatedly, showing results and/or highlighting changes when-changed: runs any command you specify whenever it sees file changed. See inotifywait and entr as well. tac: print files in reverse comm: compare sorted files line by line strings: extract text from binary files tr: character translation or manipulation iconv or uconv: conversion for text encodings split and csplit: splitting files sponge: read all input before writing it, useful for reading from then writing to the same file, e.g., grep -v something some-file | sponge some-file units: unit conversions and calculations; converts furlongs per fortnight to twips per blink (see also /usr/share/units/definitions.units) apg: generates random passwords xz: high-ratio file compression ldd: dynamic library info nm: symbols from object files ab or wrk: benchmarking web servers strace: system call debugging mtr: better traceroute for network debugging cssh: visual concurrent shell rsync: sync files and folders over SSH or in local file system wireshark and tshark: packet capture and network debugging ngrep: grep for the network layer host and dig: DNS lookups lsof: process file descriptor and socket info dstat: useful system stats glances: high level, multi-subsystem overview iostat: Disk usage stats mpstat: CPU usage stats vmstat: Memory usage stats htop: improved version of top last: login history w: who's logged on id: user/group identity info sar: historic system stats iftop or nethogs: network utilization by socket or process ss: socket statistics dmesg: boot and system error messages sysctl: view and configure Linux kernel parameters at run time hdparm: SATA/ATA disk manipulation/performance lsblk: list block devices: a tree view of your disks and disk partitions lshw, lscpu, lspci, lsusb, dmidecode: hardware information, including CPU, BIOS, RAID, graphics, devices, etc. lsmod and modinfo: List and show details of kernel modules. fortune, ddate, and sl: um, well, it depends on whether you consider steam locomotives and Zippy quotations \"useful\" macOS only These are items relevant only on macOS. Package management with brew (Homebrew) and/or port (MacPorts). These can be used to install on macOS many of the above commands. Copy output of any command to a desktop app with pbcopy and paste input from one with pbpaste. To enable the Option key in macOS Terminal as an alt key (such as used in the commands above like alt-b, alt-f, etc.), open Preferences -> Profiles -> Keyboard and select \"Use Option as Meta key\". To open a file with a desktop app, use open or open -a /Applications/Whatever.app. Spotlight: Search files with mdfind and list metadata (such as photo EXIF info) with mdls. Be aware macOS is based on BSD Unix, and many commands (for example ps, ls, tail, awk, sed) have many subtle variations from Linux, which is largely influenced by System V-style Unix and GNU tools. You can often tell the difference by noting a man page has the heading \"BSD General Commands Manual.\" In some cases GNU versions can be installed, too (such as gawk and gsed for GNU awk and sed). If writing cross-platform Bash scripts, avoid such commands (for example, consider Python or perl) or test carefully. To get macOS release information, use sw_vers. Windows only These items are relevant only on Windows. Ways to obtain Unix tools under Windows Access the power of the Unix shell under Microsoft Windows by installing Cygwin. Most of the things described in this document will work out of the box. On Windows 10, you can use Windows Subsystem for Linux (WSL), which provides a familiar Bash environment with Unix command line utilities. If you mainly want to use GNU developer tools (such as GCC) on Windows, consider MinGW and its MSYS package, which provides utilities such as bash, gawk, make and grep. MSYS doesn't have all the features compared to Cygwin. MinGW is particularly useful for creating native Windows ports of Unix tools. Another option to get Unix look and feel under Windows is Cash. Note that only very few Unix commands and command-line options are available in this environment. Useful Windows command-line tools You can perform and script most Windows system administration tasks from the command line by learning and using wmic. Native command-line Windows networking tools you may find useful include ping, ipconfig, tracert, and netstat. You can perform many useful Windows tasks by invoking the Rundll32 command. Cygwin tips and tricks Install additional Unix programs with the Cygwin's package manager. Use mintty as your command-line window. Access the Windows clipboard through /dev/clipboard. Run cygstart to open an arbitrary file through its registered application. Access the Windows registry with regtool. Note that a C:\\ Windows drive path becomes /cygdrive/c under Cygwin, and that Cygwin's / appears under C:\\cygwin on Windows. Convert between Cygwin and Windows-style file paths with cygpath. This is most useful in scripts that invoke Windows programs. More resources awesome-shell: A curated list of shell tools and resources. awesome-osx-command-line: A more in-depth guide for the macOS command line. Strict mode for writing better shell scripts. shellcheck: A shell script static analysis tool. Essentially, lint for bash/sh/zsh. Filenames and Pathnames in Shell: The sadly complex minutiae on how to handle filenames correctly in shell scripts. Data Science at the Command Line: More commands and tools helpful for doing data science, from the book of the same name Disclaimer With the exception of very small tasks, code is written so others can read it. With power comes responsibility. The fact you can do something in Bash doesn't necessarily mean you should! ;) License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. ",
        "_version_":1718939844013457408},
      {
        "story_id":[19238277],
        "story_author":["tosh"],
        "story_descendants":[8],
        "story_score":[71],
        "story_time":["2019-02-24T12:09:20Z"],
        "story_title":"Asami: Datomic-Like Graph Database",
        "search":["Asami: Datomic-Like Graph Database",
          "https://github.com/threatgrid/asami/blob/master/README.md",
          "asami A graph database, for Clojure and ClojureScript. The latest version is : Asami is a schemaless database, meaning that data may be inserted with no predefined schema. This flexibility has advantages and disadvantages. It is easier to load and evolve data over time without a schema. However, functionality like upsert and basic integrity checking is not available in the same way as with a graph with a predefined schema. Asami also follows an Open World Assumption model, in the same way that RDF does. In practice, this has very little effect on the database, beyond what being schemaless provides. If you are new to graph databases, then please read our Introduction page. Asami has a query API that looks very similar to a simplified Datomic. More details are available in the Query documentation. Features There are several other graph databases available in the Clojure ecosystem, with each having their own focus. Asami is characterized by the following: Clojure and ClojureScript: Asami runs identically in both systems. Schema-less: Asami does not require a schema to insert data. Query planner: Queries are analyzed to find an efficient execution plan. This can be turned off. Analytics: Supports fast graph traversal operations, such as transitive closures, and can identify subgraphs. Integrated with Loom: Asami graphs are valid Loom graphs, via Asami-Loom. Open World Assumption: Related to being schema-less, Asami borrows semantics from RDF to lean towards an open world model. Pluggable Storage: Like Datomic, storage in Asami can be implemented in multiple ways. There are currently 2 in-memory graph systems, and durable storage available on the JVM. Usage Installing Using Asami requires Clojure or ClojureScript. Asami can be made available to clojure by adding the following to a deps.edn file: { :deps { org.clojars.quoll/asami {:mvn/version \"2.2.2\"} } } This makes Asami available to a repl that is launched with the clj or clojure commands. Alternatively, Asami can be added for the Leiningen build tool by adding this to the :dependencies section of the project.clj file: [org.clojars.quoll/asami \"2.2.2\"] Important Note for databases before 2.1.0 Asami 2.1.0 now uses fewer files to manage data. This makes it incompatible with previous versions. To port data from an older store to a new one, use the asami.core/export-data function on a database on the previous version of Asami, and asami.core/import-data to load the data into a new connection. Running The Asami API tries to look a little like Datomic. Once a repl has been configured for Asami, the following can be copy/pasted to test the API: (require '[asami.core :as d]) ;; Create an in-memory database, named dbname (def db-uri \"asami:mem://dbname\") (d/create-database db-uri) ;; Create a connection to the database (def conn (d/connect db-uri)) ;; Data can be loaded into a database either as objects, or \"add\" statements: (def first-movies [{:movie/title \"Explorers\" :movie/genre \"adventure/comedy/family\" :movie/release-year 1985} {:movie/title \"Demolition Man\" :movie/genre \"action/sci-fi/thriller\" :movie/release-year 1993} {:movie/title \"Johnny Mnemonic\" :movie/genre \"cyber-punk/action\" :movie/release-year 1995} {:movie/title \"Toy Story\" :movie/genre \"animation/adventure\" :movie/release-year 1995}]) @(d/transact conn {:tx-data first-movies}) The transact operation returns an object that can be dereferenced (via clojure.core/deref or the @ macro) to provide information about the state of the database before and after the transaction. (A future in Clojure, or a delay in ClojureScript). Note that the transaction data can be provided as the :tx-data in a map object if other parameters are to be provided, or just as a raw sequence without the wrapping map. For more information about loading data and executing transact see the Transactions documentation. With the data loaded, a database value can be retrieved from the database and then queried. NB: The transact operation will be executed asynchronously on the JVM. Retrieving a database immediately after executing a transact will not retrieve the latest database. If the updated database is needed, then perform the deref operation as shown above, since this will wait until the operation is complete. (def db (d/db conn)) (d/q '[:find ?movie-title :where [?m :movie/title ?movie-title]] db) This returns a sequence of results, with each result being a sequence of the selected vars in the :find clause (just ?movie-title in this case): ([\"Explorers\"] [\"Demolition Man\"] [\"Johnny Mnemonic\"] [\"Toy Story\"]) A more complex query could be to get the title, year and genre for all movies after 1990: (d/q '[:find ?title ?year ?genre :where [?m :movie/title ?title] [?m :movie/release-year ?year] [?m :movie/genre ?genre] [(> ?year 1990)]] db) Entities found in a query can be extracted back out as objects using the entity function. For instance, the following is a repl session that looks up the movies released in 1995, and then gets the associated entities: ;; find the entity IDs. This variation in the :find clause asks for a list of just the ?m variable => (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db) (:tg/node-10327 :tg/node-10326) ;; get a single entity => (d/entity db :tg/node-10327) #:movie{:title \"Toy Story\", :genre \"animation/adventure\", :release-year 1995} ;; get all the entities from the query => (map #(d/entity db %) (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db)) (#:movie{:title \"Toy Story\", :genre \"animation/adventure\", :release-year 1995} #:movie{:title \"Johnny Mnemonic\", :genre \"cyber-punk/action\", :release-year 1995}) See the Query Documentation for more information on querying. Refer to the Entity Structure documentation to understand how entities are stored and how to construct queries for them. Local Storage The above code uses an in-memory database, specified with a URL of the form asami:mem://dbname. Creating a database on disk is done the same way, but with the URL scheme changed to asami:local://dbname. This would create a database in the dbname directory. Local databases do not use keywords as entity IDs, as keywords use up memory, and a local database could be gigabytes in size. Instead, these are InternalNode objects. These can be created with asami.graph/new-node, or by using the readers in asami.graph. For instance, if the above code were all done with a local graph instead of a memory graph: => (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db) (#a/n \"3\" #a/n \"4\") ;; get a single entity => (require '[asami.graph :as graph]) => (d/entity db (graph/new-node 4)) #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} ;; nodes can also be read from a string, with the appropriate reader => (set! *data-readers* graph/node-reader) => (d/entity db #a/n \"4\") #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} Updates The Open World Assumption allows each attribute to be multi-arity. In a Closed World database an object may be loaded to replace those attributes that can only appear once. To do the same thing with Asami, annotate the attributes to be replaced with a quote character at the end of the attribute name. => (def toy-story (d/q '[:find ?ts . :where [?ts :movie/title \"Toy Story\"]] db)) => (d/transact conn [{:db/id toy-story :movie/genre' \"animation/adventure/comedy\"}]) => (d/entity (d/db conn) toy-story) #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} Addressing nodes by their internal ID can be cumbersome. They can also be addressed by a :db/ident field if one is provided. (def tx (d/transact conn [{:db/ident \"sense\" :movie/title \"Sense and Sensibility\" :movie/genre \"drama/romance\" :movie/release-year 1996}])) ;; ask the transaction for the node ID, instead of querying (def sense (get (:tempids @tx) \"sense\")) (d/entity (d/db conn) sense) This returns the new movie. The :db/ident attribute does not appear in the entity: #:movie{:title \"Sense and Sensibility\", :genre \"drama/romance\", :release-year 1996} However, all of the attributes are still present in the graph: => (d/q '[:find ?a ?v :in $ ?s :where [?s ?a ?v]] (d/db conn) sense) ([:db/ident \"sense\"] [:movie/title \"Sense and Sensibility\"] [:movie/genre \"drama/romance\"] [:movie/release-year 1996]) The release year of this movie is incorrectly set to the release in the USA, and not the initial release. That can be updated using the :db/ident field: => (d/transact conn [{:db/ident \"sense\" :movie/release-year' 1995}]) => (d/entity (d/db conn) sense) #:movie{:title \"Sense and Sensibility\", :genre \"drama/romance\", :release-year 1995} More details are provided in Entity Updates. Analytics Asami also has some support for graph analytics. These all operate on the graph part of a database value, which can be retrieved with the asami.core/graph function. NB: local graphs on disk are not yet supported. These will be available soon. Start by populating a graph with the cast of \"The Flintstones\". So that we can refer to entities after they have been created, we can provide them with temporary ID values. These are just negative numbers, and can be used elsewhere in the transaction to refer to the same entity. We will also avoid the :tx-data wrapper in the transaction: (require '[asami.core :as d]) (require '[asami.analytics :as aa]) (def db-uri \"asami:mem://data\") (d/create-database db-uri) (def conn (d/connect db-uri)) (def data [{:db/id -1 :name \"Fred\"} {:db/id -2 :name \"Wilma\"} {:db/id -3 :name \"Pebbles\"} {:db/id -4 :name \"Dino\" :species \"Dinosaur\"} {:db/id -5 :name \"Barney\"} {:db/id -6 :name \"Betty\"} {:db/id -7 :name \"Bamm-Bamm\"} [:db/add -1 :spouse -2] [:db/add -2 :spouse -1] [:db/add -1 :child -3] [:db/add -2 :child -3] [:db/add -1 :pet -4] [:db/add -5 :spouse -6] [:db/add -6 :spouse -5] [:db/add -5 :child -7] [:db/add -6 :child -7]]) (d/transact conn data) Fred, Wilma, Pebbles, and Dino are all connected in a subgraph. Barney, Betty and Bamm-Bamm are connected in a separate subgraph. Let's find the subgraph from Fred: (def db (d/db conn)) (def graph (d/graph db)) (def fred (d/q '[:find ?e . :where [?e :name \"Fred\"]] db)) (aa/subgraph-from-node graph fred) This returns the nodes in the graph, but not the scalar values. For instance: #{:tg/node-10330 :tg/node-10329 :tg/node-10331 :tg/node-10332} These nodes can be used as the input to a query to get their names: => (d/q '[:find [?name ...] :in $ [?n ...] :where [?n :name ?name]] db (aa/subgraph-from-node graph fred)) (\"Fred\" \"Pebbles\" \"Dino\" \"Wilma\") We can also get all the subgraphs: => (count (aa/subgraphs graph)) 2 ;; execute the same query for each subgraph => (map (partial d/q '[:find [?name ...] :where [?e :name ?name]]) (aa/subgraphs graph)) ((\"Fred\" \"Wilma\" \"Pebbles\" \"Dino\") (\"Barney\" \"Betty\" \"Bamm-Bamm\")) Transitive Queries Asami supports transitive properties in queries. A property (or attribute) is treated as transitive if it is followed by a + or a * character. (d/q '[:find ?friend-of-a-friend :where [?person :name \"Fred\"] [?person :friend+ ?foaf] [?foaf :name ?friend-of-a-friend]] db) This will find all friends, and friends of friends for Fred. Loom Asami also implements Loom via the Asami-Loom package. Include the following dependency for your project: [org.clojars.quoll/asami-loom \"0.2.0\"] Graphs can now be analyzed with Loom functions. If functions are provided to Loom, then they can be used to provide labels for creating a visual graph. The following creates some simple queries to get the labels for edges and nodes: (require '[asami-loom.index]) (require '[asami-loom.label]) (require '[loom.io]) (defn edge-label [g s d] (str (d/q '[:find ?e . :in $ ?a ?b :where (or [?a ?e ?b] [?b ?e ?a])] g s d))) (defn node-label [g n] (or (d/q '[:find ?name . :where [?n :name ?name]] g n) \"-\")) ;; create a PDF of the graph (loom-io/view (graph db) :fmt :pdg :alg :sfpd :edge-label edge-label :node-label node-label) Command Line Tool A command line tool is available to load data into an Asami graph and query it. This requires GraalVM CE 21.1.0 or later, and the native-image executable. Leiningen needs to see GraalVM on the classpath first, so if there are problems with building, check to see if this is the case. To build from sources: lein with-profile native uberjar lein with-profile native native This will create a binary called asami in the target directory. Execute with the -? flag for help: $ ./target/asami -? Usage: asami URL [-f filename] [-e query] [--help | -?] -? | --help: This help URL: the URL of the database to use. Must start with asami:mem://, asami:multi:// or asami:local:// -f filename: loads the filename into the database. A filename of \"-\" will use stdin. Data defaults to EDN. Filenames ending in .json are treated as JSON. -e query: executes a query. \"-\" (the default) will read from stdin instead of a command line argument. Multiple queries can be specified as edn (vector of query vectors) or ; separated. Available EDN readers: internal nodes - #a/n \"node-id\" regex - #a/r \"[Tt]his is a (regex|regular expression)\" Example: Loading a json file, and querying for keys (attributes) that are strings with spaces in them: asami asami:mem://tmp -f data.json -e ':find ?a :where [?e ?a ?v][(string? ?a)][(re-find #a/r \" \" ?a)]' The command will also work on local stores, which means that they can be loaded once and then queried multiple times. License Copyright 2016-2021 Cisco Systems Copyright 2015-2021 Paula Gearon Portions of src/asami/cache.cljc are Copyright Rich Hickey Distributed under the Eclipse Public License either version 1.0 or (at your option) any later version. ",
          "Ah neat -- We're always looking for new graph databases to connect Graphistry to!<p>Any guidance on wire protocol (BOLT, TinkerPop, some custom HTTP, ...?) + early users? Guessing ThreatGrid..",
          "For CRUD stuff I have found datomic to be hard to maintain if your domain demands lots of attributes (even if trying to be as generic as possible), you need discipline (good docs) and lots of application code for checks and to force constraints. In sql/rdbms world you have all the atrributes nicely organized(within tables) with good check/constraints available at hand (waits for easy vs simple comment).<p>With datomic you get fleixble schema but at a high cost IMO."],
        "story_type":["Normal"],
        "url":"https://github.com/threatgrid/asami/blob/master/README.md",
        "comments.comment_id":[19239013,
          19239827],
        "comments.comment_author":["lmeyerov",
          "Scarbutt"],
        "comments.comment_descendants":[0,
          2],
        "comments.comment_time":["2019-02-24T15:53:16Z",
          "2019-02-24T18:30:57Z"],
        "comments.comment_text":["Ah neat -- We're always looking for new graph databases to connect Graphistry to!<p>Any guidance on wire protocol (BOLT, TinkerPop, some custom HTTP, ...?) + early users? Guessing ThreatGrid..",
          "For CRUD stuff I have found datomic to be hard to maintain if your domain demands lots of attributes (even if trying to be as generic as possible), you need discipline (good docs) and lots of application code for checks and to force constraints. In sql/rdbms world you have all the atrributes nicely organized(within tables) with good check/constraints available at hand (waits for easy vs simple comment).<p>With datomic you get fleixble schema but at a high cost IMO."],
        "id":"8935435f-0faf-4b53-839e-c8e02e40a12e",
        "url_text":"asami A graph database, for Clojure and ClojureScript. The latest version is : Asami is a schemaless database, meaning that data may be inserted with no predefined schema. This flexibility has advantages and disadvantages. It is easier to load and evolve data over time without a schema. However, functionality like upsert and basic integrity checking is not available in the same way as with a graph with a predefined schema. Asami also follows an Open World Assumption model, in the same way that RDF does. In practice, this has very little effect on the database, beyond what being schemaless provides. If you are new to graph databases, then please read our Introduction page. Asami has a query API that looks very similar to a simplified Datomic. More details are available in the Query documentation. Features There are several other graph databases available in the Clojure ecosystem, with each having their own focus. Asami is characterized by the following: Clojure and ClojureScript: Asami runs identically in both systems. Schema-less: Asami does not require a schema to insert data. Query planner: Queries are analyzed to find an efficient execution plan. This can be turned off. Analytics: Supports fast graph traversal operations, such as transitive closures, and can identify subgraphs. Integrated with Loom: Asami graphs are valid Loom graphs, via Asami-Loom. Open World Assumption: Related to being schema-less, Asami borrows semantics from RDF to lean towards an open world model. Pluggable Storage: Like Datomic, storage in Asami can be implemented in multiple ways. There are currently 2 in-memory graph systems, and durable storage available on the JVM. Usage Installing Using Asami requires Clojure or ClojureScript. Asami can be made available to clojure by adding the following to a deps.edn file: { :deps { org.clojars.quoll/asami {:mvn/version \"2.2.2\"} } } This makes Asami available to a repl that is launched with the clj or clojure commands. Alternatively, Asami can be added for the Leiningen build tool by adding this to the :dependencies section of the project.clj file: [org.clojars.quoll/asami \"2.2.2\"] Important Note for databases before 2.1.0 Asami 2.1.0 now uses fewer files to manage data. This makes it incompatible with previous versions. To port data from an older store to a new one, use the asami.core/export-data function on a database on the previous version of Asami, and asami.core/import-data to load the data into a new connection. Running The Asami API tries to look a little like Datomic. Once a repl has been configured for Asami, the following can be copy/pasted to test the API: (require '[asami.core :as d]) ;; Create an in-memory database, named dbname (def db-uri \"asami:mem://dbname\") (d/create-database db-uri) ;; Create a connection to the database (def conn (d/connect db-uri)) ;; Data can be loaded into a database either as objects, or \"add\" statements: (def first-movies [{:movie/title \"Explorers\" :movie/genre \"adventure/comedy/family\" :movie/release-year 1985} {:movie/title \"Demolition Man\" :movie/genre \"action/sci-fi/thriller\" :movie/release-year 1993} {:movie/title \"Johnny Mnemonic\" :movie/genre \"cyber-punk/action\" :movie/release-year 1995} {:movie/title \"Toy Story\" :movie/genre \"animation/adventure\" :movie/release-year 1995}]) @(d/transact conn {:tx-data first-movies}) The transact operation returns an object that can be dereferenced (via clojure.core/deref or the @ macro) to provide information about the state of the database before and after the transaction. (A future in Clojure, or a delay in ClojureScript). Note that the transaction data can be provided as the :tx-data in a map object if other parameters are to be provided, or just as a raw sequence without the wrapping map. For more information about loading data and executing transact see the Transactions documentation. With the data loaded, a database value can be retrieved from the database and then queried. NB: The transact operation will be executed asynchronously on the JVM. Retrieving a database immediately after executing a transact will not retrieve the latest database. If the updated database is needed, then perform the deref operation as shown above, since this will wait until the operation is complete. (def db (d/db conn)) (d/q '[:find ?movie-title :where [?m :movie/title ?movie-title]] db) This returns a sequence of results, with each result being a sequence of the selected vars in the :find clause (just ?movie-title in this case): ([\"Explorers\"] [\"Demolition Man\"] [\"Johnny Mnemonic\"] [\"Toy Story\"]) A more complex query could be to get the title, year and genre for all movies after 1990: (d/q '[:find ?title ?year ?genre :where [?m :movie/title ?title] [?m :movie/release-year ?year] [?m :movie/genre ?genre] [(> ?year 1990)]] db) Entities found in a query can be extracted back out as objects using the entity function. For instance, the following is a repl session that looks up the movies released in 1995, and then gets the associated entities: ;; find the entity IDs. This variation in the :find clause asks for a list of just the ?m variable => (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db) (:tg/node-10327 :tg/node-10326) ;; get a single entity => (d/entity db :tg/node-10327) #:movie{:title \"Toy Story\", :genre \"animation/adventure\", :release-year 1995} ;; get all the entities from the query => (map #(d/entity db %) (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db)) (#:movie{:title \"Toy Story\", :genre \"animation/adventure\", :release-year 1995} #:movie{:title \"Johnny Mnemonic\", :genre \"cyber-punk/action\", :release-year 1995}) See the Query Documentation for more information on querying. Refer to the Entity Structure documentation to understand how entities are stored and how to construct queries for them. Local Storage The above code uses an in-memory database, specified with a URL of the form asami:mem://dbname. Creating a database on disk is done the same way, but with the URL scheme changed to asami:local://dbname. This would create a database in the dbname directory. Local databases do not use keywords as entity IDs, as keywords use up memory, and a local database could be gigabytes in size. Instead, these are InternalNode objects. These can be created with asami.graph/new-node, or by using the readers in asami.graph. For instance, if the above code were all done with a local graph instead of a memory graph: => (d/q '[:find [?m ...] :where [?m :movie/release-year 1995]] db) (#a/n \"3\" #a/n \"4\") ;; get a single entity => (require '[asami.graph :as graph]) => (d/entity db (graph/new-node 4)) #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} ;; nodes can also be read from a string, with the appropriate reader => (set! *data-readers* graph/node-reader) => (d/entity db #a/n \"4\") #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} Updates The Open World Assumption allows each attribute to be multi-arity. In a Closed World database an object may be loaded to replace those attributes that can only appear once. To do the same thing with Asami, annotate the attributes to be replaced with a quote character at the end of the attribute name. => (def toy-story (d/q '[:find ?ts . :where [?ts :movie/title \"Toy Story\"]] db)) => (d/transact conn [{:db/id toy-story :movie/genre' \"animation/adventure/comedy\"}]) => (d/entity (d/db conn) toy-story) #:movie{:title \"Toy Story\", :genre \"animation/adventure/comedy\", :release-year 1995} Addressing nodes by their internal ID can be cumbersome. They can also be addressed by a :db/ident field if one is provided. (def tx (d/transact conn [{:db/ident \"sense\" :movie/title \"Sense and Sensibility\" :movie/genre \"drama/romance\" :movie/release-year 1996}])) ;; ask the transaction for the node ID, instead of querying (def sense (get (:tempids @tx) \"sense\")) (d/entity (d/db conn) sense) This returns the new movie. The :db/ident attribute does not appear in the entity: #:movie{:title \"Sense and Sensibility\", :genre \"drama/romance\", :release-year 1996} However, all of the attributes are still present in the graph: => (d/q '[:find ?a ?v :in $ ?s :where [?s ?a ?v]] (d/db conn) sense) ([:db/ident \"sense\"] [:movie/title \"Sense and Sensibility\"] [:movie/genre \"drama/romance\"] [:movie/release-year 1996]) The release year of this movie is incorrectly set to the release in the USA, and not the initial release. That can be updated using the :db/ident field: => (d/transact conn [{:db/ident \"sense\" :movie/release-year' 1995}]) => (d/entity (d/db conn) sense) #:movie{:title \"Sense and Sensibility\", :genre \"drama/romance\", :release-year 1995} More details are provided in Entity Updates. Analytics Asami also has some support for graph analytics. These all operate on the graph part of a database value, which can be retrieved with the asami.core/graph function. NB: local graphs on disk are not yet supported. These will be available soon. Start by populating a graph with the cast of \"The Flintstones\". So that we can refer to entities after they have been created, we can provide them with temporary ID values. These are just negative numbers, and can be used elsewhere in the transaction to refer to the same entity. We will also avoid the :tx-data wrapper in the transaction: (require '[asami.core :as d]) (require '[asami.analytics :as aa]) (def db-uri \"asami:mem://data\") (d/create-database db-uri) (def conn (d/connect db-uri)) (def data [{:db/id -1 :name \"Fred\"} {:db/id -2 :name \"Wilma\"} {:db/id -3 :name \"Pebbles\"} {:db/id -4 :name \"Dino\" :species \"Dinosaur\"} {:db/id -5 :name \"Barney\"} {:db/id -6 :name \"Betty\"} {:db/id -7 :name \"Bamm-Bamm\"} [:db/add -1 :spouse -2] [:db/add -2 :spouse -1] [:db/add -1 :child -3] [:db/add -2 :child -3] [:db/add -1 :pet -4] [:db/add -5 :spouse -6] [:db/add -6 :spouse -5] [:db/add -5 :child -7] [:db/add -6 :child -7]]) (d/transact conn data) Fred, Wilma, Pebbles, and Dino are all connected in a subgraph. Barney, Betty and Bamm-Bamm are connected in a separate subgraph. Let's find the subgraph from Fred: (def db (d/db conn)) (def graph (d/graph db)) (def fred (d/q '[:find ?e . :where [?e :name \"Fred\"]] db)) (aa/subgraph-from-node graph fred) This returns the nodes in the graph, but not the scalar values. For instance: #{:tg/node-10330 :tg/node-10329 :tg/node-10331 :tg/node-10332} These nodes can be used as the input to a query to get their names: => (d/q '[:find [?name ...] :in $ [?n ...] :where [?n :name ?name]] db (aa/subgraph-from-node graph fred)) (\"Fred\" \"Pebbles\" \"Dino\" \"Wilma\") We can also get all the subgraphs: => (count (aa/subgraphs graph)) 2 ;; execute the same query for each subgraph => (map (partial d/q '[:find [?name ...] :where [?e :name ?name]]) (aa/subgraphs graph)) ((\"Fred\" \"Wilma\" \"Pebbles\" \"Dino\") (\"Barney\" \"Betty\" \"Bamm-Bamm\")) Transitive Queries Asami supports transitive properties in queries. A property (or attribute) is treated as transitive if it is followed by a + or a * character. (d/q '[:find ?friend-of-a-friend :where [?person :name \"Fred\"] [?person :friend+ ?foaf] [?foaf :name ?friend-of-a-friend]] db) This will find all friends, and friends of friends for Fred. Loom Asami also implements Loom via the Asami-Loom package. Include the following dependency for your project: [org.clojars.quoll/asami-loom \"0.2.0\"] Graphs can now be analyzed with Loom functions. If functions are provided to Loom, then they can be used to provide labels for creating a visual graph. The following creates some simple queries to get the labels for edges and nodes: (require '[asami-loom.index]) (require '[asami-loom.label]) (require '[loom.io]) (defn edge-label [g s d] (str (d/q '[:find ?e . :in $ ?a ?b :where (or [?a ?e ?b] [?b ?e ?a])] g s d))) (defn node-label [g n] (or (d/q '[:find ?name . :where [?n :name ?name]] g n) \"-\")) ;; create a PDF of the graph (loom-io/view (graph db) :fmt :pdg :alg :sfpd :edge-label edge-label :node-label node-label) Command Line Tool A command line tool is available to load data into an Asami graph and query it. This requires GraalVM CE 21.1.0 or later, and the native-image executable. Leiningen needs to see GraalVM on the classpath first, so if there are problems with building, check to see if this is the case. To build from sources: lein with-profile native uberjar lein with-profile native native This will create a binary called asami in the target directory. Execute with the -? flag for help: $ ./target/asami -? Usage: asami URL [-f filename] [-e query] [--help | -?] -? | --help: This help URL: the URL of the database to use. Must start with asami:mem://, asami:multi:// or asami:local:// -f filename: loads the filename into the database. A filename of \"-\" will use stdin. Data defaults to EDN. Filenames ending in .json are treated as JSON. -e query: executes a query. \"-\" (the default) will read from stdin instead of a command line argument. Multiple queries can be specified as edn (vector of query vectors) or ; separated. Available EDN readers: internal nodes - #a/n \"node-id\" regex - #a/r \"[Tt]his is a (regex|regular expression)\" Example: Loading a json file, and querying for keys (attributes) that are strings with spaces in them: asami asami:mem://tmp -f data.json -e ':find ?a :where [?e ?a ?v][(string? ?a)][(re-find #a/r \" \" ?a)]' The command will also work on local stores, which means that they can be loaded once and then queried multiple times. License Copyright 2016-2021 Cisco Systems Copyright 2015-2021 Paula Gearon Portions of src/asami/cache.cljc are Copyright Rich Hickey Distributed under the Eclipse Public License either version 1.0 or (at your option) any later version. ",
        "_version_":1718939822089830400},
      {
        "story_id":[19470890],
        "story_author":["rayraegah"],
        "story_descendants":[14],
        "story_score":[177],
        "story_time":["2019-03-23T15:44:02Z"],
        "story_title":"Pulling JPEGs out of thin air (2014)",
        "search":["Pulling JPEGs out of thin air (2014)",
          "http://lcamtuf.blogspot.com/2014/11/pulling-jpegs-out-of-thin-air.html",
          "This is an interesting demonstration of the capabilities ofafl; I was actually pretty surprised that it worked!$ mkdir in_dir $ echo 'hello' >in_dir/hello $ ./afl-fuzz -i in_dir -o out_dir ./jpeg-9a/djpegIn essence, I created a text file containing just \"hello\" and asked the fuzzer to keep feeding it to a program that expects a JPEG image (djpegis a simple utility bundled with the ubiquitousIJG jpegimage library;libjpeg-turboshould also work). Of course, my input file does not resemble a valid picture, so it gets immediately rejected by the utility:$ ./djpeg '../out_dir/queue/id:000000,orig:hello' Not a JPEG file: starts with 0x68 0x65Such a fuzzing run would be normally completely pointless: there is essentially no chance that a \"hello\" could be ever turned into a valid JPEG by a traditional, format-agnostic fuzzer, since the probability that dozens of random tweaks would align just right is astronomically low.Luckily,afl-fuzzcan leverage lightweight assembly-level instrumentation to its advantage - and within a millisecond or so, it notices that although setting the first byte to0xffdoes not change the externally observable output, it triggers a slightly different internal code path in the tested app. Equipped with this information, it decides to use that test case as a seed for future fuzzing rounds:$ ./djpeg '../out_dir/queue/id:000001,src:000000,op:int8,pos:0,val:-1,+cov' Not a JPEG file: starts with 0xff 0x65When later working with that second-generation test case, the fuzzer almost immediately notices that setting the second byte to0xd8does something even more interesting:$ ./djpeg '../out_dir/queue/id:000004,src:000001,op:havoc,rep:16,+cov' Premature end of JPEG file JPEG datastream contains no imageAt this point, the fuzzer managed to synthesize the valid file header - and actually realized its significance. Using this output as the seed for the next round of fuzzing, it quickly starts getting deeper and deeper into the woods. Within several hundred generations and several hundred millionexecve()calls, it figures out more and more of the essential control structures that make a valid JPEG file - SOFs, Huffman tables, quantization tables, SOS markers, and so on:$ ./djpeg '../out_dir/queue/id:000008,src:000004,op:havoc,rep:2,+cov' Invalid JPEG file structure: two SOI markers ... $ ./djpeg '../out_dir/queue/id:001005,src:000262+000979,op:splice,rep:2' Quantization table 0x0e was not defined ... $ ./djpeg '../out_dir/queue/id:001282,src:001005+001270,op:splice,rep:2,+cov' >.tmp; ls -l .tmp -rw-r--r-- 1 lcamtuf lcamtuf 7069 Nov 7 09:29 .tmpThe first image, hit after about six hours on an 8-core system, looks very unassuming: it's a blank grayscale image, 3 pixels wide and 784 pixels tall. But the moment it is discovered, the fuzzer starts using the image as a seed - rapidly producing a wide array of more interesting pics for every new execution path:Of course, synthesizing a complete image out of thin air is an extreme example, and not necessarily a very practical one. But more prosaically, fuzzers are meant to stress-test every feature of the targeted program. With instrumented, generational fuzzing, lesser-known features (e.g., progressive, black-and-white, or arithmetic-coded JPEGs) can bediscovered and locked ontowithout requiring a giant, high-quality corpus of diverse test cases to seed the fuzzer with.The cool part of thelibjpegdemo is that it works without any special preparation: there is nothing special about the \"hello\" string, the fuzzer knows nothing about image parsing, and is not designed or fine-tuned to work with this particular library. There aren't even any command-line knobs to turn. You can throwafl-fuzzat many other types of parsers with similar results: with bash, it willwrite valid scripts; withgiflib, it will make GIFs; withfileutils, it will create and flag ELF files, Atari 68xxx executables, x86 boot sectors, and UTF-8 with BOM. In almost all cases, the performance impact of instrumentation is minimal, too.Of course, not all is roses; at its core,afl-fuzzis still a brute-force tool. This makes it simple, fast, and robust, but also means that certain types of atomically executed checks with a large search space may pose an insurmountable obstacle to the fuzzer; a good example of this may be:if (strcmp(header.magic_password, \"h4ck3d by p1gZ\")) goto terminate_now;In practical terms, this means thatafl-fuzzwon't have as much luck \"inventing\" PNG files or non-trivial HTML documents from scratch - and will need a starting point better than just \"hello\". To consistently deal with code constructs similar to the one shown above, a general-purpose fuzzer would need to understand the operation of the targeted binary on a wholly different level. There is some progress on this in the academia, but frameworks that can pull this off across diverse and complex codebases in a quick, easy, and reliable way are probably still years away.PS. Several folks asked me about symbolic execution and other inspirations forafl-fuzz; I put together some notes inthis doc. ",
          "Really cool! I recently happened to have a similar idea when I came across the rust-fuzz crate, which abstracts over afl, honggfuzz etc.<p>If you're interested, my notes are here: <a href=\"https://github.com/lachenmayer/insta-fuzz\" rel=\"nofollow\">https://github.com/lachenmayer/insta-fuzz</a><p>I ended up \"cheating\" a little bit by providing a valid JPEG header - I found that rust-fuzz seemed to take far too long to generate any valid JPEG at all by starting from eg. an empty file. But maybe I just wasn't patient enough, after all I was only running it on my laptop :)",
          "I've used this in the past (inspired by this) to produce some valid json.  Of course, a jpeg is at least pretty.  Valid JSON is still just boring old JSON."],
        "story_type":["Normal"],
        "url":"http://lcamtuf.blogspot.com/2014/11/pulling-jpegs-out-of-thin-air.html",
        "comments.comment_id":[19479183,
          19479307],
        "comments.comment_author":["lachenmayer",
          "Twirrim"],
        "comments.comment_descendants":[2,
          0],
        "comments.comment_time":["2019-03-24T22:40:06Z",
          "2019-03-24T23:14:42Z"],
        "comments.comment_text":["Really cool! I recently happened to have a similar idea when I came across the rust-fuzz crate, which abstracts over afl, honggfuzz etc.<p>If you're interested, my notes are here: <a href=\"https://github.com/lachenmayer/insta-fuzz\" rel=\"nofollow\">https://github.com/lachenmayer/insta-fuzz</a><p>I ended up \"cheating\" a little bit by providing a valid JPEG header - I found that rust-fuzz seemed to take far too long to generate any valid JPEG at all by starting from eg. an empty file. But maybe I just wasn't patient enough, after all I was only running it on my laptop :)",
          "I've used this in the past (inspired by this) to produce some valid json.  Of course, a jpeg is at least pretty.  Valid JSON is still just boring old JSON."],
        "id":"e9c2336e-4b7b-40c4-a3d2-f00ca166871d",
        "url_text":"This is an interesting demonstration of the capabilities ofafl; I was actually pretty surprised that it worked!$ mkdir in_dir $ echo 'hello' >in_dir/hello $ ./afl-fuzz -i in_dir -o out_dir ./jpeg-9a/djpegIn essence, I created a text file containing just \"hello\" and asked the fuzzer to keep feeding it to a program that expects a JPEG image (djpegis a simple utility bundled with the ubiquitousIJG jpegimage library;libjpeg-turboshould also work). Of course, my input file does not resemble a valid picture, so it gets immediately rejected by the utility:$ ./djpeg '../out_dir/queue/id:000000,orig:hello' Not a JPEG file: starts with 0x68 0x65Such a fuzzing run would be normally completely pointless: there is essentially no chance that a \"hello\" could be ever turned into a valid JPEG by a traditional, format-agnostic fuzzer, since the probability that dozens of random tweaks would align just right is astronomically low.Luckily,afl-fuzzcan leverage lightweight assembly-level instrumentation to its advantage - and within a millisecond or so, it notices that although setting the first byte to0xffdoes not change the externally observable output, it triggers a slightly different internal code path in the tested app. Equipped with this information, it decides to use that test case as a seed for future fuzzing rounds:$ ./djpeg '../out_dir/queue/id:000001,src:000000,op:int8,pos:0,val:-1,+cov' Not a JPEG file: starts with 0xff 0x65When later working with that second-generation test case, the fuzzer almost immediately notices that setting the second byte to0xd8does something even more interesting:$ ./djpeg '../out_dir/queue/id:000004,src:000001,op:havoc,rep:16,+cov' Premature end of JPEG file JPEG datastream contains no imageAt this point, the fuzzer managed to synthesize the valid file header - and actually realized its significance. Using this output as the seed for the next round of fuzzing, it quickly starts getting deeper and deeper into the woods. Within several hundred generations and several hundred millionexecve()calls, it figures out more and more of the essential control structures that make a valid JPEG file - SOFs, Huffman tables, quantization tables, SOS markers, and so on:$ ./djpeg '../out_dir/queue/id:000008,src:000004,op:havoc,rep:2,+cov' Invalid JPEG file structure: two SOI markers ... $ ./djpeg '../out_dir/queue/id:001005,src:000262+000979,op:splice,rep:2' Quantization table 0x0e was not defined ... $ ./djpeg '../out_dir/queue/id:001282,src:001005+001270,op:splice,rep:2,+cov' >.tmp; ls -l .tmp -rw-r--r-- 1 lcamtuf lcamtuf 7069 Nov 7 09:29 .tmpThe first image, hit after about six hours on an 8-core system, looks very unassuming: it's a blank grayscale image, 3 pixels wide and 784 pixels tall. But the moment it is discovered, the fuzzer starts using the image as a seed - rapidly producing a wide array of more interesting pics for every new execution path:Of course, synthesizing a complete image out of thin air is an extreme example, and not necessarily a very practical one. But more prosaically, fuzzers are meant to stress-test every feature of the targeted program. With instrumented, generational fuzzing, lesser-known features (e.g., progressive, black-and-white, or arithmetic-coded JPEGs) can bediscovered and locked ontowithout requiring a giant, high-quality corpus of diverse test cases to seed the fuzzer with.The cool part of thelibjpegdemo is that it works without any special preparation: there is nothing special about the \"hello\" string, the fuzzer knows nothing about image parsing, and is not designed or fine-tuned to work with this particular library. There aren't even any command-line knobs to turn. You can throwafl-fuzzat many other types of parsers with similar results: with bash, it willwrite valid scripts; withgiflib, it will make GIFs; withfileutils, it will create and flag ELF files, Atari 68xxx executables, x86 boot sectors, and UTF-8 with BOM. In almost all cases, the performance impact of instrumentation is minimal, too.Of course, not all is roses; at its core,afl-fuzzis still a brute-force tool. This makes it simple, fast, and robust, but also means that certain types of atomically executed checks with a large search space may pose an insurmountable obstacle to the fuzzer; a good example of this may be:if (strcmp(header.magic_password, \"h4ck3d by p1gZ\")) goto terminate_now;In practical terms, this means thatafl-fuzzwon't have as much luck \"inventing\" PNG files or non-trivial HTML documents from scratch - and will need a starting point better than just \"hello\". To consistently deal with code constructs similar to the one shown above, a general-purpose fuzzer would need to understand the operation of the targeted binary on a wholly different level. There is some progress on this in the academia, but frameworks that can pull this off across diverse and complex codebases in a quick, easy, and reliable way are probably still years away.PS. Several folks asked me about symbolic execution and other inspirations forafl-fuzz; I put together some notes inthis doc. ",
        "_version_":1718939831560568832},
      {
        "story_id":[21045987],
        "story_author":["dhruvkar"],
        "story_descendants":[25],
        "story_score":[73],
        "story_time":["2019-09-23T04:47:46Z"],
        "story_title":"Bedrock – A modular, WAN-replicated, blockchain-based database",
        "search":["Bedrock – A modular, WAN-replicated, blockchain-based database",
          "https://bedrockdb.com/",
          "Why Code Install Use Jobs Cache vs MySQL Replication Blockchain Multizone Chat Contact Bedrock Rock-solid distributed data Bedrock is a simple, modular, WAN-replicated, Blockchain-based data foundation for global-scale applications. Taking each of those in turn: Bedrock is simple. This means it exposes the fewest knobs necessary, with appropriate defaults at every layer. Bedrock is modular. This means its functionality is packaged into separate plugins that are decoupled and independently maintainable. Bedrock is WAN-replicated. This means it is designed to endure the myriad real-world problems that occur across slow, unreliable internet connections. Bedrock is Blockchain-based. This means it uses a private blockchain to synchronize and self organize. Bedrock is a data foundation. This means it is not just a simple database that responds to queries, but rather a platform on which data-processing applications (like databases, job queues, caches, etc) can be built. Bedrock is for global-scale applications. This means it is built to be deployed in a geo-redundant fashion spanning many datacenters around the world. Bedrock was built by Expensify, and is a networking and distributed transaction layer built atop SQLite, the fastest, most reliable, and most widely distributed database in the world. Why to use it If youre building a website or other online service, youve got to use something. Why use Bedrock rather than the alternatives? Weve provided a more detailed comparision against MySQL, but in general Bedrock is: Faster. This is true for networked queries using the Bedrock::DB plugin, but especially true for custom plugins you write yourself because SQLite is just a library that operates inside your processs memory space. That means when your plugin queries SQLite, it isnt serializing/deserializing over a network: its directly accessing the RAM of the database itself. This is great in a single node, but if you still want more (because who doesnt?) then install any number of nodes and load-balance reads across all of them. This means every CPU of every database server is available for parallel reads, each of which has direct access to the database RAM. Simpler. This is because Bedrock is written for modern hardware with large SSD-backed RAID drives and generous RAM file caches, and thereby doesnt mess with the zillion hacky tricks the other databases do to eke out high performance on largely obsolete hardware. This results in fewer esoteric knobs, and sane defaults that just work. More reliable. This is because Bedrocks synchronization engine supports active/active distributed transactions with automatic failover, and can be clustered not just inside a single datacenter, but across multiple datacenters spanning the internet. This means Bedrock continues functioning not only if a single node goes down, but even if you lose an entire datacenter. After all, it doesnt matter who you are using: your datacenter will fail, eventually. But you neednt fail along with it. More powerful. Most people dont realize just how powerful SQLite is. Indexes, triggers, foreign key constraints, native JSON support, expression indexes check the full list here. Youll be amazed, but thats just the start. On top of this Bedrock layers a robust plugin system, and includes a fully functional job queue and replicated cache all the basics you need for modern service design, wrapped into one simple package. Bedrock is not only production ready, but actively used by Expensifys many thousands of customers, and millions of users. (Curious why an expense reporting company built their own database? Read what the First Round Review has to say about it.) How to get it Bedrock can be compiled from source using the Expensify/Bedrock public repo, or installed with the commands below: Ubuntu Linux You can build from scratch as follows: # Clone out this repo: git clone https://github.com/Expensify/Bedrock.git # Install some dependencies sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt-get update sudo apt-get install build-essential gcc-9 g++-9 libpcre++-dev zlib1g-dev # Build it cd Bedrock make # Create an empty database (See: https://github.com/Expensify/Bedrock/issues/489) touch bedrock.db # Run it (press Ctrl^C to quit, or use -fork to make it run in the backgroud) ./bedrock # Connect to it in a different terminal using netcat nc localhost 8888 # Type \"Status\" and then enter twice to verify it's working # See here to use the default DB plugin: http://bedrockdb.com/db.html Arch Linux Copy/paste this command into your terminal: This will tansparently download the latest version from GitHub, compile it, package it up, and install it. MacOSX You can build from scratch as follows: # Clone out this repo: git clone https://github.com/Expensify/Bedrock.git # Install some dependencies with Brew (see: https://brew.sh/) brew update brew install gcc@6 # Configure PCRE to use C++17 and compile from source brew uninstall --ignore-dependencies pcre brew edit pcre # Add these to the end of the `system \"./configure\"` command: # \"--enable-cpp\", # \"--enable-pcre64\", # \"CXX=/usr/local/bin/g++-9\", # \"CXXFLAGS=--std=gnu++14\" brew install --build-from-source pcre # Build it cd Bedrock make # Create an empty database (See: https://github.com/Expensify/Bedrock/issues/489) touch bedrock.db # Run it (press Ctrl^C to quit, or use -fork to make it run in the backgroud) ./bedrock # Connect to it in a different terminal using netcat nc localhost 8888 # Type \"Status\" and then enter twice to verify it's working # See here to use the default DB plugin: http://bedrockdb.com/db.html How to use it Bedrock is so easy to use, youll think youre missing something. Once installed, Bedrock listens on localhost port 8888, and stores its database in /var/lib/bedrock. The easiest way to talk with Bedrock is using netcat as follows: $ nc localhost 8888 Query: SELECT 1 AS foo, 2 AS bar; That query can be any SQLite-compatible query including schema changes, foreign key constraints, partial indexes, native JSON expressions, or any of the tremendous amount of functionality SQLite offers. The result will be returned in an HTTP-like response format: 200 OK Content-Length: 16 foo | bar 1 | 2 By default, Bedrock optimizes the output for human consumption. If you are a robot, request JSON output: $ nc localhost 8888 Query query: SELECT 1 AS foo, 2 AS bar; format: json 200 OK Content-Length: 40 {\"headers\":[\"foo\",\"bar\"],\"rows\":[[1,2]]} Some people are creeped out by sockets, and prefer tools. No problem: Bedrock supports the MySQL protocol, meaning you can continue using whatever MySQL client you prefer: $ mysql -h 127.0.0.1 Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 1 Server version: bedrock 09b08f82e6eefe69f79bb8414882dd64182e3e8c Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> SELECT 1 AS foo, 2 AS bar; +------+------+ | foo | bar | +------+------+ | 1 | 2 | +------+------+ 1 row in set (0.01 sec) mysql> That also means you can continue using whatever MySQL language binding you already know and love. Alternatively, if you dont know or love any of them, Bedrock also provides a PHP binding that looks something like this: $bedrock = new Bedrock(); $result = $bedrock->db->query(\"SELECT 1 AS foo, 2 AS bar;\"); It really can be that easy. Bedrock plugins Additionally, Bedrock::DB is just one plugin to the overall Bedrock platform. Bedrock itself is less a database, and more a tool that can be used to build a wide variety of data-management applications with a database being just one example. Each plugin implements and exposes new externally-visible commands (essentially equivalent to stored procedures). However, unlike simple stored procedures, plugins can also include schema changes. Plugins can be enabled via the -plugins command line parameter. Current plugins include: Status - Provides basic status about the health the Bedrock cluster. DB - Provides direct SQL access to the underlying database. Jobs - Provides a simple job queue. Cache - Provides a simple replicated cache. MySQL - Emulates MySQL How to help and get helped So many ways! Run bedrock -? on the command line to see all the available command-line options Chat with us live on Bedrocks Gitter page Post to the Bedrock mailing list by emailing bedrock@googlegroups.com Create an issue in Bedrocks GitHub issue list Submit a PR to Bedrocks GitHub repo Email David, the CEO of Expensify (and biggest Bedrock fanboy ever) directly: dbarrett@expensify.com Join Expensify and you can work on Bedrock (and other, even cooler things) full time! ",
          "This looks promising. A typical blockchain discussion in my org (kind of averse to change with a few hundred database people, smart, but Microsoft stack) goes like this:\nSomeone: “We need a private blockchain for C use case. We’re about to pay consulting firm X a gazillion dollars to implement EthiHyperBitLedget”\nArchitect: “Silly bird, you just need a distributed database...”\nSomeone:”Fine then how do I get one of those”\nArchitect:”you just hire me and I’ll write a bunch of database stuff for you that is a one off spaghetti mess of sql server stuff”\n75% of the time they buy that blockchain thing\n25% of the time they try to spatchcock the sql thing\nSo far we are 0 for 10 for stuff being useful.<p>If there were a nice distributed database with the features of blockchain I think this could merge my excited Someones with my fuddyduddy Architects.",
          "The \"blockchain-based\" part is misleading.<p>For one, when people say \"blockchain\", they're almost always including the clever protocol that enables a currency with no central authority.  That's the interesting part.  Bedrock doesn't do any of that.<p>Without that, \"blockchain\" is just a simple technique for incremental hashing.  Bedrock uses that, but it's not substantial enough to sensibly list \"blockchain-based\" as one of the three top attributes.<p>(They could have said \"Paxos-based\".  That's the protocol they use to ensure things don't get out of sync.)<p>Just trying to capitalize on the cryptocurrency hype, I guess.  FaunaDB did something similar: <a href=\"https://fauna.com/blog/distributed-ledger-without-the-blockchain\" rel=\"nofollow\">https://fauna.com/blog/distributed-ledger-without-the-blockc...</a>"],
        "story_type":["Normal"],
        "url":"https://bedrockdb.com/",
        "comments.comment_id":[21046442,
          21046501],
        "comments.comment_author":["prepend",
          "cakoose"],
        "comments.comment_descendants":[3,
          5],
        "comments.comment_time":["2019-09-23T06:37:06Z",
          "2019-09-23T06:50:41Z"],
        "comments.comment_text":["This looks promising. A typical blockchain discussion in my org (kind of averse to change with a few hundred database people, smart, but Microsoft stack) goes like this:\nSomeone: “We need a private blockchain for C use case. We’re about to pay consulting firm X a gazillion dollars to implement EthiHyperBitLedget”\nArchitect: “Silly bird, you just need a distributed database...”\nSomeone:”Fine then how do I get one of those”\nArchitect:”you just hire me and I’ll write a bunch of database stuff for you that is a one off spaghetti mess of sql server stuff”\n75% of the time they buy that blockchain thing\n25% of the time they try to spatchcock the sql thing\nSo far we are 0 for 10 for stuff being useful.<p>If there were a nice distributed database with the features of blockchain I think this could merge my excited Someones with my fuddyduddy Architects.",
          "The \"blockchain-based\" part is misleading.<p>For one, when people say \"blockchain\", they're almost always including the clever protocol that enables a currency with no central authority.  That's the interesting part.  Bedrock doesn't do any of that.<p>Without that, \"blockchain\" is just a simple technique for incremental hashing.  Bedrock uses that, but it's not substantial enough to sensibly list \"blockchain-based\" as one of the three top attributes.<p>(They could have said \"Paxos-based\".  That's the protocol they use to ensure things don't get out of sync.)<p>Just trying to capitalize on the cryptocurrency hype, I guess.  FaunaDB did something similar: <a href=\"https://fauna.com/blog/distributed-ledger-without-the-blockchain\" rel=\"nofollow\">https://fauna.com/blog/distributed-ledger-without-the-blockc...</a>"],
        "id":"91e550d7-d9dc-4b44-9d54-bed6095a8247",
        "url_text":"Why Code Install Use Jobs Cache vs MySQL Replication Blockchain Multizone Chat Contact Bedrock Rock-solid distributed data Bedrock is a simple, modular, WAN-replicated, Blockchain-based data foundation for global-scale applications. Taking each of those in turn: Bedrock is simple. This means it exposes the fewest knobs necessary, with appropriate defaults at every layer. Bedrock is modular. This means its functionality is packaged into separate plugins that are decoupled and independently maintainable. Bedrock is WAN-replicated. This means it is designed to endure the myriad real-world problems that occur across slow, unreliable internet connections. Bedrock is Blockchain-based. This means it uses a private blockchain to synchronize and self organize. Bedrock is a data foundation. This means it is not just a simple database that responds to queries, but rather a platform on which data-processing applications (like databases, job queues, caches, etc) can be built. Bedrock is for global-scale applications. This means it is built to be deployed in a geo-redundant fashion spanning many datacenters around the world. Bedrock was built by Expensify, and is a networking and distributed transaction layer built atop SQLite, the fastest, most reliable, and most widely distributed database in the world. Why to use it If youre building a website or other online service, youve got to use something. Why use Bedrock rather than the alternatives? Weve provided a more detailed comparision against MySQL, but in general Bedrock is: Faster. This is true for networked queries using the Bedrock::DB plugin, but especially true for custom plugins you write yourself because SQLite is just a library that operates inside your processs memory space. That means when your plugin queries SQLite, it isnt serializing/deserializing over a network: its directly accessing the RAM of the database itself. This is great in a single node, but if you still want more (because who doesnt?) then install any number of nodes and load-balance reads across all of them. This means every CPU of every database server is available for parallel reads, each of which has direct access to the database RAM. Simpler. This is because Bedrock is written for modern hardware with large SSD-backed RAID drives and generous RAM file caches, and thereby doesnt mess with the zillion hacky tricks the other databases do to eke out high performance on largely obsolete hardware. This results in fewer esoteric knobs, and sane defaults that just work. More reliable. This is because Bedrocks synchronization engine supports active/active distributed transactions with automatic failover, and can be clustered not just inside a single datacenter, but across multiple datacenters spanning the internet. This means Bedrock continues functioning not only if a single node goes down, but even if you lose an entire datacenter. After all, it doesnt matter who you are using: your datacenter will fail, eventually. But you neednt fail along with it. More powerful. Most people dont realize just how powerful SQLite is. Indexes, triggers, foreign key constraints, native JSON support, expression indexes check the full list here. Youll be amazed, but thats just the start. On top of this Bedrock layers a robust plugin system, and includes a fully functional job queue and replicated cache all the basics you need for modern service design, wrapped into one simple package. Bedrock is not only production ready, but actively used by Expensifys many thousands of customers, and millions of users. (Curious why an expense reporting company built their own database? Read what the First Round Review has to say about it.) How to get it Bedrock can be compiled from source using the Expensify/Bedrock public repo, or installed with the commands below: Ubuntu Linux You can build from scratch as follows: # Clone out this repo: git clone https://github.com/Expensify/Bedrock.git # Install some dependencies sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt-get update sudo apt-get install build-essential gcc-9 g++-9 libpcre++-dev zlib1g-dev # Build it cd Bedrock make # Create an empty database (See: https://github.com/Expensify/Bedrock/issues/489) touch bedrock.db # Run it (press Ctrl^C to quit, or use -fork to make it run in the backgroud) ./bedrock # Connect to it in a different terminal using netcat nc localhost 8888 # Type \"Status\" and then enter twice to verify it's working # See here to use the default DB plugin: http://bedrockdb.com/db.html Arch Linux Copy/paste this command into your terminal: This will tansparently download the latest version from GitHub, compile it, package it up, and install it. MacOSX You can build from scratch as follows: # Clone out this repo: git clone https://github.com/Expensify/Bedrock.git # Install some dependencies with Brew (see: https://brew.sh/) brew update brew install gcc@6 # Configure PCRE to use C++17 and compile from source brew uninstall --ignore-dependencies pcre brew edit pcre # Add these to the end of the `system \"./configure\"` command: # \"--enable-cpp\", # \"--enable-pcre64\", # \"CXX=/usr/local/bin/g++-9\", # \"CXXFLAGS=--std=gnu++14\" brew install --build-from-source pcre # Build it cd Bedrock make # Create an empty database (See: https://github.com/Expensify/Bedrock/issues/489) touch bedrock.db # Run it (press Ctrl^C to quit, or use -fork to make it run in the backgroud) ./bedrock # Connect to it in a different terminal using netcat nc localhost 8888 # Type \"Status\" and then enter twice to verify it's working # See here to use the default DB plugin: http://bedrockdb.com/db.html How to use it Bedrock is so easy to use, youll think youre missing something. Once installed, Bedrock listens on localhost port 8888, and stores its database in /var/lib/bedrock. The easiest way to talk with Bedrock is using netcat as follows: $ nc localhost 8888 Query: SELECT 1 AS foo, 2 AS bar; That query can be any SQLite-compatible query including schema changes, foreign key constraints, partial indexes, native JSON expressions, or any of the tremendous amount of functionality SQLite offers. The result will be returned in an HTTP-like response format: 200 OK Content-Length: 16 foo | bar 1 | 2 By default, Bedrock optimizes the output for human consumption. If you are a robot, request JSON output: $ nc localhost 8888 Query query: SELECT 1 AS foo, 2 AS bar; format: json 200 OK Content-Length: 40 {\"headers\":[\"foo\",\"bar\"],\"rows\":[[1,2]]} Some people are creeped out by sockets, and prefer tools. No problem: Bedrock supports the MySQL protocol, meaning you can continue using whatever MySQL client you prefer: $ mysql -h 127.0.0.1 Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 1 Server version: bedrock 09b08f82e6eefe69f79bb8414882dd64182e3e8c Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> SELECT 1 AS foo, 2 AS bar; +------+------+ | foo | bar | +------+------+ | 1 | 2 | +------+------+ 1 row in set (0.01 sec) mysql> That also means you can continue using whatever MySQL language binding you already know and love. Alternatively, if you dont know or love any of them, Bedrock also provides a PHP binding that looks something like this: $bedrock = new Bedrock(); $result = $bedrock->db->query(\"SELECT 1 AS foo, 2 AS bar;\"); It really can be that easy. Bedrock plugins Additionally, Bedrock::DB is just one plugin to the overall Bedrock platform. Bedrock itself is less a database, and more a tool that can be used to build a wide variety of data-management applications with a database being just one example. Each plugin implements and exposes new externally-visible commands (essentially equivalent to stored procedures). However, unlike simple stored procedures, plugins can also include schema changes. Plugins can be enabled via the -plugins command line parameter. Current plugins include: Status - Provides basic status about the health the Bedrock cluster. DB - Provides direct SQL access to the underlying database. Jobs - Provides a simple job queue. Cache - Provides a simple replicated cache. MySQL - Emulates MySQL How to help and get helped So many ways! Run bedrock -? on the command line to see all the available command-line options Chat with us live on Bedrocks Gitter page Post to the Bedrock mailing list by emailing bedrock@googlegroups.com Create an issue in Bedrocks GitHub issue list Submit a PR to Bedrocks GitHub repo Email David, the CEO of Expensify (and biggest Bedrock fanboy ever) directly: dbarrett@expensify.com Join Expensify and you can work on Bedrock (and other, even cooler things) full time! ",
        "_version_":1718939868216688641}]
  }}
